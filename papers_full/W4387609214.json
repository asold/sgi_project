{
    "title": "TraHGR: Transformer for Hand Gesture Recognition via Electromyography",
    "url": "https://openalex.org/W4387609214",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2926963461",
            "name": "Soheil Zabihi",
            "affiliations": [
                "Concordia University"
            ]
        },
        {
            "id": "https://openalex.org/A2511481292",
            "name": "Elahe Rahimian",
            "affiliations": [
                "Concordia University"
            ]
        },
        {
            "id": "https://openalex.org/A2146702413",
            "name": "Amir Asif",
            "affiliations": [
                "York University"
            ]
        },
        {
            "id": "https://openalex.org/A2129602610",
            "name": "Arash Mohammadi",
            "affiliations": [
                "Concordia University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6772310668",
        "https://openalex.org/W2125585124",
        "https://openalex.org/W2129566274",
        "https://openalex.org/W4385562449",
        "https://openalex.org/W6684671651",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2962879438",
        "https://openalex.org/W3011962222",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2123167643",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4308772919",
        "https://openalex.org/W2762706434",
        "https://openalex.org/W2082480549",
        "https://openalex.org/W2886986697",
        "https://openalex.org/W2922311477",
        "https://openalex.org/W2981013175",
        "https://openalex.org/W3151753634",
        "https://openalex.org/W2186425670",
        "https://openalex.org/W2155692114",
        "https://openalex.org/W2979689127",
        "https://openalex.org/W2788541406",
        "https://openalex.org/W3038817036",
        "https://openalex.org/W4220835975",
        "https://openalex.org/W6733544091",
        "https://openalex.org/W2797760407",
        "https://openalex.org/W2997304337",
        "https://openalex.org/W3157280810",
        "https://openalex.org/W2555541061",
        "https://openalex.org/W2019889305",
        "https://openalex.org/W2766273385",
        "https://openalex.org/W2066327120",
        "https://openalex.org/W2898716605",
        "https://openalex.org/W2912302853",
        "https://openalex.org/W2981857663",
        "https://openalex.org/W3180620879",
        "https://openalex.org/W2796589614",
        "https://openalex.org/W3163178735",
        "https://openalex.org/W6749825310",
        "https://openalex.org/W2089884466",
        "https://openalex.org/W3009784468",
        "https://openalex.org/W2807631444",
        "https://openalex.org/W2684229413",
        "https://openalex.org/W2103403570",
        "https://openalex.org/W1984252493",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3038211132",
        "https://openalex.org/W2516710120",
        "https://openalex.org/W3016127821",
        "https://openalex.org/W3004392413",
        "https://openalex.org/W2999738219",
        "https://openalex.org/W2466222407",
        "https://openalex.org/W2883951752",
        "https://openalex.org/W2990110246",
        "https://openalex.org/W3162754028",
        "https://openalex.org/W2775447708",
        "https://openalex.org/W3207526834",
        "https://openalex.org/W4252684946",
        "https://openalex.org/W2169931829",
        "https://openalex.org/W3099835169",
        "https://openalex.org/W2792764867",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2590017328",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4287977455"
    ],
    "abstract": "Deep learning-based Hand Gesture Recognition (HGR) via surface Electromyogram (sEMG) signals have recently shown considerable potential for development of advanced myoelectric-controlled prosthesis. Although deep learning techniques can improve HGR accuracy compared to their classical counterparts, classifying hand movements based on sparse multichannel sEMG signals is still a challenging task. Furthermore, existing deep learning approaches, typically, include only one model as such can hardly extract representative features. In this paper, we aim to address this challenge by capitalizing on the recent advances in hybrid models and transformers. In other words, we propose a hybrid framework based on the transformer architecture, which is a relatively new and revolutionizing deep learning model. The proposed hybrid architecture, referred to as the Transformer for Hand Gesture Recognition (TraHGR), consists of two parallel paths followed by a linear layer that acts as a fusion center to integrate the advantage of each module. We evaluated the proposed architecture TraHGR based on the commonly used second Ninapro dataset, referred to as the DB2. The sEMG signals in the DB2 dataset are measured in real-life conditions from 40 healthy users, each performing 49 gestures. We have conducted an extensive set of experiments to test and validate the proposed TraHGR architecture, and compare its achievable accuracy with several recently proposed HGR classification algorithms over the same dataset. We have also compared the results of the proposed TraHGR architecture with each individual path and demonstrated the distinguishing power of the proposed hybrid architecture. The recognition accuracies of the proposed TraHGR architecture for the window of size 200ms and step size of 100ms are 86.00%, 88.72%, 81.27%, and 93.74%, which are 2.30%, 4.93%, 8.65%, and 4.20% higher than the state-of-the-art performance for DB2 (49 gestures), DB2-B (17 gestures), DB2-C (23 gestures), and DB2-D (9 gestures), respectively.",
    "full_text": "TraHGR: Transformer for Hand Gesture\nRecognition via ElectroMyography\nSoheil Zabihi, Elahe Rahimian, Amir Asif, and Arash Mohammadi\nAbstract—Deep learning-based Hand Gesture Recognition\n(HGR) via surface Electromyogram (sEMG) signals have re-\ncently shown considerable potential for development of advanced\nmyoelectric-controlled prosthesis. Although deep learning tech-\nniques can improve HGR accuracy compared to their classical\ncounterparts, classifying hand movements based on sparse mul-\ntichannel sEMG signals is still a challenging task. Furthermore,\nexisting deep learning approaches, typically, include only one\nmodel as such can hardly extract representative features. In this\npaper, we aim to address this challenge by capitalizing on the\nrecent advances in hybrid models and transformers. In other\nwords, we propose a hybrid framework based on the transformer\narchitecture, which is a relatively new and revolutionizing deep\nlearning model. The proposed hybrid architecture, referred to\nas the Transformer for Hand Gesture Recognition (TraHGR),\nconsists of two parallel paths followed by a linear layer that acts\nas a fusion center to integrate the advantage of each module.\nWe evaluated the proposed architecture TraHGR based on the\ncommonly used second Ninapro dataset, referred to as the DB2.\nThe sEMG signals in the DB2 dataset are measured in real-life\nconditions from 40 healthy users, each performing 49 gestures.\nWe have conducted an extensive set of experiments to test and val-\nidate the proposed TraHGR architecture, and compare its achiev-\nable accuracy with several recently proposed HGR classification\nalgorithms over the same dataset. We have also compared the\nresults of the proposed TraHGR architecture with each individual\npath and demonstrated the distinguishing power of the proposed\nhybrid architecture. The recognition accuracies of the proposed\nTraHGR architecture for the window of size 200ms and step\nsize of 100ms are 86.00%, 88.72%, 81.27%, and 93.74%, which\nare 2.30%, 4.93%, 8.65%, and 4.20% higher than the state-of-\nthe-art performance for DB2 ( 49 gestures), DB2-B ( 17 gestures),\nDB2-C (23 gestures), and DB2-D ( 9 gestures), respectively.\nIndex Terms—Electromyogram (EMG), Deep Neural Networks\n(DNNs), Machine Learning (ML), Transformers, Prosthetic, Clas-\nsification, Hand Gesture.\nI. I NTRODUCTION\nTo improve the quality of life of people with upper limb\namputation, recently, there has been a growing interest in\ndevelopment of learning-based myoelectric prostheses using\nmulti-channel surface Electromyogram (sEMG) signals [1]–\n[3]. The information obtained from the sEMG signals, which\nare related to the neural activities of the underlying muscles, is\nused to decode the movements of the targeted limb. Generally\nspeaking, sEMG signals can be collected via two different\nThis work was supported in part by the Natural Sciences and\nEngineering Research Council (NSERC) of Canada under NSERC Discovery\nGrant RGPIN-2023-05654.\nS. Zabihi is with Electrical and Computer Engineering, Concordia University,\nMontreal, QC, Canada, H3G-2W1 (email: s zab@encs.concordia.ca).\nE. Rahimian and A. Mohammadi are with Concordia Institute\nfor Information System Engineering (CIISE), Concordia University,\nMontreal, QC, Canada, H3G-2W1 (emails: e ahimia@encs.concordia.ca,\narash.mohammadi@concordia.ca). A. Asif is with Electrical Engineering\n& Computer Science, York University, Toronto, Canada, M3J-1P3 (email:\nasif@eecs.yorku.ca).\nrecording techniques, i.e., sparse multi-channel sEMG, and\nHigh-Density sEMG (HD-sEMG). The former, typically, con-\nsists of a limited number of electrodes with sparse placement,\nwhich is the commonly used sEMG recording modality in\nwearable systems [4], [5]. On the other hand, an HD-sEMG\ndevice consists of a grid of electrodes that collect information\nabout the temporal and spatial electrical activities of the\nunderlying muscles enabling them to capture large amounts\nof data [5]–[7]. This, however, results in increased complexity\nof the underlying system, which in turn challenges their ease\nof applicability in wearable systems and adds latency to the\nprocessing pipeline. Recent studies [8], therefore, focus on\nthe use of sparse multi-channel sEMG devices given their\nease of use and reduced complexity [9]. The use of sparse\nmulti-channel sEMG datasets can, however, challenge the\ngesture recognition performance due to its sensitivity to the\nelectrode location. In particular, while Deep Neural Networks\n(DNNs) achieve high performance in gesture recognition using\nHD-EMG devices, their efficacy is limited for sparse multi-\nchannel sEMG devices in which a shallow dataset is collected\nwith a lower sampling rate and limited number of electrodes.\nInspired by the above discussion, as well as the significant\ngap between the performance of existing methods for HD-\nEMG and the sparse multi-channel sEMG approaches, the\npaper aims to present a new hybrid learning framework for\nachieving superior performance using sparse multi-channel\nsignals. Specifically, the primary goal of this study is to\ninvestigate the potential of transformer-based hybrid models\nto achieve high accuracy for hand gesture recognition based\non sEMG signals.\nContributions: In most of the recent studies for sEMG-based\nhand recognition, the existing deep learning approaches in-\nvolve only a single model, which can barely capable to extract\nmore generic representations in the single network structures.\nThe paper addresses this gap by designing a Transformer-\nbased hybrid solution that has great potential for extracting\ntemporal and spatio-temporal representation to improve HGR\naccuracy. In particular, capitalizing on the recent success of\nTransformers in various fields of Machine Learning [10]–\n[13], we aim to examine its applicability and potential for\nsEMG-based hand gesture recognition. The proposed method,\nreferred to as the Transformer for Hand Gesture Recognition\n(TraHGR), increases the accuracy of sEMG decoding for the\nclassification of hand movements.\nThe proposed framework consists of two parallel paths\n(one Temporal transformer Network (TNet) and one Feature\ntransformer Network (FNet)) followed by linear layers, which\nintegrate the output of paths to provide better representation\nfor different numbers of hand movement recognition. The\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nTNet is used to extract temporal features while simultaneously\nthe FNet is utilized to extract spatio-temporal features, which\nare then fused to augment the discriminating power of the\nmodel leading to improvement of the overall performance\nof the HGR classification task. Performance of the proposed\nTraHGR framework is evaluated using the second and fifth\nNinapro database [14]–[17], which are widely used datasets\nproviding sparse multi-channel sEMG signals from diverse\nhand movements similar to those obtained in real-life condi-\ntions. As a result, Ninapro dataset enables development of in-\nnovative DNN-based recognition solutions for the HGR tasks.\nWe have conducted an extensive set of experiments to test and\nvalidate the proposed TraHGR architecture, and compare its\nachievable accuracy with several recently proposed HGR clas-\nsification algorithms based on the same datasets. Results show\nthat the proposed TraHGR outperforms all existing solutions\nover the DB2 dataset and its sub-exercises. In summary, the\ncontributions of the paper are as follows:\n• The paper for the first time, to the best of our knowledge,\ndevelops a hybrid Transformer based architecture for the\ntask of HGR via sparse multi-channel sEMG.\n• The paper demonstrates the superior performance of the\nproposed hybrid architecture TraHGR and its ability to\nextract more distinctive information for gesture recogni-\ntion over the single models; i.e., TNet and FNet.\n• The paper investigates the impact of various window\nsizes, i.e., 200ms, 150ms, and 100ms, on both the overall\nperformance and model complexity.\n• The paper classifies a high number ( 49) of ges-\ntures with a high accuracy. More specifically, com-\npared with the proposed architectures in the recent\nstate-of-the-art studies, TraHGR improves the recog-\nnition accuracies to 86.18% on DB2 ( 49 gestures),\nto 88.91% on the DB2-B ( 17 gestures), to 81.44%\non DB2-C ( 23 gestures), and to 93.84% on the\nDB2-D (9 gestures).\nThe rest of the paper is organized as follows: In Section II, an\noverview of related works is provided. Section III describes the\ndatabase and pre-processing step, we also present details of the\nproposed TraHGR architecture in this section. The experiments\nand results are presented in Section IV. Finally, the conclusion\nis presented in Section V.\nII. R ELATED WORKS\nDeep MyoLearning can be categorized into regression-based\ntechniques and classification-based techniques. Regression-\nbased techniques (typically, via a combination of several\nregressors) have been used to identify and estimate different\nmovements in a continuous space across several degree-of-\nfreedoms (DoFs). Unfortunately, the prediction accuracy of\nsuch techniques is not yet at a level to be used in practical set-\ntings. As an alternative, some researchers such as [18] adopt a\nregression Convolutional Neural Network (CNNs) to estimate\nwrist motions based on sEMG signals. Regression CNNs allow\nfor independent and simultaneous control so that several DoFs\ncan be manipulated concurrently with different magnitudes.\nThis allows for more dextrous and realistic prosthetic motion\nthan low-DoF proportional control or discrete classification\ncontrol. This CNN-based regression technique was validated\nby real-time control tests that demonstrated superior perfor-\nmance compared to SVM-based techniques [18]. In [19],\nthe grasping force levels based on a combination of Principal\nComponent Analysis (PCA) and DNN are predicted for the\ncontrol of a prosthetic hand. More specifically, dimension\nreduction of time domain features are achieved by PCA, and\nthen an sEMG-force regression model is generated by using\nDNN. This is an important step toward improving the grip of\nprosthetic hand based on force prediction.\nGenerally speaking, the developed methods for classifying\nhand movements can be categorized into the following three\nmain categories: (i) Traditional approaches based on classic\nMachine Learning (ML) methods [17], [20]–[23]; (ii) DNN-\nbased techniques [5], [23]–[29], and; (iii) Hybrid methodolo-\ngies [6] that combine multiple models. The existing researches\non prosthetic myoelectric control focus primarily on traditional\nML approaches as a common strategy for HGR task [30]. In\nsuch methods, handcrafted features, in time domain, frequency\ndomain, or time-frequency domain [6], are first extracted by\nhuman experts, which are then fed to a ML classifier. Ex-\ntraction and feature selection, however, can affect the overall\nperformance [31], as such some researchers [5] have explored\nand integrated several classical feature sets that provide multi-\nview of sEMG signals to achieve higher gesture recognition\naccuracy. On the other hand, different classifiers such as Dis-\ncriminant Analysis (LDA), Support Vector Machine (SVM),\nand Random Forests (RF), and Principal Components Analysis\n(PCA) are utilized in the literature [14], [17], [23], [31], [32]\nto increase the discriminating power of the model and improve\ngesture recognition performance.\nAlthough the traditional ML-based approaches have shown\nstrong potential for HGR task, more recently, there has been\na great deal of interest in using deep-learning architectures\nto process multi-channel sEMG signals and increase the dis-\ncrimination power of the model. In particular, it has been\nshown [23] that the automatic feature extraction used in\ndeep learning architecture can lead to higher classification\naccuracy. More specifically, the authors in [23], for the first\ntime, used the Convolutional Neural Network (CNN) archi-\ntecture to classify hand gestures, which showed its potential\nto improve the overall performance compared to existing\ntraditional approaches. This achievement was the starting point\nfor considering CNN as a promising approach in the context\nof sEMG data classification [26], [29], [33]. For example,\nin Reference [7], the authors proposed and used the CNN\narchitecture to extract spatial information from sEMG signals\nand perform HGR classification. In addition to CNN-based\narchitectures, some researches [34], [35] used Recurrent Neu-\nral Networks (RNNs) to extract the temporal features from\nthe sEMG signals. RNNs are used because sEMG signals are\nsequential in the nature, and recurrent-based networks such as\nLong Short Term Memory (LSTM) can extract the patterns\nin a sequence of sEMG data treating HGR as a sequence\nmodeling task. In addition, it has been shown [36] that the\nproper design of CNN architectures can outperform RNNs\nin sequence modeling. In this respect, some researchers [8],\n[27], [28] used temporal convolutions for HGR and showed\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nits potentials to extract temporal features.\nAlternatively, hybrid architectures such as CNN-RNN have\nshown promising results in classifying hand movements [6],\n[25], as they benefit from advantages of different modules\nin extracting temporal and spatial features. Meanwhile, with\nthe advent of the attention mechanism [10], transformers are\nbeing considered as a new ML technique for sequential data\nmodeling [37], [38]. Capitalizing on the recent success of\ntransformers in various fields such as machine translation [11],\n[39], speech recognition [40] and computer vision [12], we\naim to examine its applicability and potentials for sEMG-\nbased gesture recognition. In other words, we recognized an\nurgent need to develop a transformer-based hybrid architecture\nto augment the recognition accuracy of HGR. In this paper,\nwe introduce the TraHGR architecture, which increases the\naccuracy of sEMG decoding in the classification of human\nhand movements. In addition, we examine the complexity and\nperformance of different types of TraHGR architectures.\nWe would like to point out that one inherent problem in\nthe sEMG-based hand gesture recognition task is the time-\nand user-dependent nature of the sEMG signal [41]. In other\nwords, due to physiological differences in muscle activities\nfrom one user to another, a pre-trained model on existing users\n(source) cannot be expected to perform well on a new user\n(target) [42], [43]. In addition, various electrophysiological and\nuser-related variables can affect sEMG signals. These include\nmuscle fatigue [44], changes in electrode-skin impedance due\nto perspiration/humidity [45], electrode displacement [46], and\nuser-related issues such as variations in contraction intensity,\nhand orientations, and arm postures [47]. As a result of these\nchanges, the accuracy of a pre-trained model on source data\nmay degrade when testing on the target user due to the\ndomain shift. To this end, domain adaptation methods are\nhighly recommended in this field of study, where learning\nalgorithms involve some techniques to transfer information\nfrom the source to the target domain despite the existence\nof a distribution mismatch among them. As a result, Transfer\nLearning (TL) techniques in upper-limb hand motion estimates\nhave received a lot of attention in recent years [48]–[53].\nFurthermore, in [8], as a domain adaptation methodology,\na novel Few-Shot learning method is introduced with the\nobjective of inferring the desired output based on just one\nor a few observations from the target domain, resulting\nin a promising performance for unseen user hand gesture\nrecognition. However, in this paper, the primary goal is to\ninvestigate the potential of a transformer-based hybrid solution\nfor offline hand gesture recognition based on sEMG signals.\nTherefore, following the previous studies [5], [6], [14], [23],\n[32], [33], training of the TraHGR network is conducted in a\nsupervised fashion for each user independently. Although we\nconducted an experiment to investigate the impact of a simple\nTL technique on the performance of the proposed model,\ninvestigating domain adaptation techniques is not the primary\ngoal of the study. Moreover, as mentioned earlier, we have an\noffline setting in this study. From offline gesture recognition\nin experiment settings to the real-time clinical environment,\nthe reported accuracy might drop [54]. These may be seen as\nboth a constraint for the current study and a promising area\nfor future work.\nIII. M ATERIAL AND METHODS\nA. Database\nThe proposed TraHGR architecture is evaluated on the\nsecond Ninapro database [14]–[16], which is a publicly avail-\nable dataset for sEMG-based human-machine interfacing. The\nsecond database Ninapro, which is referred to as the DB2, was\ncollected from 40 users. Each user performs 49 movements in\nwhich each movement is repeated 6 times, each time lasting\nfor 5 seconds, followed by 3 seconds of rest. The sEMG\nsignals were gathered using Delsys Trigno Wireless EMG\nsystem with 12 wireless electrodes, sampled at 2 kHz. The\nDB2 dataset was presented in three exercises B, C, and D,\nwhich consist of different types of movements. In particular,\nExercise B, C, and D consists of 17, 23, and 9 movements,\nrespectively. For the rest of this paper, Exercise B, C, and D are\nreferenced to DB2-B ( 17 gestures), DB2-C ( 23 gestures), and\nDB2-D (9 gestures), respectively. Following the recommen-\ndations provided by Ninapro benchmark datasets [14]–[16],\nwe consider the repetitions 2 and 5 of each movement as the\nholdout test set, and the remaining four repetitions, i.e., 1, 3, 4,\nand 6, as the training set. However, since the benchmark does\nnot clearly specify the validation set, the first, second, third,\nand fourth quarters of repetitions 1, 3, 4, and 6, respectively,\nare used in this study as the validation set.\nB. Pre-processing Step\nThe EMG signals are pre-processed for classification pur-\nposes before being fed into the proposed architecture. The\npre-processing phase consists of three steps, i.e., low-pass\nfiltering, normalization, and segmentation. More specifically,\nwe followed the procedure outlined in previous studies [7],\n[23], [55] and used the low-pass Butterworth filter. To enhance\nthe performance of the proposed architecture, we applied\nthe low-pass filter three times with different order of filters,\nnamely 1, 3, and 5, and then concatenated all filtered signals\ntogether to form three-channel sEMG signals. Then, for the\nnormalization step, we are inspired by the µ-law normalization\ntechnique introduced in [8], [26], which is defined as follows\nF(xt) =sign(xt)ln\n\u0000\n1 +µ|xt|\n\u0001\nln\n\u0000\n1 +µ\n\u0001 , (1)\nwhere the input scaler is represented by xt, and the new range\nof the signal is indicated by parameter µ. After normalization,\nthe sEMG signals are segmented with a sliding window. Each\ninput from the sEMG signal segmentation phase is denoted\nby X ∈ RS×W×C, where S shows the number of sensors in\nthe DB2 dataset, W shows the number of samples of electrical\nactivities of muscles obtained at the rate of 2 kHz for a window\nof 200ms, 150ms, or 100ms, and C denotes the number of\nchannels of the sEMG signals.\nThe acceptable amount of delay in sEMG-based models can\nvary depending on the specific application and user require-\nments. While earlier literature has traditionally considered a\n300ms threshold as an acceptable delay [56], recent research\nhas suggested that the optimal controller delay should ideally\nbe below 175ms, with preferred values even below 125ms [57].\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nThese values are based on systematic investigations into the\noptimal delay for improved controller performance. However,\nachieving low delays in sEMG-based models can be chal-\nlenging due to the processing and computational requirements\ninvolved. As a result, researchers have explored techniques\nsuch as overlapping windows to mitigate the delay. In this\nstudy, we considered window sizes of 150ms and 100ms in\naddition to the commonly used 200ms window size to ensure\nfair comparisons. The step size of the sliding window is set to\n10ms for all experiments, results for a step size of 100ms are\nalso provided in TABLE VI, allowing for a fair comparison\nwith other studies.\nC. Overview of the TraHGR Architecture\nThe TraHGR architectural design is based on transform-\ners in which the attention mechanism is the main building\nblock. The attention mechanism has been used in previous\nstudies [6], [8], [27] along with CNNs and/or recurrent-\nbased architectures for HGR task. However, in this paper,\nwe show that transformer-based architectures that rely solely\non attention mechanisms can perform better than previous\nstudies in which hybrid architectures (e.g., attention integrated\nwith CNN or RNN) have been adopted. The overall proposed\narchitecture is illustrated in Fig. 1, which is inspired by the\nVision Transformer (ViT) [12], in which each input is divided\ninto patches, and the network is supposed to perform label\nprediction based on the sequence of these patches. As shown\nin Fig. 1, the proposed TraHGR consists of a TNet path\nimplemented in parallel with a FNet path followed by a linear\nlayer, which acts as the fusion centre combining the extracted\nfeatures from each of the two parallel paths in order to classify\nthe hand gestures. In the following subsections, we will further\nelaborate on the details of the proposed architecture.\nD. Embedded Patches\nIn this sub-section, we focus on the input of the trans-\nformer encoder, which is a sequence of embedded patches.\nAs illustrated in Fig. 1, the embedded patches are constructed\nfrom patch embeddings and position embeddings, which are\ndescribed below.\n1) Patch Embeddings: As mentioned earlier, we split each\nsegment of sEMG signals X into non-overlapping patches\nXp = {xi\np}N\ni=1. More specifically, each segment X ∈\nRS×W×C is divided into N non-overlapping patches in which\neach patch is flattened. We represented the sequence of these\nflatten patches with Xp ∈ RN×(P1.P2.C), where C denotes the\nnumber of channels, ( P1, P2) shows the size of each patch,\nand N = S.W/P1.P2 represents the length of this sequence,\ni.e., the number of patches. As shown in Fig. 1, we applied\ntwo types of patching:\n• Temporal Patching:Here, the size of each patch is (1, W);\ntherefore, the number of patches is N = S. This type of\npatching is called Temporal Patching because each patch\ncontains the information from only one of the sensors in\nthe dataset for a sequence with length W. The TNet path\nis designed in such a way that they can take into account\nthe temporal patches as the input.\n• Featural Patching: We set the size of each patch to ( S,\nS), i.e., P1 = P2 = S, therefore, the number of patches\nis N = W/S. We refer to this type of patching as\nFeatural because each patch contains the information of\nall S sensors for a sequence with length of S. Therefore,\nboth spatial and temporal information are included in a\nFeatural patch. The Featural patches are provided as the\ninput only to the FNet layer as shown in Fig. 1.\nFinally, a linear mapping is introduced to create the embed-\nding for each of these patches (Fig. 1). More specifically, a\nmatrix E ∈ R(P1.P2.C)×D is shared among different patches\nto linearly project each patch into the model dimension D\n(Eq. (2)). The output of this projection is known as the Patch\nEmbeddings.\n2) Class Token: Similar to the Bert framework [11], a\ntrainable embedding is prepended to the sequence of patch em-\nbeddings (Z0\n0 = xclass) with the goal of capturing the meaning\nof the entire segmented input as a whole. More specifically,\nthe class token’s embedding after the last transformer encoder\nlayer (Z0\nL) is used for classification purposes (Eq. (10)).\n3) Position Embeddings: As HGR based on sEMG signals\nis a time-series processing task, the order of data is an essential\npart for sequence modeling. Recurrent-based architectures\nsuch as LSTM inherently consider signal order, however,\ntransformers do not process the input sequentially and combine\nthe information of all the elements through attention mecha-\nnism. Therefore, there is a need to encode the order of each\nelement in the sequence. This is where positional embedding\ncomes in. In fact, position embedding allows the network\nto determine where a particular patch came from. There are\nseveral ways to retain position information at the transformer\ninput, e.g., Sinusoidal positional embedding, 1-dimensional\npositional embedding, 2-dimensional positional, and Relative\npositional embeddings embedding [12], [40]. Following [12],\nwe used the standard trainable 1-dimensional positional em-\nbeddings. As shown in Fig. 1, position embeddings indicated\nby Epos ∈ R(N+1)×D is added to the patch embeddings. The\nformulation which governs patch and position embeddings is\nas follows\nZ0 = [xclass; x1\npE; x2\npE; . . .; xN\np E] +Epos. (2)\nThe output of Eq. (2) is called Embedded Patches, which are\nfed as an the input to the transformer encoder.\nE. Transformer Encoder\nThe transformer encoder takes the Z0 as an input. This block\nis inspired from the main transformer encoder introduced\nin [10], which treats all embedded patches as tokens. As\nillustrated in Fig. 1, the transformer encoder consists of L lay-\ners. Each layer contains two modules, namely the Multihead\nSelf-Attention (MSA) and a Multi-Layer Perceptron (MLP)\nmodule, i.e.,\nZ\n′\nl = MSA(LayerNorm(Zl−1)) +Zl−1, l = 1. . . L (3)\nZl = MLP(LayerNorm(Z\n′\nl)) +Z\n′\nl, l = 1. . . L(4)\nIt is worth noting that a layer-normalization [58] is used before\nMSA and MLP modules, and the residual connections are\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nN\n2\n3\nN-1\nN\n+ 1\n+\n+\nTransformer Encoder\n+\nLinear Linear\ny\nLinear Projection of Flatten  \nPatches\n1\nN-2\nN-1\n        D\nyTNet\nLinear\nClass token 0\nTemporal Patching\nFeatural Patching\nLinear Projection of Flatten  \nPatches\nyFNet\nFNet \n+\n+\n+\n+\n+\n+\nTransformer Encoder\n+Class token\nPatch EmbeddingPosition Embedding\n+\nMulti-Layer Perceptron\n(MLP)\nMultihead Self-\nAttention (MSA)\nL x\nLayerNorm\n+\nLayerNorm\n+\nTransformer Encoder\nScaled Dot-Product \n Attention\nLinear\nScaled Dot-Product \n Attention\nLinear LinearLinearLinear LinearLinearLinear\nScaled Dot-Product \n Attention\nK Q V\nConcat\nLinear\nLinear\nh\nMultihead Self-Attention (MSA)\nMatMul\nScale\nSoftMax\nMatMul\n \n \n \n \n \n \n \n \nK Q V\nScaled Dot-Product Attention\nEmbedded Patches\n0TNet \nFig. 1: The proposed TraHGR architecture consists of two parallel paths (one TNet and one FNet). Each segment of sEMG signals X\nis divided into N non-overlapping patches. The TraHGR uses the TNet path to get the temporal patches while simultaneously the FNet is\nutilized to consider the featural patches. In both TNet and FNet, the patches are mapped linearly into the model dimension D. We refer\nto the output of this step as “Patch Embedding”. Then, a “class token” is prepended to the sequence of patch embeddings which is finally\nused for the classification purpose. The “Positional Embedding” is added to the “Patch Embedding” to retain the positional information.\nThe output of this step is called “Embedded Patches” and is fed to the Transformer encoder consisting of L layers, each layer consisting of\nMSA and MLP modules. Finally, we add the output of the TNet and FNet class tokens to get the final representation, which then acts as\nthe input to the linear layer.\napplied to address degradation problem. The MLP module\nconsists of two linear layers in which the first layer is followed\nby Gaussian Error Linear Unit (GELU) activation function.\nMoreover, the MSA module is defined based on the Self-\nAttention (SA) mechanism, which is discussed next.\n1) Self-Attention (SA): The SA mechanism [10] measures\nthe pairwise similarity of each query and all the keys and\nobtains a weight for each value. Finally, the output is computed\nbased on the weighted sum over of all values. In particular, if\nwe define an input Z ∈ RN×D consisting of N vectors, each\nof length D, the three matrices, i.e., Queries Q, Keys K, and\nValues V, are calculated as follows\n[Q, K, V] =ZWQKV , (5)\nwhere WQKV ∈ RD×3Dh denotes the trainable weight matrix\nand Dh shows the length of each vector in Q, K, and V. To\nmeasure the weights for V, the dot-product of Q and K is\ncalculated, then scaled with √Dh. These weights are converted\nto the probabilities P ∈ RN×N using the softmax function as\nfollows\nP = softmax(QKT\n√Dh\n). (6)\nFinally, he output of SA mechanism is computed as follows\nSA(Z) =PV. (7)\nBy using the attention mechanism, the model pinpoints a\nspecific information in the input sequence.\n2) Multihead Self-Attention (MSA): Here, the SA mecha-\nnism is used for h times in parallel, allowing the architecture\nto pinpoint specific pieces of information in the input sequence\nfor each head differently. In particular, each head has its\nown trainable weight matrix. The final matrix in the MSA\nmechanism is a projection of the concatenated outputs of the\nh heads, which is formulated as follows\nMSA (Z) = [SA1(Z); SA2(Z); . . .; SAh(Z)]WMSA , (8)\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nTABLE I: The number of parameters in different variants of TNet, FNet, and TraHGR architectures with respect to the number of layers,\nmodel dimension (D), and the number of heads ( h) and MLP size in Transformer Encoder. The number of parameters (#Params) is reported\nfor window sizes 200ms, 150ms, and 100ms.\nModel #Layers ( L) Model dimension ( D)\nMLP size\n#Heads (h)\n#Params\n200ms 150ms 100ms 200ms 150ms 100ms\nTraHGR-Base 1 32 128 4 83,731 74,259 63,603\nTraHGR-Large 2 64 256 4 316,051 297,107 275,795\nTraHGR-Huge 1 144 720 8 846,579 803,955 756,003\nTNet 1 144 720 8 472,513 431,041 384,385\nFNet 1 144 720 8 366,673 365,521 364,225\nTNet-Huge 1 200 1084 1120 1162 8 846,733 803,569 756,611\nFNet-Huge 1 224 1176 1085 1102 8 846,377 803,726 756,247\nTABLE II: Comparing different variants of TraHGR. The average\naccuracy of hand gesture recognition across all subjects in the\nDB2 (49 gestures) dataset for different variants of TraHGR archi-\ntecture on several window sizes ( 200ms, 150ms, and 100ms).\nModel\nAccuracy ± STD\n200ms 150ms 100ms\nTraHGR-Base 78.60 ± 6.03 77 .54 ± 5.99 76 .17 ± 6.09\nTraHGR-Large 83.58 ± 5.48 82 .58 ± 5.60 81 .30 ± 5.87\nTraHGR-Huge 86.18 ± 4.99 85.43 ± 5.24 84.13 ± 5.21\nwhere WMSA ∈ Rh.Dh×D. Here, Dh is set to D/h to keep\nthe number of parameters constant when h changes.\nF . TraHGR’s Output\nAs shown in Fig. 1, the TraHGR consists of two paths,\ni.e., TNet and FNet. For each path, the aforementioned cal-\nculations (Eqs. (2)-(8)) are performed in parallel. Then, the\npredicted class labels of each path is calculated based on its\ncorresponding Z0\nL as follows\nypath = Linear(LayerNorm(Z0\nL)path), (9)\nwhere path ∈ {TNet, FNet}. Finally, the output of the TraHGR\nis calculated based on the sum of Z0\nL in the TNet and FNet\nas follows\ny =Linear(LayerNorm[(Z0\nL)TNet + (Z0\nL)FNet]). (10)\nIt is worth mentioning that yTNet, yFNet, and y are used for\nTraHGR training. More details are provided in the subsec-\ntion III-G. This completes description of the proposed TraHGR\narchitecture, next, its performance is evaluated through several\nexperiments.\nG. Loss Function\nThe loss function L of the TraHGR consists of the following\nthree components\nL = LTNet + LFNet + LTraHGR, (11)\nTABLE III: Comparison of architectures with the same structure.\nThe average accuracy of hand gesture recognition across all subjects\nin the DB2 ( 49 gestures) dataset for TNet, FNet, and TraHGR-Huge\narchitectures on several window sizes ( 200ms, 150ms, and 100ms).\nAs shown in TABLE I, the network structure in TNet and FNet is\nnot changed compared to the TraHGR-Huge structure.\nModel\nAccuracy ± STD\n200ms 150ms 100ms\nTraHGR-Huge 86.18 ± 4.99 85.43 ± 5.24 84.13 ± 5.21\nTNet 83.39 ± 5.44 82 .81 ± 5.60 81 .43 ± 5.88\nFNet 80.72 ± 5.82 80 .05 ± 6.03 79 .38 ± 6.15\nTABLE IV: Comparison of architectures with the same scale. The\naverage accuracy of hand gesture recognition across all subjects\nin the DB2 ( 49 gestures) dataset for TNet-Huge, FNet-Huge, and\nTraHGR-Huge architectures on several window sizes (200ms, 150ms,\nand 100ms). As shown in TABLE I, the number of parameters in\nTNet-Huge and FNet-Huge is on the same scale as TraHGR-Huge.\nModel\nAccuracy ± STD\n200ms 150ms 100ms\nTraHGR-Huge 86.18 ± 4.99 85.43 ± 5.24 84.13 ± 5.21\nTNet-Huge 83.80 ± 5.78 83 .25 ± 5.34 82 .21 ± 5.45\nFNet-Huge 81.10 ± 5.68 80 .44 ± 5.48 79 .94 ± 5.83\nwhere the first term LTNet is loss of the TNet path in the pro-\nposed TraHGR architecture. More specifically, cross-entropy\nloss is considered for measuring classification performance\nusing the TNet’s output yTNet (Eq. (9)) and the target values.\nSimilarly, the second term LFNet is the cross-entropy loss com-\nputed using the second path (FNet) of the TraHGR architecture\nwhere FNet’s outputs yFNet (Eq. (9)) are considered. Finally,\nthe last term LTraHGR is calculated using the TraHGR’s output\ny (Eq. (10)).\nH. Different Variants of TraHGR\nTABLE I shows different configurations of the hyperparam-\neters in the TraHGR architecture resulting in different vari-\nants of the model denoted by TraHGR-Base, TraHGR-Large,\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nand TraHGR-Huge. The introduction of different variants\nof the model, namely TraHGR-Base, TraHGR-Large, and\nTraHGR-Huge, in our study, serves the purpose of exploring\nthe impact of various hyperparameter configurations on the\nperformance of the model. By investigating these variants,\nwe can gain insights into how different choices of hyperpa-\nrameters, such as the number of layers ( l), model dimension\n(D), MLP size, and the number of heads ( h), influence the\naccuracy of the model. This analysis allows us to understand\nthe trade-off between model complexity and performance.\nMoreover, the single deep networks TNet and FNet and their\ncorresponding huge version were designed and trained as\nindividual models. The goal of designing and training these\nsingle networks is to evaluate their performance and compare it\nagainst the performance of the hybrid models in order to assess\nthe effectiveness of the TraHGR architecture in achieving\nbetter results. More precisely, the comparison provides insights\ninto the effectiveness and advantages of the proposed hybrid\napproach for representative feature extraction and classifica-\ntion tasks. In TABLE I, the number of parameters (Params)\nis calculated for DB2 ( 49 gestures) while this number will\nbe less for DB2-B ( 17 gestures), DB2-C ( 23 gestures), and\nDB2-D (9 gestures).\nIV. E XPERIMENTS AND RESULTS\nIn this section, we evaluate performance of the proposed\nTraHGR architecture through a series of experiments. In all\nexperiments, the Adam optimizer [59] was used with learning\nrate of 0.0001 and the weight decay of 0.001. Moreover, the\nbatch size is set to 512. Different variants of TraHGR, TNet,\nand FNet, provided in TABLE I, are used for training and eval-\nuation purposes with window size ∈ {200ms,150ms,100ms}.\nA. Evaluation of the Proposed TraHGR Architecture\nThis subsection provides evaluations on the prediction per-\nformance of the proposed hybrid transformer-based architec-\nture. In this regard, first, we compare different variants of\nthe TraHGR architecture and show the effect of different\nhyperparameters (e.g., number of layers, model dimension,\nMLP size, and number of heads) on the overall accuracy. Then,\nto demonstrate the performance of the hybrid transformer,\nwe also compare the TraHGR architecture with single deep\nmodels, i.e., TNet and FNet and their Huge version.\nTABLE II, III, and IV show HGR recognition accuracy,\nwhich is averaged over all subjects for the test set. From\nTABLE II, it can be observed that the proposed TraHGR-Huge\narchitecture outperformed other TraHGR architecture variants\n(TraHGR-Base and TraHGR-Large) when evaluated based on\nthe DB2 ( 49 gestures) for the same window size. However,\nas shown in TABLE I, the number of parameters of the\nTraHGR-Huge is much higher than that of the TraHGR-Base\nand TraHGR-Large models. This fact indicates that increasing\nthe number of layers ( l), model dimension ( D), MLP size,\nand number of heads ( h) have a positive effect on the model’s\naccuracy, however, this comes with the cost of increasing the\ncomplexity. In addition, as shown in TABLE I, each model\nhas a larger number of trainable parameters for window size\n200ms than its counterpart in the window size of 150ms or\n100ms, resulting in higher complexity. However, as shown in\nTABLE II, a larger window size can further improve the results\nbecause the model has access to a longer sequence length.\nB. TraHGR Hybrid Architecture Versus TNet and FNet\nWe independently trained and evaluated the proposed\nmodel on DB2 subsets, i.e., DB2-B ( 17 gestures),\nDB2-C (23 gestures), and DB2-D ( 9 gestures). In Fig. 2,\nthe performance of the proposed architectures for\nDB2 (49 gestures) and its three sub exercises, i.e., B,\nC, and D are shown. It can be observed that for both\nwindow sizes of 200ms and 150ms achieving a high accuracy\nfor DB2-C is more challenging than DB2-B and DB2-D\nsubsets. More specifically, DB2-C consists of 23 grasping\nand functional movements for which everyday objects like\nbottle and knife are presented to the user for grasping, in\norder to mimic daily-life actions such as opening a bottle or\ncutting something [14]. Therefore, the performance reduction\nin the DB2-C subset is not far from expectation as the muscle\ngroups which are predominantly used during movements\nof DB2-C are more complicated than basic hand posture\nand wrist movements in DB2-B and finger force patterns in\nDB2-D subsets.\nAs shown in TABLE I, when comparing the number of\ntrainable parameters in TNet and FNet against the different\nvariants of proposed TraHGR architectures, TraHGR-Large,\nalthough smaller, has the closest number of trainable parame-\nters to these single networks. More precisely, for window sizes\n200ms and 150ms, TNet has approximately 1.5× more pa-\nrameters than TraHGR-Large, and FNet is almost 1.2× larger.\nFor the window size of 100ms, both single networks have\nalmost 1.3× more parameters than TraHGR-Large. However,\nas shown in TABLE II and III, TraHGR-Large has comparable\nperformance to TNet, and it outperforms FNet, showing that a\nhybrid model with fewer number of parameters is capable to\nextract more generic representations resulting in comparable or\neven better performance compared to larger single networks,\nTNet and FNet. According to TABLE I, the network structure\nin TNet and FNet is completely different than TraHGR-Large.\nTherefore, we conducted a new experiment in which the\nstructure of the single and hybrid networks remained un-\nchanged. To do so, we can compare the performance of\nTraHGR-Huge against the TNet and FNet (see TABLE I).\nAs shown in TABLE III, the TraHGR-Huge outperforms the\nsingle deep models (TNet and FNet) when the structure of\nthe networks is preserved. However, since the number of\ntrainable parameters is TraHGR-Huge is considerably larger\nthan TNet and FNet, the performance improvement could be\nconducted due to the TraHGR-Huge capacity to represent\nmore complex hypothesis space. As a result, we conducted\nnew experiments in which the number of parameters for new\nvariants of TNet and FNet architectures is expanded to be\non the same scale as TraHGR-Huge. Specifically, to increase\nthe number of parameters in new variants of TNet and FNet,\nwe began by increasing model dimension D and stopped just\nbefore exceeding the number of parameters in TraHGR-Huge.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nFig. 2: The performance of TNet, FNet and the different variant TraHGR on DB2 ( 49 gestures), as well as its sub-experiments including\nDB2-B (17 gestures), DB2-C ( 23 gestures), and DB2-D ( 9 gestures).\nThen, the size of the MLP layer in the transformer encoder\nis enlarged to fill the remaining gap in terms of the number\nof parameters as much as possible, resulting in TNet-Huge\nand FNet-Huge architectures. Detailed information about the\nstructure of different variants of these single networks and\ntheir number of parameters are provided in TABLE I. As\nshown in TABLE IV, TraHGR-Huge significantly outperforms\nTNet-Huge and FNet-Huge architectures while they are all in\nthe same scale.\nAs shown in TABLE III and IV, although the number of\ntrainable parameters in TNet-Huge and FNet-Huge are signif-\nicantly increased compared to TNet and FNet, their average\nrecognition accuracy improvement for different window sizes\nis not significant. As a result, since the single networks were\nnot capable to achieve high performance even with massive\nparameters expansion and given the outstanding performance\nof TraHGR-Huge architecture, it can be concluded that the\nhybrid approach integrates the advantages of two parallel paths\nto model better and more generic representation resulting in\nperformance improvement. It is worth mentioning that for\nhybrid models such as TraHGR-Base, TraHGR-Large, and\nTraHGR-Huge, the classification accuracy is calculated using\nthe output of Eq. (10), while for single deep models such as\nTNet and FNet this number is computed using the output of\nEq. (9).\nC. Computation Time\nThe window size of 200ms is a well-established value\nin previous works [5], [6], [14], [23], [32], [33], [60], [61]\nin sEMG-based hand gesture recognition. Following these\nstudies, most of the experiments conducted in this study are\nperformed in windows of size 200 ms, which lets us perform\na fair comparison with prior works. However, we trained and\nevaluated two other window sizes; i.e. 100 ms and 150 ms\nfor our proposed architecture to meet the requirements of the\nreal-time threshold of up to 175ms mentioned in [57], and\nto study the impact of input window size in the TraHGR\narchitecture. Comparing outcomes in each row of TABLE II\nshows that increasing window size led to better performance\nfor all TraHGR model variants. The average test time for each\nsample for different window sizes for different variants of\nTraHGR is summarized in TABLE V. It is noteworthy to say\nthat the processing time depends on the hardware. In this work,\nwe used a “Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz” to\nget the average computation time of the samples. We used\nCPU instead of running the models in GPU in other to have\na better representation of differences in computation time for\nTraHGR variants in TABLE V. As shown in TABLE V, the\ncomputation time compared to the sizes of the window is\nnegligible. Therefore, the data collection time (window size)\nis the major factor contributing to the output lag in peripheral\nhuman-machine intelligence systems for the task at hand.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nTABLE V: Comparing average computation time of different vari-\nants of TraHGR for hand gesture recognition across test sets of all\nsubjects in the DB2 ( 49 gestures) dataset for different variants of\nTraHGR architecture on several window sizes ( 200ms, 150ms, and\n100ms).\nModel\nAverage Time (ms) Per-Sample\n200ms 150ms 100ms\nTraHGR-Base 0.113 0 .083 0 .042\nTraHGR-Large 0.207 0 .123 0 .072\nTraHGR-Huge 0.241 0 .150 0 .095\nD. Statistical Analysis\nFollowing [8], [62], we considered each user as a sepa-\nrate dataset and conduct Wilcoxon signed-rank test [63] and\nBonferroni correction to ensure the reliability and validity of\nour results. To do so, given that we have 40 users, for each\nmodel we will have 40 accuracies resulting from each user’s\ntest set. Having accuracies for each model, we performed\nstatistical analysis on the effectiveness of the observations for\nDB2 (49 gestures) For the window size of 200ms. According\nto the results shown in Fig. 3, the difference in accuracy\nbetween TraHGR-Huge and other proposed architectures such\nas TraHGR-Base, TraHGR-Large, TNet, and FNet, for window\nsizes 200ms were considered statistically significant by the\nWilcoxon signed-rank. In Fig. 3, the p-value of significance is\nconsidered 0.05 and the annotated ∗ mark represents p ≤ 0.05.\nWorth to mention that, in Fig. 3, the adjusted p-values resulting\nfrom the Bonferroni correction are below the threshold of\n10−4 (i.e., p ≤ 10−4) for statistically significant comparisons.\nFurthermore, Figure 3 provides a visual representation of\nthe performance distribution across 40 users for each of the\nproposed models. Each boxplot shows the Interquartile Range\n(IQR), which presents the performance of each model for all\nusers into quartiles. More specifically, the upper and lower\nwhiskers show the 75th and 25th percentiles. In a sense that,\nin each boxplot, the achieved accuracy for 25% of the users,\ni.e., 10 users, are in the range defined by the lower whisker and\nthe other 25% of the users have accuracy in the range defined\nby the upper whisker. The horizontal lines at the beginning of\nthe lower whisker and the end of the upper whisker indicate\nthe models’ minimum and maximum accuracies, respectively.\nFinally, the boxplot covers the range of accuracy for 50%\nof the users. The horizontal line in each boxplot illustrates\nthe median performance. In a sense that the accuracy of 25%\nof users falls into the bottom portion of the box, while the\nother 25% of users fall into the higher part of the box. As\nshown in Fig. 3, The boxplot corresponding to TraHGR-Huge\ncompared to other counterparts is shifted up. In other words,\nTraHGR-Huge has improved the performance of all users.\nFurthermore, when comparing different TraHGR variations,\nit is clear that increasing the number of parameters led to an\nincrease in accuracy due to the models’ enhanced capacity to\nextract more generic representations. However, increasing the\nnumber of parameters does not have a significant improvement\non the TNet and FNet as shown in the Fig. 3, “ns” stands for\nnot significant, i.e., 0.05 < p-value ≤ 1.\nTo evaluate the robustness of the proposed approach, we\nFig. 3: The accuracy boxplots for all TraHGR architecture variants,\nTNet, and FNet for all 49 gestures in Ninapro DB2 dataset. The IQR\nof each model is shown by a boxplot for all users. The Wilcoxon\nsigned-rank test followed by Bonferroni correction is used to compare\nthe TraHGR-Huge with other architectures, and different variants of\nTNet and FNet. p-value is annotated by the following markers: (i)\n0.05 < p-value ≤ 1 is marked as not significant (ns); (ii) p-value ≤\n0.05 is depicted with ∗. Worth to mention that, the adjusted p-values\nresulting from the Bonferroni correction are below the threshold of\n10−4 (i.e., p ≤ 10−4) for statistically significant comparisons.\nconducted 100 Monte Carlo (MC) runs, where sensor mea-\nsurements were intentionally contaminated with additive Gaus-\nsian noise at predefined signal-to-noise ratio (SNR) levels.\nSpecifically, in each MC run, we introduced noise based on\nan SNR of 25dB. The MC simulation results, obtained by\naveraging the accuracy over the 100 runs, showed that the\nproposed TraHGR-Huge achieved an accuracy of 85.68% with\na standard deviation of 5.32%. Furthermore, we compared\nthese results to the accuracy obtained without MC simulation,\nwhich yielded a slightly higher accuracy of 86.18% with a\nstandard deviation of 4.99% (as shown in TABLE VI). These\nfindings highlight the consistent and stable performance of the\nproposed model, reinforcing its robustness in the presence of\nnoise.\nE. Position-Wise Cosine Similarity\nAs illustrated in the proposed TraHGR architecture in Fig. 1,\neach patch in the in TNet only consists of the temporal\ninformation of one sensor for the length of window size\n(e.g., 200ms, 150ms). As a result, the positional embeddings\nrepresent their associated sensors. Therefore, as shown in\nFig. 4, the position-wise cosine similarity of the positional\nembedding vectors in the TNet captures the mutual corre-\nlation/entanglement of the sensors in the hand movements.\nAs depicted in Fig. 4, the sensory information is highly\ncorrelated for the TraHGR-Base as the smallest network, when\nthe network gets larger (left to right) and the sequence length\ngets longer (down to up), the network’s capacity to cherry-pick\nthe sensors to associate is increased.\nOn the other hand, each patch in FNet consists of both\ntemporal and spatial information. As illustrated in Fig. 1,\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nFig. 4: Position embedding similarities for TNet path in TraHGR-Base, TraHGR-Large, and TraHGR-Huge architectures: (a) window size\nis 200ms, and (b) window size is 150ms. Each row in each figure represents the cosine similarity between one embedding position and all\nthe other embeddings. The brightness of the pixels in the figures indicates more similarity.\nunlike the patching mechanism in TNet, there is temporal\ninformation flow from one path to another in the FNet patch-\ning mechanism which makes the order of the sequence of\npatches/information important. These sequential correlations\nof the patches are expected to be deduced by the FNet.\nThe optimal similarity should result in a matrix with bright\ncolors on the main diagonal and its neighbors. In a sense that\nconsecutive positions are required to be more similar/brighter\nto reflect the importance of the sequence of patches’ order. As\nshown in Fig. 5, TraHGR-Huge captures the position meanings\nbetter than TraHGR-Large, and TraHGR-Large better than\nTraHGR-Base for both window sizes 200ms and 150ms. As\na consequence, it is possible to conclude that a more com-\nplex architecture can improve position embedding inference\nand includes more location data for transformer encoders.\nMoreover, as shown in Fig. 5, for longer window sizes, the\nsequential nature of sEMG signals can be better encoded. For\ninstance, as shown in Fig. 5(b), the position embeddings for\nthe TraHGR-Base architecture did not adequately infer the\nconcept of positions. As a result, it is reasonable to deduce\nthat the window size has a direct influence on the transformer\nencoder’s ability to infer position information.\nF . Comparison with State-of-the-Art DNN Approaches\nTABLE VI provides a comparison between our pro-\nposed approach, TraHGR-Huge, and the available method-\nologies, which shows the superiority of the TraHGR ar-\nchitecture over the experimental results obtained from the\nstate-of-the-art researches [5], [6], [14], [23], [32], [33],\n[60], [61]. This comparison was evaluated based on the\nsame settings for the DB2 ( 49 gestures) dataset and its sub-\nexercises, i.e., DB2-B ( 17 gestures), DB2-C ( 23 gestures),\nand DB2-D ( 9 gestures). For instance, most of the recent\nstudies, including the Ninapro dataset [14], predominantly em-\nployed a window size of 200ms with a step size of 100ms. Tak-\ning this into account, besides the reported results for the step\nsize of 10ms in TABLE VI, we conducted additional experi-\nments to evaluate the performance of the proposed model using\na step size of 100ms. As demonstrated in TABLE VI, increas-\ning the step size from 10ms to 100ms resulted in minimal per-\nformance degradation of the TraHGR, while the proposed ap-\nproach still outperforms other counterparts even with the larger\nstep size. In this study, we segmented sEMG signals with three\nwindow sizes, i.e., 200ms, 150ms, and 100ms. As shown in\nTABLE VI, our proposed approach TraHGR-Huge achieved\nhigher accuracy than the existing methodologies when eval-\nuated based on DB2 ( 49 gestures), DB2-B ( 17 gestures),\nDB2-C (23 gestures), DB2-D (9 gestures), different time win-\ndow sizes, and step size. We compared the proposed architec-\nture with both advanced DNNs and classical ML approaches.\nFor example, Reference [23] showed the average classifica-\ntion accuracy obtained using all the classical methods such\nas SVM, RF, K-Nearest Neighbors (K-NN), and LDA on\nthe DB2 ( 49 gestures) dataset is 60.28%. They achieved the\nhighest gesture recognition accuracy for RF which is 75.27%.\nMoreover, in Reference [32], they achieved the recognition\naccuracy 77.44% using SVM over all the movements. In\naddition, the recognition accuracy of 72.25% is reported in\nReference [17] for the RF classifier. For DNN architectures,\non the other hand, the best detection accuracy is reported in\nReference [5] using CNN, which is 83.70%. As shown in\nTABLE VI, for window size of 200ms, our proposed architec-\nture achieved 86.18% classification accuracy which is 2.48%\nhigher than the state-of-the-art DNN approach and 8.74%\nhigher than state-of-the-art classical ML method. Moreover, it\ncan be observed that for other window sizes, the classification\naccuracy of our proposed approach achieved better gesture\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nFig. 5: Position embedding similarities for FNet path in TraHGR-Base, TraHGR-Large, and TraHGR-Huge architectures: (a) window size\nis 200ms, and (b) window size is 150ms. Each row in each figure represents the cosine similarity between one embedding position and all\nthe other embeddings. The brightness of the pixels in the figures indicates more similarity.\nTABLE VI: Comparison between our methodology (TraHGR-Huge) and previous works [5], [6], [14], [23], [32], [33], [60], [61].\nMethod Database Step size\nWindow size\n200ms 150ms 100ms\nCNN [5] DB2 ( 49 gestures) - 83 .70 82 .70 81 .10\nAttention-based Hybrid CNN-RNN [6] DB2 ( 49 gestures) 100 ms 82 .20 - -\nCNN [33] DB2 ( 49 gestures) 10 ms - - 78 .86\nCNN [32] DB2 ( 49 gestures) 100 ms 78 .71 - -\nCNN [23] DB2 ( 49 gestures) - - 60 .27 -\nSVM [32] DB2 ( 49 gestures) 100 ms 77 .44 - -\nRF [14] DB2 ( 49 gestures) 100 ms 75 .27 - -\nRF [17] DB2 ( 49 gestures) 100 ms 72 .25 - -\nTraHGR-Huge DB2 ( 49 gestures)\n10 ms 86.18 85.43 84.13\n100 ms 86.00 85.22 83.95\nHDCAM [60] DB2-B ( 17 gestures) 10 ms 81 .10 80 .53 -\nTC-HGR [61] DB2-B ( 17 gestures) 10 ms 80 .72 - -\nCNN [32] DB2-B ( 17 gestures) 100 ms 82 .22 - -\nCNN [33] DB2-B ( 17 gestures) 10 ms - - 83 .79\nSVM [32] DB2-B ( 17 gestures) 100 ms 81 .07 - -\nTraHGR-Huge DB2-B ( 17 gestures)\n10 ms 88.91 88.14 86.46\n100 ms 88.72 87.97 86.25\nCNN [32] DB2-C ( 23 gestures) 100 ms 72 .62 - -\nSVM [32] DB2-C ( 23 gestures) 100 ms 71 .08 - -\nTraHGR-Huge DB2-C ( 23 gestures)\n10 ms 81.44 79.99 78.72\n100 ms 81.27 79.78 78.48\nCNN [32] DB2-D ( 9 gestures) 100 ms 89 .54 - -\nSVM [32] DB2-D ( 9 gestures) 100 ms 88 .56 - -\nTraHGR-Huge DB2-D ( 9 gestures)\n10 ms 93.84 93.58 91.62\n100 ms 93.74 93.33 91.49\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nrecognition performances than its counterparts. For example,\nwhen the window size is set to 100ms, our proposed approach\nTraHGR-Huge was able to achieve gesture recognition accu-\nracy of 84.13%, but using the proposed approach of [5], accu-\nracy of 81.1% is achieved. It should be noted that the accuracy\nof 84.13% obtained by TraHGR-Huge with a window size of\n100ms is still higher than the case where the window size\nin Reference [5] has doubled, i.e., 200ms. We also evaluated\nand compared our proposed method for DB2-B ( 17 gestures),\nDB2-C (23 gestures), and DB2-D ( 9 gestures) with the previ-\nous studies [32], [33], which demonstrates the superiority of\nour hybrid transformer-based framework.\nAuthors in [64] introduced a hybrid CNN-LSTM model\nachieving 79% average accuracy on the window size of 200ms\nas shown in TABLE VI. They reduced the number of param-\neters in the proposed model using dilated LSTM, resulting in\n1, 102, 801 parameters for 17 gesture classifications. However,\nas shown in TABLE VI, TraHGR-Huge outperforms [64]\nfor 17 gesture classification with less number of parameters\n(832, 659).\nG. Transfer Learning Impact on TraHGR Performance\nIn this experiment, The 5th Ninapro database [17], referred\nto as the DB5, is used for the ease of comparison with\nRef. [53]. The DB5 dataset is recorded with two Thalmic Myo-\narmbands recording muscular activity at a rate of 200Hz. The\nDB5 dataset, in particular, consists of signals collected from\n10 users executing 52 actions/movements. Each movement in\nthe DB5 dataset is repeated 6 times, each lasting for 5 seconds\nfollowed by 3 seconds of rest. The DB5 dataset is provided\nin three sets of exercises [17]. In this paper, we only consider\ndata collected by the lower armband in DB5 in the second\nexercise of the DB5 to follow the same criteria in [53] and\nalso have a fair comparison. Moreover, out of 6 movement\nrepetition for each target user, following [53], the first four\nrepetitions are used to fine-tune the pre-trained network, and\nthe last two repetitions serve as the test set.\nTABLE VII shows the average accuracy on the sec-\nond experiment of the Ninapro DB5 dataset. As shown in\nTABLE VII, the TraHGR-Huge outperforms ConvNet [53]\nwhether the training process of the network is involved with\nthe TL stage or solely trained for each user. In TABLE VII,\nthe networks without TL training stage are independently\ntrained for each user. However, to integrate TL techniques\ninto the training process, we conducted a typical TL method to\nutilize the knowledge learned in the source domain to promote\nthe learning process in a target domain. Specifically, given a\nuser as the target, in the first stage, the training sets of the\nremaining nine participants/users are employed to pre-train\nthe TraHGR-Huge network. Then, to fine-tune the pre-trained\nnetwork, the weights of the TNet and FNet in TraHGR-Huge\nare maintained intact by freezing them, and the non-frozen\nparts of the network are updated using one, two, three, or four\nrepetitions of the target data (see TABLE VII). As shown in\nTABLE VII, using transfer learning as a domain adaptation\napproach is conducted to performance improvement of both\nthe TraHGR-Huge and ConvNet models compared to their\nFig. 6: The accuracy for TraHGR-Huge, TNet, and FNet\nwhen they are trained simultaneously for DB2 ( 49 gestures) and\nits sub-exercises, DB2-B ( 17 gestures), DB2-C ( 23 gestures), and\nDB2-D (9 gestures).\nFig. 7: Results of the ablation study on loss functions with\nTraHGR-Huge model which is trained by Eq. 11 (green) and\nEq. 12 (red) evaluated on DB2 ( 49 gestures), DB2-B ( 17 gestures),\nDB2-C (23 gestures), and DB2-D ( 9 gestures).\ncorresponding user-specific trained models. When comparing\nour transformer-based model to ConvNet with convolutional\nstructure, we can infer that TraHGR-Huge achieves higher\naccuracies, demonstrating the proposed model’s ability to\nextract more useful representations from raw sEMG data.\nH. Ablation Study\nFor the proposed hybrid architectures such as\nTraHGR-Huge, TraHGR-Large, and TraHGR-Base, the\nclassification accuracy is calculated using the prediction\nvalues y obtained from Eq. (10). To show that our proposed\narchitecture based on a developed hybrid strategy has great\npotentials for improving gesture recognition accuracy, we\nalso calculated the other two accuracies, i.e., yTNet or yFNet,\nbased on the Eq. (9). More specifically, we trained the hybrid\narchitectures by computing the loss function in Eq. (11).\nHowever, output y is used to calculate the accuracy, reported\nin this paper. Here, in Fig. 6, it is shown that the accuracy\nobtained using the y is better than those calculated using the\nyTNet or yFNet for DB2 ( 49 gestures) and its sub-exercises.\nIn particular, from Fig. 6, it can be observed that the\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\nTABLE VII: The average accuracy of hand gesture recognition across all subjects in the second experiment of Ninapro DB5 dataset on\nthe window size of 260ms. The average accuracy is reported on 5 and 6 repetitions for all models in Ninapro DB5 dataset.\nRepetitions (Rep.) Used for\nTraining/Fine-tuning\nAccuracy ± STD\nConvNet [53] ConvNet+TL [53] TraHGR-Huge TraHGR-Huge +TL\nRep. 1, 2, 3, 4 66.30 ± 3.77 68.98 ± 4.09 71.21 ± 1.99 74.63 ± 2.52\nRep. 1, 2, 3 61.91 ± 3.94 65.16 ± 4.46 66.82 ± 2.07 69.01 ± 2.77\nRep. 1, 2 55.65 ± 4.38 60.12 ± 4.79 58.68 ± 2.81 62.07 ± 2.70\nRep. 1 46.06 ± 6.09 49.41 ± 5.82 51.33 ± 2.93 53.42 ± 3.31\nhybrid architecture takes advantage of two parallel paths and\nimproved the recognition accuracy.\n1) Evaluation of Different Loss Functions: As described in\nsub-section III-G, our proposed hybrid architectures’ param-\neters are learned by optimizing the loss function L, which\nconsists of three components. To demonstrate the advan-\ntage of training our proposed hybrid architecture with loss\nfunction L defined in Eq. (11), we evaluated performance\nof TraHGR-Huge when the loss function L has only one\ncomponent as follows\nL = LTraHGR. (12)\nFig. 7 shows the performance of TraHGR-Huge in\nDB2 (49 gestures) and its sub-exercises for two different loss\nfunctions. We can see that training TraHGR-Huge with a\nloss function with three components (Eq. (11)) improves the\nresults compared to the case where loss function has only one\ncomponent (Eq. (12)).\nV. C ONCLUSION\nIn this paper, we proposed a hybrid architecture based on\ntransformers for the task of hand gesture recognition. We have\nshown that the proposed hybrid architecture, referred to as the\nTraHGR framework, could augment the power of model dis-\ncrimination in different scenarios for various exercises. More-\nover, we investigated the ability of transformers for sEMG-\nbased hand gesture recognition as they have revolutionized\nother fields such as NLP, CV , and speech recognition. In this\nstudy, a comprehensive comparison was conducted between\nthe proposed TraHGR model, traditional ML approaches, and\nDNN-based techniques. The aim was to evaluate the effective-\nness of the TraHGR model in representative feature extraction\nand assess its performance relative to other methods. The\nexperimental results clearly demonstrated that the TraHGR\nmodel outperformed the competing approaches. A potential\ndirection for future research is to use the proposed transformer-\nbased architecture to develop an adaptive learning method\nwith a focus on increasing the robustness of sEMG classifiers\nand improving inter-subject accuracy will be an interesting\ndirection for future research. Another direction for future\nresearch is to use and extend the proposed transformer-based\nhybrid architecture to other machine learning fields.\nREFERENCES\n[1] N. Jiang, S. Dosen, K.R. Muller, D. Farina, “Myoelectric Control of\nArtificial Limbs- Is There a Need to Change Focus?” IEEE Signal\nProcess. Mag., vol. 29, pp. 150-152, 2012.\n[2] D. Farina, R. Merletti, R.M. Enoka, “The Extraction of Neural Strategies\nfrom the Surface EMG,” J. Appl. Physiol. , vol. 96, pp. 1486-95, 2004.\n[3] D. Farina, N. Jiang, H. Rehbaum, A. Holobar, B. Graimann, H, Dietl, and\nO. C. Aszmann, “The Extraction of Neural Information from the Surface\nEMG for the Control of Upper-limb Prostheses: Emerging Avenues and\nChallenges.,” IEEE Trans. Neural Syst. Rehabil. Eng. , vol. 22, no.4, pp.\n797-809, 2014.\n[4] M. Ergeneci et al. , “An embedded, eight channel, noise canceling,\nwireless, wearable sEMG data acquisition system with adaptive muscle\ncontraction detection,” IEEE Trans. Biomed. Circuits Syst. , vol. 12, no.\n1, pp. 68–79, Feb. 2018.\n[5] W. Wei et al. , “Surface Electromyography-based Gesture Recognition\nby Multi-view Deep Learning,” IEEE Trans. Biomed. Eng. , vol. 66, no.\n10, pp. 2964-2973, 2019.\n[6] Y . Hu et al., “A Novel Attention-based Hybrid CNN-RNN Architecture\nfor sEMG-based Gesture Recognition,” PloS one 13 , no. 10, 2018.\n[7] W. Geng, et al., “Gesture Recognition by Instantaneous Surface EMG\nImages,” Scientific Reports, 6, p. 36571, 2016.\n[8] E. Rahimian, S. Zabihi, A. Asif, D. Farina, S.F. Atashzar, and A. Mo-\nhammadi, “FS-HGR: Few-shot Learning for Hand Gesture Recognition\nvia ElectroMyography,” IEEE Trans. Neural Syst. Rehabil. Eng. , 2021.\n[9] A. Stango, F. Negro, and D. Farina, “Spatial Correlation of High Density\nEMG Signals Provides Features Robust to Electrode Number and Shift in\nPattern Recognition for Myocontrol,” IEEE Trans. Neural Syst. Rehabil.\nEng., vol. 23, no. 2, pp.189-198. 2014.\n[10] A. Vaswani et al. , “Attention is All You Need,” arXiv preprint\narXiv:1706.03762, 2017a.\n[11] J. Devlin, M.W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\nDeep Bidirectional Transformers for Language Understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[12] A. Dosovitskiy et al., “An Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale,” arXiv preprint arXiv:2010.11929 ,\n2020.\n[13] G. Krishna, C. Tran, M. Carnahan, and A.H. Tewfik, “EEG based\nContinuous Speech Recognition using Transformers,” arXiv preprint\narXiv:2001.00501, 2019.\n[14] M. Atzori, et al., “Electromyography Data for Non-Invasive Naturally-\nControlled Robotic Hand Prostheses,” Scientific data 1 , no. 1, pp. 1-13,\n2014.\n[15] A. Gijsberts, et al., “Movement Error Rate for Evaluation of Machine\nLearning Methods for sEMG-based Hand Movement Classification,”\nIEEE Trans. Neural Syst. Rehabil. Eng. , vol. 22, no. 4, pp. 735-744,\n2014.\n[16] M. Atzori et al. , “A Benchmark Database for Myoelectric Movement\nClassification,” IEEE Trans. Neural Syst. Rehabil. Eng. , 2013.\n[17] S. Pizzolato, L. Tagliapietra, M. Cognolato, M. Reggiani, H. Muller, and\nM. Atzori, “Comparison of Six Electromyography Acquisition Setups\non Hand Movement Classification Tasks,” PLoS ONE,,vol. 12, no. 10,\npp. 1-7, 2017.\n[18] A. Ameri, M.A. Akhaee, E. Scheme, and K. Englehart, “Regression\nconvolutional neural network for improved simultaneous EMG control,”\nJournal of Neural Engineering , vol. 16, no. 3, p.036015, 2019.\n[19] C. Li, J. Ren, H. Huang, B. Wang,Y . Zhu, and H. Hu, “PCA and\ndeep learning based myoelectric grasping control of a prosthetic hand.\nBiomedical engineering online,” Biomedical engineering online, vol. 17,\nno. 1,p.107, 2018.\n[20] D. Esposito et al. , “A Piezoresistive Array Armband with Reduced\nNumber of Sensors for Hand Gesture Recognition,” Frontiers in\nNeurorobotics, vol. 13, p. 114, 2020.\n[21] M. Tavakoli, C. Benussi, P.A. Lopes, L.B. Osorio, and A.T. de Almeida,\n“Robust Hand Gesture Recognition with a Double Channel Surface\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nEMG Wearable Armband and SVM Classifier,” Biomedical Signal\nProcessing and Control, vol. 46, pp. 121-130, 2018.\n[22] G.R. Naik, A.H. Al-Timemy, H.T. Nguyen, “Transradial Amputee\nGesture Classification using an Optimal Number of sEMG Sensors: an\nApproach using ICA Clustering,” IEEE Trans. Neural Syst. Rehabil.\nEng., vol. 24, no. 8, pp. 837–846, 2015.\n[23] M. Atzori, M. Cognolato, and H. M ¨uller, “Deep Learning with\nConvolutional Neural Networks Applied to Electromyography Data: A\nResource for the Classification of Movements for Prosthetic Hands,”\nFrontiers in neurorobotics 10, p.9, 2016.\n[24] A. K. Clarke et al., “Deep Learning for Robust Decomposition of High-\nDensity Surface EMG Signals,” IEEE Trans. Biomed. Eng. , 2020, In\nPress.\n[25] E. Rahimian, S. Zabihi, S. F. Atashzar, A. Asif, and A. Mohammadi,\n“Surface EMG-Based Hand Gesture Recognition via Hybrid and Di-\nlated Deep Neural Network Architectures for Neurorobotic Prostheses,”\nJournal of Medical Robotics Research , 2020, pp. 1-12.\n[26] E. Rahimian, S. Zabihi, F. Atashzar, A. Asif, A. Mohammadi, “Xcep-\ntionTime: Independent Time-Window XceptionTime Architecture for\nHand Gesture Classification,” International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP) , 2020.\n[27] E. Rahimian, S. Zabihi, A. Asif, S.F. Atashzar, and A. Mohammadi,\n“Few-Shot Learning for Decoding Surface Electromyography for Hand\nGesture Recognition,” IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2021, pp. 1300-1304.\n[28] E. Rahimian, S. Zabihi, S. F. Atashzar, A. Asif, and A. Mohammadi,\n“Semg-based Hand Gesture Recognition via Dilated Convolutional Neu-\nral Networks,” IEEE Global Conference on Signal and Information\nProcessing (GlobalSIP), 2019.\n[29] W. Wei et al. , “A Multi-stream Convolutional Neural Network for\nsEMG-based Gesture Recognition in Muscle-computer Interface,” Pat-\ntern Recognition Letters , 119, pp. 131-138, 2019.\n[30] U. C ˆot´e-Allard, et al. , “Interpreting Deep Learning Features for Myo-\nelectric Control: A Comparison with Handcrafted Features,” Frontiers\nin bioengineering and biotechnology , 8, p.158, 2020.\n[31] R. N. Khushaba and S. Kodagoda, “Electromyogram (EMG) feature\nreduction using mutual components analysis for multifunction prosthetic\nfingers control,” 12th Int. Conf. Control Autom. Robot. Vis.(ICARCV) ,\n2012, pp. 1534–1539.\n[32] X. Zhai, B. Jelfs, R. H. Chan, and C. Tin, “Self-recalibrating Surface\nEMG Pattern Recognition for Neuroprosthesis Control based on Convo-\nlutional Neural Network,” Frontiers in neuroscience, 11, p.379, 2017.\n[33] Z. Ding, et al. , “sEMG-based Gesture Recognition with Convolution\nNeural Networks,” Sustainability 10, no. 6, p. 1865, 2018.\n[34] F. Quivira, et al., “Translating sEMG Signals to Continuous Hand Poses\nUsing Recurrent Neural Networks.,” in Proc. IEEE EMBS Int. Conf.\nBiomed. Health Informat. , 2018, pp. 166–169.\n[35] T. Sun, Q. Hu, P. Gulati, and S.F. Atashzar, “Temporal Dilation of\nDeep LSTM for Agile Decoding of sEMG: Application in Prediction\nof Upper-limb Motor Intention in NeuroRobotics.,” IEEE Robotics and\nAutomation Letters, 2021.\n[36] S. Bai, J.Z. Kolter, and V . Koltun, “ An Empirical Evaluation of Generic\nConvolutional and Recurrent Networks for Sequence Modeling,” arXiv\npreprint arXiv:1803.01271, 2018.\n[37] J. Guan, W. Wang, P. Feng, X. Wang, and W. Wang, “Low-\nDimensional Denoising Embedding Transformer for ECG Classifica-\ntion,” International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2021, pp. 1285-1289.\n[38] Y . Song, X. Jia, L. Yang, and L. Xie, “Transformer-based Spatial-\nTemporal Feature Learning for EEG Decoding,” arXiv preprint\narXiv:2106.11170, 2021.\n[39] T.B. Brown, et al. , “Language Models are Few-shot Learners,” arXiv\npreprint arXiv:2005.14165, 2020.\n[40] Y . Wang, et al. , “Transformer-based Acoustic Modeling for Hybrid\nSpeech Recognition,” International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2020, pp. 6874-6878.\n[41] T. Bao, S. Q. Xie, P. Yang, P. Zhou and Z.Q. Zhang, “Toward Robust,\nAdaptiveand Reliable Upper-Limb Motion Estimation Using Machine\nLearning and Deep Learning–A Survey in Myoelectric Control,” IEEE\nJournal of Biomedical and Health Informatics , vol. 26, no. 8, pp. 3822-\n3835, 2022.\n[42] M. Kim, W. K. Chung, and K. Kim, “Subject-Independent sEMG\n[43] K. Watanabe, M. Kouzaki, M. Ogawa, H. Akima, and T. Moritani,\n“Relationships between muscle strength and multi-channel surface EMG\nPattern Recognition by Using a Muscle Source Activation Model,” IEEE\nRobotics and Automation Letters ,vol. 5, no. 4, pp. 5175-5180, 2020.\nparameters in eighty eight elderly,” European Review of Aging and\nPhysical Activity, vol. 15, 2018.\n[44] E. C. Hill et al., “Effect of sex on torque, recovery, EMG, and mmg\nresponses to fatigue,” J. Musculoskelet Neuronal Interact , vol. 16, no.\n4 pp. 310–317, 2016.\n[45] J. He, D. Zhang, N. Jiang, X. Sheng, D. Farina, and X. Zhu, “User\nadaptation in long-term, open-loop myoelectric training: Implications\nfor EMG pattern recognition in prosthesis control,” J. Neural Eng., vol.\n12, no. 4, 2015.\n[46] L. Pan, D. Zhang, N. Jiang, X. Sheng, and X. Zhu, “Improving\nrobustness against electrode shift of high density EMG for myoelectric\ncontrol through common spatial patterns,” J. Neuroeng. Rehabil. , vol.\n12, 2015\n[47] M. Jochumsen, A. Waris, and E. N. Kamavuako, “The effect of arm\nposition on classification of hand gestures with intramuscular EMG,”\nBiomed. Signal Process. Control , vol. 43, pp. 1–8, 2018.\n[48] K.T. Kim, C. Guan, and S.W. Lee, “A subject-transfer framework based\non single-trial EMG analysis using convolutional neural networks,”IEEE\nTransactions on Neural Systems and Rehabilitation Engineering, vol. 28,\nno. 1, pp. 94-103, 2020.\n[49] A. Ameri, M. A. Akhaee, E. Scheme, and K. Englehart, “A deep\ntransfer learning approach to reducing the effect of electrode shift in\nEMG pattern recognition-based control,” IEEE Transactions on Neural\nSystems and Rehabilitation Engineering , vol. 28, no. 2, pp. 370-379,\n2020.\n[50] Z. Yu, J. Zhao, Y . Wang, L. He, and S. Wang, “Surface EMG-\nbased instantaneous hand gesture recognition using convolutional neural\nnetwork with the transfer learning method,”Sensors, vol. 21, no. 7, 2021.\n[51] F. Demir, V . Bajaj, M. C. Ince, S. Taran, and A. ¸Seng ¨ur “Surface EMG\nsignals and deep transfer learning-based physical action classification,”\nNeural Comput. Appl. , vol. 31, no. 12, pp. 8455–8462, 2019.\n[52] J. J. Bird, J. Kobylarz, D. R. Faria, A. Ek ´art, and E. P. Ribeiro, “Cross\ndomain MLP and CNN transfer learning for biological signal processing:\nEEG and EMG,” IEEE Access, vol. 8, pp. 54789–54801, 2020.\n[53] U. C ˆot´e-Allard et al., “Deep learning for electromyographic hand gesture\nsignal classification using transfer learning,” IEEE Trans. Neural Syst.\nRehabil. Eng., vol. 27, no. 4, pp. 760–771, 2019.\n[54] M. Asfour, C. Menon, X. Jiang, “Feature–Classifier Pairing Compatibil-\nity for sEMG Signals in Hand Gesture Recognition under Joint Effects\nof Processing Procedures.,” Bioengineering, vol. 9, no. 11, p.634, 2022.\n[55] W. Wei, Y .Wong, Y . Du, Y . Hu, M. Kankanhalli, W. Geng, “A\nmulti-stream Convolutional Neural Network for sEMG-based Gesture\nRecognition in Muscle-computer Interface,” Pattern Recognition Letters,\n2017.\n[56] B. Hudgins, P. Parker, and R.N. Scott, “A New Strategy for Multifunc-\ntion Myoelectric Control,” IEEE Trans. Biomed. Eng. , vol. 40, no. 1,\np.82-94, 1993.\n[57] T.R. Farrell and R.F. Weir, “The optimal controller delay for myoelectric\nprostheses.” IEEE Transactions on neural systems and rehabilitation\nengineering, no. 1, vol. 15, pp.111-118, 2007.\n[58] JL Ba, JR Kiros, and G.E. Hinton, “Layer normalization,” arXiv preprint\narXiv:1607.06450, 2016.\n[59] DP. Kingma, and J. Ba, “Adam: A Method for Stochastic Optimization,”\nICLR, 2015.\n[60] S. Zabihi, E. Rahimian, A. Asif, and A. Mohammadi, “Light-Weight\nCNN-Attention Based Architecture for Hand Gesture Recognition Via\nElectromyography” IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pp.1-5, 2023.\n[61] E. Rahimian, S. Zabihi, A. Asif, D. Farina, S.F. Atashzar, and A.\nMohammadi, “Hand Gesture Recognition Using Temporal Convolutions\nand Attention Mechanism,” IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pp. 1196-1200, 2022.\n[62] U. C ˆot´e-Allard, et al. , “Deep Learning for Electromyographic Hand\nGesture Signal Classification using Transfer Learning,” IEEE Trans.\nNeural Syst. Rehabil. Eng. , vol. 27, no. 4, pp. 760-771, 2019.\n[63] F. Wilcoxon, “Individual comparisons by ranking methods,” Biometrics\nBull.,vol. 1, no. 6, pp. 80–83, 1945.\n[64] T. Sun, Q. Hu, P. Gulati, and S.F. Atashzar, and M. Atzori, ““Temporal\nDilation of Deep LSTM for Agile Decoding of sEMG: Application in\nPrediction of Upper-limb Motor Intention in NeuroRobotics,” IEEE\nRobotics and Automation Letters ,2021.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3324252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}