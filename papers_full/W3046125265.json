{
  "title": "Transformer based unsupervised pre-training for acoustic representation learning",
  "url": "https://openalex.org/W3046125265",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5029228290",
      "name": "Ruixiong Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079117782",
      "name": "Haiwei Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050050390",
      "name": "Wubo Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110582135",
      "name": "Dongwei Jiang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101808317",
      "name": "Wei Zou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081173423",
      "name": "Xiangang Li",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2926827382",
    "https://openalex.org/W2533262878",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1568716973",
    "https://openalex.org/W3036237438",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2973048981",
    "https://openalex.org/W3037217258",
    "https://openalex.org/W4398958419",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3015707499",
    "https://openalex.org/W2062170755",
    "https://openalex.org/W2981991061",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2052666245",
    "https://openalex.org/W2936451900",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "Recently, a variety of acoustic tasks and related applications arised. For many acoustic tasks, the labeled data size may be limited. To handle this problem, we propose an unsupervised pre-training method using Transformer based encoder to learn a general and robust high-level representation for all acoustic tasks. Experiments have been conducted on three kinds of acoustic tasks: speech emotion recognition, sound event detection and speech translation. All the experiments have shown that pre-training using its own training data can significantly improve the performance. With a larger pre-training data combining MuST-C, Librispeech and ESC-US datasets, for speech emotion recognition, the UAR can further improve absolutely 4.3% on IEMOCAP dataset. For sound event detection, the F1 score can further improve absolutely 1.5% on DCASE2018 task5 development set and 2.1% on evaluation set. For speech translation, the BLEU score can further improve relatively 12.2% on En-De dataset and 8.4% on En-Fr dataset.",
  "full_text": "TRANSFORMER BASED UNSUPERVISED PRE-TRAINING FOR ACOUSTIC\nREPRESENTATION LEARNING\nRuixiong Zhang, Haiwei Wu, Wubo Li, Dongwei Jiang, Wei Zou, Xiangang Li\nDiDi Chuxing, Beijing, China\nABSTRACT\nRecently, a variety of acoustic tasks and related applications\narised. For many acoustic tasks, the labeled data size may be\nlimited. To handle this problem, we propose an unsupervised\npre-training method using Transformer based encoder to learn\na general and robust high-level representation for all acous-\ntic tasks. Experiments have been conducted on three kinds\nof acoustic tasks: speech emotion recognition, sound event\ndetection and speech translation. All the experiments have\nshown that pre-training using its own training data can signif-\nicantly improve the performance. With a larger pre-training\ndata combining MuST-C, Librispeech and ESC-US datasets,\nfor speech emotion recognition, the UAR can further improve\nabsolutely 4.3% on IEMOCAP dataset. For sound event de-\ntection, the F1 score can further improve absolutely 1.5% on\nDCASE2018 task5 development set and 2.1% on evaluation\nset. For speech translation, the BLEU score can further im-\nprove relatively 12.2% on En-De dataset and 8.4% on En-Fr\ndataset.\nIndex Terms— unsupervised pre-training, Transformer,\nacoustic representation learning\n1. INTRODUCTION\nThe goal of acoustic representation learning is to transform\nraw or surface features into the high-level feature which are\nmore accessible to acoustic tasks[1]. It is critical to make\nacoustic representations more general and robust to improve\nthe performance of acoustic tasks. However, the labeled data\nsize of the speciﬁc acoustic task may be limited so that the\nlearned representations can be less robust and the perfor-\nmance can be vulnerable to unseen data. On the other hand,\nthere exists varieties of acoustic tasks which range from\nspeaker veriﬁcation, speech recognition to event and scene\ndetection. For supervised learning, the learned representation\nuseful for one task may be less suited for another task. It\nis worthwhile to explore how to utilize all kinds of datasets\nto learn a general and robust representation for all kinds of\nacoustic tasks.\nUnsupervised pre-training can provide an appealing\nmethod to learn more general and robust high-level features\nthat are less specialized towards solving a single supervised\ntask. The training objective of unsupervised pre-training is\nonly related with acoustic features themselves and is not de-\npendent on any other downstream target. Because of this\nadvantage, much more unlabeled data can be utilized so that\na larger and more general model can be learned. At the same\ntime, the learned representations can be directly utilized or\nﬁne-tuned for speciﬁc downstream tasks.\nContrastive Predictive Coding(CPC)[2] has provided a\nuniversal unsupervised learning approach to extract useful\nrepresentations from high-dimensional data. The autoregres-\nsive mechanism is used for predicting future information.\nHowever, it can only be applied in uni-directional models.\nMasked Predictive Coding(MPC)[3] has been proposed to\nutilize speech data in an unsupervised manner for speech\nrecognition. It uses the bidirectional transformer based ar-\nchitecture and uses Masked-LM[4] like structure to perform\npredictive coding. The pre-trained representations can be fur-\nther ﬁne-tuned to improve speciﬁc speech recognition tasks.\nHowever, the speech or acoustic representation pre-trained\nfrom this method has not yet been applied to other kinds\nof acoustic tasks and also the performance of this unsuper-\nvised pre-training method on non-speech audio tasks remains\nunknown.\nIn this paper, we get intuition from MPC and utilize a\nTransformer[5] based unsupervised pre-training method for\nacoustic representation learning. Transformer based encoder\ncan be pre-trained by a large amount of unlabeled audio from\nvarious kinds of datasets. After pre-training, all we should do\nis to add a decoder layer targeted for downstream tasks and\nﬁne-tune the whole model. we have demonstrated that our\nmethod can learn a more general and robust acoustic repre-\nsentation which can signiﬁcantly improve the performance of\nvarious kinds of acoustic tasks.\n2. RELATED WORK\nContrastive Predictive Coding(CPC) provided a universal\nunsupervised learning approach and the learned representa-\ntion is able to achieve strong performance on four domains:\nspeech, images, text and reinforcement learning in 3D en-\nvironments. This model is mainly composed of two parts:\na non-linear encoder genc and an autoregressive model gar .\nGiven an input sequence(x1, x2, ..., xT ), genc encodes obser-\narXiv:2007.14602v3  [eess.AS]  8 Feb 2021\nvations xt to a latent embedding space zt = genc(xt) and gar\naccepts zt to produce a context representation ct = gar(z≤t).\nTargeting at predicting future observationsxt+k ,a density ra-\ntio f(xt+k, ct) is modelled to maximally preserve the mutual\ninformation between xt+k and ct. To optimize genc and gar ,\nthe contrastive loss is minimized:\nŁN = −E\nX\n[log f(xt+k, ct)∑\nxj ∈X fk(xj, ct)], (1)\nwhere N represents number of samples inX = x1, x2, ..., xN ,\nwith one positive sample from distributionp(xt+k|ct) and the\nrest being negative samples from distribution p(xt+k).\nAutoregressive Predictive Coding(APC)[6] also proposed\nan autoregressive model for unsupervised speech representa-\ntion learning. It used a deep LSTM network and make the\nmodel to predict further steps ahead of the current frame dur-\ning training. APCs have demonstrated a strong capability of\nextracting useful phone and speaker information.\n3. METHODOLOGY\nTo learn a general high-level acoustic representation, we use\nTransformer based encoder in an unsupervised manner. The\narchitecture of Transformer based encoder is illustrated in\nFigure 1(a).\nFor unsupervised pre-training, Figure 1(b) shows our pre-\ntraining procedure. 15% of frames of the acoustic feature\nsequence will be masked by zeros and the object of unsuper-\nvised pre-training is similar as that of [3] which is to restore\nthe masked frames given the left and right context features.\nHowever, we have two aspects that are different from that of\n[3]. On one hand, we have different masking mechanisms.\nGenerally speaking, the CNN modules of Transformer based\nencoder provide a downsampling mechanism, by which the\nframes would be N-fold downsampled. Therefore, to reserve\nthe masked information after downsampling operations, we\nsplit frames into chunks each of which contains N frames and\n15% of all chunks will be selected randomly and all frames\nof the selected chunks will be masked by zeros. On the other\nhand, Transformer encoder is followed by a feed-forward\nlayer to transform each chunk-level prediction into frame-\nlevel predictions. With these changes, we also use L1 loss\nto minimize the gap between the predicted frames and the\ncorresponding real frames.\nFor ﬁne-tuning, Transformer encoder needs to be pre-\ntrained only once and can be adapted to varieties of acoustic\ntasks no matter whether the downstream task deal with the\nspeech or non-speech acoustic sequences, and no matter\nwhether the output of the task is a sequence or a tag. All\nwe should do is to add a decoder layer after the pre-trained\nencoder to ﬁne-tune the whole model for speciﬁc tasks. The\nchoice of decoder layers is based on the tasks as shown in\nFigure 1(c). We can use Transformer decoder for seq-to-seq\ntasks and speciﬁc pooling layers for tagging tasks.\n4. EXPERIMENTS\nTo prove the effectiveness of our unsupervised pre-training\nmethod on various kinds of acoustic tasks, we selected three\nrepresentative kinds of tasks: speech emotion recognition,\nacoustic event detection and speech translation.\n4.1. Data\nTo pre-train the model using a larger dataset which can be\nadapted to various kinds of downstream tasks, we merge\nMuST-C En-De[7](408 hours), Librispeech[8](960 hours)\nand ESC-US[9]( 347 hours) datasets into one dataset(almost\n1715 hours) and we call it OpenAudio. Among them, ESC is\nan open dataset for environmental sound classiﬁcation while\nESC-US is a compilation of 250k unlabeled clips which were\nextracted from public ﬁeld recordings. MuST-C is a mul-\ntilingual corpus for speech translation from English into 8\nlanguages. For each target language, MuST-C comprises at\nleast 385 hours of audio recordings from English TED Talks.\nLibriSpeech is a corpus of reading English speech with sam-\npling rate of 16 kHz. The data has been carefully segmented\nand aligned.\nFor pre-training, we did not use speed perturbation but for\nﬁne-tuning in every downstream task, we used speed pertur-\nbation with factor of 0.9 and 1.1 for data augmentation. We\nuse 40-dimensional Mel ﬁlter-banks extracted from the audio\nsignals using window size of 25 ms and step size of 10 ms for\npre-training and ﬁne-tuning in all downstream tasks.\n4.2. Experimental setups\nFor Transformer based model, we use the structure discussed\nbefore with hidden dimension size of 256, feed-forward size\nof 2048, attention heads of 4, dropout rate of 0.1 and encoder\nlayers of 12 for all tasks.\nWe pre-trained our model using OpenAudio only once\nand ﬁne-tuned it in each downstream task. It was trained on\n4 GPUs with a total batch size of 256 for 50 epochs. We used\nthe Adam optimizer[10] with warmup schedule[5] according\nto the formula:\nlrate = k ∗d0.5\nmodel ∗min(n−0.5, n∗warmup n−1.5) (2)\nwhere n is the step number. k = 0.5 and warmup n = 8000\nwere chosen for all experiments. For comparison, we also\npre-trained our model on each task using its own training data\nwith the same setups as discussed before.\n4.3. Speech emotion recognition\nThe IEMOCAP database[11] was commonly used in previ-\nous speech emotion studies[12]. We also use it for our ex-\nperiments. We used the recordings where majority of anno-\ntators agreed on the emotion labels and it contains 4 kinds of\nCNN\tmodules\nMulti-head\t\nattention\nDropout\t&\tAdd\t\n&\tNorm\t\nMasked\tAcoustic\tFeature\nPositional\t\nEncoding\nTransformer\t\nencoder\nAcoustic\tFeature\tPredictionFeed-forward\t\nLayer\nDropout\t&\tAdd\t\n&\tNorm\t\nFeed-forward\t\nLayer\n......\n......\nTransformer\t\nencoder\n\tAcoustic\tFeature\n......\nDecoder\tLayer\nTransformer\t\nDecoder\nPooling\tLayer\nFeed-forward\t\nLayer\nL1\tLoss\nTarget\tPrediction\nCross\tEncropy\t\nLoss\n......\n(a) (b) (c)\nFig. 1. training structure and procedure: (a) The structure of Transformer based encoder. (b) Pre-training: it is trained to predict\nthe masked acoustic feature using L1 loss. (c) Fine-tuning: the pre-trained transformer encoder is ﬁne-tuned with an additional\ndecoder layer to adapt to the speciﬁc task.\nemotions: angry, happy, sad and neutral state. Happy and ex-\ncited emotions were combined as happy in order to balance\nthe number of samples in each emotion class. The dataset\ncontains 5,531 utterances (1,103 angry, 1,636 happy, 1,708\nneutral, 1,084 sad) grouped into 5 sessions. We conducted\n5-fold cross validation on IEMOCAP, taking samples from 8\nspeakers for training and the others for evaluation. For ﬁne-\ntuning, we add an average pooling layer followed by one feed-\nforward layer. It was trained on 4 GPUs with a total batch size\nof 64 for 25 epochs. We also use the optimizer which is the\nsame as that of pre-training. For evaluating the performance,\nwe restore the checkpoint averaged from best 5 checkpoints\nduring training. We used UAR which is deﬁned as the un-\nweighted average of the class-speciﬁc recalls achieved by the\nsystem as our metrics.\nIn our experiments as shown in Table 1, we achieve a\nmean UAR of 64.9% which is signiﬁcantly better than the\nstate-of-the-art result on this setup. According to [13] and the\nbest of our knowledge, [14] and [15] presented the best results\nin the condition that almost match our setups. Speciﬁcally,\nthey all use 4 emotion classes and merge happy and excited as\none class, except that they used leave-one-speaker-out cross\nvalidation and we use leave-one-session-out cross validation.\nCompared with [13] which has provided another unsuper-\nvised pre-training method, our Transformer based model with\npre-training can achieve better performance.\nTable 1. Results of speech emotion recognition (Note:\nMethod and Data represent pre-training method and pre-\ntraining data respectively)\nMethod Data IEMOCAP\nRozgic et al.[14] - - 60.9\nXia et al.[15] - - 62.5\nMichael et al.[13] Autoencoder TED-LIUM 59.5\nTransformer - - 60.6\nTransformer Ours IEMOCAP 61.8\nTransformer Ours OpenAudio 64.9\n4.4. Sound event detection\nWe used DCASE2018 task5 dataset[16] for sound event de-\ntection. It contains a continuous recording of one person liv-\ning in a vacation home over a period of one week. The con-\ntinuous recordings were split into audio segments of 10s and\neach segment represents one activity. The dataset presents\n10 kinds of activities like cooking, eating and so on. The\nDCASE2018 task5 has provided development and evaluation\ndatasets for evaluation and test. We use the macro-averaged\nF1-score as the metrics of this task. It was trained on 4 GPUs\nwith a total batch size of 128 for 50 epochs. We also use\nthe optimizer which is the same as that of pre-training except\nthat k = 0.3. For evaluating the performance, we restore the\ncheckpoint averaged from best 5 checkpoints during training.\nSimilar to speech emotion recognition, we used an average\npooling layer as the decoder layer for ﬁnetuning.\nWe compared our work with top three teams’ technical\nreports[17, 18, 19] listed on the DCASE community website.\nTable 2 shows that with pre-training using OpenAudio, Trans-\nformer based model can achieve better performance than all\nof them on the development set and one of them on the evalu-\nation set. Consider that they used well-designed hand-crafted\nfeatures with various kinds of data augmentation and ensem-\nble tricks, our method presents a simple but effective training\nscheme.\nTable 2. Results of sound event detection (Note: Method\nand Data represent pre-training method and pre-training data\nrespectively, DCASE represets DCASE2018 task5 dataset)\nMethod Data Dev. Eval.\nInoue et al.[17] - - 90.0 88.4\nLiu et al.[18] - - 89.8 87.5\nLiao et al.[19] - - 89.8 86.7\nTransformer - - 89.5 85.4\nTransformer Ours DCASE 90.4 86.6\nTransformer Ours OpenAudio 91.0 87.5\n4.5. Speech translation\nThe aim of speech translation is to translate one language di-\nrectly from the speech into another language. We used MuST-\nC English-to-German(En-De) and English-to-French(En-Fr)\ndatasets[7] which were commonly used in previous speech\ntranslation studies[20, 21]. For ﬁne-tuning, we used a 6-layer\nTransformer decoder as the decoder layer. To avoid overﬁt-\nting, we also used label smoothing with the rate of 0.1. Simi-\nlar to [21], we used 8k vocabularies based on byte pair encod-\ning (BPE)[22]. It was trained on 4 GPUs with a total batch\nsize of 512 for 50 epochs. We also use the optimizer which\nis the same as that of pre-training except that k = 2.5 and\nwarmup n = 25000. For evaluating the performance, we re-\nstore the checkpoint averaged from best 5 checkpoints dur-\ning training. We used beam search with beam size of 10\nand performance was evaluated using case-sensitive 4-gram\nBLEU[23] on the tst-COMMON set.\nAccording to [21] and [20], for end-to-end speech trans-\nlation, Transformer based model has provided state-of-the-art\nresults on MuST-C datasets. However, its performance de-\npends on ASR pre-training which needs English transcripts.\nIn our experiments as shown in Table 3, we do not need\nEnglish transcripts and the performance of Transformer pre-\ntrained by its own training audio can be comparable with\nthat of Transformer pre-trained by ASR. Furthermore, be-\ncause we can easily extend our pre-training data without\nany speciﬁc label, we evaluated the results of Transformer\npre-trained by OpenAudio. The results have shown that the\nBLEU scores have exceeded that of [21] pre-trained by ASR\non both datasets.\nWe can see that different from current end-to-end speech\ntranslation methods, our methods provide not only better per-\nformance but an easier training scheme without transcripts of\nspeech in same language which is more practical for industrial\napplication. It is also promising that combining our unsu-\npervised pre-training method with the current supervised pre-\ntraining mechanism will further improve the performance.\nTable 3. Results of speech translation (Note: Method and\nData represent pre-training method and pre-training data re-\nspectively)\nMethod Data En-De En-Fr\nPipeline[20] - - 18.50 27.90\nTransformer[21] - - 16.40 N/A\nTransformer[21] ASR MuST-C 21.77 31.56\nTransformer - - 19.64 29.40\nTransformer ASR MuST-C 21.93 31.70\nTransformer Ours MuST-C 21.50 31.32\nTransformer Ours OpenAudio 22.04 31.88\n5. CONCLUSION\nIn this work, we explored Transformer based encoder with\nMasked-LM like pre-training for acoustic representation\nlearning. We conducted experiments on three kinds of\ntasks: speech emotion recognition, sound event detection and\nspeech translation. We pre-train the model with a large dataset\ncombining Librispeech, MuST-C and ESC-US datasets and\nﬁne-tune it on each task. Results have shown that for speech\ntranslation, the BLEU score can improve relatively 12.2%\nand 8.4% on MuST-C En-De and En-Fr datasets respectively\ncompared with that of Transformer without pre-training. For\nsound event detection, the F1 score can improve absolutely\n1.5% and 2.1% on DCASE2018 task5 development set and\nevaluation set compared with that of our base Transformer.\nFor speech emotion recognition, the UAR can improve abso-\nlutely 4.3% on IEMOCAP dataset compared with that of our\nbase Transformer.\nCompared with current state-of-the-art acoustic systems,\nour method is able to provide a more general and robust\nacoustic representation for all acoustic tasks and it is easy\nto be transferred, easy to be built without many hand-crafted\ndesigns and is more practical for industrial applications. It\nsuggests that our method can provide a promising alternative\nfor acoustic representation learning.\n6. REFERENCES\n[1] G. Tzanetakis and P. Cook, “Marsyas: A framework for\naudio analysis,”Organised sound, vol. 4, no. 3, pp. 169–\n175, 2000.\n[2] A. v. d. Oord, Y . Li, and O. Vinyals, “Representa-\ntion learning with contrastive predictive coding,” arXiv\npreprint arXiv:1807.03748, 2018.\n[3] D. Jiang, X. Lei, W. Li et al., “Improving transformer-\nbased speech recognition using unsupervised pre-\ntraining,”arXiv preprint arXiv:1910.09932, 2019.\n[4] J. Devlin, M.-W. Chang, K. Lee et al. , “Bert: Pre-\ntraining of deep bidirectional transformers for language\nunderstanding,” arXiv preprint arXiv:1810.04805 ,\n2018.\n[5] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is\nall you need,” in Advances in neural information pro-\ncessing systems, 2017, pp. 5998–6008.\n[6] Y .-A. Chung, W.-N. Hsu, H. Tang et al. , “An unsu-\npervised autoregressive model for speech representation\nlearning,”arXiv preprint arXiv:1904.03240, 2019.\n[7] M. A. Di Gangi, R. Cattoni, L. Bentivogli et al., “Must-\nc: a multilingual speech translation corpus,” in 2019\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Lan-\nguage Technologies. Association for Computational\nLinguistics, 2019, pp. 2012–2017.\n[8] V . Panayotov, G. Chen, D. Povey et al., “Librispeech:\nan asr corpus based on public domain audio books,”\nin 2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2015,\npp. 5206–5210.\n[9] K. J. Piczak, “Esc: Dataset for environmental sound\nclassiﬁcation,” in Proceedings of the 23rd ACM interna-\ntional conference on Multimedia, 2015, pp. 1015–1018.\n[10] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,”arXiv preprint arXiv:1412.6980, 2014.\n[11] C. Busso, M. Bulut, C.-C. Lee et al., “Iemocap: Inter-\nactive emotional dyadic motion capture database,” Lan-\nguage resources and evaluation, vol. 42, no. 4, p. 335,\n2008.\n[12] R. Pappagari, T. Wang, J. Villalbaet al., “x-vectors meet\nemotions: A study on dependencies between emotion\nand speaker recognition,” in ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2020, pp. 7169–7173.\n[13] M. Neumann and T. Vu, “Improving speech emotion\nrecognition with unsupervised representation learning\non unlabeled speech,” in International Conference on\nAcoustics, Speech, and Signal Processing, 2019, 2019.\n[14] V . Rozgic, S. Ananthakrishnan, S. Saleem et al., “En-\nsemble of svm trees for multimodal emotion recog-\nnition,” in Signal Information Processing Association\nSummit And Conference, 2012.\n[15] R. Xia and Y . Liu, “Leveraging valence and activa-\ntion information via multi-task learning for categori-\ncal emotion recognition,” in 2015 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2015, pp. 5301–5305.\n[16] G. Dekkers, S. Lauwereins, B. Thoen et al., “The SINS\ndatabase for detection of daily activities in a home en-\nvironment using an acoustic sensor network,” in Pro-\nceedings of the Detection and Classiﬁcation of Acous-\ntic Scenes and Events 2017 Workshop (DCASE2017) ,\nNovember 2017, pp. 32–36.\n[17] T. Inoue, P. Vinayavekhin, S. Wanget al., “Domestic ac-\ntivities classiﬁcation based on CNN using shufﬂing and\nmixing data augmentation,” DCASE2018 Challenge,\nTech. Rep., September 2018.\n[18] H. Liu, F. Wang, X. Liu et al., “An ensemble system for\ndomestic activity recognition,” DCASE2018 Challenge,\nTech. Rep., September 2018.\n[19] H.-W. Liao, J.-Y . Huang, S.-S. Lanet al., “DCASE 2018\ntask 5 challenge technical report: Sound event classiﬁ-\ncation by a deep neural network with attention and min-\nimum variance distortionless response enhancement,”\nDCASE2018 Challenge, Tech. Rep., September 2018.\n[20] M. A. D. Gangi, M. Negri, and M. Turchi, “Adapting\ntransformer to end-to-end spoken language translation,”\nin Interspeech 2019, 2019.\n[21] H. Inaguma, S. Kiyono, K. Duh et al. , “Espnet-st:\nAll-in-one speech translation toolkit,” arXiv preprint\narXiv:2004.10234, 2020.\n[22] R. Sennrich, B. Haddow, and A. Birch, “Neural machine\ntranslation of rare words with subword units,” in Pro-\nceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers).\nBerlin, Germany: Association for Computational Lin-\nguistics, Aug. 2016, pp. 1715–1725.\n[23] K. Papineni, S. Roukos, T. Ward et al., “Bleu: a method\nfor automatic evaluation of machine translation,” inPro-\nceedings of the 40th annual meeting on association for\ncomputational linguistics. Association for Computa-\ntional Linguistics, 2002, pp. 311–318.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7648677825927734
    },
    {
      "name": "Transformer",
      "score": 0.6748273372650146
    },
    {
      "name": "Speech recognition",
      "score": 0.6566401720046997
    },
    {
      "name": "Speech translation",
      "score": 0.5833461284637451
    },
    {
      "name": "Training set",
      "score": 0.5377481579780579
    },
    {
      "name": "Encoder",
      "score": 0.4376308023929596
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4222061336040497
    },
    {
      "name": "Representation (politics)",
      "score": 0.415298730134964
    },
    {
      "name": "Machine translation",
      "score": 0.37733227014541626
    },
    {
      "name": "Machine learning",
      "score": 0.3251349925994873
    },
    {
      "name": "Voltage",
      "score": 0.07624632120132446
    },
    {
      "name": "Engineering",
      "score": 0.06583952903747559
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "cited_by": 7
}