{
    "title": "Semi-automating abstract screening with a natural language model pretrained on biomedical literature",
    "url": "https://openalex.org/W4386979600",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5066901038",
            "name": "Sheryl Hui-Xian Ng",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5036511861",
            "name": "Kiok Liang Teow",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5085448555",
            "name": "Gary Yee Ang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5036167026",
            "name": "Woan Shin Tan",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5000704046",
            "name": "Allyn Hum",
            "affiliations": [
                null,
                "Tan Tock Seng Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2961191798",
        "https://openalex.org/W4213127247",
        "https://openalex.org/W2982683456",
        "https://openalex.org/W3104156511",
        "https://openalex.org/W4282937925",
        "https://openalex.org/W3208587672",
        "https://openalex.org/W4386704768",
        "https://openalex.org/W3121490819",
        "https://openalex.org/W3112337105"
    ],
    "abstract": "Abstract We demonstrate the performance and workload impact of incorporating a natural language model, pretrained on citations of biomedical literature, on a workflow of abstract screening for studies on prognostic factors in end-stage lung disease. The model was optimized on one-third of the abstracts, and model performance on the remaining abstracts was reported. Performance of the model, in terms of sensitivity, precision, F1 and inter-rater agreement, was moderate in comparison with other published models. However, incorporating it into the screening workflow, with the second reviewer screening only abstracts with conflicting decisions, translated into a 65% reduction in the number of abstracts screened by the second reviewer. Subsequent work will look at incorporating the pre-trained BERT model into screening workflows for other studies prospectively, as well as improving model performance.",
    "full_text": "Ng et al. Systematic Reviews          (2023) 12:172  \nhttps://doi.org/10.1186/s13643-023-02353-8\nLETTER\nSemi-automating abstract screening \nwith a natural language model pretrained \non biomedical literature\nSheryl Hui‑Xian Ng1*  , Teow Kiok Liang1, Gary Yee Ang1, Tan Woan Shin1† and Allyn Hum2,3† \nAbstract \nWe demonstrate the performance and workload impact of incorporating a natural language model, pretrained \non citations of biomedical literature, on a workflow of abstract screening for studies on prognostic factors in end‑\nstage lung disease. The model was optimized on one‑third of the abstracts, and model performance on the remain‑\ning abstracts was reported. Performance of the model, in terms of sensitivity, precision, F1 and inter‑rater agree‑\nment, was moderate in comparison with other published models. However, incorporating it into the screening \nworkflow, with the second reviewer screening only abstracts with conflicting decisions, translated into a 65% \nreduction in the number of abstracts screened by the second reviewer. Subsequent work will look at incorporating \nthe pre‑trained BERT model into screening workflows for other studies prospectively, as well as improving model \nperformance.\nKeywords Abstract, Classification, Semi‑automation\nIntroduction\nIn recent years, there has been a growth in interest in \nusing artificial intelligence methods in systematic reviews \n(SRs) [1], in particular for the stage of literature screening \n[2]. As the number of titles and abstracts to be screened \nfor suitability for inclusion in a review often involves \nnumerous hours of repetitive work, semi-automation of \nthis stage has been suggested to deliver workload and \ntime savings with acceptable recall and precision [3–5].\nOne approach targets the automated classification of \nstudies for inclusion using prediction models. In recent \nwork, Aum et  al. developed a Bidirectional Encoder \nRepresentations from Transformer (BERT) algorithm \nthat was pretrained on published SRs and fine-tuned on \nanother SR, with good classification performance. The \nauthors recommended generalizing the use of BERT-\nbased models for this purpose, by pre-training with \ninformation from a particular clinical domain and opti -\nmizing the predictions for the individual review only at \nthe last fine-tuning step [6]. In this letter, we demonstrate \nthe performance and workload impact of incorporating a \nBERT model pretrained on citations of biomedical litera -\nture in our own abstract screening workflow.\nMethods\nWe used abstracts retrieved from a previous literature \nsearch on prognostic factors in end-stage lung disease \n[7]. Bibliographic databases such as MEDLINE, Embase, \nPubMed, CINAHL, Cochrane Library and Web of \nOpen Access\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecom‑\nmons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nSystematic Reviews\n†Tan Woan Shin and Allyn Hum are joint senior authors.\n*Correspondence:\nSheryl Hui‑Xian Ng\nsheryl_hx_ng@nhg.com.sg\n1 Health Services and Outcomes Research, National Healthcare Group, 3 \nFusionopolis Link, #03‑08, Singapore 138543, Singapore\n2 Department of Palliative Medicine, Tan Tock Seng Hospital, 11 Jalan Tan \nTock Seng, Singapore 308433, Singapore\n3 The Palliative Care Centre for Excellence in Research and Education, \nDover Park Hospice, 10 Jalan Tan Tock Seng, Singapore 308436, Singapore\nPage 2 of 3Ng et al. Systematic Reviews          (2023) 12:172 \nScience were searched using a pre-defined search strat -\negy and inclusion criteria (Additional file  1). A total of \n21,645 abstracts were retrieved, and based on screening \nby reviewers, 530 (2.5%) of the studies were included in \nthe subsequent stage, where the full text of the articles \nwas retrieved for thorough reading.\nThe dataset of 21,645 abstracts consisted of the text \nwithin the abstract, excluding the title, as well as an indi -\ncation of whether the abstract was classified as included \nby the human reviewers. For model validation, the data -\nset was split into a training set of 7142 abstracts (33%), \nand a test set of 14,503 abstracts (67%). We then used \nthe training set to fine-tune a BERT model pretrained on \ncitations from MEDLINE/PubMed (pBERT). A batch size \nof 64 was used, and convergence over 100 epochs was \nassessed [8].\nWe then applied the fine-tuned pBERT to the test set \nand labelled 2.5% of articles with the highest predicted \nprobabilities of inclusion as included in the review by \npBERT. Based on this set of labels, we assessed sensitiv -\nity, precision, F1 and accuracy of pBERT, as well as the \nproportion of conflicts and level of inter-rater agreement, \nwhich was measured by Cohen’s kappa (Table 1). We also \nreport the reduction in workload in a hypothetical sce -\nnario where pBERT performs screening as the second \nreviewer for 67% of the articles.\nResults\nOf the 14,503 abstracts in the test set, the human review -\ners deemed 355 (2.5%) to be relevant and suitable for \ninclusion in the subsequent stage of review. Sensitivity, \nprecision and F1 of pBERT were 37.7%, while disagree -\nment occurred for 3.0% of all articles screened. Cohen’s \nKappa was 0.70, indicating moderate agreement between \nthe reviewers and pBERT (Table 1).\nIn the traditional screening process, each of the two \nhuman reviewers would have to screen all 21,645 arti -\ncles for relevance to the study, before reviewing any \nconflicts in their decisions. With pBERT incorporated \ninto the screening workflow, both reviewers would \nscreen the first 33% of articles, and pBERT would be \nfine-tuned based on this training set.\nFor the remaining 14,503 articles, the first reviewer \n(R1) would screen all the articles in accordance with \nthe traditional workflow. pBERT would then replace the \nsecond human reviewer (R2) in identifying studies for \ninclusion, while R2 would only step in to review arti -\ncles with conflicting decisions. In this scenario, R1 and \npBERT would have agreed on decisions for 14,061 arti -\ncles, leaving 442 articles (3% of 14,503) for R2 to review. \nHence, R2 would have to review only 7,584 articles \n(7,142 + 442) or 35% of the original 21,645 articles.\nDiscussion\nWe applied a BERT model pretrained on biomedical lit -\nerature to our data, with moderate model performance. \nHaving a sensitivity of 37.7% entails that pBERT can \nonly be used as an assistant alongside a human reviewer \nto increase the efficiency of screening, as opposed to \nbeing a standalone tool for automation of screening. \nWhile pBERT did not perform as well in terms of tra -\nditional metrics compared to recent models [6 , 9], our \ndataset did have a lower inclusion rate of 2.5%, com -\npared to 11% and 19% in both studies, impacting the \npredictive ability of the model.\nNonetheless, despite the constrained performance of \npBERT, we were able to demonstrate that incorporat -\ning pBERT in our workflow would have reduced the \nworkload of a second human reviewer to a third of the \ninitial volume. Our results suggest that involving pre -\ndictive tools to screen out irrelevant articles, which \noften comprise the bulk of the abstracts, can improve \nefficiency of screening processes in comparison to tra -\nditional approaches. However, while there is interest \nto fully automate the task of screening without human \nintervention, we emphasize that the role of a human \nreviewer remains pertinent to ensure all potentially rel -\nevant articles are included in the study [10].\nTable 1 List of performance measures assessed\nMeasure Definition Estimate\nRecall/sensitivity Number of abstracts included by human reviewer and pBERT\nNumber of abstracts included by human reviewer   37.7%\nPrecision/positive predictive value Number of abstracts included by human reviewer and pBERT\nNumber of abstracts included by pBERT   37.7%\nF1 2 × precision×recall\nprecision+recall 37.7%\nAccuracy Number of abstracts included by human reviewer and pBERT\nTotal number of abstracts screened   70.2%\nDisagreement Number of abstracts with diﬀerent decisions by human reviewer a nd pBERT\nTotal number of abstracts screened   3.0%\nPage 3 of 3\nNg et al. Systematic Reviews          (2023) 12:172 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nConclusion\nFor semi-automation of screening of literature on prog -\nnostic factors in end-stage lung disease, we used a \nBERT model trained on biomedical literature to identify \nabstracts that were relevant to the topic and demon -\nstrated a substantial reduction in screening workload. \nSubsequent work will look at integrating the current ver -\nsion of pBERT into screening workflows for other stud -\nies prospectively, as well as incorporating other ensemble \nmethods to develop models with improved sensitivity to \nidentify abstracts of relevance to the research question.\nAbbreviations\nAI  Artificial intelligence\nBERT  Bidirectional Encoder Representations from Transformer\npBERT  PubMed BERT\nR1  Reviewer 1\nR2  Reviewer 2\nSR  Systematic reviews\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13643‑ 023‑ 02353‑8.\nAdditional file 1. Search criteria and strategy. Appendix 1. Study eligibil‑\nity criteria. Appendix 2. Search strategy.\nAcknowledgements\nThe authors acknowledge the contributions by the Medical Library team from \nthe Lee Kong Chian School of Medicine, National Technological University, \nSingapore, for creating and optimizing the database search. The authors also \nacknowledge the support on this project rendered by our colleague, Eric \nChua, Senior Executive, from the Health Services and Outcomes Research \nDepartment, National Healthcare Group, Singapore.\nAuthors’ contributions\nSHXN drafted the manuscript. TKL analysed the data. GYA, TWS, and AH made \nmajor revisions to the draft. TWS and AH acquired the data.\nFunding\nThis work was supported by the National Medical Research Council, Singapore \n(grant number HSRGEoL18may‑0003). The funder had no involvement in any \naspect of this study or decision to publish.\nAvailability of data and materials\nData arising from the review may be made available from the corresponding \nauthor upon reasonable request.\nDeclarations\nEthics approval and consent to participate\nEthics approval was granted by the review board of the National Healthcare \nGroup as part of a larger study on prognosticating end‑stage organ failure \n(Domain‑Specific Review Board Study Reference No. 2019/00032). Consent to \nparticipate is not applicable for this study.\nConsent for publication\nConsent for publication is not applicable for this study.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 11 August 2023   Accepted: 13 September 2023\nReferences\n 1. Marshall IJ, Wallace BC. Toward systematic review automation: a practical \nguide to using machine learning tools in research synthesis. Syst Rev. \n2019;8(1):163.\n 2. Blaizot A, Veettil SK, Saidoung P , Moreno‑Garcia CF, Wiratunga N, Aceves‑\nMartins M, et al. Using artificial intelligence methods for systematic \nreview in health sciences: a systematic review. Res Synth Methods. \n2022;13(3):353–62.\n 3. Gates A, Guitard S, Pillay J, Elliott SA, Dyson MP , Newton AS, et al. Per‑\nformance and usability of machine learning for screening in systematic \nreviews: a comparative evaluation of three tools. Syst Rev. 2019;8(278).\n 4. Gates A, Gates M, DaRosa D, Elliott SA, Pillay J, Rahman S, et al. Decoding \nsemi‑automated title‑abstract screening: findings from a convenience \nsample of reviews. Syst Rev. 2020;9(272).\n 5. Feng Y, Liang S, Zhang Y, Chen S, Wang Q, Huang T, et al. Automated \nmedical literature screening using artificial intelligence: a systematic \nreview and meta‑analysis. J Am Med Inform Assoc. 2022;29(8):1425–32.\n 6. Aum S, Choe S. srBERT: automatic article classification model for system‑\natic review using BERT. Syst Rev. 2021;10(285).\n 7. Ng SHX, Chai GT, Gunapal PPG, Kaur P , Yip WF, Chiam ZY, et al. Prognostic \nfactors of mortality in non‑COPD chronic lung disease: a scoping review. \nJ Palliat Med. 2023. https:// doi. org/ 10. 1089/ jpm. 2023. 0263.\n 8. TensorFlow Hub. TF2.0 Saved Model (v2). 2023 (Available from: https:// \ntfhub. dev/ google/ exper ts/ bert/ pubmed/2).\n 9. Qin X, Liu J, Wang Y, Liu Y, Deng K, Ma Y, et al. Natural language process‑\ning was effective in assisting rapid title and abstract screening when \nupdating systematic reviews. J Clin Epidemiol. 2021;133:121–9.\n 10. Popoff E, Besada M, Jansen JP , Cope S, Kanters S. Aligning text mining and \nmachine learning algorithms with best practices for study selection in \nsystematic literature reviews. Syst Rev. 2020;9(293).\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub‑\nlished maps and institutional affiliations."
}