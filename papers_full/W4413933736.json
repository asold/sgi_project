{
  "title": "Leveraging Large Language Models in Extracting Drug Safety Information from Prescription Drug Labels",
  "url": "https://openalex.org/W4413933736",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5061872875",
      "name": "Undina Gisladottir",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5037563245",
      "name": "Michael Zietz",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5116065993",
      "name": "Sophia Kivelson",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5103120798",
      "name": "Yutaro Tanaka",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5056871857",
      "name": "Gaurav Sirdeshmukh",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5026177027",
      "name": "Kathleen LaRow Brown",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5086753691",
      "name": "Nicholas P. Tatonetti",
      "affiliations": [
        "Cedars-Sinai Medical Center",
        "Columbia University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W145028589",
    "https://openalex.org/W1690976696",
    "https://openalex.org/W4243829886",
    "https://openalex.org/W2810132515",
    "https://openalex.org/W4225004767",
    "https://openalex.org/W2017851316",
    "https://openalex.org/W2113158945",
    "https://openalex.org/W4365503173",
    "https://openalex.org/W4393156180",
    "https://openalex.org/W2791213314",
    "https://openalex.org/W2119002393",
    "https://openalex.org/W3120621182",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4390745503",
    "https://openalex.org/W4380591192",
    "https://openalex.org/W4310463254",
    "https://openalex.org/W2970198438",
    "https://openalex.org/W2142572836",
    "https://openalex.org/W2145578524",
    "https://openalex.org/W4390546587",
    "https://openalex.org/W4391292768",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W2794085611",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W4404515031",
    "https://openalex.org/W3146879076",
    "https://openalex.org/W4404781779",
    "https://openalex.org/W3124481204",
    "https://openalex.org/W2979250794",
    "https://openalex.org/W2129767020",
    "https://openalex.org/W2095896451",
    "https://openalex.org/W4386187806",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4386576685",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W4380729715",
    "https://openalex.org/W4401165605",
    "https://openalex.org/W4410415802",
    "https://openalex.org/W4402499684"
  ],
  "abstract": "Generative language models demonstrate significant potential for automating drug safety information extraction from SPLs, offering a promising avenue for improving post-market surveillance and reducing ADRs. Future work should focus on refining prompting strategies and expanding the models' capabilities to handle increasingly complex and nuanced drug safety information.",
  "full_text": "Vol.:(0123456789)\nDrug Safety \nhttps://doi.org/10.1007/s40264-025-01594-x\nORIGINAL RESEARCH ARTICLE\nLeveraging Large Language Models in Extracting Drug Safety \nInformation from Prescription Drug Labels\nUndina Gisladottir1 · Michael Zietz1,2 · Sophia Kivelson2 · Yutaro Tanaka1,4,5,6 · Gaurav Sirdeshmukh3 · \nKathleen LaRow Brown1 · Nicholas P . Tatonetti1,2,3 \nAccepted: 15 July 2025 \n© The Author(s) 2025\nAbstract\nIntroduction Adverse drug reactions (ADRs), including those resulting from drug interactions, remain a leading cause of \nmorbidity and mortality. Structured product labels (SPLs) serve as a primary source for drug safety information. Having \nmachine-readable product labels, including adverse reactions (ARs) and drug interactions, readily available would allow \nresearchers to streamline medication safety studies. However, extracting this information is complex and requires the use of \nnatural language processing (NLP) methods.\nObjective In this study, we explored the application of generative language models in the extraction of drug safety informa-\ntion from SPLs.\nMethods We compared multiple generative LLMs (GPT, Llama, and Mixtral) to two baseline methods in the task of extract-\ning adverse reactions (ARs) from SPLs. We explored various factors, such as prompting strategies and term complexity, \nthat impact the performance of these models in the extraction of ARs. Finally, we explored the generative models' capacity \nto extract drug interactions from a separate section of SPLs without additional fine-tuning or training, demonstrating their \nflexibility and adaptability for information retrieval.\nResults We found that generative language models, specifically GPT-4, are able to match or exceed the performance of \nprevious state-of-the-art models without additional training or fine-tuning. Additionally, we found that the specific SPL sec-\ntion, surrounding context, and complexity of the AR term impacted the extraction performance. Finally, we demonstrated \nthe generalizability of these models by applying them to a separate task of extracting drug names from the drug interaction \nsection where curated training data are not available.\nConclusion Generative language models demonstrate significant potential for automating drug safety information extraction \nfrom SPLs, offering a promising avenue for improving post-market surveillance and reducing ADRs. Future work should \nfocus on refining prompting strategies and expanding the models’ capabilities to handle increasingly complex and nuanced \ndrug safety information.\nExtended author information available on the last page of the article\n U. Gisladottir et al.\nKey Points \nGenerative models, like GPT-4, can effectively extract \nimportant drug safety information directly from struc-\ntured product labels, without additional training.\nMultiple factors, such as the surrounding context and \ntext length, impact the performance of these models, \nhighlighting the complexity of information extraction.\n1 Introduction\nAdverse drug reactions (ADRs) are a leading cause of mor-\nbidity and mortality and are a significant source of economic \nburden in the USA [1 –7]. A comprehensive data source of \ndrug safety information is critical for ensuring the ongoing \nsafety of medication and informing clinical practice. High-\nthroughput systematic studies rely on comprehensive, acces-\nsible, and up-to-date data sources to determine previously \nunidentified drug safety signals quickly and efficiently [8 , \n9]. Structured Product Labeling (SPL), a standardized docu-\nment format mandated by the US Food and Drug Admin-\nistration (FDA), provides critical information on marketed \ndrugs. SPLs contains sections, such as the “ Adverse Reac-\ntions” (ARs), “Warnings and Precautions” (WP), “Boxed \nWarnings” (BWs), and “Drug Interactions” (DIs), which \ncontain insights into potential drug-related risks [ 10–13]. \nExtracting this information in a machine-readable format is \nessential for high-throughput drug safety research.\nDue to the volume and complexity of SPLs there are \nsignificant challenges to manual review and information \nretrieval. However, the automated extraction of this infor -\nmation is complex and requires the use of natural language \nprocessing (NLP) methods [12, 14, 15]. The current stand-\nard for extracting drug safety information involves the use \nof bidirectional transformer (BERT) based models [9 , 16, \n17]. BERT-based models have demonstrated impressive \nperformance across various NLP tasks. Specifically, mod-\nels like BioBERT and PubMed-BERT have been fine-tuned \nfor extracting biomedical information from various text \nsources [14, 15, 18–23]. Despite their success, these models \nrequire extensive training and large sets of manual annota-\ntions, which are both time-consuming and resource inten-\nsive. Moreover, BERT-based models are typically designed \nfor specific tasks, limiting their flexibility and applicability \nacross different drug safety information extraction needs.\nThe task of extracting ARs from SPLs has a defined his-\ntory within the biomedical NLP research community. Founda-\ntional work in adverse event extraction led to databases such as \nSPLICER [24] and SIDER [25, 26], which compiled adverse \nevents extracted from drug labels. With the emergence of more \nadvanced NLP methods, in 2017, the Text Analysis Confer-\nence (TAC) launched the Adverse Drug Reaction Extraction \nfrom Drug Labels task, aimed at fostering the development of \ncomputational techniques to identify AR mentions within drug \nlabels and additional modifiers [27].\nThe current state-of-the-art method for extracting ARs from \nSPLs is DeepCADRME, introduced by El-Allaly et al. [17]. \nDeepCADRME is a deep neural network designed specifically \nto extract both simple and complex ARs from SPL text, trained \nusing the annotated dataset released by TAC. Alongside Deep-\nCADRME, other approaches such as RxBERT have also been \nproposed, leveraging BERT-based models for the extraction of \ninformation from SPLs [17, 28]. Building on DeepCADRME \n[17], Tanaka et al. [9] developed OnSIDES (ON-label SIDE \neffects resource), a compiled, computable database of over 3.6 \nmillion drug and adverse event pairs. However, currently the \ndatabase did not reach sufficient performance to include ARs \nextracted from all relevant sections of the SPL.\nThe emergence of generative large language models \n(LLMs) provides an opportunity to improve the extraction \nof drug safety information from SPLs. Generative LLMs, \nsuch as Generative Pre-trained Transformer (GPT) models, \nLlama, and Mixtral, have shown impressive performance \nin processing and generating text across diverse domains, \nincluding biomedical text [29–31]. These models have been \nincreasingly leveraged in the medical field to address tasks \nranging from clinical note summarization to pharmacovig-\nilance signal detection [32]. Yet, their application to AR \nextraction from SPLs—a task demanding precision and con-\ntextual understanding—remains unexplored.\nIn this study, we evaluate the ability of generative LLMs \nto extract drug safety information from SPLs, comparing \ntheir performance to traditional approaches, including exact \nMedical Dictionary for Regulatory Activities (MedDRA) \nstring matching and the state-of-the-art supervised model, \nDeepCADRME [17]. Our analysis not only evaluates the \nperformance of these models but also explores factors influ-\nencing their performance, such as SPL section origin, text \nlength, and the surrounding context and complexity of the \nextracted terms. By evaluating generative LLMs against \ntraditional methods and exploring their limitations, this \nwork furthers our understanding of the application of gen-\nerative LLMs in biomedical text mining, providing insights \ninto their potential for improving pharmacovigilance and \ndrug safety research.\nLeveraging Large Language Models in Extracting Drug Safety Information\n2  Materials and Methods\n2.1  Data Description\nStructured Product Labeling (SPL) is a document mark-up \nstandard approved by Health Level Seven (HL7) and adopted \nby the Food and Drug Administration (FDA) [11]. The SPLs \nsupport the distribution of up-to-date information regarding \nmarketed pharmaceutical products organized thematically \nin different sections of the label [12]. SPLs contain sections \nrelevant to drug safety such as: “Adverse Reactions” (AR), \n“Warnings and Precautions” (WP), and “Boxed Warnings” \n(BW) [27, 33]. The AR section primarily lists adverse events \nexperienced by patients enrolled in the clinical trials phases, \nsometimes including information on frequency and severity. \nIt may also list adverse events identified during post-mar -\nketing pharmacovigilance studies. The WP section is geared \ntoward practitioners and provides details on the more signifi-\ncant risks associated with the drug and includes guidelines \non preventing and managing these risks. Finally, the BW \nsection highlights the most serious or life-threatening risks \nassociated with the drug. It is the strongest warning that the \nFDA requires and is used to alert healthcare professionals to \nmake sure they are aware of these critical safety concerns. \nSPLs are made publicly available on the National Library \nof Medicine’s (NLM) DailyMed website as XML files and \nare updated periodically (daily, weekly, or monthly) by the \nFDA [34].\nIn this study, we used a dataset of over two thousand pre-\nprocessed SPLs, which included a training set of 101 labels, \na test set of 99 labels, and an additional 2208 unannotated \nlabels. We used the 200 SPLs that were manually annotated \nwith ARs developed in a study by Demner-Fushman et al. \n[33]. These SPLs, both the annotated and unannotated labels, \nare publicly available as XML files (https:// bionlp. nlm. nih. \ngov/ tac20 17adv erser eacti ons/). The manually extracted ARs \nwere mapped to the MedDRA preferred term (PT) concepts. \nIn addition to the extracted AR term, the annotated data \nreported a start position (start tag) as well as relationships \nto other modifiers such as severity or negation. In NLP, start \nand stop tagging involves the delineation of specific textual \nentities within a document by denoting their respective start-\ning and ending character indices.\n2.2  Large Language Models and Baseline Methods\nWe considered many of the latest generative LLMs to \ninvestigate their use in the extraction of adverse reactions \n(ARs). These models have been trained on a vast amount of \ndiverse text, allowing them to perform a wide variety of NLP \ntasks such as text completion, translation, summarization, \nand question-answering [35]. Specifically, we explored the \nperformance of Generative Pre-trained Transformer (GPT) \nmodels developed by OpenAI, Llama (Large Language \nModel Meta AI) models developed by Meta, Mixtral of \nexperts (a pretrained generative Sparse Mixture of Experts) \nmodel developed by Mixtral AI (Table 1) [29, 36–38].\nWe considered two baseline methods. The first was the \ndirect extraction of exact Medical Dictionary for Regulatory \nActivities Preferred Term (MedDRA PT) string matches \n(referred to as 'Exact MedDRA'). The second is the cur -\nrent state-of-the-art model, DeepCADRME, a Bidirectional \nEncoder Representations from Transformers (BERT)-based \nmodel specifically fine-tuned to extract ARs from SPLs \nusing the same prescription drug labels used in this study \n[17].\n2.3  Large Language Model Parameters\n2.3.1  Context Window\nThe context window length—a key measure of a LLM—is \nthe number of tokens (a combination of words, punctua-\ntion, and spaces) it can consider at once when generating \nor analyzing text. The concept of a context window refers \nto the total limit of tokens (often around 4 characters) that \ncan be used between the prompt (input from the user) and \nthe response (output from the LLM). The language models \nused in this study have different context window lengths, \nand their performance can be reduced when inputs exceed \nthe context window [ 36, 39]. The relevant SPL sections \nfor AR extraction vary in token length, from just a few \ntokens to roughly 10,000 tokens in length (Fig.  1). Such \nlarge input sizes pose a challenge for models like GPT3.5 \nand Llama 2, which have a context window of only 4096 \ntokens (marked with a red dotted line on Fig.  1). However, \nthe context window represents the total number of tokens \nencompassing both the prompt and the response. This is \nshown in Fig.  1, where the purple dotted line represents \na 4096-token context window after subtracting the largest \nresponse (1000 tokens). Any SPL text exceeding this limit \nwould be truncated when using these smaller models. The \nGPT models have a variety of context window sizes with \nrecent models such as gpt-4-1106-preview being able to \nhandle a context window of 128,000 tokens (Table 1). This \nallows such models to incorporate more information by \nhandling larger inputs.\n2.3.2  Prompting Strategies\nWith LLMs, it is important to consider both system and user \nprompts. A system prompt guides the model in understanding \nthe context and purpose of the desired output and influences \nthe generated responses accordingly. This prompt generally \nlays out the role or persona the model should take and the \n U. Gisladottir et al.\nappropriate tone for the response. A user prompt refers to \nthe input or query provided by a user to a language model. \nThis prompt is generally the question or specific task the user \nwants to ask the language model. Given the inherent com-\nplexity and diversity of language, different prompts may elicit \nvaried responses [40–42]. In this study, we explored multi-\nple system- and user-prompts (Supplementary Tables 1 and \n2, respectively). All user prompts included the text from the \nrelevant section of the SPL and prompted the generative model \nto extract all ARs. The prompts were first tested on GPT-4, and \nthe top prompts were then applied to the other LLMs. Addi-\ntionally, we prompted GPT to write a prompt specific to each \nLLM. Generative models are non-deterministic, meaning that \nthe LLM can produce different outputs even when given the \nsame input multiple times [43, 44]. To determine consistency, \nwe ran the same prompt and parameters combination three \ntimes and evaluated performance to assess the variability in \noutput. Finally, due to the nature of generative models, they \nare prone to hallucinations. Hallucinations in generative LLMs \nrefer to the generation of content that is irrelevant, made up, \nor inconsistent with the input data [ 45, 46]. In LLMs, tem-\nperature is a numerical parameter that controls the balance \nbetween predictability and creativity in its output. The lower \nthe ‘temperature’, the more deterministic the results while a \nhigher temperature leads to more randomness, which encour-\nages more diverse or creative outputs [47]. To decrease the \nlikelihood of hallucinations, we set the ‘temperature’ param-\neter to zero. Additionally, hallucinations are captured as false \npositives in the evaluation.\nFinally, we explored one-shot (one example) and two-\nshot (two examples) prompting using GPT-4, which has a \ncontext window of 128k tokens. Shot prompting is a method \nused with transformer-based models, like GPT, to guide the \nmodel's output using examples of desired outputs for a given \nprompt/input, known as ‘shots’ [48]. The examples were \ndrawn from the “Adverse Reactions” section of the SPLs \nand the expected output was created by concatenating the \ncorresponding adverse reaction terms as a comma-separated \nlist (Supplementary Table 3).\nTable 1  Overview of \ngenerative large language \nmodels\nThe table shows the name of the generative large language models (LLM), the specific model used in this \nstudy, the developers, the latest date of training data retrieval, and the context size\nModel name Specific model Developer Training data Context window\nGPT-3.5 gpt-3.5-turbo-0125 OpenAI Up to Sep 2021 4096 tokens\nGPT-4 gpt-4-1106-preview OpenAI Up to Apr 2023 128,000 tokens\nLlama 2 code-llama-34b Meta Up to Sep 2022 4096 tokens\nLlama 3 CodeLlama-34b-Instruct-hf Meta Up to Mar 2023 16,000 tokens\nMixtral Mixtral-8x7B-Instruct-v0.1 Mixtral AI – 32,000 tokens\nFig. 1  Histogram showing the number of tokens needed to (left) \ninclude the entire SPL section in the user prompt and (right) include \nall extracted adverse drug events (ADEs) in generative language \nmodel response. Additionally, the graph is annotated to mark a con-\ntext window size of 4096 tokens (red dotted line) as well as after \nremoving the number of tokens needed for the maximum response \ntoken length (purple dotted line). A context window size of 4096 \ntokens was common at the start of this analysis; however, more recent \ngenerative language models have much larger context windows. The \nnumber of tokens needed has implications for the necessary context \nwindow size of the large language models\nLeveraging Large Language Models in Extracting Drug Safety Information\n2.4  Evaluation\n2.4.1  Performance Metrics\nWe sought to evaluate whether LLMs can comprehensively \nextract terms without incorrectly including or hallucinat-\ning absent adverse reaction terms. We evaluated the mod -\nels using the following performance metrics: precision, \nrecall, and F1-score. As we evaluated the precision, recall, \nand F1-score for each drug label individually there are two \nmethods of aggregating these metrics, micro-averaged and \nmacro-averaged. We considered both. The micro-averaged \nmetrics are calculated by summing the true positives (TP), \nfalse positives (FP), and false negatives (FN) across the drug \nlabels and recomputing the precision, recall, and F 1-score. \nThe macro-averaged metrics are calculated by taking the \nmean of the performance metrics across all labels separately \nfor each section.\n2.4.2  Strict Matching\nTo evaluate the ability of these methods to extract ARs, it \nwas important to define TPs, FPs, and FNs. The most tradi-\ntional and stringent criteria involve considering exact string \nmatches between the manual annotation and the model-\nextracted terms. In other words, the term extracted by the \nLLM must match one of the manually annotated terms from \nthe reference set exactly. For the exact matches, TPs were the \nnumber of terms in the intersection of the set of manually \nannotated terms and the set of extracted terms. The set dif-\nference between the extracted terms and manually annotated \nterms was considered FP and the set difference between the \nmanually annotated terms and extracted terms was consid-\nered FN. However, this may not offer a complete picture \nof the overall performance of these models, which is why \nmany researchers in this space consider lenient measures of \nperformance.\nAllowing lenient measures is motivated both by the exam-\nples above and the low inter-annotator agreement when \ndetermining the exact boundary of adverse event entities \n[15, 49, 50]. Instead of limiting our evaluation to exact \nmatches, we evaluated our models using two methods for \nlenient matching: lexical matching using the longest com-\nmon substring, and semantic matching using cosine similar-\nity of word embeddings. These are explained in the follow-\ning two sections.\n2.4.3  Lenient Lexical Matching\nThe lenient metrics are calculated by considering a TP when \nthe extracted entity is partially or completely correct [15, 23, \n51]. In the lenient evaluation, we consider a manually anno-\ntated term as an FN if it does not have a partial match to any \nextracted terms. In identifying TPs and FPs, previous work \nhas defined a lenient or relaxed match as an entity whose \nboundaries overlap with the annotated boundaries and shares \nthe same entity type with the gold standard as acceptable \n[31]. Using overlapping boundaries, however, is not appli-\ncable to the output of generative models. Generative models, \nlike those in the GPT series, do not explicitly provide start \ntagging when extracting information from text. These mod-\nels generate text based on the learned patterns in the training \ndata without referencing the boundaries of extracted infor -\nmation in the source [52, 53]. Instead of using the annotated \nboundaries of the terms, we calculated the longest common \nsubstring proportion of the extracted term and the manual \nannotation. In this study, we defined an extracted term as \na TP if the overlap (longest common string) proportion \nexceeded 0.8, a threshold identified through manual review. \nFinally, an extracted term was marked as a FP if it didn't \nhave a partial match to any manually annotated terms.\n2.4.4  Semantic Matching\nWe further explored the performance on a semantic level. \nThis was motivated by cases where the generative models \nreturned terms or acronyms that were synonymous with the \nmanual annotation. For example, the acronym “PRES” can \nrepresent the phrase “posterior reversible encephalopathy \nsyndrome”. To perform semantic matching, we embedded \nevery extracted AR term and the AR terms in the manual \nannotation. We used the Massive Text Embedding Bench-\nmark (MTEB) Leaderboard to identify top-performing \nembedding models [54]. We considered the top performing \nmodel in the ‘Pair Classification’ performance metrics after \nexcluding models above 14 GB in size (Table  2). Larger \nmodels were excluded since they significantly increased the \ncomputational complexity with only a marginal improve-\nment in performance. Therefore, we moved forward with \nthe embedding model: llmrails/ember-v1. To determine the \nappropriate threshold for cosine similarity, we created a \ndataset of randomly sampled ARs extracted by a generative \nmodel, the best-matching manual annotation (according to \nmax cosine similarity), and the cosine similarity. Then, two \nindependent labelers went through 500 pairs and marked \nwhether or not it was a match. We calculated Cohen's Kappa \nScore to determine the inter-rater reliability [55] and defined \nthe cosine similarity threshold using the maximum F1-score.\n2.5  Investigating Factors that Impact Performance\n2.5.1  SPL section and Character Length\nWe explored the extraction of ARs from three separate sec-\ntions: “Adverse Reactions” (ARs), “Warnings and Precau-\ntions” (WPs), and “Box Warnings” (BWs). Each section \n U. Gisladottir et al.\nserves a different purpose and lists adverse reactions at \ndifferent levels of risk. Effects listed in ARs are often an \nexhaustive list of all side effects observed during clinical \ntrials and post-market experience, whether they are caus-\nally linked to the drug. Warnings and Precautions serve as a \nset of guidelines to prescribing physicians on what adverse \nevents to look out for and how the drug should be prescribed. \nBoxed Warnings are usually added after drugs are approved \nafter the FDA becomes aware of significant toxicities associ-\nated with the drug. Therefore, we evaluated the difference in \nperformance for each of the three sections used in the extrac-\ntion of ADRs. Further, we investigated whether character \nlength impacted the overall performance as this provides \ninsight into the amount of context a language model can \nhandle.\n2.5.2  Context and Term Complexity\nWe aimed to further understand the factors specific to the \nAR terms that influence performance. Specifically, we evalu-\nated the impact surrounding context (e.g., negation, uncer -\ntainty) and complexity (e.g., discontinuous terms) had on \nperformance. To assess this, we categorized AR terms from \nthe manual annotation by Demner-Fushman et al. [33] into \nfive categories: exact MedDRA matches, non-exact matches, \nnegated, hypothetical, and discontinuous terms (Table  3). \nWe classified a manually extracted term as an exact Med -\nDRA match if it mapped to a MedDRA PT concept that was \nan exact string match, otherwise, the term was categorized \nas a non-exact match. The manual annotation by Demner-\nFushman et al. [33] also included annotation on whether an \nAR term had a relation with negation or hypothetical/uncer-\ntainty terms. We used these annotations to define a term \nas negated or hypothetical. The annotated dataset defined \nnegated terms such as: ‘no’, ‘without’, and ‘excluding’. \nAdditionally, they defined the term ‘placebo’ as a negation \nterm since it applied to the placebo group and not the treat-\nment group. Hypothetical terms, on the other hand, included \nwords such as ‘may’, ‘can’, ‘risk’, and ‘possible’. They also \nincluded drug classes e.g., ’BRAF inhibitors’ and ’antide -\npressants’ as hypothetical terms. The rationale is that the \nreaction might not necessarily apply to the individual drug \nbut including it in the label ensures a comprehensive warn-\ning of potential harm. Finally, we considered a term discon-\ntinuous if the AR term had multiple start indices in the drug \nlabel text according to the manual annotation. We included \nthe term in the AR category evaluation if there was any case \nin which the term satisfied the criteria of a category defini-\ntion. Table 4 presents the number of distinct terms in each \ncategory, arranged in descending order by sample size. The \nexact MedDRA match category contains the largest number \nof terms (44.5%) while the negated category contains the \nfewest number of terms (3.2%).\n2.6  Extracting Drug Interactions\nTo explore the broader applicability of the generative mod-\nels, we applied the best-performing generative model in the \ntask of extracting drug names from the ‘Drug Interaction’ \nsection of the SPL. All mentioned drugs and products were \nextracted without regard to negation of interaction claims. \nTo evaluate the ability of these models to extract drug names \nfrom the drug interaction section, two labelers manually \nannotated the drug interaction section from 100 SPLs. The \nlabelers were tasked with extracting any mention of a drug \nname. The two sets of manual annotations were merged by \nhand and the SPLs were re-consulted to ensure that no drug \nname was missed or falsely included to build a consensus \nmanual annotation.\nWe evaluated the performance in two ways. First, we \nevaluated how well the extracted drug names matched the \nmanual annotations through semantic matching, using the \nsame embedding model ( llmrails/ember-v1 ) and cosine \nsimilarity threshold determined previously (see Section \nSemantic Matching). Any extracted drug name with a cosine \nsimilarity greater than the threshold was considered a match \n(TP). We evaluated the precision (fraction of extracted drug \nnames that matched a manual annotation), recall (fraction of \nTable 2  Massive text embedding benchmark (MTEB) leaderboard for pairwise classification\nData retrieved 05/10/24\nThe table shows the model names, model size (number of parameters), memory usage (GB), embedding dimensions, max number of tokens, \naverage performance across all tasks, and pair classification performance. The models are in descending order according to the pair classification \nperformance\nEmbedding model Model size \n(params)\nMemory (GB) Embed dim Max tokens Average per-\nformance\nPair classification\nSFR-Embedding-Mistral 7111 26.49 4096 32768 67.56 88.54\ne5-mistral-7b-instruct 7111 26.49 4096 32768 66.63 88.34\nember-v1 335 1.25 1024 512 63.54 87.37\necho-mistral-7b-instruct-lasttoken 7111 26.49 4096 32768 64.68 87.34\nUAE-Large-V1 335 1.25 1024 512 64.64 87.25\nLeveraging Large Language Models in Extracting Drug Safety Information\nmanual annotations that were matched by an extracted drug \nname), and F1 score on a per-label basis.\nSecond, we mapped both the extracted drug names and \nmanual annotations to RxNorm ingredients and evaluated \nhow reliably the two methods selected the same RxNorm \nconcepts. To match annotations to RxNorm concepts, we \nused the same embedding and cosine similarity method, but \nthis time with a more stringent cutoff (0.9), based on an \ninspection of the matches for manual annotations. A thresh-\nold of 0.9 was more appropriate when considering the terms \nwere mapped to the standardized vocabulary. Once again, \nwe evaluated the precision, recall, and F1 score using the \nmapped RxNorm ingredients.\n3  Results\n3.1  Strong Inter‑rater Reliability Allows \nfor Semantic Evaluation\nTo investigate the performance of generative models in their \nability to extract adverse reactions (ARs) from Structured \nProduct Labels (SPLs), we considered three evaluation \nmethods:\n• Strict matching. Matches determined using exact string \nmatches.\n• Lenient lexical matching. Matches determined using the \nproportion of the longest common substring (≥0.8).\n• Semantic matching. Matches determined using cosine \nsimilarity.\nThe first two evaluation methods have clear definitions for \ntrue positives (TP), false positives (FP), and false negatives \n(FN). For the third evaluation method, semantic mapping, \nwe had to determine the appropriate semantic similarity \nthreshold. To determine this threshold, we constructed a \ndata source of randomly sampled extracted adverse reac-\ntion terms paired with the best-matching manually anno-\ntated term (using max cosine similarity of embeddings) \nand the cosine similarity between the two terms based on \ntheir embeddings. Two labelers then marked whether these \nterms were or were not a match. To evaluate the agreement, \nwe calculated Cohen’s Kappa score, which revealed that \nTable 3  Adverse reaction category definitions\nTable shows the ADR category name, the definition, as well as a few examples from the structured product label text and manual annotation\nCategory Definition Example(s)\nExact MedDRA match A term that has an exact string match to the MedDRA \nPT concept according to the manual annotation\nManually extracted term: \"hypersensitivity\"\nMedDRA concept: \"Hypersensitivity\" (ID: 10020751)\nNon-exact MedDRA match A term that does not have an exact string match with \nthe MedDRA concept according to the manual \nannotation\nManually extracted term: \"breathing difficulties\"\nMedDRA concept: \"Dyspnea\" (ID: 10013968)\nNegated (context) A term that has a manually annotated relation with a \nnegation term (‘placebo’, ’no’, ‘without’, ‘excluding’, \netc.)\n\"Cardiac failure occurred in 0.2% of patients taking \nplacebo\"\n\"was not associated with increases in BUN\"\n\"without affecting renal glomerular function\"\nHypothetical (context) A term that has a manually annotated relation with \na hypothetical term. Hypothetical terms: modifier \n(’may’, ‘can’, ‘risk’, ‘possible’, ‘potentially’, etc.) or \ndrug class (‘antidepressants’, ‘BRAF inhibitors’, etc.)\n“Patients may experience substantial procedural pain”\n“increased risk of adverse cardiovascular effects”\n”Increased cell proliferation can occur with BRAF \ninhibitors”\n“effects of [drug name] and all botulinum toxin products \nmay spread from the area”\nDiscontinuous (complexity) A multi-word term that according to the manual \nannotation has multiple start indices in the drug label, \nmeaning that it is separated by one or more words\nTerm: \"Interstitial infiltration of the chest\"\nText: \"Interstitial infiltration and/or inflammation were \nobserved in the chest\"\nTerm: \"peripheral neuropathy sensory\"\nText: \"peripheral neuropathy that is predominantly \nsensory\"\nTable 4  Category sample sizes\nThe table shows the categories used to determine the impact of con-\ntext and complexity. It shows the category name, the number of dis-\ntinct terms, and the proportion\nCategory N Proportion (%)\nAll 3623 100.00\nExact MedDRA 1613 44.52\nNon-Exact MedDRA 2010 55.48\nHypothetical 744 20.54\nDiscontinuous 705 19.46\nNegated 116 3.20\n U. Gisladottir et al.\nthe labelers had very strong inter-rater reliability (Cohen’s \nKappa = 0.871).\nWe then determined the cosine similarity threshold \nusing the max F1-score (Cosine Similarity Threshold = \n0.7097613, Max F1 = [0.975–0.980], Fig. 2).\n3.2  Generative Language Models Can Extract \nAdverse Reaction Terms from Drug Labels\nTo investigate the ability of generative models to extract \nARs from structured product labels (SPLs), we considered \nmultiple generative LLMs (Table  1). We compared these \nmodels to two baseline methods: extracting MedDRA \nterms directly (referred to as 'Exact MedDRA') and Deep-\nCADRME [17], the current state-of-the-art model trained \nspecifically to extract ARs from SPLs. In the task of extract-\ning ARs from the 'Adverse Reaction' section of SPLs, our \nresults show that GPT-4 consistently outperformed the other \ngenerative language models across all evaluation methods \n(macro-averaged F1 strict = 0.816, lenient = 0.898, and \nsemantic = 0.9657) while Llama 2 consistently performed \nthe worst (Table 5, Fig. 3). Additionally, we found that the \nsemantic matching consistently produced the best perfor -\nmance compared to other evaluation methods. Specifically, \nit improved the recall but, in some cases, such as for Llama \n3, it decreased the precision. The results indicate that Deep-\nCADRME [17], the state-of-the-art supervised model spe-\ncifically trained for this task and data source, outperformed \nthe other models. We found that GPT-4 closely approaches \nDeepCADRME's [17] performance and, in certain metrics, \nGPT-4 outperformed DeepCADRME [ 17] even without \nadditional or biomedical-specific training.\nWe explored multiple system prompts and user prompts \nand ran each combination of prompts three times to deter -\nmine consistency. We found that performance remained con-\nsistent across runs with little variability in extraction per -\nformance. The specific system prompt did not significantly \nimpact the performance of GPT-4 in the extraction of ARs \nfrom the “ Adverse Reaction ” section of the SPL (Supple -\nmentary Fig. 1). We found that the performance for this task \nwas mainly driven by the base LLM model instead of the \nspecific system + user prompt combination (Supplementary \nFig. 2). Furthermore, we found that shot prompting did not \nsignificantly improve performance (Supplementary Fig. 3).\n3.3  SPL Section Impacts Performance\nIn this study, we used a data source of previously manu-\nally annotated SPLs. The data source contained a set of \n200 SPLs, specifically sections: “Adverse Reactions” (AR), \n“Warnings and Precautions”  (WP), and “Box Warn-\nings” (BW). The character length of the three sections \nvaried, with the AR section being the longest on average \n(median = 7701 chars, IQR = [4593–13,984]), followed by \nthe WP section (median = 5129 chars, IQR = [3089–7945]), \nwith BW being the shortest (median = 1853 chars, IQR = \n[1383–2258]) (Table 6). To further understand what impacts \nthe performance of LLMs in the extraction of ARs from \nSPLs, we investigated whether the specific section and the \ncharacter length of the section impacted performance.\nWhen comparing the best performing parameters for each \ngenerative LLM model (according to max macro-F 1), we \nfound that an increase in the character length of the text \nnegatively impacted the precision of term extraction. Recall \nremained consistent, with Llama 2 showing some improve-\nment in recall with an increase in text length. However, \nLlama 2 seemed to be impacted the most by the character \nlength of the AR section. Overall, there is a slight decrease \nin the F1 score for all AR extraction methods as the charac-\nter length increases (Fig.  4). Additionally, we investigated \nhow performance varied across the different sections in the \ndrug label. The best performance was observed in the extrac-\ntion of ARs from the AR section (Fig.  5, Supplementary \nFig. 2  Cosine similarity \nthresholding. The A receiver-\noperating characteristic (ROC) \ncurve and B precision-recall \n(PR) curve. The PR curves are \nmarked with the point repre-\nsenting the maximum F1-score\n\nLeveraging Large Language Models in Extracting Drug Safety Information\nTable 4). We found that for all models, performance was the \nworst in the extraction of ARs from the WP section.\n3.4  Context and Term Complexity Impact \nPerformance\nIn addition to exploring how the SPL section and charac-\nter length impact performance, we investigated the impact \nthe surrounding context (e.g., negation, uncertainty) and \nterm complexity (e.g., discontinuous terms) had on extrac-\ntion performance. We did this by categorizing AR terms \ninto (1) exact MedDRA matches, (2) non-exact MedDRA \nmatches, (3) negated, (4) hypothetical, and (5) discontinu-\nous terms (see Table  3). It is important to note that terms \nthat fell into the negated, hypothetical, and discontinuous \ncategories were limited in sample size. Additionally, since \nthe extraction models do not classify a term into these cat-\negories, it was not possible to extract false positives. There-\nfore, we could only evaluate these categories using recall. \nWe found that the complexity (discontinuous terms) and \nthe context (e.g., negation) did negatively impact extraction \nperformance. Across all evaluation methods, discontinuous \nterms and negated terms were poorly captured by the best-\nperforming generative language model (Fig. 6).\n3.5  Extraction of Drug Interactions\nTo investigate whether generative large language mod-\nels (LLMs) could be applied to other sections relevant to \ndrug safety research without the requirement of manually \ncreated training data, we applied the best-performing model \nfrom the previous section, GPT-4, to the “ Drug Interac -\ntions” (DI) sections. Specifically, we extracted the drug \nnames from the DI sections for 100 SPLs that we manu-\nally annotated. For these 100 DI sections, GPT extracted a \ntotal of 807 drug names (496 distinct drugs). In comparison, \nthe manual annotation had a total of 1144 drug names (631 \ndistinct drugs). In the extraction of drug names from the \nDI section, GPT achieved great performance with excellent \nprecision (F1 = 0.841, precision = 0.989, recall = 0.732). \nTo evaluate whether performance would be impacted by \nmapping to a standardized vocabulary, we mapped the drug \nTable 5  Overall adverse \nreaction extraction performance\nPrecision, recall and F1 score, micro- and macro-averaged. These metrics were measured using three meth-\nods (1) semantic matching, (2) lenient lexical matching, and (3) strict exact string matching. Top perfor -\nmances are bolded\nMethod Micro Macro\nPrecision Recall F1 Precision Recall F1\nSemantic\n DeepCADRME [17] 0.9920 0.9753 0.9836 0.9932 0.9680 0.9797\n GPT-4 0.9488 0.9831 0.9657 0.9546 0.9774 0.9641\n GPT-3.5 0.8575 0.9805 0.9149 0.8800 0.9677 0.9094\n Mixtral 0.8203 0.9679 0.8880 0.8600 0.9539 0.8959\n Llama 3 0.8003 0.9419 0.8654 0.8347 0.8820 0.8739\n Llama 2 0.6885 0.9744 0.8069 0.7505 0.9625 0.8273\n Exact MedDRA 0.9553 0.7875 0.8633 0.9523 0.7422 0.8257\nLenient matching\n DeepCADRME [17] 0.9575 0.9606 0.9591 0.9539 0.9618 0.9571\n GPT-4 0.9632 0.8417 0.8984 0.9615 0.8712 0.9095\n GPT-3.5 0.9560 0.7331 0.8298 0.9516 0.7690 0.8357\n Exact MedDRA 0.7694 0.9242 0.8397 0.7333 0.9275 0.8098\n Llama 3 0.9233 0.6473 0.7610 0.8755 0.7189 0.7829\n Mixtral 0.9089 0.6324 0.7458 0.9193 0.7064 0.7791\n Llama 2 0.9560 0.5206 0.6741 0.9508 0.6088 0.7128\nStrict matching\n DeepCADRME [17] 0.8283 0.8786 0.8527 0.8164 0.8795 0.8438\n GPT-4 0.9065 0.7417 0.8159 0.8870 0.7639 0.8311\n GPT-3.5 0.8856 0.6251 0.7329 0.8753 0.6670 0.7391\n Exact MedDRA 0.8595 0.5178 0.6463 0.8026 0.5898 0.6885\n Llama 3 0.8264 0.5214 0.6394 0.8228 0.5842 0.6695\n Mixtral 0.5193 0.7556 0.6156 0.4859 0.7590 0.5852\n Llama 2 0.7636 0.3714 0.4997 0.7327 0.4254 0.5532\n U. Gisladottir et al.\nnames to RxNorm ingredients and then re-evaluated perfor-\nmance. Approximately half (49.6–51.8%) of the extracted \nand manually annotated drug names were successfully \nmapped to RxNorm ingredients. This resulted in 257 and \n313 distinct RxNorm ingredients from the GPT extraction \nand manual annotation, respectively, and when mapped to \nRxNorm, the performance decreased slightly (F 1 = 0.723, \nprecision = 0.928, recall = 0.592)—mostly due to a marked \ndrop in recall. We then applied this to the most recent active \nlabels (1486 SPLs) and GPT extracted 13,204 drug names \nfrom the DI section (2506 distinct drugs). Of these, 8111 \nwere successfully mapped to RxNorm drugs (900 distinct \ndrugs).\n4  Discussion\nFig. 3  Overall AR extraction \nperformance. Boxplot show-\ning the performance for AR \nextraction from the “Adverse \nReactions” section of the SPL. \nPerformance is measured by \nmacro-averaged F1-score (top), \nPrecision (center), and Recall \n(bottom). Each column shows \na different evaluation method: \nstrict exact string matching, \nlenient lexical matching, and \nsemantic matching. A point \nrepresents each drug label. The \nboxplot and points are color-\ncoded according to the method \nor model used to extract the \nAR terms. The figure shows the \nresults from the best-performing \nparameters for each method/\nmodel\nTable 6  Character length of sections\nMedian and the interquartile range (IQR) of the character length of \nthe different sections of the structured product labels (SPLs)\nSection Character length\nMedian IQR range\nAdverse reactions (ARs) 7701 [4593–13,984]\nWarnings and precautions (WPs) 5129 [3089–7945]\nBox warnings (BWs) 1853 [1383–2258]\nLeveraging Large Language Models in Extracting Drug Safety Information\nIn this study, we demonstrate the application of generative \nlarge language models (LLMs) in the extraction of drug \nsafety information from Structured Product Labels (SPLs). \nSpecifically, we found generative LLMs, particularly GPT-\n4, was effective in extracting adverse reactions (ARs) and \ndrug interactions (DIs) with no additional training or fine \ntuning. The results show that for these extraction tasks, per-\nformance was not sensitive to different prompting strategies. \nFurthermore, we found that incorporating shot prompting \ndid not improve performance. Recent work has shown that \nshot prompting may bias the generative models and decrease \nperformance [56]. The examples used for few-shot prompt-\ning likely failed to capture the complexity and specific char-\nacteristics of the SPL text, resulting in poor generalization \nand a decrease in performance.\nThe generative models, especially GPT-4, achieved per -\nformance on par with—and in some cases exceeding—the \nstate-of-the-art Bidirectional Encoder Representations from \nTransformers (BERT)-based model, DeepCADRME [17], \nwhich required task-specific training. Unlike traditional \nmodels, generative models do not simply extract exact \nspans of text; instead, they generate responses that may \ninclude paraphrase or rephrase terms. As a result, adopting \na semantic evaluation approach was essential to ensure a fair \ncomparison between generative models and more traditional \nextraction methods. We found that the semantic evaluation \nmethod generally produced the highest performance metrics, \nalthough this was occasionally accompanied by a decrease \nin precision. This trade off suggests that while generative \nmodels are effective at capturing information that is concep-\ntually relevant, they may also introduce a higher proportion \nof closely related but potentially imprecise outputs, high-\nlighting the importance of aligning evaluation frameworks \nwith the inherent flexibility of these models.\nDespite the high precision achieved by the best-perform-\ning model (micro-precision = 0.949, macro-precision = \n0.955), the nature of the FPs warrants closer examination. \nOne potential contributor is hallucination, where the model \ngenerates ARs unsupported by the input text, potentially \nreflecting pretraining biases or overgeneration when input \ncontext is sparse. However, hallucination is likely not the \nsole driver of FPs. Other contributing factors include mis-\nmatch in term granularity, where model-generated terms \nare closely related to but are not synonymous with the gold \nstandard (e.g., \"gastrointestinal discomfort\" vs \"nausea\"), \nand evaluation threshold sensitivity, where the semantic sim-\nilarity cutoff may be too permissive. Additionally, some FPs \nmay represent valid but unannotated reactions or ambiguous \ncases with reasonable expert disagreement. Distinguishing \nbetween hallucinations and other contributing factors will \nbe essential for optimizing the use of generative models in \npharmacovigilance.\nFig. 4  Performance by character length. Scatter plot showing the \nF1-score, precision, and recall performance metrics relative to the \ncharacter length of the “Adverse Reactions” section of the Structured \nProduct Labels (SPL). For illustrative purposes, we split the character \nlengths into 15 quantiles and calculated the mean performance with \nerror bars to show the standard deviation. Shown are the results of \nthe best-performing parameters for each generative language model \n(defined by macro-F1). Color represents the respective generative \nlarge language model (LLM). The Pearson correlation coefficient (R) \nis annotated on the graphs for further clarity\n U. Gisladottir et al.\nFig. 5  Performance by \nstructured product label (SPL) \nsection. Boxplot illustrates the \ndistribution of F1-scores for \nadverse reaction extraction \nacross SPL sections, using the \noptimal parameters for each \nmodel. Horizontal lines show \nthe benchmark performance \nof the OnSIDES database [9]. \nColors correspond to the SPL-\nsection\nFig. 6  Impact of context and \ncomplexity. Boxplot showing \nrecall using semantic match-\ning. The color represents the \ncategory of the adverse reaction \nterm\n\nLeveraging Large Language Models in Extracting Drug Safety Information\nThis study also identified several factors that influence \nperformance, such as the SPL section origin, textual context, \nand complexity of AR terms. The section of the SPL signifi-\ncantly influenced the extraction of ARs. Models performed \nbest on the “Adverse Reactions” (AR) section and worst on \nthe “Warning and Precautions” (WP) section of the SPLs. \nThe decrease in performance on the WP section compared \nto the AR section is likely due to the more complex and less \nstructured nature of the text. The WP section often contains \nbroader information, including general safety guidelines, \ninteractions, and risk management strategies, which can \ncomplicate AR extraction. The less explicit and more varied \nphrasing in this section could lead to a decrease in extrac-\ntion accuracy, as the models may struggle with identifying \nand standardizing the relevant information. Similarly, our \nresults indicated that an increase in SPL text character length \ndecreased the precision of the extraction of these models. \nThis performance decrease was most notable for Llama 2, \nwhich, along with GPT-3.5, had the smallest context window \n(4096 tokens). Furthermore, negated or discontinuous terms \nposed challenges, as these require nuanced interpretation of \ncontext, leading to a decrease in performance. Considering \nthese factors, it appears that additional text, whether con-\ntextual (e.g., negation) or supplementary information (e.g., \nin the WP section), negatively impacted the ability of the \nmodel to accurately identify and extract relevant adverse \nreactions. Overall, GPT-4’s superior performance compared \nto the other generative LLM models was likely due to its \nlarger context window and improved instruction-following \ncapabilities.\nWhile DeepCADRME [17] slightly outperformed GPT-4 \nin some metrics, its applicability is limited to predefined \ntasks and necessitates computationally intensive training. \nIn contrast, GPT-4 provides a broader and more flexible \napproach, performing well on multiple tasks, including \nextracting information from the Drug Interactions (DI) sec-\ntion, which DeepCADRME [17] would not be equipped to \ndo. Specifically, GPT-4 was able to extract drug names from \nthe DI section with great performance with excellent preci-\nsion (F1 = 0.841, precision = 0.989, recall = 0.732). Recent \nwork by ValizadehAslani et al. [57] that explored fine-tuning \nand pretraining of BERT-based models for the extraction of \ndrug interactions was able to reach F 1 of 0.805. While we \nwere not able to incorporate this model in our pipeline for \na more direct comparison, the high performance of GPT-4 \nshows the promise of generative LLMs in this context. In this \nstudy, we evaluated the ability of GPT-4 to extract drug and \nproduct names assessing both the terms directly extracted \nby the model and the corresponding RxNorm ingredient \nmappings. Only half of the manually annotated drug names \nwere successfully mapped to RxNorm ingredients. When \nmapped to RxNorm, we observed a decrease in performance, \nparticularly in recall. This decline may be attributed to the \nmapping process itself, as drugs were mapped to the ingredi-\nent level, which can introduce inconsistencies or incomplete \nmappings.\nIn the context of this work, which aims to create a \nmachine-readable database of ARs extracted from SPLs, it \nis crucial to balance recall and precision. High recall ensures \nthe comprehensive capture of ARs across products, while \nadequate precision minimizes inaccuracies and reduces the \nneed for extensive manual review. The performance dem-\nonstrated in this study suggests that generative models, par-\nticularly GPT-4, may already achieve performance levels \nsuitable for research and internal database development. \nHowever, for both regulatory applications and patient safety, \nadditional validation and human oversight may be required \nto ensure that appropriate recall and precision thresholds are \nmet. Additionally, the optimal approach for ensuring both \ncomprehensiveness and accuracy may involve a combination \nof generative models with specialized BERT-based models, \nsuch as DeepCADRME [17], RxBERT [28], and Label-\nComp [58], alongside manual review. The development of \na high-recall system with a user-friendly interface may be \nan efficient method for the curation of a public repository of \nthis information. Future work may consider the development \nof such a human-in-the-loop (HITL) system. Additionally, \nwhile this study focused on the use of generative models \nfor extracting drug safety information from SPLs, future \nresearch should explore integrating these generative models \nwith other task-specific tools to enhance overall performance \nand facilitate the development of comprehensive, machine-\nreadable AR databases.\nAlthough the findings highlight the capabilities of gen-\nerative models, there are some limitations and potential for \nfuture work. First, while the TAC dataset provides a valuable \nbenchmark, it is important to note that it was developed with \na focus on specific linguistic interests of the NLP commu-\nnity. As such, its definition of adverse events may not align \nwith all drug safety applications. Future work should explore \naligning model outputs with task-specific requirements used in \nregulatory or clinical settings. Current evaluation frameworks, \ndesigned for strict extraction paradigms such as those used \nin BERT models, may underestimate the generative models’ \nperformances due to their inherent flexibility in producing \nparaphrased and more concise outputs. Additionally, this study \ndid not explore advanced prompting techniques, such as soft \nprompting or fine-tuning, which might further enhance perfor-\nmance. The reliance on manual annotations tailored for exact \nstring matches also highlights the need for evaluation methods \nbetter aligned with generative models' outputs. To address the \nidentified challenges, future work should focus on exploring \nadvanced prompting and training techniques, including fine-\ntuning, knowledge distillation, and section-specific prompt-\ning strategies. Additionally, improving the performance with \nnegated or discontinuous terms may require novel evaluation \n U. Gisladottir et al.\nmetrics or a more tailored approach. While we found semantic \nsimilarity to be the most suitable approach to evaluating this \ntask, further development of new evaluation frameworks that \nbetter capture the generative models’ capabilities is essential to \nfully leverage the potential of these models. These frameworks \ncould consider the semantic correctness, context-awareness of \nthe extracted information, and model reasoning. Finally, in this \nstudy, we limited the scope to the extraction of AR terms and \ndid not map these terms to Medical Dictionary for Regulatory \nActivities (MedDRA) Preferred Term (PT) concepts. Mapping \nextracted terms to MedDRA concepts is an essential next step \nto enable downstream tasks. As a preliminary investigation, \nwe evaluated how well extracted terms aligned with MedDRA \nPT concepts. We found that over 73% of terms extracted by \nthe best-performing generative model semantically matched \na MedDRA PT at a high cosine similarity (threshold ≥ 0.9), \nsuggesting that normalization can be reliably incorporated \ninto future pipeline development (see Supplementary Materi-\nals). Additionally, recent work by Berkowitz et al. [59] has \nshown promising results using retrieval augmented genera-\ntion (RAG) to enable LLMs to map terms to MedDRA PT \nconcepts. Future work will explore the development of a high \nthroughput pipeline for the extraction and standardization of \nthe drug safety information extracted from SPLs.\nThis study demonstrates the potential of generative LLMs \nas effective tools for extracting and standardizing drug safety \ninformation from SPLs. Having demonstrated the ability to \nsuccessfully extract both ARs and drug names, this study \nhighlights the potential to extend these methods to capture \nadditional key safety information, such as adverse reaction \nfrequency, severity grades, and other essential drug safety \ninformation. For example, extending the methodology to \nextract contextual information for drug interactions, such as \nassociated toxicities, could significantly enhance its utility \nin post-market surveillance and pharmacovigilance studies. \nThe generative LLMs have the potential to advance drug \nsafety research in several key areas, including assessing the \nimpact of accelerated drug approvals on safety profiles, facil-\nitating long-term ADR tracking, and the determination of \nwhen an ADR was identified. Understanding when an ADR \nis identified and added to an SPL is crucial for observational \ndrug safety research, as it may influence prescription pat-\nterns. Failing to account for this timing could introduce bias \nand lead to skewed or false-negative results. Additionally, \nthese findings indicate that LLMs may be useful in Risk \nEvaluation and Mitigation Strategies (REMS) research, \nfacilitating the automated extraction and analysis of safety \ndata derived from SPLs and thereby enhancing regulatory \noversight. This work highlights the potential of generative \nLLMs to enable drug safety research. By offering compara-\nble or superior performance to traditional NLP methods with \nminimal training, generative models present a scalable, cost-\neffective solution for extracting critical safety information.\n5  Conclusion\nEffective high-throughput drug safety research requires \nmachine-readable data sources of current drug safety knowl-\nedge. Structured product labels are a valuable resource, but \ntheir transformation into computationally usable formats \nrequires advanced natural language processing (NLP) tech-\nniques. While Bidirectional Encoder Representations from \nTransformers (BERT)-based models have been used suc-\ncessfully for this task, they require extensive training on \nlarge manually annotated datasets and are then limited to \nthat specific application. In this study we systematically \napplied generative large language models (LLMs) to Struc-\ntured Product Labels (SPLs) and rigorously evaluated their \ncapabilities in extracting critical drug safety information. We \nfound that generative LLMs, such as GPT-4, offer greater \nflexibility and have demonstrated robust performance across \ndiverse tasks. Through this work, we identified key factors \ninfluencing their performance and extended the scope of \nsafety information extraction by exploring previously unan-\nnotated SPL sections, such as drug interactions. These con-\ntributions significantly advance our understanding of gen -\nerative models in drug safety research, moving us closer to \ncreating a comprehensive, machine-readable data source to \nsupport the detection of emerging safety signals.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s40264- 025- 01594-x.\nDeclarations \nFunding Open access funding provided by SCELC, Statewide Cali-\nfornia Electronic Library Consortium. This work was supported by \na training grant from the U.S. National Library of Medicine (UOG \nand KLB: T15-LM007079). NPT, UOG, MZ and KLB are funded by \nNational Institute of General Medical Sciences (R35GM131905). The \nstudy funders had no role in the design and conduct of the study; col-\nlection, management, analysis, and interpretation of the data; prepara-\ntion, review, or approval of the manuscript; and decision to submit the \nmanuscript for publication. The content is solely the responsibility of \nthe authors and does not necessarily represent the official views of a \nfunder.\nConflicts of interest Nicholas Tatonetti is an Editorial Board member \nof Drug Safety. Nicholas Tatonetti was not involved in the selection of \npeer reviewers for the manuscript nor in any of the subsequent editorial \ndecisions. The authors have no other competing interests to declare.\nEthics approval Not applicable as the data in this study do not rely on \npatient data but instead the use of publicly available structured product \nlabels.\nConsent to participate Not applicable as the data in this study did not \nrequire the participation of patients.\nConsent for publication Not applicable as this study does not involve \nresearch on human subjects.\nLeveraging Large Language Models in Extracting Drug Safety Information\nData availability Data used in this study can be found at: https:// bionlp. \nnlm. nih. gov/ tac20 17adv erser eacti ons/\nCode availability Code used for our analysis is available on GitHub: \nhttps:// github. com/ taton etti- lab/ onsid es- task1\nContributors’ statement All authors had full access to the data in the \nstudy. They take responsibility for the integrity of the data and the \naccuracy of the analysis and have approved the final manuscript. Con-\ncept and design: all authors. Acquisition, analysis, or interpretation of \ndata: all authors. Drafting and critical revision of the manuscript for \nimportant intellectual content: all authors. Obtained funding: NPT, \nSupervision: NPT.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution-NonCommercial 4.0 International License, which permits any \nnon-commercial use, sharing, adaptation, distribution and reproduction \nin any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other \nthird party material in this article are included in the article's Creative \nCommons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article's Creative Commons \nlicence and your intended use is not permitted by statutory regula-\ntion or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit \nhttp:// creat iveco mmons. org/ licen ses/ by- nc/4. 0/.\nReferences\n 1. Medication Safety Basics|Medication Safety Program|CDC. 2022. \nhttps:// www. cdc. gov/ medic ation safety/ basics. html. Accessed 15 \nJuly 2022.\n 2. Coleman JJ, Pontefract SK. Adverse drug reactions. Clin Med. \n2016;16:481–5. https:// doi. org/ 10. 7861/ clinm edici ne. 16-5- 481.\n 3. Iftikhar S, Sarwar MR, Saqib A, et al. Causality and prevent-\nability assessment of adverse drug reactions and adverse drug \nevents of antibiotics among hospitalized patients: a multicenter, \ncross-sectional study in Lahore, Pakistan. PLoS ONE. 2018;13: \ne0199456. https:// doi. org/ 10. 1371/ journ al. pone. 01994 56.\n 4. De Pretis F, van Gils M, Forsberg MM. A smart hospital-driven \napproach to precision pharmacovigilance. Trends Pharmacol Sci. \n2022;43:473–81. https:// doi. org/ 10. 1016/j. tips. 2022. 03. 009.\n 5. Garcia L. An Exploratory Study of the Food and Drug Admin-\nistration Adverse Events Reporting System (FAERS) (Doctoral \ndissertation, University of Pittsburgh).\n 6. Agbabiaka TB, Savovic J, Ernst E. Methods for causality assess-\nment of adverse drug reactions: a systematic review. Drug Saf. \n2008;31:21–38.\n 7. McDowell SE, Coleman JJ, Ferner RE. Systematic review and \nmeta-analysis of ethnic differences in risks of adverse reactions to \ndrugs used in cardiovascular medicine. BMJ. 2006;332:1177–81. \nhttps:// doi. org/ 10. 1136/ bmj. 38803. 528113. 55.\n 8. Aronson JK. When I use a word … Medical definitions: pharma-\ncovigilance. BMJ. 2023;381: p855. https:// doi. org/ 10. 1136/ bmj. \np855.\n 9. Tanaka Y, Chen HY, Belloni P, et al. OnSIDES database: Extract-\ning adverse drug events from drug labels using natural language \nprocessing models. Med. 2025.\n 10. Guidance for industry on providing regulatory submissions in \nelectronic format-content of labeling; availability. Fed. Regist. \n2005. https:// www. feder alreg ister. gov/ docum ents/ 2005/ 04/ 21/ \n05- 7946/ guida nce- for- indus try- on- provi ding- regul atory- submi \nssions- in- elect ronic- format- conte nt- of- label ing. Accessed 9 Feb \n2024.\n 11. Commissioner O of the. Structured product labeling resources. \nFDA. 2023. https:// www. fda. gov/ indus try/ fda- data- stand ards- \nadvis ory- board/ struc tured- produ ct- label ing- resou rces. Accessed \n9 Feb 2024.\n 12. Pandey A, Kreimeyer K, Foster M, et al. Adverse event extraction \nfrom structured product labels using the Event-based Text-mining \nof Health Electronic Records (ETHER) system. Health Inform J. \n2019;25:1232–43. https:// doi. org/ 10. 1177/ 14604 58217 749883.\n 13. Research C for DE and. Warnings and precautions, contraindi-\ncations, and boxed warning sections of labeling for human pre-\nscription drug and biological products—content and format. 2020. \nhttps:// www. fda. gov/ regul atory- infor mation/ search- fda- guida \nnce- docum ents/ warni ngs- and- preca utions- contr aindi catio ns- and- \nboxed- warni ng- secti ons- label ing- human- presc ripti on. Accessed \n24 July 2024.\n 14. Wang W, Haerian K, Salmasian H, et al. A drug-adverse event \nextraction algorithm to support pharmacovigilance knowledge \nmining from PubMed citations. AMIA Annu Symp Proc AMIA \nSymp. 2011;2011:1464–70.\n 15. Gu Y, Zhang S, Usuyama N, et al. Distilling large language mod-\nels for biomedical knowledge extraction: A case study on adverse \ndrug events. arXiv preprint arXiv: 2307. 06439. 2023.\n 16. Tatonetti NP, Ye PP, Daneshjou R, et al. Data-driven prediction \nof drug effects and interactions. Sci Transl Med. 2012;4:125ra31. \nhttps:// doi. org/ 10. 1126/ scitr anslm ed. 30033 77.\n 17. El-allaly E, Sarrouti M, En-Nahnahi N, et al. DeepCADRME: a \ndeep neural model for complex adverse drug reaction mentions \nextraction. Pattern Recognit Lett. 2021;143:27–35. https:// doi. org/ \n10. 1016/j. patrec. 2020. 12. 013.\n 18. Lee J, Yoon W, Kim S, et al. BioBERT: a pre-trained biomedi-\ncal language representation model for biomedical text mining. \nBioinformatics. 2020;36:1234–40. https:// doi. org/ 10. 1093/ bioin \nforma tics/ btz682.\n 19. Alsentzer E, Murphy JR, etc al. Publicly available clinical BERT \nembeddings. arXiv preprint arXiv: 1904. 03323. 2019.\n 20. Guevara M, Chen S, Thomas S, et al. Large language models \nto identify social determinants of health in electronic health \nrecords. Npj Digit Med. 2024;7:1–14. https:// doi. org/ 10. 1038/  \ns41746- 023- 00970-0.\n 21. Peng C, Yang X, Yu Z, et al. Clinical concept and relation extrac-\ntion using prompt-based machine reading comprehension. J Am \nMed Inform Assoc. 2023;30:1486–93. https:// doi. org/ 10. 1093/ \njamia/ ocad1 07.\n 22. Ali SR, Strafford H, Dobbs TD, et al. Development and validation \nof an automated basal cell carcinoma histopathology information \nextraction system using natural language processing. Front Surg. \n2022;9: 870494. https:// doi. org/ 10. 3389/ fsurg. 2022. 870494.\n 23. Yang X, Bian J, Fang R, et al. Identifying relations of medica-\ntions with adverse drug events using recurrent convolutional \nneural networks and gradient boosting. J Am Med Inform Assoc. \n2020;27:65–72. https:// doi. org/ 10. 1093/ jamia/ ocz144.\n 24. Duke JD, Friedlin J. ADESSA: a real-time decision support ser -\nvice for delivery of semantically coded adverse drug event data. \nAMIA Annu Symp Proc. 2010;2010:177–81.\n 25. Kuhn M, Campillos M, Letunic I, et al. A side effect resource to \ncapture phenotypic effects of drugs. Mol Syst Biol. 2010;6:343. \nhttps:// doi. org/ 10. 1038/ msb. 2009. 98.\n 26. Kuhn M, Letunic I, Jensen LJ, et al. The SIDER database of drugs \nand side effects. Nucleic Acids Res. 2016;44:D1075–9. https:// doi. \norg/ 10. 1093/ nar/ gkv10 75.\n 27. Roberts K, Demner-Fushman D, Tonning JM. Overview of the \nTAC 2017 Adverse Reaction Extraction from Drug Labels Track. \nInTAC 2017.\n U. Gisladottir et al.\n 28. Wu L, Gray M, Dang O, et al. RxBERT: Enhancing drug labeling \ntext mining and analysis with AI language modeling. Exp Biol \nMed. 2024;248:1937–43. https:// doi. org/ 10. 1177/ 15353 70223 \n12206 69.\n 29. OpenAI, Achiam J, Adler S, et al. GPT-4 technical report. 2024.\n 30. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabili-\nties of gpt-4 on medical challenge problems. arXiv preprint arXiv: \n2303. 13375. 2023.\n 31. Hu Y, Chen Q, Du J, et al. Improving large language models for \nclinical named entity recognition via prompt engineering. J Am \nMed Inform Assoc. 2024. https:// doi. org/ 10. 1093/ jamia/ ocad2 59.\n 32. Van Veen D, Van Uden C, Blankemeier L, et al. Adapted large \nlanguage models can outperform medical experts in clinical \ntext summarization. Nat Med. 2024. https:// doi. org/ 10. 1038/  \ns41591- 024- 02855-5.\n 33. Demner-Fushman D, Shooshan SE, Rodriguez L, et al. A dataset \nof 200 structured product labels annotated for adverse drug reac-\ntions. Sci Data. 2018;5: 180001. https:// doi. org/ 10. 1038/ sdata. \n2018.1.\n 34. DailyMed-SPL resources. https:// daily med. nlm. nih. gov/ daily med/ \nspl- resou rces. cfm. Accessed 20 Nov 2024.\n 35. Kasneci E, Sessler K, Küchemann S, et al. ChatGPT for good? On \nopportunities and challenges of large language models for educa-\ntion. Learn Individ Differ. 2023;103: 102274. https:// doi. org/ 10. \n1016/j. lindif. 2023. 102274.\n 36. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lac-\nroix T, Rozière B, Goyal N, Hambro E, Azhar F, Rodriguez A. \nLlama: Open and efficient foundation language models. arXiv \npreprint arXiv: 2302. 13971. 2023.\n 37. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei \nY, Bashlykov N, Batra S, Bhargava P, Bhosale S, Bikel D. Llama \n2: Open foundation and fine-tuned chat models. arXiv preprint \narXiv: 2307. 09288. 2023.\n 38. Jiang AQ, Sablayrolles A, Roux A, Mensch A, Savary B, Bamford \nC, Chaplot DS, Casas DD, Hanna EB, Bressand F, Lengyel G. \nMixtral of experts. arXiv preprint arXiv: 2401. 04088. 2024.\n 39. Pan S, Luo L, Wang Y, Chen C, Wang J, Wu X. Unifying large \nlanguage models and knowledge graphs: A roadmap. IEEE Trans \nKnowl Data Eng. 2024;36(7):3580–99.\n 40. Zheng C, Liu Z, Xie E, Li Z, Li Y. Progressive-hint prompting \nimproves reasoning in large language models. arXiv preprint \narXiv: 2304. 09797. 2023.\n 41. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin \nP, Zhang C, Agarwal S, Slama K, Ray A, Schulman J. Training \nlanguage models to follow instructions with human feedback. Adv \nNeural Inf Process Syst. 2022;35:27730–44.\n 42. Zhu K, Wang J, Zhou J, Wang Z, Chen H, Wang Y, Yang L, Ye \nW, Zhang Y, Zhenqiang Gong N, Xie X. Promptbench: Towards \nevaluating the robustness of large language models on adversarial \nprompts. arXiv e-prints. 2023 Jun:arXiv-2306.\n 43. Cen W, Herbert EA, Haas PJ. NIM: modeling and generation of \nsimulation inputs via generative neural networks. In: 2020 Winter \nsimulation conference (WSC); 2020. p. 584–95.\n 44. Ramasinghe S, Ranasinghe K, Khan S, Barnes N, Gould S. Con-\nditional generative modeling via learning the latent space. arXiv \npreprint arXiv: 2010. 03132. 2020.\n 45. Luo J, Xiao C, Ma F. Zero-resource hallucination prevention for \nlarge language models. arXiv preprint arXiv: 2309. 02654. 2023.\n 46. Zi Y, Roy K, Narayanan V, et al. IERL: interpretable ensemble \nrepresentation learning—combining crowdsourced knowledge and \ndistributed semantic representations. 2023.\n 47. Ueda N, Nakano R. Deterministic annealing variant of the EM \nalgorithm. Advances in neural information processing systems. \nCambridge: MIT Press; 1994.\n 48. Ma J, Fong SH, Luo Y, et al. Few-shot learning creates predic-\ntive models of drug response that translate from high-throughput \nscreens to individual patients. Nat Cancer. 2021;2:233–44. https:// \ndoi. org/ 10. 1038/ s43018- 020- 00169-2.\n 49. Henry S, Buchan K, Filannino M, et al. 2018 n2c2 shared task on \nadverse drug events and medication extraction in electronic health \nrecords. J Am Med Inform Assoc. 2020;27:3–12. https:// doi. org/ \n10. 1093/ jamia/ ocz166.\n 50. Gurulingappa H, Rajput AM, Roberts A, et al. Development \nof a benchmark corpus to support the automatic extraction of \ndrug-related adverse effects from medical case reports. J Biomed \nInform. 2012;45:885–92. https:// doi. org/ 10. 1016/j. jbi. 2012. 04. \n008.\n 51. Kovačević A, Dehghan A, Filannino M, et al. Combining rules \nand machine learning for extraction of temporal expressions \nand events from clinical narratives. J Am Med Inform Assoc. \n2013;20:859–66. https:// doi. org/ 10. 1136/ amiaj nl- 2013- 001625.\n 52. Liu X, Zheng Y, Du Z, et al. GPT understands, too. AI Open. \n2023. https:// doi. org/ 10. 1016/j. aiopen. 2023. 08. 012.\n 53. Ray PP. ChatGPT: a comprehensive review on background, appli-\ncations, key challenges, bias, ethics, limitations and future scope. \nInternet Things Cyber Phys Syst. 2023;3:121–54. https:// doi. org/ \n10. 1016/j. iotcps. 2023. 04. 003.\n 54. Muennighoff N, Tazi N, Magne L, Reimers N. Mteb: Massive text \nembedding benchmark. arXiv preprint arXiv: 2210. 07316. 2022.\n 55. McHugh ML. Interrater reliability: the kappa statistic. Biochem \nMed. 2012;22:276–82.\n 56. Zhao Z, Wallace E, Feng S, et al. Calibrate before use: improving \nfew-shot performance of language models. In: Proceedings of the \n38th international conference on machine learning. PMLR; 2021. \np. 12697–706.\n 57. ValizadehAslani T, Shi Y, Ren P, et al. PharmBERT: a domain-\nspecific BERT model for drug labels. Brief Bioinform. \n2023;24:bbad226. https:// doi. org/ 10. 1093/ bib/ bbad2 26.\n 58. Neyarapally GA, Wu L, Xu J, et al. Description and validation of \na novel AI tool, LabelComp, for the identification of adverse event \nchanges in FDA labeling. Drug Saf. 2024;47:1265–74. https:// doi. \norg/ 10. 1007/ s40264- 024- 01468-8.\n 59. Berkowitz JS, Srinivasan A, et al. Biomedical text normalization \nthrough generative modeling. J Biomed Inform. 2025;104850.\nAuthors and Affiliations\nUndina Gisladottir1 · Michael Zietz1,2 · Sophia Kivelson2 · Yutaro Tanaka1,4,5,6 · Gaurav Sirdeshmukh3 · \nKathleen LaRow Brown1 · Nicholas P . Tatonetti1,2,3 \n * Nicholas P. Tatonetti \n Nicholas.Tatonetti@cshs.org\n1 Department of Biomedical Informatics, Columbia \nUniversity, New York, NY, USA\n2 Department of Computational Biomedicine, Cedars-Sinai \nMedical Center, Los Angeles, CA, USA\nLeveraging Large Language Models in Extracting Drug Safety Information\n3 Cedars-Sinai Cancer, Cedars-Sinai Medical Center, \nLos Angeles, CA, USA\n4 Department of Applied Physics and Applied Mathematics, \nFu Foundation School of Engineering and Applied Sciences, \nColumbia University, New York, NY 10027, USA\n5 Department of Pediatric Oncology, Dana-Farber Cancer \nInstitute, Boston, MA 02215, USA\n6 Cancer Program, Broad Institute of Harvard and MIT, \nCambridge, MA 02142, USA",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.9126241207122803
    },
    {
      "name": "Drug",
      "score": 0.7601525187492371
    },
    {
      "name": "Medical prescription",
      "score": 0.47274789214134216
    },
    {
      "name": "Prescription drug",
      "score": 0.4332996606826782
    },
    {
      "name": "Pharmacology",
      "score": 0.37770652770996094
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1282927834",
      "name": "Cedars-Sinai Medical Center",
      "country": "US"
    }
  ],
  "cited_by": 1
}