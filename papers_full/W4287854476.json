{
  "title": "YNU-HPCC at SemEval-2022 Task 4: Finetuning Pretrained Language Models for Patronizing and Condescending Language Detection",
  "url": "https://openalex.org/W4287854476",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2323304865",
      "name": "Wenqiang Bai",
      "affiliations": [
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A2105468542",
      "name": "Jin Wang",
      "affiliations": [
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A2115527613",
      "name": "XueJie Zhang",
      "affiliations": [
        "Yunnan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2008056655",
    "https://openalex.org/W2041122820",
    "https://openalex.org/W2806828122",
    "https://openalex.org/W1485009520",
    "https://openalex.org/W2584424117",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2753437440",
    "https://openalex.org/W2293023260",
    "https://openalex.org/W3117768806",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W1967542092",
    "https://openalex.org/W1533946607",
    "https://openalex.org/W4287889344"
  ],
  "abstract": "This paper describes a system built in the SemEval-2022 competition. As participants in Task 4: Patronizing and Condescending Language Detection, we implemented the text sentiment classification system for two subtasks in English. Both subtasks involve determining emotions; subtask 1 requires us to determine whether the text belongs to the PCL category (single-label classification), and subtask 2 requires us to determine to which PCL category the text belongs (multi-label classification). Our system is based on the bidirectional encoder representations from transformers (BERT) model. For the single-label classification, our system applies a BertForSequenceClassification model to classify the input text. For the multi-label classification, we use the fine-tuned BERT model to extract the sentiment score of the text and a fully connected layer to classify the text into the PCL categories. Our system achieved relatively good results on the competition’s official leaderboard.",
  "full_text": "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 454 - 458\nJuly 14-15, 2022 ©2022 Association for Computational Linguistics\nYNU-HPCC at SemEval-2022 Task 4: Finetuning Pretrained Language\nModels for Patronizing and Condescending Language Detection\nWenqiang Bai, Jin Wang and Xuejie Zhang\nSchool of Information Science and Engineering\nYunnan University\nKunming, China\nContact: ynubwq@mail.ynu.edu.cn, {wangjin, xjzhang}@ynu.edu.cn\nAbstract\nThis paper describes a system built for the\nSemEval-2022 competition. As participants\nin Task 4: Patronizing and Condescending Lan-\nguage Detection, we implemented the text sen-\ntiment classiﬁcation system for two subtasks\nin English. Both subtasks involve determin-\ning emotions; subtask 1 requires us to deter-\nmine whether the text belongs to the PCL cate-\ngory (single-label classiﬁcation), and subtask\n2 requires us to determine to which PCL cate-\ngory the text belongs (multi-label classiﬁca-\ntion). Our system is based on the bidirec-\ntional encoder representations from transform-\ners (BERT) model. For the single-label classiﬁ-\ncation, our system applies a BertForSequence-\nClassiﬁcation model to classify the input text.\nFor the multi-label classiﬁcation, we use the\nﬁne-tuned BERT model to extract the sentiment\nscore of the text and a fully connected layer to\nclassify the text into the PCL categories. Our\nsystem achieved relatively good results on the\ncompetition’s ofﬁcial leaderboard.\n1 Introduction\nText classiﬁcation is an area of natural language\nprocessing (NLP) that aims to classify text using\ncertain features. Previous studies on text classiﬁca-\ntion tasks used traditional machine learning meth-\nods, which require researchers to manually design\nfeatures. Feature extraction methods such as term\nfrequency–inverse document frequency (TF-IDF)\n(Hakim et al., 2014) and N-Gram (Cavnar et al.,\n1994) are used to extract features from original\ndocuments, and then the features are input into\nclassiﬁers such as naive Bayes(Berrar, 2019), sup-\nport vector machines (SVMs) (Hearst et al., 1998),\nand decision trees (Vens et al., 2008). Since the\nadvent of deep learning, text classiﬁcation tasks\nare achievable without manual extraction of text\nfeatures. Researchers must simply pretreat the text\nand incorporate it into a deep learning model for\ntraining. For text classiﬁcation using deep learn-\ning methods, the classiﬁcation accuracy is often\nhigher than that of traditional machine learning\nmethods. With their continuous improvement, deep\nlearning models, such as recurrent neural networks\n(RNNs)(Zaremba et al., 2014), multi-channel CNN-\nLSTM (Zhang et al., 2017),gate recurrent units\n(GRUs) (Rana, 2016), long short-term memory\n(LSTM) (Shi et al., 2015), bidirectional long short-\nterm memory (Bi-LSTM) (Zhang et al., 2015), and\nattention-based Bi-LSTM (Zhang et al., 2018) net-\nworks, can be used to solve text classiﬁcation prob-\nlems. In recent years, bidirectional encoder repre-\nsentations from transformers (BERT) (Devlin et al.,\n2018), a new deep learning model, has achieved,\nor even surpassed, human performance in multi-\nple tasks within the NLP domain, including text\nclassiﬁcation.\nTask 4 of the SemEval-2022 consists of the fol-\nlowing two subtasks.\n• Subtask 1: identifying whether the sentence\ncontains any kind of PCL.\n• Subtask 2: identifying which types of PCL\nthe sentence contains.\nIn this paper, we introduce a deep learning\nsystem for SemEval-2022 Task 4: Patronizing\nand Condescending Language Detection (Pérez-\nAlmendros et al., 2022). We applied the pretrained\nBERT model as the base model. This task con-\ntains two subtasks: single-label classiﬁcation and\nmulti-label classiﬁcation. To accomplish both sub-\ntasks, we used ﬁne-tuning methods on the base\nmodel with an additional classiﬁcation layer. Our\ncontributions are as follows:\n• For the sentiment analysis task, we used the\npretrained BERT model as the base model.\n• To obtain the classiﬁcation results, we added\na fully connected layer at the end of the base\nmodel.\n454\nThe remainder of this paper is organized as fol-\nlows. Section 2 provides an overview of our system\nfor the two subtasks. Section 3 presents the spe-\nciﬁc details of our system. Section 4 discusses the\nresults of the experiments, and ﬁnally, we draw our\nconclusions in Section 5.\n2 Overview\nThis section presents an overview of our system\nand experiments, consisting of the following steps:\n1. The data processing step, in which we use text\nprocessing tools to clean the text content, such\nas removing HTML tags in the text.\n2. The model training step, in which we build,\ntrain, and evaluate the model.\n3. The result generating step, in which we eval-\nuate the model and predict the results on the\ntest dataset.\nTask description. The two subtasks involved\ntext sentiment analysis and classiﬁcation. The\ndifference between them is that subtask 1 only\nrequires us to determine whether the text con-\ntains any kind of PCL. Subtask 2 is the multi-\nlabel classiﬁcation task, and the data of subtask\n2 are marked by a list of 0s and 1s, which in-\ndicate the type of linguistic techniques (Unbal-\nanced_power_relations, Shallow_solution, Presup-\nposition, Authority_voice, Metaphors, Compas-\nsion, The_poorer_the_merrier) used to express con-\ndescension.\n2.1 Data processing\nTo use the original text as much as possible and\nreduce the impact of meaningless text on the model,\nwe built text cleaning tools that can be used to re-\nmove redundant text from the original. In addition,\nto complete the text classiﬁcation task, a special\ntoken is added to the front of the original sentence.\nPreprocessing. The texts may have been retrieved\nfrom the Internet by an automated program and\ninevitably there will be some unnatural language\nin the text. Text processing tools, such as regu-\nlar expressions and Beautiful Soup, are used to\nremove impurities, such as HTML tags and redun-\ndant punctuation, from the text. Because the origi-\nnal sentence cannot be used in the pretrained BERT\nmodel, a special token [CLS] is added to the front\nof the sentence, and the model receives the new\nsequence (with the added token) as input.\nFigure 1: Embedding blocks\n2.2 Deep learning models\nIn recent years, the use of deep learning for NLP\ntext classiﬁcation has become the most commonly\nadopted method in the industry. We used the pre-\ntrained BERT model to accomplish the tasks men-\ntioned in the task description.\nBidirectional Encoder Representations from\nTransformers (BERT).As the name suggests, the\nBERT model is the encoder of the bidirectional\ntransformer. BERT uses masked LM and next-\nsentence prediction to capture the representation\nat the word and sentence levels, respectively, and\npretrains the model in a self-supervised manner.\nSince the BERT model was proposed by Google\nin 2018, the entire ﬁeld of NLP has entered a new\nstage. With BERT, we can easily ﬁne-tune a pre-\ntrained model to achieve outstanding results that\nmay even surpass human performance.\nBERT consists of two main blocks: the embed-\nding block and transformer encoder block, whose\ndetails are as follows.\n1. Embedding Block. After preprocessing the\noriginal text, the output is fed to the embed-\nding block, whose structure is shown in Figure\n1.\nThe embedding block has three embedding\nlayers: the Token Embeddings, which con-\nvert each word into a ﬁxed-dimensional vec-\ntor similar to most deep learning models; Seg-\nment Embeddings, which distinguish between\nthe two sentences; and Position Embeddings,\nwhich represent the position of each word in\nthe sentence. These embedding layers trans-\nform the input text into a three-dimensional\nmatrix X ∈RN×n×d, where N is the number\nof sentences in the text, n is the number of\nwords in the sentence, and d is the dimension\nof the embedding vector.\n455\n……\nsoftmax\nargmax\n……\n……\n……\n……\nＢＥＲＴ\nE[CLS]\n[CLS] X1 X2 XN\nE1 E2 EN\nT[CLS] T1 T2 TN\n……\nOriginal sentence\nBERT Layer\nFully Connected\nLayer\nOutput Layer\nLabel\nFigure 2: Single-label classiﬁcation system\n2. Encoder Block. The encoder block com-\nprises a series of transformer encoder blocks.\nEach transformer encoder block comprises\ntwo layers: the multi-head self-attention and\nfeed-forward layers. The self-attention layer\nincluded in the encoder block of the trans-\nformer allows each word in the sentence to\nuse the information of all other words in the\nsentence. The output of the current word does\nnot need to depend on the output of the pre-\nvious word, making the training well paral-\nlelized and greatly reducing the time to train\nthe model. Because each word has a different\nimpact on the sentence category, the atten-\ntion mechanism can dynamically change the\nweight of each word.\n3 Model Description\nA pretrained BERT model is used to accomplish\nboth subtasks with the two independent datasets.\nThe details of the model built for these two subtasks\nare as follows.\n3.1 Subtask 1: single-label classiﬁcation\nThe architecture of the system built for subtask 1\nhas three different layers, as shown in Figure 2.\n3.2 Subtask 2: multi-label classiﬁcation\nThe system built for subtask 2 is similar to that\nfor subtask 1, and the architecture of this system\nis only slightly different in the output layer. The\nstructure is shown in Figure 3.\n……\nsigmoid\nvalue > 0.5\n……\n……\n……\n……\nＢＥＲＴ\nE[CLS]\n[CLS] X1 X2 XN\nE1 E2 EN\nT[CLS] T1 T2 TN\n……\nOriginal sentence\nBERT Layer\nFully Connected\nLayer\nOutput Layer\nFigure 3: Multi-label classiﬁcation system\n3.3 Details of the model architecture\nBERT Layer. After preprocessing, the texts are\ninput into the z BERT model, which contains the\nembedding and encoder blocks. Each word in the\ninput sequence will output a ﬁxed-dimensional (d)\nvector. In our BERT model (bert-based-uncased),\nd is 768.\nFully Connected Layer. The fully connected layer\nis used to convert a d-dimensional vector into a\nvector with the number of categories or labels as\nthe dimension. In the text classiﬁcation task, only\nthe output of the ﬁrst word, which is [CLS] at the\nBERT layer, is fed to the fully connected layer\nbecause it integrates the semantic information.\nOutput Layer. A matrix X ∈RN×c is output by\nthe fully connected layer, in whichN is the number\nof sentences and c is the number we manually set.\nIn the single-label two-category classiﬁcation task,\nit is set to 2, and the fully connected layer converts\nthe 768-dimensional vector into a 2-dimensional\nvector. In the multi-label two-category classiﬁca-\ntion task, it is set to the number of labels, 7, and the\nfully connected layer converts the 768-dimensional\nvector into a 7-dimensional vector.\nTo obtain the ﬁnal result for the single-label clas-\nsiﬁcation task, the output of the fully connected\nlayer is input into the softmax function to calcu-\nlate the probability of the sentence belonging to\nthe category, and the outcomes of the softmax func-\ntion are fed to the argmax function to obtain the\nclassiﬁcation result.\n456\nclass =\n{\n0, value0 ≥value1,\n1, value0 < value1\n(1)\nIf the output value is 1, the sentence belongs to\nthe label, that is, this sentence contains some kind\nof PCL; otherwise, the sentence does not contain\nany kind of PCL.\nFor the multi-label classiﬁcation task, we input\nthe result of the fully connected layer into asigmoid\nfunction that maps each value in the output vector\nto a value between 0 and 1. Each value in the vector\nis then mapped to 0 or 1 according to the rounding\nrules.\nlabeli =\n{\n0, labeli ≤0.5,\n1, labeli > 0.5. (2)\nThe output is a 7-dimensional vector that con-\nsists of 0 or 1. If the value is 1, the sentence used\nthe technique corresponding to the vector element\nnumber to express the condescension.\n3.4 Training and Hyperparameters\nFor these two classiﬁcation tasks, we used the BCE-\nwithLogits loss function and Adam (Kingma and\nBa, 2017) optimizer to train both models. Both\nmodels use a stochastic gradient with mini-batches\nof size 16. The hyperparameters are as follows:\nHyperparameters The maximum input sequence\nlength of the BERT model is 512, the dimension\nof word embeddings (d) is 768, the dropout ratio is\n0.1 at each layer in the models, the learning rate is\n1e-5, and the number of epochs is 15.\n4 Experiment\nDataset. For the two subtasks, the corpus we used\nto train the model are from the competition(Pérez-\nAlmendros et al., 2020), without other external\ndata.\ndontpatronizeme_pcl.tsv This dataset contains\n10,469 paragraphs, and each paragraph is annotated\nwith a label ranging from 0 to 4. In the single-label\nclassiﬁcation subtask, the original label annotated\nas either 0 or 1 is replaced with 0, and the other\nlabels with 1.\ndontpatronizeme_categories.tsv This dataset con-\ntains 993 unique paragraphs with a total of 2,760\ninstances of PCL. In the multi-label classiﬁcation\ntask, each paragraph is annotated with 7 labels\nranging from 0 to 1.\nTable 1: Subtask 1 result\nPrecision Recall F1_Score\n0.5097 0.4132 0.4564\nTable 2: Subtask 2 result\nLabel Score\nUnbalanced_Power_Relations 0.1600\nShallow_Solution 0.1245\nPresupposition 0.0721\nAuthority_V oice 0.0968\nMetaphor 0.0696\nCompassion 0.1139\nThe_poorer_the_merrier 0.0385\nAverage 0.0965\nEvaluation Methods. For subtask 1 (single-label\nclassiﬁcation), the competition metrics given by\nthe competition organizer are precision, recall, and\nF1 score. For subtask 2 (multi-label classiﬁcation),\nthere are two competition metrics: prediction accu-\nracy of each label and average prediction accuracy\nof all labels.\nResults. The results of the two subtasks are shown\nin Tables 1 and 2.\nFor subtask 1, we ranked 42/81 in precision,\n47/81 in recall, and 52/81 in F1 score.\nFor subtask 2, we ranked 35, 33, 35, 34, 33,\n34, and 24 out of 81 for the seven labels: Un-\nbalanced_power_relations, Shallow_solution, Pre-\nsupposition, Authority_voice, Metaphors, Compas-\nsion, and The_poorer_the_merrier, respectively.\nExperiments and Analysis. We used 80% of the\ntraining data as the training set and 20% of the\ntraining data as the validation set. We trained our\nmodel on the training set and used the validation\nset to evaluate the accuracy of the model. Our\nsystem achieved relatively good results on the com-\npetition’s ofﬁcial leaderboard, which is insepara-\nble from the excellence of the pretrained BERT\nmodel. The outstanding advantage of the pretrained\nmodel is that it can learn the language from a large\namount of unlabeled data and then ﬁne-tune on a\nsmall amount of labeled data. Thus, downstream\ntasks often lead to better learning of language and\ntask-speciﬁc features.. Compared to traditional\nRNN and LSTM models, BERT can perform con-\ncurrently and simultaneously extract relational fea-\ntures of words in a sentence at several different lev-\nels, thus comprehensively reﬂecting the sentence\nsemantics. Compared to word2vec, the meanings\n457\nof words can also be obtained according to the con-\ntext of the sentence, which would avoid ambiguity.\n5 Conclusion\nIn this paper, we described our system, which is\nbased on the pretrained BERT model, for the text\nclassiﬁcation task SemEval 2022 Task 4: Patroniz-\ning and Condescending Language Detection. We\nadded a classiﬁcation layer to the pretrained BERT\nmodel to address both subtasks. The results gener-\nated by the proposed system achieved a relatively\ngood ranking. In the future, we hope to explore\nother models and methods in the sentiment analysis\nﬁeld.\nAcknowledgement\nThis work was supported by the National Natural\nScience Foundation of China (NSFC) under Grants\nNo. 61966038. The authors would like to thank the\nanonymous reviewers for their constructive com-\nments.\nReferences\nD. Berrar. 2019. Bayes’ theorem and naive bayes classi-\nﬁer - sciencedirect. Encyclopedia of Bioinformatics\nand Computational Biology, 1:403–412.\nWilliam Cavnar, , William B. Cavnar, and John M. Tren-\nkle. 1994. N-gram-based text categorization. In\nProceedings of SDAIR-94, 3rd Annual Symposium on\nDocument Analysis and Information Retrieval, pages\n161–175.\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nKristina Toutanova Google, and A I Language.\n2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv\npreprint arXiv:1810.04805.\nAri Aulia Hakim, Alva Erwin, Kho I Eng, Maulahikmah\nGalinium, and Wahyu Muliady. 2014. Automated\ndocument classiﬁcation for news article in bahasa\nindonesia based on term frequency inverse document\nfrequency (tf-idf) approach. In Proceedings of 6th\nInternational Conference on Information Technology\nand Electrical Engineering (ICITEE), pages 1–4.\nM.A. Hearst, S.T. Dumais, E. Osuna, J. Platt, and\nB. Scholkopf. 1998. Support vector machines. IEEE\nIntelligent Systems and their Applications, 13(4):18–\n28.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nCarla Pérez-Almendros, Luis Espinosa-Anke, and\nSteven Schockaert. 2020. Don’t Patronize Me! An\nAnnotated Dataset with Patronizing and Condescend-\ning Language towards Vulnerable Communities. In\nProceedings of the 28th International Conference on\nComputational Linguistics, pages 5891–5902.\nCarla Pérez-Almendros, Luis Espinosa-Anke, and\nSteven Schockaert. 2022. SemEval-2022 Task 4:\nPatronizing and Condescending Language Detection.\nIn Proceedings of the 16th International Workshop on\nSemantic Evaluation (SemEval-2022). Association\nfor Computational Linguistics.\nRajib Rana. 2016. Gated recurrent unit (gru) for emo-\ntion classiﬁcation from noisy speech. arXiv preprint\narXiv:1612.07778.\nXingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan\nYeung, Wai-Kin Wong, Wang-Chun Woo, and\nHong Kong Observatory. 2015. Convolutional lstm\nnetwork: A machine learning approach for precipita-\ntion nowcasting. In Advances in Neural Information\nProcessing Systems, volume 28, pages 802–810. Cur-\nran Associates, Inc.\nCeline Vens, Jan Struyf, Leander Schietgat, Sašo\nDžeroski, Hendrik Blockeel, C Vens, J Struyf, L Schi-\netgat, H Blockeel, and S Džeroski. 2008. Decision\ntrees for hierarchical multi-label classiﬁcation. Ma-\nchine Learning 2008 73:2, 73:185–214.\nWojciech Zaremba, Ilya Sutskever, Oriol Vinyals, and\nGoogle Brain. 2014. Recurrent neural network regu-\nlarization. arXiv preprint arXiv:1409.2329.\nHaowei Zhang, Jin Wang, Jixian Zhang, and Xue-\njie Zhang. 2017. YNU-HPCC at SemEval 2017\ntask 4: Using a multi-channel CNN-LSTM model\nfor sentiment classiﬁcation. In Proceedings of the\n11th International Workshop on Semantic Evaluation\n(SemEval-2017), pages 796–801, Vancouver, Canada.\nAssociation for Computational Linguistics.\nShu Zhang, Dequan Zheng, Xinchen Hu, and Ming\nYang. 2015. Bidirectional long short-term memory\nnetworks for relation classiﬁcation. In Proceedings\nof the 29th Paciﬁc Asia Conference on Language, In-\nformation and Computation, pages 73–78, Shanghai,\nChina.\nYou Zhang, Jin Wang, and Xuejie Zhang. 2018. YNU-\nHPCC at SemEval-2018 task 1: BiLSTM with atten-\ntion based sentiment analysis for affect in tweets. In\nProceedings of The 12th International Workshop on\nSemantic Evaluation, pages 273–278, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\n458",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8601149320602417
    },
    {
      "name": "SemEval",
      "score": 0.8459197282791138
    },
    {
      "name": "Encoder",
      "score": 0.7034879922866821
    },
    {
      "name": "Task (project management)",
      "score": 0.6636772155761719
    },
    {
      "name": "Language model",
      "score": 0.6364890336990356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.63338702917099
    },
    {
      "name": "Transformer",
      "score": 0.6271206736564636
    },
    {
      "name": "Natural language processing",
      "score": 0.608002781867981
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I189210763",
      "name": "Yunnan University",
      "country": "CN"
    }
  ]
}