{
  "title": "Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference",
  "url": "https://openalex.org/W4390309921",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2182987635",
      "name": "Chen Hongzheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2395935069",
      "name": "Zhang Jiahao",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Du, Yixiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2373928926",
      "name": "Xiang Shao-jie",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Yue, Zichao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2372570383",
      "name": "Zhang Niansong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1890193761",
      "name": "Cai Yaohui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347854325",
      "name": "Zhang Zhi-ru",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3137550677",
    "https://openalex.org/W2565125333",
    "https://openalex.org/W3129734321",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4312037452",
    "https://openalex.org/W4383749446",
    "https://openalex.org/W4319166707",
    "https://openalex.org/W3037585619",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4312060029",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4362598949",
    "https://openalex.org/W4321636575",
    "https://openalex.org/W4321637273",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2094756095",
    "https://openalex.org/W4280611847",
    "https://openalex.org/W4387321091",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3161542527",
    "https://openalex.org/W4281758439",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4320558514",
    "https://openalex.org/W3210312974",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W4321853806",
    "https://openalex.org/W4388093177",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W2798956872",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3082569543",
    "https://openalex.org/W3043034704",
    "https://openalex.org/W4308760184",
    "https://openalex.org/W4297097408",
    "https://openalex.org/W3036879053",
    "https://openalex.org/W4300865759",
    "https://openalex.org/W4287284729",
    "https://openalex.org/W2950896714",
    "https://openalex.org/W4287279045",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4379260375",
    "https://openalex.org/W4214792499",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W4321276705",
    "https://openalex.org/W3206314279",
    "https://openalex.org/W4322718253",
    "https://openalex.org/W4309500223",
    "https://openalex.org/W3115388607",
    "https://openalex.org/W4281651027",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4239385313",
    "https://openalex.org/W3162542754",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3155487259",
    "https://openalex.org/W3187255235",
    "https://openalex.org/W4293023328",
    "https://openalex.org/W3199934250",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4299286379",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385326807",
    "https://openalex.org/W4226479682",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4389162879",
    "https://openalex.org/W2953384591"
  ],
  "abstract": "Recent advancements in large language models (LLMs) boasting billions of parameters have generated a significant demand for efficient deployment in inference workloads. The majority of existing approaches rely on temporal architectures that reuse hardware units for different network layers and operators. However, these methods often encounter challenges in achieving low latency due to considerable memory access overhead. This paper investigates the feasibility and potential of model-specific spatial acceleration for LLM inference on FPGAs. Our approach involves the specialization of distinct hardware units for specific operators or layers, facilitating direct communication between them through a dataflow architecture while minimizing off-chip memory accesses. We introduce a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA. Through our analysis, we can determine the scenarios in which FPGA-based spatial acceleration can outperform its GPU-based counterpart. To enable more productive implementations of an LLM model on FPGAs, we further provide a library of high-level synthesis (HLS) kernels that are composable and reusable. This library will be made available as open-source. To validate the effectiveness of both our analytical model and HLS library, we have implemented BERT and GPT2 on an AMD Alveo U280 FPGA device. Experimental results demonstrate our approach can achieve up to 13.4x speedup when compared to previous FPGA-based accelerators for the BERT model. For GPT generative inference, we attain a 2.2x speedup compared to DFX, an FPGA overlay, in the prefill stage, while achieving a 1.9x speedup and a 5.7x improvement in energy efficiency compared to the NVIDIA A100 GPU in the decode stage.",
  "full_text": "Understanding the Potential of FPGA-Based Spatial\nAcceleration for Large Language Model Inference\nHONGZHENG CHEN, Cornell University, USA\nJIAHAO ZHANGâˆ—, Tsinghua University, China\nYIXIAO DU, SHAOJIE XIANG, and ZICHAO YUE, Cornell University, USA\nNIANSONG ZHANG, YAOHUI CAI, and ZHIRU ZHANG, Cornell University, USA\nRecent advancements in large language models (LLMs) boasting billions of parameters have generated\na significant demand for efficient deployment in inference workloads. While hardware accelerators for\nTransformer-based models have been extensively studied, the majority of existing approaches rely on temporal\narchitectures that reuse hardware units for different network layers and operators. However, these methods\noften encounter challenges in achieving low latency due to considerable memory access overhead.\nThis paper investigates the feasibility and potential of model-specific spatial acceleration for LLM inference\non FPGAs. Our approach involves the specialization of distinct hardware units for specific operators or layers,\nfacilitating direct communication between them through a dataflow architecture while minimizing off-chip\nmemory accesses. We introduce a comprehensive analytical model for estimating the performance of a spatial\nLLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA.\nThis model can be extended to multi-FPGA settings for distributed inference. Through our analysis, we can\nidentify the most effective parallelization and buffering schemes for the accelerator and, crucially, determine\nthe scenarios in which FPGA-based spatial acceleration can outperform its GPU-based counterpart.\nTo enable more productive implementations of an LLM model on FPGAs, we further provide a library of\nhigh-level synthesis (HLS) kernels that are composable and reusable. This library will be made available as\nopen-source. To validate the effectiveness of both our analytical model and HLS library, we have implemented\nBERT and GPT2 on an AMD Xilinx Alveo U280 FPGA device. Experimental results demonstrate our approach\ncan achieve up to 13.4Ã—speedup when compared to previous FPGA-based accelerators for the BERT model.\nFor GPT generative inference, we attain a 2.2Ã—speedup compared to DFX, an FPGA overlay, in the prefill\nstage, while achieving a 1.9Ã—speedup and a 5.7Ã—improvement in energy efficiency compared to the NVIDIA\nA100 GPU in the decode stage.\nCCS Concepts: â€¢ Hardware â†’Hardware-software codesign; â€¢ Computing methodologies â†’Neural\nnetworks.\nAdditional Key Words and Phrases: FPGA, high-level synthesis, large language models, hardware acceleration\n1 Introduction\nThe rapid advancement of Transformer-based large language models (LLMs) [5, 74] has sparked a\nrevolution across a wide range of natural language processing tasks, such as conversational AI [13,\n54, 104] and code generation [10, 42, 52]. Recent research has brought to light the phenomenon of\nâ€œemergenceâ€ in LLMs, where advanced capabilities become evident as the models scale up to billions\nof parameters [77, 78]. However, supporting this unprecedented scale poses significant challenges,\nparticularly in terms of computational and memory resources. At the same time, the increasing use\nof LLMs in interactive applications like voice assistants and autonomous systems requires hardware\naccelerators capable of providing both low latency and high energy efficiency [17, 54, 62].\nRecent efforts have primarily focused on improving the performance of LLM inference on\nGPUs [2, 53], although GPUs are known for their high power consumption and are less suitable\nfor latency-sensitive workloads [ 32, 62]. There is also an active body of research dedicated to\ndeveloping specialized hardware accelerators tailored for Transformer models, with several of these\nefforts using FPGAs as the target platforms [23, 26, 39, 46, 59, 63].\nâˆ—Work was done when interning at Cornell.\n1\narXiv:2312.15159v2  [cs.LG]  7 Apr 2024\nChen et al.\nf1 f2 f3 f4 f1 f2 f3 f4PE1\ntime\nspace\n(a) Temporal architecture (i.e., overlay).\nf1 f3f2 f4 f1 f3f2 f4PE1 PE2\ntime\nspace\n(b) Partially unfolded spatial architecture with two PEs.\nf1\nf2\nf3\nf4\nf1\nf2\nf3\nf4\nPE3PE4\nPE1 PE2\ntime\nspace\n(c) Fully unfolded spatial architecture with four PEs.\nFig. 1. Temporal and spatial architectures â€”PE stands for processing engine;ğ‘“1-ğ‘“4 represent different operators\nin the model.\nFPGA-based LLM accelerators can be broadly categorized into two architectural paradigms:\ntemporal architecture and spatial architecture . In a temporal architecture, a processing engine\n(PE) capable of performing various tasks is constructed and reused across different layers and\nmodels, as shown in Figure 1(a). For flexibility, these accelerators typically employ an overlay\napproach [23, 28, 39], where a virtual hardware architecture that executes instructions is â€œlaidâ€ on\ntop of the physical FPGA fabric. Overlays provide a more restricted configuration space, allowing\nfor quicker compilation with bitstream reuse across multiple models. However, the use of such\ntemporal architecture requires more frequent off-chip memory access, as intermediate results must\nbe written back to memory. This incurs a cost in terms of both latency and energy consumption\nthat is significantly higher than direct on-chip memory access. Additionally, one could argue that\nan FPGA overlay will inherently be less efficient than its hardened ASIC counterpart.\nIn contrast, an FPGA-based spatial architecture typically involves the specialization of distinct\nPEs for specific operators or layers, facilitating direct communication between them using streaming\nbuffers (e.g., FIFOs or multi-buffers) [60, 72, 75, 80], as depicted in Figure 1(b-c). This dataflow-style\nexecution substantially reduces off-chip memory accesses and enables the concurrent processing of\nmultiple PEs in a pipelined manner. Moreover, the fine-grained programmability of FPGAs allows\nefficient support of model-specific spatial architectures, which can further leverage efficiency\noptimizations such as low-bitwidth quantization, custom numerical types, and sparsity [58, 69, 93,\n102]. These capabilities can potentially enable highly efficient LLM inference implementations that\nsurpass GPUs, especially in small-batch low-latency scenarios.\nHowever, implementing a spatial architecture for LLM inference presents significant challenges.\nChallenge 1: Navigating diverse parallelism in LLMs. The generative inference process of\nLLMs typically consists of two distinct stages: (1) simultaneously processing user prompts and\n(2) sequentially generating new tokens in an autoregressive manner. These two stages exhibit\nsignificantly different computational and memory characteristics (detailed in Â§3), making it nec-\nessary to tailor hardware accelerators for their specific needs. This challenge cannot be directly\naddressed by leveraging techniques from the traditional convolutional neural network (CNN)\ndesigns [32, 97]. The large number of parameters and intermediate tensors further complicates the\nchoice between on-chip and off-chip storage. Additionally, harnessing multiple accelerators for\ndistributed LLM inference adds complexity, particularly when dealing with intricate parallelization\nschemes [23, 49, 68].\n2\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\nChallenge 2: Lack of standard LLM building blocks in hardware accelerators. The rapid\nevolution of LLM architectures [5, 54, 70] contrasts with the comparatively slow pace of hardware\ndevelopment. While a plethora of building blocks for Transformers have been proposed in the\nsoftware domain [ 14, 19, 38], the absence of reusable blocks for hardware accelerator design\nhampers development progress. Many frameworks have been designed to automatically map deep\nlearning models to FPGAs [3, 20, 72, 98, 99], but they are constrained to small CNN designs and lack\nsupport for complicated Transformer models. It is also hard to scale their designs to accommodate\nlarge models and multi-die FPGAs.\nTo tackle these challenges, this paper is to provide a comprehensive set of hardware design\nconsiderations for LLMs and try to answer the following question: What role can FPGA-based\nspatial accelerators play in enabling efficient LLM inference? We start by conducting an in-depth\nanalysis of the computational and memory requirements associated with each operator within\nTransformer models across two distinct stages of LLM generative inference â€“ prefill and decode.\nSubsequently, we extend our analysis to reveal the potential benefits of distributed inference using\nmultiple FPGAs. We believe that providing such an analysis, rather than presenting only\npositive results in selectively chosen settings for an FPGA LLM accelerator, offers more\nvaluable insights to the community. To validate the feasibility of our analytical framework,\nwe implement a specific design point and demonstrate its viability. Leveraging this analytical\nframework, we employ specific optimizations in HLS to craft each kernel and compose them into a\nhardware accelerator that achieves the expected performance. While our primary focus is not to\npropose a new LLM accelerator architecture, we demonstrate that by using the analytical model,\nwe can create a high-performance design that surpasses previous efforts. Our major contributions\nare as follows:\nâ€¢We introduce an analytical framework that presents the first in-depth analysis of both the\nadvantages and limitations of FPGA-based LLM spatial acceleration. This framework not\nonly allows us to estimate the performance of a specific accelerator configuration on a given\nFPGA device but also provides guidance for designing accelerators for LLM inference.\nâ€¢We create a suite of modular and reusable HLS kernels designed for building FPGA-based\nspatial accelerators for different Transformer models. We plan to open-source this kernel\nlibrary1 and expect it to serve as a valuable resource for benchmarking HLS and FPGA\nacceleration more broadly.\nâ€¢Leveraging our kernel library, we design and implement a range of high-performance\nFPGA-based LLM accelerators that achieve speedups comparable to previous GPU and\nFPGA-based accelerators. Specifically, for the BERT model, we achieve a 13.4 Ã—speedup\nover prior FPGA-based accelerators. For GPT generative inference, we achieve speedups\nof 2.2Ã—and 1.1Ã—in prefill and decode stages respectively, when compared to DFX, an\nFPGA-based overlay architecture. Additionally, our accelerator is 1.9Ã—faster and 5.7Ã—more\nenergy-efficient than the A100 GPU in the decode stage.\n2 Background\nThis section provides backgrounds on Transformer models and introduces parallelization schemes\nfor LLM inference.\n2.1 Transformer Models\nThe Transformer model consists of both encoder and decoder blocks [ 74]. Recent employment\non LLMs mostly uses decoder-only models, which leverage an auto-regressive approach for text\n1https://github.com/cornell-zhang/allo/tree/main/examples\n3\nChen et al.\nInput\nEmbedding\nLayer 1\nLinear\nSoftmax\nLayer ...\nLayer N\nPropmts\n(seq_len > 1)\n1st output\ntoken\nPositional\nEncoding\nMask (opt.)\nSoftmax\nHidden states\nLayerNorm & Add\nGELU\nSDP\nAttn\nMHA\nFFN\nLayerNorm & Add\nInput\nEmbedding\nLayer 1\nLinear\nSoftmax\nLayer ...\nLayer N\nNew token\n(seq_len=1)\n2nd output\ntoken\nMask (opt.)\nSoftmax\nHidden states\nLayerNorm & Add\nGELU\nSDP\nAttn\nMHA\nFFN\nLayerNorm & Add\nPrefill Stage\n(a)\nDecode Stage\n(b)\nKV\nCache\nFig. 2. Transformer model. Red blocks represent linear operators, and blue blocks signify non-linear operators.\ngeneration [54, 65, 70]. We will mainly discuss decoder-only models in this paper, but since encoders\nand decoders share the core building blocks with subtle architectural variances, our approach can\nalso be extended for encoder-only models [16, 36, 45].\nAs illustrated in Figure 2, generative inference of LLMs has two stages: prefill stage and decode\nstage [62]. In the prefill stage, the model takes in user prompts, normally a long sequence withğ‘™input\ntokens, goes through the whole Transformer model, and generates the first token. In the decode\nstage, the model takes in the previously generated token and generates ğ‘™gen new tokens one at a\ntime in an auto-regressive way. Since each token depends on the previously generated tokens, the\ndecode stage is purely sequential.\nWe then go through the detailed model architecture. The input tokens are first passed into an\nembedding layer that maps the discrete tokens into high-dimensional continuous representations\nwhile incorporating positional encoding for each token. Subsequently, it generates a tensor (i.e.,\nhidden states) of shape (ğ‘™,ğ‘‘), where ğ‘™ represents sequence length, and ğ‘‘ is the size of hidden\ndimensions. We omit the batch dimension to simplify the analysis, focusing solely on single-batch\ninference in this paper, but our approach can be easily extended to different batch sizes for LLM\nserving by adding an additional batch dimension [34, 43].\nThe hidden states then pass through a series of ğ‘ Transformer blocks. Each Transformer block\nconsists of two sublayers: a multi-head attention (MHA) module and a feed-forward network (FFN).\nResidual connections and layer normalization (LayerNorm) functions are applied between these\nsublayers, although the specific order and application may vary across different models [91]. The\nMHA module plays a crucial role in capturing token relationships within the input sequence. The\ninput is initially partitioned intoâ„segments, where â„corresponds to the number of attention heads.\nTo compute the attention scores for each head, the input sequence of length ğ‘™ undergoes three\n4\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\nSM\nSM\nall_reduce\nGL\nGL\nall_reduce\nTP rank#1\nTP rank#2\nLN & add\nLN & add\nFig. 3. An example of tensor parallelism of a Transformer layer with two devices. TP rank is the unique\nidentifier given to a device within a TP group. SM is the softmax function, LN is LayerNorm, and GL is the\nGeLU function.\nlinear projections: query, key, and value. These projections, which are trainable, yield matrices\nğ‘„, ğ¾, and ğ‘‰ respectively. Attention scores are then computed using a scaled dot-product (SDP)\noperator between ğ‘„, ğ¾, and ğ‘‰, as specified by the formula:\nAttention(ğ‘„,ğ¾,ğ‘‰ )= softmax\n\u0010\nğ‘„ğ¾ğ‘‡\n.âˆšï¸\nğ‘‘ğ‘˜\n\u0011\nğ‘‰, (1)\nwhere ğ‘‘ğ‘˜ is the size of the hidden dimension. The output from this operator involves â„outputs,\nwhich are subsequently concatenated and processed through an additional linear projection. In the\nprefill stage, the generated ğ¾ and ğ‘‰ tensors will be stored as KV cache and later be concatenated\nbefore SDP during the decode stage [62].\nThe FFN module comprises a linear layer followed by a non-linear activation function and\nanother linear layer. This module transforms the outputs of MHA into embedding matrices, which\nare then further processed by subsequent Transformer layers.\nFinally, the output tensor will go through a softmax function to obtain a distribution. The model\nwill sample a token from this distribution and feed it into the decode stage. For encoder-only\nmodels, there is only a prefill stage involved, and the distribution will be directly used for different\ndownstream tasks like text classification [16, 36, 45].\nIn this paper, we only focus on analyzing the core Transformer blocks and accelerating them on\nFPGAs. Embedding layers and output sampling [8, 25] require extensive random memory accesses,\nwhich may not be suitable for FPGA acceleration. Also, they only take a small fraction of overall\ncompute that does not affect the overall latency [32], so we leave them to execute on CPUs or GPUs\nas usual.\n2.2 Parallelization Schemes\nAs model sizes continue to expand, it becomes increasingly common for a single device to be insuffi-\ncient for accommodating the entire model. Consequently, the exploration of diverse parallelization\nschemes within a device and across devices becomes necessary. Parallelism techniques in deep\nlearning can be roughly classified into data, tensor, and pipeline parallelism, together known as\n3D parallelism [9, 33, 51, 68]. During the era of CNNs, data parallelism was the norm, involving\nthe partitioning of input tensors along the batch dimension and their distribution across multiple\ndevices [1, 41]. DeepSpeed ZeRO [66] and FSDP [47] extended data parallelism by proposing a\nthree-stage parallelism strategy that partitions optimizer states, gradients, and parameters to mini-\nmize memory usage. However, this approach may incur high communication overheads during\ninference.\nA more recent parallelism scheme for LLM inference is tensor parallelism (TP) [ 2, 43, 51, 62,\n68], which distributes model parameters across multiple devices and conducts explicit collective\n5\nChen et al.\nTable 1. MACs of the prefill and decode stages of the linear layers in the Transformer model in Figure 2\nâ€” ğ‘™ denotes input sequence length, ğ‘‘ denotes input feature dimension size, and ğ‘‘FFN denotes FFN hidden\ndimension size.\nLinear Layer Abbreviations Input Matrices Prefill Decode\nQ/K/V linear ğ‘, ğ‘˜, ğ‘£ ğ‘‹ğ‘Š ğ‘„,ğ‘‹ğ‘Šğ¾,ğ‘‹ğ‘Šğ‘‰ 3ğ‘™ğ‘‘2 3ğ‘‘2\nMatmul1 ğ‘1 ğ‘„ğ¾T ğ‘™2ğ‘‘ (ğ‘™+1)ğ‘‘\nMatmul2 ğ‘2 ğ‘‹smğ‘‰ ğ‘™ 2ğ‘‘ (ğ‘™+1)ğ‘‘\nProjection ğ‘ ğ‘‹ sdpğ‘ŠProj ğ‘™ğ‘‘2 ğ‘‘2\nFFN1 ğ‘“1 ğ‘‹mhağ‘ŠFFN1 ğ‘™ğ‘‘ğ‘‘FFN ğ‘‘ğ‘‘FFN\nFFN2 ğ‘“2 ğ‘‹actğ‘ŠFFN2 ğ‘™ğ‘‘ğ‘‘FFN ğ‘‘ğ‘‘FFN\noperations to ensure model correctness. Megatron-LM [68] is the first to explore tensor parallelism\nfor Transformer-based models, proving to be efficient in both training and inference due to relatively\nlow communication costs. As shown in Figure 3, tensor parallelism requires two all_reduce\noperations inside a Transformer layer to ensure the results are correct. Our accelerator design also\nexplores tensor parallelism, as detailed in Â§3.4.2.\nLastly, pipeline parallelism [49, 50, 92] divides the model across network layers. Multiple layers\nare grouped into a pipeline stage, and different stages are assigned to different devices. Pipeline\nparallelism is typically employed across multiple nodes. Since both tensor parallelism and pipeline\nparallelism handle only portions of the network model, they are collectively referred to as model\nparallelism. We revisit these parallelization schemes in Â§3.4.2.\n3 Analytical Modeling Framework\nIn this section, we propose a comprehensive analytical modeling framework aimed at understanding\nthe computational requirements of a Transformer layer. Our investigation begins by analyzing the\ncompute demands and resource constraints on a single device. We base our estimations on these\nconstraints. Finally, we extend the framework to the analysis of multiple devices.\n3.1 Computational Demands\nOur first imperative is to calculate the computational demands of the model. Given that the\npredominant computation in the Transformer model is general matrix-matrix multiplication (GEMM\nor Matmul) or general matrix-vector multiplication (GEMV) [32, 62], we employ the number of\nmultiplyâ€“accumulates (MACs) as the proxy metric for quantifying compute requirements of the\nlinear layers, as depicted in Table 1. For non-linear layers such as softmax and GeLU functions, they\nare elementwise operators that can be easily fused with the GEMM kernels in a pipeline design\nwithout affecting the final performance. More experimental results are provided in Â§6.3.\nWe denote ğ‘‹(Â·) as the output tensors from preceding layers, and ğ‘Š(Â·) represents the weights of\ncorresponding linear layers. For example, ğ‘‹sm is the output of the softmax operator. Our analysis\nhere is restricted to a single batch; hence the tensors only have two dimensions. We can observe\nthat the computational demand during the prefill stage far surpasses that of the decode stage. In the\nprefill stage, the required MACs of the two Matmuls within the SDP are quadratic to the sequence\nlength (i.e., ğ‘™2ğ‘‘). Consequently, when input sequences exhibit substantial length, attention layers\nmay extend computation time significantly. On the contrary, in the decode stage, each operator\nprocesses a single token at a time, making the MACs independent of the sequence length except\nfor SDP.\n6\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\n3.2 Resource Constraints\nWe then model the compute and memory resource constraints on an FPGA. In this section, we\nassume that one FPGA device can effectively compute at least a single Transformer layer, but our\nframework can be easily extended to more resource-constrained cases using a similar analysis\nproposed in Â§3.4.\n3.2.1 Compute Resource Constraints. The core computational element for linear operators is the\nMAC unit. Let ğ‘€ğ‘– denote the compute power, in terms of the number of MACs per cycle allocated\nto each matrix multiplication kernel, where ğ‘– ranges over ğ‘, ğ‘˜, ğ‘£, ğ‘1, ğ‘2, ğ‘, ğ‘“1, and ğ‘“2, based on\nthe notation in Table 1. We quantize the matrix multiplication to integer inputs for maximum\nefficiency, which has been proven to be effective by many recent studies [15, 30, 67, 81]. Quantization\nenables single-cycle accumulation. As a result, one multiply-accumulator (MAC) unit can provide\na 1 MAC/cycle throughput with a properly pipelined multiplier. Therefore, the latency for theğ‘„\nprojection can be calculated asğ‘™ğ‘‘2/ğ‘€ğ‘ cycles, considering that the total number of MACs computed\nin this operator is ğ‘™ğ‘‘2.\nSuppose we want to deploy ğ¶ Transformer model layers on an FPGA. The total MAC units\nmust not exceed the capacity of the device. Since we employ a dataflow design that unfolds all the\nlayers on-board, the required MAC units are simply the sum of the MAC units for each layer. This\nrequirement can be expressed as:\nâˆ‘ï¸\nğ‘€ğ‘–ğ¶ < ğ‘€tot,ğ‘– âˆˆ{ğ‘,ğ‘˜,ğ‘£,ğ‘ 1,ğ‘2,ğ‘,ğ‘“ 1,ğ‘“2}, (2)\nwhere ğ‘€tot represents the total available compute power of an FPGA in terms of MACs per cycle,\nwhich can be obtained from the official data sheets. For FPGAs with specialized compute blocks\n(e.g., AI Engine [86] and AI Tensor Blocks [37]), we can convert their compute power to match the\nfrequency of the programming logic, thus obtaining an overall value ğ‘€tot for the entire FPGA. For\nexample, the VCK5000 FPGA [86] has 400 AI Engines, each of which can compute 128 MACs/cycle\nat 1GHz. Therefore, the equivalent compute power at 250MHz is 128Ã—400Ã—1GHz/250MHz, which\nis 204800 MACs/cycle.\n3.2.2 Memory Capacity Constraints. The demand for memory capacity stems from a variety of\non-chip buffers, including weight buffers for parameters, buffers for ğ¾ and ğ‘‰ matrices, and FIFOs\ninterconnecting different stages.\nParameter buffers. To optimize an FPGA-based dataflow design, we assume that all the quan-\ntized parameters can be accommodated in on-chip or off-chip memory. Suppose all the linear\nweights are quantized to ğ‘ğ‘Š bits, and the size of the linear operator ğ‘– is ğ‘ ğ‘–. The total size of the\nbuffers is ğ‘†param = Ã\nğ‘–âˆˆ{ğ‘,ğ‘˜,ğ‘£,ğ‘,ğ‘“1,ğ‘“2 }ğ‘ ğ‘–ğ‘ğ‘Š = (4ğ‘‘2 +2ğ‘‘ğ‘‘FFN)ğ‘ğ‘Š if storing on-chip. If the parameters are\ntoo large to fit in on-chip memory, we can store the parameters in DRAM and tile the parameters\nwith size ğ‘€ğ‘– on-chip, then the total tiled buffer size is ğ‘†tile = Ã\nğ‘–âˆˆ{ğ‘,ğ‘˜,ğ‘£,ğ‘,ğ‘“1,ğ‘“2 }ğ‘€ğ‘–ğ‘ğ‘Š. To hide the\nmemory access latency, we need to double buffer those parameters, so the final buffer size of the\nğ‘–-th linear operator is 2ğ‘†tile.\nKV Cache. When conducting matrix multiplication, at least one of the matricesâ€™ elements must\nbe accessed repeatedly so that a buffer is required. Given that parameters are already buffered, only\nthe SDP requires buffering for at least one of the input matrices. In our case, we choose to buffer ğ¾\nand ğ‘‰, which will be later passed to the decode stage as the KV cache. We also double buffer ğ¾ and\nğ‘‰ matrices to improve throughput. The final buffer size isğ‘†KV = 4ğ‘™maxğ‘‘ğ‘ğ´, where ğ‘ğ´ is the bitwidth\nof the activation and ğ‘™max is the maximum sequence length supported by the model. Notice KV\ncache can also be tiled on-chip, which can leverage a similar analysis above.\n7\nChen et al.\nFIFOs. The intermediate results between linear operators flow in FIFOs since the linear operators\nsequentially access them. For the initial residual connection, we assume that the input tensors are\nfetched from off-chip memory to obviate the need for additional buffering. However, for the second\nresidual connection related to the FFN, it is necessary to use an intermediate buffer to store the\nprojectionâ€™s activation ğ‘‹act before the FFN. This buffer simultaneously serves as a bypass path. To\navoid deadlock, the buffer must possess sufficient capacity to store ğ‘‹act. We simply create a FIFO\nof size ğ‘™ğ‘‘ğ‘ğ´ to store it. For other FIFO connections, we assume a FIFO depth of ğ‘  and one FIFO\nconnecting each layer in Figure 2, so the total FIFO size is equal to ğ‘†FIFO = 16ğ‘ ğ‘ğ´ +ğ‘™ğ‘‘ğ‘ğ´.\nIn summary, the memory capacity constraint is expressed as:\nğ‘†paramğ¶ < ğ·ğ‘…ğ´ğ‘€tot ,\nâˆ‘ï¸\nğ‘†ğ‘–ğ¶ < ğ‘†ğ‘…ğ´ğ‘€tot ,ğ‘– âˆˆ{tile,KV,FIFO}, (3)\nif the parameters are stored off-chip. ğ·ğ‘…ğ´ğ‘€tot and ğ‘†ğ‘…ğ´ğ‘€tot are the total available off-chip and\non-chip memory.\n3.2.3 Memory Port Constraints. Besides memory capacity, we also need to consider constraints\non memory ports in a highly paralleled design. For matrix multiplication, if different MAC units\nwork in parallel, they will visit the weight/result buffers simultaneously, hence contending for\nmemory ports. This issue can be addressed by either partitioning the buffer, effectively offering\nmore memory ports; or packing data to create wider elements, subsequently reducing the number\nof memory ports required.\nSRAM resources. The on-chip SRAM resources of FPGAs are typically organized as blocks.\nEach block has a fixed capacity and may support configurable bitwidth. For example, on AMD\nUltraScale+ FPGAs, there are two types of SRAM resources: Block RAM (BRAM) and Ultra RAM\n(URAM). BRAM blocks can be configured to 1Ã—36 Kb block or 2Ã—18 Kb blocks, with two read and\nwrite ports each. URAM blocks are 288 Kb with one read and one write port. The port width of the\nBRAM block is flexible; it can be configured to 1, 2, 4, 9, 18, 36, or 72 (in 36 Kb mode) bits, while the\nport width of the URAM block is fixed at 72 bits. Similar to BRAM and URAM, Intel FPGAs have\nM20K and eSRAM with different configurable port widths.\nMemory blocks needed without data packing. To begin with, we analyze the port constraints\nwithout data packing. In this case, to eliminate the port contention, different MAC units may\nneed different memory ports. Consider the linear operator ğ‘– with the size of ğ‘ ğ‘– with ğ‘€ğ‘– MAC units\nworking in parallel, each loaded weight may feed multiple MAC units due to intrinsic data reuse\nin GEMM. We use ğ‘Ÿğ‘– to represent the data reuse factor (number of MAC units sharing the loaded\nweight). Therefore, the weight buffer needs to be partitioned into ğ‘€ğ‘–/ğ‘Ÿğ‘– parts. If we store all the\nweight buffers on-chip, then the number ofğ‘ğ‘Š-bit elements in each partition isğ‘ ğ‘–/(ğ‘€ğ‘–/ğ‘Ÿğ‘–). However,\nğ‘ğ‘Š may not fully occupy one memory word as the memory bitwidth can only take limited options.\nWe introduce the effective bit width,ğ‘ğµğ‘…ğ´ğ‘€, to be the smallest memory bitwidth larger than ğ‘ğ‘Š.\nLet ğ‘†ğµğ‘…ğ´ğ‘€ be the total capacity (in bits) of one memory block, we can deduce the total number of\nmemory blocks for one linear operator:\nğ‘…ğ‘– =\n\u0018 ğ‘ ğ‘–ğ‘ğµğ‘…ğ´ğ‘€\nğ‘€ğ‘–/ğ‘Ÿğ‘– Ã—ğ‘†ğµğ‘…ğ´ğ‘€\n\u0019\nÃ—ğ‘€ğ‘–/ğ‘Ÿğ‘– . (4)\nIf the parameters are loaded from off-chip memory and we only store a tile of the weight on-chip,\nthen ğ‘ ğ‘– is simply ğ‘€ğ‘–, and ğ‘…ğ‘– also becomes ğ‘€ğ‘– as ğ‘ğµğ‘…ğ´ğ‘€ â‰ªğ‘†ğµğ‘…ğ´ğ‘€. Since we need to double buffer\nthose parameters, the final buffer size of the ğ‘–-th linear operator is 2ğ‘€ğ‘–. Notice the ğ‘˜ and ğ‘£ layers\nneed to be double-buffered, so the required BRAM also doubles in these two layers. We can obtain\n8\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\nthe total required BRAM as below:âˆ‘ï¸\nğ‘–âˆˆ{ğ‘,ğ‘˜,ğ‘£,ğ‘,ğ‘“1,ğ‘“2 }\nğ¶ğ‘…ğ‘– +2ğ¶(ğ‘…ğ‘1 +ğ‘…ğ‘2 )< ğ‘€ğ‘’ğ‘štot . (5)\nMemory blocks needed with data packing. Data packing can alleviate the strain on memory\nport contention by consolidating multiple narrow data into a single, wider data element. This\nprocess allows multiple MAC units to access data from the same memory port. We consider packing\ndata into ğ‘ğ‘ğ‘ğ‘ğ‘˜ bits for the linear weights, and we have ğ‘ğ‘ğ‘ğ‘ğ‘˜ = ğ‘˜ğ‘ğ‘Š. Again, we denote ğ‘ğµğ‘…ğ´ğ‘€ as\nthe smallest memory bitwidth larger than ğ‘ğ‘ğ‘ğ‘ğ‘˜. We need to partition ğ‘€ğ‘–/ğ‘Ÿğ‘– MAC units to ğ‘€ğ‘–/ğ‘Ÿğ‘–/ğ‘˜\nparts, and each partition has âŒˆğ‘ ğ‘–/ğ‘˜Ã—ğ‘ğµğ‘…ğ´ğ‘€/(ğ‘€ğ‘–/ğ‘Ÿğ‘–/ğ‘˜)âŒ‰bits. Therefore, the total number of memory\nblocks needed is:\nğ‘…ğ‘– =\n\u0018 ğ‘ ğ‘–ğ‘ğµğ‘…ğ´ğ‘€\nğ‘€ğ‘–/ğ‘Ÿğ‘– Ã—ğ‘†ğµğ‘…ğ´ğ‘€\n\u0019\nÃ—ğ‘€ğ‘–/ğ‘Ÿğ‘–\nğ‘˜ . (6)\n3.2.4 Memory Bandwidth Constraints. If the parameters are stored off-chip, we need to consider\nthe impact of off-chip memory bandwidth. Similar to Â§4.2.2, we use ğ‘Ÿğ‘– to denote the data reuse\nfactor of a linear operator with ğ‘€ğ‘– MAC units. Effectively, ğ‘€ğ‘–/ğ‘Ÿğ‘– weights must be loaded from\noff-chip memory per cycle to feed the MAC units, requiring a bandwidth of:\nğµğ‘– = ğ‘ğ‘Š Ã—ğ‘€ğ‘–/ğ‘Ÿğ‘– Ã—ğ‘“ğ‘Ÿğ‘’ğ‘, (7)\nwhere ğ‘“ğ‘Ÿğ‘’ğ‘ is the achieved frequency of FPGA. If the total required bandwidth, Ã\nğ‘–ğ¶ğµğ‘–(ğ‘– âˆˆ\n{ğ‘,ğ‘˜,ğ‘£,ğ‘,ğ‘“ 1,ğ‘“2}), exceeds the maximum device bandwidth, the inference becomes bandwidth bound.\nNotice this bandwidth requirement needs to be analyzed for each operator individually if the data\nloading requires accessing multiple DDR or HBM channels.\n3.3 Performance Estimation\nIn this section, we estimate the overall latency based on the constraints and conduct work balancing\nfor the dataflow.\n... ... \nLayer 1\nLayer 2\nT ime \nPE \n... \nFig. 4. Pipeline diagram. Different colors stand for different input samples. Different blocks stand for different\nlinear operators which also constitute the pipeline stages. â„is the number of attention heads.\n3.3.1 Latency Estimation. We construct the pipeline diagram as shown in Figure 4. As mentioned\nin Â§3.2.2, since we need to store the ğ¾ and ğ‘‰ values after the linear operators, there is an implicit\nsynchronization point between the ğ‘/ğ‘˜/ğ‘£ operator and the latter SDP and FFN parts. The com-\nputation of them cannot be overlapped. Notice the ğ‘/ğ‘˜/ğ‘£ operator can be performed in parallel\nsince they do not have any dependencies. After ğ‘˜ and ğ‘£ have been fully calculated, the subsequent\ncomputations of SDP and FFN can be greatly overlapped. This is because these operations do not\n9\nChen et al.\nneed to wait for all the results to perform the next operation. The results of the previous operation\ncan be directly streamed into the next operation as input. Moreover, since different Transformer\nlayers share the same architecture, their computation can also be overlapped without waiting for\nthe result of the previous layer.\nSuppose the Transformer model has ğ‘ layers in total. Since we have ğ¶ layers on one FPGA, it\nneeds to iterate ğ‘/ğ¶ times to process the whole model. We can calculate the latency of different\nstages, and the overall latency is the maximum latency of these stages (which defines the initiation\ninterval of the pipeline) times the number of iterations, i.e.,\nğ‘‡prefill = 1\nğ‘“ğ‘Ÿğ‘’ğ‘\nğ‘\nğ¶\n\u0012ğ‘™ğ‘‘2\nğ‘€ğ‘˜\n+ğ¶max\n\u0012ğ‘™ğ‘‘2\nğ‘€ğ‘˜\n, ğ‘™2ğ‘‘\nğ‘€ğ‘1\n,ğ‘™ğ‘‘ğ‘‘FFN\nğ‘€ğ‘“1\n,ğ‘‡mem\n\u0013\u0013\n, (8)\nğ‘‡decode = 1\nğ‘“ğ‘Ÿğ‘’ğ‘\nğ‘\nğ¶\n\u0012 ğ‘‘2\nğ‘€ğ‘˜\n+ğ¶max\n\u0012 ğ‘‘2\nğ‘€ğ‘˜\n, (ğ‘™max +1)ğ‘‘\nğ‘€ğ‘1\n,ğ‘‘ğ‘‘FFN\nğ‘€ğ‘“1\n,ğ‘‡mem\n\u0013\u0013\n, (9)\nwhere the first term inside the parentheses is the latency of the ğ‘/ğ‘˜/ğ‘£ linear operator (i.e., ğ‘¡ in\nFigure 4).ğ‘‡mem is the off-chip memory access latency, which can be calculated based on Equation(7).\n3.3.2 Work Balancing. As the overall latency is determined by the slowest stage in the dataflow,\nwe can balance the execution time of each stage; hence we have\nğ‘™ğ‘‘2\nğ‘€ğ‘,ğ‘˜,ğ‘£,ğ‘\n= ğ‘™2ğ‘‘/â„\nğ‘€ğ‘1,ğ‘2\nâ„= ğ‘™ğ‘‘ğ‘‘FFN\nğ‘€ğ‘“1,ğ‘“2\n(10)\n=â‡’ğ‘€ = ğ‘€ğ‘,ğ‘˜,ğ‘£,ğ‘ = ğ‘‘/ğ‘™ğ‘€ğ‘1,ğ‘2 = ğ‘‘/ğ‘‘FFNğ‘€ğ‘“1,ğ‘“2 , (11)\nwhere ğ‘€ is defined as the global compute power in MACs/cycle. Finally, Equation (8) can be\nsimplified to\nğ‘‡prefill = 1\nğ‘“ğ‘Ÿğ‘’ğ‘ ğ‘\n\u0012\n1 +1\nğ¶\n\u0013 ğ‘™ğ‘‘2\nğ‘€ , (12)\nwhich shows the overall latency with work balancing. We can obtain the latency for the decode\nstage using a similar analysis.\nTo derive the optimal ğ‘€ for a given model, we devise a linear search algorithm to identify the\nmaximum available ğ‘€ based on the constraints in Equations (2), (3), and (6). Notice the optimal ğ‘€\nrepresents an upper bound of the compute power. In practice, we also need to consider the routing\nissue to adjust the actual achievable ğ‘€ as discussed in Â§5.2.\n3.4 Distributed Inference\nAs a single FPGA may not be sufficient to process some extremely large models, we next extend our\nmodeling to multiple FPGAs. We first characterize the communication cost between two FPGAs\nand discuss the impact of different parallelism schemes.\n3.4.1 Communication. Various methods exist for facilitating inter-FPGA communication, including\ncommunication through the host, PCI-E Peer-to-Peer (P2P), and on-device Ethernet. We mainly\nconsider the third approach since it does not necessitate orchestration from the host and provides\nhigher bandwidth compared to other alternatives. For example, the AMD Alveo U280 FPGA provides\ntwo QSFP ports [85], each capable of carrying 100 Gb/s Ethernet data over optical fibers, which\nensures robust and high-speed inter-FPGA communication. Most of the time, we cannot fully utilize\nthe network bandwidth and need to pay for the package header overheads. Suppose the theoretical\nnetwork bandwidth between two FPGA devices is ğµbits per second (bps), and the efficiency of\nthe network is ğ›¼, so we can have the effective bandwidth as ğ›¼ğµ, where ğ›¼ can be obtained through\nnetwork benchmarking.\n10\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\n3.4.2 Parallelization Schemes. As mentioned in Â§2.2, we have various parallelization schemes when\nconsidering multiple devices. We first analyze tensor parallelism (TP). As shown in Figure 3, the\nparameters of the linear operations are partitioned across different devices. For example, suppose\nthe weight parameters of the two FFN layers ğ‘“1 and ğ‘“2 are ğ´and ğµ, then we can partition ğ´along\nits column and partition ğµalong its row, and obtain\nğœ(ğ‘ğ´)ğµ = ğœ \u0000ğ‘\n\u0002ğ´1 ğ´2\n\u0003\u0001 \u0014ğµ1\nğµ2\n\u0015\n= ğœ(ğ‘ğ´1)ğµ1 +ğœ(ğ‘ğ´2)ğµ2 ,\nwhere ğœ is the GeLU function. Therefore, apart from partitioning ğ´and ğµ, we need to insert an\nall-reduce operation to aggregate the partial results on each device to ensure correctness. The\npartitioned parameters will be stored on different devices. For example,ğ´1 will be on the first FPGA,\nand ğ´2 will be on the second FPGA. A similar partition scheme can be applied for MHA, and we\nrefer the readers to [68] for more details.\nBased on this partition scheme, TP requires two all-reduce operations within one Transformer\nlayer. However, these communicative operations are implemented in a blocking way. Figure 5(a)\nshows the subsequent FFN module needs to wait for the completion of the all-reduce process before\nit can conduct computation [76]. Notice that the all-reduce operation only involves fetching results\nfrom other devices and adding the result to its local tensor. Given that the output of MHA is a\nsequential stream, we can perform elementwise addition in a non-blocking manner. As soon as the\nkernel receives enough data, it can initiate data transfer to other devices without waiting for the\nremaining data to be computed. This leads to substantial synchronization time savings as shown in\nFigure 5(b).\n(a) Blocking all-\nreduce in TPFFNMHA\nFFN\nMHA\nall_reduceall_reduce\nall_reduce\nall_reduce\nTime saved\n(b) Proposed\nnon-blocking\nall-reduce\nTime\nFig. 5. Blocking and non-blocking all-reduce in TP. The latency of different stages is not drawn to scale.\nSince the size of the output tensor of MHA and FFN are bothğ‘™ğ‘‘, the communication time for one\nall-reduce is\nğ‘‡comm = ğ‘™ğ‘‘ğ‘ğ´/(ğ›¼ğµ). (13)\nAs we have already implemented dataflow inside a device, pipeline parallelism (PP) essentially\nextends the dataflow to ğ‘2 devices with a tensor of size ğ‘™ğ‘‘ communicated in between. Here, we\nonly split the pipeline between two Transformer layers so the results of the previous device can be\ndirectly streamed to the next device in the same PP group. Notice TP and PP can be combined to\nconduct model inference [51], and the latency of Equation (8) becomes\nğ‘‡prefill = 1\nğ‘“ğ‘Ÿğ‘’ğ‘\nğ‘\nğ‘2ğ¶\n \nğ‘™ğ‘‘2\nğ‘1ğ‘€ğ‘˜\n+ğ‘2ğ¶max\n \nğ‘™ğ‘‘2\nğ‘1ğ‘€ğ‘˜\n, ğ‘™2ğ‘‘\nğ‘1ğ‘€ğ‘1\n,ğ‘™ğ‘‘ğ‘‘FFN\nğ‘1ğ‘€ğ‘“1\n,ğ‘‡mem,ğ‘‡comm\n!!\n, (14)\nwhere ğ‘1 and ğ‘2 are the size of a TP group and a PP group [ 68]. Additionally, the memory re-\nquirements of Equations (3) and (5) need to be divided by ğ‘1 to satisfy the constraints of multiple\ndevices.\n11\nChen et al.\nTable 2. Models used in Â§4 and Â§6.\nModel Type # of params # Layers # Heads Hidden FFN size\nğ‘ â„ Size ğ‘‘ ğ‘‘ ğ¹ğ¹ğ‘\nBERT [16] Encoder 110M 12 12 768 3072\nGPT2 [65] Decoder 355M 24 16 1024 4096\nLLaMA2 [71] Decoder 7B 32 32 4096 11008\nVicuna [11] Decoder 13B 40 40 5120 13824\nTable 3. Summary of FPGA and GPU devices.\nAMD Xilinx FPGA Intel FPGA Nvidia GPU\nAlveo\nU280 [83]\nVersal\nVCK5000 [86]\nVersal\nVHK158 [90]\nStratix 10\nNX 2100 [37]\nAgilex 7\nAGM039 [27]\nGeForce\nRTX 2080 TiTesla A100\nProcess Node TSMC 16nm TSMC 7nm TSMC 7nm Intel 14nm Intel 7nm TSMC 12nm TSMC 7nm\nRelease Date 2018 2022 2023 2020 2022 2018 2021\nThermal Design Power225W 225W 180W 225W 225W 250W 300W\nPeak Throughput24.5 INT8 TOPS 145 INT8 TOPS 56 INT8 TOPS 143 INT8 TOPS 88.6 INT8 TOPS 14.2 TFLOPS 312 TFLOPS\nSpecialized Blocks- 400Ã—\nAI Engine - 3960Ã—\nAI Tensor Block- 544Ã—\nTensor Cores\n432Ã—\nTensor Cores\nDSP/CUDA Cores 9024 1968 7392 - 12300 4352 6912\nBRAM18K/M20K 4032 967 5063 6847 18960 - -\nURAM/eSRAM 960 463 1301 2 - - -\nOn-chip Memory\nCapacity 41MB 24MB 63.62MB 30MB 46.25MB 5.5MB 40MB\nOff-chip Memory\nCapacity\n8GB HBM2 &\n32GB DDR 16GB DDR 32GB HBM2e &\n32GB DDR 16GB HBM2 32GB HBM2e 11GB DDR 80GB HBM2e\nOn-chip Memory\nBandwidth\n460GB/s &\n38GB/s 102.4GB/s 819.2GB/s &\n102.4GB/s 512GB/s 820GB/s 616GB/s 1935GB/s\nNotice we only discuss two basic parallelism schemes for Transformer models. Some recent\nworks may partition the sequence dimension and leverage reduce-scatter and all-gather to reduce\nthe overheads of all-reduce [33, 51]. The communication time can be similarly analyzed, and we\nwill not discuss them here. The optimal parallelism scheme on multiple devices [48, 62, 73, 82, 105]\nis out of the scope of this paper, and we will leave it as future works.\n4 Case Study\nIn this section, we leverage actual hardware configurations to estimate the model performance\nusing our analytical framework and provide insights for LLM accelerator design.\n4.1 Overview of Workloads and Hardware\nTable 2 lists several widely used models that we choose for performance estimation. Such models\ninclude BERT-base [16], a representative encoder-only model, GPT2 [65], the only open-sourced\nmodel in the GPT family, LLaMA-7B [70, 71], an open-sourced model trained by Meta, and Vicuna-\n13B [11], the best non-commercial model on Chatbot Arena [104].\nTo see the performance differences between FPGAs and GPUs, we pick and list several represen-\ntative devices in Table 3. For FPGAs, Alveo U280 and Agilex 7 are two FPGAs that are widely used\nin cloud servers but not specially optimized for AI workloads; Versal VCK5000 and Stratix 10 NX\nFPGAs are designed for accelerating AI applications with specialized hardware units such as AI\nEngine (AIE) [84] or Tensor Blocks [37]. Versal VHK158 is the latest FPGA with HBM2e released\nby AMD in 2023. For GPUs, RTX 2080Ti is a high-end GPU designed for personal usage with a\nsimilar process node and release date to U280; A100 is the most deployed GPU in data centers to\nconduct LLM training and inference [54].\n12\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\n25 26 27 28 29 210 211 212\nSequence length (tokens)\n101\n102\n103\nLatency (ms)\n(a) BERT\n25 26 27 28 29 210 211 212\nInput sequence length (tokens)\n101\n102\n103\nLatency (ms)\n(b) GPT2 Prefill Stage\n20 21 22 23 24 25 26 27\nOutput sequence length (tokens)\n100\n101\n102\n103\nLatency (ms)\n(c) GPT2 Decode Stage\nRTX 2080Ti\nT esla A100\nAlveo U280 (est.)\nVersal VCK5000 (est.)\nVersal VHK158 (est.)\nStratix 10-NX2100 (est.)\nAgilex 7-AGM039 (est.)\nFig. 6. Latency estimation of BERT and GPT2 on different FPGAs. GPU results are obtained from actual\nprofiling.\n4.2 Single-Device Performance Estimation\nWe first conduct experiments on a single device with pre-trained BERT-base and GPT2 models\nfrom HuggingFace Hub [79]. For GPU baselines, we run the models with PyTorch 2.0 [ 57] and\nCUDA 11.7, and measure the performance on both RTX 2080Ti and A100 GPUs. The host machine\nruns an Intel Xeon Silver 4114 CPU at 2.20GHz with 40 cores. We follow the common practice to\nmeasure the out-of-the-box FP16 performance [2, 53, 65].\nIn this section, we only estimate the performance of FPGAs using the proposed framework in\nÂ§3, and in Â§6 we will evaluate the actual performance on FPGAs. The quantization scheme is 4-bit\nweight and 8-bit activation (W4A8) configurations for FPGA estimations, unless otherwise noted.\n4.2.1 Latency of Different Stages. Based on the constraints of Equations (2), (3), and (6), we can\ncalculate the latency one device can achieve at the maximum compute power ğ‘€ (defined in\nEquation (11)).\nFrom Figure 6(a) and (b), we observe that the 2080Ti and A100 GPUs maintain an almost constant\ncurve when the sequence length is less than 1024. This behavior is primarily attributed to the\nhigh kernel launch overheads for hundreds of CUDA kernels in PyTorch, which overshadow the\ncomputation time. However, when the sequence length exceeds 1024, GPUs become computation-\nbound, resulting in higher latency. In contrast, the latency of FPGAs increases linearly with the\nsequence length, as described in Equation (8), and demonstrates significantly longer latency when\nthe sequence length is large (e.g., 512). It is worth noting that even when making use of state-\nof-the-art AI-optimized FPGAs like the Stratix 10 NX and Versal VCK5000, the situation does\nnot improve significantly. This is because these modern FPGAs are equipped with DDR or older\nversions of HBM, which makes parameter loading from off-chip memory the bottleneck. Even\nthough these FPGAs have highly efficient computation blocks, the compute units have to wait for\nthe memory to fetch data, resulting in suboptimal performance. Moreover, many FPGAs struggle\nto achieve both high memory bandwidth and compute power simultaneously. Consequently, for\nVHK158, performance deteriorates when the sequence length reaches 4096, as it shifts from being\nmemory-bandwidth bound to compute-bound. Further scaling the sequence length larger than\n4096 may lead to out-of-memory for both FPGAs and GPUs, and only A100 can handle such large\nsequence lengths, so the latency is not plotted on the figure.\nThe decode stage is on the contrary, as shown in Figure 6(c). When the sequence length is small,\nGPUs suffer from underutilizing the compute resources, and FPGAs can achieve a significantly\nlower latency compared to GPUs. The latency of RTX2080 and A100 GPUs increases since the\nmemory access takes control compared to computation in the decode stage. Although FPGAs are\nalso bounded by off-chip memory bandwidth, they can always perform better than GPUs since\nthe computation is small when the sequence length is equal to one. Therefore, FPGAs can easily\nachieve GPU-level performance even with a small ğ‘€.\n13\nChen et al.\n0 10000 20000 30000\n# of MACs/cycle M\n101\n102\n103\n104\nLatency (ms)\n(a) LLaMA2 Prefill Stage\nU280\nVCK5000\nVHK158\nIdeal\nA100\n0 500 1000 1500 2000 2500 3000\n# of MACs/cycle M\n101\n102\nLatency (ms)\n(b) LLaMA2 Decode Stage\nU280\nVCK5000\nVHK158\nA100\nFig. 7. Latency estimation of LLaMA2 model. The sequence length is set as 128, and the W4A8 quantization\nscheme is used in this experiment. GPU results are obtained from actual profiling.\nInsight I: Existing FPGAs are inferior in the compute-intensive prefill stage but can\noutperform GPUs in the memory-intensive decode stage.\nTo further investigate what constrains the performance of FPGAs, we conduct an analysis on the\nLLaMA2 model by varying different ğ‘€ and observing the changes in latency. As shown in Figure 7,\nthe VCK5000 FPGA exhibits the smallest off-chip memory bandwidth, which leads it to reach a\nlatency plateau rather quickly. Conversely, the VHK158 FPGA has the largest off-chip memory\nbandwidth, so it can achieve the lowest latency in both prefill and decode stages. Moreover, we\ninclude the curve of ideal FPGA performance in Figure 7 to assess the compute power required to\nattain A100-level performance. Based on this estimation, we need around 30,000 MACs/cycle in\norder to achieve the A100-level performance in the prefill stage, assuming no memory bandwidth\nconstraints. This is achievable by those AI-optimized FPGAs, which can conduct a large number\nof MACs efficiently. On the contrary, for the decode stage, once an FPGA has enough memory\nbandwidth, such as U280, it can reach the A100-level performance easily.\nInsight II: The prefill stage requires large compute power ğ‘€ to achieve the GPU-level\nperformance, while the decode stage only requires a small ğ‘€.\n4.2.2 Quantization Schemes. We then investigate the impact of different quantization schemes and\nmemory packing. We consider quantizing the weight parameters to ğ‘¥ bits and the activation to ğ‘¦\nbits (abbreviated as W{ğ‘¥}A{ğ‘¦}). As shown in Figure 8(a), the red dashed line depicts the maximum\navailable MACs/cycle on-board, which is calculated based on Equation (2). Different quantization\nschemes may have different requirements on BRAM usage constrained by Equation (3). W4A8 is\nthe scheme that can almost fully utilize the compute resources. W8A8 and W16A16 require more\nmemory resources, resulting in lower performance since the computation is bound by the limited\nBRAM resources on-board. Also, we can see quantizing the weights gives the most benefits, but\nquantizing activation only gives little benefit ( ğ‘€ does not change a lot under the same weight\nbitwidth), which is due to the fact that we employ a dataflow architecture and do not require large\nbuffers to store the intermediate tensors on-board.\nInsight III: Weight quantization is necessary for reducing memory usage, while activa-\ntion quantization only has limited benefit.\n4.2.3 Memory Packing. Next, we further consider the impact of memory packing under the W4A8\nsetting. As shown in Figure 8(b), if we do not conduct memory packing, it even cannot satisfy\nthe memory port constraint (Equation (5)) when ğ‘€ is small (blue curve). This is because a large\nnumber of partitioned arrays require more BRAMs, and many BRAMs are not fully utilized causing\na large waste of resources. The orange curve shows packing twoint4 elements to int8, and we can\nachieve a small ğ‘€ under the resource constraint since the number of partitioned arrays is reduced.\nThe green curve packs 9Ã—int4 elements to int36, and it can achieve more than four times of ğ‘€\n14\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\n500 1000 1500\n# of MACs/cycle M\n25\n50\n75\n100\n125Latency (ms)\n(a) Quantization Schemes\n500 1000 1500\n# of MACs/cycle M\n103\n104\nBRAM 36K Usage\n160 720 1440\n(b) Memory Packing\n0 1000 2000 3000\n# of MACs/cycle M\n102\nLatency (ms)\n(c) Memory Bandwidth\nbW=2\nbW=4\nbW=8\nbW=16\nMax available M\nW4A8\nW4A16\nW8A8\nW8A16\nW16A8\nW16A16\nW4A8 w/o packing\nW4A8 w/ 8-bit packing\nW4A8 w/ 32-bit packing\nW4A8 w/ 72-bit packing\nMax available BRAM\nFig. 8. (a) Impact of different quantization techniques on GPT2 prefilling stage on U280. The sequence length\nis set as 128. The cyan line shows the theoretical latency under different ğ‘€ without memory bandwidth\nconstraints. Thin dashed lines depict the maximum ğ‘€ constrained by available BRAM resources. (b) Impact\nof memory packing in the W4A8 setting. (c) Impact of different weight quantization schemes on memory\nbandwidth and overall latency.\ncompared to the int8 packing. The purple curve packs 18Ã—int4 elements to int72, and the curve\ncan almost intersect with the red line before intersecting with the blue line, which means it reaches\nthe maximum DSP constraint on-board (Equation (2)). This study shows that it is important to pack\nthe parameters to reduce on-chip memory usage.\nInsight IV: Memory packing can efficiently reduce the required BRAMs to store the\ntensors.\n4.2.4 Memory Bandwidth. Lastly, we investigate how quantization impacts the required memory\nbandwidth. As shown in Figure 8(c), the low-bit weight quantization can significantly alleviate the\ndemands of off-chip memory access. By reducing the volume of data needed in each cycle, it can\nachieve a larger compute power ğ‘€, thus leading to a better performance. In particular, quantizing\nthe model to a 2-bit representation yields a performance boost exceeding an order of magnitude\nwhen compared to a 16-bit weight quantization scheme. Recent research [7, 103] has demonstrated\nthat 4-bit or even 2-bit quantization can be implemented without compromising model accuracy,\nwhich makes efficient LLM deployment on FPGAs possible.\nInsight V: Low-bit weight quantization can further help alleviate the demands of off-\nchip memory access.\n4.3 Multi-Device Performance Estimation\n500 1000 1500 2000\n# of MACs per cycle per FPGA (M)\n102\n103\nLatency (ms)\n(a) Vicuna-13B Prefill\n500 1000 1500 2000\n# of MACs per cycle per FPGA (M)\n100\n101\nLatency (ms)\n(b) Vicuna-13B Decode\nFPGAÃ—2\nFPGAÃ—4\nFPGAÃ—8\nMax M on U280\nMax M on VHK158\nMax M on VCK5000\nMax M on Stratix 10\nFig. 9. Latency estimations of Vicuna-13B model on multiple FPGAs.\nFor multiple devices, we use the Vicuna-13B model to estimate the performance of 2, 4, and\n8 FPGAs based on our analytical model. As shown in Figure 9, the latency can scale well when\nthe number of devices increases. Since we employ a non-blocking communication scheme in our\ndataflow design as discussed in Â§ 3.4, communication will not be the bottleneck of the design.\n15\nChen et al.\nMultiple FPGAs can reduce the number of required MACs on each device, but cannot increase\nthe number of available MACs on an FPGA, so the performance is still limited by the maximum\navailable resources on-board and the off-chip memory bandwidth. For the decode stage, leveraging\ntwo FPGAs can already reduce the inference latency of the Vicuna-13B model to less than 10ms\nbased on the estimation.\nInsight VI: Multiple FPGAs help reduce overall latency under the same ğ‘€ on each\ndevice.\n5 Implementations\nIn this section, we describe the kernel implementation and accelerator design to show how to\nefficiently achieve the design points in the analytical framework.\n5.1 HLS Kernels\nThis section is not intended to propose new HLS kernels. Instead, we explore the efficient ways to\nreach the maximum available ğ‘€ on FPGAs and implement the standard kernels as a library with\nparameterized quantization support, which is reusable across different models (e.g., BERT and GPT\nmodels implemented in Â§6).\nLinear Operators. Linear operators are the key operators in Transformer models because they\nare ubiquitous and compute-intensive. There are two types of linear operators in the Transformer\nlayers, activation-weight matrix multiply (A-W GEMM) with bias, and activation-activation (A-A\nGEMM) matrix multiply. A-W GEMM includes the projection layers and the linear layers in the\nFFN, with weights buffered on-chip and activations streamed from an input FIFO; A-A GEMM\nincludes the two GEMM operators in the SDP attention, with one of the input activations stored in\na double buffer and the other one streamed.\nWe adopt an output-stationary systolic array architecture to implement the A-W and A-A GEMM\nprocess engines. As shown in Figure 10(a), the systolic array is a 2-D array of MAC units of shape\nğ‘š1 Ã—ğ‘š2 with FIFOs connecting different MAC units. The number of MAC units of linear operatorğ‘–\nis actually ğ‘€ğ‘– defined in Â§3.1. We mainly discuss the A-W GEMM in the following, and the idea\ncan also be applied to the A-A GEMM. The input activation from the previous layer will be first\nbuffered in an activation buffer. After ğ‘š1 elements are buffered, they will be streamed into the\nsystolic array together with the ğ‘š2 weights. There is also a fully partitioned output buffer on-chip\nthat allows the outputs to directly write back.\nWeight / RHS Buffer\nActivation / LHS Buffer 43218765BA09FEDC21\n43876509\n.\n18-bit\n43218765BA09FEDC21\n43218765BA09FEDC1\nA 27-bit\n43876509 2A. 45-bit\nin[7:0]\nw[3:0]w[16:13]\nout[11:0]out[24:13]\n(a) Output-stationary systolic array (b) DSP packing for matrix\nmultiplication of int4 x int8\nFig. 10. Systolic array and DSP packing. The yellow blocks in the systolic array represent output buffers.\nEach MAC unit can be implemented with a single DSP block and can provide one-MAC-per-cycle\nthroughput. Based on the discussion in Â§4.2.2, we adopt the W4A8 quantization scheme for our\n16\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\ninput \nSM \nLN \nGL \nLN \noutput SDP\nRegion 0\n(SLR0)\nRegion 1\n(SLR1)\nRegion 2\n(SLR2)\nMask \nFig. 11. Overall dataflow architecture of a single Transformer layer that uses post-LayerNorm scheme [ 65].\nSDP denotes scaled dot-product. Orange nodes denote the GEMM kernels. Yellow nodes are the non-linear\nkernels, including softmax (SM), LayerNorm (LN), and GELU (GL). Green rectangles represent the FIFOs\nbetween kernels, and purple rectangles are the data loaders.\naccelerator design, which maximizes the utilization of available resources. As a result, the matrix\nmultiplications involve either int4 by int8 or int8 by int8 operations, which are too large for\nLUTs, thus relying primarily on DSPs or specialized compute blocks (e.g., AIE [ 84]). In AMD\nFPGAs, the DSP48E2 hard blocks can support 18-bit by 27-bit multiplication and accumulation [83],\nenabling the packing of two multiplications into one slice for a W4A8 quantized model to save\nDSP resources and achieve a larger ğ‘€. Figure 10(b) shows the bit packing method for 4-bit by\n8-bit integer multiplications. One activation is filled into the lower 8 bits of the 18-bit DSP input,\nand two weights are filled into 0-to-3 and 13-to-16 bit positions of the 27-bit DSP input to avoid\noverlapping results. Finally, the two multiplication results are extracted by bit-slicing the 45-bit\nDSP result. Notice that since the DSP output is wide enough, we can also pack two 8-bit by 8-bit\ninteger multiplications into one DSP slice by further offsetting the second weight and output. With\nDSP packing, we can easily double ğ‘€ to achieve higher performance but with much fewer DSPs.\nNon-Linear Operators. Since quantizing non-linear operators can lead to a significant degradation\nin model accuracy [67, 81], and these non-linear operators are not the bottleneck of the design, we\ndirectly implement the floating-point version of the operators in HLS. Specifically, we buffer a row\nof elements for softmax and LayerNorm functions, which requires conducting reduction along the\nlast dimension. Consequently, this approach eliminates the need to wait for the complete results\nfor the computation of these non-linear operators and effectively prevents dataflow stalling.\n5.2 Accelerator Design\nWe integrate the proposed kernels in Â§5.1 to create a high-performance hardware accelerator.\nThe overall architecture of our proposed accelerator is depicted in Figure 11. Different operators\nincluding linear and non-linear operators are connected with FIFOs, except that the KV cache\ndiscussed in Â§3.2.2 leverages double buffers. This design reads input tensors from off-chip memory\nand stores the results back to memory after each layer. Intermediate activation values are directly\nstreamed to the next operator. Initially, all parameters are stored in DRAM, and data loadersğ¿(Â·) are\nresponsible for loading data from DRAM or streaming buffers. Storing parameters off-chip reduces\nthe number of FPGA devices needed and avoids the need to build different bitstreams for different\nnetwork layers. After completing a layer, the accelerator fetches new parameters from the host\nto the device for the subsequent layer. Since data loading is hidden from the computation, overall\nlatency remains unaffected. Given the contemporary trend of multi-die FPGA designs [ 18, 22],\nexplicit dataflow partitioning becomes necessary to meet timing requirements. Our target device,\nAlveo U280 FPGA [83], has three chip dies called super logic regions (SLRs). Consequently, we\npartition the dataflow into three regions to confine each region fully inside SLR. According to our\n17\nChen et al.\nFig. 12. Physical layout of the implemented spatial accelerator.\nplacement constraints, AMD Vitis toolchain will automatically insert AXI Register Slice IPs to\npipeline SLR crossing.\nWe leverage the proposed analytical framework to guide our accelerator design. Since typical\nTransformer models haveğ‘‘FFN = 4ğ‘‘[16, 65] andğ‘™ < ğ‘‘, according to work balancing of Equation(11),\nwe have ğ‘€ğ‘,ğ‘˜,ğ‘£,ğ‘ = ğ‘€, ğ‘€ğ‘1,ğ‘2 < ğ‘€, and ğ‘€ğ‘“1,ğ‘“2 = 4ğ‘€. A straight-forward division is to put the PEs for\nğ‘, ğ‘˜, ğ‘£, SDP, andğ‘on SLR0, ğ‘“1 on SLR1, and ğ‘“2 on SLR2 so that each SLR roughly contains4ğ‘€MAC\nunits. However, we observe that scaling up the linear operators in FFN poses significant challenges\nto timing closure. Among various configurations of systolic arrays we tested, the maximum capacity\nof one SLR at 250 MHz is three of 8Ã—16 systolic arrays; a single 16 Ã—16 one fails timing. Therefore,\nwe only leverage 8 Ã—8 and 8 Ã—16 systolic arrays for simplicity. We also explore using LUT-based\nmultipliers as they provide greater flexibility for placement compared to DSPs. However, the\npresence of additional inter-LUT wires results in a much lower frequency (191 MHz) compared to\nthe DSP-based multipliers. To minimize the number of SLR crossings, we put ğ‘, ğ‘˜, and ğ‘£ on SLR0\nand use 8 Ã—16 systolic arrays, which also ensures a relatively low latency for the first stage based\non Equation (8). MHA and the ğ‘ projection are on SLR1, with ğ‘1 and ğ‘2 using 8 Ã—8, and ğ‘ using\n8 Ã—16 systolic arrays. ğ‘“1 and ğ‘“2 operators on SLR2 using 8 Ã—16 systolic arrays. Therefore, it can\nstill form a relatively balanced 3:2:2 resource utilization ratio for linear operators.\n6 Evaluation on FPGAs\nIn this section, we implement two design points studied in Â§4 to validate the feasibility of our\nframework. We first describe our experimental setup and perform evaluation on a single FPGA.\n6.1 Experiment Setup\nWe test the publicly available BERT and GPT2 models listed in Table 2. We conduct post-training\nquantization and use the W4A8 quantization scheme for BERT and the W8A8 scheme for GPT,\nwhich are prevalent settings in nowadays LLM inference [81, 103].\nWe implement and run the actual design on an Alveo U280 FPGA [ 83] with ğ‘€ = 256 for the\nkernels. This FPGA has 4032 BRAM18K blocks, 9024 DSP slices, 2.6M flip-flops, 1.3M LUTs, and\n960 URAM blocks. It has three SLRs with an almost equal amount of resources. All the kernels are\nimplemented in C++ with Vitis HLS v2022.1 [89], and synthesized with Vivado backend toolchains.\n18\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\nTable 4. Experimental results compared with other FPGA-based accelerators. Sequence lengths are set as 512.\nName Device Freq(MHz)QuantizationLatency (ms)[Est. (Â§3)] Throughput(samples/sec)Speedup BRAM DSP FF LUT URAM\nOurs U280 245 W4A8 26.01 [24.07] 38.45 - 389 (19%) 1780 (20%) 653K (25%) 569K (44%) 111 (12%)- SLR0 - - - 4.86 - - 130 (19%) 482 (17%) 200K (23%) 167K (38%) 3 (1%)- SLR1 - - - 14.63 - - 136 (20%) 590 (19%) 240K (28%) 212K (49%) 50 (16%)- SLR2 - - - 19.81 - - 123 (18%) 708 (23%) 213K (25%) 191K (44%) 58 (18%)FQ-BERT [46] ZCU111 214 W4A8 95.16 10.51 3.66Ã— 679 (31%) 3287 (77%) 201K (24%) 190K (45%) N/ATRAC [61] ZCU106 200 Fixed8 347.48 2.88 13.36Ã— 181 (29%) 1379 (80%) 128K (28%) 126K (55%) N/A\n32 64 128 256 512\nInput sequence length (tokens)\n101\n102\n103\nLatency (ms)\nGPT2 Prefill\n32 64 128 256 512\nInput sequence length (tokens)\n101\n102\nEnergy Efficiency (tokens/J)\nGPT2 Prefill\n1 4 16 64 128\nOutput sequence length (tokens)\n101\n102\n103\nLatency (ms)\nGPT2 Decode\n1 4 16 64 128\nOutput sequence length (tokens)\n100\n101\nEnergy Efficiency (tokens/J)\nGPT2 Decode\nEst. Ours DFX T esla A100 RTX 2080Ti\nFig. 13. Latency and energy efficiency of GPT2 model on different devices. The GPU results are obtained\nfollowing the same setting in Â§4.\nFigure 12 shows the final device layout of the implemented accelerator. We use OpenCL with\nXilinx RunTime (XRT) for hardware execution and Xilinx Board Utility (xbutil) for computing\npower measurements. The environment for GPU experiments is listed in Â§4.1, and NVIDIA system\nmanagement interface (nvidia-smi) is used for measuring GPU power. Notice the quantized models\non GPUs are slower than the FP16 models, as the quantization methods normally leverage fake\nquantization and lack high-performance GPU kernels to support efficient inference. Therefore, we\ndirectly compare our accelerator with the best FP16 GPU results. The FPGA on-board results match\nthe outputs from the quantized model in PyTorch and are able to achieve the same accuracy. The\nlatency results are the average across fifty runs.\n6.2 On-Board Evaluation\nWe first compare our BERT accelerator with FQ-BERT [46] and TRAC [61], two FPGA-based accel-\nerators for the BERT model. To make a fair comparison, we employ the same W4A8 quantization\nprecision with FQ-BERT and assess the model accuracy using the CoLA dataset [16], a widely-used\nlanguage understanding task. The fp16 model achieves an accuracy of 56.84%, while our W4A8\nquantized model attains 56.76% accuracy. We only compare the performance of the core encoder\nlayers, and report the best on-board latency results of the baselines obtained from the original\npapers. Both baselines use a sequence length of 128, so we scale their latency results to match\nour sequence length of 512. FTrans [39] also targets BERT-variant models, but it does not provide\nfrequency and sequence length in the paper, so we cannot make a direct comparison. From Table 4,\nwe can see our spatial architecture delivers a much lower latency and a higher throughput with a\nmuch lower DSP usage. Even though our evaluation device is not exactly the same as the baselines,\nour throughput improvement still surpasses the resource increment of FF and LUT. Specifically,\nour accelerator is 3.66Ã—faster than FQ-BERT [46] and 13.36Ã—faster than TRAC [61]. Compared\nto temporal architectures, our spatial architecture can efficiently overlap the execution of the\noperators in the model and eliminate most of the data movement overhead. In our design, one layer\ncan start operation once the previous layer finishes computing one tile of the feature map, which\ntypically takes only tens of cycles. In contrast, temporal architectures need to wait for the entire\n19\nChen et al.\ntensor to be produced, which may take hundreds of cycles. Therefore, even if the per-layer latency\nof spatial architectures is longer, the end-to-end latency can still be significantly lower than the\ntemporal architectures employed by FQ-BERT and TRAC. Furthermore, our analytical framework\nprecisely predicts the performance of the accelerator with less than 2ms of differences, showing\nthe practicality of our approach.\nWe next design an accelerator for the GPT2 model. We support importing quantized models\nfrom different quantization frameworks [7, 81, 103]. Specifically, we export the W8A8 model from\nSmoothQuant [81] and achieve 62.2% on the LAMBADA dataset [ 56], whereas the FP16 model\ndemonstrates an accuracy of 65.0%. We compare our GPT accelerator with the state-of-the-art GPT\naccelerator, DFX [23], which employs a temporal architecture with an instruction set and uses the\nsame U280 FPGA device for on-board evaluation. On average, we are 2.16Ã—and 1.10Ã—faster than\nDFX in the prefill and decode stage respectively. This is because our spatial architecture overlaps\nthe computation and greatly eliminates off-chip memory access. We can also see our estimations in\nÂ§3 align closely with the actual performance, achieving a 92% accuracy for the prefill stage. For the\ndecode stage, the estimated latencies are lower than the actual results, which is mainly because the\ninitial interval between two operators is not significantly smaller than the execution time of one\nstage, contributing to a notable increase in latency.\nWe also include the GPU results in Â§4 for a more comprehensive evaluation. As shown in\nFigure 13, neither DFX nor our design performs well during the prefill stage compared to GPUs\nthat have more compute resources to exploit the abundant parallelism. Notably, the latency of\nFPGAs in the prefill stage increases linearly, while the GPU ones almost remain constant as the\nmodel does not fully utilize GPUs. For the decode stage, the situation is reversed. FPGA-based\naccelerators are more efficient than GPUs, and our accelerator can achieve a 1.85Ã—speedup and is\n5.69Ã—more energy efficient compared to the A100 GPU. This is because the generation of each\ntoken is fully sequential, and GPUs cannot leverage their abundant parallelism, and suffer from\nextensive memory access overheads. On the contrary, our dataflow accelerator eliminates most of\nthe off-chip memory accesses and overlaps the compute as much as possible. Thus, we can achieve\na better performance compared to GPUs, aligning with our estimation results in Â§4. Notice the\nU280 FPGA only uses a 16nm process while the A100 GPU has a more advanced 7nm process node\nbased on the data in Table 3, but we can still achieve higher speedup, demonstrating the efficiency\nof our spatial accelerators. It also indicates the potential of further optimizing our HLS design and\nscaling it up to achieve even higher performance.\n6.3 Ablation Study\nWe begin by examining the latency of different SLRs. As shown in rows 3-5 in Table 4, the overall\nlatency of the BERT accelerator is nearly the sum of the latency of SLR0 and SLR2, aligning with\nthe pipeline diagram in Figure 4. Moreover, the computation in SLR1 largely overlaps with that\nof SLR2 due to the fully pipelined design. The resource usage across different SLRs is also similar,\nresulting in a balanced design.\nWe further investigate the efficiency of our kernel functions. We conduct experiments on our tem-\nplate systolic array function with the AutoSA-generated systolic array, which is a highly optimized\nstate-of-the-art systolic array implementation [75]. From Table 5, we can see our implementation\nachieves the same level of performance compared to AutoSA. While maintaining the same DSP\nusage, the resource usage of our kernel function is much smaller than AutoSA. Since the prediction\nof a single GEMM kernel is accurate and can achieve the theoretical performance, our analytical\nmodel can precisely predict the performance of spatial accelerators when combining multiple\nlinear operators. The presence of latency-predictable kernels as foundational components plays\n20\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\na pivotal role in this predictability. Additionally, our function offers enhanced customizability,\naccommodating varying sizes and the choice of different quantization schemes.\nMoreover, employing DSP packing further reduces the DSP usage, allowing one DSP to handle\ntwo MAC operations within a single cycle, a feature not supported in AutoSA. This experiment\nshows the efficiency of our kernels, facilitating the development of high-performance Transformer\naccelerators.\nTable 5. Latency and resource usage of our systolic array library function. Results are directly derived from\nthe HLS report in 300MHz. The GEMM kernel is extracted from the first FFN layer in the BERT-base model\nwith size (512,768)Ã—(768,3072). We use a 16Ã—16 systolic array to calculate theint8 GEMM. The theoretical\npeak performance without DSP packing is (512 Ã—768 Ã—3072)/(16 Ã—16)cycles Ã—3.33 ns/cycle = 15.71 ms.\nLatency (ms) Effectiveğ‘€ BRAM DSP FF LUT\nOurs (w/o DSP packing) 15.73 256 0 (0%) 256 (2%) 88284 (3%) 168190 (12%)\nOurs (w/ DSP packing) 15.73 128 0 (0%) 128 (1%) 79969 (3%) 244439 (18%)\nAutoSA [75] 15.71 N/A 514 (12%) 256 (2%) 100138 (3%) 244032 (18%)\nLastly, we analyze the performance of the non-linear operators. As shown in Table 6, we observe\nthat the softmax operator in the MHA module incurs the highest latency, primarily due to the need\nto compute the exponential function. Since these operators are elementwise and only require a row\nof data to start the computation, they can be easily fused with the preceding linear operators in\nthe pipeline, thereby not significantly impacting the overall latency. For instance, the combined\nlatency of a GEMM kernel (10.77ms) and the softmax operator (6.67ms) greatly exceeds the latency\nof SLR1 in Table 4 (14.63ms/300MHzÃ—245MHz), indicating substantial overlap between the softmax\noperator and other operators. Again, these ablation studies show that considering only the linear\noperators in the analytical framework is sufficient to achieve an accurate latency estimation.\nTable 6. Performance and resource usage of non-linear operators in our kernel library. Kernel sizes are set to\nmatch those of the BERT model in Table 4. Results are directly derived from the HLS report in 300MHz.\nOperator Latency (ms) BRAM DSP FF LUT\nSoftmax 6.67 8 38 4835 7447\nLayerNorm 0.85 20 80 18751 12746\nGeLU 0.67 0 256 26193 16472\n7 Discussion\nIn the previous sections, we provide details of the analytical framework and prove that it can\nachieve high accuracy compared to the latency of actual implementation. However, our framework\nmay have limitations when analyzing the overlay designs or compressed models with sparsity,\nwhich requires changes in the resource and latency estimation. In this section, we will delve into\nseveral unanswered questions and open challenges.\nAI-Optimized FPGAs. In Â§4, we demonstrate the potential of leveraging FPGAs with specialized\ncompute engines to accelerate LLMs. Although AIEs and tensor blocks provide massive compute\npower [37, 84], the memory layout and bandwidth requirements remain undiscovered. Future FPGAs\nfor AI workloads should provide enough memory bandwidth and efficient on-chip interconnect to\nfacilitate the local data movements in a spatial architecture. Moreover, these specialized hardware\nblocks usually adopt a unique programming model with custom compilation flows. It is still an\n21\nChen et al.\nopen question whether existing practices for programming those hardware blocks enable efficient\nexecution of Transformer models.\nTiming Closure on Multi-Die FPGAs. We encounter timing problems in partitioning and\nscaling our design in Â§5.2. In general, it is hard to adequately explore the design space of multi-\ndie partitioning and scaling. There are automated frameworks [22, 29] to generate floorplanning\nconstraints, but they are currently not expressive enough to capture the various data movement\nschemes (e.g., residual connection, multi-head splitting) within Transformer models. We hope\nsimilar tools for Transformers could be derived from our analytical framework to speed up the\ndesign closure.\nHeterogeneous Deployment. Nowadays, data centers are increasingly heterogeneous, with\nCPUs, GPUs, and FPGAs available at scale [ 6, 12, 51]. Therefore, it is possible to leverage the\nadvantages of different hardware to accelerate Transformer models. For example, GPUs are good\nfor the GPT prefill stage due to their high compute power; FPGAs can achieve low-latency decode\nstage with customized spatial architecture. The key challenge is to build a distributed system\nthat efficiently manages hundreds of heterogeneous devices. We hope our analysis on resource\nconstraints, latency, and scaling could assist future deployment and evaluation of LLMs in a\nheterogeneous and distributed environment.\n8 Related Work\nFPGA-Based Transformer Accelerators. Most of the prior works on hardware accelerators\nleverage temporal or overlay architecture with one FPGA [ 26, 28, 39, 40, 46, 59, 63, 64]. Their\nperformance usually suffers from frequent data movements of intermediate results. DFX [ 23]\nexplores using multiple FPGAs to accelerate GPT2 inference, but it is still an overlay design. Some\nresearch has delved into software-hardware co-design to optimize the attention kernel [100]. These\nendeavors often lack in-depth analysis on resource utilization and cannot be easily generalized to\nother kernels.\nQuantization on LLMs. Initial investigations [15, 81, 94] demonstrate lossless 8-bit quantization\nfor LLMs. Subsequent studies [ 21, 31, 44, 94, 96, 103] keep lowering the bit width; the latest\nadvancements reveal that 2-bit [ 7] and even 1-bit (binary) quantization [ 101] are adequate for\nan accurate LLM. While these approaches offer valuable insights, our focus remains orthogonal\nto quantization, as we illustrate optimization techniques and provide high-performance building\nblocks for deploying quantized LLMs on FPGAs.\nHLS Kernel Libraries. Despite the existence of kernel libraries for accelerating Transformer\nmodels on GPUs [14, 38, 79], the hardware domain has seen only a handful of initiatives in this\nregard. AMD provides Vitis HLS library [87, 88] that only has basic kernel-level examples without\ncomprehensive designs tailored for Transformer models. TRAC [61] attempts to provide an HLS-\nbased Transformer library, but its kernel performance is unpredictable, and it exclusively focuses on\nthe BERT model using a temporal architecture. Some frameworks map deep learning frameworks\nto FPGAs [4, 20, 72, 98, 99], but can only handle small CNN designs and do not cater to LLMs.\nMore recent tools allow hardware design using Python [ 24, 35, 55, 80, 95], but are still general-\npurpose and require hardware engineers to construct and optimize kernels from scratch. Our work\nprovides a Transformer kernel library designed for dataflow implementations and demonstrates\ntheir composability in constructing high-performance hardware accelerators.\n9 Conclusion\nIn this paper, we propose an analytical framework for large language models and point out the bot-\ntlenecks and potential optimizations across the prefill and decode stages in the generative inference.\nTo verify the feasibility of our framework, we provide a reusable HLS kernel library to quickly\n22\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\ncompose Transformer kernels into different LLMs that can achieve the expected performance.\nBased on these proposed kernels, we design FPGA-based spatial accelerators for both BERT and\nGPT models and achieve high performance and energy efficiency on par with high-end GPUs. By\noffering insights into performance bottlenecks, a suite of reusable kernels, and a high-performance\naccelerator, we propel the deployment of LLMs for real-world applications while pushing the\nboundaries of hardware innovation.\nAcknowledgments\nThis work was supported in part by ACE, one of the seven centers in JUMP 2.0, a Semiconductor\nResearch Corporation (SRC) program sponsored by DARPA and NSF Awards #2007832, #2019306,\nand #2118709. We would like to thank anonymous reviewers, Keisuke Kamahori, and Zihao Ye for\nproviding insightful feedback. We also thank Jiajie Li, Jie Liu, and Zhanqiu Hu for their contributions\nto the initial LLM modeling and benchmarking.\nReferences\n[1] MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G.\nMurray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.\nTensorflow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Conference on Operating\nSystems Design and Implementation , 2016.\n[2] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji\nRuwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. Deepspeed-inference: Enabling efficient inference\nof transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance\nComputing, Networking, Storage and Analysis , 2022.\n[3] Suhail Basalama, Atefeh Sohrabizadeh, Jie Wang, Licheng Guo, and Jason Cong. Flexcnn: An end-to-end framework\nfor composing cnn accelerators on fpga. ACM Trans. Reconfigurable Technol. Syst. , 16(2), mar 2023.\n[4] Michaela Blott, Thomas B PreuÃŸer, Nicholas J Fraser, Giulio Gambardella, Kenneth Oâ€™brien, Yaman Umuroglu, Miriam\nLeeser, and Kees Vissers. Finn-r: An end-to-end deep-learning framework for fast exploration of quantized neural\nnetworks. ACM Transactions on Reconfigurable Technology and Systems (TRETS) , 11(3):1â€“23, 2018.\n[5] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021.\n[6] Adrian M. Caulfield, Eric S. Chung, Andrew Putnam, Hari Angepat, Jeremy Fowers, Michael Haselman, Stephen Heil,\nMatt Humphrey, Puneet Kaur, Joo-Young Kim, Daniel Lo, Todd Massengill, Kalin Ovtcharov, Michael Papamichael,\nLisa Woods, Sitaram Lanka, Derek Chiou, and Doug Burger. A cloud-scale acceleration architecture. In 2016 49th\nAnnual IEEE/ACM International Symposium on Microarchitecture (MICRO) , pages 1â€“13, 2016.\n[7] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language\nmodels with guarantees. arXiv preprint arXiv:2307.13304 , 2023.\n[8] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating\nlarge language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318 , 2023.\n[9] Hongzheng Chen, Cody Hao Yu, Shuai Zheng, Zhen Zhang, Zhiru Zhang, and Yida Wang. Slapo: A schedule language\nfor progressive optimization of large deep learning model training. In Proceedings of the 29th ACM International\nConference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOSâ€™24) , 2024.\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak,\nJie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan\nLeike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,\nKatie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\nEvaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\n[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4\n23\nChen et al.\nwith 90%* chatgpt quality, March 2023.\n[12] Eric Chung, Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Adrian Caulfield, Todd Massengill, Ming Liu,\nDaniel Lo, Shlomi Alkalay, Michael Haselman, Maleen Abeydeera, Logan Adams, Hari Angepat, Christian Boehn,\nDerek Chiou, Oren Firestein, Alessandro Forin, Kang Su Gatlin, Mahdi Ghandi, Stephen Heil, Kyle Holohan, Ahmad\nEl Husseini, Tamas Juhasz, Kara Kagi, Ratna K. Kovvuri, Sitaram Lanka, Friedel van Megen, Dima Mukhortov, Prerak\nPatel, Brandon Perez, Amanda Rapsang, Steven Reinhardt, Bita Rouhani, Adam Sapek, Raja Seera, Sangeetha Shekar,\nBalaji Sridharan, Gabriel Weisz, Lisa Woods, Phillip Yi Xiao, Dan Zhang, Ritchie Zhao, and Doug Burger. Serving\ndnns in real time at datacenter scale with project brainwave. IEEE Micro, 38(2):8â€“20, 2018.\n[13] Aaron Daniel Cohen, Adam Roberts, Alejandra Molina, Alena Butryna, Alicia Jin, Apoorv Kulshreshtha, Ben Hutchin-\nson, Ben Zevenbergen, Blaise Hilary Aguera-Arcas, Chung ching Chang, Claire Cui, Cosmo Du, Daniel De Freitas\nAdiwardana, Dehao Chen, Dmitry (Dima) Lepikhin, Ed H. Chi, Erin Hoffman-John, Heng-Tze Cheng, Hongrae Lee,\nIgor Krivokon, James Qin, Jamie Hall, Joe Fenton, Johnny Soraker, Kathy Meier-Hellstern, Kristen Olson, Lora Mois\nAroyo, Maarten Paul Bosma, Marc Joseph Pickett, Marcelo Amorim Menegali, Marian Croak, Mark DÃ­az, Matthew\nLamm, Maxim Krikun, Meredith Ringel Morris, Noam Shazeer, Quoc V. Le, Rachel Bernstein, Ravi Rajakumar, Ray\nKurzweil, Romal Thoppilan, Steven Zheng, Taylor Bos, Toju Duke, Tulsee Doshi, Vincent Y. Zhao, Vinodkumar\nPrabhakaran, Will Rusch, YaGuang Li, Yanping Huang, Yanqi Zhou, Yuanzhong Xu, and Zhifeng Chen. Lamda:\nLanguage models for dialog applications. In arXiv preprint arXiv:2201.08239 , 2022.\n[14] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. Flashattention: Fast and memory-efficient\nexact attention with io-awareness. arXiv preprint arXiv:2205.14135 , 2022.\n[15] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for\ntransformers at scale. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in\nNeural Information Processing Systems , 2022.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\n[17] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth,\nSergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and\nPete Florence. Palm-e: An embodied multimodal language model. In arXiv preprint arXiv:2303.03378 , 2023.\n[18] Yixiao Du, Yuwei Hu, Zhongchun Zhou, and Zhiru Zhang. High-Performance Sparse Linear Algebra on HBM-\nEquipped FPGAs Using HLS: A Case Study on SpMV. Intâ€™l Symp. on Field-Programmable Gate Arrays (FPGA) ,\n2022.\n[19] ELS-RD. kernl.ai. https://github.com/ELS-RD/kernl, 2022.\n[20] Farah Fahim, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni,\nGiuseppe Di Guglielmo, Philip Harris, Jeffrey Krupa, Dylan Rankin, Manuel Blanco Valentin, Josiah Hester, Yingyi Luo,\nJohn Mamish, Seda Orgrenci-Memik, Thea Aarrestad, Hamza Javed, Vladimir Loncar, Maurizio Pierini, Adrian Alan\nPol, Sioni Summers, Javier Duarte, Scott Hauck, Shih-Chieh Hsu, Jennifer Ngadiuba, Mia Liu, Duc Hoang, Edward\nKreinar, and Zhenbin Wu. hls4ml: An open-source codesign workflow to empower scientific low-power machine\nlearning devices, 2021.\n[21] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for\ngenerative pretrained transformers. arXiv preprint arXiv:2210.17323 , 2022.\n[22] Licheng Guo, Yuze Chi, Jie Wang, Jason Lau, Weikang Qiao, Ecenur Ustun, Zhiru Zhang, and Jason Cong. Autobridge:\nCoupling coarse-grained floorplanning and pipelining for high-frequency hls design on multi-die fpgas. In The 2021\nACM/SIGDA International Symposium on Field-Programmable Gate Arrays , FPGA â€™21, page 81â€“92, New York, NY, USA,\n2021. Association for Computing Machinery.\n[23] Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae Lee, Minsub Kim, Dongsoo Lee, and Joo-Young Kim. Dfx:\nA low-latency multi-fpga appliance for accelerating transformer-based text generation. In 2022 55th IEEE/ACM\nInternational Symposium on Microarchitecture (MICRO) , pages 616â€“630, 2022.\n[24] Sitao Huang, Kun Wu, Hyunmin Jeong, Chengyue Wang, Deming Chen, and Wen-Mei Hwu. Pylog: An algorithm-\ncentric python-based fpga programming and synthesis flow. IEEE Transactions on Computers , 70(12):2015â€“2028,\n2021.\n[25] HuggingFace. Text generation strategies. https://huggingface.co/docs/transformers/generation_strategies, 2023.\n[26] Suyeon Hur, Seongmin Na, Dongup Kwon, Joonsung Kim, Andrew Boutros, Eriko Nurvitadhi, and Jangwoo Kim. A\nfast and flexible fpga-based accelerator for natural language processing neural networks. ACM Trans. Archit. Code\nOptim., 20(1), feb 2023.\n[27] Intel. Intel agilex 7 fpga and soc fpga. https://www.intel.com/content/www/us/en/products/details/fpga/agilex/7.html,\n2022.\n24\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\n[28] Hamza Khan, Asma Khan, Zainab Khan, Lun Bin Huang, Kun Wang, and Lei He. Npe: An fpga-based overlay\nprocessor for natural language processing. In The 2021 ACM/SIGDA International Symposium on Field-Programmable\nGate Arrays, FPGA â€™21, page 227, New York, NY, USA, 2021. Association for Computing Machinery.\n[29] Moazin Khatti, Xingyu Tian, Yuze Chi, Licheng Guo, Jason Cong, and Zhenman Fang. Pasta: Programming and\nautomation support for scalable task-parallel hls programs on modern multi-die fpgas. In 2023 IEEE 31st Annual\nInternational Symposium on Field-Programmable Custom Computing Machines (FCCM) , pages 12â€“22, 2023.\n[30] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization.\nIn Proceedings of the International Conference on Machine Learning (ICML) , 2021.\n[31] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael Mahoney, and Kurt Keutzer.\nSqueezellm: Dense-and-sparse quantization. arXiv, 2023.\n[32] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing\nHuang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, and Amir Gholami. Full stack optimization of\ntransformer inference: a survey. arXiv preprint arXiv:2302.14017 , 2023.\n[33] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and\nBryan Catanzaro. Reducing activation recomputation in large transformer models. arXiv preprint arXiv:2205.05198 ,\n2022.\n[34] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao\nZhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In\nProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles , 2023.\n[35] Yi-Hsiang Lai, Yuze Chi, Yuwei Hu, Jie Wang, Cody Hao Yu, Yuan Zhou, Jason Cong, and Zhiru Zhang. Heterocl: A\nmulti-paradigm programming infrastructure for software-defined reconfigurable computing. In Proceedings of the\n2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays , 2019.\n[36] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite\nbert for self-supervised learning of language representations. In International Conference on Learning Representations ,\n2020.\n[37] Martin Langhammer, Eriko Nurvitadhi, Bogdan Pasca, and Sergey Gribok. Stratix 10 nx architecture and applications.\nIn The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays , FPGA â€™21, page 57â€“67, New\nYork, NY, USA, 2021. Association for Computing Machinery.\n[38] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru\nHu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable transformer\nmodelling library. https://github.com/facebookresearch/xformers, 2022.\n[39] Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang Chen, Mimi Xie, Lipeng Wan, Hang Liu, and\nCaiwen Ding. Ftrans: Energy-efficient acceleration of transformers using fpga. In Proceedings of the ACM/IEEE\nInternational Symposium on Low Power Electronics and Design , ISLPED â€™20, page 175â€“180, New York, NY, USA, 2020.\nAssociation for Computing Machinery.\n[40] Qin Li, Xiaofan Zhang, Jinjun Xiong, Wen-Mei Hwu, and Deming Chen. Efficient methods for mapping neural\nmachine translator on fpgas. IEEE Transactions on Parallel and Distributed Systems (TPDS) , 32(7):1866â€“1877, 2021.\n[41] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian\nVaughan, Pritam Damania, and Soumith Chintala. Pytorch distributed: Experiences on accelerating data parallel\ntraining. Proc. VLDB Endow. , 2020.\n[42] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling,\nFelix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dâ€™Autume, Igor Babuschkin,\nXinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level\ncode generation with alphacode. Science, 378(6624):1092â€“1097, 2022.\n[43] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao\nZhang, Joseph E. Gonzalez, and Ion Stoica. AlpaServe: Statistical multiplexing with model parallelism for deep\nlearning serving. In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23) , pages\n663â€“679, Boston, MA, July 2023. USENIX Association.\n[44] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight\nquantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978 , 2023.\n[45] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.\n[46] Zejian Liu, Gang Li, and Jian Cheng. Hardware acceleration of fully quantized bert for efficient natural language\nprocessing. 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pages 513â€“516, 2021.\n[47] Meta. Fully sharded data parallel: faster ai training with fewer gpus. https://engineering.fb.com/2021/07/15/open-\nsource/fsdp/, 2021.\n25\nChen et al.\n[48] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui. Galvatron: Efficient\ntransformer training over multiple gpus using automatic parallelism. Proc. VLDB Endow. , 16(3):470â€“479, nov 2022.\n[49] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger,\nPhillip B. Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings\nof the 27th ACM Symposium on Operating Systems Principles , 2019.\n[50] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient pipeline-parallel\ndnn training. In Proceedings of the 38th International Conference on Machine Learning , 2021.\n[51] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri\nVainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training\non gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing,\nNetworking, Storage and Analysis , 2021.\n[52] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\nCodegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International\nConference on Learning Representations , 2023.\n[53] Nvidia. Fastertransformer. https://github.com/NVIDIA/FasterTransformer, 2022.\n[54] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.\n[55] Debjit Pal, Yi-Hsiang Lai, Shaojie Xiang, Niansong Zhang, Hongzheng Chen, Jeremy Casas, Pasquale Cocchini,\nZhenkun Yang, Jin Yang, Louis-NoÃ«l Pouchet, and Zhiru Zhang. Accelerator design with decoupled hardware\ncustomizations: benefits and challenges: invited. In Proceedings of the 59th ACM/IEEE Design Automation Conference ,\nDAC â€™22, page 1351â€“1354, New York, NY, USA, 2022. Association for Computing Machinery.\n[56] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle,\nMarco Baroni, Gemma Boleda, and R. FernÃ¡ndez. The lambada dataset: Word prediction requiring a broad discourse\ncontext. arXiv preprint arXiv:1606.06031 , 2016.\n[57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas KÃ¶pf, Edward Yang, Zach DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\nimperative style, high-performance deep learning library. In Proceedings of the 33rd International Conference on\nNeural Information Processing Systems , 2019.\n[58] Hongwu Peng, Shaoyi Huang, Shiyang Chen, Bingbing Li, Tong Geng, Ang Li, Weiwen Jiang, Wujie Wen, Jinbo Bi,\nHang Liu, and Caiwen Ding. A length adaptive algorithm-hardware co-design of transformer on fpga through sparse\nattention and dynamic pipelining. In Proceedings of the 59th ACM/IEEE Design Automation Conference , DAC â€™22, page\n1135â€“1140, New York, NY, USA, 2022. Association for Computing Machinery.\n[59] Hongwu Peng, Shaoyi Huang, Tong Geng, Ang Li, Weiwen Jiang, Hang Liu, Shusen Wang, and Caiwen Ding.\nAccelerating transformer-based deep learning models on fpgas using column balanced block pruning. In 2021 22nd\nInternational Symposium on Quality Electronic Design (ISQED) , pages 142â€“148, 2021.\n[60] Lucian Petrica, TobÃ­as Alonso, Mairin Kroes, Nicholas J. Fraser, Sorin Dan Cotofana, and Michaela Blott. Memory-\nefficient dataflow inference for deep cnns on fpga. 2020 International Conference on Field-Programmable Technology\n(ICFPT), pages 48â€“55, 2020.\n[61] Patrick Plagwitz, Frank Hannig, and JÃ¼rgen Teich. Trac: Compilation-based design of transformer accelerators for\nfpgas. In 2022 32nd International Conference on Field-Programmable Logic and Applications (FPL) , 2022.\n[62] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan\nHeek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. InProceedings of Machine\nLearning and Systems , 2023.\n[63] Panjie Qi, Edwin Hsing-Mean Sha, Qingfeng Zhuge, Hongwu Peng, Shaoyi Huang, Zhenglun Kong, Yuhong Song,\nand Bingbing Li. Accelerating framework of transformer by hardware design and model compression co-optimization.\nIn 2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD) , page 1â€“9. IEEE Press, 2021.\n[64] Panjie Qi, Yuhong Song, Hongwu Peng, Shaoyi Huang, Qingfeng Zhuge, and Edwin Hsing-Mean Sha. Accommodating\ntransformer onto fpga: Coupling the balanced model compression and fpga-implementation optimization. In\nProceedings of the 2021 on Great Lakes Symposium on VLSI , GLSVLSI â€™21, page 163â€“168, New York, NY, USA, 2021.\nAssociation for Computing Machinery.\n[65] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n[66] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training\ntrillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage\nand Analysis, 2020.\n[67] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer.\nQ-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial\n26\nUnderstanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference\nIntelligence, pages 8815â€“8821. AAAI Press, 2020.\n[68] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm:\nTraining multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 , 2019.\n[69] Mengshu Sun, Zhengang Li, Alec Lu, Haoyu Ma, Geng Yuan, Yanyue Xie, Hao Tang, Yanyu Li, Miriam Leeser,\nZhangyang Wang, Xue Lin, and Zhenman Fang. Fpga-aware automatic acceleration framework for vision transformer\nwith mixed-scheme quantization: Late breaking results. In Proceedings of the 59th ACM/IEEE Design Automation\nConference, DAC â€™22, page 1394â€“1395, New York, NY, USA, 2022. Association for Computing Machinery.\n[70] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste\nRoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume\nLample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[71] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,\nAndrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith,\nRanjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert\nStojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023.\n[72] Yaman Umuroglu, Nicholas J. Fraser, Giulio Gambardella, Michaela Blott, Philip Leong, Magnus Jahre, and Kees\nVissers. Finn: A framework for fast, scalable binarized neural network inference. InProceedings of the 2017 ACM/SIGDA\nInternational Symposium on Field-Programmable Gate Arrays , FPGA â€™17, pages 65â€“74. ACM, 2017.\n[73] Colin Unger, Zhihao Jia, Wei Wu, Sina Lin, Mandeep Baines, Carlos Efrain Quintero Narvaez, Vinay Ramakrishnaiah,\nNirmal Prajapati, Pat McCormick, Jamaludin Mohd-Yusof, Xi Luo, Dheevatsa Mudigere, Jongsoo Park, Misha Smelyan-\nskiy, and Alex Aiken. Unity: Accelerating DNN training through joint optimization of algebraic transformations\nand parallelization. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22) , pages\n267â€“284, Carlsbad, CA, July 2022. USENIX Association.\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural Information Processing Systems , 2017.\n[75] Jie Wang, Licheng Guo, and Jason Cong. Autosa: A polyhedral compiler for high-performance systolic arrays on\nfpga. In The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays , FPGA â€™21, page 93â€“104,\nNew York, NY, USA, 2021. Association for Computing Machinery.\n[76] Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivasa\nMurthy, Marcello Maggioni, Qiao Zhang, Sameer Kumar, Tongfei Guo, Yuanzhong Xu, and Zongwei Zhou. Overlap\ncommunication with dependent computation via decomposition in large deep learning models. In Proceedings of\nthe 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems,\nVolume 1, ASPLOS 2023, page 93â€“106, New York, NY, USA, 2023. Association for Computing Machinery.\n[77] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William\nFedus. Emergent abilities of large language models. Transactions on Machine Learning Research , 2022.\n[78] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny\nZhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages\n24824â€“24837. Curran Associates, Inc., 2022.\n[79] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, RÃ©mi Louf, Morgan Funtowicz, et al. Huggingfaceâ€™s transformers: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771 , 2019.\n[80] Shaojie Xiang, Yi-Hsiang Lai, Yuan Zhou, Hongzheng Chen, Niansong Zhang, Debjit Pal, and Zhiru Zhang. Heteroflow:\nAn accelerator programming model with decoupled data placement for software-defined fpgas. In Proceedings of the\n2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays , 2022.\n[81] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate and\nefficient post-training quantization for large language models. In Proceedings of the 40th International Conference on\nMachine Learning , 2023.\n[82] Ningning Xie, Tamara Norman, Dominik Grewe, and Dimitrios Vytiniotis. Synthesizing optimal parallelism placement\nand reduction strategies on hierarchical systems for deep learning. In D. Marculescu, Y. Chi, and C. Wu, editors,\n27\nChen et al.\nProceedings of Machine Learning and Systems , volume 4, pages 548â€“566, 2022.\n[83] AMD Xilinx. Alveo u280 data center accelerator card. https://www.xilinx.com/products/boards-and-kits/alveo/u280.\nhtml#specifications, 2021.\n[84] AMD Xilinx. AI Engines and Their Applications. White paper, AMD Xilinx, Dec 2022.\n[85] AMD Xilinx. QSFP Module Connector, 2022.\n[86] AMD Xilinx. Vck5000 versal development card. https://www.xilinx.com/products/boards-and-kits/vck5000.html#\nspecs, 2022.\n[87] AMD Xilinx. Vitis accelerated libraries. https://github.com/Xilinx/Vitis_Libraries, 2022.\n[88] AMD Xilinx. Vitis ai: Adaptable & real-time ai inference acceleration. https://github.com/Xilinx/Vitis-AI, 2022.\n[89] AMD Xilinx. Vitis hls v2022.1. https://www.xilinx.com/products/design-tools/vitis/vitis-platform.html, 2022.\n[90] AMD Xilinx. Versal vhk158. https://www.xilinx.com/products/boards-and-kits/vhk158.html, 2023.\n[91] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei\nWang, and Tie-Yan Liu. On layer normalization in the transformer architecture. InProceedings of the 37th International\nConference on Machine Learning , ICMLâ€™20. JMLR.org, 2020.\n[92] Bowen Yang, Jian Zhang, Jonathan Li, Christopher Re, Christopher Aberger, and Christopher De Sa. Pipemare:\nAsynchronous pipeline parallel dnn training. In Proceedings of Machine Learning and Systems , 2021.\n[93] Zhuoping Yang, Jinming Zhuang, Jiaqi Yin, Cunxi Yu, Alex K. Jones, and Peipei Zhou. Aim: Accelerating arbitrary-\nprecision integer multiplication on heterogeneous reconfigurable computing platform versal acap. In 2023 IEEE/ACM\nInternational Conference On Computer Aided Design (ICCAD) , 2023.\n[94] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient\nand affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing\nSystems, 35:27168â€“27183, 2022.\n[95] Hanchen Ye, Cong Hao, Jianyi Cheng, Hyunmin Jeong, Jack Huang, Stephen Neuendorffer, and Deming Chen.\nScalehls: A new scalable high-level synthesis framework on multi-level intermediate representation. In 2022 IEEE\nInternational Symposium on High-Performance Computer Architecture (HPCA) , 2022.\n[96] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang\nWu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models. arXiv preprint\narXiv:2304.01089, 2023.\n[97] Chen Zhang, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Cong. Optimizing fpga-based accelerator\ndesign for deep convolutional neural networks. In Proceedings of the 2015 ACM/SIGDA International Symposium on\nField-Programmable Gate Arrays , FPGA â€™15, page 161â€“170, New York, NY, USA, 2015. Association for Computing\nMachinery.\n[98] Xiaofan Zhang, Junsong Wang, Chao Zhu, Yonghua Lin, Jinjun Xiong, Wen-mei Hwu, and Deming Chen. Dnnbuilder:\nan automated tool for building high-performance dnn hardware accelerators for fpgas. In2018 IEEE/ACM International\nConference on Computer-Aided Design (ICCAD) , pages 1â€“8, 2018.\n[99] Xiaofan Zhang, Hanchen Ye, Junsong Wang, Yonghua Lin, Jinjun Xiong, Wen-mei Hwu, and Deming Chen. Dnnex-\nplorer: A framework for modeling and exploring a novel paradigm of fpga-based dnn accelerator. In Proceedings of\nthe 39th International Conference on Computer-Aided Design , ICCAD â€™20, New York, NY, USA, 2020. Association for\nComputing Machinery.\n[100] Xinyi Zhang, Yawen Wu, Peipei Zhou, Xulong Tang, and Jingtong Hu. Algorithm-hardware co-design of attention\nmechanism on fpga devices. ACM Trans. Embed. Comput. Syst. , 20(5s), sep 2021.\n[101] Yichi Zhang, Ankush Garg, Yuan Cao, Åukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized\nneural machine translation. arXiv preprint arXiv:2302.04907 , 2023.\n[102] Yichi Zhang, Junhao Pan, Xinheng Liu, Hongzheng Chen, Deming Chen, and Zhiru Zhang. FracBNN: Accurate and\nFPGA-Efficient Binary Neural Networks with Fractional Activations. The 2021 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays , 2021.\n[103] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi\nChen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint\narXiv:2310.19102, 2023.\n[104] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\nDacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and\nchatbot arena, 2023.\n[105] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong\nXu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, and Ion Stoica. Alpa: Automating inter- and Intra-Operator\nparallelism for distributed deep learning. In 16th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI 22), pages 559â€“578, Carlsbad, CA, July 2022. USENIX Association.\n28",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.8147637844085693
    },
    {
      "name": "Computer science",
      "score": 0.812047004699707
    },
    {
      "name": "Field-programmable gate array",
      "score": 0.750324010848999
    },
    {
      "name": "Dataflow",
      "score": 0.7011358737945557
    },
    {
      "name": "Inference",
      "score": 0.6434999704360962
    },
    {
      "name": "Computer architecture",
      "score": 0.5395246744155884
    },
    {
      "name": "Design space exploration",
      "score": 0.4642394781112671
    },
    {
      "name": "Toolchain",
      "score": 0.4622160792350769
    },
    {
      "name": "Reuse",
      "score": 0.4508377015590668
    },
    {
      "name": "Hardware acceleration",
      "score": 0.4506707787513733
    },
    {
      "name": "Embedded system",
      "score": 0.4381347894668579
    },
    {
      "name": "Parallel computing",
      "score": 0.41054049134254456
    },
    {
      "name": "Computer engineering",
      "score": 0.3582342267036438
    },
    {
      "name": "Artificial intelligence",
      "score": 0.17896974086761475
    },
    {
      "name": "Programming language",
      "score": 0.10618457198143005
    },
    {
      "name": "Software",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ]
}