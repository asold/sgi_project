{
    "title": "Are Transformers Effective for Time Series Forecasting?",
    "url": "https://openalex.org/W4382203079",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2892973432",
            "name": "Ailing Zeng",
            "affiliations": [
                "Digital Science (United States)",
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2566542011",
            "name": "Muxi Chen",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2082114532",
            "name": "Lei Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096590084",
            "name": "Qiang Xu",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2892973432",
            "name": "Ailing Zeng",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2082114532",
            "name": "Lei Zhang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6642591857",
        "https://openalex.org/W2127334389",
        "https://openalex.org/W1536447791",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W1678356000",
        "https://openalex.org/W6822349517",
        "https://openalex.org/W2954731415",
        "https://openalex.org/W3171665133",
        "https://openalex.org/W6889955440",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2607045400",
        "https://openalex.org/W6742924651",
        "https://openalex.org/W3111507638",
        "https://openalex.org/W4225494949",
        "https://openalex.org/W4225301559",
        "https://openalex.org/W3212890323",
        "https://openalex.org/W4385763767",
        "https://openalex.org/W2230528047",
        "https://openalex.org/W1969852690",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2604847698",
        "https://openalex.org/W2792764867",
        "https://openalex.org/W4302438404"
    ],
    "abstract": "Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.",
    "full_text": "Are Transformers Effective for Time Series Forecasting?\nAiling Zeng1,2*, Muxi Chen1*, Lei Zhang2, Qiang Xu1\n1The Chinese University of Hong Kong\n2International Digital Economy Academy\n{zengailing, leizhang}@idea.edu.cn,{mxchen21, qxu}@cse.cuhk.edu.hk\nAbstract\nRecently, there has been a surge of Transformer-based solu-\ntions for the long-term time series forecasting (LTSF) task.\nDespite the growing performance over the past few years,\nwe question the validity of this line of research in this work.\nSpecifically, Transformers is arguably the most successful so-\nlution to extract the semantic correlations among the elements\nin a long sequence. However, in time series modeling, we are\nto extract the temporal relations in an ordered set of contin-\nuous points. While employing positional encoding and using\ntokens to embed sub-series in Transformers facilitate preserv-\ning some ordering information, the nature of thepermutation-\ninvariant self-attention mechanism inevitably results in tem-\nporal information loss.\nTo validate our claim, we introduce a set of embarrassingly\nsimple one-layer linear models named LTSF-Linear for com-\nparison. Experimental results on nine real-life datasets show\nthat LTSF-Linear surprisingly outperforms existing sophisti-\ncated Transformer-based LTSF models in all cases, and often\nby a large margin. Moreover, we conduct comprehensive em-\npirical studies to explore the impacts of various design ele-\nments of LTSF models on their temporal relation extraction\ncapability. We hope this surprising finding opens up new re-\nsearch directions for the LTSF task. We also advocate revisit-\ning the validity of Transformer-based solutions for other time\nseries analysis tasks (e.g., anomaly detection) in the future.\nIntroduction\nTime series are ubiquitous in today’s data-driven world.\nGiven historical data, time series forecasting (TSF) is a\nlong-standing task that has a wide range of applications,\nincluding but not limited to traffic flow estimation, energy\nmanagement, and financial investment. Over the past sev-\neral decades, TSF solutions have undergone a progression\nfrom traditional statistical methods (e.g., ARIMA (Ariyo,\nAdewumi, and Ayo 2014)) and machine learning techniques\n(e.g., GBRT (Friedman 2001)) to deep learning-based solu-\ntions, e.g., (Bai, Kolter, and Koltun 2018; Liu et al. 2022).\nTransformer (Vaswani et al. 2017) is arguably the most\nsuccessful sequence modeling architecture, demonstrating\nunparalleled performances in various applications, such as\nnatural language processing (NLP) (Devlin et al. 2018),\n*These authors contributed equally.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nspeech recognition (Dong, Xu, and Xu 2018), and computer\nvision (Liu et al. 2021b). Recently, there has also been a\nsurge of Transformer-based solutions for time series analy-\nsis, as surveyed in (Wen et al. 2022). Most notable mod-\nels, which focus on the less explored and challenging long-\nterm time series forecasting (LTSF) problem, include Log-\nTrans (Li et al. 2019) (NeurIPS 2019), Informer (Zhou et al.\n2021) (AAAI 2021 Best paper), Autoformer (Xu et al. 2021)\n(NeurIPS 2021), Pyraformer (Liu et al. 2021a) (ICLR 2022\nOral), Triformer (Cirstea et al. 2022) (IJCAI 2022) and the\nrecent FEDformer (Zhou et al. 2022) (ICML 2022).\nThe main working power of Transformers is from its\nmulti-head self-attention mechanism, which has a remark-\nable capability of extracting semantic correlations among el-\nements in a long sequence (e.g., words in texts or 2D patches\nin images). However, self-attention ispermutation-invariant\nand “anti-order” to some extent. While using various types\nof positional encoding techniques can preserve some order-\ning information, it is still inevitable to have temporal infor-\nmation loss after applying self-attention on top of them. This\nis usually not a serious concern for semantic-rich applica-\ntions such as NLP, e.g., the semantic meaning of a sentence\nis largely preserved even if we reorder some words in it.\nHowever, when analyzing time series data, there is usually\na lack of semantics in the numerical data itself, and we are\nmainly interested in modeling the temporal changes among\na continuous set of points. That is, the order itself plays\nthe most crucial role. Consequently, we pose the following\nintriguing question: Are Transformers really effective for\nlong-term time series forecasting?\nMoreover, while existing Transformer-based LTSF so-\nlutions have demonstrated considerable prediction accu-\nracy improvements over traditional methods, in their exper-\niments, all the compared (non-Transformer) baselines per-\nform autoregressive or iterated multi-step (IMS) forecast-\ning (Ariyo, Adewumi, and Ayo 2014; Salinas, Flunkert, and\nGasthaus 2017; Bahdanau, Cho, and Bengio 2014; Taylor\nand Letham 2017), which are known to suffer from sig-\nnificant error accumulation effects for the LTSF problem.\nTherefore, in this work, we challenge Transformer-based\nLTSF solutions with direct multi-step (DMS) forecasting\nstrategies to validate their real performance.\nNot all time series are predictable, let alone long-term\nforecasting (e.g., for chaotic systems). We hypothesize that\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n11121\nlong-term forecasting is only feasible for those time series\nwith a relatively clear trend and periodicity. As linear mod-\nels can already extract such information, we introduce a set\nof embarrassingly simple models named LTSF-Linear as a\nnew baseline for comparison. LTSF-Linear regresses histor-\nical time series with a one-layer linear model to forecast fu-\nture time series directly. We conduct extensive experiments\non nine widely-used benchmark datasets that cover various\nreal-life applications: traffic, energy, economics, weather,\nand disease predictions. Surprisingly, our results show that\nLTSF-Linear outperforms existing complex Transformer-\nbased models in all cases, and often by a large margin (20%\n∼ 50%). Moreover, we find that, in contrast to the claims in\nexisting Transformers, most of them fail to extract temporal\nrelations from long sequences, i.e., the forecasting errors are\nnot reduced (sometimes even increased) with the increase of\nlook-back window sizes. Finally, we conduct various abla-\ntion studies on existing Transformer-based TSF solutions to\nstudy the impact of various design elements in them.\nTo sum up, the contributions of this work include:\n• To the best of our knowledge, this is the first work to\nchallenge the effectiveness of the booming Transformers\nfor the long-term time series forecasting task.\n• To validate our claims, we introduce a set of embar-\nrassingly simple one-layer linear models, named LTSF-\nLinear, and compare them with existing Transformer-\nbased LTSF solutions on nine benchmarks. LTSF-Linear\ncan be a new baseline for the LTSF problem.\n• We conduct comprehensive empirical studies on various\naspects of existing Transformer-based solutions, includ-\ning the capability of modeling long inputs, the sensitivity\nto time series order, the impact of positional encoding\nand sub-series embedding, and efficiency comparisons.\nOur findings would benefit future research in this area.\nWith the above, we conclude that the temporal model-\ning capabilities of Transformers for time series are exagger-\nated, at least for the existing LTSF benchmarks. At the same\ntime, while LTSF-Linear achieves a better prediction accu-\nracy compared to existing works, it merely serves as a sim-\nple baseline for future research on the challenging long-term\nTSF problem. With our findings, we also advocate revisiting\nthe validity of Transformer-based solutions for other time\nseries analysis tasks (e.g., anomaly detection) in the future.\nPreliminaries: TSF Problem Formulation\nFor time series containing C variates, given historical data\nX = {Xt\n1, ..., Xt\nC}L\nt=1, wherein L is the look-back window\nsize and Xt\ni is the value of theith variate at thetth time step.\nThe time series forecasting task is to predict the values ˆX =\n{ ˆXt\n1, ...,ˆXt\nC}L+T\nt=L+1 at the T future time steps. WhenT >1,\niterated multi-step (IMS) forecasting (Taieb, Hyndman et al.\n2012) learns a single-step forecaster and iteratively applies it\nto obtain multi-step predictions. Alternatively, direct multi-\nstep (DMS) forecasting (Chevillon 2007) directly optimizes\nthe multi-step forecasting objective at once.\nCompared to DMS forecasting results, IMS predictions\nhave smaller variance thanks to the autoregressive estima-\ntion procedure, but they inevitably suffer from error accu-\nmulation effects. Consequently, IMS forecasting is prefer-\nable when there is a highly-accurate single-step forecaster,\nand T is relatively small. In contrast, DMS forecasting gen-\nerates more accurate predictions when it is hard to obtain an\nunbiased single-step forecasting model, or T is large.\nTransformer-Based LTSF Solutions\nTransformer-based models (Vaswani et al. 2017) have\nachieved unparalleled performances in many long-standing\nAI tasks in natural language processing and computer vi-\nsion fields, thanks to the effectiveness of the multi-head\nself-attention mechanism. This has also triggered lots of re-\nsearch interest in Transformer-based time series modeling\ntechniques (Wen et al. 2022). In particular, a large amount\nof research works are dedicated to the LTSF task (e.g., (Li\net al. 2019; Liu et al. 2021a; Xu et al. 2021; Zhou et al. 2021,\n2022)). Considering the ability to capture long-range depen-\ndencies with Transformer models, most of them focus on the\nless-explored long-term forecasting problem (T ≫ 1)1.\nWhen applying the vanilla Transformer model to the\nLTSF problem, it has some limitations, including the\nquadratic time/memory complexity with the original self-\nattention scheme and error accumulation caused by the au-\ntoregressive decoder design. Informer (Zhou et al. 2021) ad-\ndresses these issues and proposes a novel Transformer ar-\nchitecture with reduced complexity and a DMS forecasting\nstrategy. Later, more Transformer variants introduce various\ntime series features into their models for performance or effi-\nciency improvements (Liu et al. 2021a; Xu et al. 2021; Zhou\net al. 2022). We summarize the design elements of existing\nTransformer-based LTSF solutions as follows (see Figure 1).\nTime series decomposition:For data preprocessing, nor-\nmalization with zero-mean is common in TSF. Besides, Aut-\noformer (Xu et al. 2021) first applies seasonal-trend de-\ncomposition behind each neural block, which is a standard\nmethod in time series analysis to make raw data more pre-\ndictable (Cleveland 1990; Hamilton 2020). Specifically, they\nuse a moving average kernel on the input sequence to extract\nthe trend-cyclical component of the time series. The differ-\nence between the original sequence and the trend component\nis regarded as the seasonal component. On top of the de-\ncomposition scheme of Autoformer, FEDformer (Zhou et al.\n2022) further proposes the mixture of experts’ strategies to\nmix the trend components extracted by moving average ker-\nnels with various kernel sizes.\nInput embedding strategies:The self-attention layer in the\nTransformer architecture cannot preserve the positional in-\nformation of the time series. However, local positional infor-\nmation, i.e. the ordering of time series, is important. Besides,\nglobal temporal information, such as hierarchical times-\ntamps (week, month, year) and agnostic timestamps (holi-\ndays and events), is also informative (Zhou et al. 2021). To\nenhance the temporal context of time-series inputs, a practi-\ncal design in the SOTA Transformer-based methods is inject-\ning several embeddings, like a fixed positional encoding, a\n1Due to page limit, we leave the discussion of non-Transformer\nforecasting solutions in the Appendix.\n11122\n(d) Decoder(c) Encoder(b) Embedding(a) Preprocessing\nForecasting Output\n(d) Decoder(c) Encoder(b) Embedding(a) Preprocessing\nOutput\nInput\nSeasonal-trend \ndecomposition\nNormalization\nTimestamp\npreparation\nChannel projection\nFixed position\nLocal timestamp\nGlobal timestamp\nProbSparse and distilling \nself-attention @Informer\nSeries auto-correlation with \ndecomposition @Autoformer\nMulti-resolution pyramidal     \nattention @Pyraformer\nFrequency enhanced block with \ndecomposition @FEDformer\nLogSparse and convolutional \nself-attention @LogTrans\nDirect Multi-Step \n(DMS) @Informer\nDMS with auto-correlation and \ndecomposition @Autoformer\nDMS along spatio-temporal \ndimension @Pyraformer\nDMS with frequency attention \nand decomposition@FEDformer\nIterated Multi-Step \n(IMS) @LogTrans\nFigure 1: The pipeline of existing Transformer-based TSF solutions. In (a) and (b), the solid boxes are essential operations, and\nthe dotted boxes are applied optionally. (c) and (d) are distinct for different methods (Li et al. 2019; Zhou et al. 2021; Xu et al.\n2021; Liu et al. 2021a; Zhou et al. 2022).\nchannel projection embedding, and learnable temporal em-\nbeddings into the input sequence. Moreover, temporal em-\nbeddings with a temporal convolution layer (Li et al. 2019)\nor learnable timestamps (Xu et al. 2021) are introduced.\nSelf-attention schemes: Transformers rely on the self-\nattention mechanism to extract the semantic dependencies\nbetween paired elements. Motivated by reducing theO\n\u0000\nL2\u0001\ntime and memory complexity of the vanilla Transformer, re-\ncent works propose two strategies for efficiency. On the one\nhand, LogTrans and Pyraformer explicitly introduce a spar-\nsity bias into the self-attention scheme. Specifically, Log-\nTrans uses a Logsparse mask to reduce the computational\ncomplexity to O (LlogL) while Pyraformer adopts pyrami-\ndal attention that captures hierarchically multi-scale tem-\nporal dependencies with an O (L) time and memory com-\nplexity. On the other hand, Informer and FEDformer use\nthe low-rank property in the self-attention matrix. Informer\nproposes a ProbSparse self-attention mechanism and a self-\nattention distilling operation to decrease the complexity to\nO (LlogL), and FEDformer designs a Fourier enhanced\nblock and a wavelet enhanced block with random selection\nto obtain O (L) complexity. Lastly, Autoformer designs a\nseries-wise auto-correlation mechanism to replace the origi-\nnal self-attention layer.\nDecoders: The vanilla Transformer decoder outputs se-\nquences in an autoregressive manner, resulting in a slow in-\nference speed and error accumulation effects, especially for\nlong-term predictions. Informer designs a generative-style\ndecoder for DMS forecasting. Other Transformer variants\nemploy similar DMS strategies. For instance, Pyraformer\nuses a fully-connected layer concatenating Spatio-temporal\naxes as the decoder. Autoformer sums up two refined de-\ncomposed features from trend-cyclical components and the\nstacked auto-correlation mechanism for seasonal compo-\nnents to get the final prediction. FEDformer also uses a de-\ncomposition scheme with the proposed frequency attention\nblock to decode the final results.\nThe premise of Transformer models is the semantic cor-\nrelations between paired elements, while the self-attention\nmechanism itself is permutation-invariant, and its capabil-\nity of modeling temporal relations largely depends on posi-\ntional encodings associated with input tokens. Considering\nthe raw numerical data in time series (e.g., stock prices or\nelectricity values), there are hardly any point-wise semantic\ncorrelations between them. In time series modeling, we are\nmainly interested in the temporal relations among a contin-\nuous set of points, and the order of these elements instead\nof the paired relationship plays the most crucial role. While\nemploying positional encoding and using tokens to embed\nsub-series facilitate preserving some ordering information,\nthe nature of the permutation-invariant self-attention mech-\nanism inevitably results in temporal information loss. Due\nto the above observations, we are interested in revisiting the\neffectiveness of Transformer-based LTSF solutions.\nAn Embarrassingly Simple Baseline for LTSF\nIn the experiments of existing Transformer-based LTSF so-\nlutions (T ≫ 1), all the compared (non-Transformer) base-\nlines are IMS forecasting techniques, which are known to\nsuffer from significant error accumulation effects. We hy-\npothesize that the performance improvements in these works\nare largely due to the DMS strategy used in them.\nFigure 2: Illustration of the basic linear model.\nTo validate this hypothesis, we present the simplest DMS\nmodel via a temporal linear layer, named LTSF-Linear, as\na baseline for comparison. The basic formulation of LTSF-\nLinear directly regresses historical time series for future pre-\ndiction via a weighted sum operation (as illustrated in Fig-\nure 2). The mathematical expression is ˆXi = WXi, where\n11123\nDatasets ETTh1&ETTh2 ETTm1 &ETTm2 Traffic Electricity Exchange-Rate Weather ILI\nVariates 7 7 862 321 8 21 7\nTimesteps 17,420 69,680 17,544 26,304 7,588 52,696 966\nGranularity 1hour 5min 1hour 1hour 1day 10min 1week\nTable 1: The statistics of the nine popular datasets for the LTSF problem.\nMethods IMP. Linear* NLinear* DLinear* FEDformer Autoformer Informer Pyraformer* Repeat*\nMetric MSE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nElectricity\n96 27% 0.140 0.237 0.141 0.237 0.140 0.237 0.193 0.308 0.201 0.317 0.274 0.368 0.386 0.449 1.588 0.946\n192 24% 0.153 0.250 0.154 0.248 0.153 0.249 0.201 0.315 0.222 0.334 0.296 0.386 0.386 0.443 1.595 0.950\n336 21% 0.169 0.268 0.171 0.265 0.169 0.267 0.214 0.329 0.231 0.338 0.300 0.394 0.378 0.443 1.617 0.961\n720 17% 0.203 0.301 0.210 0.297 0.203 0.301 0.246 0.355 0.254 0.361 0.373 0.439 0.376 0.445 1.647 0.975\nExchange\n96 45% 0.082 0.207 0.089 0.208 0.081 0.203 0.148 0.278 0.197 0.323 0.847 0.752 0.376 1.105 0.081 0.196\n192 42% 0.167 0.304 0.180 0.300 0.157 0.293 0.271 0.380 0.300 0.369 1.204 0.895 1.748 1.151 0.167 0.289\n336 34% 0.328 0.432 0.331 0.415 0.305 0.414 0.460 0.500 0.509 0.524 1.672 1.036 1.874 1.172 0.305 0.396\n720 46% 0.964 0.750 1.033 0.780 0.643 0.601 1.195 0.841 1.447 0.941 2.478 1.310 1.943 1.206 0.823 0.681\nTraffic\n96 30% 0.410 0.282 0.410 0.279 0.410 0.282 0.587 0.366 0.613 0.388 0.719 0.391 2.085 0.468 2.723 1.079\n192 30% 0.423 0.287 0.423 0.284 0.423 0.287 0.604 0.373 0.616 0.382 0.696 0.379 0.867 0.467 2.756 1.087\n336 30% 0.436 0.295 0.435 0.290 0.436 0.296 0.621 0.383 0.622 0.337 0.777 0.420 0.869 0.469 2.791 1.095\n720 26% 0.466 0.315 0.464 0.307 0.466 0.315 0.626 0.382 0.660 0.408 0.864 0.472 0.881 0.473 2.811 1.097\nWeather\n96 19% 0.176 0.236 0.182 0.232 0.176 0.237 0.217 0.296 0.266 0.336 0.300 0.384 0.896 0.556 0.259 0.254\n192 21% 0.218 0.276 0.225 0.269 0.220 0.282 0.276 0.336 0.307 0.367 0.598 0.544 0.622 0.624 0.309 0.292\n336 23% 0.262 0.312 0.271 0.301 0.265 0.319 0.339 0.380 0.359 0.395 0.578 0.523 0.739 0.753 0.377 0.338\n720 20% 0.326 0.365 0.338 0.348 0.323 0.362 0.403 0.428 0.419 0.428 1.059 0.741 1.004 0.934 0.465 0.394\nILI\n24 48% 1.947 0.985 1.683 0.858 2.215 1.081 3.228 1.260 3.483 1.287 5.764 1.677 1.420 2.012 6.587 1.701\n36 36% 2.182 1.036 1.703 0.859 1.963 0.963 2.679 1.080 3.103 1.148 4.755 1.467 7.394 2.031 7.130 1.884\n48 34% 2.256 1.060 1.719 0.884 2.130 1.024 2.622 1.078 2.669 1.085 4.763 1.469 7.551 2.057 6.575 1.798\n60 34% 2.390 1.104 1.819 0.917 2.368 1.096 2.857 1.157 2.770 1.125 5.264 1.564 7.662 2.100 5.893 1.677\nETTh1\n96 1% 0.375 0.397 0.374 0.394 0.375 0.399 0.376 0.419 0.449 0.459 0.865 0.713 0.664 0.612 1.295 0.713\n192 4% 0.418 0.429 0.408 0.415 0.405 0.416 0.420 0.448 0.500 0.482 1.008 0.792 0.790 0.681 1.325 0.733\n336 7% 0.479 0.476 0.429 0.427 0.439 0.443 0.459 0.465 0.521 0.496 1.107 0.809 0.891 0.738 1.323 0.744\n720 13% 0.624 0.592 0.440 0.453 0.472 0.490 0.506 0.507 0.514 0.512 1.181 0.865 0.963 0.782 1.339 0.756\nETTh2\n96 20% 0.288 0.352 0.277 0.338 0.289 0.353 0.346 0.388 0.358 0.397 3.755 1.525 0.645 0.597 0.432 0.422\n192 20% 0.377 0.413 0.344 0.381 0.383 0.418 0.429 0.439 0.456 0.452 5.602 1.931 0.788 0.683 0.534 0.473\n336 26% 0.452 0.461 0.357 0.400 0.448 0.465 0.496 0.487 0.482 0.486 4.721 1.835 0.907 0.747 0.591 0.508\n720 14% 0.698 0.595 0.394 0.436 0.605 0.551 0.463 0.474 0.515 0.511 3.647 1.625 0.963 0.783 0.588 0.517\nETTm1\n96 21% 0.308 0.352 0.306 0.348 0.299 0.343 0.379 0.419 0.505 0.475 0.672 0.571 0.543 0.510 1.214 0.665\n192 21% 0.340 0.369 0.349 0.375 0.335 0.365 0.426 0.441 0.553 0.496 0.795 0.669 0.557 0.537 1.261 0.690\n336 17% 0.376 0.393 0.375 0.388 0.369 0.386 0.445 0.459 0.621 0.537 1.212 0.871 0.754 0.655 1.283 0.707\n720 22% 0.440 0.435 0.433 0.422 0.425 0.421 0.543 0.490 0.671 0.561 1.166 0.823 0.908 0.724 1.319 0.729\nETTm2\n96 18% 0.168 0.262 0.167 0.255 0.167 0.260 0.203 0.287 0.255 0.339 0.365 0.453 0.435 0.507 0.266 0.328\n192 18% 0.232 0.308 0.221 0.293 0.224 0.303 0.269 0.328 0.281 0.340 0.533 0.563 0.730 0.673 0.340 0.371\n336 16% 0.320 0.373 0.274 0.327 0.281 0.342 0.325 0.366 0.339 0.372 1.363 0.887 1.201 0.845 0.412 0.410\n720 13% 0.413 0.435 0.368 0.384 0.397 0.421 0.421 0.415 0.433 0.432 3.379 1.338 3.625 1.451 0.521 0.465\n- Methods* are implemented by us; Other results are from FEDformer (Zhou et al. 2022).\nTable 2: Multivariate long-term forecasting errors in terms of MSE and MAE, the lower the better. Among them, ILI dataset is\nwith forecasting horizon T ∈ {24,36, 48, 60}. For the others, T ∈ {96,192, 336, 720}. The best results are highlighted in bold\nand the best results of Transformers are highlighted with an underline. IMP. is the best result of linear models compared to the\nresults of Transformer-based solutions.\nW ∈ RT×L is a linear layer along the temporal axis. ˆXi\nand Xi are the prediction and input for each ith variate.\nNote that LTSF-Linear shares weights across different vari-\nates and does not model any spatial correlations.\nLTSF-Linear is a set of linear models. Vanilla Linear is a\none-layer linear model. To handle time series across differ-\nent domains (e.g., finance, traffic, and energy domains), we\nfurther introduce two variants with two preprocessing meth-\nods, named DLinear and NLinear.\n• Specifically, DLinear is a combination of a Decompo-\nsition scheme used in Autoformer and FEDformer with\nlinear layers. It first decomposes a raw data input into a\ntrend component by a moving average kernel and a re-\nmainder (seasonal) component. Then, two one-layer lin-\near layers are applied to each component, and we sum up\nthe two features to get the final prediction. By explicitly\n11124\nhandling trend, DLinear enhances the performance of a\nvanilla linear when there is a clear trend in the data.\n• Meanwhile, to boost the performance of LTSF-Linear\nwhen there is a distribution shift in the dataset, NLin-\near first subtracts the input by the last value of the se-\nquence. Then, the input goes through a linear layer, and\nthe subtracted part is added back before making the final\nprediction. The subtraction and addition inNLinear are a\nsimple normalization for the input sequence.\nExperiments\nExperimental Settings\nDataset. We conduct extensive experiments on nine\nwidely-used real-world datasets, including ETT (Electric-\nity Transformer Temperature) (Zhou et al. 2021) (ETTh1,\nETTh2, ETTm1, ETTm2), Traffic, Electricity, Weather, ILI,\nExchange-Rate (Lai et al. 2017). All of them are multivariate\ntime series. We leave data descriptions in the Appendix.\nEvaluation metric. Following previous works (Zhou\net al. 2021; Xu et al. 2021; Zhou et al. 2022), we use Mean\nSquared Error (MSE) and Mean Absolute Error (MAE).\nCompared methods. We include four recent\nTransformer-based methods: FEDformer (Fourier) (Zhou\net al. 2022), Autoformer (Xu et al. 2021), Informer (Zhou\net al. 2021), Pyraformer (Liu et al. 2021a). Besides, we\ninclude a naive DMS method: Closest Repeat (Repeat ),\nwhich repeats the last value in the look-back window.\nComparison with Transformers\nQuantitative results. In Table 2, we extensively evaluate\nall mentioned Transformers on nine benchmarks, following\nthe experimental setting of previous work (Xu et al. 2021;\nZhou et al. 2022, 2021). Surprisingly, the performance of\nLTSF-Linear surpasses the SOTA FEDformer in most cases\nby 20% ∼ 50% improvements on the multivariate forecast-\ning, where LTSF-Linear even does not model correlations\namong variates. For different time series benchmarks,NLin-\near and DLinear show the superiority to handle the dis-\ntribution shift and trend-seasonality features. We also pro-\nvide results for univariate forecasting of ETT datasets in\nthe Appendix, where LTSF-Linear still consistently outper-\nforms Transformer-based LTSF solutions by a large mar-\ngin. In general, these results reveal that existing complex\nTransformer-based LTSF solutions are not seemingly effec-\ntive on the existing nine benchmarks while LTSF-Linear\ncan be a powerful baseline. Another interesting observa-\ntion is that even though the naive Repeat method shows\nworse results when predicting long-term seasonal data (e.g.,\nElectricity ), it surprisingly outperforms all Transformers on\nExchange-Rate (around 45%). This is mainly caused by the\nwrong prediction of trends in Transformer-based solutions,\nwhich may overfit toward sudden change noises in the train-\ning data, resulting in significant accuracy degradation.\nQualitative results. As shown in Figure 3, we plot the\nprediction results on three selected time series datasets with\nTransformer-based solutions and LTSF-Linear: Electricity\n(Sequence 1951, Variate 36) , where it has different tem-\nporal patterns. When the input length is 96 steps, and the\n0 50 100 150 200 250 300\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\nGrouthTruth Autoformer Informer FEDformer DLinear\nFigure 3: Illustration of the long-term forecasting outputs\n(Y-axis) of five models with an input lengthL=96 and output\nlength T=192 (X-axis) on Electricity.\noutput horizon is 336 steps, Transformers fail to capture the\nscale and bias of the future data . Moreover, they can hardly\npredict a proper trend on aperiodic data.\nMore Analyses on Transformer-Based Solutions\nCan existing LTSF-Transformers extract temporal rela-\ntions well from longer input sequences? The size of the\nlook-back window greatly impacts forecasting accuracy as\nit determines how much we can learn from historical data.\nGenerally speaking, a powerful TSF model with a strong\ntemporal relation extraction capability should be able to\nachieve better results with larger look-back window sizes.\nTo study the impact of input look-back window sizes, we\nconduct experiments with L ∈ {24, ...,720} for long-term\nforecasting (T=720). Similar to the observations from pre-\nvious studies (Zhou et al. 2021; Wen et al. 2022), existing\nTransformers’ performance deteriorates or stays stable when\nthe look-back window size increases. In contrast, the perfor-\nmances of allLTSF-Linear are significantly boosted with the\nincrease of look-back window size. Thus, existing solutions\ntend to overfit temporal noises instead of extracting tempo-\nral information if given a longer sequence, and the input size\n96 is exactly suitable for most Transformers.\nWhat can be learned for long-term forecasting? While\nthe temporal dynamics in the look-back window signifi-\ncantly impact the forecasting accuracy of short-term time\nseries forecasting, we hypothesize that long-term forecasting\ndepends on whether models can capture the trend and peri-\nodicity well only.That is, the farther the forecasting horizon,\nthe less impact the look-back window itself has.\nMethods FEDformer Autoformer\nInput Close Far Close Far\nElectricity 0.251 0.265 0.255 0.287\nTraffic 0.631 0.645 0.677 0.675\nTable 3: The MSE comparisons of different input sequences.\nTo validate the above hypothesis, in Table 3, we com-\n11125\n24 48 72 96 120 144 168 192 336 504 672 720\n0.20\n0.25\n0.30\n0.35\n0.40\nTransformer\nInformer\nAutoformer\nFEDformer\nPyraformer\nLinear\nNLinear\nDLinear\nFigure 4: The MSE results (Y-axis) of models with different\nlook-back window sizes (X-axis) of long-term forecasting\n(T=720) on Electricity.\npare the forecasting accuracy for the same future 720 time\nsteps with data from two different look-back windows: (i).\nthe original input L=96 setting (called Close) and (ii). the\nfar input L=96 setting (called Far) that is before the original\n96 time steps. The performance of the SOTA Transformers\ndrops slightly, indicating these models only capture simi-\nlar temporal information from the adjacent time series se-\nquence. Since capturing the intrinsic characteristics of the\ndataset generally does not require a large number of param-\neters, i,e. one parameter can represent the periodicity. Using\ntoo many parameters will even cause overfitting.\nAre the self-attention scheme effective for LTSF? We\nverify whether these complex designs in the existing Trans-\nformer (e.g., Informer) are essential. In Table 4, we grad-\nually transform Informer to Linear. First, we replace each\nself-attention layer with a linear layer, called Att.-Linear,\nsince a self-attention layer can be regarded as a fully-\nconnected layer where weights are dynamically changed.\nFurthermore, we discard other auxiliary designs (e.g., FFN)\nin Informer to leave embedding layers and linear layers,\nnamed Embed + Linear. Finally, we simplify the model to\none linear layer. As can be observed, the performance of In-\nformer grows with the gradual simplification, thereby chal-\nlenging the necessity of these modules.\nMethods Informer Att.-Linear Embed + Linear Linear\nExchange\n96 0.847 1.003 0.173 0.084\n192 1.204 0.979 0.443 0.155\n336 1.672 1.498 1.288 0.301\n720 2.478 2.102 2.026 0.763\nETTh1\n96 0.865 0.613 0.454 0.400\n192 1.008 0.759 0.686 0.438\n336 1.107 0.921 0.821 0.479\n720 1.181 0.902 1.051 0.515\nTable 4: The MSE comparisons of gradually transforming\nInformer to a Linear from the left to right columns.\nCan existing LTSF-Transformers preserve temporal or-\nder well? Self-attention is inherently permutation-invariant,\ni.e., regardless of the order. However, in time-series forecast-\ning, the sequence order often plays a crucial role. We argue\nthat even with positional and temporal embeddings, existing\nTransformer-based methods still suffer from temporal infor-\nmation loss. In Table 5, we shuffle the raw input before\nthe embedding strategies. Two shuffling strategies are pre-\nsented: Shuf. randomly shuffles the whole input sequences\nand Half-Ex. exchanges the first half of the input sequence\nwith the second half. Interestingly, compared with the orig-\ninal setting (Ori.) on the Exchange Rate, the performance\nof all Transformer-based methods does not fluctuate even\nwhen the input sequence is randomly shuffled. By contrary,\nthe performance of LTSF-Linear is damaged significantly.\nThese indicate that LTSF-Transformers with different posi-\ntional and temporal embeddings preserve quite limited tem-\nporal relations and are prone to overfit on noisy financial\ndata, while the simpleLTSF-Linear can model the order nat-\nurally and avoid overfitting with fewer parameters.\nFor the ETTh1 dataset, FEDformer and Autoformer in-\ntroduce time series inductive bias into their models, mak-\ning them can extract certain temporal information when the\ndataset has more clear temporal patterns (e.g., periodicity)\nthan the Exchange Rate. Therefore, the average drops of\nthe two Transformers are 73.28% and 56.91% under the\nShuf. setting, where it loses the whole order information.\nMoreover, Informer still suffers less from both Shuf. and\nHalf-Ex. settings due to its no such temporal inductive bias.\nOverall, the average drops of LTSF-Linear are larger than\nTransformer-based methods for all cases, indicating the ex-\nisting Transformers do not preserve temporal order well.\nHow effective are different embedding strategies? In Ta-\nble 6, the forecasting errors of Informer largely increase\nwithout positional embeddings (wo/Pos.). Without times-\ntamp embeddings (wo/Temp.) will gradually damage the\nperformance of Informer as the forecasting lengths increase.\nSince Informer uses a single time step for each token, it is\nnecessary to introduce temporal information in tokens.\nRather than using a single time step in each token, FED-\nformer and Autoformer input a sequence of timestamps to\nembed the temporal information. Hence, they can achieve\ncomparable or even better performance without fixed po-\nsitional embeddings. However, without timestamp embed-\ndings, the performance of Autoformer declines rapidly be-\ncause of the loss of global temporal information. Instead,\nthanks to the frequency-enhanced module proposed in FED-\nformer to introduce temporal inductive bias, it suffers less\nfrom removing any position/timestamp embeddings.\nIs training data size a limiting factor for existing LTSF-\nTransformers? Some may argue that the poor performance\nof Transformer-based solutions is due to the small sizes of\nthe benchmark datasets. Unlike computer vision or natural\nlanguage processing tasks, TSF is performed on collected\ntime series, and it is difficult to scale up the training data\nsize. In fact, the size of the training data would indeed have a\nsignificant impact on the model performance. Accordingly,\nwe conduct experiments on Traffic, comparing the perfor-\nmance of the model trained on a full dataset (17,544*0.7\nhours), named Ori., with that training on a shortened dataset\n11126\nMethods Linear FEDformer Autoformer Informer\nPredict Length Ori. Shuf. Half-Ex. Ori. Shuf. Half-Ex. Ori. Shuf. Hal\nf-Ex. Ori. Shuf. Half-Ex\n.\nExchange\n96 0.080 0.133 0.169 0.161 0.160 0.162 0.152 0.158 0.160 0.952 1.004 0.959\n192 0.162 0.208 0.243 0.274 0.275 0.275 0.278 0.271 0.277 1.012 1.023 1.014\n336 0.286 0.320 0.345 0.439 0.439 0.439 0.435 0.430 0.435 1.177 1.181 1.177\n720 0.806 0.819 0.836 1.122 1.122 1.122 1.113 1.113 1.113 1.198 1.210 1.196\nAverage Drop N/A 27.26% 46.81% N/A -0.09% 0.20% N/A 0.09% 1.12% N/A -0.12% -0.18%\nETTh1\n96 0.395 0.824 0.431 0.376 0.753 0.405 0.455 0.838 0.458 0.974 0.971 0.971\n192 0.447 0.824 0.471 0.419 0.730 0.436 0.486 0.774 0.491 1.233 1.232 1.231\n336 0.490 0.825 0.505 0.447 0.736 0.453 0.496 0.752 0.497 1.693 1.693 1.691\n720 0.520 0.846 0.528 0.468 0.720 0.470 0.525 0.696 0.524 2.720 2.716 2.715\nAverage Drop N/A 81.06% 4.78% N/A 73.28% 3.44% N/A 56.91% 0.46% N/A 1.98% 0.18%\nTable 5: The MSE comparisons of models when shuffling the raw input sequence. Shuf. randomly shuffles the input sequence.\nHalf-EX. randomly exchanges the first half of the input sequences with the second half. We run five times.\nMethods Embedding Traffic\n96 192 336 720\nFEDformer\nAll 0.597 0.606 0.627 0.649\nwo/Pos. 0.587 0.604 0.621 0.626\nwo/Temp. 0.613 0.623 0.650 0.677\nwo/Pos.-Temp. 0.613 0.622 0.648 0.663\nAutoformer\nAll 0.629 0.647 0.676 0.638\nwo/Pos. 0.613 0.616 0.622 0.660\nwo/Temp. 0.681 0.665 0.908 0.769\nwo/Pos.-Temp. 0.672 0.811 1.133 1.300\nInformer\nAll 0.719 0.696 0.777 0.864\nwo/Pos. 1.035 1.186 1.307 1.472\nwo/Temp. 0.754 0.780 0.903 1.259\nwo/Pos.-Temp. 1.038 1.351 1.491 1.512\nTable 6: The MSE comparisons of different embedding\nstrategies on Transformer-based methods with look-back\nwindow size 96 and forecasting lengths {96, 192, 336, 720}.\n(8,760 hours, i.e., 1 year), called Short. Unexpectedly, Ta-\nble 7 presents that the prediction errors with reduced train-\ning data are usually lower. This might be because the whole-\nyear data maintain clearer temporal features than a longer\nbut incomplete data size. While we cannot conclude that we\nshould use fewer data for training, it demonstrates that the\ntraining data scale is not the limiting reason.\nMethods FEDformer Autoformer\nDataset Ori. Short Ori. Short\n96 0.587 0.568 0.613 0.594\n192 0.604 0.584 0.616 0.621\n336 0.621 0.601 0.622 0.621\n720 0.626 0.608 0.660 0.650\nTable 7: The MSE comparisons of two training data sizes.\nIs efficiency really a top-level priority? Existing LTSF-\nTransformers claim that the O\n\u0000\nL2\u0001\ncomplexity of the\nvanilla Transformer is unaffordable for the LTSF problem.\nAlthough they prove to be able to improve the theoretical\ntime and memory complexity from O\n\u0000\nL2\u0001\nto O (L), it is\nunclear whether 1) the actual inference time and memory\ncost on devices are improved, and 2) the memory issue is\nunacceptable and urgent for today’s GPU (e.g., an NVIDIA\nTitan XP here). In Table 8, we compare the average prac-\ntical efficiencies with 5 runs. Interestingly, compared with\nthe vanilla Transformer (with the same DMS decoder), most\nTransformer variants incur similar or worse inference time\nand parameters in practice. These follow-ups introduce more\nadditional design elements to make practical costs high.\nMethod MACs Parameter\nTime Memory\nDLinear 0.04G 139.7K 0.4ms 687MiB\nTransformer× 4.03G 13.61M 26.8ms 6091MiB\nInformer 3.93G 14.39M 49.3ms 3869MiB\nAutoformer 4.41G 14.91M 164.1ms 7607MiB\nPyraformer 0.80G 241.4M 3.4ms 7017MiB\nFEDformer 4.41G 20.68M 40.5ms 4143MiB\n× the same one-step decoder.\nTable 8: Comparison of practical efficiency of LTSF-\nTransformers under L=96 and T=720 on the Electricity.\nMACs are the number of multiply-accumulate operations.\nThe inference time averages 5 runs.\nConclusion and Future Work\nConclusion. This work questions the effectiveness of\nemerging favored Transformer-based solutions for the long-\nterm time series forecasting problem. We use an embarrass-\ningly simple linear model LTSF-Linear as a DMS forecast-\ning baseline to verify our claims. Note that our contributions\ndo not come from proposing a linear model but rather from\nthrowing out an important question, showing surprising\ncomparisons, and demonstrating why LTSF-Transformers\nare not as effective as claimed in these works through var-\nious perspectives. We sincerely hope our comprehensive\nstudies can benefit future work in this area.\nFuture work.LTSF-Linear has a limited model capacity,\nand it merely serves a simple yet competitive baseline with\nstrong interpretability for future research. Consequently, we\nbelieve there is great potential for new model designs, data\nprocessing, and benchmarks to tackle LTSF.\nAcknowledgments\nThis work was supported in part by Alibaba Group Holding\nLtd. under Grant No. TA2015393. We thank the anonymous\nreviewers for their constructive comments and suggestions.\n11127\nReferences\nAriyo, A. A.; Adewumi, A. O.; and Ayo, C. K. 2014. Stock\nprice prediction using the ARIMA model. In 2014 UKSim-\nAMSS 16th International Conference on Computer Mod-\nelling and Simulation, 106–112. IEEE.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural Ma-\nchine Translation by Jointly Learning to Align and Trans-\nlate. arXiv: Computation and Language arXiv:1409.0473.\nBai, S.; Kolter, J. Z.; and Koltun, V . 2018. An empirical\nevaluation of generic convolutional and recurrent networks\nfor sequence modeling. arXiv preprint arXiv:1803.01271.\nChevillon, G. 2007. Direct multi-step estimation and fore-\ncasting. Journal of Economic Surveys, 21(4): 746–785.\nCirstea, R.-G.; Guo, C.; Yang, B.; Kieu, T.; Dong, X.;\nand Pan, S. 2022. Triformer: Triangular, Variable-Specific\nAttentions for Long Sequence Multivariate Time Series\nForecasting–Full Version.arXiv preprint arXiv:2204.13767.\nCleveland, R. B. 1990. STL : A Seasonal-Trend Decomposi-\ntion Procedure Based on Loess. Journal of Office Statistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDong, L.; Xu, S.; and Xu, B. 2018. Speech-transformer:\na no-recurrence sequence-to-sequence model for speech\nrecognition. In 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 5884–\n5888. IEEE.\nFriedman, J. H. 2001. Greedy function approximation: a\ngradient boosting machine. Annals of statistics, 1189–1232.\nHamilton, J. D. 2020. Time series analysis. Princeton uni-\nversity press.\nLai, G.; Chang, W.-C.; Yang, Y .; and Liu, H. 2017. Modeling\nLong- and Short-Term Temporal Patterns with Deep Neural\nNetworks. international acm sigir conference on research\nand development in information retrieval.\nLi, S.; Jin, X.; Xuan, Y .; Zhou, X.; Chen, W.; Wang, Y .-X.;\nand Yan, X. 2019. Enhancing the locality and breaking the\nmemory bottleneck of transformer on time series forecast-\ning. Advances in Neural Information Processing Systems,\n32.\nLiu, M.; Zeng, A.; Chen, M.; Xu, Z.; Lai, Q.; Ma, L.; and\nXu, Q. 2022. SCINet: Time Series Modeling and Forecast-\ning with Sample Convolution and Interaction. Thirty-sixth\nConference on Neural Information Processing Systems.\nLiu, S.; Yu, H.; Liao, C.; Li, J.; Lin, W.; Liu, A. X.; and\nDustdar, S. 2021a. Pyraformer: Low-complexity pyramidal\nattention for long-range time series modeling and forecast-\ning. In International Conference on Learning Representa-\ntions.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021b. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012–10022.\nSalinas, D.; Flunkert, V .; and Gasthaus, J. 2017. DeepAR:\nProbabilistic Forecasting with Autoregressive Recurrent\nNetworks. International Journal of Forecasting.\nTaieb, S. B.; Hyndman, R. J.; et al. 2012. Recursive and\ndirect multi-step forecasting: the best of both worlds, vol-\nume 19. Citeseer.\nTaylor, S. J.; and Letham, B. 2017. Forecasting at Scale.\nPeerJ Prepr.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWen, Q.; Zhou, T.; Zhang, C.; Chen, W.; Ma, Z.; Yan, J.; and\nSun, L. 2022. Transformers in Time Series: A Survey.arXiv\npreprint arXiv:2202.07125.\nXu, J.; Wang, J.; Long, M.; et al. 2021. Autoformer: Decom-\nposition transformers with auto-correlation for long-term se-\nries forecasting. Advances in Neural Information Processing\nSystems, 34.\nZhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.;\nand Zhang, W. 2021. Informer: Beyond Efficient Trans-\nformer for Long Sequence Time-Series Forecasting. In\nThe Thirty-Fifth AAAI Conference on Artificial Intelligence,\nAAAI 2021, Virtual Conference, volume 35, 11106–11115.\nAAAI Press.\nZhou, T.; Ma, Z.; Wen, Q.; Wang, X.; Sun, L.; and Jin, R.\n2022. FEDformer: Frequency enhanced decomposed trans-\nformer for long-term series forecasting. In International\nConference on Machine Learning.\n11128"
}