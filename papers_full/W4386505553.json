{
  "title": "Automated ICD coding using extreme multi-label long text transformer-based models",
  "url": "https://openalex.org/W4386505553",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2152438523",
      "name": "Leibo Liu",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A1989184110",
      "name": "Óscar Pérez Concha",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2046956051",
      "name": "Anthony Nguyen",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Australian e-Health Research Centre"
      ]
    },
    {
      "id": "https://openalex.org/A2116657651",
      "name": "Vicki Bennett",
      "affiliations": [
        "Australian Institute of Health and Welfare"
      ]
    },
    {
      "id": "https://openalex.org/A220057491",
      "name": "Louisa Jorm.",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2152438523",
      "name": "Leibo Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989184110",
      "name": "Óscar Pérez Concha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2046956051",
      "name": "Anthony Nguyen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116657651",
      "name": "Vicki Bennett",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A220057491",
      "name": "Louisa Jorm.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6779667404",
    "https://openalex.org/W3211454967",
    "https://openalex.org/W3004598824",
    "https://openalex.org/W2020026361",
    "https://openalex.org/W2100676408",
    "https://openalex.org/W6782540692",
    "https://openalex.org/W4224541024",
    "https://openalex.org/W4287854971",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3134691471",
    "https://openalex.org/W6770221138",
    "https://openalex.org/W3212490246",
    "https://openalex.org/W3121194378",
    "https://openalex.org/W3116041582",
    "https://openalex.org/W6803799213",
    "https://openalex.org/W4221011356",
    "https://openalex.org/W6789091562",
    "https://openalex.org/W6779440454",
    "https://openalex.org/W6801930474",
    "https://openalex.org/W4310568840",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W2046788142",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W6785110541",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6808848689",
    "https://openalex.org/W6739076403",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3211566171",
    "https://openalex.org/W4241597302",
    "https://openalex.org/W4250316874",
    "https://openalex.org/W2914874661",
    "https://openalex.org/W3096784882",
    "https://openalex.org/W2997050424"
  ],
  "abstract": "Encouraged by the success of pretrained Transformer models in many natural language processing tasks, their use for International Classification of Diseases (ICD) coding tasks is now actively being explored. In this study, we investigated two existing Transformer-based models (PLM-ICD and XR-Transformer) and proposed a novel Transformer-based model (XR-LAT), aiming to address the extreme label set and long text classification challenges that are posed by automated ICD coding tasks. The Transformer-based model PLM-ICD, which currently holds the state-of-the-art (SOTA) performance on the ICD coding benchmark datasets MIMIC-III and MIMIC-II, was selected as our baseline model for further optimisation on both datasets. In addition, we extended the capabilities of the leading model in the general extreme multi-label text classification domain, XR-Transformer, to support longer sequences and trained it on both datasets. Moreover, we proposed a novel model, XR-LAT, which was also trained on both datasets. XR-LAT is a recursively trained model chain on a predefined hierarchical code tree with label-wise attention, knowledge transferring and dynamic negative sampling mechanisms. Our optimised PLM-ICD models, which were trained with longer total and chunk sequence lengths, significantly outperformed the current SOTA PLM-ICD models, and achieved the highest micro-F1 scores of 60.8 % and 50.9 % on MIMIC-III and MIMIC-II, respectively. The XR-Transformer model, although SOTA in the general domain, did not perform well across all metrics. The best XR-LAT based models obtained results that were competitive with the current SOTA PLM-ICD models, including improving the macro-AUC by 2.1 % and 5.1 % on MIMIC-III and MIMIC-II, respectively. Our optimised PLM-ICD models are the new SOTA models for automated ICD coding on both datasets, while our novel XR-LAT models perform competitively with the previous SOTA PLM-ICD models.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.738588809967041
    },
    {
      "name": "Transformer",
      "score": 0.7029780745506287
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4964314103126526
    },
    {
      "name": "Machine learning",
      "score": 0.4319643974304199
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.4311528205871582
    },
    {
      "name": "Language model",
      "score": 0.42704877257347107
    },
    {
      "name": "Data mining",
      "score": 0.3363202214241028
    },
    {
      "name": "Natural language processing",
      "score": 0.32117509841918945
    },
    {
      "name": "Engineering",
      "score": 0.10499301552772522
    },
    {
      "name": "Voltage",
      "score": 0.10120159387588501
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}