{
  "title": "Delving Deep into the Generalization of Vision Transformers under Distribution Shifts",
  "url": "https://openalex.org/W3168378457",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5024002344",
      "name": "Chongzhi Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100761766",
      "name": "Mingyuan Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5085436669",
      "name": "Shanghang Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5021922190",
      "name": "Daisheng Jin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101463643",
      "name": "Qiang Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5054947207",
      "name": "Zhongang Cai",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076824143",
      "name": "Haiyu Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101696728",
      "name": "Shuai Yi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024067284",
      "name": "Xianglong Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100406050",
      "name": "Ziwei Liu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2902617128",
    "https://openalex.org/W3022061250",
    "https://openalex.org/W2885106262",
    "https://openalex.org/W2981720610",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3036438747",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2982505672",
    "https://openalex.org/W2962935454",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2975758481",
    "https://openalex.org/W2159291411",
    "https://openalex.org/W2986381065",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2603777577",
    "https://openalex.org/W2798658180",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3144102935",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W2970692043",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W3202200367"
  ],
  "abstract": "Vision Transformers (ViTs) have achieved impressive performance on various vision tasks, yet their generalization under distribution shifts (DS) is rarely understood. In this work, we comprehensively study the out-of-distribution (OOD) generalization of ViTs. For systematic investigation, we first present a taxonomy of DS. We then perform extensive evaluations of ViT variants under different DS and compare their generalization with Convolutional Neural Network (CNN) models. Important observations are obtained: 1) ViTs learn weaker biases on backgrounds and textures, while they are equipped with stronger inductive biases towards shapes and structures, which is more consistent with human cognitive traits. Therefore, ViTs generalize better than CNNs under DS. With the same or less amount of parameters, ViTs are ahead of corresponding CNNs by more than 5% in top-1 accuracy under most types of DS. 2) As the model scale increases, ViTs strengthen these biases and thus gradually narrow the in-distribution and OOD performance gap. To further improve the generalization of ViTs, we design the Generalization-Enhanced ViTs (GE-ViTs) from the perspectives of adversarial learning, information theory, and self-supervised learning. By comprehensively investigating these GE-ViTs and comparing with their corresponding CNN models, we observe: 1) For the enhanced model, larger ViTs still benefit more for the OOD generalization. 2) GE-ViTs are more sensitive to the hyper-parameters than their corresponding CNN models. We design a smoother learning strategy to achieve a stable training process and obtain performance improvements on OOD data by 4% from vanilla ViTs. We hope our comprehensive study could shed light on the design of more generalizable learning architectures.",
  "full_text": "Delving Deep into the Generalization of Vision Transformers\nunder Distribution Shifts\nChongzhi Zhang1,* , Mingyuan Zhang2,* , Shanghang Zhang3,* , Daisheng Jin1, Qiang Zhou4,\nZhongang Cai2,5 , Haiyu Zhao2,5 , Xianglong Liu1, Ziwei Liu2, \f\n1Beihang University 2S-Lab, Nanyang Technological University\n3Peking University 4AIR, Tsinghua University 5Shanghai AI Laboratory\nAbstract\nVision Transformers (ViTs) have achieved impressive per-\nformance on various vision tasks, yet their generalization un-\nder distribution shifts (DS) is rarely understood. In this work,\nwe comprehensively study the out-of-distribution (OOD) gen-\neralization of ViTs. For systematic investigation, we ï¬rst\npresent a taxonomy of DS. We then perform extensive evalu-\nations of ViT variants under different DS and compare their\ngeneralization with Convolutional Neural Network (CNN)\nmodels. Important observations are obtained: 1) ViTs learn\nweaker biases on backgrounds and textures, while they are\nequipped with stronger inductive biases towards shapes and\nstructures, which is more consistent with human cognitive\ntraits. Therefore, ViTs generalize better than CNNs under\nDS. With the same or less amount of parameters, ViTs are\nahead of corresponding CNNs by more than 5% in top-1\naccuracy under most types of DS. 2) As the model scale\nincreases, ViTs strengthen these biases and thus gradually\nnarrow the in-distribution and OOD performance gap. To\nfurther improve the generalization of ViTs, we design the\nGeneralization-Enhanced ViTs (GE-ViTs) from the perspec-\ntives of adversarial learning, information theory, and self-\nsupervised learning. By comprehensively investigating these\nGE-ViTs and comparing with their corresponding CNN mod-\nels, we observe: 1) For the enhanced model, larger ViTs\nstill beneï¬t more for the OOD generalization. 2) GE-ViTs\nare more sensitive to the hyper-parameters than their cor-\nresponding CNN models. We design a smoother learning\nstrategy to achieve a stable training process and obtain per-\nformance improvements on OOD data by 4% from vanilla\nViTs. We hope our comprehensive study could shed light\non the design of more generalizable learning architectures.\nCodes are available here.\n*These authors contributed equally to this work.\n\fCorresponding author.\n1. Introduction\nRecently, transformer has made remarkable achievements\nin vision tasks, such as e.g. image classiï¬cation [7,8,30], ob-\nject detection [4, 39], and image processing [6]. Despite the\nencouraging performance achieved on standard benchmarks\nand several properties revealed in recent works [1, 5, 23, 24],\nthe generalization ability of Vision Transformers (ViTs) is\nstill less understood. While the traditional train-test scenario\nassumes the test data for model evaluation are independent\nidentically distributed (IID) with sampled training data, this\nassumption does not always hold in real-world scenarios.\nThus, out-of-distribution (OOD) generalization is a highly\ndesirable capability of machine learning models. Recent\nworks indicate current CNN architectures generalize poorly\non various distribution shifts (DS) [11, 13, 14], whereas the\ninvestigation on ViTs remains scarce. Therefore, in this\npaper, we mainly focus on delving deep into the OOD gener-\nalization of ViTs under DS.\nTo comprehensively study the OOD generalization ability\nof ViTs, we ï¬rst deï¬ne a categorization of commonly appear-\ning DS based on the modiï¬ed semantic concepts in images.\nGenerally, an image for classiï¬cation contains a foreground\nobject and background information. The foreground object\nconsists of hierarchical semantic concepts including pixel-\nlevel elements, object textures, and shapes, object parts, and\nobject itself [38]. A distribution shift usually causes variance\non one or more semantics and we thus present a taxonomy\nof DS into four conceptual groups: background shifts, cor-\nruption shifts, texture shifts, and style shifts.\nWith the taxonomy of DS, we investigate the OOD gen-\neralization of ViTs by comparison with CNNs in each case.\nWhile models are desired to generalize to arbitrary OOD\nscenarios, the no-free-lunch theorem for machine learn-\ning [3, 12, 35] demonstrates that there is no entirely general-\npurpose learning algorithm, and that any learning algorithm\nimplicitly or explicitly will generalize better on some dis-\ntributions and worse on others. Thus some set of induc-\ntive biases are demanded to acquire generalization. Hence,\narXiv:2106.07617v4  [cs.CV]  8 Mar 2022\nto achieve human-level generalization capability, machine\nlearning models are supposed to have inductive biases that\nare most relevant to the human prior in the world. There\nhave been many attempts to inject inductive biases into deep\nlearning models that humans may exploit for the cognition\noperating at the level of conscious processing, e.g. the con-\nvolution [17] and self-attention mechanism [32]. Therefore,\nwe examine whether transformers are equipped with induc-\ntive biases that are more related to human cognitive traits\nto better investigate the generalization properties of ViTs\nunder DS. Extensive evaluations reveal the following ob-\nservations on the OOD generalizations of ViTs: 1) ViTs\nlearn weaker biases on backgrounds and textures, while they\nare equipped with stronger inductive biases towards shapes\nand structures, which is more consistent with human cogni-\ntive traits. Therefore, ViTs generalize better than CNNs in\nmost cases. Speciï¬cally, ViT not only achieves better per-\nformance on OOD data but also has smaller generalization\ngaps between IID and OOD datasets. 2) As the model scale\nincreases, ViTs strengthen these biases and thus gradually\nnarrow the IID and OOD generalization gaps, especially in\nthe case of corruption shifts and background shifts. In other\nwords, larger ViTs are better at diminishing the effect of\nlocal changes. 3) ViTs trained with larger patch size deal\nwith texture shifts better, yet are inferior in other cases.\nAfter validating the superiority of ViTs in dealing with\nOOD data, we focus on further improving their general-\nization capacity. Speciï¬cally, we design Generalization-\nEnhanced ViTs (GE-ViTs) from the perspectives of adversar-\nial training [10], information theory [27] and self-supervised\nlearning [37]. Equipped with GE-ViTs, we achieve signif-\nicant performance boosts towards OOD data by 4% from\nvanilla ViTs. By performing an in-depth investigation on\ndifferent models, we draw the following conclusions: 1) For\nthe enhanced transformer models, larger ViTs still beneï¬t\nmore for the OOD generalization. 2) GE-ViTs are more\nsensitive to the hyper-parameters than their corresponding\nCNN models.\n2. Related Work\nVision Transformers. Recently, Transformers have been\napplied to various vision tasks including image classiï¬cation\n[7, 8, 30], object detection [4, 39], segmentation [34] and\nimage processing [6]. Among them, the Vision Transformer\n(ViT) [8] is the ï¬rst fully-transformer model applied for\nimage classiï¬cation and competitive with state-of-the-art\nCNNs. It heavily relies on large-scale datasets for model\npre-training, requiring huge computation resources. Later,\n[30] propose the Data-efï¬cient image Transformer (DeiT),\nwhich achieves competitive results against the state-of-the-\nart CNNs on ImageNet without external data by simply\nchanging training strategies from ViT. Due to its efï¬ciency,\nwe use this family of models to investigate generalizations\nTable 1. Illustration of our taxonomy of DS. We build the tax-\nonomy upon what kinds of semantic concepts are modiï¬ed from\nthe original image and divide the DS into four cases: background\nshifts, corruption shifts, texture shifts, and style shifts. âœ“ denotes\nthe unmodiï¬ed vision cues under certain type of DS.\nShift Type background foreground\npixel texture shape structure\nBackground Shift âœ“ âœ“ âœ“ âœ“\nCorruption Shift âœ“ âœ“ âœ“\nTexture Shift âœ“ âœ“\nStyle Shift âœ“\nof Vision Transformers in this paper.\nOut-of-distribution Generalization. Attracting much at-\ntention recently, various works have been proposed for OOD\ngeneralization under different settings. Most domain adapta-\ntion literatures aim at promoting the modelâ€™s performance\nunder distribution shift with access to the unlabeled target\ndata [10,20,29]. Another setting for OOD generalization con-\ncentrates on learning representations without access to target\ndata, commonly referred as domain generalization [9,18,33].\nIn addition, some recent works model OOD generalization\non their newly-built benchmarks [11, 13, 14]. Though recent\nworks [1, 5, 23, 24] have studied several properties of ViTs,\nthe generalization of ViTs is still under explored.\n3. Distribution Shifts and Evaluation Protocols\n3.1. Taxonomy of Distribution Shifts\nTo make an extensive study on OOD generalization, we\nbuild the taxonomy of DS upon what kinds of semantic con-\ncepts are modiï¬ed from the original image. Therefore, we\ndivide the DS into four cases: background shifts, corruption\nshifts, texture shifts and style shifts, as shown in Tab. 1. The\nelaborately divided DS permit us to investigate model biases\ntowards every visual cue respectively.\nâ€¢ Background Shifts. Image backgrounds are usually re-\ngarded as auxiliary cues in assigning images to correspond-\ning labels in the image classiï¬cation task. However, pre-\nvious works have demonstrated that backgrounds may\ndominate in prediction [2, 26], which is undesirable to us.\nWe focus on the modelâ€™s invariance towards background\nchange and thus deï¬ne the background shifts. ImageNet-\n9 [36] is adopted for background shifts.\nâ€¢ Corruption Shifts. The concept of corruption was pro-\nposed in [14], which stands for those naturally occurring\nvicinal impurities mixed in images. These corruptions ei-\nther come from environmental inï¬‚uence during the shoot-\ning stage or from the image processing stage. We deï¬ne\nthese cases as corruption shifts, which only impact on\nobject pixel-level elements while can still cause models\nobvious performance decrease. ImageNet-C [14] is used\nto examine generalization ability under corruption shifts.\nâ€¢ Texture Shifts. Generally, the texture gives us informa-\ntion about the spatial arrangement of the colors or inten-\nsities in an image, which is critical for the classiï¬ers in\nobtaining a correct prediction. Thus, a replacement of\nobject textures can inï¬‚uence model prediction. We deï¬ne\nthese variations as texture shifts. Cue Conï¬‚ict Stimuli and\nStylized-ImageNet [11] are used to investigate generaliza-\ntion under texture shifts.\nâ€¢ Style Shifts. Typically, style is a complicated concept\ndetermined by the characteristics that describe the artwork,\nsuch as the form, color, composition, etc. The variance of\nstyle often reï¬‚ects in multiple concept levels, including\ntexture, shape and object part, etc. ImageNet-R [13] and\nDomainNet [25] are used for the case of style shifts.\n3.2. Model Zoo\nâ€¢ Vision Transformer. We follow the implementation in\nDeiT [30] and choose a range of models with different\nscales for experiments. The ViT architecture takes as input\na grid of non-overlapping contiguous image patches of\nresolution N Ã—N. In this paper we typically use N = 16\n(â€œ/16â€) or N = 32 (â€œ/32â€). Besides the ofï¬cial DeiT\nmodels, we also utilize the data-efï¬cient training scheme\nto train ViT-L/16 and ViT-B/32 and rename them DeiT-\nL/16 and DeiT-B/32.\nâ€¢ Big Transfer. Big Transfer models [16] are build on\nResNet-V2 models. We select BiT-S-R50X1 based on\na ResNet-50 backbone. Besides the ofï¬cial implemen-\ntation, we also train a version using the identical data\naugmentation strategy from DeiTs for comparison. We\nrespectively name them BiT and BiTda.\n3.3. Evaluation Protocols\nIn image classiï¬cation tasks, a model generally consists\nof a feature encoder F and a classiï¬er C. Suppose the\nmodel is trained on a training set Dtrain = {(xi,yi)}Ntrain\ni=1 .\nWe respectively introduce a set of independent identically\ndistributed (IID) validation data Diid = {(xi,yi)}Niid\ni=1 and a\nset of out-of-distributed (OOD) data Dood = {(xi,yi)}Nood\ni=1\nin the same semantic space. Ntrain,Niid,Nood represent\nthe number of data in Dtrain,Diid,Dood respectively. Then\nwe use the following evaluations.\nâ€¢ Accuracy on OOD Data. A direct measurement is to\ncalculate the accuracy on the OOD dataset:\nAcc(F,C; Dood) = 1\n|Dood|\nâˆ‘\n(x,y)âˆˆDood\n1 (C(F(x)) =y),\n(1)\nwhere 1 is the indicator function.\nâ€¢ IID/OOD Generalization Gap. In this paper, we also\nfocus on how well a model could behave towards the\nOOD data compared with the IID data. Hence, we use the\nIID/OOD generalization gap to measure the performance\ndifference caused by the distribution shift:\nGap(F,C; Diid,Dood) =Acc(F,C; Diid)âˆ’Acc(F,C; Dood).\n(2)\n4. Generalization-Enhanced ViTs\nAfter investigating the OOD generalization properties of\nViTs, it is natural to ï¬gure out strategies to further improve\nthem. Thus we further design Generalization-Enhanced ViTs\n(GE-ViTs) from the perspectives of adversarial training [10],\ninformation theory [27] and self-supervised learning [37],\nnamed as T-ADV , T-MME, and T-SSL respectively. By\nmaking a full comparison of these three designs, we ï¬gure\nout the most suitable strategy for GE-ViTs.\n4.1. Adversarial Learning\nTo learn domain-invariant representations, we introduce\na domain discriminator [10] to promote the backbone to\nproduce domain-confused features by adversarial training.\nSpeciï¬cally, as shown in Fig 1 (a), the network consists\nof a shared feature encoder F, a label predictor C, and a\ndomain classiï¬er D. The feature encoder aims at minimizing\nthe domain confusion loss LADV for all samples and label\nprediction loss LCLS for labeled source samples while the\ndomain classiï¬er focus on maximizing the domain confusion\nloss LADV. The overall objectives are:\nLCLS =\nâˆ‘\n(x,y)âˆˆDs\nH(Ïƒ(C(F(x))),y), (3)\nLADV =\nâˆ‘\n(x,yd)âˆˆDs,Dt\nH(Ïƒ(D(F(x))),yd), (4)\n(Ë†Î¸F,Ë†Î¸C) = arg min\nÎ¸F ,Î¸C\nLCLS + Î»advLADV, (5)\nË†Î¸D = arg max\nÎ¸D\nLADV, (6)\nwhere y and yd denote the class label and binary domain\nlabel respectively. Ïƒ(Â·) stands for the Softmax function and\nH(Â·,Â·) returns the cross-entropy of two input distributions.\nÎ»adv is an adaptive coefï¬cient that gradually changed from\n0 to 1 by the schedule proposed in [10]. Furthermore, to\nfacilitate training, a gradient reversal layer (GRL) is applied\nto implement the opposite objective of two parts.\n4.2. Minimax Entropy\nWe leverage the minimax process on the conditional\nentropy of target data [27] to reduce the distribution gap\nwhile learning discriminative features for the task. As the\npipeline is shown in Fig. 1 (b), a cosine similarity-based\nclassiï¬er architecture C is exploited to produce class pro-\ntotypes. The cosine classiï¬er C consists of weight vectors\nğ“›ğ‚ğ‹ğ’\nğ“›ğ€ğƒğ•\nTarget Domain\nTransformer \nEncoder\nğ‘­\nClass\nToken\nLabel\nPredictor\nğ‘ª\nDomain\nClassifier\nğ‘«\nSource Domain\nğğ“›ğ‚ğ‹ğ’\nğğœ½ğ‘­\nğğ“›ğ‚ğ‹ğ’\nğğœ½ğ‘ª\nâˆ’ğ€ğ’‚ğ’…ğ’—\nğğ“›ğ€ğƒğ•\nğğœ½ğ‘­\nğğ“›ğ€ğƒğ•\nğğœ½ğ‘«\n(a) T-ADV\nSource Domain\nTransformer \nEncoder\nğ‘­\ngradient \nreversal \nlayer\nClass\nToken\nCosine\nClassifier\nğ‘ª\nğ“›ğ‚ğ‹ğ’\nğ“›ğ„Target Domain\nâˆ’ğ€ğ’†\nğğ“›ğ„\nğğœ½ğ‘ªğ€ğ’†\nğğ“›ğ„\nğğœ½ğ‘­\nğğ“›ğ‚ğ‹ğ’\nğğœ½ğ‘ª\nğğ“›ğ‚ğ‹ğ’\nğğœ½ğ‘­\nğ’ğŸ normalization (b) T-MME\nSource Domain\nTransformer \nEncoder\nğ‘­\nClass\nToken\nCosine\nClassifier\nğ‘ª\nTarget Domain\nSource\nMemory\nBank\nTarget\nMemory\nBank\nğ’Œ-means\nğ’Œ-means\nNormalized \nSource \nPrototypes\nNormalized \nTarget \nPrototypes\nğ“›ğˆğ’\nğ“›ğŒğˆğŒ\nğ“›ğ‚ğ‹ğ’\nupdate\nupdate\nğğ“›ğ‚ğ‹ğ’\nğğœ½ğ‘ª\nğğ“›ğ‚ğ‹ğ’\nğğœ½ğ‘­\nğ€ğ¢ğ¬\nğğ“›ğˆğ’\nğğœ½ğ‘­\nğ€ğ¦ğ¢ğ¦\nğğ“›ğŒğˆğŒ\nğğœ½ğ‘ª\nğ€ğ¦ğ¢ğ¦\nğğ“›ğŒğˆğŒ\nğğœ½ğ‘­\nğğ“›\nğğœ½\nğğ“›\nğğœ½\nğğ“›\nğğœ½\nSource data forward\nTarget data forward\nAll data forward\nSource data backward\nTarget data backward\nAll data backward\nğ’ğŸ normalization\n(c) T-SSL\nFigure 1. A framework overview of the three designed generalization-enhanced ViTs. All networks use a ViT F as feature encoder\nand a label prediction head C. Under this setting, the inputs to the models have labeled source examples and unlabeled target examples.\na) T-ADV promotes the network to learn domain-invariant representations by introducing a domain classiï¬er D for domain adversarial\ntraining. b) T-MME leverage the minimax process on the conditional entropy of target data to reduce the distribution gap while learning\ndiscriminative features for the task. The network uses a cosine similarity-based classiï¬er architecture C to produce class prototypes. c)\nT-SSL is an end-to-end prototype-based self-supervised learning framework. The architecture uses two memory banks V s and V t to\ncalculate cluster centroids. A cosine classiï¬er C is used for classiï¬cation in this framework.\nW = [w1,..., wnc], where nc denotes the total number of\nclasses, and a temperature T. Ctakes â„“2 normalized F(x)\nâˆ¥F(x)âˆ¥\nas an input and output 1\nT\nWTF(x)\nâˆ¥F(x)âˆ¥. The key idea is to mini-\nmize the distance between the class prototypes and neighbor-\ning unlabeled target samples, thus extracting discriminative\ntarget features. To overcome the dominant impact of labeled\nsource data on prototypes, prototypes are moved towards\nthe target by maximizing the entropy LE of unlabeled target\nexamples. Meanwhile, the feature extractor aims at mini-\nmizing the entropy of the unlabeled examples, to make them\nbetter clustered around the prototypes. Therefore, a minimax\nprocess is formulated between the weight vectors and the fea-\nture extractor. Additionally, the label prediction lossLCLS is\nalso utilized on source samples. The overall objectives are:\nLCLS =\nâˆ‘\n(x,y)âˆˆDs\nH(Ïƒ(C(F(x))),y), (7)\nLE =\nâˆ‘\nxâˆˆDt\nH(Ïƒ(C(F(x)))), (8)\nË†Î¸F = arg min\nÎ¸F\nLCLS + Î»eLE, (9)\nË†Î¸C = arg min\nÎ¸C\nLCLS âˆ’Î»eLE, (10)\nwhere H(Â·,Â·) returns the cross-entropy of two input distri-\nbutions and H(Â·) returns the entropy. Î»e is a coefï¬cient to\nbalance two loss terms.\n4.3. Self-Supervised Learning\nWe integrate an end-to-end prototypical self-supervised\nlearning framework [37] into ViT. As shown in Fig. 1 (c),\nthe framework also uses a cosine classiï¬er Cas introduced\nin Sec. 4.2. It ï¬rst encodes semantic structure of data into\nthe embedding space. ProtoNCE [19] is respectively applied\nin source and target domains. Speciï¬cally, two memory\nbanks Vs and Vt are maintained to store feature vectors\nof every sample from source and target. These vectors are\nupdated with momentum after each batch. k-means cluster-\ning is performed on memory banks to generate normalized\nprototypes {Âµs\nj}k\nj=1 and {Âµt\nj}k\nj=1. Then the similarity distri-\nbution vector between â„“2 normalized source feature vectors\nfs\ni = F(xs\ni )\nâˆ¥F(xs\ni )âˆ¥ from current batch and normalized source\nprototypes {Âµs\nj}k\nj=1 as Ps\ni = [Ps\ni,1,...,P s\ni,k] with Ps\ni,j =\nexp(Âµs\njÂ·fs\ni /Ï†)âˆ‘k\nr=1 exp(ÂµsrÂ·fs\ni /Ï†) , where Ï† is a temperature value. Then\nthe in-domain prototypical self-supervision loss is formed\nas: LIS = âˆ‘|Ds|\ni=1 H(Ps\ni ,cs(i))+âˆ‘|Dt|\ni=1 H(Pt\ni,ct(i)), where\ncs(Â·) and ct(Â·) return the cluster index of the sample, and\n|Â·| returns the cardinal of the set. H(Â·,Â·) returns the cross-\nentropy of two input distributions.\nIn addition, since a network is desired to have high-\nconï¬dent and diversiï¬ed predictions, an objective is set for\nmaximizing the mutual information between the input image\nand the network prediction. This objective is split into two\nterms: entropy maximization of expected network prediction\nand entropy minimization on the network output. Therefore,\nthe objective is formulated as: LMIM = Ex[H(p(y|x; Î¸)] âˆ’\n/uni00000032/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000032/uni00000051/uni0000004f/uni0000005c/uni00000010/uni00000029/uni0000002a/uni00000030/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000036/uni00000044/uni00000050/uni00000048/uni00000030/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000035/uni00000044/uni00000051/uni00000047/uni00000030/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000031/uni00000048/uni0000005b/uni00000057\n/uni00000025/uni0000002a/uni00000010/uni00000056/uni0000004b/uni0000004c/uni00000049/uni00000057/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018\n/uni0000001b/uni00000013\n/uni0000001b/uni00000018\n/uni0000001c/uni00000013\n/uni0000001c/uni00000018\n/uni00000014/uni00000013/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni0000002f/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000025/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000036/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000037/uni0000004c/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000025/uni00000012/uni00000016/uni00000015\n/uni00000025/uni0000004c/uni00000037\n/uni00000025/uni0000004c/uni00000037da\n(a)\n/uni00000032/uni00000051/uni0000004f/uni0000005c/uni00000010/uni00000029/uni0000002a/uni00000030/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000036/uni00000044/uni00000050/uni00000048/uni00000030/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000035/uni00000044/uni00000051/uni00000047/uni00000030/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000031/uni00000048/uni0000005b/uni00000057\n/uni00000025/uni0000002a/uni00000010/uni00000056/uni0000004b/uni0000004c/uni00000049/uni00000057/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048\n/uni00000013/uni00000011/uni00000013\n/uni00000015/uni00000011/uni00000018\n/uni00000018/uni00000011/uni00000013\n/uni0000001a/uni00000011/uni00000018\n/uni00000014/uni00000013/uni00000011/uni00000013\n/uni00000014/uni00000015/uni00000011/uni00000018\n/uni00000014/uni00000018/uni00000011/uni00000013\n/uni00000014/uni0000001a/uni00000011/uni00000018\n/uni00000015/uni00000013/uni00000011/uni00000013/uni0000002c/uni0000002c/uni00000027/uni00000012/uni00000032/uni00000032/uni00000027/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002a/uni00000044/uni00000053 (b)\n/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000003/uni00000046/uni0000004f/uni00000048/uni00000044/uni00000051/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000045/uni0000004f/uni00000058/uni00000055/uni0000005a/uni00000048/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000055/uni00000047/uni0000004c/uni0000004a/uni0000004c/uni00000057/uni00000044/uni0000004f\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000058/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n (c)\n/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000045/uni0000004f/uni00000058/uni00000055/uni0000005a/uni00000048/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000055/uni00000047/uni0000004c/uni0000004a/uni0000004c/uni00000057/uni00000044/uni0000004f\n/uni00000026/uni00000052/uni00000055/uni00000055/uni00000058/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni0000002c/uni0000002c/uni00000027/uni00000012/uni00000032/uni00000032/uni00000027/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002a/uni00000044/uni00000053 (d)\nFigure 2. Results on ImageNet-9 and ImageNet-C. (a)-(b) and (c)-(d) respectively illustrate the OOD Accuracy and IID/OOD Gener-\nalization Gap for different models on ImageNet-9 and ImageNet-C datasets. From (a) and (b), we conclude that 1) ViTs perform with\na weaker background-bias than CNNs, 2) a larger ViT extracts a more background-irrelevant representation. From (c) and (d), we draw\nthe conclusions that 1) ViTs deal with corruption shifts better than CNNs and generalize better along with model size scaling up, 2) ViTs\ndo beneï¬t from diverse augmentation in enhancing generalization towards vicinal impurities, but their architectural advantage cannot be\noverlooked as well, 3) patch size for training has little inï¬‚uence on ViTsâ€™ generalization ability.\nH(ExâˆˆDsâˆªDt[p(y|x; Î¸]). The last term of training objective\nis the supervision loss on source domain measured by cross-\nentropy: LCLS = âˆ‘\n(x,y)âˆˆDs H(Ïƒ(C(F(x))),y).\nFinally, the overall learning objective is formulated as:\n(Ë†Î¸F,Ë†Î¸C) = arg min\nÎ¸F ,Î¸C\nLCLS + Î»isLIS + Î»mimLMIM, (11)\nwhere Î»is and Î»mim denotes the coefï¬cients of correspond-\ning loss terms.\n5. Systematic Study on ViTs Generalization\nIn-Distribution Generalization. We ï¬rst examine the in-\ndistribution generalization of different models on the Ima-\ngeNet benchmark. As results are shown in Fig. 2 (c) col-\numn 1, we have the following observations. 1) With the\ndata-efï¬cient training scheme, DeiT models tend to perform\nbetter as scales increase from tiny to large, but the gain of\nscale growth gradually dwindles. 2) Having almost the same\nparameters and both trained without external data, DeiT-S/16\ncould beat BiT and BiTda.\n5.1. Background Shifts Generalization Analysis\nWe utilize ImageNet-9, a variety of foreground-\nbackground recombination plans, to investigate model bias\ntowards background signal. These datasets empower us to\ninvestigate to what extent model decisions rely on the back-\nground signal. The OOD accuracy and IID/OOD gap results\nof four varieties of background shifts are illustrated in Fig. 2\n(a) and (b) respectively.\n- ViTs perform with a weaker background-bias than\nCNNs. By calculating accuracy gaps between Mixed-Same\nwith class-relevant backgrounds and Mixed-Rand with neu-\ntral background signals, we can measure classiï¬ersâ€™ reliance\non the correct background. From Fig. 2 (a), the lower gaps\nachieved by ViTs indicate that ViTs depend less on corre-\nsponding background signals when the correct foreground\nis present. Likewise, it can be concluded that ViTs are less\nmisled by the conï¬‚ict background based on the accuracy\ngaps between Mixed-Same and Mixed-Next. In addition,\ncomparing two BiT models, BiTda outperforms normal BiT\nin OOD accuracy and achieves lower IID/OOD Gaps, in-\ndicating that diverse augmentation during training exerts\na salutary effect on model generalization on background-\nshifted data. Nonetheless, it is worth noticing that BiT da\nobtain a larger Same-Rand gap and Same-Next gap, which\ndemonstrates that the augmentation training scheme can-\nnot alleviate the modelâ€™s dependence on correct background\ninformation. Therefore, ViTs perform with a weaker back-\nground bias than CNNs, and such property is brought by\ntheir architectures.\n- A larger ViT extracts a more background-irrelevant\nrepresentation. Via comparing ViTs of different sizes, we\ncan observe that a larger ViT architecture contributes to\na better OOD performance as well as a smaller IID/OOD\ngap. Even DeiT-L/16 could further narrow the gap by about\n2% from DeiT-B/16, while they achieve almost the same\nin distribution accuracy results. Meanwhile, a larger ViT\nalso achieves a lower Same-Rand gap and Same-Next gap,\nshowing that there exists a positive correlation between the\nViT scale and their ability to exclude distraction provided by\nirrelevant or conï¬‚ict backgrounds. Hence, it is clear that a\nlarger ViT tends to focus more attention on the foreground\nand learn a more background-irrelevant representation.\n5.2. Corruption Shifts Generalization Analysis\nThe corruption results of 4 categories averaged over all\nsubclasses and all severities, are shown in Fig. 2 (c) and (d).\n- ViTs deal with corruption shifts better than CNNs and\ngeneralize better along with model size scaling up.There\nexist similar phenomena with the background shifts cases\nthat most ViTs lead the BiT models to a large extent under\nboth evaluations in all situations, and that a larger ViT archi-\ntecture achieves a better OOD performance and narrows the\nIID/OOD generalization gap.\n- ViTs beneï¬t from diverse augmentation in enhancing\ngeneralization towards vicinal impurities, but their ar-\n/uni0000002c/uni0000002c/uni00000027/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000032/uni00000032/uni00000027/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000002c/uni0000002c/uni00000027/uni00000012/uni00000032/uni00000032/uni00000027/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002a/uni00000044/uni00000053\n/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013/uni00000008\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni0000002f/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000025/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000036/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000037/uni0000004c/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000025/uni00000012/uni00000016/uni00000015\n/uni00000025/uni0000004c/uni00000037\n/uni00000025/uni0000004c/uni00000037da\n(a) Stylized-ImageNet\n/uni00000036/uni0000004b/uni00000044/uni00000053/uni00000048/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013/uni00000008 (b) Cue Conï¬‚ict Stimuli\n/uni0000002c/uni0000002c/uni00000027/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000032/uni00000032/uni00000027/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000002c/uni0000002c/uni00000027/uni00000012/uni00000032/uni00000032/uni00000027/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002a/uni00000044/uni00000053\n/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013/uni00000008 (c) ImageNet-R\nFigure 3. Results on Stylized-ImageNet, Cue Conï¬‚ict Stimuli and ImageNet-R.(a), (b) and (c) respectively illustrate the OOD Accuracy\nand IID/OOD Generalization Gap for different models on Stylized-ImageNet, Cue Conï¬‚ict Stimuli, and ImageNet-R data sets. From (a) and\n(b) we could draw the following conclusions that 1) ViTsâ€™ stronger bias towards shape enables them to generalize better under texture shifts\nand their shape biases have a positive correlation with their sizes, 3) ViTs with larger patch size exhibit a stronger bias towards the shape.\nFrom (c) we observe that most ViTs beat BiTs in OOD accuracy while having little difference in the IID/OOD generalization gap.\n/uni00000046/uni0000004f/uni00000053/uni00000053/uni00000051/uni00000057/uni00000055/uni00000048/uni0000004f/uni00000056/uni0000004e/uni00000057/uni00000024/uni00000059/uni0000004a/uni00000011/uni0000002a/uni00000044/uni00000053\nt\n/uni00000046/uni0000004f/uni00000053\n/uni00000053/uni00000051/uni00000057\n/uni00000055/uni00000048/uni0000004f\n/uni00000056/uni0000004e/uni00000057\ns\n/uni0000001b/uni00000014/uni00000011/uni00000016/uni0000001c/uni00000016/uni00000018/uni00000011/uni00000019/uni0000001a/uni00000018/uni0000001c/uni00000011/uni00000013/uni0000001a/uni00000017/uni00000018/uni00000011/uni00000019/uni0000001c/uni00000016/uni00000017/uni00000011/uni00000018/uni0000001b\n/uni00000017/uni00000013/uni00000011/uni00000016/uni0000001a/uni0000001a/uni0000001a/uni00000011/uni00000015/uni0000001a/uni00000018/uni00000018/uni00000011/uni00000016/uni00000016/uni00000016/uni00000016/uni00000011/uni00000015/uni0000001b/uni00000016/uni00000017/uni00000011/uni00000015/uni0000001b\n/uni00000018/uni00000017/uni00000011/uni00000019/uni00000017/uni00000017/uni0000001b/uni00000011/uni00000017/uni00000013/uni0000001b/uni00000019/uni00000011/uni0000001b/uni00000016/uni00000017/uni00000014/uni00000011/uni00000016/uni00000014/uni00000016/uni0000001b/uni00000011/uni0000001a/uni00000014\n/uni00000018/uni00000019/uni00000011/uni00000019/uni00000017/uni00000016/uni00000019/uni00000011/uni0000001a/uni00000018/uni00000018/uni00000015/uni00000011/uni00000014/uni00000015/uni0000001a/uni00000017/uni00000011/uni00000019/uni0000001c/uni00000015/uni00000019/uni00000011/uni00000014/uni0000001c\n/uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000008\n(a) DeiT-B/16\n/uni00000046/uni0000004f/uni00000053/uni00000053/uni00000051/uni00000057/uni00000055/uni00000048/uni0000004f/uni00000056/uni0000004e/uni00000057/uni00000024/uni00000059/uni0000004a/uni00000011/uni0000002a/uni00000044/uni00000053\nt\n/uni00000046/uni0000004f/uni00000053\n/uni00000053/uni00000051/uni00000057\n/uni00000055/uni00000048/uni0000004f\n/uni00000056/uni0000004e/uni00000057\ns\n/uni0000001b/uni00000013/uni00000011/uni00000015/uni00000014/uni00000016/uni00000016/uni00000011/uni0000001a/uni00000015/uni00000018/uni00000018/uni00000011/uni00000015/uni00000018/uni00000017/uni00000016/uni00000011/uni00000016/uni0000001c/uni00000016/uni00000019/uni00000011/uni00000013/uni0000001c\n/uni00000016/uni00000019/uni00000011/uni00000013/uni0000001c/uni0000001a/uni00000018/uni00000011/uni00000016/uni00000013/uni00000018/uni00000015/uni00000011/uni00000013/uni0000001b/uni00000016/uni00000014/uni00000011/uni00000014/uni00000013/uni00000016/uni00000018/uni00000011/uni00000018/uni00000017\n/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000017/uni00000018/uni00000011/uni0000001b/uni00000015/uni0000001b/uni00000017/uni00000011/uni0000001a/uni00000019/uni00000016/uni0000001c/uni00000011/uni00000015/uni0000001c/uni00000016/uni0000001c/uni00000011/uni00000018/uni00000015\n/uni00000018/uni00000015/uni00000011/uni00000014/uni00000018/uni00000016/uni00000018/uni00000011/uni00000015/uni00000017/uni00000017/uni0000001b/uni00000011/uni00000014/uni00000018/uni0000001a/uni00000014/uni00000011/uni0000001c/uni00000013/uni00000015/uni00000019/uni00000011/uni0000001a/uni00000015\n/uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000008\n (b) DeiT-S/16\n/uni00000046/uni0000004f/uni00000053/uni00000053/uni00000051/uni00000057/uni00000055/uni00000048/uni0000004f/uni00000056/uni0000004e/uni00000057/uni00000024/uni00000059/uni0000004a/uni00000011/uni0000002a/uni00000044/uni00000053\nt\n/uni00000046/uni0000004f/uni00000053\n/uni00000053/uni00000051/uni00000057\n/uni00000055/uni00000048/uni0000004f\n/uni00000056/uni0000004e/uni00000057\ns\n/uni0000001a/uni00000018/uni00000011/uni00000013/uni00000016/uni00000015/uni0000001b/uni00000011/uni00000017/uni0000001b/uni00000017/uni0000001c/uni00000011/uni0000001a/uni0000001b/uni00000016/uni00000019/uni00000011/uni00000019/uni00000014/uni00000016/uni00000019/uni00000011/uni0000001a/uni00000017\n/uni00000016/uni00000013/uni00000011/uni00000019/uni00000016/uni0000001a/uni00000013/uni00000011/uni00000018/uni0000001b/uni00000017/uni0000001c/uni00000011/uni00000015/uni0000001c/uni00000015/uni00000018/uni00000011/uni00000019/uni0000001a/uni00000016/uni00000018/uni00000011/uni00000016/uni0000001b\n/uni00000017/uni00000015/uni00000011/uni0000001b/uni00000016/uni00000017/uni00000014/uni00000011/uni00000013/uni00000019/uni0000001b/uni00000016/uni00000011/uni00000016/uni0000001a/uni00000016/uni00000014/uni00000011/uni00000017/uni00000016/uni00000017/uni00000017/uni00000011/uni0000001c/uni00000016\n/uni00000017/uni00000017/uni00000011/uni0000001c/uni00000018/uni00000015/uni0000001c/uni00000011/uni00000014/uni0000001c/uni00000017/uni00000017/uni00000011/uni00000013/uni00000015/uni00000019/uni0000001a/uni00000011/uni0000001c/uni00000019/uni00000015/uni0000001b/uni00000011/uni00000018/uni0000001a\n/uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000008\n (c) BiT\n/uni00000046/uni0000004f/uni00000053/uni00000053/uni00000051/uni00000057/uni00000055/uni00000048/uni0000004f/uni00000056/uni0000004e/uni00000057/uni00000024/uni00000059/uni0000004a/uni00000011/uni0000002a/uni00000044/uni00000053\nt\n/uni00000046/uni0000004f/uni00000053\n/uni00000053/uni00000051/uni00000057\n/uni00000055/uni00000048/uni0000004f\n/uni00000056/uni0000004e/uni00000057\ns\n/uni0000001a/uni00000018/uni00000011/uni0000001a/uni0000001c/uni00000015/uni0000001a/uni00000011/uni0000001b/uni00000018/uni00000017/uni0000001b/uni00000011/uni0000001a/uni00000016/uni00000016/uni0000001a/uni00000011/uni00000013/uni00000014/uni00000016/uni0000001a/uni00000011/uni0000001c/uni00000016\n/uni00000016/uni00000013/uni00000011/uni0000001a/uni00000015/uni0000001a/uni00000014/uni00000011/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000011/uni00000018/uni00000017/uni00000015/uni00000019/uni00000011/uni00000016/uni00000013/uni00000016/uni00000019/uni00000011/uni00000013/uni0000001c\n/uni00000017/uni00000015/uni00000011/uni00000014/uni0000001b/uni00000017/uni00000014/uni00000011/uni00000014/uni00000017/uni0000001b/uni00000015/uni00000011/uni00000017/uni00000019/uni00000016/uni00000015/uni00000011/uni00000019/uni00000017/uni00000017/uni00000016/uni00000011/uni0000001b/uni00000014\n/uni00000017/uni00000017/uni00000011/uni0000001a/uni00000016/uni00000015/uni0000001b/uni00000011/uni00000015/uni00000016/uni00000017/uni00000014/uni00000011/uni00000018/uni00000014/uni00000019/uni0000001c/uni00000011/uni00000014/uni0000001c/uni00000016/uni00000014/uni00000011/uni00000013/uni00000016\n/uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000008\n (d) BiTda\nFigure 4. Results on DomainNet. From the results, we can conclude that 1) DeiT-S/16 performs better on the small-scale datasets in IID\nconditions. Thus, the model easily outperforms BiTs in OOD accuracy, 2) when inspecting the IID/OOD generalization gap, the results\ndiffer a lot. When models are trained on clipart and painting, there is no obvious difference of gap between DeiT-S/16 and BiTs.\nchitectural advantage cannot be overlooked. Compared\nwith BiT, BiTdaconstantly achieves about 4% better in OOD\nperformances and IID/OOD gaps, emphasizing the contribu-\ntion of diverse augmentation to model insensitivity towards\npixel-level shifts. However, most ViT models are still ahead\nof BiTda under both evaluations, manifesting ViTsâ€™ perfor-\nmance can be partially attributed to the architecture design.\n- Patch size for training has little inï¬‚uence on ViTsâ€™ gen-\neralization ability. Though DeiT-B/16 achieves higher\nOOD accuracy than DeiT-B/32, its counterpart trained with\na larger patch size 32 Ã—32, there is little difference be-\ntween their IID/OOD gaps. Therefore, patch size for train-\ning exert a peripheral effect on generalization ability from\nin-distribution data to out-of-distribution data, but only act\non the model in-distribution generalization.\n5.3. Texture Shifts Generalization Analysis\nThe results on Stylized-ImageNet and Cue Conï¬‚ict Stim-\nuli are shown in Fig. 3 (a) and (b).\n- ViTsâ€™ stronger bias towards shape enables them to gen-\neralize better under texture shifts and their shape biases\nhave a positive correlation with their sizes. It could be\nobserved from results on Stylized-ImageNet that ViTs lead\nBiT models under both evaluations and a larger ViT archi-\ntecture achieves a better OOD performance, which indicates\nthat ViTs deal with the texture shifts better and a larger\nViT contributes to better leveraging global semantic features\n(such as shape and object parts) and less affected by local\nchanges. These phenomena reappear in results on Cue Con-\nï¬‚ict Stimuli that most ViTs achieve higher shape accuracy\nand lower texture accuracy than BiTs, which demonstrates\nthat ViTsâ€™ insensitivities towards texture shifts are owed to\ntheir stronger bias on shape than CNNs. Meanwhile, there\nexists an uptrend of shape accuracy and a downtrend of tex-\nture accuracy as the ViT size increases. Hence, ViTsâ€™ shape\nbiases have a positive correlation with their sizes.\n- ViTs with larger patch size exhibit a stronger bias to-\nwards the shape. On Stylized-ImageNet, DeiT-B/32 be-\nhaves better than DeiT-B/16 in OOD accuracy and IID/OOD\ngeneralization gap, which is opposite to their performances\non ImageNet. Meanwhile, on Cue Conï¬‚ict Stimuli, DeiT-\nB/32 is less affected by the misleading texture than DeiT-\nB/16, resulting in a higher shape accuracy. Therefore, ViTs\nwith larger patch size rely less on local texture features and\nfocus more on global high-level features, i.e. they show\nstronger bias towards shape lower bias towards texture.\n5.4. Style Shifts Generalization Analysis\n- ViTs have diverse performance on IID/OOD general-\nization gap under Style shifts. The results on ImageNet-R\nare shown in Fig. 3 (c). As ImageNet-R only contains 200\nclasses of ImageNet, we follow [13] to record accuracy on\nthe ImageNet subset (ImageNet-200) and regard it as the\nIID result. When focusing on the accuracy on ImageNet-R,\nwe observe most ViTs beat BiTs in OOD accuracy, while\nhaving similar performance in IID/OOD generalization gap.\nAccordingly, ViTs do not have competitive edges in gen-\neralizing from real images to art renditions. For Domain-\noriginalBiT\nreal painting sketch quickdraw\nDeiT-S\n(a)\n/uni00000055/uni00000048/uni00000044/uni0000004f/uni00000053/uni00000044/uni0000004c/uni00000051/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000054/uni00000058/uni0000004c/uni00000046/uni0000004e/uni00000047/uni00000055/uni00000044/uni0000005a\n/uni00000027/uni00000052/uni00000050/uni00000044/uni0000004c/uni00000051\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000025/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000037/uni00000010/uni00000036/uni00000012/uni00000014/uni00000019\n/uni00000025/uni0000004c/uni00000037\n/uni00000025/uni0000004c/uni00000037da\n(b)\nFigure 5. Structure bias investigation. (a) illustrates examples of\nclass parachute of four domains and the Grad-CAM [28] attention\nmaps of both BiT and DeiT-S. We shall observe that, as the color,\ntexture, and shape cues become less and less informative from real\nto quickdraw and even there is only abstract structure preserved in\nquickdraw, DeiT-S constantly concentrates on the key structural\ninformation of parachutes while BiT fails to capture such essential\nfeature. (b) shows the accuracies of models trained with real on\ndifferent domains. From the results, we can see that the gap between\nViTs and CNNs is getting larger when the tested domain contains\nfewer visual cues (i.e. from real to quickdraw). Therefore, we can\nconclude that ViTs are less affected by the shift of color, texture,\nand shape features, indicating that ViTs focus more on structures.\nNet, we mainly compare the models with the same scale,\ni.e. DeiT-S/16 and BiTs, whose results are shown in Fig. 4.\nWe observe DeiT-S/16 performs better on the small-scale\ndatasets under IID and thus the model easily outperforms\nBiTs in OOD accuracy. When inspecting the IID/OOD gen-\neralization gap, the results differ a lot. When models are\ntrained on clipart and painting, there is no obvious differ-\nence between DeiT-S/16 and BiTs. But for real, DeiT-S/16\nleads BiTs over 4%, which can be explained as ViTs utilize\nthe knowledge from pre-train data better if the pre-train data\nand downstream data are from similar distributions.\n- ViTs shows stronger bias towards object structure. We\nfurther investigate how models shall behave as the other\navailable visual cues come to degrade until there only re-\nmains structural information. We illustrate examples of class\nparachute of four domains and the Grad-CAM [28] attention\nmaps of both BiT and DeiT-S in Fig. 5 (a). We shall ob-\nserve that, as the color, texture, and shape cues become less\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40\n60\nreal painting\n(a) rel vs. pnt, L8\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40\n60\nreal sketch (b) rel vs. skt, L8\n40\n 20\n 0 20 40 60\n60\n40\n20\n0\n20\n40\n60\n80\nreal quickdraw (c) rel vs. qcd, L8\n60\n 40\n 20\n 0 20 40\n60\n40\n20\n0\n20\n40\nreal painting\n(d) rel vs. pnt, L12\n60\n 40\n 20\n 0 20 40\n60\n40\n20\n0\n20\n40\nreal sketch (e) rel vs. skt, L12\n60\n 40\n 20\n 0 20 40\n60\n40\n20\n0\n20\n40\n60\nreal quickdraw (f) rel vs. qcd, L12\nFigure 6. T-SNE visualization results. (a)-(c) and (d)-(e) repec-\ntively illustrates the comparison of visualizing Class Token data of\nreal vs. painting, real vs. sketch and real vs. quickdraw of layer 8\nand layer 12 from four domains. Please zoom for better view.\nand less informative from real to quickdraw and even there\nis only abstract structure preserved in quickdraw, DeiT-S\nconstantly concentrates on the key structural information of\nparachutes while BiT fails to capture such essential feature.\nIn addition, we test the accuracies of models trained with\nreal on different domains. Since there are a considerable\nnumber of unrecognizable data in quickdraw, we exclude\nclasses on which both ViTs and CNNs achieve accuracies\nless than 10%. We show the results in Fig. 5 (b), from which\nwe can see that the gap between ViTs and CNNs are get-\nting larger when the tested domain contain less visual cues\n(i.e. from real to quickdraw). Based on observations and\nanalyses above, we can conclude that ViTs are less effected\nby the shift of color, texture, and shape features, indicating\nthat ViTs focus more on structures.\n- ViTs will eliminate different levels of DS in different\nlayers. We select a set of classes of four domains in Do-\nmainNet shown in Fig. 5 (a) and the class lists are shown\nin the supplementary material. By extracting the intermedi-\nate Class Token and implementing dimensionality reduction\nvia T-SNE technique [31], we generate the visualizations of\nClass Token data of layer 8 and layer 12 from four domains\nand respectively show the comparison of real vs. painting,\nreal vs. sketch and real vs. quickdraw. As shown in Fig. 6,\nwe can ï¬rst observe from pictures in the ï¬rst row that data\nfrom different domains are clustered together to a certain\nextent only in the real vs. painting condition at layer 8. As\nfor real vs. sketch, the data become well clustered until at\nlayer 12 (Fig. 6 (e)), whereas the real vs. quickdraw con-\ndition fails to mix up data from different domains together\nbut there exists the decision boundary that can well divide\ndata of different classes for both domains at layer 12 (Fig. 6\n(f)). From the above analysis, we conclude that ViTs will\neliminate different levels of DS in different layers.\n/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000037/uni00000055/uni00000044/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000039/uni00000044/uni0000004f/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003adv/uni0000001d/uni000000030 0.1\n/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003adv/uni0000001d/uni000000030 0.1\n/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003adv/uni0000001d/uni000000030 1\n(a) T-ADV\n/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000037/uni00000055/uni00000044/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000039/uni00000044/uni0000004f/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003e/uni0000001d/uni000000030 0.1\n/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003e/uni0000001d/uni000000030 0.1\n/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003e/uni0000001d/uni000000031\n (b) T-MME\n/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000037/uni00000055/uni00000044/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000039/uni00000044/uni0000004f/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003IS/uni0000001d/uni000000030 0.1\n/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003IS/uni0000001d/uni000000030 0.1\n/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000024/uni0000000f/uni00000003IS/uni0000001d/uni000000031\n (c) T-SSL\nFigure 7. Investigation of Generalization-enhanced methods with different training strategies. (a)-(c) show training curves on both\nsource domain and target domain. From the results, we can conclude that classical training strategies (the green lines) on CNNs are not\nsuitable for ViTs, which need smoother strategies (the red lines) to align features in both domains.\nTable 2. Results of Generalization-enhanced methods. Speciï¬-\ncally, we compare three types of GE-ViTs with their corresponding\nCNNs. From the results we could conclude that 1) equipped with\nGE-ViTs, we achieve signiï¬cant performance boosts towards out-\nof-distribution data by 4% from vanilla ViTs. 2) three GE-ViTs\nhave almost the same improvement from vanilla models on OOD\naccuracy. 3) for the enhanced transformer models, larger ViTs still\nbeneï¬t more for the out-of-distribution generalization.\nModel Method R to C R to P P to C C to S S to P R to S P to R Avg.\nDeiT-B/16\n- 54.64 48.40 40.37 45.69 36.75 41.31 55.33 46.07T-ADV 58.19 50.85 41.91 51.18 46.12 47.47 55.65 50.20T-MME60.59 51.9842.30 50.32 45.7947.9254.87 50.54T-SSL 56.80 49.0645.96 51.79 46.9545.9560.98 51.07\nDeiT-S/16\n- 50.60 45.82 36.09 43.39 35.24 39.29 52.08 43.22T-ADV 53.60 47.84 37.99 47.10 41.61 41.94 52.82 46.13T-MME56.86 49.1538.97 46.48 42.9542.0752.49 47.00T-SSL 53.86 46.7142.79 47.25 43.0140.9457.07 47.37\nBiT\n- 42.18 41.14 30.72 37.01 28.23 32.64 48.54 36.78DANN [10] 45.20 42.86 32.96 40.44 36.63 35.26 49.25 40.37MME [27] 50.2144.6134.75 40.27 38.41 37.83 47.58 41.95SSL [37]52.5542.8039.03 45.72 39.08 39.65 56.07 44.98\nVGG-16\n- 39.39 37.32 26.36 32.96 25.55 27.79 45.70 33.58DANN [10] 43.26 40.09 28.68 36.22 31.6335.4544.73 37.15MME [27] 42.6542.4627.4136.9333.94 32.5845.8737.41SSL [37]43.7941.8832.1935.7336.9931.05 55.1839.54\n6. Studies on Generalization-Enhanced ViTs\nSettings. We use DomainNet [25] for the following exper-\niments. Following [27], we focus on the 7 scenarios listed\nin Tab. 2. To make a full comparison, we implement these\nenhancing techniques on two representative CNNs VGG-16\nand BiT, and two ViTs including DeiT-S/16 and DeiT-B/16.\nWe explore their performance on both the vanilla version\nand the generalization-enhanced version. Implementation\ndetails can be found in supplementary materials.\nPerformance Analysis. The results of three GE-ViTs com-\nparing with CNNs are shown in Tab. 2. From the results\nwe have the following observations: 1) equipped with GE-\nViTs, we achieve signiï¬cant performance boosts towards\nout-of-distribution data by 4% from vanilla ViTs. 2) Three\nGE-ViTs have almost the same improvement from vanilla\nmodels on OOD accuracy. In contrast, CNNs beneï¬t more\nfrom the self-supervised learning method than the others. 3)\nDeiT-B/16 has a larger gain on those enhancing methods\nthan DeiT-S/16. Therefore, we conclude that 1) ViTs and\nCNNs share many characteristics, and both can be beneï¬cial\nfrom the generalization-enhancement methods. 2) For the\nenhanced transformer models, larger ViTs still beneï¬t more\nfor the out-of-distribution generalization.\nSmooth Feature Alignment. Fig. 7 shows the performance\nof GE-ViTs with different training strategies. The green line\nrepresents the same training strategies used in CNNs. The\nother two lines use smoother strategies. From the compari-\nson of these strategies, we observe that 1) the generally used\nautomated augmentation schemes shall cause performance\ndegradation on T-ADV while they have little inï¬‚uence on\nT-MME and T-SSL.2) smoother learning strategies are sig-\nniï¬cant for ViT convergence, especially in the adversarial\ntraining mode. As for T-MME and T-SSL, smoothness of\nauxiliary losses also signiï¬cantly improves the performance.\nBased on these observations, we conclude that GE-ViTs are\nmore sensitive to the hyper-parameters than their correspond-\ning CNN models.\n7. Discussion and Conclusion\nWe provide a comprehensive study on the OOD gener-\nalization of ViTs, with the following contributions: 1) We\ndeï¬ne a taxonomy on data distribution shifts according to\nthe modiï¬ed semantic concepts in images. 2) We perform a\ncomprehensive study on OOD generalization and inductive\nbias properties of ViTs under the ï¬ve categorized distribu-\ntion shifts. Several valuable observations are obtained. 3)\nWe further improve the OOD generalization of ViTs by de-\nsigning GE-ViTs through adversarial learning, information\ntheory, and self-supervised learning with smoother training\nstrategies. Our work serves as an early attempt, thus there is\nplenty of room for developing more powerful GE-ViTs.\nBroader Impacts. Some models utilized in this paper de-\nmand a large number of computing resources for training\nprocedures. The consumption of electricity may exert an\nenvironmental impact.\nAcknowledgements\nThis work is supported by NTU NAP, and under the\nRIE2020 Industry Alignment Fund â€“ Industry Collabora-\ntion Projects (IAF-ICP) Funding Initiative, as well as cash\nand in-kind contribution from the industry partner(s).\nReferences\n[1] Yutong Bai, Jieru Mei, Alan Yuille, and Cihang Xie. Are\ntransformers more robust than cnns? arXiv preprint\narXiv:2111.05464, 2021. 1, 2\n[2] Andrei Barbu, David Mayo, Julian Alverio, William Luo,\nChristopher Wang, Dan Gutfreund, Josh Tenenbaum, and\nBoris Katz. Objectnet: A large-scale bias-controlled dataset\nfor pushing the limits of object recognition models. Ad-\nvances in neural information processing systems, 32:9453â€“\n9463, 2019. 2\n[3] Jonathan Baxter. A model of inductive bias learning. Journal\nof artiï¬cial intelligence research, 12:149â€“198, 2000. 1\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Computer Vision\n- ECCV 2020 - 16th European Conference, Glasgow, UK,\nAugust 23-28, 2020, Proceedings, Part I, volume 12346 of\nLecture Notes in Computer Science, pages 213â€“229. Springer,\n2020. 1, 2\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, HervÂ´e JÂ´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. arXiv\npreprint arXiv:2104.14294, 2021. 1, 2\n[6] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020. 1, 2\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo\nJun, David Luan, and Ilya Sutskever. Generative pretraining\nfrom pixels. In International Conference on Machine Learn-\ning, pages 1691â€“1703. PMLR, 2020. 1, 2\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 11\n[9] Qi Dou, Daniel C Castro, Konstantinos Kamnitsas, and Ben\nGlocker. Domain generalization via model-agnostic learning\nof semantic features. arXiv preprint arXiv:1910.13580, 2019.\n2\n[10] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain\nadaptation by backpropagation. In International conference\non machine learning, pages 1180â€“1189. PMLR, 2015. 2, 3,\n8, 12, 14, 15\n[11] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,\nMatthias Bethge, Felix A. Wichmann, and Wieland Bren-\ndel. Imagenet-trained CNNs are biased towards texture; in-\ncreasing shape bias improves accuracy and robustness. In\nInternational Conference on Learning Representations, 2019.\n1, 2, 3, 11\n[12] Anirudh Goyal and Yoshua Bengio. Inductive biases for\ndeep learning of higher-level cognition. arXiv preprint\narXiv:2011.15091, 2020. 1\n[13] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, et al. The many faces of robust-\nness: A critical analysis of out-of-distribution generalization.\narXiv preprint arXiv:2006.16241, 2020. 1, 2, 3, 6, 11\n[14] Dan Hendrycks and Thomas Dietterich. Benchmarking neural\nnetwork robustness to common corruptions and perturbations.\nIn International Conference on Learning Representations ,\n2019. 1, 2, 11\n[15] Xun Huang and Serge Belongie. Arbitrary style transfer\nin real-time with adaptive instance normalization. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 1501â€“1510, 2017. 11\n[16] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\nIn Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-\nMichael Frahm, editors, Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020,\nProceedings, Part V, volume 12350 of Lecture Notes in Com-\nputer Science, pages 491â€“507. Springer, 2020. 3, 11\n[17] Yann LeCun, Yoshua Bengio, et al. Convolutional networks\nfor images, speech, and time series. The handbook of brain\ntheory and neural networks, 3361(10):1995, 1995. 2\n[18] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot.\nDomain generalization with adversarial feature learning. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 5400â€“5409, 2018. 2\n[19] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and\nSteven CH Hoi. Prototypical contrastive learning of unsu-\npervised representations. arXiv preprint arXiv:2005.04966,\n2020. 4\n[20] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-\ndan. Learning transferable features with deep adaptation\nnetworks. In International conference on machine learning,\npages 97â€“105. PMLR, 2015. 2\n[21] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. arXiv preprint arXiv:1608.03983,\n2016. 11\n[22] Ilya Loshchilov and Frank Hutter. Fixing weight decay regu-\nlarization in adam. 2018. 11\n[23] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan,\nMunawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. Intriguing properties of vision transformers. arXiv\npreprint arXiv:2105.10497, 2021. 1, 2\n[24] Sayak Paul and Pin-Yu Chen. Vision transformers are robust\nlearners. arXiv preprint arXiv:2105.07581, 2021. 1, 2\n[25] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1406â€“1415,\n2019. 3, 8, 11\n[26] Amir Rosenfeld, Richard Zemel, and John K Tsotsos. The\nelephant in the room. arXiv preprint arXiv:1808.03305, 2018.\n2\n[27] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell,\nand Kate Saenko. Semi-supervised domain adaptation via\nminimax entropy. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 8050â€“8058,\n2019. 2, 3, 8, 11, 14, 15\n[28] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-\ncam: Visual explanations from deep networks via gradient-\nbased localization. In Proceedings of the IEEE international\nconference on computer vision, pages 618â€“626, 2017. 7\n[29] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros.\nUnsupervised domain adaptation through self-supervision.\narXiv preprint arXiv:1909.11825, 2019. 2\n[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and HervÂ´e JÂ´egou. Training\ndata-efï¬cient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877 , 2020. 1, 2, 3,\n11\n[31] Laurens Van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of machine learning research, 9(11),\n2008. 7\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Isabelle Guyon,\nUlrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob\nFergus, S. V . N. Vishwanathan, and Roman Garnett, editors,\nAdvances in Neural Information Processing Systems 30: An-\nnual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA , pages\n5998â€“6008, 2017. 2\n[33] Riccardo V olpi, Hongseok Namkoong, Ozan Sener, John\nDuchi, Vittorio Murino, and Silvio Savarese. Generalizing\nto unseen domains via adversarial data augmentation. In\nProceedings of the 32nd International Conference on Neural\nInformation Processing Systems, pages 5339â€“5349, 2018. 2\n[34] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers.arXiv preprint\narXiv:2011.14503, 2020. 2\n[35] David H Wolpert, William G Macready, et al. No free lunch\ntheorems for search. Technical report, Technical Report SFI-\nTR-95-02-010, Santa Fe Institute, 1995. 1\n[36] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander\nMadry. Noise or signal: The role of image backgrounds in\nobject recognition. arXiv preprint arXiv:2006.09994, 2020.\n2, 11\n[37] Xiangyu Yue, Zangwei Zheng, Shanghang Zhang, Yang Gao,\nTrevor Darrell, Kurt Keutzer, and Alberto Sangiovanni Vin-\ncentelli. Prototypical cross-domain self-supervised learning\nfor few-shot unsupervised domain adaptation. arXiv preprint\narXiv:2103.16765, 2021. 2, 3, 4, 8, 13, 14, 15\n[38] Matthew D Zeiler and Rob Fergus. Visualizing and under-\nstanding convolutional networks. In European conference on\ncomputer vision, pages 818â€“833. Springer, 2014. 1\n[39] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable {detr}: Deformable transformers\nfor end-to-end object detection. In International Conference\non Learning Representations, 2021. 1, 2\nA. Brief Outline of Our Work\nTo help reviewers better understand our work, we make an\noutline of this paper in Fig. 8. In brief, we have investigated\ngeneralization of ViTs under distribution shifts and designed\ngeneralization enhanced ViTs.\nB. Experimental Setup\nB.1. Dataset Zoo\nâ€¢ ImageNet-9 [36] is adopted for background shifts.\nImageNet-9 is a variety of 9-class datasets with differ-\nent foreground-background recombination plans, which\nhelps disentangle the impacts of foreground and back-\nground signals on classiï¬cation. In our case, we use\nthe four varieties of generated background with fore-\nground unchanged, including â€™Only-FGâ€™, â€™Mixed-Sameâ€™,\nâ€™Mixed-Randâ€™ and â€™Mixed-Nextâ€™. The â€™Originalâ€™ data\nset is used to represent in-distribution data.\nâ€¢ ImageNet-C [14] is used to examine generalization abil-\nity under corruption shifts. ImageNet-C includes 15\ntypes of algorithmically generated corruptions, grouped\ninto 4 categories: â€˜noiseâ€™, â€˜blurâ€™, â€˜weatherâ€™, and â€˜digitalâ€™.\nEach corruption type has ï¬ve levels of severity, resulting\nin 75 distinct corruptions.\nâ€¢ Cue Conï¬‚ict Stimuli and Stylized-ImageNet are used\nto investigate generalization under texture shifts. Utiliz-\ning style transfer, [11] generated Cue Conï¬‚ict Stimuli\nbenchmark with conï¬‚icting shape and texture informa-\ntion, that is, the image texture is replaced by another\nclass with other object semantics preserved. In this case,\nwe respectively report the shape and texture accuracy of\nclassiï¬ers for analysis. Meanwhile, Stylized-ImageNet\nis also produced in [11] by replacing textures with the\nstyle of randomly selected paintings through AdaIN style\ntransfer [15].\nâ€¢ ImageNet-R [13] and DomainNet [25] are used for the\ncase of style shifts. ImageNet-R [13] contains 30000\nimages with various artistic renditions of 200 classes of\nthe original ImageNet validation data set. The renditions\nin ImageNet-R are real-world, naturally occurring vari-\nations, such as paintings or embroidery, with textures\nand local image statistics which differ from those of Im-\nageNet images. DomainNet [25] is a recent benchmark\ndataset for large-scale domain adaptation that consists\nof 345 classes and 6 domains. As labels of some do-\nmains are very noisy, we follow the 7 distribution shift\nscenarios in [27] with 4 domains (Real, Clipart, Painting,\nSketch) picked.\nTable 3. Basic description of used model architectures.\nModel patch size embedding dimension #heads #layers #params\nDeiT-Ti/16 16Ã—16 192 3 12 5M\nDeiT-S/16 16Ã—16 384 6 12 22M\nDeiT-B/16 16Ã—16 768 12 12 86M\nDeiT-B/32 32Ã—32 768 12 12 86M\nDeiT-L/16 16Ã—16 1024 16 24 307M\nBiT-S-R50X1 - - - - 23M\nB.2. Implementation Details\nB.2.1 Vanilla Model Implementation\nâ€¢ DeiT. For Vision Transformers, we pre-train the DeiT\nmodels on the ImageNet dataset with the AdamW op-\ntimizer [22], a batch size of 1024 and the resolution\nof 224 Ã—224. The learning rate is linearly ramped up\nduring the ï¬rst 5 epochs to its base value determined\nwith the following linear scaling rule: lr= 0.0005*batch\nsize/512. After the warmup, we decay the learning rate\nwith a cosine schedule [21] with the weight decay =\n0.05 and train for 300 epochs. We follow the data aug-\nmentations scheme in the ofï¬cial paper [30]. For the\ndownstream ï¬ne-tuning, we scale up the resolution to\n384 Ã—384 by adopting perform 2D interpolation of the\npre-trained position embeddings proposed in [8]. We\ntrain models with learning rate lr= 5e-6, weight decay\n= 1e-8 for 75000 iters and the other settings identical to\npre-training stage.\nâ€¢ BiT. BiT models use the augmentation schemes men-\ntioned in [16]. We train them upstream using SGD\nwith momentum. We use an initial learning rate of 0.03,\nweight decay 0.0001, and momentum 0.9. We train for\n90 epochs and decay the learning rate by a factor of 10\nat 30, 60, and 80 epochs. We use a global batch size of\n4096 and multiply the learning rate by batch size/256\nwith the resolution 224 Ã—224. For downstream ï¬ne-\ntuning, we use SGD with an initial learning rate of 0.003,\nmomentum 0.9, and batch size 512 with the resolution\n384 Ã—384.\nâ€¢ BiTda. BiTda models use identical data augmentation\nstrategy from DeiTs. The other training setups are con-\nsistent with BiT models.\nThe basic description of used model architectures is\nshown in Tab. 3.\nB.2.2 Generalization-Enhanced Model Implementa-\ntion\nâ€¢ T-ADV .T-ADV consists of a feature encoder, a label\npredictor, and a domain classiï¬er. The feature encoder is\nimplemented by the DeiT backbone and the label predic-\ntor is a linear layer projecting CLS token to logit. The\nViTs vs. CNNs\nStyle Shifts Corruption Shifts\nTexture Shifts\nIn Distribution\nBackground Shifts\nStyle Shifts Corruption Shifts\nTexture Shifts\nIn Distribution\nBackground Shifts\nSource Domain\nTransformer \nEncoder\nClass\nToken\nCosine\nClassifier\nTarget Domain\nSource\nMemory\nBank\nTarget\nMemory\nBank\n-means\n-means\nNormalized \nSource \nPrototypes\nNormalized \nTarget \nPrototypes\nupdate\nupdate\nSource data forward\nTarget data forward\nAll data forward\nSource data backward\nTarget data backward\nAll data backward\nnormalization\nSource Domain\nTransformer \nEncoder\nClass\nToken\nCosine\nClassifier\nTarget Domain\nSource\nMemory\nBank\nTarget\nMemory\nBank\n-means\n-means\nNormalized \nSource \nPrototypes\nNormalized \nTarget \nPrototypes\nupdate\nupdate\nSource data forward\nTarget data forward\nAll data forward\nSource data backward\nTarget data backward\nAll data backward\nnormalization\nSource Domain\nTransformer \nEncoder\ngradient \nreversal \nlayer\nClass\nToken\nCosine\nClassifier\nTarget Domain\nnormalization\nSource Domain\nTransformer \nEncoder\ngradient \nreversal \nlayer\nClass\nToken\nCosine\nClassifier\nTarget Domain\nnormalization\nTarget Domain\nTransformer \nEncoder\nClass\nToken\nLabel\nPredictor\nDomain\nClassifier\nSource Domain\nTarget Domain\nTransformer \nEncoder\nClass\nToken\nLabel\nPredictor\nDomain\nClassifier\nSource Domain\nâ€¢ Accuracy on OOD Data.\nâ€¢ IID/OOD Generalization Gap.\nâ€¢ Background Bias\nâ€¢ Texture Bias\nâ€¢ Shape Bias\nâ€¢ Structure Bias\nInvestigating Generalization \nunder Distribution Shifts\nDesigning Generalization-\nEnhanced ViTs\nDesigning Generalization-\nEnhanced ViTs\nT-ADV T-MME T-SSLT-ADVT-ADV\nTaxonomy of \nDistribution Shifts Systematic AnalysisSystematic Analysis\nOOD generalization \nAnalysis Bias Investigation\nFigure 8. A brief outline of our work.\ndomain classiï¬er is a three-layer MLP with hidden di-\nmension 1024, aiming at predicting domain labels. We\nimplement T-ADV on downstream tasks using ImageNet\npre-trained DeiT backbones. We train models with learn-\ning rate lr = 5e-5 and weight decay = 1e-7 for 10000\niters except that the domain classiï¬er use a learning rate\nlr= 2.5e-4. Furthermore, we make some adjustments to\nthe training scheme due to the special architectures of Vi-\nsion Transformers. Based on our practice, we restrict the\nmagnitude of gradient reverse coefï¬cientÎ»adv by further\nmultiplying 0.1 from the original setting, to avoid great\nï¬‚uctuation during training. These adjustments contribute\nto a stable adversarial training process.\nâ€¢ T-MME. T-MME consists of a feature encoder and a\ncosine similarity-based classiï¬er. The classiï¬er is im-\nplemented by a three-layer MLP with a hidden size of\n1024. The learning rate of this part is scaled up 10 times,\nwhich is consistent with the original setting. We make\nfurther adjustments on DeiT architectures by introducing\nthe adaptive update scheme in [10] on the coefï¬cient Î»e\nfrom 0 to 0.1, instead of the original constant setting.\nThe other training setups are consistent with T-ADV .\nâ€¢ T-SSL. T-SSL consists of a feature encoder and a co-\nsine similarity-based classiï¬er. The output of the fea-\nture encoder is linearly embedded into 512-d and then\nâ„“2-normalized. The normalized vectors are used for k-\nmeans clustering and label prediction. We train models\nwith learning rate lr = 5e-5 and weight decay = 1e-7\nfor 10000 iters. The balancing coefï¬cient Î»mim is con-\nstantly assigned to 0.5 and Î»is is adaptively updated\nfrom 0 to 0.1 using the scheme in [10].\nâ€¢ BiT-DANN BiT-DANN consists of a feature encoder,\na label predictor, and a domain classiï¬er. The domain\nclassiï¬er has the same architecture as the one in T-ADV .\nSimilarly, we implement BiT-DANN on downstream\ntasks using ImageNet pre-trained BiT backbones. We\ntrain models with learning rate lr = 3e-3 and weight\ndecay = 5e-4 for 10000 iters except that the domain\nclassiï¬er use a learning rate lr= 0.015.\nâ€¢ BiT-MME BiT-MME consists of a feature encoder and\na cosine similarity-based classiï¬er. The classiï¬er has the\nsame architecture as the one in T-ADV . The learning rate\nof this part is also scaled up 10 times from the base set.\nThe coefï¬cient Î»e is a constant value of 0.1. The other\noriginalBiT\nreal painting sketch quickdraw\nDeiT-S\n(a) bowtie\noriginalBiT\nreal painting sketch quickdraw\nDeiT-S\n (b) coffee cup\noriginalBiT\nreal painting sketch quickdraw\nDeiT-S\n(c) duck\noriginalBiT\nreal painting sketch quickdraw\nDeiT-S\n (d) hourglass\nFigure 9. Grad-CAM visualization on more classes. (a)-(d) corresponds to the class of bowtie, coffee cup, duck and hourglass respectively.\nAll attention maps are generated using models trained on real.\nTable 4. SSL ablation.\nModel Method R to C R to P P to C C to S S to P R to S P to R Avg.\nDeiT-S/16\n- 50.60 45.82 36.09 43.39 35.24 39.29 52.08 43.22\nT-MIM 53.67 44.80 42.31 47.00 43.28 41.70 58.38 47.30\nT-IS 53.31 47.19 40.93 46.71 41.93 38.77 55.62 46.35\nT-SSL 53.86 46.71 42.79 47.25 43.01 40.94 57.07 47.37\nBiT\n- 42.18 41.14 30.72 37.01 28.23 32.64 48.54 36.78\nMIM [37] 53.02 41.64 40.24 45.10 38.26 40.17 54.70 44.73\nIS [37] 48.31 44.12 36.32 43.84 38.30 35.81 53.31 42.85\nSSL [37] 52.55 42.80 39.03 45.72 39.08 39.65 56.07 44.98\nVGG-16\n- 39.39 37.32 26.36 32.96 25.55 27.79 45.70 33.58\nMIM [37] 48.41 42.18 36.34 43.08 38.45 37.51 54.32 42.89\nIS [37] 42.05 42.36 31.30 38.68 36.59 30.74 51.07 38.97\nSSL [37] 43.79 41.88 32.19 35.73 36.99 31.05 55.18 39.54\ntraining setups are consistent with BiT-DANN. â€¢ BiT-SSL The architecture of BiT-SSL is consistent with\nTable 5. Results of Generalization-enhanced methods under corruption shifts including noises and blurs.\nModel Method Noise Blur\nGauss. Impulse Shot Avg. Defocus Glass Motion Zoom Avg.\nDeiT-S/16\n- 55.51 54.70 54.74 54.98 45.11 27.82 51.51 41.51 41.49\nT-ADV 58.96 62.35 60.02 60.44 55.95 54.94 62.38 62.07 58.84\nT-MME 60.40 63.85 60.96 61.74 55.86 56.02 63.69 63.35 59.73\nT-MIM 59.10 62.17 59.37 60.21 53.97 54.39 62.00 61.46 57.96\nT-IS 39.78 41.18 39.67 40.21 32.87 21.48 41.59 34.16 32.52\nT-SSL 56.54 60.26 56.99 57.93 50.24 51.56 59.68 59.04 55.13\nBiT\n- 37.48 33.46 34.70 35.21 22.62 10.04 30.54 31.44 23.66\nDANN [10] 37.21 41.08 39.85 39.38 30.55 28.54 42.70 40.60 35.60\nMME [27] 43.51 46.14 43.81 44.49 15.07 30.75 47.17 42.82 33.95\nMIM [37] 36.23 36.33 35.81 36.12 23.95 25.59 40.11 34.68 31.08\nIS [37] 18.33 17.24 17.04 17.54 9.77 6.07 15.41 17.33 12.15\nSSL [37] 35.00 35.28 34.95 35.08 25.90 26.19 36.42 20.15 27.16\nTable 6. Results of Generalization-enhanced methods under corruption shifts including weathers and digital corruptions.\nModel Method Weather Digital\nBright Fog Frost Snow Avg. Contrast Elastic JPEG Pixel Avg.\nDeiT-S/16\n- 72.11 53.67 50.45 53.79 57.51 66.58 63.10 61.25 60.51 62.86\nT-ADV 71.54 67.07 59.81 66.43 66.21 67.73 69.83 64.43 68.25 67.56\nT-MME 71.93 67.62 60.57 67.03 66.79 68.44 70.04 65.65 68.92 68.26\nT-MIM 72.37 66.98 59.76 66.38 66.37 68.22 70.19 64.39 68.56 67.84\nT-IS 66.50 51.91 39.62 51.65 52.42 56.06 58.77 53.53 50.07 54.60\nT-SSL 70.85 65.42 57.04 64.80 64.53 66.38 68.83 63.71 66.43 66.34\nBiT\n- 65.96 49.08 32.23 34.68 45.49 54.57 44.61 52.74 48.27 50.04\nDANN [10] 60.88 54.67 37.95 48.10 50.40 55.75 56.12 48.52 54.57 53.74\nMME [27] 61.94 57.05 41.45 52.43 53.22 57.46 57.93 51.28 56.61 55.82\nMIM [37] 60.38 56.37 34.66 48.92 50.08 56.29 57.17 50.55 55.92 54.98\nIS [37] 49.45 34.50 17.77 22.01 30.93 35.03 32.27 32.19 28.44 31.98\nSSL [37] 54.78 48.88 34.25 42.20 45.03 49.26 49.54 44.67 15.68 39.79\nT-SSL. except the backbone is replaced by a BiT model.\nWe train models with learning rate lr= 0.01 for 10000\niters. The balancing coefï¬cient Î»mim and Î»is is respec-\ntively assigned to 0.5 and 1.0.\nB.2.3 T-SNE Visualization Implementation\nTo begin with, we select ï¬ve classes from DomainNet dataset\nfor visualization: boomerang, bowtie, circle, duck, envelope,\nsince samples of these classes in quickdraw are basically\nrecognizable and ViTs could obtain decent performance.\nWe then utilize samples of these classes in four domains to\ngenerate Class Token data of DeiT-S trained on real. We\nextract Class Tokens of layer 8 and layer 12, and employ\nT-SNE to embed Class Tokens of four domains into the same\nfeature space for each layer.\nC. More Experiment Results\nC.1. Grad-CAM Visualization Results\nWe conduct Grad-CAM visualization on more classes in-\ncluding bowtie, coffee cup, duck and hourglass, as shown in\nFig. 9. All attention maps are generated using models trained\non real. We shall have the same observation as in the main\ntext that DeiT-S is capable of capturing the key structural\ninformation of objects in whichever domain. Take for ex-\nample the class hourglass. DeiT-S consistently concentrate\non the core area of a hourglass, e.g. the glass portion. By\ncontrast, BiT model tends to focus on the peripheral support\nin the case of real, painting and sketch. The above results\nreconï¬rm the conclusion that ViTs focus more on structures.\nWe further investigate the properties of the generalization-\nenhanced methods. Speciï¬cally, we generate the Grad-CAM\nattention maps of domain sketch of T-ADV , T-MME and\nT-SSL, as illustrated in Fig. 10. T-SSL can capture more\nTable 7. Results of Generalization-enhanced methods under background shifts.\nModel Method Background\nOnly-FG Mixed-Same Mixed-Rand Mixed-Next Avg.\nDeiT-S/16\n- 88.80 90.21 84.08 82.70 86.44\nT-ADV 94.26 96.54 92.79 92.86 94.11\nT-MME 93.89 96.39 92.13 91.69 93.52\nT-MIM 95.51 96.83 92.72 92.42 84.37\nT-IS 95.36 96.17 88.67 87.57 91.94\nT-SSL 94.92 96.98 93.01 93.16 94.52\nBiT\n- 82.04 83.81 77.07 74.19 79.27\nDANN [10] 94.19 95.44 91.25 91.76 93.16\nMME [27] 87.57 92.75 83.72 87.27 87.83\nMIM [37] 94.04 95.29 90.07 90.73 92.53\nIS [37] 85.07 92.42 82.50 83.82 85.95\nSSL [37] 93.67 95.14 88.82 90.22 91.96\nTable 8. Results of Generalization-enhanced methods under\ntexture shifts and style shifts.\nModel Method Texture Style\nStylized ImageNet ImageNet-R\nDeiT-S/16\n- 13.11 25.91\nT-ADV 27.16 48.97\nT-MME 17.90 47.71\nT-MIM 15.79 46.60\nT-IS 9.49 37.08\nT-SSL 13.78 47.33\nBiT\n- 6.05 23.46\nDANN [10] 4.49 35.33\nMME [27] 5.33 31.84\nMIM [37] 3.99 37.79\nIS [37] 3.46 28.36\nSSL [37] 4.15 38.29\na) T-ADVb) T-MME c) T-SSL\nFigure 10. Visualization of attention weights\ncomprehensive structure than two other methods, which may\nbe the major factor why it outperforms T-ADV and T-MME.\nC.2. Ablation Study on Self-supervised Generaliza-\ntion Enhancing Method\nSince the self-supervised learning method T-SSL and\nBiT-SSL contains two major loss terms LIS and LMIM in\nenhancing out-of-distribution generalization, we separately\ntest their effectiveness. As the results are shown in Tab. 4, we\ncould observe that 1) LMIM works the best for VGG-16, 2)\nbut the combination of two parts perform the best on average\nfor larger models including DeiT-S/16 and BiT. Thus, we\ncould conclude that there exists a mutual promotion between\nthe in-domain self-supervision and mutual information max-\nimization towards large models.\nC.3. Generalization-Enhanced ViTs Results under\nMultiple Shifts\nWe examine the effectiveness of the generalization-\nenhanced methods under multiple shifts, including corrup-\ntion shifts, texture shifts, and style shifts. We respectively\nuse ImageNet-C, Stylized-ImageNet, and ImageNet-R for\nexperiments. Because of the lack of training sets, we make a\n2:1 split on these benchmarks and the ImageNet validation\nset for training and testing. Speciï¬cally, we use severity 3 of\ncorruptions for use. The results are shown in Tab. 5, Tab. 6,\nTab. 7 and Tab. 8. From the results we could observe that 1)\nMME dominates the results under corruption shift for both\ntypes of models, 2) T-SSL performance the best under back-\nground shifts while DANN works the best for BiT models.\n3) these generalization-enhancing methods may be harmful\nto generalization under certain distribution shifts for BiTs,\ne.g. defocus and brightness, while having little infection on\nDeiTs.\nC.4. Relationship between the attention mechanism\nand the inductive bias\nWe further investigate how the attention mechanism,\nspeciï¬cally the receptive ï¬eld size of attention, contributes\nto the bias of ViTs. Due to the limited time, we make mod-\niï¬cations on the well-trained ViTs using global attention\nby change the test-time attention receptive ï¬eld size. As\nshown in Fig. 11, we observe the interesting phenomenon\nthat model shape bias decreases as the attention receptive\nï¬eld size is reduced, while the model texture bias behave\nin the opposite way. For DeiT-S and DeiT-B, model tex-\nture bias even increases. Based on the above results, we\ncould conclude that larger size of attention receptive ï¬eld\ncontributes to a better shape-biased ViT model.\n/uni00000031/uni00000024/uni00000031/uni00000015/uni00000015/uni00000015/uni00000013/uni00000014/uni0000001b/uni00000014/uni00000019/uni00000014/uni00000017/uni00000014/uni00000015\n/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000035/uni00000048/uni00000046/uni00000048/uni00000053/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000029/uni0000004c/uni00000048/uni0000004f/uni00000047\n/uni00000014/uni00000013/uni00000011/uni00000013\n/uni00000014/uni00000015/uni00000011/uni00000018\n/uni00000014/uni00000018/uni00000011/uni00000013\n/uni00000014/uni0000001a/uni00000011/uni00000018\n/uni00000015/uni00000013/uni00000011/uni00000013\n/uni00000015/uni00000015/uni00000011/uni00000018\n/uni00000015/uni00000018/uni00000011/uni00000013\n/uni00000015/uni0000001a/uni00000011/uni00000018\n/uni00000016/uni00000013/uni00000011/uni00000013/uni00000036/uni0000004b/uni00000044/uni00000053/uni00000048/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000027/uni00000048/uni0000004c/uni00000057/uni00000010/uni00000037/uni0000004c/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000057/uni00000010/uni00000036/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000057/uni00000010/uni00000025/uni00000012/uni00000014/uni00000019\n(a) shape bias change\n/uni00000031/uni00000024/uni00000031/uni00000015/uni00000015/uni00000015/uni00000013/uni00000014/uni0000001b/uni00000014/uni00000019/uni00000014/uni00000017/uni00000014/uni00000015\n/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000035/uni00000048/uni00000046/uni00000048/uni00000053/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000029/uni0000004c/uni00000048/uni0000004f/uni00000047\n/uni00000018/uni00000013/uni00000011/uni00000013\n/uni00000018/uni00000015/uni00000011/uni00000018\n/uni00000018/uni00000018/uni00000011/uni00000013\n/uni00000018/uni0000001a/uni00000011/uni00000018\n/uni00000019/uni00000013/uni00000011/uni00000013\n/uni00000019/uni00000015/uni00000011/uni00000018\n/uni00000019/uni00000018/uni00000011/uni00000013\n/uni00000019/uni0000001a/uni00000011/uni00000018\n/uni0000001a/uni00000013/uni00000011/uni00000013/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000027/uni00000048/uni0000004c/uni00000057/uni00000010/uni00000037/uni0000004c/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000057/uni00000010/uni00000036/uni00000012/uni00000014/uni00000019\n/uni00000027/uni00000048/uni0000004c/uni00000057/uni00000010/uni00000025/uni00000012/uni00000014/uni00000019 (b) texture bias change\nFigure 11. Process of change of model shape and texture biases\nduring the change of test-time attention receptive ï¬eld sizes.",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.7977867126464844
    },
    {
      "name": "Computer science",
      "score": 0.637726902961731
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6275821924209595
    },
    {
      "name": "Transformer",
      "score": 0.5114823579788208
    },
    {
      "name": "Convolutional neural network",
      "score": 0.48936885595321655
    },
    {
      "name": "Machine learning",
      "score": 0.4824400544166565
    },
    {
      "name": "Mathematics",
      "score": 0.23395392298698425
    },
    {
      "name": "Engineering",
      "score": 0.08511370420455933
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}