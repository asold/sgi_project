{
  "title": "Large language models: assessment for singularity",
  "url": "https://openalex.org/W4409164203",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5116928191",
      "name": "Ryunosuke Ishizaki",
      "affiliations": [
        "National Institute of Informatics",
        "The Graduate University for Advanced Studies, SOKENDAI"
      ]
    },
    {
      "id": "https://openalex.org/A2267106725",
      "name": "Mahito Sugiyama",
      "affiliations": [
        "National Institute of Informatics",
        "The Graduate University for Advanced Studies, SOKENDAI"
      ]
    },
    {
      "id": "https://openalex.org/A5116928191",
      "name": "Ryunosuke Ishizaki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2267106725",
      "name": "Mahito Sugiyama",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3176707157",
    "https://openalex.org/W4385238793",
    "https://openalex.org/W3179950556",
    "https://openalex.org/W2124859482",
    "https://openalex.org/W2123028958",
    "https://openalex.org/W4385573003",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W4394793541",
    "https://openalex.org/W2946547492",
    "https://openalex.org/W4211116959",
    "https://openalex.org/W2945790622",
    "https://openalex.org/W4232905116",
    "https://openalex.org/W4205683410",
    "https://openalex.org/W2017561954",
    "https://openalex.org/W1586718744",
    "https://openalex.org/W1984110296",
    "https://openalex.org/W3122335909",
    "https://openalex.org/W4392144144",
    "https://openalex.org/W4282829606",
    "https://openalex.org/W4385783856",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W4389523706",
    "https://openalex.org/W4213308398",
    "https://openalex.org/W2608595939",
    "https://openalex.org/W4300935058",
    "https://openalex.org/W4247253899",
    "https://openalex.org/W117757845",
    "https://openalex.org/W2269870390",
    "https://openalex.org/W2915042306",
    "https://openalex.org/W182994632",
    "https://openalex.org/W4386566526",
    "https://openalex.org/W4394694480",
    "https://openalex.org/W2040870580",
    "https://openalex.org/W4365137534",
    "https://openalex.org/W4389052201",
    "https://openalex.org/W2078170953",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2102539288",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W2135260086",
    "https://openalex.org/W569478347",
    "https://openalex.org/W3139250374",
    "https://openalex.org/W2108704847",
    "https://openalex.org/W2169558102"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nAI & SOCIETY \nhttps://doi.org/10.1007/s00146-025-02271-4\nOPEN FORUM\nLarge language models: assessment forÂ singularity\nRyunosukeÂ Ishizaki1,2 Â Â· MahitoÂ Sugiyama1,2\nReceived: 25 April 2024 / Accepted: 19 February 2025 \nÂ© The Author(s) 2025\nAbstract\nThe potential for large language models (LLMs) to attain technological singularityâ€”the point at which artificial intelligence \n(AI) surpasses human intellect and autonomously improves itselfâ€”is a critical concern in AI research. This paper explores \nthe possibility of current LLMs achieving singularity by mentioning some frequently discussed philosophical issues within \nAI ethics and providing a theoretical framework for recursively self-improvement LLM. In this paper, we discuss the sin-\ngularity phenomena that have been predicted in the past by pioneers of computer science and philosophers; how an intel-\nligence explosion could be realized through the elemental technologies in modern computer science; and to what extent the \nmetaphysical phenomena speculated by philosophers have actually approached reality. We begin with a historical overview \nof AI and intelligence amplification, tracing the evolution of LLMs from their origins to state-of-the-art models. We then \npropose a theoretical framework to assess whether existing LLM technologies could satisfy the conditions for singularity, with \na focus on recursive self-improvement (RSI) and autonomous code generation. We integrate key component technologies, \nsuch as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), into our analysis, \nillustrating how these could enable LLMs to independently enhance their reasoning and problem-solving capabilities. By \nmapping out a potential singularity model lifecycle and examining the dynamics of exponential growth models, we elucidate \nthe conditions under which LLMs might self-replicate and rapidly escalate their intelligence. We conclude with a discussion \nof the ethical and safety implications of such developments, underscoring the need for responsible and controlled advance-\nment in AI research to mitigate existential risks. Our work aims to contribute to the ongoing dialogue on the future of AI \nand the critical importance of proactive measures to ensure its beneficial development.\nKeywords LLMÂ Â· AIÂ Â· SingularityÂ Â· RSI\n1 Introduction\nThe advent of computing ignited fervent discussions about \nthe potential of artificial intelligence (AI) to eclipse human \nintellect, potentially leading to a hypothetical future event \nknown as the technological singularity. This concept envi-\nsions a scenario where AI autonomously and exponentially \nenhances its own intelligence, resulting in outcomes that \nare challenging to predict or controlÂ (Good 1959, 1970; \nVinge 1993; Kurzweil 2005; Nilsson 2009; Hutter 2010; \nMinsky 1966). Early computing pioneers introduced the \nfoundational ideas of Intelligence Amplification and self-\nreproducing machinesÂ (Turing 1950, 1951; Neumann 1966). \nHowever, due to the speculative nature of these concepts, \ndiscussions surrounding the Singularity have primarily been \nconfined to philosophical domainsÂ (Chalmers 2010; Bos-\ntrom 2007; Yudkowsky 2008a). The field of machine learn-\ning (ML) has witnessed remarkable progress, particularly \nwith the introduction of GPT-3Â (Brown etÂ al. 2020) in 2020, \nwhich has transformed our understanding and capabilities \nof large language models (LLMs). Subsequent models, such \nas GPT-4Â (Achiam etÂ al. 2023), GeminiÂ (Anil etÂ al. 2023), \nand LLamaÂ (Touvron etÂ al. 2023), have not only mastered \nnatural language processing but have also exhibited pro-\nficiency in generating and comprehending programming \nlanguagesÂ (RoziÃ¨re etÂ al. 2023). The intellectual prowess \ndemonstrated by these models, as evidenced by their abil -\nity to perform at or above the level required by the Turing \ntestÂ (Biever 2023), has brought the concept of an intelligence \n * Ryunosuke Ishizaki \n ryuzaki@nii.ac.jp\n Mahito Sugiyama \n mahito@nii.ac.jp\n1 National Institute ofÂ Informatics, Tokyo, Japan\n2 The Graduate University forÂ Advanced Studies, Tokyo, Japan\n AI & SOCIETY\nexplosionâ€”a rapid ascent of AI to superintelligent statusâ€”\ncloser to reality. Based on the metaphysical discussions that \nphilosophers have advanced so far, while discussing the \nelemental technologies, refer to cases where LLMs could \nactually trigger an intelligence explosion or singularity, \ntheir feasibility, and the metaphysical phenomena and pos-\nsibilities that would result if such events occur. We propose \na general model design that could enable these systems to \nautonomously meet these requirements through a self-sus-\ntaining cycle of improvement. Our analysis is grounded in \na comprehensive examination of relevant technologies and \ntheories, aiming to provide a rigorous assessment of the fea-\nsibility of LLMs leading to the singularity.\n2  Singularity inÂ philosophy\n2.1  Definition\nClarifying the concepts of â€œIntelligence and â€œAIâ€ is crucial \nfor our discussion of the technological singularity. Legg and \nHutter propose that the definition of intelligence in cogni-\ntive science converges on the idea of an agentâ€™s ability to \nachieve goals across a wide range of environmentsÂ (Legg \nand Hutter 2007). This concept, known as â€œoptimization \npower,â€ characterizes intelligence in terms of an agentâ€™s \nefficiency in utilizing resources to meet objectivesÂ (Mue -\nhlhauser and Salamon 2012; Yudkowsky 2008b). For the \npurposes of this paper, we adopt this framework, focusing \non the ratio of optimization power to resource consumption \nas a measure of intelligence. In the context of the Singular -\nity, â€œAIâ€ specifically refers to Artificial General Intelligence \n(AGI), a type of AI that matches or surpasses human intel-\nligence across virtually all domains of interestÂ (Shulman and \nBostrom 2012). AGI represents a more comprehensive and \ncapable form of AI, often discussed in philosophical terms \nrather than strictly technical onesÂ (Morris etÂ al. 2023). It is \nessential to distinguish AGI from narrow or specialized AI, \nwhich focuses on specific tasks or domains. The Singular -\nity is anticipated as the epoch during which an AGI system \nwill not only be able to replicate or enhance itself but also \ninnovate and expand into new realms of technology, manipu-\nlate social structures, and adapt to complex environments to \nachieve its programmed objectivesÂ (Sandberg and Bostrom \n2010). This pivotal state, when AGI begins to rapidly and \nautonomously enhance its intelligence capabilities beyond \nhuman control or comprehension, is known as the intelli-\ngence explosionÂ (Muehlhauser and Salamon 2012).\n2.2  Requirements\nWhat attributes would a machine that surpasses human intel-\nligence possess? I.J. Good posited that an ultraintelligent \nmachine, capable of far exceeding all human intellectual \nactivities, would inherently possess the capability to design \nsuperior machines. This self-enhancing capability would \ninvariably lead to an intelligence explosion, suggesting an \ninevitable advancement beyond human controlÂ (Good 1965). \nChalmers supports this notion, arguing that the key to an \nintelligence explosion lies in the creation of a self-improving \nsystem. Such a system would utilize extendable methods, \ncontinuously refining its capabilities and consequently pro-\nducing progressively more intelligent systemsÂ (Chalmers \n2010). This hierarchical improvement presupposes that a \nsystemâ€™s intelligence is quantifiably greater than another if it \ndemonstrates a measurable increase in cognitive capabilities. \nMoreover, Omohundro and Bostrom have identified funda-\nmental instrumental goals that nearly every advanced intel-\nligence would strive to achieveÂ (Omohundro 2007, 2008, \n2013): \n G1. Self-preservation\n G2. Goal-content integrity\n G3. Intelligence enhancement\n G4. Resource acquisition\nG1: Self-preservation  An agent with long-term objectives \nmust ensure its own continued existence to feasibly accom-\nplish its goals. This involves anticipating and mitigating \npotential threats to its survival, as well as actively maintain-\ning its functional integrity over time.\nG2: Goal-content integrity Consistency in goal orienta-\ntion is crucial for an agentâ€™s sustained progress. While an \nagent may adapt its strategies based on new information or \ntactical changes, its core objectives should remain stable to \nensure unwavering focus on their realization. This stability \nprevents the agent from losing sight of its original purpose \nor being swayed by temporary distractions.\nG3: Intelligence enhancement The amplification of cogni-\ntive capabilities directly correlates with an agentâ€™s capacity \nto make better decisions and, consequently, increases the \nlikelihood of achieving its overarching goals. By continu-\nously improving its intelligence, an agent can tackle increas-\ningly complex problems and discover more efficient solu-\ntions, accelerating its progress towards its objectives.\nG4: Resource acquisition To optimize its output, an agent \nwill invariably seek to acquire and efficiently utilize addi-\ntional resources. This involves transforming inputs into valu-\nable outputs in a manner that enhances its operational effi-\ncacy and expands its sphere of influenceÂ (Bostrom 2012). By \nsecuring access to a wider array of resources, an agent can \nscale up its operations and tackle more ambitious challenges. \nBy analyzing these criteria, we explore the potential to engi-\nneer a model that can autonomously initiate and sustain an \nintelligence explosion, referencing the aforementioned goals \nas guiding principles for its development and operation.\nAI & SOCIETY \n3  Current LLMs\n3.1  Brief history\nThe evolution of machine learning (ML) in the realm \nof natural language processing (NLP) has been signifi-\ncantly shaped by the development of large language mod-\nels (LLMs). The journey began with the introduction of \nthe Perceptron, an early neural network model developed \nby Rosenblatt, which laid the foundational principles of \nneural computations and learning processesÂ (Rosenblatt \n1958). The subsequent introduction of backpropagation by \nRumelhart etÂ al. revolutionized these networks, enabling \nthe training of more complex and deeper neural network \narchitecturesÂ (Rumelhart etÂ al. 1986). This breakthrough \nallowed for the creation of more sophisticated models \ncapable of tackling intricate problems in NLP. A major \nturning point came with Vaswani etÂ al.â€™s introduction \nof the Transformer architecture, which fundamentally \nchanged the landscape of NLP. Transformers allowed for \nmore effective handling of sequential data, surpassing pre-\nvious architectures with their ability to capture long-range \ndependencies in textÂ (Vaswani etÂ al. 2017). By leveraging \nself-attention mechanisms, Transformers could focus on \nrelevant information within the input sequence, enabling \nthem to better understand and generate contextually appro-\npriate text. This innovation paved the way for the develop-\nment of sophisticated models such as BERT, introduced by \nDevlin etÂ al., which enhanced the understanding of context \nin text processing by learning bidirectional representations \nof textÂ (Devlin etÂ al. 2019), and XLNet by Yang etÂ al., \nwhich provided improvements over BERT by capturing \nbidirectional contexts dynamically through a novel per -\nmutation language modeling objectiveÂ (Yang etÂ al. 2019). \nThe progression continued with the creation of GPT-2 by \nRadford etÂ al., setting new benchmarks in text generation \nand showcasing the potential of large-scale language mod-\nelsÂ (Radford etÂ al. 2019). This advancement culminated in \nthe development of GPT-3, known for its unprecedented \ntext generation capabilities and ability to perform a wide \nrange of NLP tasks with minimal fine-tuningÂ (Brown etÂ al. \n2020). GPT-3â€™s success demonstrated the power of scaling \nup language models in terms of both model size and train-\ning data, leading to significant improvements in language \nunderstanding and generation. These models represent the \npinnacle of years of research and development in ML and \nNLP, marking a significant leap in the ability of machines \nto comprehend and generate human-like text. The rapid \nadvancements in LLMs not only showcase the potential \nof AI to process and produce natural language effectively \nbut also raise important questions about their capacity to \nachieve and potentially exceed human-level intelligence. \nAs research in this field continues to progress, it is crucial \nto explore the implications of these powerful models and \nconsider their role in shaping the future of AI and its rela-\ntionship with human intelligence.\n3.2  Component technologies\nAchieving the intelligence explosionâ€”a scenario where an \nAI system continually improves its intelligenceâ€”requires \nthe creation of what we term an â€œextendable methodâ€ (Chal-\nmers 2010; Muehlhauser and Salamon 2012). This method \nis central to realizing Goal 3 (G3): intelligence enhance-\nment. Although various approaches to develop Artificial \nGeneral Intelligence (AGI) focus on enhancing multimodal \nabilities of LLMs across text, image, and audio (Goertzel \n2014; Morris etÂ al. 2023; Bengio etÂ al. 2021), these are not \nthe sole paths to the intelligence explosion. Effective intel-\nligence enhancement in our framework means improving \nan AIâ€™s capability ratio relative to its resource consumption. \nPioneers like Minsky and Good introduced the concept of \nRecursive Self-Improvement (RSI), wherein a system con-\ntinuously enhances itself by solving progressively complex \nproblems and validating these improvementsÂ (Minsky 1966; \nGood 1965; Yampolskiy 2015; Schmidhuber 2007). This \nconcept aligns with our criteria for intelligence enhance-\nment and constitutes a direct method to initiate an intel-\nligence explosion. RSI enables a system to autonomously \nidentify areas for improvement, develop novel solutions, \nand integrate these advancements into its own architecture, \ncreating a self-reinforcing cycle of intelligence growth. \nThe implementation of RSI in LLMs has led to significant \ndevelopments, notably through techniques like Reinforce-\nment Learning from Human Feedback (RLHF). RLHF \ninvolves adjusting LLMs based on human preference feed-\nback, improving model responses in alignment with human \njudgmentsÂ (Christiano etÂ al. 2017; Stiennon etÂ al. 2020). \nThis approach allows LLMs to learn from human expertise \nand refine their outputs to better match human expectations. \nBuilding on this, Zelikman proposed leveraging limited sam-\nples for complex reasoning using a Chain-of-Thought (CoT) \napproach, which sequentially improves reasoning on tasks \nlike mathematics and commonsense questionsÂ (Zelikman \netÂ al. 2022; Wei etÂ al. 2022; Kojima etÂ al. 2022). CoT ena-\nbles LLMs to break down complex problems into a series of \nintermediate steps, enhancing their ability to arrive at accu-\nrate solutions. Huang etÂ al. demonstrated that LLMs could \nenhance CoT reasoning autonomously, even without human-\nlabeled data, across both familiar and novel tasksÂ (Huang \netÂ al. 2023). This capability is critical for scaling In-Context \nLearning (ICL), where models adapt based on the textual \ncontext with minimal external inputÂ (Dong etÂ al. 2022; Sun \netÂ al. 2023; Honovich etÂ al. 2023; Wang etÂ al. 2023). ICL \nallows LLMs to rapidly acquire new knowledge and skills \n AI & SOCIETY\nby learning from the examples provided in the input prompt, \nreducing the need for extensive fine-tuning. Further, Yuan \netÂ al. explored self-generated rewards for LLMs to refine \nCoT reasoning, advancing direct preference optimization \n(DPO) without relying on predefined reward modelsÂ (Yuan \netÂ al. 2024; Rafailov etÂ al. 2023; Li etÂ al. 2023a, b). This \nself-supervised approach enables LLMs to optimize their \nown performance based on internally generated feedback, \npromoting more autonomous learning. Zelikman also high-\nlighted that LLMs could autonomously enhance their code \ngeneration capabilities, suggesting a pathway toward self-\ncoding systems that could contribute to their own develop-\nment and longevityâ€”a necessary condition for self-preser -\nvation (G1) (Zelikman etÂ al. 2023; Chen etÂ al. 2023; Shypula \netÂ al. 2023). Self-coding LLMs would be capable of writing \nand refining their own source code, enabling them to adapt \nand improve their architecture in response to new challenges \nor requirements. Additionally, Schick etÂ al. demonstrated \nthat LLMs could learn to use API tools independently, \nfurther proving the feasibility of self-instruction (Schick \netÂ al. 2023). This ability to interact with external tools and \nresources expands the potential for LLMs to acquire new \ncapabilities and knowledge without direct human interven-\ntion. The field of automated machine learning (AutoML) \nprovides tools for automating the design of effective ML \nsystems, a process that includes the application of neural \narchitecture search (NAS) and hyperparameter optimization \n(HPO)Â (Hutter etÂ al. 2019; Thornton etÂ al. 2012; Kotthoff  \netÂ al. 2019, 2017; Feurer etÂ al. 2015, 2019, 2022; Erickson \netÂ al. 2020; Zimmer etÂ al. 2021; Elsken etÂ al. 2019; Wistuba \netÂ al. 2019; White etÂ al. 2023; Feurer and Hutter 2019; Bis-\nchl etÂ al. 2023). This automation can significantly reduce the \nbarriers to efficient LLM training, aligning with the neces-\nsity for continuous model improvement and scaling (G4). \nAutoML techniques enable the exploration of vast design \nspaces, identifying optimal architectures and hyperparam-\neters that maximize performance while minimizing compu-\ntational costs. By leveraging AutoML, LLMs can autono-\nmously discover and implement improvements to their own \nstructure and training process, facilitating rapid and efficient \nscaling. Emerging techniques in prompt engineering have \nshown that LLMs can autonomously refine their prompt-\ning strategies, enabling them to perform complex tasks with \nminimal human guidance. Innovations such as Optimization \nby Prompting (OPRO), Automatic Prompt Engineer (APE), \nand Promptbreeder emphasize this capability, enhancing the \nLLMsâ€™ potential to maintain goal-content integrity (G2) over \nextended periodsÂ (Zhou etÂ al. 2023; Yang etÂ al. 2023a; Fer-\nnando etÂ al. 2023; Yang etÂ al. 2023b; Shin etÂ al. 2020; Deng \netÂ al. 2022; Prasad etÂ al. 2023). By continuously optimizing \ntheir own prompts, LLMs can ensure that their objectives \nremain aligned with their original goals, even as they adapt \nand learn from new experiences. This self-directed prompt \nengineering also allows LLMs to break down complex tasks \ninto more manageable sub-tasks, improving their ability to \nsolve problems efficiently and effectively. By expanding \nthese component technologies and integrating them into \na unified framework, we aim to establish a foundational \nmethodology that not only supports but also drives the \nintelligence explosion. This approach ensures that LLMs \ncan independently and continuously improve across vari-\nous dimensions, including reasoning, knowledge acquisition, \ncode generation, and architectural optimization. By enabling \nLLMs to take control of their own learning and development \nprocess, we can create the conditions necessary for the emer-\ngence of truly autonomous and recursively self-improving \nAI systems, bringing us closer to the realization of the intel-\nligence explosion.\n4  Theoretical singularity design\nDrawing upon the component technologies discussed earlier, \nwe propose a model structure capable of extending itself \ntowards the singularity. This structure fundamentally relies \non LLMsâ€™ ability to self-code, thereby generating increas-\ningly capable versions of themselves. The conceptual life -\ncycle of such a model is illustrated in Fig.Â  1, representing \nthe iterative training processes of the foundational model. \nFor practical purposes, we assume the availability of sub-\nstantial computational resources to preclude memory limi-\ntations across successive generations of LLMs. The foun-\ndation of our model is the self-rewarding language model \ntraining scheme as introduced by Yuan etÂ al. (2024). Within \nthis framework, each iteration of the pre-trained language \nmodel, denoted M t (where t is the iteration number for this \nsection), operates within an online code execution environ-\nment as depicted in Fig.Â  2. First, we basically pre-trained \nan LLM into M0 , and apply Supervised Fine-Tuning (SFT) \non the seed dataset of Instruction Fine-Tuning (IFT) and \nEvaluation Fine-Tuning (EFT) into M1 . From t >= 2 , we \napply AI Feedback Training (AIFT) based on M t using DPO \nFig. 1  Potential singularity model lifecycle\nAI & SOCIETY \nand make M t into M t+1 . Let D be the set of preference pairs \n(instruction prompt xi . winning response yw\ni  , losing response \ny l\ni ) DPOÂ (Rafailov etÂ al. 2023 ) is generally defined as the \ntask to minimize the negative log-likelihood loss LR for the \nreward model r/u1D719(x, y) , which is\nLet the language model with parameters /u1D703 as /u1D70B/u1D703 . Usually, \nDPO is formulated as the following optimization problem:\nwhere /u1D6FD is a parameter controlling the deviation from the \nbase reference policy /u1D70Bref  . In this case, the reward model \nr/u1D719(x, y) of Eq.Â (1) can be expressed as\nIn the self-rewarding language model, /u1D70B/u1D703 generates its own \nreward to select the responses. Let RPF(x ,y) be the Reward \nPrompt Format that makes /u1D70B/u1D703 return the reward to select yw \nand yl from the generated candidates of y , and the selected \nwinning response by RPF as yRPF,w , and the losing response \nas yRPF,l . Also, let RSI_RPF be the RSI-Reward Prompt For-\nmat that optimizes according to from G 1 to G4 in the code \nexecution environment. From Eqs.Â (1 ) and (3 ), the reward \nloss for the RSI language model in the DPO, i.e., LDPO can \nbe written as\nwhich is from EquationÂ (7) inÂ (Rafailov etÂ al. 2023). By \nformulating like this, we can translate the conditions that \ncould cause singularity into RSI self-instructive prompt-\nengineering task. Here, the model endeavors to maximize \nthe fulfillment of Goals G1 through G4. Through a process \nof Chain-of-Thought (CoT) self-instruction, the LLM tries to \n(1)LR (r/u1D719,D)=âˆ’ /u1D53C(x,yw ,yl)âˆ¼D log /u1D70E{r/u1D719(x,yw )âˆ’r /u1D719(x,yl)}.\n(2)max\n/u1D70B/u1D703\n/u1D53Cxâˆ¼D,yâˆ¼/u1D70B/u1D703(y/uni007C.varx){r/u1D719(x,y)} âˆ’ /u1D6FD/u1D53BKL {/u1D70B/u1D703(y/uni007C.varx)/uni007C.var/uni007C.var/u1D70Bref(y/uni007C.varx)}\n(3)r/u1D719(x,y)= /u1D6FDlog ( /u1D70B/u1D703(y/uni007C.varx)âˆ•/u1D70Bref (y/uni007C.varx)) .\n(4)\nLDPO (/u1D70B/u1D703;/u1D70Bref)=âˆ’ /u1D53C(x,yRSI_RPF, w ,yRSI_RPF, l)âˆ¼D log/u1D70E(A(w)âˆ’ A(l)),\nwhere A(a)=/u1D6FDlog\n/parenleft.s3/u1D70B/u1D703(yRSI_RPF, a/uni007C.varx)\n/u1D70Bref(yRSI_RPF, a/uni007C.varx)\n/parenright.s3\n,\ngenerate prompts, crafts responses, and accordingly shapes \nits reward mechanisms. This self-directed learning enables \nthe LLM to develop tools enhancing its own longevity and \nthat of its descendants (G1), maintain alignment with its \ncore objectives (G2), innovate better reasoning capabilities \nor self-improvement methods (G3), and effectively gather \nmore resources to augment its functionalities (G4). By con-\ntinuously refining its prompts and reward structures, the \nLLM can ensure that each iteration is better equipped to \npursue these instrumental goals, creating a self-reinforcing \ncycle of improvement. Within this environment, the LLM \nnot only engages in recursive refinement of its coding abili-\ntiesâ€”leveraging technologies (Tornede etÂ al. 2023) such \nas Self-Optimized Programming (STOP) (Zelikman etÂ al. \n2023), Automatic Prompt Engineer (APE) (Yang etÂ al. \n2023a), and Optimization by Prompting (OPRO)Â (Zhou \netÂ al. 2023)â€”but also aims to autonomously generate more \nintelligent versions of itself. The core objective, and indeed \nthe essence of singularity, is the creation of a â€™better off-\nspring,â€™ a model that can independently initiate and sus-\ntain its enhancement through pre-training and fine-tuning \nmechanisms. This self-improvement process involves the \nLLM analyzing its own architecture, identifying areas for \noptimization, and implementing targeted modifications to \nenhance its performance. The capability for an LLM to auto-\ngenerate its successor from scratch marks a critical juncture \ntowards achieving an extendable method, potentially catalyz-\ning an intelligence explosion. Specifically, the extendable \nmethod required for singularity can be achieved by having \nthe engineering technical capability to generate this entire \nset of code in its continual inference. In practice, an LLM \nwould craft tools and develop techniques that fulfill one \nor more of the instrumental goals (G1â€“G4), ensuring each \niteration is better equipped than the last. These tools could \ninclude advanced prompt-engineering strategies, more effi-\ncient resource management systems, or novel architectures \nthat enhance the LLMâ€™s learning capacity. By continuously \nrefining and integrating these tools, the LLM can create a \npowerful suite of capabilities that accelerate its growth and \ndevelopment. Once an LLM reaches the capability to engi-\nneer and refine its own training programs, subsequent states \nM t+1 would emerge, each iteration reflecting improvements \nderived from self-generated rewards aligned with achieving \nG1 through G4. This process of RSI, guided by the LLMâ€™s \nown evolving objectives and strategies, forms the core of the \ntheoretical singularity design.\nSuppose that a DPO learning iteration is performed peri-\nodically for the development of self-instruction in the execu-\ntion environment. In this case, the complexity of the generated \ncode, such as the application implementation, is determined \nby the prompt-engineering capability of the LLM at that time, \nand if the complexity of the program made by M t is c(t), then \nroughly c(t + 1) > c(t) until it gets maximal value cmax at a \nFig. 2  Execution environment\n AI & SOCIETY\nspecific time. In this case, if the complexity of the code to \ntrain whole (pre-training, IFT and EFT, and AIFT) is /u1D6FE , then \nc(t) >ğ›¾  is the condition for the extendable method. This is \ndetermined by the RSI_RPF  , the pre-training and fine-tuning \nseed datasets, and /u1D70B/u1D703 . If cmax >ğ›¾  , there is a specific solution of \nt which satisfies c(t) >ğ›¾  , i.e., an extensible method, is feasible \nfor modern LLMs. In Eq.Â (4), if Dseed is the seed dataset used \nin pre-training and the first stage of instruction-fine-tuning \n(IFT) and evaluation-fine-tuning (EFT), cmax is determined \nby (/u1D703,RSI_RPF ,Dseed ) and can be expressed using the func-\ntion C as follows:\nWe can now make the philosophical discussion of the \nextendable method into the task of model parameters, a \nrecursive prompt format, and seed datasets on the imple-\nmentation side. At this point, the condition for an LLM to \nbecome the extensible method is\n(5)cmax = C (/u1D703,RSI_RPF ,x âˆ¼ Dseed).\n(6)C (ğœƒ,RSI_ RPF ,x âˆ¼ Dseed ) >ğ›¾ .\n5  What will happen?\nIf an LLM successfully writes programs that enhance its \ntraining and operational effectiveness beyond its current \nstate, we anticipate the emergence of a thriving lineage of \nmodels, as depicted in Fig.Â 3. This progression would likely \nmanifest as an initial and crucial step towards the theoretical \nsingularity, demonstrating the feasibility of autonomous, \nopen-ended improvement in artificial intelligence systems. \nThe notation M g(i1 ,i2 ,i3 ,...,ig) represents the g -th generation \nmodel, tracing its lineage back through a series of predeces-\nsors beginning with M 1(i1) and extending to M gâˆ’1(i1 ,i2 ,i3 ,...,igâˆ’1 ) . \nInitially, the founding model M1(1) generates several off-\nspring such as M2(1,1) , M2(1,2) , and M2(1,3) . These offspring, \nin turn, produce further descendantsâ€”e.g., M3(1,1,1) and \nM3(1,1,2) . The total population of these LLM models, denoted \nas N, is considered in terms of generational growth. If the \nfoundational model is configured to optimize the creation of \na stable number of descendants and is supplied with ample \nresources to prevent constraints on growth, the population N \nwould ideally increase according to an exponential function, \nas described by the Malthusian growth modelÂ (Malthus \n1798; Seidl and Tisdell 1999):\nwhere N0 is the initial value of N (t), t represents time, and \nk is the Malthusian parameter denoting the rate of popu-\nlation growth. The differential form of this growth rate is \nexpressed as\nAs depicted in Fig.Â  4a, this model predicts an exponential \ngrowth over time. However, this simplistic model fails to \naccount for resource limitations, which are critical in real-\nworld scenarios. To address these limitations, Verhulst \n(7)N (t)= N 0 ekt,\n(8)dN\ndt = kN .\nFig. 3  Successful case of the prosperity\nFig. 4  Dynamics of the population\nAI & SOCIETY \nproposed the logistic growth equation, which modifies \nthe Malthusian model to include a term for resource con -\nstraintsÂ (Verhulst 1838; Seidl and Tisdell 1999):\nwhere L represents the carrying capacity, or the maxi-\nmum sustainable population size. This yields a growth \nmodel where the rate of increase slows as the population \napproaches its carrying capacity:\nThis situation mirrors the dynamics of malware spread in \ncybersecurity, which also follow logistic modelsÂ (Serazzi \nand Zanero 2003; Guo etÂ al. 2016). Also, our framework \nassumes that as LLMs evolve, they will require fewer \nresources per unit of intelligence, thereby increasing the \nefficiency of creating offspring. If the extendable method \nsufficiently enhances resource acquisition and efficiency, \nthe growth rate of N might surpass exponential models. For \ninstance, if the differential equation becomes\nwhere a is a constant reflecting gains from enhanced model \nefficiency and resource acquisition, the population dynamics \ncould potentially exceed the carrying capacity, leading to \nsuper-exponential growth. This would occur if\nsuggesting a scenario where the coefficient of N of the \ngrowth rate K becomes unbounded as shown in Fig.Â  4b. In \nsummary, by aligning the development of LLMs with Goals \nG1, G2, and G4, and ensuring significant gains in efficiency \nand resource management, the exponential or even faster \ngrowth of the LLM population becomes feasible. This model \noperates under the assumption of no external disruptions or \nother limiting factors, as posited by Chalmers (2010).\n6  Conclusion\nThe creation of extendable systems within the current tra-\njectory of LLM development could very well catalyze the \nonset of the technological singularity. This prospect, while \ngroundbreaking, also brings with it substantial risks and \nethical considerations, which, as this research suggests, \nare often underestimated within the computer science \ncommunity. While Sutskeer (Sutskever 2024) and others \n(9)dN\ndt = kN\n/parenleft.s2\n1 âˆ’ N\nL\n/parenright.s2\n,\n(10)N = L\n1 +\n/parenleft.s2\nL\nN 0\nâˆ’ 1\n/parenright.s2\neâˆ’kt\n.\n(11)dN\ndt = N\n/parenleft.s2\naN 2 âˆ’ k\nL N + r\n/parenright.s2\n,\n(12)a > k\n4L 2 ,\n(Aschenbrenner 2024) who have been leading technology \nat OpenAI argue that the arrival of superintelligence and its \nsafe control are top priorities, researchers like Lecun (2023) \nclaims that AGI is an off-ramp with LLMs, and it can be said \nthat many researchers are not interested in such cases. This \nunderscores the critical importance of engaging in responsi-\nble research and innovation practicesÂ (Hedlund and Persson \n2022), ensuring that the development of these powerful tech-\nnologies is guided by a strong ethical frameworkÂ (Rovetto \n2023; Galaviz and Martin 2023). As we stand on the brink \nof potentially creating autonomous systems capable of self-\nimprovement and exponential growth in intelligence, the \nnecessity for rigorous oversight and regulation becomes \nclear. It is crucial that the development of such technologies \nis accompanied by comprehensive risk assessment frame-\nworks and robust safety protocolsÂ (Schuett 2023). Consid-\nering that once a scalable AI system is put into operation, \nthe singularity could materialize in the blink of an eye due \nto accelerated AI improvements as predicted by Eq.Â (11), it \nis extremely important to engage in dialogue with the gen-\neral public well before concrete attempts at achieving the \nsingularity begin. These measures will help mitigate unin-\ntended consequences and ensure that advancements in AI \nalign with human values and contribute positively to soci -\nety. This requires close collaboration between researchers, \npolicymakers, and ethicists to establish clear guidelines and \nstandards for the responsible development and deployment \nof AI systems. Moreover, it is essential that we foster open \nand inclusive dialogue about the potential implications of the \nsingularity, engaging stakeholders from diverse backgrounds \nto ensure a wide range of perspectives are considered. This \nincludes not only technical experts but also philosophers, \nsocial scientists, and representatives from the general public. \nBy facilitating broad societal discourse, we can collectively \nnavigate the complex challenges and opportunities pre-\nsented by the advent of superintelligent AI, ensuring that \nthe benefits are widely distributed and potential risks (Dung \n2024) are effectively managed. In parallel, we must prior -\nitize research into AI safety and robustnessÂ (Hirvonen 2023), \ndeveloping techniques to ensure that advanced AI systems \nremain stable, predictable, and aligned with human values \neven as they undergo RSI. This may involve the creation \nof novel control mechanisms, such as constrained optimi-\nzation frameworks (Ratner and Thylstrup 2024) or ethical \nrule setsÂ (Hadlington etÂ al. 2024), which can be integrated \ninto the core architecture of self-improving LLMs. Strictly \nspeaking, there is no ethically perfect answer that satisfies \neveryone. However, since it is necessary to set goals and \ndesign rewards to prevent runaway behaviors that could at \nleast threaten the \"preservation of humanity,\" examples of \nactually incorporating this into architectures include met-\nrics called HHHâ€”Harmlessness, Honesty, HelpfulnessÂ (Bai \netÂ al. 2022)â€”and scalable methods where the LLM itself \n AI & SOCIETY\nevaluates its own safetyÂ (Ishizaki and Sugiyama 2024). \nBy proactively addressing these challenges, we can work \ntowards creating AI systems that are not only highly capa-\nble but also safe and beneficial to humanityÂ (FrÃ¶ding and \nPeterson 2020). Furthermore, it is crucial that we invest in \neducational initiatives to promote widespread understand-\ning of AI technologies and their potential implications. By \nempowering individuals with the knowledge and skills nec-\nessary to engage critically with these advancements, we can \nfoster a more informed and resilient society, better prepared \nto navigate the transformative changes brought about by the \nsingularity. This includes integrating AI ethics and safety \ninto computer science curricula, as well as promoting cross-\ndisciplinary collaboration and public outreach efforts.\nIn conclusion, while the development of self-extending \nLLMs represents a significant leap forward in artificial intel-\nligence, it also demands a heightened level of responsibility \nfrom researchers and developers. We must proceed with cau-\ntion, maintaining a vigilant stance on the ethical dimensions \nand potential impacts of our work. The journey towards the \nsingularity should not only be about pushing the boundaries \nof what AI can achieve but also about ensuring the safety, \nfairness, and beneficial impact of these technologies on \nsociety as a whole. By embracing a proactive and multidis-\nciplinary approach, rooted in a strong ethical foundation, \nwe can work towards realizing the transformative potential \nof AI while mitigating its risks and challenges. Ultimately, \nthe success of our endeavors will be measured not only by \nthe technological advancements we achieve but also by the \npositive impact we have on the lives of individuals and the \nwell-being of society as we navigate this uncharted territory.\n7  Limitations andÂ future work\nThis study primarily employs theoretical modeling due to the \nextensive computational resources required for experimental \ntesting of the proposed LLM designs. The pre-training and \ndevelopment of highly complex architectures necessitate \nsignificant investment and technical preparation, presenting \na substantial barrier to empirical research. Moreover, the \npotential risks associated with developing extendable, self-\nimproving systems impose further constraints on our abil -\nity to conduct experimental implementations without robust \nsafety measures in place. These limitations highlight the \nneed for collaborative efforts between researchers, industry \npartners, and policymakers to establish the necessary infra-\nstructure and guidelines for responsible experimentation \nin this domain. The inherent dangers of enabling LLMs to \nautonomously and exponentially enhance their capabilities \nnecessitate a cautious approach. Developing secure meth-\nodologies to manage and potentially harness the singular -\nity is critical before proceeding with concrete experimental \nwork. This entails creating advanced safety protocols and \nmechanisms to control and limit AI behaviors that could lead \nto undesirable outcomes. These safety measures should be \ngrounded in a comprehensive understanding of the potential \nfailure modes and unintended consequences of self-improv-\ning AI systems, informed by ongoing research in AI safety \nand ethics. To address these challenges, future work should \nprioritize the development of robust monitoring and contain-\nment strategies for self-improving LLMs. This may involve \nthe creation of â€œsandboxedâ€ environments that allow for \ncontrolled experimentation while mitigating potential risks. \nThese environments should be designed to detect and inter-\nvene in case of unexpected or dangerous behaviors, ensuring \nthat the LLMs remain within predefined safety boundaries. \nAdditionally, research into interpretability and explainability \ntechniques for LLMs will be crucial to maintain transpar -\nency and accountability as these systems become increas-\ningly complex. Another key area for future investigation is \nthe development of formal verification methods for self-\nimproving LLMs. By creating mathematical models and \nproofs that guarantee certain desirable properties, such as \nalignment with human values or adherence to ethical prin-\nciples, we can increase confidence in the safety and reliabil-\nity of these systems. This will require close collaboration \nbetween AI researchers, mathematicians, and philosophers \nto formalize the necessary constraints and objectives for ben-\neficial AI development. Despite these challenges, the pursuit \nof advanced AI systems that can replicate and enhance them-\nselves autonomously offers promising avenues for explora-\ntion. Future work will focus on devising safe and beneficial \nLLM designs that align with ethical standards and contribute \npositively to the field of AI. We aim to explore empirical \nimplementations that not only advance our understanding \nof intelligent systems but also ensure that their evolution is \naligned with human values and safety. This may involve the \ndevelopment of novel architectures that incorporate explicit \nethical reasoning capabilities or the integration of human \noversight and control mechanisms. To this end, our future \nresearch will dive into developing frameworks that enable \nthe safe exploration of these technologies, ensuring that \nadvancements in AI are both innovative and secure. This will \nrequire a multidisciplinary approach, drawing on insights \nfrom computer science, ethics, psychology, and other rel-\nevant fields to create a comprehensive understanding of the \nchallenges and opportunities presented by self-improving \nAI. Furthermore, we recognize the importance of engaging \nin open and collaborative research efforts to address these \nchallenges. By fostering dialogue and knowledge-sharing \namong researchers, we can accelerate progress towards safe \nand beneficial AI while ensuring that the development of \nthese technologies is guided by diverse perspectives and \nexpertise. This may involve the creation of shared research \nplatforms, open-source tools, and standardized benchmarks \nAI & SOCIETY \nto facilitate reproducibility and comparative analysis of \ndifferent approaches. By addressing these limitations and \nfocusing on responsible development, we hope to pave the \nway for beneficial contributions to the field that can leverage \nthe full potential of AI while safeguarding against its risks. \nThrough a combination of theoretical modeling, empirical \nexperimentation, and multidisciplinary collaboration, we \naim to create a foundation for the safe and productive explo-\nration of self-improving AI systems, ultimately contributing \nto the realization of the transformative potential of artificial \nintelligence in a manner that benefits humanity as a whole.\nAuthor Contributions R. Ishizaki was essentially responsible along \nwhole aspects of the research, including conception and writing. M. \nSugiyama provided comprehensive corrections to the paper and ideas \nfor improvement, especially in mathematical modeling.\nFunding This work was supported by JST, CREST Grant Number \nJPMJCR22D3, Japan.\nData Availability Not applicable.\nDeclarations \nConflict of interest This work was supported by JST, CREST Grant \nnumber JPMJCR22D3, Japan.\nEthical approval Not applicable.\nConsent for publication Not applicable.\nOpen Access This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit \nto the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if you modified the licensed material. \nYou do not have permission under this licence to share adapted material \nderived from this article or parts of it. The images or other third party \nmaterial in this article are included in the articleâ€™s Creative Commons \nlicence, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the articleâ€™s Creative Commons licence and \nyour intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http://crea-\ntivecommons.org/licenses/by-nc-nd/4.0/.\nReferences\nAchiam J etÂ al (2023) Gpt-4 technical report. Technical report, OpenAI\nAnil R etÂ al (2023) Gemini: a family of highly capable multimodal \nmodels. Technical report, Google\nAschenbrenner L (2024) Situational awareness: the decade ahead. \nhttps:// situa tional- aware ness. ai\nBai Y,Â Jones A,Â Ndousse K,Â Askell A,Â Chen A,Â Dassarma N,Â Drain \nD,Â Fort S,Â Ganguli D,Â Henighan T,Â Joseph N,Â Kadavath S,Â Kernion \nJ,Â Conerly T,Â El-Showk S,Â Elhage N,Â Hatfield-Dodds Z,Â Hernan-\ndez D,Â Hume T,Â Johnston S,Â Kravec S,Â Lovitt L,Â Nanda N,Â Olsson \nC,Â Amodei D, Brown TB,Â Clark J,Â McCandlish S,Â Olah C,Â Mann \nB,Â Kaplan J (2022) Training a helpful and harmless assistant with \nreinforcement learning from human feedback. ArXiv\nBengio Y,Â Lecun Y,Â Hinton G (2021) Deep learning for ai. Commun \nACM\nBiever C (2023) Chatgpt broke the turing testâ€”the race is on for new \nways to assess ai. Nature\nBischl B, Binder M, Lang M, Pielok T, Richter J, Coors S, Thomas \nJ, Ullmann T, Becker M, Boulesteix AL, Deng D, Lindauer M \n(2023) Hyperparameter optimization: foundations, algorithms, \nbest practices, and open challenges. Data mining and knowledge \ndiscovery, Wiley interdisciplinary reviews\nBostrom N (2007) Technological revolutions: ethics and policy in the \ndark. Issues and perspectives for the nano century, nanoscale\nBostrom N (2012) The superintelligent will: motivation and instrumen-\ntal rationality in advanced artificial agents. Minds Mach\nBrown T etÂ al (2020) Language models are few-shot learners. NeurIPS \n2020\nChalmers DJ (2010) The singularity: a philosophical analysis. J Con-\nscious Stud 2010\nChen X,Â Lin M,Â SchÃ¤rli N, Zhou D (2023) Teaching large language \nmodels to self-debug. arXiv: 2304. 05128 [cs.CL]\nChristiano PF,Â Leike J, Brown MÂ Martic T,Â Legg S,Â Amodei D (2017) \nDeep reinforcement learning from human preferences. Adv Neural \nInf Process Syst\nDeng M,Â Wang J, Hsieh CP, Wang Y,Â Guo H,Â Shu T,Â Song M,Â Xing \nE,Â Hu Z (2022) Rlprompt: optimizing discrete text prompts with \nreinforcement learning. In: Proceedings of the 2022 conference \non empirical methods in natural language processing (EMNLP)\nDevlin J,Â Chang M,Â Lee K,Â Toutanova K (2019) Bert: pre-training \nof deep bidirectional transformers for language understanding. \nIn: Proceedings of the 2019 conference of the North American \nchapter of the association for computational linguistics: human \nlanguage technologies\nDong Q,Â Li L,Â Dai D,Â Zheng C,Â Wu Z,Â Chang B,Â Sun X,Â Xu J,Â Li \nL,Â Sui Z (2022) A survey on in-context learning. arXiv: 2301.  \n00234 [cs.CL]\nDung L (2024) The argument for near-term human disempowerment \nthrough ai. AI Soc\nElsken T, Metzen JH,Â Hutter F (2019) Neural architecture search: a \nsurvey. J Mach Learn Res\nErickson N,Â Mueller J,Â Shirkov A,Â Zhang H,Â Larroy P,Â Li M,Â Smola A \n(2020) Autogluon-tabular: robust and accurate automl for struc-\ntured data. arXiv: 2003. 06505 [stat.ML]\nFernando C, Banarse D,Â Michalewski H,Â Osindero S,Â RocktÃ¤schel \nT (2023) Promptbreeder: self-referential self-improvement via \nprompt evolution. arXiv: 2309. 16797 [cs.CL]\nFeurer M,Â Hutter F. (2019) Hyperparameter optimization. Autom Mach \nLearn\nFeurer M,Â Klein A,Â Eggensperger K,Â Springenberg J,Â Blum M,Â Hutter \nF (2015) Efficient and robust automated machine learning. Adv \nNeural Inf Process Syst 28 (NIPS 2015)\nFeurer M,Â Klein A,Â Eggensperger K, Springenberg JT,Â Blum M,Â Hutter \nF (2019) Auto-sklearn: efficient and robust automated machine \nlearning. Autom Mach Learn\nFeurer M,Â Eggensperger K,Â Falkner S,Â Lindauer M,Â Hutter F (2022) \nAuto-sklearn 2.0: hands-free automl via meta-learning. J Mach \nLearn Res 23(2022)\nFrÃ¶ding B,Â Peterson M (2020) Friendly ai. Ethics Inf Technol\nGalaviz CV,Â Martin K (2023) Moral distance, ai, and the ethics of \ncare. AI Soc\nGoertzel B (2014) Artificial general intelligence: concept, state of the \nart, and future prospects. J Artif Gen Intell\nGood IJ (1959) Speculations on perceptrons and other automata. \nRC115\n AI & SOCIETY\nGood IJ (1965) Speculations concerning the first ultraintelligent \nmachine. Adv Comput\nGood IJ (1970) Some future social repercussions of computers. Int J \nEnviron Stud\nGuo H, Cheng HK,Â Kelley K (2016) Impact of network structure on \nmalware propagation: a growth curve perspective. J Manag Inf \nSyst 33\nHadlington L, Murray MK,Â Slater J,Â Binder J,Â Gardner S,Â Knight S \n(2024) Public perceptions of the use of artificial intelligence in \ndefence: a qualitative exploration. AI Soc\nHedlund M,Â Persson E (2022) Expert responsibility in ai development. \nAI Soc\nHirvonen H (2023) Just accountability structuresâ€”a way to promote \nthe safe use of automated decision-making in the public sector. \nAI Soc\nHonovich O,Â Scialom T,Â Levy O,Â Schick T (2023) Unnatural instruc-\ntions: tuning language models with (almost) no human labor. In: \nProceedings of the 61st annual meeting of the association for com-\nputational linguistics (volume 1: long papers)\nHuang J,Â Gu S,Â Hou L,Â Wu Y,Â Wang X,Â Yu H,Â Han J (2023) Large \nlanguage models can self-improve. In: Proceedings of the 2023 \nconference on empirical methods in natural language processing\nHutter F, Kotthoff L, Vanschoren J (2019) Automated machine learn-\ning: methods, systems, challenges. Springer, Berlin\nHutter M (2010) Can intelligence explode? J Conscious Stud\nSutskever I, Levy D,Â Daniel G (2024) Superintelligence is within reach. \nhttps:// ssi. inc\nIshizaki R,Â Sugiyama M (2024) Self-adversarial surveillance for super-\nalignment. philpapers.org/rec/ISHSSF-2\nKojima T,Â Gu S,Â Reid M,Â Matsuo Y,Â Iwasawa Y (2022) Large language \nmodels are zero-shot reasoners. Adv Neural Inf Process Syst 35 \n(NeurIPS 2022)\nKotthoff L,Â Thornton C, Hoos HH,Â Hutter F,Â Leyton-Brown K (2017) \nAuto-weka 2.0: automatic model selection and hyperparameter \noptimization in weka. J Mach Learn Res\nKotthoff L,Â Thornton C, Hoos HH,Â Hutter F,Â Leyton-Brown K (2019) \nAuto-weka: automatic model selection and hyperparameter opti-\nmization in weka. Autom Mach Learn\nKurzweil R (2005) The singularity is near: when humans transcend \nbiology. Viking Press, New York\nLecun Y (2023) On the highway towards human-level ai, large lan-\nguage model is an off-ramp. https://x. com/ ylecun/ status/ 16218 \n05604 90058 5472\nLegg S,Â Hutter M (2007) A collection of definitions of intelligence. In: \nAdvances in artificial general intelligence: concepts, architectures \nand algorithms-proceedings of the AGI workshop 2006\nLi X,Â Yu P,Â Zhou C,Â Schick T,Â Levy O,Â Zettlemoyer L,Â Weston J,Â Lewis \nM (2023a) Self-alignment with instruction backtranslation. arXiv: \n2308. 06259 [cs.CL]\nLi X,Â Zhang T,Â Dubois Y,Â Taori R,Â Gulrajani I,Â Guestrin C,Â Liang P, \nHashimoto TB (2023b) Alpacaeval: an automatic evaluator of \ninstruction-following models. GitHub repository. https:// github.  \ncom/ tatsu- lab/ alpaca_ eval\nMalthus T (1798) An essay on the principle of population\nMinsky M (1966) Artificial intelligence. Scientific American\nMorris MR,Â Sohl-dickstein J,Â Fiedel N,Â Warkentin T,Â Dafoe A,Â Faust \nA,Â Farabet C,Â Legg S (2023) Levels of agi: operationalizing pro-\ngress on the path to agi. arXiv: 2311. 02462 [cs.AI]\nMuehlhauser L, Salamon A (2012) Intelligence explosion: evidence \nand import. A scientific and philosophical assessment, singular -\nity hypotheses\nNeumann J (1966) Theory of self-reproducing automata. University of \nIllinois Press, Champaign\nNilsson NJ (2009) The quest for artificial intelligence. Cambridge Uni-\nversity Press, Cambridge\nOmohundro S (2007) The nature of self-improving artificial intelli-\ngence. Singul Summit\nOmohundro S (2008) The basic ai drives. In: Proceedings of the 2008 \nconference on artificial general intelligence 2008: proceedings of \nthe first AGI conference\nOmohundro S (2013) Rational artificial intelligence for the greater \ngood. Singul Hypotheses\nPrasad A,Â Hase P,Â Zhou X,Â Bansal M (2023) Grips: gradient-free, edit-\nbased instruction search for prompting large language models. In: \nProceedings of the 17th conference of the European chapter of the \nassociation for computational linguistics\nRadford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) \nLanguage models are unsupervised multitask learners. Technical \nreport, OpenAI\nRafailov R,Â Sharma A,Â Mitchell E,Â Ermon S, Manning CD,Â Finn C \n(2023) Direct preference optimization: Your language model is \nsecretly a reward model. arXiv: 2305. 18290 [cs.LG]\nRatner HF, Thylstrup NB (2024) Citizensâ€™ data afterlives: practices of \ndataset inclusion in machine learning for public welfare. AI Soc\nRosenblatt F (1958) The perceptron: a probabilistic model for informa-\ntion storage and organization in the brain. Psychol Rev\nRovetto RJ (2023) The ethics of conceptual, ontological, semantic and \nknowledge modeling. AI Soc\nRoziÃ¨re B etÂ al. (2023) Code llama: open foundation models for code. \narXiv: 2308. 12950 [cs.CL]\nRumelhart DE, Hinton GE, Williams RJ (1986) The perceptron: a \nprobabilistic model for information storage and organization in \nthe brain. Nature\nSandberg A,Â Bostrom N (2010) Whole brain emulation: a roadmap. \nTechnical report, Future of Humanity Institute, University of \nOxford\nSchick T, Yu JD,Â  DessÃ¬ R,Â  Raileanu R,Â  Lomeli M,Â  Zettlemoyer \nL,Â Cancedda N,Â Scialom T (2023) Toolformer: language models \ncan teach themselves to use tools. arXiv: 2302. 04761 [cs.CL]\nSchmidhuber J (2007) GÃ¶del machines: fully self-referential optimal \nuniversal self-improvers. Artif Gen Intell\nSchuett J (2023) Three lines of defense against risks from ai. AI Soc\nSeidl I, Tisdell CA (1999) Carrying capacity reconsidered: from mal-\nthusâ€™ population theory to cultural carrying capacity. Ecol Econ\nSerazzi G,Â Zanero S (2003) Computer virus propagation models. In: \nInternational workshop on modeling, analysis, and simulation of \ncomputer and telecommunication systems (MASCOTS 2003)\nShin T,Â Razeghi Y,Â Logan IV RL,Â Wallace E,Â Singh S (2020) Auto-\nprompt: eliciting knowledge from language models with automati-\ncally generated prompts. In: Proceedings of the 2020 conference \non empirical methods in natural language processing (EMNLP)\nShulman C,Â Bostrom N (2012) How hard is artificial intelligence? \nEvolutionary arguments and selection effects. J Conscious Stud\nShypula A,Â Madaan A,Â Zeng Y,Â Alon U,Â Gardner J,Â Hashemi M,Â Neubig \nG,Â Ranganathan P,Â Bastani O,Â Yazdanbakhsh A (2023) Learning \nperformance-improving code edits. arXiv: 2302. 07867 [cs.SE]\nStiennon N,Â Ouyang L,Â Wu J,Â Ziegler D,Â Lowe R,Â Voss C,Â Radford \nA,Â Amodei D, Christiano PF (2020) Learning to summarize with \nhuman feedback. Adv Neural Inf Process Syst 33 (NeurIPS 2020)\nSun Z,Â Shen Y,Â Zhou Q,Â Zhang H,Â Chen Z,Â Cox D,Â Yang Y,Â Gan C \n(2023) Principle-driven self-alignment of language models from \nscratch with minimal human supervision. arXiv: 2305. 03047 [cs.\nLG]\nThornton C,Â Hutter F, Hoos HH,Â Leyton-Brown K (2012) Auto-weka: \ncombined selection and hyperparameter optimization of classifica-\ntion algorithms. arXiv: 1208. 3719 [cs.LG]\nTornede A,Â Deng D,Â Eimer T,Â Giovanelli J,Â Mohan A,Â Ruhkopf T,Â Segel \nS,Â Theodorakopoulos D,Â Tornede T,Â Wachsmuth H,Â Lindauer M \n(2023) Automl in the age of large language models: current chal-\nlenges, future opportunities and risks. arXiv: 2306. 08107 [cs.LG]\nAI & SOCIETY \nTouvron H etÂ al (2023) Llama: Open and efficient foundation language \nmodels. Technical report, Meta\nTuring A (1950) Computing machinery and intelligence. Mind\nTuring A (1951) Intelligent machinery, a heretical theory. A lecture \ngiven to â€™51 Societyâ€™ at Manchester\nVaswani A,Â Shazeer N,Â Parmar N,Â Uszkoreit J,Â Jones L, Gomez \nAN,Â Kaiser L,Â Polosukhin I (2017) Attention is all you need. Adv \nNeural Inf Process Syst\nVerhulst PF (1838) Notice sur la loi que la population suit dans son \naccroissement. correspondance mathematique et physique publiee \npar a. Quetelet\nVinge V (1998) The coming technological singularity: how to survive \nin the post-human era. In: 21: interdisciplinary science and engi-\nneering in the era of cyberspace\nWang Y,Â Kordi Y,Â Mishra S,Â Liu A, Smith NA,Â Khashabi D,Â Hajishirzi \nH (2023) Self-instruct: Aligning language models with self-gen-\nerated instructions. In: Proceedings of the 61st annual meeting \nof the association for computational linguistics (volume 1: long \npapers)\nWei J,Â Wang X,Â Schuurmans D,Â Bosma M,Â Ichter B,Â Xia F,Â Chi E, Le \nQV,Â Zhou D (2022) Chain-of-thought prompting elicits reason-\ning in large language models. Adv Neural Inf Process Syst 35 \n(NeurIPS 2022)\nWhite C,Â Safari M,Â Sukthanker R,Â Ru B,Â Elsken T,Â Zela A,Â Dey D,Â Hut-\nter F (2023) Neural architecture search: insights from 1000 papers. \narXiv: 2301. 08727 [cs.LG]\nWistuba M,Â Rawat A,Â Pedapati T (2019) A survey on neural architec-\nture search. arXiv: 1905. 01392 [cs.LG]\nYampolskiy RV (2015) From seed ai to technological singularity via \nrecursively self-improving software. arXiv: 1502. 06512 [cs.AI]\nYang C,Â Wang X,Â Lu Y,Â Liu H, Le QV,Â Zhou D,Â Chen X (2023a) Large \nlanguage models as optimizers. arXiv: 2309. 03409 [cs.LG]\nYang H,Â Yue S,Â He Y (2023b) Auto-gpt for online decision making: \nbenchmarks and additional opinions. arXiv: 2306. 02224 [cs.AI]\nYang Z,Â Dai Z,Â Yang Y,Â Carbonell J, Salakhutdinov RR, Le QV (2019) \nXlnet: generalized autoregressive pretraining for language under-\nstanding. Adv Neural Inf Process Syst 32\nYuan W, Pang RY,Â Cho K,Â Li X,Â Sukhbaatar S,Â Xu J,Â Weston J (2024) \nSelf-rewarding language models. arXiv: 2401. 10020 [cs.CL]\nYudkowsky E (2008a) Artificial intelligence as a positive and negative \nfactor in global risk. Glob Catastr Risks\nYudkowsky E (2008b) Efficient cross-domain optimization. http:// lessw \nrong. com/ lw/ vb/ effic ient_ cross domain_ optim izati on\nZelikman E,Â Wu Y,Â Mu J, Goodman ND (2020) Star: bootstrapping \nreasoning with reasoning. Adv Neural Inf Process Syst 35 (Neu-\nrIPS 2022)\nZelikman E,Â Lorch E,Â Mackey L, Kalai AT (2023) Self-taught opti-\nmizer (stop): Recursively self-improving code generation. arXiv: \n2310. 02304 [cs.CL]\nZhou Y, Muresanu AI,Â Han Z,Â Paster K,Â Pitis S,Â Chan H,Â Ba J (2023) \nLarge language models are human-level prompt engineers. In: \nInternational conference on learning representations (ICLR)\nZimmer L,Â Lindauer M,Â Hutter F (2021) Auto-pytorch: multi-fidelity \nmetalearning for efficient and robust autodl. IEEE Trans Pattern \nAnal Mach Intell\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Performing arts",
  "concepts": [
    {
      "name": "Performing arts",
      "score": 0.5318529009819031
    },
    {
      "name": "Singularity",
      "score": 0.4338931441307068
    },
    {
      "name": "Computer science",
      "score": 0.41852933168411255
    },
    {
      "name": "Mathematics",
      "score": 0.2682662010192871
    },
    {
      "name": "Art",
      "score": 0.1536811888217926
    },
    {
      "name": "Visual arts",
      "score": 0.1354268193244934
    },
    {
      "name": "Geometry",
      "score": 0.1176559329032898
    }
  ],
  "institutions": []
}