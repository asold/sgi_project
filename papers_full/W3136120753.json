{
  "title": "Continual Domain-Tuning for Pretrained Language Models",
  "url": "https://openalex.org/W3136120753",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225499556",
      "name": "Rongali, Subendhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226896120",
      "name": "Jagannatha, Abhyuday",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4280834971",
      "name": "Rawat, Bhanu Pratap Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978159643",
      "name": "Yu Hong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2888120268",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2922551710",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2111051539",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2962724315",
    "https://openalex.org/W2473930607",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2963850662",
    "https://openalex.org/W2120354757",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2891113091",
    "https://openalex.org/W2737492962",
    "https://openalex.org/W2036963181",
    "https://openalex.org/W2765101016",
    "https://openalex.org/W1840435438"
  ],
  "abstract": "Pre-trained language models (LM) such as BERT, DistilBERT, and RoBERTa can be tuned for different domains (domain-tuning) by continuing the pre-training phase on a new target domain corpus. This simple domain tuning (SDT) technique has been widely used to create domain-tuned models such as BioBERT, SciBERT and ClinicalBERT. However, during the pretraining phase on the target domain, the LM models may catastrophically forget the patterns learned from their source domain. In this work, we study the effects of catastrophic forgetting on domain-tuned LM models and investigate methods that mitigate its negative effects. We propose continual learning (CL) based alternatives for SDT, that aim to reduce catastrophic forgetting. We show that these methods may increase the performance of LM models on downstream target domain tasks. Additionally, we also show that constraining the LM model from forgetting the source domain leads to downstream task models that are more robust to domain shifts. We analyze the computational cost of using our proposed CL methods and provide recommendations for computationally lightweight and effective CL domain-tuning procedures.",
  "full_text": "Continual Domain-Tuning for Pretrained Language Models\nSubendhu Rongali∗, Abhyuday Jagannatha ∗, Bhanu Pratap Singh Rawat ∗, Hong Yu\nUniversity of Massachusetts Amherst\n{srongali, abhyuday, brawat, hongyu}@cs.umass.edu\nAbstract\nPre-trained language models (LM) such as\nBERT , DistilBERT , and RoBERT a can be tuned\nfor different domains (domain-tuning) by con-\ntinuing the pre-training phase on a new tar-\nget domain corpus. This simple domain tun-\ning (SDT) technique has been widely used to\ncreate domain-tuned models such as BioBERT ,\nSciBERT and ClinicalBERT . However, during\nthe pretraining phase on the target domain, the\nLM models may catastrophically forget the\npatterns learned from their source domain. In\nthis work, we study the effects of catastrophic\nforgetting on domain-tuned LM models and\ninvestigate methods that mitigate its negative\neffects. We propose continual learning (CL)\nbased alternatives for SDT, that aim to reduce\ncatastrophic forgetting. We show that these\nmethods may increase the performance of LM\nmodels on downstream target domain tasks.\nAdditionally, we also show that constraining\nthe LM model from forgetting the source do-\nmain leads to downstream task models that\nare more robust to domain shifts. We analyze\nthe computational cost of using our proposed\nCL methods and provide recommendations for\ncomputationally lightweight and effective CL\ndomain-tuning procedures.\n1 Introduction\nRecently proposed pre-trained contextual word\nembedding and language models (LM) such as\nBERT (Devlin et al., 2018), ELM o (Peters et al.,\n2018), GPT (Radford et al., 2018), XLN et (Yang\net al., 2019) RoBERT a (Liu et al., 2019), DistilBERT\n(Sanh et al., 2019) and ALBERT (Lan et al., 2019)\nare widely used in natural language processing\ntasks. These LM models use unsupervised pre-\ntraining to train RNN or Transformer based neural\nnetwork models on large unsupervised text corpora\nlike WikiText and Gigaword (Devlin et al., 2018;\n∗Equal contribution\nYang et al., 2019). Simply replacing the hidden\nlayers in an existing neural architecture by a pre-\ntrained LM model and ﬁne-tuning the model on\na supervised task leads to large performance im-\nprovements 1.\nThe corpora that LM models are typically trained\non consist of text from multiple topics such as news,\nﬁnance, law, sciences (Devlin et al., 2018; Peters\net al., 2018). Due to their success in NLP applica-\ntions and their ease of use, researchers have worked\non adapting these models for speciﬁc downstream\ndomains. We refer to the initial domain as the\nsource domain and the downstream domain as the\ntarget domain. Here, adaptation works as a sim-\nple domain transfer technique (SDT) by using the\npretrained LM model weights as initialization and\ncontinuing the pretraining on a large target domain\ntext corpus. The target domain LM model is then\nused as initialization for supervised ﬁnetuning on\ntarget domain downstream tasks. This process has\nbeen followed in the biomedical, clinical, and scien-\ntiﬁc domains to produce BioBERT (Lee et al., 2019),\nClinicalBERT (Alsentzer et al., 2019) and SciBERT\n(Beltagy et al., 2019) models. For this paper, we\ncall the pretraining on source domain as “pretrain-\ning”, the continued pretraining on target domain\ncorpora as “domain-tuning”, and the ﬁnal super-\nvised training on the downstream domain-speciﬁc\ntask as “ﬁnetuning”.\nDomain-tuning through SDT can cause models\nto catastrophically forget (Kirkpatrick et al., 2017;\nMcCloskey and Cohen, 1989) the learned source\ndomain information. Continual learning (CL) tech-\nniques aim to reduce catastrophic forgetting in\nmodels that are sequentially trained on an array\nof tasks. Reducing catastrophic forgetting may re-\nsult in positive forward transfer (PFT) and positive\n1LM model perfromances can be seen at GLUE and Su-\nperGLUE leader-boards. https://super.gluebenchmark.com/,\nhttps://gluebenchmark.com/\narXiv:2004.02288v2  [cs.CL]  19 Mar 2021\nbackward transfer (PBT) as demonstrated by CL\nmethods like Kirkpatrick et al. (2017); Lopez-Paz\nand Ranzato (2017). Therefore, we study the ef-\nfects of these CL methods on the domain-tuning\nprocedure for LM models like BERT . In CL, PFT\nhappens when a previous task improves the perfor-\nmance of a later task in the sequence. PBT happens\nif a later task improves the performance of a previ-\nous task in the sequence.\nHowever, CL concepts like PBT and PFT may\nnot be directly applicable to LM domain-tuning.\nThe domain-tuning task sequence only contains\ntwo tasks, the self-supervised pretraining tasks on\nthe source and the target domain corpora. Addition-\nally, the evaluation of an LM model is not based on\nits self-supervised loss. Instead, it is based on how\n“useful” that LM model is for a downstream super-\nvised task. So in this context of domain-tuning, we\ndeﬁne forward transfer as the effect of source do-\nmain pretraining on tasks in the target domain, and\nbackward transfer as the effect of target domain-\ntuning on tasks in the source domain. We hypothe-\nsize that retaining more source domain information\nduring domain-tuning may help the LM produce\nimproved representations for both target and source\ndomain texts.\nTo exhaustively examine the effect of contin-\nual learning methods on LM domain-tuning pro-\ncedure, we evaluate ﬁve different commonly used\nCL methods. We design an extensive experimental\nsetup to test the PFT and PBT capabilities of these\nCL domain-tuning methods on three commonly\nused LM models: BERT , RoBERT a, and DistilBERT .\nWe use extractive question answering and natural\nlanguage inference as our supervised downstream\ntasks.\nOur main ﬁndings are as follows:\n• Different CL methods have varying advan-\ntages and disadvantages for domain-tuning\ndepending on model size and transfer direc-\ntion. We use our experiments to identify and\nrecommend CL methods that adapt well to the\ntask of domain-tuning.\n• LM models outputs for our recommended CL\nmethods like Elastic Weight Consolidation\n(Kirkpatrick et al., 2017) contain more infor-\nmation about downstream task output random\nvariables compared to baseline SDT.\n• Finetuned downstream task models that are\ninitialized with our recommended CL tuned\nmodels are more robust to target−→source\ndomain shifts.\nFinally, we analyze various CL based domain-\ntuning methods in the context of their computa-\ntional cost and discuss their practical beneﬁts.\n2 Background and Methods\nLM models such as BERT , are pretrained on large\nunsupervised text corpora such as WikiText that\ncontain text from multiple domains. We assume\nthat the text corpora used for pretraining is the\nsource corpus. Source is the ﬁrst domain to be\ntrained, therefore all CL methods are reduced to\nstandard pretraining. As a result, we can use the\npublished BERT , DistilBERT and RoBERT a models\nas models that are pretrained on our source corpus\nfor SDT and all CL methods. We deﬁne the target\ndomain and downstream tasks for both source and\ntarget domains in the next section.\nNeural network LM models use self-supervised\nobjectives such as masked language modeling\n(MLM) loss for training. We abstract out the model-\nspeciﬁc details and refer to all model losses as\nℓdomain(D,θ). Here θdenotes the parameters of\nthe neural network and Dis the unlabeled training\ndataset. We denote the source domain dataset as\nDs, and the target domain dataset as Dt. An LM\nmodel pretrained on Ds with loss ℓs is domain-\ntuned on Dt with loss ℓt to obtain a target domain\nLM model. This model can then be used for super-\nvised ﬁnetuning on a target domain downstream\ntask.\nSeveral of our proposed CL based methods use\nan additional regularization objective Ralong with\nthe domain-tuning loss ℓt. The domain-tuned\nmodel is obtained by solving the following uncon-\nstrained optimization\nargmin\nθ\n(ℓt + λR). (1)\nBioBERT and other domain-tuned LM models were\ntrained using SDT. Therefore we use SDT as our\nbaseline. SDT’s domain-tuning phase only uses the\nloss ℓt. We describe four proposed CL methods in\nthis section. We also experimented with a distil-\nlation based CL method, however due to its high\ncomputational cost we only describe and report its\nresults in Appendices A.1 and E.\n2.1 Rehearsal (RH)\nLRH = ℓt + λℓs (2)\nRehearsal scheme (Ratcliff, 1990) avoids catas-\ntrophic forgetting by using a few examples from\nDs during domain-tuning on Dt. The regulariza-\ntion term in this loss is RRH = λ×ℓs to ensure\nthat loss on Ds does not increase signiﬁcantly. A\nsmall subset of Ds data is used for this regulariza-\ntion. This scheme can be interpreted as a multi-task\nlearning model with a smaller weight associated\nwith Ds.\n2.2 L2 Regularization (L2)\nLL2 = ℓt + λ\n2\n∑\ni\n(θi −θ∗\ns,i)2 (3)\nThis scheme (Daum ´e III, 2007) uses the L2 dis-\ntance between current parameter values, θ, and the\nparameter values of the pre-trained source domain\nmodel, θ∗\ns, as its CL regularization. It imposes an\nisotropic Gaussian prior on the parameters with\nσ2 = 1and µ= θ∗\ns.\n2.3 Elastic Weight Consolidation (EWC)\nLEWC = ℓt + λ\n2\n∑\ni\nFi(θi −θ∗\ns,i)2 (4)\nEWC (Kirkpatrick et al., 2017) also imposes a\nGaussian prior over the parameters with µ = θ∗\ns\nlike L2. However, it uses a diagonal covariance\nF. F here is the diagonal of the Fisher informa-\ntion matrix for the source domain loss ℓs. There-\nfore, the regularization term in EWC is a weighted\nL2 distance between parameter value at any given\nSGD step θand the initial parameter value θ∗\ns. The\nweight for the ith parameter is given by Fi, the ith\nelement in the diagonal. The term F stops param-\neters that are important for the previous task Ds\nfrom changing too much during EWC training. A\nmore detailed explanation of EWC is provided in\nAppendix C.1.\n2.4 Gradient Episodic Memory (GEM)\nGEM does not use a regularization objective as\nshown in Equation 1. Instead, it poses continual\nlearning as a constrained optimization problem. In\nour context, the problem aims to minimize ℓt sub-\nject to ℓs(Ds,θ) ≤ℓs(Ds; θ∗). Recall that ℓs and\nℓt refer to losses on the source and target domain\ncorpora. GEM solves this constraint optimization\nproblem by making local corrections to gradients\nat each SGD step.\nGEM assumes that the loss function is locally lin-\near at each SGD step. As a result, we can rephrase\nthe previous constrained optimization problem as\nthat of ﬁnding a gradient g, which is very close to\nthe target domain gradient ∂ℓt\n∂θ and does not have\nany negative component in the direction of source\ndomain gradient ∂ℓs\n∂θ . Since we only have one con-\nstraint in our formulation, we can analytically solve\nfor ginstead of using a quadratic program solver\nas suggested by GEM. Details about the solution\nare provided in Appendix C.2.\n3 Experimental Design\nWe collect source domain and target domain cor-\npora based on related efforts in literature. We use\nthe WikiText corpus, which is a large common part\nof the datasets used for pretrainingBERT , RoBERT a\nand DistilBERT , as our source domain unlabeled\ncorpus Ds. We perform our experiments over three\ndifferent variants of transformer (Vaswani et al.,\n2017) based language models: BERT (Devlin et al.,\n2018), RoBERT a (Liu et al., 2019) and DistilBERT\n(Sanh et al., 2019).\n3.1 Target domain unlabeled corpora\nOur selection of target domains was dictated by\nthe availability of the unlabeled text corpus and\nstandard downstream tasks for that domain. For our\nprimary analysis, we construct a target domain text\ncorpora containing various biomedical and clinical\ntext documents. We also evaluate our methods\non a smaller clinical only target domain corpus\nand the results on this second corpus along with\nfurther details about corpus selection are included\nin Appendix D and E.\nThe bio-medical text is obtained by crawling\nthrough a snapshot of all the abstracts of papers\nmade available by PubMed Central (PMC) 2. For\nthe clinical text, we extract all the clinical notes\nfrom the MIMIC-III dataset (Johnson et al., 2016),\nwhich consists of electronic health records of pa-\ntients who stayed within the intensive care units at\nBeth Israel Deaconess Medical Center. This cor-\npus is identical to the one used by Alsentzer et al.\n(2019) and a superset of Lee et al. (2019).\n3.2 Domain Tuning Details\nFor evaluating all our approaches, we perform\ndomain-tuning by allowing one full iteration over\nthe target domain corpus 3. Further experimental\ndetails are provided in Appendix B.\n2https://www.ncbi.nlm.nih.gov/pmc/\n3Code and trained model parameters available at\nhttps://anonymized\n3.3 Downstream Tasks\nWe use two widely used NLP tasks, Question-\nAnswering (QA) and Natural Language Inference\n(NLI) as our downstream tasks. These tasks have\ndomain speciﬁc datasets for both source and target\ndomains. For the source domain, we pick SQuAD\n2.0 (Rajpurkar et al., 2018) and SNLI (Bowman\net al., 2015) datasets. For the target domain, we\nuse EMRQA (Pampari et al., 2018) 4 and MedNLI\n(Romanov and Shivade, 2018). We run each down-\nstream experiment three times and report the mean\nvalues in our tables. Tables with standard devia-\ntions and relevant implementation details are pro-\nvided in the Appendices B, D and E.\n3.4 Experiments for Forward Transfer\nWe investigate our hypothesis forpositive forward\ntransfer, that retaining source domain information\nduring domain-tuning may result in “better adapted”\nLM models for downstream target domain tasks, in\ntwo ways: 1) We use the target domain-tuned mod-\nels as initialization for ﬁnetuning on target domain\ndownstream tasks. An improved test performance\nof the trained downstream task can indicate bet-\nter adaptation. 2) We examine the hidden layer\nand output layer representations of the CL domain-\ntuned models. This may also provide indications of\nbetter adaptation. Speciﬁcally, we examine the mu-\ntual information between the model’s output and\noutput random variable of the downstream task.\n3.5 Experiments for Backward Transfer\nWe design two experiments to evaluate positive\nbackward transfer. The ﬁrst experiment uses the\ntarget domain-tuned model as initialization for\ntraining downstream source domain tasks. Addi-\ntionally, we also hypothesize that retaining source\ndomain information may result in downstream\ntarget domain models that are more robust to\ntarget−→source domain shifts. To test this hy-\npothesis, we study the performance of EMRQA\nand MedNLI ﬁnetuned models on source domain\nSQuAD and SNLI tasks, respectively.\n4 Experiments and Results\n4.1 Forward Transfer\nDownstream Performance We ﬁnetuned all\ndomain-tuned LM models on EMRQA and\nMedNLI tasks for the target domain. The results\n4The emrQA dataset was transformed into SQuAD-style\nand examples which could not be transformed were removed.\nfor target domain downstream performances are\npresent in right half of Table 1. We see that in\nseveral cases, the best performing CL model out-\nperforms the baseline SDT model. RH appears to\nbe the best performing continual learning model\nin terms of downstream performance for target do-\nmain tasks. GEM and EWC appear to be the next\nbest performing CL models.\nThe high performance of RH model can be ex-\nplained by its use of both Ds and Dt losses in its\nobjective. RH is effectively a multi-task learning\napproach, that is simultaneously trained on both\nsource and target domain samples during domain-\ntuning. Unlike EWC or L2, it does not introduce\nexplicit model parameter constraints, and there-\nfore it does not suffer from the problem of over-\nconstraining the model. As a result, it has bet-\nter source domain regularization. EWC and GEM\nshow mostly competitive or improved performance\nas compared to SDT, except for a few instances.\nEWC tuned models show signiﬁcant regressions in\nDistilBERT . We believe this is due to the smaller\nsize of DistilBERT compared to other models. EWC\nis known to over-constrain the parameters in a con-\ntinual learning setup, preventing the network from\nlearning new information. This aspect may more\nseverely affect networks with smaller number of\nparameters. Therefore EWC performance is lower\nin DistilBERT models. L2, which over-constrains\nthe parameters even more than EWC, leads to a\nlarger decrease in downstream performance.\nDownstream Performance Analysis on Varying\nData-set Lengths We see a more pronounced\ntrend of model difference when we study the down-\nstream results with varying labeled training data\nsizes in Figure 1. We use MedNLI for the varying\ndataset size experiment. Each domain-tuned model\nis ﬁnetuned on 25%, 50%, 75% and 100% of the\nMedNLI task training set. In the case of BERT\nmodels, EWC and RH remain competitive to or\nare better than SDT’s performance. We see similar\nbehavior for RoBERT a. Due to the aforementioned\nover-constraining issue in EWC and L2, we see\nreduced performances from them for Distil BERT .\nGEM and RH are still competitive for DistilBERT .\nAnalysis of layer outputs in LM models To fur-\nther analyze CL domain-tuned models for the target\ndomain, we analyse the quality of each model’s hid-\nden representation in the context of downstream\ntarget domain task. We use MedNLI dataset for\nDownstream tasks related toSourcedomain Downstream tasks related toTargetdomain\nSQuAD 2.0 SNLI EMRQA MedNLI\nEM F-score Accuracy EM F-score Accuracy\nBERT\nNDT 71.78 75.50 90.45 74.86 80.43 78.49\nSDT 71.25 74.63 88.45 76.3 81.86 79.92\nRH 70.76(-0.49) 74.41(-0.22) 89.17(0.72) 76.26( -0.04) 81.86(0) 80.46(0.54)\nEWC 71.11(-0.14) 74.81(0.18) 90.14(1.69) 75.8(-0.5) 81.29(-0.57) 80.93(1.01)\nGEM 68.88(-2.37) 72.43 (-2.2) 88.37 (-0.08) 75.9(-0.4) 81.71 (-0.15) 78.47 (-1.45)\nL2 72.18(0.93) 75.86(1.23) 90.4(1.95) 75.76(-0.54) 80.96(-0.9) 78.14 (-1.78)\nRoBERTa\nNDT 78.16 81.79 91.38 74.57 80.45 81.04\nSDT 72.61 76.34 89.59 74.24 80.09 83.98\nRH 75.61(3.00) 79.31(2.97) 90.75(1.16) 74.93(0.69) 80.7(0.61) 85.43 (1.45)\nEWC 77.67(5.06) 81.44(5.1) 91.3(1.71) 74.98 (0.74) 80.81(0.72) 83.65 (-0.33)\nGEM 72.75(0.14) 76.57(0.23) 89.87(0.28) 74.27 (0.03) 80.11(0.02) 84.43 (0.45)\nL2 78.25(5.64) 81.84(5.5) 91.44(1.85) 74.68 (0.44) 80.47 (0.38) 80.44 (-3.54)\nDistilBERT\nNDT 65.47 69.18 89.69 73.56 79.3 76.07\nSDT 64.88 68.4 87.25 74.21 80.26 78.27\nRH 65.14 (0.26) 68.89 (0.49) 88.87 (1.62) 74.35 (0.14) 80.2 (-0.06) 78.46 (0.19)\nEWC 65.22(0.34) 69.01(0.61) 89.47(2.22) 73.71(-0.5) 79.5(-0.76) 76.26(-2.01)\nGEM 64.17(-0.71) 67.78(-0.62) 87.98(0.73) 74.26(0.05) 80.05(-0.21) 78.58(0.31)\nL2 65.27(0.39) 68.92(0.52) 89.67(2.42) 73.57(-0.64) 79.38(-0.88) 75.81(-2.46)\nTable 1: Downstream task results from our experiments across all models and techniques for target domain. LM\nmodels are pretrained on source domain, CL domain-tuned on target domain and ﬁnetuned on each downstream\ntask to obtain the results. All results are averaged over three runs. NDT refers to the base model that is not domain\ntuned.\nModel EWC GEM L2 RH\nBERT +1.71 +0.71 -1.71 -0.25\nDistilBERT +1.26 -0.50 -1.38 +0.25\nRoBERTa +1.46 +2.52 +1.14 +1.33\nAvg +1.48 +0.91 -0.65 +0.44\nTable 2: This table shows (I(y; φCL)−I(y; φSDT ))×\n102 for CL models. Values are averaged over three\nruns.\nthese sets of experiments. To estimate the quality\nof hidden representation, we use mutual informa-\ntion (MI) I(Y; φ(.)(X)) between a model’s hidden\nrepresentation φ(.)(X) and the output variable of\nMedNLI, Y . Table 2 shows the difference in mu-\ntual information I(Y; φ(.)(X)) −I(Y; φSDT(X))\nfor our proposed CL methods. Here φ(.)(X) de-\nnotes the concatenation of all hidden layer and ﬁnal\nlayer output representations of a domain-tuned (but\nnot ﬁne-tuned) model. Details of MI estimation are\nprovided in Appendix C.3.\nFrom Table 2, we see that hidden representations\nobtained from CL trained models like EWC, GEM,\nRH have higher mutual information with down-\nstream task outputs. This observation further sup-\nports our hypothesis that CL domain-tuning may\nresult in positive forward transfer. EWC model\nhas the highest mutual information amongst all CL\nmodels, followed by RH and GEM.\n4.2 Backward Transfer\nDownstream Task Performance The left half\nsection of Table 1 showssource task downstream\nresults for target domain-tuned models that were\nﬁnetuned on source domain downstream tasks. We\nsee that L2 outperforms all other models on source\ndomain tasks. This is mainly due to the heavy con-\nstraints that L2 puts on model parameters. These\nconstraints reduce the model adaptation on target\ndomain. As a result, L2 performs worst on target\ndomain and best on source domain. EWC, RH and\nGEM are the next best models in order.\nPerformance Under Domain Shift Ensuring\nthat LM models retain their knowledge about the\nsource domain may also contribute to making our\ndownstream task models more robust. For in-\nstance, a CL trained LM model that is ﬁnetuned\nfor MedNLI task may produce an NLI model that\nis more robust to domain shifts. This is especially\ntrue if the domain shift is towards the source do-\nmain. To test this hypothesis, we used models ﬁne-\ntuned on target domain downstream tasks (Right\nFigure 1: MedNLI test set accuracy (averaged over three runs) of models with varying training set size.\nModel EWC GEM L2 SDT RH\nB (BERT) 31.77/21.07 29.74/19.46 30.54/19.50 30.71/20.34 32.50/22.23\nR (RoBERTa) 33.08/22.21 27.50/17.07 34.97/23.80 25.00/15.17 30.07/19.49\nD (DistilBERT) 25.53/16.34 23.43/13.85 24.31/15.06 22.20/13.20 26.35/16.56\nAvg 30.13/19.87 26.89/16.79 29.94/19.45 25.97/16.24 29.64/19.42\nTable 3: F1-score and Exact Match (F1/EM) of all models (averaged over three runs) trained on EMRQA training\nset and evaluated on the answerable only validation set of SQuAD 2.0. All models trained using EWC and RH\nperform better than SDT.\nLM EWC GEM L2 SDT RH\nB 41.13 38.01 39.89 39.56 39.16\nR 51.18 42.29 43.28 45.05 51.75\nD 39.80 37.81 41.19 37.83 39.43\nAvg 44.03 39.37 41.45 40.81 43.45\nTable 4: Accuracy (averaged over three runs) of models\ntrained on MedNLI and evaluated on SNLI test set.\nhalf of Table 1) and evaluated them on the same\ntask’s source domain variant. So models trained\non EMRQA were evaluated on SQuAD, and those\ntrained on MedNLI were evaluated on SNLI. Ta-\nbles 3 and 4 show the results for these experiments.\nRH and EWC models show the best performance in\nthis evaluation. These models show large improve-\nments against the baseline SDT in both EMRQA\nand MedNLI tasks. Surprisingly, the L2 method\nthat has the strongest constraints against model\nadaptation does not have the best performance in\nthese experiments. GEM also has reduced perfor-\nmance compared to EWC and RH.\n4.3 Self-Supervised Loss Analysis\nFigures 2 and 3 show self-supervised loss (MLM\nloss) values during domain-tuning for target and\nFigure 2: Self-supervised loss ℓt on target domain val-\nidation set for BERT CL models. L2 model’s loss for\nBERT is very high, and is not included in the graph to\nshow higher y-axis resolution for other CL models.\nsource domains for BERT 5. Loss values were av-\neraged over 5 subsets of 50% randomly selected\nsamples from validation dataset. Source domain\n(Figure 3) loss values show that CL domain-tuning\nmethods preserve low loss values on the source\ndomain throughout the domain tuning process.\nFrom Figure 2, we see that GEM and RH are\nclose to, or have lower loss values that SDT on the\ntarget domain. EWC and L2 have higher loss val-\n5Figures for Ro BERT a and Distil BERT are provided in\nAppendix.\nFigure 3: Self-supervised loss ℓs on source domain val-\nidation set for BERT CL models.\nues compared to SDT, which seems to contradict\nthe forward transfer results. An explanation for\nthis may be the over-constrained last layer. Fast\nadaptation of the ﬁnal softmax layer is crucial in\nreducing the target MLM loss. Some continual\nlearning works use separate tasks-speciﬁc last layer\n(Li and Hoiem, 2017), but we keep it same to en-\nsure comparison with SDT. Both EWC and L2\nimpose constraints over model parameters which\nmay restrict the adaptation of last layer. EWC, in\nparticular, may suffer from slow adaptation of the\nlast layer, because last layer may have high empiri-\ncal Fisher estimates. As a result, the large precision\nmatrix entries for last layer may result in very slow\nadaptation of last layer, even when hidden layers\nof the model are rapidly adapting to the new target\ndomain. This may result in slow target domain\nself-supervised loss decrease even though the hid-\nden layers of the network are adapting well to the\nﬁnal domain. We examine the Fisher information\nestimates and ﬁnd that last layer parameters do\nhave higher constraints. For instance, BERT ﬁnal\nlayer Fisher estimates are two orders of magnitude\nhigher than those of the 12 transformer layer pa-\nrameters. In GEM and RH, CL constraints are not\nexplicitly enforced on the model parameters or the\nlast layer. Therefore, they do not suffer from this\nphenomenon. RH has the best decrease in target\nloss, which aligns with our forward transfer results.\n5 Discussions and Recommendations\nIn the previous sections, we use multiple experi-\nmental designs to isolate the effect of CL based\ndomain-tuning methods in the context of backward\nand forward transfer for LMs.\nBackward Transfer In our domain shift experi-\nments, we see strong evidence suggesting that back-\nward transfer may make downstream task mod-\nels more robust to target−→source domain shift.\nThey show one of the main practical beneﬁts of\nCL based domain tuning. Most real-world appli-\ncations have to deal with heterogeneous text con-\nsisting of sentence samples from multiple topics\nor sub-domains. For instance, narrative notes from\nphysicians, or medical question-answering chat-bot\nresponses may contain sentences that are outside\ntheir speciﬁc sub-domains. Since the source do-\nmain in most pretrained models is a collection of\ntexts from a diverse range of topics (such as Wiki-\nText or Common Crawl Corpus), our exhibited do-\nmain robustness may be useful in a variety of such\nreal-world applications.\nForward Transfer and Model Capacity Our\nforward transfer experiments show that models\nwith larger size like BERT and RoBERT a have the\ncapacity to effectively domain-tune on target do-\nmain text even when they are constrained by CL\nobjectives. Both BERT and Ro BERT a show per-\nformance similar to the SDT model on our down-\nstream tasks. For CL methods EWC, RH and GEM\nboth BERT and RoBERT a models either show im-\nprovement in performance or are close to SDT per-\nformance, while retaining source domain informa-\ntion. For smaller models such as Distil BERT , CL\nmethods like L2 and EWC prove to be overly tight\nconstraints. We see that RH and GEM, lead to\nimproved downstream task performance of Dis-\ntilBERT models. For both GEM and RH, Dis-\ntilBERT models are competitive to baseline SDT\nfor EMRQA and have higher than baseline perfor-\nmance on MedNLI. This is because RH and GEM\ndo not enforce an explicit constraint on model pa-\nrameter values and therefore do not over-constraint\nthe lower capacity DistilBERT model.\nComputational Complexity In terms of compu-\ntational complexity, our best model is L2. It only\nrequires an additional squared norm objective in the\nloss function and is therefore only marginally more\ncomputationally expensive for auto-differentiation\nlibraries like PyTorch. EWC has similar computa-\ntional complexity as L2, with the added one-time\noverhead of estimating diagonal entries of the em-\npirical Fisher matrix. We estimate these entries us-\ning a small section of the source WikiText dataset.\nSince the Fisher matrix is only computed once dur-\ning the entire domain-tuning run, EWC only in-\ncreases the computational cost by a small fraction.\nBoth L2 and EWC methods are almost similar to\nSDT in terms of computational complexity.\nRH requires two mini-batch computations for ev-\nery SGD iteration and is therefore almost twice as\ncomputationally expensive as SDT. GEM, which\ncomputes gradients for the source domain at ev-\nery SGD mini-batch, also has higher memory re-\nquirements. In our experiments, we use the same\nmini-batch size for source domain and target do-\nmain. As a result, in our setup, RH and GEM are\ntwice as expensive as SDT, EWC and L2. Reduc-\ning the batch size forsource domain will reduce the\ncomputational cost at the expense of more noisy\nmini-batch gradient estimates. Distillation (Hinton\net al., 2015) CL runs require three times the com-\nputational cost as SDT, and therefore we do not\nuse it in our main results. Further explanation is\nprovided in Appendices A.1 and E.\nRecommendations We have shown that RH and\nEWC domain-tuning methods result in the best\nbackward and forward transfer performance for\nlarger capacity models like BERT and RoBERT a.\nRH has better forward transfer results on average,\nand EWC edges ahead in the backward transfer\nresults. One major deciding factor between the two\nis the signiﬁcantly lower computational complexity\nof EWC. As a result, we recommend using EWC\nfor domain-tuning. It leads to better latent repre-\nsentation and more robust downstream models as\ncompared to SDT. It also leads to competitive or\nbetter forward transfer performance as compared\nto SDT. EWC does not show good performance on\na smaller capacity model such as Distil BERT due\nto reasons discussed previously. GEM and RH are\nlikely candidates for CL based domain-tuning for\nDistilBERT . They both have better forward transfer\nthan EWC and are better or competitive as com-\npared to SDT results. However, these methods are\nmore computationally expensive than EWC.\nOpen Questions and future work. In this work,\nwe investigated continual learning in the context of\ndomain-tuning. Continual learning can be easily\nextended to include downstream target tasks. CL\nbased downstream task ﬁnetuning has the potential\nto improve both performance and robustness on\ndownstream tasks. The CL methods themselves\ncan also be modiﬁed to better accommodate the\nchallenges of LM domain-tuning. For instance,\nGEM source domain gradient constraints are cal-\nculated for each mini-batch. These estimates can\nhave very high variance in domain-tuning when the\ndomain corpus is very large. Efﬁcient variance re-\nduction schemes for gradients may improve GEM\ndomain-tuning performance.\n6 Related Work\nDomain-Speciﬁc Pretraining: The task of\nadapting pretrained language models for a speciﬁc\ndomain is popular in literature. BioBERT (Lee et al.,\n2019) and Clinical BERT (Alsentzer et al., 2019)\nwere built for the bio-medical domain from a base-\nline BERT model by ﬁnetuning on a biomedical cor-\npus consisting of PubMed articles and the MIMIC-\nIII dataset, just like ours. However, these models\nare ﬁnetuned by simply training further on new text.\nSciBERT (Beltagy et al., 2019) is also trained in this\nfashion but also has its own domain-speciﬁc vocab-\nulary. We train simple domain transfer models on\nour data as one of our baselines.\nContinual Learning: Several works in continual\nlearning focus on overcoming catastrophic forget-\nting when learning on new tasks. Methods like\nL2 (Daum ´e III, 2007), EWC (Kirkpatrick et al.,\n2017), Variational Continual Learning (Nguyen\net al., 2017), and Synaptic Intelligence (Zenke et al.,\n2017) use different regularization approaches to\nconstrain the training on new tasks. Methods like\nProgress and Compress (Schwarz et al., 2018) use\nmodiﬁcations to the neural network architecture\nto increase the capacity of the neural network for\na new task. We also explore GEM (Lopez-Paz\nand Ranzato, 2017), a CL approach that was pre-\nviously shown to outperform EWC on synthetic\nvision-based continual learning tasks.\n7 Conclusion\nWe have investigated the effects of catastrophic\nforgetting on domain-tuning of large pretrained\nLMs. We proposed CL based domain-tuning meth-\nods that constrain the LM from catastrophically\nforgetting the source domain while maintaining\ncompetitive or improved performance on target\ndownstream tasks. We show that our methods\ncreate downstream task models that are more ro-\nbust to domain shifts. In conclusion, we identify\nand recommend domain-tuning methods like EWC,\nthat have beneﬁcial forward and backward transfer\nproperties, while only marginally increasing the\ncomputational cost.\nReferences\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nIz Beltagy, Arman Cohan, and Kyle Lo. 2019. Scibert:\nPretrained contextualized embeddings for scientiﬁc\ntext. arXiv preprint arXiv:1903.10676.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\narXiv preprint arXiv:1508.05326.\nHal Daum´e III. 2007. Frustratingly easy domain adap-\ntation. In Proceedings of the 45th Annual Meeting of\nthe Association of Computational Linguistics, pages\n256–263, Prague, Czech Republic. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntiﬁc data, 3:160035.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2019. Biobert: pre-trained biomed-\nical language representation model for biomedical\ntext mining. arXiv preprint arXiv:1901.08746.\nZhizhong Li and Derek Hoiem. 2017. Learning with-\nout forgetting. IEEE transactions on pattern analy-\nsis and machine intelligence, 40(12):2935–2947.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDavid Lopez-Paz and Marc’Aurelio Ranzato. 2017.\nGradient episodic memory for continual learning. In\nAdvances in Neural Information Processing Systems,\npages 6467–6476.\nDavid JC MacKay. 1992. A practical bayesian frame-\nwork for backpropagation networks. Neural compu-\ntation, 4(3):448–472.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation, volume 24, pages 109–165. El-\nsevier.\nCuong V Nguyen, Yingzhen Li, Thang D Bui, and\nRichard E Turner. 2017. Variational continual learn-\ning. arXiv preprint arXiv:1710.10628.\nAnusri Pampari, Preethi Raghavan, Jennifer Liang, and\nJian Peng. 2018. emrqa: A large corpus for ques-\ntion answering on electronic medical records. arXiv\npreprint arXiv:1809.00732.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. arXiv preprint arXiv:1806.03822.\nRoger Ratcliff. 1990. Connectionist models of recog-\nnition memory: constraints imposed by learning\nand forgetting functions. Psychological review ,\n97(2):285.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from natural language inference in the clin-\nical domain. arXiv preprint arXiv:1808.06752.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nJonathan Schwarz, Jelena Luketina, Wojciech M Czar-\nnecki, Agnieszka Grabska-Barwinska, Yee Whye\nTeh, Razvan Pascanu, and Raia Hadsell. 2018.\nProgress & compress: A scalable framework for con-\ntinual learning. arXiv preprint arXiv:1805.06370.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nFriedemann Zenke, Ben Poole, and Surya Ganguli.\n2017. Continual learning through synaptic intelli-\ngence. In Proceedings of the 34th International\nConference on Machine Learning-Volume 70, pages\n3987–3995. JMLR. org.\nA Domain-Tuning\nA.1 Distillation\nDistillation is a technique used to distil the knowl-\nedge from one neural network to another (Hinton\net al., 2015). It was used in DistilBERT (Sanh et al.,\n2019) to train a smaller model by distilling the\nknowledge from a full BERT model into the smaller\nmodel. We use distillation as a domain adaptation\ntechnique by distilling the knowledge of the pre-\ntrained source domain model, denoted by Ms, into\nthe domain-tuned model, Mt, while domain-tuning\nwith the target domain corpus. To do this, we use\nthe distillation loss as our regularization term R.\nLDIS = ℓt + λ\n2 Ldistillation (5)\nIn neural networks with output classiﬁcation lay-\ners, the distillation loss corresponds to a cross en-\ntropy loss between the ﬁnal output logits of the\nmodel being trained and the source model who’s\nknowledge is being distilled into the model. In the\nscenario of domain tuning, the output logits corre-\nspond to the masked token prediction in the MLM\nobjective. So we want our domain-tuned model,\nMt’s logit distribution on a sample of source do-\nmain text, ds, to match that of the pre-trained\nsource domain model Ms. The distillation loss\nis hence given by\nLdistillation = CE(Ms(ds),Mt(ds)) (6)\nThis technique is computationally very expen-\nsive. For each loss computation, we have the tradi-\ntional forward pass of the current model on atarget\ndomain batch, and two additional forward passes,\none of the current model on a source domain batch,\nand one of the source domain pre-trained model\non the source domain batch. Because of this huge\ncost, we leave this method out of our main paper.\nB Implementation Details\nB.1 Domain-tuning\nWe used standard values from the huggingface li-\nbrary (Wolf et al., 2019) for hyper-parameters such\nas learning rate, warm-up steps etc. for each of\nour three BERT variants. Due to the breadth of our\nexperiments and associated computational costs,\nwe were unable to perform hyper-parameter tun-\ning. The CL approaches all have varying memory\nfootprints but we ﬁxed the batch-size to 8 (7 for\nclinical-only domain-tuning dataset), with each in-\nstance in the batch containing 512 tokens, to ensure\na fair comparison across them.\nThere were also a fair amount of CL method-\nspeciﬁc hyper-parameters and we chose the recom-\nmended values from the original implementations\nof these methods wherever possible. Here, we note\nthe values we changed. The loss multiplier in EWC\nwas set to 1e-4 and that of L2 was set to 1. The\nGEM updates were performed in an online fash-\nion as described in Lopez-Paz and Ranzato (2017).\nFor rehearsal and distillation, we need access to\ndata from the source domain, so we sample an-\nother mini-batch of 512-token instances from the\nWikiText corpus. We use a multiplier of 0.1 on the\nsource batch loss in these two approaches.\nTo train models using Masked LM (MLM) ob-\njective, we create instances containing contiguous\nspans for text from the text corpus and mask a small\nrandom percentage of words in them. Following\nBERT ’s implementation detail, we also mask 15%\nof the words. The model is then trained to predict\nthese masked out words using all the visible words.\nFor BERT we also used a next sentence prediction\n(NSP) objective which enables the model to learn\nlanguage inference features by tasking the model to\ndifferentiate between two continuous spans of text\nand two randomly chosen spans of text. RoBERT a\nhas shown that the NSP objective can be removed\nwithout affecting the performance of the overall\nmodel and hence doesn’t use NSP prediction ob-\njective for training. We use a default learning rate\nof 5e−5 for all domain-tuning and ﬁne-tuning ex-\nperiments. Our batch-size for domain-tuning was\neither 7 or 8 depending on GPU availability.\nOur experiments are based on Huggingface’s\nPyTorch library (Wolf et al., 2019). We use the\npublicly released model weights as our pretrained\nmodels6. We use titanx and 1080ti GPUs for our\ndomain-tuning. We ran domain-tuning for one\nepoch on the target dataset. Total time for one\nepoch ranged from one week to four weeks depend-\ning on the target dataset size and the computation\nrequirements of the CL method.\nThe λvalues used in domain-tuning are1.0, 1e4,\n0.1 and 0.1 for L2, EWC, DIS and RH respectively.\nGEM does not require a λhyper-parameters. We\ndo not run a hyper-parameter grid search due to\ncomputational cost constraints.\n6bert-base-uncased, roberta-base, and distilbert-base-\nuncased from https://github.com/huggingface/\ntransformers\nB.2 Downstream Finetuning\nFor all SNLI and MedNLI downstream tasks,\nwe used the default hyper-parameters pro-\nvided by https://github.com/huggingface/\ntransformers for NLI tasks. For SQuAD and\nemrQA, all the default parameters are kept same\nexcept the maximum number of training epochs.\nFor emrQA, all models are run for 5 epochs. For\nSQuAD, BERT and RoBERT a are run for 2 epochs\nwhereas DistilBERT is only run for 1 epoch.\nSQuAD and emrQA were ﬁnetuned with a total\nbatch-size of 24. MedNLI and SNLI were trained\nusing a larger total batch-size of 40, since they\nhave shorter text sequences. All these methods\nwere trained on 2 GPUs (1080ti, TitanX or M40).\nC Methodological Details\nC.1 Elastic Weight Consolidation (EWC)\nThe regularization term in EWC is a weighted\nL2 distance between parameter value at any given\nSGD step θand the initial parameter value θ∗\ns. The\nweight for the ith parameter is given by Fi. Fi in\nEquation 4 is the ith element in the diagonal Fisher\nestimate.\nThe EWC regularization penalty is inspired\nby bayesian online learning. EWC uses a\nlaplace approximation of posterior distribution\nof θ obtained from previous tasks as a prior\nfor the current task. In our framework it\ntranslates to log P(θ|Ds,Dt) ∝ log P(Dt|θ) +\nlog P(θ|Ds). The procedure for elastic weight con-\nsolidation (Kirkpatrick et al., 2017) approximates\nlog P(θ|Ds) by using Laplace approximation\n(MacKay, 1992). Intuitively, the term log P(θ|Ds)\ndenotes information about the weights θin the con-\ntext of the source dataset Ds. Kirkpatrick et al.\n(2017) remark that this information refers to which\nparameter values are important for the previous\ntask.\nThe term log P(θ|Ds) is approximated by a\nGuassian with mean θ∗\ns and diagonal precision F.\nThe parameters θ∗\ns are produced by estimating the\nEmpirical Fisher Information on Ds. The EWC\nterm stops parameters that are important for the\nprevious task Ds from changing too much during\nEWC training. For more details, refer to Kirk-\npatrick et al. (2017).\nC.2 Gradient Episodic Memory\nHere, we describe our solution to obtain the GEM\ngradient for our CL experiments. Since we only\nhave one constraint in our formulation, we can an-\nalytically solve for ginstead of using a quadratic\nprogram solver as suggested by GEM. We ﬁrst\ncheck whether the target domain gradient has a\nnegative component in the direction of source do-\nmain gradient. This is done by checking if the\nvalue of the inner product < ∂ℓt\n∂θ ,∂ℓs\n∂θ >is negative.\nIf the inner product is positive, we can step in the\ndirection of ∂ℓt\n∂θ (within the local linear neighbor-\nhood) without increasing the source domain loss.\nIf the inner product is negative, we subtract the\nnegative component of ∂ℓt\n∂θ along ∂ℓs\n∂θ to obtain a\ngradient g. The resulting gradient vector g has a\nnon-negative inner product with ∂ℓs\n∂θ and is used for\nthe SGD model update.\nC.3 Mutual Information\nMutual information I(Y; φ(.)(X)) can be decom-\nposed as\nI(Y; φ(.)(X)) =H(Y)−\nEP(φ(.)(X),Y)[log P(Y|φ(.)(X))] (7)\nThe second term is obtained by rewriting the con-\nditional entropy H(Y|φ(.)(X)) as an expectation\nover P(φ(.)(X),Y ).\nThe ﬁrst term H(Y) is the entropy of the output\nrandom variable and is independent of the model.\nTherefore, we can ignoreH(Y) if we only examine\nthe difference in MI of two methods. Table 2 shows\nthe difference I(Y; φ(.)(X)) −I(Y; φSDT(X))\nfor our proposed CL methods. Here φ(.)(X) de-\nnotes the concatenation of all hidden layer and ﬁ-\nnal layer output representations of a domain-tuned\n(but not ﬁne-tuned) model. To estimate the term\nEp(φ(.)(X),Y)[log P(Y|φ(.)(X))], we train a two\nlayer neural network on(yi,φ(.)(xi)) samples from\nMedNLI training data. The expectation is approx-\nimated by aggregating over (yi,φ(.)(xi)) samples\nfrom MedNLI test data.\nD Dataset Details\nD.1 Domain-Tuning Datasets\nWe use a block randomized sub-section of Wiki-\nText corpus as a substitute for our source domain\ntext. The main domain-tuning experiments were\ndone on Pubmed+MIMIC. We use a subset of the\ntotal Pubmed+MIMIC as ourtarget domain dataset,\ndue to resource contraints. We also performed ex-\nperiment on a smaller target domain set comprised\non MIMIC-III documents only. The size for each\ndataset is provided in Table 5.\nDataset Source Use Number of words\nPubmed +MIMIC-III As Target domain dataset for Pubmed+Mimic experi-\nments\n2,454,355,020\nMIMIC-III As Target domain dataset for Clinical-only experi-\nments\n516,586,695\nWikiText Source domain dataset used for CL regularization in\nRH,EWC,GEM and DIS\n538,061,015\nTable 5: The Dataset size for all datasets used for domain-tuning training and CL regularization are provided.\nOur choice of clinical+ medical dataset was\nbased on :\n• Pubmed and MIMIC-III datasets are openly\navailable\n• Widely used tasks such as NLI and QA are\navailable for clinical+ medical target domain.\nOther possible domains such as scientiﬁc and\nlaw, either did not have open domain datasets or\ndid not have target domain QA and SNLI datasets.\nD.2 Downstream Datasets\nFor the question answering datasets, SQuAD and\nemrQA, we used a doc stride of 128 and a win-\ndow size of 384 across all the datapoints for each\nmodel7. This results in upsampling of certain ques-\ntion answer pairs with different context passage\nwindows. We also reject question answer pairs\nwhere the answer is not within the context size. For\nSNLI, the datapoints with ‘-’ as their gold label\nwere ignored resulting in slightly fewer datapoints\nafter processing. These processing steps results in\ndifferent pre and post processed dataset size. These\nstatistics are presented in Table 6.\nWe use the standard train/test set deﬁned for\nSNLI and MedNLI . For SQuAD 2.0 we use the\nreleased dev set as the test set. For EMRQA, we\nsplit the documents into train and test. We then\nextract Question, Answer and Context from rele-\nvant documents to obtain the train/test QA set. The\ntrain split contains 602 documents and the test set\ncontains 151 documents.\nE Additional Experiments\nAn expanded version of Table 1 with Distillation\nCL method and standard deviations are provided in\nTable 7.\n7These are default values from https://github.\ncom/huggingface/transformers implementation\nPre-processed Post-processed\nDataset # Train # Test # Train # Test\nSQuAD 130,319 11,873 135,228 12,661\nemrQA 262,998 61,398 280,888 66,457\nSNLI 560,151 9,999 559,208 9,823\nMedNLI 12,626 1,421 12,626 1,421\nTable 6: Number of original and post-processed data-\npoints for each dataset.\nWe also performed downstream task experi-\nments on a smaller target domain using MIMIC-III\ndocuments as the target domain text source. The\nresults are available in Table 8.\nFigure 4: Self-supervised loss LT on target domain validation set for all models. L2 model’s loss is very high, and\nis not included in the graph to show higher y-axis resolution for other CL models.\nFigure 5: Self-supervised loss LS on source domain validation set for all CL models.\nDownstream tasks related toSourcedomain Downstream tasks related toTargetdomain\nSQuAD 2.0 SNLI EMRQA MedNLI\nEM F-score Accuracy EM F-score Accuracy\nBERT\nNDT 71.78±0.36 75.5 ±0.32 90.45 ±0.13 74.86±0.28 80.43 ±0.3 78.49 ±0.56\nSDT 71.25±0.25 74.63 ±0.3 88.45 ±0.16 76.3±0.21 81.86 ±0.12 79.92 ±0.46\nRH 70.76±0.26 74.41 ±0.29 89.17 ±0.05 76.26±0.23 81.86 ±0.28 80.46 ±0.57\nEWC 71.11±0.33 74.81 ±0.31 90.14 ±0.03 75.8±0.26 81.29 ±0.18 80.93 ±0.3\nGEM 68.88±0.34 72.43 ±0.31 88.37 ±0.2 75.9±0.19 81.71 ±0.14 78.47 ±0.45\nL2 72.18±0.49 75.86 ±0.52 90.4 ±0.12 75.76±0.3 80.96 ±0.13 78.14 ±1.03\nDIS 69.6±0.57 73.27 ±0.51 88.72 ±0.08 75.95±0.04 81.47 ±0.08 80.51 ±0.53\nRoBERTa\nNDT 78.16±0.23 81.79 ±0.15 91.38 ±0.03 74.57±0.07 80.45 ±0.18 81.04 ±0.34\nSDT 72.61±0.17 76.34 ±0.17 89.59 ±0.12 74.24±0.25 80.09 ±0.18 83.98 ±0.39\nRH 75.61±0.15 79.31 ±0.16 90.75 ±0.17 74.93±0.26 80.7 ±0.11 85.43 ±0.01\nEWC 77.67±0.16 81.44 ±0.19 91.3 ±0.07 74.98±0.26 80.81 ±0.3 83.65 ±0.24\nGEM 72.75±0.09 76.57 ±0.13 89.87 ±0.24 74.27±0.54 80.11 ±0.27 84.43 ±0.2\nL2 78.25±0.09 81.84 ±0.06 91.44 ±0.01 74.68±0.22 80.47 ±0.21 80.44 ±0.4\nDIS 74.7±0.29 78.41 ±0.12 90.59 ±0.08 74.4±0.23 80.45 ±0.08 83.91 ±0.24\nDistilBERT\nNDT 65.47±0.16 69.18 ±0.17 89.69 ±0.07 73.56±0.12 79.3 ±0.28 76.07 ±0.19\nSDT 64.88±0.06 68.4 ±0.14 87.25 ±0.29 74.21±0.07 80.26 ±0.04 78.27 ±0.18\nRH 65.14±0.33 68.89 ±0.35 88.87 ±0.1 74.35±0.04 80.2 ±0.05 78.46 ±0.94\nEWC 65.22±0.34 69.01 ±0.29 89.47 ±0.07 73.71±0.4 79.5 ±0.28 76.26 ±0.24\nGEM 64.17±0.11 67.78 ±0.15 87.98 ±0.06 74.26±0.18 80.05 ±0.02 78.58 ±0.29\nL2 65.27±0.55 68.92 ±0.48 89.67 ±0.22 73.57±0.09 79.38 ±0.09 75.81 ±0.32\nDIS 64.98±0.13 68.72 ±0.05 88.71 ±0.12 74.2±0.26 80.03 ±0.2 78.11 ±0.44\nTable 7: Downstream task results with standard deviations from our experiments across all models and techniques\nfor target domain. CL domain-tuned models are ﬁnetuned for each task to obtain the results. All results are\naveraged over three runs. NDT refers to the base model that is not domain tuned.\nDownstream tasks related to Source domain Downstream tasks related to Target domain\nSQuAD 2.0 SNLI EMRQA MedNLI\nEM F-score Accuracy EM F-score Accuracy\nBERT\nNDT 71.83 ±0.12 75.59 ±0.04 89.89 ±0.06 74.84 ±0.03 80.51 ±0.18 78.65 ±0.5\nSDT 65.92 ±0.12 69.64 ±0.06 88.38 ±0.24 76.04 ±0.22 81.62 ±0.18 80.15 ±0.5\nRH 69.16 ±0.15 (3.24) 72.77 ±0.13 (3.13) 88.72 ±0.39 (0.34) 75.63 ±0.28 (-0.41) 81.32 ±0.13 (-0.30) 81.21 ±0.47 (1.06)\nEWC 72.16 ±0.12 (6.24) 75.79 ±0.16 (6.15) 89.88 ±0.12 (1.50) 75.08 ±0.19 (-0.96) 80.43 ±0.12 (-1.19) 80.58 ±0.62 (0.43)\nGEM 67.31 ±0.12 (1.39) 71.1 ±0.02 (1.46) 88.8 ±0.08 (0.42) 75.83 ±0.3 (-0.21) 81.5 ±0.2 (-0.12) 80.63 ±0.31 (0.48)\nL2 71.79 ±0.53 (5.87) 75.51 ±0.53 (5.87) 89.88 ±0.11 (1.50) 75.0 ±0.07 (-1.04) 80.5 ±0.1 (-1.12) 78.87 ±0.18 (-1.28)\nDIS 66.83 ±0.3 (0.91) 70.54 ±0.34 (0.90) 88.66 ±0.21 (0.28) 75.89 ±0.25 (-0.15) 81.42 ±0.29 (-0.20) 81.02 ±0.09 (0.87)\nRoBERTa\nNDT 78.18 ±0.14 81.76 ±0.1 90.8 ±0.07 74.76 ±0.17 80.53 ±0.19 80.98 ±0.44\nSDT 75.11 ±0.04 78.77 ±0.02 90.37 ±0.2 75.2 ±0.14 80.86 ±0.21 84.47 ±0.38\nRH 75.9 ±0.25 (0.79) 79.65 ±0.23 (0.88) 90.6 ±0.07 (0.23) 74.71 ±0.1 (-0.49) 80.33 ±0.17 (-0.53) 84.35 ±0.35 (-0.12)\nEWC 78.1 ±0.03 (2.99) 81.73 ±0.09 (2.96) 90.76 ±0.01 (0.39) 74.95 ±0.27 (-0.25) 80.85 ±0.22 (-0.01) 83.39 ±0.3 (-1.08)\nGEM 75.21 ±0.04 (0.10) 78.86 ±0.04 (0.09) 90.34 ±0.11 (-0.03) 74.9 ±0.36 (-0.30) 80.65 ±0.18 (-0.21) 84.82 ±0.14 (0.35)\nL2 78.22 ±0.04 (3.11) 81.65 ±0.08 (2.88) 90.56 ±0.09 (0.19) 74.41 ±0.17 (-0.79) 80.29 ±0.1 (-0.57) 79.33 ±0.49 (-5.14)\nDIS 75.41 ±0.13 (0.30) 79.0 ±0.1 (0.23) 90.95 ±0.05 (0.58) 75.38 ±0.12 (0.18) 80.82 ±0.15 (-0.04) 84.64 ±0.26 (0.17)\nDistilBERT\nNDT 65.14 ±0.3 68.78 ±0.41 89.51 ±0.13 73.02 ±0.08 78.76 ±0.1 76.26 ±0.29\nSDT 61.72 ±0.22 65.57 ±0.19 88.43 ±0.1 74.37 ±0.07 80.17 ±0.04 78.47 ±0.32\nRH 63.83 ±0.13 (2.11) 67.45 ±0.07 (1.88) 88.97 ±0.26 (0.54) 74.77 ±0.24 (0.40) 80.47 ±0.23 (0.30) 78.98 ±0.16 (0.51)\nEWC 64.86 ±0.25 (3.14) 68.53 ±0.28 (2.96) 89.07 ±0.05 (0.64) 74.37 ±0.21 (0.00) 80.19 ±0.19 (0.02) 78.28 ±0.07 (-0.19)\nGEM 62.37 ±0.23 (0.65) 66.21 ±0.1 (0.64) 88.75 ±0.07 (0.32) 74.62 ±0.22 (0.25) 80.32 ±0.24 (0.15) 78.56 ±0.35 (0.09)\nL2 65.12 ±0.43 (3.40) 68.79 ±0.44 (3.22) 89.52 ±0.07 (1.09) 73.7 ±0.09 (-0.67) 79.51 ±0.16 (-0.66) 76.33 ±0.15 (-2.14)\nDIS 62.36 ±0.3 (0.64) 66.16 ±0.19 (0.59) 88.71 ±0.14 (0.28) 74.31 ±0.18 (-0.06) 79.96 ±0.13 (-0.21) 78.94 ±0.26 (0.47)\nTable 8: Downstream task results from our experiments across all models and techniques for “mimic-only” target domain.",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.8283214569091797
    },
    {
      "name": "Computer science",
      "score": 0.7655892372131348
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6679559350013733
    },
    {
      "name": "Language model",
      "score": 0.5512649416923523
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5402182936668396
    },
    {
      "name": "Task (project management)",
      "score": 0.5397412180900574
    },
    {
      "name": "Domain model",
      "score": 0.46561646461486816
    },
    {
      "name": "Machine learning",
      "score": 0.38366228342056274
    },
    {
      "name": "Domain knowledge",
      "score": 0.13217124342918396
    },
    {
      "name": "Engineering",
      "score": 0.11250871419906616
    },
    {
      "name": "Psychology",
      "score": 0.07137200236320496
    },
    {
      "name": "Mathematics",
      "score": 0.068808913230896
    },
    {
      "name": "Cognitive psychology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "topic": "Forgetting",
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    }
  ],
  "cited_by": 8
}