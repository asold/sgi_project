{
  "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers",
  "url": "https://openalex.org/W3172801447",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4202207709",
      "name": "Jiang, Zihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3174905547",
      "name": "Hou, Qibin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103884630",
      "name": "Yuan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2563060429",
      "name": "Zhou, Daquan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096110918",
      "name": "Shi Yujun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350479366",
      "name": "Jin, Xiaojie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2364201584",
      "name": "Wang Anran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2594576826",
      "name": "Feng, Jiashi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2971315489",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3034502973",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3034756453",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3168547821",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W2963855133",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3129603602",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3107331169",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W2108598243"
  ],
  "abstract": "In this paper, we present token labeling -- a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classification loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to compute the training loss in a dense manner. Specifically, token labeling reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result can be further increased to 86.4% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous models (250M+) reaching 86%. We also show that token labeling can clearly improve the generalization capability of the pre-trained models on downstream tasks with dense prediction, such as semantic segmentation. Our code and all the training details will be made publicly available at https://github.com/zihangJiang/TokenLabeling.",
  "full_text": "All Tokens Matter: Token Labeling for Training\nBetter Vision Transformers\nZihang Jiang1∗ Qibin Hou1 Li Yuan1 Daquan Zhou1 Yujun Shi1\nXiaojie Jin2 Anran Wang2 Jiashi Feng1\n1National University of Singapore 2ByteDance\n{jzh0103,andrewhoux,ylustcnus,zhoudaquan21,shiyujun1016}@gmail.com\nxjjin0731@gmail.com, anran.wang@bytedance.com, elefjia@nus.edu.sg\nAbstract\nIn this paper, we present token labeling—a new training objective for training\nhigh-performance vision transformers (ViTs). Different from the standard training\nobjective of ViTs that computes the classiﬁcation loss on an additional trainable\nclass token, our proposed one takes advantage of all the image patch tokens to com-\npute the training loss in a dense manner. Speciﬁcally, token labeling reformulates\nthe image classiﬁcation problem into multiple token-level recognition problems and\nassigns each patch token with an individual location-speciﬁc supervision generated\nby a machine annotator. Experiments show that token labeling can clearly and con-\nsistently improve the performance of various ViT models across a wide spectrum.\nFor a vision transformer with 26M learnable parameters serving as an example,\nwith token labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet.\nThe result can be further increased to 86.4% by slightly scaling the model size up\nto 150M, delivering the minimal-sized model among previous models (250M+)\nreaching 86%. We also show that token labeling can clearly improve the generaliza-\ntion capability of the pretrained models on downstream tasks with dense prediction,\nsuch as semantic segmentation. Our code and all the training details are publicly\navailable at https://github.com/zihangJiang/TokenLabeling.\n1 Introduction\nTransformers [39] have achieved great performance for almost all the natural language processing\n(NLP) tasks over the past years [4, 15, 25]. Motivated by such success, recently, many researchers\nattempt to build transformer models for vision tasks, and their encouraging results have shown the\ngreat potential of transformer based models for image classiﬁcation [7, 16, 26, 36, 40, 46], especially\nthe strong beneﬁts of the self-attention mechanism in building long-range dependencies between\npairs of input tokens.\nDespite the importance of gathering long-range dependencies, recent work on local data augmenta-\ntion [57] has demonstrated that well modeling and leveraging local information for image classiﬁca-\ntion would avoid biasing the model towards skewed and non-generalizable patterns and substantially\nimprove the model performance. However, recent vision transformers normally utilize class tokens\n∗Work done as an intern at ByteDance AI Lab.\nPreprint. Under review.\narXiv:2104.10858v3  [cs.CV]  9 Jun 2021\nFigure 1: Comparison between the proposed LV-ViT and other recent works based on vision trans-\nformers, including T2T-ViT [46], ConViT [13], BoTNet [31], DeepViT [59], DeiT [36], ViT [16],\nSwin Transformer [26], LambdaNet [1], CvT [43], CrossViT [7], PVT [40], CaiT [37]. Note that\nwe only show models whose model sizes are under 100M. As can be seen, our LV-ViT achieves the\nbest results using the least amount of learnable parameters. The default test resolution is 224 ×224\nunless speciﬁed after @.\nthat aggregate global information to predict the output class while neglecting the role of other patch\ntokens that encode rich information on their respective local image patches.\nIn this paper, we present a new training objective for vision transformers, termed token labeling, that\ntakes advantage of both the patch tokens and the class tokens. Our method takes a K-dimensional\nscore map generated by a machine annotator as supervision to supervise all the tokens in a dense\nmanner, where K is the number of categories for the target dataset. In this way, each patch token\nis explicitly associated with an individual location-speciﬁc supervision indicating the existence of\nthe target objects inside the corresponding image patch, so as to improve the object grounding and\nrecognition capabilities of vision transformers with negligible computation overhead. To the best\nof our knowledge, this is the ﬁrst work demonstrating that dense supervision is beneﬁcial to vision\ntransformers in image classiﬁcation.\nAccording to our experiments, utilizing the proposed token labeling objective can clearly boost\nthe performance of vision transformers. As shown in Figure 1, our model, named LV-ViT, with\n56M parameters, yields 85.4% top-1 accuracy on ImageNet [14], behaving better than all the other\ntransformer-based models having no more than 100M parameters. When the model size is scaled up\nto 150M, the result can be further improved to 86.4%. In addition, we have empirically found that the\npretrained models with token labeling are also beneﬁcial to downstream tasks with dense prediction,\nsuch as semantic segmentation.\n2 Related Work\nTransformers [39] refer to the models that entirely rely on the self-attention mechanism to build\nglobal dependencies, which are originally designed for natural language processing tasks. Due to their\nstrong capability of capturing spatial information, transformers have also been successfully applied\nto a variety of vision problems, including low-level vision tasks like image enhancement [8, 45], as\nwell as more challenging tasks such as image classiﬁcation [10, 16], object detection [5, 12, 55, 61],\nsegmentation [8, 33, 41] and image generation [29]. Some works also extend transformers for video\nand 3D point cloud processing [50, 53, 60].\nVision Transformer (ViT) is one of the earlier attempts that achieved state-of-the-art performance\non ImageNet classiﬁcation, using pure transformers as basic building blocks. However, ViTs need\npretraining on very large datasets, such as ImageNet-22k and JFT-300M, and huge computation\nresources to achieve comparable performance to ResNet [19] with a similar model size trained on\nImageNet. Later, DeiT [ 36] manages to tackle the data-inefﬁciency problem by simply adjusting\nthe network architecture and adding an additional token along with the class token for Knowledge\nDistillation [22, 47] to improve model performance.\n2\nPatch Tokens\nClass Token\nClass\nInput Image\nDense score map\nOutput Tokens\nToken Labeling\nFigure 2: Pipeline of training vision transformers with token labeling. Other than utilizing the\nclass token (pink rectangle), we also take advantage of all the output patch tokens (orange rounded\nrectangle) by assigning each patch token an individual location-speciﬁc prediction generated by a\nmachine annotator [3] as supervision (see the part in the red dash rectangle). Our proposed token\nlabeling method can be treated as an auxiliary objective to provide each patch token the local details\nthat aid vision transformers to more accurately locate and recognize the target objects. Note that the\ntraditional vision transformer training does not include the red dash rectangle part.\nSome recent works [ 7, 17, 43, 46] also attempt to introduce the local dependency into vision\ntransformers by modifying the patch embedding block or the transformer block or both, leading to\nsigniﬁcant performance gains. Moreover, there are also some works [21, 26, 40] adopting a pyramid\nstructure to reduce the overall computation while maintaining the model’s ability to capture low-level\nfeatures.\nUnlike most aforementioned works that design new transformer blocks or transformer architectures,\nwe attempt to improve vision transformers by studying the role of patch tokens that embed rich local\ninformation inside image patches. We show that by slightly tuning the structure of vision transformers\nand employing the proposed token labeling objective, we can achieve strong baselines for transformer\nmodels at different model size levels.\n3 Token Labeling Method\nIn this section, we ﬁrst brieﬂy review the structure of the vision transformer [16] and then describe\nthe proposed training objective— token labeling.\n3.1 Revisiting Vision Transformer\nA typical vision transformer [16] ﬁrst decomposes a ﬁxed-size input image into a sequence of small\npatches. Each small patch is mapped to a feature vector, or called a token, by projection with a linear\nlayer. Then, all the tokens combined with an additional learnable class token for classiﬁcation score\nprediction are sent into a stack of transformer blocks for feature encoding.\nIn loss computing, the class token from the output tokens of the last transformer block is usually\nselected and sent into a linear layer for the classiﬁcation score prediction. Mathematically, given\nan image I, denote the output of the last transformer block as [Xcls,X1,...,X N ], where N is the\ntotal number of patch tokens, and Xcls and X1,...,X N correspond to the class token and the patch\ntokens, respectively. The classiﬁcation loss for image I can be written as\nLcls = H(Xcls,ycls), (1)\nwhere H(·,·) is the softmax cross-entropy loss and ycls is the class label.\n3\nFigure 3: Comparison between CutMix [48]\n(Left) and our proposed MixToken ( Right).\nCutMix is operated on the input images. This\nresults in patches containing mixed regions\nfrom the two images (see the patches enclosed\nby red bounding boxes). Differently, MixTo-\nken targets at mixing tokens after patch em-\nbedding. This enables each token after patch\nembedding to have clean content as shown in\nthe right part of this ﬁgure. The detailed ad-\nvantage of MixToken can be found in Sec. 4.2.\n3.2 Token Labeling\nThe above classiﬁcation problem only adopts an image-level label as supervision whereas it neglects\nthe rich information embedded in each image patch. In this subsection, we present a new training\nobjective— token labeling—that takes advantage of the complementary information between the\npatch tokens and the class tokens.\nToken Labeling:Different from the classiﬁcation loss as formulated in Eqn. (1) that measures the\ndistance between the single class token (representing the whole input image) and the corresponding\nimage-level label, token labeling emphasizes the importance of all output tokens and advocates that\neach output token should be associated with an individual location-speciﬁc label. Therefore, in our\nmethod, the ground truth for an input image involves not only a single K-dimensional vector ycls but\nalso a K×N matrix or called a K-dimensional score map as represented by [y1,...,y N ], where N\nis the number of the output patch tokens.\nSpeciﬁcally, we leverage a dense score map for each training image and use the cross-entropy loss\nbetween each output patch token and the corresponding aligned label in the dense score map as an\nauxiliary loss at the training phase. Figure 2 provides an intuitive interpretation. Given the output\npatch tokens X1,...,X N and the corresponding labels [y1,...,y N ], the token labeling objective can\nbe deﬁned as\nLtl = 1\nN\nN∑\ni=1\nH(Xi,yi). (2)\nRecall that H is the cross-entropy loss. Therefore, the total loss function can be written as\nLtotal = H(Xcls,ycls) + β·Ltl, (3)\n= H(Xcls,ycls) + β· 1\nN\nN∑\ni=1\nH(Xi,yi), (4)\nwhere βis a hyper-parameter to balance the two terms. In our experiment, we empirically set it to\n0.5.\nAdvantages: Our token labeling offers the following advantages. First of all, unlike knowledge\ndistillation methods that require a teacher model to generate supervision labels online, token labeling\nis a cheap operation. The dense score map can be generated by a pretrained model in advance (e.g.,\nEfﬁcientNet [34] or NFNet [3]). During training, we only need to crop the score map and perform\ninterpolation to make it aligned with the cropped image in the spatial coordinate. Thus, the additional\ncomputations are negligible. Second, rather than utilizing a single label vector as supervision as done\nin most classiﬁcation models and the ReLabel strategy [49], we also harness score maps to supervise\nthe models in a dense manner and thereby the label for each patch token provides location-speciﬁc\ninformation, which can aid the training models to easily discover the target objects and improve the\nrecognition accuracy. Last but not the least, as dense supervision is adopted in training, we found\nthat the pretrained models with token labeling beneﬁt downstream tasks with dense prediction, like\nsemantic segmentation.\n4\n3.3 Token Labeling with MixToken\nWhile training vision transformer, previous studies [36, 46] have shown that augmentation methods,\nlike MixUp [ 52] and CutMix [ 48], can effectively boost the performance and robustness of the\nmodels. However, vision transformers rely on patch-based tokenization to map each input image to a\nsequence of tokens and our token labeling strategy also operates on patch-based token labels. If we\napply CutMix directly on the raw image, some of the resulting patches may contain content from two\nimages, leading to mixed regions within a small patch as shown in Figure 3. When performing token\nlabeling, it is difﬁcult to assign each output token a clean and correct label. Taking this situation into\naccount, we rethink the CutMix augmentation method and present MixToken, which can be viewed\nas a modiﬁed version of CutMix operating on the tokens after patch embedding as illustrated in the\nright part of Figure 3.\nTo be speciﬁc, for two images denoted asI1,I2 and their corresponding token labelsY1 = [y1\n1,...,y N\n1 ]\nas well as Y2 = [y1\n2,...,y N\n2 ], we ﬁrst feed the two images into the patch embedding module to tokenize\neach as a sequence of tokens, resulting in T1 = [t1\n1,...,t N\n1 ] and T2 = [t1\n2,...,t N\n2 ]. Then, we produce a\nnew sequence of tokens by applying MixToken using a binary maskM as follows:\nˆT = T1 ⊙M + T2 ⊙(1 −M), (5)\nwhere ⊙is element-wise multiplication. We use the same way to generate the mask M as in [48].\nFor the corresponding token labels, we also mix them using the same mask M:\nˆY = Y1 ⊙M + Y2 ⊙(1 −M). (6)\nThe label for the class token can be written as\nˆycls = ¯Mycls\n1 + (1 − ¯M)ycls\n2 , (7)\nwhere ¯M is the average of all element values of M.\n4 Experiments\n4.1 Experiment Setup\nWe evaluate our method on the ImageNet [14] dataset. All experiments are built and conducted upon\nPyTorch [30] and the timm [42] library. We follow the standard training schedule and train our\nmodels on the ImageNet dataset for 300 epochs. Besides normal augmentations like CutOut [57] and\nRandAug [11], we also explore the effect of applying MixUp [52] and CutMix [48] together with our\nproposed token labeling. Empirically, we have found that using MixUp together with token labeling\nbrings no beneﬁt to the performance, and thus we do not apply it in our experiments.\nFor optimization, by default, we use the AdamW optimizer [28] with a linear learning rate scaling\nstrategy lr = 10−3 ×batch_size\n640 and 5 ×10−2 weight decay rate. For Dropout regularization, we\nobserve that for small models, using Dropout hurts the performance. This has also been observed in\na few other works related to training vision transformers [36, 37, 46]. As a result, we do not apply\nDropout [32] and use Stochastic Depth [24] instead. More details on hyper-parameters and ﬁnetuning\ncan be found in our supplementary materials.\nWe use the NFNet-F6 [ 3] trained on ImageNet with an 86.3% Top-1 accuracy as the machine\nannotator to generate dense score maps for the ImageNet dataset, yielding a 1000-dimensional score\nmap for each image for training. The score map generation procedure is similar to [ 49], but we\nlimit our experiment setting by training all models from scratch on ImageNet without extra data\nsupport, such as JFT-300M and ImageNet-22K. This is different from the original ReLabel paper\n[49], in which the EfﬁcientNet-L2 model pretrained on JFT-300M is used. The input resolution for\nNFNet-F6 is 576 ×576, and the dimension of the corresponding output score map for each image\nis L ∈R18×18×1000. During training, the target labels for the tokens are generated by applying\nRoIAlign [18] on the corresponding score map. In practice, we only store the top-5 score maps for\neach position in half-precision to save space as storing the entire score maps for all the images results\nin 2TB storage. In our experiment, we only need 10GB of storage to store all the score maps.\n5\nTable 1: Performance of the proposed LV-ViT with different model sizes. Here, ‘depth’ denotes\nthe number of transformer blocks used in different models. By default, the test resolution is set to\n224 ×224 except the last one which is 288 ×288.\nName Depth Embed dim. MLP Ratio #Heads #Parameters Resolution Top-1 Acc. (%)\nLV-ViT-T 12 240 3.0 4 8.5M 224x224 79.1\nLV-ViT-S 16 384 3.0 6 26M 224x224 83.3\nLV-ViT-M 20 512 3.0 8 56M 224x224 84.1\nLV-ViT-L 24 768 3.0 12 150M 288x288 85.3\n4.2 Ablation Analysis\nModel Settings:The default settings of the proposed LV-ViT are given in Table 1, where both token\nlabeling and MixToken are used. A slight architecture modiﬁcation to ViT [16] is that we replace the\npatch embedding module with a 4-layer convolution to better tokenize the input image and integrate\nlocal information. Detailed ablation about patch embedding can be found in our supplementary\nmaterials. As can be seen, our LV-ViT-T with only 8.5M parameters can already achieve a top-1\naccuracy of 79.1% on ImageNet. Increasing the embedding dimension and network depth can further\nboost the performance. More experiments compared to other methods can be found in Sec. 4.3. In\nthe following ablation experiments, we will set our LV-ViT-S as baseline and show the advantages of\nthe proposed token labeling and MixToken methods.\nMixToken: We use MixToken as a substitution for CutMix while applying token labeling. Our\nexperiments show that MixToken performs better than CutMix for token-based transformer models.\nAs shown in Table 2, when training with the original ImageNet labels, using MixToken is 0.1%\nhigher than using CutMix. When using the ReLabel supervision, we can also see an advantage of\n0.2% over the CutMix baseline. Combining with our token labeling, the performance can be further\nraised to 83.3%.\nTable 2: Ablation on the proposed MixToken\nand token labeling augmentations. We also\nshow results with either the ImageNet hard\nlabel and the ReLabel [49] as supervision.\nAug. Method Supervision Top-1 Acc.\nMixToken Token labeling 83.3\nMixToken ReLabel 83.0\nCutMix ReLabel 82.8\nMixtoken ImageNet Label 82.5\nCutMix ImageNet Label 82.4\nTable 3: Ablation on different widely-used data aug-\nmentations. We have empirically found our proposed\nMixToken performs even better than the combination\nof MixUp and CutMix in vision transformers.\nMixToken MixUp CutOut RandAug Top-1 Acc.\n✓ \u0017 ✓ ✓ 83.3\n\u0017 \u0017 ✓ ✓ 81.3\n✓ ✓ ✓ ✓ 83.1\n✓ \u0017 \u0017 ✓ 83.0\n✓ \u0017 ✓ \u0017 82.8\nData Augmentation: Here, we study the compatibility of MixToken with other augmentation\ntechniques, such as MixUp [52], CutOut [57] and RandAug [11]. The ablation results are shown in\nTable 3. We can see when all the four augmentation methods are used, a top-1 accuracy of 83.1% is\nachieved. Interestingly, when the MixUp augmentation is removed, the performance can be improved\nto 83.3%. This may be explained as, using MixToken and MixUp at the same time would bring\ntoo much noise in the label, and consequently cause confusion of the model. Moreover, the CutOut\naugmentation, which randomly erases some parts of the image, is also effective and removing it\nbrings a performance drop of 0.3%. Similarly, the RandAug augmentation also contributes to the\nperformance and using it brings an improvement of 0.5%.\nAll Tokens Matter:To show the importance of involving all tokens in our token labeling method, we\nattempt to randomly drop some tokens and use the remaining ones for computing the token labeling\nloss. The percentage of the remaining tokens is denoted as Token Participation Rate. As shown in\nFigure 4 (Left), we conduct experiments on two models: LV-ViT-S and LV-ViT-M. As can be seen,\nusing only 20% of the tokens to compute the token labeling loss decreases the performance (−0.5%\nfor LV-ViT-S and−0.4% for LV-ViT-M). Involving more tokens for loss computation consistently\nleads to better performance. Since involving all tokens brings negligible computation cost and gives\nthe best performance, we always set the token participation rate as100% in the following experiments.\n6\n0 20 40 60 80 100\nToken Participation Rate\n82.0\n82.5\n83.0\n83.5\n84.0\n84.5ImageNet Top-1 Acc. (%)\n82.4\n82.8\n83.1 83.1\n83.3\n82.9\n83.7\n83.9\n84.0\n84.1\nModel\nLV-ViT-S  \nLV-ViT-M  \n81 82 83 84 85 86 87\nAnnotator Top-1 Acc. (%)\n82.4\n82.6\n82.8\n83.0\n83.2\n83.4LV-ViT-S Top-1 Acc. (%)\n--------- Baseline (82.4) ----------\nEfficientNet-B3\nEfficientNet-B4\nEfficientNet-B5\nEfficientNet-B6\nEfficientNet-B7\nEfficientNet-B8\nResNeSt269E\nNFNet-F6\nFigure 4: Left: LV-ViT ImageNet Top-1 Accuracy w.r.t. the token participation rate while applying\ntoken labeling. Token participation rate indicates the percentage of patch tokens involved in computing\nthe token labeling loss. This experiment reﬂects that all tokens matter for vision transformers. Right:\nLV-ViT-S ImageNet Top-1 Accuracy w.r.t. different annotator models. The point size indicates the\nparameter number of the annotator model. Clearly, our token labeling objective is robust to different\nannotator models.\nRobustness to Different Annotators:To evaluate the robustness of our token labeling method, we\nuse different pretrained CNNs, including EfﬁcientNet-B3,B4,B5,B6,B7,B8 [34], NFNet-F6 [3] and\nResNest269E [51], as annotator models to provide dense supervision. Results are shown in the right\npart of Figure 4. We can see that, even if we use an annotator with relatively lower performance, such\nas EfﬁcientNet-B3 whose Top-1 accuracy is 81.6%, it can still provide multi-label location-speciﬁc\nsupervision and help improve the performance of our LV-ViT-S model. Meanwhile, annotator models\nwith better performance can provide more accurate supervision, bringing even better performance, as\nstronger annotator models can generate better token-level labels. The largest annotator NFNet-F6 [3],\nwhich has the best performance of 86.3%, allows us to achieve the best result for LV-ViT-S, which is\n83.3%. In addition, we also attempt to use a better model, EfﬁcientNet-L2 pretrained on JFT-300M\nas described in [49] which has 88.2% Top-1 ImageNet accuracy, as our annotator. The performance\nof LV-ViT-S can be further improved to83.5%. However, to fairly compare with the models without\nextra training data, we only report results based on dense supervision produced by NFNet-F6 [3] that\nuses only ImageNet training data.\nDeiT-S (22M) DeiT-B (86M)\nModels\n79\n80\n81\n82\n83\n84ImageNet Top-1 Acc. (%)\n81.0\n80.1\n83.1\n81.8\nToken Labeling\nw/ TL\nw/o TL\nT2T-19 (39M) T2T-24 (64M)\nModels\n80\n81\n82\n83\n84\n85ImageNet Top-1 Acc. (%)\n82.5\n81.9\n84.2\n83.3\nToken Labeling\nw/ TL\nw/o TL\nLV-ViT-S (26M) LV-ViT-M (56M)LV-ViT-L (150M)\nModels\n81\n82\n83\n84\n85\n86ImageNet Top-1 Acc. (%)\n83.3\n82.4\n84.0\n82.9\n85.3\n84.4\nToken Labeling\nw/ TL\nw/o TL\nFigure 5: Performance of the proposed token labeling objective on three different vision transformers:\nDeiT [ 36] ( Left), T2T-ViT [ 46] ( Middle), and LV-ViT ( Right). Our method has a consistent\nimprovement on all 7 different ViT models.\nRobustness to Different ViT Variants:To further evaluate the robustness of our token labeling, we\ntrain different transformer-based networks, including DeiT [36], T2T-ViT [3] and our model LV-ViT,\nwith the proposed training objective. Results are shown in Figure 5. It can be found that, all the\nmodels trained with token labeling consistently outperform their vanilla counterparts, demonstrating\nthe robustness of token labeling with respect to different variants of patch-based vision transformers.\nMeanwhile, for different scales of the models, the improvement is also consistent. Interestingly, we\nobserve larger improvements for larger models. These indicate that our proposed token labeling\nmethod is widely applicable to a large range of patch-based vision transformer variants.\n7\nTable 4: Top-1 accuracy comparison with other methods on ImageNet [14] and ImageNet Real [2].\nAll models are trained without external data. With the same computation and parameter constraint,\nour model consistently outperforms other CNN-based and transformer-based counterparts. The\nresults of CNNs and ViT are referenced from [37].\nNetwork Params FLOPs Train size Test size Top-1(%) Real Top-1 (%)CNNs\nEfﬁcientNet-B5 [34] 30M 9.9B 456 456 83.6 88.3\nEfﬁcientNet-B7 [34] 66M 37.0B 600 600 84.3 _\nFix-EfﬁcientNet-B8 [34, 38] 87M 89.5B 672 800 85.7 90.0\nNFNet-F3 [3] 255M 114.8B 320 416 85.7 89.4\nNFNet-F4 [3] 316M 215.3B 384 512 85.9 89.4\nNFNet-F5 [3] 377M 289.8B 416 544 86.0 89.2\nTransformers\nViT-B/16 [16] 86M 55.4B 224 384 77.9 83.6\nViT-L/16 [16] 307M 190.7B 224 384 76.5 82.2\nT2T-ViT-14 [46] 22M 5.2B 224 224 81.5 _\nT2T-ViT-14↑384 [46] 22M 17.1B 224 384 83.3 _\nCrossViT [7] 45M 56.6B 224 480 84.1 _\nSwin-B [26] 88M 47.0B 224 384 84.2 _\nTNT-B [17] 66M 14.1B 224 224 82.8 _\nDeepViT-S [59] 27M 6.2B 224 224 82.3 _\nDeepViT-L [59] 55M 12.5B 224 224 83.1 _\nDeiT-S [36] 22M 4.6B 224 224 79.9 85.7\nDeiT-B [36] 86M 17.5B 224 224 81.8 86.7\nDeiT-B↑384 [36] 86M 55.4B 224 384 83.1 87.7\nBoTNet-S1-128 [31] 79.1M 19.3B 256 256 84.2 -\nBoTNet-S1-128↑384 [31] 79.1M 45.8B 256 384 84.7 -\nCaiT-S36↑384 [37] 68M 48.0B 224 384 85.4 89.8\nCaiT-M36 [37] 271M 53.7B 224 224 85.1 89.3\nCaiT-M36↑448 [37] 271M 247.8B 224 448 86.3 90.2\nOurs LV-ViT\nLV-ViT-S 26M 6.6B 224 224 83.3 88.1\nLV-ViT-S↑384 26M 22.2B 224 384 84.4 88.9\nLV-ViT-M 56M 16.0B 224 224 84.1 88.4\nLV-ViT-M↑384 56M 42.2B 224 384 85.4 89.5\nLV-ViT-L 150M 59.0B 288 288 85.3 89.3\nLV-ViT-L↑448 150M 157.2B 288 448 85.9 89.7\nLV-ViT-L↑448 150M 157.2B 448 448 86.2 89.9\nLV-ViT-L↑512 151M 214.8B 448 512 86.4 90.1\n4.3 Comparison to Other Methods\nWe compare our proposed model LV-ViT with other state-of-the-art methods in Table 4. For small-\nsized models, when the test resolution is set to 224 ×224, we achieve an 83.3% accuracy on\nImageNet with only 26M parameters, which is 3.4% higher than the strong baseline DeiT-S [36].\nFor medium-sized models, when the test resolution is set to 384 ×384 we achieve the performance\nof 85.4%, the same as CaiT-S36 [37], but with much less computational cost and parameters. Note\nthat both DeiT and CaiT use knowledge distillation to improve their models, which introduce much\nmore computations in training. However, we do not require any extra computations in training\nand only have to compute and store the dense score maps in advance. For large-sized models, our\nLV-ViT-L with a test resolution of512 ×512 achieves an 86.4% top-1 accuracy, which is better than\nCaiT-M36 [37] but with far fewer parameters and FLOPs.\n4.4 Semantic Segmentation on ADE20K\nIt has been shown in [ 20] that different training techniques for pretrained models have different\nimpacts on downstream tasks with dense prediction, like semantic segmentation. To demonstrate\nthe advantage of the proposed token labeling objective on tasks with dense prediction, we apply our\npretrained LV-ViT with token labeling to the semantic segmentation task.\nSimilar to previous work [ 26], we run experiments on the widely-used ADE20K [ 58] dataset.\nADE20K contains 25K images in total, including 20K images for training, 2K images for validation\n8\nTable 5: Transfer performance of the proposed LV-ViT in semantic segmentation. We take two classic\nmethods, FCN and UperNet, as segmentation architectures and show both single-scale (SS) and\nmulti-scale (MS) results on the validation set.\nMethod Token Labeling Model Size mIoU (SS) P. Acc. (SS) mIoU (MS) P. Acc. (MS)\nLV-ViT-S + FCN \u0017 30M 46.1 81.9 47.3 82.6\nLV-ViT-S + FCN \u0013 30M 47.2 82.4 48.4 83.0\nLV-ViT-S + UperNet \u0017 44M 46.5 82.1 47.6 82.7\nLV-ViT-S + UperNet \u0013 44M 47.9 82.6 48.6 83.1\nand 3K images for test, and covering 150 different foreground categories. We take both FCN [27]\nand UperNet [44] as our segmentation frameworks and use the mmseg toolbox to implement. During\ntraining, following [ 26], we use the AdamW optimizer with an initial learning rate of 6e-5 and\na weight decay of 0.01. We also use a linear learning schedule with a minimum learning rate of\n5e-6. All models are trained on 8 GPUs and with a batch size of 16 (i.e., 2 images on each GPU).\nThe input resolution is set to 512 ×512. In inference, a multi-scale test with interpolation rates of\n[0.75, 1.0, 1.25, 1.5, 1.75] is used. As suggested by [ 58], we report results in terms of both mean\nintersection-over-union (mIoU) and the average pixel accuracy (Pixel Acc.).\nIn Table 5, we test the performance of token labeling on both FCN and UperNet frameworks. The\nFCN framework has a light convolutional head and can directly reﬂect the performance of the\npretrained models in terms of transferable capability. As can be seen, pretrained models with token\nlabeling perform better than those without token labeling. This indicates token labeling is indeed\nbeneﬁcial to semantic segmentation.\nWe also compare our segmentation results with previous state-of-the-art segmentation methods in\nTable 6. Without pretraining on large-scale datasets such as ImageNet-22K, our LV-ViT-M with\nthe UperNet segmentation architecture achieves an mIoU score of 50.6 with only 77M parameters.\nThis result is much better than the previous CNN-based and transformer-based models. Furthermore,\nusing our LV-ViT-L as the pretrained model yields a better result of 51.8 in terms of mIoU. As far as\nwe know, this is the best result reported on ADE20K with no pretraining on ImageNet-22K or other\nlarge-scale datasets.\nTable 6: Comparison with previous work on ADE20K validation set. As far as we know, our LV-\nViT-L + UperNet achieves the best result on ADE20K with only ImageNet-1K as training data in\npretraining. †Pretrained on ImageNet-22K.\nBackbone Segmentation Architecture Model Size mIoU (MS) Pixel Acc. (MS)\nCNNs\nResNet-269 PSPNet [54] - 44.9 81.7\nResNet-101 UperNet [44] 86M 44.9 -\nResNet-101 Strip Pooling [23] - 45.6 82.1\nResNeSt200 DeepLabV3+ [9] 88M 48.4 -\nTransformers\nDeiT-S UperNet 52M 44.0 -\nViT-Large† SETR [56] 308M 50.3 83.5\nSwin-T [26] UperNet 60M 46.1 -\nSwin-S [26] UperNet 81M 49.3 -\nSwin-B [26] UperNet 121M 49.7 -\nSwin-B†[26] UperNet 121M 51.6 -\nLV-ViT\nLV-ViT-S FCN 30M 48.4 83.0\nLV-ViT-S UperNet 44M 48.6 83.1\nLV-ViT-M UperNet 77M 50.6 83.5\nLV-ViT-L UperNet 209M 51.8 84.1\n9\n5 Conclusions and Discussion\nIn this paper, we introduce a new token labeling method to help improve the performance of vision\ntransformers. We also analyze the effectiveness and robustness of our token labeling with respect\nto different annotators and different variants of patch-based vision transformers. By applying token\nlabeling, our proposed LV-ViT achieves 84.4% Top-1 accuracy with only 26M parameters and 86.4%\nTop-1 accuracy with 150M parameters on ImageNet-1K benchmark.\nDespite the effectiveness, token labeling has a limitation of requiring a pretrained model as the\nmachine annotator. Fortunately, the machine annotating procedure can be done in advance to\navoid introducing extra computational cost in training. This makes our method quite different from\nknowledge distillation methods that rely on online teaching. For users with limited machine resources\non hand, our token labeling provides a promising training technique to improve the performance of\nvision transformers.\nReferences\n[1] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint\narXiv:2102.08602, 2021.\n[2] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we\ndone with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n[3] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image\nrecognition without normalization. arXiv preprint arXiv:2102.06171, 2021.\n[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. arXiv preprint arXiv:2005.12872, 2020.\n[6] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. arXiv\npreprint arXiv:2012.09838, 2020.\n[7] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer\nfor image classiﬁcation. arXiv preprint arXiv:2103.14899, 2021.\n[8] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv preprint arXiv:2012.00364,\n2020.\n[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European\nconference on computer vision (ECCV), pages 801–818, 2018.\n[10] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International Conference on Machine Learning, pages 1691–1703.\nPMLR, 2020.\n[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pages 702–703, 2020.\n[12] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsupervised pre-training for object\ndetection with transformers. arXiv preprint arXiv:2011.09094, 2020.\n[13] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021.\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n10\n[17] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021.\n[18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2961–2969, 2017.\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[20] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image\nclassiﬁcation with convolutional neural networks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 558–567, 2019.\n[21] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\nRethinking spatial dimensions of vision transformers. arXiv preprint arXiv:2103.16302, 2021.\n[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\n[23] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng. Strip pooling: Rethinking spatial pooling for\nscene parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 4003–4012, 2020.\n[24] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In European conference on computer vision, pages 646–661. Springer, 2016.\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[27] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-\ntion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431–3440,\n2015.\n[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[29] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. Image transformer. arXiv preprint arXiv:1802.05751, 2018.\n[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. In Advances in neural information processing systems, pages 8026–8037, 2019.\n[31] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. arXiv preprint arXiv:2101.11605, 2021.\n[32] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:\na simple way to prevent neural networks from overﬁtting. The journal of machine learning research ,\n15(1):1929–1958, 2014.\n[33] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set prediction\nfor object detection. arXiv preprint arXiv:2011.10881, 2020.\n[34] Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.\narXiv preprint arXiv:1905.11946, 2019.\n[35] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,\nJessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture for\nvision. arXiv preprint arXiv:2105.01601, 2021.\n[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[37] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper\nwith image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[38] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution\ndiscrepancy. arXiv preprint arXiv:1906.06423, 2019.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30:5998–6008, 2017.\n11\n[40] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021.\n[41] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia.\nEnd-to-end video instance segmentation with transformers. arXiv preprint arXiv:2011.14503, 2020.\n[42] Ross Wightman. Pytorch image models. https://github.com/rwightman/\npytorch-image-models, 2019.\n[43] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[44] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing for scene\nunderstanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418–434,\n2018.\n[45] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture transformer network\nfor image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5791–5800, 2020.\n[46] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021.\n[47] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label\nsmoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3903–3911, 2020.\n[48] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 6023–6032, 2019.\n[49] Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, and Sanghyuk Chun.\nRe-labeling imagenet: from single to multi-labels, from global to localized labels. arXiv preprint\narXiv:2101.05022, 2021.\n[50] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transformations for\nvideo inpainting. In European Conference on Computer Vision, pages 528–543. Springer, 2020.\n[51] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas\nMuller, R. Manmatha, Mu Li, and Alexander Smola. Resnest: Split-attention networks. arXiv preprint\narXiv:2004.08955, 2020.\n[52] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412, 2017.\n[53] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer. arXiv preprint\narXiv:2012.09164, 2020.\n[54] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\nnetwork. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n2881–2890, 2017.\n[55] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end object detection\nwith adaptive clustering transformer. arXiv preprint arXiv:2011.09315, 2020.\n[56] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n[57] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 13001–13008, 2020.\n[58] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\nSemantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision,\n127(3):302–321, 2019.\n[59] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng. Deepvit:\nTowards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n[60] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense video\ncaptioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 8739–8748, 2018.\n[61] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n12\nA More Experiment Details\nWe show the default hyper-parameters for our ImageNet classiﬁcation experiments in Table 7. In\naddition, for ﬁne-tuning on larger image resolution, we set batch size to 512, learning rate to 5e-6,\nweight decay to 1e-8 and ﬁne-tune 30 epochs. Other hyper-parameters are set the same as default.\nDuring training, a machine node with 8 NVIDIA V100 GPUs (32G memory) is required. When\nﬁne-tuning our large model with image resolution of 448 ×448, we need 4 machine nodes with the\nsame GPU settings as above.\nTable 7: Default hyper-parameters for our experiments. Note that we do not use the MixUp augmen-\ntation method when ReLabel or token labeling is used.\nSupervision Standard ReLabel Token labeling\nEpoch 300 300 300\nBatch size 1024 1024 1024\nLR 1e-3 ·batch_size\n1024 1e-3·batch_size\n1024 1e-3·batch_size\n640\nLR decay cosine cosine cosine\nWeight decay 0.05 0.05 0.05\nWarmup epochs 5 5 5\nDropout 0 0 0\nStoch. Depth 0.1 0.1 0.1\nMixUp alpha 0.8 - -\nErasing prob. 0.25 0.25 0.25\nRandAug 9/0.5 9/0.5 9/0.5\nB More Experiments\nB.1 Training Technique Analysis\nWe present a summary of our modiﬁcation and proposed token labeling method to improve vision\ntransformer models in Table 8. We take the DeiT-Small [36] model as our baseline and show the\nperformance increment as more training techniqeus are added. In this subsection, we will ablate the\nproposed modiﬁcations and evaluate the effectiveness of them.\nTable 8: Ablation path from the DeiT-Small [36] baseline to our LV-ViT-S. All experiments expect\nfor larger input resolution can be ﬁnished within 3 days using a single server node with 8 V100 GPUs.\nClearly, with only 26M learnable parameters, the performance can be boosted from 79.9 to 84.4\n(+4.5) using the proposed Token Labeling and other proposed training techniques.\nTraining techniques #Param. Top-1 Acc. (%)\nBaseline (DeiT-Small [36]) 22M 79.9\n+ More transformers (12 →16) 28M 81.2 ( +1.2)\n+ Less MLP expansion ratio (4 →3) 25M 81.1 ( +1.1)\n+ More convs for patch embedding 26M 82.2 ( +2.3)\n+ Enhanced residual connection 26M 82.4 ( +2.5)\n+ Token labeling with MixToken 26M 83.3 ( +3.4)\n+ Input resolution (224 →384) 26M 84.4 ( +4.5)\nExplicit inductive bias for patch embedding:Ablation analysis of patch embedding is presented\nin Table 9. The baseline is set to the same as the setting as presented in the third row of Table 8.\nClearly, by adding more convolutional layers and narrow the kernel size in the patch embedding,\nwe can see a consistent increase in the performance comparing to the original single-layer patch\nembedding. However, when further increasing the number of convolutional layer in patch embedding\nto 6, we do not observe any performance gain. This indicates that using 4-layer convolutions in patch\nembedding is enough. Meanwhile, if we use a larger stride to reduce the size of the feature map,\nwe can largely reduce the computation cost, but the performance also drops. Thus, we only apply a\n13\nconvolution of stride 2 and kernel size 7 at the beginning of the patch embedding module, followed\nby two convolutional layers with stride 1 and kernel size 3. The feature map is ﬁnally tokenized to\na sequence of tokens using a convolutional layer of stride 8 and kernel size 8 (see the ﬁfth line in\nTable 9).\nTable 9: Ablation on patch embedding. Baseline is set as 16 layer ViT with embedding size 384 and\nMLP expansion ratio of 3. All convolutional layers except the last block have 64 ﬁlters. #Convs\nindicatie the total number of convolutions for patch embedding, while the kernel size and stride\ncorrespond to each layer are shown as a list in the table.\n#Convs Kerenl size Stride Params Top-1 Acc. (%)\n1 [16] [16] 25M 81.1\n2 [7,8] [2,8] 25M 81.4\n3 [7,3,8] [2,2,4] 25M 81.4\n3 [7,3,8] [2,1,8] 26M 81.9\n4 [7,3,3,8] [2,1,1,8] 26M 82.2\n6 [7,3,3,3,3,8] [2,1,1,1,1,8] 26M 82.2\nEnhanced residual connection:We found that introducing a residual scaling factor can also bring\nbeneﬁt as shown in Table 10. We found that using smaller scaling factor can lead to better performance\nand faster convergence. Part of the reason is that more information can be preserved in the main\nbranch, leading to less information loss and better performance.\nTable 10: Ablation on enhancing residual connection by applying a scaling factor. Baseline is a\n16-layer vision transformer with 4-layer convolutional patch embedding. Here, function F represents\neither self-attention (SA) or feed forward (FF).\nForward Function #Parameters Top-1 Acc. (%)\nX ←−X + F(X) 26M 82.2\nX ←−X + F(X)/2 26M 82.4\nX ←−X + F(X)/3 26M 82.4\nLarger input resolution:To adapt our model to larger input image, we interpolate the positional\nencoding and ﬁne-tune the model on larger image resolution for a few epochs. Token labeling\nobjective as well as MixToken are also used during ﬁne-tuning. As can be seen from Table 8,\nﬁne-tuning on larger input resolution of 384 ×384 can improve the performance by 1.1% for our\nLV-ViT-S model.\nB.2 Beyond Vision Transformers: Performance on MLP-Based and CNN-Based Models\nWe further explore the performance of token labeling on other CNN-based and MLP-based models.\nResults are shown in Table 11. Besides our re-implementation with more data augmentation and\nregularization technique, we also list the results from the original papers. It shows that for both\nMLP-based and CNN-based models, token labeling objective can still improve the performance over\nthe strong baselines by providing the location-speciﬁc dense supervision.\nTable 11: Performance of the proposed token labeling objective on representative CNN-based\n(ResNeSt) and MLP-based (Mixer-MLP) models. Our method has a consistent improvement on all\ndifferent models. Here †indicates results reported in original papers.\nModel Mixer-S/16 [35] Mixer-B/16 [35] Mixer-L/16 [35] ResNeSt-50 [51]\nToken Labeling \u0017 \u0017 ✓ \u0017 \u0017 ✓ \u0017 \u0017 ✓ \u0017 \u0017 ✓\nParameters 18M 18M 18M 59M 59M 59M 207M 207M 207M 27M 27M 27M\nTop-1 Acc. (%) 73.8† 75.6 76.1 76.4† 78.3 79.5 71.6† 77.7 80.1 81.1† 80.9 81.5\n14\nB.3 Comparison with CaiT\nCaiT [37] is currently the best transformer-based model. We list the comparison of training hyper-\nparameters and model conﬁguration with CaiT in Table 12. It can be seen that using less training\ntechniques, computations, and smaller model size, our LV-ViT achieves identical result to the state-\nof-the-art CaiT model.\nTable 12: Comparison with CaiT [37]. Our model exploits less training techniques, model size, and\ncomputations but achieve identical result to CaiT.\nSettings LV-ViT (Ours) CaiT [37]\nTransformer Blocks 20 36\n#Head in Self-attention 8 12\nMLP Expansion Ratio 3 4\nEmbedding Dimension 512 384\nStochastic Depth [24] 0.2 (Linear) 0.2 (Fixed)\nRand Augmentation [11] \u0013 \u0013\nCutMix Augmentation [48] \u0013\nMixUp Augmentation [52] \u0013\nLayerScaling [37] \u0013\nClass Attention [37] \u0013\nKnowledge Distillation \u0013\nEnhanced Residuals (Ours) \u0013\nMixToken (Ours) \u0013\nToken Labeling (Ours) \u0013\nTest Resolution 384 ×384 384 ×384\nModel Size 56M 69M\nComputations 42B 48B\nTraining Epoch 300 400\nImageNet Top-1 Acc. 85.4 85.4\nC Visualization\nWe apply the method proposed in [ 6] to visualize both DeiT-base and our LV-ViT-S. Results are\nshown in Figure 6 and Figure 7. In Figure 6, we can observe that our LV-ViT-S model performs\nbetter in locating the target objects and hence yields better classiﬁcation performance with high\nconﬁdence. In Figure 7, we visualize the top-2 classes predicted by the two models. Noted that we\nfollow [6] to select images with at least 2 classes existing. It can be seen that our LV-ViT-S trained\nwith token labeling can accurately locate both classes while the DeiT-base sometimes fails in locating\nthe entire target object for a certain class. This demonstrates that our token labeling objective does\nhelp in improving models’ visual grounding capability because of the location-speciﬁc token-level\ninformation.\n15\nOriginal OriginalDeiT-Base DeiT-Base LV-ViT-S LV-ViT-S\nBittern BramblingBittern 38.1% Brambling 42.8% Bittern 88.0% Brambling 90.1%\nDalmatian JacamarDalmatian 40.3% Jacamar 44.8% Dalmatian 85.9% Jacamar 87.4%\nShih-Tzu BramblingShih-Tzu 40.3% Brambling 53.8% Shih-Tzu 85.9% Brambling 88.0%\nFigure 6: Visual comparisons between DeiT-base and LV-ViT-S.\nOriginal DeiT-Base Top-2 LV-ViT-S Top 2\nBasset  26.5% Basset 76.4%Lorikeet 3.8% Lorikeet 10.6%\nTusker 16.3% Zebra 13.0% African \nelephant 42.6%\nZebra 19.3%\nZebra 32.7% Tusker 9.8% Zebra 58.5% African \nelephant 18.1%\nZebra 52.5% Ostrich 0.4% Zebra 80.3% African \nelephant 5.8%\nBull mastiff  \n19.0%\nBull mastiff \n48.3%\nTiger cat 8.4% Tabby cat 8.9%\nFigure 7: Visual comparisons between DeiT-base and LV-ViT-S for the top-2 predicted classes.\n16",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.948904275894165
    },
    {
      "name": "Computer science",
      "score": 0.7918943166732788
    },
    {
      "name": "Transformer",
      "score": 0.6371445655822754
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5137451887130737
    },
    {
      "name": "Sequence labeling",
      "score": 0.41245758533477783
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3962744176387787
    },
    {
      "name": "Machine learning",
      "score": 0.35173457860946655
    },
    {
      "name": "Speech recognition",
      "score": 0.32162895798683167
    },
    {
      "name": "Computer network",
      "score": 0.07741758227348328
    },
    {
      "name": "Engineering",
      "score": 0.05747029185295105
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}