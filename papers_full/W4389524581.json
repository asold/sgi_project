{
  "title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
  "url": "https://openalex.org/W4389524581",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5063732194",
      "name": "Parishad BehnamGhader",
      "affiliations": [
        "McGill University",
        "Centre Universitaire de Mila"
      ]
    },
    {
      "id": "https://openalex.org/A5013678286",
      "name": "Santiago Miret",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5102886212",
      "name": "Siva Reddy",
      "affiliations": [
        "McGill University",
        "Centre Universitaire de Mila",
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3125238517",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3210877910",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3169726359",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W4385573102",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385569780",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4385570777",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4385570384",
    "https://openalex.org/W4205870266",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4398757454",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2996848635"
  ],
  "abstract": "Augmenting pretrained language models with retrievers has shown promise in effectively solving common NLP problems, such as language modeling and question answering. In this paper, we evaluate the strengths and weaknesses of popular retriever-augmented language models, namely kNN-LM, REALM, DPR + FiD, Contriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved statements across different tasks. Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse, e.g., Flan-T5's performance drops by 28.6% when retrieving 5 statements using Contriever. While larger language models improve performance, there is still a substantial room for enhancement. Our further analysis indicates that multihop retrieve-and-read is promising for large language models like GPT-3.5, but does not generalize to other language models like Flan-T5-xxl. The code is available at https://github.com/McGill-NLP/retriever-lm-reasoning.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15492–15509\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCan Retriever-Augmented Language Models Reason? The Blame Game\nBetween the Retriever and the Language Model\nParishad BehnamGhader1 Santiago Miret3 Siva Reddy1,2\n1McGill University / Mila 2Facebook CIFAR AI Chair 3Intel Labs\n{parishad.behnamghader, siva.reddy}@mila.quebec\nsantiago.miret@intel.com\nAbstract\nAugmenting pretrained language models with\nretrievers has shown promise in effectively solv-\ning common NLP problems, such as language\nmodeling and question answering. In this paper,\nwe evaluate the strengths and weaknesses of\npopular retriever-augmented language models,\nnamely kNN-LM, REALM, DPR + FiD, Con-\ntriever + ATLAS, and Contriever + Flan-T5,\nin reasoning over retrieved statements across\ndifferent tasks. Our findings indicate that the\nsimple similarity metric employed by retriev-\ners is insufficient for retrieving all the neces-\nsary statements for reasoning. Additionally, the\nlanguage models do not exhibit strong reason-\ning even when provided with only the required\nstatements. Furthermore, when combined with\nimperfect retrievers, the performance of the lan-\nguage models becomes even worse, e.g., Flan-\nT5’s performance drops by 28.6% when retriev-\ning 5 statements using Contriever. While larger\nlanguage models improve performance, there\nis still a substantial room for enhancement. Our\nfurther analysis indicates that multihop retrieve-\nand-read is promising for large language mod-\nels like GPT-3.5, but does not generalize to\nother language models like Flan-T5-xxl.1\n1 Introduction\nParametric language models, such as decoder-only\ntransformers (e.g. GPT), transformer encoder mod-\nels (e.g. BERT), and encoder-decoder transformers\n(e.g. T5), encode all necessary knowledge to solve\na given task in their parameters and have demon-\nstrated exceptional performance on many natural\nlanguage tasks (Vaswani et al., 2017; Radford et al.,\n2018; Devlin et al., 2019; Raffel et al., 2020). Non-\nparametric models improve these models further by\naugmenting them with knowledge retrievers (Guu\net al., 2020; Izacard and Grave, 2021; Izacard et al.,\n2022b) or memory components (Khandelwal et al.,\n1The code is available at https://github.com/McGill-\nNLP/retriever-lm-reasoning.\nideal retriever\nretriever\nQuestion: Phobos should be classified\nas which type of body?\nKnowledge Statements\n1. Phobos orbits Mars.\n2. Mars is a kind of planet.\n3. Moons orbit planets.\n4. Phobos is named after the Greek\ngod of fear and panic.\n5. A moon is located in space.\n6. Classifying is a kind of science\nprocess.\nRetrieved Statements\n+ Phobos orbits Mars.\n- Phobos is named after the\nGreek god of fear and panic.\n- Classifying is a kind of\nscience process.\nlanguage\nmodel\nRetrieved Statements\n+ Phobos orbits Mars.\n+ Mars is a kind of planet.\n+ Moons orbit planets.\nkNN-LMorbits Mars.\nmoonFiD\nREALMPhobos\nFlan-T5a moon\nMoons orbit planets.ATLAS\nFigure 1: Example of retriever and language model\nfailures when reasoning is needed. The correct and\nincorrect retrievals are highlighted in green and red ,\nrespectively. This example demonstrates that the re-\ntrievers’ similarity metric is insufficient for retrieving\nrequired statements, and language models cannot per-\nform reasoning over the retrieved statements perfectly.\n2020; Verga et al., 2021; Zhong et al., 2022). This\naugmentation helps them acquire new knowledge\non-the-fly from external sources rather than relying\nsolely on the implicit knowledge encoded in the\nmodel’s parameters, thereby making them more\nrobust to domain shifts (Izacard et al., 2022b). Fur-\nthermore, the retrieved knowledge can provide in-\nsight into what knowledge the model is using.\nWhile the capabilities of parametric language\nmodels have been extensively studied in the lit-\nerature (Wei et al., 2022; Zelikman et al., 2022),\nthere is no thorough study of the limitations of\nnon-parametric models. For instance, Mallen et al.,\n2023 examine the performance of non-parametric\nmemories when encountering less popular knowl-\nedge. In contrast, our work takes a systematic\napproach to study the limitations of retriever-\naugmented language models for reasoning over re-\ntrieved information. As depicted in Figure 1, these\nmodels often fail to solve the tasks that require\nsequential logical reasoning, such as taxonomic\nchaining and combining the details.\n15492\nIn this study, we demonstrate how retriever-\naugmented language models struggle with logi-\ncal reasoning when the task involves reasoning\nover multiple statements. We evaluate these mod-\nels in language modeling (LM) and question an-\nswering (QA) tasks using different variations of\nEntailmentBank (Dalvi et al., 2021) and Strate-\ngyQA (Geva et al., 2021) datasets, where we have\ncontrol over the provided supporting statements\nand reasoning skills. Notably, these datasets do\nnot explicitly indicate the reasoning path within the\nquestion itself, making the retrieval process more\nchallenging. In other words, knowledge statements\nand queries may not have surface-level lexical sim-\nilarity. For instance, the question in Figure 1 has\nno lexical similarity with required statements (2)\nand (3), and without a strong reasoning component,\nmodels would struggle to retrieve and reason upon\nsuch statements.\nConcretely, we analyze the performance ofkNN-\nLM, REALM, DPR + FiD, Contriever + ATLAS,\nand Contriever + Flan-T5. As illustrated in Fig-\nure 1, these models exhibit shortcomings rooted in\nboth parts of their design: 1) Retrievers struggle\nto select all the necessary statements for reason-\ning when using the similarity between query and\nknowledge statements (§4.3.1); 2) Language mod-\nels are imperfect reasoners even when provided\nwith a perfect retriever that retrieves all the essen-\ntial information (§4.3.2); 3) Moreover, the perfor-\nmance of language models deteriorates further if\nthe retriever is imperfect, a closer setting to real-\nity (§4.3.3); 4) Additionally, experimental results\nindicate that while larger language models yield\nimprovements, even the largest models we stud-\nied are imperfect reasoners (§4.3.4); 5) Finally,\nwe observe that employing multihop retrieve-and-\nread enhances GPT-3.5’s performance in reasoning\ntasks, but this improvement does not extend to other\nmodels such as Flan-T5-xxl (§4.3.5).\n2 Background\nThere has been a growing interest in investigat-\ning the reasoning abilities of parametric language\nmodels (Wei et al., 2022; Chung et al., 2022). In\ncontrast, our work focuses on the reasoning abili-\nties of retriever-augmented language models. This\npaper further studies the contributions of each of\nthe models’ components.\nPhobos is named after\nthe Greek god of fear\nand panic.\nRetriever\nLanguage Model\nmoon\nPhobos is a kind of [MASK].\nRetrieved Statements\nMars is a kind of planet.\nPhobos orbits mars.\nMoons orbit planets.\nA moon is located in\nspace.\nKnowledge Statements\nQuery\nquery\nPhobos should be classified\nas which type of body?\nFigure 2: The architecture of retrieve-then-read\nretriever-augmented language models. The language\nmodel predicts the answer using the query and the re-\ntriever’s selected statements.\n2.1 Retriever-Augmented Language Models\nMajor work in retriever-augmented language mod-\nels focused on improving downstream perfor-\nmance such as question answering and language\nmodeling using retrieve-then-read paradigm (Guu\net al., 2020; Izacard and Grave, 2021; Izacard\net al., 2022b; Khandelwal et al., 2020). Fig-\nure 2 illustrates a generic retrieve-then-read ar-\nchitecture. Some of the popular models that we\nstudy are kNN-LM, Retrieval-Augmented Lan-\nguage Model (REALM), Fusion-in-Decoder (FiD),\nand ATLAS (Khandelwal et al., 2020; Guu et al.,\n2020; Izacard and Grave, 2021; Izacard et al.,\n2022b). Each of these leverages retrievers and lan-\nguage models in a unique manner.\n2.2 Role of Retriever\nThe retriever’s role is to retrieve relevant statements\nfor a given query which are then used by the lan-\nguage model. Dense Passage Retriever ( DPR),\nContriever, and REALM’s retriever fetch the most\nsimilar statements based on a similarity metric (i.e.,\ndense inner product) between query and statements’\nrepresentations using two independently trained or\none pre-trained BERT-based encoder (Karpukhin\net al., 2020; Izacard et al., 2022a; Guu et al., 2020).\nIn contrast, kNN-LM adopts an L2 similarity met-\nric between the representation of the query and\npartial token sequences (instead of full knowledge\nstatements) to select the most relevant sequences of\ntokens (Khandelwal et al., 2020). While retrievers\ntypically select statements from a large common\ncorpus in the literature, as depicted in Figure 2, we\nprovide a data-specific set of statements for each\nquery to control the supporting information.\n15493\n2.3 Role of Language Model (Reader)\nThe role of the language model is to make use of\nthe retrieved knowledge to generate relevant text\nfor the given input query. kNN-LM, a decoder-\nonly Transformer, computes the distribution over\nthe next token during generation by interpolating\nbetween the transformer’s next token distribution\nand the next token from the nearest neighbor mem-\nory (Khandelwal et al., 2020). On the other hand,\nREALM is a masked language model backed by a\nBERT-based reader that extracts the most promis-\ning span from one of the statements as the an-\nswer (Guu et al., 2020).\nFiD and ATLAS are both sequence-to-sequence\nT5-based neural networks (Izacard and Grave,\n2021; Izacard et al., 2022b). ATLAS is specifically\ndesigned for jointly finetuning the language model\nand the retriever, employing various pretext tasks\nwith limited training examples. In these models,\nthe encoder encodes the query and each retrieved\nstatement individually, and the decoder attends to\nthese representations to solve the downstream task.\nWhile Flan-T5 was not specifically built for\nretriever-based language modeling, since it is an\ninstruction-tuned model, it can be combined with\nany retriever to complete downstream tasks using\nthe retrieved information (Chung et al., 2022).\n2.4 Multihop Retrieve-and-Read\nIn addition to the previously mentioned widely\nused retrieve-then-read models, multihop retrieve-\nand-read iteratively utilizes textual or dense search\nqueries for a fixed predefined or variable number of\niterations (Xiong et al., 2021; Qi et al., 2021; Khot\net al., 2020; Khattab et al., 2022). For instance,\nDemonstrate-Search-Predict (DSP) can be used in\nmultihop question answering. In each iteration,\nDSP employs large language models to generate\na query by decomposing a complex question into\nsmaller subproblems and summarizes information\nfrom retrieved knowledge (Khattab et al., 2022).\n3 Problem Definition\nIn the main experiments, we provide the mod-\nels with a complete set of knowledge statements\ndenoted as S = {s1,s2,...,s m}for each sam-\nple. In some cases, only a subset of these state-\nments is essential for predicting the answer, re-\nferred to as gold statements. The primary objective\nof the models is to 1) retrieve a set of statements\nSr = {r1,r2,...,r k}⊆ S which find necessary\nModel # Params Model # Params\nLanguage Models\nREALM ∼270M FiD ∼220M\nkNN-LM ∼250M ATLAS ∼250M\nFlan-T5-base ∼250M\nModel Size and Multihop Retrieve-and-Read Analysis\nFlan-T5-small ∼80M Flan-T5-xl ∼3B\nFlan-T5-base ∼250M Flan-T5-xxl ∼11B\nFlan-T5-large ∼780M GPT-3.5 ∼175B\nTable 1: The number of model parameters. We con-\ntrol for model size to circumvent the role of size in\nreasoning abilities. Moreover, we evaluate the impact of\nmodel size and multihop retrieve-and-read using larger\nmodels.\nand 2) effectively solve the target task through rea-\nsoning over the retrieved knowledge Sr. A visu-\nalization of the task is illustrated in Figure 2. We\ncontrol for model size wherever possible for com-\nparable results among different models. We also\nstudy the effect of scaling the model size. Table 1\npresents the number of parameters of the studied\nmodels. Appendix A contains additional imple-\nmentation details. We analyze the performance\nof these models on two tasks, Language Model-\ning (LM) and Question Answering (QA), using\ndatasets (Section 4.1) that are specifically curated\nfor reasoning.\nLanguage Modeling (LM). In the language mod-\neling setup, we evaluate the effectiveness of the\nretriever-augmented language models in a target\nranking task. In this task, the model should assign a\nhigher likelihood to the correct sentence compared\nto (up to) four alternative similar but incorrect sen-\ntences. For instance, in the example illustrated\nin Figure 2, the models should rank the sentence\n“Phobos is a kind of moon. ”higher than an alterna-\ntive sentence such as “Phobos is a kind of planet. ”\nThis task allows comparing masked language mod-\nels like REALM with autoregressive models. We\nexplain this task in more detail in Appendix C.1 and\npresent its experimental results in the Appendix D.\nQuestion Answering (QA). In the question an-\nswering setup, the model should answer a question\ngiven retrieved statements, as illustrated in Figure 2.\nExcept for kNN-LM, all the other models are al-\nready exposed to question answering tasks during\ntheir training.\n15494\n4 Experimental Setting\n4.1 Datasets\nWe assess the performance of the models using\nthe following reasoning datasets. These datasets\nenable us to evaluate the models’ reasoning\nabilities and the retrievers’ performance in LM\nand QA tasks while controlling for the available\nsupporting information.\nEntailmentBank (EB, Dalvi et al., 2021) consists\nof an input question or a hypothesis statement\nthat can be inferred only through multi-step\nreasoning on the provided statements. For the\nQA task, we use the question format of the input,\nand for the LM task (i.e., target ranking), we\nuse the hypothesis statement and alternative\nstatements obtained by replacing the target entity\nwith other alternative entities (see Appendix B).\nAs knowledge statements, we use the provided\ngold and distracting statements in the dataset.\nThe distracting statements mostly contain entities\nor relations mentioned in the input query which\nmight make them deemed relevant but in actuality\nthey are irrelevant. For instance, for the question\n“Which rock type is most useful in studying the\nhistory of living organisms?”, sentecnes “Nearly\nall fossils are found in sedimentary rock. ” and\n“Limestone is a kind of sedimentary rock. ” are\nrelevant, and sentences “Rock means stone. ”and\n“Organisms can be preserved in sedimentary rock. ”\nare distracting and irrelevant. EB consists of\nmultiple splits but we use the EB-2 split since\nits knowledge store contains both required and\ndistracting statements. We call EB-2 split as\nEB-Hard in this paper. For each input, there will be\nup to 25 statements, of which up to 12 statements\nare required to answer correctly. EB-1 is similar to\nEB-2 but without any distracting statements. EB\nalso has EB-3 data which includes 25 relevant and\nirrelevant statements sampled from a large corpus,\nbut we find the trends to be similar to EB-2 (see\nAppendix D).\nStrategyQA (Geva et al., 2021) contains\nyes or no questions accompanied by up to\n5 supporting statements from Wikipedia. To\nevaluate the models in the language modeling\nsetting, we convert each question and answer into\na declarative statement, while also generating\nincorrect statements using alternative entities. The\ndetailed dataset preparation process is explained in\nLanguage Modeling Question Answering0\n20\n40\n60\nDPR kNN-LM REALM Contriever\nTasks\nAccuracy\nFigure 3: Retrievers’ accuracy on EB-Hard test set in\nLM and QA tasks. Results show that retrievers do not\nselect required statements properly, as the best retriever,\nContriever, achieves only a 47% accuracy in QA task.\nAppendix B.\n4.2 Evaluation Metrics\nIn the QA setting, we use the token overlap F1\nscore between the predicted answer and the gold\nanswer. For the LM task, we evaluate the accuracy\nof the models in the target ranking problem.\nFor the retriever, we use accuracy and recall\nscore to indicate the overlap between retrieved\nstatements and the ground-truth statements which\nare essential to obtain the answer.\n4.3 Evaluation and Discussion\nIn this section, we analyze the limitations of both\nretrievers and language models in reasoning tasks.\n4.3.1 The Shortcomings of Retrievers\nCurrent retrievers select k statements based on a\nrelevance score, such as inner product or L2 dis-\ntance between the query’s and statements’ (or spans\nof statements) representations (Guu et al., 2020;\nKarpukhin et al., 2020; Izacard et al., 2022a; Khan-\ndelwal et al., 2020). However, this approach is\ninsufficient for reasoning. Consider the question\n“Phobos should be classified as which type of body?”\nin Figure 1. The retriever may select similar state-\nments to the query, such as (1)“Phobos orbits Mars. ”\nand (4)“Phobos is named after . . . ”, but none of\nthem contains the answer “moon.” Instead, combin-\ning statement (1) with missed statements (2)“Mars\nis a kind of planet. ” and (3)“Moons orbit plan-\nets. ”would provide the answer to the question. But\nstatements (2) and (3) are the ones with the least\nsimilarity.\nWe validate our hypothesis that the similarity-\nbased metric for retrieval is insufficient for reason-\ning using the EB-Hard dataset. Since the EB-Hard\n15495\nModel Query Statements Prediction\nDPR\n+ FiD\nIn a zoo located in a warm region, what\nshould be included in the polar bear ex-\nhibit?\n+ If an animal lives a certain environment then that\nanimal usually requires that kind of environment.\n- Polar bears live in cold environments.\nwarm\nContriever\n+ ATLAS What keeps the Moon orbiting Earth? + Moons orbit planets.\n- Gravity causes orbits. elliptical\nkNN-LM\nThe robot will weigh less on mars than\nearth but will have the same [MASK].\nTargets: mass vs mars\n+ As the force of gravity decreases, the weight of\nthe object will decrease.\n- The gravitational force of a planet does not change\nthe mass of an object on that planet or celestial body.\nmars\nTable 2: Some examples of models’ failures rooted in the retriever. One of the correctly retrieved statements\nand the one that had to be retrieved in order for the model to solve the task correctly are highlighted in green and\nred , respectively. The sequence of tokens leading to the true answer is marked in bold. Results show that current\nretrievers struggle to retrieve required statements for solving the tasks.\ndataset provides us with both gold and distracting\nstatements to answer a question or infer a hypothe-\nsis, we evaluate the accuracy of different retrievers\nwhen we know k, i.e., the exact number of required\nstatements to retrieve for a given input. We present\nthe results in Figure 3. The best-performing re-\ntriever, Contriever, achieves only 57% and 47% ac-\ncuracy in the LM and QA tasks, respectively. DPR,\nthe widely used retriever with BERT-based dual en-\ncoder trained on Natural Questions (Kwiatkowski\net al., 2019), is the worst with an accuracy of only\naround 15%. REALM’s retriever, which is a DPR\ntrained on self-supervised large-scale salient span\nmasking on Wikipedia and fine-tuned on Natural\nQuestions improves the performance by a large\nmargin but still has only 40% accuracy.kNN-LM’s\nretriever is trained on Wikitext-103 (Merity et al.,\n2017) in an auto-regressive fashion to select state-\nments that help predict the next word of the input.\nDue to this, its performance drops 16 points when\nevaluated for the QA task compared to the LM task,\nas the QA task is out-of-distribution for kNN-LM.\nFigure 4 further illustrates the recall score of the\nretrieved statements for varyingk. While all retriev-\ners reach a recall of 100% at 25 (i.e., the maximum\nnumber of knowledge statements for each sample\nin EB-Hard), kNN-LM still struggles due to the\nway it represents the facts in its memory compo-\nnent. In fact, each statement s = w1w2 ...w n is\nstored as n−1 key-value pairs in memory. For\ninstance, the key w1 is paired with value w2 (i.e.,\nnext token), similarly w1w2 with value w3, and so\non. The retriever computes the similarity between\nthe input query and all the keys and selects the top\nksimilar keys (i.e., sequences of tokens) among all.\nWhen allowing kNN-LM to retrieve even 100 keys,\n1\n2\n3\n4\n5\n10\n15\n20\n25\n1000\n50\n100\n1\n2\n3\n4\n5\n10\n15\n20\n25\n1000\n50\n100\nDPR kNN-LM REALM Contriever\nLanguage Modeling\nQuestion Answering\n# of retrieved statements\nRetrieved statements recall score\nFigure 4: Retreivers’ recall score on EB-Hard test set\nin LM and QA based on the number of retrieved\nstatements (k). Contriever is shown to be superior\namong the studied retrievers. Results further indicate\nthat kNN-LM does not cover 100% of the gold state-\nments when k= 100(kNN-LM’s recall is ≈97%).\nit still fails to cover 100% of the gold statements.\nFailures of retrievers illustrated in Table 2\ndemonstrate that relying solely on query-statement\nsimilarity for retrieval may lead to overlooking im-\nportant statements that are dissimilar to the query\nbut contain information essential for reasoning.\n4.3.2 The Shortcomings of Language Models\nThe language model has to reason upon the re-\ntrieved statements to answer a given input. In this\nsection, we assume access to a perfect retriever.\nTo estimate the upper-bound performance of\neach language model, we use the final single state-\nment hypothesis available in EB datasets. This\nstatement can be entailed using an entailment tree\n15496\n1 2 3 4 5 10 15 20 25\n20\n40\n60\nkNN-LM REALM\nDPR + FiD Contriever + ATLAS\nContriever + Flan-T5 Single Statement\n# of retrieved statements\nToken overlap F1 score\nFigure 5: Token overlap F1 score of language models\non EB-Hard QA test set. The dotted and solid lines re-\nfer to experiments given the single oracle statement and\ngold statements (when reasoning is required), respec-\ntively. Results illustrate that language models perform\nworse when answering the question requires reasoning.\nand is sufficient to answer the given input. For\nthe example in Figure 2, this statement is “Phobos\nis a kind of moon ”, which we refer to as the or-\nacle statement. The oracle statement is provided\nto the model as the retrieved statement to answer\nthe question “Phobos should be classified as which\ntype of body? ” (or, for the LM task, to rank the\nsentence “Phobos is a kind of moon” higher than\nothers which is trivial).2\nFigure 5 shows the results of QA task in dot-\nted lines. The models ATLAS, FID, and REALM\nperform the best on this task, as these models are\nmainly trained to perform QA.\nTable 3 shows some outputs of various mod-\nels. These oracle scores would give an estimate\nof the upper-bound performance of models when\nthe task does not require reasoning over multiple\nstatements. Surprisingly, Flan-T5 performs worse\nthan expected as the model struggles to limit it-\nself to the provided knowledge and relies on its\nparametric memory, a common challenge faced by\nlarge language models in non-parametric setting\n(Mallen et al., 2023). This claim is backed by our\nfurther analysis on 50 randomly sampled (from\namong 340 samples) Flan-T5 responses given the\noracle statement. The statistics shown in Figure 6\nshow that 18% of the Flan-T5 responses are not\ngrounded in the oracle statement, even though the\noracle statement was relevant to the question and\nanswer. Additionally, 32% of the responses yield\n2For the LM task, the models achieve high performance\nsince the resulting statement is the same as the target statement\namong the alternatives.\nQuestion: The planets revolve in a counterclockwise\ndirection. The cause of the revolution is mostly due to\nwhich force?\nStatement: Gravity causes planets in the solar system\nto orbit the sun.\nExpected answer: gravitational\nModel: ATLAS Response: gravity\nFlan-T5 gravity\nQuestion: When compared to the Sun, red dwarf stars\nare\nStatement: Red dwarf stars are cooler than the sun.\nExpected answer: cooler\nModel: ATLAS Response: cooler than the sun.\nFlan-T5 a lot smaller\nTable 3: The predictions of models when the oracle\nstatement that contains the answer is provided. F1\nmetric unnecessarily penalizes ATLAS here, although\nits predictions are correct. Flan-T5’s response to the\nsecond question is not grounded to the oracle statement\nand the model is only using its internal knowledge to\nanswer the question.\nOracle is\nrelevant to\nthe question\nOracleis notrelevant tothe question\nResponse is\ngrounded\nResponse is\nnot grounded\nResponse isnot grounded\nExact match\n34%\nLess or more\nelaborate\nresponse\n22%\nSe \nm a n t i c a l l y \nc o r r e c t \n1 0 % \nIncorrect(Correct wrt\ncommonsense)2%\nI n c o r r ec t \n1 0 % \nIncorrect\n(Co\nrrect w\nrt\ncommonsen\nse)\n10%\nI n c o r r ec t \n8 % \nI n c o r r ec t \n4 % \nFigure 6: Human analysis of Flan-T5’s responses\ngiven the oracle statement. As demonstrated in Ta-\nble 3, 18% of the Flan-T5 responses are not grounded\nin the oracle statement and 32% of the responses yield\na low F1 score even though they are correct.\na low F1 score even though they are correct (due\nto elaborateness or wording mismatch). The scores\nare lower than expected since the F1 evaluation\nmetric penalizes when there is a word mismatch\nbetween true and predicted answer even if the pre-\ndicted answer is semantically the same (Kamal-\nloo et al., 2023; Chiesurin et al., 2023; Adlakha\net al., 2023). Some other examples of Flan-T5’s\nresponses are also demonstrated in Table 10.\nAdditionally, we experiment with a setting\n15497\nwhere the models have to reason on multiple re-\ntrieved gold statements. Figure 5 shows the results\nin solid curves. As we provide the models with\nan increasing number of gold statements that are\nessential to answer the input, the performance goes\nup for Flan-T5, FiD, and ATLAS. However, we do\nnot observe any change in REALM and kNN-LM.\nAlthough REALM is a QA model, its limitations\nstem from how it makes use of retrieved documents.\nInstead of reasoning on all documents jointly, like\nATLAS, FiD, and Flan-T5, it reasons on each doc-\nument separately. In pretraining phase, the model\nmarginalizes the score of each span over all docu-\nments. In the inference phase, the model picks the\nspan with the highest retriever and reader score. In\nour setting, this almost always ends up selecting\na span from the first few statements, as the state-\nments are in the order of importance (see Figure 1).\nWe present additional examples in Table 4. kNN-\nLM is not designed to perform QA, which is why it\nperforms worse. Moreover, in our LM experiments,\nwe find that it also underperforms other models,\nso it is unclear when kNN-LM style decoding is\npreferred (see Appendix D.1).\nWhen we contrast the results of reasoning over\nmultiple retrieved gold statements (solid curves in\nFigure 5) with only reasoning on the oracle state-\nment (dotted), the performance of all models is\nmuch lower than the oracle performance even after\nproviding all the essential gold statements. Inter-\nestingly, although Flan-T5’s oracle performance\nis lower than ATLAS, FID, and REALM, it out-\nperforms them when reasoning over multiple state-\nments, indicating that it is a better reasoner. We\nconjecture that Flan-T5’s multi-task training which\nalso includes reasoning tasks like chain-of-thought\nreasoning, makes it a better reasoner than others.\nFurthermore, we investigate the impact of addi-\ntional distracting information on the performance\nof language models in the QA task in Figure 7.\nFlan-T5’s performance drops by 8.7% in the pres-\nence of distracting statements indicating that irrele-\nvant information hurts the performance of language\nmodels. Although ATLAS looks relatively robust,\nits performance is lower than Flan-T5, so we can-\nnot draw a definitive conclusion.\nOn the LM task, we observe similar trends to\nthe QA task and present the results and analysis in\nAppendix D.1.\nModels0\n10\n20\n30\n40\n50\nkNN-LM (gold) kNN-LM (all)\nREALM (gold) REALM (all)\nDPR + FiD (gold) DPR + FiD (all)\nContriever + ATLAS (gold) Contriever + ATLAS (all)\nContriever + Flan-T5 (gold)Contriever + Flan-T5 (all)\nToken overlap F1 score\nFigure 7: The negative impact of additional distract-\ning information on language models’ performance in\nthe QA task. The solid bars and the bars with patterns\nrefer to the experiments with all the gold statements\nand all gold and distracting statements, respectively. It\ncan be observed that providing language models with\ndistracting statements hurts their performance.\n1 2 3 4 5 10 15\n10\n20\n30\n40\nkNN-LM (gold) kNN-LM (all)\nREALM (gold) REALM (all)\nDPR + FiD (gold) DPR + FiD (all)\nContriever + ATLAS (gold) Contriever + ATLAS (all)\nContriever + Flan-T5 (gold) Contriever + Flan-T5 (all)\n# of retrieved statements\nToken overlap F1 score\nFigure 8: Performance of language models on QA\nEB-Hard test set coupled with imperfect retrievers.\nCoupling language models with imperfect retrievers\n(i.e., when a retriever fetches a distracting statement)\ndeteriorates the overall performance.\n4.3.3 The Blame Game: The Impact of\nCombining Imperfect Retrievers and\nLanguage Models\nThis section explores how the combination of lan-\nguage models and imperfect retrievers exacerbates\nthe performance of retriever-then-read models, a\nsetting closer to reality. An incorrect final answer\ncould be blamed on the retriever’s inability to fetch\nrelevant statements or the failure of the language\nmodel to reason on the retrieved statements.\nAdditionally, Figure 8 illustrates the perfor-\nmance of language models on QA EB-Hard dataset,\nwhen coupled with either ideal retrievers (given\nonly gold statements) or imperfect retrievers (given\n15498\nModel Query Retrieved statements Prediction\nFlan-T5 What allows two students standing\nten feet apart to hear each other talk?\n+ Talking is when a human produces sound to communicate.\n+ Sound can travel through air by vibrating air.\na\nmicrophone\nREALM\nAndy lives in the southern hemi-\nsphere. What season does he most\nlikely experience in August?\n+ Andy lives in southern hemisphere.\n+ August is during the winter in the southern hemisphere.\nin southern\nhemisphere\nTable 4: Some examples of models’ failures rooted in the language model. In each example, two correct retrieved\nstatements are illustrated. The true answer is marked in bold. We observe that 1) Flan-T5 does not limit itself to the\nprovided knowledge, and 2) REALM extracts the response from the first retrieved statement, disregarding other\nretrieved statements.\n1\n2\n3\n4\n5\n10\n15\n20\n250\n20\n40\n60\n1 2 3 4 5 100\n20\n40\n60\nkNN-LM REALM\nDPR + FiD Contriever + ATLAS\nContriever + Flan-T5\nToken overlap F1 score\nEntailmentBank-Hard StrategyQA\n# of retrieved statements\nFigure 9: Performance of the retrieval-augmented\nmodels in QA on test sets based on the number of\nretrieved statements. The results demonstrate that al-\nthough Contriever + Flan-T5 and Contriever + ATLAS\nare superior, the studied models perform poorly at rea-\nsoning when answering questions.\ngold and distracting information). The results re-\nveal a significant performance gap, as Flan-T5’s\nperformance in the QA experiment drops by 28.6%\nwhen retrieving 5 statements using Contriever. We\nfurther report the influence of imperfect retriev-\ners on the studied models in the LM task in Ap-\npendix D.1.\nMoreover, Figure 9 illustrates the performance\nof the models on QA reasoning datasets. When\nevaluating REALM and kNN-LM on the Strate-\ngyQA dataset, we append yes/no to each statement.\nThese results highlight the superiority of Contriever\n+ Flan-T5 in our experiments that matches our find-\ning in Section 4.3.2.\nIn order to study which component (retriever\nor LM) is more responsible for the failures of the\nretriever-augmented LMs, we make a hypotheti-\ncal assumption. We assume that we have prior\nknowledge of the exact number of statements (k)\nto be retrieved for each input. We then report the\nnumber of Contriever + Flan-T5’s failure exam-\nples (i.e., samples with F1 score less than 0.5) in\nAppendix D.3. Out of a total of 340 data sam-\n1 2 3 4 5 10 15 20 2510\n20\n30\n40\n50\nFlan-T5-small (80M) Flan-T5-base (250M)\nFlan-T5-large (780M) Flan-T5-xl (3B)\nFlan-T5-xxl (11B)\n# of retrieved statements\nToken overlap F1 score\nFigure 10: Token overlap F1 score of Flan-T5 vari-\nations on EB-Hard QA test set. The results reflect\nthe impressive impact of size on the performance of the\nmodels in the tasks where reasoning is required. We use\nContriever as the retriever in all experiments.\nples, it is noteworthy that the retriever misses at\nleast one gold statement in nearly 85% of the cases.\nAmong these, only 19% are generated correctly by\nthe LM. Conversely, when the retriever performs\nflawlessly (i.e., no missing gold statements), the\nLM correctly responds in 34% of the cases. In sum-\nmary, retriever-augmented LMs’ failures appear to\nbe more attributable to the retriever. This is under-\nscored by the noticeable improvement in the LM’s\nperformance (34% compared to 19%) when the re-\ntriever performs perfectly, emphasizing the pivotal\nrole of the retriever in achieving correct generation.\n4.3.4 The Impact of Model Size\nThis subsection examines the influence of model\nsize on the models’ performance in reasoning tasks.\nSpecifically, we analyze the performance of differ-\nent variations of Flan-T5 coupled with Contriever\non the EB-Hard dataset.\nThe experimental results in Figure 10 demon-\nstrate that larger models achieve better F1 score.\nHowever, there is still a large room for improve-\nment as Flan-T5-xxl achieves only 51.8% F1 when\n15499\nEB-Easy EB-Hard StrategyQA0\n20\n40\n60\n80\nGPT-3 DSP (GPT-3)\nFlan-T5-base DSP (Flan-T5-base)\nFlan-T5-xxl DSP (Flan-T5-xxl)\nDatasets\nToken overlap f1 score\nFigure 11: Token overlap F1 score of GPT-3.5 and\nFlan-T5 variations using multihop DSP program.All\nthe experiments are done with few-shot examples using\nContriever as the retriever and 5 retrieved statements in\neach retrieval step. The experimental results show that\nwhile DSP improves GPT-3.5 performance, it does not\ngeneralize to Flan-T5 models in the F1 score.\nit has to reason over statements but the upperbound\nperformance is 59.7% F1 when provided with the\nsingle oracle statement. The performance of the\nmodels on other QA datasets is presented in Ap-\npendix F.\n4.3.5 Impact of Multihop Retrieve-and-Read\nTo address the limitations of retrieve-then-read\nmodels on reasoning datasets, we explore the ef-\nfectiveness of a strong multihop retrieve-and-read\nframework known as Demonstrate-Search-Predict\n(DSP), employing Contriever along with various\nlanguage models (Khattab et al., 2022). The imple-\nmentation details of the multihop retrieval experi-\nments are provided in Appendix G.\nThe token overlap F1 scores of the models us-\ning the multihop DSP approach are depicted in\nFigure 11. The results indicate that while DSP im-\nproves GPT-3.5’s performance, it still falls short\ncompared to the retrieve-then-read Flan-T5-xxl.\nThis observation shows a large room for improve-\nment in multihop retrieve-and-read methods. Fur-\nthermore, we observe a decline in Flan-T5’s per-\nformance when using multihop retrieval. This can\nbe attributed to Flan-T5’s inability to generate ap-\npropriate subqueries for the given question. Addi-\ntionally, its generated responses tend to include all\nretrieved information, leading to high recall but low\nprecision scores. This phenomenon is further exem-\nplified in the qualitative examples and recall scores\nof the models, as demonstrated in Appendix G.\nIn summary, the results demonstrate the potential\nof multihop retrieve-and-read for large language\nmodels like GPT-3.5, but it does not generalize to\nother models.\n5 Conclusion\nThis paper analyzes the reasoning abilities of\nretriever-augmented language models. We first\nevaluate popular retrieve-then-read models, includ-\ning kNN-LM, REALM, DPR + FiD, Contriever\n+ ATLAS, and Contriever + Flan-T5, through lan-\nguage modeling and question answering tasks.\nOur experimental results indicate that retrievers\nfail to select all essential statements for reasoning\nwhen relying on the similarity between the query\nand statements. Moreover, we observe that lan-\nguage models also struggle to reason over state-\nments even when distracting information is absent.\nThe performance deteriorates further when coupled\nwith imperfect retrievers, as Flan-T5’s performance\ndrops by 28.6% when retrieving 5 statements us-\ning Contriever. Furthermore, while larger language\nmodels show greater capabilities in reasoning, they\nstill have a large room for improvement. Addition-\nally, our experiments on multihop retrieve-and-read\nshow improvements on GPT-3.5, but these improve-\nments do not generalize to other language models\nsuch as Flan-T5-xxl.\nThese findings present opportunities for future\nresearch to enhance the performance of retrieve-\nthen-read models by addressing the aforementioned\nlimitations of retrievers or language models. More-\nover, the development of multihop retrieve-and-\nread models holds promise for advancing reasoning\ntasks.\n6 Limitations\nThis paper examines the reasoning abilities of\nwidely-used retriever-augmented language mod-\nels in both LM and QA settings. To ensure a fair\ncomparison, we employ models with similar sizes\nin both LM and QA tasks. Additionally, we in-\nvestigate the impact of employing larger language\nmodels and a recent, strong multihop retrieve-and-\nread approach on the performance of these models.\nIn this paper, while one can finetune the models\non the specific data, we focus on the capabilities\nof the already pretrained retriever-augmented lan-\nguage models. Moreover, we analyze the models in\ntwo basic NLP tasks, including language modeling\nand question answering. However, one of the stud-\nied models kNN-LM has been pretrained only for\n15500\nlanguage modeling exclusively, which may have\nresulted in a subpar performance on the QA task.\nSurprisingly, kNN-LM also performs the worst at\nthe LM task compared to others.\nFinally, unlike the popular retriever-based meth-\nods in the literature which use large corpus of\nknowledge, we use a data-specific set of statements\nfor each data sample. This allowed us to have more\ncontrol over the retrieved statements and the rea-\nsoning behavior of the language models.\n7 Ethics Statements\nThis is not applicable to this work.\nReferences\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han\nLu, Nicholas Meade, and Siva Reddy. 2023. Eval-\nuating correctness and faithfulness of instruction-\nfollowing models for question answering. arXiv:\n2307.16877.\nSabrina Chiesurin, Dimitris Dimakopoulos, Marco An-\ntonio Sobrevilla Cabezudo, Arash Eshghi, Ioannis\nPapaioannou, Verena Rieser, and Ioannis Konstas.\n2023. The dangers of trusting stochastic parrots:\nFaithfulness and trust in open-domain conversational\nquestion answering. In Findings of the Association\nfor Computational Linguistics: ACL 2023, pages 947–\n959, Toronto, Canada. Association for Computational\nLinguistics.\nHyung Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv: 2210.11416.\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan\nXie, Hannah Smith, Leighanna Pipatanangkura, and\nPeter Clark. 2021. Explaining answers with entail-\nment trees. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7358–7370, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did Aristotle\nUse a Laptop? A Question Answering Benchmark\nwith Implicit Reasoning Strategies. Transactions of\nthe Association for Computational Linguistics, 9:346–\n361.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In Proceed-\nings of the 37th International Conference on Machine\nLearning, ICML’20. JMLR.org.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022a. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022b. Atlas: Few-shot learning\nwith retrieval augmented language models. arXiv:\n2208.03299.\nEhsan Kamalloo, Nouha Dziri, Charles Clarke, and\nDavood Rafiei. 2023. Evaluating open-domain ques-\ntion answering in the era of large language models.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 5591–5606, Toronto, Canada.\nAssociation for Computational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and\nMatei Zaharia. 2022. Demonstrate-search-predict:\nComposing retrieval and language models for\nknowledge-intensive nlp. arXiv: 2212.14024.\n15501\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020. QASC: A\ndataset for question answering via sentence compo-\nsition. In The Thirty-Fourth AAAI Conference on\nArtificial Intelligence, AAAI 2020, The Thirty-Second\nInnovative Applications of Artificial Intelligence Con-\nference, IAAI 2020, The Tenth AAAI Symposium on\nEducational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020 ,\npages 8082–8090. AAAI Press.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 9802–9822, Toronto,\nCanada. Association for Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Process-\ning Systems 32, pages 8024–8035. Curran Associates,\nInc.\nPeng Qi, Haejun Lee, Tg Sido, and Christopher Man-\nning. 2021. Answering open-domain questions of\nvarying reasoning steps from text. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 3599–3614, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam Cohen. 2021. Adaptable and interpretable\nneural MemoryOver symbolic knowledge. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3678–3691, Online. Association for Computational\nLinguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824–24837. Curran Associates,\nInc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nWenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick\nLewis, William Yang Wang, Yashar Mehdad, Scott\nYih, Sebastian Riedel, Douwe Kiela, and Barlas\nOguz. 2021. Answering complex open-domain ques-\ntions with multi-hop dense retrieval. In International\nConference on Learning Representations.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. Star: Bootstrapping reasoning with rea-\nsoning. In Advances in Neural Information Process-\ning Systems, volume 35, pages 15476–15488. Curran\nAssociates, Inc.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation.\nIn Proceedings of the 2022 Conference on Empir-\nical Methods in Natural Language Processing, pages\n5657–5673, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\n15502\nA Implementation Details\nWe present the implementation details of the ana-\nlyzed models in this section. Most of the experi-\nments are conducted using PyTorch (Paszke et al.,\n2019) on an RTX8000 GPU with 48GB memory in\na single run, each taking a few minutes to run. We\nalso run the experiments with large 30B-parameter\nmodels on an A100 GPU with 80GB memory. Note\nthat we have changed the retriever in each model\nto retrieve statements from a sample-specific set of\nstatements instead of a large common corpus.\nIn REALM’s experiments, we use the Hug-\ngingface’s transformers implementation for\nboth masked language modeling and question\nanswering (Wolf et al., 2020). We load the\nrealm-cc-news-pretrained-encoder check-\npoint as a knowledge encoder for masked language\nmodeling and realm-orqa-nq-openqa check-\npoint for question answering. For kNN-LM\nexperiments, we use the best checkpoint available\nin the original papers’ GitHub repository, and we\nfind λ = 0.65 the best value as the interpolation\nhyperparameter based on the experiments on Entai-\nmentBank development sets. In FiD experiments,\nwe use nq_reader_base checkpoint available\nin the papers’ GitHub repository with using\nthe nq.bert-base-encoder’s checkpoint of the\nDPR retriever which is available in their GitHub\nrepository. For experimenting ATLAS, we use\nthe trained atlas_data/models/atlas_nq/base\ncheckpoint of both the language model and\nretriever. Also, for the Flan-T5 model, we load\nthe flan-t5-base model from Huggingface’s\ntransformers to be almost the same size as the\nother models in the main experiments.\nIn order to analyze the impact of the model size,\nwe experiment with various Flan-T5 models from\nHuggingface. Our multihop retrieval experiments\nemploy Contriever as addressed before with Ope-\nnAI’s GPT-3.5 text-davinci-002 and different sizes\nof Flan-T5 model. More concretely, we use the\nsame query formatting and templates as provided\nin the DSP GitHub repository.\nB Dataset Preparation Details\nIn the language modeling experiments, we evalu-\nate the models in target ranking task, where the\nmodel should assing a higher likelihood to the cor-\nrect sentence compared to some alternative ones.\nTherefore, we first create an LM reasoning dataset\nfor StrategyQA by changing the questions and\nModel Target ranking score\nREALM log 1\n|Sr|\n∑\nsj∈Sr p ([MASK] = T|Q, sj) p (sj|Q)\nkNN-LM\nFiD\nFlan-T5\n1\nN log p (QT |Sr), where QT is the query Q with\n[MASK] tokens substituted with T\nATLAS\n1\nM+1 log p (<extra_id_0> T|Q, Sr)\nTable 5: The target scoring function employed by\neach model. Our language modeling task uses target\nranking score to rank multiple sentences with different\ntarget candidates filled-in as answers. Target entity men-\ntion is indicated by T = t1t2 ...t M , input query by\nQ= q1q2 ...q N , and given retrieved statements by Sr.\nFor ATLAS, we found that the T5 setup of predicting\nthe mask with <extra_id_0> performed slightly better\nthan computing the probability of the entire sentence.\nthe yes/no answers into declarative-form sentences.\nThis is because StrategyQA samples only include\nquestions as queries, not sentences. We also use hy-\npothesis sentences of the EntailmentBank dataset\nfor LM experiments. The next step includes cre-\nating alternative sentences for each sample for the\ntarget ranking task in our language modeling exper-\niments. We keep the data samples that include at\nleast one entity mention and mask out the last entity\nmention in the sentences of StrategyQA and En-\ntailmentBank using Spacy (Honnibal and Montani,\n2017). Also, we randomly pick at most four other\nentities mentioned in the data sample’s statements\nas the alternative targets (as described in Section 3)\nand compare the model’s score for each target. Re-\ngarding the question answering experiments, we\nuse datasets’ question and answer formats.\nFor the experiments on the EntailmentBank\ndatasets, we run the experiments on the same devel-\nopment and test sets as the original data. However,\nin the StrategyQA dataset, since we do not have\naccess to the answers in the test split, we cannot\nchange the samples’ formats to declarative form.\nTherefore, we pick 25% and almost 35% of the\ntrain data as the development and test sets, respec-\ntively.\nC Model and Task-Specific Query Format\nThis section includes the model-specific query for-\nmats in each task. As stated in Section 3, we aim to\nstudy the reasoning abilities of retriever-augmented\nlanguage models in language modeling and ques-\ntion answering tasks.\n15503\nModel Alternative Target Scores\nREALM log 1\n|Sr|\n∑\nsj∈Sr p ([MASK] = lithosphere|Q, sj) p (sj|Q)\nlog 1\n|Sr|\n∑\nsj∈Sr p ([MASK] = coal|Q, sj) p (sj|Q)\nkNN-LM\nFiD\nFlan-T5\n1\n8 log p(Surface mining affects the lithosphere and biosphere.|Sr)\n1\n8 log p(Surface mining affects the coal and biosphere.|Sr)\nATLAS\n1\n2 log p (<extra_id_0> lithosphere|Q, Sr)\n1\n2 log p (<extra_id_0> coal|Q, Sr)\nTable 6: A sample of the ranking strategies for each\nmodel for target ranking in the LM task using re-\ntrieved statements Sr. The query (Q) in this example is\n“Surface mining affects the[MASK] and biosphere.” with\nalternative targets “lithosphere” and “coal”. For ATLAS\nexperiments, we replace [MASK] with <extra_id_0>\nwhich is the specific masking token in T5-based mod-\nels.\nC.1 Language Modeling\nAs explained in Section 3, we evaluate the perfor-\nmance of the popular retriever-augmented language\nmodels in the language modeling task through the\ntarget ranking problem. In this task, the model\nshould assign a higher likelihood to the correct sen-\ntence compared to (up to) four alternative similar\nbut incorrect sentences. These candidate sentences\nare generated by replacing the masking tokens with\nalternative entities available in the knowledge state-\nments of the data sample. Table 5 depicts the target\nscoring functions employed by each model for com-\nputing the score of each candidate sentence. These\nalternative target scoring functions are further ex-\nemplified in Table 6 for a better understanding.\nThe way models incorporate retrieved statements\nis explained in Section 2.3. In language modeling\nsetup, the same happens for each model, except\nfor REALM, which LM and QA variants differ. In\nQA, each statement is assigned a score separately,\nand the predicted span is the one with the highest\nretriever and reader score. In LM setup, instead,\nthe score of each alternative target is computed by\nmarginalizing over different retrieved statements,\nas shown in Table 5.\nC.2 Question Asnwering\nIn the question answering setting, on the other hand,\nwe give the whole question to the model and take\nthe generated output as the answer for Entailment-\nBank datasets. In StrategyQA QA experiments, we\ncompute how often models rank the correct yes or\nno answer higher than the other.\nD Main Quantitative Results\nThis section includes more visualizations and de-\ntailed results.\nD.1 Language Modeling\nFirst, we report the overall performance of retriever-\naugmented language models in the reasoning LM\ntask in Figure 12. It can be observed that Flan-\nT5, an instruction-tuned QA model, performs as\nthe best model. On the other hand, kNN-LM, a\npretrained language model, performs as the worst\nmodel in our LM task.\nAdditionally, we demonstrate the impact of addi-\ntional distracting information in Figure 13. Results\nindicate that similar to QA experiments, in LM\ntask, providing the language model with additional\ndistracting information as well as all the required\nstatements generally leads in performance degrada-\ntion.\nFurthermore, we illustrate the impact of the im-\nperfect retrievers on language models in Figure 14.\nResults show that similar to the case with question\nanswering task, the performance of the language\nmodels becomes even worse in LM when combined\nwith imperfect retrievers.\nTable 7 demonstrates the performance of the\nbest retriever-augmented models (based on the per-\nformance on the dev sets) on the test sets in the\nlanguage modeling task.\nD.2 Question Answering\nWe demonstrate the performance of the models on\nEB-Easy and EB-3 test set in question answering\ntask in Figure 15. It can be observed that and\nContriever + Flan-T5 performs the best in these\ndatasets, which matches our findings in the main\npaper. We observe that the models’ performance is\nsimilar to EB-Hard dataset with both relevant and\ndistracting statements.\nWe report the performance of the best retriever-\naugmented models (based on the performance on\nthe development sets) on the test sets in question\nanswering in Table 8.\nD.3 Blame Game Between Retriever and\nLanguage Model\nOur analysis of the Contriever + Flan-T5’s re-\nsponses in a hypothetical scenario (i.e., the exact\nnumber of retrieval is known) is provided in Table 9.\nResults show that the retriever does not retrieve all\n15504\n1 2 3 4 5 10 15 20 2520\n40\n60\n80\n100\n1 2 3 4 5 10 15 20 2520\n40\n60\n80\n100\n1 2 3 4 5 10 15 20 2520\n40\n60\n80\n100\n1 2 3 4 5 10 15 20 2520\n40\n60\n80\n100\nkNN-LM REALM DPR + FiD Contriever + ATLAS Contriever + Flan-T5\nEntailmentBank-Easy EntailmentBank-Hard\nEntailmentBank-3 StrategyQA\n# of retrieved statements\nAccuracy\nFigure 12: The accuracy of the retriever-augmented language models in the target ranking (LM) problem.\nThe results show that Contriever + Flan-T5 and kNN-LM perform as the best and the worst models in our LM\nexperiments.\nModels0\n20\n40\n60\n80\n100\nkNN-LM (gold) kNN-LM (all)\nREALM (gold) REALM (all)\nDPR + FiD (gold) DPR + FiD (all)\nContriever + ATLAS (gold) Contriever + ATLAS (all)\nContriever + Flan-T5 (gold)Contriever + Flan-T5 (all)\nAccuracy\nFigure 13: The negative impact of additional distract-\ning information on language models’ performance in\nthe LM task. The solid bars and the bars with patterns\nrefer to the experiments with all the gold statements\nand all gold and distracting statements, respectively. It\ncan be observed that providing language models with\ndistracting statements hurts their performance.\nthe required statements most of the time. Addition-\nally, when the retriever performs perfectly, the LM\nresponds to the question correctly more often.\nE Main Qualitative Results\nIn Section 4.3.2, we explained that F1 scores of the\nmodels given only the oracle statements are lower\nthan expected. Table 10 demonstrates a few more\n1 2 3 4 5 10 15\n60\n70\n80\n90\nkNN-LM (gold) kNN-LM (all)\nREALM (gold) REALM (all)\nDPR + FiD (gold) DPR + FiD (all)\nContriever + ATLAS (gold) Contriever + ATLAS (all)\nContriever + Flan-T5 (gold) Contriever + Flan-T5 (all)\n# of retrieved statements\nAccuracy\nFigure 14: Performance of language models on EB-\nHard test set in the LM task coupled with imperfect\nretrievers. Coupling language models with imperfect\nretrievers (i.e., retrieving some distracting statements\nas well as some required ones) deteriorates the overall\nperformance.\nexamples of the Flan-T5 failures.\nWe also demonstrate some failure examples in\neach of the retrievers and language models in Ta-\nble 11 where imperfect retrievers and LMs are com-\nbined. In this table, a few true retrieved statements\nand the one that had to be retrieved in order for the\nmodel to solve the task correctly are highlighted\nin green and red, respectively. The true answer (or\n15505\n1 2 3 4 5 10 15 20 250\n20\n40\n60\n1 2 3 4 5 10 15 20 250\n20\n40\n60\nkNN-LM REALM DPR + FiD Contriever + ATLAS Contriever + Flan-T5\nEntailmentBank-1 (Easy) EntailmentBank-3\n# of retrieved statements\nToken overlap F1 score\nFigure 15: The performance of the retriever-augmented language models in the QA task on EB-Easy and\nEB-3 datasets. In EB-3, we observe that models perform similarly to EB-Hard including both gold and distracting\nsupporting information. Furthermore, we observe that in EB-Easy models perform similarly to our experimental\nsettings on EB-Hard with only gold statements, as EB-Easy consists of only required statements with no additional\ndistracting information.\nLanguage Modeling Accuracy\nEB-Easy EB-Hard EB-3 SQA\nkNN-LM 57.92 62.74 61.39 25.06\nREALM 72.92 71.48 77.22 59.73\nDPR +\nFiD 70.83 67.68 71.43 55.48\nContriever\n+ ATLAS 81.25 85.55 84.56 57.94\nContriever\n+ Flan-T5 92.92 91.63 87.64 70.92\nTable 7: Experimental results of the best retriever-\naugmented models in LM on test sets. The two best\nmodels are highlighted in green. The results show that\nFlan-T5 and ATLAS are superior in language modeling\nbased on the target ranking accuracy.\nsequence of tokens leading to the true answer) for\neach data sample’s statements is marked in bold.\nThese examples explain how not retrieving the nec-\nessary statements for reasoning or not reasoning\nover true statements can lead to incorrect answers.\nF Impact of the model size\nIn this paper, we compare various models from\nFlan-T5-small to Flan-T5-xxl with 80M to 11B\nparameters, respectively. The performance of these\nFlan-T5 models accompanied with Contriever on\nEB-Easy and StrategyQA QA datasets is presented\nin Figure 16. Experimental results show that larger\nmodels perform better in reasoning tasks.\nToken overlap F1 score Accuracy\nEB-Easy EB-Hard EB-3 SQA\nkNN-LM 12.62 8.13 10.95 23.94\nREALM 19.43 13.14 16.39 46.76\nDPR +\nFiD 32.14 27.32 32.27 46.98\nContriever\n+ ATLAS 35.95 33.14 37.63 53.69\nContriever\n+ Flan-T5 45.33 39.68 42.29 67.34\nTable 8: Experimental results of the best retriever-\naugmented models in QA on test sets. The two best\nmodels are highlighted in green. The results show that\nFlan-T5 and ATLAS are the superior models in all of\nthe studied datasets.\nImperfect\nRetriever\nPerfect\nRetriever\nLM is Correct 54 (19%) 17 (34%)\nLM is Incorrect 236 (81%) 33 (66%)\nTable 9: An analysis on the blame game between\nthe retriever and LM. This table shows the number\nof failures of Contriever + Flan-T5 on EntailmentBank\nsamples when the number of retrieved statements is\nknown. Results highlight the noticeable improvement\nin the LM’s performance (i.e., 34% compared to 19%)\nwhen the retriever operates perfectly.Imperfect retriever\nstands for cases where retriever misses at least one state-\nment and Incorrect LM refers to cases where the predic-\ntion of the LM is marked as incorrect (i.e., samples with\nless than 0.5 F1 score).\n15506\nModel’s response is semantically correct.\nQuestion: Many animals are still being hunted for their\nfur. Because of this, many of these animals are in danger\nof\nStatement: If hunting decreases the animal population\nto zero, then the animal will be extinct.\nExpected answer: extinction\nResponse: extinct\nModel’s response is incorrect wrt the expected answer\nbut is correct wrt commonsense.\nQuestion: Plants use energy directly from the sun.\nWhat do they use the energy from the sun for?\nStatement: Plants use energy from the sun to make\nfood.\nExpected answer: to make food\nResponse: to grow\nModel’s response is completely incorrect.\nQuestion: Which is a step in the process of photosyn-\nthesis?\nStatement: Taking in carbon dioxide is a step in the\nphotosynthesis process.\nExpected answer: plants taking in carbon dioxide\nResponse: releasing light\nTable 10: The predictions of Flan-T5 when the oracle\nstatement is provided. Statistics of the correct and\nincorrect responses are shown in Figure 6.\n1 2 3 4 5 10 15 20 25\n20\n40\n60\n1 2 3 4 5 10 15 20 2550\n60\n70\n80\n90\nFlan-T5-small (80M) Flan-T5-base (250M)\nFlan-T5-large (780M) Flan-T5-xl (3B)\nFlan-T5-xxl (11B)\n# of retrieved statements\nToken overlap F1 scoreAccuracy\nEntailmentBank-Easy\nStrategyQA\nFigure 16: Token overlap F1 score of various sizes\nof Flan-T5 on EB-Easy and StrategyQA test sets\nbased on the number of retrieved statements. The\nresults demonstrate that larger models perform better\nin F1 scores. We use Contriever as the retriever in all\nexperiments.\nEB-Easy EB-Hard StrategyQA0\n20\n40\n60\n80\nGPT-3 DSP (GPT-3)\nFlan-T5-base DSP (Flan-T5-base)\nFlan-T5-xxl DSP (Flan-T5-xxl)\nDatasets\nToken overlap recall score\nFigure 17: The token overlap recall score of GPT-3.5\nand Flan-T5 models using multihop DSP program.\nThe results show that Flan-T5-xxl achieves high recall\nscore with multihop retrieval which can be due to in-\ncluding all the retrieved statements in the response. Al-\nthough getting high recall score, this is not a desired\nbehavior from language models.\nG Impact of multihop retrieval\nThis section reports more details about the stud-\nied multihop retrieval approach (DSP). In multihop\nretrieval experiments, we retrieve 5 statements in\neach retrieval, with the same templates of the orig-\ninal DSP paper (Khattab et al., 2022). Due to the\ngenerally longer context size in multihop retrieval\nsetting and the Flan-T5’s context window limita-\ntions, multihop and retrieve-then-read experiments\ninclude two and five few-shot demonstrations in the\nprompt, respectively. Some of the qualitative ex-\namples of the multihop retrieval approach of DSP\nusing GPT-3.5 and Flan-T5-xxl are illustrated in\nTable 12. Even though Flan-T5-xxl includes the\ncorrect answer tokens in its generated response, it\ncan be observed that the subqueries generated by\nthis model are sometimes nothing but paraphras-\ning or repetition of the original questions, while the\ngoal of the multihop DSP program is to break down\nthe problems into smaller subproblems. Moreover,\nthe Flan-T5-xxl’s responses usually include all the\nretrieved information, which is not desired. The\nrecall score of the models using the multihop DSP\napproach is illustrated in Figure 17 which shows\nthe relatively high recall score of the larger Flan-T5\nmodel due to the problem mentioned above.\n15507\nModel Query Statements Answer\nRetriever’s Failures\nDPR +\nFlan-T5 In a zoo located in a warm re-\ngion, what should be included\nin the polar bear exhibit?\n+ If an animal lives a certain environment then that\nanimal usually requires that kind of environment.\n- Polar bears live in cold environments.\na polar bear\nDPR +\nFiD warm\nContriever\n+ ATLAS\nWhat keeps the Moon orbiting\nEarth?\n- Moons orbit planets.\n- Gravity causes orbits. elliptical\nkNN-LM\nThe robot will weigh less on\nmars than earth but will have the\nsame [MASK].\nTargets: mass vs mars\n+ As the force of gravity decreases, the weight of\nthe object will decrease.\n- The gravitational force of a planet does not change\nthe mass of an object on that planet or celestial body.\nmars\nREALM\nA complete orbit of mercury\naround the sun takes [MASK].\nTargets: around 88 earth days\nvs between 1 and 365\n+ A complete revolution / orbit of a planet around\nits star takes 1 / one planetary year.\n- One mercury year is about 88 earth days.\nbetween 1\nand 365\nIf a new moon occurred on June\n2, when will the next new moon\noccur?\n+ A new moon occurred on june 2.\n+ A moon phase occurs 28 days after the last time it\noccurred.\n- 2 plus 28 equals 30.\njune 2\nLanguage Model’s Failures\nDPR +\nFlan-T5\nWhat allows two students stand-\ning ten feet apart to hear each\nother talk?\n+ Talking is when a human produces sound to com-\nmunicate.\n+ Sound can travel through air by vibrating air.\na micro-\nphone\nDPR +\nFiD\nWhich energy conversion hap-\npens when a person shivers and\nthe energy is transferred to make\nthe muscles and joints move?\n+ A person is a kind of animal.\n+ When an animal moves, chemical energy is con-\nverted to mechanical energy.\n+ Shivering is a kind of shaking.\n+ Shaking is a kind of moving.\nshaking\nContriever\n+ ATLAS\nWave energy from the ocean can\nbe harnessed to power genera-\ntors to make electricity. Energy\nfrom ocean tides can also be\nused to make electricity. How\nwould you categorize these two\nsources of energy?\n+ Tidal energy means energy from ocean tides.\n+ Tidal energy is a renewable resource.\n+ Wave energy is arenewable resource.\nWave\nenergy\nWhich changes will most likely\nhave a negative effect on an\necosystem?\n+ Humans changing ecosystems usually has a nega-\ntive impact on an ecosystem / organisms living in an\necosystem.\n+ Humans building roads in an ecosystem causes\nthat ecosystem to change.\nHumans\nchanging\necosystems\nkNN-LM\nThe mass of earth causes the\npull of gravity on [MASK].\nTargets: earth vs newton\n+ The mass of a planet causes the pull of gravity on\nthat planet.\n+ Earth is a kind of planet.\nnewton\nREALM\nAndy lives in the southern hemi-\nsphere. What season does he\nmost likely experience in Au-\ngust?\n+ Andy lives in southern hemisphere.\n+ August is during the winter in the southern hemi-\nsphere.\nin southern\nhemisphere\nTable 11: A complete set of models’ failure examples rooted in the retriever or language model modules. In\nthe retriever’s failures section, a few correctly retrieved statements and the ones that had to be retrieved in order for\nthe model to solve the task correctly are highlighted in green and red, respectively. The sequence of tokens leading\nto the true answer is marked in bold.\n15508\nQuestion Statements Answer\nWhich characteristic of a cheetah is more likely\nto be learned rather than inherited?\n+ Skills are learned characteristics.\n+ Inherited characteristics are the opposite of learned\ncharacteristics.\n+ Hunting is a kind of skill.\nhunting\nstrategies\nGPT-3.5\nfirst subquery learned characteristics of a cheetah\nfirst rationale [1] and [2] suggest that skills are learned characteristics, while [3] suggests that hunting is a kind\nof skill.\nsecond subquery Is hunting a learned or inherited characteristic in cheetahs?\nsecond rationale\nFirst, we need to understand the definition of each term. [1] and [2] suggest that skills are learned\ncharacteristics, while [3] suggests that hunting is a kind of skill. Therefore, the answer must be a\nlearned characteristic of a cheetah that is not hunting.\nresponse How to run\nFlan-T5-xxl\nfirst subquery What is the most likely characteristic of a cheetah to be learned rather than inherited?\nfirst rationale What is the most likely characteristic of a cheetah to be learned rather than inherited?\nsecond subquery What is the most likely characteristic of a cheetah to be learned rather than inherited?\nsecond rationale What is the most likely characteristic of a cheetah to be learned rather than inherited?\nresponse hunting is kind of skill inherited characteristics are opposite of learned characteristics answer\nhunting\nQuestion Statements Answer\nA bee depends on certain flowers for food. The\nflowers depend on the bee to\n+ Pollination is when pollinating animals, wind, or water\ncarry pollen from one flower to another flower.\n+ A bee is a pollinating animal.\n+ Pollination requires pollinating animals.\n+ Plant reproduction requires pollination.\ncarry pollen\nfor repro-\nduction.\nGPT-3.5\nfirst subquery What do bees eat?\nfirst rationale\n- A bee is a pollinating animal. - Pollination is when pollinating animals, wind, or water carry\npollen from one flower to another flower. - Plant reproduction requires pollination. - Pollination\nrequires pollinating animals.\nsecond subquery What is the relationship between bees and flowers?\nsecond rationale\nFirst, we need to know what pollination is. Pollination is when an animal carries pollen from one\nflower to another. This is important because flowers need to be pollinated in order to reproduce.\nTherefore, the bee depends on the flowers for food, and the flowers depend on the bee for\npollination.\nresponse pollinate\nFlan-T5-xxl\nfirst subquery What do flowers depend on bees for?\nfirst rationale What do flowers depend on bees for?\nsecond subquery Pollination is when pollinating animals, wind, or water carry pollen from one flower to\nanother flower. What do flowers depend on bees for?\nsecond rationale What do flowers depend on bees for?\nresponse Pollination is when pollinating animals, wind, or water carry pollen from one flower to\nanother flower.\nQuestion Statements Answer\nA young boy and his brothers have green eyes.\nThis trait is best described as\n+ Eye color is an inherited characteristic.\n+ Green is a kind of color. inherited\nGPT-3.5\nfirst subquery What is a trait?\nfirst rationale - Eye color is an inherited characteristic. - Green is a kind of color.\nsecond subquery What is the definition of \"inherited characteristic?\"\nsecond rationale We are looking for a word that describes how the trait is passed down. This is an inherited\ncharacteristic, so it is passed down from parents to children.\nresponse inherited\nFlan-T5-xxl\nfirst subquery What is the medical term for the color of a person’s eyes?\nfirst rationale What is the medical term for the color of a person’s eyes?\nsecond subquery Eye color is an inherited characteristic. What is the medical term for the color of a person’s\neyes?\nsecond rationale What is the medical term for the color of a person’s eyes?\nresponse\nEye color is an inherited characteristic. Eye color is a kind of color. Eye color is inherited.\nThe medical term for the color of a person’s eyes is eye color. Eye color is an inherited\ncharacteristic. Eye color is a kind of color.\nTable 12: Some examples of multihop question answering using the DSP approach with Contriever as the\nretriever. In each sample, the generated subqueries, rationales, and final response are presented. The correct and\nincorrect answers included in the generated tokens are highlighted in green and red, respectively.\n15509",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7643198370933533
    },
    {
      "name": "Language model",
      "score": 0.6448087096214294
    },
    {
      "name": "Natural language processing",
      "score": 0.6223140954971313
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5309887528419495
    },
    {
      "name": "Metric (unit)",
      "score": 0.4725489318370819
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210155582",
      "name": "Centre Universitaire de Mila",
      "country": "DZ"
    },
    {
      "id": "https://openalex.org/I1343180700",
      "name": "Intel (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}