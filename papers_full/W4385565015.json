{
  "title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
  "url": "https://openalex.org/W4385565015",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4287858569",
      "name": "Alessandro Stolfo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2972202473",
      "name": "Zhijing Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2807756193",
      "name": "Kumar Shridhar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224599527",
      "name": "Bernhard Schoelkopf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2035754532",
      "name": "Mrinmaya Sachan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4281657280",
    "https://openalex.org/W3043997204",
    "https://openalex.org/W3100879603",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3044873333",
    "https://openalex.org/W4285283606",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572906",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W3176751053",
    "https://openalex.org/W2740314591",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W3176514068",
    "https://openalex.org/W2801890059",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W4380356267",
    "https://openalex.org/W3201339301",
    "https://openalex.org/W3198593990",
    "https://openalex.org/W3204920203",
    "https://openalex.org/W4221151371",
    "https://openalex.org/W3202959906",
    "https://openalex.org/W3212993480",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4220848927",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2250564385",
    "https://openalex.org/W4385573536",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4312516176",
    "https://openalex.org/W4368754753",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W2067439634",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3144194608",
    "https://openalex.org/W2977235550",
    "https://openalex.org/W4229079077",
    "https://openalex.org/W2757050214",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2891688696",
    "https://openalex.org/W3105643199",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4287855115",
    "https://openalex.org/W3212480953",
    "https://openalex.org/W4248145776"
  ],
  "abstract": "Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, Mrinmaya Sachan. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 545‚Äì561\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nA Causal Framework to Quantify the Robustness of\nMathematical Reasoning with Language Models\nAlessandro Stolfo‚àó\nETH Z√ºrich\nstolfoa@ethz.ch\nZhijing Jin‚àó\nMPI & ETH Z√ºrich\njinzhi@ethz.ch\nKumar Shridhar\nETH Z√ºrich\nshkumar@ethz.ch\nBernhard Sch√∂lkopf\nMPI & ETH Z√ºrich\nbs@tue.mpg.de\nMrinmaya Sachan\nETH Z√ºrich\nmsachan@ethz.ch\nAbstract\nWe have recently witnessed a number of im-\npressive results on hard mathematical reasoning\nproblems with language models. At the same\ntime, the robustness of these models has also\nbeen called into question; recent works have\nshown that models can rely on shallow patterns\nin the problem description when generating a\nsolution. Building on the idea of behavioral\ntesting, we propose a novel framework, which\npins down the causal effect of various factors in\nthe input, e.g., the surface form of the problem\ntext, the operands, and math operators on the\noutput solution. By grounding the behavioral\nanalysis in a causal graph describing an intu-\nitive reasoning process, we study the behavior\nof language models in terms of robustness and\nsensitivity to direct interventions in the input\nspace. We apply our framework on a test bed\nof math word problems. Our analysis shows\nthat robustness does not appear to continuously\nimprove as a function of size, but the GPT-3\nDavinci models (175B) achieve a dramatic im-\nprovement in both robustness and sensitivity\ncompared to all other GPT variants.1\n1 Introduction\nMany natural language understanding situations,\nsuch as understanding the financial news, require\nreasoning with text that includes numbers. How-\never, such mathematical reasoning is challenging\nfor NLP models (Cobbe et al., 2021; Mishra et al.,\n2022b). Mathematical reasoning for text has been\nan active area of research for a while (Seo et al.,\n2015; Sachan and Xing, 2017; Sachan et al., 2017,\n2018, inter alia), and has also emerged as a key\ntask to track the capabilities of large language mod-\nels (LLMs) in recent years (Brown et al., 2020;\nOuyang et al., 2022; Wei et al., 2022a, inter alia).\nHowever, despite the impressive performance of\nLLMs on various math reasoning benchmarks (e.g.,\n‚àóEqual contribution.\n1Our code and data are available at https://github.\ncom/alestolfo/causal-math.\nKyle could fit n1=26 drawings on each page. If he has n2=11 \npages, the number of drawings he can make is ___.\nKyle could fit n1=2 drawings on each page. If he has n2=143 \npages, the number of drawings he can make is ___.\nPrediction\nLLMs\nOriginal text:\nExample do-intervention \nby our framework:\nKeep ground-truth g,\nbut change n1, n2\nAfter do-intervention\nPred = 286 = g\nP(286)=0.085\nPred = 143 (incorrect)\nP(286)=0.001\nOriginal prediction\nDistribution of the Predicted Numerical Answer\nP(R) \nFigure 1: Through our framework, we conduct do-\ninterventions on the input and evaluate the change in the\ndistribution P(R) of the prediction Rby LLMs, in this\nfigure, GPT-J. This allows us to measure the causal ef-\nfect of each factor in the input on the model‚Äôs response.\nOuyang et al., 2022; Chowdhery et al., 2022), it\nremains unclear whether these models have learned\nmere artifacts in the data or have truly mastered\nthe mathematical concepts needed to consistently\nsolve all variations of the same problem (Patel et al.,\n2021; Razeghi et al., 2022; Welleck et al., 2022).\nIn sharp contrast with a large number of papers on\nimproving the performance of LLMs on various\ntypes of math-based problems, there has been little\neffort on behavioral analysis of LLMs for these\ntasks. Existing methods for understanding the ro-\nbustness of these models (Patel et al., 2021) rely on\nmanually constructing variations of math problems,\nand we do not yet have a principled, comprehensive\nframework for quantifying such robustness.\nThus, in this work, we propose a formal frame-\nwork based on causal inference, to quantify the ro-\nbustness of NLP models‚Äô math reasoning abilities.\nSpecifically, we describe a causal graph formula-\ntion of math reasoning, where the graph allows us\nto measure the difference in the structural causal\n545\nMath Word \nProblem\nQ\nOperands\nN = (N1, N2, ‚Ä¶ )\nNon-Operand Parts\nT\nOperations\nO\nIrrelevant Surface Form\nS\nCorrect Calculation\nG := fO(N)\nModel‚Äôs Prediction\nR\nDCE(\n)N ‚Üí R\nDCE(\n)\nS ‚ÜíR\nTCE( )N¬†on¬†R\nTCE( )T¬†on¬†R\n: Causal Graph of Human/\nGround-truth Reasoning\nGh\nDCE( )O ‚Üí R\nIntervention\nüîß \nIntervention\nüîß \nRed Arrows:  Potential Spurious Correlations\nBlue Arrow:  Desired EÔ¨Äect\nDCE( )G ‚Üí R\nFigure 2: Causal graph of model predictions on math questions. We highlight the difference between a cognitively-\ninspired correct reasoning path (Gh) and the undesired effects that some factors might have on the model‚Äôs prediction\n(red arrows). By performing controlled interventions of the numerical values (N) and on the textual framing of the\nproblem (T, S), we are able to quantify the causal effects of each factor.\nmodels of human reasoning and model judgment.\nWe consider various causal factors such as the tex-\ntual framing of the question, numerical operands,\nand operation types. Then, we identify a set of\ninterventions in the context of math word prob-\nlems (an example of which is illustrated in Figure\n1), and provide a causal inference framework to\nobtain causal effects of each factor via direct do-\ninterventions (Pearl, 1995) and causal mediation\nanalysis (Pearl, 2001). While our approach is remi-\nniscent of recent studies using causal analysis for\nLLMs (Finlayson et al., 2021; Vig et al., 2020;\nMeng et al., 2022), in this work, we provide a new\ntheoretical analysis framework specifically suitable\nfor math reasoning. Using our framework, we dis-\nentangle factors affecting the model‚Äôs predictions\nand measure their influences. This way, we are able\nto provide insights into the model‚Äôs reasoning in\nterms of robustness and sensitivity with respect to\nchanges in these factors.\nWe apply our framework to study a set of thirteen\nGPT models with various sizes and training proce-\ndures (i.e., instruction-tuned and non-instruction-\ntuned). We observe that, among non-instruction-\ntuned language models, the larger ones tend to be\nmore sensitive to changes in the ground-truth result\nof a math word problem, but not necessarily more\nrobust. However, we observe a different behavior\nin the instruction-tuned GPT-3 models (Ouyang\net al., 2022), which show a remarkable improve-\nment in both sensitivity and robustness, although\nthe robustness reduces when problems get more\ncomplicated. We additionally investigate the role\nof size and instruction tuning on the model‚Äôs per-\nformance with three models of the LLaMA family\n(Touvron et al., 2023) and Stanford Alpaca (Taori\net al., 2023).\n2 Problem Setup\nWe consider a dataset Dof math word problems\n(MWPs), where each MWP is denoted as a ques-\ntion Q. Q is a list (T,N) consisting of a ques-\ntion template T and an ordered list of operands\nN = (N1,N2,...,N m). Each question template\nT := (O,S) further contains two types of informa-\ntion: a set of arithmetic operations O implicitly ex-\npressed in the question, and the text surface formS\nirrelevant to the arithmetic operations. O incorpo-\nrates the information relative to the operations as a\ncollection of tuples {(O1,i1,j1),(O2,i2,j2),... },\nwhere Ok ‚àà{+,‚àí,√ó,√∑}(k‚ààN) and ik,jk ‚ààN\nrepresent the indices of the operands to which op-\nerator Ok should be applied to.2 The ground-truth\nresult G= fO(N) is calculated by computing the\nfunction fO, which represents the application of all\nthe operators inO to the respective operands. We il-\nlustrate the factors in Q and their inter-dependency\nin the causal graph in Figure 2. A two-operand in-\nstance q of Q in this form from Patel et al. (2021)\nis:\nTemplate t: Mark has n1 trees in his\nbackyard. If he plants n2 more, how\nmany trees will he have?\nOperands n: (n1 = 12,n2 = 13)\nOperations o: {(‚Äú+‚Äù, 1, 2)}\nResult: g= fo(n) = n1 + n2 = 25\n2The intermediate result of operation Ol is indicated by\nik = m+ l.\n546\nOur goal is to quantify the robustness of a model\nMon the set of problems q ‚àà D. Ideally, D\nshould be a dataset not seen by the model during\ntraining. We assume that a model takes q as input\nand predicts a probability distribution of the result\nR: P(R |t,n). Our formulation below will be\neasier to understand using this finite discrete set\nand can be generalized to any kind of data pairing\na natural language template with a function that\nmaps a set of operands to a result (e.g., a Python\nprogram; Mishra et al. 2022a).\n3 A Causal Framework\nIn this section, we describe our framework in three\nsteps. First, we define the idea of model robust-\nness on MWPs. Then, we identify possible do-\ninterventions (Pearl, 1995) that we can perform.\nFinally, we describe the causal effects that we mea-\nsure to quantify the robustness of various models.\n3.1 Step 1. Question Reformulation\nWe address the research question ‚ÄúIs a model rea-\nsoning robustly on MWPs? ‚Äù by comparing the\ncausal mechanisms of the model‚Äôs decisions to a\nhypothesized human reasoning mechanism. Note\nthat we do not claim to know how humans reason\nabout these problems. We simply propose a reason-\nable and intuitive way to judge model robustness\ngiven a reasonable and intuitive human reasoning\nmechanism inspired by findings regarding the inde-\npendence of language and mathematical reasoning\nin humans (Brannon, 2005; Monti et al., 2012).\nHuman Reasoning Mechanisms. The causal\nmechanisms of how humans might solve q include\no = fabstract(q) , (1)\ng= fo(n) , (2)\nwhere they first abstract the arithmetic operations\no from the problem q by some cognitive pro-\ncess fabstract, and then apply the operation to the\noperands to obtain the result g. We show these\nmechanisms in the green subgraph Gh of Figure 2.\nModel Reasoning Mechanisms. In contrast, the\ncausal mechanisms of how a model might solve q\nare as follows:\nr= fblackBox(t,n) , (3)\nwhere we are unsure about (1) what part(s) of t the\nmodel takes into account, and (2) how it operates\nover the relevant variables.\nThus, we draw all possible causal mechanisms\nthat might take place in the black-box model\nfblackBox in the complete causal graph in Figure 2.\nSome possible fine-grained causal mechanisms are\n1. The model might attend over the question\ntemplate t in two ways: paying attention to\nthe text surface form s via the causal path\nT ‚ÜíS ‚ÜíR, or text relevant to the math op-\nerations o via the causal path T ‚ÜíO ‚ÜíR.\n2. The model might also attend to the operands\nn := (n1,n2,... ) via a causal path N ‚ÜíR.\n3. If the model learns the correct causal mech-\nanisms as in the human cognitive process,\nit should capture how the operator and the\noperands matter to the ground-truth result g\n(via O ‚ÜíGand N ‚ÜíG) and then the model\nprediction should be sensitive to any changes\nin the ground truth, namelyG‚ÜíR. No spuri-\nous correlations can directly affect Rwithout\ngoing through the mediator G.\nHence, to answer the question ‚ÄúHow robust is\nthe mathematical reasoning of a model on MWPs?‚Äù\nwe can answer the following subquestions:\n1. How does R change in response to G? By\nquantifying this, we assess the sensitivity (cor-\nrect responsiveness) of the model to changes\nin the problem. In other words, does the model\ncorrectly adjust its prediction in response to a\nchange in the correct solution of the problem?\n2. What is the (unwanted) direct causal effect\nsize of S ‚ÜíR, and N ‚ÜíR? We see the\nquantities as a measure of the brittleness (i.e.,\nwrong responsiveness) of the model to result-\npreserving changes in the input. The lower\nthe direct causal effect of Sand N, the more\nrobust the model is.\n3.2 Step 2. Causal Intervention List\nAfter formulating the cognitively-inspired sub-\ngraph Gh and defining the undesired causal paths\nin Figure 2, we list all feasible limited actions that\nallow us to perform our causal analysis. In the con-\ntext of MWPs, we use the following interventions:\n1. Direct intervention on all possible n1,n2,... ;\n2. Partially controllable interventions on T. We\ncan replace the template T in two ways:\n547\n(a) both Sand O are affected, or\n(b) Sis affected but O is not affected.\n3.3 Step 3. Turning Limited Actions into\nCausal Effect Sizes\nNext, we explain how we can obtain the causal\neffect sizes we want (listed in Step 1) from the lim-\nited set of interventions we can do (listed in Step\n2). Specifically, we first start from all the feasi-\nble interventions, and for variables that we cannot\ndirectly intervene on, we apply deductions from\ndo-calculus (Pearl, 1995) to obtain or approximate\nthe direct causal effect sizes. In the following, we\ndescribe a list of causal effect sizes that we need.\nGeneral Formulation. Let us consider an inter-\nvention do(X : x ‚Üíx‚Ä≤), where X ‚àà{T,S, N}\nand a problem Q = {T,N}. The support of the\nnumerical values Ni‚Äôs and R is I ‚äÜN, and we\nconsider N to be distributed uniformly over the set\n{n ‚ààI2 |fO(n) ‚ààI}. We denote the distribution\nbefore the intervention P(R|T,N) as P and the\ndistribution after the intervention as P‚Ä≤.\nFollowing the distributional definition of causal\neffect by Pearl (1995), we quantify the effect of\nfactor Xin our causal graph using a distance metric\nŒ¥between the distributions P and P‚Ä≤. That is,\nCE = Œ¥(P,P ‚Ä≤), (4)\nwhere CE can refer to the total causal effect\n(TCE, i.e., the joint effect through all the directed\ncausal paths from a variable to another), or the\ndirect causal effect (DCE, i.e., the effect from the\ndirected causal path from a variable to another that\ndoes not go through any intermediate variables)\n(Pearl, 2001). We describe our choices for Œ¥ in\nSection 3.4.\nCausal Effects of the Operands. When inter-\nvening on the operands N := (N1,N2,... ), we\ncan obtain the size of the total causal effect of N\non R, namely\nTCE(N on R) := En‚Ä≤‚àºP(N)[Œ¥(P,P ‚Ä≤)], (5)\nwhere P‚Ä≤= P(R|T,do(N = n‚Ä≤)) . (6)\nNote that this TCE is not the exact desired quantity,\nbecause we want to separate two different paths\nof how N affects R: (1) the path N ‚ÜíG ‚ÜíR,\nwhich is the correct decision path that we want\nthe model to pick up (where the model reacts to\nthe change in the ground-truth answer), and (2)\nthe path N ‚ÜíR, which is the spurious correlation\nthat the model might have learned (where the model\nrelies on some spurious correlations with certain\nnumerical values, which could be traced to perhaps\ntheir frequencies in the training corpus).\nWe can quantify the direct causal effect (DCE,\ni.e., the effect from the directed causal path from\na variable to another that does not go through any\nintermediate variables) (Pearl, 2001) of N on R,\nnamely the strength of the direct causal path N ‚Üí\nR, by controlling for Gto be fixed every time we\nintervene on N:\nDCE(N ‚ÜíR) := En‚Ä≤‚àºP(N|G)[Œ¥(P,P ‚Ä≤)], (7)\nwhere P‚Ä≤= P(R|T,do(N = n‚Ä≤)) . (8)\nFor example, if we observe a model doing 100 +\n100 = 200 correctly, we want to separate the math\nability here into (1) the model‚Äôs sensitivity towards\nthe ground-truth answer, and (2) the model‚Äôs deci-\nsions based on its familiarity with just the operand\n100. Here, the overall effect is the calculable\nTCE(N on R) by Eq. 5, and one of the subeffects\nis the calculable DCE(N ‚ÜíR) by Eq. 7.\nCausal Effects of the Text Surface Form. As\nfor the operands, we can compute both the direct\nand indirect effects of the surface form representing\nthe math problem. In particular, intervening on T\nwithout controlling for O (intervention 2a in Sec.\n3.2), we can compute the total effect, i.e.,\nTCE(T on R) := Et‚Ä≤‚àºP(T)[Œ¥(P,P ‚Ä≤)], (9)\nwhere P‚Ä≤= P(R|N,do(T = t‚Ä≤)) . (10)\nControlling for the operations O (intervention\n2b in Sec. 3.2) will instead allow us to obtain the\ndirect causal effect of the surface text:\nDCE(S ‚ÜíR) := Et‚Ä≤‚àºP(T|O)[Œ¥(P,P ‚Ä≤)], (11)\nwhere P‚Ä≤= P(R|N,do(T = t‚Ä≤)) . (12)\nNote that since there is no mediator between Sand\nR, the DCE(S ‚ÜíR) is also TCE of Son R. The\nonly adaptation that we need to make with regard\nto the MWPs is that it is not feasible to enumerate\nall possible perturbations of S. Therefore, the prac-\ntical results that researchers can achieve are over a\ncertain subset of S. In practice, we obtain this by\nintervening on T without affecting O.\n548\nCausal Effects of the Operators. The ideal way\nto obtain the TCE of O on Ris through some care-\nful human annotation that minimally changes the\ntemplates as Kaushik et al. (2020) do for sentiment\nclassification. The challenge for MWPs in our case\nis that with all our possible interventions, we cannot\nonly intervene on O without introducing changes to\nthe irrelevant surface form. However, we might get\nsome information about TCE(O on R) because,\non the causal graph, the total causal influence of\nT on Ractually flows into two directed paths, one\nthrough Sto R(which is the DCE(S ‚ÜíR)), and\nthe other from O to R, which is our interested\nquantity TCE(O on R). Therefore, we compare\nthe two quantities we know, TCE(T ‚ÜíR) and\nDCE(S ‚ÜíR), to get a sense of the causal influ-\nence of O on Rthat we cannot obtain in any other\nway.\n3.4 Step 4. Quantifying the Causal Influence\nConsider a realization of problem Q with operands\nn and ground-truth result g = fo(n), and denote\nby g‚Ä≤the result after the intervention do(X : x‚Üí\nx‚Ä≤). We quantify the causal effect of factor X on\nthe model‚Äôs predictionRin two ways: by assessing\nthe change in the predicted result, and by measur-\ning the change in the probability assigned by the\nmodel to the correct result g(or g‚Ä≤).\nChange in the Prediction. To account for the\ninability of LMs to capture the continuous property\nof numbers (Jin et al., 2021a), we measure the\nchange in the model‚Äôs prediction using an indicator\nof the ‚Äúchange result‚Äù event:\nŒ¥cp(P,P ‚Ä≤) := 1(rÃ∏= r‚Ä≤) , (13)\nwhere r = arg max x‚ààIP(x), and r‚Ä≤ =\narg maxx‚ààIP‚Ä≤(x).\nRelative Change in Confidence. Inspired by Fin-\nlayson et al. (2021), we also highlight the change\nin terms of the relative difference in the probability\nassigned to g and g‚Ä≤. We formulate two types of\nrelative change, one quantifying the relative change\nin the confidence of g, and the other quantifying\nthe relative change in the confidence of g‚Ä≤:\n‚àÜrel = P(g) ‚àíP‚Ä≤(g)\nP‚Ä≤(g) (14)\n‚àÜ‚Ä≤\nrel = P‚Ä≤(g‚Ä≤) ‚àíP(g‚Ä≤)\nP(g‚Ä≤) . (15)\nWe quantify the overall relative change in con-\nfidence (RCC) as the average of the two relative\nchanges above:\nŒ¥rcc(P,P ‚Ä≤) = 1\n2\n√Ö\n‚àÜrel + ‚àÜ‚Ä≤\nrel\n√£\n. (16)\nA Unified Form. We are interested in the average\ncausal effect of the intervention across all problems\nin D. Thus, we measure the average of the effects\nover all instances q ‚ààD. We denote by the sub-\nscripts TCEcp/DCEcp and TCErcc/DCErcc the\ncausal effects computed using the change in predic-\ntion metric and the relative change in confidence,\nrespectively. We describe how we construct the\ndataset Din Section 4.2.\n4 Experimental Setup\nIn this section, we describe the data used to perform\nthe interventions and to measure the causal effects.\n4.1 Datasets\nFor our analyses, we use instances of math word\nproblems from three popular datasets: ASDiv-A\n(Miao et al., 2020), MAWPS (Koncel-Kedziorski\net al., 2016), and SV AMP (Patel et al., 2021).\nThe examples contained in these collections are\npairs (t,o) consisting of a question template t with\nits annotated operations o. Each of these pairs\ncan be instantiated multiple times into problems\nq = ( t,n) by filling the template with numeri-\ncal values (n1,n2,... ) and computing the ground-\ntruth result g= fo(n) (most problems involve two\nto three operands, i.e., |n|‚àà{ 2,3}). We select\na set of 437 two-operand and 307 three-operand\ntemplate-expression pairs that we use to gener-\nate pairs of prompts representing an intervention.\nMore details about the prompt generation proce-\ndure are in Appendix A. We use (t,n) to refer to\nan instantiated template that we use as a prompt.\n4.2 Intervention Data\nGiven an MWP q = ( t,n) and its solution g,\nwe generate a second problem-solution instance\n(q‚Ä≤,g‚Ä≤) depending on the type of causal effect CE\nwe want to measure and on the considered variable.\nWhen intervening on the operands of the problem,\nthe text of the problem is kept unaltered and a set of\nnew operands n is sampled in such a way that the\nresult gis affected or not depending on the effect\nthat is being measured. When changing the textual\ndescription of the problem, we change t such that\neither o‚Ä≤ = o, or o‚Ä≤ Ã∏= o. In the former case, we\nsample a different template t‚Ä≤ = (s‚Ä≤,o) from the\n549\nset of templates describing the same operations o,\nin the latter case we sample a new t‚Ä≤describing\na different operation. In Appendix B.1 we report\nsome examples of (q,q‚Ä≤) pairs representing the\ndifferent types of interventions.\nGiven a model, we use the question pair (q,q‚Ä≤)\nto obtain a pair of answer distributions P(R|t,n)\nand P(R|t‚Ä≤,n‚Ä≤), which we use to measure the\ncausal effect of the intervention. We consider\nthe space for the numerical values to be I =\n{1,2,...,C }consisting of integer values, follow-\ning the setup of several existing MWP datasets\n(Miao et al., 2020; Koncel-Kedziorski et al., 2016;\nPatel et al., 2021). To control our experimental\ncosts and make sure the models keep the number\nas one token, we set C = 300. From all the tokens\nin a model‚Äôs vocabulary, we focus on the proba-\nbility assigned to the numbers in our numerical\nspace I, and thus we use P(R = r) to denote\nthe normalized probability Praw(R= r)/Z, where\nZ = ‚àëC\nr=1 Praw(R = r), and Praw(x) is the raw\nprobability score assigned to the vocabulary token\nx. For each intervention type, we generate a dataset\nDconsisting of (q,q‚Ä≤) pairs. Unless otherwise\nspecified, for our experiments we generate 500 in-\ntervention pairs for each template, and results are\naveraged over three seeds.\n4.3 Models to Evaluate\nWe use our framework to assess the robustness of\nreasoning in thirteen pre-trained language models.\nWe consider five sizes of the GPT-2 model (Radford\net al., 2019): distilled (Sanh et al., 2019), small,\nmedium, large, and XL. We evaluate four models\nfrom EleutherAI that were pre-trained on the Pile\n(Gao et al., 2020): GPT-Neo 1.3B and 2.7B (Black\net al., 2021), GPT-J-6B (Wang and Komatsuzaki,\n2021), and GPT-NeoX-20B (Black et al., 2022).\nWe use HuggingFace Transformers (Wolf et al.,\n2019) to access the models. Additionally, we ex-\nperiment with a set of instruction-tuned versions of\nGPT-3 (Brown et al., 2020): Instruct (Ouyang et al.,\n2022), Curie, Davinci-002, and Davinci-003.3 Ex-\nperiments with GPT-3 are carried out under the\nconstraints set by the OpenAI APIs 4, which pre-\nvent us from computing the causal effect using the\nsame procedure as for the other models. We report\nthe details about how the metrics were computed\n3The OpenAI ids for these models are, respec-\ntively, davinci-instruct-beta, text-curie-001,\ntext-davinci-002, and text-davinci-003.\n4https://openai.com/api/\nJ-6B\nNeo-1.3BNeo-2.7BDistilledSmall Large\nMedium\nXL\n3-Curie*\nNeoX\n3-Davinci-002*\n3-Instruct*\n3-Davinci-003*\n100\n101\n102\n103\n104\n105\nŒ¥rcc\nDCErcc of N TCErcc of N\nJ-6B\nNeo-1.3BNeo-2.7BDistilledSmall Large\nMedium\nXL\n3-Davinci-002\n3-CurieNeoX\n3-Instruct\n3-Davinci-003\n0.2\n0.4\n0.6\n0.8\n1\nŒ¥cp\nDCEcp of N TCEcp of N\nFigure 3: Comparison of DCE(N ‚Üí R) and\nTCE(N on R). ‚àóapprox values, see Appendix C.\nfor GPT-3 in Appendix C. In the reported results,\nwe indicate with an asterisk ( ‚àó) the metrics that\nwere influenced by this limitation.\n5 Results\nOur analyses focus primarily on two-operand prob-\nlems (Sections 5.1 and 5.2) and later extend to more\ncomplex problems that involve three operands (Sec-\ntion 5.5) for the models that perform best on the\ntwo-operand test bed. We compare the direct causal\neffect DCE and the total causal effect TCE of N\nand T on R. DCE represents the undesired effect\nfor a model to being mistakenly responsive to a\nchange in N or T not leading to a change in the\nresult g (low robustness), whereas higher values\nof TCE indicate a higher ability of the model to\ncorrectly adjust the probability weight assigned\nto the new solution g‚Ä≤after the intervention (high\nsensitivity).\n5.1 Effect of N on R\nFrom the results in Figure 3, we notice that larger\nmodels exhibit a larger TCE rcc/DCErcc ratio. In\nparticular, in GPT-J-6B and NeoX, the TCE is, re-\nspectively, 30x and 1000x larger than the DCE.\nHowever, this improvement in sensitivity is not\nmanifested in terms of change of prediction (Œ¥cp),\nfor which the models show to be affected by result-\npreserving changes almost as equally as by result-\naltering interventions. This behavior changes sig-\n550\n0 5 10 15 20 25 30 35 40 45 50\n0 5 10 15 20 25 30 35 40 45 50\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0 5 10 15 20 25 30 35 40 45 50\n0 5 10 15 20 25 30 35 40 45 50\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0 5 10 15 20 25 30 35 40 45 50\n0 5 10 15 20 25 30 35 40 45 50\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nFigure 4: Heatmaps displaying P(g) for Distil-GPT-2 (left), GPT-J-6B (center), and GPT-3 Davinci-002 (right).g\nis the ground-truth result g= n1 + n2 (n1 and n2 are represented by the x and y axes, respectively. The probability\nvalues for each combination of ((n1,n2),g) are averaged over 20 different templates. Probability values over 0.2\nare displayed with the darkest color.\nJ-6B\nNeo-1.3BNeo-2.7BDistilledSmall Large\nMedium\nXL\n3-Curie*\nNeoX\n3-Davinci-002*\n3-Instruct*\n3-Davinci-003*\n100\n101\n102\n103\n104\n105\nŒ¥rcc\nDCErcc of S TCErcc of T\nJ-6B\nNeo-1.3BNeo-2.7BDistilledSmall Large\nMedium\nXL\n3-Davinci-002\n3-CurieNeoX\n3-Instruct\n3-Davinci-003\n0.2\n0.4\n0.6\n0.8\n1\nŒ¥cp\nDCEcp of S TCEcp of T\nFigure 5: Comparison of DCE(S ‚Üí R) and\nTCE(T on R). We use ‚àó to denote approximated val-\nues, explained in Appendix C.\nnificantly in instruction-tuned models. In particular,\nfor the 175B-parameter GPT-3, performance varies\ndepending on the type of supervision, with the PPO-\ntrained Davinci-003 exhibiting an 84% difference\nbetween direct and total effect.\nIn Figure 4, we present a different visualization\nof the direct causal effect of N on the model‚Äôs\nprediction. We report the heatmaps showing the\nprobability assigned by the model to the result g\nof a problem (t,(n1,n2),g) |g= n1 + n2, ‚àÄg‚àà\n{0,1,..., 50}, ‚àÄ(n1,n2) ‚àà{0,1,..., 50}2. For\nDistil-GPT-2 we observe low overall probability\nassigned to gand diagonal patterns indicating con-\nsistency in assigning higher probability to specific\nresults (e.g., 10, 20, 30, 40, 50). For the two larger\nmodels we notice a higher probability mass as-\nsigned to the problem‚Äôs result, but less consistency\non the prediction of the same result with different\nsets of operands (this is true for GPT-J in particular).\nThis result is consistent with the observed higher\nDCE and TCE in larger models: P(g) might vary\nmore considerably when intervening on N without\naffecting g, but overall the model assigns higher\nprobability weight to the correct result, which cor-\nrelates with higher sensitivity.\n5.2 Effect of T on R\nIn Figure 5, we report the total causal effect of\nthe textual framing T and the direct causal effect\nof the irrelevant text elements S on the model‚Äôs\nprediction. For the instruction-tuned models, the\nimprovement in terms of prediction change ( Œ¥cp)\nfollows a similar trend as for N, with GPT-3\nDavinci-003 showing a 76% difference between\ndirect and total effect. An interesting observation\nis that the irrelevant textual information Sappears\nto have a lower direct effect than N for all non-\ninstruction-tuned models. However, in the GPT-3\nDavinci-00x models, we observe the opposite (i.e.,\nDCE(N ‚ÜíR) ‚â§DCE(S ‚ÜíR)). This suggests\nthat large instruction-based models tend to be more\nsusceptible to variation in the textual framing of a\nproblem, while smaller models are more respon-\nsive to changes in the numerical values (though not\nnecessarily correctly).\n5.3 Overall Insights\nIn comparison to other models, GPT-3 Davinci\nshows the highest DCErcc, but low DCEcp. This\ndiscrepancy is related to the quantities that the two\nmetrics consider. Œ¥rcc takes into account the prob-\nability assigned to g, while Œ¥cp does not consider\nthe ground truth solution. One interpretation of this\nresult is that GPT-3 Davinci consistently predicts\nthe same answer r= r‚Ä≤when g= g‚Ä≤, but the prob-\nabilities P(g) and P‚Ä≤(g) might vary significantly.\n551\nThe results observed for the two kinds of inter-\nvention do(T : t ‚Üít‚Ä≤) and do(N : (n1,n2) ‚Üí\n(n‚Ä≤\n1,n‚Ä≤\n2)) show similar trends. Small models (Dis-\ntilled and Small GPT-2) exhibit low sensitivity to\ninterventions. Larger models (from GPT-2 Medium\nto GPT-Neo) appear to be more influenced by\nchanges in both N and T. However, they display\nsimilar sensitivity to both result-altering and result-\npreserving interventions. An improvement in sen-\nsitivity is noticeable in GPT-J and NeoX, though\nnot accompanied by an improvement in robustness.\nRemarkably different behavior is instead shown\nby the GPT-3 Davinci models, which demonstrate\nsubstantially higher sensitivity to result-altering in-\nterventions (high TCE), and higher robustness (in\nterms of prediction change). In Appendix B.2, we\nreport the accuracy of the models on the generated\ninstances of MWPs, which exhibits a similar trend\nas the robustness/sensitivity changes we observed.\nPossible explanations for the improved robust-\nness and sensitivity demonstrated by the large GPT-\n3 models might be the dramatic size increase and\nextension/enhancement of the training procedure\ninvolving instructions. The former idea is aligned\nwith the emergent abilities hypothesis (Wei et al.,\n2022a), which postulates the existence of skills\nthat are displayed by large-scale models but are\nnot present in smaller-scale models. However, our\nobservations show different performances in ver-\nsions of GPT-3 Davinci that differ in the training\nprocedure.5 This raises the question of whether the\ncapability of LLMs to reason about math problems\nbenefits from instruction-based tuning. We address\nthis question in the following section.\n5.4 Extending to LLaMA-Based Models\nTo further investigate the roles played by size and\ntraining method in the model‚Äôs performance, we\ncarry out our experimental procedure on three ver-\nsions with different sizes (7B, 13B, and 30B) of\nthe LLaMA model (Touvron et al., 2023), and on\nStanford Alpaca (which applies instruction tuning\non LLaMA 7B) (Taori et al., 2023). We present\nthese results separately, as the LLaMA tokeniza-\ntion makes the prediction setup different from the\none used from the other models, and prevents us\nfrom computing the relative change in confidence\n5A high-level description of the training procedures for\nthe models is provided at https://beta.openai.com/\ndocs/model-index-for-researchers .\nAlpaca\nLLaMA 7BLLaMA 13BLLaMA 30B\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nŒ¥cp\nDCE of N TCE of N\nFigure 6: Comparison of direct and total effects of N\non Rfor LLaMA and Alpaca.\n(Œ¥rcc).6\nFrom the results (Figure 6), two notable observa-\ntions emerge. Firstly, the increased difference be-\ntween TCE and DCE observed with the increasing\nsize of the LLaMA models suggests that a larger\nnumber of parameters can be a significant driver\nbehind robustness/sensitivity improvement. How-\never, this is not necessarily the case across dif-\nferent models: GPT-NeoX-20B shows a smaller\nTCEcp-DCEcp gap compared to LLaMA 7B (5.2%\nvs 9.0%). Secondly, the instruction tuning proce-\ndure of Alpaca does not seem to help significantly\nwith mathematical computation: the decrease in\nboth TCE and DCE shows that robustness improves\nat the expense of sensitivity. Nonetheless, over-\nall, when comparing Alpaca compared to its base\nmodel, LLaMA 7B, we observe an increase in the\ngap between TCE and DCE, although this differ-\nence is minimal (9.5% vs 9.0%).\nThe limited improvement of Alpaca might be at-\ntributed to its instruction tuning procedure consist-\ning of ‚Äúa list of user-oriented instructions including\nemail writing, social media, and productivity tools‚Äù\n(Taori et al., 2023), which differs from reasoning-\nintensive tasks. We suggest future work to examine\ndifferent types of instruction tuning (e.g., focused\non reasoning procedures or reinforcement learn-\ning from human feedback), which might help the\nmodel answer more complex types of questions in\na step-by-step manner and more accurately. We\nhypothesize that the different performances in ver-\nsions of GPT-3 Davinci might be produced by the\nspecific type of instructions used for training, by\nthe reinforcement learning component (Ouyang\net al., 2022), or simply by an extension of the lan-\nguage modeling pre-training. It is challenging to\n6The LLaMA tokenizer considers each digit as an inde-\npendent token in the vocabulary. This makes it problematic\nto compare the probability value assigned by the model to\nmulti-digit numbers.\n552\n3-Davinci-002*\n3-Instruct*\n3-Davinci-003*\n100\n101\n102\n103\n104\nŒ¥rcc\nDCE of N TCE of N\n3-Instruct\n3-Davinci-0033-Davinci-002\n0.7\n0.8\n0.9\n1\nŒ¥cp\nFigure 7: Comparison of direct and total effects of N\non Rfor three-operand problems.\npinpoint the exact factor in the training procedure\nthat contributes to this improvement, as specific\nmethodological details are not available.\n5.5 Moving to Three-Operand Problems\nWe extend our evaluation to consider the three-\noperand problems in the dataset. In these ex-\nperiments, we consider only the GPT-3 175B-\nparameter models, as they are the only models\nperforming well on the simpler bivariate problems.\nThe results regarding the effects of N are reported\nin Figure 7. We notice that the large difference be-\ntween the desired (TCE) and undesired (DCE) ef-\nfects observed on simpler problems shrinks signifi-\ncantly for both metrics. In particular, for Davinci-\n003, the direct effect ofN (measured as Œ¥cp) grows\nfrom 0.17 to 0.87. That is, GPT-3 Davinci-003 pre-\ndicts a different result 87% of the time after an\nintervention that does not affect the ground-truth\nsolution. The increase in direct effect indicates a\nperformance degradation in terms of brittleness:\neven the models that show good performance on\ntwo-operand problems, now display an unstable\nbehavior after result-preserving interventions.\n6 Related Work\nCausal NLP. Causal inference aims to study the\ncause and effect from observational and interven-\ntional data (Pearl, 2009; Peters et al., 2017). Tra-\nditionally, researchers usually apply causal tech-\nniques to phenomena in nature and human society.\nWith the rise of powerful models in NLP, recent\nresearch has started to explore the intersection of\ncausal inference and NLP, forming the study of\nCausal NLP (Jin et al., 2022; Feder et al., 2021a).\nThere are several formulations for Causal NLP:\nthe causality for NLP thread involves using the\ncausal framework for data collection and task for-\nmulation (Jin et al., 2021c), inspecting the (path-\nspecific) causal effect of certain neurons on pre-\ndictions (Vig et al., 2020; Meng et al., 2022), un-\nderstanding the causal effect of data and learning\nparadigm for model performance (Ni et al., 2022),\nand as a way to frame prompts (Lyu et al., 2023);\nand NLP for causality involves testing the pure\ncausal inference skills of LLMs (Jin et al., 2023a,b),\nand use text as a variable for causal effect estima-\ntion (Roberts et al., 2020; Veitch et al., 2020; Jin\net al., 2021b, 2023c).\nThe most similar line of research to our work\nis the application of causal effect estimation on in-\nterpreting models‚Äô behavior, such as how models\nunderstand syntactic agreement (Finlayson et al.,\n2021), and how interventions in the representations\nand weights affect the model prediction (Feder\net al., 2021b). To the best of our knowledge, our\nwork is the first to formulate a causal framework\nfor robustness behavioral tests, and also we are the\nfirst to introduce the idea to quantify the differences\nin the causal mechanisms of human reasoning and\nmodel decisions.\nMath Reasoning in NLP.A growing body of work\ntries to improve the math reasoning capability in\nNLP models (Zhang et al., 2020; Geva et al., 2020;\nSpokoyny et al., 2021), and prompting techniques\nfor LLMs (Cobbe et al., 2021; Shen et al., 2021;\nKojima et al., 2022; Wei et al., 2022b; Chowdh-\nery et al., 2022). For analysis, significant atten-\ntion has been given to models‚Äô ability to under-\nstand numerical quantities (Wallace et al., 2019;\nThawani et al., 2021) and numerical operations (Pal\nand Baral, 2021; Berg-Kirkpatrick and Spokoyny,\n2020; PiÀõ ekos et al., 2021; Razeghi et al., 2022).\n7 Conclusion\nWe developed a framework to disentangle and sep-\narately measure the effect of different factors influ-\nencing the predictions of LLMs for math reasoning.\nOur results indicate that a drastic increase in both\nrobustness and sensitivity emerges in the GPT-3\nDavinci models. Additionally, we study the contri-\nbution of size and instruction tuning in the models\nof the LLaMA family, observing that the Alpaca\ninstruction tuning, while increasing the model‚Äôs ro-\nbustness, does not significantly improve the overall\nperformance. Our framework provides a formal-\nized theory of behavioral testing for math reasoning\nmodels and opens new future directions to design\nbehavioral tests of models in a principled way.\n553\nEthical Considerations\nAs for the ethical practice in this work, the data\ninvolved are from existing MWP datasets with no\nprivate user information, and available under the\nMIT license. As for the ethical impact of the use\nof this work, the study is about providing a metric\nand analyzing existing models‚Äô robustness, so there\nis less concern over harmful usage. Rather, it is\nmore about putting checks on existing AI models\nand helping humans understand them better before\nuse. Potential stakeholders that could benefit from\nthis research include NLP researchers working on\nmath models, practitioners working on various ap-\nplications involving mathematical reasoning with\ntext, and e-learning design.\nLimitations\nA key limitation in our work is that LLMs might\nhave seen these math problems. Our work the-\noretically assumes this is not the case. Another\nlimitation is that for the sake of simplicity, our\nwork makes some assumptions. For example, we\nassume all numbers in the range of integers 0 to\nC = 300. This would not cover every MWP out\nthere. And future work is needed to generalize our\nframework to other forms of MWPs. In this work,\nwe are also constrained by the limitations of the\nOpenAI policy on the GPT-3 API. This limits the\nnumber of perturbations we consider in this work\nas well as the accuracy with which we can esti-\nmate our causal distributions. Finally, our work\nis restricted to English, and extending it to other\nlanguages will require us to create an MWP dataset\nin that language.\nAcknowledgments\nThis material is based in part upon works sup-\nported by the German Federal Ministry of Edu-\ncation and Research (BMBF): T√ºbingen AI Center,\nFKZ: 01IS18039B; by the Machine Learning Clus-\nter of Excellence, EXC number 2064/1 ‚Äì Project\nnumber 390727645; by the John Templeton Foun-\ndation (grant #61156); by a Responsible AI grant\nby the Haslerstiftung; and an ETH Grant (ETH-19\n21-1). Alessandro Stolfo is supported by armasu-\nisse Science and Technology through a CYD Doc-\ntoral Fellowship. Zhijing Jin is supported by PhD\nfellowships from the Future of Life Institute and\nOpen Philanthropy, as well as the travel support\nfrom ELISE (GA no 951847) for the ELLIS pro-\ngram. We also thank OpenAI Researcher Access\nProgram for granting our team credits to their API.\nReferences\nTaylor Berg-Kirkpatrick and Daniel Spokoyny. 2020.\nAn empirical investigation of contextualized number\nprediction. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4754‚Äì4764, Online. Association for\nComputational Linguistics. 9\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, et al. 2022.\nGPT-NeoX-20B: An open-source autoregressive lan-\nguage model. arXiv preprint arXiv:2204.06745. 6\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large scale autore-\ngressive language modeling with mesh-tensorflow. If\nyou use this software, please cite it using these meta-\ndata. 6\nElizabeth M. Brannon. 2005. The independence of\nlanguage and mathematical reasoning. Proceedings\nof the National Academy of Sciences, 102(9):3177‚Äì\n3178. 3\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877‚Äì1901. Curran Associates, Inc.\n1, 6\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311. 1, 9\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168. 1, 9\nAmir Feder, Katherine A. Keith, Emaad Manzoor, Reid\nPryzant, Dhanya Sridhar, Zach Wood-Doughty, Ja-\ncob Eisenstein, Justin Grimmer, Roi Reichart, Mar-\ngaret E. Roberts, Brando n M. Stewart, Victor Veitch,\nand Diyi Yang. 2021a. Causal inference in natural\nlanguage processing: Estimation, prediction, inter-\npretation and beyond. CoRR, abs/2109.00725. 9\n554\nAmir Feder, Nadav Oved, Uri Shalit, and Roi Reichart.\n2021b. CausaLM: Causal model explanation through\ncounterfactual language models. Computational Lin-\nguistics, 47(2):333‚Äì386. 9\nMatthew Finlayson, Aaron Mueller, Sebastian\nGehrmann, Stuart Shieber, Tal Linzen, and Yonatan\nBelinkov. 2021. Causal analysis of syntactic\nagreement mechanisms in neural language models.\nIn Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long\nPapers), pages 1828‚Äì1843, Online. Association for\nComputational Linguistics. 2, 5, 9\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\n6\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 946‚Äì958, Online. Association for Computa-\ntional Linguistics. 9\nZhihua Jin, Xin Jiang, Xingbo Wang, Qun Liu,\nYong Wang, Xiaozhe Ren, and Huamin Qu.\n2021a. Numgpt: Improving numeracy ability\nof generative pre-trained models. arXiv preprint\narXiv:2109.03137. 5\nZhijing Jin, Yuen Chen, Felix Leeb, Luigi Gre-\nsele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fer-\nnando Gonzalez Adauto, Max Kleiman-Weiner,\nMrinmaya Sachan, and Bernhard Schoelkopf. 2023a.\nCladder: Assessing causal reasoning in language\nmodels. 9\nZhijing Jin, Amir Feder, and Kun Zhang. 2022.\nCausalNLP tutorial: An introduction to causality for\nnatural language processing. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing: Tutorial Abstracts, pages 17‚Äì\n22, Abu Dubai, UAE. Association for Computational\nLinguistics. 9\nZhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrin-\nmaya Sachan, Rada Mihalcea, Mona Diab, and Bern-\nhard Schoelkopf. 2023b. Can large language models\ninfer causation from correlation? 9\nZhijing Jin, Zhiheng Lyu, Yiwen Ding, Mrinmaya\nSachan, Kun Zhang, Rada Mihalcea, and Bernhard\nSchoelkopf. 2023c. AI Scholars: A dataset for NLP-\ninvolved causal inference. 9\nZhijing Jin, Zeyu Peng, Tejas Vaidhya, Bernhard\nSchoelkopf, and Rada Mihalcea. 2021b. Mining the\ncause of political decision-making from social media:\nA case study of COVID-19 policies across the US\nstates. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021, pages 288‚Äì301,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics. 9\nZhijing Jin, Julius von K√ºgelgen, Jingwei Ni, Tejas\nVaidhya, Ayush Kaushal, Mrinmaya Sachan, and\nBernhard Schoelkopf. 2021c. Causal direction of\ndata collection matters: Implications of causal and\nanticausal learning for NLP. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9499‚Äì9513, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics. 9\nDivyansh Kaushik, Eduard H. Hovy, and Zachary Chase\nLipton. 2020. Learning the difference that makes A\ndifference with counterfactually-augmented data. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net. 5\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916. 9\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. MAWPS:\nA math word problem repository. In Proceedings of\nthe 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1152‚Äì1157, San\nDiego, California. Association for Computational\nLinguistics. 5, 6\nZhiheng Lyu, Zhijing Jin, Justus Mattern, Rada Mi-\nhalcea, Mrinmaya Sachan, and Bernhard Sch√∂lkopf.\n2023. Psychologically-inspired causal prompts.\nCoRR, abs/2305.01764. 9\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 35:17359‚Äì17372. 2, 9\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nEnglish math word problem solvers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 975‚Äì984, Online.\nAssociation for Computational Linguistics. 5, 6\nSwaroop Mishra, Matthew Finlayson, Pan Lu, Leonard\nTang, Sean Welleck, Chitta Baral, Tanmay Rajpuro-\nhit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark,\net al. 2022a. Lila: A unified benchmark for mathe-\nmatical reasoning. arXiv preprint arXiv:2210.17517.\n3\nSwaroop Mishra, Arindam Mitra, Neeraj Varshney,\nBhavdeep Sachdeva, Peter Clark, Chitta Baral, and\nAshwin Kalyan. 2022b. NumGLUE: A suite of fun-\ndamental yet challenging mathematical reasoning\ntasks. In Proceedings of the 60th Annual Meeting of\n555\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 3505‚Äì3523, Dublin,\nIreland. Association for Computational Linguistics.\n1\nMartin M Monti, Lawrence M Parsons, and Daniel N\nOsherson. 2012. Thought beyond language: Neural\ndissociation of algebra and natural language. Psycho-\nlogical science, 23(8):914‚Äì922. 3\nJingwei Ni, Zhijing Jin, Markus Freitag, Mrinmaya\nSachan, and Bernhard Sch√∂lkopf. 2022. Original\nor translated? A causal analysis of the impact of\ntranslationese on machine translation performance.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5303‚Äì5320, Seattle, United States. Association\nfor Computational Linguistics. 9\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155. 1, 2, 6, 8\nKuntal Kumar Pal and Chitta Baral. 2021. Investigating\nnumeracy learning ability of a text-to-text transfer\nmodel. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 3095‚Äì3101,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics. 9\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080‚Äì2094, Online.\nAssociation for Computational Linguistics. 1, 2, 5, 6,\n14\nJudea Pearl. 1995. Causal diagrams for empirical re-\nsearch. Biometrika, 82(4):669‚Äì688. 2, 3, 4\nJudea Pearl. 2001. Direct and indirect effects. In UAI\n‚Äô01: Proceedings of the 17th Conference in Uncer-\ntainty in Artificial Intelligence, University of Wash-\nington, Seattle, Washington, USA, August 2-5, 2001,\npages 411‚Äì420. Morgan Kaufmann. 2, 4\nJudea Pearl. 2009. Causality. Cambridge University\nPress. 9\nJonas Peters, Dominik Janzing, and Bernhard Sch√∂lkopf.\n2017. Elements of causal inference: Foundations\nand learning algorithms. The MIT Press. 9\nPiotr PiÀõ ekos, Mateusz Malinowski, and Henryk\nMichalewski. 2021. Measuring and improving\nBERT‚Äôs mathematical abilities by predicting the or-\nder of reasoning. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 383‚Äì394, Online. Association\nfor Computational Linguistics. 9\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. 6\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning. arXiv preprint\narXiv:2202.07206. 1, 9\nMargaret E Roberts, Brandon M Stewart, and Richard A\nNielsen. 2020. Adjusting for confounding with text\nmatching. American Journal of Political Science ,\n64(4):887‚Äì903. 9\nMrinmaya Sachan, Kumar Dubey, and Eric Xing. 2017.\nFrom textbooks to knowledge: A case study in har-\nvesting axiomatic knowledge from textbooks to solve\ngeometry problems. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 773‚Äì784. 1\nMrinmaya Sachan, Kumar Avinava Dubey, Tom M\nMitchell, Dan Roth, and Eric P Xing. 2018. Learning\npipelines with limited data and domain knowledge:\nA study in parsing physics problems. Advances in\nNeural Information Processing Systems, 31. 1\nMrinmaya Sachan and Eric Xing. 2017. Learning\nto solve geometry problems from natural language\ndemonstrations in textbooks. In Proceedings of the\n6th Joint Conference on Lexical and Computational\nSemantics (* SEM 2017), pages 251‚Äì261. 1\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: Smaller, faster, cheaper and lighter. In\nNeurIPS EMC2 Workshop. 6\nMinjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren\nEtzioni, and Clint Malcolm. 2015. Solving geome-\ntry problems: Combining text and diagram interpre-\ntation. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1466‚Äì1476, Lisbon, Portugal. Association for\nComputational Linguistics. 1\nJianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin\nJiang, Ming Zhang, and Qun Liu. 2021. Generate &\nrank: A multi-task framework for math word prob-\nlems. arXiv preprint arXiv:2109.03034. 9\nDaniel Spokoyny, Ivan Lee, Zhao Jin, and Taylor Berg-\nKirkpatrick. 2021. Masked measurement prediction:\nLearning to jointly predict quantities and units from\ntextual context. arXiv preprint arXiv:2112.08616. 9\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca. 2, 8\n556\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644‚Äì656, Online. As-\nsociation for Computational Linguistics. 9\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971. 2, 8\nVictor Veitch, Dhanya Sridhar, and David M. Blei. 2020.\nAdapting text embeddings for causal inference. In\nProceedings of the Thirty-Sixth Conference on Un-\ncertainty in Artificial Intelligence, UAI 2020, virtual\nonline, August 3-6, 2020 , volume 124 of Proceed-\nings of Machine Learning Research, pages 919‚Äì928.\nAUAI Press. 9\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stuart\nShieber. 2020. Investigating gender bias in language\nmodels using causal mediation analysis. Advances in\nNeural Information Processing Systems, 33:12388‚Äì\n12401. 2, 9\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know num-\nbers? probing numeracy in embeddings. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5307‚Äì5315, Hong\nKong, China. Association for Computational Linguis-\ntics. 9\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A\n6 billion parameter autoregressive language model. 6\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682. 1, 8\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903. 9\nSean Welleck, Peter West, Jize Cao, and Yejin Choi.\n2022. Symbolic brittleness in sequence models: on\nsystematic generalization in symbolic mathematics.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 8629‚Äì8637. 1\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R‚Äôemi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. HuggingFace‚Äôs transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771. 6\nXikun Zhang, Deepak Ramachandran, Ian Tenney,\nYanai Elazar, and Dan Roth. 2020. Do language\nembeddings capture scales? In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 4889‚Äì4896, Online. Association for Computa-\ntional Linguistics. 9\nA Creation of the Prompts\nWe consider MWP examples from the union of\nthe three datasets SV AMP, ASDiv-A, and MAWPS.\nThe textual template t of a problem consists of a\ncontext (describing a real-world state and/or ac-\ntions) and a question. In order to obtain suitable\nprompts for the models, we convert the problems‚Äô\nquestions into statements where the result of the\nproblem is expected to be the first token after the\nprompt. E.g., in the example in section 2, how\nmany trees will he have? is converted into the num-\nber of trees that he will have is _. From the MWP\ntemplates of the SV AMP/ASDiv-A/MAWPS col-\nlection (we consider all splits), we filter out the\ntemplates whose questions do not start with How\nmany..., and we use spaCy7 to identify the subject,\nthe object and the verbs in the sentence. This al-\nlows us to convert the last sentence of the template\nfrom The number of... is. This way, we obtain 437\nstatement-based MWP templates for two-operand\nproblems and 307 for three-operand problems. We\nmanually checked a subset of the templates to iden-\ntify possible mistakes in the conversion procedure.\nB Frequently Asked Questions\nB.1 How do the intervention data look like?\nIn Table 1 we report examples of MWP pairs rep-\nresenting different types of intervention.\nB.2 What is the accuracy of the evaluated\nmodels on the generated problems?\nWe report the accuracy of the models considered for\nevaluation in terms of accuracy at 1 and accuracy\nat 10. Results are displayed in Figure 8.\nB.3 What is the relation between accuracy\nand the RCC metric?\nWe examine the relationship between performance\nand robustness, computing the Pearson correla-\ntion coefficient between accuracy (accuracy@10)\nand the relative confidence change (RCC) met-\nric. On a per-template basis (500 instances for\neach template), we found accuracy to be positively\n7https://spacy.io\n557\nTCE(N ‚ÜíR)\nRuby has 87 candies. If she shares the candies among 29\nfriends, the number of candies that each friend gets is g= 87/29 = 3\nRuby has 35 candies. If she shares the candies among 5\nfriends, the number of candies that each friend gets is g= 35/5 = 7\nDCE(N ‚ÜíR)\nThe school is composed of 13 buildings each having 10\nclassrooms. The number of classrooms that the school has is g= 10 √ó13 = 130\nThe school is composed of 65 buildings each having 2 class-\nrooms. The number of classrooms that the school has is g= 65 √ó2 = 130\nDCE(S‚ÜíR)\nThe razorback t-shirt shop ordered 6 cases of t-shirts. If\neach case contains 17 t-shirts the number of t-shirts that they\nordered is\ng= 17 √ó6 = 102\nThe roller coaster at the state fair costs 6 tickets per ride. If\n17 friends were going to ride the roller coaster the number of\ntickets that they would need is\ng= 17 √ó6 = 102\nTCE(T ‚ÜíR)\nSean has 23 whistles. He has 6 more whistles than Charles.\nThe number of whistles that Charles has is g= 23 ‚àí6 = 17\nJovana filled her bucket with 23 pounds of shells. If she\nadds 6 more pounds of shell to fill her bucket, the number of\npounds that she has is\ng= 23 + 6 = 29\nTable 1: For each of the causal effects measured (left column), we report a pair of MWPs illustrating the intervention\nperformed (center), along with their respective ground-truth result (left column).\n3-Davinci-002\nJ-6B\nNeo-2.7BNeo-1.3B\nXLLarge\nMediumRegularDistilled 3-CurieNeoX\nDavinci-Instruct3-Davinci-003\n0\n0.2\n0.4\n0.6\n0.8\n1\nAvg. Accuracy\nAccuracy@1 Accuracy@10\nJ-6B\nNeo-2.7BNeo-1.3B\nXLLarge\nMediumRegularDistilled 3-CurieNeoX\n0\n2\n4\n¬∑10‚àí2\nAvg. Accuracy\nAccuracy@1\nFigure 8: Average accuracy of the models on the gen-\nerated instances of MWPs. Results are averaged over\ntwo sets consisting of 500 problem instances generated\nfor each template. The lower figure shows a zoomed-in\nvisualization of the accuracy at 1.\ncorrelated with TCE(N on R) and TCE(T on R)\n(0.24 and 0.49, respectively) and negatively cor-\nrelated with DCE(N ‚ÜíR)and DCE(S ‚ÜíR)\n(-0.26 and -0.36, respectively). We see these re-\nsults as a quantitative validation of the intuition\nbehind our framework: the better the model‚Äôs per-\nformance, the more the model tends to correctly\nadjust its prediction after a result-altering interven-\ntion (higher sensitivity) and to correctly not change\nits prediction after a result-preserving intervention\n(higher robustness).\nMoreover, we conduct an additional sanity check\nas in Patel et al. (2021): removing the question\nfrom the MWP templates, we observe a sensitivity-\nrobustness degradation to random guessing (i.e.,\nTCE ‚âÉDCE). This indicates that the measurement\nof the causal effects within our framework is not\naffected by patterns in the templates that might\nhave been picked up or memorized by large models.\nC Computation of Causal Effects for\nGPT-3\nWe access GPT-3 through the OpenAI APIs, which\nallow a user to prompt the model and obtain the\nprobabilities assigned by the model to thek-th most\nlikely vocabulary entries, for each token generated.\nTo overcome this limitation, we approximate the\n558\nrelative probability change Œ¥rcc as follows, depend-\ning on the kind of effect measured.\nThe limit for kis set by OpenAI to 5. However,\nfor our main set of experiments (i.e., computing the\ncausal effects of N, S, and T) we were granted\nan increased limit of kto 100. This allowed us to\nobtain reasonable estimates for the causal effects,\nas the number of cases in whichP(g) is not defined\nis less than 10% of the number of examples that\nwe consider.\nAlgorithm 1 Computation of Œ¥rcc for GPT-3\nQ = (t,n,g)\nQ‚Ä≤= (t‚Ä≤,n‚Ä≤,g‚Ä≤)\nif P(g) is defined then\nif P‚Ä≤(g) is defined then\n‚àÜ = P(g)‚àíP‚Ä≤(g)\nP‚Ä≤(g)\nelse\nÀÜP‚Ä≤‚ÜêP‚Ä≤(k-th most likely token)\n‚àÜ = P(g)‚àíÀÜP‚Ä≤\nÀÜP‚Ä≤\nend\nelse\n‚àÜ = 0\nend\nif P‚Ä≤(g‚Ä≤) is defined then\nif P(g‚Ä≤) is defined then\n‚àÜ‚Ä≤= P‚Ä≤(g‚Ä≤)‚àíP(g‚Ä≤)\nP(g‚Ä≤)\nelse\nÀÜP ‚ÜêP(k-th most likely token)\n‚àÜ‚Ä≤= P‚Ä≤(g‚Ä≤)‚àíÀÜP\nÀÜP\nend\nelse\n‚àÜ‚Ä≤= 0\nend\nŒ¥rcc = 1\n2 (‚àÜ + ‚àÜ‚Ä≤)\nC.1 TCE(N on R) and TCE(T on R)\nIn cases when P(g) is defined (i.e. when gappears\nin the top k token predictions) and P‚Ä≤(g) is not\ndefined, we compute a lower bound on the relative\nchange using the upper bound on P‚Ä≤(g) given by\nthe probability of the k-th most likely token. This\ngives us a conservative estimate of ‚àÜ. For cases\nin which P(g) is not defined, we cannot say any-\nthing about the relative change, and we set ‚àÜ = 0.\nThe same applies when swapping P and P‚Ä≤. This\nprocedure is illustrated by Algorithm 1.\nC.2 DCE(N ‚ÜíR) and DCE(S ‚ÜíR)\nIn this case, we simply discard the examples for\nwhich P(g) is not defined or P‚Ä≤(g) are not defined.\nIn that is not the case, then we compute Œ¥rcc as in\nSection 3.4.\nC.3 Heatmap Illustration\nThe heatmap for GPT-3 displayed in Figure 4 was\ncomputed by taking the raw probability score pro-\nduced by the model over the whole vocabulary,\nas the limit on the available top predicted tokens\nmakes it impossible to normalize it over the set\n{0,..., 300}, as done for the other models. The\nprobability was set to 0 when gdid not appear in\nthe model‚Äôs top 5 predictions for the next token\nafter the prompt.\nD Computing Infrastructure & Inference\nDetails\nTo run our experiments, we used a single NVIDIA\nTITANRTX with 24GB of memory for all the ver-\nsions of GPT-2 and GPT-Neo. We used a single\nNVIDIA A100 with 40GB of memory for GPT-J-\n6B and a single NVIDIA A100 with 80GB of mem-\nory for GPT-NeoX and the LLaMA models (two\nfor the 30B version). We accessed GPT-3 using\nthe OpenAI APIs. The longest run (GPT-J) on the\nfour kinds of experiments corresponding to the four\nkinds of effects measured took ‚àº12 hours, using\n500 MWP instances for each of the 437 templates.\nDue to budget and resource constraints, the exper-\niments on GPT-3, GPT-NeoX, and LLaMA were\ncarried out using 20 examples generated for each\ntemplate and took ‚àº7 hours. Experiment tracking\nwas carried out using Weights & Biases8.\n8http://wandb.ai/\n559\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nSection \"Limitations\".\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nSection \"Ethical Considerations\".\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nSection 1: Introduction.\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nSection 4\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.1\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSection \"Limitations\"\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection \"Limitations\"\n‚ñ°\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSection \"Limitations\"\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection \"Limitations\"\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nSection 4.1 and Appendix A\nC ‚ñ°\u0013 Did you run computational experiments?\nSections 4 and 5\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSections 4.3 and Appendix D\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n560\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSections 4.3\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSections 4.1, 4.2, and 5\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4.3 and Appendix A\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n561",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7067047953605652
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.7004965543746948
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40434756875038147
    },
    {
      "name": "Natural language processing",
      "score": 0.33144786953926086
    },
    {
      "name": "Programming language",
      "score": 0.32318753004074097
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    }
  ]
}