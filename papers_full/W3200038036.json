{
  "title": "OH-Former: Omni-Relational High-Order Transformer for Person Re-Identification",
  "url": "https://openalex.org/W3200038036",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5000390160",
      "name": "Xianing Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066662339",
      "name": "Jialang Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101925114",
      "name": "Jiale Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5034339267",
      "name": "Shenghua Gao",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3183430956",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2963330186",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2962761264",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W2905016804",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W2965046076",
    "https://openalex.org/W3035186652",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W3180659574",
    "https://openalex.org/W3097870364",
    "https://openalex.org/W2104657103",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2963365374",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3034611771",
    "https://openalex.org/W2967515867",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W2981420411",
    "https://openalex.org/W2967359135",
    "https://openalex.org/W3096285474",
    "https://openalex.org/W3169482982",
    "https://openalex.org/W2963180826",
    "https://openalex.org/W3035539956",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2990317318",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2998493658",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2962926870",
    "https://openalex.org/W2724213014",
    "https://openalex.org/W2964302946"
  ],
  "abstract": "Transformers have shown preferable performance on many vision tasks. However, for the task of person re-identification (ReID), vanilla transformers leave the rich contexts on high-order feature relations under-exploited and deteriorate local feature details, which are insufficient due to the dramatic variations of pedestrians. In this work, we propose an Omni-Relational High-Order Transformer (OH-Former) to model omni-relational features for ReID. First, to strengthen the capacity of visual representation, instead of obtaining the attention matrix based on pairs of queries and isolated keys at each spatial location, we take a step further to model high-order statistics information for the non-local mechanism. We share the attention weights in the corresponding layer of each order with a prior mixing mechanism to reduce the computation cost. Then, a convolution-based local relation perception module is proposed to extract the local relations and 2D position information. The experimental results of our model are superior promising, which show state-of-the-art performance on Market-1501, DukeMTMC, MSMT17 and Occluded-Duke datasets.",
  "full_text": "OH-Former: Omni-Relational High-Order Transformer for Person\nRe-Identiﬁcation\nXianing Chen1, Chunlin Chen1, Qiong Cao2, Jialang Xu3, Yujie Zhong4, Jiale Xu1, Zhengxin Li1,\nJingya Wang1, Shenghua Gao1\n1Shanghaitech University\n2JD Explore Academy, JD.com\n3University of Electronic Science and Technology of China\n4Meituan Inc.\nAbstract\nTransformers have shown preferable performance on many\nvision tasks. However, for the task of person re-identiﬁcation\n(ReID), vanilla transformers leave the rich contexts on high-\norder feature relations under-exploited and deteriorate local\nfeature details, which are insufﬁcient due to the dramatic vari-\nations of pedestrians. In this work, we propose an Omni-\nRelational High-Order Transformer (OH-Former) to model\nomni-relational features for ReID. First, to strengthen the ca-\npacity of visual representation, instead of obtaining the at-\ntention matrix based on pairs of queries and isolated keys at\neach spatial location, we take a step further to model high-\norder statistics information for the non-local mechanism. We\nshare the attention weights in the corresponding layer of each\norder with a prior mixing mechanism to reduce the compu-\ntation cost. Then, a convolution-based local relation percep-\ntion module is proposed to extract the local relations and\n2D position information. The experimental results of our\nmodel are superior promising, which show state-of-the-art\nperformance on Market-1501, DukeMTMC, MSMT17 and\nOccluded-Duke datasets.\nIntroduction\nPerson re-identiﬁcation (ReID) aims to identify the same\nperson from a set of pedestrian images captured by mul-\ntiple cameras. This task is very challenging, since the at-\ntributes (e.g.clothing, gender, hair) of pedestrians vary dra-\nmatically and their pictures are taken under various condi-\ntions ( e.g.illumination, occlusion, background clutter, and\ncamera type). Hence, learning distinctive and robust features\nplays a decisive role in the ﬁeld of person ReID.\nCNN-based methods (Luo et al. 2019) have achieved\ngreat success in this ﬁeld due to their strong ability in ex-\ntracting deep discriminative features. In order to mine ﬁne-\ngrained local information from different body parts, part-\nbased methods (Yao et al. 2019) are proposed to extract part-\ninformed representations. By partitioning the backbone net-\nwork’s feature map horizontally into multiple parts (Fu et al.\n2019; Sun et al. 2018) as shown in Figure 1(a), the deep neu-\nral networks can concentrate on learning more ﬁne-grained\nsalient features in each individual local part. However, due to\nthe hard inductive bias of CNN, their models suffer from one\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Hand-craft Partition\n (b) Attention\n(c) Transformer\n (d) Ours\nFigure 1: Examples of different feature representation and\nmatching methods for person ReID. (a) Hand-crafted par-\ntition methods require well-aligned body parts and breaks\nwith-part consistency. (b) Attention-based methods consider\nonly coarse region-level information. (c) Vanilla transform-\ners ignore the rich contexts on high-order feature relations\nand local feature details. (d) Omni-relational high-order fea-\nture produced by our proposed OH-Former.\ncommon drawback, i.e., they require relatively well-aligned\nbody parts for the same person. Besides, strict uniform par-\ntitioning of the feature map breaks with-part consistency.\nAttention-based methods alleviate these problems (Cai,\nWang, and Cheng 2019) by locating human parts automati-\ncally as shown in Figure 1(b). However, they consider only\ncoarse region-level attention whilst ignoring ﬁne-grained\npixel-level saliency and relational information.\nRecently, with the success of Transformers in a variety\nof vision tasks, some previous work (He et al. 2021; Li et al.\n2021a) has attempted to introduce the power of self-attention\narXiv:2109.11159v2  [cs.CV]  24 Nov 2021\nmechanism into ReID. By cutting the image into patches,\ntransformers are capable of globally attending all the patches\nat every layer, making the spatial correspondences between\ninput and intermediate features weaker. Nevertheless, most\nof the models obtain the attention matrices based on pairs of\nqueries and isolated keys at each spatial location as shown\nin Figure 1(c), while leaving the rich contexts on high-\norder feature relations under-exploited. Moreover, although\nvision transformers can capture long-distance feature depen-\ndencies, they deteriorate local feature details (Peng et al.\n2021; Li et al. 2021b). This low-capacity representation is\nnot robust to pedestrians with dramatic variations. In ad-\ndition, the performance of Transformers still lags behind\nCNNs in the low data regime (Dosovitskiy et al. 2020) since\nvanilla Transformers lack certain desirable inductive biases\nprocessed by CNNs (Dai et al. 2021; d’Ascoli et al. 2021;\nGuo et al. 2021), such as locality and weight sharing. These\ndrawbacks clearly limit the application of Transformers on\nthe ReID task.\nTo solve these problems, we propose a novel Transformer-\nbased model, i.e.Omni-Relational High-Order Transformer\n(OH-Former), which is equipped with a high-order trans-\nformer module with a prior mixing attention weight sharing\nmechanism and a local relation perception module (LRP).\nSpeciﬁcally, we ﬁrst obtain the ﬁrst-order relation infor-\nmation by computing the non-local relations from pairs of\nqueries and all other isolated keys at each spatial loca-\ntion, which named basic relations (Zhou et al. 2021). Then\nwe leverage the LRP which consists of a deformable and\ndepth-wise convolution branch to dynamically model the lo-\ncal ﬁne-grained feature relations and 2D spatial positions,\nwhich we call local relations. After that, by modeling the\ncorrelations of the positions in the feature map using non-\nlocal operations, the proposed module can integrate the lo-\ncal information captured by convolution operations and ﬁrst-\norder long-range dependency captured by non-local opera-\ntions. We call this robust feature representation as the omni-\nrelational high-order feature, which contains both the high-\norder non-local information and the local detailed informa-\ntion as shown in Figure 1(d). For example, given a pedes-\ntrian image, there are some noticeable signals on blue and\nyellow dots for the red dot. After constructing the basic and\nlocal relations, the correlations among them are then cap-\ntured by computing self-attention for the tokens. Using this\nmechanism, we explicitly tell the model that there are corre-\nlations among those tokens, the detailed information around\nthe signals, the long-range fused information for those sig-\nnals. Then the latter layers will learn under which circum-\nstances such correlations are related to the identity informa-\ntion of the person.\nHigh-order statistics can improve the model classiﬁcation\nperformance (Li et al. 2018; Lin, RoyChowdhury, and Maji\n2015), but they lead to high-dimensional representation and\nexpensive computation cost simultaneously. Although the\ntoken number in our high-order layer is small enough af-\nter the aggregation of LRP, the O(n2) non-local computa-\ntion cost cannot be ignored. For efﬁcient modeling, we pro-\npose a Prior Mixing Attention Sharing Mechanism for the\nhigh-order layer in OH-Former. Xiao et al. (2019) found\nthat good attention similarities exist among adjacent layers\nin Transformers. Intuitively, we want the high-order self-\nattention to extract high-order information for each location,\nthus the spatial correspondences can guaranteed for each or-\nder within a transformer layer. And we compute the Jensen-\nShannon (JS) divergence to verify that each order within a\nlayer actually generates similar attention weights. Therefore,\nwe can just compute the weight matrix once in the ﬁrst-order\nlayer and reuse it in high-order layers. Moreover, pedestrians\nhave some ﬁxed and dominant patterns which can serve as\na prior for identiﬁcation. Different from hand-crafted atten-\ntion prior (Guo, Zhang, and Liu 2019; Yang et al. 2018), our\nshared attention is augmented by a learned prior to aggregate\ndominant and diverse information for high-order layer.\nIn summary, the contributions of this paper are as follows:\n(1) We propose a novel high-order transformer mod-\nule that explores high-order relation information for person\nReID. To the best of our knowledge, OH-Former is the ﬁrst\nwork to explore high-order information in Transformer.\n(2) We propose a Prior Mixing Attention Weights Sharing\nMechanism to reduce the computation cost and model the\ndominant and diverse features.\n(3) To incorporate with the inductive biases of CNNs, we\ndesign a local relation perception module (LRP) to extract\nand aggregate the local information.\n(4) Comprehensive experiments on four person ReID\nbenchmark datasets demonstrate that our proposed model\nOH-Former achieves state-of-the-art performance.\nRelated Works\nPerson Re-identiﬁcation\nSeveral person ReID methods based on CNNs have recently\nbeen proposed. To extract ﬁne-grained features from differ-\nent body parts, they often utilize part-based methods (Yao\net al. 2019) to enhance the discriminative capabilities of var-\nious body parts. The ﬁne-grained parts are usually generated\nby roughly horizontal stripes (Sun et al. 2018; Fu et al. 2019;\nWang et al. 2018; He et al. 2018). They require relatively\nwell-aligned body parts for the same person. However, per-\nson pictures localized by the off-the-shelf object detectors\noften have the spatial semantics misaligned problem. Other\nmethods make use of external cues such as pose (Zheng et al.\n2019; Miao et al. 2019; Wang et al. 2020a; Gao et al. 2020),\nparsing (Kalayeh et al. 2018; He et al. 2019; He and Liu\n2020) information to align body region across images. These\napproaches usually require extra sub-networks and compu-\ntation cost in inference and the accuracy is limited to the\npower of the estimator. Attention-based methods (Li, Zhu,\nand Gong 2018; Liu et al. 2017; Cai, Wang, and Cheng\n2019) learn feature representations robust to background\nvariations and focus on most informative body parts, how-\never, they are not capable of modeling ﬁne-grained infor-\nmation and relations. Thus, transformer-based methods are\nintroduced to explore the power of self-attention to model\nrobust person feature representation.\nVision Transformers\nWith the success of Transformers (Vaswani et al. 2017)\nin natural language processing, many studies (Dosovitskiy\net al. 2020; Chen et al. 2021; Touvron et al. 2021) have\nshown that they can be applied in computer vision as well.\nTransformer-based methods have boosted various vision\ntasks such as image classiﬁcation (Yuan et al. 2021), object\ndetection (Carion et al. 2020; Zhu et al. 2020b), and seg-\nmentation (Xie et al. 2021). Recently, some work (He et al.\n2021; Li et al. 2021a; Wu et al. 2021) explores the power of\ntransformers in the ﬁeld of ReID. However, they do not con-\nsider the subtle high-order representation differences among\npedestrians which is missing in the vanilla transformers.\nMore importantly, transformers show preferable perfor-\nmance on large datasets while lagging behind CNNs in the\nlow data regime since they lack the inductive biases which is\ninherent in CNNs (Dosovitskiy et al. 2020; Dai et al. 2021),\ne.g.locality and translation equivariance. To solve this prob-\nlem, previous work (Peng et al. 2021; Li et al. 2021b; Guo\net al. 2021; d’Ascoli et al. 2021; Li et al. 2021c) attempts\nto introduce convolutions to Transformers to employ all the\nbeneﬁts of CNNs while keeping the advantages of Trans-\nformers. However, their designs only introduce 2D local-\nity but ignore local feature similarity, i.e., local relations.\nIt will damage Transformers’ representation obtained from\nself-attention. In contrast, our model is aware of local re-\nlations by augmenting the spatial sampling locations with\nlearned offsets (Dai et al. 2017).\nHigh-Order Information\nThe high-order statistics have been successfully exploited in\nﬁned-grained visual classiﬁcation tasks (Zhou et al. 2021; Li\net al. 2018; Lin, RoyChowdhury, and Maji 2015; ?). For the\nReID task, Chen, Deng, and Hu (2019) models high-order\nattention statistics information to capture the subtle differ-\nences. Zhang et al. (2021) introduces a second-order infor-\nmation bottleneck to cope with redundancy and irrelevance\nfeatures. Xia et al. (2019) models local features’ long-range\nrelationships. Wang et al. (2020b) learns high-order topol-\nogy information for occluded ReID. However, non of them\nconsider the high-order statistics of non-local information\nwhich is important for modeling pedestrians. We instead\nmodel an omni-relational feature to capture both the local\nand non-local relation differences among pedestrians.\nProposed Method\nOverall Architecture\nThe overview of OH-Former is presented in Figure 2. Given\nan image of x∈RH×W×C, where H,W,C denote the im-\nage’s height, width, and channel number, respectively. We\nutilize the stem architecture as our patch embedding layer\nwhich has a 5 ×5 convolution with stride 5 to reduce the\nfeature size, followed by a 3 ×3 convolution with stride 2\nfor better local information extraction (Guo et al. 2021) to\nget a 2D feature x ∈Rh×w×c. Then we ﬂatten the feature\ninto a sequence of tokens X ∈Rn×d, where n = h×wis\nthe resulting number of patches and dis the channel num-\nber. Following the design of (Dosovitskiy et al. 2020), we\nFigure 2: The overall architecture of our OH-Former for\nReID. We embed an image into a sequence of ﬂattened to-\nkens, concatenate a class token, add position embeddings\nand feed them into stacked MHSA and our OH-Former lay-\ners to exploit the omni-relational high-order information.\nThe output tokens without the class token are fed into an\nadaptive average pooling layer to get part tokens. Finally,\nwe use the class and part tokens to train classiﬁers or do in-\nference.\nconcatenate a class token to those tokens and add a learn-\nable position embedding on them. Afterwards, we process\nthose tokens X ∈ RT×d with stacked Multi-head Self-\nAttention (MHSA) and our OH-Former layers. The result-\ning tokens without class token zcls are passed to an adaptive\naverage pooling layer (Sun et al. 2018) to get part tokens\nzi(i= 1,2,...,p ), where pis the number of parts.\nFinally, we optimize the network by minimizing the sum\nof Cross-Entropy and Triplet losses with BNNeck (Luo et al.\n2019) over the class and part tokens. Speciﬁcally, our model\nis optimized with the following losses:\nL= LCE (zcls) +LTriplet (zcls)+\n1\np\np∑\ni=1\n(LCE (zi) +LTriplet (zi)) (1)\nwhere triplet loss is computed as LTriplet = [dp −dn + α].\ndp, dn are feature distances of positive pair and negative pair,\nrespectively. αis the margin parameter.\nDuring inference, we concatenate the class and part to-\nkens to form the ﬁnal feature representation and compute\nthe Euclidean distances between them to determine the iden-\ntities of different people.\nOmni-Relational High-Order Transformer\nIn this section, we will describe our proposed OH-Former\nlayer in details.\nFirst-Order Self-Attention. Given an input sequence\nX ∈RT×d, the ﬁrst-order self-attention performs a scaled\ndot product attention following the vanilla Transformer\n(Vaswani et al. 2017; Dosovitskiy et al. 2020), deﬁned as:\nS1 = Q1 (K1)T\n√dk\n,s.t.Q1 = XWQ1 ,K1 = XWK1 (2)\nwhere WQ1 ∈Rd×dk ,WK1 ∈Rd×dk are linear transfor-\nmations. Sis an l×lmatrix, entry (i,j) represents the sim-\nilarity between position i and position j. For simplicity, the\nmulti-head self-attention mechanism is omitted here.\nFigure 3: Detailed structure of our OH-Former layer.\nThe output of the ﬁrst-order self-attention is deﬁned as the\nweighted sum of values:\nA1 = Softmax (S1) ×V1,s.t.V1 = XWV1 (3)\nwhere V1 is generated from the same source with a linear\ntransformation WV1 ∈Rd×dv . There basic relations (Zhou\net al. 2021) construct the one- vs.-rest relations (Park and\nHam 2020) of body tokens.\nLocal Relation Perception Module. Although vision\ntransformers can capture long-distance feature dependen-\ncies, they ignore the local feature details (Yuan et al. 2021).\nFortunately, convolutions’s hard inductive biases encoded by\nlocality and weight sharing can compensate for this draw-\nback (Peng et al. 2021). Previous methods have introduced\ninductive bias to Transformers by ﬁxed grid depthwise con-\nvolution (Li et al. 2021b,c; Dai et al. 2021), which are un-\naware of local relations. They actually bring in 2D positions\nto transformers (Xie et al. 2021). In contrast, we propose\na Local Relation Perception Module (LRP), which consists\nof deformable (Dai et al. 2017) and depthwise convolution\n(Tan and Le 2019) branches as shown in Figure 3. The de-\nformable convolution branch dynamically aggregates local\nrelation information across patches by attending to sparse\nspatial locations, producing the reﬁned local-relation-aware\nfeature. The depthwise convolution branch uses a ﬁxed3×3\ndepthwise convolution with stride 2 to extract positional in-\nformation (Chu et al. 2021; Xie et al. 2021) for high-order\nfeatures. Speciﬁcally, LRP is deﬁned as:\nLRP(Ai) =DeformConv(Ai) +DWConv(Ai) (4)\nwhere Ai is the i-th order feature. DeformConv(·) denotes\nthe deformable convolution and DWConv(·) denotes the\ndepth-wise convolution. Without loss of generality for other\nTransformer variants, we feed the ﬁrst-order features with-\nout class token to our LRP. For simplicity, the reshape oper-\nations are omitted here.\nOmni-Relational High-Order Self-Attention. After cap-\nturing the basic and local relations, we take a step further\nto model our omni-relational high-order features. For con-\nstructing the relations among basic and local relations, we\nuse multi-head self-attention to process features processed\nby LRP. Given the i-th order features Ai, the (i+1)-th order\ninformation Ai+1 is extract by\nSi+1 = Qi+1 (Ki+1)T\n√dk\n(5)\nAi+1 = Softmax (Si+1) ·Vi+1 (6)\nwhere Qi+1 = AiWQ,Ki+1 = AiWK,Vi+1 = AiWK.\nAfter that, Ai+1 is processed by LRP again for computing\nhigher-order self-attention.\nCompared to OH-Former, vanilla transformers can also\ncapture high-order statistics, but they need to stack many\nlayers which leads to high computation cost, and deep trans-\nformers are very hard to train.\nPrior Mixing Attention Sharing Mechanism. Although\nthe number of tokens in high-order layer is small enough af-\nter being aggregated by LRP, the O(n2) self-attention com-\nputation cost is still too high. For efﬁcient modeling, we pro-\npose a Prior Mixing Attention Sharing Mechanism for each\norder within an OH-Former layer. The idea is that we only\ncompute the weight matrix once and reuse it in high-order\nlayers.\nIntuitively, we want the high-order self-attention to ex-\ntract high-order information for each location, so the spatial\ncorrespondence relationship should be guaranteed for each\norder within an OH-Former layer. And we compute JS di-\nvergence to measure the similarity between the high-order\nand the ﬁrst-order self-attention weights. Figure 4(b) shows\nthat the model generates similar weights over orders. Thus,\nwe can share S1 without class token similarity to high-order\nlayers:\nSi = S1,i = 2,... (7)\nMoreover, person images have some ﬁxed pattern for\nidentiﬁcation which can serve as a prior. To aggregate dom-\ninant and diverse information, we propose a prior mix-\ning mechanism. After computing attention from inputs\n(e.g.Softmax(QKT )), the attention is further augmented\nby a learned prior. Speciﬁcally, the Prior Mixing Attention\nSharing Mechanism is deﬁned as:\nSi = S1 ·W,i = 2,... (8)\nwhere W is the learnable parameters.\nFigure 4: Illustration of our proposed mechanism. (a) Pro-\ncess of our Prior Mixing Attention Sharing Mechanism. (b)\nJensen-Shannon divergence of the attention weights for dif-\nferent orders within a OH-Former layer (Darker cell indi-\ncate a more similar distribution). We use a three-order OH-\nFormer layer as an example.\nOmni-Relational Feature Fusion Feed-Forward Net-\nwork. The relations modeled by each order layer are al-\nready omni-relational high-order relations. We then fuse\nthem in an effective way. For each high-order feature x ∈\nRT×d, we reshape the feature to x ∈ Rh×w×d and use\nthe nearest interpolation to upsample it to x ∈RH×W×d\nwhich has the same shape as the reshaped ﬁrst-order feature\nwithout class token. Then we ﬂatten it, concatenate it with\na zero-value vector z0 ∈ R1×d and sum it with the ﬁrst-\norder feature. Finally, the fused feature is fed into a Layer-\nNormalization layer followed by a Feed-Forward layer with\nskip connection.\nModel Variations. Although the vanilla transformers can\nlearn high-order statistics in the high-level layer by stacking\nnon-local operations and they need to extract simple context\npatterns in the low-level layer (d’Ascoli et al. 2021), the or-\nder setting of OH-Former is still ﬂexible. So we show our\ninsight on this in the ablation studies chapter.\nAdditionally, we build two variants of our model, named\nOH-Former and OH-Former Share. OH-Former Share uses\nour prior mixing attention sharing mechanism to share and\nre-calibrate attention weights of different orders to trade-off\nmodel performance and computational complexity.\nExperiments\nDatasets\nWe evaluate our model on the following publicly available\nReID datasets and compare its performance with state-of-\nthe-art methods. 1) The Market1501 dataset (Zheng et al.\n2015) containing 32 668person images of 1501 identities.\n2) The DukeMTMC-reID (Ristani et al. 2016) composed\nof 36 441images of 1404 identities. 3) The MSMT17 (Wei\net al. 2018) containing 126 441images of 4101 identities.\n4) The Occluded-Duke dataset (Miao et al. 2019) with oc-\ncluded person images selected from DukeMTMC-reID.\nImplementation\nTraining. All person images are resized to368×128. The\ntraining images are augmented with random horizontal ﬂip-\nping and random erasing. The batch size is set to256 with 16\nimages per ID. Stochastic gradient descent optimizer (SGD)\nis employed with momentum of 0.9 and weight decay of\n1 ×10−4. The learning rate is initialized as 0.01 with co-\nsine learning rate decay. For the MHSA and ﬁrst-order self-\nattention layers, we use the imagenet-pretrained model of\nViT-B (Dosovitskiy et al. 2020). Our model is implemented\nwith PyTorch library (Paszke et al. 2017) and trained on four\nNvidia Titan X GPUs.\nEvaluation Protocols. We use standard metrics in ReID\ncommunity, namely Cumulative Matching Characteristic\n(CMC) curves and mean Average Precision (mAP), to eval-\nuate all methods. We report ReID results under the setting\nof a single query for a fair comparison.\nAblation Study of LRP\nThe effectiveness of the proposed LRP Module is validated\nin Table 2. We compare the performance of several variants\nof our LRP in terms of mAP and rank-1 accuracy. From\nthe ﬁrst three rows, we can see that convolution improves\nthe retrieval performance by introducing 2D position infor-\nmation. Although its ﬁxed grid convolution kernel can cap-\nture local information, it is unaware of local relation. Thus,\nfor dynamically aggregating high-order information which\nhas complex relations aggregated by non-local operations, a\nlocal-relation-aware operation is critical to our model. Also,\nthe worst result in the ﬁrst row demonstrates that without\nlocal downsampling operations the high-order self-attention\nwill learn homogeneous information and the effective of our\nLRP is signiﬁcant. From the fourth, ﬁfth and seventh rows,\nwe show that compared with convolution, pooling opera-\ntions will lead to the loss of large amount of information,\nThis is due to the fact that local information in non-local op-\nerations is more complex than it is in CNN features, not only\nbecause pooling is an parameter-free operation. The second,\nsixth and last rows show exploring local relations to con-\nstruct our omni-relational feature is important, and the per-\nformance reaches to the peak when using all components.\nAblation Study of OH-Former\nWe demonstrate the effectiveness of the proposed OH-\nFormer layer in Table 3. From the second row, we can ob-\nserve that high-order information will slightly decrease the\nperformance since Transformers need to learn simple con-\ntext patterns on the low level. The third row shows that high-\norder statistics can only brings a small performance boost\nsince vanilla Transformers can learn high-order statistics in\nthe high level layers by stacking transformer layers. Thus,\nthe model beneﬁts more in the middle levels as shown in the\nMethod DukeMTMC-reID Market1501 MSMT17\nmAP R-1 mAP R-1 mAP R-1\nHACNN (Li, Zhu, and Gong 2018) 63.8 80.5 75.7 91.2 - -\nPCB (Sun et al. 2018) 66.1 81.7 77.4 92.3 - -\nCBN (Zhuang et al. 2020) 67.3 82.5 77.3 91.3 42.9 72.8\nPCB+RPP (Sun et al. 2018) 69.2 83.3 81.6 93.8 - -\nHPM (Fu et al. 2019) 74.3 86.6 82.7 94.2 - -\nGASM (He and Liu 2020) 74.4 88.3 84.7 95.3 52.5 79.5\nCTF (Zhang et al. 2021) 74.9 87.4 87.7 94.8 - -\nSAN (Jin et al. 2020) 75.7 87.9 88.0 96.1 55.7 79.2\nBoT (Luo et al. 2019) 76.4 86.4 85.9 94.5 - -\nMHN (Wang et al. 2018) 77.2 89.1 85.0 95.1 - -\nMGN (Wang et al. 2018) 78.4 88.7 86.9 95.7 52.1 76.9\nSCSN (Chen et al. 2020) 79.0 91.0 88.5 95.7 58.5 83.8\nViT-BoT (He et al. 2021) 79.3 88.8 86.8 94.7 61.0 81.8\nISP (Zhu et al. 2020a) 80.0 89.6 88.6 95.3 - -\nTransReID (He et al. 2021) 82.6 90.7 89.5 95.2 69.4 86.2\nOH-FormerShare (Ours) 81.9 90.2 88.0 94.9 67.6 85.6\nOH-Former (Ours) 82.8 91.0 88.7 95.0 69.2 86.6\nTable 1: Performance comparision with state-of-the-art methods in holistic ReID. The best two results are in bold and underline.\nLRP Type mAP R-1\nNone 80.1 89.4\nDWC 81.7 90.2\nNC 81.8 90.1\nAP 80.7 89.9\nMP 80.4 89.7\nDWC + AP 81.8 90.2\nAP + DFC 82.3 90.8\nDWC + DFC (Ours) 82.8 91.0\nTable 2: Performance comparison with different variants of\nLRP on DukeMTMC-reID dataset. DWC: Depthwise Con-\nvolution; AP: Average Pooling; MP: Max Pooling; NC:\nNormal Convolution; DFC: Deformable Convolution. None\nmeans we do not add any downsampling operation after the\nself-attention operation.\nOH-Former order mAP R-1\n[None] 78.6 88.9\n[H0,1\n2 ] 78.5 88.9\n[H9,10,11\n2 ] 78.8 89.0\n[H2,4,6,8\n2 ] 82.4 90.9\n[H2,4,6,8\n3 ] 83.0 91.2\n[H2,4,6,8\n4 ] 82.9 91.3\n[H2,8\n2 ,H4,6\n3 ] 82.8 91.0\nTable 3: Ablation results for the OH-Former with different\norders on DukeMTMC-reID dataset. Hj\ni means we use an\ni-order OH-Former layer at the j-th layer. Note that we only\nshow OH-Former layers for simplicity.\nlast ﬁve rows. And the order higher than three in OH-Former\nlayer just delivers a negligible boost as shown in the sixth\nrow. As a result, we choose the conﬁguration [H2,8\n2 ,H4,6\n3 ]\nin the last row to trade off speed and performance.\nAblation experiments from overall and individual per-\nspectives are also conducted on our proposed method. Our\nOH-Former layers bring 5.34% mAP improvement over the\nbaseline by modeling omni-relational high-order features.\nOH-Former achieves the best performance, which shows\nthat advantages of LRP and OH-Former layer are comple-\nmentary and their coherent innovations contribute to a high-\nperformance ReID model. The forth row shows that Atten-\ntion Weights Sharing will decrease the model performance.\nBut after adding our Attention Prior (PM), the model shows\ncomparable performance to our OH-Former model by fusing\ndominant and diverse information.\nMethod LRP OH AS PM mAP R-1\nBaseline × × × × 78.6 88.9\nOH-Former w/o LRP × ✓ × × 80.1 89.4\nOH-Former ✓ ✓ × × 82.8 91.0\nOH-FormerShare w/o PM ✓ ✓ ✓ × 80.3 90.0\nOH-FormerShare ✓ ✓ ✓ ✓ 81.9 90.2\nTable 4: Ablation results for the proposed OH-Former on\nDukeMTMC-reID dataset. LRP: Local Relation Perception\nmodule for OH-Former layer. OH: OH-Former layer. AS:\nAttention Sharing. PM: Prior Mixing.\nComparison with State-of-the-Art Methods\nResults on Holistic Datasets. In Table 1, we compare\nour proposed OH-Former and OH-Former Share with the\nstate-of-the-art ReID methods. On DukeMTMC-reID, we\ncan see that OH-Former obtains the highest mAP ( 82.8%)\nand R-1 ( 91.0%) with 0.2–16.7% and 0.3–9.3% improve-\nments compared to other methods, respectively. On Mar-\nket1501 and MSMT17, OH-Former achieves comparable\nperformance with the latest methods, especially on the R-1\nmetric. As for the OH-FormerShare, it surpasses all methods\nexcept TransReID by a considerable improvement (at least\n+1.9%/+6.6% mAP) on DukeMTMC-reID/MSMT17.\nResults on Occluded Dataset. To further explore the gen-\neralization ability of ReID models in a complex environ-\nment, we utilize the training set of Market1501 to train our\nmodel and directly test it on Occluded-Duke dataset. The\nperformance comparison is shown in Table 5. On Occluded-\nDuke dataset, both OH-Former and OH-Former Share out-\nperform the previous state-of-the-art methods by1.6–23.5%\nand 0.4–22.3% in mAP, yielding a result of 60.8% and\n59.6% in mAP, respectively. The above fact indicates that\nour proposed method has outstanding robustness for com-\nplex ReID tasks, e.g., occlusion ReID.\nOur model constructs an universal representation that can\ndeal with pedestrians with large variations and even unseen\npedestrians as shown in Table 5. Instead of mapping im-\nages to a feature space with small intra-identity distance\nand large inter-identity distance through learning speciﬁc in-\nductive bias, e.g.part correspondence (Sun et al. 2018), part\nrelations (Park and Ham 2020; He et al. 2021), our model\nembeds person images by considering all-order features for\nboth local and non-local relations and learning inductive bias\nby itself. Thus our omni-relational high-order features are\nrobust to all kinds of variations.\nMethod mAP R-1\nPGFA (Miao et al. 2019) 37.3 51.4\nHOReID (Wang et al. 2020b) 43.8 55.1\nISP (Zhu et al. 2020a) 52.3 62.8\nTransReID (He et al. 2021) 59.2 66.4\nOH-FormerShare (Ours) 59.6 66.7\nOH-Former (Ours) 60.8 67.1\nTable 5: Performance comparison with state-of-the-art\nmethods on Occluded-Duke dataset.\nConclusion\nIn this paper, We present a transformer-based model (OH-\nFormer) for person ReID which takes into account the omni-\nrelational high-order statistics on body tokens, thus making\nglobal class and local part tokens discriminative. Further-\nmore, we propose a Prior Mixing Attention Sharing Mech-\nanism for our OH-Former layer to reduce the computation\ncost. The local relation perception module introduces induc-\ntive biases to augment local information. The effectiveness\nof each proposed component is sufﬁciently validated by the\nablation studies. The experimental results on various bench-\nmark datasets show that the proposed OH-Former achieves\nstate-of-the-art performance.\nReferences\nCai, H.; Wang, Z.; and Cheng, J. 2019. Multi-scale body-\npart mask guided attention for person re-identiﬁcation. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, 0–0.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Proceedings of the European Confer-\nence on Computer Vision, 213–229.\nChen, B.; Deng, W.; and Hu, J. 2019. Mixed high-order at-\ntention network for person re-identiﬁcation. In Proceedings\nof the IEEE International Conference on Computer Vision ,\n371–381.\nChen, H.; Wang, Y .; Guo, T.; Xu, C.; Deng, Y .; Liu, Z.; Ma,\nS.; Xu, C.; Xu, C.; and Gao, W. 2021. Pre-trained image pro-\ncessing transformer. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 12299–12310.\nChen, X.; Fu, C.; Zhao, Y .; Zheng, F.; and Yang, Y . 2020.\nSalience-Guided Cascaded Suppression Network for Person\nRe-Identiﬁcation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition.\nChu, X.; Tian, Z.; Zhang, B.; Wang, X.; Wei, X.; Xia, H.; and\nShen, C. 2021. Conditional positional encodings for vision\ntransformers. arXiv preprint arXiv:2102.10882.\nDai, J.; Qi, H.; Xiong, Y .; Li, Y .; Zhang, G.; Hu, H.; and\nWei, Y . 2017. Deformable convolutional networks. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, 764–773.\nDai, Z.; Liu, H.; Le, Q. V .; and Tan, M. 2021. CoAtNet:\nMarrying convolution and attention for all data sizes. arXiv\npreprint arXiv:2106.04803.\nd’Ascoli, S.; Touvron, H.; Leavitt, M.; Morcos, A.; Biroli,\nG.; and Sagun, L. 2021. Convit: Improving vision trans-\nformers with soft convolutional inductive biases. arXiv\npreprint arXiv:2103.10697.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFu, Y .; Wei, Y .; Zhou, Y .; Shi, H.; Huang, G.; Wang, X.; Yao,\nZ.; and Huang, T. 2019. Horizontal pyramid matching for\nperson re-identiﬁcation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 33, 8295–8302.\nGao, S.; Wang, J.; Lu, H.; and Liu, Z. 2020. Pose-guided\nvisible part matching for occluded person ReID. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 11744–11752.\nGuo, J.; Han, K.; Wu, H.; Xu, C.; Tang, Y .; Xu, C.; and\nWang, Y . 2021. CMT: Convolutional Neural Networks Meet\nVision Transformers. arXiv preprint arXiv:2107.06263.\nGuo, M.; Zhang, Y .; and Liu, T. 2019. Gaussian transformer:\na lightweight approach for natural language inference. In\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 33, 6489–6496.\nHe, L.; Liang, J.; Li, H.; and Sun, Z. 2018. Deep spatial\nfeature reconstruction for partial person re-identiﬁcation:\nAlignment-free approach. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 7073–\n7082.\nHe, L.; and Liu, W. 2020. Guided saliency feature learn-\ning for person re-identiﬁcation in crowded scenes. In Pro-\nceedings of the European Conference on Computer Vision ,\n357–373.\nHe, L.; Wang, Y .; Liu, W.; Zhao, H.; Sun, Z.; and Feng,\nJ. 2019. Foreground-aware pyramid reconstruction for\nalignment-free occluded person re-identiﬁcation. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, 8450–8459.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang, W.\n2021. Transreid: Transformer-based object re-identiﬁcation.\narXiv preprint arXiv:2102.04378.\nJin, X.; Lan, C.; Zeng, W.; Wei, G.; and Chen, Z. 2020.\nSemantics-aligned representation learning for person re-\nidentiﬁcation. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, 11173–11180.\nKalayeh, M. M.; Basaran, E.; G¨okmen, M.; Kamasak, M. E.;\nand Shah, M. 2018. Human semantic parsing for person re-\nidentiﬁcation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 1062–1071.\nLi, P.; Xie, J.; Wang, Q.; and Gao, Z. 2018. Towards faster\ntraining of global covariance pooling networks by iterative\nmatrix square root normalization. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, 947–955.\nLi, W.; Zhu, X.; and Gong, S. 2018. Harmonious attention\nnetwork for person re-identiﬁcation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2285–2294.\nLi, Y .; He, J.; Zhang, T.; Liu, X.; Zhang, Y .; and Wu,\nF. 2021a. Diverse part discovery: Occluded person re-\nidentiﬁcation With part-aware transformer. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2898–2907.\nLi, Y .; Yao, T.; Pan, Y .; and Mei, T. 2021b. Contextual\ntransformer networks for visual recognition. arXiv preprint\narXiv:2107.12292.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021c. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707.\nLin, T.-Y .; RoyChowdhury, A.; and Maji, S. 2015. Bilinear\ncnn models for ﬁne-grained visual recognition. In Proceed-\nings of the IEEE International Conference on Computer Vi-\nsion, 1449–1457.\nLiu, X.; Zhao, H.; Tian, M.; Sheng, L.; Shao, J.; Yi, S.; Yan,\nJ.; and Wang, X. 2017. Hydraplus-net: Attentive deep fea-\ntures for pedestrian analysis. In Proceedings of the IEEE\nInternational Conference on Computer Vision, 350–359.\nLuo, H.; Gu, Y .; Liao, X.; Lai, S.; and Jiang, W. 2019. Bag of\ntricks and a strong baseline for deep person re-identiﬁcation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, 0–0.\nMiao, J.; Wu, Y .; Liu, P.; Ding, Y .; and Yang, Y . 2019.\nPose-guided feature alignment for occluded person re-\nidentiﬁcation. In Proceedings of the IEEE International\nConference on Computer Vision, 542–551.\nPark, H.; and Ham, B. 2020. Relation network for person\nre-identiﬁcation. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, 11839–11847.\nPaszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;\nDeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer,\nA. 2017. Automatic differentiation in pytorch. In Proceed-\nings of the International Conference on Neural Information\nProcessing Systems Workshop.\nPeng, Z.; Huang, W.; Gu, S.; Xie, L.; Wang, Y .; Jiao, J.;\nand Ye, Q. 2021. Conformer: Local features coupling\nglobal representations for visual recognition. arXiv preprint\narXiv:2105.03889.\nRistani, E.; Solera, F.; Zou, R.; Cucchiara, R.; and Tomasi,\nC. 2016. Performance measures and a data set for multi-\ntarget, multi-camera tracking. In Proceedings of the Euro-\npean conference on computer vision, 17–35.\nSun, Y .; Zheng, L.; Yang, Y .; Tian, Q.; and Wang, S. 2018.\nBeyond part models: person retrieval with reﬁned part pool-\ning (and a strong convolutional baseline). In Proceedings of\nthe European Conference on Computer Vision, 480–496.\nTan, M.; and Le, Q. 2019. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. InProceedings of\nthe International Conference on Machine Learning , 6105–\n6114.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In Proceedings of\nthe International Conference on Machine Learning, 10347–\n10357.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Proceedings of the Interna-\ntional Conference on Neural Information Processing Sys-\ntems, 5998–6008.\nWang, G.; Yang, S.; Liu, H.; Wang, Z.; Yang, Y .; Wang, S.;\nYu, G.; Zhou, E.; and Sun, J. 2020a. High-order information\nmatters: Learning relation and topology for occluded person\nre-identiﬁcation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 6449–6458.\nWang, G.; Yang, S.; Liu, H.; Wang, Z.; Yang, Y .; Wang, S.;\nYu, G.; Zhou, E.; and Sun, J. 2020b. High-order information\nmatters: Learning relation and topology for occluded person\nre-identiﬁcation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 6449–6458.\nWang, G.; Yuan, Y .; Chen, X.; Li, J.; and Zhou, X. 2018.\nLearning discriminative features with multiple granularities\nfor person re-identiﬁcation. In Proceedings of the 26th ACM\nInternational Conference on Multimedia, 274–282.\nWei, L.; Zhang, S.; Gao, W.; and Tian, Q. 2018. Per-\nson transfer gan to bridge domain gap for person re-\nidentiﬁcation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 79–88.\nWu, S.; Bai, Y .; Wang, C.; and Duan, L. 2021. Person re-\ntrieval with conv-transformer. In Proceedings of the IEEE\nInternational Conference on Multimedia and Expo, 1–6.\nXia, B. N.; Gong, Y .; Zhang, Y .; and Poellabauer, C. 2019.\nSecond-order non-local attention networks for person re-\nidentiﬁcation. In Proceedings of the IEEE International\nConference on Computer Vision, 3760–3769.\nXiao, T.; Li, Y .; Zhu, J.; Yu, Z.; and Liu, T. 2019. Sharing\nattention weights for fast transformer. In Proceedings of the\nInternational Joint Conference on Artiﬁcial Intelligence.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efﬁcient design\nfor semantic segmentation with transformers. arXiv preprint\narXiv:2105.15203.\nYang, B.; Tu, Z.; Wong, D. F.; Meng, F.; Chao, L. S.; and\nZhang, T. 2018. Modeling localness for self-attention net-\nworks. In Proceedings of the International Conference on\nEmpirical Methods in Natural Language Processing, 4449–\n4458.\nYao, H.; Zhang, S.; Hong, R.; Zhang, Y .; Xu, C.; and Tian,\nQ. 2019. Deep representation learning with part loss for\nperson re-identiﬁcation. IEEE Transactions on Image Pro-\ncessing, 28(6): 2860–2871.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.; Tay,\nF. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Train-\ning vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986.\nZhang, A.; Gao, Y .; Niu, Y .; Liu, W.; and Zhou, Y .\n2021. Coarse-to-ﬁne person re-identiﬁcation With auxiliary-\ndomain classiﬁcation and second-order information bottle-\nneck. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 598–607.\nZheng, L.; Huang, Y .; Lu, H.; and Yang, Y . 2019. Pose-\ninvariant embedding for deep person re-identiﬁcation. IEEE\nTransactions on Image Processing, 28(9): 4500–4509.\nZheng, L.; Shen, L.; Tian, L.; Wang, S.; Wang, J.; and Tian,\nQ. 2015. Scalable person re-identiﬁcation: A benchmark. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 1116–1124.\nZhou, J.; Lin, K.-Y .; Li, H.; and Zheng, W.-S. 2021. Graph-\nbased high-order relation modeling for long-term action\nrecognition. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 8984–8993.\nZhu, K.; Guo, H.; Liu, Z.; Tang, M.; and Wang, J. 2020a.\nIdentity-guided human semantic parsing for person re-\nidentiﬁcation. In Proceedings of the European Conference\non Computer Vision, 346–363.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020b.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. arXiv preprint arXiv:2010.04159.\nZhuang, Z.; Wei, L.; Xie, L.; Zhang, T.; Zhang, H.; Wu, H.;\nAi, H.; and Tian, Q. 2020. Rethinking the distribution gap of\nperson re-identiﬁcation with camera-based batch normaliza-\ntion. In Proceedings of the European Conference on Com-\nputer Vision, 140–157.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6847819685935974
    },
    {
      "name": "Transformer",
      "score": 0.681874692440033
    },
    {
      "name": "Computation",
      "score": 0.5261608958244324
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4738341271877289
    },
    {
      "name": "Perception",
      "score": 0.45998117327690125
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3252538740634918
    },
    {
      "name": "Algorithm",
      "score": 0.22538182139396667
    },
    {
      "name": "Voltage",
      "score": 0.11637735366821289
    },
    {
      "name": "Engineering",
      "score": 0.11544528603553772
    },
    {
      "name": "Psychology",
      "score": 0.0768132209777832
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 18
}