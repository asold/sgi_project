{
    "title": "Mention Flags (MF): Constraining Transformer-based Text Generators",
    "url": "https://openalex.org/W3174037047",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5100374831",
            "name": "Yufei Wang",
            "affiliations": [
                "Commonwealth Scientific and Industrial Research Organisation",
                "Data61"
            ]
        },
        {
            "id": "https://openalex.org/A5023924528",
            "name": "I. G. Wood",
            "affiliations": [
                "Commonwealth Scientific and Industrial Research Organisation",
                "Data61",
                "Oracle (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5008748003",
            "name": "Stephen Wan",
            "affiliations": [
                "Oracle (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5022216838",
            "name": "Mark Dras",
            "affiliations": [
                "Commonwealth Scientific and Industrial Research Organisation",
                "Data61"
            ]
        },
        {
            "id": "https://openalex.org/A5034461489",
            "name": "Mark Johnson",
            "affiliations": [
                "Macquarie University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2904565150",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3125115471",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2946375144",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2964029788",
        "https://openalex.org/W2150824314",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W1948566616",
        "https://openalex.org/W2963877622",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2949413855",
        "https://openalex.org/W2929900303",
        "https://openalex.org/W3115264425",
        "https://openalex.org/W3035540807",
        "https://openalex.org/W3026997957",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2971167892",
        "https://openalex.org/W2963592583",
        "https://openalex.org/W2996176596",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2786660442",
        "https://openalex.org/W2914397182",
        "https://openalex.org/W2963910262",
        "https://openalex.org/W3091177855",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W3176750236",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2964207259",
        "https://openalex.org/W3104279398",
        "https://openalex.org/W2429300145",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3099366427",
        "https://openalex.org/W3153841200",
        "https://openalex.org/W2506483933"
    ],
    "abstract": "Yufei Wang, Ian Wood, Stephen Wan, Mark Dras, Mark Johnson. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 103–113\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n103\nMention Flags (MF): Constraining Transformer-based Text Generators\nYufei Wang1, Ian D. Wood1,2, Stephen Wan2, Mark Dras1 and Mark Johnson3\nMacquarie University, Sydney, Australia1\nCSIRO Data61, Sydney, Australia2\nOracle Digital Assistant, Oracle Corporation3\nyufei.wang@students.mq.edu.au, ian.wood@mq.edu.au\nstephen.wan@data61.csiro.au, mark.dras@mq.edu.au\nmark.mj.johnson@oracle.com\nAbstract\nThis paper focuses on Seq2Seq (S2S) con-\nstrained text generation where the text gener-\nator is constrained to mention speciﬁc words,\nwhich are inputs to the encoder, in the gen-\nerated outputs. Pre-trained S2S models such\nas T5 or a Copy Mechanism can be trained to\ncopy the surface tokens from encoders to de-\ncoders, but they cannot guarantee constraint\nsatisfaction. Constrained decoding algorithms\nalways produce hypotheses satisfying all con-\nstraints. However, they are computationally\nexpensive and can lower the generated text\nquality. In this paper, we propose Mention\nFlags ( MF), which trace whether lexical con-\nstraints are satisﬁed in the generated outputs\nof an S2S decoder. The MF models are trained\nto generate tokens until all constraints are satis-\nﬁed, guaranteeing high constraint satisfaction.\nOur experiments on the Common Sense Gen-\neration task ( CommonGen) (Lin et al., 2020),\nEnd2end Data-to-Text task (E2ENLG) (Duˇsek\net al., 2020) and Novel Object Captioning task\n(nocaps) (Agrawal et al., 2019) show that the\nMF models maintain higher constraint satisfac-\ntion and text quality than the baseline mod-\nels and other constrained text generation algo-\nrithms, achieving state-of-the-art performance\non all three tasks. These results are achieved\nwith a much lower run-time than constrained\ndecoding algorithms. We also show that the\nMF models work well in the low-resource set-\nting. 1\n1 Introduction\nThis paper focuses on Seq2Seq (S2S) constrained\ntext generation where a set of encoder input to-\nkens are required to be present in the generated\noutputs. For example, Keyword-to-Text (Lin et al.,\n2020), Data-to-Text (Gardent et al., 2017; Duˇsek\net al., 2020) and Image-to-Text (Lin et al., 2014;\n1The source code for this paper is released at https:\n//github.com/GaryYufei/ACL2021MF\nFigure 1: An overview of the Mention Flag mechanism\nfor Transformer-based S2S models. Here, the tokens\nﬂower and bee are required to appear in the generated\noutputs. Each generated token has a corresponding set\nof Mention Flags which informs the decoder whether\neach lexical constraint has been satisﬁed in the cur-\nrent decoder input sequence. For example, the Men-\ntion Flag for ﬂower is set (indicated by orange dots)\nfrom the third token because it is generated at the sec-\nond step. Both token and Mention Flag embeddings\nare the input to the decoder, but Mention Flags are in-\njected into the decoder in a different way to the tokens\n(see Fig. 3). Note that task speciﬁc encoder inputs have\nbeen omitted for brevity.\nAgrawal et al., 2019) require the models to men-\ntion all or some of the input keywords, key-value\npairs and image object labels (respectively), po-\ntentially with linguistic variants, in the generated\noutputs. Large (pre-trained) Transformer-based\nS2S models such as T5 (Raffel et al., 2019) can be\ntrained (ﬁne-tuned) to perform this task. However,\nthey only learn to copy the surface tokens from\nencoder inputs to the decoder outputs and there is\nno underlying mechanism guaranteeing good con-\nstraint satisfaction (the ratio of satisﬁed lexical con-\nstraints to given lexical constraints). Constrained\nBeam Search (CBS) (Anderson et al., 2017) and\nrelated algorithms can guarantee outputs satisfy-\ning all constraints, however they are much slower\nthan the standard beam search algorithm. In ad-\ndition, as they are all inference-based algorithms,\ntheir corresponding models are not aware of the\n104\nconstraint words or phrases, the resulting genera-\ntion could be poor. Ideally, a method for producing\nconstrained text should: a) generate high-quality\ntext; b) achieve high constraint satisfaction;c) have\nan efﬁcient inference procedure.\nTo this end, we propose Mention Flags ( MF),\nwhich trace whether a lexical constraint has been\nrealized in partial decoder outputs. Speciﬁcally,\neach decoder input token is provided with a set of\nﬂags indicating which constraints have been sat-\nisﬁed up to that token. As shown in Fig 1, the\nMention Flags for ﬂower is set from the third step,\nbecause ﬂower is generated at the second step. We\nrepresent the three possible Mention Flags as sepa-\nrate trainable embeddings and inject them into the\ndecoder of the S2S Transformer-based Text gener-\nator. The dynamic Mention Flags explicitly inform\nthe model about which constraints have been sat-\nisﬁed, which is helpful for the models to produce\nhigh-quality text satisfying the constraints (Goal\na). During training, all the mention ﬂags are set\nwhen the model is tasked to generate the End-of-\nSequence (EOS) token, strongly encouraging the\nmodel not to stop generation until all constraints\nare satisﬁed (Goal b). The MF models only require\nordinary decoding algorithms. Their inference time\nand memory requirements are similar to their base-\nline models (Goal c).\nWe conduct experiments on three benchmarks:\nCommonsense Generative Reasoning ( Common-\nGen) (Lin et al., 2020), where the only input is a\nset of words representing concepts, and the output\ntext is constrained to include all of them; End-to-\nEnd Data-to-Text (E2ENLG) (Duˇsek et al., 2020),\nwhere the constraints are meaning representations\nwith lexicalised attributes and values that the output\ntext should mention; and Novel Object Captioning\nat scale (nocaps) (Agrawal et al., 2019), where con-\nstraints are salient image objects that should be\nmentioned in the generated caption. Compared to\nthe constrained decoding algorithms, the MF mod-\nels can produce higher-quality text with a simi-\nlar level of constraint satisfaction and much less\ninference run-time and memory. Mention Flags\nare a general mechanism that improves constraint\nsatisfaction in the non-pre-trained and pre-trained\nS2S Transformer-based models. Furthermore, our\nexperiments show that the MF models can satisfy\nnovel constraints (i.e, involving words or phrases\nnot seen during training) and they work well in\nlow-resource settings. Our MF models set a new\nstate-of-the-art in these three tasks.\n2 Background\nIn this paper, we focus on constraining transformer-\nbased text generation models due to their popularity\nand success in various domains, especially in large-\nscale pre-trained language models (Raffel et al.,\n2019; Lewis et al., 2020). Previous work can be\nroughly categorized into two streams: S2S training\napproaches and Constrained decoding approaches:\nTraining S2S Models S2S models can implicitly\ncapture the co-occurrence between encoder and de-\ncoder sequences, particularly pre-trained ones such\nas T5 (Raffel et al., 2019) and BART (Lewis et al.,\n2020). Wen et al. (2015) uses a special gate to con-\ntrol what information will be generated in the fol-\nlowing steps. Kale and Rastogi (2020) have shown\nthat the T5 models achieve state-of-the-art results\nin various Data-to-Text tasks, requiring copying\nfrom encoder to decoder, after ﬁne-tuning. As an\nalternative, the Copy Mechanism (Gu et al., 2016)\nexplicitly learns where to copy the input constraints\ninto the output by adding an extra copy pathway to\nthe models. However, these approaches cannot con-\ntrol or guarantee their constraint satisfaction. Lin\net al. (2020) also have observed lower constraint\nsatisfaction in the above methods, compared to the\nconstrained decoding approaches.\nConstrained Decoding These algorithms, in-\ncluding Constrained Beam Search (CBS) (An-\nderson et al., 2017) and Grid Beam Search\n(GBS) (Hokamp and Liu, 2017), maintain a set of\nstates which have their own size-k beams and only\nallow hypotheses satisfying speciﬁc constraints to\nbe considered during inference. Each CBS state\ncorresponds to the hypotheses satisfying differ-\nent constraints (exponential in the number of con-\nstraints) and the GBS states correspond to the hy-\npotheses satisfying the same number of constraints\n(linear to constraint number). Balakrishnan et al.\n(2019); Juraska et al. (2018); Duˇsek and Jurˇc´ıˇcek\n(2016) also modify their inference algorithm in a\nsimilar way to fulﬁll speciﬁc output requirements.\nHowever, they signiﬁcantly increase the inference\nrun-time and memory and can produce sub-optimal\noutputs.\n3 Method\nThis section ﬁrst formulates constrained text gener-\nation tasks, then introduces Mention Flags and their\n105\nintegration with Transformer-based text generators.\n3.1 S2S Constrained Text Generation\nIn the S2S constrained text generation tasks, we\nare given encoder inputs x = [x1, . . . , xlx] ∈X\nthat describe the task, where some xi correspond\nto lexical constraints that must be satisﬁed in the\ngenerated outputs. At generation stept, the decoder\ntakes as input the tokens generated so far y:t =\n[y1, ··· , yt] ∈ Y and generates the next output\ntoken yt+1.\n3.2 Mention Flag\nAt generation step t, a set of Mention Flags in-\ndicates whether each lexical constraint has been\nsatisﬁed up to this step (i.e., in the decoder input\nsequence y:t). Formally, they can be deﬁned as\nm :X ×Y →{0, 1, 2}lx where |m(x, y:t)|= |x|.\nSpeciﬁcally, Mention Flag m(x, y:t)i is for the in-\nput token xi in x:\nm(x,y:t)i =\n\n\n0 xi is not a constraint\n1 xi is not mentioned iny:t\n2 xi is mentioned iny:t\n(1)\nThe values 1 and 2 represent the status of constraint\nsatisfaction. Once y:t satisﬁes the constraints, the\nvalue of the corresponding Mention Flag(s) are\nupdated from 1 to 2. Value 0 is a static default\nvalue for all tokens xi that do not correspond to\nany constraints. They are not required to be men-\ntioned in the outputs. These typically act as in-\nstructions to the model. At the start, Mention\nFlags m(x, ε) ∈ {0, 1}lx where ε is the empty\nstring because the empty string does not mention\nanything. During generation, m is monotonic in\ny∗: given decoder input sequence y:t and y:(t+1),\nm(x, y:t)i ≤m(x, y:(t+1))i. The Mention Flags\nfor any token xi can only remain unchanged or\nupdate from value 1 to 2.\nExample In Figure 2, given encoder input to-\nkens x = [name, Tetas, area, South, Bank], we\nstart from m(x, ε) = [0, 1, 0, 1, 1] because name\nand area are not lexical constraints. At step 4,\nm(x, [Tetas, is, located]) = [0, 2, 0, 1, 1] because\nTetas has already been mentioned in the current\ndecoder input sequence [Tetas, is, located].\nValue Update for Multi-Word Constraints As\nshown in Figure 2, Mention Flags for the tokens\ncorresponding to the same constraint are updated\ntogether. Given encoder input tokens xi, ··· , xj,\nforming a multi-word constraint, we require that\nx\ny:t\n<S> Tetas is located in the South Bank .\n\u0017 name 0 0 0 0 0 0 0 0 0\n\u0013 Tetas 1 2 2 2 2 2 2 2 2\n\u0017 area 0 0 0 0 0 0 0 0 0\n\u0013 South 1 1 1 1 1 1 1 2 2\n\u0013 Bank 1 1 1 1 1 1 1 2 2\nFigure 2: An example of Mention Flag Matrix. \u0013\nfor constrained encoder input tokens and \u0017 for non-\nconstrained ones. Both name and area start with value\n0 because they are not parts of lexical constraints. The\nlexical constraints Tetas and South Bank start from\nValue 1. The Mention Flags are updated to value 2\nwhen y:t satisﬁes the constraints. The Mention Flags\nfor multi-word constraints are updated simultaneously.\nm(x, y∗)i = ··· = m(x, y∗)j for all (partial) out-\nputs y∗, and m(x, y:t)i = ··· = m(x, y:t)j = 2\niff xi, ··· , xj are mentioned in y:t. We use con-\nventions from the relevant data set to determine\nwhether a constraint is a multi-word constraint.\nThis avoids false update when the models only\ngenerate the preﬁx of the constraints, rather than\nthe full constraints. For example, given constraint\n“washing machine”, the output could be “I put my\nwashing in the new washing machine.” The situa-\ntion becomes more complicated when both wash-\ning and washing machine are given lexical con-\nstraints. When we ﬁnd this case, we delay the\nvalue 2 update for washing until the word in is\ngenerated. Modern tokenization methods, such as\nBPE (Sennrich et al., 2016), make this situation\nfrequent.\nDeﬁnition of Mentions We deliberately allow a\nﬂexible notion of mentions in the Function m().\nWe can deﬁne various types of mentions to fulﬁll\nthe requirements of different applications and tasks.\nWith this ﬂexibility, the end-users can use Men-\ntion Flags in many constraint scenarios. For tasks\nwith strict constraints, we deﬁne mentions to be the\nexact string match in y:t. Otherwise, inﬂectional\nvariants or synonyms of words in the lexical con-\nstraints are allowed when checking for mentions.\nOur Mention Flag mechanism thus supports lex-\nical constraints with multiple verbalizations. We\nleave more sophisticated constraints (e.g., using\nNLP parsers) to future work.\nMention Flag Matrix Given x, y:t, We deﬁne\nthe two-dimensional Mention Flag Matrix F ∈\n106\n{0, 1, 2}lx×t as follows:\nF = [m(x, ε); m(x, y:1); ··· ; m(x, y:t)] (2)\nDuring training, given x and ground-truth output\nY gt (with lgt tokens), we can construct the ground-\ntruth Mention Flag Matrix Fgt ∈{0, 1, 2}lx×lgt by\nﬁnding the mentioning position of tokens in the lex-\nical constraints in Y gt. Fgt follows the same mask-\ning strategy as the decoder input tokens y:t. For\nthe tokens whose corresponding lexical constraints\nhaving no alignment with Y gt, their Mention Flags\nare also assigned value 0. During inference, we\nbuild the Mention Flag matrix incrementally, start-\ning from Finf ,0 = [m(x, ε)] ∈{0, 1}lx×1. In step\nt, we add a new column m(x, y:t) to Finf ,t−1 ∈\n{0, 1, 2}lx×(t−1) and obtain the new Mention Flag\nmatrix Finf ,t ∈{0, 1, 2}lx×t.\nWhy Mention Flags work During the training\nof MF models, the ground-truth always has all MFs\nset to “completed” before stopping the generation\n(i.e., before generating EOS Token). This provides\na strong signal to satisfy all constraints before com-\npleting generation. The value update from 1 to 2\nin MF provides implicit signals about where the\nconstraints are satisﬁed during training. Otherwise,\nthe model has to learn this information via the co-\noccurring sub-sequences between input sequence\nand output sequence. These two signals allow the\nmodel to achieve high constraint satisfaction and\nhelp to maintain high text quality (Sec. 4.5). Since\nthere are only 3 added embeddings, learning does\nnot require a substantial amount of training data\n(Sec. 4.7). Since these embeddings are indepen-\ndent of particular lexical constraints, we expect\nthat performance on novel constraints, not seen\nduring training, is improved (Sec. 4.5).\n3.3 Integration with S2S Transformer\nAs shown in Figure 3, Mention Flags are injected\ninto the Transformer decoder. We ﬁrst review the\nstandard S2S Transformer proposed in Vaswani\net al. (2017), then discuss how to inject Mention\nFlags information into the S2S Transformer model.\nStandard S2S Transformer Model The en-\ncoder input tokens x is fed into the Transformer\nEncoder he = Enc(x) where he ∈Rlx×d and d is\nthe model hidden size. In the Transformer decoder,\nthere are two self-attention modules, Self Multi-\nHead Attention ( SA) which handles the current\ndecoder input sequence y:t, and Cross Multi-Head\nFigure 3: In each decoder layer, the Cross-Attention\n(CA) module (light blue) integrates Mention Flags as\nadditional inputs describing relationship between en-\ncoder contents and decoder input tokens. There are\nseparated representations for Mention Flags in differ-\nent decoder layers.\nAttention (CA) which handles the interaction be-\ntween encoder output he and y:t:\nSA(y:t) =KV (Ws\nq y:t, Ws\nk y:t, Ws\nv y:t) (3)\nCA(hd\nt , he) =KV (Wc\nq hd\nt , Wc\nkhe, Wc\nv he) (4)\nwhere hd\nt = SA(y:t). KV is the standard key-\nvalue self-attention proposed in Vaswani et al.\n(2017). The outputs of CA(hd\nt , he) further deter-\nmine the model output yt+1 via a Feed Forward\nlayer, a Residual Connection and a softmax layer.\nIncorporating Mention Flag Matrix Our\ntwo-dimensional Mention Flag matrix\nF ∈ {0, 1, 2}lx×t is associated with the ele-\nments from encoder output he and current decoder\ninput y:t. The optimal way is to incorporate the\nfull F matrix into a component in the Transformer\ndecoder. We note that the CA module in the\nTransformer decoder already uses y:t as query\nand he as key. The resulting query-key similarity\nmatrix has the same size of our Mention Flag\nmatrix, making it suitable to incorporate F.\nMention Flag Matrix as Relative Position In-\nspired by Shaw et al. (2018) which incorporates\ntoken relative positions into the SA module, we\npropose to inject Mention Flags as the “relative\npositions” between encoder output he and current\ndecoder input y:t in the CA module. In each de-\ncoder layer, we represent F as two sets of train-\nable embeddings Mention Flag key mk = Ek(F)\nand Mention Flag Value mv = Ev(F) where\n107\nEk, Ev ∈R3×d are the Mention Flag embedding\ntables. mk and mv ∈Rlx×t×d. We have separated\nMention Flags representations for each decoder\nlayer. Eq. 4 is changed to:\nCA(hd\nt , he, mk, mv) =\nR(Wc\nq hd\nt , Wc\nkhe, Wc\nv he, mk, mv) (5)\nwhere R is the Self-Attention function with relative\nposition, deﬁned as follows:\nR(q, k, v, mk, mv)j =\nlx∑\ni=1\nai,j(vi + mv\ni,j) (6)\na∗,j = Softmax(e∗,j) (7)\nei,j =\nqj(ki + mk\ni,j)T\n√\nd\n(8)\nAs an alternative to representing F as mk and mv,\nwe could follow the approach to relative position\nin the T5 model (Raffel et al., 2019) and represent\nF as scalars that are added to the corresponding\nlogits ei,j in Eq. 7 used for computing the attention\nweights. However, we ﬁnd this scalar approach less\neffective than our proposed one in Sec. 4.6.\n4 Experiments\nWe conduct experiments on three benchmarks with\ndifferent forms of constraints including Common-\nsense Generative Reasoning (CommonGen) (Lin\net al., 2020) with keyword constraints, End-to-End\nrestaurants dialog (E2ENLG) (Duˇsek et al., 2020)\nwith key-value constraints, and Novel Object Cap-\ntioning at scale (nocaps) (Agrawal et al., 2019) with\nvisual object word constraints. We integrate Men-\ntion Flags with a three-layer standard S2S Trans-\nformer models (Trans, L3) (Vaswani et al., 2017)\nand pre-trained T5 models (Raffel et al., 2019) for\neach task. The T5 models achieve state-of-the-art\nresults in various Data-to-Text tasks (Kale and Ras-\ntogi, 2020). For the T5-Base and T5-Large models,\nwe use the implementation of T5 models in the\nhuggingface transformers 2. The Trans, L3 mod-\nels share the same implementation of the T5-Base\nmodels, except that it is not initialized with the pre-\ntrained parameters and it only uses 3 layers, rather\nthan 12 layers, for both encoder and decoder. In\naddition, to improve the generalization of our pre-\ntrained model, we freeze the parameters in the Self-\nAttention module and Feed-Forward Layers in each\n2https://github.com/huggingface/\ntransformers\nlayer of the T5 decoder. This parameters freezing\ntechnology is applied to both T5 baseline models\nand the MF models in all of our experiments. We\nreport constraint satisfaction for all tasks. We use\nGBS in the CommonGen task (max 5 constraints)\nand CBS in the E2ENLG (max 1 constraint) and\nnocaps (max 2 constraints) task.\n4.1 CommonGen\nIn this task, the encoder input is a sequence of\nconcepts C = [c1, ··· , ck], k≤5. The models\nshould generate a coherent sentence describing all\nconcepts in C. m(C, ε) = [1, 1, ··· , 1] and m\nallows inﬂectional variants to satisfy lexical con-\nstraints. We train (ﬁne-tune) Trans, L3, T5-Base\nand T5-Large model as our baselines. We apply\nMention Flags to the T5-Base and T5-Large model\n(+ MF). Following the suggestions in Lin et al.\n(2020), we report CIDEr (Vedantam et al., 2015)\nand SPICE (Anderson et al., 2016) as generated\ntext quality metrics. We calculate constraint satis-\nfaction for all constraints (ALL), novel constraints\n(Novel) and seen constraints (Seen).\nConstraintMethod CIDEr SPICE Seen Novel ALL\nw/o Pre-training\nTrans, L3 79.5 20.1 62.6 2.3 58.0\nTrans, L3 + MF 113.9 24.6 93.8 49.2 90.4\nLevenTrans.♣ 74.5 16.8 - - 63.8\nConstLeven.♣ 108.0 20.1 - - 94.5\nw/ Pre-training\nT5-Base 164.4 32.1 95.7 94.6 95.6\nT5-Base + G 110.7 27.8 100 100 100\nT5-Base + MF 170.1 32.7 99.6 99.2 99.6\nT5-Base + MF + G 115.0 27.6 100 100 100\nT5-Large 167.3 33.0 93.9 93.8 93.9\nT5-Large + MF 174.8 33.4 99.2 99.0 99.1\nLiu et al. (2021) 168.3 32.7 - - 98.6\nTable 1: Experiment Results onCommonGen Test Split.\nThe T5-Base + MF model achieves high text quality\nwith high constraint satisfaction. G for GBS. ♣ results\ntaken from Lin et al. (2020). Bold is the highest score\nand underline is the second highest score.\nResults Table 1 shows that the MF model im-\nproves the constraint satisfaction over the baselines\nfor all cases, achieving close to 100% (i.e., 99.6%\nand 99.1%). Notably, Mention Flags improve novel\nconstraint satisfaction from 2.3% to 49.2% in the\nrandomly initialized Transformer models. Com-\npared to the LevenTrans (Gu et al., 2019) and Con-\n108\nstLeven (Susanto et al., 2020) models, our Trans,\nL3 + MF model achieves higher CIDEr and SPICE\nscores with constraint satisfaction 4.1% lower than\nthe non-autoregressive ConstLeven model. While\nGBS provides a way to maximise constraint satis-\nfaction (i.e., 100%), doing so signiﬁcantly degrades\nthe output text quality (more than 50 CIDEr). Our\nMF model achieves near optimum constraint sat-\nisfaction while improving text quality (5.7 CIDEr\nscore improvement inT5-Base and 6.5 CIDEr score\nimprovement in T5-Large). Finally, our T5-Large +\nMF model outperforms the previous state-of-the-art\nresult (Liu et al., 2021), which integrates the Con-\nceptNet (Speer et al., 2017) into the BART model,\nby 6.5 CIDEr and 0.7 SPICE, suggesting that pre-\ntrained language models with textual concepts may\nprovide sufﬁcient information for this task.\n4.2 E2ENLG\nIn this task, the encoder input is a sequence\nof key-value meaning representations C =\n[k1, v1, ··· , kn, vn], n ≤ 8. We lists all given\nkey-value information as a space-separated string.\nm(C, ε) = [0, 1, 0, 1, ··· , 0, 1] and m allows syn-\nonyms to satisfy lexical constraints. For example,\nwelcome children and is family friendly are both\nmentions of familyFriendly[yes]. The models must\ngenerate a ﬂuent and coherent dialog response us-\ning all key-value pairs in the encoder. E2ENLG in-\ncludes 79 different in-domain key-value constraints.\nWe use the scripts from Du ˇsek et al. (2019) 3 to\nconstruct the synonyms set for these inputs. We\nuse Trans, L3 and T5-Base model as our baselines.\nWe use CBS to constrain the T5 model to satisfy\nall missing constraints (T5-Base + C). We report\nNIST (Lin and Hovy, 2003), BLEU (Papineni et al.,\n2002) and METEOR (Banerjee and Lavie, 2005) as\nthey are common metrics for evaluating the quality\nof long text in the E2ENLG outputs (more than 20\ntokens).\nResults Table 2 shows that the MF models con-\nsistently achieve higher output text quality and con-\nstraint satisfaction than the baseline models (99.9%\nvs. 95.1% and 100% vs. 96.6%). CBS improves\nthe T5 model’s constraint satisfaction, but nega-\ntively affects the text quality (0.3 BLUE points\nlower). Shen et al. (2019), the previous state-of-\nthe-art, trained the model via a complex speaker-\nlistener approach inspired by cognitive science.\n3https://github.com/tuetschek/\ne2e-cleaning/blob/master/slot_error.py\nWith a much simpler model architecture (S2S), our\nT5 + MF model achieves full constraint satisfaction\nand outperforms Shen et al. (2019) by 0.2 NIST\nand 0.3 METEOR.\nMethod BLEU NIST METEOR Constraint\nw/o Pre-training\nTrans, L3 64.7 8.5 43.8 95.1\nTrans, L3 + MF 65.4 8.6 44.9 99.9\nw/ Pre-training\nT5 67.4 8.7 45.5 96.6\nT5 + CBS 67.1 8.7 45.6 100.0\nT5 + MF 68.3 8.9 45.6 100.0\nShen et al. (2019) 68.6 8.7 45.3 -\nTable 2: Experiment Results in the E2ENLG Test Split.\nThe T5 + MF model achieves high text quality with\nhigh constraint satisfaction.\n4.3 nocaps\nUsing T5 for Image Captioning In Image Cap-\ntioning, each input image is represented by a se-\nquence of visual objects. Each of these objects\nis assigned (by the object detector) with a tex-\ntual label. The encoder input is a sequence of\nobjects followed by the same textual labels C =\n[v1\n1, ··· , vs1\n1 , l1, ··· , v1\nk, ··· , vsk\nk , lk] where v∗\ni is\nthe visual feature vector (similar to the one in Li\net al. (2020)) and li is the corresponding textual\nlabel. The visual features are used in the same way\nof normal textual tokens in the T5 models. We\nﬁnd this approach works well for both nocaps and\nstandard COCO image captioning task.\nExperiment Setup Traditional image captioning\nmodels select and describe a subset of input objects\njointly (Anderson et al., 2018). However, Pudup-\npully et al. (2019) shows the beneﬁts of separating\ncontent selection and text planning steps for gen-\neral data-to-text tasks. Following this, we propose\nto ﬁrst select salient objects and incorporate the\nselected objects into the description using Mention\nFlags. m(C, ε) = [0, 0, ··· , 1, ··· , 0, 0, ··· , 1]\nwhere only salient object labels receive value 1.\nm() allows inﬂectional variants to satisfy lexical\nconstraints. We use T5-base model in this exper-\niment. The T5 + C and T5 + MF + C models\nare constrained with CBS. Following Wang et al.\n(2021), we report CIDEr and SPICE as output text\nquality metrics and constraint satisfaction for novel\nconstraints (Novel) and all constraints (ALL). We\npresent the performance for all evaluation images\n109\n(Overall) and for the challenging images with\nonly novel objects (out-of-domain split).\nSalient Object Selector We use a transformer-\nbased salient object detector to select a subset of\nobject labels as lexical constraints. The visual rep-\nresentations of detected image objects are ﬁrst fed\ninto the 3-layer standard Transformer model with-\nout any positional embedding. We train this detec-\ntor using binary Cross-Entropy loss averaged over\nall detected input objects. The training data for\nsalient object detection is the training data in no-\ncaps. We use COCO 2017 Dev set as the evaluation\ndataset to select the best checkpoint.\nout-of-dom. Overall ConstraintMethod CIDEr S CIDEr S Novel ALL\nnocaps Val. (w/o Pre-training)\nTrans, L3 34.2 8.6 58.7 10.6 16.3 35.8\nTrans, L3 + MF 39.8 9.1 60.4 11.2 49.3 71.5\nECOL w/o LM3 34.8 9.2 58.0 11.2 - -\nnocaps Val. (w/ Pre-training)\nT5 63.4 9.9 72.7 11.3 35.8 47.5\nT5 + C 80.2 10.5 79.2 11.6 100 100\nT5 + MF 79.9 10.8 79.9 11.9 96.9 98.3\nT5 + MF + G 79.6 10.6 79.2 11.8 100 100\nT5 + MF + C 79.7 10.7 79.5 11.8 100 100\nOSCARL + C♥ 77.4 10.5 78.6 11.8 - -\nVIVO + C§ 83.0 10.7 85.3 12.2 - -\nnocaps Test\nT5 + MF 71.5 10.4 77.7 12.1 96.3 97.8\nUpDown (E&C)♠ 66.7 9.7 73.1 11.2 - -\nECOL + IB3 67.0 10.3 76.0 11.9 - -\nTable 3: Evaluation Results for nocaps. The T5 +\nMF model produces high-quality text with high con-\nstraint satisfaction, setting a new state-of-the-art among\nthe comparable previous works. C: CBS. G: GBS.\nS: SPICE. Con.: Constraint Satisfaction. §Hu et al.\n(2020), a non-comparable model that uses additional\nvisual-text aligned training data. ♠Agrawal et al.\n(2019). ♥Li et al. (2020). 3 Wang et al. (2021).\nResults Mention Flags achieve optimal con-\nstraint satisfaction in almost all cases. In partic-\nular the Trans, L3 + MF model shows marked im-\nprovement (i.e., from 16.3% to 49.3%) on novel\nconstraints, despite the fact that the correspond-\ning token embeddings are not changed from their\nrandom initialisation. The generated text quality\nis also improved, particularly in the out-of-domain\nsplit. The T5 + C model is 0.3 SPICE lower in both\noverall and theout-of-domain split than the T5 +MF\nmodel, indicating that the MF model correctly cap-\ntures more long-range relationships (calculated by\nthe parsing trees used in SPICE) among the (novel)\nobjects than CBS. OurT5 + MF model outperforms\nthe existing state-of-the-art end-to-end single-stage\nimage captioning systems (Agrawal et al., 2019; Li\net al., 2020; Wang et al., 2021) by 1.3 CIDEr and\n0.1 SPICE on the validation set and 1.7 CIDEr and\n0.2 SPICE on the test set, showing the advantage\nof our two-stage captioning model empowered by\nMention Flags. VIVO + C (Hu et al., 2020) is not\ncomparable as it uses additional visual-text aligned\ntraining data. Finally, we investigate the relatively\nlower constraint satisfaction in nocaps (98.3% vs.\n99.5+%) compared to the MF models in the other\ntwo tasks and ﬁnd that missing cases frequently\nhappen in the instances with two constraints involv-\ning a) (near-) synonymy (e.g., mule and horse) and\nb) hyponymy (e.g., hot dog and fast food). A more\nadvanced salient object detector would solve this\nissue.\n4.4 Model Efﬁciency\nThe MF models use standard beam search and run\nmuch faster with less memory than the constrained\nbeam search algorithms. For comparison, we se-\nlect the GBS algorithm because its resource use is\nlinear in the number of constraints and uses less\nrun time and memory than CBS. We run the MF\nmodels and the models with GBS using beam size\n5 and compare their run time (RT) and memory\nrequirement (#M) in Table 4. Compared to the MF\nmodels, GBS runs one to two orders of magnitude\nslower, and uses 4.4 to 23.4 times more memory.\nCompared to the T5-Base model, the MF models\nonly increases the inference time slightly.\nE2ENLG CommonGen nocapsTask RT #M RT #M RT #M\nT5-Base + G 438 m 16.9 645 m 23.4 93 m 4.4\nT5-Base + MF 19 m 1 10 m 1 18 m 1\nT5-Base 17 m 1 8 m 1 16 m 1\nTable 4: Efﬁciency of the MF and GBS model. RT:\ninference Run Time (in minutes). #M: the number of\nGBS states (indicating the memory required).\n4.5 Main Result Discussion\nConstraint Satisfaction & Text Quality In all\ntasks, MF models improve the text quality over their\nbaselines (including CBS and GBS) while achiev-\ning constraint satisfaction that is close to 100%.\n110\nThis supports the claim in Sec 3.2 that training\nsignals from Mention Flags can help to improve\nconstraint satisfaction and text quality.\nNon-Pre-trained vs. Pre-trained Models In all\ntasks, Mention Flags have a similar effect (higher\ntext quality and constraint satisfaction) on both non-\npre-trained and pre-trained models. This indicates\nthat Mention Flags do not rely on information from\npre-trained models to be effective.\nNovel Constraints In the CommonGen and no-\ncaps tasks, the Trans, L3 +MF model achieve much\nhigher coverage (i.e., 2.3% to 49.2% in Common-\nGen; 16.3% to 49.3% in nocaps) for constraints\nwith novel lexical items than the baseline models.\nHere, the MF models can satisfy novel constraints,\neven where the corresponding token representa-\ntions did not receive any training signals. As Men-\ntion Flags decouples with model representations,\nthe MF models learn lexicon-independent indica-\ntors to mention the novel words.\n4.6 Design Choices for Mention Flags\nWe conduct experiments for following choices of\nMention Flag: Static MF where value 2 ( is men-\ntioned) and 1 (not mentioned) are merged; Merged\nMF where value 0 ( not a constraint ) is merged\nwith value 1; Scalar MF where Mention Flags are\nrepresented as scalars added to the attention log-\nits in the CA module; and Shared MF where all\ndecoder layers use the same Mention Flag embed-\ndings. We apply Static MF, Scalar MF and Shared\nMF to all three tasks. We only use Merged MF\nin E2ENLG because a CommonGen model does\nnot include value 0 and a nocaps model without\nvalue 0 cannot distinguish between constrained and\nnon-constrained objects. As shown in Table 5, in\nthe CommonGen and nocaps tasks, the Static MF\nmodels achieve much lower constraint satisfaction,\n99.6% vs. 94.5% and 98.3% vs. 87.2% respec-\ntively. The explicit update from value 1 to 2 is im-\nportant for high constraint satisfaction. The merged\nMF model produces lower constraint satisfaction\n(100% to 98.9%) and generated text quality (68.3\nBLEU to 67.7 BLEU) in E2ENLG, indicating the\nutility of value 0 in this task. Compared to the MF\nmodels, Scalar MF models produce lower constraint\nsatisfaction in the CommonGen and nocaps task\n(99.6% to 97.1%, 98.3% to 91.5%, respectively)\nand lower-quality generated text in all three tasks\n(1.2 BLEU, 3.2 CIDEr and 0.6 CIDEr lower). Rep-\nresenting Mention Flags as Key and Value dense\nE2ENLG BLEU NIST METEOR Con.\nScalar MF 67.1 8.8 45.3 100\nStatic MF 67.7 8.8 45.8 100\nMerged MF 67.7 8.8 45.3 98.9\nShared MF 67.2 8.8 45.5 99.9\nMF 68.3 8.9 45.6 100.0\nCommonGen CIDEr SPICE C-Novel C-ALL\nScalar MF 166.9 32.7 97.5 97.1\nStatic MF 160.5 32.0 93.5 94.5\nShared MF 168.1 32.8 99.0 99.4\nMF 170.1 32.7 99.4 99.6\nnocaps METEOR CIDEr SPICE Con.\nScalar MF 25.3 79.3 11.8 91.5\nStatic MF 25.3 80.4 11.7 87.2\nShared MF 25.4 78.7 11.8 95.8\nMF 25.6 79.9 11.9 98.3\nTable 5: Ablation Study For MF Status. Static MF re-\nmoves value 2 and Merged MF merges value 0 and\n1. Full MF achieves the highest constraint satisfac-\ntion and output text quality among all other variants.\nCon., C-Novel, C-ALL: constraint satisfaction (resp.\nfor novel/all constraints).\nvectors works better than scalars. Finally, using\nshared MF across all decoder layers has negative\nimpact (e.g., all constraint satisfaction ratio drop)\nin all three tasks.\n4.7 Low-Resource Learning\nThis section shows that Mention Flags are still use-\nful for improving the constraint satisfaction and\ngenerated text quality when trained with many\nfewer instances. We use 0.1%, 1% and 10% of\nthe original training instances to train the models.\nIn the ﬁrst two tasks (E2ENLG and CommonGen),\nwe compare the MF models with T5-Base models.\nIn the nocaps task, we additionally compare the T5-\nBase + MF model with the T5-Base + C model. We\nreport BLEU in E2ENLG CIDEr in CommonGen\nand nocaps. As shown in Table 6, the MF models\nconsistently generate higher-quality text (higher\nMETEOR or CIDEr Score) and achieve higher con-\nstraint satisfaction than the baseline models. The\nMF models reach 97+% when only training with\n10% of theE2ENLG and CommonGen training data.\nThis conﬁrms our claim in Sec. 3.2 that the three\nadded Mention Flag embeddings can be learned\nwith relatively little training data.\n4.8 Qualitative Analysis\nWe chose three representative examples that illus-\ntrate successful use of Mention Flags (Table 7).\n111\nTraining Sample 0.1 % 1 % 10 %\nE2ENLG BLEU Con. BLEU Con. BLEU Con.\nT5-Base 51.3 83.5 60.5 94.7 67.1 95.9\nT5-Base + MF 52.4 87.4 61.1 99.8 67.3 99.9\nCommonGen CIDEr Con. CIDEr Con. CIDEr Con.\nT5-Base 77.9 87.2 95.4 81.5 140.6 91.1\nT5-Base + MF 78.5 89.5 98.7 85.4 149.4 97.6\nnocaps CIDEr Con. CIDEr Con. CIDEr Con.\nT5-Base 43.5 46.2 49.4 44.0 60.8 48.2\nT5-Base + C 50.7 72.4 58.7 82.8 69.3 92.7\nT5-Base + MF 51.7 72.4 60.2 82.8 71.9 92.7\nTable 6: Low-resource Learning. We use 0.1%, 1% and\n10% of the training instances to train the models. Con.:\nconstraint satisfaction.\ni) E2ENLG\nname[Punter], eatType[restaurant], area[riverside],\npriceRange[£20-25], familyFriendly[yes]\nT5-B Punter is a restaurant in the £20-25 price range. It\nis in the riverside area\n+ C Punter is a kid friendly restaurant in the riverside\narea. It has a price range of £20-25.\n+ MF Punter is a kid friendly restaurant in riverside with\na price range of £20-25\nii) CommonGen\nmother, washer, clothes, toddler, help\nT5-B a mother helps a toddler to wash his clothes\n+ G mother helping her toddler clothe in washer\n+ MF a mother helps a toddler to wash clothes in the\nwasher\nGT the mother helps her toddler put the clothes in the\nwasher\niii) nocaps\nSalient Obj: bee, ﬂower; non-Salient Obj: plant, leaf\nT5-B a close up of a ﬂower on a tree\n+ C a close up of a bee ﬂower on a tree\n+ MF a small white ﬂower with a bee in it\nGT a white ﬂower has a bee on it with green around.\nTable 7: Representative examples illustrate successful\nuse of the MF models. GT: ground truth text. +C/+G:\nwith constrained/grid beam search. T5-B: T5 base.\ni) The MF model generates the most concise dia-\nlogue response, compared to the baseline and con-\nstrained decoding model; ii) The MF model is the\nonly model that generates a ﬂuent and coherent sen-\ntence satisfying all input constraints; iii) The MF\nmodel is the only model that accurately describes\nthe relationship between bee and ﬂower, grounding\nto the input images and constraints.\nHuman Evaluation We have shown that our pro-\nposed MF model can achieve higher constraint sat-\nisfaction ratio and automatic metrics. However, the\nautomatic metrics do not necessarily reﬂect human\npreference of the generated text. We therefore se-\nlect 100 output samples from the T5 baseline and\nour MF model in all three tasks (300 in total). For\neach sample pair, we ask three annotators to judge\nwhich sample is “more human-like”. Table 8 shows\nthat more than 70% of output of our MF model is\ngenerally better or similar than the output of the\nbaseline model, verifying the output quality of our\nMF model.\nTask Baseline Equal MF\nCommonGen 27.3% 22.0% 50.7 %\nE2ENLG 30% 25% 45%\nnocaps 28% 26.7% 45.3%\nTable 8: Human Evaluation over output samples in the\nCommonGen, E2ENLG and nocaps task.\n5 Conclusion and Future Work\nIn this paper, we propose Mention Flags to con-\nstrain Transformer-based text generators via inject-\ning mention status embeddings into text decoders.\nOur extensive experiments on three different tasks\nhave shown the effectiveness of Mention Flags in\nmaintaining high generated text quality and excel-\nlent constraint satisfaction, comparing favourably\nto competitive constrained decoding algorithms.\nWe plan to expand Mention Flags i) to control\nlarger input source text such as constrained text\nsummarization and machine translation; ii) to han-\ndle larger granularity such as sentence-level.\nAcknowledgments\nWe thank anonymous reviewers for their insight-\nful suggestions to improve this paper. This re-\nsearch was supported by a Google award through\nthe Natural Language Understanding Focused Pro-\ngram, by a MQ Research Excellence Scholar-\nship and a CSIRO’s DATA61 Top-up Scholarship,\nand under the Australian Research Councils Dis-\ncovery Projects funding scheme (project number\nDP160102156).\n112\nReferences\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. 2019. no-\ncaps: novel object captioning at scale. In Proceed-\nings of the IEEE International Conference on Com-\nputer Vision, pages 8948–8957.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. Spice: Semantic proposi-\ntional image caption evaluation. InEuropean confer-\nence on computer vision, pages 382–398. Springer.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2017. Guided open vocabulary im-\nage captioning with constrained beam search. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n936–945, Copenhagen, Denmark. Association for\nComputational Linguistics.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\nAnusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani,\nMichael White, and Rajen Subba. 2019. Con-\nstrained decoding for neural NLG from composi-\ntional representations in task-oriented dialogue. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 831–\n844, Florence, Italy. Association for Computational\nLinguistics.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization , pages 65–72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics.\nOndˇrej Duˇsek, David M. Howcroft, and Verena Rieser.\n2019. Semantic noise matters for neural natural lan-\nguage generation. In Proceedings of the 12th Inter-\nnational Conference on Natural Language Genera-\ntion, pages 421–426, Tokyo, Japan. Association for\nComputational Linguistics.\nOndˇrej Duˇsek and Filip Jur ˇc´ıˇcek. 2016. Sequence-to-\nsequence generation for spoken dialogue via deep\nsyntax trees and strings. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n45–51, Berlin, Germany. Association for Computa-\ntional Linguistics.\nOndˇrej Duˇsek, Jekaterina Novikova, and Verena Rieser.\n2020. Evaluating the State-of-the-Art of End-to-End\nNatural Language Generation: The E2E NLG Chal-\nlenge. Computer Speech & Language, 59:123–156.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The WebNLG\nchallenge: Generating text from RDF data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 124–133, San-\ntiago de Compostela, Spain. Association for Compu-\ntational Linguistics.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1631–1640, Berlin, Germany. Association for\nComputational Linguistics.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019.\nLevenshtein transformer. In Advances in Neural In-\nformation Processing Systems , volume 32. Curran\nAssociates, Inc.\nChris Hokamp and Qun Liu. 2017. Lexically con-\nstrained decoding for sequence generation using grid\nbeam search. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1535–1546,\nVancouver, Canada. Association for Computational\nLinguistics.\nXiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei\nZhang, Jianfeng Gao, and Zicheng Liu. 2020. Vivo:\nSurpassing human performance in novel object cap-\ntioning with visual vocabulary pre-training. arXiv\npreprint arXiv:2009.13682.\nJuraj Juraska, Panagiotis Karagiannis, Kevin Bowden,\nand Marilyn Walker. 2018. A deep ensemble model\nwith slot alignment for sequence-to-sequence natu-\nral language generation. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 152–162, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nMihir Kale and Abhinav Rastogi. 2020. Text-to-text\npre-training for data-to-text tasks. In Proceedings of\nthe 13th International Conference on Natural Lan-\nguage Generation, pages 97–102, Dublin, Ireland.\nAssociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.\n2020. Oscar: Object-semantics aligned pre-training\n113\nfor vision-language tasks. In Computer Vision –\nECCV 2020, pages 121–137, Cham. Springer Inter-\nnational Publishing.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nChin-Yew Lin and Eduard Hovy. 2003. Auto-\nmatic evaluation of summaries using n-gram co-\noccurrence statistics. In Proceedings of the 2003 Hu-\nman Language Technology Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics, pages 150–157.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S.\nYu. 2021. Kg-bart: Knowledge graph-augmented\nbart for generative commonsense reasoning. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-text generation with content selection and\nplanning. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, 33(01):6908–6915.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nSheng Shen, Daniel Fried, Jacob Andreas, and Dan\nKlein. 2019. Pragmatically informative text gen-\neration. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4060–4067, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of\ngeneral knowledge. In Proceedings of the Thirty-\nFirst AAAI Conference on Artiﬁcial Intelligence ,\nAAAI’17, page 4444–4451. AAAI Press.\nRaymond Hendy Susanto, Shamil Chollampatt, and\nLiling Tan. 2020. Lexically constrained neural ma-\nchine translation with Levenshtein transformer. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3536–\n3543, Online. Association for Computational Lin-\nguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nYufei Wang, Ian D. Wood, Stephen Wan, and Mark\nJohnson. 2021. ECOL-R: Encouraging Copying in\nNovel Object Captioning with Reinforcement Learn-\ning. arXiv preprint arXiv:2101.09865.\nTsung-Hsien Wen, Milica Ga ˇsi´c, Nikola Mrk ˇsi´c, Pei-\nHao Su, David Vandyke, and Steve Young. 2015.\nSemantically conditioned LSTM-based natural lan-\nguage generation for spoken dialogue systems. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , pages\n1711–1721, Lisbon, Portugal. Association for Com-\nputational Linguistics."
}