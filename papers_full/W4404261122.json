{
  "title": "Optimizing LLMs with Direct Preferences: A Data Efficiency Perspective",
  "url": "https://openalex.org/W4404261122",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5114601870",
      "name": "Pietro Bernardelle",
      "affiliations": [
        "The University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A5052565959",
      "name": "Gianluca Demartini",
      "affiliations": [
        "The University of Queensland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4378771755"
  ],
  "abstract": "Aligning the output of Large Language Models (LLMs) with human preferences\\n(e.g., by means of reinforcement learning with human feedback, or RLHF) is\\nessential for ensuring their effectiveness in real-world scenarios. Despite\\nsignificant advancements in LLM alignment techniques, the impact of different\\ntype of preference data on model performance has yet to be systematically\\nexplored. In this study, we investigate the scalability, data efficiency, and\\neffectiveness of Direct Preference Optimization (DPO) in fine-tuning\\npre-trained LLMs, aiming to reduce their dependency on extensive amounts of\\npreference data, which is expensive to collect. We (1) systematically compare\\nthe performance of models fine-tuned with varying percentages of a combined\\npreference judgement dataset to define the improvement curve of DPO and assess\\nits effectiveness in data-constrained environments; and (2) provide insights\\nfor the development of an optimal approach for selective preference data usage.\\nOur study reveals that increasing the amount of data used for training\\ngenerally enhances and stabilizes model performance. Moreover, the use of a\\ncombination of diverse datasets significantly improves model effectiveness.\\nFurthermore, when models are trained separately using different types of\\nprompts, models trained with conversational prompts outperformed those trained\\nwith question answering prompts.\\n",
  "full_text": "Optimizing LLMs with Direct Preferences: A Data Efficiency\nPerspective\nPietro Bernardelle\nThe University of Queensland\nBrisbane, Australia\np.bernardelle@uq.edu.au\nGianluca Demartini\nThe University of Queensland\nBrisbane, Australia\ndemartini@acm.org\nABSTRACT\nAligning the output of Large Language Models (LLMs) with human\npreferences (e.g., by means of reinforcement learning with human\nfeedback, or RLHF) is essential for ensuring their effectiveness in\nreal-world scenarios. Despite significant advancements in LLM\nalignment techniques, the impact of different type of preference\ndata on model performance has yet to be systematically explored.\nIn this study, we investigate the scalability, data efficiency, and ef-\nfectiveness of Direct Preference Optimization (DPO) in fine-tuning\npre-trained LLMs, aiming to reduce their dependency on extensive\namounts of preference data, which is expensive to collect. We (1)\nsystematically compare the performance of models fine-tuned with\nvarying percentages of a combined preference judgement dataset\nto define the improvement curve of DPO and assess its effective-\nness in data-constrained environments; and (2) provide insights for\nthe development of an optimal approach for selective preference\ndata usage. Our study reveals that increasing the amount of data\nused for training generally enhances and stabilizes model perfor-\nmance. Moreover, the use of a combination of diverse datasets signif-\nicantly improves model effectiveness. Furthermore, when models\nare trained separately using different types of prompts, models\ntrained with conversational prompts outperformed those trained\nwith question answering prompts.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíLanguage models.\nKEYWORDS\nLLMs, Direct Preference Optimization, Data Selection\n1 INTRODUCTION\nThe rise of LLMs, such as OpenAI‚Äôs GPT [ 2] and Google‚Äôs BERT\n[5] families, has marked a revolutionary advancement in natural\nlanguage processing. These models excel in syntactic and semantic\nunderstanding yet aligning them with human preferences remains\nchallenging.\nOver the past few years, significant work has attempted to ad-\ndress the misalignment challenge. This has led to the adoption of\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR-AP ‚Äô24, December 9‚Äì12, 2024, Tokyo, Japan\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0724-7/24/12. . . $15.00\nhttps://doi.org/xxxxxx\nReinforcement Learning (RL) techniques that incorporate human\npreferences to guide LLMs optimization. Rafailov et al. [8] intro-\nduced one of the most innovative approach in this field: Direct\nPreference Optimization (DPO). Despite its promise, to date, only a\nlimited number of studies have explored its applications and poten-\ntial benefits, suggesting that much remains to be discovered and\nunderstood about its effectiveness. This gap in research highlights\nthe need to further investigate how DPO can enhance the align-\nment of language models with human preferences efficiently and\neffectively.\nOur research conducts an experimental exploration of the DPO\nmethod, examining its nuances and evaluating its efficacy in real-\nworld applications. The objective is to improve our understanding\nof the impact of DPO, setting the stage for future development of\nefficient methods for utilizing human preference data and stream-\nlining the LLM training process. To achieve our goal, we aim to\naddress the following research questions:\nRQ1: How does the performance of LLMs fine-tuned with DPO\nevolve as increasingly larger subsets of preference data are used\nfor training?\nRQ2: How does the nature of the training data, specifically con-\nversational versus question answering datasets, impact the model\nperformance under DPO?\n2 RELATED WORK\nIn recent years, numerous influential studies have tackled the chal-\nlenge of misalignment in language models. Christiano et al. [3] are\nthe pioneers in merging the concepts of Reinforcement Learning\n(RL) with Human Feedback (RLHF), showcasing the potential of\nthis method in guiding the training of models with direct human\npreference judgements. Their work laid the foundation for future\nenhancements in model behavior through human evaluation. Build-\ning on these foundational insights, Ouyang et al. [7] and Bai et al.\n[1] extended the application of RLHF to a broader range of lan-\nguage tasks, further developing the methodology for LLMs. Their\ncontributions have been crucial in detailing the operational frame-\nwork of RLHF, which involves a three-phase alignment process: (1)\npretraining and supervised fine-tuning of the language model; (2)\ntraining of a reward model using explicit human preferences; and\n(3) alignment of the language model via reinforcement learning\nthrough the reward model.\nWhile effective in some scenarios, questions arise about the\nefficiency, scalability, and extensive data requirements of this ap-\nproach during the training process. Recent developments in the\nfield have introduced a significant paradigm shift, aiming to miti-\ngate these challenges. Rafailov et al. [8] introduced a methodology\narXiv:2410.16586v1  [cs.AI]  22 Oct 2024\nSIGIR-AP ‚Äô24, December 9‚Äì12, 2024, Tokyo, Japan Pietro Bernardelle and Gianluca Demartini\nTable 1: Overview of the datasets used in the analysis. The table details dataset size, partitioning\ninto training, evaluation, and testing sets, and the types of prompts included.\nDataset Size Training (80%) Evaluation (10%) Testing (10%) Prompt Type\nDataset Aa 7,560 6,048 756 756 Conversational\nDataset Bb 12,900 10,320 1,290 1,290 Question-Answering\nDataset Cc 63,600 50,880 6,360 6,360 Question-Answering\nCombination 84,060 67,248 8,406 8,406 Conversational &\nQuestion-Answering\na https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized\nb https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs\nc https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences\nknown as DPO, which seeks to optimize language model align-\nment directly based on human preferences, thus eliminating the\nneed for an explicit reward model (having it embedded in a loss\nfunction) and for reinforcement learning. By eliminating the re-\nliance on reinforcement learning, this approach streamlines the\ntraining process and challenges the conventional wisdom of rein-\nforcement learning-based alignment frameworks. Although there\nhave been considerable advancements in language model align-\nment techniques, focusing primarily on refining architectures for\nimproved efficiency, the crucial aspect of human preference data\nutilization remains largely under-explored.\n3 METHODOLOGY\nData. To address the research questions outlined above, we\nconducted our experiments using three open-source preference\njudgement datasets. Table 1 provides an aggregated view of their\ncharacteristics. All selected datasets are sourced from Hugging Face\nand provided by Argilla1. Two of the three datasets employed in\nour study originate from established datasets: OpenOrca [6] and\nUltraFeedback [4]. The third dataset is based on a newly curated\ncollection featuring conversational style prompts: Capybara2. The\nselected datasets provide a broad spectrum of scenarios and data\ncharacteristics, enabling a thorough assessment of the DPO-aligned\nmodels‚Äô behavior when exposed to different volumes and types of\npreference data.\nExperimental setup. Our study is structured into two experi-\nments. The first experiment, addressing RQ1, is designed to deter-\nmine the performance of DPO-aligned models as the amount of data\nincreases. We combined the individual datasets into a single pool\nto control for content variability3. From this combined dataset, we\nrandomly sampled five subsets‚Äî20%, 40%, 60%, 80%, and 100%‚Äîof\nthe training split. Each subset was used to train separate instances\nof the base model. OpenHermes-2.5-Mistral-7B 4 was adopted as\nthe base model, primarily to ensure our experiments remain re-\nproducible and grounded in a high-performing, state-of-the-art\nopen-source model. The process was repeated three times with\n1https://huggingface.co/argilla\n2https://huggingface.co/datasets/LDJnr/Capybara\n3The pool, formed by combining the entire individual datasets, was then split into\ntraining, evaluation, and testing segments, allocated as 80%, 10%, and 10% respectively.\n4https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\ndifferent random seeds, and resulted in a total of 15 different DPO-\naligned models. By incrementally increasing the data volume and\nrepeating the training multiple times, we accounted for variability\nand aimed to smooth the results by averaging them.\nThe second experiment, addressing RQ2, focuses on the individ-\nual characteristics of the three datasets. This approach is designed\nto discern whether specific types of data, particularly those with\nconversational versus question answering style prompts, have a\nmore pronounced effect on the efficiency and effectiveness of the\ntraining under DPO. In the second experiment, we mirrored the\nmethodology used in the initial experiment but applied it to each\nindividual dataset separately. This approach yielded 45 distinct\nDPO-aligned models.\nPerformance Evaluation. Each DPO-aligned model was eval-\nuated relative to the base model using MT-Bench [9]. MT-Bench\npresents a series of questions to both the base model and the DPO-\naligned model, and for each question, it determines a ‚Äòwin‚Äô for the\nmodel that provides the best answer or records a ‚Äòtie‚Äô if no clear best\nanswer is identified. Based on the structured evaluation framework\nprovided by MT-Bench, we calculated the improvement over the\nbase model by subtracting the percentage of wins of the base model\nfrom the percentage of wins of the model aligned with DPO, as\nthe amount of data used for alignment increases. Additionally, we\ndefined and calculated the tie rate between the DPO-aligned model\nand the base model at each data percentage subset as follows:\nTie Rate = 1 ‚àí(Win Rate +Lose Rate)\nwhere the win rate and lose rate are derived from the combined\nresults of the three runs for each data percentage subset.\nComputational Resources. For our experiments, we utilized\na single H100 GPU card and approximately 80GB of RAM to ac-\ncommodate the datasets. The training duration varied depending\non the dataset size. For models relying on the largest datasets, the\ntraining process took up to one day, while models trained on the\nsmallest datasets completed in approximately two hours.\n4 RESULTS\n4.1 Defining DPO‚Äôs Improvement Curve\nFigure 1 illustrates the percentage improvement in model perfor-\nmance across the three random seeds for the first experiment. Based\non the results summarized in Figure 1, there is noticeable variability\nOptimizing LLMs with Direct Preferences: A Data Efficiency Perspective SIGIR-AP ‚Äô24, December 9‚Äì12, 2024, Tokyo, Japan\nFigure 1: The top plot illustrates the improvement curves\nof DPO across three different experimental runs using the\ncombined dataset. The bottom plot presents the averaged\nimprovement curve of DPO, aggregating the results from the\nthree experimental runs. Error bars indicate the standard\ndeviation across the runs.\nin performance improvement across the three experimental runs,\nindicating that the models‚Äô performance may vary significantly\ndepending on the specific data subsets used for alignment. Runs 2\nand 3 exhibit similar patterns (Pearson ùëÖ = 0.94, ùëÉ = .006, ùõº = .05),\ndiverging noticeably from run 1, especially in the middle data usage\npercentages (40%, 60% and 80%). The similarity between runs 2 and\n3 compared to run 1, coupled with the marked dip at 60% followed\nby a significant rebound, highlights the need for a more detailed\nanalysis into how different data samples may distinctly affect the\ntraining process. Which preference judgements we use does matter.\nA comprehensive examination of these data subsets could reveal\ncritical insights into optimizing the training process for more con-\nsistent and predictable improvements. Despite these fluctuations, a\nconsistent trend emerges in which increased data usage generally\ncorrelates with enhanced performance improvements. This pattern\nsupports the hypothesis that greater data volumes positively influ-\nence the model‚Äôs alignment with human preferences. However, the\noverall trend is not as smooth as initially hypothesized, which may\nbe attributed to the variability across the three runs. Additional\nruns could potentially smooth out these irregularities, providing a\nclearer depiction of the trend.\nAdditionally, it is interesting to point out that, as the data volume\nincreases, the model aligned with DPO becomes distinctly more\naccurate in its responses compared to the base model. The tie rate\nbetween models decreases as the percentage of data used for DPO\nalignment increases, suggesting that the judging mechanism finds\nFigure 2: Relationship between the percentage of data and tie\nrate. The plot demonstrates how the tie rate tend to decrease\nas the percentage of data increases.\nit easier to discern and prefer the responses of the DPO-aligned\nmodel. This indicates clearer differentiation and better alignment\nwith desired outputs.\nFigure 2 reveals that the tie rate consistently diminishes as the\ndata volume increases, with the exception of the 100% data usage\nmark. This pattern provides two significant insights:\n(1) The judging model is convinced of the performance dip ob-\nserved around 60% of data usage shown in Figure 1, indicat-\ning that at this stage, the DPO-aligned model responses are\nless distinguishable from the base model responses.\n(2) The increased tie rate at 100% data usage suggests that the\njudging model confidence in distinguishing between the\nDPO-aligned and base model decreases when the improve-\nment curve plateaus. This phenomenon implies that while\nadditional data contributes to performance gains, there might\nbe a threshold beyond which the added data does not signif-\nicantly enhance model differentiation.\n4.2 Impact of Data Nature\nWhen we delve into the unique characteristics of the three individ-\nual datasets, we can identify the distinct contributions and impacts\nof different data types on model performance. As Figure 3 shows:\n‚Ä¢Dataset A, the smallest dataset in our study, shows a pos-\nitive trend in model performance with increased data us-\nage, achieving improvements comparable to those of models\ntrained on much larger datasets. This highlights the signif-\nicant role that conversational prompts play in providing\ndynamic and context-rich interactions.\n‚Ä¢Dataset B shows the potential for significant performance\nimprovements through DPO, albeit with some variability\nand non-linear trends. The unexpected dip in performance\nat intermediate data volumes underscores the need for care-\nful curation of training data to maximize the benefits of\nadditional data.\n‚Ä¢Dataset C large size allows for more robust learning, reducing\nthe impact of any individual data points that might introduce\nnoise or anomalies. The improvement curve suggests that\nthe model effectively leverages the additional data, steadily\nrefining its alignment with human preferences as more data\nSIGIR-AP ‚Äô24, December 9‚Äì12, 2024, Tokyo, Japan Pietro Bernardelle and Gianluca Demartini\nFigure 3: Comparison of the percentage of improvement\nacross three datasets (Dataset A, Dataset B, Dataset C) relative\nto the percentage of data used for alignment.\nTable 2: Performance improvements of the DPO-aligned mod-\nels across the four datasets. The table lists both peak and av-\nerage percentage improvements compared to the base model\nwithout alignment.\nDataset Peak Avg\nImprovement (%) Improvement (%)\nDataset A 7.5% 4.93%\nDataset B 8.54% 3.925%\nDataset C 8.325% 7.6%\nCombination 19.126% 15.35%\nis introduced. This contrasts with the more variable perfor-\nmance observed in the smaller datasets, where the model\nimprovements were less consistent and more sensitive to\nspecific subsets of data.\n4.3 Implications and Observations\nThe results from the two experiments provide several important\nimplications for the application of DPO in language model training:\n‚Ä¢Significance of Data Volume : The trend across all datasets\nhighlights that increased data volume for alignment gener-\nally enhances model performance and stability.\n‚Ä¢Combined Datasets for Alignment : Combining multiple\ndatasets for alignment consistently results in superior per-\nformance compared to using individual datasets, as shown\nin Table 2. This finding underscores the importance of di-\nversity and comprehensiveness in training data, suggesting\nthat a holistic approach to dataset selection can significantly\nenhance model effectiveness.\n‚Ä¢Impact of Data Type : The type of data significantly in-\nfluences the model improvement. Conversational prompts\n(Dataset A) lead to steady enhancements despite its smaller\nsize, likely because the model can grasp a better understand-\ning of the context with this type of data. This indicates that\nconversational prompts are particularly effective in improv-\ning model performance and could be prioritized in future\ndata selection approaches.\n‚Ä¢Performance Dips and Training Dynamics: The observed\ndips in performance across different data volumes, though\nnot entirely explainable through our current analysis, point\nto the inherent complexities in training dynamics. These fluc-\ntuations suggest that there are underlying factors affecting\nperformance that warrant further investigation. The random\nsampling of data likely contributed to the dispersion of noise\nacross the performance curves, indicating a need for more\ncontrolled and systematic data sampling methods in future\nstudies.\nThis finding underscores the value of using a more extensive\nand varied dataset for alignment, as it can capture a broader range\nof linguistic patterns and nuances, thereby enhancing the overall\nperformance of the model.\n5 CONCLUSION AND FUTURE WORK\nIn this study, we have explored the effectiveness and efficiency\nof DPO in aligning LLMs to human preference judgments. Our\nfindings indicate that the relationship between data volume and\nmodel performance improvement is not strictly linear, as might be\nexpected; rather, it shows notable fluctuations influenced heavily\nby the specific data subsets being used. Thus, preference judgement\nquality over quantity might be the way forward for LLM alignment.\nOverall, this research enriches our knowledge of the DPO method,\nhighlighting the need for data selection strategies that can system-\natically and consistently yield favorable results, even with limited\namounts of data.\nThere is considerable potential in developing targeted strategies\nfor optimizing preference data selection, which could streamline\nLLM training processes, enhancing efficiency and reducing costs. By\npursuing this direction, future research can continue to refine and\nenhance the alignment of LLMs with human preferences, ultimately\nadvancing natural language processing technologies.\nACKNOWLEDGMENTS\nThis work is partially supported by the Australian Research Coun-\ncil (ARC) Training Centre for Information Resilience (Grant No.\nIC200100022).\nOptimizing LLMs with Direct Preferences: A Data Efficiency Perspective SIGIR-AP ‚Äô24, December 9‚Äì12, 2024, Tokyo, Japan\nREFERENCES\n[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Das-\nSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,\nSaurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,\nZac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna\nKravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,\nJack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Train-\ning a Helpful and Harmless Assistant with Reinforcement Learning from Human\nFeedback. arXiv:2204.05862 [cs.CL]\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al. 2020. Language models are few-shot learners. Advances in neural information\nprocessing systems 33 (2020), 1877‚Äì1901.\n[3] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario\nAmodei. 2017. Deep Reinforcement Learning from Human Preferences. Advances\nin Neural Information Processing Systems 30 (2017).\n[4] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong\nXie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language\nmodels with high-quality feedback. arXiv preprint arXiv:2310.01377 (2023).\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProc. NAACL-HLT 2019 . Association for Computational Linguistics, Minneapolis,\nMinnesota, 4171‚Äì4186.\n[6] Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,\nand \"Teknium\". 2023. OpenOrca: An Open Dataset of GPT Augmented FLAN\nReasoning Traces. huggingface.co (2023).\n[7] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin Neural Information Processing Systems 35 (2022), 27730‚Äì27744.\n[8] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language\nModel is Secretly a Reward Model. Advances in Neural Information Processing\nSystems 36 (2023), 53728‚Äì53741.\n[9] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information\nProcessing Systems 36 (2024).",
  "topic": "Preference",
  "concepts": [
    {
      "name": "Preference",
      "score": 0.7344607710838318
    },
    {
      "name": "Computer science",
      "score": 0.7076533436775208
    },
    {
      "name": "Scalability",
      "score": 0.6252260208129883
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.6013354659080505
    },
    {
      "name": "Machine learning",
      "score": 0.5817217826843262
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5348901152610779
    },
    {
      "name": "Preference learning",
      "score": 0.45198312401771545
    },
    {
      "name": "Judgement",
      "score": 0.43921053409576416
    },
    {
      "name": "Statistics",
      "score": 0.07968232035636902
    },
    {
      "name": "Database",
      "score": 0.078244149684906
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "cited_by": 4
}