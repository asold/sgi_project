{
  "title": "Augmenting Transformers with KNN-Based Composite Memory for Dialogue",
  "url": "https://openalex.org/W3020775333",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221722489",
      "name": "Fan, Angela",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224068542",
      "name": "Gardent, Claire",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4260715000",
      "name": "Braud, Chloé",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3088693642",
      "name": "Bordes, Antoine",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2971883198",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2539809671",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2970401203",
    "https://openalex.org/W2963614749",
    "https://openalex.org/W2615146352",
    "https://openalex.org/W2963703197",
    "https://openalex.org/W2964210218",
    "https://openalex.org/W2963945575",
    "https://openalex.org/W2995289474",
    "https://openalex.org/W2969574947",
    "https://openalex.org/W2250384498",
    "https://openalex.org/W2885421725",
    "https://openalex.org/W2962805889",
    "https://openalex.org/W2584341106",
    "https://openalex.org/W2550893117",
    "https://openalex.org/W1732222442",
    "https://openalex.org/W2604368306",
    "https://openalex.org/W2964458951",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W2995154514",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3035448310",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2400680200",
    "https://openalex.org/W2971300525",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2982260276",
    "https://openalex.org/W2963475460",
    "https://openalex.org/W3016829315",
    "https://openalex.org/W2540419089",
    "https://openalex.org/W2970579055",
    "https://openalex.org/W1544092585",
    "https://openalex.org/W2950729111"
  ],
  "abstract": "Various machine learning tasks can benefit from access to external information of different modalities, such as text and images. Recent work has focused on learning architectures with large memories capable of storing this knowledge. We propose augmenting generative Transformer neural networks with KNN-based Information Fetching (KIF) modules. Each KIF module learns a read operation to access fixed external knowledge. We apply these modules to generative dialog modeling, a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation. We demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images, and human-written dialog utterances, and show that leveraging this retrieved information improves model performance, measured by automatic and human evaluation.",
  "full_text": "Augmenting Transformers with KNN-Based\nComposite Memory for Dialog\nAngela Fan\nFacebook AI Research\nUniversit´e de Lorraine\nLORIA\nangelafan@fb.com\nClaire Gardent\nCNRS/LORIA\nclaire.gardent@loria.fr\nChlo´e Braud\nCNRS/IRIT\nchloe.braud@irit.fr\nAntoine Bordes\nFacebook AI Research\nabordes@fb.com\nAbstract\nVarious machine learning tasks can beneﬁt from\naccess to external information of different modal-\nities, such as text and images. Recent work has\nfocused on learning architectures with large mem-\nories capable of storing this knowledge. We pro-\npose augmenting generative Transformer neural\nnetworks with KNN-based Information Fetching\n(KIF) modules. Each KIF module learns a read\noperation to access ﬁxed external knowledge. We\napply these modules to generative dialog model-\ning, a challenging task where information must\nbe ﬂexibly retrieved and incorporated to maintain\nthe topic and ﬂow of conversation. We demon-\nstrate the effectiveness of our approach by iden-\ntifying relevant knowledge required for knowl-\nedgeable but engaging dialog from Wikipedia, im-\nages, and human-written dialog utterances, and\nshow that leveraging this retrieved information\nimproves model performance, measured by auto-\nmatic and human evaluation.\n1 Introduction\nMachine learning approaches to various tasks, such as\ngame-playing or dialog, are often dependent on ex-\nternal information. This information can take multi-\nmodal forms, including structured knowledge bases,\nfree text, and images, and also comes in overwhelm-\ningly large quantities. A pressing challenge is to cre-\nate models that can identify which speciﬁc elements of\nmultiple information sources are relevant in a particular\ncontext, and incorporate them into standard architec-\ntures on each task. In this work, we focus on human-\nmachine dialog and how to efﬁciently retrieve external\nknowledge that is relevant to the dialog. We consider\ntwo scenarios and for each scenario, retrieve two types\nof knowledge: (i) knowledge about similar dialog con-\ntexts and (ii) external knowledge used to ground the\nconversation into real world information.\nKnowledge about similar dialog contexts allows for\na hybrid retrieval/generative approach to dialog where\nthe system response is generated based not only on\na representation of the current dialog context and of\nthe relevant world knowledge, but also based on a re-\nsponse retrieved from a similar dialog context. The re-\ntrieved knowledge can be viewed as providing informa-\ntion about structure and dialog sentences, or utterances:\nwhich response is likely given a similar context?\nExternal knowledge is also retrieved to improve the\nsemantic content of the dialog model. In one scenario,\nWizard of Wikipedia (Dinan et al., 2018), general top-\nics are provided to crowdworkers, who are asked to\nhave in-depth and speciﬁc conversations about these\ntopics by referencing speciﬁc Wikipedia sentences as\nknowledge. In this scenario, external knowledge is re-\ntrieved from a pre-selected set of Wikipedia sentences\nassociated with the current dialog topic. Retrieval aims\nto select the sentence that is most relevant at each step\nof the dialog and thereby to ground system responses\nin relevant world knowledge (e.g. by referring to Star\nWars when talking about science ﬁction).\nIn the other scenario, Engaging ImageChat (Shuster\net al., 2020), crowdworkers are provided with images\nand asked to have a conversation inspired by or about\nthe image. In this case, the retrieved external knowl-\nedge is images and their associated dialogs. By retriev-\ning images that are similar to the image being talked\nabout, we aim to enrich system responses with knowl-\nedge about what is typically mentioned when describ-\ning similar images (e.g. when talking about an image\nwith dogs, mentioning their breed).\nOur work on incorporating different types and\nmodalities of knowledge is related to methods that\nstrive to add external memory, such as knowledge\nbases, to neural networks. Previous work has explored\nincorporating large external memories into neural net-\nwork layers (Weston et al., 2015; Sukhbaatar et al.,\n2015, 2019; Lample et al., 2019). Many existing ap-\nproaches focus on using attention over the memory\nslots, which is computationally intensive and becomes\nless effective as the the size of the memory grows. In\nthis work, we propose representing multiple sources of\nexternal information as ﬁxed encodings and using K\nNearest Neighbors search to fetch relevant information.\nKNN search is computationally efﬁcient and scalable,\nand libraries like faiss (Johnson et al., 2019) allow\nKNN to be easily used on GPUs and integrated into\nneural networks. Further, the external memories are\npre-encoded, so the information encoding is only com-\nputed once. As the external memories are kept ﬁxed,\nthey do not require any training to learn the memories\nalong with the model. We can thus scale easily to larger\narXiv:2004.12744v2  [cs.CL]  10 Nov 2020\nmemories by learning only the KNN-based read opera-\ntion to identify relevant information from the memory.\nOur core contribution proposes an efﬁcient, KNN-\nbased Information Fetching (KIF) module that can ac-\ncess relevant external knowledge, combine knowledge\nfrom different sources, and integrate this information\ninto standard sequence to sequence architectures. We\napply these ﬂexible modules to two dialog datasets that\nchallenge generative models to leverage external infor-\nmation to write coherent, on-topic responses. Both of\nour chosen tasks require models to leverage external\ninformation, such as information from Wikipedia or\nimages, to engage in the conversation. We show that\nrelevant information can be identiﬁed from hundreds\nof thousands of candidates in a multi-modal, multi-\nknowledge-source setting to improve the performance\nof generative dialog models. Further, the output of\nthe KIF modules is interpretable as speciﬁc human-\nreadable knowledge elements are selected, allowing\nusers to better understand the information the gener-\native model conditions upon when writing the subse-\nquent utterance. On both datasets, we achieve state-of-\nthe-art results compared to generative models and ﬁnd\nthere is no statistically signiﬁcant difference in the in-\nterestingness or human preference of our model output\ncompared to state-of-the-art retrieval models.\n2 Related Work\nWe discuss related work on learning to incorporate ex-\nternal knowledge into neural networks and efﬁciently\naccess relevant information. We then describe work in\ngenerative dialog that incorporates knowledge.\n2.1 Incorporating External Knowledge\nAugmenting neural networks with memory, or longer\nterm components that can be accessed with read and\nwrite operations, has been explored in various proposed\narchitectures. For example, Memory Networks (We-\nston et al., 2015; Sukhbaatar et al., 2015, 2019) intro-\nduce attention mechanisms over large external memo-\nries. Neural cache models (Grave et al., 2017b) sim-\nplify these to access previous memories with a dot\nproduct. Previous work has also studied how to read\nand write into these memory architectures (Rae et al.,\n2016; Graves et al., 2014; Joulin and Mikolov, 2015).\nIn contrast, we focus on how to read large memories.\nAnother line of research has focused on computa-\ntional scalability for larger external memories to allow\nefﬁcient access of information. For example, Chandar\net al. (2016) propose a hierarchical memory network\nrather than a ﬂat one and Rae et al. (2016) learn sparse\noperations to read and write. Lample et al. (2019) fo-\ncus on learning memories of up to one million slots\nand how to efﬁciently access the slots using product\nkeys. Khandelwal et al. (2019) use nearest neighbor\noperations to augment language models by perform-\ning retrieval at the token level — in contrast, we focus\non multi-modal retrieval of multiple pieces of knowl-\nedge based on an entire dialog context. Beyond ex-\nplicit memory representations, it may be possible to\nstore information implicitly during training time by\nmemorizing common patterns present in text (Petroni\net al., 2019). We focus on learning to fetch relevant in-\nformation from multiple explicit external multi-modal\nknowledge sources and integrate them into one net-\nwork. Further, our work allows the retrieved informa-\ntion to be interpreted as each memory slot is an explicit\nfact that can be read as text, rather than a learned vector\nsuch as in Lample et al. (2019).\nWork has also focused on computationally efﬁcient\nsoftmax operations (Mnih and Hinton, 2009; Grave\net al., 2017a; Chen et al., 2016). Many approximate\nsoftmax techniques use KNN-like operations to form\nclusters, and the overall softmax operation is con-\nstrained by the slow calculation of the exponential. Our\nusage of KNN beneﬁts from efﬁcient and scalable li-\nbraries such as faiss and nmslib.\n2.2 Generative Dialog\nWe develop a general architecture for incorporating ex-\nternal information and apply it to the case of generative\ndialog models. Previous work in dialog has leveraged\nknowledge as necessary information to accomplish the\ntask. For example, airline and restaurant booking tasks\noften use API calls to access information about reser-\nvation times and availability (Bordes et al., 2017). In\ncontrast, our work focuses on how to incorporate un-\nstructured knowledge, such as free text found on the\nweb. Previous work has employed architectures that\nattend over the available knowledge and identify rel-\nevant pieces of information, which scales poorly with\nlarge quantities of information (Dinan et al., 2018; Qin\net al., 2019; Lian et al., 2019). We replace the use of\nattention over external information with the output of\na KNN module. Other work has investigated incorpo-\nrating information retrieval in language modeling and\nquestion answering (Chen et al., 2017; Fan et al., 2019;\nSeo et al., 2019; Guu et al., 2020), while we focus on\ndialog applications and ﬂexibly incorporating knowl-\nedge from multiple, multi-modal sources.\nOn the modeling side, work has explored both gener-\native (Serban et al., 2016b,a) and retrieval based mod-\nels (Zhang et al., 2018), which identify the best utter-\nance from the training set to return as the dialog re-\nsponse. This often leverages self-attention or cross-\nattention mechanisms (Humeau et al., 2019). Further\nwork has explored hybrid models, for example using\nthe output of a retrieval model as input for a gener-\native model (Dinan et al., 2018; Weston et al., 2018;\nCai et al., 2019; Zhu et al., 2020). Some of this work\nhas specialized to use both types of models to gener-\nate conversations in an ensemble (Song et al., 2016) or\nto speciﬁcally improve consistency (Song et al., 2020).\nWe extend these approaches by augmenting generative\nmodels with retrieval-like operations based on KNN\nsearch, allowing dialog models to ﬂexibly incorporate\nFigure 1: KIF modules fetch relevant information from multi-modal external knowledge. External knowledge\nsources E1 and E2 are pre-encoded by encoder M (green). In the model, input xi is encoded by encoder M′\n(blue) to produce M′(xi). KIF modules (orange) operate on M′(xi) and identify the nearest neighbors encoded\nin M(E1) and M(E2) using KNN. Identiﬁed relevant elements from E1 and E2 are re-encoded by M′ in a\ngating mechanism with a weighted sum (represented byσ(WS1i) ·WS1i, where WS stands for weighted sum), then\nconcatenated to M′(xi). Full description with notation can be found in Section 3.\nvarious sources of external knowledge at the same time\nand scale to large quantities of retrieval candidates.\n3 KNN-based Information Fetching\nModules\nBroadly, the KNN-based Information Fetching (KIF)\nmodule assumes an encoder model M can access in-\nputs X = {x1,x2,...,x n}. For example, X can be\na collection of sentences, and xi represents an individ-\nual sentence. In a setting without additional support-\ning information, the encoder will process an input xi\nand produce the encoder output M(xi). If xi is a se-\nquence such as a sentence, then M(xi) is a representa-\ntion of the variable size of the sequence length by the\nﬁxed size encoder M’s hidden size. However, in many\ntasks, additional information is present, represented as\nE = {e1,e2,...,e m}. We encode each element of\nX and E into a vector representation using the en-\ncoder. To identify the closest information in E that\nis relevant to xi, our general approach will be to use\nK Nearest Neighbors by comparing the representation\nof xi with the representation of each element in the set\nE. K Nearest Neighbors is a fully differentiable op-\neration (Pl¨otz and Roth, 2018), so can be incorporated\nin a straightforward way into neural models. The most\nrelevant information in E will then be available in the\nmodel. We display a KIF-Augmented model in Fig-\nure 1 and describe how the KIF module operates.\nOne challenge to overcome is that the representa-\ntion of all elements of the knowledge source E are\npre-computed and kept ﬁxed, creating M(E) — we\ndo not backpropagate to affect the embeddings of the\npre-encoded knowledge. In the early stages of train-\ning, the model receives large amounts of loss, which\nwould affect the quality of the pre-encoded embeddings\nif we backpropagated to them. Further, encoding the\nﬁxed external knowledge once and re-using it allows\nfor greater scalability. However, this lack of backprop-\nagation can introduce a mismatch between the encod-\ning of E and the encodings produced by a model that is\ntraining, as the training model has constantly changing\nrepresentations because the weights are being learned.\nWe useMto represent the original encoder model used\nto encode E and M′to represent the constantly train-\ning model that is encoding X. The model must learn a\nfunction to align M′(xi) to the pre-encoded elements\nof the external memory M(E).\nTo circumvent this misalignment, we learn a map-\nping operator fE(M′(xi)) that trains to map elements\nof the model’s representation of X, or M′(X), into\nthe additional information representation spaceM(E).\nConcretely, fE(M′(xi)) is a multi-layer perceptron\nwith ReLU nonlinearities. From the input elements\nof X, fE(M′(xi)) learns representations of an out-\nput close to the corresponding projection of X into\nE. This can be interpreted as learning a read oper-\nation on a ﬁxed external memory. If there was no\nchange to the encoding of the model compared to the\npre-computed knowledge, then the ideal mapping oper-\nator would be the identity function (as M′would equal\nM). However, as the model changes signiﬁcantly dur-\ning the training process, the nonlinear mapping capa-\nbility of fE(M′(xi)) is essential to be able to identify\nthe correct knowledge E from the input X.\nThus, a model augmented with KIF will incorporate\nexternal knowledge in the following manner. First, we\nﬁnd the k nearest elements to fE(M′(xi)) in M(E),\nbased on KNN search with inner product. Then, the\nrelevant elements identiﬁed by KNN are re-encoded by\nM′. For example, if element ej is retrieved by KIF, it\nwould produce M′(ej). We use the optimized faiss\nlibrary for KNN search, which can conduct billion-\nscale KNN efﬁciently on GPUs.\nThe KNN output for an element xi is produced by\nusing faiss to search for the k nearest representa-\ntions to fE(M′(xi)) in M(E). Note that as the en-\ncoders M and M′ produce output representations of\nvariable length (for example, in the case where xi is a\nvariable length sequence, such as a sentence), we aver-\nage across the length dimension to produce a ﬁxed-size\nrepresentations rto conduct the KNN search.\nrxi = Avg\n(\nfE(M′(xi))\n)\n(1)\nRE =\n{\nAvg(M(e)) |e∈E\n}\n(2)\nKNNxi = KNearest\n(\nk,rxi ,RE\n)\n(3)\nThen, the KIF module output for an element xi is\nthe set of all re-encoded representations of the KNN-\nretrieved knowledge:\nKIFxi =\n{\nM′(e) |e∈KNNi\n}\n(4)\nThese elements are weighted by their normalized near-\nest neighbor scores and then summed. This is subse-\nquently concatenated to M′(xi) to form the ﬁnal en-\ncoder output:\n[M′(xi),WeightedSum(KIFi)] (5)\nThis can be easily extended to using multiple mod-\nules simultaneously. For instance, two sources of exter-\nnal information, E1 and E2, can be combined by iden-\ntifying the top candidates of each information source.\nThe weighted sum of the KIF output on each infor-\nmation source is concatenated with the encoded input\nM′(xi). The KIF output dimensionality is the same\nsize as the hidden size of M′(xi), so they can be di-\nrectly concatenated.\nFinally, different sources of information may not\nbe required for every prediction and some information\nsources can be more important than others. To allow\nthe model to make more ﬁne-grained decisions about\nwhat information to use from what source, and how\nmuch of it, we add a gating mechanism using a sigmoid\nfunction around each weighted sum of KNN represen-\ntations. KIF1i and KIF2i denote the KIF module from\nEquation 4 applied to E1 and E2 respectively.\nWS1i = WeightedSum(KIF1i) (6)\nWS2i = WeightedSum(KIF2i) (7)\nwhich produces the ﬁnal encoder output, a concatena-\ntion of M′(xi) with the output of multiple KIF mod-\nules:\n[\nM′(xi), σ(WS1i) ·WS1i, σ(WS2i) ·WS2i\n]\n(8)\nThis concatenation represents the output of the en-\ncoder M′and can be used for various purposes, such\nas providing the encoder output to a decoder in a se-\nquence to sequence model.\n4 Applying KIF to Dialog Tasks\nWe describe how to apply KIF to the task of generative\ndialog, a setting where models must generate engaging\nand on-topic responses. We investigate dialog for two\nreasons: ﬁrst, dialog agents must be able to consult rel-\nevant information to maintain the topic of the conversa-\ntion. Second, retrieval-based agents have strong perfor-\nmance compared to generative ones, due to their ability\nto copy dialog utterances from the training set. Using\nKIF, we can incorporate the beneﬁts of retrieval archi-\ntectures into generative, knowledge-based models.\n4.1 KIF for Generative Dialog\nIn dialog, xi represents the text of the conversation i.\nA conversation consists of multiple back-and-forth ut-\nterances (or turns). For example, a conversation could\nconsist of 4 turns:xi = [xi,1,xi,2,xi,3,xi,4] where xi,4\nis the direct utterance the model should respond to, and\nthe earlier utterances are the conversation context.\nStandard generative dialog models use a Trans-\nformer neural network as the encoder M and want to\nproduce an output that is an appropriate response to\nthe conversation. However, in many cases, the con-\nversation history alone does not include all of the in-\nformation required to produce an appropriate response.\nFor example, if a model needs to chat about a speciﬁc\nmovie, it can be helpful to provide the model with more\ninformation about that movie so a more interesting dia-\nlog response could be produced. To incorporate knowl-\nedge, models often concatenate a knowledge source\nE such as Wikipedia to xi and use attention mod-\nules to identify the most relevant knowledge. How-\never, this approach is computationally intensive when\nhandling large quantities of information. Further, at-\ntention mechanisms have been found to operate poorly\nover long sequences, as the mechanism becomes blurry\ndue to the softmax and struggles to make ﬁne-grained\ndecisions (Fan et al., 2018b). The same is true for hier-\narchical approaches, which lack scalability.\nWe augment Transformer sequence to sequence\n(seq2seq) networks on the encoder side with KIF to im-\nprove generative dialog models. We experiment on two\ndialog tasks, Wizard of Wikipedia (Dinan et al., 2018)\nand Engaging ImageChat (Shuster et al., 2020). In both\ndatasets, models must leverage information external to\nthe dialog history alone — in Wizard of Wikipedia, the\nchat requires access to knowledgeable facts and in En-\ngaging ImageChat, discussion about a speciﬁc image.\nAs models must process multiple inputs and ground\nresponses in the knowledgeable facts or images, these\ntasks challenge existing seq2seq approaches.\n4.2 Wizard of Wikipedia\nThe goal of the Wizard of Wikipedia dataset is to\ntrain knowledgeable agents that can chat in any do-\nmain. The dataset contains 1,365 various topics dis-\ncussed in 18,430 dialogs in the training set, totalling\n166,787 training utterances. Each topic is a general\nconcept, such as dogs or ice cream, and is included as\nthe ﬁrst utterance of the conversation. The conversa-\ntion is meant to be in-depth and detailed, so individual\nutterances must reference speciﬁc knowledge as a ba-\nsis for the utterance. The knowledge takes the form of\nWikipedia sentences. For example, the chat utterance\nI love Toy Story! It was released in 1995 would refer-\nence the Wikipedia sentence Toy Story is a 1995 Amer-\nican computer-animated buddy comedy [...] . For each\nutterance, a set of sentences are identiﬁed by an infor-\nmation retrieval system, and the crowdworker selected\none knowledge sentence as the basis for their utterance.\nKnowledge Sources. Our model for Wizard of\nWikipedia has access to two sources of external infor-\nmation, E1 and E2:\n• E1 is Wikipedia Knowledge provided by the\ndataset as evidence to support knowledgeable\nchitchat (initially curated by the information re-\ntrieval system used in Dinan et al. (2018)). The\nscale of this KNN search is to ﬁlter through an\naverage of 34 sentences. The KIF module uses di-\nalog features to fetch relevant knowledge to con-\ndition upon to generate the subsequent utterance.\n• E2 is Training Utterances . To incorporate the\nbeneﬁts of retrieval-based dialog models to the\ngenerative setting, we use KIF to identify rele-\nvant utterances from the training set and take their\nresponses as input. If many conversations about\ndogs have already occurred, models should be\nable to take advantage of these human-written ex-\namples to improve their generations. For example,\nlikely conversation could occur about the breed of\nthe dog, daily routine with a pet, and similar top-\nics. There are around 170K dialog utterances as\ninputs to KNN search. This can be interpreted as\nincorporating the beneﬁts of retrieval models by\nidentifying an utterance with similar structure as\nthe text the model would like to generate. We do\nnot allow the module to fetch the correct response\nof the current conversation context.\nAccess to these two sources of knowledge can be\nseen as learning a template and a topic separately. Sam-\nple templates can be identiﬁed from the training ut-\nterances, and topic-speciﬁc information learned by ac-\ncessing the Wikipedia knowledge.\nAdditional KNN Features. To better identify rele-\nvant training utterances from the large quantity avail-\nable, we break down xi into conversation sub-features\nfor a more ﬁne-grained match in the KNN search step.\nBy conducting KNN on more features, we can achieve\nhigher quality retrieval. We leverage the nature of dia-\nlog to decide these features.\nWe concatenate the encoding of the most recent dia-\nlog utterance (e.g. xi,last) with the encoding of the di-\nalog context from the current conversation and the turn\nnumber t, such that M′(xi,last),M′(xi,−last),t is the\nrepresentation used for KNN search. Concretely, if the\nmodel is trying to produce the 5th turn of the conversa-\ntion, then xi,last is the most recent utterance from the\ndialog partner, xi,−last would be the last 3 turns of ex-\nchange, and twould be 4. Note that the turn number is\nrepresented as a standalone number. These are known\nto be salient conversation features. The most recent di-\nalog utterance is the direct turn the model is responding\nto, and the dialog context may provide additional clues.\nThe turn number is important, as earlier turns are often\ngeneric (e.g. how are you doing today ) and later turns\nare more speciﬁc.\n4.3 Engaging ImageChat\nThe goal of Engaging ImageChat is to create agents\ncapable of chitchatting about images selected from\nthe YFFC100M dataset (Thomee et al., 2016). The\ndataset contains 186,782 dialogs in the training set,\neach about a unique image, totalling 355,862 utter-\nances. Agents are assigned one of 215 personalities\n(e.g. sweet, caring, excited ) to increase engagingness.\nPrevious work (Shuster et al., 2020, 2019) identiﬁed\nthat both crowdworkers and models, when provided\nwith personalities, produced more diverse, interesting\nresponses, as evaluated by humans.\nWe use a Multi-Modal neural network designed to\nhandle both image input and text input. Following\nShuster et al. (2020), the images are encoded using a\npre-trained ResNeXt network (Xie et al., 2017). To\nextract the ﬁnal image representation, we project the\n2048-dimensional output of the image encoder to 512-\ndimensions using a deep multi-layer perceptron with\nReLU activation units. The conversation history, which\nincludes the one-word personality, is encoded with a\nTransformer encoder network. The image and con-\nversation are integrated using the Multimodal-Sum-\nCombiner module proposed in Shuster et al. (2020).\nKnowledge Sources. Our model for Engaging Im-\nageChat has access to two sources of external informa-\ntion, E1 and E2:\n• E1 is Chat on Similar Images . While there are\nover 180K different images in this dataset, many\nof the images are similar. For example, conversa-\ntions associated with two pictures of dogs could\nbe relevant to each other. The model is able to\nuse KIF directly on the current image features to\nfetch from around 180K different images and re-\nturn 6 turns of related chat for each fetched image.\nFetching from E1 consists of identifying related\nimage chats, or conversations on related topics.\n• E2 is Training Utterances. Similar to the motiva-\ntion for the previous dataset, we allow the model\nto identify training utterances that could be useful\nfor responding in the current conversation. The\nscale of this fetching task is large: 350K dialog\nutterances. This could be interpreted as identify-\ning utterances with similar structure to what the\nmodel would like to generate, and is complemen-\ntary to the topic-based related image chats.\nModel Test F1 Test F1\n(Seen) (Unseen)\nRetrieval Baselines\nRetrieval Transformer MemNet (Dinan et al., 2018) 15.4 12.4\nGenerative Baselines\n2-Stage Generative MemNet (Dinan et al., 2018) 18.9 17.4\nGenerative Transformer MemNet (Dinan et al., 2018) 16.9 14.4\n+ Reddit Pre-Training 17.6 16.3\nRetrieve and Reﬁne (Weston et al., 2018) 18.2 17.9\nResponse Generation with MR (Qin et al., 2019) 17.5 16.8\nKIF-Augmented Transformer 25.9 22.3\nTable 1: Results on the Wizard of Wikipediadataset. We implement the Retrieve and Reﬁne and Response Gen-\neration with MR approaches, all with Reddit Pre-Training, and evaluate them on Wizard of Wikipedia. The Seen\ntest set consists of conversations on topics seen at training time, and the Unseen test set consists of conversations\nabout new topics that were not in the training set.\nAdditional KNN Features. To identify relevant in-\nformation from training utterances, we use the same\ndialog features as Wizard of Wikipedia in the KNN\nsearch step, with one modiﬁcation: we add the person-\nality provided by the dataset. We represent the per-\nsonality feature as the personality word, such as car-\ning, and embed it with the encoder M′. As utter-\nances from speakers with the same personality are more\nlikely to be similar, this feature improves the quality of\nthe fetched information. For example, conversations\nwith the sweet personality often include similar text\nsuch as aww, that’s wonderful. We use two additional\nfeatures for the KNN search: t, the turn number, and p,\nthe personality. This feature is explicitly used in Shus-\nter et al. (2020) to improve the engagingness and ﬂow\nof the conversation. Similar to Wizard of Wikipedia,\nwe represent the conversation turn tas a number. The\nTransformer model is used to encode text xi and pro-\nduce a representation of the text, then the turn number\ntand personality pare represented separately. As the\npersonality is a word, we use the same Transformer to\nencode it. The concatenation of features used for KNN\nsearch is: M′(xi,last),M′(xi,−last),t,p .\n5 Experimental Setup\n5.1 Implementation Details\nParameter Settings. We use parl.ai (Miller\net al., 2017) to implement our models. The data\nfor both datasets used is available for download from\nparl.ai as well. We use byte-pair encoding (Sen-\nnrich et al., 2016) to represent the text to better handle\nthe rare word problem (Dinan et al., 2018; Fan et al.,\n2018a). Our generative Transformer models have 8 en-\ncoder layers and 8 decoder layers, with FFN size 2048,\nembedding dimension 512, and 4 attention heads. We\noptimize using Adam (Kingma and Ba) and the inverse\nsquare root learning schedule (Vaswani et al., 2017)\nwith 10k warmup updates. The initial learning rate is\n0.0001 and we optimize for model perplexity. We use\na dropout of 0.5 and set gradient clipping to 0.1. We\nset k = 5 for all cases. For both datasets, we model a\nvocabulary size of 54944 based on the BPE-based vo-\ncabulary from the Reddit pre-training. We tuned the\nlearning rate and batchsize hyperparameters together.\nPre-training. We pre-train the Transformer seq2seq\nmodel used for both datasets on 250M comments from\nReddit. The Reddit dataset was made available by\npushshift.io. The comments are parsed to main-\ntain conversational threads of users responding to each\nother, so the encoder network has been exposed to con-\nversational context at training time. Note that the Red-\ndit dataset does not include aspects such as personality,\nas those are unique to speciﬁc datasets such as Engag-\ning ImageChat. The context size in pre-training is set\nto 512 tokens. The ResNeXt encoder used to model\nimages for the Engaging ImageChat dataset was pre-\ntrained on 3.5 billion images (Mahajan et al., 2018).\n5.2 Evaluation\nGeneration. We generate with beam search, setting\nthe beam size to 4. We use 3-gram blocking. This tech-\nnique disallows repeated n-grams from being generated\nmultiple times and reduces repetition.\nAutomatic Metrics. Following Dinan et al. (2018),\nwe compute F1, a metric of unigram overlap, between\nthe generated utterance and the human-written refer-\nence utterance from the dataset. For generative models,\nutterances are generated using beam search. For re-\ntrieval models, the next utterance is predicted by rank-\ning the entire set of training utterances, and the highest\nscoring utterance is chosen.\nIn Wizard of Wikipedia, there are two test sets: a set\nof seen topics, or topics that have been seen at train-\ning time with new test-time dialogs. The second set is\nunseen, or topics that have not been encountered at all\nduring training time. We evaluate on both subsets.\nModel Test F1\nRetrieval Baselines\nRetrieval Transformer (Shuster et al., 2020) 9.8 1\nGenerative Baselines\nGenerative Transformer MemNet (Dinan et al., 2018) 7.1\n+ Reddit Pre-Training 12.8\nRetrieve and Reﬁne(Weston et al., 2018) 13.6\nResponse Generation with MR (Qin et al., 2019) 13.2\nKIF-Augmented Transformer 14.4\nTable 2: Results on the Engaging ImageChatdataset. We implement the Generative Transformer Memory Net-\nwork, Retrieve and Reﬁne, and Response Generation with MR approaches, all with Reddit Pre-Training, and\nevaluate them on Engaging ImageChat.\nFigure 2: Human Evaluation Results on both Datasets. More than 50% indicates the KNN Model is preferred.\nStars indicate statistical signiﬁcance at p< 0.05.\nFigure 3: Human Evaluation on the Unseen Test set\nof Wizard of Wikipedia. More than 50% indicates the\nKNN Model is preferred. Stars indicate statistical sig-\nniﬁcance at p< 0.05.\nHuman Evaluation. We follow the setup and use the\nanalysis questions proposed in the Acute-Eval di-\nalog evaluation system (Li et al., 2019). For repro-\nducibility, we adopt this existing evaluation setting that\nhas been applied to several dialog datasets. We use the\nquestion wording suggested by Acute-Eval and fol-\nlow their self-chat procedure and interface. As one of\nthe original datasets assessed in this system was Wizard\nof Wikipedia, their evaluation setting extends naturally\nto ours. We collect 100 human-bot conversational di-\nalogs on a crowdsourcing platform for both datasets.\nThe dialogs are eight turns long. Then, we show pairs\nof the collected conversations side by side, one conver-\nsation with a human and model A and the other conver-\nsation with a human and model B. We ask annotators\nthe following questions:\n• Who would you prefer to talk to for a long conver-\nsation?\n• If you had to say one of the speakers is interest-\ning and one is boring, who would you say is more\ninteresting?\n• Which speaker sounds more human?\n• Which speaker has more coherent responses in the\nconversation?\n• If you had to say that one speaker is more knowl-\nedgeable and one is more ignorant, who is more\nknowledgeable? (Wizard of Wikipedia only)\nWe measure the percentage of time one model was\nchosen over the other, taking the majority agreement\nbetween three evaluators. To reduce variance, dialogs\npaired in the evaluation were collected on the same\ntopic for Wizard of Wikipedia and collected on the\nsame image and personalities for Engaging ImageChat.\nTopic and images selected for evaluation are unique\nand taken randomly from the test set.\n5.3 Baselines\nWe compare Transformers augmented with KIF to\nother existing approaches on Wizard of Wikipedia and\nEngaging ImageChat. The best approaches, judged by\nhuman evaluation, are retrieval models, the Retrieval\nTransformer Memory Network from (Dinan et al.,\n2018) and the Retrieval Transformer from Shuster\net al. (2020). These have been shown to be strong base-\nlines compared to other retrieval techniques based on\nTF-IDF (Chen et al., 2017). Thus, we report the ex-\nisting retrieval models for both datasets, but focus on\nHuman: I love watching disney movies! Do you like Disney?\nModel: I love disney, I love watching disney movies and different animations and characters that make them unique\nFetched Knowledge: disney consumer products and interactive media - the company specializes in pro-\nducing animated ﬁlms\nFetched Training Utterance:i love music, I love going to music shows and dancing and having fun\nHuman: What’s your favorite disney movie? There are so many good ones.\nModel: I love the incredibles, they are my favorite disney movie.\nFetched Knowledge:disney announced intentions to develop additional superhero ﬁlms after the success\nof the incredibles\nFetched Training Utterance:i love kiteboarding, it is one of my favorite activities on the water.\nHuman: omg, me too! the second one just came out i think. I don’t watch a lot of sequels though, I think the last\none I saw was toy story.\nModel: That’s cool! I remember when toy story was ﬁrst released in 1995.\nFetched Knowledge: talk of a second ﬁlm following toy story began around a month after the ﬁlm’s\nopening, in december 1995\nFetched Training Utterance:that’s neat! i like so many of her songs! i remember when her ﬁrst album\nwas released in 2008.\nHuman: I’ve actually never seen toy story before. Kind of embarrassing...\nModel: Ohhh ... I think toy story is a classic? It was ﬁrst released in 1999.\nFetched Knowledge: toy story 2 is a 1999 american computer-animated comedy ﬁlm produced by pixar\nanimation studios for walt disney pictures\nFetched Training Utterance:Ohhh ... What do people with color blindness do to cope with the effects?\nFigure 4: Conversation between Human and KIF-Augmented Transformer on Wizard of Wikipedia. The\ntop-scoring Wikipedia knowledge and training utterances fetched by KIF are displayed with model output.\nHuman: Hey, how are you doing\nFetched Training Utterances:I’m great, thanks for asking. Craving some chocolate. Do you like chocolate?\nHello, how is it going? I know some trivia about this movie\nHello, it’s lunch time here, and I’m in the mood for a great steak\nModel Response: Hello, I’m great, thanks for asking\nHuman: What are your hobbies?\nFetched Training Utterances:I work at an elementary school. I hope you ﬁnd a job you love too [...]\nI have a hound, we just got her. Although, I grew up with Labrador Retrievers.\nI just love ice cream. I love the types with fruits and ﬂavours. Do you like ice cream?\nModel Response: I work at a tennis court. I like to play tennis. Do you like your hobby?\nHuman: hi buddy, what do you think about cinematography?\nGold Chosen Knowledge:cinematographers use a lens to focus reﬂected light from objects into a real image [...]\nFetched Knowledge: cinematography is the art of motion-picture photography\ntypically, a lens is used to repeatedly focus the light reﬂected from objects [...]\nthe modern photographic camera evolved from the camera obscura\nModel Response: I love cinematography, it is my hobby. It is a kind of motion-picture photography.\nHuman: Speaking of blue skies, have you seen the 1946 movie staring bing crosby?\nGold Chosen Knowledge:blue skies is a 1946 american musical comedy ﬁlm [...] and starring bing crosby [...]\nFetched Knowledge: blue skies is a 1946 american musical comedy ﬁlm [...] and starring bing crosby [...]\nblue skies the band has since broken up\nblue skies was was composed in 1926 as a last - minute addition to betsy the musical\nModel Response: Oh cool, that’s an American comedy ﬁlm starring Bing Crosby. Do you like watching\nmovies?\nFigure 5: Examples of Top-3 Fetched Training Utterances and Fetched Knowledgewhen responding to a\nhuman chat from the dataset using a trained Wizard of Wikipedia model. Examples are taken from validation.\ncomparing to other generative baselines.\nWe compare to three additional generative baselines.\nNote that in Wizard of Wikipedia, the construction\nof the dataset is that sentences of Wikipedia knowl-\nedge are provided with the utterances in a concatenated\nform. Models must identify the relevant information in\nthis provided knowledge, or can access more Wikipedia\nknowledge beyond the provided sentences. The fol-\nlowing baseline methods always have access to the in-\nformation provided in the dataset already, but no addi-\ntional Wikipedia knowledge beyond that.\n• Transformer Memory Networks . To contrast the\nability of KIF to existing work, we compare our\nmodels to published Transformer Memory Net-\nworks (Dinan et al., 2018). These models encode\neach piece of external information independently\nwith a Transformer Encoder, and these are stored\nas memory slots. To access information in the\nmemory slots, a model performs dot-product at-\ntention between the memory slots and the dialog\ncontext. In Dinan et al. (2018), the knowledge se-\nlection from Wikipedia was supervised with either\n(a) a two-stage model where the ﬁrst model was\ntrained to predict the right knowledge and a sec-\nond model conditions on the predicted knowledge\nto generate the next utterance, or (b) an end-to-\nend model with an auxiliary loss for knowledge\nprediction accuracy.\n• Retrieve and Reﬁne . We implement a hybrid\nmodel (Weston et al., 2018) that incorporates top\nretrieval candidates as additional input to Genera-\ntive Transformer MemNets. In Retrieve and Re-\nﬁne, a ﬁxed number of candidates are retrieved\nand concatenated to the conversational history in\nthe encoder, making the input much longer. For\nboth datasets, the Retrieve and Reﬁne mechanism\nthat fetches a ﬁxed number of training utterances\nis added to the Generative Transformer MemNet\nwith Reddit Pre-Training baseline.\nUnlike the KIF-Augmented Transformer, the re-\ntrieval is conducted with a separate model so there\nis no backpropagation to affect the retrieval. With\nKIF, models can alter the retrieved candidates by\nlearning the mapping operator. Further, a ﬁxed\namount of information is always retrieved, with-\nout the capability to easily rescale to focus on\nspeciﬁc candidates. KIF modules have weight-\ning mechanisms to focus more on certain infor-\nmation, and the modules are combined with gat-\ning so models can learn which knowledge sources\nare more important and adjust ﬂexibly. Lastly,\nRetrieve and Reﬁne is only used to retrieve one\nsource of information: training set utterances.\n• Response Generation with MR. We implement the\nmodel proposed in Qin et al. (2019), which en-\ncodes the conversation history and document con-\ntextually with a biLSTM before generating the\nnext dialog utterance. The initial model was ap-\nplied to a machine reading task where a knowl-\nedge document was provided along with the con-\nversation history. For Wizard of Wikipedia, we re-\nplace the knowledge document with the Wikipedia\nsentences provided in the dataset. The model\nthen uses the conversation to identify the most\nrelevant information in the document using a\ncross-attention mechanism. For the Engaging Im-\nageChat dataset, as there is no document provided\nwith the dataset, we replace the expected docu-\nment with the conversation history, and use the\nmost recent utterance in the conversation to attend\nto the conversation history.\nWe make an additional improvement to this base-\nline: in Qin et al. (2019), the embeddings used\npre-trained CoVE vectors (McCann et al., 2017).\nWe found our Reddit pre-trained Transformer em-\nbeddings to work more effectively as they are\ntrained for dialog. Thus, we replace CoVE em-\nbeddings with domain-speciﬁc ones.\nAll of Transformer generative baselines are initial-\nized with the same pre-training on Reddit that we use\nfor our models for fair comparison on modeling quality.\n6 Results\nWe describe the results of incorporating KIF modules\ninto Transformer networks. We display an example\nconversation between a human and our model in Fig-\nure 4, and show the top scoring Wikipedia knowl-\nedge and Training Utterance fetched by KIF modules.\nWe compare to various baselines using automatic and\nhuman evaluation, and discuss our experiments. We\npresent various ablation settings to understand the key\nfeatures that make our method function.\n6.1 KIF is Effective for Incorporating Knowledge\nAutomatic Evaluation. Comparing KIF augmented\nTransformer networks to published baselines and Re-\ntrieve and Reﬁne, we ﬁnd improved results.\nFor Wizard of Wikipedia, the improvement in F1\nscore over the best baseline is around 8 points (see Ta-\nble 1). A major contributing factor is the construction\nof the dataset — as each dialog turn is grounded in a\nspeciﬁc knowledge sentence from Wikipedia, improv-\ning the ability to identify the relevant fact strongly im-\nproves performance. Contrasting the results from the\nseen and unseen test sets in Table 1, the improvement\non unseen is worse — it is harder to fetch training ut-\nterances for unseen topics.\nWhile Imagechat has no explicit dependency on\nknowledge, we still see a 2 point improvement com-\npared to the Generative Transformer MemNet (with the\nadditional Reddit pre-training), indicating that KIF can\nbe generally useful (see Table 2). Compared to an even\nstronger baseline that we tune in this work, Retrieve\nand Reﬁne, we see 1 point improvement.\nHuman Evaluation. Results are shown in Figure 2.\nOn both datasets, we ﬁnd there is large improve-\nment over existing generative models (green bars)\nthat is statistically signiﬁcant for some of the evalua-\ntion questions. Evaluators agree that KIF-augmented\nTransformers are generally more coherent and human-\nsounding compared to the Generative MemNet.\nCompared to existing retrieval models (blue) is more\nnuanced. Along the lines of existing work (Zhang et al.,\n2018; Dinan et al., 2018), we ﬁnd that retrieval-based\nmodels score very well in human evaluations that ask\nhow human or interesting a dialog sounds. This is be-\ncause retrieval models return human-written utterances\nfrom the training set and do not suffer from decoding\nmistakes present in generative models. For example,\non Engaging ImageChat, while our model has signiﬁ-\ncantly improved over the generative baseline (see green\nFigure 6: Ablations on Wizard of Wikipedia. (a) KIF can scale to hundreds of relevant sentences (blue) while\nthe baseline model, the Generative Transformer MemNet (gray), scales poorly (b) Gating can remove irrelevant\ninformation. In the 3 Sources case, one source of external information is unrelated. (c) Performance askvaries.\nbars in Figure 2, right), it does not beat retrieval based\nmethods in sounding more human or being more in-\nteresting (see blue bars in Figure 2, right). As the Re-\ntrieval baseline returns human-written text for other hu-\nmans to evaluate, we hypothesize that humans score\neach other’s writing quite well. Compared to gener-\native models, which we focus on improving, retrieval\nmodels often produce longer text with more interesting,\nnuanced vocabulary usage, and do not make generation\nmistakes such as repetition. These factors often lead to\nthe stronger performance of retrieval models.\nA surprising result is that KIF-augmented Trans-\nformers are more human sounding than retrieval mod-\nels on Wizard of Wikipedia. This is because the\ndataset’s utterances are long and factual due to the ten-\ndency of crowdworkers to copy Wikipedia. Sometimes\nhumans chatting with the retrieval bot would respond\nuh... that’s an interesting fact? Otherwise, our model\nscores similarly to retrieval models, with most evalua-\ntions not having statistically signiﬁcant difference.\nWe conduct a second evaluation on the Unseen Test\nSet of the Wizard of Wikipedia dataset. Results are\nshown in Figure 3. Trends are similar compared to the\nresults on the Seen Test set, though the preference for\nthe KIF-augmented Transformer is greater over the re-\ntrieval baseline. We hypothesize that because the Un-\nseen Test Set is on entirely held out topics, the retrieval\nbaseline can struggle to identify relevant utterances. In\ncontrast, the KIF-augmented Transformer, similar to\nthe generative baseline from Dinan et al. (2018), can\nuse the generative capability to produce utterances.\nLastly, we conduct an additional study to examine\nthe variance of the comparative dialog judgements. The\nevaluation study for Wizard of Wikipedia is repeated\nthree times on different days, and evaluators who have\nanswered on previous days are not allowed to evaluate\nagain in any subsequent experiments. Overall, we ﬁnd\nreasonable interannotator agreement rates, around 73%\naveraged across all evaluations, which is similar to the\nagreement rates reported in (Li et al., 2019). We ﬁnd\nthere is greater variance on questions asking which di-\nalog is more human and more interesting, most likely\nas different evaluators can interpret these in different\nways. Further, we see that comparison with the Re-\ntrieval model has less variance compared to the Gen-\nerative model, possibly because the Retrieval model’s\nhuman written text is devoid of mistakes. Overall, we\nﬁnd that the conclusions (and statistical signiﬁcance)\nare stable across multiple evaluations.\n6.2 Analysis of Fetched Knowledge\nExample conversations from our KIF-augmented gen-\nerative model are shown in Figure 4 on Wizard of\nWikipedia. We ﬁnd that relevant knowledge is iden-\ntiﬁed that affects the content of the generated utter-\nance. For example, the model ﬁnds knowledge sen-\ntences about Disney movies as the human conversa-\ntionalist starts the conversation discussing Disney. The\nmodel leverages the fetched knowledge to write the\ncontent of the generated utterance. In a concrete exam-\nple, the fetched sentence disney announced intentions\n[...] after the success of the incrediblesleads the model\nto generate the utterance i love the incredibles, they are\nmy favorite disney movie.\nIn contrast, the model uses the form of the fetched\ntraining utterance often as a template for writing a re-\nsponse. For example, the model copies the training ut-\nterance Ohhh ... what do people with color blindness\ndo to cope with the effects? and starts the model gen-\neration with Ohhh ... and continues with the question i\nthink toy story is a classic? following the form of the\nselected training utterance.\nFigure 5 displays the top-3 fetched training set ut-\nterances and knowledge sentences on the Wizard of\nWikipedia dataset when responding to a human ut-\nterance. KIF modules can identify multiple relevant\nitems. In response to the human question about blue\nskies the 1946 moviethe model identiﬁes both the com-\nedy ﬁlm and the band.\nFinally, the elements retrieved by KIF modules pro-\nvide a more interpretable understanding of what the\nmodel is conditioning upon to generate a dialog re-\nsponse. In Table 3, we display for the same dialog\nhistory, changing the model’s fetched training utter-\nance and knowledge sentence for our own examples.\nThe model heavily incorporates our manual changes\nof the fetched information into the generated utterance.\nFor example, changing the knowledge directly affects\nwhat the model generates as the favorite character —\nfrom buzz lightyear to mr potato head to slinky dog —\nKnowledge Training Utterance Generation\nbuzz lightyear’s name is in honor of\nastronaut edwin ‘buzz’ aldrin\nmy favorite character in that book\nseries is hermione granger\ncool! my favorite character in that\nmovie is buzz lightyear\nmr potato head is based on thereal-life\nmr. potato headtoy\nmy favorite character in that book\nseries is hermione granger\nmy favorite character in that movie is\nreal-life mr potato head\nslinky dogis a toy dachschund with a\nmetal slinky for a body\nmy favorite character in that book\nseries is hermione granger\ncool! my favorite character is the\nslinky dog\nslinky dog is a toy dachschund with a\nmetal slinky for a body\ni really like the character hermione\ngranger\ncool! i really likeslinky dog\nslinky dog is a toy dachschund with a\nmetal slinky for a body\nmy favorite characterof all time has\nto behermione granger\ni love that movie, my favorite charac-\nter has to beslinky dog the dachshund\nslinky dog is a toy dachschund with a\nmetal slinky for a body\ni agree with you! that’s my favorite\ncharacter as well\ni think so too! my favoriteis slinky\nTable 3: Effect of Fetched Information on Generated Utterances.The top section provides examples for a\nﬁxed training utterance, changing the knowledge — the generated text maintains the construction of the training\nutterance but changes the favorite character to match the knowledge. The bottom section provides examples for\nﬁxed knowledge but changing the training utterance — the generated text modiﬁes its form to match the training\nutterance, but the favorite character information remains consistent.\nwhile changing the fetched training utterance changes\nthe form of the generated sentence.\n6.3 Scaling KIF to Challenging Retrieval Settings\nKIF modules can be used in more realistic and chal-\nlenging settings for knowledge retrieval that test the\nscalability of the module. In Figure 6(a), we compare\nthe Generative Transformer MemNet Baseline with\nKIF-Augmented Transformers in three settings. The\nﬁrst is the standard Wikipedia sentences provided by\nthe dataset (average 34 sentences). Then, we extend\nto providing the model with the full Wikipedia arti-\ncle (on average, 57 sentences) and ﬁnally to multiple\nWikipedia articles (on average, totaling 205 sentences),\nidentiﬁed using the conversation’s topic. This increas-\ning size of available knowledge could be realistic for\nsettings where it is unclear what information is most\nrelevant, if ﬁltering steps to preprocess the data remove\npotentially relevant information, or if information syn-\nthesis from multiple knowledge sources is necessary to\nproduce a high quality generation. As the Wikipedia\nknowledge becomes more difﬁcult to identify, perfor-\nmance decreases, but still outperforms the baseline that\nuses the dataset-provided set of 34 sentences.\nComparing the scaling capability of KIF to the stan-\ndard Generative Transformer MemNet Baseline high-\nlights the advantage of using KNN. The attention-based\nmechanism used in Dinan et al. (2018) struggles to\nidentify salient information when given increasingly\nlarger quantities of knowledge, unlike the KNN infor-\nmation fetch. We hypothesize the attention mechanism\nis challenged by softmax-ing over a larger quantity of\ninputs, as it can be difﬁcult to make sharp distinctions.\n6.4 Ablations\nImportance of Multiple Knowledge Sources. One\nbeneﬁt of the KIF module approach is that several\nmodules can be combined, each capturing informa-\ntion from a different source. In both settings, Wizard\nof Wikipedia and Engaging ImageChat, two modules\nwere used to incorporate multiple forms of knowledge\n— training utterances to capture the capability of a\nretrieval-based model and knowledge from Wikipedia\nor related chats based on image features. We perform\nhere an ablation study to evaluate the impact of using\nonly one source of information. As can be seen in Ta-\nble 4, performance decreases when only one source of\ninformation is used (see Table 4).\nFor Engaging ImageChat, this study also underlines\nthe importance of being able to fetch in a multi-modal\nfashion. The general form of the KIF module — re-\nquiring only a feature vector to ﬁnd nearest neighbors\nfrom — allows fetching on multiple modalities such\nas text and images. In Table 4, using the Image-based\nKIF to fetch text from Related Images is important to\nreach the strongest performance (compare Training Ut-\nterances Only that uses text-based KIF and using both\nTraining Utterances and Related Images).\nUsing dialog Features for KNN Performance.The\nquality of the KNN search is critical to the perfor-\nmance of KIF modules. As the external knowledge is\nkept ﬁxed, KIF must be able to align the dialog con-\n1In Shuster et al. (2020), retrieval Transformer models report\nHits@N using a ﬁxed candidate set of 99 distractor candidates and\n1 true candidate. We compute F1 using their open-sourced model by\nscoring the entire training set of over350K utterances with the model\nand taking the top scoring candidate as the response.\nModel Test F1\nWizard of Wikipedia\nTraining Utterances Only 18.1\nWiki Knowledge Only 23.9\nTraining Utterances and Wiki Knowledge 25.9\nEngaging ImageChat\nTraining Utterances Only 13.9\nRelated Images Only 13.8\nTraining Utterances and Related Images 14.4\nTable 4: Using Multiple KIF Modules on Multiple\nSources is important for improved performance.\nModel Valid F1\nWizard of Wikipedia\nPrevious Utterance Only 24.6\n+ dialog Context 26.4\n+ Turn Embedding 27.4\nEngaging ImageChat\nPrevious Utterance Only 13.3\n+ dialog Context 14.5\n+ Turn Embedding + Personality 15.1\nTable 5: Important Features for KNN Searchus-\ning KIF. Salient conversation features improve perfor-\nmance on both datasets.\ntext with the knowledge to identify relevant pieces of\ninformation. In Table 5, we show that matching on\nmore features can improve the quality of the retrieved\ninformation. Using only the encoding of the immedi-\nate previous utterance can improve results on Wizard of\nWikipedia by 7 F1 points, but this is further improved\nby also leveraging the encoding of context (+1.8 F1)\nand using the dialog turn number (+1 F1). These fea-\ntures are available in the datasets, and we leverage them\nto improve the relatedness of retrieved knowledge.\nMulti-Hop Retrieval with KIF. Work in memory\nnetworks (Weston et al., 2015; Sukhbaatar et al., 2015)\nemployed multi-hop mechanisms. Such capacity could\nModel Valid F1\nKIF-Augmented Transformer 27.4\nOne KIF Module fetches multiple times\n2 Fetches 26.9\n3 Fetches 26.0\nMultiple KIF Modules fetch once each\n2 Fetches 26.5\n3 Fetches 25.9\nTable 6: Multi-hop with KIFto retrieve information\nwith multiple fetch steps.\nbe useful when multiple sources are necessary or infor-\nmation is incrementally fetched. To emulate multi-hop\nmemory mechanisms, we use KIF to retrieve relevant\ninformation for N = 2 or N = 3 ﬁxed hops. As\nthe number of hops is ﬁxed, the multi-hop operation\nremains differentiable. We do not allow the model to\nretrieve the same information in a second hop.\nWe experimented in two settings. First, the same\nKIF module is used multiple times to fetch different\ninformation, and then all of the fetched knowledge is\nconcatenated. Results are shown in Table 6 (top). Sec-\nond, we examine spreading the fetches into different\nKIF modules at various encoder depths. This could be\ninterpreted as the model learning to access more infor-\nmation each layer. As the model progresses deeper,\nmore abstract and high level representations are built,\nwhich could allow different knowledge to be retrieved.\nResults are shown in Table 6 (bottom).\nIn both multi-hop settings, no improvement in per-\nformance on the Wizard of Wikipedia dataset is ob-\nserved. We hypothesize this can be partially attributed\nto the construction of the dataset — as humans explic-\nitly based their written dialog utterance on one knowl-\nedge sentence. Further, it is possible that concatenation\nbrings together too much information for the model to\nincorporate, and thus adding additional fetches makes\nthe retrieval more noisy.\nEffect of Gating. We analyze the effect of the gating\nmechanism by evaluating the capability of the gate to\nidentify and focus on salient information. On Wizard of\nWikipedia, we concatenate a third source of informa-\ntion: dialog turns from a completely different corpus\ncalled PersonaChat (Zhang et al., 2018). This dataset\nlooks quite different — short utterances without factual\nknowledge — and should be easy for the model to iden-\ntify as distinct from Wizard of Wikipedia. As shown in\nFigure 6(b), if KIF on PersonaChat is included without\ngating, it has a harmful effect as the model includes\nirrelevant information. When equipped with gating,\nthe model learns to use the gate to ignore some inputs,\nand can recover almost the full performance of a model\nwithout this irrelevant information source.\nSize of K in KNN. Figure 6(c) shows the perfor-\nmance on Wizard of Wikipedia when varying the\namount of knowledge. Being able to access multiple\nrelevant pieces of information is helpful, but too much\ninformation can be harmful. This is likely because the\nweighted sum becomes blurry if too many sentences\nare incorporated.\n7 Conclusion\nWe present a KNN-based Information Fetching mod-\nule that learns to identify relevant information from ex-\nternal knowledge sources by learning a mapping-based\nread operation. KIF modules beneﬁt from the scalabil-\nity and efﬁciency of K Nearest Neighbors search, en-\nabling computation with large external memories. We\nshow in the context of two dialog datasets that relevant\nknowledge can be identiﬁed and incorporated to create\nmore engaging, high quality dialog.\nAcknowledgements\nWe thank the reviewers and action editor for their com-\nments and insightful discussion. We thank Emily Di-\nnan and Kurt Shuster for providing assistance to repro-\nduce their original works.\nReferences\nAntoine Bordes, Y-Lan Boureau, and Jason Weston.\n2017. Learning End-to-End Goal-Oriented Dialog.\nIn 5th International Conference on Learning Repre-\nsentations, ICLR 2017, Toulon, France, April 24-26,\n2017, Conference Track Proceedings.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-\njiang Liu, and Shuming Shi. 2019. Retrieval-guided\nDialogue Response Generation via a Matching-to-\nGeneration Framework. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1866–1875.\nSarath Chandar, Sungjin Ahn, Hugo Larochelle, Pas-\ncal Vincent, Gerald Tesauro, and Yoshua Bengio.\n2016. Hierarchical Memory Networks. CoRR,\nabs/1605.07427.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to Answer Open-\nDomain Questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1870–\n1879.\nWenlin Chen, David Grangier, and Michael Auli. 2016.\nStrategies for Training Large V ocabulary Neural\nLanguage Models. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1975–\n1985.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof Wikipedia: Knowledge-Powered Conversational\nAgents. In International Conference on Learning\nRepresentations.\nAngela Fan, Claire Gardent, Chlo ´e Braud, and An-\ntoine Bordes. 2019. Using Local Knowledge Graph\nConstruction to Scale Seq2Seq Models to Multi-\nDocument Inputs. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 4177–4187.\nAngela Fan, David Grangier, and Michael Auli. 2018a.\nControllable Abstractive Summarization. In Pro-\nceedings of the 2nd Workshop on Neural Machine\nTranslation and Generation, pages 45–54.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018b.\nHierarchical Neural Story Generation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 889–898.\nEdouard Grave, Armand Joulin, Moustapha Ciss ´e,\nDavid Grangier, and Herv ´e J´egou. 2017a. Efﬁcient\nSoftmax Approximation for GPUs. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1302–1310.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017b. Improving Neural Language Models with a\nContinuous Cache. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\n2014. Neural Turing Machines. arXiv preprint\narXiv:1410.5401.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. Retrieval Aug-\nmented Language Model Pre-Training. In Proceed-\nings of the International Conference on Machine\nLearning, pages 5695–5704.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2019. Poly-encoders: Architec-\ntures and Pre-training Strategies for Fast and Accu-\nrate Multi-sentence Scoring. In International Con-\nference on Learning Representations.\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data.\nArmand Joulin and Tomas Mikolov. 2015. Inferring\nAlgorithmic Patterns with Stack-Augmented Recur-\nrent Nets. In Advances in Neural Information Pro-\ncessing Systems, pages 190–198.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In International Conference on Learning\nRepresentations.\nDiederik P. Kingma and Jimmy Ba. Adam: A\nMethod for Stochastic Optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nGuillaume Lample, Alexandre Sablayrolles,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and\nHerv´e J ´egou. 2019. Large Memory Layers with\nProduct Keys. In Advances in Neural Information\nProcessing Systems, pages 8548–8559.\nMargaret Li, Jason Weston, and Stephen Roller. 2019.\nACUTE-EV AL: Improved Dialogue Evaluation with\nOptimized Questions and Multi-turn Comparisons.\narXiv preprint arXiv:1909.03087.\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\nand Hua Wu. 2019. Learning to Select Knowledge\nfor Response Generation in Dialog Systems. In Pro-\nceedings of the 28th International Joint Conference\non Artiﬁcial Intelligence , pages 5081–5087. AAAI\nPress.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan,\nKaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. 2018. Ex-\nploring the limits of weakly supervised pretraining.\nIn Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 181–196.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in Translation: Con-\ntextualized Word Vectors. InAdvances in Neural In-\nformation Processing Systems, pages 6294–6305.\nAlexander Miller, Will Feng, Dhruv Batra, Antoine\nBordes, Adam Fisch, Jiasen Lu, Devi Parikh, and Ja-\nson Weston. 2017. parl.ai: A Dialog Research Soft-\nware Platform. pages 79–84.\nAndriy Mnih and Geoffrey Hinton. 2009. A Scalable\nHierarchical Distributed Language Model. In Ad-\nvances in Neural Information Processing Systems ,\npages 1081–1088.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language Models as\nKnowledge Bases? In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473.\nTobias Pl ¨otz and Stefan Roth. 2018. Neural Nearest\nNeighbors Networks. In Advances in Neural Infor-\nmation Processing Systems, pages 1087–1098.\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, Bill Dolan, Yejin Choi, and Jian-\nfeng Gao. 2019. Conversing by Reading: Content-\nful Neural Conversation with On-demand Machine\nReading. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5427–5436.\nJack Rae, Jonathan J. Hunt, Ivo Danihelka, Timo-\nthy Harley, Andrew W Senior, Gregory Wayne,\nAlex Graves, and Timothy Lillicrap. 2016. Scaling\nMemory-Augmented Neural Networks with Sparse\nReads and Writes. In Advances in Neural Informa-\ntion Processing Systems, pages 3621–3629.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.\nReal-Time Open-Domain Question Answering with\nDense-Sparse Phrase Index. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4430–4441.\nIulian V . Serban, Ryan Lowe, Laurent Charlin, and\nJoelle Pineau. 2016a. Generative deep neural net-\nworks for dialogue: A short review. arXiv preprint\narXiv:1611.06216.\nIulian V . Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016b. Build-\ning End-to-End Dialogue Systems using Generative\nHierarchical Neural Network Models. In Thirtieth\nAAAI Conference on Artiﬁcial Intelligence.\nKurt Shuster, Samuel Humeau, Antoine Bordes, and\nJason Weston. 2020. Image-chat: Engaging\nGrounded Conversations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2414–2429.\nKurt Shuster, Samuel Humeau, Hexiang Hu, Antoine\nBordes, and Jason Weston. 2019. Engaging Image\ncaptioning via Personality. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 12516–12526.\nHaoyu Song, Yan Wang, Wei-Nan Zhang, Xiaojiang\nLiu, and Ting Liu. 2020. Generate, Delete and\nRewrite: A Three-Stage Framework for Improving\nPersona Consistency of Dialogue Generation. arXiv\npreprint arXiv:2004.07672.\nYiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and\nMing Zhang. 2016. Two are better than one: An en-\nsemble of retrieval-and generation-based dialog sys-\ntems. arXiv preprint arXiv:1610.07149.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Herve Jegou, and Armand Joulin. 2019.\nAugmenting Self-attention with Persistent Memory.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\n2015. End-to-End Memory Networks. In Ad-\nvances in neural information processing systems ,\npages 2440–2448.\nBart Thomee, David A Shamma, Gerald Fried-\nland, Benjamin Elizalde, Karl Ni, Douglas Poland,\nDamian Borth, and Li-Jia Li. 2016. YFCC100M:\nThe new data in multimedia research. Communica-\ntions of the ACM, 59(2):64–73.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nJason Weston, Sumit Chopra, and Antoine Bordes.\n2015. Memory Networks. In 3rd International Con-\nference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and Reﬁne: Improved Sequence\nGeneration Models For Dialogue. In Proceedings of\nthe 2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nSaining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu,\nand Kaiming He. 2017. Aggregated Residual Trans-\nformations for Deep Neural Networks. In Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, pages 1492–1500.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing Dialogue Agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2204–2213.\nYutao Zhu, Zhicheng Dou, Jian-Yun Nie, and Ji-Rong\nWen. 2020. ReBoost: a retrieval-boosted sequence-\nto-sequence model for neural response generation.\nInformation Retrieval Journal, 23(1):27–48.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8576854467391968
    },
    {
      "name": "Dialog box",
      "score": 0.827180027961731
    },
    {
      "name": "Transformer",
      "score": 0.7731006145477295
    },
    {
      "name": "Generative grammar",
      "score": 0.6454892158508301
    },
    {
      "name": "Conversation",
      "score": 0.6195414066314697
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5451220870018005
    },
    {
      "name": "Artificial neural network",
      "score": 0.45704299211502075
    },
    {
      "name": "Information flow",
      "score": 0.43721094727516174
    },
    {
      "name": "Modalities",
      "score": 0.4158194065093994
    },
    {
      "name": "Machine learning",
      "score": 0.41357800364494324
    },
    {
      "name": "Natural language processing",
      "score": 0.3811032772064209
    },
    {
      "name": "World Wide Web",
      "score": 0.09188258647918701
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I4210121838",
      "name": "Laboratoire Lorrain de Recherche en Informatique et ses Applications",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210119061",
      "name": "Institut de Recherche en Informatique de Toulouse",
      "country": "FR"
    }
  ],
  "cited_by": 18
}