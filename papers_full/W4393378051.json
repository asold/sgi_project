{
  "title": "Traffic Transformer: Transformer-based framework for temporal traffic accident prediction",
  "url": "https://openalex.org/W4393378051",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4373347279",
      "name": "Mansoor G. Al-Thani",
      "affiliations": [
        "Hamad bin Khalifa University"
      ]
    },
    {
      "id": "https://openalex.org/A3096449062",
      "name": "Ziyu Sheng",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2124193103",
      "name": "Yuting Cao",
      "affiliations": [
        "Hamad bin Khalifa University"
      ]
    },
    {
      "id": "https://openalex.org/A2097531389",
      "name": "Yin Yang",
      "affiliations": [
        "Hamad bin Khalifa University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2415438719",
    "https://openalex.org/W2025518482",
    "https://openalex.org/W2951569056",
    "https://openalex.org/W2907465014",
    "https://openalex.org/W2183877882",
    "https://openalex.org/W3022643593",
    "https://openalex.org/W1577927906",
    "https://openalex.org/W2111759790",
    "https://openalex.org/W2562005416",
    "https://openalex.org/W3095867322",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2921467240",
    "https://openalex.org/W2965092899",
    "https://openalex.org/W3009823791",
    "https://openalex.org/W2937612154",
    "https://openalex.org/W4307543476",
    "https://openalex.org/W4319595903",
    "https://openalex.org/W3093308276",
    "https://openalex.org/W4318589291",
    "https://openalex.org/W3123909522",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2169818249",
    "https://openalex.org/W2794284562",
    "https://openalex.org/W4226206033",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4221044734",
    "https://openalex.org/W4379209990",
    "https://openalex.org/W4361981113",
    "https://openalex.org/W3168997536",
    "https://openalex.org/W2944851425",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2792643794",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3132782787"
  ],
  "abstract": "&lt;abstract&gt;&lt;p&gt;Reliable prediction of traffic accidents is crucial for the identification of potential hazards in advance, formulation of effective preventative measures, and reduction of accident incidence. Existing neural network-based models generally suffer from a limited field of perception and poor long-term dependency capturing abilities, which severely restrict their performance. To address the inherent shortcomings of current traffic prediction models, we propose the Traffic Transformer for multidimensional, multi-step traffic accident prediction. Initially, raw datasets chronicling sporadic traffic accidents are transformed into multivariate, regularly sampled sequences that are amenable to sequential modeling through a temporal discretization process. Subsequently, Traffic Transformer captures and learns the hidden relationships between any elements of the input sequence, constructing accurate prediction for multiple forthcoming intervals of traffic accidents. Our proposed Traffic Transformer employs the sophisticated multi-head attention mechanism in lieu of the widely used recurrent architecture. This significant shift enhances the model's ability to capture long-range dependencies within time series data. Moreover, it facilitates a more flexible and comprehensive learning of diverse hidden patterns within the sequences. It also offers the versatility of convenient extension and transference to other diverse time series forecasting tasks, demonstrating robust potential for further development in this field. Extensive comparative experiments conducted on a real-world dataset from Qatar demonstrate that our proposed Traffic Transformer model significantly outperforms existing mainstream time series forecasting models across all evaluation metrics and forecast horizons. Notably, its Mean Absolute Percentage Error reaches a minimal value of only 4.43%, which is substantially lower than the error rates observed in other models. This remarkable performance underscores the Traffic Transformer's state-of-the-art level of in predictive accuracy.&lt;/p&gt;&lt;/abstract&gt;",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6093949675559998
    },
    {
      "name": "Computer science",
      "score": 0.553667426109314
    },
    {
      "name": "Distribution transformer",
      "score": 0.45032256841659546
    },
    {
      "name": "Real-time computing",
      "score": 0.3686596155166626
    },
    {
      "name": "Engineering",
      "score": 0.26507115364074707
    },
    {
      "name": "Electrical engineering",
      "score": 0.19852370023727417
    },
    {
      "name": "Voltage",
      "score": 0.05596628785133362
    }
  ]
}