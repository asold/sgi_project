{
  "title": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
  "url": "https://openalex.org/W4385570246",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2137476013",
      "name": "Xinyu Zhu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2097441002",
      "name": "Junjie Wang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A153488826",
      "name": "Lin Zhang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2101108701",
      "name": "Yuxiang Zhang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2117581150",
      "name": "Yongfeng Huang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2343603905",
      "name": "Ruyi Gan",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2124456965",
      "name": "jiaxing Zhang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2309990480",
      "name": "Yujiu Yang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251935656",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3212493691",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W1625390266",
    "https://openalex.org/W4281975731",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2075780421",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2572127022",
    "https://openalex.org/W1539746312",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2276364082",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4283646806",
    "https://openalex.org/W4287084089",
    "https://openalex.org/W3174192410",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W4311398184",
    "https://openalex.org/W2962970011"
  ],
  "abstract": "Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4471‚Äì4485\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nSolving Math Word Problems via Cooperative Reasoning induced\nLanguage Models\nXinyu Zhu ‚ô¢‚àó Junjie Wang ‚ô†‚àó Lin Zhang ‚ô° Yuxiang Zhang ‚ô†\nYongfeng Huang ‚ô£ Ruyi Gan ‚ô° Jiaxing Zhang ‚ô° Yujiu Yang‚ô¢‚Ä†\n‚ô¢Tsinghua University ‚ô†Waseda University\n‚ô°IDEA ‚ô£The Chinese University of Hong Kong\nzhuxy21@mails.tsinghua.edu.cn yang.yujiu@sz.tsinghua.edu.cn\nwjj1020181822@toki.waseda.jp joel0495@asagi.waseda.jp\n{zhanglin, ganruyi, zhangjiaxing}@idea.edu.cn\nAbstract\nLarge-scale pre-trained language models\n(PLMs) bring new opportunities to challeng-\ning problems, especially those that need\nhigh-level intelligence, such as the math\nword problem (MWPs). However, directly\napplying existing PLMs to MWPs can fail\nas the generation process lacks sufÔ¨Åcient\nsupervision and thus lacks fast adaptivity as\nhumans. We notice that human reasoning\nhas a dual reasoning framework that consists\nof an immediate reaction system (system 1)\nand a delicate reasoning system (system 2),\nwhere the entire reasoning is determined by\ntheir interaction. This inspires us to develop\na cooperative reasoning-induced PLM for\nsolving MWPs, called Cooperative Reasoning\n(CoRe), resulting in a human-like reasoning\narchitecture with system 1 as the generator\nand system 2 as the veriÔ¨Åer. In our approach,\nthe generator is responsible for generating\nreasoning paths, and the veriÔ¨Åers are used to\nsupervise the evaluation in order to obtain re-\nliable feedback for the generator. We evaluate\nour CoRe framework on several mathematical\nreasoning datasets and achieve decent im-\nprovement over state-of-the-art methods, up\nto 9.6% increase over best baselines. 1\n1 Introduction\nAddressing math problems is a hallmark of human\nintelligence, which allows reasoning and adapting\nfrom limited data. We want neural models to be\nable to do the same, however, quick and Ô¨Çexible\nreasoning is challenging to current neural models\nas they must possess a certain level of prior expe-\nrience from a limited amount of new data while\navoiding overÔ¨Åtting. The rapid growth of large-\nscale Pre-trained Language Models (PLMs) offers\n*Equal contribution.\n‚Ä†Corresponding Author.\n1Our codes are available at https://github.com/\nTianHongZXY/CoRe\n(a) Prompt-based Methods (zero-shot/few-shot)\nPrompt Large-scale \nPLMs\nReasoning Path\nReasoning Path\n‚Ä¶ Answer\n(b) Dual Process Theory (System 1&2) (few-shot/finetuning)\nChain-of-Thought: 1 path;\nSelf-consistency: multiple paths.\nLarge-scale \nPLMsPrompt\nIf few-shot: Reasoning Path\nReasoning Path\n‚Ä¶ Rules /\nVerifiers Answer\n(c) CoRe (ours) (zero-shot)\nGenerators\nVerifiers\nData\nCooperative Training\nCooperative Inference\nSelf-Thinking\nUnseen \ndatasets\nFigure 1: Comparing our CoRe with popular methods\nin mathematical logic reasoning tasks.\nunprecedented potential for this issue, often rely-\ning on well-designed trigger prompts ( Wei et al. ,\n2022c; Li et al. , 2022; Brown et al. , 2020). Al-\nthough appealing in terms of efÔ¨Åciency, its success\nrelies on memorizing patterns with a sufÔ¨Åciently\nlarge number of parameters ( ‚â• 100 billion) ( Wei\net al. , 2022b), differentiating it from the fast adap-\ntivity in the human reasoning process.\nActive disciplines like neuroscience and cogni-\ntive science attempt to uncover the mechanism of\nhuman reasoning, and agree that our learning pro-\ncess is governed by an interaction mechanism, of-\nten referred to as System 1 and System 2 ( Evans,\n2003; Kahneman, 2011). In particular, System 1\noffers fast responses like human instinct, and Sys-\ntem 2 performs deliberate reasoning. Interactions\nbetween them are important for adapting to a con-\ntinuously changing environment. PLMs behave\nmore like System 1, according to the above theory,\nand thus lack the generalization ability in reason-\ning ( Nye et al. , 2021).\n4471\nIn this work, we explore a new line of zero-shot\nmath problem reasoning, using a human reasoning-\nalike framework with feedback in the solution gen-\neration loop as opposed to pure PLM-based meth-\nods, called Cooperative Reasoning ( CoRe). In-\ntuitively, System 1 and System 2 are embodied\nas generators and veriÔ¨Åers, respectively, and they\nare deÔ¨Åned as follows: generators for generating\nreasoning paths, and veriÔ¨Åers for supervising the\npaths‚Äô evaluation. SpeciÔ¨Åcally, we train a LM be-\nyond the question-answer paradigm by integrat-\ning in-the-loop reasoning, i.e., we let the LM out-\nput both the answer and the corresponding reason-\ning process for a given question. Meanwhile, we\nintroduce two types of veriÔ¨Åers, including token-\nlevel and sentence-level, allowing us to provide\nfeedback in the whole solution generation life-\ncycle. Notice that the solution path is generated\nby selecting candidate tokens with some proba-\nbility so that it is tree-alike and much coincides\nwith the tree search process of Monte Carlo Tree\nSearch (MCTS) ( Kocsis and Szepesv√°ri , 2006).\nWith this in mind, the veriÔ¨Åers can score tokens\nalong the solution generation process from start to\nend when using the MCTS. Therefore, we can use\nthe score to evaluate the quality of the generation\nprocess during inferring before Ô¨Ånalizing the solu-\ntion, making timely feedback available for super-\nvising the generation process. With this, the evalu-\nation goes beyond the quality of the Ô¨Ånal result at\nthe granularity of each reasoning step, extending\nthe supervision from the solution level to the path\nlevel. We combine the solution score and the per-\nplexity of its corresponding reasoning path to en-\ncourage the overall training towards high-quality\naugmented solutions while aligning with the reli-\nable reasoning process, aiming to improve gener-\nalization ability.\nOur experimentally evaluate CoRe on multi-\nple mathematical reasoning datasets in both zero-\nshot and Ô¨Åne-tuning settings. CoRe consistently\nachieves better performance than competing base-\nlines. Notably, CoRe has up to 9.6% improve-\nments on MultiArith over SoTA baselines, which\nare dozens of times larger than our model.\nIn summary, our contributions are as follows.\n‚Ä¢ We propose a novel reasoning method\nfor mathematical problem solving, called\nCooperative Reasoning ( CoRe), that intro-\nduces feedback in the loop during solution\ngeneration as opposed to the sequential learn-\ning process in the previous ones, resulting in\nthe Ô¨Årst method for this task that builds on\ntop of the learning mechanism in the human\nbrain.\n‚Ä¢ We develop a self-thinking strategy for fur-\nther boosting reasoning ability with gener-\nated data from the cooperation between Sys-\ntem 1 and System 2.\n‚Ä¢ We demonstrate the superiority of CoRe com-\nparing to other zero-shot and Ô¨Åne-tuning\nmethods, which has 9.6% improvements on\nMultiArith over SoTA baselines.\n2 Related Work\n2.1 Dual Process System\nDual-process theory ( Evans, 2003; Kahneman,\n2011) argues there are two cognitive systems un-\nderpinning human reasoning: System 1 and Sys-\ntem 2. The purpose of clarifying these systems is\nthat they have the potential to help us construct\nartiÔ¨Åcial intelligence systems that beneÔ¨Åt from hu-\nman Ô¨Çexibility and methodical generalization.\nDual process system model guidance is not new.\nNye et al. (2021) simulated Systems 1 and 2 to\nimprove consistency and coherence of neural net-\nworks. Similar to several studies Cobbe et al.\n(2021); Li et al. (2022); Scialom et al. (2021), in\naddition to System 1 for the generation, we de-\nvelop a distinct model as System 2, called Veri-\nÔ¨Åer. The VeriÔ¨Åer checks the feasibility and correct-\nness of the generator‚Äôs content and collaboratively\nsolves the reasoning task together.\n2.2 Multi-step Reasoning\nMany works exploit the multi-step reasoning abil-\nity of language models. Cobbe et al. (2021)\nshowed that training a veriÔ¨Åer to score the solu-\ntions generated by a Ô¨Åne-tuned GPT-3 could im-\nprove the performance compared to solely Ô¨Åne-\ntuning a GPT-3. Nye et al. (2022) discovered that\nasking the language model to write the intermedi-\nate process could achieve better results on various\nNLP tasks. Likewise, Chain-of-Thought (CoT)\nprompts ( Wei et al. , 2022c) prepended exemplars\nwith intermediate reasoning steps as prompts and\nachieved SoTA on several reasoning benchmarks\nby using large-scale PLMs. Wang et al. (2022) fur-\nther boosted CoT‚Äôs performance by sampling a\nbunch of possible solutions and then obtained the\nÔ¨Ånal answer by majority voting. DIVERSE ( Li\net al. , 2022) proved diverse CoT prompts and an\n4472\nextra veriÔ¨Åer were both helpful for PLMs to solve\nreasoning problems. Kojima et al. (2022) found\nthat by simply adding ‚ÄúLet‚Äôs think step by step‚Äù\nafter the question. PLMs could successfully step\nby step solve the problems, called Zero-shot-CoT.\nThese above methods rely on extremely large\nlanguage models, resulting in high computa-\ntional cost and time-consuming. Moreover, sev-\neral works ( Wei et al. , 2022c; Kojima et al. , 2022)\npoint out that neither CoT nor Zero-shot-CoT is\nhelpful to smaller models. While our method does\nnot necessarily require extremely large PLMs and\ncan work with models with different size scales,\nthus reducing computational cost and inference\ntime. Our approach has competitive zero-shot per-\nformance thanks to the efÔ¨Åcient and collaborative\napplication of a dual-process system.\n3 Cooperative Reasoning\nIn this section, we will present the proposed co-\noperative reasoning framework, CoRe, that en-\nforces System 1 and System 2 mutually cooperat-\ning, which includes 3 sequential steps: cooperative\ntraining, cooperative inference, and self-thinking.\n3.1 Preparation\nAs discussed in Sec. 1, we expect a PLM ( G) to\nfast generate multiple reasoning paths like System\n1. Then, considering that System 2 is responsible\nfor deliberate evaluations of the reasoning paths,\nwe employ two modules: a step veriÔ¨Åer ( Vstep) for\nreasoning steps, and a path veriÔ¨Åer ( Vpath) for rea-\nsoning paths.\n3.2 Cooperative Training\nBefore applying System 1&2 to inference, a crit-\nical issue for them is learn how to generate rea-\nsoning paths and evaluate reasoning steps/paths .\nInspired by a widely-used training strategy for rea-\nsoners ( Cobbe et al. , 2021), we present a cooper-\native training method as shown in Fig. 2 Step 1.\nMoreover, we discuss hyper-parameter conÔ¨Ågura-\ntions and extra training details in Appendix B.1\nand Appendix B.2.\nStep 1.1: We Ô¨Årst Ô¨Åne-tune G on a dataset D =\n{(qi, pi, gti)}N\ni=1 consisting of N samples. Each\nsample x is composed of a question q, a reason-\ning path p and a ground truth answer gt. We Ô¨Åne-\ntuen G with standard language modeling objective\nLLM as Eq. ( 1).\nLLM = ‚àí\n|p|+|gt|‚àë\ni=1\nlog P (xi | x<i) (1)\nStep 1.2: Once G has learned how to generate\nsolutions, we employ it on questions q from D.\nAs a result, we obtain a new dataset D+ ={(\nqi, rpi,j, ai,j\n)}i=1,...,N\nj=1,...,M with M generated rea-\nsoning paths ( rp) and answers ( a) for each q.\nStep 1.3: Different from the popular methods, we\ntrain two veriÔ¨Åers to model human reasoning pro-\ncedure with deliberate analysis for each step and\nthe whole path. To evaluate several reasoning steps\nin a path, we desire a token-level scorer, which\nis named step veriÔ¨Åer Vstep. Therefore, we Ô¨Åne-\ntune a PLM with two tasks jointly: 1) the language\nmodeling task mentioned before; 2) the veriÔ¨Åca-\ntion task to predict a score for each token in the\nsolution. The veriÔ¨Åcation loss LV S is calculated\nas the Mean Squared Error (MSE) of the predicted\nscore with respect to the label as follows:\nLV S=\n|rp|+|a|‚àë\ni=1\n(scorei ‚àí I(a == gt))2, (2)\nwhere, (rp, a) from D+ and gt with same q from\nD.\nOn the other hand, we need a path-level scorer\nfor reasoning paths. Different from step veriÔ¨Åer,\nwe simply extract an overall presentation of the\nreasoning path for prediction. SpeciÔ¨Åcally, we em-\nploy a BERT-like model and take the [CLS] token\nto calculate MSE loss LV P similar to LV S.\nIn summary, the overall training objective for\nveriÔ¨Åers is given by:\nLV = LV S+ LLM + LV P. (3)\n3.3 Cooperative Inference\nAfter obtaining a generator and two veriÔ¨Åers, we\npropose cooperative inference to generate solu-\ntions for unseen questions. Instead of treating veri-\nÔ¨Åers as voters, we argue that veriÔ¨Åers should offer\nappropriate guidance and feedback during the rea-\nsoning process. Therefore, we integrate a cooper-\native search algorithm. In particular, we adopt the\npopular Monte Carlo Tree Search (MCTS) ( Kocsis\nand Szepesv√°ri , 2006) to enable controlled reason-\ning. The cooperative inference starts from the root\nnode, which preserves question tokens. We detail\nthe cooperative inference process as follows.\nSelection. If the current node has children, with\n50% probability, we select a node from its children\n4473\nü§îSystem 1\n(Generator)\nStep 1: Cooperative Training \nSystem 1&2\nStep 1.1 Fine-tuning\n({Q, P , GT})\nStep 1.2 Generating \nReasoning Paths ({Q})\nGenerated\n{Q, RP , A}\nüí°System2\n(Vstep & Vpath)Step 1.3 Fine-tuning\n({Q, RP , A})\nD+:{Q, RP, A}\nD:{Q, P, GT}\nGT: Ground Truth\nP: GT Reasoning Path\nRP: Generated Reasoning Path\nStep 2: Cooperative inference by Trained System 1&2\nD:{Q}\nü§îSystem 1\n(Generator)Generating \nReasoning Steps ({Q})\nüí°System2\n(Vstep)\nRP with scores \nScoring \nReasoning Steps\nScoring \nReasoning Paths\nüí°System2\n(Vpath)\nStep 3: Self-Thinking\nD:{Q, P, GT} Dùëõùëíùë§:{Q, RP, A, S}\nStep 2\nFiltering\nDùëõùëíùë§:{Q, RP, A}\nStep 1\nA: Generated Answers\nS: Scores\nReasoning Step\nMerge (ùê∑ùëõùëíùë§ , ùê∑)\nFigure 2: Cooperative reasoning framework.\nwith the modiÔ¨Åed PUCT formula ( Czech et al. ,\n2021) as Eq. ( 4),\nn‚àó = arg max\nn‚ààC\n(R(n)+cpuctœÄ(n|s)\n‚àö‚àë\nb‚ààC N(s, b)\n1 +N(s, n) ), (4)\nwhere the state s represents the sequence consist-\ning of all tokens in the current search path. And,\nN(s, n) means the times that node n has been\nselected in state s. Reward R(n) records all the\nscores received from the backup. We perform se-\nlection again with the selected node as the current\nnode. Otherwise, we perform expansion once and\nchoose the returned new node as current node.\nExpansion. During expansion, the generator is re-\nquired to generate a sequence of tokens based on\nthe current state. A new node is created to store the\ngenerated tokens and added to the current node‚Äôs\nchildren. Then, Vstep evaluates the current reason-\ning path and predict a score scorestep. Finally, the\nnew node is returned.\nRoll-Out. After selection and expansion, we start\nfrom the current node and let the generator com-\nplete the reasoning path until it meets [EOS] to-\nken or reaches the max token length limit. Next,\nVpath evaluates the whole reasoning path and pro-\nduces a score scorepath. Remember that Vstep also\nprovides a score scorestep during the expansion.\nTherefore to leverage both scores, we introduce a\nhyper-parameter Œ± to adjust their contributions to\nthe node‚Äôs reward,\ns = scorepath + Œ± √ó scorestep (5)\nwhere s is the Ô¨Ånal score that each node receives\nby the backup.\nBackup. We update the rewards back from the cur-\nrent node to the root node. The scores produced\nAlgorithm 1 Self-Thinking\nInput: Generator G; Step veriÔ¨Åer Vstep; Path ver-\niÔ¨Åer Vpath; Dataset D.\n1: Combine generator and veriÔ¨Åers with a coop-\nerative search algorithm.\n2: repeat\n3: Generate a new dataset Dnew from input\nquestions.\n4: Filter Dnew.\n5: Merge Dnew with D in Step 1.\n6: Do Step 1.\n7: Do Step 2.\n8: until performance is saturated.\nby veriÔ¨Åers are added to R(n) and the visited time\nN(s, n) is increased by 1.\n3.4 Self-Thinking\nIt is challenging to Ô¨Åne-tune models on the data\nsynthesized by themselves, which indicates they\nhave to be very conÔ¨Ådent in the content they gen-\nerate. A proper self-training method can enhance\nthe robustness of the whole system and allow deep\ndata mining. Therefore, we introduce self-thinking\nas described in Fig. 2 Step 3 and Algorithm 1. Con-\nsidering the noise contained in generated data, we\nbuild a Ô¨Ålter by using scores from veriÔ¨Åers and per-\nplexity (PPL) from the generator. In detail, we se-\nlect high-quality reasoning paths by setting a score\nthreshold. Moreover, we only keep the reasoning\npaths with no higher PPL than the ground truth\nsolutions. After Ô¨Åltering, we merge Dnew with D\nand send it to Step 1. Once the several iterations\nare completed, we obtain a powerful System 1&2.\nMore details can be found in Appendix B.3.\n4474\n3.5 Zero-shot Inference\nWe simply perform cooperative inference as Fig. 2\nStep 2 with trained System 1&2 on unseen\ndatasets. After obtaining several reasoning paths\nwith scores, we arrive at the Ô¨Ånal answer by\nweighted voting based on scores following ( Li\net al. , 2022).\n4 Experiments\n4.1 Experimental Setup\n4.1.1 Datasets\nWe consider several widely-used math word prob-\nlem datasets: GSM8K ( Cobbe et al. , 2021),\nASDiv-A ( Miao et al. , 2020), SingleOp ( Roy et al. ,\n2015), SinlgeEq ( Koncel-Kedziorski et al. , 2015)\nand MultiArith ( Roy and Roth , 2015). (Details\nin Appendix A). Following the general setting as\nin ( Kojima et al. , 2022; Wei et al. , 2022c), we\nemploy accuracy as the evaluation metric for all\ndatasets.\n4.1.2 Baselines\nFor comparison under the zero-shot setting, the re-\nsults of Instruct GPT-3 ( 175B) and PaLM ( 540B)\nwith their various methods are from Kojima et al.\n(2022). The zero-shot ‚àó and zero-shot-CoT ‚àó im-\nply not the standard prompt (see details in Ap-\npendix B.4). We also provide our generator as a\nbaseline when compared to previous Ô¨Åne-tuning\nmethods. Regarding to sampling multiple solu-\ntions, we search 40 paths with the same setting as\nSelf-Consistency ( Wang et al. , 2022).\nFor GSM8K, we select various powerful PLMs\nenhanced by the chain of thought prompt as base-\nlines, including LaMDA ( 137B) ( Thoppilan et al. ,\n2022), GPT-3 ( 175B) ( Brown et al. , 2020) and\nPaLM ( 540B) ( Chowdhery et al. , 2022). Except\nfor the few-shot methods, we also include a Ô¨Åne-\ntuned baseline that applies two GPT-3 ( 175B), one\nas the generator and the other as veriÔ¨Åer ( Cobbe\net al. , 2021).\n4.1.3 Implementation Details\nSince cooperative training requires a high-\nquality dataset with reasoning paths, we treat\nGSM8K ( Cobbe et al. , 2021) as the seed dataset\nD in Sec. 3.2. Unless otherwise, we employ GPT-\nJ ( Wang and Komatsuzaki , 2021) as the generator\nand the step veriÔ¨Åer, DeBERTa-large ( He et al. ,\n2021) as the path veriÔ¨Åer. Since the default set-\nting consists of two GPT-J ( 6B) and a DeBERTa-\nBackbone Method SingleEq MultiArith\nInstruct GPT-3 175B zero-shot 74.6 17.7\nzero-shot‚àó 78.7 22.7\nzero-shot-CoT 78.0 78.7\nzero-shot-CoT‚àó 78.7 79.3\nPaLM 540B zero-shot - 25.5\nzero-shot-CoT - 66.1\n+Self-Consistency- 89.0\nGPT-J 12B CoRe (ours) 79.5 97.5\nTable 1: Zero-shot results on SingleEq and MultiArith.\nlarge ( 0.4B), we note our backbone as ‚ÄúGPT-J\n12B‚Äù, which implies around 12.4 billion parame-\nters in total. During generation, we apply calcula-\ntor as assistant following Cobbe et al. (2021). We\nrun all the experiments for 3 times and report the\nbest result, detailed hyper-parameters setting can\nbe found in Appendix B.1. Our zero-shot setting\nis similar to the transferring setting in T0 ( Sanh\net al. , 2022) and FLAN ( Wei et al. , 2022a). All the\ntraining and testing procedures are done on a DGX\nstation with 8 A100 GPUs.\n4.2 Main Results\n4.2.1 Zero-shot Results\nTable 1 presents main results on two mathemat-\nical reasoning datasets, demonstrating the zero-\nshot generalization ability. CoRe achieves supe-\nrior performance on both datasets, demonstrating\nits capability of mathematical reasoning on unseen\ndatasets. Note that the baselines are several dozen\ntimes larger than ours and still underperform our\nmodel. The improvement might be explained by\ntwo potential reasons. One is that applying the\nCoRe framework on PLMs can activate their rea-\nsoning ability, even though their scales are small\n(‚â§ 100B). Another one is that self-thinking can\nprovide valuable self-produced data to teach Sys-\ntems 1&2. Therefore, the results present the effec-\ntiveness of cooperative working with System 1&2\nand self-thinking.\n4.2.2 Zero-shot v.s. Fine-tuning\nWe compare CoRe with previous Ô¨Åne-tuned SoTA\nbaselines on four datasets, and results are pre-\nsented in Table 2. To show the importance of co-\noperative reasoning, we apply our generator as a\nbaseline. The results demonstrate that without any\nguidance generator underperforms previous meth-\nods on most datasets. Despite the gain from self-\nconsistency, it still lags behind other Ô¨Åne-tuned So-\nTAs. While after applying our method CoRe, it sur-\n4475\nBackbone Method ASDiv-A SingleOp SingleEq MultiArith\nFine-tune\nPrevious SoTA 75.3a 80.1b 72.3c 60.5d\nZero-shot\nGPT-J 6B Generator only 51.7 53.2 49.2 77.3\n+ Self-Consistency 63.7 59.6 60.2 92.3\nGPT-J 12B CoRe (ours) 90.5 85.2 79.5 97.5\nTable 2: Zero-shot results v.s. previous Ô¨Åne-tuned SoTA results on math reasoning tasks. The previous SoTA\nbaselines are obtained from: a: ( Lan et al. , 2022), b: LogicForm ( Liang et al. , 2016), c: UNITDEP ( Roy and Roth ,\n2017), d: Relevance and LCA operation classiÔ¨Åer ( Roy and Roth , 2015). The best scores are in bold.\nBackbone Method GSM8K\nfew-shot\nLaMDA 137B CoT 17.1\n+ Self-Consistency 27.7\nGPT-3 175B CoT 49.6\n+ Self-Consistency -\nPaLM 540B CoT 56.5\n+ Self-Consistency 74.4\nÔ¨Åne-tune\nGPT-3 350B - 57.0\nGPT-J 12B CoRe ( ours) 63.2\nTable 3: Fine-tuning v.s. Few-shot results on GSM8K\nwith various PLMs. Results are reported from Cobbe\net al. (2021); Wei et al. (2022c); Wang et al. (2022).\nThe best score is in bold and the second is underlined.\npasses previous Ô¨Åne-tuned SoTAs on all datasets in\na zero-shot setting. The results clearly demonstrate\nthe capability of CoRe to greatly boost PLMs‚Äô rea-\nsoning ability.\n4.2.3 GSM8K Results\nBeyond improvements on zero-shot results, we ob-\nserve that the Ô¨Åne-tuning setting can beneÔ¨Åt a lot\nfrom our CoRe framework, as shown in Table 3.\nCompared to previous Ô¨Åne-tuned SoTA ( Cobbe\net al. , 2021) (GPT-3 350B), CoRe outperforms it\nwith much fewer parameters, computation and in-\nference time. Note that it samples 100 solutions\nfor each question while we only search 40 paths.\nFor a comprehensive comparison, we include\nfew-shot results with large-scale PLMs due to a\nlimited number of ‚ÄúÔ¨Åne-tune‚Äù competitors. With\nregard to few-shot methods applied on large-scale\nPLMs ( ‚â• 100B parameters), CoRe only under-\nperforms PaLM-540B strengthened by chain of\nthought prompt and self-consistency, further prov-\ning the effectiveness of our method.\nGuidance Œ± SingleOp MultiArith\nw/o veriÔ¨Åers - 59.6 92.3\nVpath only 0 80.2 95.8\nVpath + Vstep 0.1 81.3 95.8\n1 82.9 96.8\nTable 4: Zero-shot results with different levels of guid-\nance from veriÔ¨Åers. Œ± comes from Eq. ( 5).\n4.3 Ablation Study\n4.3.1 Is guidance important during path\nsearching reasoning?\nWe argued that it is important to introduce guid-\nance in the loop during reasoning path searching.\nTo validate this argument, we adjust the weight\nof reward provided by veriÔ¨Åers during reasoning.\nThe experiments are conducted using models with-\nout self-thinking. Table 4 summarizes the perfor-\nmance on zero-shot datasets with different set-\ntings of guidance. For ‚Äúw/o veriÔ¨Åers‚Äù, the solu-\ntions are predicted by a generator only and ap-\nplied with ‚ÄúSelf-Consistency‚Äù. As demonstrated\nin Table 4, guidance from Vpath can provide per-\nformance gains on SingleOp, with a 20.6% ab-\nsolute improvement. We further incorporate the\nguidance from the step-level veriÔ¨Åer Vstep. As de-\nscribed in Eq. ( 5), increasing the weight of reward\n(Œ±) from Vstep, CoRe achieves a higher accuracy\non both SingleOp and MultiArith. Thanks to the\nfeedback and guidance during the reasoning stage,\nthe generator tends to explore more often on a path\nwith a higher reward score. As a result, CoRe in-\ncreases the accuracy on SingleOP from 59.6% to\n82.9% and MultiArith from 92.3% to 96.8%.\n4.3.2 How much does self-thinking boost the\nreasoning ability of a language model?\nTo examine the effect of self-thinking, we explore\nit along with two axes: 1) the number of itera-\ntions and 2) the type of search strategy. Since we\n4476\n0\n20\n40\n60\n80\n100\n5 10 15 20 25 30 35 40\nAccuracy (%)\nNumber of reasoning paths\nSingleOp\nSelf-Consistency CoRe\n0\n20\n40\n60\n80\n100\n5 10 15 20 25 30 35 40\nAccuracy (%)\nNumber of reasoning paths\nAsDiv-A\nSelf-Consistency CoRe\n0\n20\n40\n60\n80\n100\n5 10 15 20 25 30 35 40\nAccuracy (%)\nNumber of reasoning paths\nSingleEq\nSelf-Consistency CoRe\n0\n20\n40\n60\n80\n100\n5 10 15 20 25 30 35 40\nAccuracy (%)\nNumber of reasoning paths\nMultiArith\nSelf-Consistency CoRe\nFigure 3: Zero-shot results with different search strategies in cooperative inference.\n# of iterations 0 1 2\nGenerator only (Greedy) 29.9 34.7 34.9\nGenerator + Self-Consistency 42.0 43.1 45.9\nCoRe 60.0 63.2 61.6\nTable 5: Results on GSM8K with models undergone a\ndifferent number of self-thinking iterations. Outcomes\nof various search strategies are provided.\napply the self-thinking procedure on the GSM8K\ndataset, we investigate the performance of mod-\nels under different settings on GSM8K, as shown\nin Table 5. First, increasing the number of itera-\ntions can always improve the performance for both\ngreedy decode and self-consistency. Our CoRe\nreaches saturation in one round, which might be\nattributed to the fact that System 1&2 learns better\nand faster on self-generated data by collaborative\nworking. Second, regardless of the search strategy,\nself-thinking consistently boost the model‚Äôs perfor-\nmance, which veriÔ¨Åes that self-thinking boost lan-\nguage model‚Äôs reasoning ability.\n4.3.3 Do self-thinking generalize to other\ndatasets?\nWe have performed self-thinking on GSM8K and\nproved that it improves the model‚Äôs reasoning abil-\nity in 4.3.2. Furthermore, we explore whether the\nimprovement on GSM8K comes at the cost of per-\nformance degradation on other datasets, i.e. the\nmodel overÔ¨Åts the dataset. As presented in Table 6,\nwe vary the number of self-thinking iterations for\nthe generator and veriÔ¨Åers respectively and pro-\nvide results on SingleOp and MultiArith. The re-\nsults show that the performance of the generator\nsuffers a little, but veriÔ¨Åers can eliminate this un-\ndesirable effect and beneÔ¨Åt a lot from self-thinking.\nThe best results are obtained when only the ver-\niÔ¨Åers are further Ô¨Åne-tuned, with the 2.3% and\n0.7% absolute improvement on the two datasets\nrespectively. This observation implies that we can\n# of iterations SingleOp MultiArithGenerator VeriÔ¨Åers\n0 0 82.9 96.8\n0 1 85.2 97.5\n1 0 81.9 96.3\n1 1 83.3 97.2\nTable 6: Zero-shot results with a different number of\nself-thinking iterations for generator and veriÔ¨Åers re-\nspectively.\neconomize on training costs and time where target\ndatasets are not included in self-thinking data.\n4.3.4 How performance varies as the number\nof search iterations for different search\nstrategies changes?\nAs shown in Fig. 3, accuracy on 4 datasets consis-\ntently increases along with the growth of search\niterations for both search strategies. However, the\nscaling curves of self-consistency and CoRe are\nquite different. The performance gain quickly sat-\nurates with self-consistency. Sampling 40 paths\ncan not further improve the accuracy, while the\nscaling curve of CoRe is much sharper. Due to\nthe heuristic algorithm that requires the model to\ncontinue exploring on the previously generated\npaths, CoRe starts from a relatively lower level in\nthe beginning, whereas the accuracy quickly im-\nproves as the number of search iterations increases.\nThe result demonstrates the effectiveness of CoRe\nin searching reasoning paths, with a fast growth\ncurve and a slow saturation rate.\n4.4 Case studies\n4.4.1 Improvements from CoRe\nA typical exemplar from GSM8K is presented\nin Table 7. Greedy decode fails to Ô¨Ånd a reasonable\npath due to the limited exploration in the output\nspace. In contrast, self-consistency samples multi-\nple reasoning paths randomly, resulting in a richer\ncandidate set. Although it Ô¨Ånds some right solu-\n4477\nQuestion Sophia and Rose went together to the market to buy onions and potatoes. Rose bought 4 times the number of onions and potatoes Sophia\nbought. If Rose bought 12 onions and 4 potatoes, how many onions and potatoes in total did Sophia buy at the market?\nScore\nGreedy\nDecode\nRose bought 12 onions * 4 = 12 * 4 = 48 onions. Rose bought 4 potatoes * 4 = 4 * 4 = 16 potatoes. Sophia and Rose bought 48 onions\n+ 16 potatoes = 64 onions and potatoes in total . [ANS] 64.\n‚àí\nMost ap-\npropriate\nSC Path\nIf Rose bought 12 onions at the market, then Sophia bought 12 / 4 = 3 onions . If Rose bought 4 potatoes at the market, then Sophia\nbought 4 / 4 = 1 potato . Since Rose bought 12 onions and 4 potatoes, Sophia bought 3 + 1 = 4 onions and potatoes in total . [ANS] 4.\n‚àí\nMost con-\nsistent\nSC Path\nRose bought 4 x 48 = 192 potatoes . In total, Sophia bought 192 + 48 = 240 onions and potatoes at the market. [ANS] 240. ‚àí\nCoRe\nPath 1\nSince Rose bought 12 onions, this means there were 12 / 4 = 3 onions that Sophia bought . The number of potatoes that Sophia bought\nis 4, meaning there were 4 / 4 = 1 potato that Sophia bought . The total number of onions and potatoes that Sophia bought is 3 + 1 = 4 .\n[ANS] 4.\n0.994\nCoRe\nPath 2\nThe number of onions Rose bought is 12 onions / 4 = 3. Sophia bought 4 / 12 = 1 / 3 of the number of onions Rose bought. The number\nof potatoes Rose bought is 4 potatoes / 4 = 1 .33 of the number of potatoes Rose bought is 4 potatoes / 12 onions = 0.33 . The total number\nof onions and potatoes Rose bought at the market is 3 onions + 1 potatoes = 4. [ANS] 4.\n0.363\nCoRe\nPath 3\nLet x be the number of onions Sophia bought. Rose bought 4 * x onions . The total number of onions and potatoes Rose bought is 4 * x\n+ x = 12. 5 * x = 12. x = 4 . Sophia bought 4 onions . [ANS] 4.\n0.023\nTable 7: GSM8K samples generated through different searching methods. SC refers to Self-Consistency. The erro-\nneous parts of each path are highlighted in red and the corresponding correct parts within other paths are blue.\nQuestion Gunter is trying to count the jelly beans in a jar. He asks his friends how many they think are in the jar. One says 80. Another says 20\nmore than half the Ô¨Årst one. A third says 25% more than the Ô¨Årst one. What is their average guess?\nNo Self-Thinking Half the Ô¨Årst guess is 80 / 2 = 40. The third guess is 40 * .25 = 10. Their total guesses are 80 + 20 + 40 + 10 = 150. Their average\nguess is 150 / 4 = 37.5. [ANS] 37.5.\nWith Self-Thinking Half the Ô¨Årst friend‚Äôs guess is 80 / 2 = 40. The second friend‚Äôs guess is 40 + 20 = 60. The third friend‚Äôs guess is 80 * 1.25 = 100.\nTheir total guesses are 80 + 60 + 100 = 240. Their average guess is 240 / 3 = 80. [ANS] 80.\nTable 8: An example of GSM8K, model with self-thinking reasoned correctly, while the non-self-thinking model\ngenerated a wrong reasoning path and therefore failed.\ntions occasionally, without any guidance, it fails to\nexplore more frequently on the high-quality paths,\nthus ending up with a wrong answer obtained by\nmajority voting as shown in the fourth row.\nAs a comparison, results generated by CoRe are\nlisted with their scores. Similar to random sam-\npling, the reasoning paths might be partially illog-\nical, even though the Ô¨Ånal answers happen to be\ncorrect. Despite this challenge, CoRe is capable\nof distinguishing those poor-quality paths from the\nsuperior ones thanks to the veriÔ¨Åers. Adhering to\nthe philosophy of cooperative reasoning we have\nemphasized, the veriÔ¨Åers managed to harness the\ngenerator throughout the reasoning procedure with\nthe help of MCTS. Therefore, CoRe enjoys not\nonly the advantage of having a diverse candidate\nset, but also the merit of being wiser and efÔ¨Åcient\nduring reasoning path searching.\n4.4.2 Improvements from Self-Thinking\nTable 8 shows an example that the vanilla model\nfailed to solve the given question, whereas after\nthe self-thinking, the model rectiÔ¨Åed the faulty\nparts and successfully addressed it. This displays\nthat self-thinking boosts language models‚Äô inner\nreasoning ability regardless of the search strategy,\nwhich is also proved in Sec. 4.3.2.\n5 Discussion\nAlthough we only Ô¨Åne-tune the language model\non GSM8K due to the scarcity of QA datasets an-\nnotated with intermediate rationales, zero-shot re-\nsults on several arithmetic datasets prove that basic\nreasoning capability is transferable across datasets\nwithin the same domain. This observation implies\nthat when it comes to a new domain, we only need\nto collect a limited number of question-answer\npairs with reasoning paths, model‚Äôs reasoning abil-\nity can generalize to other unseen datasets and can\nbe further strengthened by our approach CoRe ac-\ncording to the experimental results.\n6 Conclusions\nIn this work, we mimic the dual system of human\ncognition to develop an effective reasoning frame-\nwork for solving the math word problems. The pro-\nposed approach is consisting of two ingredients:\nthe generator as System 1 and the veriÔ¨Åers as Sys-\ntem 2, and overall reasoning is conducted based\non their mutual reinforcement. From the robust-\nness and generalization aspects, CoRe activates su-\nperior reasoning ability of LMs, and thus outper-\nforms PLMs that are dozens of times larger.\n4478\nLimitations\nThe outcome on multiple datasets veriÔ¨Åes the pow-\nerful reasoning ability, which even works on mod-\nels with only several billion parameters. How-\never, our self-thinking procedure utilizes only one\ndataset, GSM8K, and the available training set\nsize is only 7.5K. The main reason is the scarcity\nof high-quality datasets with rich reasoning paths.\nAnd, collecting such data incurs huge computation\ncosts and expensive human resources. Another\nlimitation is that we have not conducted experi-\nments on bigger language models, such as GPT-3\nand PaLM, due to the expensive usage costs and\nthe fact of no open-source codes. In a nutshell, in\nthe future, we will focus on collecting more high-\nquality labeled data and exploring our method on\nmore powerful language models.\nEthics Statement\nIn this work, our CoRe shows impressive rea-\nsoning capability, however, it also comes with\nsocial risks. Here, we summarize three possible\nethical impacts: i) PLMs with bias, ii) gener-\nated data with social stereotypes and iii) prob-\nlematic data environments. Considering utilizing\nPLMs as backbones, several works present var-\nious potential risks in PLMs ( Lucy and Bam-\nman, 2021; Amin and Kabir , 2022). Fortunately,\nour method supports the replacement of different\nPLMs. Therefore, we encourage deploying some\nrisk-free PLMs, expecting to reduce the potential\nethical risks. Furthermore, once deploying harm-\nful PLMs, the self-thinking process might gener-\nate several undesired data and those data are fed\ninto language models, which deepens the bias and\ncauses unintended social impacts. For reducing the\naforementioned cases, we suggest recording gener-\nated sentences. In real-world applications, a good\nchoice is to monitor generated content and then\nhand them over for human review. In addition to\nthe two risks posed by PLMs, the data in down-\nstream tasks is of great concern. In particular, pri-\nvate data might cause unpredictable inÔ¨Çuence be-\ncause of their nature as a non-open source. There-\nfore, we believe that a data cleaning workÔ¨Çow is\nnecessary to mitigate potential risks, such as Pri-\nvateClean ( Krishnan et al. , 2016). Finally, we en-\ncourage open debating about its utilization for in-\ncreasing transparency and reducing the potential\nfor misuse.\nAcknowledgements\nThis work was partly supported by the National\nKey Research and Development Program of China\n(No. 2020YFB1708200) , the \"Graph Neural Net-\nwork Project\" of Ping An Technology (Shenzhen)\nCo., Ltd. and the Shenzhen Science and Technol-\nogy Program (JCYJ20220818101001004).\nReferences\nAkhter Al Amin and Kazi Sinthia Kabir. 2022. A dis-\nability lens towards biases in GPT-3 generated open-\nended languages. CoRR, abs/2206.11993.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In NeurIPS.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nCoRR, abs/2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Ja-\ncob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training veriÔ¨Åers to\nsolve math word problems. CoRR, abs/2110.14168.\nJohannes Czech, Patrick Korus, and Kristian Kersting.\n2021. Improving alphazero using monte-carlo graph\nsearch. In ICAPS, pages 103‚Äì111. AAAI Press.\n4479\nJonathan St.B.T. Evans. 2003. In two minds: dual-\nprocess accounts of reasoning . Trends in Cognitive\nSciences, 7(10):454‚Äì459.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In ICLR. OpenRe-\nview.net.\nDaniel Kahneman. 2011. Thinking, fast and slow . Far-\nrar, Straus and Giroux.\nLevente Kocsis and Csaba Szepesv√°ri. 2006. Ban-\ndit based monte-carlo planning. In ECML, volume\n4212 of Lecture Notes in Computer Science , pages\n282‚Äì293. Springer.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. CoRR,\nabs/2205.11916.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics , 3:585‚Äì597.\nSanjay Krishnan, Jiannan Wang, Michael J. Franklin,\nKen Goldberg, and Tim Kraska. 2016. Privateclean:\nData cleaning and differential privacy. In SIGMOD\nConference, pages 937‚Äì951. ACM.\nYihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan,\nBing Tian Dai, Yan Wang, Dongxiang Zhang, and\nEe-Peng Lim. 2022. Mwptoolkit: An open-source\nframework for deep learning-based math word prob-\nlem solvers. In AAAI, pages 13188‚Äì13190. AAAI\nPress.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2022. On the\nadvance of making language models better reason-\ners. CoRR, abs/2206.02336.\nChao-Chun Liang, Kuang-Yi Hsu, Chien-Tsung\nHuang, Chung-Min Li, Shen-Yu Miao, and Keh-Yih\nSu. 2016. A tag-based statistical english math\nword problem solver with understanding, reasoning\nand explanation. In IJCAI, pages 4254‚Äì4255.\nIJCAI/AAAI Press.\nLi Lucy and David Bamman. 2021. Gender and repre-\nsentation bias in GPT-3 generated stories . In Pro-\nceedings of the Third Workshop on Narrative Un-\nderstanding, pages 48‚Äì55, Virtual. Association for\nComputational Linguistics.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and develop-\ning English math word problem solvers . In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 975‚Äì984, On-\nline. Association for Computational Linguistics.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena.\n2022. Show your work: Scratchpads for interme-\ndiate computation with language models . In Deep\nLearning for Code Workshop .\nMaxwell I. Nye, Michael Henry Tessler, Joshua B.\nTenenbaum, and Brenden M. Lake. 2021. Improv-\ning coherence and consistency in neural sequence\nmodels with dual-system, neuro-symbolic reasoning.\nIn NeurIPS, pages 25192‚Äì25204.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems . In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2015, Lisbon, Portugal,\nSeptember 17-21, 2015 , pages 1743‚Äì1752. The As-\nsociation for Computational Linguistics.\nSubhro Roy and Dan Roth. 2017. Unit dependency\ngraph and its application to arithmetic word problem\nsolving. In AAAI, pages 3082‚Äì3088. AAAI Press.\nSubhro Roy, Tim Vieira, and Dan Roth. 2015. Rea-\nsoning about quantities in natural language . Trans.\nAssoc. Comput. Linguistics , 3:1‚Äì13.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafÔ¨Ån, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Tae-\nwoon Kim, Gunjan Chhablani, Nihal V . Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault F√©vry, Ja-\nson Alan Fries, Ryan Teehan, Teven Le Scao, Stella\nBiderman, Leo Gao, Thomas Wolf, and Alexan-\nder M. Rush. 2022. Multitask prompted training en-\nables zero-shot task generalization. In ICLR. Open-\nReview.net.\nThomas Scialom, Paul-Alexis Dray, Jacopo Staiano,\nSylvain Lamprier, and Benjamin Piwowarski. 2021.\nTo beam or not to beam: That is a question of cooper-\nation for language gans. In NeurIPS, pages 26585‚Äì\n26597.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kath-\nleen S. Meier-Hellstern, Meredith Ringel Morris,\nTulsee Doshi, Renelito Delos Santos, Toju Duke,\nJohnny Soraker, Ben Zevenbergen, Vinodkumar\nPrabhakaran, Mark Diaz, Ben Hutchinson, Kristen\n4480\nOlson, Alejandra Molina, Erin Hoffman-John, Josh\nLee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton,\nAaron Cohen, Rachel Bernstein, Ray Kurzweil,\nBlaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd H. Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications. CoRR,\nabs/2201.08239.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. CoRR, abs/2203.11171.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In ICLR.\nOpenReview.net.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022b.\nEmergent abilities of large language models. CoRR,\nabs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou.\n2022c. Chain of thought prompting elicits reasoning\nin large language models. CoRR, abs/2201.11903.\n4481\nA Dataset Details\nThe mathematical reasoning datasets with details\nare as follows (Detailed description of the statis-\ntics in Table 9). We follow the licenses for their\npapers.\nThe dataset in Ô¨Åne-tuning:\nGSM8K ( Cobbe et al. , 2021) is a high-quality\ndataset with reasoning paths. It consists of 8.8K\ngrade school math problems created by human\nwriters, which are divided into a train set ( 7.5K)\nand a test set ( 1.3K). The reasoning paths include\n2 to 8 steps with considering basic arithmetic oper-\nations. Furthermore, we conduct cooperative train-\ning and self-thinking on its training set.\nThe datasets in zero-shot inference:\nASDiv-A ( Miao et al. , 2020) includes diverse\nmath word problems, which are required to answer\na number for each question.\nSingleOP ( Roy et al. , 2015) is proposed with ele-\nmentary math problems of a single operation.\nSingleEq ( Koncel-Kedziorski et al. , 2015) is con-\nstrued with both single-step and multi-step math\nproblems from mixed sources.\nMultiArith ( Roy and Roth , 2015) includes ele-\nmentary math problems with multiple steps.\nB Experimental Settings\nB.1 Hyper-parameters Setting\nFor the generator and the step veriÔ¨Åer, we train\nthem for two epochs. The batch size is set to 16.\nThe learning rate (LR) is set to 1e ‚àí 5 at the Ô¨Årst\nepoch and 1e‚àí6 at the second epoch for generator.\nOn the hand of step veriÔ¨Åer we apply the warmup\nmethod then linearly decaying scheduler, LR is set\nto 1e ‚àí 6 and warmup ratio is 0.1.\nFor the path veriÔ¨Åer, we train it for three epochs\nwith batch size set to 128 and LR set to 1e ‚àí 5.\nSame LR scheduler as the step veriÔ¨Åer has been\napplied for the path veriÔ¨Åer. We set the gradient\nclip norm to 1.0 and the sampling temperature to\n0.7. The random seed is set to 19990303 through-\nout the training process.\nFor MCTS, we set max search iterations to 40\nduring inference. In expansion, we search 20 to-\nkens each time. In order to avoid expanding too\nmany homogeneous children for the same node,\nwe simply penalize the probability of Ô¨Årst token\nif it has appeared in other child nodes. We set the\nmax token number to 300 in roll out and limit the\ntotal token number of reasoning path to 400.\nDataset # of samples Avg # of words in questions\nGSM8K 1319 46.9\nASDiv-A 1218 29.2\nSingleOp 562 20.9\nSingleEq 508 27.2\nMultiArith 600 31.8\nTable 9: Dataset statistics.\nB.2 Details of Training VeriÔ¨Åers\nBefore two veriÔ¨Åers are Ô¨Åne-tuned, we utilize the\ngenerator to sample 100 solutions for each ques-\ntion following Cobbe et al. (2021). Then we train\nthe two veriÔ¨Åers on the generated data as described\nin Sec. 3.2 Step 1.3.\nB.3 Details of Self-Thinking\nIn each iteration of self-thinking, we initialize the\nmodel with the weights obtained from the previ-\nous round so as to save the computational costs.\nSince we use cooperative inference rather than ran-\ndom sampling to generate data for further training,\nsolutions are expected more high-quality. Thus,\nthe number of generated solutions M mentioned\nin Sec. 3.2 is set to 50 for saving computational\ncost and time. Due to the Ô¨Çexibility of MCTS, we\nhave also tried to limit the time for searching rather\nthan the number of iterations, which makes the to-\ntal search time controllable and predictable. More-\nover, this allows the model to adaptively adjust\nthe Ô¨Ånal number of solutions searched for each\nquestion, due to the different levels of difÔ¨Åculty\nin questions. In our experiments, we observe that\nsetting the time limit to 320 seconds provides bet-\nter results than setting the iteration limit to 50,\nwhile maintaining approximately the same time\nconsumption. Therefore, we use time control to\ngenerate data during self-thinking.\nB.4 Baseline Settings\nAs shown in Table 1, the Instruct GPT-3 is based\non text-davinci-002 version. Moreover, since Ko-\njima et al. (2022) provides difference prompt set-\nting, we list them in Table 10. For few-shot scenar-\nios with the chain of thought prompts, we follow\nthe original paper ( Wei et al. , 2022c).\nC Extended Experiments\nThis section we replicate the work of Cobbe et al.\n(2021) with GPT-J and report the results in Ta-\nble 11 for comprehensive comparison. CoRe fully\nsurpasses Cobbe et al. (2021) when the number of\n4482\nBackbone Method Reasoning Extraction Prompts Answer Extraction Prompts\nInstruct GPT-3 175B zero-shot Let‚Äôs think step by step. The answer (arabic numerals) is\nzero-shot‚àó Let‚Äôs think step by step. The answer is\nzero-shot-CoT Let‚Äôs think step by step. The answer (arabic numerals) is\nzero-shot-CoT‚àó Let‚Äôs think step by step. The answer is\nPaLM 540B zero-shot Let‚Äôs think step by step. The answer (arabic numerals) is\nzero-shot-CoT Let‚Äôs think step by step. The answer (arabic numerals) is\n+ Self-Consistency Let‚Äôs think step by step. The answer (arabic numerals) is\nTable 10: Prompt setting for few-shot baselines.\n# of reasoning paths ASDiv-A SingleOp SingleEq MultiArith\nCobbe et al.\n5 71.9 70.5 68.5 92.3\n10 76.9 73.1 74.6 95.0\n20 79.6 74.6 76.0 95.5\n30 81.4 76.2 76.2 95.2\n40 81.4 76.9 78.1 94.8\nCoRe\n5 13.7 22.2 14.6 4.3\n10 41.7 47.7 33.7 26.8\n20 78.4 77.0 64.6 80.7\n30 88.9 84.9 77.4 95.0\n40 90.5 85.2 79.5 97.5\nTable 11: Comparison between Cobbe et al. (2021) and CoRe with GPT-J as backbone model. The best scores are\nin bold.\nreasoning paths reaches 30 and maintains a faster\nincreasing rate after that. As a result, CoRe has\na superior performance over Cobbe et al. (2021)\non all the datasets and achieves a 9.1% and 8.3%\nimprovement compared to it on ASDiv-A and Sin-\ngleOp.\nD Future Work\nWe focus on measuring our method in boosting the\nlanguage model‚Äôs arithmetic reasoning ability in\nthis work. Nevertheless, we believe that our frame-\nwork can also be applied to other reasoning tasks\nseamlessly, e.g., commonsense reasoning and sym-\nbolic reasoning. We choose arithmetic reasoning\nbecause it is the fundamental type of reasoning\ntask. Additionally, we believe solving arithmetic\nreasoning is the Ô¨Årst step toward a general cogni-\ntive reasoning system. In the future, we will ex-\nplore other reasoning tasks and put more effort\ninto low-resource scenarios.\n4483\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nAfter conclusion section\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nAfter limitations section\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\n1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\n4.1\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\n4.1\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix A\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nEthics Statement\n‚ñ°\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nEthics Statement\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAppendix A\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\n4.4\nC ‚ñ°\u0013 Did you run computational experiments?\n4\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4484\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4, Appendix A\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4.1\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4485",
  "topic": "Zh√†ng",
  "concepts": [
    {
      "name": "Zh√†ng",
      "score": 0.9084990620613098
    },
    {
      "name": "Computer science",
      "score": 0.5332847237586975
    },
    {
      "name": "Computational linguistics",
      "score": 0.4869190454483032
    },
    {
      "name": "Word (group theory)",
      "score": 0.4501294791698456
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4314424395561218
    },
    {
      "name": "Natural language processing",
      "score": 0.4213795065879822
    },
    {
      "name": "Linguistics",
      "score": 0.3995066285133362
    },
    {
      "name": "Cognitive science",
      "score": 0.39821332693099976
    },
    {
      "name": "Philosophy",
      "score": 0.23686444759368896
    },
    {
      "name": "Psychology",
      "score": 0.2169453501701355
    },
    {
      "name": "History",
      "score": 0.0685894787311554
    },
    {
      "name": "China",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I150744194",
      "name": "Waseda University",
      "country": "JP"
    }
  ]
}