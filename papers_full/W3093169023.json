{
  "title": "Cold-start Active Learning through Self-supervised Language Modeling",
  "url": "https://openalex.org/W3093169023",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287426100",
      "name": "Yuan, Michelle",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288574532",
      "name": "Lin, Hsuan-Tien",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222507791",
      "name": "Boyd-Graber, Jordan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2011674654",
    "https://openalex.org/W2963696295",
    "https://openalex.org/W603830301",
    "https://openalex.org/W2149464712",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2073459066",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W3099859218",
    "https://openalex.org/W3020899366",
    "https://openalex.org/W2963902936",
    "https://openalex.org/W1672197616",
    "https://openalex.org/W1599956645",
    "https://openalex.org/W1599935123",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2964282813",
    "https://openalex.org/W2396420927",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W199218251",
    "https://openalex.org/W2995188922",
    "https://openalex.org/W2144452292",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2798803565",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2109378394",
    "https://openalex.org/W2963756980",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W77777798",
    "https://openalex.org/W2152748974",
    "https://openalex.org/W2899181341",
    "https://openalex.org/W2970693987",
    "https://openalex.org/W2127017215",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2970450573",
    "https://openalex.org/W2963639288",
    "https://openalex.org/W2085989833",
    "https://openalex.org/W2963742748",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3048000637",
    "https://openalex.org/W2147148726",
    "https://openalex.org/W2128678390",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W1968200975",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2962849408",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.",
  "full_text": "Cold-start Active Learning through Self-supervised Language Modeling\nMichelle Yuan∗\nUniversity of Maryland\nmyuan@cs.umd.edu\nHsuan-Tien Lin\nNational Taiwan University\nhtlin@csie.ntu.edu.tw\nJordan Boyd-Graber\nUniversity of Maryland\njbg@umiacs.umd.edu\nAbstract\nActive learning strives to reduce annotation\ncosts by choosing the most critical examples\nto label. Typically, the active learning strat-\negy is contingent on the classiﬁcation model.\nFor instance, uncertainty sampling depends\non poorly calibrated model conﬁdence scores.\nIn the cold-start setting, active learning is\nimpractical because of model instability and\ndata scarcity. Fortunately, modern NLP pro-\nvides an additional source of information: pre-\ntrained language models. The pre-training\nloss can ﬁnd examples that surprise the model\nand should be labeled for efﬁcient ﬁne-tuning.\nTherefore, we treat the language modeling loss\nas a proxy for classiﬁcation uncertainty. With\nBERT , we develop a simple strategy based\non the masked language modeling loss that\nminimizes labeling costs for text classiﬁcation.\nCompared to other baselines, our approach\nreaches higher accuracy within less sampling\niterations and computation time.\n1 Introduction\nLabeling data is a fundamental bottleneck in ma-\nchine learning, especially for NLP , due to annota-\ntion cost and time. The goal of active learning (AL)\nis to recognize the most relevant examples and then\nquery labels from an oracle. For instance, policy-\nmakers and physicians want to quickly ﬁne-tune\na text classiﬁer to understand emerging medical\nconditions (V oorhees et al., 2020). Finding labeled\ndata for medical text is challenging because of pri-\nvacy issues or shortage in expertise (Dernoncourt\nand Lee, 2017). Using AL, they can query labels\nfor a small subset of the most relevant documents\nand immediately train a robust model.\nModern transformer models dominate the leader-\nboards for several NLP tasks (Devlin et al., 2019;\nYang et al., 2019). Yet the price of adopting\n∗⋆Work done while visiting National Taiwan University.\ntransformer-based models is to use more data. If\nthese models are not ﬁne-tuned on enough exam-\nples, their accuracy drastically varies across differ-\nent hyperparameter conﬁgurations (Dodge et al.,\n2020). Moreover, computational resources are a\nmajor drawback as training one model can cost\nthousands of dollars in cloud computing and hun-\ndreds of pounds in carbon emissions (Strubell et al.,\n2019). These problems motivate further work in\nAL to conserve resources.\nAnother issue is that traditional AL algorithms,\nlike uncertainty sampling (Lewis and Gale, 1994),\nfalter on deep models. These strategies use model\nconﬁdence scores, but neural networks are poorly\ncalibrated (Guo et al., 2017). High conﬁdence\nscores do not imply high correctness likelihood,\nso the sampled examples are not the most uncertain\nones (Zhang et al., 2017). Plus, these strategies\nsample one document on each iteration. The single-\ndocument sampling requires training the model\nafter each query and increases the overall expense.\nThese limitations of modern NLP models illus-\ntrate a twofold effect: they show a greater need for\nAL and make AL more difﬁcult to deploy. Ideally,\nAL could be most useful during low-resource situa-\ntions. In reality, it is impractical to use because the\nAL strategy depends on warm-starting the model\nwith information about the task (Ash and Adams,\n2019). Thus, a ﬁtting solution to AL for deep clas-\nsiﬁers is a cold-start approach, one that does not\nrely on classiﬁcation loss or conﬁdence scores.\nTo develop a cold-start AL strategy, we should\nextract knowledge from pre-trained models like\nBERT (Devlin et al., 2019). The model encodes\nsyntactic properties (Tenney et al., 2019), acts as\na database for general world knowledge (Petroni\net al., 2019; Davison et al., 2019), and can de-\ntect out-of-distribution examples (Hendrycks et al.,\n2020). Given the knowledge already encoded in\npre-trained models, the annotation for a new task\narXiv:2010.09535v2  [cs.CL]  22 Oct 2020\nshould focus on the information missing from pre-\ntraining. If a sentence contains many words that\nperplex the language model, then it is possibly un-\nusual or not well-represented in the pre-training\ndata. Thus, the self-supervised objective serves as\na surrogate for classiﬁcation uncertainty.\nWe develop ALPS (Active Learning by Process-\ning Surprisal), an AL strategy for BERT -based mod-\nels.1 While many AL methods randomly choose\nan initial sample, ALPS selects the ﬁrst batch of\ndata using the masked language modeling loss. As\nthe highest and most extensive peaks in Europe\nare found in the Alps, the ALPS algorithm ﬁnds\nexamples in the data that are both surprising and\nsubstantial. To the best of our knowledge, ALPS\nis the ﬁrst AL algorithm that only relies on a self-\nsupervised loss function. We evaluate our approach\non four text classiﬁcation datasets spanning across\nthree different domains. ALPS outperforms AL\nbaselines in accuracy and algorithmic efﬁciency.\nThe success of ALPS highlights the importance of\nself-supervision for cold-start AL.\n2 Preliminaries\nWe formally introduce the setup, notation, and ter-\nminology that will be used throughout the paper.\nPre-trained Encoder Pre-training uses the lan-\nguage modeling loss to train encoder parameters for\ngeneralized representations. We call the model in-\nput x= (wi)l\ni=1 a “sentence”, which is a sequence\nof tokens w from a vocabulary Vwith sequence\nlength l. Given weights W, the encoder hmaps x\nto a d-dimensonal hidden representation h(x; W).\nWe use BERT (Devlin et al., 2019) as our data en-\ncoder, so his pre-trained with two tasks: masked\nlanguage modeling (MLM ) and next sentence pre-\ndiction. The embedding h(x; W) is computed as\nthe ﬁnal hidden state of the [CLS] token in x. We\nalso refer to h(x; W) as the BERT embedding.\nFine-tuned Model We ﬁne-tune BERT on the\ndownstream task by training the pre-trained model\nand the attached sequence classiﬁcation head. Sup-\npose that f represents the model with the classi-\nﬁcation head, has parameters θ = (W,V ), and\nmaps input xto a C-dimensional vector with conﬁ-\ndence scores for each label. Speciﬁcally, f(x; θ) =\nσ(V ·h(x; W)) where σis a softmax function.\nLet D be the labeled data for our classiﬁca-\ntion task where the labels belong to set Y =\n1https://github.com/forest-snow/alps\nAlgorithm 1 AL for Sentence Classiﬁcation\nRequire: Inital model f(x; θ0) with pre-trained\nencoder h(x; W0), unlabeled data pool U,\nnumber of queries per iteration k, number of\niterations T, sampling algorithm A\n1: D= {}\n2: for iterations t= 1,...,T do\n3: if Ais cold-start for iteration tthen\n4: Mt(x) =f(x; θ0)\n5: else\n6: Mt(x) =f(x; θt−1)\n7: Qt ←Apply Aon model Mt(x), data U\n8: Dt ←Label queries Qt\n9: D= D∪Dt\n10: U= U\\D t\n11: θt ←Fine-tune f(x; θ0) on D\n12: return f(x; θT )\n{1,...,C }. During ﬁne-tuning, we take a base\nclassiﬁer f with weights W0 from a pre-trained\nencoder hand ﬁne-tune f on Dfor new parame-\nters θt. Then, the predicted classiﬁcation label is\nˆy= arg maxy∈Yf(x; θt)y.\nAL for Sentence Classiﬁcation Assume that\nthere is a large unlabeled dataset U = {(xi)}n\ni=1\nof nsentences. The goal of AL is to sample a sub-\nset D⊂U efﬁciently so that ﬁne-tuning the classi-\nﬁer f on subset Dimproves test accuracy. On each\niteration t, the learner uses strategy Ato acquire k\nsentences from dataset U and queries for their la-\nbels (Algorithm 1). Strategy Ausually depends on\nan acquisition model Mt (Lowell et al., 2019). If\nthe strategy depends on model warm-starting, then\nthe acquisition model Mt is f with parameters θt−1\nfrom the previous iteration. Otherwise, we assume\nthat Mt is the pre-trained model with parameters\nθ0. After T rounds, we acquire labels for Tk sen-\ntences. We provide more concrete details about AL\nsimulation in Section 5.\n3 The Uncertainty–Diversity Dichotomy\nThis section provides background on prior work\nin AL. First, we discuss two general AL strategies:\nuncertainty sampling and diversity sampling. Then,\nwe explain the dichotomy between the two con-\ncepts and introduce BADGE (Ash et al., 2020), a\nSOTA method that attempts to resolve this issue.\nFinally, we focus on the limitations of BADGE and\nother AL strategies to give motivation for our work.\nDasgupta (2011) describes uncertainty and di-\nversity as the “two faces of AL”. While uncer-\ntainty sampling efﬁciently searches the hypothesis\nspace by ﬁnding difﬁcult examples to label, diver-\nsity sampling exploits heterogeneity in the feature\nspace (Xu et al., 2003; Hu et al., 2010; Bodó et al.,\n2011). Uncertainty sampling requires model warm-\nstarting because it depends on model predictions,\nwhereas diversity sampling can be a cold-start ap-\nproach. A successful AL strategy should integrate\nboth aspects, but its exact implementation is an\nopen research question. For example, a naïve idea\nis to use a ﬁxed combination of strategies to sample\npoints. Nevertheless, Hsu and Lin (2015) experi-\nmentally show that this approach hampers accuracy.\nBADGE optimizes for both uncertainty and diver-\nsity by using conﬁdence scores and clustering. This\nstrategy beats uncertainty-based algorithms (Wang\nand Shang, 2014), sampling through bandit learn-\ning (Hsu and Lin, 2015), and CORESET (Sener and\nSavarese, 2018), a diversity-based method for con-\nvolutional neural networks.\n3.1 BADGE\nThe goal of BADGE is to sample a diverse and\nuncertain batch of points for training neural net-\nworks. The algorithm transforms data into repre-\nsentations that encode model conﬁdence and then\nclusters these transformed points. First, an unla-\nbeled point x passes through the trained model\nto obtain its predicted label ˆy. Next, a gradient\nembedding gx is computed for xsuch that it em-\nbodies the gradient of the cross-entropy loss on\n(f(x; θ),ˆy) with respect to the parameters of the\nmodel’s last layer. The gradient embedding is\n(gx)i = (f(x; θ)i −1 (ˆy= i))h(x; W). (1)\nThe i-th block of gx is the hidden representa-\ntion h(x; W) scaled by the difference between\nmodel conﬁdence score f(x; θ)i and an indicator\nfunction 1 that indicates whether the predictive la-\nbel ˆyis label i. Finally, BADGE chooses a batch to\nsample by applying k-MEANS ++ (Arthur and Vas-\nsilvitskii, 2006) on the gradient embeddings. These\nembeddings consist of model conﬁdence scores and\nhidden representations, so they encode information\nabout both uncertainty and the data distribution. By\napplying k-MEANS ++ on the gradient embeddings,\nthe chosen examples differ in feature representation\nand predictive uncertainty.\n3.2 Limitations\nBADGE combines uncertainty and diversity sam-\npling to proﬁt from advantages of both methods\nbut also brings the downsides of both: reliance on\nwarm-starting and computational inefﬁciency.\n3.2.1 Model Uncertainty and Inference\nDodge et al. (2020) observe that training is highly\nunstable when ﬁne-tuning pre-trained language\nmodels on small datasets. Accuracy signiﬁcantly\nvaries across different random initializations. The\nmodel has not ﬁne-tuned on enough examples, so\nmodel conﬁdence is an unreliable measure for un-\ncertainty. While BADGE improves over uncertainty-\nbased methods, it still relies on conﬁdence scores\nf(x; θ)i when computing the gradient embeddings\n(Equation 1). Also, it uses labels inferred by the\nmodel to compensate for lack of supervision in AL,\nbut this inference is inaccurate for ill-trained mod-\nels. Thus, warm-start methods may suffer from\nproblems with model uncertainty or inference.\n3.2.2 Algorithmic Efﬁciency\nMany diversity-based methods involve distance\ncomparison between embedding representations,\nbut this computation can be expensive, especially\nin high-dimensional space. For instance, CORESET\nis a farthest-ﬁrst traversal in the embedding space\nwhere it chooses the farthest point from the set of\npoints already chosen on each iteration (Sener and\nSavarese, 2018). The embeddings may appropri-\nately represent the data, but issues, like the “curse\nof dimensionality” (Beyer et al., 1999) and the\n“hubness problem” (Tomasev et al., 2013), persist.\nAs the dimensionality increase, the distance be-\ntween any two points converges to the same value.\nMoreover, the gradient embeddings inBADGE have\ndimensionality of Cd for a C-way classiﬁcation\ntask with data dimensionality of d (Equation 1).\nThese issues make distance comparison between\ngradient embeddings less meaningful and raises\ncosts to compute those distances.\n4 A Self-supervised Active Learner\nCold-start AL is challenging because of the short-\nage in labeled data. Prior work, like BADGE , of-\nten depend on model uncertainty or inference, but\nthese measures can be unreliable if the model has\nnot trained on enough data (Section 3.2.1). To\novercome the lack of supervision, what if we ap-\nply self-supervision to AL? For NLP , the language\nmodeling task is self-supervised because the label\nBERT\nMLM Head\nCross Entropy \nLoss\nL2 Normalization\n<0, 0, 0, 0, 0, 0.32, 0, 0, 0, 0.49, 0, 0, 0, 0, 0, 0, 0.81>\n^* \n* \n* \n* \n* \nhighest * \n* \n* \nmountain* \n* \n* \n* \n*\n*\n europe \nv\n[CLS] the alps are the highest and most extensive \nmountain range system that entirely lies in europe\nInput\nToken\nLabels\nSurprisal Embeddings\nFigure 1: To form surprisal embedding sx for sentence\nx, we pass in unmasked xthrough the BERT MLM head\nand compute cross-entropy loss for a random 15% sub-\nsample of tokens against the target labels. The unsam-\npled tokens have entries of zero in sx. ALPS clusters\nthese surprisal embeddings to sample sentences forAL.\nfor each token is the token itself. If the task has\nimmensely improved transfer learning, then it may\nreduce generalization error in AL too.\nFor our approach, we adopt the uncertainty-\ndiversity BADGE framework for clustering embed-\ndings that encode information about uncertainty.\nHowever, rather than relying on the classiﬁcation\nloss gradient, we use the MLM loss to bootstrap un-\ncertainty estimates. Thus, we combine uncertainty\nand diversity sampling for cold-start AL.\n4.1 Masked Language Modeling\nTo pre-train BERT with MLM , input tokens are ran-\ndomly masked, and the model needs to predict the\ntoken labels of the masked tokens. BERT is bidirec-\ntional, so it uses context from the left and right of\nthe masked token to make predictions. BERT also\nuses next sentence prediction for pre-training, but\nthis task shows minimal effect for ﬁne-tuning (Liu\net al., 2019). So, we focus on applying MLM to\nAL. The MLM head can capture syntactic phenom-\nena (Goldberg, 2019) and performs well on psy-\ncholinguistic tests (Ettinger, 2020).\nAlgorithm 2 Single iteration of ALPS\nRequire: Pre-trained encoder h(x; W0), unla-\nbeled data pool U, number of queries k\n1: for sentences x∈U do\n2: Compute sx with MLM head of h(x; W0)\n3: M= {sx |x∈U}\n4: C← k-MEANS cluster centers of M\n5: Q= {arg minx∈U∥c−sx∥|c∈C}\n6: return Q\n4.2 ALPS\nSurprisal Embeddings Inspired by how BADGE\nforms gradient embeddings from the classiﬁcation\nloss, we create surprisal embeddings from lan-\nguage modeling. For sentence x, we compute sur-\nprisal embedding sx by evaluating xwith the MLM\nobjective. To evaluate MLM loss, BERT randomly\nmasks 15% of the tokens in xand computes cross-\nentropy loss for the masked tokens against their\ntrue token labels. When computing surprisal em-\nbeddings, we make one crucial change: none of\nthe tokens are masked when the input is passed\ninto BERT . However, we still randomly choose\n15% of the tokens in the input to evaluate with\ncross-entropy against their target token labels. The\nunchosen tokens are assigned a loss of zero as they\nare not evaluated (Figure 1).\nThese decisions for not masking input (Ap-\npendix A.1) and evaluating only 15% of tokens (Ap-\npendix A.2) are made because of experiments on\nthe validation set. Proposition 1 provides insight on\nthe information encoded in surprisal embeddings.\nFinally, the surprisal embedding isl2-normalized as\nnormalization improves clustering (Aytekin et al.,\n2018). If the input sentences have a ﬁxed length of\nl, then the surprisal embeddings have dimensional-\nity of l. The length lis usually less than the hidden\nsize of BERT embeddings.\nProposition 1. For an unnormalized surprisal em-\nbedding sx, each nonzero entry (sx)i estimates\nI(wi), the surprisal of its corresponding token\nwithin the context of sentence x.\nProof. Extending notation from Section 2, assume\nthat m is the MLM head, with parameters φ =\n(W,Z), which maps input x to a l×|V| matrix\nm(x; φ). The ith row m(x; φ)i contains prediction\nscores for wi, the ith token in x. Suppose that wi\nis the jth token in vocabulary V. Then, m(x; φ)i,j\nis the likelihood of predicting wi correctly.\nNow, assume that context is the entire input x\nand deﬁne the language model probability pm as,\npm(wi |x) =m(x; φ)i,j. (2)\nSalazar et al. (2020) have a similar deﬁnition as\nEquation 2 but instead have deﬁned it in terms of\nthe masked input. We argue that their deﬁnition\ncan be extended to the unmasked input x. During\nBERT pre-training, the MLM objective is evaluated\non the [MASK] token for 80% of the time, random\ntoken for 10% of the time, and the original token\nfor 10% of the time. This helps maintain consis-\ntency across pre-training and ﬁne-tuning because\n[MASK] never appears in ﬁne-tuning (Devlin et al.,\n2019). Thus, we assume that mestimates occur-\nrence of tokens within a maskless context as well.\nNext, the information-theoretic surprisal (Shan-\nnon, 1948) is deﬁned as I(w) =−log p(w|c), the\nnegative log likelihood of word wgiven context c.\nIf wi is sampled and evaluated, then the ith entry\nof the unnormalized surprisal embedding is,\n(sx)i = −log m(x; φ)i,j = −log pm(wi |x)\n= I(wi).\nProposition 1 shows that the surprisal embed-\ndings comprise of estimates for token-context sur-\nprisal. Intuitively, these values can help withAL be-\ncause they highlight the information missing from\nthe pre-trained model. For instance, consider the\nsentences: “this is my favorite television show” and\n“they feel ambivalent about catholic psychedelic\nsynth folk music”. Tokens from the latter have\nhigher surprisal than those from the former. If this\nis a sentiment classiﬁcation task, the second sen-\ntence is more confusing for the classiﬁer to learn.\nThe surprisal embeddings indicate sentences chal-\nlenging for the pre-trained model to understand and\ndifﬁcult for the ﬁne-tuned model to label.\nThe most surprising sentences contain many rare\ntokens. If we only train our model on the most sur-\nprising sentences, then it may not generalize well\nacross different examples. Plus, we may sample\nseveral atypical sentences that are similar to each\nother, which is often an issue for uncertainty-based\nmethods (Kirsch et al., 2019). Therefore, we incor-\nporate clustering in ALPS to maintain diversity.\nk-MEANS Clustering After computing surprisal\nembeddings for each sentence in the unlabeled\npool, we use k-MEANS to cluster the surprisal em-\nbeddings. Then, for each cluster center, we select\nthe sentence that has the nearest surprisal embed-\nding to it. The ﬁnal set of sentences are the queries\nto be labeled by an oracle (Algorithm 2). Although\nBADGE uses k-MEANS ++ to cluster, experiments\nshow that k-MEANS works better for surprisal em-\nbeddings (Appendix A.3).\n5 Active Sentence Classiﬁcation\nWe evaluate ALPS on sentence classiﬁcation for\nthree different domains: sentiment reviews, news\narticles, and medical abstracts (Table 1). To simu-\nlate AL, we sample a batch of 100 sentences from\nthe training dataset, query labels for this batch,\nand then move the batch from the unlabeled pool\nto the labeled dataset (Algorithm 1). The initial\nencoder h(x; θ0), is an already pre-trained, BERT -\nbased model (Section 5.2). In a given iteration, we\nﬁne-tune the base classiﬁer f(x; θ0) on the labeled\ndataset and evaluate the ﬁne-tuned model with clas-\nsiﬁcation micro-F1 score on the test set. We do not\nﬁne-tune the model f(x; θt−1) from the previous\niteration to avoid issues with warm-starting (Ash\nand Adams, 2019). We repeat for ten iterations,\ncollecting a total of 1,000 sentences.\n5.1 Baselines\nWe compare ALPS against warm-start methods (En-\ntropy, BADGE , FT-BERT -KM) and cold-start meth-\nods (Random, BERT -KM). For FT-BERT -KM, we\nuse BERT -KM to sample data in the ﬁrst iteration.\nFor other warm-start methods, data is randomly\nsampled in the ﬁrst iteration.\nEntropy Sample k sentences with\nhighest predictive entropy measured by∑C\ni=1(f(x; θ)i) ln(f(x; θ)i)−1 (Lewis and\nGale, 1994; Wang and Shang, 2014).\nBADGE Sample k sentences based on diversity\nin loss gradient (Section 3.1).\nBERT -KM Cluster pre-trained, l2-normalized\nBERT embeddings with k-MEANS and sample the\nnearest neighbors of the k cluster centers. The\nalgorithm is the same as ALPS except that BERT\nembeddings are used.\nFT-BERT -KM This is the same algorithm as\nBERT -KM except the BERT embeddings h(x; Wt−1)\nfrom the previously ﬁne-tuned model are used.\nDataset Domain Train Dev Test # Labels\nAG NEWS News articles 110,000 10,000 7,600 4\nIMDB Sentiment reviews 17,500 7,500 25,000 2\nPUBMED 20k RCT Medical abstracts 180,040 30,212 30,135 5\nSST-2 Sentiment reviews 60,615 6,736 873 2\nTable 1: Sentence classiﬁcation datasets used in experiments.\n0 200 400 600 800 1000\n0\n0.2\n0.4\n0.6\n0.8F1 Score\nAG News\nStrategy\nRandom\nEntropy\nBERT-KM\nFT-BERT-KM\nBADGE\nALPS\n0 200 400 600 800 1000\nIMDB\n0 200 400 600 800 1000\nPubMed\n0 200 400 600 800 1000\nNumber of Labeled Sequences\nSST-2\nFigure 2: Test accuracy of simulated AL over ten iterations with 100 sentences queried per iteration. The dashed\nline is the test accuracy when the model is ﬁne-tuned on the entire dataset. Overall, models trained with data\nsampled from ALPS have the highest test accuracy, especially for the earlier iterations.\n5.2 Setup\nFor each sampling algorithm and dataset, we run\nthe AL simulation ﬁve times with different random\nseeds. We set the maximum sequence length to\n128. We ﬁne-tune on a batch size of thirty-two for\nthree epochs. We use AdamW (Loshchilov and\nHutter, 2019) with learning rate of 2e-5, β1 = 0.9,\nβ2 = 0.999, and a linear decay of learning rate.\nFor IMDB (Maas et al., 2011), SST-2 (Socher\net al., 2013), and AG NEWS (Zhang et al., 2015), the\ndata encoder is the uncased BERT -Base model with\n110M parameters.2 For PUBMED (Dernoncourt and\nLee, 2017), the data encoder is SCIBERT , a BERT\nmodel pre-trained on scientiﬁc texts (Beltagy et al.,\n2019). All experiments are run on GeForce GTX\n1080 GPU and 2.6 GHz AMD Opteron 4180 CPU\nprocessor; runtimes in Table 2.\n2https://huggingface.co/transformers/\n5.3 Results\nThe model ﬁne-tuned with data sampled by ALPS\nhas higher test accuracy than the baselines (Fig-\nure 2). For AG NEWS , IMDB , and SST-2, this is true\nin earlier iterations. We often see the most gains in\nthe beginning for crowdsourcing (Felt et al., 2015).\nInterestingly, clustering the ﬁne-tuned BERT em-\nbeddings is not always better than clustering the\npre-trained BERT embeddings for AL. The ﬁne-\ntuned BERT embeddings may require training on\nmore data for more informative representations.\nFor PUBMED , test accuracy greatly varies be-\ntween the strategies. The dataset belongs to a spe-\ncialized domain and is class-imbalanced, so naïve\nmethods show poor accuracy. Entropy sampling\nhas the lowest accuracy because the classiﬁcation\nentropy is uninformative in early iterations. The\nmodels ﬁne-tuned on data sampled by ALPS and\nBADGE have about the same accuracy. Both meth-\nods strive to optimize for uncertainty and diversity,\nwhich alleviates problems with class imbalance.\nAG NEWS PUBMED\nRandom <1 <1\nEntropy 7 10\nALPS 14 24\nBADGE 23 70\nBERT -KM 28 58\nFT-BERT -KM 33 79\nTable 2: Average runtime (minutes) per sampling iter-\nation during AL simulation for large datasets. BADGE ,\nFT-BERT -KM, and BERT -KM take much longer to run.\nOur experiments cover the ﬁrst ten iterations\nbecause we focus on the cold-start setting. As\nsampling iterations increase, test accuracy across\nthe different methods converges. Both ALPS and\nBADGE already approach the model trained on the\nfull training dataset across all tasks (Figure 2).\nOnce the cold-start issue subsides, uncertainty-\nbased methods can be employed to further query\nthe most confusing examples for the model to learn.\n6 Analyzing ALPS\nSampling Efﬁciency Given that the gradient em-\nbeddings are computed, BADGE has a time com-\nplexity of O(Cknd) for a C-way classiﬁcation\ntask, kqueries, npoints in the unlabeled pool, and\nd-dimensional BERT embeddings. Given that the\nsurprisal embeddings are computed, ALPS has a\ntime complexity of O(tknl) where tis the ﬁxed\nnumber of iterations for k-MEANS and l is the\nmaximum sequence length. In our experiments,\nk = 100, d= 768, t= 10, and l = 128. In prac-\ntice, twill not change much, but nand Ccould be\nmuch higher. For large dataset PUBMED , the aver-\nage runtime per iteration is 24 minutes for ALPS\nand 70 minutes for BADGE (Table 2). So, ALPS can\nmatch BADGE ’s accuracy more quickly.\nDiversity and Uncertainty We estimate diver-\nsity and uncertainty for data sampled across differ-\nent strategies. For diversity, we look at the overlap\nbetween tokens in the sampled sentences and to-\nkens from the rest of the data pool. A diverse batch\nof sentences should share many of the same tokens\nwith the data pool. In other words, the sampled\nsentences can represent the data pool because of\nthe substantial overlap between their tokens. In\nour simulations, the entire data pool is the training\ndataset (Section 5). So, we compute the Jaccard\nsimilarity between VD, set of tokens from the sam-\n1.344\n1.346\n1.348\n1.350Uncertainty\nAG News\nStrategy\nRandom\nEntropy\nBERT-KM\nFT-BERT-KM\nBADGE\nALPS\n0.1 0.2 0.3\nDiversity\n1.5725\n1.5750\n1.5775\n1.5800\nPubMed\nFigure 3: Plot of diversity against uncertainty estimates\nfrom AL simulations for AG NEWS and PUBMED . Each\npoint represents a sampled batch of sentences from the\nAL experiments. The shape indicates the strategy used\nto sample the sentences. The color indicates the sample\niteration. The lightest color corresponds to the ﬁrst iter-\nation and the darkest color represents the tenth iteration.\nWhile uncertainty estimates are similar across different\nbatches, ALPS shows a consistent increase in diversity\nwithout drops in uncertainty.\npled sentences D, and VD′, set of tokens from the\nunsampled sentences U\\D ,\nGd(D) =J(VD,VD′) =|VD ∩VD′|\n|VD ∪VD′|. (3)\nIf Gd is high, this indicates high diversity because\nthe sampled and unsampled sentences have many\ntokens in common. If Gd is low, this indicates poor\ndiversity and representation.\nTo measure uncertainty, we use f(x,θ∗), the\nclassiﬁer trained on the full training dataset. In our\nexperiments, classiﬁer f(x,θ∗) has high accuracy\n(Figure 2) and inference is stable after training on\nmany examples. Thus, we can use the logits from\nthe classiﬁer to understand its uncertainty toward a\nparticular sentence. First, we compute predictive\nentropy of sentence x when evaluated by model\nf(x,θ∗). Then, we take the average of predictive\n(a) BERT embeddings with k-MEANS centers\n (b) Surprisal embeddings with k-MEANS centers\nFigure 4: T-SNE plots of BERT embeddings and surprisal embeddings for each sequence in the IMDB training\ndataset. The enlarged points are the centers determined by k-MEANS (left) and k-MEANS ++ (right). The points are\ncolored according to their classiﬁcation labels. In both sets of embeddings, we cannot clearly separate the points\nfrom their labels, but the distinction between clusters in surprisal embeddings seems more obvious.\nentropy over all sentences in a sampled batch D.\nWe use the average predictive entropy to esimate\nuncertainty of the sampled sentences,\nGu(D) = 1\n|D|\n∑\nx∈D\nC∑\ni=1\n(f(x; θ∗)i) ln(f(x; θ∗)i)−1.\n(4)\nWe compute Gd and Gu for batches sampled in the\nAL experiments of AG NEWS and PUBMED . Di-\nversity is plotted against uncertainty for batches\nsampled across different iterations and AL strate-\ngies (Figure 3). For AG NEWS , Gd and Gu are\nrelatively low for ALPS in the ﬁrst iteration. As\niterations increase, samples from ALPS increase\nin diversity and decrease minimally in uncertainty.\nSamples from other methods have a larger drop in\nuncertainty as iterations increase. For PUBMED ,\nALPS again increases in sample diversity without\ndrops in uncertainty. In the last iteration, ALPS has\nthe highest diversity among all the algorithms.\nSurprisal Clusters Prior work use k-MEANS to\ncluster feature representations as a cold-start AL ap-\nproach (Zhu et al., 2008; Bodó et al., 2011). Rather\nthan clustering BERT embeddings, ALPS clusters\nsurprisal embeddings. We compare the clusters\nbetween surprisal embeddings and BERT embed-\ndings to understand the structure of the surprisal\nclusters. First, we use t-SNE (Maaten and Hinton,\n2008) to plot the embeddings for each sentence in\nthe IMDB training set (Figure 4). The labels are\nnot well-separated for both embedding sets, but the\nsurprisal embeddings seem easier to cluster. To\nquantitively measure cluster quality, we use the Sil-\nhouette Coefﬁcient for which larger values indicate\ndesirable clustering (Rousseeuw, 1987). The sur-\nprisal clusters have a coefﬁcient of 0.38, whereas\nthe BERT clusters have a coefﬁcient of only 0.04.\nThese results, along with the classiﬁcation exper-\niments, show that naïvely clustering BERT embed-\ndings is not suited for AL. Possibly, more compli-\ncated clustering algorithms can capture the intrinsic\nstructure of the BERT embeddings. However, this\nwould increase the algorithmic complexity and run-\ntime. Alternatively, one can map the feature repre-\nsentations to a space where simple clustering algo-\nrithms work well. During this transformation, im-\nportant information for AL must be preserved and\nextracted. Our approach uses the MLM head, which\nhas already been trained on extensive corpora, to\nmap the BERT embeddings into the surprisal em-\nbedding space. As a result, simple k-MEANS can\nefﬁciently choose representative sentences.\nSingle-iteration Sampling In Section 5, we sam-\nple data iteratively (Algorithm 1) to fairly compare\nthe different AL algorithms. However, ALPS does\nnot require updating the classiﬁer because it only\ndepends on the pre-trained encoder. Rather than\nsampling data in small batches and re-training the\nmodel, ALPS can sample a batch of ksentences in\none iteration (Algorithm 2). Between using ALPS\niteratively and deploying the algorithm for a single\niteration, the difference is insigniﬁcant (Table 3).\nPlus, sampling 1,000 sentences only takes about 97\nminutes for PUBMED and 7 minutes for IMDB .\nDataset k Iterative Single\nIMDB 200 0.63 ±0.04 0.61 ±0.03\n500 0.74 ±0.05 0.76 ±0.04\n1000 0.82 ±0.01 0.82 ±0.01\nPUBMED 200 0.63 ±0.03 0.64 ±0.03\n500 0.80 ±0.02 0.82 ±0.01\n1000 0.84 ±0.00 0.84 ±0.00\nTable 3: Test accuracy on IMDB and PubMed between\ndifferent uses of ALPS for various k, the number of sen-\ntences to query. We compare using ALPS iteratively (It-\nerative) as done in Section 5 with using ALPS to query\nall k sentences in one iteration (Single). The test ac-\ncuracy does not change much, showing that ALPS is\nﬂexible to apply in different settings.\nWith this ﬂexibility in sampling, ALPS can ac-\ncommodate different budget constraints. For exam-\nple, re-training the classiﬁer may be costly, so users\nwant a sampling algorithm that can query k sen-\ntences all at once. In other cases, annotators are not\nalways available, so the number of obtainable anno-\ntations is unpredictable. Then, users would prefer\nan AL strategy that can query a variable number of\nsentences for any iteration. These cases illustrate\npractical needs for a cold-start algorithm like ALPS .\n7 Related Work\nActive learning has shown success in tasks, such\nas named entity recognition (Shen et al., 2004),\nword sense disambiguation (Zhu and Hovy, 2007),\nand sentiment analysis (Li et al., 2012). Wang and\nShang (2014) are the ﬁrst to adapt prior AL work to\ndeep learning. However, popular heuristics (Settles,\n2009) for querying individual points do not work as\nwell in a batch setting. Since then, more research\nhas been conducted on batch AL for deep learning.\nZhang et al. (2017) propose the ﬁrst work on AL\nfor neural text classiﬁcation. They assume that\nthe classiﬁer is a convolutional neural network and\nuse expected gradient length (Settles et al., 2008)\nto choose sentences that contain words with the\nmost label-discriminative embeddings. Besides\ntext classiﬁcation, AL has been applied to neural\nmodels for semantic parsing (Duong et al., 2018),\nnamed entity recognition (Shen et al., 2018), and\nmachine translation (Liu et al., 2018).\nALPS makes use of BERT , a model that excels\nat transfer learning. Other works also combine AL\nand transfer learning to select training data that\nreduce generalization error. Rai et al. (2010) mea-\nsures domain divergence from the source domain\nto select the most informative texts in the target\ndomain. Wang et al. (2014) use AL to query points\nfor a target task through matching conditional dis-\ntributions. Additionally, combining word-level and\ndocument-level annotations can improve knowl-\nedge transfer (Settles, 2011; Yuan et al., 2020).\nIn addition to uncertainty and diversity sam-\npling, other areas of deep AL focus on Bayesian\napproaches (Siddhant and Lipton, 2018; Kirsch\net al., 2019) and reinforcement learning (Fang et al.,\n2017). An interesting research direction can inte-\ngrate one of these approaches with ALPS .\n8 Conclusion\nTransformers are powerful models that have revolu-\ntionized NLP . Nevertheless, like other deep models,\ntheir accuracy and stability require ﬁne-tuning on\nlarge amounts of data. AL should level the playing\nﬁeld by directing limited annotations most effec-\ntively so that labels complement, rather than du-\nplicate, unsupervised data. Luckily, transformers\nhave generalized knowledge about language that\ncan help acquire data for ﬁne-tuning. Like BADGE ,\nwe project data into an embedding space and then\nselect the most representative points. Our method\nis unique because it only relies on self-supervision\nto conduct sampling. Using the pre-trained loss\nguides the AL process to sample diverse and uncer-\ntain examples in the cold-start setting. Future work\nmay focus on ﬁnding representations that encode\nthe most important information for AL.\nAcknowledgments\nWe thank Kuen-Han Tsai, Chien-Min Yu, Si-An\nChen, Pedro Rodriguez, Eleftheria Briakou, and the\nanonymous reviewers for their feedback. Michelle\nYuan is supported by JHU Human Language Tech-\nnology Center of Excellence (HLTCOE). Jordan\nBoyd-Graber is supported in part by the Ofﬁce\nof the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activ-\nity (IARPA), via the BETTER Program contract\n#2019-19051600005. The views and conclusions\ncontained herein are those of the authors and should\nnot be interpreted as necessarily representing the\nofﬁcial policies, either expressed or implied, of\nODNI, IARPA, or the U.S. Government. The U.S.\nGovernment is authorized to reproduce and dis-\ntribute reprints for governmental purposes notwith-\nstanding any copyright annotation therein.\nReferences\nDavid Arthur and Sergei Vassilvitskii. 2006. k-\nmeans++: The advantages of careful seeding. Tech-\nnical report, Stanford.\nJordan T. Ash and Ryan P. Adams. 2019. On warm-\nstarting neural network training. arXiv preprint\narXiv:1910.08475.\nJordan T. Ash, Chicheng Zhang, Akshay Krishna-\nmurthy, John Langford, and Alekh Agarwal. 2020.\nDeep batch active learning by diverse, uncertain gra-\ndient lower bounds. In Proceedings of the Interna-\ntional Conference on Learning Representations.\nCaglar Aytekin, Xingyang Ni, Francesco Cricri, and\nEmre Aksu. 2018. Clustering and unsupervised\nanomaly detection with L2 normalized deep auto-\nencoder representations. In International Joint Con-\nference on Neural Networks.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of Empirical Methods in Natural\nLanguage Processing.\nKevin Beyer, Jonathan Goldstein, Raghu Ramakrish-\nnan, and Uri Shaft. 1999. When is “nearest neigh-\nbor” meaningful? In International Conference on\nDatabase Theory.\nZalán Bodó, Zsolt Minier, and Lehel Csató. 2011. Ac-\ntive learning with clustering. In Active Learning and\nExperimental Design Workshop in Conjunction with\nAISTATS 2010.\nSanjoy Dasgupta. 2011. Two faces of active learning.\nTheoretical computer science, 412(19):1767–1781.\nJoe Davison, Joshua Feldman, and Alexander M. Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of Empirical Meth-\nods in Natural Language Processing.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence classiﬁ-\ncation in medical abstracts. International Joint Con-\nference on Natural Language Processing , 2:308–\n313.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Conference of the North American\nChapter of the Association for Computational Lin-\nguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nLong Duong, Hadi Afshar, Dominique Estival, Glen\nPink, Philip R Cohen, and Mark Johnson. 2018. Ac-\ntive learning for deep semantic parsing. In Proceed-\nings of the Association for Computational Linguis-\ntics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nMeng Fang, Yuan Li, and Trevor Cohn. 2017. Learning\nhow to active learn: A deep reinforcement learning\napproach. In Proceedings of Empirical Methods in\nNatural Language Processing.\nPaul Felt, Eric Ringger, Kevin Seppi, Kevin Black, and\nRobbie Haertel. 2015. Early gains matter: A case\nfor preferring generative over discriminativecrowd-\nsourcing models. In Proceedings of the Association\nfor Computational Linguistics.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. Journal of Machine Learning Research ,\n70:1321–1330.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the Association for\nComputational Linguistics.\nWei-Ning Hsu and Hsuan-Tien Lin. 2015. Active learn-\ning by learning. In Association for the Advancement\nof Artiﬁcial Intelligence.\nRong Hu, Brian Mac Namee, and Sarah Jane Delany.\n2010. Off to a good start: Using clustering to select\nthe initial training set in active learning. In Florida\nArtiﬁcial Intelligence Research Society Conference.\nAndreas Kirsch, Joost van Amersfoort, and Yarin Gal.\n2019. BatchBALD: Efﬁcient and diverse batch ac-\nquisition for deep Bayesian active learning. In Pro-\nceedings of Advances in Neural Information Pro-\ncessing Systems.\nDavid D. Lewis and William A. Gale. 1994. A sequen-\ntial algorithm for training text classiﬁers. In Pro-\nceedings of the ACM SIGIR Conference on Research\nand Development in Information Retrieval.\nShoushan Li, Shengfeng Ju, Guodong Zhou, and Xiao-\njun Li. 2012. Active learning for imbalanced sen-\ntiment classiﬁcation. In Proceedings of Empirical\nMethods in Natural Language Processing.\nMing Liu, Wray Buntine, and Gholamreza Haffari.\n2018. Learning to actively learn neural machine\ntranslation. In Conference on Computational Nat-\nural Language Learning.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In Proceedings of the\nInternational Conference on Learning Representa-\ntions.\nDavid Lowell, Zachary C. Lipton, and Byron C. Wal-\nlace. 2019. Practical obstacles to deploying active\nlearning. In Proceedings of Empirical Methods in\nNatural Language Processing.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the Association for Computational\nLinguistics.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Machine\nLearning Research, 9:2579–2605.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of Empirical Methods\nin Natural Language Processing.\nPiyush Rai, Avishek Saha, Hal Daumé III, and Suresh\nVenkatasubramanian. 2010. Domain adaptation\nmeets active learning. In Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nPeter J. Rousseeuw. 1987. Silhouettes: A graphical aid\nto the interpretation and validation of cluster analy-\nsis. Journal of Computational and Applied Mathe-\nmatics, 20:53–65.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the Association for Computa-\ntional Linguistics.\nOzan Sener and Silvio Savarese. 2018. Active learn-\ning for convolutional neural networks: A core-set\napproach. In Proceedings of the International Con-\nference on Learning Representations.\nBurr Settles. 2009. Active learning literature survey.\nTechnical report, University of Wisconsin-Madison\nDepartment of Computer Sciences.\nBurr Settles. 2011. Closing the loop: Fast, interactive\nsemi-supervised annotation with queries on features\nand instances. In Proceedings of Empirical Methods\nin Natural Language Processing.\nBurr Settles, Mark Craven, and Soumya Ray. 2008.\nMultiple-instance active learning. In Proceedings\nof Advances in Neural Information Processing Sys-\ntems.\nClaude Elwood Shannon. 1948. A mathematical the-\nory of communication. Bell system technical jour-\nnal, 27.\nDan Shen, Jie Zhang, Jian Su, Guodong Zhou, and\nChew-Lim Tan. 2004. Multi-criteria-based active\nlearning for named entity recognition. In Proceed-\nings of the Association for Computational Linguis-\ntics.\nYanyao Shen, Hyokun Yun, Zachary C. Lipton,\nYakov Kronrod, and Animashree Anandkumar.\n2018. Deep active learning for named entity recogni-\ntion. In Proceedings of the International Conference\non Learning Representations.\nAditya Siddhant and Zachary C. Lipton. 2018. Deep\nbayesian active learning for natural language pro-\ncessing: Results of a large-scale empirical study. In\nProceedings of Empirical Methods in Natural Lan-\nguage Processing.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of Empirical Methods in\nNatural Language Processing.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the Asso-\nciation for Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, et al. 2019. What do you learn from con-\ntext? Probing for sentence structure in contextual-\nized word representations. In Proceedings of the\nInternational Conference on Learning Representa-\ntions.\nNenad Tomasev, Milos Radovanovic, Dunja Mladenic,\nand Mirjana Ivanovic. 2013. The role of hub-\nness in clustering high-dimensional data. IEEE\nTransactions on Knowledge and Data Engineering ,\n26(3):739–751.\nEllen V oorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R. Hersh, Kyle Lo,\nKirk Roberts, Ian Soboroff, and Lucy Lu Wang.\n2020. TREC-COVID: Constructing a pandemic in-\nformation retrieval test collection. arXiv preprint\narXiv:2005.04474.\nDan Wang and Yi Shang. 2014. A new active label-\ning method for deep learning. In International Joint\nConference on Neural Networks.\nXuezhi Wang, Tzu-Kuo Huang, and Jeff Schneider.\n2014. Active transfer learning under model shift.\nIn Proceedings of the International Conference on\nLearning Representations.\nZhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi\nWang. 2003. Representative sampling for text classi-\nﬁcation using support vector machines. In Proceed-\nings of the European Conference on Information Re-\ntrieval.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Proceedings of Ad-\nvances in Neural Information Processing Systems.\nMichelle Yuan, Mozhi Zhang, Benjamin Van Durme,\nLeah Findlater, and Jordan Boyd-Graber. 2020. In-\nteractive reﬁnement of cross-lingual word embed-\ndings. In Proceedings of Empirical Methods in Nat-\nural Language Processing.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Proceedings of Advances in Neural In-\nformation Processing Systems.\nYe Zhang, Matthew Lease, and Byron C. Wallace.\n2017. Active discriminative text representation\nlearning. In Association for the Advancement of Ar-\ntiﬁcial Intelligence.\nJingbo Zhu and Eduard Hovy. 2007. Active learning\nfor word sense disambiguation with methods for ad-\ndressing the class imbalance problem. In Proceed-\nings of Empirical Methods in Natural Language Pro-\ncessing.\nJingbo Zhu, Huizhen Wang, Tianshun Yao, and Ben-\njamin K. Tsou. 2008. Active learning with sampling\nby uncertainty and density for word sense disam-\nbiguation and text classiﬁcation. In Proceedings of\nInternational Conference on Computational Linguis-\ntics.\nA Appendices\n0 200 400 600 800 1000\n0\n0.2\n0.4\n0.6\n0.8F1 Score\nIMDB\nStrategy\nALPS ALPS-KMPP\n0 200 400 600 800 1000\nNumber of Labeled Sequences\nPubMed\nFigure 5: Comparing validation accuracy between us-\ning k-MEANS and k-MEANS ++ to select centroids in\nthe surprisal embeddings. Using k-MEANS reaches\nhigher accuracy.\nA.1 Token Masking\nIn our preliminary experiments on the validation\nset, we notice improvement in accuracy after pass-\ning in the original input with no masks (Table 4).\nThe purpose of the [MASK] token during pre-\ntraining is to train the token embeddings to learn\ncontext so that it can predict the token labels. Since\nwe are not training the token embeddings to learn\ncontext, masking the tokens does not help much for\nAL. We use AL for ﬁne-tuning, so the input should\nbe in the same format for AL and ﬁne-tuning. Oth-\nerwise, there is a mismatch between the two stages.\nA.2 Token Sampling for Evaluation\nWhen BERT evaluates MLM loss, it only focuses on\nthe masked tokens, which are from a 15% random\nsubsample of tokens in the sentence. We experi-\nment with varying this subsample percentage on\nthe validation set (Table 4). We try sampling 10%,\n15%, 20%, and 100%. Overall, we notice that mean\naccuracy are roughly the same, but variance in ac-\ncuracy across different runs is slightly higher for\npercentages other than 15%.\nAfter the second AL iteration, we notice that ac-\ncuracy mean and variance between the different to-\nken sampling percentages converge. So, the token\nsampling percentage makes more of a difference in\nearly stages of AL. Devlin et al. (2019) show that\n(a) Surprisal embeddings with k-MEANS ++ centers\n(b) Surprisal embeddings with k-MEANS centers\nFigure 6: T-SNE plots of surprisal embeddings for\nIMDB training data. The centers are either picked by\nk-MEANS ++ (right) or k-MEANS (left). There is less\noverlap between the centers with k-MEANS compared\nto k-MEANS ++. So, using k-MEANS is better for ex-\nploiting diversity in the surprisal embedding space.\nthe difference in accuracy between various mask\nstrategies is minimal for ﬁne-tuning BERT . We\nbelieve this can also be applied to what we have\nobserved for ALPS .\nA.3 k-MEANS vs. k-MEANS ++\nThe state-of-the-art baseline BADGE applies k-\nMEANS ++ on gradient embeddings to select points\nto query. Initially, we also use k-MEANS ++ on\nthe surprisal embeddings but validation accuracy is\nonly slightly higher than random sampling. Since\nk-MEANS ++ is originally an algorithm for robust\ninitialization of k-MEANS , we instead apply k-\nMEANS on the surprisal embeddings. As a result,\nwe see more signiﬁcant increase in accuracy over\nbaselines, especially for PubMed (Figure 5). Addi-\ntionally, the t-SNE plots show that k-MEANS selects\ncenters that are further apart compared to the ones\nchosen by k-MEANS ++ (Figure 6). This shows that\nk-MEANS can help sample a more diverse batch of\ndata.\nIMDB SST-2\nk= 100 k= 200 k= 100 k= 200\nALPS 0.60 ±0.03 0.69 ±0.04 0 .57 ±0.06 0 .64 ±0.04\nALPS -tokens-0.1 0.61 ±0.05 0.63 ±0.11 0 .56 ±0.07 0 .63 ±0.04\nALPS -tokens-0.2 0.55 ±0.07 0 .65 ±0.05 0.57 ±0.05 0.63 ±0.05\nALPS -tokens-1.0 0.59 ±0.05 0 .65 ±0.07 0 .56 ±0.05 0 .62 ±0.05\nALPS -masked 0.59 ±0.03 0 .63 ±0.09 0 .56 ±0.03 0 .60 ±0.02\nTable 4: Comparison of validation accuracy between the variants of ALPS to sample data for IMDB and SST-2 in\nthe ﬁrst two iterations. ALPS -tokens-pvaries the percentage pof tokens evaluated with MLM loss when computing\nsurprisal embeddings. ALPS -masked passes in the input with masks as originally done in pre-training. Overall, we\nobserve that ALPS has higher mean and smaller variance in accuracy.\nAG NEWS PUBMED\nALPS Jason Thomas matches a career-high with26\npoints and American wins its ﬁfth straight\nby beating visiting Ohio, 64-55, Saturday at\nBender Arena (Sports)\nThe results showed that physical activity and\nexercise capacity in the intervention group\nwas signiﬁcantly higher than the control\ngroup after the intervention . (results)\nSainsbury says it will take a 550 million\npound hit to proﬁts this year as it invests to\nboost sales and reverse falling market share\n(Business)\nFlumazenil was administered after the\ncompletion of endoscopy under sedation to\nreduce recovery time and increase patient\nsafety . (objective)\nRandom Bernhard Langer and Hal Sutton stressed\nthe importance of playing this year’s 135th\nRyder Cup . . . (Sports)\nThe study population consisted of 20 interns\nand medical students (methods)\nBLOOMFIELD TOWNSHIP, Mich. –\nWhen yesterday’s Ryder Cup pairings were\nannounced, Bernhard Langer knew his team\nhad been given an opportunity. (Sports)\nThe subject , health care provider , and re-\nsearch staff were blinded to the treatment .\n(methods)\nTable 5: Sample sentences from AG News and PubMed while using ALPS and Random in the ﬁrst iteration. For\nALPS , highlighted tokens are the ones that have a nonzero entry in the surprisal embedding. Compared to random\nsampling, ALPS samples sentences with more diverse content.\nA.4 Sample Sentences\nSection 6 quantitatively analyzes diversity of ALPS .\nHere, we take a closer look at the kind of sen-\ntences that are sampled by ALPS . Table 5 compares\nsentences that are chosen by ALPS and random\nsampling in the ﬁrst AL iteration. The tokens high-\nlighted are the ones evaluated with surprisal loss.\nRandom sampling can fall prey to data idiosyncra-\ncies. For example, AG News has sixty-two articles\nabout the German golfer Bernhard Langer, and ran-\ndom sampling picks multiple articles about him\non one of ﬁve runs. For PubMed, many sentences\nlabeled as “methods” are simple sentences with a\nshort, independent clause. While random sampling\nchooses many sentences of this form, ALPS seems\nto avoid this problem. Since the surprisal embed-\nding encodes the ﬂuctuation in information con-\ntent across the sentence, ALPS is less likely to re-\npeatedly choose sentences with similar patterns in\nsurprisal. This may possibly diversify syntactic\nstructure in a sampled batch.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7615305185317993
    },
    {
      "name": "Proxy (statistics)",
      "score": 0.6171679496765137
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6169513463973999
    },
    {
      "name": "Language model",
      "score": 0.6168217658996582
    },
    {
      "name": "Surprise",
      "score": 0.6148676872253418
    },
    {
      "name": "Machine learning",
      "score": 0.602405846118927
    },
    {
      "name": "Active learning (machine learning)",
      "score": 0.5560612678527832
    },
    {
      "name": "Annotation",
      "score": 0.47782281041145325
    },
    {
      "name": "Computation",
      "score": 0.46503040194511414
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.4644133746623993
    },
    {
      "name": "Scarcity",
      "score": 0.43256378173828125
    },
    {
      "name": "Semi-supervised learning",
      "score": 0.4139166474342346
    },
    {
      "name": "Natural language processing",
      "score": 0.33379244804382324
    },
    {
      "name": "Algorithm",
      "score": 0.09154552221298218
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66946132",
      "name": "University of Maryland, College Park",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ]
}