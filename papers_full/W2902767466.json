{
  "title": "CUNI Transformer Neural MT System for WMT18",
  "url": "https://openalex.org/W2902767466",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2161956338",
      "name": "Martin Popel",
      "affiliations": [
        "Charles University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W222053410",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W2251610689",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W2962801832",
    "https://openalex.org/W2917452219",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2740433069",
    "https://openalex.org/W2152400901",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2774875515",
    "https://openalex.org/W2982551930",
    "https://openalex.org/W2963216553"
  ],
  "abstract": "We describe our NMT system submitted to the WMT2018 shared task in news translation. Our system is based on the Transformer model (Vaswani et al., 2017). We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data. We apply a simple but effective filtering of the synthetic data. We pre-process the input sentences using coreference resolution in order to disambiguate the gender of pro-dropped personal pronouns. Finally, we apply two simple post-processing substitutions on the translated output. Our system is significantly (p < 0.05) better than all other English-Czech and Czech-English systems in WMT2018.",
  "full_text": "Proceedings of the Third Conference on Machine Translation (WMT), V olume 2: Shared Task Papers, pages 482–487\nBelgium, Brussels, October 31 - Novermber 1, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/W18-64051\nCUNI Transformer Neural MT System for WMT18\nMartin Popel\nCharles University, Faculty of Mathematics and Physics,\nInstitute of Formal and Applied Linguistics,\nPrague, Czechia\npopel@ufal.mff.cuni.cz\nAbstract\nWe describe our NMT system submitted to\nthe WMT2018 shared task in news translation.\nOur system is based on the Transformer model\n(Vaswani et al., 2017). We use an improved\ntechnique of backtranslation, where we iterate\nthe process of translating monolingual data in\none direction and training an NMT model for\nthe opposite direction using synthetic parallel\ndata. We apply a simple but effective ﬁltering\nof the synthetic data. We pre-process the input\nsentences using coreference resolution in or-\nder to disambiguate the gender of pro-dropped\npersonal pronouns. Finally, we apply two sim-\nple post-processing substitutions on the trans-\nlated output.\nOur system is signiﬁcantly ( p < 0.05) bet-\nter than all other English-Czech and Czech-\nEnglish systems in WMT2018.\n1 Introduction\nThe quality of Neural Machine Translation (NMT)\ndepends heavily on the amount and quality of the\ntraining parallel sentences as well as on various\ntraining tricks, which are sometimes surprisingly\nsimple and effective.\nIn this paper, we describe our NMT system\n“CUNI Transformer” (Charles University version\nof Transformer), submitted to the English→Czech\nand Czech→English news translation shared task\nwithin WMT2018. We describe ﬁve techniques,\nwhich helped to improve our system, so that it out-\nperformed all other systems in these two transla-\ntion directions: training data ﬁltering (Section 3),\nimproved backtranslation (Section 4), tuning two\nseparate models based on the original language of\nthe text to be translated (Section 5), coreference\npre-processing (Section 6) and post-processing us-\ning regular expressions (Section 7). Our sys-\ntem signiﬁcantly outperformed all other systems\nin WMT2018 evaluation (Section 8).\nsentence words (k)data set pairs (k) EN CS\nCzEng 1.7 57 065 618 424 543 184\nEuroparl v7 647 15 625 13 000\nNews Commentary v12 211 4 544 4 057\nCommonCrawl 162 3 349 2 927\nEN NewsCrawl 2016–17 47 483 934 981\nCS NewsCrawl 2007–17 65 383 927 348\ntotal 170 951 1 576 923 1 490 516\nTable 1: Training data sizes (in thousands).\n2 Experimental Setup\nOur training data is constrained to the data allowed\nin the WMT2018 shared task. Parallel (authentic)\ndata are: CzEng 1.7, Europarl v7, News Commen-\ntary v11 and CommonCrawl. In our backtrans-\nlation experiments (Section 4), we used synthetic\ndata translated by backtranslation of monolingual\ndata: Czech and (a subset of) English NewsCrawl\narticles. We ﬁltered out ca. 3% of sentences from\nthe synthetic data (Section 3). Data sizes are re-\nported in Table 1.\nNote that usually the amount of available mono-\nlingual data is orders of magnitude larger than the\navailable parallel data, but in our case it is compa-\nrable (58M parallel vs. 65M/48M monolingual).\nWe used all the Czech monolingual data allowed\nin the constrained task.\nWe used the Transformer self-attentional\nsequence-to-sequence model (Vaswani et al.,\n2017) implemented in the Tensor2Tensor\nframework.1 We followed the training setup\nand tips of Popel and Bojar (2018), but we\ntrained our models with the Adafactor opti-\nmizer (Shazeer and Stern, 2018) instead of the\ndefault Adam: We used T2T version 1.6.0,\ntransformer_big and hyper-parameters\nlearning_rate_schedule=rsqrt_decay,\n1 https://github.com/tensorflow/tensor2tensor\n482\nlearning_rate_warmup_steps=8000,\nbatch_size=2900, max_length=150,\nlayer_prepostprocess_dropout=0,\noptimizer=Adafactor. For decoding, we\nused alpha=1.\nWe stored model checkpoints each hour and av-\neraged the last eight checkpoints. We used eight\nGTX 1080 Ti GPUs.\n3 Training Data Filtering\nWe found out that the Czech monolingual data set\n(NewsCrawl 2007–2017) contains many English\nsentences. Those sentences were either kept un-\ntranslated or paraphrased when preparing the syn-\nthetic data with backtranslation. Thus the syn-\nthetic data included many English-English sen-\ntence pairs. Consequently, the synth-trained mod-\nels had a higher probability of keeping a sentence\nuntranslated.\nIn order to ﬁlter out the English sentences from\nthe Czech data, we kept only sentences containing\nat least one accented character. 2 We also ﬁltered\nout sentences longer than 500 characters from the\nsynthetic data. Most of these sentences would\nbe ignored anyway because we are training our\nTransformer with max_length=150, i.e. ﬁlter-\ning out sentences longer than 150 subwords (cf.\nPopel and Bojar, 2018, § 4.4). Sometimes a Czech\nsentence was much shorter than its English trans-\nlation (especially for the translations by Nema-\ntus2016) – because of ﬁller words repeated many\ntimes, which is a well-known problem of NMT\nsystems (e.g. Sudarikov et al., 2016). We ﬁltered\nout all sentences with a word (or a pair of words)\nrepeated more than twice using a regular expres-\nsion / (\\S+ ?\\S+) \\1 \\1 /. This way, we\nﬁltered out ca. 3% of sentences and re-trained our\nsystems. After this ﬁltering, we did not observe\nany untranslated sentences in the synth-trained\noutput.\n4 Improved Backtranslation\nSennrich et al. (2016b) introduced backtranslation\nas a simple way how to utilize target-language\nmonolingual data in NMT. The monolingual data\n2 m/[ˇešˇcˇržýáíéú˚ ud’t’ˇn]/i – this simple heuris-\ntics is surprisingly effective for Czech. In addition to English\nsentences, it ﬁlters out also some short Czech sentences, sen-\ntences in other languages (e.g. Chinese) and various “non-\nlinguistic” content, such as lists of football or stock-market\nresults.\nsets are translated (by a target-to-source MT sys-\ntem) to the source language, resulting in synthetic\nparallel data, which is used as additional training\ndata (in addition to authentic parallel) for the ﬁnal\n(source-to-target) NMT system.\nSennrich et al. (2017) compared two regimes\nof how to incorporate synthetic training data cre-\nated using backtranslation of monolingual data. In\nthe ﬁne-tuned regime, a system is trained ﬁrst on\nthe authentic parallel data and then after several\nepochs it is trained on a 1:1 mix of authentic and\nsynthetic data. In the mixed regime, the 1:1 mixed\ndata is used from the beginning of training. In\nboth cases, the 1:1 mix means shufﬂing the data\nrandomly at the sentence level, possibly oversam-\npling the smaller of the two data sources.\nWe used a third approach, termed concat\nregime, where the authentic and synthetic parallel\ndata are simply concatenated (without shufﬂing).\nWe observed that this regime leads to improve-\nments in translation quality relative to both mixed\nand ﬁne-tuned regimes, especially when check-\npoint averaging is used.\nFor obtaining the ﬁnal English→Czech system,\nwe iterated the backtranslation process:\n1. We downloaded the Nematus2016 models\ntrained by Sennrich et al. (2016a) using ﬁne-\ntuned backtranslation of English NewsCrawl\n2015 articles, which were translated “ with\nan earlier NMT model trained on WMT15\ndata” (Sennrich et al., 2016a). We used these\nNematus2016 models to translate Czech\nNewsCrawl 2007–2017 articles to English.\n2. We trained an English →Czech Transformer\non this data (ﬁltered as described in Sec-\ntion 3) using concat backtranslation with\ncheckpoint averaging. We used this\nTransformer model to translate English\nNewsCrawl 2016–2017 articles into Czech.\n3. We trained our Czech →English Transformer\nmodel (used for our WMT18 submission) on\nthis data using concat backtranslation with\naveraging. We translated Czech NewsCrawl\n2016–2017 articles into English using this\nsystem, producing a higher-quality synthetic\ndata than in step 1 (but smaller because of\nlack of time and resources).\n4. We trained our ﬁnal English →Czech system\n483\non this data, again using concat backtransla-\ntion with averaging.\nEach training (steps 2, 3 and 4) took eight days\non eight GPUs. Translating the monolingual data\nwith Nematus2016 (step 1) took about two weeks\nand with our Transformer models (steps 2 and 3)\ntook about ﬁve days. The ﬁnal model trained in\nstep 4 is +0.83 BLEU better than the model trained\nin step 2 without data ﬁltering, as measured on\nnewstest2017 (cf. Table 2).\n5 CZ/nonCZ Tuning\nIn WMT test sets since 2014, half of the sentences\nfor a language pair X-EN originate from English\nnews servers (e.g. bbc.com) and the other half\nfrom X-language news servers. All WMT test\nsets include the server name for each document\nin metadata, so we were able to split our test set\n(and dev set newstest2013) into two parts: CZ\n(for Czech-domain articles, i.e. documents with\ndocid containing “.cz”) and nonCZ (for non-\nCzech-domain articles). We noticed that when\ntraining on synthetic data, the model performs\nmuch better on the CZtest set than on the nonCZ\ntest set. When trained on authentic data, it is the\nother way round. Intuitively, this makes sense:\nThe target side of our synthetic data are original\nCzech sentences from Czech newspapers, simi-\nlarly to the CZ test set. In our authentic data,\nover 90% of sentences were originally written in\nEnglish about “non-Czech topics” and translated\ninto Czech (by human translators), similarly to the\nnonCZtest set. There are two closely related phe-\nnomena: a question of domain (topics) in the train-\ning data and a question of so-called translationese\neffect, i.e. which side of the parallel training data\n(and test data) is the original and which is the\ntranslation.\nBased on these observations, we prepared aCZ-\ntuned model and a nonCZ-tuned model. Both\nmodels were trained in the same way, they differ\nonly in the number of training steps. For the CZ-\ntuned model, we selected a checkpoint with the\nbest performance on wmt13-CZ (Czech-origin\nportion of newstest2013), which was at 774k\nsteps. Similarly, for the nonCZ-tuned model, we\nselected the checkpoint with the best performance\non wmt13-nonCZ, which was at 788k steps.\nNote that both the models were trained jointly in\none experiment, just selecting checkpoints at two\ndifferent moments.\n6 Coreference Pre-processing\nIn Czech, as a pro-drop language, it is common\nto omit personal pronouns in subject positions.\nUsually, the information about gender and num-\nber of the subject is encoded in the verb inﬂec-\ntion, but present-tense verbs have the same form\nfor the feminine and masculine gender. For ex-\nample, “Není doma” can mean either “ She is not\nhome” or “He is not home”. When translating such\nsentences from Czech to English, we must use the\ncontext of neighboring sentences in a given docu-\nment, in order to disambiguate the gender and se-\nlect the correct translation. However, our Trans-\nformer system (similarly to most current NMT\nsystems) translates each sentence independently of\nother sentences. We observed that in practice it al-\nways prefers the masculine gender if the informa-\ntion about gender could not be deduced from the\nsource sentence.\nWe implemented a simple pre-processing of the\nCzech sentences, which are then translated with\nour Czech→English Transformer system – we in-\nserted pronoun ona (she), where it was “miss-\ning”. We analyzed the source Czech documents\nin the Treex NLP framework (Popel and Žabokrt-\nský, 2010), which integrates a coreference re-\nsolver (Novák, 2017). We found sentences where\na female-gender pronoun subject was dropped and\nthe coreference link was pointing to a different\nsentence (usually the previous one). We restricted\nthe insertion of ona only to the cases in which the\nantecedent in the coreference chain represents a\nhuman (i.e. excluding grammatical-only female\ngender of inanimate objects and animals). We\nused a heuristic detection of human entities, which\nis integrated in Treex.\nThis preprocessing affected only 1% of sen-\ntences in our nestest2017 dev set and for most of\nthem the English translation was improved (ac-\ncording to our judgment), although the overall\nBLEU score remained the same. We consider\nthis solution as a temporary workaround before\ndocument-level NMT (e.g. Kuang et al., 2017) is\navailable in T2T. That said, the advantage of the\ndescribed preprocessing is that it can be applied to\nany (N)MT system – without changing its archi-\ntecture and even without retraining it.\n7 RegEx Post-processing\nWe applied two simple post-processings to the\ntranslations, using regular expressions.\n484\nEnglish→Czech BLEU BLEU chrF2\nsystem cased uncased cased\nNematus (Sennrich et al., 2016b) 22.80 23.29 0.5059\nT2T (Popel and Bojar, 2018) 23.84 24.40 0.5164\nour mixed backtranslation 24.85 (+1.01) 25.33 0.5267\nour concat backtranslation 25.77 (+0.92) 26.29 0.5352\n+ higher quality backtranslation 26.60 (+0.83) 27.10 0.5410\n+ CZ/nonCZ tuning 26.81 (+0.21) 27.30 0.5431\nTable 2: Automatic evaluation on (English →Czech) newstest2017. The three scores in parenthesis show\nBLEU difference relative to the previous line.\nWe deleted phrases repeated more than twice\n(immediately following each other); we kept just\nthe ﬁrst occurrence. We considered phrases of one\nup to four words. With the training-data ﬁlter-\ning described in Section 3, less than 1% sentences\nneeded this post-processing.\nFor English →Czech, we converted quotation\nsymbols in the translations to the correct-Czech\n„lower and upper“ quotes using two regexes:\ns/(ˆ|[ ({[])(\"|,,|’’|‘‘)/$1„/g and\ns/(\"|’’)($|[ ,.?!:;)}\\]])/“$2/g. In\nEnglish, the distinction between \"straight\" and\n“curly” quotes is considered as a rather typograph-\nical (or style-related) issue. However, in Czech,\na mismatch between lower (opening) and upper\n(closing) quotes is considered as an error in for-\nmal writing.\n8 Evaluation\n8.1 WMT2017 Evaluation\nTable 2 evaluates the relative improvements de-\nscribed in Sections 4 and 5 on English →Czech\nnewstest2017 and compares the results with the\nWMT2017 winner – Nematus (Sennrich et al.,\n2016b), and with the result of Popel and Bojar\n(2018) – T2T without any backtranslation.\nThe three reported automatic metrics are: case-\nsensitive (cased) BLEU, case-insensitive (un-\ncased) BLEU and a character-level metric chrF2\n(Popovi´c, 2015). We compute all the three metrics\nwith sacreBLEU (Post, 2018). The reported cased\nand uncased variants of BLEU differ also in the\ntokenization. The cased variant uses the default\n(ASCII-only) for better comparability with the re-\nsults at http://matrix.statmt.org. The\nuncased variant uses the international tokeniza-\ntion, which has higher correlation with humans\n(Macháˇcek and Bojar, 2013). The sacreBLEU sig-\nnatures of the three metrics are:\n•BLEU+case.mixed+lang.en-cs+\nnumrefs.1+smooth.exp+\ntest.wmt17+tok.13a,\n•BLEU+case.lc+lang.en-cs+\nnumrefs.1+smooth.exp+\ntest.wmt17+tok.intland\n•chrF2+case.mixed+lang.en-cs+\nnumchars.6+numrefs.1+\nspace.False+test.wmt17.\nWe performed a small-scale manual evaluation\non newstest2017 and noticed that in many cases\nthe human reference translation is actually worse\nthan our Transformer output. Thus the results of\nBLEU (or any other automatic metric comparing\nsimilarity with references) may be misleading.\n8.2 WMT2018 Evaluation\nTable 3 the reports results of all English ↔Czech\nsystems submitted to WMT2018, according to\nboth automatic and manual evaluation. For the\nautomatic evaluation, we use the same three met-\nrics as in the previous section (just with wmt18\ninstead of wmt17). For the manual evaluation,\nwe report the reference-based direct assessment\n(refDA) scores, provided by the WMT organizers.\nOur Transformer is the best system in\nEnglish→Czech and Czech →English WMT2018\nnews task. It is signiﬁcantly ( p < 0.05) better\nthan the second-best system – UEdin NMT, in\nboth translation directions and both according to\nBLEU bootstrap resampling test (Koehn, 2004)\nand according to refDA Wilcoxon rank-sum test.\n9 Conclusion\nWe have presented ﬁve simple but effective tech-\nniques for improving (N)MT quality. All ﬁve tech-\n485\nEnglish→Czech Czech →English\nBLEU BLEU chrF2 refDA BLEU BLEU chrF2 refDA\nsystem uncased cased cased Ave. % uncased cased cased Ave. %\nour Transformer 26.82 26.01 0.5372 67.2 35.64 33.91 0.5876 71.8\nUEdin NMT 24.30 23.42 0.5166 60.6 34.12 33.06 0.5801 67.9\nOnline-B 20.16 19.45 0.4854 52.1 33.58 31.78 0.5736 66.6\nOnline-A 16.84 15.74 0.4584 46.0 28.47 26.78 0.5447 62.1\nOnline-G 16.33 15.11 0.4560 42.0 25.20 22.53 0.5310 57.5\nTable 3: WMT2018 automatic (BLEU, chrF2) and manual (refDA = reference-based direct assessment) evaluation\non newstest2018.\nniques can be applied to virtually any NMT sys-\ntem. According to the preliminary results of the\nmanual evaluation, the ﬁnal translation quality is\ncomparable to or even better than the quality of\nhuman references.\nAs a future work, we would like to assess the\nrelative improvement of each of the ﬁve tech-\nniques based on manual evaluation (because au-\ntomatic single-reference evaluation is not reliable\nwhen the MT quality is near to the quality of ref-\nerence translations).\nAcknowledgements\nThe work described in this paper has been sup-\nported by the “NAKI II - Systém pro trvalé\nuchování dokumentace a prezentaci historichých\npramen˚ u z období totalitních režim˚ u”, project No.\nDG16P02B048, funded by the Ministry of Culture\nof the Czech Republic.\nReferences\nPhilipp Koehn. 2004. Statistical signiﬁcance tests for\nmachine translation evaluation. In Proceedings of\nEMNLP, volume 4, pages 388–395.\nShaohui Kuang, Deyi Xiong, Weihua Luo, and\nGuodong Zhou. 2017. Cache-based Document-\nlevel Neural Machine Translation. CoRR,\narXiv/1711.11221.\nMatouš Macháˇcek and Ondˇrej Bojar. 2013. Results of\nthe WMT13 Metrics Shared Task. InProceedings of\nthe Eighth Workshop on Statistical Machine Trans-\nlation, pages 45–51, Soﬁa, Bulgaria. Association for\nComputational Linguistics.\nMichal Novák. 2017. Coreference resolution system\nnot only for czech. In Proceedings of the 17th con-\nference ITAT 2017: Slovenskoˇ ceský NLP workshop\n(SloNLP 2017) , pages 193–200, Praha, Czechia.\nCreateSpace Independent Publishing Platform.\nMartin Popel and Ond ˇrej Bojar. 2018. Training Tips\nfor the Transformer Model. The Prague Bulletin of\nMathematical Linguistics, 110:43–70.\nMartin Popel and Zdenˇek Žabokrtský. 2010. TectoMT:\nModular NLP Framework. Advances in Natural\nLanguage Processing, pages 293–304.\nMaja Popovi ´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. ACL.\nMatt Post. 2018. A Call for Clarity in Reporting BLEU\nScores. CoRR, arXiv/1804.08771.\nRico Sennrich, Alexandra Birch, Anna Currey, Ulrich\nGermann, Barry Haddow, Kenneth Heaﬁeld, An-\ntonio Valerio Miceli Barone, and Philip Williams.\n2017. The University of Edinburgh’s Neural MT\nSystems for WMT17. In Proceedings of the Sec-\nond Conference on Machine Translation, Volume 2:\nShared Task Papers, pages 389–399, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Edinburgh Neural Machine Translation Sys-\ntems for WMT16. In Proceedings of the First\nConference on Machine Translation , pages 371–\n376, Berlin, Germany. Association for Computa-\ntional Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Improving Neural Machine Translation\nModels with Monolingual Data. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Pa-\npers), pages 86–96, Berlin, Germany. Association\nfor Computational Linguistics.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive Learning Rates with Sublinear Memory\nCost. CoRR, arXiv/1804.04235.\nRoman Sudarikov, Martin Popel, Ond ˇrej Bojar,\nAljoscha Burchardt, and Ond ˇrej Klejch. 2016. Us-\ning MT-ComparEval. In Translation Evaluation:\n486\nFrom Fragmented Tools and Data Sets to an Inte-\ngrated Ecosystem, pages 76–82, Portorož, Slovenia.\nLREC.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 6000–6010. Curran Asso-\nciates, Inc.\n487",
  "topic": "Coreference",
  "concepts": [
    {
      "name": "Coreference",
      "score": 0.8964036703109741
    },
    {
      "name": "Transformer",
      "score": 0.7954519391059875
    },
    {
      "name": "Computer science",
      "score": 0.7904451489448547
    },
    {
      "name": "Czech",
      "score": 0.7523484230041504
    },
    {
      "name": "Natural language processing",
      "score": 0.5722636580467224
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5219266414642334
    },
    {
      "name": "Machine translation",
      "score": 0.47986358404159546
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4192301630973816
    },
    {
      "name": "Process (computing)",
      "score": 0.41762399673461914
    },
    {
      "name": "Synthetic data",
      "score": 0.4120883345603943
    },
    {
      "name": "Resolution (logic)",
      "score": 0.2901546061038971
    },
    {
      "name": "Programming language",
      "score": 0.15018245577812195
    },
    {
      "name": "Linguistics",
      "score": 0.08642169833183289
    },
    {
      "name": "Voltage",
      "score": 0.08466634154319763
    },
    {
      "name": "Engineering",
      "score": 0.07415845990180969
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    }
  ],
  "cited_by": 36
}