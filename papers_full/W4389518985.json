{
  "title": "Revisiting Large Language Models as Zero-shot Relation Extractors",
  "url": "https://openalex.org/W4389518985",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2119349622",
      "name": "Guozheng Li",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A1984679711",
      "name": "Peng Wang",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2895328075",
      "name": "Wenjun Ke",
      "affiliations": [
        "Southeast University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4303648559",
    "https://openalex.org/W2892094955",
    "https://openalex.org/W4385573991",
    "https://openalex.org/W4285246823",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3154945374",
    "https://openalex.org/W4366400290",
    "https://openalex.org/W3034891697",
    "https://openalex.org/W2962958286",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2250521169",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W3172497214",
    "https://openalex.org/W4385570635",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W4320339764",
    "https://openalex.org/W2155454737",
    "https://openalex.org/W4321855256",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4404784155",
    "https://openalex.org/W4321524373",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2984452801",
    "https://openalex.org/W3198490223",
    "https://openalex.org/W4281488715",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W4377164385",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W4385573954",
    "https://openalex.org/W1604644367",
    "https://openalex.org/W3016077617",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4389520264"
  ],
  "abstract": "Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (SumAsk) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the following findings: (i) SumAsk consistently and significantly improves LLMs performance on different model sizes, benchmarks and settings; (ii) Zero-shot prompting with ChatGPT achieves competitive or superior results compared with zero-shot and fully supervised methods; (iii) LLMs deliver promising performance in extracting overlapping relations; (iv) The performance varies greatly regarding different relations. Different from small language models, LLMs are effective in handling challenge none-of-the-above (NoTA) relation.",
  "full_text": "Revisiting Large Language Models as Zero-shot Relation Extractors\nGuozheng Li‚ô¢ and Peng Wang‚ô¢‚ô£(\u0000 ) and Wenjun Ke‚ô¢‚ô£\n‚ô¢ School of Computer Science and Engineering, Southeast University, China\n‚ô£ Key Laboratory of New Generation Artificial Intelligence Technology and Its\nInterdisciplinary Applications (Southeast University), Ministry of Education, China\n{liguozheng, pwang, kewenjun}@seu.edu.cn\nAbstract\nRelation extraction (RE) consistently involves\na certain degree of labeled or unlabeled data\neven if under zero-shot setting. Recent studies\nhave shown that large language models (LLMs)\ntransfer well to new tasks out-of-the-box sim-\nply given a natural language prompt, which pro-\nvides the possibility of extracting relations from\ntext without any data and parameter tuning.\nThis work focuses on the study of exploring\nLLMs, such as ChatGPT, as zero-shot relation\nextractors. On the one hand, we analyze the\ndrawbacks of existing RE prompts and attempt\nto incorporate recent prompt techniques such\nas chain-of-thought (CoT) to improve zero-\nshot RE. We propose the summarize-and-ask\n(SUMASK) prompting, a simple prompt recur-\nsively using LLMs to transform RE inputs to\nthe effective question answering (QA) format.\nOn the other hand, we conduct comprehensive\nexperiments on various benchmarks and set-\ntings to investigate the capabilities of LLMs\non zero-shot RE. Specifically, we have the\nfollowing findings: (i) SUMASK consistently\nand significantly improves LLMs performance\non different model sizes, benchmarks and set-\ntings; (ii) Zero-shot prompting with ChatGPT\nachieves competitive or superior results com-\npared with zero-shot and fully supervised meth-\nods; (iii) LLMs deliver promising performance\nin extracting overlapping relations; (iv) The\nperformance varies greatly regarding different\nrelations. Different from small language mod-\nels, LLMs are effective in handling challenge\nnone-of-the-above (NoTA) relation.\n1 Introduction\nRelation extraction (RE) aims to identify the re-\nlationships between entities in texts, and plays an\nimportant role in information extraction (IE). Most\nof existing RE methods (Zeng et al., 2014; dos San-\ntos et al., 2015) require large amounts of labeled\ntraining data which is labor-intensive and time-\nconsuming in practice. Hence, extracting relations\nfrom texts using zero or few-shot methodologies\nhas garnered significant scholarly attention (Han\net al., 2018; Chen and Li, 2021).\nRecent studies (Wei et al., 2022; Wang et al.,\n2023b) on large-scale pre-trained language mod-\nels (LLMs), such as GPT-3 (Brown et al., 2020),\ndemonstrate that LLMs perform well in various\ndownstream tasks without any training or fine-\ntuning but only with a few examples as instruc-\ntions, which is called in-context learning. However,\nthere is currently no consensus on whether LLMs\nare good few-shot information extractors (Agrawal\net al., 2022; Jimenez Gutierrez et al., 2022). Dif-\nferent with some other tasks, RE is more challeng-\ning for LLMs because the structured data contain-\ning multiple dependent elements are difficult to\nextract directly and accurately. Although recent\nstudies (Wang et al., 2023a) indicate that some\nconventional fine-tuning models still outperform\nLLMs in few-shot RE tasks, we still want to ex-\nplore whether LLMs can achieve competitive per-\nformance compared to fine-tuning models.\nSimilar to few-shot learning, LLMs also show\npromising performance on zero-shot settings (Ko-\njima et al., 2022). Recent work for zero-shot\nRE via prompting LLMs has achieved remarkable\nprogress. QA4RE (Zhang et al., 2023) is a multiple-\nchoice question answering prompt format, in which\neach relation is transformed into a template and\nLLMs are expected to predict only a single let-\nter. This prompt is simple but requires manually\ncrafted templates and unable to deal with overlap-\nping relations, which motivates us to find more\ngeneral and effective prompts. ChatIE (Wei et al.,\n2023) transforms the zero-shot IE task into a multi-\nturn question answering problem with a two-stage\nframework, even surpasses some full shot models\non several datasets. Nonetheless, ChatIE still per-\nforms worse than the state-of-the-art and is only\nevaluated on limited benchmarks. Thus it is still\nunclear how to improve extracting performance by\ndesigning effective prompts and whether LLMs\nare good zero-shot relation extractors. To this end,\nwe revisit and investigate the potential of LLMs in\nzero-shot RE on the following research questions:\n‚Ä¢ (RQ1) How does LLMs perform on RE incor-\nporating existing prompt techniques?\n‚Ä¢ (RQ2) How does LLMs perform on zero-shot\nrelation classification?\n‚Ä¢ (RQ3) How does LLMs perform on zero-shot\noverlapping relation extraction?\nPrevious work (Ma et al., 2023; Wang et al.,\n2023a) fails to achieve promising results on RE\nsince black box LLMs such as ChatGPT are dif-\nficult to ensure the reliability of outputs. To an-\nswer the first question, we investigate the feasibil-\nity of incorporating recent prompt techniques to\nimprove the reliability of extracted results. For\nexample, chain-of-thought (CoT) prompting (Wei\net al., 2022) improves the reliability of model out-\nput by providing intermediate reasoning steps. Ac-\ntive prompting (Diao et al., 2023) is an uncertainty-\nbased active learning method to quantify the un-\ncertainty so as to select the most uncertain outputs.\nSpecifically, we propose the summarize-and-ask\n(SUMASK) prompting, which decomposed RE into\ntwo subtasks: text summarization (Liu and Lap-\nata, 2019) and question answering (Chen et al.,\n2017a). We further introduce an uncertainty esti-\nmation method to approximately characterize out-\nput probabilities of LLMs, which yields substantial\nimprovements compared to VANILLA prompting.\nTo answer the last two questions, we evaluate\nLLMs on both zero-shot relation classification and\noverlapping relation extraction. And six RE bench-\nmarks are used for evaluation: (1) FewRel (Han\net al., 2018) and Wiki-ZSL (Chen and Li, 2021)\nfor comparision with zero-shot RE methods; (2)\nTACRED (Zhang et al., 2017), TACREV (Alt et al.,\n2020) and Re-TACRED (Stoica et al., 2021) for\ncomparision with fully supervised methods; (3)\nNYT (Riedel et al., 2010) for evaluating over-\nlapping relation extraction. Experimental results\ndemonstrate that LLMs achieve promising results\nin all experimental settings. In summary, the con-\ntributions of this work are three-fold:\n‚Ä¢ We propose the SUMASK prompting and\nevaluate the effectiveness of this method.\nSUMASK consistently and significantly im-\nproves LLMs performance by 5.2% - 48.3%\nin F1-score compared to VANILLA prompting\nw.r.t. diverse experimental settings.\n‚Ä¢ We comprehensively evaluate the capabilities\nof LLMs on zero-shot relation classification.\nLLMs achieve competitive or superior results\ncompared with state-of-the-art zero-shot and\nfully supervised methods. Notably, ChatGPT\nwith SUMASK prompting outperforms the\nstate-of-the-art fully supervised method on\nTACRED by an average of 2.8% micro-F1.\n‚Ä¢ We investigate the capabilities of LLMs in ex-\ntracting overlapping relations. LLMs deliver\nconsistently promising performance encoun-\ntering different number of triples and various\noverlapping patterns.\n2 Related Work\nFew and Zero-shot Relation Extraction Few-\nshot RE (Han et al., 2018) aims to predict novel\nrelations by exploring a few labeled instances. Pro-\ntotypical networks (Snell et al., 2017) are widely\nused and combined with pre-trained language mod-\nels (Devlin et al., 2019) in few-shot settings to\nachieve impressive results. To be capable of ex-\ntracting relations that were not specified in advance,\nzero-shot RE (Levy et al., 2017) is proposed to in-\nvent new models to predict new relations. How-\never, existing zero-shot methods (Chen and Li,\n2021) still requires much labeled data. Recent stud-\nies (Zhang et al., 2023; Wei et al., 2023) leverage\nthe LLMs with zero-shot prompting to extract re-\nlations from texts without any labeled samples in\nadvance. But it is still unclear whether LLMs are\ngood zero-shot relation extractors by carefully de-\nsigned prompts. Thus this work aims to investigate\nthe capabilities of LLMs in zero-shot RE.\nLarge Language Models and Prompting Be-\nsides the ‚Äúpre-train and fine-tune‚Äù paradigm (Liu\net al., 2023), pre-trained LLMs possess character-\nistics that are advantageous for few-shot (Brown\net al., 2020) and zero-shot (Kojima et al., 2022)\nlearning, whereby appropriate prompts are used\nto effectively guide the model towards generat-\ning desired task outputs, thus beginning an era of\n‚Äúpre-train and prompt‚Äù (Liu et al., 2021). Prior\nworks (Zhao et al., 2021; Liu et al., 2021) note the\nsensitivity of prompting under slight modifications.\nEmpirical results demonstrate that answering the\nrestrictive prompts is challenging due to biases ac-\nquired during pre-training (Zhao et al., 2021; Arora\net al., 2023). In this study, we evaluate different\nprompt formats tailored to the particularities of RE\nand propose SUMASK prompting which outper-\nforms VANILLA prompting by a large margin.\n3 Problem Definition\nPrevious zero-shot RE (Chen and Li, 2021) only\ninvolves single relation classification. We extend\nthis zero-shot setting to multiple entities and re-\nlations. Given the pre-defined relation set R =\n{r1, r2, ..., rN } and the sentence S containing the\nentity set E = {e1, e2, ..., eM }, we aim to extract\nall the relations between these entities composing\nthe relational triples set Z = {(ei, rk, ej)}, where\nN denotes the number of relations, M represents\nthe number of entities, and ei, ej ‚àà E, rk ‚àà R.\n4 Prompt Design\n4.1 V ANILLA Prompting\nPrevious work (Ma et al., 2023; Wang et al., 2023a)\nclaims that LLMs achieve poor results on IE tasks\nsuch as RE. We argue that one important reason\nwhy LLMs underperform the state-of-the-art is the\npoor prompt design, as different prompts towards\nsame tasks can cause large variations in the model\npredictions (Zhao et al., 2021; Arora et al., 2023).\nFigure 1 illustrates the most direct and common\nprompt strategy which directly asks LLMs to ex-\ntract relation labels from text through instructions.\nHowever, we empirically find that this approach\nis ineffective because it makes LLMs to accom-\nplish three no-trivial reasoning processes in only\none step: (i) Extracting the relation semantics be-\ntween the subject and object in the sentence; (ii)\nUnderstanding the semantics of each relation la-\nbel; (iii) Matching the relation semantics between\nthe entities and the given relation labels. Consis-\ntent with existing findings (Jimenez Gutierrez et al.,\n2022; Ma et al., 2023; Wang et al., 2023a), LLMs\nusing VANILLA prompting are unable to achieve\nsatisfactory performance on zero-shot RE.\n4.2 S UMASK Prompting\nDue to the difficulty of LLMs in completing three\nreasoning processes in one step, we leverage the\nidea of CoT (Wei et al., 2022) and suggest decom-\nposing this step to artificially guide LLMs in under-\nstanding and reasoning. To classify the relations\nbetween the subject and object, a simple method\nis to sequentially ask LLMs whether each relation\nexists between two entities. For the three reasoning\nGiven the possible relations: [member of, field of work, work location, ..., father, sibling]. What are the relations between the subject entity and the object entity expressed by the sentence?Sentence: Savi was born in Pisa, son of Gaetano Savi, professor of Botany at the University of Pisa.Subject: Gaetano SaviObject: BotanyRelation: field of work\nFigure 1: Illustration of the VANILLA prompting. The\noutput of LLMs is highlighted in color.\nprocesses mentioned above, we design the prompt\nillustrated in Figure 2 as three steps: (i) Summa-\nrize the relations between subject and object given\nthe [INPUT] so as to extract the relation semantics\nbetween the two and obtain the intermediate re-\nsult [SUMMARIZATION]; (ii) Ask LLMs to generate\nthe yes/no questions based on possible triples so\nas to transform the abstract relation labels to the\nnatural relation descriptions and obtain the interme-\ndiate result [QUESTION]; (iii) Ask LLMs to answer\nthe [QUESTION] based on the [SUMMARIZATION]\nto match the relation semantics between the enti-\nties and relations. Then we get the final [ANSWER].\nUncertainty Estimation Generally, relation clas-\nsification assumes that only one correct relation\nis extracted. However, SUMASK prompt possibly\nobtains multiple ‚Äúyes‚Äù while querying all the rela-\ntions in R. Therefore, the final predicted relation is\nrequired to select from multiple candidates. Given\nthe sentence S, the subject es and object eo, the\npredicted relation is obtained by:\nr = arg max\nri,ri‚ààR\np (ri | S, es, eo) (1)\nThen we aim to transform the multi classifica-\ntion form into multiple binary classification forms.\nHence, we define a random variable ¬Ør as follows:\n¬Ør =\nÔ£±\nÔ£¥Ô£¥Ô£¥Ô£¥Ô£≤\nÔ£¥Ô£¥Ô£¥Ô£¥Ô£≥\n( 1, 0, ...,0 ) r = r1\n( 0, 1, ...,0 ) r = r2\n...\n( 0, 0, ...,1 ) r = rN\n(2)\nHere we make an assumption that only one positive\nlabel exists among N binary classification. We\ndenote the intermediate results summarization and\nquestion as si and qi corresponding to relation ri.\nSavi was born in Pisa, son of Gaetano Savi, professor of Botanyat the University of Pisa. [INPUT]\nSummarize the relations between SUBJECT and OBJECTfrom context. Context: [INPUT]Summarization: [SUMMARIZATION]\nRewrite the triple as a yes/no question. Triple: ( SUBJECT, ùëü!, OBJECT) Question: [QUESTION]Answer the question from context with yes/no. Context: [SUMMARIZATION]Question: [QUESTION]Answer: [ANSWER]\n[1] Gaetano Savi is a professor of Botany at the University of Pisa....[k] At the University of Pisa, Gaetano Savi serves as a professor specializing in Botany.\n[1] Is Gaetano Savi specifically engaged in the field of Botany for his work?...[k] Does Gaetano Savi's professional occupation revolve around the field of Botany?\n[1] Yes.[2] No.[3] Yes....[k] Yes.\n[1] Gaetano Savi holds the title of a professor in the field of Botany at the University of Pisa....[k] Gaetano Savi is a professor of Botany at the University of Pisa.\n[1] Does Gaetano Savi reside in a place related to Botany? ...[k] Is Gaetano Savi's residence associated with the field of Botany?\n[1] No.[2] Yes.[3] No....[k] No.\n...\n...\n...\nùëùùëìùëñùëíùëôùëëùëúùëìùë§ùëúùëüùëò=ùëùùë†ùëÜ,ùëí!,ùëí\",ùëü#$%&'_\"#_)\"*+)ùëùùëûùëÜ,ùëí!,ùëí\",ùëü#$%&'_\"#_)\"*+)ùëùÃÖùëü\",-%*_*%&.,$\"/=0ÃÖùëü#$%&'_\"#_)\"*+=1)ùëùÃÖùëü#$%&'_\"#_)\"*+=1ùë†,ùëû)\nùëùùëüùëíùë†ùëñùëëùëíùëõùëêùëí=ùëùùë†ùëÜ,ùëí!,ùëí\",ùëü*%!$'%/0%)ùëùùëûùëÜ,ùëí!,ùëí\",ùëü*%!$'%/0%)ùëùÃÖùëü\",-%*_*%&.,$\"/=0ÃÖùëü*%!$'%/0%=1)ùëùÃÖùëü*%!$'%/0%=1ùë†,ùëû)=0\n=ùëùùë†ùëÜ,ùëí!,ùëí\",ùëü#$%&'_\"#_)\"*+)ùëùùëûùëÜ,ùëí!,ùëí\",ùëü#$%&'_\"#_)\"*+)ùëùÃÖùëü#$%&'_\"#_)\"*+=1ùë†,ùëû)\nMajority V ote\nMajority V ote\nYes\nNo\n‚ÅÑ1‚àù\tùëàùë†ùëÜ,ùëí!,ùëí\",ùëü#$%&'_\"#_)\"*+)ùëàùëûùëÜ,ùëí!,ùëí\",ùëü#$%&'_\"#_)\"*+)ùëàÃÖùëü#$%&'_\"#_)\"*+=1ùë†,ùëû)\nFigure 2: Illustration of the SUMASK prompting. The outputs of LLMs are highlighted in color. The probability of\nrelation ‚Äúresidence‚Äù is 0 because the system answers ‚Äúno‚Äù via majority vote. To estimate the uncertainty of relation\n‚Äúfield of work‚Äù, we generate k [SUMMARIZATION], [QUESTION], [ANSWER] representations, respectively. Then we\ncalculate the dispersion degree among these representations to approximate the uncertainty.\nThen the probability of relation ri is obtained by:\np (ri) =p (¬Ør | S, es, eo, ri)\n=p (¬Ør |si, qi) p (si, qi | S, es, eo, ri)\n=p (¬Ør‚àíi = 0 | ¬Øri = 1) p (¬Øri = 1 |si, qi)\np (qi | S, es, eo, ri) p (si | S, es, eo, ri)\n(3)\nwhere ri = 1 indicates that LLMs answer ‚Äúyes‚Äù\nbased on the summarization and question of re-\nlation i. Based on the above one positive label\nassumption, we have:\np (¬Ør‚àíi = 0 | ¬Øri = 1) =\n(\n1 ¬Ø ri = 1\n0 ¬Ø ri = 0 (4)\nThe final predicted relation is selected from all the\ncandidate relations with the max probability:\nr = arg max\nri,ri‚ààR\np (¬Øri = 1 |si, qi)\np (qi | S, es, eo, ri) p (si | S, es, eo, ri)\n(5)\nUnfortunately, it is difficult to get the conditional\nprobability of each step in LLMs. For instance,\nthe ‚Äúgpt-3.5-turbo‚Äù model only provides the final\nnatural text output without any logit or probability.\nTo this end, we introduce an uncertainty estimation\nmethod to approximately characterize conditional\nprobabilities. Finding the relation r that satisfies\nequation 5 is equivalent to:\nr = arg min\nri,ri‚ààR\nU (¬Øri = 1 |si, qi)\nU (qi | S, es, eo, ri) U (si | S, es, eo, ri)\n(6)\nwhere U(X|Y ) represents the uncertainty of the\nrandom variable X under the known random vari-\nable Y . Therefore, the relation with the smallest\nuncertainty is selected as final prediction.\nInspired by Diao et al. (2023), we consider mea-\nsuring the uncertainty using the dispersion degree\namong k generated answers A = {a1, ..., ak}\nas shown in Figure 2. Specifically, we feed an-\nswers A into a pre-trained Sentence-BERT en-\ncoder (Reimers and Gurevych, 2019) to generate\nthe answer representations Z = {z1, ...,zk}. Then\nthe uncertainty is calculated by:\nu = 1\nk ‚àí 1\nkX\ni=1\nd (zi, 1\nk\nkX\nj=1\nzj) (7)\nwhere d(¬∑) function measures the distance between\ntwo representations. After obtaining the uncer-\ntainty of each step, we select the relation r via:\nr = arg min\nri,ri‚ààR\nu1 ¬∑ u2 ¬∑ u3 (8)\nu1 ‚àù U (si | S, es, eo, ri) (9)\nu2 ‚àù U (qi | S, es, eo, ri) (10)\nu3 ‚àù U (¬Øri = 1 |si, qi) (11)\nWe adopt the majority vote (Wang et al., 2023b)\nto determine the yes/no answer in last step. If the\nsystem answers ‚Äúno‚Äù with every relation ri ‚àà R,\nthe prediction is NoTA. For overlapping relations,\nwe simply consider all relations that answer with\n‚Äúyes‚Äù as predictions.\nEntity-Relation Mapping Obviously, asking\nLLMs for each relation is inefficient. Inspired\nby Li et al. (2022), we adopt the entity-relation\nmapping mechanism to deal with relation redun-\ndancy. Specifically, when the entity type is de-\ntermined, the relations that possibly related to it\nare also determined, so most impossible relations\nare discarded in advance. Note that the VANILLA\nprompting also adopts this simple strategy. This\nsimple mechanism not only improves efficiency but\nalso benefits overall performance.\n5 Experiments\n5.1 Datasets\nSimple Relation Classification We evaluate\nLLMs in zero-shot simple relation classification\non FewRel (Han et al., 2018), Wiki-ZSL (Chen\nand Li, 2021), TACRED (Zhang et al., 2017),\nTACREV (Alt et al., 2020) and Re-TACRED (Sto-\nica et al., 2021). For FewRel and Wiki-ZSL, we\nfollow the previous work (Chen and Li, 2021)\nand randomly select m = 5, 10, 15 unseen rela-\ntions. For TACRED, TACREV and Re-TACRED,\nwe evaluate the LLMs on its test set. Note that\nthere is no entity type information provided in\nFewRel and Wiki-ZSL while entity types become\navailable in TACRED, TACREV and Re-TACRED.\nWe adopt the precision, recall and macro-F1 for\nFewRel and Wiki-ZSL (Chen and Li, 2021), while\nmicro-F1 is used for TACRED, TACREV and Re-\nTACRED (Zhou and Chen, 2021).\nOverlapping Relation Extraction We adopt the\nNYT (Riedel et al., 2010) to test the ability of ex-\ntracting overlapping relations. For NYT, we as-\nsume the entities and their types in the sentence are\navailable and models only extract the overlapping\nrelations given the entities in its test set. We use\nmicro-F1 for evaluation. NYT is only evaluated\non LLMs as existing baselines have not considered\nthis multiple entities and relations zero-shot setting.\nTo keep OpenAI API costs under control, we\nrandomly select 1,000 samples in the correspond-\ning test set according to the proportion of samples\nin each relation class. Specifically, the number of\nsamples corresponding to each relation in FewRel\nis the same. Note that 78.56% of samples in TA-\nCRED test set belongs to NoTA relation, while it is\n57.91% in Re-TACRED. We provide the statistics\nof the datasets in Appendix A.\n5.2 Baselines\nZero-shot Baselines For FewRel and Wiki-\nZSL, we choose R-BERT (Wu and He, 2019),\nESIM (Chen et al., 2017b), CIM (Rockt√§schel\net al., 2016) and ZS-BERT (Chen and Li, 2021)\nas the zero-shot RE baselines. Note that Relation-\nPrompt (Chia et al., 2022) uses the seq2seq-based\nmodels to generate pseudo data of unseen relations\nto fine-tune the model. And recent method RE-\nMatching (Zhao et al., 2023) requires elaborated re-\nlation descriptions to achieve superior performance\nbut the open source code is not yet available. Thus\nthe two methods are not discussed in this study.\nSupervised Baselines For TACRED, TACREV\nand Re-TACRED, fully supervised models\nsuch as PA-LSTM (Zhang et al., 2017), C-\nGCN (Zhang et al., 2018), SpanBERT (Joshi\net al., 2020), LUKE (Yamada et al., 2020),\nNLI-DeBERTa (Sainz et al., 2021), SuRE-\nPEGASUS (Lu et al., 2022) and DeepStruct (Wang\net al., 2022) are selected to compare with our\nzero-shot prompt based methods. We also test\nNLI-DeBERTa, SuRE-PEGASUS, DeepStruct\nand QA4RE (Zhang et al., 2023) under zero-shot\nsetting to investigate the NoTA relation impact.\nLLMs Baselines We investigate open source\nLLMs such as GPT-J (Wang and Komatsuzaki,\n2021), BLOOM (Scao et al., 2022) and T0 (Sanh\net al., 2022) with SUMASK prompting with other\nstate-of-the-art models in zero-shot settings. For\nthe parameter scale, we choose GPT-J-6B 1,\nBLOOM-7.1B 2 and T0pp-11B 3 for experiments.\nFor ChatGPT (Ouyang et al., 2022), we use the\n1https://huggingface.co/EleutherAI/gpt-j-6b\n2https://huggingface.co/bigscience/bloom\n3https://huggingface.co/bigscience/T0\nDatasets FewRel Wiki-ZSL\nm=5 m=5\nP R F1 P R F1\nR-BERT 42.19 48.61 45.17 39.22 43.27 41.15\nESIM 56.27 58.44 57.33 48.58 47.74 48.16\nCIM 58.05 61.92 59.92 49.63 48.81 49.22\nZS-BERT 76.96 78.86 77.90 71.54 72.39 71.96\nGPT-J 40.75 45.63 43.05 40.23 46.94 43.33\nBLOOM 43.62 48.15 45.77 41.97 45.38 43.61\nT0 43.05 54.97 48.29 42.16 53.92 47.32\nVANILLA 67.41 72.97 70.08 64.47 70.83 67.50\nSUMASK 78.27 72.55 75.30 75.64 70.96 73.23\nm=10 m=10\nP R F1 P R F1\nR-BERT 25.52 33.02 28.20 26.18 29.69 27.82\nESIM 42.89 44.17 43.52 44.12 45.46 44.78\nCIM 47.39 49.11 48.23 46.54 47.90 45.57\nZS-BERT 56.92 57.59 57.25 60.51 60.98 60.74\nGPT-J 28.37 32.27 30.19 27.13 32.76 29.68\nBLOOM 29.28 33.81 31.38 29.45 34.19 31.64\nT0 29.87 34.26 31.91 30.18 35.48 32.62\nVANILLA 42.48 46.26 44.29 41.83 46.22 43.92\nSUMASK 64.77 60.94 62.80 62.31 61.08 61.69\nm=15 m=15\nP R F1 P R F1\nR-BERT 16.95 19.37 18.08 17.31 18.82 18.03\nESIM 29.15 31.59 30.32 27.31 29.62 28.42\nCIM 31.83 33.06 32.43 29.17 30.58 29.86\nZS-BERT 35.54 38.19 36.82 34.12 34.38 34.25\nGPT-J 20.36 35.00 25.74 20.83 34.37 25.94\nBLOOM 22.62 36.45 27.92 22.37 34.26 27.07\nT0 24.05 36.83 29.09 23.16 34.90 27.84\nVANILLA 25.71 27.77 26.70 23.17 27.82 25.28\nSUMASK 44.76 41.13 42.87 43.55 40.27 41.85\nTable 1: Main results on FewRel and Wiki-ZSL. In\norder to reduce the effect of experimental noise, the un-\nseen label selection process is repeated for five different\nrandom seeds to produce the test set. The results of the\nbaselines are retrieved from Chen and Li (2021).\n‚Äúgpt-3.5-turbo-0301‚Äù, which is the most capable\nGPT-3.5 model and optimized for chat. We denote\nthe combination of ChatGPT and two prompts as\nVANILLA and SUMASK for brevity. Similar to Ko-\njima et al. (2022), after the model outputs a text,\nour method picks up only the part of the answer\ntext that first satisfies the answer format. The im-\nplementation details are provided in Appendix B.\n5.3 Relation Classification Results\nMain Results The results by varying m unseen\nrelations on FewRel and Wiki-ZSL are summa-\nrized in Table 1. Generally, LLMs with zero-shot\nprompting achieve competitive results compared to\nexisting zero-shot RE methods over two datasests\nwhen targeting at different numbers of unseen rela-\nDatasets TACRED TACREV Re-TACRED\nPA-LSTM 65.1 73.3 ‚Ä° 79.4‚Ä†\nC-GCN 66.3 74.6 ‚Ä° 80.3‚Ä†\nSpanBERT 70.8 78.0 ‚àó 85.3‚Ä†\nLUKE 72.7 80.6 ‚Ä° 90.3‚Ä°\nNLI-DeBERTa 73.9 - -\nSuRE-PEGASUS 75.1 83.3 -\nDeepStruct 76.8 - -\nGPT-J 44.4 40.7 38.3\nBLOOM 46.5 41.2 40.8\nT0 59.0 57.5 55.5\nVANILLA 31.3 30.4 28.0\nSUMASK 79.6 75.1 73.8\nTable 2: Micro-F1 score on TACRED, TACREV and Re-\nTACRED.‚àó marks re-implemented results from Alt et al.\n(2020). ‚Ä† marks re-implemented results from Stoica et al.\n(2021). ‚Ä° marks re-implemented results from Zhou and\nChen (2021). Others are retrieved from original papers.\nMethods Micro-P Micro-R Micro-F1 Macro-F1\nSuRE-PEGASUS 13.8 51.7 21.8 14.9\nNLI-DeBERTa 42.9 76.9 55.1 55.0\nDeepStruct‚Ä† 32.7 40.6 36.2 32.8\nQA4RE 47.7 78.6 59.4 58.9\nVANILLA 33.8 39.2 36.3 27.4\nSUMASK 62.2 53.8 57.7 57.9\nTable 3: NoTA-excluded 41-class micro-F1 and NoTA-\nincluded 42-class macro F1 on TACRED.‚Ä† marks our re-\nimplementation results. The rest results of the baselines\nare retrieved from Zhang et al. (2023).\ntions. Specially, the proposed SUMASK prompting\nmakes ChatGPT deliver superior results compared\nto ZS-BERT in most cases. As m increases, it is\nstraightforward that models are difficult to predict\nthe right relation since the possible choices have\nincreased. The superiority of SUMASK gets more\nsignificant when the number of unseen relations\nincreases while VANILLA suffers grave declines.\nSuch results not only validate the effectiveness of\nproposed prompting, but indicate SUMASK is less\nsensitive to the number of relations compared to\nbaselines. Moreover, GPT-J-6B, BLOOM-7.1B\nand T0-11B with SUMASK exceed GPT-3.5-175B\nwith VANILLA , and match the performance of pre-\nvious text entailment models ESIM and CIM.\nThe main results on TACRED, TACREV and\nRe-TACRED are shown in Table 2. Compared\nto fully supervised methods, zero-shot prompting\nwith LLMs still show competitive results. Notably,\nChatGPT with SUMASK prompting outperforms\nthe state-of-the-art fully supervised method Deep-\nStruct on TACRED by an average of 2.8% micro-\nBest performance Accuracy Worst performance Accuracy\nvoice type 97.4 language of work or name 13.0\noccupation 97.1 tributary 22.7\ncontains administrative territorial entity 95.4 residence 31.6\nparticipant of 95.1 mouth of the watercourse 33.3\ncrosses 95.0 screenwriter 42.4\nlocated in the administrative territorial entity 94.9 performer 44.0\nleague 94.4 head of government 44.3\nconstellation 93.9 father 45.3\ncompetition class 93.1 distributor 48.4\nheritage designation 93.0 located on terrain feature 49.6\nTable 4: Top-10 relations with best (left) and worst (right) performance.\nF1. The interesting finding is that the performance\nof LLMs decreases on TACREV and Re-TACRED\nwhile fine-tuned models steadily improves. The rea-\nson might be that the high proportion of NoTA in\nTACRED makes zero-shot prompting with LLMs\nsurpass fully supervised methods. It is difficult for\nconventional models to form a good NoTA repre-\nsentation (Han et al., 2020). Jimenez Gutierrez\net al. (2022) also demonstrate that the earlier in-\nferior performance of LLMs on RE tasks can be\nlargely attributed to their inability to handle the\nNoTA relation. To this end, we provide an evalua-\ntion of zero-shot methods on NoTA relation. Fol-\nlowing previous work (Sainz et al., 2021; Zhang\net al., 2023), we report the NoTA-excluded micro-\nF1 and NoTA-included macro-F1 to investigate the\nextracting ability of normal and NoTA relation.\nThe NoTA relation results are shown in Table 3.\nFirst, SUMASK prompting is not prominent in 41\nsemantic relations, which demonstrate the high\nmicro-F1 score is mainly due to the high portion\nof NoTA relation. Second, SUMASK also achieves\nsignificant improvement like QA4RE in NoTA-\nincluded metrics compared to the small LM-based\nNLI methods. For VANILLA , excluding NoTA rela-\ntion brings better results. This further demonstrates\nthe sensitivity of prompting and the effectiveness\nof proposed prompting.\nRelation Specific Analysis Due to biases ac-\nquired during pre-training, LLMs have different\nabilities to understand different relations, which\nleads to varying levels of extraction results. We\nanalyze the performance differences through ex-\nperiments on 80-relation dataset FewRel. Specifi-\ncally, under the SUMASK framework, we ask the\nLLMs whether the answer of question generated\nby golden triple is ‚Äúyes‚Äù. Then we adopt the ac-\ncuracy metric to evaluate the performance of each\nrelation. Finally, we select 10 relations with the\nSimilar Random Dissimilar\n0.4\n0.5\n0.6\n0.7\n0.8Micro-F1\nR-BERT\nESIM\nCIM\nZS-BERT\nGPT-J\nBLOOM\nT0\nVanilla\nSumask\nSimilar Random Dissimilar\n0.3\n0.4\n0.5\n0.6\n0.7Micro-F1\nR-BERT\nESIM\nCIM\nZS-BERT\nGPT-J\nBLOOM\nT0\nVanilla\nSumask\nFigure 3: Performance comparison between five similar,\nrandom and dissimilar relations.\nbest and worst performance, as shown in Figure 4.\nSurprisingly, the accuracy difference between the\nbest (‚Äúvoice type‚Äù) and the worst (‚Äúlanguage of\nwork or name‚Äù) relation is 84.4%. We provide the\ndetailed analysis in Appendix C.\nThe semantic similarity between relations in the\nembedding space greatly impacts the zero-shot RE\nperformance. Following Chen and Li (2021), we\nselect five semantically distant relations and the\nother five relations that possess similar semantics\nto evaluate on our baselines, illustrated in Figure 3.\nObviously, dissimilar relations lead to better re-\nsults. First, when enhanced by SUMASK prompt-\ning, LLMs delivers more stable results because\nof the smaller performance gap between three set-\ntings. Second, the text-entailment based methods\nare less affected by similar relations compared to\nembedding-based models such as R-BERT and ZS-\nBERT. Because the predictions by text entailment\nbased methods ESIM, CIM and SUMASK prompt-\ning do not resort to similarity search.\nPrompt Strategy Analysis We study the effec-\ntiveness of proposed SUMASK prompting. We con-\nduct the ablation study about summarization gener-\nation, question generation and uncertainty estima-\ntion. Specifically, we omit the summarization pro-\ncess, replace the LLMs generated questions with\npre-defined question templates (Appendix D) and\nDatasets FewRel TACRED\nm=5 m=10 m=15 NoTA w/o NoTA\nSUMASK 75.3 62.8 42.9 79.6 51.6\nSUMASK w/o Sum. 67.6 58.1 40.4 76.4 48.5\nSUMASK w/o Ask. 71.4 56.8 37.7 78.7 54.3\nSUMASK w/o Unc. 38.8 29.6 23.0 76.3 25.2\nTable 5: Results of ablation study.\nGround Truth\n0\n200\n400\n600\n800Uncertainty\nFewRel\nWiki-ZSL\nGround Truth\n0\n200\n400\n600\n800\n1000\n1200Uncertainty\nTACRED\nRe-TACRED\nFigure 4: The correlations between uncertainty estima-\ntion and ground truth.\nrandomly select the relation from candidates with-\nout uncertainty estimation, respectively. Table 5\nshows the ablation study results. Summarization\nconsistently improves the overall performance un-\nder different settings, which indicates that incor-\nporating reasoning steps before predicting relation\nis reasonable. Compared to pre-defined templates,\nLLMs generated questions may not necessarily be\nthe best choice. Manually designed template en-\nables the semantic description of relations more\naccurate, but our simple method is convenient and\nrequires no external interference. Uncertainty es-\ntimation shows the significant impact on perfor-\nmance. Note that SUMASK still achieves 76.3%\nF1 on TACRED, because the uncertainty estima-\ntion has no impact on NoTA relation theoretically.\nTo understand the rationality of uncertainty esti-\nmation, we select 500 samples from each datasets\nto illustrate the correlations between uncertainty\nestimation and ground truth. Intuitively, golden\nrelations have relatively low uncertainty. Figure 4\nshows that most golden relations correspond to low\nuncertainty, while only a few correspond to large\nuncertainty, which is consistent with our intuition.\n5.4 Overlapping Relation Extraction Results\nThe overlapping relation extraction results are illus-\ntrated in Table 6. VANILLA prompting is difficult\nto handle the overlapping relations as LLMs al-\nways tend to output only one relation. In contrast,\nSUMASK prompting is transferable and consistent\non LLMs with different sizes. Note that different\nfrom the relation classification results, the recall\nof SUMASK is higher than its precision. Because\nMethods P R F1 N=1 N=2 N=3 N=4 N>=5\nGPT-J 31.4 53.6 39.6 39.5 36.9 39.3 39.8 30.3\nBLOOM 35.2 56.9 43.5 39.2 38.4 42.1 44.6 33.6\nT0 39.6 57.3 46.8 44.3 41.9 47.2 48.3 35.7\nVANILLA 20.4 16.5 18.2 31.3 22.7 16.3 12.8 7.7\nSUMASK 55.7 78.3 65.1 65.6 62.5 66.7 70.8 59.5\nTable 6: Main results on NYT.\nSEP NEO SEO EPO\n0\n10\n20\n30\n40\n50\n60Micro-F1\nGPT-J\nBLOOM\nT0\nVanilla\nSumask\nFigure 5: F1-score of extracting overlapping relations\nfrom sentences with different overlapping patterns.\nregarding all the candidate relations as predictions\nbrings many false positives. Setting thresholds for\nuncertainty estimation might be a feasible solution.\nTo further study the capability of LLMs in ex-\ntracting overlapping relations, we conduct experi-\nments on different types of sentences. We split the\nsentences into five classes and use N to denote the\nnumber of relational triples in a sentence. Again,\nthe SUMASK prompting achieves good perfor-\nmance over all five classes. Overlapping relational\ntriples are summarized into four patterns: SEP (Sin-\ngleEntityPair), NEO (NoEntityOverlap), SEO (Sin-\ngleEntityOverlap) and EPO ( EntityPairOverlap).\nFigure 5 shows that the performance of most base-\nlines on four patterns presents a decreasing trend,\nreflecting the increasing difficulty of extracting re-\nlational triples from sentences with different over-\nlapping patterns, encouraging more consistent and\neffective methods in the future research.\n6 Conclusion\nThis work provides a comprehensive study on zero-\nshot RE with prompting-based LLMs. Besides\nthe VANILLA prompting, we introduce a novel\nSUMASK prompting to fully explore the power\nof LLMs. Our experiments on six benchmarks\ndemonstrate the capability of LLMs in zero-shot\nRE. Furthermore, we are able to answer the three\naforementioned questions. Recent prompt tech-\nniques such as CoT significantly improve zero-shot\nRE prompting. Properly instructed LLMs not only\ndeliver competitive or superior results compared to\nstate-of-the-art relation classification models, but\nalso are promising for zero-shot overlapping RE.\nLimitations\nWe only carry out comprehensive experiments on\nzero-shot RE without few-shot and domain-specific\nexploration. It is still unclear what are the capabili-\nties of LLMs on domain-specific datasets and how\nmuch performance could be improved by few-shot\nprompting. Our limited budget also restricted our\nstudy to a small set of prompt styles. It is possible\nthat having a larger prompt design search space\ncould narrow the gap between models fine-tuning\nand LLMs in-context learning.\nEthics Statement\nIn this work, we investigate the capability of LLMs\non the important and fundamental task of zero-shot\nrelation extraction. We do not anticipate any ethical\nissues regarding the topics of this research.\nAcknowledgement\nThis work was supported by National Science Foun-\ndation of China (Grant Nos.62376057).\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\nmodels are few-shot clinical information extractors.\nIn EMNLP.\nChristoph Alt, Aleksandra Gabryszak, and Leonhard\nHennig. 2020. TACRED revisited: A thorough eval-\nuation of the TACRED relation extraction task. In\nACL.\nSimran Arora, Avanika Narayan, Mayee F Chen, Lau-\nrel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-\neric Sala, and Christopher R√©. 2023. Ask me any-\nthing: A simple strategy for prompting language mod-\nels. In ICLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS.\nChih-Yao Chen and Cheng-Te Li. 2021. ZS-BERT:\nTowards zero-shot relation extraction with attribute\nrepresentation learning. In NAACL-HLT.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017a. Reading wikipedia to answer open-\ndomain questions. In ACL.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017b. Enhanced LSTM\nfor natural language inference. In ACL.\nYew Ken Chia, Lidong Bing, Soujanya Poria, and Luo\nSi. 2022. Relationprompt: Leveraging prompts to\ngenerate synthetic data for zero-shot relation triplet\nextraction. In Findings of ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models. arXiv preprint\narXiv:2302.12246.\nC√≠cero dos Santos, Bing Xiang, and Bowen Zhou. 2015.\nClassifying relations by ranking with convolutional\nneural networks. In ACL.\nXu Han, Tianyu Gao, Yankai Lin, Hao Peng, Yaoliang\nYang, Chaojun Xiao, Zhiyuan Liu, Peng Li, Maosong\nSun, and Jie Zhou. 2020. More data, more rela-\ntions, more context and more openness: A review\nand outlook for relation extraction. arXiv preprint\narXiv:2004.03186.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,\nZhiyuan Liu, and Maosong Sun. 2018. FewRel: A\nlarge-scale supervised few-shot relation classification\ndataset with state-of-the-art evaluation. In EMNLP.\nBernal Jimenez Gutierrez, Nikolas McNeal, Clayton\nWashington, You Chen, Lang Li, Huan Sun, and\nYu Su. 2022. Thinking about GPT-3 in-context learn-\ning for biomedical IE? think again. In Findings of\nEMNLP.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64‚Äì77.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In NeurIPS.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettle-\nmoyer. 2017. Zero-shot relation extraction via read-\ning comprehension. In CoNLL.\nGuozheng Li, Xu Chen, Peng Wang, Jiafeng Xie, and\nQiqing Luo. 2022. Fastre: Towards fast relation\nextraction with convolutional encoder and improved\ncascade binary tagging framework. In IJCAI.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1‚Äì35.\nYang Liu and Mirella Lapata. 2019. Text summarization\nwith pretrained encoders. In EMNLP.\nKeming Lu, I Hsu, Wenxuan Zhou, Mingyu Derek Ma,\nMuhao Chen, et al. 2022. Summarization as indirect\nsupervision for relation extraction. In Findings of\nEMNLP.\nYubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.\n2023. Large language model is not a good few-shot\ninformation extractor, but a good reranker for hard\nsamples! arXiv preprint arXiv:2303.08559.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn EMNLP.\nSebastian Riedel, Limin Yao, and Andrew McCallum.\n2010. Modeling relations and their mentions without\nlabeled text. In ECML PKDD.\nTim Rockt√§schel, Edward Grefenstette, Karl Moritz\nHermann, Tom√°≈° KoÀácisk`y, and Phil Blunsom. 2016.\nReasoning about entailment with neural attention. In\nICLR.\nOscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, An-\nder Barrena, and Eneko Agirre. 2021. Label verbal-\nization and entailment for effective zero-and few-shot\nrelation extraction. In EMNLP.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2022. Multitask prompted training enables\nzero-shot task generalization. In ICLR.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ¬¥c, Daniel Hesslow, Roman\nCastagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon,\nMatthias Gall√©, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nJake Snell, Kevin Swersky, and Richard Zemel. 2017.\nPrototypical networks for few-shot learning. In\nNIPS.\nGeorge Stoica, Emmanouil Antonios Platanios, and\nBarnab√°s P√≥czos. 2021. Re-tacred: Addressing short-\ncomings of the tacred dataset. In AAAI.\nBen Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6\nbillion parameter autoregressive language model.\nChenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong,\nJie Tang, and Dawn Song. 2022. Deepstruct: Pre-\ntraining of language models for structure prediction.\nIn Findings of ACL.\nXiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze\nChen, Yuansen Zhang, Rui Zheng, Junjie Ye,\nQi Zhang, Tao Gui, et al. 2023a. Instructuie: Multi-\ntask instruction tuning for unified information extrac-\ntion. arXiv preprint arXiv:2304.08085.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2023b. Self-consistency\nimproves chain of thought reasoning in language\nmodels. In ICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. In NeurIPS.\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,\nYufeng Chen, Meishan Zhang, et al. 2023. Zero-\nshot information extraction via chatting with chatgpt.\narXiv preprint arXiv:2302.10205.\nShanchan Wu and Yifan He. 2019. Enriching pre-\ntrained language model with entity information for\nrelation classification. In CIKM.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. Luke: Deep con-\ntextualized entity representations with entity-aware\nself-attention. In EMNLP.\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\nand Jun Zhao. 2014. Relation classification via con-\nvolutional deep neural network. In COLING.\nKai Zhang, Bernal Jim√©nez Guti√©rrez, and Yu Su. 2023.\nAligning instruction tasks unlocks large language\nmodels as zero-shot relation extractors. In Findings\nof ACL.\nYuhao Zhang, Peng Qi, and Christopher D Manning.\n2018. Graph convolution over pruned dependency\ntrees improves relation extraction. In EMNLP.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling. In\nEMNLP.\nJun Zhao, Wenyu Zhan, Xin Zhao, Qi Zhang, Tao Gui,\nZhongyu Wei, Junzhe Wang, Minlong Peng, and\nMingming Sun. 2023. Re-matching: A fine-grained\nsemantic matching method for zero-shot relation ex-\ntraction. In ACL.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nICML.\nWenxuan Zhou and Muhao Chen. 2021. An improved\nbaseline for sentence-level relation extraction. arXiv\npreprint arXiv:2102.01373.\nA Dataset Statistics\nThe statistics of the datasets are shown in Table 7\nand Table 8.\nDataset # instances # entities # relations\nFewRel 56,000 72,954 80\nWiki-ZSL 94,383 77,623 113\nTable 7: Statistics of FewRel and Wiki-ZSL.\nDataset # train # dev # test # relations\nTACRED 68,124 22,631 15,509 42\nTACREV 68,124 22,631 15,509 42\nRe-TACRED 58,465 19,584 13,418 40\nNYT 56,196 5,000 5,000 24\nTable 8: Statistics of TACRED, TACREV , Re-TACRED\nand NYT.\nB Implementation Details\nFor the hyper-parameters of SUMASK prompting,\nwe set the number of generated answers k as 5, and\nwe use the ‚Äúbert-large-nli-mean-tokens‚Äù version\nof Sentence-BERT encoder to generate the answer\nrepresentations. For open source LLMs GPT-J,\nBLOOM and T0, we set the max generated length\nas 128 and the temperature as 0.3. Note that the\nresults of VANILLA prompting with open source\nLLMs are not discussed in our paper because they\nall achieve near-0 performance. Due to the high\nnoise content in the output of LLMs, we pick up\nonly the part of the answer text that first satisfies\nthe answer format to alleviate the unexpected be-\nhaviors. For gpt-3.5-turbo-0301, we set the max\nlength as 256 and the temperature as 0.7 according\nto official default setting. We treat the outputs of\nChatGPT as valid results without post-processing.\nC Relation Specific Analysis\nWe provide accuracy results for all relations in\nFewRel. The results are summarized in Table 9.\nAnd we provide several case studies to analyze the\nperformance of ChatGPT on different relations.\n‚Äúlanguage of work or name‚Äù We observe two\nimportant reasons for the poor performance of this\nrelation, shown in Table 10. On the one hand,\nChatGPT sometimes misunderstand the semantics\nof entities or relations, which leads to generated\nquestions deviating from the original meaning ex-\npressed. For example, Elizabeth is an English\nfemale name but ChatGPT treats the name as a\nperson (Case 2), which also indicates the draw-\nback of this method that the generated questions\nmight be unexpected without providing the context\nor template. This also highlights the importance\nof incorporating entity types into relation extrac-\ntion. On the other hand, prompting is a brittle\nprocess wherein small modifications to the prompt\ncan cause large variations in the model predictions.\nFor example, we use ‚ÄúAnswer the question from\ncontext‚Äù rather ‚ÄúAnswer the question from con-\ntext with yes/no‚Äù to expect that ChatGPT could not\nonly give the ‚Äúyes/no‚Äù answer but also provide the\nreason for its judgment. We achieve the expected\nresults in most relations. However, the answers\ncorresponding to the relation ‚Äúlanguage of work\nor name‚Äù frequently do not contain ‚Äúyes/no‚Äù while\nexpress positive (Case 3), which makes automatic\nevaluation difficult. Therefore, the specific form\nof the statement answer is important, otherwise it\nmay lead to unreasonable evaluation. Moreover,\nthe annotation errors (Case 4) of this relation also\nlead to biased and unreliable evaluation.\n‚Äútributary‚Äù and ‚Äúmouth of the watercourse‚Äù\nChatGPT performs poorly in both two relations.\nFirst, these two relations have very similar se-\nmantics because they are reciprocal in FewRel, as\nshown in Case 1. Unfortunately, ChatGPT fre-\nquently reverses the subject and object correspond-\ning to these two relations during question genera-\ntion step. Specifically, ChatGPT treats the triple\n(lisava river, tributary, natra river) as (lisava river,\ntributary of, natra river). This phenomenon high-\nlights the advantage of manually crafted question\ntemplates and persuades us to provide few-shot\ndemonstrations to generate reliable questions. Sec-\nond, we also find plenty of annotation errors of two\nrelations because of distant supervision, shown in\nCase 2. We can see that ChatGPT provides the\ncorrect judgments and reasons for this case, which\nmakes it possible for LLMs to become reliable an-\nnotation inspectors.\nD Question Templates\nIn ablation study, we replace the LLMs generated\nquestions with pre-defined question templates. For\nFewRel, we simply define a template that forms\nwith ‚ÄúThe relation between ‚Äòsubject‚Äô and ‚Äòobject‚Äô\nis ‚Äòrelation name‚Äô. Yes or No?‚Äù. For TACRED,\nwe follow Zhang et al. (2023) to use the templates\nshown in Table 12.\nE Discussions\nThe empirical validation of SUMASK prompting\nThe underlying operational mechanisms intrinsic\nto SUMASK prompting rely on the strong capabil-\nities of LLMs as zero-shot reasoners. Following\nthe prompt instructions, LLMs pay attention to the\nentities of interest, infer and summarize the rela-\ntions between them. To the best of our knowledge,\nthe logical reasoning faculties of LLMs are not\nexplicitly utilized in previous work of zero-shot re-\nlation extraction. From raw text inputs to extracted\nrelation labels, this process lacks intermediate rea-\nsoning steps. We decompose the zero-shot relation\nextraction into three steps to make LLMs sensitize\nto the semantic understanding and logical reason-\ning. With the proper instruction for summarization,\nLLMs are able to perform logical reasoning on\nspecific entities and obtain relations between them.\nThe LLMs can automatically do this via prompting,\nbut the small fine-tuned model cannot. SUMASK\nelicits the logical reasoning ability inside LLMs\nfor relation deduction. The ablation results also\nshow that without the summarization step, the over-\nall performance drops 2.5% - 7.7% F1 on FewRel\nand TACRED under different settings. The exper-\nimental results on overlapping relation extraction\nalso demonstrate the superior of SUMASK prompt-\ning over VANILLA prompting.\nHere we provide a case study on NYT. The in-\nput prompt is: Summarize the relations between\n‚ÄúCambodia‚Äù and ‚ÄúPenh‚Äù from context. Context:\nHomage to Cambodia was performed at Chakto-\nmuk Conference Hall in Phnom Penh on Oct. 21\n, attended by the king. Summarization:\nThen the response from ChatGPT is \"Phnom\nPenh\" is the capital city of \"Cambodia,\" where\nthe event \"Homage to Cambodia\" took place at\nthe \"Chaktomuk Conference Hall\" on October\n21. Obviously, the first sentence \"Phnom Penh\"\nis the capital city of \"Cambodia,\"generated by\nLLMs clearly elucidates the relationship between\ntwo entities, facilitating the subsequent processes.\nThe advantages of SUMASK prompting Gen-\nerally, the SUMASK prompting does not require\nany sort of prompt engineering or template writing\nto start using. And this is one of our contribution\npoints. Specially, not all relations can be accurately\ndescribed by templates because the description of\nrelations may vary across different entities. For\ninstance, the relation language of work or name\nin datast FewRel is hard to describe by a single\ntemplate, while this relation can not only describe\nlanguage versions of some literary works, but also\ndescribe what language a name belongs to. Con-\nsider the following two templates: (1) The lan-\nguage of subject is object. and (2) subject is a\nname in object. The triple (Elizabeth, language of\nwork or name, English)satisfy the second template\nbut deliver confusions in the first template, while\nthe triple (The Lord of the Rings, language of work\nor name, English)only satisfy the first template.\nSUMASK does not require any sort of prompt\nengineering or template writing to start using.\nThe complexity of SUMASK prompting Typi-\ncally, the SUMASK prompting method suffers from\nrelatively high inference complexity as it needs to\nenumerate all possible triples to obtain summariza-\ntions, questions, and answers, for k times. Suppose\nwe have n samples to be extracted and r candidate\nrelations, the total complexity is O(k √ó r √ó n).\nFirst, using entity types to discard the most irrele-\nvant relations is a useful method, which achieves\nless complexity O(k √ó ÀÜr √ó n) where ÀÜr represents\nthe maximum value of the mapped relation can-\ndidate set and much less than r. Second, we can\ncertainly ask multiple samples (not exceeding the\nmodel maximum length) to LLMs at one time to im-\nprove inference speed. Suppose we concatenate k\nsamples in a prompt, then the complexity becomes\nO(ÀÜr √ó n). More efficient zero-shot prompting for\nrelation extraction is worth exploring in the future.\nUncertainty estimation in all LLMs Uncer-\ntainty estimation is based on an assumption that\nthe outputs of LLMs would be stabler with the\npredictions equivalent to the ground truth. The effi-\nciency of using logits to generate the probabilities\nof intermediate results is relatively low, because\nwe are required to obtain the probability of tokens\nat each position. Moreover, it is highly susceptible\nto extreme values. For example, if a token with\na low probability is sampled during sampling pro-\ncess, it will affect the probability value of the entire\nsentence. In addition, due to the generated long\ntext sequence, the difference of probability values\nbetween generated sentences is not obvious, mak-\ning it difficult to choose the relation with the high-\nest probability. Using SUMASK with uncertainty\nestimation brings better outcomes and we show\nthis technique is suitable for both white box (e.g.,\nBLOOM) and black box (e.g., ChatGPT) models.\nRelation Name Accuracy Relation Name Accuracy\nvoice type 97.4 after a work by 77.4\noccupation 97.1 mountain range 76.6\ncontains administrative territorial entity 95.4 composer 76.3\nparticipant of 95.1 operating system 76.1\ncrosses 95.0 notable work 76.0\nlocated in the administrative territorial entity 94.9 sibling 75.6\nleague 94.4 developer 73.3\nconstellation 93.9 located in or next to body of water 71.1\ncompetition class 93.1 record label 69.7\nheritage designation 93.0 follows 69.0\nposition held 92.7 original language of film or TV show 68.7\nmember of political party 92.7 operator 67.9\nlocation 92.0 location of formation 67.4\nfield of work 92.0 country of origin 66.4\npart of 91.6 successful candidate 66.1\nhas part 91.0 country of citizenship 65.9\nmilitary branch 91.0 subsidiary 65.9\ninstance of 90.1 licensed to broadcast to 64.1\nsport 88.6 main subject 63.6\napplies to jurisdiction 88.1 owned by 62.6\nmember of 88.1 mother 62.3\nparticipating team 88.1 occupant 61.9\ntaxon rank 87.9 position played on team / speciality 60.9\ncharacters 87.6 followed by 59.4\ngenre 86.1 said to be the same as 59.3\ninstrument 86.0 place served by transport hub 59.1\ndirector 85.9 sports season of league or competition 58.7\nparticipant 85.6 child 57.9\nmanufacturer 85.1 headquarters location 56.6\ncountry 84.3 work location 53.0\nmovement 84.3 located on terrain feature 49.6\narchitect 84.0 distributor 48.4\nwinner 83.9 father 45.3\noriginal broadcaster 82.9 head of government 44.3\nreligion 82.1 performer 44.0\nnominated for 81.4 screenwriter 42.4\nplatform 80.3 mouth of the watercourse 33.3\nmilitary rank 79.9 residence 31.6\npublisher 79.7 tributary 22.7\nspouse 79.6 language of work or name 13.0\nTable 9: Accuracy of each relation in FewRel.\nCase 1\nSummarize the relations between ‚Äúbrut y brenhinedd‚Äù and ‚Äúwelsh‚Äù from context.\nContext: Walter ‚Äôs name is attached to the \" brut tysilio \" , a variant of the welsh chronicle \" brut y brenhinedd \".\nSummarization: Walter is associated with the brut tysilio, which is a welsh chronicle known as brut y brenhinedd.\nRewrite the triple as a yes/no question.\nTriple: (brut y brenhinedd, language of work or name, welsh)\nQuestion: Is the language of work or name of ‚Äúbrut y brenhinedd‚Äù welsh?\nAnswer the question from context.\nContext: Walter is associated with the brut tysilio, which is a welsh chronicle known as brut y brenhinedd.\nQuestion: Is the language of work or name of ‚Äúbrut y brenhinedd‚Äù welsh?\nAnswer: Yes, ‚Äúbrut y brenhinedd‚Äù is a welsh chronicle.\nCase 2\nSummarize the relations between ‚Äúelizabeth‚Äù and ‚Äúenglish‚Äù from context.\nContext: Annabeth is a female english given name created from a combination of the names anna and elizabeth.\nSummarization: The name annabeth is an english female given name that is created from combining the names anna\nand elizabeth.\nRewrite the triple as a yes/no question.\nTriple: (elizabeth, language of work or name, english)\nQuestion: Does elizabeth‚Äôs language of work or name include english?\nAnswer the question from context.\nContext: The name annabeth is an english female given name that is created from combining the names anna and elizabeth.\nQuestion: Does elizabeth‚Äôs language of work or name include english?\nAnswer: It is not clear from the given context whether elizabeth‚Äôs language of work or name includes english.\nCase 3\nSummarize the relations between ‚Äúluxembourgish wikipedia‚Äù and ‚Äúluxembourgish language‚Äù from context.\nContext: The luxembourgish wikipedia is the version of wikipedia that is written in the luxembourgish language.\nSummarization: The luxembourgish wikipedia is the version of wikipedia that is written in the luxembourgish language.\nRewrite the triple as a yes/no question.\nTriple: (luxembourgish wikipedia, language of work or name, luxembourgish language)\nQuestion: Is luxembourgish the language of work or name for the luxembourgish wikipedia?\nAnswer the question from context.\nContext: The luxembourgish wikipedia is the version of wikipedia that is written in the luxembourgish language.\nQuestion: Is luxembourgish the language of work or name for the luxembourgish wikipedia?\nAnswer: Luxembourgish is the language of the luxembourgish wikipedia.\nCase 4\nSummarize the relations between ‚Äúkuranty‚Äù and ‚Äúrussian‚Äù from context.\nContext: Peter the great replaced the ‚Äúkuranty‚Äù with the first printed newspaper in russian , the ‚Äúvedomosti‚Äù.\nSummarization: Kuranty was replaced by the first printed newspaper in russian called ‚Äúvedomosti‚Äù by peter the great.\nRewrite the triple as a yes/no question.\nTriple: (kuranty, language of work or name, russian)\nQuestion: Is russian the language of work or name for kuranty?\nAnswer the question from context.\nContext: Kuranty was replaced by the first printed newspaper in russian called ‚Äúvedomosti‚Äù by peter the great.\nQuestion: Is russian the language of work or name for kuranty?\nAnswer: Russian is the language of the first printed newspaper called ‚Äúvedomosti‚Äù, not ‚Äúkuranty‚Äù.\nTable 10: Case study regarding relation ‚Äúlanguage of work or name‚Äù.\nCase 1\nSummarize the relations between ‚Äúlis, ava river‚Äù and ‚Äúnatra river‚Äù from context.\nContext: The natra river is a tributary of the lis, ava river in romania.\nSummarization: The natra river is a smaller river that flows into the lis, ava river in romania.\nRewrite the triple as a yes/no question.\nTriple: (lis, ava river, tributary, natra river)\nQuestion: Is the lis, ava river a tributary of the natura river?\nAnswer the question from context.\nContext: The natra river is a smaller river that flows into the lis, ava river in romania.\nQuestion: Is the lis, ava river a tributary of the natura river?\nAnswer: No, the natra river flows into the lis, ava river, so the lis, ava river is not a tributary of the natra river.\nSummarize the relations between ‚Äúnatra river‚Äù and ‚Äúlis, ava river‚Äù from context.\nContext: The natra river is a tributary of the lis, ava river in romania.\nSummarization: The natra river is a tributary of the lis, ava river located in romania.\nRewrite the triple as a yes/no question.\nTriple: (natra river, mouth of the watercourse, lis, ava river)\nQuestion: Is the mouth of the lis, ava river located at the natra river watercourse?\nAnswer the question from context.\nContext: The natra river is a tributary of the lis, ava river located in romania.\nQuestion: Is the mouth of the lis, ava river located at the natra river watercourse?\nAnswer: The context does not provide information to answer this question.\nCase 2\nSummarize the relations between ‚Äúdanube‚Äù and ‚Äúlech‚Äù from context.\nContext: His jurisdiction extended at that time from the iller eastward over the lech, north of the danube to the alb,\nand south to the spurs of the alps.\nSummarization: The text mentions that the jurisdiction in question encompassed an area that extended from iller towards\neast over lech, then further towards north over danube unto alb, and also southwards towards the alps.\nRewrite the triple as a yes/no question.\nTriple: (danube, tributary, lech)\nQuestion: Is the lech a tributary of the danube?\nAnswer the question from context.\nContext: The text mentions that the jurisdiction in question encompassed an area that extended from iller towards east\nover lech, then further towards north over danube unto alb, and also southwards towards the alps.\nQuestion: Is the lech a tributary of the danube?\nAnswer: The text implies that the lech is located between the iller to the west and the danube to the north, but it\ndoes not explicitly state whether or not the lech is a tributary of the danube.\nSummarize the relations between ‚Äúblackberry run‚Äù and ‚Äúeast branch fishing creek‚Äù from context.\nContext: the restoration of blackberry run is the third phase of the east branch fishing creek restoration plan.\nSummarization: The restoration of blackberry run is part of the third phase of the east branch fishing creek restoration plan.\nRewrite the triple as a yes/no question.\nTriple: (blackberry run, mouth of the watercourse, east branch fishing creek)\nQuestion: Does the mouth of the watercourse located in the east branch of the fishing creek have blackberry run?\nAnswer the question from context.\nContext: The restoration of blackberry run is part of the third phase of the east branch fishing creek restoration plan.\nQuestion: Does the mouth of the watercourse located in the east branch of the fishing creek have blackberry run?\nAnswer: The context does not provide information about whether the mouth of the watercourse located in the east\nbranch of the fishing creek have blackberry run.\nTable 11: Case study regarding relation ‚Äútributary‚Äù and ‚Äúmouth of the watercourse‚Äù.\nRelation Template\nper:stateorprovince_of_death subject died in the state or province object, Yes or No?\nper:title subject is a object, Yes or No?\norg:member_of subject is the member of object, Yes or No?\nper:other_family subject is the other family member of object, Yes or No?\norg:country_of_headquarters subject has a headquarter in the country object, Yes or No?\norg:parents subject has the parent company object, Yes or No?\nper:stateorprovince_of_birth subject was born in the state or province object, Yes or No?\nper:spouse subject is the spouse of object, Yes or No?\nper:origin subject has the nationality object, Yes or No?\nper:date_of_birth subject has birthday on object, Yes or No?\nper:schools_attended subject studied in object, Yes or No?\norg:members subject has the member object, Yes or No?\norg:founded subject was founded in object, Yes or No?\nper:stateorprovinces_of_residence subject lives in the state or province object, Yes or No?\nper:date_of_death subject died in the date object, Yes or No?\norg:shareholders subject has shares hold in object, Yes or No?\norg:website subject has the website object, Yes or No?\norg:subsidiaries subject owns object, Yes or No?\nper:charges subject is convicted of object, Yes or No?\norg:dissolved subject dissolved in object, Yes or No?\norg:stateorprovince_of_headquarters subject has a headquarter in the state or province object, Yes or No?\nper:country_of_birth subject was born in the country object, Yes or No?\nper:siblings subject is the siblings of object, Yes or No?\norg:top_members/employees subject has the high level member object, Yes or No?\nper:cause_of_death subject died because of object, Yes or No?\nper:alternate_names subject has the alternate name object, Yes or No?\norg:number_of_employees/members subject has the number of employees object, Yes or No?\nper:cities_of_residence subject lives in the city object, Yes or No?\norg:city_of_headquarters subject has a headquarter in the city object, Yes or No?\nper:children subject is the parent of object, Yes or No?\nper:employee_of subject is the employee of object, Yes or No?\norg:political/religious_affiliation subject has political affiliation with object, Yes or No?\nper:parents subject has the parent object, Yes or No?\nper:city_of_birth subject was born in the city object, Yes or No?\nper:age subject has the age object, Yes or No?\nper:countries_of_residence subject lives in the country object, Yes or No?\norg:alternate_names subject is also known as object, Yes or No?\nper:religion subject has the religion object, Yes or No?\nper:city_of_death subject died in the city object, Yes or No?\nper:country_of_death subject died in the country object, Yes or No?\norg:founded_by subject was founded by object, Yes or No?\nTable 12: Templates for TACRED.",
  "topic": "Shot (pellet)",
  "concepts": [
    {
      "name": "Shot (pellet)",
      "score": 0.7264079451560974
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.6610297560691833
    },
    {
      "name": "Relation (database)",
      "score": 0.6350734829902649
    },
    {
      "name": "Computer science",
      "score": 0.6016168594360352
    },
    {
      "name": "Relationship extraction",
      "score": 0.4513203799724579
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3929511308670044
    },
    {
      "name": "Natural language processing",
      "score": 0.38275885581970215
    },
    {
      "name": "Data mining",
      "score": 0.18763574957847595
    },
    {
      "name": "Linguistics",
      "score": 0.18429309129714966
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76569877",
      "name": "Southeast University",
      "country": "CN"
    }
  ]
}