{
  "title": "Escaping the Big Data Paradigm with Compact Transformers",
  "url": "https://openalex.org/W3159778524",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2620913852",
      "name": "Hassani Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224892697",
      "name": "Walton, Steven",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Shah, Nikhil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3085010563",
      "name": "Abuduweili Abulikemu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743204614",
      "name": "Li, Jiachen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202107229",
      "name": "Shi, Humphrey",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964259004",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3041133507",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3169090692",
    "https://openalex.org/W3174726724",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W3137080992",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2042755403",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W102708294",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W2076063813",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2963263347"
  ],
  "abstract": "With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.",
  "full_text": "Escaping the Big Data Paradigm with\nCompact Transformers\nAli Hassani1*, Steven Walton1* , Nikhil Shah1,\nAbulikemu Abuduweili1, Jiachen Li2,1, Humphrey Shi1,2,3\n1SHI Lab @ University of Oregon, 2University of Illinois at Urbana-Champaign, 3Picsart AI Research (PAIR)\nAbstract\nWith the rise of Transformers as the standard for lan-\nguage processing, and their advancements in computer vi-\nsion, there has been a corresponding growth in parameter\nsize and amounts of training data. Many have come to be-\nlieve that because of this, transformers are not suitable for\nsmall sets of data. This trend leads to concerns such as:\nlimited availability of data in certain scientiﬁc domains and\nthe exclusion of those with limited resource from research\nin the ﬁeld. In this paper, we aim to present an approach for\nsmall-scale learning by introducing Compact Transformers.\nWe show for the ﬁrst time that with the right size, convolu-\ntional tokenization, transformers can avoid overﬁtting and\noutperform state-of-the-art CNNs on small datasets. Our\nmodels are ﬂexible in terms of model size, and can have as\nlittle as 0.28M parameters while achieving competitive re-\nsults. Our best model can reach 98% accuracy when train-\ning from scratch on CIFAR-10 with only 3.7M parameters,\nwhich is a signiﬁcant improvement in data-efﬁciency over\nprevious Transformer based models being over 10x smaller\nthan other transformers and is 15% the size of ResNet50\nwhile achieving similar performance. CCT also outper-\nforms many modern CNN based approaches, and even some\nrecent NAS-based approaches. Additionally, we obtain a\nnew SOTA result on Flowers-102 with 99.76% top-1 accu-\nracy, and improve upon the existing baseline on ImageNet\n(82.71% accuracy with 29% as many parameters as ViT),\nas well as NLP tasks. Our simple and compact design for\ntransformers makes them more feasible to study for those\nwith limited computing resources and/or dealing with small\ndatasets, while extending existing research efforts in data\nefﬁcient transformers.\nConvolution\n · · ·\n Patching\nReshape\n210 3 4\n(Optional)\nPositional\nEmbedding\nTransformer Encoders\nSequence Pooling\nMLP Head\nClass\nDuck\nTransformer\nConvolutional\nCompact\nTransformer\nVision\nCompact\nFigure 1: Overview of CVT (right), the basic compact\ntransformer, and CCT (left), the convolutional variant of\nour compact transformer models. CCT can be quickly\ntrained from scratch on small datasets, while achieving high\naccuracy (in under 30 minutes one can get 90% on an\nNVIDIA 2080Ti GPU or 80% on an AMD 5900X CPU on\nCIFAR-10 dataset).\n1. Introduction\nConvolutional neural networks (CNNs) [23] have been\nthe standard for computer vision, since the success of\nAlexNet [22]. Krizhevsky et al. showed that convolutions\nare adept at vision based problems due to their invariance to\nspatial translations as well as having low relational induc-\ntive bias. He et al. [16] extended this work by introducing\nresidual connections, allowing for signiﬁcantly deeper mod-\nels to perform efﬁciently. Convolutions leverage three im-\nportant concepts that lead to their efﬁciency:sparse interac-\ntion, weight sharing, and equivariant representations [14].\nTranslational equivariance and invariance are properties of\nthe convolutions and pooling layers, respectively [14, 36].\nThey allow CNNs to leverage natural image statistics and\nsubsequently allow models to have higher sampling efﬁ-\nciency [34, 34].\n*Equal contribution. Our code and pre-trained models are publicly\navailable at https://github.com/SHI-Labs/Compact-Transformers\n1\narXiv:2104.05704v4  [cs.CV]  7 Jun 2022\nOn the other end of the spectrum, Transformers have be-\ncome increasingly popular and a major focus of modern\nmachine learning research. Since the advent of Attention\nis All You Need [41], the research community saw a spike\nin transformer-based and attention-based research. While\nthis work originated in natural language processing, these\nmodels have been applied to other ﬁelds, such as computer\nvision. Vision Transformer (ViT) [12] was the ﬁrst major\ndemonstration of a pure transformer backbone being ap-\nplied to computer vision tasks. ViT highlights not only\nthe power of such models, but also that large-scale training\ncan trump inductive biases. The authors argued that “Trans-\nformers lack some of the inductive biases inherent to CNNs,\nsuch as translation equivariance and locality, and therefore\ndo not generalize well when trained on insufﬁcient amounts\nof data.” Over the past few years, an explosion in model\nsizes and datasets has also become noticeable which has led\nto a “data hungry” paradigm, making training transformers\nfrom scratch seem intractable for many types of pressing\nproblems, where there are typically several orders of mag-\nnitude less data. It also limits major contributions in the\nresearch to those with vast computational resources.\nAs a result, CNNs are still the go-to models for smaller\ndatasets because they are more efﬁcient, both computation-\nally and in terms of memory, when compared to transform-\ners. Additionally, local inductive bias shows to be more\nimportant in smaller images. They require less time and\ndata to train while also requiring a lower number of pa-\nrameters to accurately ﬁt data. However, they do not enjoy\nthe long range interdependence that attention mechanisms\nin transformers provide. Reducing machine learning’s de-\npendence on large sums of data is important, as many do-\nmains, such as science and medicine, would hardly have\ndatasets the size of ImageNet [10]. This is because events\nare far more rare and it would be more difﬁcult to properly\nassign labels, let alone create a set of data which has low\nbias and is appropriate for conventional neural networks. In\nmedical research, for instance, it may be difﬁcult to com-\npile positive samples of images for a rare disease without\nother correlating factors, such as medical equipment being\nattached to patients who are actively being treated. Addi-\ntionally, for a sufﬁciently rare disease there may only be\na few thousand images for positive samples, which is typi-\ncally not enough to train a network with good statistical pre-\ndiction unless it can sufﬁciently be pre-trained on data with\nsimilar attributes. This inability to handle smaller datasets\nhas impacted the scientiﬁc community where they are much\nmore limited in the models and tools that they are able to ex-\nplore. Frequently, problems in scientiﬁc domains have little\nin common with domains of pre-trained models and when\ndomains are sufﬁciently distinct pre-training can have little\nto no effect on the performance within a new domain [54].\nIn addition, it has been shown that strong performance on\nImageNet does not necessarily result in equally strong per-\nformance in other domains, such as medicine [20]. Further-\nmore, the requisite of large data results in a requisite of large\ncomputational resources and this prevents many researchers\nfrom being able to provide insight. This not only limits the\nability to apply models in different domains, but also lim-\nits reproducibility. Veriﬁcation of state of the art machine\nlearning algorithms should not be limited to those with large\ninfrastructures and computational resources.\nThe above concerns motivated our efforts to build more\nefﬁcient models that can be effective in less data intensive\ndomains and allow for training on datasets that are orders of\nmagnitude smaller than those conventionally seen in com-\nputer vision and natural language processing (NLP) prob-\nlems. Both Transformers and CNNs have highly desirable\nqualities for statistical inference and prediction, but each\ncomes with their own costs. In this work, we try to bridge\nthe gap between these two architectures and develop an ar-\nchitecture that can both attend to important features within\nimages, while also being spatially invariant, where we have\nsparse interactions and weight sharing. This allows for\na Transformer based model to be trained from scratch on\nsmall datasets like CIFAR-10 and CIFAR-100, providing\ncompetitive results with fewer parameters and low compu-\ntational requirements.\nIn this paper we introduce ViT-Lite, a smaller and more\ncompact version of ViT, which can obtain over 90% accu-\nracy on CIFAR-10. We expand on ViT-Lite by introducing\na sequence pooling and forming the Compact Vision Trans-\nformer (CVT). We further iterate by adding convolutional\nblocks to the tokenization step and thus creating the Com-\npact Convolutional Transformer (CCT). Both of these sim-\nple additions add to signiﬁcant increases in performance,\nleading to a top-1%accuracy of 98% on CIFAR-10. This\nmakes our work the only transformer based model in the\ntop 25 best performing models on CIFAR-10, without pre-\ntraining, and signiﬁcantly smaller than the vast majority.\nOur model also outperforms most comparable CNN-based\nmodels within this domain, with the exception of certain\nNeural Architectural Search techniques [5]. Additionally,\nwe show that our model can be lightweight, only needing\n0.28 million parameters and still reach close to 90% top-\n1% accuracy on CIFAR-10. On ImageNet, CCT achieves\n80.67% accuracy while still maintaining a small number\nof parameters and reduced computation. CCT outperforms\nViT, while containing less than a third of the number of pa-\nrameters with about a third of the computational complexity\n(MACs). Additionally, CCT outperform similarly sized and\nmore recent models, such as DeiT [19]. This demonstrates\nthe scalability of our model while maintaining compactness\nand computational efﬁciency.\n2\nThe main contributions of this paper are:\n• Extending transformer-based research to small data\nregimes, by introducing ViT-Lite, which can be trained\nfrom scratch and achieve high accuracy on datasets\nsuch as CIFAR-10.\n• Introducing Compact Vision Transformer (CVT) with\na new sequence pooling strategy, which pools over out-\nput tokens and improves performance.\n• Introducing Compact Convolutional Transformer\n(CCT) to increase performance and provide ﬂexibility\nfor input image sizes while also demonstrating that\nthese variants do not depend as much on Positional\nEmbedding compared to the rest.\nIn addition, we demonstrate that our CCT model is\nfast, obtaining 90% accuracy on CIFAR-10 using a single\nNVIDIA 2080Ti GPU and 80% when trained on a CPU\n(AMD 5900X), both in under 30 minutes. Additionally,\nsince our model has a relatively small number of param-\neters, it can be trained on the majority of GPUs, even if\nresearchers do not have access to top of the line hardware.\nThrough these efforts, we aim to help enable and extend\nresearch around Transformers to cases with limited data\nand/or researchers with limited resources.\n2. Related Works\nIn NLP research, attention mechanisms [15, 2, 28]\ngained popularity for their ability to weigh different features\nwithin sequential data. Transformers [41] were introduced\nas a fully attention-based model, primarily for machine\ntranslation and NLP in general. Following this, attention-\nbased models, speciﬁcally transformers have been applied\nto a wide variety of tasks beyond machine translation [11,\n25, 46], including: visual question answering [27, 38], ac-\ntion recognition [4, 13], and the like. Many researchers also\nleveraged a combination of attention and convolutions in\nneural networks for visual tasks [42, 18, 3, 51]. Ramachan-\ndran et al . [33] introduced one of the ﬁrst vision models\nthat rely primarily on attention. Dosovitskiy et al. [12] in-\ntroduced the ﬁrst stand-alone transformer based model for\nimage classiﬁcation (ViT). In the following subsections, we\nbrieﬂy revisit ViT and several other related works.\n2.1. Vision Transformer\nDosovitskiy et al. [12] introduced ViT primarily to show\nthat reliance on CNNs or their structure is unnecessary, as\nprior to it, most attention-based models for vision were used\neither with convolutions [42, 3, 51, 6], or kept some of their\nproperties [33]. The motivation, beyond self-attention’s\nmany desirable properties for a network, speciﬁcally its\nability to make long range connections, was scalability. It\nwas shown that ViT can successfully keep scaling, while\nCNNs start saturating in performance as the number of\ntraining samples grew. Through this, they concluded that\nlarge-scale training triumphs over the advantage of induc-\ntive bias that CNNs have, allowing their model to be com-\npetitive with CNN based architectures given sufﬁciently\nlarge amount of training data. ViT is composed of several\nparts: Image Tokenization, Positional Embedding, Classiﬁ-\ncation Token, the Transformer Encoder, and a Classiﬁcation\nHead. These subjects are discussed in more detail below.\nImage Tokenization: A standard transformer takes as\ninput a sequence of vectors, called tokens. For traditional\nNLP based transformers, word ordering provides a natu-\nral order to sequence the data, but this is not so obvious\nfor images. To tokenize an image, ViT subdivides an im-\nage into non-overlapping square patches in raster-scan or-\nder. The sequence of patches, xp ∈RH×(P2C) with patch\nsize P, are ﬂattened into 1D vectors and transformed into\nlatent vectors of dimension d. This is equivalent to a con-\nvolutional layer with d ﬁlters, and P ×P kernel size and\nstride. This simple patching and embedding method has a\nfew limitations, in particular: loss of information along the\nboundary regions.\nPositional Embedding: Positional embedding adds spa-\ntial information into the sequence. Since the model does not\nactually know anything about the spatial relationship be-\ntween tokens, adding extra information to reﬂect that can\nbe useful. Typically, this is either a learned embedding or\ntokens are given weights from two sine waves with high fre-\nquencies, which is sufﬁcient for the model to learn that there\nexists a positional relationship between these tokens.\nTransformer Encoder: A transformer encoder consists\nof a series of stacked encoding layers. Each encoder layer is\ncomprised of two sub-layers: Multi-Headed Self-Attention\n(MHSA) and a Multi-Layer Perceptron (MLP) head. Each\nsub-layer is preceded by a layer normalization (LN), and\nfollowed by a residual connection to the next sub-layer.\nClassiﬁcation: Vision transformers typically add an ex-\ntra learnable [class] token to the sequence of the embed-\nded patches, representing the class parameter of an entire\nimage and its state after transformer encoder can be used\nfor classiﬁcation. [class] token contains latent informa-\ntion, and through self-attention accumulates more informa-\ntion about the sequence, which is later used for classiﬁca-\ntion. ViT [12] also explored averaging output tokens in-\nstead, but found no signiﬁcant difference in performance.\nThey did however ﬁnd that the learning rates have to be\nadjusted between the two variants: [class] token vs. av-\nerage pooling.\n2.2. Data-Efﬁcient Transformers\nIn an effort to reduce dependence on data, Tou-\nvron et al. [40] proposed Data-Efﬁcient Image Transform-\n3\ners (DeiT). Using more advanced training techniques, and\na novel knowledge transfer method, DeiT improves the\nclassiﬁcation performance of ViT on ImageNet-1k without\nlarge-scale pre-training on datasets such as JFT-300M [39]\nor ImageNet-21k [10]. By relying only on more augmenta-\ntions [8] and training techniques [50, 49], it is shown that\nmuch smaller ViT variants that were unexplored by Doso-\nvitskiy et al. can outperform the larger ones on ImageNet-\n1k without pre-training. Furthermore, DeiT variants were\npushed even further through their novel knowledge transfer\ntechnique, speciﬁcally when using a convolutional model as\nthe teacher. This work pushes forward accessibility of trans-\nformers in medium-sized datasets, and we aim to follow by\nextending the study to even smaller sets of data and smaller\nmodels. However, we base our work on the notion that if\na small dataset happens to be sufﬁciently novel, pre-trained\nmodels will not help train on that domainand the model will\nnot be appropriate for that dataset. While knowledge trans-\nfer is a strong technique, it requires a pre-trained model for\nany given dataset, adding to training time and complexity,\nwith an additional forward pass, and as pointed out by Tou-\nvron et al. is usually only signiﬁcant when there’s a con-\nvolutional teacher available to transfer the inductive biases.\nAs a result, it can be argued that if a network utilized just\nthe bare minimum of convolutions, while keeping the pure\ntransformer structure, it may need to rely less on large-scale\ntraining and transfer of inductive biases through knowledge\ntransfer.\nYuan et al . [48] proposed Tokens-to-token ViT (T2T-\nViT), which adopts a window- and attention-based tok-\nenization strategy. Their tokenizer extracts patches of the\ninput feature map, similar to a convolution, applies three\nsets of kernel weights, and produces three sets of feature\nmaps, which are fed to self-attention as query and key-\nvalue pairs. This process is equivalent to convolutions\nproducing the QKV projections in a self-attention module.\nFinally, this strategy is repeated twice, followed by a ﬁ-\nnal patching and embedding. The entire process replaces\npatch and embedding in ViT. This strategy, along with\ntheir small-strided patch extraction, allows their network to\nmodel local structures, including along the boundaries be-\ntween patches. This attention-based patch interaction leads\nto ﬁner-grained tokens which allow T2T-ViT to outperform\nprevious Transformer-based models on ImageNet. T2T-ViT\ndiffers from our work, in that it focuses on medium-sized\ndatasets like ImageNet, which are not only far too large for\nmany research problems in science and medicine but also\nresource demanding. T2T tokenizer also has more parame-\nters and complexity compared to a convolutional one.\n2.3. Convolution-inspired Transformers\nMany works have been motivated to improve vision\ntransformers and eliminate the need for large-scale pre-\ntraining. ConViT [9] introduces a gated positional self-\nattention (GPSA) that allows for a “soft” convolutional in-\nductive bias within their model. GPSA allows their network\nto have more ﬂexibility with respect to positional informa-\ntion. Since GPSA is able to be initialized as a convolu-\ntional layer, this allows their network to sometimes have\nthe properties of convolutions or alternatively having the\nproperties of attention. Its gating parameter can be ad-\njusted by the network, allowing it to become more expres-\nsive and adapt to the needs of the dataset. Convolution-\nenhanced image Transformers (Ceit) [47] utilize convolu-\ntions throughout their model. They propose a convolution-\nbased Image-to-Token module for tokenization. They also\nre-design the encoder with layers of multi-headed self-\nattention and their novel Locally Enhanced Feedforward\nLayer, which processes the spatial information form the ex-\ntracted token. This allows creates a network that is com-\npetitive with other works such as DeiT on ImageNet. Con-\nvolutional vision Transformer (CvT) [45] introduces con-\nvolutional transformer encoder layers, which use convo-\nlutions instead of linear projections for the QKV in self-\nattention. They also introduce convolutions into their to-\nkenization step, and report competitive results compared\nto other vision transformers on ImageNet-1k. All of these\nworks report results when trained from scratch on ImageNet\n(or larger) datasets.\n2.4. Comparison\nOur work differs from the aforementioned in several\nways, in that it focuses on answering the following ques-\ntion: Can vision transformers be trained from scratch on\nsmall datasets? Focusing on a small datasets, we seek to\ncreate a model that can be trained, from scratch, on datasets\nthat are orders of magnitude smaller than ImageNet. Having\na model that is compact, small in size, and efﬁcient allows\ngreater accessibility, as training on ImageNet is still a difﬁ-\ncult and data intensive task for many researchers. Thus our\nfocus is on an accessible model, with few parameters, that\ncan quickly and efﬁciently be trained on smaller platforms\nwhile still maintaining SOTA results.\n3. Method\nIn order to provide empirical evidence that vision trans-\nformers are trainable from scratch when dealing with small\nsets of data, we propose three different models: ViT-\nLite, Compact Vision Transformers (CVT), and Compact\nConvolutional Transformers (CCT). ViT-Lite is nearly\nidentical to the original ViT in terms of architecture, but\nwith a more suitable size and patch size for small-scale\nlearning. CVT builds on this by using our Sequence\nPooling method (SeqPool), that pools the entire sequence\nof tokens produced by the transformer encoder. SeqPool\nreplaces the conventional [class] token. CCT builds\n4\nInputs ConvLayer Pooling Reshape\n Transformer\nEncoder\nSequence\nPooling\nLinear\nLayer Output\nOptional\nPositionalEmbedding\nCompact Convolutional Transformer (CCT)\nConvolutional Tokenization Transformer with Sequence Pooling\nInputs Embed to\nPatches\nLinear\nProjection Reshape\n Transformer\nEncoder\nSequence\nPooling\nLinear\nLayer\nPositionalEmbedding\nOutput\nCompact Vision Transformer (CVT)\nPatch-Based Tokenization Transformer with Sequence Pooling\nInputs Embed to\nPatches\nLinear\nProjection Reshape\n Transformer\nEncoder Slice Linear\nLayer\nClassToken\n PositionalEmbedding\n ClassToken\nOutput\nVision Transformer (ViT)\nPatch-Based Tokenization Transformer with Class Tokenization\nFigure 2: Comparing ViT (top) to CVT (middle) and CCT (bottom). CVT can be thought of as an ablated version of CCT,\nonly utilizing sequence pooling and not a convolutional tokenizer. CVT may be preferable with more limited compute, as the\npatch-based tokenization is faster.\non CVT and utilizes a convolutional tokenizer, generating\nricher tokens and preserving local information. The con-\nvolutional tokenizer is better at encoding relationships be-\ntween patches compared to the original ViT [12]. A detailed\nmodular-level comparison of these models can be viewed in\nFigure 2.\nThe components of our compact transformers are further\ndiscussed in the following subsections: Transformer-based\nBackbone, Small and Compact Models, SeqPool, and Con-\nvolutional Tokenizer.\n3.1. Transformer-based Backbone\nIn terms of model design, we follow the original Vision\nTransformer [12], and original Transformer [41]. As men-\ntioned in Section 2.1, the encoder consists of transformer\nblocks, each including an MHSA layer and an MLP block.\nThe encoder also applies Layer Normalization, GELU ac-\ntivation, and dropout. Positional embeddings can be learn-\nable or sinusoidal, both of which are effective.\n3.2. Small and Compact Models\nWe propose smaller and more compact vision transform-\ners. The smallest ViT variant, ViT-Base, includes a 12 layer\ntransformer encoder with 12 attention heads, 64 dimensions\nper head, and 2048-dimensional hidden layers in the MLP\nblocks. This, along with the classiﬁer and 16x16 patch and\nembedder results in over 85M parameters. We propose vari-\nants with as few as 2 layers, 2 heads, and 128-dimensional\nhidden layers. In Appendix A, we summarized the details of\nthe variants we propose, the smallest of which can have as\nlittle as 0.22M parameters, while the largest (for small-scale\nlearning) only have 3.8M parameters. We also adjust the to-\nkenizer (patch size) according to the dataset we’re training\non, based on its image resolution. These variants, which\nare mostly similar in architecture to ViT, but different in\nsize, are referred to as ViT-Lite. In our notation, we use\nthe number of layers to specify size, as well as tokenization\ndetails: for instance, ViT-Lite-12/16 has 12 transformer en-\ncoder layers, and a 16×16 patch size.\n3.3. SeqPool\nIn order to map the sequential outputs to a singular class\nindex, ViT [12] and most other common transformer-based\nclassiﬁers follow BERT [11], in forwarding a learnable\nclass or query token through the network and later feeding\nit to the classiﬁer. Other common practices include global\naverage pooling (averaging over tokens), which have been\nshown to be preferable in some scenarios. We introduce Se-\nqPool, an attention-based method which pools over the out-\nput sequence of tokens. Our motivation is that the output se-\nquence contains relevant information across different parts\nof the input image, therefore preserving this information\ncan improve performance, and at no additional parameters\ncompared to the learnable token. Additionally, this change\n5\nslightly decreases computation, due one less token being\nforwarded. This operation consists of mapping the output\nsequence using the transformation T : Rb×n×d ↦→Rb×d.\nGiven:\nxL = f(x0) ∈Rb×n×d\nwhere xL is the output of an L layer transformer encoder\nf, b is batch size, n is sequence length, and d is the total\nembedding dimension. xL is fed to a linear layer g(xL) ∈\nRd×1, and softmax activation is applied to the output:\nx′\nL = softmax\n(\ng(xL)T )\n∈Rb×1×n\nThis generates an importance weighting for each input to-\nken, which is applied as follows:\nz = x′\nLxL = softmax\n(\ng(xL)T )\n×xL ∈Rb×1×d (1)\nBy ﬂattening, the output z∈Rb×d is produced. This output\ncan then be sent through a classiﬁer.\nSeqPool allows our network to weigh the sequential em-\nbeddings of the latent space produced by the transformer\nencoder and correlate data across the input data. This can\nbe thought of this as attending to the sequential data, where\nwe are assigning importance weights across the sequence\nof data, only after they have been processed by the en-\ncoder. We tested several variations of this pooling method,\nincluding learnable and static methods, and found that the\nlearnable pooling performs the best. Static methods, such\nas global average pooling have already been explored by\nViT as well, as pointed out in section 2.1. We believe that\nthe learnable weighting is more efﬁcient because each em-\nbedded patch does not contain the same amount of entropy.\nThis allows the model to apply weights to tokens with re-\nspect to the relevance of their information. Additionally, se-\nquence pooling allows our model to better utilize informa-\ntion across spatially sparse data. We will further study the\neffects of this pooling in the ablation study (Sec 4.4). By\nreplacing the conventional class token in ViT-Lite with\nSeqPool, Compact Vision Transformer is created. We use\nthe same notations for this model: for instance, CVT- 7/4\nhas 7 transformer encoder layers, and a 4×4 patch size.\n3.4. Convolutional Tokenizer\nIn order to introduce an inductive bias into the model,\nwe replace patch and embedding in ViT-Lite and CVT, with\na simple convolutional block. This block follows conven-\ntional design, which consists of a single convolution,ReLU\nactivation, and a max pool. Given an image or feature map\nx ∈RH×W×C:\nx0 = MaxPool(ReLU(Conv2d(x))) (2)\nwhere the Conv2d operation has dﬁlters, same number as\nthe embedding dimension of the transformer backbone. Ad-\nditionally, the convolution and max pool operations can be\noverlapping, which could increase performance by inject-\ning inductive biases. This allows our model to maintain\nlocally spatial information. Additionally, by using this con-\nvolutional block, the models enjoy an added ﬂexibility over\nmodels like ViT, by no longer being tied to the input reso-\nlution strictly divisible by the pre-set patch size. We seek\nto use convolutions to embed the image into a latent rep-\nresentation, because we believe that it will be more efﬁ-\ncient and produce richer tokens for the transformer. These\nblocks can be adjusted in terms of downsampling ratio (ker-\nnel size, stride and padding), and are repeatable for even\nfurther downsampling. Since self-attention has a quadratic\ntime and space complexity with respect to the number of\ntokens, and number of tokens is equal to the resolution of\nthe input feature map, more downsampling results in fewer\ntokens which noticeably decreases computation (at the ex-\npense of performance). We found that on top of the added\nperformance gains, this choice in tokenization also gives\nmore ﬂexibility toward removing the positional embedding\nin the model, as it manages to maintain a very good perfor-\nmance. This is further discussed in Appendix C.1.\nThis convolutional tokenizer, along with SeqPool and the\ntransformer encoder create Compact Convolutional Trans-\nformers. We use a similar notation for CCT variants, with\nthe exception of also denoting the number of convolutional\nlayers: for instance, CCT- 7/3x2 has 7 transformer encoder\nlayers, and a 2-layer convolutional tokenizer with 3×3 ker-\nnel size.\n4. Experiments\n100 101\n77.5\n80\n82.5\n85\n87.5\n90\n92.5\n95\n97.5 CCT⋆ Proxyless-G\nCCT\nViT-Lite\nMobileNetV2\nResNet164 ResNet1k\nResNet18\n# Parameters (M)\nTop-1 validation accuracy\nFigure 3: CIFAR-10 accuracyvs. model size (sizes<12M).\nCCT⋆ was trained longer.\n6\nTable 1: Top-1 validation accuracy comparisons. ⋆variants were trained longer (see Table 2 )\nModel C-10 C-100 Fashion MNIST # Params MACs\nConvolutional Networks (Designed for ImageNet)\nResNet18 90.27% 66 .46% 94 .78% 99 .80% 11.18 M 0.04 G\nResNet34 90.51% 66 .84% 94 .78% 99 .77% 21.29 M 0.08 G\nMobileNetV2/0.5 84.78% 56 .32% 93 .93% 99 .70% 0.70 M <0.01 G\nMobileNetV2/2.0 91.02% 67 .44% 95 .26% 99 .75% 8.72 M 0.02 G\nConvolutional Networks (Designed for CIFAR)\nResNet56[16] 94.63% 74 .81% 95 .25% 99 .27% 0.85 M 0.13 G\nResNet110[16] 95.08% 76 .63% 95 .32% 99 .28% 1.73 M 0.26 G\nResNet1k-v2⋆[17] 95.38% − − − 10.33 M 1.55 G\nProxyless-G[5] 97.92% − − − 5.7 M −\nVision Transformers\nViT-12/16 83.04% 57 .97% 93 .61% 99 .63% 85.63 M 0.43 G\nViT-Lite-7/16 78.45% 52 .87% 93 .24% 99 .68% 3.89 M 0.02 G\nViT-Lite-7/8 89.10% 67 .27% 94 .49% 99 .69% 3.74 M 0.06 G\nViT-Lite-7/4 93.57% 73 .94% 95 .16% 99 .77% 3.72 M 0.26 G\nCompact Vision Transformers\nCVT-7/8 89.79% 70 .11% 94 .50% 99 .70% 3.74 M 0.06 G\nCVT-7/4 94.01% 76 .49% 95 .32% 99 .76% 3.72 M 0.25 G\nCompact Convolutional Transformers\nCCT-2/3×2 89.75% 66 .93% 94 .08% 99 .70% 0.28 M 0.04 G\nCCT-7/3×2 95.04% 77 .72% 95 .16% 99 .76% 3.85 M 0.29 G\nCCT-7/3×1 96.53% 80 .92% 95.56% 99.82% 3.76 M 1.19 G\nCCT-7/3×1⋆ 98.00% 82.72% − − 3.76 M 1.19 G\nTable 2: CCT-7/3×1 top-1 accuracy on CIFAR-10/100\nwhen trained longer\n# Epochs Pos. Emb. CIFAR-10 CIFAR-100\n300 Learnable 96.53% 80 .92%\n1500 Sinusoidal 97.48% 82 .72%\n5000 Sinusoidal 98.00% 82.87%\n4.1. Datasets\nWe conducted image classiﬁcation experiments using\nour method on the following datasets: CIFAR-10, CIFAR-\n100 (MIT License) [21], MNIST, Fashion-MNIST, Oxford\nFlowers-102 [30], and ImageNet-1k [10]. The ﬁrst four\ndatasets not only have a small number of training sam-\nples, but they are also small in resolution. Additionally,\nMNIST and Fashion-MNIST only contain a single chan-\nnel, greatly reducing the information density. Flowers-102\nhas a relatively small number of samples, while having rel-\natively higher resolution images and 102 classes. We di-\nvided these datasets into three categories: small-scale small\nresolution datasets (CIFAR-10/100, MNIST, and Fashion-\nMNIST), small-scale larger resolution (Flowers-102), and\nmedium-scale (ImageNet-1k) datasets. We also include a\nstudy on NLP classiﬁcation, presented in appendix G.\n4.2. Hyperparameters\nWe used the timm package [43] to train the models (see\nAppendix E for details), except for cited works which are\nreported directly. For all experiments, we conducted a hy-\nperparameter sweep for every different method and report\nthe best results we were able to achieve. We will release all\ncheckpoints corresponding to the reported numbers, and de-\ntailed training settings in the form of Y AML ﬁles, with our\n7\nTable 3: ImageNet Top-1 validation accuracy comparison (no extra data or pretraining). This shows that larger variants of\nCCT could also be applicable to medium-sized datasets\nModel Top-1 # Params MACs Training Epochs\nResNet50 [16] 77.15% 25.55 M 4.15 G 120\nResNet50 (2021) [44] 79.80% 25.55 M 4.15 G 300\nViT-S[19] 79.85% 22.05 M 4.61 G 300\nCCT-14/7×2 80.67% 22.36 M 5.53 G 300\nDeiT-S [19] 81.16% 22.44M 4.63 G 300\nCCT-14/7×2 Distilled 81.34% 22.36 M 5.53 G 300\nTable 4: Flowers-102 Top-1 validation accuracy comparison. CCT outperforms other competitive models, having signiﬁ-\ncantly fewer parameters and GMACs. This demonstrates the compactness on small datasets even with large images\nModel Resolution Pretraining Top-1 # Params MACs\nCCT-14/7×2 224 - 97.19% 22.17 M 18.63 G\nDeiT-B 384 ImageNet-1k 98.80% 86.25 M 55.68 G\nViT-L/16 384 JFT-300M 99.74% 304.71 M 191.30 G\nViT-H/14 384 JFT-300M 99.68% 661.00 M 504.00 G\nCCT-14/7×2 384 ImageNet-1k 99.76% 22.17 M 18.63 G\ncode. We also provide a report on hyperparamter settings in\nAppendix E. Unless stated otherwise, all tests were run for\n300 epochs, and the learning rate is reduced per epoch based\non cosine annealing [26]. All transformer based models\n(ViT-Lite, CVT, and CCT) were trained using the AdamW\noptimizer.\n4.3. Performance Comparison\nSmall-scale small resolution training: In order to\ndemonstrate that vision transformers can be as effective\nas convolutional neural networks, even in settings with\nsmall sets of data, we compare our compact transformers to\nResNets [16], which are still very useful CNNs for small to\nmedium amounts of data, as well as to MobileNetV2 [35],\nwhich are very compact and small-sized CNNs. We also\ncompare with results from [17] where He et al. designed\nvery deep (up to 1001 layers) CNNs speciﬁcally for CI-\nFAR. The results are presented in Table 1, all of which\nare of models trained from scratch. We highlight the top\nperformers. CCT-7/3x2 achieves on par results with the\nCNN models, while having signiﬁcantly fewer parameters\nin some cases. We also compare our method to the origi-\nnal ViT [12] in order to express the effectiveness of smaller\nsized backbones, convolutional layers, as well our pooling\ntechnique. As these datasets were not trained from scratch\nin the original paper, we attempted to train the smallest\nvariant: ViT-B/16 (ViT-12/16). We trained our best per-\nforming model, CCT-7/3x1, for longer than the 300 epochs\nto see how far it can go. Surprisingly, this model can get\nas high as 98% accuracy on CIFAR-10, and 82.87% accu-\nracy on CIFAR-100 when trained for 5000 epochs, which is\nstill fewer iterations an ImageNet pre-training would have.\nWe present results from training on CIFAR-10/100 for 300,\n1500 and 5000 epochs in Table 2. We observed that si-\nnusoidal positional embedding had a small but noticeable\nedge over learnable when training longer. This represents\nthe only transformer based model in the top 25 results on\nPapersWithCode for CIFAR-10 where models have no ex-\ntra data or pre-training1. In addition to this, it is also one of\nthe smallest models, being 15% the size of ResNet50 while\nmaintaining similar performance. We present a plot of dif-\nferent models in Table 1 in Figure 3.\nMedium-scale training: ImageNet training results are\npresented in Table 3, and compared to ResNet50 [16], ViT,\nand DeiT. We report ResNet50 from the original paper [16],\nas well as from Wightman et al. [44] which uses a similar\ntraining schedule to ours, and is therefore a fairer compar-\nison. We also report a smaller ViT variant as proposed by\nTouvronet al. [40]. We also report CCT’s performance with\nknowledge distillation, in order to compare it to DeiT [40].\nSimilar to DeiT, we trained our CCT-14/7x2 with a con-\nvolutional teacher and hard distillation loss. We used a\nRegNetY-16GF [32] (84M parameters), the same model\n1https://paperswithcode.com/sota/image-classiﬁcation-on-cifar-10\n8\nDeiT selected as the teacher. It is noticeable that distillation\ndoes not have as signiﬁcant of an effect on CCT it does on\nDeiT. This can be attributed to the already existing induc-\ntive biases from the convolutional tokenizer. DeiT authors\nargued that a convolutional teacher would be able to transfer\ninductive biases to the student model.\nSmall-scale higher-resolution training: We also\npresent our results on Flowers-102, in which we suc-\ncessfully reach reasonable performance without any pre-\ntraining, and with the same model size as our ImageNet\nmodel. We also claim state of the art with 99.76% top-\naccuracy with ImageNet pretraining, which exceeds even\nfar larger models pre-trained on JFT-300M. In addition to\nthis we note that our model is at least a quarter the size of\nthe next best model and almost30×smaller than ViT-H/14.\nIt can also be seen that CCT is 3 −27×more computation-\nally efﬁcient.\n4.4. Ablation Study\nWe extend our previous comparisons by doing an abla-\ntion study on our methods. In this study, we progressively\ntransform the original ViT into ViT-Lite, CVT, and CCT,\nand compare their top-1 accuracy scores. In this particular\nstudy, we report the results on CIFAR-10 and CIFAR-100\nin Table 8 in Appendix F.\n5. Conclusion\nTransformers have commonly been perceived to be only\napplicable to larger-scale or medium-scale training. While\ntheir scalability is undeniable, we have shown within this\npaper that with proper conﬁguration, a transformer can be\nsuccessfully used in small data regimes as well, and outper-\nform convolutional models of equivalent, and even larger,\nsizes. Our method is simple, ﬂexible in size, and the small-\nest of our variants can be easily loaded on even a mini-\nmal GPU, or even a CPU. While part of research has been\nfocused on large-scale models and datasets, we focus on\nsmaller scales in which there is still much research to be\ndone in data efﬁciency. We show that CCT can outperform\nother transformer based models on small datasets while also\nhaving a signiﬁcant reduction in computational costs and\nmemory constraints. This work demonstrates that trans-\nformers do not require vast computational resources and\ncan allow for their applications in even the most modest of\nsettings. This type of research is important to many scien-\ntiﬁc domains where data is far more limited that the con-\nventional machine learning datasets which are used in gen-\neral research. Continuing research in this direction will help\nopen research up to more people and domains, extending\nmachine learning research.\nReferences\n[1] S ¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. Db-\npedia: A nucleus for a web of open data. In The se-\nmantic web, pages 722–735. Springer, 2007. 13\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. Neural machine translation by jointly learn-\ning to align and translate, 2016. 3\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon\nShlens, and Quoc V Le. Attention augmented convo-\nlutional networks. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages\n3286–3295, 2019. 3\n[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs space-time attention all you need for video under-\nstanding? arXiv preprint arXiv:2102.05095, 2021. 3\n[5] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas:\nDirect neural architecture search on target task and\nhardware. In International Conference on Learning\nRepresentations, 2018. 2, 7, 17\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with trans-\nformers. In European Conference on Computer Vi-\nsion, pages 213–229. Springer, 2020. 3\n[7] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay\nVasudevan, and Quoc V Le. Autoaugment: Learn-\ning augmentation strategies from data. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 113–123, 2019. 13\n[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and\nQuoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops , pages\n702–703, 2020. 4, 13\n[9] St ´ephane d’Ascoli, Hugo Touvron, Matthew Leavitt,\nAri Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional\ninductive biases. arXiv preprint arXiv:2103.10697 ,\n2021. 4\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. Imagenet: A large-scale hierarchi-\ncal image database. In 2009 IEEE conference on com-\nputer vision and pattern recognition , pages 248–255.\nIeee, 2009. 2, 4, 7\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North\n9\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4171–4186,\n2019. 3, 5\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3, 5, 8\n[13] Rohit Girdhar, Joao Carreira, Carl Doersch, and An-\ndrew Zisserman. Video action transformer network.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 244–253,\n2019. 3\n[14] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and\nYoshua Bengio. Deep learning . MIT press Cam-\nbridge, 2016. 1\n[15] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural\nturing machines, 2014. 3\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 770–778, 2016.\n1, 7, 8, 17\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Identity mappings in deep residual networks. In\nEuropean conference on computer vision, pages 630–\n645. Springer, 2016. 7, 8, 17\n[18] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-\nexcitation networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition ,\npages 7132–7141, 2018. 3\n[19] Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Mak-\nsims V olkovs. Improving transformer optimization\nthrough better initialization. In International Confer-\nence on Machine Learning, pages 4475–4483. PMLR,\n2020. 2, 8\n[20] Alexander Ke, William Ellsworth, Oishi Banerjee,\nAndrew Y . Ng, and Pranav Rajpurkar. Chextrans-\nfer: performance and parameter efﬁciency of imagenet\nmodels for chest x-ray interpretation. Proceedings of\nthe Conference on Health, Inference, and Learning ,\nApr. 2021. 2\n[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning\nmultiple layers of features from tiny images, 2009. 7\n[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. Imagenet classiﬁcation with deep convolutional\nneural networks. Advances in neural information pro-\ncessing systems, 25:1097–1105, 2012. 1\n[23] Yann LeCun, Bernhard Boser, John S Denker, Don-\nnie Henderson, Richard E Howard, Wayne Hubbard,\nand Lawrence D Jackel. Backpropagation applied to\nhandwritten zip code recognition. Neural computa-\ntion, 1(4):541–551, 1989. 1\n[24] Xin Li and Dan Roth. Learning question classiﬁers.\nIn COLING 2002: The 19th International Conference\non Computational Linguistics, 2002. 13\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019. 3\n[26] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic\ngradient descent with warm restarts. ICLR, 2017. 8\n[27] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. arXiv\npreprint arXiv:1908.02265, 2019. 3\n[28] Minh-Thang Luong, Hieu Pham, and Christopher D.\nManning. Effective approaches to attention-based\nneural machine translation, 2015. 3\n[29] Andrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. Learn-\ning word vectors for sentiment analysis. In Proceed-\nings of the 49th annual meeting of the association for\ncomputational linguistics: Human language technolo-\ngies, pages 142–150, 2011. 13\n[30] Maria-Elena Nilsback and Andrew Zisserman. Au-\ntomated ﬂower classiﬁcation over a large number of\nclasses. In 2008 Sixth Indian Conference on Computer\nVision, Graphics & Image Processing, pages 722–729.\nIEEE, 2008. 7\n[31] Jeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. Glove: Global vectors for word\nrepresentation. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1532–1543, 2014.\n14\n[32] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Gir-\nshick, Kaiming He, and Piotr Doll ´ar. Designing net-\nwork design spaces. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion, pages 10428–10436, 2020. 8\n[33] Prajit Ramachandran, Niki Parmar, Ashish Vaswani,\nIrwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. arXiv\npreprint arXiv:1906.05909, 2019. 3\n[34] Daniel L Ruderman and William Bialek. Statistics of\nnatural images: Scaling in the woods. Physical review\nletters, 73(6):814, 1994. 1\n10\n[35] Mark Sandler, Andrew Howard, Menglong Zhu, An-\ndrey Zhmoginov, and Liang-Chieh Chen. Mo-\nbilenetv2: Inverted residuals and linear bottlenecks. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 4510–4520, 2018.\n8\n[36] J ¨urgen Schmidhuber. Deep learning in neural net-\nworks: An overview. Neural networks , 61:85–117,\n2015. 1\n[37] Richard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. Recursive deep models for seman-\ntic compositionality over a sentiment treebank. InPro-\nceedings of the 2013 conference on empirical meth-\nods in natural language processing, pages 1631–1642,\n2013. 13\n[38] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei\nLu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training\nof generic visual-linguistic representations. arXiv\npreprint arXiv:1908.08530, 2019. 3\n[39] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and\nAbhinav Gupta. Revisiting unreasonable effective-\nness of data in deep learning era. In Proceedings of\nthe IEEE international conference on computer vision,\npages 843–852, 2017. 4\n[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. Training data-efﬁcient image transformers\n& distillation through attention. arXiv preprint\narXiv:2012.12877, 2020. 3, 8\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nAdvances in Neural Information Processing Systems ,\n30:5998–6008, 2017. 2, 3, 5\n[42] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang,\nCheng Li, Honggang Zhang, Xiaogang Wang, and Xi-\naoou Tang. Residual attention network for image clas-\nsiﬁcation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3156–\n3164, 2017. 3\n[43] Ross Wightman. Pytorch image models.\nhttps://github.com/rwightman/\npytorch-image-models, 2019. 7, 13\n[44] Ross Wightman, Hugo Touvron, and Herv ´e J ´egou.\nResnet strikes back: An improved training procedure\nin timm. arXiv preprint arXiv:2110.00476, 2021. 8\n[45] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Intro-\nducing convolutions to vision transformers. arXiv\npreprint arXiv:2103.15808, 2021. 4\n[46] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language\nunderstanding. arXiv preprint arXiv:1906.08237 ,\n2019. 3\n[47] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou,\nFengwei Yu, and Wei Wu. Incorporating convolu-\ntion designs into visual transformers. arXiv preprint\narXiv:2103.11816, 2021. 4\n[48] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yu-\njun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision trans-\nformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021. 4\n[49] Sangdoo Yun, Dongyoon Han, Seong Joon Oh,\nSanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classi-\nﬁers with localizable features. In Proceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion, pages 6023–6032, 2019. 4, 13\n[50] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin,\nand David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. arXiv preprint arXiv:1710.09412,\n2017. 4, 13\n[51] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and\nAugustus Odena. Self-attention generative adversar-\nial networks. In International conference on machine\nlearning, pages 7354–7363. PMLR, 2019. 3\n[52] Xiang Zhang, Junbo Zhao, and Yann LeCun.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. arXiv preprint arXiv:1509.01626, 2015. 13\n[53] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li,\nand Yi Yang. Random erasing data augmentation. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 34, pages 13001–13008, 2020. 13\n[54] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi,\nYongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing\nHe. A comprehensive survey on transfer learning.\nProceedings of the IEEE, 109(1):43–76, 2020. 2\n11\nA. Variants\nWithin this appendix, we present architectural details of\nour variants in Tables 5 and 6.\nTable 5: Transformer backbones in each variant.\nModel # Layers # Heads Ratio Dim\nViT-Lite-6 6 4 2 256\nViT-Lite-7 7 4 2 256\nCVT-6 6 4 2 256\nCVT-7 7 4 2 256\nCCT-2 2 2 1 128\nCCT-4 4 2 1 128\nCCT-6 6 4 2 256\nCCT-7 7 4 2 256\nCCT-14 14 6 3 384\nTable 6: Tokenizers in each variant.\nModel # Layers # Convs Kernel Stride\nViT-Lite-7/8 7 1 8×8 8×8\nViT-Lite-7/4 7 1 4×4 4×4\nCVT-7/8 7 1 8×8 8×8\nCVT-7/4 7 1 4×4 4×4\nCCT-2/3x2 2 2 3×3 1×1\nCCT-7/3x1 7 1 3×3 1×1\nCCT-7/7x2 7 2 7×7 2×2\nB. Computational Resources\nFor most experiments, we used a machine with an\nIntel(R) Core(TM) i9-9960X CPU @ 3.10GHz and 4\nNVIDIA(R) RTX(TM) 2080Tis (11GB). The exception was\nthe CPU test which was performed with an AMD Ryzen\n9 5900X. Each ImageNet experiment was performed on\na single machine either with 2 AMD EPYC(TM) 7662s\nand 8 NVIDIA(R) RTX(TM) A6000s (48GB), or 2 AMD\nEPYC(TM) 7713s and 8 NVIDIA(R) A100s (80GB).\nC. Additional analyses\nWithin this appendix we present some additional perfor-\nmance analyses which were conducted.\nC.1. Positional Embedding\nTo determine the effects of our small & compact design,\nsequence pooling, and convolutional tokenizer, we perform\nan ablation study focused on positional embedding, seen in\nTable 7. In this study, we experiment with ViT (original siz-\ning), ViT-Lite, CVT, and CCT, and investigate the effects\nof: a learnable positional embedding, a standard sinusoidal\nembedding, as well as no positional embedding. We ﬁnish\nthe table with our best model, which also has augmented\ntraining and an optimal tuning (refer to Appendix E). In\nthese experiments, we ﬁnd that positional encoding matters\nin all variants, but to varying degrees. In particular, CCT\nrelies less on positional encoding, and it can be safely re-\nmoved much impact in accuracy. We also tested our CCT\nmodel without SeqPool, using the standard [class] to-\nken instead, and found that there was little to no effect from\nhaving a positional encoder or not, depending on model\nsize. This suggests that convolutions are what helps pro-\nvide spatially sparse information to the transformer, while\nalso helping the model overcome some of the previous lim-\nitations, allowing for more efﬁcient use of data. We do ﬁnd\nthat SeqPool helps slightly in this respect, but overall has\na larger effect on increasing total accuracy. Lastly, we ﬁnd\nthat with proper data augmentation and tuning, the overall\nperformance can be increased, and a low dependence on po-\nsitional information can be maintained.\nC.2. Performance vs Dataset Size\nIn this experiment, we evaluated model performance on\nsmaller subsets of CIFAR-10 to determine the relationship\nbetween performance and the number of samples within\na dataset. Samples were removed uniformly from each\nclass in CIFAR-10. For this experiment, we compared ViT-\nLite and CCT. In Figure 4, we see the comparison of each\nmodel’s accuracy vs the number of samples per class. We\nshow how each model performs when given only 500, 1000,\n2000, 3000, 4000, or 5000 (original) samples per class,\nmeaning the total training set ranges from one tenth the size\nto full. It can be ovserved that CCT is more robust since\nit is able to obtain higher accuracy with a lower number of\nsamples per class, especially in the low sample regime.\nC.3. Performance vs Dimensionality\nIn order to determine whether transformers are depen-\ndant on high dimensional data, as opposed to the number\nof samples (explored in Appendix C.2), we experimented\nwith downsampled and upsampled versions of CIFAR-10.\nIn Figure 5, we present the image dimensionality vs the per-\nformance of CCT vs. ViT-Lite. Both models were trained\nwith images of sizes ranging from 16×16 to 64×64. It can be\nobserved that CCT performs better on all image sizes, with\na widening difference as the number of pixels increases.\nFrom this, it can be inferred that CCT is able to better utilize\nthe information density of an image, while ViT does not see\ncontinued performance increases after the standard 32x32\nsize.\n12\n1,000 2,000 3,000 4,000 5,000\n70\n80\n90\nViT-Lite-7/4\nCCT-7/3x2\nSamples per class\nTop-1 validation accuracy\nFigure 4: Reduced # samples / class (CIFAR-10)\n20 30 40 50 60\n84\n86\n88\n90\n92\n94\nCCT-7/3x2\nViT-Lite-7/4\nCIFAR-10 image height and width\nTop-1 validation accuracy Figure 5: Image Size vs Accuracy (CIFAR-10)\nD. Dimensionality Experiments\nWithin this appendix, we extend the analysis from Ap-\npendix C.3, showing the difference in performance when\nusing different types of positional embedding. Figure 6\nshows the difference of the accuracy when models are be-\ning trained from scratch. On the other hand, Figure 7\nshows the performance difference when models are only\nused in inference and pre-trained on the 32×32 sized im-\nages. We note that in Figure 7(a) that we do not provide\ninference for image sizes greater than the pre-trained im-\nage because the learnable positional embeddings do not al-\nlow us to extend in this direction. We draw the reader’s\nattention to Figure 6(c) and Figure 7(c) to denote the large\ndifference between the models when positional embedding\nis not used. We can see that in training CCT has very lit-\ntle difference when positional embeddings are used. Addi-\ntionally, it should be noted that when performing inference\nour non-positional embedding CCT model has much higher\ngeneralizability than its ViT-Lite counterpart.\nE. Hyperparameter tuning\nWe used the timm package [43] for our experiments (ex-\ncluding NLP experiments). We also sued CutMix [49],\nMixup [50], Randaugment [8], and Random Erasing [53].\nFor our small-scale small-resolution experiments, we con-\nducted a hyperparameter sweep for each model on each\ndataset separately. However, all experiments that trained\nmodels from scratch, were trained for 300 epochs, unless\nmentioned otherwise. ViT, CVT and CCT all used the\nweighted Adam optimizer ( β1 = 0.9 and β2 = 0.999).\nFor CNNs, we observed that some models and datasets\nachieved their best results using AdamW, while most others\nperformed best with SGD with momentum ( 0.9). We will\nrelease model checkpoints (PyTorch pickle ﬁles), as well\nas a full list of hyperparameters and training settings (in the\nform of Y AML ﬁles readable bytimm) along with our code\nfor reproduction.\nF. Ablation Study\nHere in Table 8 we present the results from section 4.4.\nWe provide a full list of ablated terms showing which fac-\ntors give the largest boost in performances. “Model” col-\numn refers to variant (see Table 5 for details), “Conv” spec-\niﬁes the number of convolutional blocks (if an), and “Conv\nSize” speciﬁes the kernel size. “Aug” denotes the use of\nAutoAugment [7]. “Tuning” speciﬁes a minor change in\ndropout, attention dropout, and/or stochastic depth (see Ta-\nble 9). The ﬁrst row in Table 8 is essentially ViT. The next\nthree rows are modiﬁed variants of ViT, which are not pro-\nposed in the original paper. These variants are more com-\npact and use smaller patch sizes. It should be noted that the\nnumbers reported in this table are best out of 4.\nG. NLP experiments\nTo demonstrate the general purpose nature of our model\nwe extended it to the domain of Natural Language Process-\ning, focusing on classiﬁcation tasks. This shows that our\nmodel is a general purpose classiﬁer and is not restricted\nto the domain of image classiﬁcation. Within this section,\nwe present our text classiﬁcation results on 5 datasets: AG-\nNews [52], TREC [24], SST [37], IMDb [29], DBpedia [1].\nThe results are summarized in Table 10. As can be seen, our\nmodel outperforms the vanilla transformer, demonstrating\nthat the techniques we use here also help with NLP tasks.\n13\nTable 7: Top-1 validation accuracy comparison when changing the positional embedding method. Augmentations and train-\ning techniques such as Mixup and CutMix were turned off for these experiments to highlight differences better. The numbers\nreported are best out of 4 runs with random initializations. †denotes model trained with extra augmentation and hyperpa-\nrameter tuning.\nModel PE CIFAR-10 CIFAR-100\nConventional Vision Transformers are more dependent on Positional Embedding\nViT-12/16\nLearnable 69.82% (+3.11%) 40.57% (+1.01%)\nSinusoidal 69.03% (+2.32%) 39.48% (−0.08%)\nNone 66.71% (baseline) 39.56% (baseline)\nViT-Lite-7/8\nLearnable 83.38% (+7.25%) 55.69% (+7.15%)\nSinusoidal 80.86% (+4.73%) 53.50% (+4.96%)\nNone 76.13% (baseline) 48.54% (baseline)\nCVT-7/8\nLearnable 84.24% (+6.52%) 55.49% (+7.23%)\nSinusoidal 80.84% (+3.12%) 50.82% (+2.56%)\nNone 77.72% (baseline) 48.26% (baseline)\nCompact Convolutional Transformers are less dependent on Positional Embedding\nCCT-7/7\nLearnable 82.03% (+0.21%) 63.01% (+3.24%)\nSinusoidal 81.15% (−0.67%) 60.40% (+0.63%)\nNone 81.82% (baseline) 59.77% (baseline)\nCCT-7/3×2\nLearnable 90.69% (+1.67%) 65.88% (+2.82%)\nSinusoidal 89.93% (+0.91%) 64.12% (+1.06%)\nNone 89.02% (baseline) 63.06% (baseline)\nCCT-7/3×2†\nLearnable 95.04% (+0.64%) 77.72% (+0.20%)\nSinusoidal 94.80% (+0.40%) 77.82% (+0.30%)\nNone 94.40% (baseline) 77.52% (baseline)\nCCT-7/3×1†\nLearnable 96.53% (+0.29%) 80.92% (+0.65%)\nSinusoidal 96.27% (+0.03%) 80.12% (−0.15%)\nNone 96.24% (baseline) 80.27% (baseline)\nCCT-7/7×1-noSeqPool\nLearnable 82.41% (+0.12%) 62.61% (+3.31%)\nSinusoidal 81.94% (−0.35%) 61.04% (+1.74%)\nNone 82.29% (baseline) 59.30% (baseline)\nCCT-7/3×2-noSeqPool\nLearnable 90.41% (+1.49%) 66.57% (+1.40%)\nSinusoidal 89.84% (+0.92%) 64.71% (−0.46%)\nNone 88.92% (baseline) 65.17% (baseline)\nThe network is slightly modiﬁed from the vision CCT. We\nuse GloVe (Apache License 2.0) [31] to provide the word\nembedding for the model, and do not train these parameters.\nNote that model sizes do not reﬂect the number of parame-\nters for GloVe, which is around 20M. We treat text as single\nchannel data and the embedding dimension as size 300. Ad-\nditionally, the convolution kernels have size 1. Finally, we\ninclude masking in the typical manner. By doing so, CCT\ncan get upwards of a 3% improvement on some datasets\nwhile using less parameters than vanilla transformers. Sim-\nilar to our vision results, we ﬁnd that CCT performs well\non small NLP datasets. We note that the CCT models that\nperform best all have less than 1M parameters, which are\nsigniﬁcantly smaller than there vanilla counterparts, while\nout performing them.\n14\nTable 8: CIFAR Top-1 validation accuracy when transforming ViT into CCT step by step. We disabled advanced training\ntechniques and augmentations for these runs.\nModel CLS # Conv Conv Size Aug Tuning C-10 C-100 # Params MACs\nViT-12/16 CT \u0017 \u0017 \u0017 \u0017 69.82% 40 .57% 85.63 M 0.43 G\nViT-Lite-7/16 CT \u0017 \u0017 \u0017 \u0017 71.78% 41 .59% 3.89 M 0.02 G\nViT-Lite-7/8 CT \u0017 \u0017 \u0017 \u0017 83.38% 55 .69% 3.74 M 0.06 G\nViT-Lite-7/4 CT \u0017 \u0017 \u0017 \u0017 83.59% 58 .43% 3.72 M 0.26 G\nCVT-7/16 SP \u0017 \u0017 \u0017 \u0017 72.26% 42 .37% 3.89 M 0.02 G\nCVT-7/8 SP \u0017 \u0017 \u0017 \u0017 84.24% 55 .49% 3.74 M 0.06 G\nCVT-7/8 SP \u0017 \u0017 \u0013 \u0017 87.15% 63 .14% 3.74 M 0.06 G\nCVT-7/4 SP \u0017 \u0017 \u0017 \u0017 88.06% 62 .06% 3.72 M 0.25 G\nCVT-7/4 SP \u0017 \u0017 \u0013 \u0017 91.72% 69 .59% 3.72 M 0.25 G\nCVT-7/4 SP \u0017 \u0017 \u0013 \u0013 92.43% 73 .01% 3.72 M 0.25 G\nCVT-7/2 SP \u0017 \u0017 \u0017 \u0017 84.80% 57 .98% 3.76 M 1.18 G\nCCT-7/7×1 SP 1 7 ×7 \u0017 \u0017 87.81% 62 .83% 3.74 M 0.26 G\nCCT-7/7×1 SP 1 7 ×7 \u0013 \u0017 91.85% 69 .43% 3.74 M 0.26 G\nCCT-7/7×1 SP 1 7 ×7 \u0013 \u0013 92.29% 72 .46% 3.74 M 0.26 G\nCCT-7/3×2 SP 2 3 ×3 \u0013 \u0013 93.65% 74 .77% 3.85 M 0.29 G\nCCT-7/3×1 SP 1 3 ×3 \u0013 \u0013 94.47% 75.59% 3.76 M 1.19 G\nTable 9: Difference between tuned and not tuned runs in\nTable 8.\nHyper Param Not Tuned Tuned\nMLP Dropout 0.1 0\nMSA Dropout 0 0.1\nStochastic Depth 0 0.1\nH. Additional experiments\nH.1. Extended small-scale experiments\nWe present the extended version of Table 1 here with\nadditional models in Table 11.\n15\nTable 10: Top-1 validation accuracy on text classiﬁcation datasets. The number of parameters does not include the word\nembedding layer, because we use pretrained word-embeddings and freeze those layers while training.\nModel AGNews TREC SST IMDb DBpedia # Params\nVanilla Transformer Encoders\nTransformer-2 93.28% 90 .40% 67 .15% 86 .01% 98 .63% 1.086 M\nTransformer-4 93.25% 92 .54% 65 .20% 85 .98% 96 .91% 2.171 M\nTransformer-6 93.55% 92 .78% 65 .03% 85 .87% 98 .24% 4.337 M\nVision Transformers\nViT-Lite-2/1 93.02% 90 .32% 67 .66% 87 .69% 98 .99% 0.238 M\nViT-Lite-2/2 92.20% 90 .12% 64 .44% 87 .39% 98 .88% 0.276 M\nViT-Lite-2/4 90.53% 90 .00% 62 .37% 86 .17% 98 .72% 0.353 M\nViT-Lite-4/1 93.48% 91 .50% 66 .81% 87 .38% 99 .04% 0.436 M\nViT-Lite-4/2 92.06% 90 .42% 63 .75% 87 .00% 98 .92% 0.474 M\nViT-Lite-4/4 90.93% 89 .30% 60 .83% 86 .71% 98 .81% 0.551 M\nViT-Lite-6/1 93.07% 91 .92% 64 .95% 87 .58% 99 .02% 3.237 M\nViT-Lite-6/2 92.56% 89 .38% 62 .78% 86 .96% 98 .89% 3.313 M\nViT-Lite-6/4 91.12% 90 .36% 60 .97% 86 .42% 98 .72% 3.467 M\nCompact Vision Transformers\nCVT-2/1 93.24% 90 .44% 67 .88% 87 .68% 98 .98% 0.238 M\nCVT-2/2 92.29% 89 .96% 64 .26% 86 .99% 98 .93% 0.276 M\nCVT-2/4 91.10% 89 .84% 62 .22% 86 .39% 98 .75% 0.353 M\nCVT-4/1 93.53% 92 .58% 66 .64% 87 .27% 99 .04% 0.436 M\nCVT-4/2 92.35% 90 .36% 63 .90% 86 .96% 98 .93% 0.474 M\nCVT-4/4 90.71% 90 .14% 61 .98% 86 .77% 98 .80% 0.551 M\nCVT-6/1 93.38% 92 .06% 65 .94% 86 .78% 99 .02% 3.237 M\nCVT-6/2 92.57% 91 .14% 64 .57% 86 .61% 98 .86% 3.313 M\nCVT-6/4 91.35% 91 .66% 61 .63% 86 .13% 98 .76% 3.467 M\nCompact Convolutional Transformers\nCCT-2/1x1 93.40% 90 .86% 68.76% 88.95% 99 .01% 0.238 M\nCCT-2/2x1 93.38% 91 .86% 67 .19% 89.13% 99.04% 0.276 M\nCCT-2/4x1 93.80% 91.42% 64 .47% 88 .92% 99 .04% 0.353 M\nCCT-4/1x1 93.49% 91 .84% 68 .21% 88 .71% 99.03% 0.436 M\nCCT-4/2x1 93.30% 93.54% 66.42% 88 .94% 99.05% 0.474 M\nCCT-4/4x1 93.09% 93 .20% 66 .57% 88 .86% 99 .02% 0.551 M\nCCT-6/1x1 93.73% 91 .22% 66 .59% 88 .81% 98 .99% 3.237 M\nCCT-6/2x1 93.29% 92 .10% 65 .02% 88 .74% 99 .02% 3.313 M\nCCT-6/4x1 92.86% 92 .96% 65 .84% 88 .68% 99 .02% 3.467 M\n16\nTable 11: Top-1 comparisons. ⋆were trained longer (see Tab 2).\nModel C-10 C-100 Fashion MNIST # Params MACs\nConvolutional Networks (Designed for ImageNet)\nResNet18 90.27% 66 .46% 94 .78% 99 .80% 11.18 M 0.04 G\nResNet34 90.51% 66 .84% 94 .78% 99 .77% 21.29 M 0.08 G\nResNet50 91.63% 68 .27% 94 .99% 99 .79% 23.53 M 0.08 G\nMobileNetV2/0.5 84.78% 56 .32% 93 .93% 99 .70% 0.70 M <0.01 G\nMobileNetV2/1.0 89.07% 63 .69% 94 .85% 99 .75% 2.24 M 0.01 G\nMobileNetV2/1.25 90.60% 65 .24% 95 .05% 99 .77% 3.47 M 0.01 G\nMobileNetV2/2.0 91.02% 67 .44% 95 .26% 99 .75% 8.72 M 0.02 G\nConvolutional Networks (Designed for CIFAR)\nResNet56[16] 94.63% 74 .81% 95 .25% 99 .27% 0.85 M 0.13 G\nResNet110[16] 95.08% 76 .63% 95 .32% 99 .28% 1.73 M 0.26 G\nResNet164-v1[17] 94.07% 74 .84% − − 1.70 M 0.26 G\nResNet164-v2[17] 94.54% 75 .67% − − 1.70 M 0.26 G\nResNet1k-v1[17] 92.39% 72 .18% − − 10.33 M 1.55 G\nResNet1k-v2[17] 95.08% 77 .29% − − 10.33 M 1.55 G\nResNet1k-v2⋆[17] 95.38% − − − 10.33 M 1.55 G\nProxyless-G[5] 97.92% − − − 5.7 M −\nVision Transformers\nViT-12/16 83.04% 57 .97% 93 .61% 99 .63% 85.63 M 0.43 G\nViT-Lite-7/16 78.45% 52 .87% 93 .24% 99 .68% 3.89 M 0.02 G\nViT-Lite-6/16 78.12% 52 .68% 93 .09% 99 .66% 3.36 M 0.02 G\nViT-Lite-7/8 89.10% 67 .27% 94 .49% 99 .69% 3.74 M 0.06 G\nViT-Lite-6/8 88.29% 66 .40% 94 .36% 99 .73% 3.22 M 0.06 G\nViT-Lite-7/4 93.57% 73 .94% 95 .16% 99 .77% 3.72 M 0.26 G\nViT-Lite-6/4 93.08% 73 .33% 95 .14% 99 .74% 3.19 M 0.22 G\nCompact Vision Transformers\nCVT-7/8 89.79% 70 .11% 94 .50% 99 .70% 3.74 M 0.06 G\nCVT-6/8 89.50% 68 .80% 94 .53% 99 .74% 3.21 M 0.05 G\nCVT-7/4 94.01% 76 .49% 95 .32% 99 .76% 3.72 M 0.25 G\nCVT-6/4 93.60% 74 .23% 95 .00% 99 .75% 3.19 M 0.22 G\nCompact Convolutional Transformers\nCCT-2/3×2 89.75% 66 .93% 94 .08% 99 .70% 0.28 M 0.04 G\nCCT-4/3×2 91.97% 71 .51% 94 .74% 99 .73% 0.48 M 0.05 G\nCCT-6/3×2 94.43% 77 .14% 95 .34% 99 .75% 3.33 M 0.25 G\nCCT-7/3×2 95.04% 77 .72% 95 .16% 99 .76% 3.85 M 0.29 G\nCCT-6/3×1 95.70% 79 .40% 95 .41% 99 .79% 3.23 M 1.02 G\nCCT-7/3×1 96.53% 80 .92% 95.56% 99.82% 3.76 M 1.19 G\nCCT-7/3×1⋆ 98.00% 82.72% − − 3.76 M 1.19 G\n17\n20 30 40 50 60\n84\n86\n88\n90\n92\n94\nCCT-7/3x2\nViT-Lite-7/4\nCIFAR-10 image height and width\nTop-1 validation accuracy\n(a) Learnable PEs\n20 30 40 50 60\n82\n84\n86\n88\n90\n92\n94\nCCT-S-7/3x2\nViT-Lite-S-7/4\nCIFAR-10 image height and width\nTop-1 validation accuracy\n(b) Sinusoidal Positional Embedding\n20 30 40 50 60\n80\n85\n90\nCCT-N-7/3x2\nViT-Lite-N-7/4\nCIFAR-10 image height and width\nTop-1 validation accuracy\n(c) No Positional Embedding\nFigure 6: CIFAR-10 resolution vs top-1% validation accu-\nracy (training from scratch). Images are square.\n5 10 15 20 25 30 35 40\n20\n40\n60\n80\n100\nCCT-7/3x1\nViT-Lite-7/4\nCIFAR-10 image height and width\nTop-1 validation accuracy\n(a) Learnable PEs (only possible up to 32x32 without\nchanging weights)\n10 20 30 40 50 60\n20\n40\n60\n80\n100\nCCT-S-7/3x2\nViT-Lite-S-7/4\nCIFAR-10 image height and width\nTop-1 validation accuracy\n(b) Sinusoidal PEs\n10 20 30 40 50 60\n50\n60\n70\n80\n90 CCT-N-7/3x2\nViT-Lite-N-7/4\nCIFAR-10 image height and width\nTop-1 validation accuracy\n(c) None\nFigure 7: CIFAR-10 resolution vs top-1% validation accu-\nracy (inference only). Images are square.\n18",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.7243187427520752
    },
    {
      "name": "Computer science",
      "score": 0.7221189737319946
    },
    {
      "name": "Transformer",
      "score": 0.7206181287765503
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4559498429298401
    },
    {
      "name": "Machine learning",
      "score": 0.4460141956806183
    },
    {
      "name": "Data mining",
      "score": 0.37049227952957153
    },
    {
      "name": "Computer engineering",
      "score": 0.3506406843662262
    },
    {
      "name": "Engineering",
      "score": 0.11317762732505798
    },
    {
      "name": "Electrical engineering",
      "score": 0.10491612553596497
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    }
  ]
}