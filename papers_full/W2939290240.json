{
  "title": "Extractive Summarization with Very Deep Pretrained Language Model",
  "url": "https://openalex.org/W2939290240",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2061178530",
      "name": "Yang Gu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229894180",
      "name": "Yanke Hu",
      "affiliations": [
        "Humana (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2083305840",
    "https://openalex.org/W2110693578",
    "https://openalex.org/W303217050",
    "https://openalex.org/W1939882552",
    "https://openalex.org/W2288604516",
    "https://openalex.org/W2574535369",
    "https://openalex.org/W2896255318",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2952138241",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3138773240",
    "https://openalex.org/W4300665782",
    "https://openalex.org/W2803930360",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2962884827",
    "https://openalex.org/W2963545005",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4289665902",
    "https://openalex.org/W2962966181",
    "https://openalex.org/W2897139265"
  ],
  "abstract": "&lt;p&gt;Recent development of generative pretrained language models has been proven very successful on a wide range of NLP tasks, such as text classification, question answering, textual entailment and so on. In this work, we present a two-phase encoder decoder architecture based on Bidirectional Encoding Representation from Transformers(BERT) for extractive summarization task. We evaluated our model by both automatic metrics and human annotators, and demonstrated that the architecture achieves the stateof-the-art comparable result on large scale corpus &ndash; CNN/Daily Mail1 . As the best of our knowledge, this is the first work that applies BERT based architecture to a text summarization task and achieved the stateof-the-art comparable result.&lt;/p&gt;",
  "full_text": null,
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9382131099700928
    },
    {
      "name": "Computer science",
      "score": 0.8767237663269043
    },
    {
      "name": "Natural language processing",
      "score": 0.624948263168335
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6158721446990967
    },
    {
      "name": "Language model",
      "score": 0.5257883071899414
    }
  ]
}