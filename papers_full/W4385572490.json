{
    "title": "Learning Better Masking for Better Language Model Pre-training",
    "url": "https://openalex.org/W4385572490",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5104668342",
            "name": "Dongjie Yang",
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Shanghai Municipal Education Commission"
            ]
        },
        {
            "id": "https://openalex.org/A5070962435",
            "name": "Zhuosheng Zhang",
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Shanghai Municipal Education Commission"
            ]
        },
        {
            "id": "https://openalex.org/A5100457332",
            "name": "Hai Zhao",
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Shanghai Municipal Education Commission"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W4287646293",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W4247880210",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4386566638",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W4303468996",
        "https://openalex.org/W3176909894",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2024060531",
        "https://openalex.org/W3105601320"
    ],
    "abstract": "Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings on masking ratio and masked content are unlikely to deliver an optimal outcome, which motivates us to explore the influence of time-variant MLM settings. We propose two scheduled masking approaches that adaptively tune the masking ratio and masked content in different training stages, which improves the pre-training efficiency and effectiveness verified on the downstream tasks. Our work is a pioneer study on time-variant masking strategy on ratio and content and gives a better understanding of how masking ratio and masked content influence the MLM pre-training.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7255–7267\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLearning Better Masking for Better Language Model Pre-training\nDongjie Yang1,2, Zhuosheng Zhang1,2,∗, Hai Zhao1,2,∗\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n{djyang.tony,zhangzs}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn\nAbstract\nMasked Language Modeling (MLM) has been\nwidely used as the denoising objective in pre-\ntraining language models (PrLMs). Existing\nPrLMs commonly adopt a Random-Token\nMasking strategy where a fixed masking ratio\nis applied and different contents are masked\nby an equal probability throughout the entire\ntraining. However, the model may receive a\ncomplicated impact from pre-training status,\nwhich changes accordingly as training time\ngoes on. In this paper, we show that such\ntime-invariant MLM settings on masking ratio\nand masked content are unlikely to deliver an\noptimal outcome, which motivates us to explore\nthe influence of time-variant MLM settings. We\npropose two scheduled masking approaches\nthat adaptively tune the masking ratio and\nmasked content in different training stages,\nwhich improves the pre-training efficiency and\neffectiveness verified on the downstream tasks.\nOur work is a pioneer study on time-variant\nmasking strategy on ratio and content and gives\na better understanding of how masking ratio\nand masked content influence the MLM pre-\ntraining1.\n1 Introduction\nPre-trained language models (PrLMs) have played\nan essential role in many natural language pro-\ncessing tasks (Radford et al., 2018; Devlin et al.,\n2019; Bao et al., 2020; Guu et al., 2020; Yu et al.,\n2021; Zhang et al., 2022). Generally speaking,\nPrLMs can be seen as an automatic denoising\nencoder and may be conveniently obtained through\na self-supervised learning way. Masked Language\nModeling (MLM) pioneered by BERT (Devlin\net al., 2019) is a widely used denoising method\nfor language model pre-training (Lan et al., 2020;\nClark et al., 2020). In MLM pre-training, a subset\n∗ Corresponding author; This paper was partially\nsupported by Key Projects of National Natural Science\nFoundation of China (U1836222 and 61733011).\n1https://github.com/mutonix/better_masking\nof tokens in a sequence is masked with a certain\nmasking ratio, and the masked sequence is fed to\nthe PrLM, which is required to predict the masked\ntokens.\nMasking in MLM is a process in terms of\nsampling masked tokens from a huge data space to\ngenerate training batches, in which MLM may be\nheavily controlled by two main factors, masking\nratio and masked contents. So far, only a few stud-\nies have ever considered optimal settings for better\nMLM from quite limited perspectives. Especially,\nall known works only take time-invariant MLM\nsetting into account despite the huge time variance\nof the model during a lengthy pre-training. For\nexample, carefully considered masked units like n-\ngram, entity and span (Sun et al., 2019; Joshi et al.,\n2020; Levine et al., 2021; Li and Zhao, 2021) are\nadopted throughout the entire pre-training. Another\nexample is that exploring a good enough (but still\nfixed) masking ratio has also been considered in\n(Wettig et al., 2022). Given the circumstances that\ntime-invariant masking applied in most MLM is\nnot adaptive to the changeable process of language\nmodel pre-training, time-invariant setting hardly\nhopefully reaches an optimal outcome. This\nmotivates us to explore the influence of masking\nratio and masked content in MLM pre-training and\npropose time-variance MLM setting to verify our\nhypothesis for better PrLMs.\n•Masking Ratio. Masking ratio controls the\nratio between the number of tokens to predict\nand the left corrupted context. It determines the\ncorruption degree that may affect the difficulty of\nrestoring the masked tokens; that is, the larger\nthe ratio is, the more masked contents model has\nto predict with less non-masked context. Our\nhypothesis is that at different training stages, the\nmodel may benefit from different masking ratios\nto balance the training from samples with different\ndifficulties compared to the fixed ratio.\nWe first explore the influence of different mask-\n7255\nTable 1: Examples of masking function words and non-function words. Intuitively, it is much easier to predict the\nmasked function words.\nMasking Strategy Example\nFunction words [MASK] apple [MASK] day keeps [MASK] doctor [MASK] .\nNon-function words An [MASK] a [MASK] [MASK] the [MASK] away.\ning ratios on downstream tasks at different stages\nthroughout the entire pre-training instead of the\nonly final stage. We find that a high masking ratio\ngives better performance for downstream tasks in\nearly stage, while a low ratio has a faster training\nspeed. Thus we choose a higher ratio as the starting\npoint and decay the masking ratio to a lower value\nduring the pre-training, namely Masking Ratio\nDecay (MRD), which can significantly outperform\nthe performance of the fixed ratio. MRD indicates\nthat MLM benefits from a time-variant masking\nratio at different training stages.\n•Masked Content. When placing all words\nwith an equal and fixed opportunity throughout the\nentire pre-training for prediction learning, it may be\nunnecessary for some ’easy’ words and insufficient\nfor some ’difficult’ words at the same time. Table\n1 shows an intuitive example that the sequence\nwith masked non-function words containing less\ninformation is much harder to predict compared\nto masked function words. Though in the very\nbeginning, all words are unfamiliar to the models.\nAs time goes on, the relative difficulties of words\nwill vary when the pre-training status changes. We\nshow that the losses of function words converge\nmuch faster than non-function words, which means\nnon-function words are much harder for models\nto learn. Therefore, the high proportion of func-\ntion words in a sequence leads to inefficiency if\nRandom-Token Masking is applied.\nTo handle training maturity for different types\nof words, we propose POS-Tagging Weighted\n(PTW) Masking to adaptively adjust the masking\nprobabilities of different types of words according\nto the current training state. PTW Masking makes\nthe model have more chance to learn ’difficult’\ntypes of words and less chance for ’easy’ ones from\nthe perspective of part-of-speech. By introducing\nthis adaptive schedule, our experimental results\nshow that MLM benefits from learning mostly non-\nfunction words.\nOur contributions are three folds: 1) We analyze\nthe insufficiency of current masking strategies\nfrom the perspectives of masking ratio and masked\ncontent and give a better understanding of MLM\npre-training in terms of masking. 2) To our best\nknowledge, this is a pioneer study to analyze the\nimpact of time-variant masking both in masking\nratio and masked content in MLM pre-training.\n3) Our analysis shows that the time-variant mask-\ning schedules can significantly improve training\nefficiency and effectiveness. Our sources will be\npublicly available.\n2 Preliminary Experiments\nThis section presents our preliminary experiments\nthat motivate us to explore time-variant masking\nschedules. We train BERT-base (Devlin et al.,\n2019) with the widely-used English Wikipedia\ncorpus to observe the influence of masking during\npre-training by measuring the downstream perfor-\nmance on the SQuAD v1.1 dataset (Rajpurkar et al.,\n2016) (more experimental details will be given in\nSection 4). The experiments aim to study how\nthe language model learns from the masked tokens\nwhen using conventional Random-Token Masking\nfrom the perspectives of the masking ratio and\nmasked content.\n2.1 Preliminaries of Masked Language Model\nIn general, Masked Language Modeling (MLM)\nis a denoising auto-encoding approach that is\nwidely used in language model pre-training by\nreconstructing the corrupted sequences. To be\nspecific, given a sequence x = {xi,x2,...,x n},\nwe use a certain masking strategy P to replace\np% tokens with special mask tokens. Accepting\nthe corrupted sequence as input, a language model\nparameterized by θis trained to predict the original\ntokens from masked ones in x using the pre-\ntraining objective stated below:\nLMLM(x,θ) =E\n(\n−\n∑\ni∈M\nlogpθ(xi |ˆx)\n)\n, (1)\nwhere ˆx is the reconstructed sequence that the\nlanguage model samples from the hidden states,\n7256\nand M denotes the index set of masked tokens\nwhere the loss will be calculated.\n2.2 Masking Ratio: Influence of Pre-training\nMasking Ratio\nThe masking ratio determines the corruption degree\nof a whole sequence for model training. We first\nconduct a simple experiment to explore the impact\nof different masking ratios on downstream tasks at\ndifferent training stages of entire pre-training.\nWe train BERT-base for 1M steps with a masking\nratio of 15%, 25%, and 35% respectively, as shown\nin Figure 1a. During pre-training, checkpoints are\nsaved every 50k steps, and finetuning is performed\non the SQuAD v1.1 to observe changes in down-\nstream performance. We find that the models using\nmasking ratios of 25% and 35% have a gap of\nmore than +1% F1 score in SQuAD compared with\nthe model using a masking ratio of 15% at the\nbeginning of training. However, in the second half\ntraining stage, the model with the masking ratio of\n15% catches up with models with higher ratios in\ndownstream performance.\n2.3 Masked Content: Influence of Different\nTypes of Words\nIn this section, we observe the influence of masked\ncontent by finding which kinds of words are more\nbeneficial to pre-training. In terms of part-of-\nspeech, words can be roughly divided into three\ncategories: non-function words, function words,\nand the others (punctuations, symbols, etc.). If\nwe mask all the function words and punctuations\nof a sentence, we can still infer roughly what the\nsentence is about. Instead, by masking all non-\nfunction words, we can hardly get any information\nfrom the sentence, as shown in Table 1.\nTo further explore the part-of-speech, we can\nspeculate that, for the language model, masking\ndifferent types of words leads to different difficul-\nties for pre-training.\nWith the help of POS-tagging tools2, we classify\nthe words in the corpus into mcategories3 when\ndoing pre-processing. In pre-training, for each\ntype of words, we calculate the corresponding\n2POS-tagging tool in spaCy: https://spacy.io/\n3Part-of-speech classification from the Universal POS tag\nset: https://universaldependencies.org/u/pos/\ncumulative loss ˜ℓk,t at tsteps as follows:\nLMLM(x,θ) =\n∑\nk∈C\nℓk,t ,\n˜ℓk,t = β·˜ℓk,t−1 + (1−β) ·ℓk,t ,\n(2)\nwhere k ∈ Cdenotes the word type k in set C\nof mcategories and β ∈(0,1) is a coefficient to\nbalance the exponential weighted average. We use\nexponential weighted average to smooth the losses\nbecause temporary losses of different batches vary\ngreatly, leading to corresponding weights jittering\n(more details will be discussed in Section 3.3).\nWe train BERT-base for 200k steps with a fixed\nmasking ratio of 15%. We record the cumulative\nlosses of different types of words separately every\n10 steps and observe the changes in losses. We find\nthat the language model does have higher losses\nfor masked non-function words and lower losses\nfor masked function words. The latter quickly\nconverges to very small values from the start, as\nshown in Figure 1b.\n2.4 Analysis\nMasking Ratio: Why Time-invariant Masking\nRatio Is Not the Best Choice? From the ex-\nperimental results in Figure 1a, there is such an\nempirical law: at the beginning, the downstream\nperformance with a high masking ratio has a higher\nstarting point but grows at a relatively slower speed\nand is caught up with the model with masking\nratio of 15%. That is, the model with the masking\nratio of 15% has a low starting point but boosts\nperformance faster in the later stage. Given this\nobservation, we show that we can apply a relatively\nhigh masking ratio to train models to get a better\nmodel using less time. On the other hand, we\napply a lower masking ratio to train models, which\nobtains better downstream performance if we train\nfor enough time. But if we use a decaying masking\nratio instead of a fixed one, we can absorb the\nadvantages of both high and low masking ratios.\nMasked Content: Why Random-Token Masking\nIs Suboptimal? For a sentence, the numbers\nof non-function words and function words are\nquite similar. Therefore, for Random-Token Mask-\ning, the model pays equal attention to learning\nfrom these two kinds of words. However, the\nexperimental results in Figure 1b show that the\nlanguage model dissipates its effort to model some\nfunction words, of which losses have been very\n7257\n82\n84\n86\n88\n90\n200k 400k 600k 800k\nbert-0.15\nbert-0.25\nbert-0.35\nStep\nSQuADv1.1 F1 Score\n(a) SQuAD v1.1 performance using\ndifferent masking ratios.\nStep\nLoss (log scale)\nADJ ADV NOUN PROPN VERB\nAUX DET PUNCT PRON ADP\n(b) Cumulative losses of different types\nof words (shown 10 of them).\nADJ ADV NOUN PROPN VERB\nAUX DET PUNCT PRON ADP\nStep (log scale)\nWeights\n/gid01092\n(c) Weighting changes of different\ntypes of words (shown 10 of them).\nFigure 1: (a) Experiment on masking ratio: The evaluation metric F1 score measures the average textual overlap\nbetween the prediction and ground truth answer. (b) Experiment on masked content: Cumulative losses of different\ntypes of words (shown 10 of them). The losses of non-function words (in full line) are much greater than the losses\nof function words and punctuation (in dotted line). (c) Weight changes of PTW according to the cumulative losses\nin (b): Based on Equation 5, the masking weights of non-function words (in full line) are much higher than the\nweights of function words and punctuation (in dotted line).\nlow. Meanwhile, Random-Token Masking lets\nthe model less likely learn those supposed-to-be-\nlearn-more non-function words, which surely gives\nsuboptimal pre-training consequences.\n3 Time-variant Masking Strategies\nIn this section, we will present our exploration\nof time-variant masking on masking ratio and\nmasked content inspired by our findings above. The\noverview of our time-variant masking is presented\nin Figure 3.\n3.1 Masking Ratio Decay (MRD)\nAccording to the observation in Section 2.2, we\ndesign an optimized Masking Ratio Decay (MRD)\nschedule. At the beginning of pre-training, we\nuse a high masking ratio and decay the masking\nratio using certain strategies, which is similar to\nlearning rate decay without warmup. Assuming\nthat the model generally adopts a fixed masking\nratio p% for training, we use a very high masking\nratio (about 2p%) as the starting point and a very\nlow masking ratio (nearly zero) as the ending point\nin MRD.\n3.1.1 Implementation of Two Decay Methods\nWe have tried two kinds of MRD to dynamically\nadjust the masking ratio, namely linear decay and\ncosine decay as follows:\nMlinear(t) = (1− t\nT) ·2p%, (3)\nMcosine(t) = (1 +cos( π\nTt)) ·p% + 0.02, (4)\nwhere M(t) is the current masking ratio at training\nstep tand T is the total training step. Linear decay\nstarts at 2p% and decays to 0, while cosine decay\nstarts at 2p% + 0.02 and decays to 0.02, as shown\nin Figure 2.\nfixed 15%\nlinear decay\ncosine decay\nMasking ratio\nStep\nFigure 2: Masking Ratio Decay schedules (p= 15).\n3.1.2 Details of Design Intention\nWe choose the starting point of 2p% and ending\npoint of 0 because the model using MRD can learn\nalmost the same number of masked tokens as the\nbaseline using a fixed masking ratio p% due to the\ncentral symmetry of linear and cosine functions for\nfair comparisons. The reason why we add 0.02 to\nthe cosine decay is that the value of cosine function\n(masking ratio) is nearly 0 in the final 5% steps,\nwhich means there are no masked tokens for model\nto learn (and loss diminishes to 0). Thus we set\na small number (0.02) to make the model keep\ntraining in the final stage.\n3.2 Analysis for How MRD Works\nMRD reminds us of the Simulated Annealing (SA)\nalgorithm (Kirkpatrick et al., 1983), which is a\ngreedy algorithm for optimization. In the SA\n7258\n                 \nAll\n             \n                 \nroads              \n                 \n.              \n                 \nRome             \n                 \nto             \n                 \nlead             \n                 \nDET\n             \n                 \nNOUN              \n                 \nPROPN\n             \n                 \nADP\n             \n                 \nVERB             \n                 \nADP              \n                 \nPUNCT              \n                 \nPOS-Tagging Weighted\n             \n                 \nMasked  Language  Model\n             \n                 \nAll\n             \n                 \n[MASK]              \n                 \n.              \n                 \nRome             \n                 \nto             \n                 \nlead             \n                 \nMasking Ratio Decay\n             \n                 \nSampling\n             \n                 \nUpdate the cumulative \nloss of NOUN\n           \n  \n                 \ninput             \n                 \nApply               to \n                 \nprobabilities\n             \nFigure 3: Illustration of our time-variant masking.\nalgorithm, the degree of acceptance of suboptimal\nsolutions depends on the annealing temperature T\naccording to the Metropolis algorithm (Metropolis\net al., 1953). That is, the higher the temperature\nparameter T is, the larger the solution space\nallowed to be explored. Thus, the model can easily\njump out the local minima if the T is large. As\nthe model converges and the annealing temperature\ndecreases, the intolerance of suboptimal solution\nrises and a better local optimal solution can be\nfound. In MRD, the magnitude of annealing\ntemperature T can be analogous to the masking\nratio p%. A high masking ratio means less\ninformation in the input sequence, allowing the\nmodel to explore more possibilities on coarse-\ngrained task. On the other hand, a low masking\nrate allows the model to focus on finding a better\nsolution close to global minima on the fine-grained\ntask.\n3.3 POS-Tagging Weighted (PTW) Masking\nIn this section, on the basis of the discussion of\nSection 2.3, we present the POS-Tagging Weighted\n(PTW) Masking, making the models have more\nchance to train on the difficult words according to\nthe current training state.\nFirstly, in the data pre-processing part, we use\nPOS-tagging tools to label the words with corre-\nsponding part-of-speech. We then use WordPiece\nTokenizer to perform tokenization and align tokens\nwith their part-of-speech tags while ignoring the\nspecial tokens [CLS], [SEP], and [PAD].\nBefore training batch starts, we first apply\nPTW Masking to corrupt the sequences, while\nthe masking ratio remains unchanged. When the\nmodel is trained at tsteps, according to Equation\n2, we can obtain cumulative loss vector ˜LMLM =\n{˜ℓ1,˜ℓ2,..., ˜ℓk,..., ˜ℓm},k ∈C for m categories\nof words, and the cumulative loss vector ˜LMLM\nis converted into the corresponding weight vector\nWPOS = {w1,w2,...,w k,...,w m},wk ∈(0,1)\nby the following equation:\nWPOS = sigmoid\n(˜LMLM −E( ˜LMLM)√\nVar( ˜LMLM) ·µ\n)\n, (5)\nwhere µ is a coefficient to adjust the input for\nsigmoid function. We apply this weight vector\nWPOS to the masking probabilities. Equation 5\nis based on Equation 2, where the process of\nsmoothing gives the weights changing relatively\nstably for masking. We do not use bias correction\nin Equation 2, which enables the cumulative losses\nfor each kind of words to grow from zero. That is,\nin the very beginning, the WPOS is initialized with\nthe same value for each type of words and weights\nthe probabilities for masking equally.\nSpecifically, the masking probability of each\nword is weighted by its corresponding part-of-\nspeech, so that words with higher losses are\nmore likely to be masked. We show that non-\nfunction words tend to have much higher losses\nthan function words, so the language model learns\nto model non-function words most of the time, but\nfewer function words and punctuation, as shown in\nFigure 1c. In special case, PTW Masking is similar\nto Named Entities Masking (Sun et al., 2019) if\nonly proper nouns have a weight of 1 and the others\nare 0.\n4 Experiment Setup\n4.1 Datasets and Setup\nFor full details of pre-training and finetuning,\nplease see the Appendix C.\n4.1.1 Pre-training\nFor pre-training, we use the BERT-base and BERT-\nlarge model as the representatives of MLMs for\n7259\ntraining. Specifically, we train BERT-base for 200k\nsteps from scratch and continue training BERT-\nlarge models for 100k steps initialized by pre-\ntrained weights 4. In practice, we explore masking\nratio with MRD and masked content with PTW\nMasking seperately to avoid mutual influence. For\nthe dataset, we train BERT on English Wikipedia\nusing WordPiece Tokenizer for tokenization.\n4.1.2 Finetuning\nWe finetune our models on GLUE (Wang et al.,\n2019) and SQuAD v1.1 (Rajpurkar et al., 2016) to\nevaluate the performance of downstream tasks. To\nfurther explore what extra skills models using PTW\nMasking have learnt, we evaluate BERT-large on\nCoNLL-2003 (Tjong Kim Sang and De Meulder,\n2003), which is a widely used NER benchmark, to\ntest the information extraction ability of the models.\nWe train all tasks 5 times respectively and report\nthe average scores. For more detailed introduction\nof downstream tasks, please see Appendix B.\n4.2 Implementation Details\nFor the implementation of MRD, we train the\nmodel using Random-Token Masking with a fixed\nmasking ratio of 15% as the baseline. Then we\napply our MRD to Random-Token Masking with\nan average masking ratio of 15% ( p = 15) using\nlinear and cosine decay, respectively.\nBecause there is the parameter T in Equation\n3, the number of training batches learned by the\nmodels under a certain masking ratio will differ\nif total training step T is different. As shown\nin Figure 4, compared to the model with 200k\nsteps, the models with 1M steps are trained for\nlarge masking ratio for longer time in early stage.\nThis question will not be raised if we use the fixed\nmasking ratio as we usually do. But in MRD,\nthough both masking ratios decay in relatively\nthe same way, the absolute difference of masking\nratio in early stage may affect the performance of\ndownstream task. Thus, we additionally train the\nmodels for 1M steps to explore if training on large\nmasking ratio longer time or decaying faster at\nearly stage is more beneficial to pre-training.\nFor the implementation of PTW Masking, we\nalso use Random-Token Masking with a fixed\nmasking ratio of 15% as baseline. We train the\nmodel with PTW Masking with the same fixed\nratio of 15% for equal comparisons.\n4https://huggingface.co/bert-large-uncased\nStep\nMasking ratio\ncosine 200k\ncosine 1M\nFigure 4: Different total training steps cause an absolute\ndifference in masking ratio in the early stage of pre-\ntraining though same MRD strategies are applied.\n5 Results\n5.1 Results of MRD\nThe experimental results show that MRD greatly\nimproves downstream task performance and pre-\ntraining efficiency (and more discussion on MRD\nin Appendix A).\n5.1.1 Decaying Masking Ratio vs. Fixed Ratio\nIn Figure 5, we show the SQuAD performance\nfor every 50k checkpoint during pre-training. We\nobserve that the large masking ratio gives a better\ndownstream performance at the start and the decay-\ning mechanism continues to boost the downstream\nperformance, which takes the advantage of high\nmasking ratio and low masking ratio discussed in\nSection 2.4. The model using cosine decay at 650k\nsteps has obtained a competitive SQuAD v1.1 F1\nscore to the baseline at 1M steps, thus reducing the\ntraining time by 35%.\n5.1.2 Influence of Masking Ratio at Early\nStage\nWe further explore the absolute difference (men-\ntioned in Section 4.2) with different training steps\nin masking ratio using the same MRD strategies.\nAs shown in Table 2, for GLUE tasks, MRD\ntraining for 1M steps has more obvious advantages\nthan 200k steps. The model trained with 1M\nsteps performs well above baseline on all subtasks\nusing MRD, with an average increase of 1+ on\nGLUE, which has a larger increment compared to\n200k steps. The comparison shows that models\nbenefit from training for a longer time with a\nlarge masking ratio from the start, especially on\nGLUE. Because the subtasks in GLUE are mainly\nsequence-level, which focus on global semantics.\nFor a higher masking ratio, the model tries to train\n7260\nTable 2: Evaluating the performance of models using MRD but trained with two different total training steps. For\nGLUE, we report STS using Spearman correlation and CoLA using Matthew’s correlation, and other tasks using\naccuracy. And we report SQuAD using F1.\nModel CoLA SST MRPC STS QQP MNLI QNLI RTE GLUE Avg SQuAD\nMasking Ratio Decay\nTrained for 200k steps\nRandom-Token 15% 43.8 90.4 85.6 85.9 90.0 81.5 88.7 62.2 78.5 87.1\nLinear Decay 44.5 90.5 86.0 85.4 90.0 81.0 89.6 63.2 78.8(↑0.3) 87.7( ↑0.6)\nCosine Decay 44.9 90.3 85.8 86.1 90.2 81.2 89.7 62.6 78.9(↑0.4) 87.7(↑0.6)\nTrained for 1M steps\nRandom-Token 15% 54.8 91.5 86.2 86.8 90.3 83.7 90.8 64.7 81.1 90.2\nLinear Decay 55.2 92.3 86.8 87.7 90.4 84.4 91.2 70.4 82.3(↑1.2) 90.6( ↑0.4)\nCosine Decay 57.2 92.3 88.2 87.8 90.5 84.7 92.3 69.5 82.8(↑1.7) 90.9(↑0.7)\nTable 3: Evaluating the performance of models using PTW Masking. We report CoNLL using F1.\nModel CoLA SST MRPC STS QQP MNLI QNLI RTE GLUE Avg SQuAD CoNLL\nPTW Masking\nBERTbase from scratch\nRandom-Token 43.8 90.4 85.6 85.9 90.0 81.5 88.7 62.2 78.5 87.1 -\nPTW 45.3 90.0 85.4 86.8 90.1 82.0 91.1 62.5 79.2 (↑0.7) 88.1(↑1.0) -\nBERTlarge from continue-training\nRandom-Token 61.8 92.8 86.5 89.3 91.3 86.3 92.4 67.2 83.4 91.3 94.9\nPTW 62.5 93.3 86.8 90.0 91.3 86.6 92.5 68.2 83.9 (↑0.5) 91.6(↑0.3) 95.4(↑0.5)\n83\n85\n87\n89\n91\n200k 400k 600k 800k 1000k\nStep\nSQuADv1.1 F1 Score\nﬁxed 1M\ncosine 1M\nﬁxed 200k\ncosine 200k\n35%\nFigure 5: Comparisons between fixed ratio and cosine\ndecay strategy on SQuAD performance during pre-\ntraining. We evaluate the saved checkpoints for every\n50k steps on SQuAD v1.1 dev set following the same\nexperimental setup.\non a coarse-grained task, inferring global semantics\nfrom fewer words, which is more suitable for\nGLUE. Therefore, in the training of 200k steps,\nthe masking ratio decays too fast, resulting in\ninsufficient training on coarse-grained tasks. In\ncontrast, the model with 1M steps can maintain the\ntraining at a high masking ratio for a longer time\nand thus perform better in the sequence-level tasks\nof GLUE.\n5.1.3 Comparisons Between Different MRD\nCompared with linear decay, cosine decay has bet-\nter downstream task performance in most subtasks\nin GLUE. The difference between these two is\nthat cosine decay keeps a higher masking ratio in\nthe early stage and decays more quickly, which is\nconsistent with the analysis mentioned above. To\nmove forward, it is necessary to maintain a high\nmasking ratio in the early stage of pre-training.\nAccording to Section 3.2’s empirical analysis, the\nmodel can explore a larger global solution space by\nusing a higher masking ratio in the early training\nperiod so as to better converge to the optimal global\nminima when the masking ratio decreases later.\n5.2 Results of PTW Masking\nResults in Table 3 show that PTW Masking has\nsignificantly improved the performance of various\ndownstream tasks.\n5.2.1 What Skills Models Have Learnt if\nTrained with Mostly Non-function\nWords with PTW Masking?\nBecause models trained with PTW Masking\npay more attention to important words like\nnon-function words, we can see that models\nare especially good at information extraction\ntasks, e.g., extractive QA and NER. The gains\n7261\nin SQuAD and CoNLL in Table 3 have shown\nmodels are sensitive to words with more semantic\ninformation, which is consistent with the design of\nthe pre-training goal.\nBesides that, PTW Masking also does well\nin several NLU tasks in GLUE. PTW Masking\nis a time-variant strategy, which aggregates the\nadvantages of both Random-Token Masking and\nNamed Entities Masking. From the start of pre-\ntraining, models can learn from all the words\nequiprobably like Random-Token Masking. And at\nthe later stage, models memorize more knowledge\nby masking important words instead of wasting\ntime on predicting meanless words. Thus models\ncan have better NLU ability with memorization of\nmore knowledge under the condition of training\nwith the same number of tokens.\n6 Related Work\nThe pre-processing of the MLM is to replace a\nsubset of the tokens in the input with [MASK]\ntokens, which has two considerations to optimize:\nhow many tokens to mask (masking ratio) and what\ntokens to mask (masked content).\n•Masking Ratio . Masking ratio is a very\nimportant hyperparameter that affects the pre-\ntraining of MLM, which is relatively seldom\nstudied. In BERT, the masking ratio of 15% is the\nmost commonly used value and is also applied in\nother MLMs. The generator of ELECTRA (Clark\net al., 2020) is a MLM, using 15% for base-sized\nmodels and 25% for large-sized models. However,\nconsidering the cooperation with the discriminator,\nit is difficult to judge the effect of 25% on MLM.\nIn a recent study, (Wettig et al., 2022) suggests that\na masking ratio of 40% performs better than 15%\nin downstream tasks of RoBERTa-large (Liu et al.,\n2019) model. T5 (Raffel et al., 2020) uses an MLM-\nstyle pre-training method and also experiments on\nthe influence of different masking ratios. They find\nthat the masking ratio has a limited effect on the\nmodel’s performance except for 50% and use 15%\nas the final choice. To our best knowledge, most\nstudies on masking ratio compare the performances\nof downstream tasks at the end of pre-training\n(Raffel et al., 2020), but few studies pay attention\nto the dynamic influence of masking ratio during\npre-training, which is very interesting. We record\nthe changes in the performance of downstream\ntasks under different masking ratios and therefore\npropose the MRD according to the empirical law\nwe observe. Instead of using a fixed masking ratio,\nwe dynamically decay the ratio and find that the\nperformance of MLM can be greatly improved.\n•Masked Content. Previous studies have ex-\nplored strategies for masked content to further im-\nprove the Random-Token Masking, though nearly\nall of them focus on how to select coherent enough\nmasked units. (Devlin et al., 2019) proposes\nWhole-Word Masking, which forces the model\nto predict complete words instead of WordPiece\ntokens. Furthermore, SpanBERT (Joshi et al.,\n2020), n-gram Masking (Levine et al., 2021; Li\nand Zhao, 2021) and LIMIT-BERT (Zhou et al.,\n2020) take into account the continuous mask of\nmultiple word combinations, making model predict\ntokens using the context with long dependencies.\nERNIE (Sun et al., 2019) improves pre-training\nperformance by especially masking named entities.\nDifferent from all existing MLM improvements,\nour PTW Masking lets different types of words\ncorrespondingly receive the matched learning in-\ntensity, which pioneers a new technical line for the\nconcerned MLM.\n7 Conclusion\nMasked language model pre-training can be gener-\nally defined by two main factors, masking ratio and\nmasked contents. The Random-Token Masking\nscheme adopted by existing studies treats all words\nequally and maintains a fixed ratio throughout\nthe entire pre-training, which has been shown\nsuboptimal in our analysis. To better unleash\nthe strength of MLM, we explore two kinds of\ntime-variant masking strategies, namely, Masking\nRatio Decay (MRD) and POS-Tagging Weighted\n(PTW) Masking. Experimental results verify our\nhypothesis that MLM benefits from time-variant\nsetting both in masking ratio and masked content\naccording to dynamic training states. Our further\nanalysis show that these two time-variant masking\nschedules greatly improve pre-training efficiency\nand the performance of downstream tasks.\nLimitations\nOne limitation of this work is that both PTW\nMasking and MRD are conducted only on BERT\ndue to limited resources, and MLMs with other\nstructures may have different reactions to the time-\nvariant masking with different contents and ratios.\nAnother limitation is that although we propose\nMRD for the first time, the strategy of time-variant\n7262\nmasking ratio is hard to design like learning rate\ndecay. In fact, other decay methods and choices of\nstarting and ending point are various, where better\nstrategies may exist and further work can be done.\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang,\nNan Yang, Xiaodong Liu, Yu Wang, Jianfeng\nGao, Songhao Piao, Ming Zhou, and Hsiao-Wuen\nHon. 2020. Unilmv2: Pseudo-masked language\nmodels for unified language model pre-training. In\nProceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 ofProceedings of Machine\nLearning Research, pages 642–652. PMLR.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. ArXiv\npreprint, abs/2002.08909.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Association\nfor Computational Linguistics, 8:64–77.\nScott Kirkpatrick, C Daniel Gelatt Jr, and Mario P\nVecchi. 1983. Optimization by simulated annealing.\nscience, 220(4598):671–680.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend,\nKevin Leyton-Brown, Moshe Tennenholtz, and Yoav\nShoham. 2021. PMI-masking: Principled masking\nof correlated spans. In International Conference on\nLearning Representations.\nYian Li and Hai Zhao. 2021. Pre-training universal\nlanguage representation. In Proceedings of\nthe 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 5122–5133, Online.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. Roberta: A robustly optimized bert pretraining\napproach. ArXiv preprint, abs/1907.11692.\nNicholas Metropolis, Arianna W Rosenbluth, Mar-\nshall N Rosenbluth, Augusta H Teller, and Edward\nTeller. 1953. Equation of state calculations by\nfast computing machines. The journal of chemical\nphysics, 21(6):1087–1092.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving lan-\nguage understanding by generative pre-training.\nURL https://s3-us-west-2.amazonaws.com/openai-\nassets/researchcovers/languageunsupervised/\nlanguage understanding paper.pdf.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods\nin Natural Language Processing , pages 2383–\n2392, Austin, Texas. Association for Computational\nLinguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng,\nXuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,\nHao Tian, and Hua Wu. 2019. Ernie: Enhanced\nrepresentation through knowledge integration. ArXiv\npreprint, abs/1904.09223.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis\nplatform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and\nDanqi Chen. 2022. Should you mask 15% in\n7263\nmasked language modeling? arXiv preprint\narXiv:2202.08005.\nWenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu,\nShuohang Wang, Yichong Xu, Michael Zeng, and\nMeng Jiang. 2021. Dict-bert: Enhancing language\nmodel pre-training with dictionary. arXiv preprint\narXiv:2110.06490.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nJunru Zhou, Zhuosheng Zhang, Hai Zhao, and\nShuailiang Zhang. 2020. LIMIT-BERT : Linguistics\ninformed multi-task BERT. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 4450–4461, Online. Association for\nComputational Linguistics.\nA Additional Investigation on MRD\nA.1 Magnitude of Masking Ratio in MRD\nWhen using MRD, we explore the influence of\nthe much higher masking ratios, which affect the\ndownstream performance of the model. Previous\nstudies (Raffel et al., 2020; Wettig et al., 2022)\nhave shown that a much higher fixed masking ratio\n(≫40%) will cause significant degradation in the\nmodel performance because the model can only\ninfer from a small amount of known information\nresulting in quickly converging to local minima.\nIn MRD, we show that the design of the decaying\nmechanism can mitigate the impact of the high\nmasking ratio. For the BERT-base model, starting\nfrom a high ratio ( 30%) and a much higher ratio\n(55%), both can outperform the baseline with a\nsimilar margin. We show that higher masking\nratios in early pre-training stage help downstream\nperformance, and MRD prevents the high masking\nratios from destroying pre-training in later stage.\nA.2 MRD Interacts with Learning Rate\nMoreover, we show a subtle relationship between\nMRD and learning rate decay. When the masking\nratio is low, using a relatively high learning rate\nwill cause a huge decline in model performance.\nTherefore, in MRD, the masking ratio and the\nlearning rate both adopt the same type of decay\nstrategy except that the learning rate has an addi-\ntional warmup stage. For example, cosine masking\nratio decay uses cosine learning rate decay.\nA.3 Other Simple Schedules in MRD\nBased on the same experiment setup, we train\nthe models with other simple schedules (shown\nin Figure 6) for 200k steps using the linear learning\nrate decay and finetune them on the SQuAD v1.1.\nThe results on SQuAD v1.1 dev set are presented\nin Table 4. We find that cosine is the best compared\nwith those alternatives.\nModel SQuAD v1.1\nEM F1\nMasking Ratio Decay\nRandom 15% 79.4 87.1\n−ax2(a> 0) Decay 79.8 87.6\nax2(a> 0) Decay 79.3 87.2\nAscending 79.6 87.4\nAscending then Decaying 79.4 86.9\nLinear Decay 79.8 87.7\nCosine Decay 79.9 87.7\nTable 4: Results of other simple schedules on the\nSQuAD v1.1 dev set.\nStep\nMasking ratio\nFigure 6: Other simple schedules of adjusting the\nmasking ratios.\nB Finetuning Benchmarks\nGLUE The General Language Understanding\nEvaluation (GLUE) benchmark is a collection of 9\nvarious tasks for sequence-level classification for\nevaluating natural language understanding systems.\nSQuAD The Stanford Question Answering\nDataset (SQuAD) is a commonly used benchmark\nfor question answering. The task is to predict\nthe text span of an answer from a given passage-\nquestion pair.\nCoNLL The CoNLL-2003 concerns language-\nindependent named entity recognition. It concen-\n7264\ntrates on four types of named entities: persons, lo-\ncations, organizations and names of miscellaneous\nentities that do not belong to the previous three\ngroups.\nC Pre-training and Finetuning Details\nC.1 Pre-training Details\nWe only use the MLM task as the training objective\nand discard the Next Sentence Prediction task, as it\nhas been shown to be redundant in previous studies\n(Liu et al., 2019; Joshi et al., 2020).\nHyperparameter Base Large\nLearning Rate 2e-4 5e-5\nWarmup Steps 10000 10000\nWeight Decay 0.01 0.01\nBatch size 256 256\nSequence Length 512 512\nGradient Clipping 1.0 1.0\nTable 5: Pre-training hyperparameters for BERT\nmodels.\nC.2 Finetuning Details\nFollowing the common finetuning practice, we do\nnot use any additional training strategies. We train\nall tasks 5 times respectively and report the average\nscores. For GLUE, We use 8 widely used tasks in\nGLUE.5\n5For a fair comparison, we exclude the WNLI following\nthe previous work (Devlin et al., 2019).\nHyperparameter Base Large\nGLUE\nLearning Rate {5e-5, 1e-4} {1e-5, 2e-5}\nBatch Size 32 32\nWeight Decay 0 0\nTraining Epochs* 3 3\nSQuAD v1.1\nLearning Rate {5e-5, 1e-4} {2e-5, 5e-5}\nBatch Size 128 32\nWeight Decay 0 0\nTraining Epochs 3 3\nCoNLL-2003\nLearning Rate - {2e-5, 5e-5}\nBatch Size - 32\nWeight Decay - 0\nTraining Epochs - 3\nTable 6: Finetuning hyperparameters for BERT models.\n*We finetune our models in RTE and STS-B for 10\nepochs and other subtasks for 3 epochs.\n7265\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\n4\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. Left blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7266\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n2\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n7267"
}