{
  "title": "Social determinants of health extraction from clinical notes across institutions using large language models",
  "url": "https://openalex.org/W4410462384",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4327705837",
      "name": "Vipina K Keloth",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2602157910",
      "name": "Salih Selek",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2115254388",
      "name": "Qing-Yu Chen",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2474195387",
      "name": "Christopher Gilman",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2722421078",
      "name": "Sunyang Fu",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2519110038",
      "name": "Yifang Dang",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2129379693",
      "name": "Xinghan Chen",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2101917445",
      "name": "Xinyue Hu",
      "affiliations": [
        "Mayo Clinic in Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2123189166",
      "name": "Yujia Zhou",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2097683557",
      "name": "Huan He",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A4297377364",
      "name": "Jungwei W. Fan",
      "affiliations": [
        "Mayo Clinic in Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2185238367",
      "name": "Karen Wang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2046099468",
      "name": "Cynthia Brandt",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2137772693",
      "name": "Cui Tao",
      "affiliations": [
        "Mayo Clinic in Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2310998875",
      "name": "Hongfang Liu",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A4327705837",
      "name": "Vipina K Keloth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2602157910",
      "name": "Salih Selek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115254388",
      "name": "Qing-Yu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2474195387",
      "name": "Christopher Gilman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2722421078",
      "name": "Sunyang Fu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2519110038",
      "name": "Yifang Dang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129379693",
      "name": "Xinghan Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101917445",
      "name": "Xinyue Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123189166",
      "name": "Yujia Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097683557",
      "name": "Huan He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297377364",
      "name": "Jungwei W. Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2185238367",
      "name": "Karen Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2046099468",
      "name": "Cynthia Brandt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137772693",
      "name": "Cui Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2310998875",
      "name": "Hongfang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2116769437",
    "https://openalex.org/W2128248134",
    "https://openalex.org/W2113596257",
    "https://openalex.org/W2116839388",
    "https://openalex.org/W3202375701",
    "https://openalex.org/W2973821471",
    "https://openalex.org/W2802380813",
    "https://openalex.org/W2891941547",
    "https://openalex.org/W2027867013",
    "https://openalex.org/W2404369708",
    "https://openalex.org/W4327703607",
    "https://openalex.org/W4286489826",
    "https://openalex.org/W3177105417",
    "https://openalex.org/W3168788431",
    "https://openalex.org/W4390921272",
    "https://openalex.org/W4367692366",
    "https://openalex.org/W4362456756",
    "https://openalex.org/W4367042504",
    "https://openalex.org/W2941900633",
    "https://openalex.org/W4394794357",
    "https://openalex.org/W4366332256",
    "https://openalex.org/W4321002059",
    "https://openalex.org/W3015667612",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W4206221224",
    "https://openalex.org/W2146089916",
    "https://openalex.org/W2620592335",
    "https://openalex.org/W3094522459",
    "https://openalex.org/W3123287807",
    "https://openalex.org/W2908358765",
    "https://openalex.org/W3128717124",
    "https://openalex.org/W4221111264",
    "https://openalex.org/W3188301986",
    "https://openalex.org/W1592969533",
    "https://openalex.org/W4398142298",
    "https://openalex.org/W2511464381",
    "https://openalex.org/W4380151643",
    "https://openalex.org/W4390745503",
    "https://openalex.org/W4402580824",
    "https://openalex.org/W3200519641",
    "https://openalex.org/W2755642596",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4393094733",
    "https://openalex.org/W4408165783",
    "https://openalex.org/W4391292768",
    "https://openalex.org/W4409190866",
    "https://openalex.org/W6614377781",
    "https://openalex.org/W6603713569",
    "https://openalex.org/W6810423267",
    "https://openalex.org/W4389519241",
    "https://openalex.org/W3012501605",
    "https://openalex.org/W4312210066",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W6600769105",
    "https://openalex.org/W4389519059",
    "https://openalex.org/W3104193169",
    "https://openalex.org/W4380204593",
    "https://openalex.org/W4313527351",
    "https://openalex.org/W2464683149",
    "https://openalex.org/W2515637793",
    "https://openalex.org/W2165748167",
    "https://openalex.org/W1911952697",
    "https://openalex.org/W4297242650",
    "https://openalex.org/W2075137165",
    "https://openalex.org/W2001322210",
    "https://openalex.org/W2146408445",
    "https://openalex.org/W4318753563",
    "https://openalex.org/W3167009549",
    "https://openalex.org/W2902065466",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3196136260",
    "https://openalex.org/W1981459575",
    "https://openalex.org/W3086590218"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01645-8\nSocial determinants of health extraction\nfrom clinical notes across institutions\nusing large language models\nCheck for updates\nVipina K. Keloth1, Salih Selek2, Qingyu Chen1, Christopher Gilman1, Sunyang Fu 3,Y i f a n gD a n g3,\nXinghan Chen4,X i n y u eH u5, Yujia Zhou1, Huan He1, Jungwei W. Fan6,K a r e nW a n g1,7,C y n t h i aB r a n d t1,\nCui Tao5,6,H o n g f a n gL i u3 &H u aX u1\nDetailed social determinants of health (SDoH) is often buried within clinical text in EHRs. Most current\nNLP efforts for SDoH have limitations, investigating limited factors, deriving data from a single\ninstitution, using speciﬁc patient cohorts/note types, with reduced focus on generalizability. We aim to\naddress these issues by creating cross-institutional corpora and developing and evaluating the\ngeneralizability of classiﬁcation models, including large language models (LLMs), for detecting SDoH\nfactors using data from four institutions. Clinical notes were annotated with 21 SDoH factors at two\nlevels: level 1 (SDoH factors only) and level 2 (SDoH factors and associated values). Compared to other\nmodels, instruction tuned LLM achieved top performance with micro-averaged F1 over 0.9 on level 1\ncorpora and over 0.84 on level 2 corpora. While models performed well when trained and tested on\nindividual datasets, cross-dataset generalization highlighted remaining obstacles. Access to trained\nmodels will be made available athttps://github.com/BIDS-Xu-Lab/LLMs4SDoH.\nIn the United States, social factors, including educational level, racism, and\npoverty, contribute to over a third of total deaths in a year1. According to a\nstudy that surveyed the number of deathsattributable to social factors in the\nUnited States in the year 2000, 245,000 deaths were attributed to low\neducation, 176, 000 to experiences of racism, 162 ,000 to low social support,\n133 ,000 to individual-level poverty, 119, 000 to income inequality, and\n39 ,000 to area-level poverty\n1. It should be noted that such mortality esti-\nmates are comparable to that of leading disease-related mortality rates. A\ngrowing body of research has reported on the negative impact of social\ndeterminants of health (SDoH) factors (socioeconomic and environmental\nfactors that describe the conditions inwhich people are born, live, work, and\nage) creating undesirable circumstances, such as health disparities and\ndiscrimination\n2–4. For example, the likelihood of premature death increases\nas income goes down2, children born to parents who haven’tc o m p l e t e dh i g h\nschool are more likely to live in environments that contain barriers to\nhealth\n3, and low education levels are directly correlated with low income,\nhigher likelihood of smoking, and shorter life expectancy2.W en o t et h a ti n\nthe context of this study the term SDoH is used in a broad sense covering not\nonly socio-economic factors, but also behavioral, environmental, and other\nclosely linked factors. This aligns with the comprehensive deﬁnition of\nSDoH as outlined by Healthy People 2030\n5 a n di ss u p p o r t e db yas y s t e m a t i c\nreview of prior literature on SDoH6.\nThe mounting body of evidence demonstrating the impact of social\ndeterminants on health has resulted in an increasing acknowledgment\nwithin the health care sector that achieving improved health and health\nequity is likely to be contingent, at least in part, on addressing unfavorable\nsocial determinants\n7–10. Over the last decade, electronic health record\n(EHR) systems have been increasingly implemented at US hospitals and\nhave generated large amounts of longitudinal patient data. These massive\nEHR databases are becoming enabling resources for diverse types of\nclinical, genomic, and translational research, with successful demon-\nstrations by large initiatives such as the Electronic Medical Records and\nGenomics (eMERGE) Network\n11, the Patient-Centered Outcomes\nResearch Institute (PCORI)12, and the Observational Health Data Science\nand Informatics (OHDSI) consortium13. However, most current studies\ndo not use the full spectrum of EHR data, and more efforts are being\ndevoted to including unstructured textual data in EHRs for real world\nevidence generation\n14.\n1Department of Biomedical Informatics and Data Science, Yale School of Medicine, New Haven, CT, USA.2Department of Psychiatry and Behavioral Sciences,\nUTHealth McGovern Medical School, Houston, TX, USA.3McWilliams School of Biomedical Informatics, University of Texas Health Science Center at Houston,\nHouston, TX, USA.4School of Public Health, University of Texas Health Science Center at Houston, Houston, TX, USA.5Department of Artiﬁcial Intelligence and\nInformatics, Mayo Clinic, Jacksonville, FL, USA.6Department of Artiﬁcial Intelligence and Informatics, Mayo Clinic, Rochester, MN, USA.7Equity Research and\nInnovation Center, Yale School of Medicine, New Haven, CT, USA.e-mail: hua.xu@yale.edu\nnpj Digital Medicine|           (2025) 8:287 1\n1234567890():,;\n1234567890():,;\nIn clinical practice, there has been a lack of systematic collection and\nanalysis of SDoH deﬁned in the EHRs15–17. While some SDoH factors (e.g.,\nrace, sex) are present as structured data in EHRs, the lack of speciﬁcE H R\nﬁelds to record SDoH information and the lack of standards for collecting\ndata related to SDoH are some of the major reasons for insufﬁcient SDoH\ndocumentation. On the other hand, clinical narratives contain more detailed\ncharacterization of several SDoH factors (e.g., living status, ﬁnancial\ninstability, social support, etc.), beyond the structured representation. The\nunstructured text data often contains detailed information to describe how\nmultiple socio-economic, behavioral factors, and situational variables\ninteract with each other. To bridge this gap, NLP techniques have emerged\nto automatically identify SDoH information from unstructured notes.\nThe current landscape of NLP approaches for identifying SDoH factors\nhas been extensively reviewed\n6,18. While multiple NLP methods are available,\nthere are four primary challenges for SDoH identiﬁc a t i o n .F i r s t l y ,m a n y\nstudies, even the recent ones19–25 have focused on a limited set of SDoH factors\n(up to 8 factors). A comprehensive review6 revealed that among the 82 papers\nexamined, only three SDoH factors were commonly addressed: smoking\nstatus (27), substance use (21), and homelessness (20). Other critical factors\nsuch as education, insurance status, and broader social issues are still in the\ndevelopmental stage. The n2c2/UW SDOH Challenge\n26 on SDoH extraction\nfocused on the status, extent, and temporality of the factors, but was limited to\nﬁve factors including the commonly studied alcohol, drug, tobacco, in\naddition to employment, and living situation using the Social History\nAnnotated Corpus (SHAC) corpus\n27. Secondly, a considerable number (22\nout of 82) of these approaches rely on rule-based methodologies, which are\nhighly dependent on speciﬁc lexicons. The lack of a standardized doc-\numentation framework for SDoH means these lexicons might not be easily\ntransferrable or adaptable across different healthcare systems, signiﬁcantly\nlimiting the methods’effectiveness. Recent work in this direction is the\nGravity Project28, a collaborative effort aimed at developing consensus-based\ndata standards for SDoH information. Some prior studies have explored the\nuse of deep neural network architectures such as CNN\n29,L S T M30,a n d\nBERT21,23,31 to automatically classify SDoH categories32, or named entity\nrecognition (NER) based approaches, utilizing tools like cTAKES33, CNN,\nand BERT-based models34, to extract SDoH factors from clinical text. Thirdly,\nmost methods were developedand evaluated within speciﬁch e a l t h c a r e\nsystems, such as a Veterans Health Administration center35,aM e d i c a l\nCenter36,a n daT r a u m aC e n t e r37. The lack of broader testing raises concerns\nabout the generalizability and adaptability of these NLP methods to diverse\nhealthcare settings. For example, Stemerman et al.\n38 designed a multi-label\nclassiﬁer to identify six SDoH categories within sentences extracted from\nclinical notes sourced from the University of North Carolina’sc l i n i c a ld a t a\nwarehouse. Finally, the decision to formulate the SDoH extraction task as\nclassiﬁcation\n15,32,38 vs. NER27,39,40 requires careful consideration.\nExisting research has also explored integrating patient SDoH data with\ngeospatial datasets like census demographics and social vulnerability index\n(SVI) to estimate SDoH factors at a community or neighborhood level\n41–44.\nOne such study analyzed to what extend the self-reported SDoH needs\nassessment by a health system are associated with census tract–level social\nvulnerability measured using the SVI42. Other work has explored mapping\nSDoH documentation and area-level deprivation to assess individual-level\nsocial risks44. While leveraging linkage between patient addresses and geo-\nreferenced datasets offers a valuable community-level perspective, our work\naims to directly identify patient-speciﬁc SDoH information from the\nunstructured text data using NLP techniques.\nRecent studies have utilized the use of large language models (LLMs)\nfor the identiﬁcation of SDoH factors19,24,45,46. Guevera et al. conducted a\nstudy using encoder-decoder language models, speciﬁcallyﬁne-tuned Flan-\nT5 XL and Flan-T5 XXL, to classify mentions of SDoH categories from\nnarrative text in EHRs45. The study aimed to identify any mention of these\nSDoH factors as well as speciﬁcally adverse mentions. However, the study\nwas limited in scope to identifying the presence or absence of only six SDoH\nfactors, without exploring temporality or nuanced assertions beyond a\nbroad adverse categorization. Thepotential of data augmentation by\nincorporating LLM-generated synthetic SDoH data into the training pro-\ncess to further enhance model performance was also investigated.\nThe inclusion of SDoH in clinical not e sc a nv a r yd e p e n d i n go nv a r i o u s\nfactors, including the healthcare system, individual clinician practices, and\nthe speciﬁc context of patient encounters\n47,48. These factors have a profound\ninﬂuence on the heterogeneity in the distribution of SDoH factors in clinical\nnotes. Focusing on a speciﬁc note type, single institution data, speciﬁc\nhospital setting or individual documenting practices and developing data-\nsets and testing NLP performance under these conditions might not always\ntranslate to a real-world scenario especially if developing models that gen-\neralize well across different settings is of prime importance. Hence, the\nprimary goal of this study is to investigate the feasibility of developing high-\nperforming and generalizable NLP approaches for extracting comprehen-\nsive SDoH information from clinical notes. We train and evaluate several\nNLP models including deep-learning architectures and LLMs assessing their\ncapabilities both within their training distribution (in-domain) as well as on\ndatasets from other sites (out-of-domain). By analyzing patient data from\ndiverse clinical settings across multiple healthcare institutions, we also shed\nlight on the real-world distribution of SDoH factors.\nIn summary, this paper presents thefollowing key contributions: (1) a\nstudy identifying the most comprehensive list of SDoH factors from clinical\nnotes; (2) curation of the largest multi-institution annotated corpora cov-\nering assertions and temporality of SDoH factors; and (3) development and\nevaluation of the performance of both traditional models and LLMs high-\nlighting the impact of varying label distribution of SDoH factors across\ndatasets on cross-domain transfer learning. To enhance accessibility and\nencourage further research, we make our models— trained andﬁne-tuned\non this multi-institutional SDoH dataset— publicly available to the broader\nscientiﬁc community.\nResults\nAnnotation and datasets\nFor the UTHealth Harris County Psychiatric Center (HCPC) dataset, after\neliminating duplicate notes/sentences, we ended up with a total of\n4953 sentences (including 344 sentences with no SDoH). The distribution of\nlevel 1 annotated corpus is shown in Fig.1 (top left). This dataset has a high\nprevalence of several factors such as adverse childhood, education level,\nisolation, geographic location where the person was born/raised, physical/\nsexual abuse and social support compared to other datasets in which these\nfactors were not documented or less frequently documented. A total of 18\nSDoH factors were annotated in this dataset.\nFor the UT Physicians (UTP) dataset, a total of 1691 sentences were\nannotated from 1000 chart notes. Several of these notes had duplicate\ninformation because of copy-pasting for multiple visits by the same patient\nw h i c hw e r ee l i m i n a t e d .F o rt h i sd a t a set comprising of ADRD patients in an\noutpatient setting, the chart notes documented substance use information in\ndetail under “social history” sections. Sex and race were almost always\nmentioned in the beginning (e.g., The patient is an 80 yr old Caucasian F… )\nfollowed by some information regarding the patient’s employment, living\nand marital status. 14 SDoH were annotated showing high prevalence of sex,\nrace, and substance abuse.\nAs notes with note type“social work”were extracted from MIMIC-III\ndatabase\n49, these notes were written by social workers majorly documenting\ntheir interactions with patients’families. This explains the towering fre-\nquency of marital status and some inference that could be made regarding\nliving status and social support. Other factors that were frequently reported\nincluded alcohol and drug use, sex, and employment. A total of 2838 sen-\ntences were annotated from 500 notes with 20 SDoH factors present.\nThe Mayo dataset was the smallest with 964 sentences annotated. The\ndataset contained all clinical notes ofa chronic pain cohort with much of the\ndata structured as templates to beﬁlled in resulting in high volumes of\nduplicate information. In addition, major part was documentation of the\ndiseases and medications explaining the high frequency of“non-SDoH”\ncategory. 18 SDoH factors were present with 10 factors having less than\n10 samples.\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 2\nThe distribution SDoH factors with level 2 annotations for all the\ndatasets are shown as part of Supplementary Figs. 1–4.\nModel performance\nIn this section, we show the results of our experiments with different models\nfor all annotated corpora. Table1 presents the results ofﬁvefold cross-\nvalidation on both level 1 and level 2 annotated corpora. Micro-averaged\nprecision, recall and F1 scores are reported by averaging across all 5 folds.\nWe observe that the instruction-tuned LLaMA model performed better\nthan all other models for all datasetsexcept on the MIMIC-III dataset (level\n1) for which the ClinicalBERT50 model was better. The ClinicalBERT model\nhad the second-best performance for level 1 annotated corpora. For level 1,\nUTP dataset has only 15 classes (Fig.1 top right) and has a comparatively\nbetter class balance. This is reﬂected in the model performance with 5-fold\ncross-validation consistently yieldinga nF 1s c o r ea b o v e0 . 9 5f o ra l lm o d e l s .\nOn the other hand, Mayo corpus (Fig.1 bottom right) is highly imbalanced,\nis the smallest dataset, and has a total of 19 classes. It is interesting to note\nthat the instruction tuned LLaMA model still achieved an F1 score of 0.94\nclosely followed by ClinicalBERT with 0.919.\nIf the number of classes increases and the number of datapoints\nremains the same, the model performance decreases as is evident from the\nresults for level 2 annotated corpora (Table1 bottom half). However, the\ndecrease in performance for the LLaMA model (ranges from 2.7 to 4.6%) is\nless compared to other models (4.6 to 11.3% for XGBoost, 10.2 to 23.9% for\nTextCNN, 4.5 to 12% for SBERT, and3 to 13.3% for ClinicalBERT). For\nU T P ,e x c e p tf o rT e x t C N Na l lo t h e rm o d e l sh a v ea nF 1s c o r ea b o v e0 . 9 1 .T h e\nmaximum difference between ClinicalBERT and LLaMA (13.2%) was\nobserved for the Mayo dataset (level 2) due to the high class-imbalance\n(Supplementary Fig. 4) and most classes not having enough samples for\nlearning.\nThe macro-averaged metrics (Table2)i sab e t t e rr eﬂection of the\nmodel performances with such class-imbalanced datasets. We notice a\nwidened performance gap between LLaMA and other models, with the LLM\nstill performing better than other models with greater than 0.45 F1 across the\nFig. 1 | Distribution of SDoH factors.Number of sentences documenting each SDoH factor for all four corpora annotated at level 1 (identifying SDoH factors only).\nTable 1 | Micro-averaged precision, recall, and F-1 metrics (average of 5-folds) of models across all datasets\nDataset XGBoost TextCNN Sent. BERT Clin. BERT LLaMA\nLevel 1: SDoH factors only (P/R/F1)\nHCPC 0.907/0.803/0.851 0.895/0.781/0.834 0.880/0.858/0.869 0.907/0.904/0.906 0.941/0.913/ 0.927\nUTP 0.982/0.935/0.958 0.980/0.927/0.952 0.979/0.948/0.963 0.984/0.971/0.978 0.990/0.979/ 0.984\nMIMIC-III 0.887/0.780/0.830 0.841/0.732/0.782 0.890/0.821/0.854 0.904/0.881/ 0.892 0.934/0.840/0.883\nMayo 0.852/0.799/0.825 0.823/0.734/0.775 0.887/0.781/0.830 0.940/0.899/0.919 0.953/0.938/ 0.945\nLevel 2: SDoH factors+ values (P/R/F1)\nHCPC 0.821/0.690/0.750 0.824/0.569/0.673 0.826/0.751/0.786 0.854/0.796/0.822 0.903/0.869/ 0.886\nUTP 0.946/0.880/0.912 0.889/0.815/0.850 0.957/0.882/0.918 0.966/0.930/0.948 0.982/0.932/ 0.956\nMIMIC-III 0.802/0.649/0.717 0.737/0.430/0.543 0.805/0.674/0.734 0.847/0.773/0.808 0.877/0.801/ 0.837\nMayo 0.795/0.711/0.750 0.770/0.572/0.656 0.878/0.629/0.732 0.866/0.720/0.786 0.935/0.901/ 0.918\nThe values in bold indicate the best F1 score for each dataset.\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 3\ndatasets and levels of annotation. This also demonstrates the ability of the\nLLaMA models to learn from very few examples for many classes.\nOur extensive prior work comparing zero/few-shot andﬁne-tuned\nLLaMA models have demonstrated the superior performance ofﬁne-\ntuned models51–54. We also evaluated LLaMA 2’s in-context learning\ncapabilities (7B-base and 7B-chat variants) using up to ten prompt\nsamples to demonstrate the utility and need ofﬁne-tuning for this task.\nThe 7B-base model produced inconsistent outputs, while the 7B-chat\nmodel, though slightly improved, generated overly verbose responses\nwith numerous false positives (outputting the entire list of SDoH fac-\ntors in majority of the cases), requiring extensive post-processing. As\nshown in Supplementary Table 12, even with optimized prompts and\nthe larger 13B-chat model, in-context learning yielded poor results.\nTop performance on level 1 annotations was observed on the HCPC\ndataset (micro-average F1 of 0.518) with the 13-B chat model. The\nperformance further dropped on level 2 annotations and macro-\naverage evaluations likely due to the complexity of this task and\nincrease in the number of classes. The prompts used are included in the\nSupplementary Materials (Supplementary Notes 2 and 3).\nGeneralizability experiments\nFigure 2 shows the performance (micro-averaged) of all models when\ntrained on one corpus and evaluated onall other corpora (level 1 annotated\ncorpora). Overall, the LLaMA model performed better than other models in\ncross-dataset generalizability. Whenﬁne-tuned on HCPC, LLaMA achieved\nan F1 score of 0.91 on both UTP and Mayo datasets and 0.82 on MIMIC-III,\ndemonstrating an overall best performance on cross-dataset general-\nizability. Other notable performances include training on MIMIC-III and\ntesting on UTP (F1 = 0.94). ClinicalBERT demonstrated better performance\nwhen trained on UTP and tested on MIMIC-III and Mayo datasets and\nwhen trained on MIMIC-III and tested on Mayo dataset. These results can\nbe attributed to the high overlap of SDoH categories between these datasets,\nwith a minimal impact from non-overlapping categories on micro-averaged\nmetrics due to their limited sample sizes.\nWith the increase in the number of classes for level 2 annotated\ncorpora, LLaMA model performed better compared to all other models\n(Fig. 3) for all cases of generalizability evaluation. The lowest performance\nfor any dataset pair was an F1score of 0.40 for LLaMA 2, 0.08 for Clin-\nicalBERT, 0.06 for SBERT, 0.04 for TextCNN, and 0.09 for XGBoost,\nTable 2 | Macro-averaged precision, recall, and F-1 metrics (average of 5-folds) of models across all datasets\nDataset XGBoost TextCNN Sent. BERT Clin. BERT LLaMA\nLevel 1: SDoH factors only (P/R/F1)\nHCPC 0.669/0.534/0.578 0.506/0.408/0.439 0.602/0.534/0.550 0.730/0.684/0.693 0.811/0.785/ 0.791\nUTP 0.657/0.609/0.626 0.575/0.532/0.549 0.673/0.627/0.636 0.778/0.733/0.746 0.868/0.828/ 0.843\nMIMIC-III 0.539/0.420/0.459 0.394/0.326/0.343 0.474/0.403/0.427 0.542/0.510/0.513 0.651/0.569/ 0.596\nMayo 0.440/0.355/0.384 0.275/0.254/0.257 0.355/0.311/0.324 0.449/0.436/0.437 0.539/0.527/ 0.526\nLevel 2: SDoH factors+ values (P/R/F1)\nHCPC 0.393/0.302/0.328 0.221/0.144/0.162 0.351/0.281/0.296 0.384/0.344/0.348 0.611/0.584/ 0.585\nUTP 0.467/0.422/0.434 0.357/0.309/0.323 0.372/0.337/0.348 0.450/0.436/0.439 0.596/0.549/ 0.564\nMIMIC-III 0.406/0.304/0.328 0.146/0.098/0.109 0.337/0.261/0.280 0.447/0.383/0.391 0.587/0.523/ 0.538\nMayo 0.225/0.185/0.192 0.126/0.091/0.100 0.140/0.117/0.122 0.181/0.176/0.170 0.471/0.458/ 0.454\nThe values in bold indicate the best F1 score for each dataset.\nFig. 2 | Heatmap of cross-dataset performance evaluation (level 1).The diagonal shows micro-averaged F1scores when trained and tested on the same dataset for level 1\nannotations. Other cells show F1 scores when trained on one dataset and tested on another dataset.\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 4\nclearly demonstrating the superior ability of instruction tuned LLaMA\nmodels to generalize better to unseen data. Micro-averaged precision and\nrecall scores along with F1 for both levels of annotation can be found in\nSupplementary Tables 1, 2.\nLLaMA models outperformed all other models in cross-dataset gen-\neralizability, showing a wider performance gap when evaluated on macro-\naverage precision, recall, and F1 (Supplementary Table 3). This performance\nadvantage is likely due to LLaMA’s ability to learn effectively from fewer\nsamples, a key factor given the datasets’high imbalance. Per-factor per-\nformance evaluations (Supplementary Tables 4–7), with training data\ninstance counts highlighted (* for <15 samples,** for >50 samples), further\nillustrate LLaMA’s few-shot abilities and their impact on macro-averaged\nevaluation. These tables include instances where LLaMA achieved a perfect\nF1 score of 1.0, as well as scenarios where all models, including LLaMA,\nperformed subpar or where LLaMA’sF 1s c o r ew a sl o w e rt h a no t h e rm o d e l s .\nA single instance of LLaMA’s zero-shot performance was also observed\n(Supplementary Table 7) forﬁnancial issues. Overall, LLaMA performs\nparticularly well on low-prevalencefactors; however, other models can\nachieve similar or even better performance in some cases when sufﬁcient\ntraining data are available.\nTo further understand generalizability of models when sufﬁcient\ntraining data are available, we performed another set of experiments\ninvolving ﬁve SDoH factors (Alcohol, Drug, Employment Status,\nMarital Status, Living Status) that are present in all four datasets and\nhaving at least 25 examples in the level 1 training data. Supplementary\nTables 8 to 11 demonstrates the performance of all models onﬁve\nfactors trained on one data source and tested across three other data\nsources. Out of 60 experiments (5 factors, 4 data sources, and each\ntested across three other data sources), LLaMA achieved top perfor-\nmance on 41 experiments, ClinicalBERT achieved top performance on\n13, XGBoost achieved top performance on 5, and TextCNN and\nSBERT on 1 each. Those results indicate that models such as Clin-\nicalBERT and XGBoost can still outperform LLaMA when sufﬁcient\ntraining data are available. We discuss in detail in the Discussion\nsection the performance of differentmodels in different scenarios and\ni t si m p l i c a t i o n sf o rc r o s s -dataset generalizability.\nFinal set of evaluations involved training models on a combined dataset\n(combining train split of all four datasets) and evaluating the performance of\nthe resulting model on individual datasets. As shown in Table3,C l i n -\nicalBERT achieved on par or sometimes better performance on level 1\nmicro-averaged metrics on combined data compared to LLaMA. However,\nLLaMA outperformed ClincialBERT on three out of four datasets at the level\n2 evaluation. When macro-averaged metrics were used, ClinicalBERT lag-\nged considerably behind LLaMA, as shown in Table4. The difference\nranged from 2.6% to 12.6% on level 1 and from 2% to 13.4% on level 2\nannotations.\nError analysis\nA thorough examination of the performance of the instruction-tuned\nLLaMA model revealed several key areas for potential improvement.\nAnalyzing the predictions, we observed instances where the model failed to\nmake any predictions, generated predictions outside the expected class\nlabels (e.g., predicting “6th grade” instead of “Education level”), and\nstruggled to differentiate between SDoH pertaining to the patient versus\nthose referring to family members. The primary reasons for the notably low\nperformance on the Mayo dataset, when evaluated on the combined dataset\n(Table3), were the instances of no responses and the generation of irrelevant\noutputs such as numerical values (e.g., dates). Additionally, an interesting\nobservation emerged regarding the level of annotation: for sentences\ndescribing the patient as a non-smoker, the LLaMA model failed to generate\n“smoking”as a label when using the level 1 annotated corpus. However, for\nthe same sentence in the level 2 annotated corpus, the model consistently\ngenerated “nonsmoker” as a response. These issueshighlight the need for\nimproved prompt design with more speciﬁc instructions to regulate such\noutputs. Incorporating detailed instructions from annotation guidelines, as\nobserved in our prior studies on LLMs\n53, could help mitigate these errors and\nenhance the model’s overall performance.\nDiscussion\nO u rw o r kb u i l d su p o na n de x t e n d sag r o w i n gb o d yo fr e s e a r c ha p p l y i n g\nNLP techniques to automatically identify individual-level SDoH informa-\ntion from unstructured clinical notes. While existing studies speciﬁcally\nFig. 3 | Heatmap of cross-dataset performance evaluation (level 2).The diagonal shows micro-averaged F1scores when trained and tested on the same dataset for level 2\nannotations. Other cells show F1 scores when trained on one dataset and tested on another dataset.\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 5\nfocus on highly documented SDoH factors or combine less frequent factors\ninto a single category, we undertook a comprehensive, multilevel annotation\nof all SDoH factors with due focus on temporality and assertion in addition\nto ﬁne-grained subcategorization of each factor. Since we do not constrain\nthe SDoH factors as mentioned above, our model performance is a better\nreﬂection of the real-world operatingconditions. We curated and con-\nstructed four annotated corpora ofS D o Hu s i n gd a t af r o mf o u rd i s t i n c t\nhealthcare systems, ensuring a diverse representation of SDoH factors.\nLeveraging these corpora, we conducted extensive experiments employing\nfour text classiﬁcation models and a generative model to detect SDoH\nfactors. We not only evaluated the performance of these models but also\nhighlighted the impact of the diverse label distribution of SDoH factors\nacross datasets on the effectiveness of cross-domain transfer learning. We\nhope that ourﬁndings provide valuable insights into the complex landscape\nof SDoH factors, paving the way for enhanced understanding and future\nadvancements in healthcare interventions and policies by facilitating and\nencouraging better documentation practices.\nA primary motivation behind our work is to provide the research\ncommunity with pre-trained models on these SDoH datasets, facilitating\ntheir customization and adaptation to local institutional contexts. We\nrecognize that many academic and healthcare organizations face signiﬁcant\nresource constraints that prohibit the utilization of LLMs at scales beyond a\ncertain threshold including our owncomputational limitations. Conse-\nquently, our study prioritized the evaluation of more modestly sized LLMs\nthat still demonstrate promising performance capabilities. By openly dis-\nseminating checkpoints of modelsﬁne-tuned on our aggregated multi-\ninstitutional SDoH dataset, we aim tolower the barrier for other research\ngroups to build upon our work. These models can serve as effective\ninitialization points for transfer learning, requiring furtherﬁne-tuning on\nrelatively small amounts of locally curated data to improve performance.\nOur results demonstrate the abilityof instruction tuned LLaMA model\nto outperform all other models especially when the number of factors\nincreases, and the performance is evaluated per factor considering the high\nclass-imbalance. The base LLaMA model being a general-domain LLM and\nthe task of SDoH identiﬁcation being a non-medical one might have ren-\ndered these models more suitable for this task compared to other biomedical\nNLP tasks which requires considerable biomedical knowledge. The Clin-\nicalBERT model performed on par or sometimes better on micro-averaged\nevaluation and when enough data is available for different factors. The\nperformance of the XGBoost model demonstrates comparability to that of\nthe SBERT model, and it is interesting to observe that the architectural\nvariances did not signiﬁcantly inﬂuence the outcomes when sufﬁcient and\ndiverse data is available, at least in this case within the scope of this study.\nComparing ClinicalBERT and SBERT, ClinicalBERT demonstrated\nbetter performance possibly due to training on MIMIC notes. The perfor-\nmance of TextCNN models was relatively much lower on two datasets -\nMIMIC-III and Mayo. MIMIC-III dataset has high lexical and semantic\nvariability, and the sentences are diverse and do not follow any patterns/\ntemplates and is mostly written in a style of reproducing conversations with\npatients and their families, which might have affected the model perfor-\nmance. As for Mayo, it is the smallest dataset and has high class imbalance\nwith more than half of the samples belonging to‘non SDoH’category. Even\nthough ClinicalBERT, SBERT and XGBoost models performed well when\ntrained and tested on the same datasets, this performance did not translate\nwhen tested on other datasets especially on level 2 corpora and when\nevaluating per factor performance. This low performance could be\nTable 4 | Performance (P/R/F1) of all models (macro-averaged) when trained on the combined dataset for both levels of\nannotation\nDataset XGBoost TextCNN Sent. BERT Clin. BERT LLaMA\nLevel 1: SDoH factors only (P/R/F1)\nHCPC 0.678/0.593/0.619 0.497/0.398/0.433 0.637/0.588/0.603 0.745/0.634/0.670 0.734/0.685/ 0.704\nUTP 0.521/0.515/0.518 0.435/0.422/0.414 0.578/0.538/0.553 0.584/0.588/0.586 0.629/0.604/ 0.612\nMIMIC-III 0.580/0.588/0.562 0.414/0.370/0.370 0.591/0.572/0.570 0.664/0.675/0.650 0.790/0.781/ 0.776\nMayo 0.470/0.439/0.452 0.387/0.298/0.297 0.481/0.462/0.466 0.474/0.481/0.470 0.553/0.520/ 0.534\nLevel 2: SDoH factors+ values (P/R/F1)\nHCPC 0.369/0.297/0.314 0.194/0.124/0.142 0.397/0.367/0.370 0.427/0.395/0.396 0.531/0.532/ 0.516\nUTP 0.322/0.290/0.297 0.203/0.148/0.163 0.371/0.347/0.355 0.375/0.354/0.360 0.391/0.374/ 0.380\nMIMIC-III 0.389/0.326/0.338 0.167/0.095/0.109 0.461/0.419/0.422 0.493/0.426/0.428 0.575/0.572/ 0.562\nMayo 0.195/0.177/0.181 0.083/0.053/0.059 0.274/0.233/0.241 0.285/0.294/0.285 0.303/0.313/ 0.305\nThe values in bold indicate the best F1 score for each dataset.\nTable 3 | Performance (P/R/F1) of all models (micro-averaged) when trained on the combined dataset for both levels of\nannotation\nDataset XGBoost TextCNN Sent. BERT Clin. BERT LLaMA\nLevel 1: SDoH factors only (P/R/F1)\nHCPC 0.893/0.825/0.858 0.822/0.712/0.763 0.891/0.862/0.876 0.909/0.887/0.897 0.940/0.924/ 0.932\nUTP 0.959/0.959/0.959 0.943/0.922/0.932 0.982/0.969/0.976 0.990/0.992/ 0.991 0.989/0.988/0.989\nMIMIC-III 0.881/0.819/0.849 0.807/0.664/0.729 0.886/0.862/0.874 0.922/0.924/ 0.923 0.935/0.906/0.920\nMayo 0.919/0.838/0.877 0.690/0.658/0.674 0.902/0.894/0.898 0.936/0.949/ 0.943 0.941/0.815/0.874\nLevel 2: SDoH factors+ values (P/R/F1)\nHCPC 0.816/0.701/0.754 0.857/0.508/0.638 0.824/0.799/0.811 0.856/0.837/0.847 0.907/0.891/ 0.899\nUTP 0.943/0.911/0.927 0.933/0.720/0.813 0.971/0.964/0.968 0.983/0.978/0.981 0.992/0.981/ 0.986\nMIMIC-III 0.795/0.637/0.708 0.773/0.360/0.491 0.814/0.777/0.795 0.832/0.788/0.809 0.900/0.870/ 0.885\nMayo 0.840/0.750/0.792 0.836/0.375/0.517 0.853/0.806/0.829 0.918/0.914/ 0.916 0.951/0.849/0.897\nThe values in bold indicate the best F1 score for each dataset.\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 6\nattributed to factors such as difference in (1) SDoH frequency distribution,\nand (2) the patterns in which SDoH factors are described differing sig-\nniﬁcantly depending on the hospital setting, preference of individual\npractitioner, templates in place and medical specialties. To address this issue,\ndata needs to be collected from diverse sources and combined to create\ndatasets that capture the variationsthat contribute to heterogeneity.\nWe want to distinguish the two distinct aspects of model general-\nizability: (1) assessing the zero-/few-shot capabilities of models on new,\nunseen tasks (i.e., extracting new SDoH factors not seen during training)\nand (2) assessing the performance on new datasets for the same task for\nwhich the model was trained/ﬁne-tuned (i.e., extracting shared SDoH fac-\ntors across institutions). While LLMs excel in zero- and few-shot settings on\nunseen tasks, making them suitablefor novel SDoH extraction, other\nmodels also show reasonable performance when evaluating generalizability\non shared SDoH factors across datasets with sufﬁcient training data. Our\nexperiments focusing on ﬁve common SDoH factors (Alcohol, Drug,\nEmployment Status, Marital Status, and Living Status), present in all four\ndatasets with at least 25 training examples each (Supplementary Tables\n8–11), demonstrate this point. In these cross-dataset evaluations, LLaMA\nachieved top performance in 41 out of 60 experiments, demonstrating\nstrong generalizability. However, ClinicalBERT and XGBoost also showed\ncompetitive performance, achieving top performance in 13 and 5 experi-\nments, respectively. For example, ClinicalBERT’s achieved a superior\nF1 score of 0.90 compared to LLaMA’s0 . 7 1o n‘living status’when trained\non the Mayo dataset and tested on the UTP dataset. This suggests there are\nother factors that affect model generalizability. With sufﬁcient training data,\nsimpler models may achieve comparable or sometimes better general-\nizability for well-deﬁned tasks. Furthermore, our exploration of LLaMA 2’s\nin-context learning capabilities (Supplementary Table 12) revealed limita-\ntions in its zero-/few-shot performance for this task, with the 13B-chat\nmodel achieving a best micro-average F1 of only 0.518 on the HCPC dataset\n(the highest among all datasets) for level 1 annotations, and performance\nfurther declining in level 2 annotations and macro-average evaluations. This\nillustrates the need forﬁne-tuning to achieve robust performance for SDoH\nextraction using LLMs.\nWhile our cross-dataset evaluati o ns h e d ss o m el i g h to nm o d e l s’ability\nto handle distribution shifts between sites, there are indeed inherent chal-\nlenges to developing universally generalizable SDoH extraction models.\nMany social and environmental risk factors are intricately tied to the unique\ncircumstances, lived experiences, andcultural contexts of different com-\nmunities. The salience and manifestations of speciﬁcS D o Hc a nv a r y\ndrastically between patient populations. For example, documentation of\nhousing instability or food insecurity may present very differently in an\nurban setting compared to a rural or afﬂuent suburban communities. Such\npopulation-level heterogeneity makes it difﬁcult to capture all potential\nlinguistic variations in a single model. As such, while our work demonstrates\nsome generalizability across datasets from different health systems, the\nhighest performances are still achieved when models are tested in-domain.\nIn cross-dataset evaluation, if a classabsent from the training dataset isn’t\npredicted and causes a performance drop, it indicates zero-shot utilization.\nThis dip in performance is anticipated due to label shift. Conversely, when\nclasses are accounted for, generalizability assessment extends to evaluating\nthe model’s capacity to manage covariate shift, which emerges when there\nare distribution differences between the training and testing data environ-\nments. Even though these factors have contributed signiﬁ\ncantly to the drop\nin performance, we still included rare SDoH factors in our evaluation as it\nallows us to assess the pipeline’s performance in real-world scenarios, even if\nit is solely based on zero-shot capabilities.\nDeveloping and deploying LLMs can be challenging due to the sig-\nniﬁcant computational resource demand for training,ﬁne-tuning, and\ninference. High energy consumption and a large carbon footprint are\nenvironmental considerations. Withparameter counts in the billions, LLMs\nmay pose storage capacity challenges for some environments. Speciﬁcally, to\nﬁne-tune the 7B LLaMA model on about7300 instruction demonstrations\n(combined training on all corpora) required approximately 10 min utilizing\nfour NVIDIA A100 80GB GPUs. We performed full modelﬁne-tuning,\nthough techniques like parameter-efﬁcient ﬁne-tuning and quantization\ncould further reduce the GPU and memory requirements considerably.\nInference with theﬁne-tuned LLaMA model was performed on a single\nA100 GPU. The TextCNN, SBERT, and ClinicalBERT models had more\nmodest training overheads, requiring only a single GPU. The XGBoost\nmodel being conventional tree-based methods, could be trained efﬁciently\non CPUs and exhibited the fastest runtimes overall.\nWe note that differing criteria were used to construct the datasets from\neach site. While ideally datasets would undergo identical preprocessing,\nsome degree of variation was required across sites to construct datasets with\nsufﬁcient densities of SDoH documentation. However, we cannot rule out\nthe possibility thatthese dataset-speciﬁc preprocessing may have inad-\nvertently introduced biases that hindered cross-dataset generalization and\nperformance difference between models. HCPC, UTP, and MIMIC datasets\nemployed random sampling and keywordﬁltering, but the Mayo dataset\noriginated from a study prescreening for chronic pain patients. This\ncondition-speciﬁc cohort likely does not reﬂect the diverse SDoH proﬁles of\nMayo’s general clinical population, which may have impacted model per-\nformance on this dataset along with its small sample size in addition to also\nhaving an impact on the SDoH distribution.\nCareful consideration must be given to mitigating algorithmic bias\nwhen developing and deploying language models for extracting SDoH\ninformation from unstructured clinical notes across different healthcare\nsettings. Models trained on data reﬂecting societal biases and disparities may\nperpetuate or amplify these inequities when deployed in clinical practice.\nHence, evaluating bias in LLMs using clinical data of patients is crucial to\nensure that the models do not perpetuate or amplify existing biases present\nin healthcare datasets\n55–57. Differences in race, ethnicity, sex, age distribu-\ntions and other traits across sites could contribute to language biases, dis-\nproportionate SDoH mention rates, or framing disparities stemming from\nclinician implicit biases. While some information about the data regarding\nthe patient population has been provided in Table5,w en o t et h a tf u r t h e r\ncharacteristics such as age, sex, race, etc. could also explain differences in\nSDoH extracted from each dataset which might also result in bias in the\nmodel performance\n56. A study design characterizing these cohort compo-\nsitions, documentation patterns, and analyzing them uniformly across\ndifferent datasets is needed to understand which factors contribute to SDoH\ndistribution variations and model performance. Since our study was not\nTable 5 | Description of the corpora used in this study\nCorpus name Care organization Disease domain Clinical note type Type of setting No: of notes No: of patients Time frame\nHCPC UTHealth Harris County\nPsychiatric Center\nMental health Psychosocial\nassessment notes\nInpatient 2000 1529 2001 –2021\nUTP UTHealth Physicians Alzheimer ’s disease and\nrelated dementias\nChart notes Outpatient 1000 698 2007 –2018\nMIMIC-III Beth Israel Deaconess\nMedical Center\nIntensive care Social work notes Inpatient 500 264 2001 –2012\nMayo Mayo Clinic and the\nOlmsted Medical Center\nChronic pain All clinical notes Inpatient and\noutpatient\n527 62 2005 –2015\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 7\noriginally designed to comprehensively investigate or report on bias eva-\nluations, a redesigned study speciﬁcally focusing on bias evaluations and\nmitigation approaches would be required to address this limitation thor-\noughly. Additionally, utilizing a priorpublished dataset and limited struc-\ntured data availability for some other datasets prevented us from providing\nfurther cohort characterization required for comprehensive bias\nevaluations.\nOur study was conducted using data from US-based medical centers.\nThe speciﬁc SDoH factors and linguistic patterns for expressing them may\ndiffer across other geographic and cultural contexts. To maximize the global\nimpact and generalizability of using language models for SDoH extraction, it\nwill be critical to create additional annotated datasets representing diverse\npopulations worldwide\n58. Evaluation of model performance and potential\nbiases must also be conducted across different languages, healthcare settings,\nand sociocultural environments. To address this limitation, this work needs\nto be expanded beyond the US context and is an important future direction\nto advance global health equity through SDoH documentation.\nIt is imperative to continually benchmark the latest advances in LLMs\nfor challenging applications like extracting SDoH from unstructured clinical\ntext. More powerful models like LLaMA 3.159 might perform better compared\nto LLaMA 2 used in this study. However, ourﬁne-tuning strategies and\nevaluations remain relevant independent of the speciﬁc model used. We will\nin future investigate commercial LLMs such as GPT-460 for SDoH extraction,\nafter appropriate arrangements that allow sending clinical data to GPT\nmodels (e.g., via GPT models on Azure) are approved by our institution.\nIf the ultimate goal is to develop a single model that can identify a\nvariety of SDoH factors for a wide range of clinical notes originating from\nmultiple institutions, federated learning\n61 could be a promising direction. In\nfederated learning, each institution will train their individual model with\ntheir privately-owned dataset andupdate their model through several\nrounds of model merging. After each round, the merged model will be\nbroadcast to each institution before the next round of training begins. Other\napproaches such as a combination ofDomain-incremental learning and\nClass-incremental learning\n62 can also be applied to sequentially update a\nsingle model with new domains and new sets of classes. We will explore\nthese in our future work.\nGuevera et al.\n45 explored the use of synthetic data for SDoH utilizing a\nsimple prompt providing the model with several examples of each category.\nWhile showing promise, the generation of synthetic data did not exhaus-\ntively capture all possible linguistic variations of expressing SDoH concepts\nthat could be encountered in real clinical text. We will systematically\ninvestigate different synthetic data generation strategies including zero-/\nfew-shot, chain-of-thought andﬁne tuning\n63–67. In addition to experi-\nmenting with advanced prompting strategies, in future, we plan to propose a\ntemplate-based conditional generation approach to increase the coverage of\nlong-tailed factors there by mitigating the factor imbalance and increasing\nthe diversity of generated samples.\nIn summary, extracting and classifying social determinants of health\nfrom EHR and clinical notes have the potential for developing effective\ntreatment plans, improving population health, and reducing health dis-\nparities. While what SDoH factors are documented and where they are\ndocumented varies widely across healthcare settings and medical specialties,\nthe effects of these variations on the model performance in a real-world\nscenario is rarely studied, especially when developing generalizable models\nis the primary goal. Hence in this study, we analyze the performance of\nmultiple models on SDoH extractionby designing and developing anno-\ntation guidelines for classifying SDoH factors and creating several annotated\ncorpora using data from four healthcare systems. The datasets were curated\nto include different patient cohorts, note types, different layers of care in\nhospital settings, and documentationpractices, thus showcasing the het-\nerogeneity in the distribution of SDoH factors. Additionally, four classiﬁ-\ncation models and a large language model were experimented with to detect\nSDoH factors with the LLM performing the best. The generalizability of the\nmodels across institutions was also evaluated. While all models perform\nrelatively well when trained and tested on a single dataset, performance\nvaried and dropped on cross-dataset evaluation, indicating the need for\nfurther research in this domain. To encourage research in this direction we\nwill make available models trained by combining all annotated datasets.\nMethods\nPrior research has often constrained the number of SDoH factors within\ndatasets due to the manual annotation burden or later integrated numerous\nfactors with only a few samples into an“Other SDoH” category to poten-\ntially improve model performance\n32. However, this oversimpliﬁed approach\nfails to mirror the intricacies of real-world scenarios. Hence in this study, we\nundertook a meticulous annotation process, annotating all SDoH factors\npresent in clinical notes, refraining from consolidation even in cases of\nlimited sample sizes, to better reﬂect the landscape of SDoH factor\ndistributions.\nWe collected data from four differenthospitals and different hospital\nsettings (inpatient and outpatient) including the publicly available MIMIC-\nIII\n49 database to acquire a rich and diverse documentation of SDoH factors.\nThe datasets include psychosocial assessment notes, chart notes, social work\nnotes, and all clinical notes from a published cohort study\n34. This variety in\nnote types also accounts for the variety in documentation practices as they\nare written by physicians, nurses, social workers, etc. While“social history”\nsections are rich in SDoH information (and many studies have solely used\nthis section), SDoH factors can be scattered under different sections\ndepending on the type of notes, note templates, and individual note-taking\nstyles of the providers. In our study, we conducted a preliminary analysis of\nnotes from each dataset to identify SDoH information across sections. In the\ncase of the HCPC dataset, SDoH details were predominantly concentrated\nin psychosocial assessments within social history sections, leading to their\nsole consideration. Conversely, notes from UTP contained diverse SDoH\ninformation spread across different sections like“History of present illness”\nand “General observation,” prompting a comprehensive review of note\ncontents to ensure inclusion of relevant data.\nWe decided to use multilabel classiﬁcation instead of NER, as this\napproach offers ﬂexibility in capturing the presence of various social\ndeterminants without requiring precise identiﬁcation of entity boundaries.\nAnnotators can focus on understanding the overall meaning and context of\nthe sentence to determine which social determinants are relevant. This\napproach can potentially speed up the annotation process compared to the\ndetailed entity-level annotation required in NER. Furthermore, there are\ncertain factors such as social support, isolation or adverse childhood\nexperiences that may not lend themselves well to strict entity identiﬁcation\nas these factors often need to be inferred from the context. By adopting a\nmultilabel sentence classiﬁcation approach, annotators have theﬂexibility to\ncapture these nuanced factors that cannot be precisely pinpointed as indi-\nvidual entities.\nThere is a better probability ofﬁnding some of the less studied SDoH\nfactors such as social support, adverse childhood, and physical abuse-related\ninformation in clinical notes of patients experiencing mental health issues as\nthese factors are highly relevant for mental health cohorts. These factors\nmight be less prevalent in clinical notes written by physicians from other\nspecialties\n47,48 and hence SDoH documentation depends also on the patient\npopulation. Given such a real-world scenario, it is necessary to incorporate\nclinical notes from multiple medical specialties to develop models that can\nextract a wide range of SDoH factors. Hence, we utilize our multi-\ninstitutional corpora to explore the variations in the distribution of SDoH\nfactors and conduct experiments towards understanding the feasibility of\ndeveloping generalizable models that can extract multiple SDoH factors\nfrom clinical notes. Apart from traditional machine learning models and\ndeep learning models we also inspect the use of an open-source LLM -\nLLaMA\n68 for SDoH extraction and conduct a thorough evaluation of the\nmodel performance under different settings.\nSelection of SDoH factors\nWe performed an extensive review of literature to identify prior work on\nsocial determinants including systematic and scoping reviews6,69 with focus\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 8\non SDoH. Several ontologies/terminologies that cover SDoH factors70,71\n(e.g., Ontology of Medically Related Social Entities (OMRSE)72, Semantic\nMining of Activity, Social, and Health data (SMASH) system ontology73)\nwere also analyzed to identify major concepts besides standards such as\nSNOMED CT74 and LOINC75. Additional information was obtained from\nmultiple surveys on SDoH including the All of Us SDoH Survey76,M i l l i o n\nVeteran Program (MVP) Lifestyle Survey77, 2020 AHIMA Social Deter-\nminants of Health (SDOH) Survey78, and the Gravity project28.F o l l o w i n g\nthis process, the domain experts wereconsulted to identify important SDoH\nfactors for each domain,ﬁnally narrowing down to 21SDoH factors, which\nwere then utilized for annotating clinical notes.\nAnnotation\nAfter obtaining the Institutional Review Board (IRB) approval for the study\nthe datasets were annotated. The Committee for Protection of Human\nSubjects at UTHealth (UTHealth CPHS) approved the study (HSC-SBMI-\n12-0754) for utilizing the clinical notes from UTHealth Harris County\nPsychiatric Center and UT Physicians. For utilization of clinical notes from\nMayo clinic, the study was approved by the Mayo Clinic and the Olmsted\nMedical Center (OMC) IRBs (Mayo Clinic: 18-006536 and Olmsted\nMedical Center: 038-OMC-18). The requirement for informed consent was\nwaived by all the governing IRBs undere x e m p tc a t e g o r y4 ,a st h er e s e a r c h\ninvolved only secondary use of clinical notes and did not involve any patient\ncontact.\nThe annotation process consists of two levels. At theﬁrst level (SDoH\nfactors only) a sentence is assigned one or more labels from the 21 SDoH\nfactors. For example, the sentence“She is single, lives with her parents, works\n3 days per week.” will be labeled with‘marital status’, ‘living status’and\n‘employment status’. The second level of annotation (SDoH factors along\nwith their corresponding values/attributes) provides more granular infor-\nmation about these factors with respect to their subtypes, presence or\nabsence, and temporality. The sentence mentioned above at level 2 will be\nlabeled with ‘marital status - single’, ‘living status – with family’ and\n‘employment status– employed, current’. The level 2 SDoH label set contains\nﬁne-grained subcategories that are not necessarily mutually exclusive within\na given level 1 category. For example, a single sentence could potentially be\nassigned multiple level 2 labels corresponding to‘living status– with family -\npast’and ‘living status– alone - current’under the broader‘living status’level\n1 category. The subcategory labels can also encode assertions of presence/\nabsence as well as temporality markers (e.g.‘education level – college -\n‘current’). During annotation, all applicable level 2 labels were assigned to a\ngiven sentence, including multiplesubcategories from the same level 1\ncategory. If a sentence does not correspond to any SDoH factor, it is labeled\n‘non SDoH’. Detailed information about the 21 SDoH factors and their\nattributes along with examples can be found in the annotation guidelines\n(see Supplementary Table 13, 14 and Supplementary Note 1).\nThe team that developed the annotation guidelines, performed the\nannotation of the dataset and facilitated conﬂict resolution consisted of an\nMD specialized in psychiatry, an internal medicine physician, a postdoctoral\nfellow trained in clinical NLP, a PhD candidate specializing in the devel-\nopment of biomedical ontologies, a master’s student in public health with a\nprior master’s degree in biomedical informatics, and a research associate\nwith a master’s degree in biomedical informatics and a bachelor’sd e g r e ei n\nbiomedical engineering. The postdoctoral fellow and the PhD candidate\nformulated the annotationguidelines under the expert guidance of the two\nphysicians. The master’s student and the research associate performed the\nannotation of the dataset, and the entire team held regular discussions to\nresolve the conﬂicts. The annotators went through multiple rounds of\ntraining starting with a detailed discussion of the annotation guidelines and\ncalculating the inter-annotator agreement (Cohen’s Kappa) after each\ntraining round. Depending on the datasets (discussed below), each round of\nannotator training utilized 10 clinical notes or 50 sentences. Once a Kappa\nvalue greater than 0.7 was achieved the remaining notes/sentences were\nannotated individually by the annotators and any discrepancies were\nresolved as mentioned above. The inter-annotator agreement after theﬁnal\nround of training was in the range of 0.73–0.90 for the datasets (which is\nsimilar to the annotator agreement reported in other studies\n32).\nDatasets\nThe prevalence of duplicate information in EHRs due to copy-pasting,\ntemplating, and summarizing is a major barrier toﬁnding relevant infor-\nmation from EHRs\n79,80. A recent study reported that the duplication\nincreased from 33% in 2015 to 54.2% for notes written in 202079.T h eo v e r\nrepresentation of certain SDoH factors because of the duplication impairs\nthe training and evaluation of machine learning and deep learning\nmodels\n81,82. Hence, we removed duplicate sentences within multiple notes\ncorresponding to a single patient from all datasets. This enabled us to reduce\nclass imbalance among the SDoH factors to some extent. Nevertheless, the\ndistribution of factors still varies, and class imbalance exists. We also\nobserved several clinical notes documenting variations of the text“Nothing\nn e wt or e p o r t”which were also eliminated.\nWe developed four datasets from de-identiﬁed and de-duplicated\nclinical notes of patients diagnosed with different health conditions from\ndifferent hospitals. This includes “psychosocial assessment” notes of\npatients experiencing mental health issues,“chart notes” of patients diag-\nnosed with Alzheimer’sd i s e a s e ,“social work” notes in the MIMIC-III\ndatabase\n49, and clinical notes of patients with chronic pain. The clinical notes\nwere sampled using different techniques based on the note type and\ndepending on where a majority of the SDoH information was present. The\ndetails of these datasets are described below and summarized in Table5.\nOur ﬁrst corpus comprises of psychosocial assessment notes from\nUTHealth Harris County Psychiatric Center (HCPC). Psychosocial\nassessments inform a comprehensive understanding of the psychological,\nsocial, and cultural context of a person guiding the development of indivi-\ndual care plans. Harris County Psychiatric Center (HCPC) is one of the\nlargest providers ofinpatient psychiatric care in the USA. About 10,000\npatients are admitted yearly, including adults, adolescents, and children.\nCommonly treated conditions are psychotic or mood disorders, patients\nwith acute crisis, and signs of endangering themselves or others. The EHR\ngoes back to 2001 and includes about 120,000 unique patients. We ran-\ndomly selected 2000 assessment notes (corresponding to 1529 patients) and\nextracted the“Social history”sections (using MedSpacy sectionizer\n83)f r o m\nthese notes which are rich in SDoH information. These notes were then\nannotated by two annotators based on the annotation guidelines.\nThe chart notes of patients diagnosed with Alzheimer’sd i s e a s ef r o m\nUT Physicians (UTP) constituted the second corpus. Several studies have\nshown the association of SDoH factors such as education level, isolation, and\nloneliness with the onset of Alzheimer’s disease and related dementias\n(ADRD) in older adults\n84–86. For the second dataset we utilized chart notes of\npatients diagnosed with ADRD from UT Physicians (UTP). UTP provides\noutpatient care with multiple satellite clinics throughout the greater\nHouston Area. We reviewed various note/document types within the EHR\n(e.g. progress notes, discharge summaries, procedure notes) and identiﬁed\nthat“chart notes”(different systems might be using different terminology as\nthere is no speciﬁc standard) were enriched with SDoH documentation. We\nthereforeﬁltered the UTP data to include only this speciﬁcn o t et y p e .U n l i k e\nthe psychosocial assessment notes from HCPC where the majority of the\nSDoH factors were described in the“Social history”section, social history in\nthe chart notes from UTP recorded mostly information related to substance\nuse. Other SDoH information was scattered under sections titled“History of\npresent illness”, “General observation”, etc. Additionally, chart notes are\ncomparatively longer and contain information regarding medications and\ndifferent body systems which are irrelevant to this study. Hence to increase\nthe annotation efﬁciency and decrease the annotation time we developed a\nlist of keywords toﬁlter notes rich in SDoH information. The list of key-\nwords was collected during the initialliterature review and expanded by\ncombining keywords from our prior work developing SDoH ontology and\nthose obtained while annotating the clinical notes from HCPC\n32,70,71,87.\nThe social work notes from MIMIC-III database were utilized for\nconstructing the third corpus. The Medical Information Mart for Intensive\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 9\nCare (MIMIC-III) database contains more than two million free text notes\nunder different categories (e.g., nursing notes, discharge summaries) with\ninformation of patients who stayed in critical care units of the Beth Israel\nDeaconess Medical Center between 2001 and 2012. It includes 2670“social\nwork” notes documenting the patient’s social and family history and the\ninteractions of the social workers with the patient and family members\nduring their stay at the hospital. We randomly selected and annotated\n500 such notes.\nFor our fourth dataset we utilized a subset of the corpus from a study\n34\nthat characterized chronic pain episodes in adult patients receiving treat-\nment at Mayo Clinic and the Olmsted Medical Center. An annotated corpus\nof 62 adults experiencing noncancer chronic pain was created using chronic\npain ICD code for the anchor diagnosis and retrieving all the patient’s\nclinical notes 6 months before and 2 years after this anchor diagnosis (for\nmore information please refer [27]). For our study, a total of 527 notes\ncorresponding to the same 62 patients(retrieved using our keyword search)\nwere annotated following our annotation guidelines. We annotated the\nentire note withoutﬁltering any sections. These notes exhibited a semi-\nstructured format characterized by template structures, particularly\nnoticeable in sections concerning substance use, along with huge amount of\nduplicated information.\nA schematic representation of the workﬂow is illustrated in Fig.4.F o r\nbrevity we will hereafter refer to the four datasets by the abbreviations of the\nnames of the hospitals/database from which the notes were extracted as\nHCPC, UTP, MIMIC-III, and Mayo. During the annotation process, it was\nobserved that some of the factors among the 21 annotated had a notably low\nprevalence and that those factors varied based on the corpus. To address the\nissue of class imbalance and enhance model performance, it is common\npractice to merge classes with fewer samples into a single category. Han et al.\n32\nemployed this approach by consolidating their initial set of 14 annotation\ncategories for SDoH classiﬁcation into eight categories, with the less frequent\nones merged into an“other-social”c a t e g o r y .H o w e v e r ,i no u rs t u d y ,w ec h o s e\nnot to follow this practice and instead trained our models on all classes,\nincluding those with a smaller number of samples. By doing so, we aimed to\nprovide a more realistic evaluation of model performance in real-world sce-\nnarios, considering the wide variation in the distribution of SDoH factors.\nFurthermore, maintaining and training models on all classes is crucial for our\ncross-dataset evaluation to ensure consistent and reliable assessment, as any\ndifferences in the total number and types of classes would otherwise impact\nthe evaluation process.\nModels\nWe experimented withﬁve different models: XGBoost, TextCNN, SBERT,\nClinicalBERT and LLaMA. Since each sentence in the annotated datasets\ncan potentially be annotated with multiple SDoH factors, we formulated the\ntask of identifying SDoH factors in a sentence as a multi-label binary rele-\nvance problem for all models except LLaMA. Formally, given an input\nsentences and a label setC of SDoH factors, a model produces a binary label\nof f0; 1g for each SDoH factor c 2 C: We performed a supervised\ninstructionﬁne-tuning for the LLaMA model. An instruction dataset was\ncreated with appropriate instructions to perform the task along with the\nsentences and expected response as input and output toﬁne-tune the model.\nBelow we provide a brief overview of theﬁv em o d e l s :X G B o o s t ,T e x t C N N ,\nSBERT, ClinicalBERT and LLaMA.\nWe used the scikit-learn implementation of XGBoost\n88 with tf-idf\nvectors as input features. OneVsRestClassiﬁer was used with parameters\nn_jobs =–1 facilitating the use of all processors and max_depth value of 4.\nWe implemented a TextCNN model following89 for multi-label text clas-\nsiﬁcation. We used pre-trained word embeddings (GloVe90) of dimension\n300 for each word in a sentence as inputs. The model applies convolutional\nﬁlters of kernel size 3, 4, and 5 to the input. The outputs of the convolutional\nﬁlters are max-pooled and fed to a set of classiﬁcation heads - each of them is\na feed-forward network corresponding to a label.\nWe used a pre-trained Sentence-BERT (SBERT)91 encoder to encode\neach sentence into a dense vector representation aka. sentence embeddings.\nSBERT ﬁne-tunes BERT\n31 in a siamese/triplet network architecture that\nproduces sentence embeddings that are semantically meaningful. We fur-\ntherﬁne-tuned SBERT for multi-label classiﬁcation with the SDoH datasets.\nThe output sentence embeddings of SBERT are passed toC binary classiﬁer\nheads, each of which is a feed-forward neural network. Note that the SBERT\nsentence encoder is shared among all classiﬁer heads. Weﬁne-tuned the\nclassiﬁer heads and the shared SBERT encoder by minimizing the binary\ncross-entropy loss of all classiﬁer heads. During inference, if a classiﬁer head\nproduces a score of 0.5 or more, the input sentence is assigned a positive label\nfor that SDoH factor.\nThe fourth model, ClinicalBERT\n50,i sy e ta n o t h e rB E R Tm o d e lt h a th a s\nbeen further pre-trained on text from all note types in the MIMIC-III v1.4\nd a t a b a s e .W eu t i l i z et h es a m ep r o c e s sa sd e t a i l e da b o v ef o rS B E R Td u r i n g\ntraining and inference just replacing the model with the ClinicalBERT model.\nWe utilized the open-sourcepretrained LLM, LLaMA 2 7B\n68,92,a n d\nadapted it for multilabel classiﬁcation by instruction ﬁne-tuning. The\ntraining data for each dataset was converted into instruction demonstrations.\nAn example of an instruction demonstration is shown in Supplementary Fig.\n5 and has three components– an instruction describing the task to perform,\nan input which in this case is the sentence from which SDoH factors needs to\nbe extracted, and an output which is the gold annotation label(s) for that\nsentence. We utilized the approach mentioned in\n93 using Hugging Face’s\ntraining framework with fully sharded data parallel and mixed precision\ntraining\n51. During inference, we provided theﬁne-tuned models with\n“instruction” and “input” to generate the“output”. We also performed\npreliminary experiments with LLaMA 2 7B and 13B (base and chat variants)\nmodels using different prompts to assess the need forﬁne-tuning for this task\ncompared to just utilizing base or chat variants of LLaMA models.\nEvaluation\nIn total we have eight annotated corpora— two each (level 1: SDoH factors\nonly; level 2: SDoH factors and values/attributes) for each corpus. We\nevaluated the models on all corpora byﬁrst training and testing on the same\n21 SDoH \nFactors\nFour Sites\nHCPC UTP MIMIC-III Mayo Clinic\nRaw \ncorpora\nLevel 1: SDoH \nfactors only\nLevel 2: SDoH \nfactors + values\nLevel 1 Performance \nEvaluation\nLevel 2 Performance \nEvaluation\nCross-dataset \nPerformance \nEvaluation\nSex / Gender Identity / Race \n/ Financial Issues / \nInsurance / Marital Status /  \nAdverse Childhood / Social \nSupport / Living Status / \nGeographic Location / \nEducation Level / \nEmployment Status / Alcohol \n/ Drugs / Smoking …\nLLaMA\nClin.BERT\nTextCNN\nXGBoost\nSDoH Factor \nSelection\nCorpora Annotation Model Training Evaluation1 2 3 4\nSBERT\nFig. 4 | A schematic representation of the workﬂow. Figure shows some of the SDoH factors, data sources, models used and evaluation process.\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 10\ncorpus and then performing cross-dataset evaluation. To assess the model’s\nperformance in handling the task of assigning multiple SDoH categories to a\nsentence, we computed micro-averaged precision (P), recall (R) and F1.\nMicro-average is calculated by considering the total number of true posi-\ntives, false positives, and false negatives across all classes.\nWe performedﬁvefold cross validation by dividing each corpus into 5\nfolds and performance of each fold was recorded and average performance\nacross all 5 folds was calculated. Models were trained and tested only on the\nSDoH categories present in the corpus and hence the number of classes\ndiffers across the datasets.\nIt is observed that the performance is usually good for most models\nwhen trained and tested on the same corpus. We wanted to evaluate how\ndifferent models would perform when trained on one corpus and evaluated\non another (cross-dataset evaluation). Each corpus was randomly divided\ninto 7:1:2 ratio for train, validation, and test, respectively. Models were\ntrained and tested on 22 classes (21 SDoH and 1 non SDoH) for level 1\nannotated corpora and 71 classes (70 SDoH+values and 1 non SDoH) for\nlevel 2 annotated corpora.\nWith factors having variations in their distribution across datasets, how\nwould the performance be affected if we combine the datasets (combined\ndataset evaluation) and train a single model? For evaluating this we retained\nthe exact same split used for cross-dataset evaluation and combined the\ntraining splits of all four corpora at level 1 to create a single training split and\ncombined the validation splits into a single validation split. The test splits\nwere preserved separately. The same process was repeated for level 2\nannotated corpora. Next, a model was trained on the combined training\ndata, validated, and tested on the test splits separately for each corpus. The\nnumber of classes, similar to cross-dataset evaluation was 22 for level 1 and\n71 for level 2.\nData availability\nThe MIMIC-III social work notes can be obtained fromhttps://physionet.\norg/content/mimiciii/1.4/and the annotated data will be available on the\nPhysioNet website. The other datasets generated and/or analyzed during the\ncurrent study are not publicly available due to privacy issues.\nCode availability\nT h ec o d ea n dm o d e l sw i l lb em a d ea vailable on our GitHub repository\nhttps://github.com/BIDS-Xu-Lab/LLMs4SDoH.\nReceived: 14 June 2024; Accepted: 16 April 2025;\nReferences\n1. Galea, S., Tracy, M., Hoggatt, K. J., DiMaggio, C. & Karpati, A.\nEstimated deaths attributable to social factors in the United States.\nAm. J. Public Health101, 1456–1465 (2011).\n2. Marmot, M. et al. Closing the gap in a generation: health equity\nthrough action on the social determinants of health.Lancet 372,\n1661–1669 (2008).\n3. Singh, G. K., Siahpush, M. & Kogan, M. D. Neighborhood\nsocioeconomic conditions, built environments, and childhood\nobesity. Health Aff. (Millwood)29, 503–512 (2010).\n4. Felitti, V. J. et al. Relationship of childhood abuse and household\ndysfunction to many of the leading causes of death in adults: The\nAdverse Childhood Experiences (ACE) Study.Am. J. Prev. Med. 14,\n245–258 (1998).\n5. Healthy People 2030, https://health.gov/healthypeople/priority-\nareas/social-determinants-health (2023).\n6. Patra, B. G. et al. Extracting social determinants of health from\nelectronic health records using natural language processing: A\nsystematic review.J. Am. Med Inf. Assoc.28, 2716–2727 (2021).\n7. Gold, R. & Gottlieb, L. National data on social risk screening\nunderscore the need for implementation research.JAMA Netw. Open\n2, e1911513–e1911513 (2019).\n8. Gottlieb, L. et al. Advancing social prescribing with implementation\nscience. J. Am. Board Fam. Med.31, 315–321 (2018).\n9. Gold, R. et al. Adoption of social determinants of health EHR tools by\ncommunity health centers.Ann. Fam. Med.16, 399–407 (2018).\n10. Integrating Social Needs Care into the Delivery of Health Care to\nImprove the Nation’s Health, https://www.nationalacademies.org/\nour-work/integrating-social-needs-care-into-the-delivery-of-health-\ncare-to-improve-the-nations-health (2019).\n11. Gottesman, O. et al. The electronic medical records and genomics\n(eMERGE) network: Past, present, and future.Genet Med. 15,\n761–771 (2013).\n12. Patient-Centered Outcomes Research Institute (PCORI), https://\nwww.pcori.org/ (2022).\n13. The Observational Health Data Science and Informatics (OHDSI)\nconsortium, https://www.ohdsi.org/ (2022).\n14. Keloth, V. K. et al. Representing and utilizing clinical textual data for\nreal world studies: An OHDSI approach.J. Biomed. Inf.142, 104343\n(2023).\n15. Winden, T. J., Chen, E. S. & Melton, G. B. Representing residence,\nliving situation, and living conditions: An evaluation of terminologies,\nstandards, guidelines, and measures/surveys.AMIA Annu Symp.\nProc. 2016, 2072 (2016).\n16. Kepper, M. M. et al. The adoption of social determinants of health\ndocumentation in clinical settings.Health Serv. Res.58,6 7–77 (2023).\n17. Reeves, R. M. et al. Adaptation of an NLP system to a new healthcare\nenvironment to identify social determinants of health.J. Biomed. Inf.\n120, 103851 (2021).\n18. Bompelli, A. et al. Social and behavioral determinants of health in the\nera of artiﬁcial intelligence with electronic health records: A scoping\nreview. Health Data Sci. 2021, 9759016 (2021).\n19. Ong, J. C. L. et al. Artiﬁcial intelligence, ChatGPT, and other large\nlanguage models for social determinants of health: Current state and\nfuture directions.Cell Rep. Med.5, (2024).\n20. Lybarger, K. et al. Leveraging natural language processing to\naugment structured social determinants of health data in the\nelectronic health record.J. Am. Med Inf. Assoc.30, 1389–1397 (2023).\n21. Richie, R., Ruiz, V. M., Han, S., Shi, L. & Tsui, F. Extracting social\ndeterminants of health events with transformer-based multitask,\nmultilabel named entity recognition.J. Am. Med. Inf. Assoc.30,\n1379–1388 (2023).\n22. Romanowski, B., Ben Abacha, A. & Fan, Y. Extracting social determinants\nof health from clinical note text with classiﬁcation and sequence-to-\nsequence approaches.J .A m .M e d .I n f .A s s o c .30, 1448–1455 (2023).\n23. Yu, Z. et al. Identifying social determinants of health from clinical\nnarratives: A study of performance, documentation ratio, and\npotential bias.J. Biomed. Inf.153, 104642 (2024).\n24. Fu, Y. et al. Extracting social determinants of health from pediatric\npatient notes using large language models: novel corpus and\nmethods. arXiv preprint arXiv:2404.00826(2024).\n25. Allen, K. S. et al. Natural language processing-driven state machines\nto extract social factors from unstructured clinical documentation.\nJAMIA Open6, ooad024 (2023).\n26. Lybarger, K., Yetisgen, M. & Uzuner, Ö. The 2022 n2c2/UW shared\ntask on extracting social determinants of health.J. Am. Med. Inf.\nAssoc 30, 1367–1368 (2023).\n27. Lybarger, K., Ostendorf, M. & Yetisgen, M. Annotating social determinants\nof health using active learning, and characterizing determinants using\nneural event extraction.J. Biomed. Inf.113, 103631 (2021).\n28. Gravity project, https://thegravityproject.net/ (2023).\n29. O ’Shea, K. & Nash, R. An introduction to convolutional neural\nnetworks. arXiv preprint arXiv:1511.08458(2015).\n30. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural\nComput\n9, 1735–1780 (1997).\n31. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of\ndeep bidirectional transformers for language understanding.\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 11\nProceedings of the 2019 conference of the North American chapter of\nthe association for computational linguistics: human language\ntechnologies. Vol. 1, 4171–4186 (2019).\n32. Han, S. et al. Classifying social determinants of health from\nunstructured electronic health records using deep learning-based\nnatural language processing.J. Biomed. Inf.127, 103984 (2022).\n33. Savova, G. K. et al. Mayo clinical Text Analysis and Knowledge\nExtraction System (cTAKES): architecture, component evaluation and\napplications. J. Am. Med Inf. Assoc.17, 507–513 (2010).\n34. Carlson, L. A. et al. Characterizing chronic pain episodes in clinical\ntext at two health care systems: Comprehensive annotation and\ncorpus analysis.JMIR Med. Inf.8, e18659 (2020).\n35. Wray, C. M. et al. Examining the interfacility variation of social\ndeterminants of health in the Veterans Health Administration.Fed.\nPract. 38, 15 (2021).\n36. Feller, D. J. et al. Towards the inference of social and behavioral\ndeterminants of sexual health: development of a gold-standard\ncorpus with semi-supervised learning.AMIA Annu Symp. Proc.2018,\n422 (2018).\n37. Afshar, M. et al. Natural language processing and machine learning to\nidentify alcohol misuse from the electronic health record in trauma\npatients: development and internal validation.J. Am. Med. Inf. Assoc.\n26, 254–261 (2019).\n38. Stemerman, R. et al. Identiﬁcation of social determinants of health\nusing multi-label classiﬁcation of electronic health record clinical\nnotes. JAMIA open4, ooaa069 (2021).\n39. Lituiev, D., et al. Automatic extraction of social determinants of health\nfrom medical notes of chronic lower back pain patients.medRxiv\nhttps://doi.org/10.1101/2022.03.04.22271541 (2022).\n40. Yu, Z. et al. A study of social and behavioral determinants of health in\nlung cancer patients using transformers-based natural language\nprocessing models.AMIA Annu Symp. Proc.2021, 1225 (2021).\n4 1 . C o m e r ,K .F . ,G r a n n i s ,S . ,D i x o n ,B .E . ,B o d e n h a m e r ,D .J .&W i e h e ,S .E .\nIncorporating geospatial capacity withinclinical data systems to address\nsocial determinants of health.Public Health Rep.126,5 4–61 (2011).\n42. Brignone, E., LeJeune, K., Mihalko, A. E., Shannon, A. L. & Sinoway, L.\nI. Self-reported social determinants of health and area-level social\nvulnerability. JAMA Netw. open7, e2412109–e2412109 (2024).\n43. Maroko, A. R. et al. Integrating social determinants of health with\ntreatment and prevention: a new tool to assess local area\ndeprivation.Prev. Chronic Dis.13, E128 (2016).\n44. Brown, E. M. et al. Assessing area-level deprivation as a proxy for\nindividual-level social risks.Am. J. Prev. Med65, 1163–1171 (2023).\n45. Guevara, M. et al. Large language models to identify social\ndeterminants of health in electronic health records.NPJ Digital Med.\n7, 6 (2024).\n46. Gabriel, R. A. et al. On the development and validation of large\nlanguage model-based classiﬁers for identifying social determinants\nof health.\nProc. Natl. Acad. Sci.121, e2320716121 (2024).\n47. Wang, M., Pantell, M. S., Gottlieb, L. M. & Adler-Milstein, J.\nDocumentation and review of social determinants of health data in the\nEHR: measures and associated insights.J. Am. Med. Inf. Assoc.28,\n2608–2616 (2021).\n48. Vest, J. R., Grannis, S. J., Haut, D. P., Halverson, P. K. & Menachemi,\nN. Using structured and unstructured data to identify patients’need\nfor services that address the social determinants of health.Int J. Med.\nInf. 107, 101–106 (2017).\n49. Johnson, A. E. et al. MIMIC-III, a freely accessible critical care\ndatabase. Sci. Data3,1 –9 (2016).\n50. Alsentzer, E. et al. Publicly available clinical BERT embeddings.\nProceedings of the 2nd Clinical Natural Language Processing\nWorkshop.7 2–78 (2019).\n51. Keloth, V. K. et al. Advancing entity recognition in biomedicine via\ninstruction tuning of large language models.Bioinformatics 40,\nbtae163 (2024).\n52. Xie, Q. et al. Medical foundation large language models for\ncomprehensive text analysis and beyond.npj Digital Medicine8, 141\n(2025).\n53. Hu, Y. et al. Improving large language models for clinical named entity\nrecognition via prompt engineering.J. Am. Med. Inform. Assoc. 31,\n1812–1820 (2024).\n54. Chen, Q. et al. Benchmarking large language models for biomedical\nnatural language processing applications and\nrecommendations.Nature Communications.16, 3280 (2025).\n55. Dai, S. et al. inProceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining. 6437-6447.\n56. Dong, X., Wang, Y., Yu, P. S. & Caverlee, J. Disclosure and mitigation\nof gender bias in llms.arXiv preprint arXiv:2402.11190(2024).\n57. Ranjan, R., Gupta, S. & Singh, S. N. A Comprehensive Survey of Bias\nin LLMs: Current Landscape and Future Directions.arXiv preprint\narXiv:2409.16430 (2024).\n58. Faisal, F. & Anastasopoulos, A. Geographic and geopolitical biases of\nlanguage models.Proceedings of the 3rd Workshop on Multi-lingual\nRepresentation Learning (MRL). 139–163 (2023).\n59. Introducing Llama 3.1: Our most capable models to date, https://ai.\nmeta.com/blog/meta-llama-3-1/ (2024).\n60. Achiam, J. et al. Gpt-4 technical report.arXiv preprint\narXiv:2303.08774 (2023).\n61. Rieke, N. et al. The future of digital health with federated learning.NPJ\nDigital Med3, 119 (2020).\n62. Van de Ven, G. M. & Tolias, A. S. Three types of incremental learning.\nNature Machine Intelligence4, 1185–\n1197 (2022).\n63. Wang, J. et al. Prompt engineering for healthcare: Methodologies and\napplications. arXiv preprint arXiv:2304.14670(2023).\n64. Reynolds, L. & McDonell, K. Prompt programming for large language\nmodels: Beyond the few-shot paradigm.Extended Abstracts of the 2021\nCHI Conference on Human Factors in Computing Systems, 1-7 (2021).\n65. Yu, Z., He, L., Wu, Z., Dai, X. & Chen, J. Towards better chain-of-\nthought prompting strategies: A survey.arXiv preprint\narXiv:2310.04959 (2023).\n66. Frayling, E., Lever, J. & McDonald, G. Zero-shot and few-shot\ngeneration strategies for artiﬁcial clinical records.arXiv preprint\narXiv:2403.08664 (2024).\n67. Li, Z., Zhu, H., Lu, Z. & Yin, M. Synthetic data generation with large\nlanguage models for text classiﬁcation: Potential and limitations.\nProceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing,10443–10461 (2023).\n68. Touvron, H. et al. Llama 2: Open foundation andﬁne-tuned chat\nmodels. arXiv preprint arXiv:2307.09288(2023).\n69. Chen, M., Tan, X. & Padman, R. Social determinants of health in\nelectronic health records and their impact on analysis and risk prediction:\nA systematic review.J. Am. Med Inf. Assoc.27,1 7 6 4–1773 (2020).\n70. Dang, Y. et al. Systematic design and data-driven evaluation of social\ndeterminants of health ontology (SDoHO).J. Am. Med. Inform. Assoc.\n30, 1465–1473 (2023).\n71. Kollapally, N. M., Keloth, V. K., Xu, J. & Geller, J. Integrating\ncommercial and social determinants of health: A uniﬁed ontology for\nnon-clinical determinants of health.AMIA Annual Symposium\nProceedings (2023).\n72. Hicks, A., Hanna, J., Welch, D., Brochhausen, M. & Hogan, W. R. The\nontology of medically related social entities: Recent developments.J.\nBiomed. Semant.7,1 –4 (2016).\n73. Phan, N., Dou, D., Wang, H., Kil, D. & Piniewski, B. Ontology-based\ndeep learning for human behavior prediction with explanations in\nhealth social networks.Inf. Sci.384, 298–313 (2017).\n74. Donnelly, K. SNOMED-CT: The advanced terminology and coding\nsystem for eHealth.Stud. Health Technol. Inf.121, 279 (2006).\n75. McDonald, C. J. et al. LOINC, a universal standard for identifying\nlaboratory observations: a 5-year update.Clin. Chem.49, 624–633\n(2003).\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 12\n76. Participant Surveys: Social Determinants of Health, https://www.\nresearchallofus.org/data-tools/survey-explorer/social-determinants-\nsurvey/ (2022).\n77. Gaziano, J. M. et al. Million veteran program: A mega-biobank to study\ngenetic inﬂuences on health and disease.J. Clin. Epidemiol.70,\n214–223 (2016).\n78. Social Determinants of Health Survey - AHIMA, https://ahima.org/\nlanding-pages/social-determinants-of-health-survey/ (2022).\n79. Steinkamp, J., Kantrowitz, J. J. & Airan-Javia, S. Prevalence and\nsources of duplicate information in the electronic medical record.\nJAMA Netw. open5, e2233348–e2233348 (2022).\n80. Markel, A. Copy and paste of electronic health records: A modern\nmedical illness.Am. J. Med123, e9 (2010).\n81. Cohen, R., Aviram, I., Elhadad, M. & Elhadad, N. Redundancy-aware\ntopic modeling for patient record notes.PLoS One9, e87555 (2014).\n82. Cohen, R., Elhadad, M. & Elhadad, N. Redundancy in electronic health\nrecord corpora: analysis, impact on text mining performance and\nmitigation strategies.BMC Bioinforma.14,1 –15 (2013).\n83. Clinical Sectionizer, https://github.com/medspacy/sectionizer(2024).\n84. Sha ﬁghi, K. et al. Social isolation is linked to classical risk factors of\nAlzheimer’s disease-related dementias.PLoS One18, e0280471 (2023).\n85. Majoka, M. A. & Schimming, C. Effect of social determinants of health\non cognition and risk of Alzheimer disease and related dementias.\nClin. Ther.43, 922–929 (2021).\n86. Barak, Y. & Glue, P. Do Social Isolation and Loneliness Kill People with\nAlzheimer’s Disease?.OBM Geriatrics2,1 –5 (2018).\n87. Patra, B. G. et al. Extracting social determinants of health from\nelectronic health records using natural language processing: a\nsystematic review - Supplementary Material.J. Am. Med Inf. Assoc.\n28, 2716–2727 (2021).\n88. Chen, T. & Guestrin, C. Xgboost: A scalable tree boosting system.\nProceedings of the 22nd ACM sigkdd international conference on\nknowledge discovery and data mining, 785-794 (2016).\n8 9 . K i m ,Y . ,J e r n i t e ,Y . ,S o n t a g ,D .&R u s h ,A .M .C h a r a c t e r - a w a r en e u r a l\nlanguage models.Thirtieth AAAI conference on artiﬁcial intelligence(2016).\n90. Pennington, J., Socher, R. & Manning, C. D. Glove: Global vectors for\nword representation.Proceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP), 1532-1543 (2014).\n91. Reimers, N. & Gurevych, I. Sentence-bert: Sentence embeddings\nusing siamese bert-networks.Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 3982–\n3992 (2019).\n92. Introducing LLaMA: A foundational, 65-billion-parameter large\nlanguage model, https://ai.meta.com/blog/large-language-model-\nllama-meta-ai/ (2023).\n93. Taori, R. et al.Alpaca: A Strong, Replicable Instruction-Following\nModel, https://crfm.stanford.edu/2023/03/13/alpaca.html (2023).\nAcknowledgements\nWe thank Dr. Rajarshi Bhowmik for their suggestions and initial feedback in\ndesigning the study. This study was funded by the following grants— NIA\nRF1AG072799, R01AG084236, RM1HG011558 and R01AG083039 and\nR01AG080429. The funders played no role in study design, data collection,\nanalysis and interpretation of data, or the writing of this manuscript.\nAuthor contributions\nV.K. contributed to the study design, conducted the literature review,\ndeveloped the methodology and annotation guidelines, contributed to the\nsoftware development, carried out validation processes, and was primarily\nresponsible for writing the original draft of the manuscript. S.S., K.W. Y.Z.,\nand C.B., C.T. were involved in providing the clinical expertise in identifying\nthe data sources, data collection, selecting SDoH factors and contributed to\nreviewing and revising the manuscript. S.S., K.W., Y.D., X.C. and X.H.\nparticipated in the development of the annotation guidelines, annotating the\nclinical notes and resolving the conﬂicts. S.F., J.F. and H.L. helped with\nobtaining the Mayo corpus, annotations and designing the study, and\nreviewing the manuscript and providing feedback. C.G. was involved in the\nsoftware implementation and packaging the models. H.H. took charge of\nvisualization, speciﬁcally in the preparation ofﬁgures to conceptualize the\nworkﬂows and reviewing the manuscript. Q.C., H.L. and H.X. contributed to\nthe design, development and evaluation of NLP techniques and reviewing\nand revising the manuscript. H.X. and S.S. provided overall supervision for\nthe project, including study design, execution, and evaluation, coordinating\nstudy team and resources, and review and revision of the manuscript. All\nauthors have read and approved the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01645-8.\nCorrespondenceand requests for materials should be addressed to\nHua Xu.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01645-8 Article\nnpj Digital Medicine|           (2025) 8:287 13",
  "topic": "Social determinants of health",
  "concepts": [
    {
      "name": "Social determinants of health",
      "score": 0.49832773208618164
    },
    {
      "name": "Sociology",
      "score": 0.3353142738342285
    },
    {
      "name": "Political science",
      "score": 0.2887725830078125
    },
    {
      "name": "Health care",
      "score": 0.1742757260799408
    },
    {
      "name": "Law",
      "score": 0.06647294759750366
    }
  ]
}