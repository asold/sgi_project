{
  "title": "MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition",
  "url": "https://openalex.org/W3174142642",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2097576564",
      "name": "Shuang Wu",
      "affiliations": [
        "Jiangnan University"
      ]
    },
    {
      "id": "https://openalex.org/A2108729116",
      "name": "Xiaoning Song",
      "affiliations": [
        "Jiangnan University"
      ]
    },
    {
      "id": "https://openalex.org/A2097138251",
      "name": "Zhenhua Feng",
      "affiliations": [
        "University of Surrey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035642486",
    "https://openalex.org/W2251435463",
    "https://openalex.org/W2970681607",
    "https://openalex.org/W2605215742",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2805715057",
    "https://openalex.org/W65845428",
    "https://openalex.org/W2567657016",
    "https://openalex.org/W2255493284",
    "https://openalex.org/W2965690110",
    "https://openalex.org/W2963472581",
    "https://openalex.org/W2988313913",
    "https://openalex.org/W3103921574",
    "https://openalex.org/W2250709962",
    "https://openalex.org/W3034379414",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W60686164",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W2970323499",
    "https://openalex.org/W3137298232",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2250999640",
    "https://openalex.org/W2759200442",
    "https://openalex.org/W2579285701",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964035777",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W2963338481",
    "https://openalex.org/W800621058",
    "https://openalex.org/W3047727388",
    "https://openalex.org/W2912473624",
    "https://openalex.org/W2890459330",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2559281960",
    "https://openalex.org/W1852412531",
    "https://openalex.org/W2250387831"
  ],
  "abstract": "Shuang Wu, Xiaoning Song, Zhenhua Feng. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1529–1539\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1529\nMECT: Multi-Metadata Embedding based Cross-Transformer for\nChinese Named Entity Recognition\nShuang Wu1, Xiaoning Song 1∗, Zhenhua Feng 2,3∗\n1School of Artiﬁcial Intelligence and Computer Science, Jiangnan University, China\n2Department of Computer Science, University of Surrey, UK\n3Centre for Vision, Speech and Signal Processing, University of Surrey, UK\nshuangwu@stu.jiangnan.edu.cn\nx.song@jiangnan.edu.cn, z.feng@surrey.ac.uk\nAbstract\nRecently, word enhancement has become very\npopular for Chinese Named Entity Recogni-\ntion (NER), reducing segmentation errors and\nincreasing the semantic and boundary informa-\ntion of Chinese words. However, these meth-\nods tend to ignore the information of the Chi-\nnese character structure after integrating the\nlexical information. Chinese characters have\nevolved from pictographs since ancient times,\nand their structure often reﬂects more informa-\ntion about the characters. This paper presents a\nnovel Multi-metadata Embedding based Cross-\nTransformer (MECT) to improve the perfor-\nmance of Chinese NER by fusing the structural\ninformation of Chinese characters. Speciﬁ-\ncally, we use multi-metadata embedding in a\ntwo-stream Transformer to integrate Chinese\ncharacter features with the radical-level em-\nbedding. With the structural characteristics\nof Chinese characters, MECT can better cap-\nture the semantic information of Chinese char-\nacters for NER. The experimental results ob-\ntained on several well-known benchmarking\ndatasets demonstrate the merits and superior-\nity of the proposed MECT method.1\n1 Introduction\nNamed Entity Recognition (NER) plays an essen-\ntial role in structuring of unstructured text. It\nis a sequence tagging task that extracts named\nentities from unstructured text. Common cate-\ngories of NER include names of people, places,\norganizations, time, quantity, currency, and some\nproper nouns. NER is the basis for many Nat-\nural Language Processing (NLP) tasks such as\nevent extraction (Chen et al., 2015), question an-\nswering (Diefenbach et al., 2018), information re-\n∗Corresponding author.\n1The source code of the proposed method is publicly\navailable at https://github.com/CoderMusou/\nMECT4CNER.\nCharacter CR HT SC\n题 (topic) 页 是页 日一走页\n榆 (elm) 木 木 俞 木人一月刂\n渡 (ferry) 氵 氵 度 氵广廿又\n脸 (face) 月 月 佥 月人一ツ一\nTable 1: Structure decomposition of Chinese charac-\nters: ‘CR’ denotes the Chinese radical, ‘HT’ denotes\nthe head and tail, and ‘SC’ denotes the structural com-\nponents of Chinese characters.\ntrieval (Khalid et al., 2008), knowledge graph con-\nstruction (Riedel et al., 2013), etc.\nCompared with English, there is no space be-\ntween Chinese characters as word delimiters. Chi-\nnese word segmentation is mostly distinguished\nby readers through the semantic information of\nsentences, posing many difﬁculties to Chinese\nNER (Duan and Zheng, 2011; Ma et al., 2020).\nBesides, the task also has many other challenges,\nsuch as complex combinations, entity nesting, and\nindeﬁnite length (Dong et al., 2016).\nIn English, different words may have the same\nroot or afﬁx that better represents the word’s seman-\ntics. For example, physiology, psychology, sociol-\nogy, technology and zoology contain the same suf-\nﬁx, ‘-logy’, which helps identify the entity of a sub-\nject name. Besides, according to the information\nof English words, root or afﬁxes often determine\ngeneral meanings (Yadav et al., 2018). The root,\nsuch as ‘ophthalmo-’ (ophthalmology), ‘esophage-\n’ (esophagus) and ‘epithelio-’ (epithelium), can\nhelp human or machine to better recognize profes-\nsional nouns in medicine. Therefore, even the state-\nof-the-art methods, such as BERT (Devlin et al.,\n2019) and GPT (Radford et al., 2018), trained on\nlarge-scale datasets, adopt this delicate word seg-\nmentation method for performance boost.\nFor Chinese characters, there is also a structure\n1530\nRadicals Denotation Examples\n鸟(bird) birds 鸡(chicken), 鸭(duck), 鹅(goose), 鹰(eagle)\n艹(grass) herbaceous plants 花 (flower), 草 (grass), 菜 (vegetable), 茶 (tea)\n月(meat) body parts 肾 (kidney), 脚 (foot), 腿 (leg), 脑 (brain)\nTable 2: Some examples of Chinese radicals, including ‘鸟’(bird), ‘艹’(grass) and ‘月’(meat).\nsimilar to the root and afﬁxes in English. Accord-\ning to the examples in Table 1, we can see that\nthe structure of Chinese characters has different\ndecomposition methods, including the Chinese rad-\nical (CR), head and tail (HT) and structural compo-\nnents (SC). Chinese characters have evolved from\nhieroglyphs since ancient times, and their structure\noften reﬂects more information about them. There\nare some examples in Table 2. The glyph structure\ncan enrich the semantics of Chinese characters and\nimprove the performance of NER. For example, the\nBi-LSTM-CRF method (Dong et al., 2016) ﬁrstly\nobtains character-level embedding through the dis-\nassembly of Chinese character structure to improve\nthe performance of NER. However, LSTM is based\non time series modeling, and the input of each cell\ndepends on the output of the previous cell. So the\nLSTM-based model is relatively complicated and\nthe parallel ability is limited.\nTo address the aforementioned issues, we\ntake the advantages of Flat-Lattice Transformer\n(FLAT) (Li et al., 2020) in efﬁcient parallel com-\nputing and excellent lexicon learning, and intro-\nduce the radical stream as an extension on its ba-\nsis. By combining the radical information, we pro-\npose a Multi-metadata Embedding based Cross-\nTransformer (MECT). MECT has the lattice- and\nradical-streams, which not only possesses FLAT’s\nword boundary and semantic learning ability but\nalso increases the structure information of Chinese\ncharacter radicals. This is very effective for NER\ntasks, and has improved the baseline method on\ndifferent benchmarks. The main contributions of\nthe proposed method include:\n• The use of multi-metadata feature embedding\nof Chinese characters in Chinese NER.\n• A novel two-stream model that combines the\nradicals, characters and words of Chinese\ncharacters to improve the performance of the\nproposed MECT method.\n• The proposed method is evaluated on sev-\neral well-known Chinese NER benchmarking\ndatasets, demonstrating the merits and superi-\nority of the proposed approach over the state-\nof-the-art methods.\n2 Related Work\nThe key of the proposed MECT method is to use\nthe radical information of Chinese characters to en-\nhance the Chinese NER model. So we focus on the\nmainstream information enhancement methods in\nthe literature. There are two main types of Chinese\nNER enhancement methods, including lexical in-\nformation fusion and glyph-structural information\nfusion.\nLexical Enhancement In Chinese NER, many\nrecent studies use word matching methods to en-\nhance character-based models. A typical method is\nthe Lattice-LSTM model (Zhang and Yang, 2018)\nthat improves the NER performance by encoding\nand matching words in the lexicon. Recently, some\nlexical enhancement methods were proposed using\nCNN models, such as LR-CNN (Gui et al., 2019a),\nCAN-NER (Zhu and Wang, 2019). Graph networks\nhave also been used with lexical enhancement. The\ntypical one is LGN (Gui et al., 2019b). Besides,\nthere are Transformer-based lexical enhancement\nmethods, such as PLT (Xue et al., 2019) and FLAT.\nAnd SoftLexicon (Ma et al., 2020) introduces lexi-\ncal information through label and probability meth-\nods at the character representation layer.\nGlyph-structural Enhancement Some studies\nalso use the glyph structure information in Chi-\nnese NER. For example, Dong et al. (2016) were\nthe ﬁrst to study the application of radical-level\ninformation in Chinese NER. They used Bi-LSTM\nto extract radical-level embedding and then con-\ncatenated it with the embedding of characters as\nthe ﬁnal input. The radical information used in\nBi-LSTM is structural components (SC) as shown\nin Table 1, which achieved state-of-the-art perfor-\nmance on the MSRA dataset. The Glyce (Meng\net al., 2019) model used Chinese character images\nto extract features such as strokes and structure\nof Chinese characters, achieving promising perfor-\n1531\nTransformerEncoder\n南South11\n京Capital22\n市City33\n长Long44\n江River55\n大Big66\n桥Bridge77\n南京Nanjing12\n南京市NanjingCity13\n市长Mayor34\n长江YangtzeRiver45\n长江大桥YangtzeRiver Bridge47\n大桥Bridge67\nMASKB-LOCI-LOCI-LOCB-LOCI-LOCI-LOCI-LOC\nFigure 1: The input and output of FLAT.\nmance in Chinese NER. Some other methods (Xu\net al., 2019; Song et al., 2020) also proposed to\nuse radical information and Tencent’s pre-trained\nembedding2 to improve the performance. In these\nworks, the structural components of Chinese char-\nacters have been proven to be able to enrich the\nsemantics of the characters, resulting in better NER\nperformance.\n3 Background\nThe proposed method is based on the Flat-Lattice\nTransformer (FLAT) model. Thus, we ﬁrst brieﬂy\nintroduce FLAT that improves the encoder structure\nof Transformer by adding word lattice information,\nincluding semantic and position boundary infor-\nmation. These word lattices are obtained through\ndictionary matching.\nFigure 1 shows the input and output of FLAT. It\nuses the relative position encoding transformed by\nhead and tail position to ﬁt the word’s boundary\ninformation. The relative position encoding, Rij,\nis calculated as follows:\nRij = ReLU(Wr(phi−hj ⊕phi−tj\n⊕pti−hj ⊕pti−tj )),\n(1)\nwhere Wr is a learnable parameter, hi and ti repre-\nsent the head position and tail position of the i-th\ncharacter, ⊕denotes the concatenation operation,\nand pspan is obtained as in Vaswani et al. (2017):\np(2k)\nspan = sin( span\n100002k/dmodel\n), (2)\np(2k+1)\nspan = cos( span\n100002k/dmodel\n), (3)\nwhere pspan corresponds to p in Eq. (1), and span\ndenotes hi −hj, hi −tj, ti −hj and ti −tj. Then\nthe scaled dot-product attention is obtained by:\nAtt(A, V ) =softmax(A)V , (4)\nAij = (Qi + u)⊤Kj + (Qi + v)⊤R∗\nij, (5)\n[Q, K, V ] =Ex[Wq, Wk, Wv], (6)\n2https://ai.tencent.com/ailab/nlp/en/\nembedding.html\nwhere R∗\nij = Rij ·WR. u, v and W2 are learnable\nparameters.\n4 The Proposed MECT Method\nTo better integrate the information of Chinese char-\nacter components, we use Chinese character struc-\nture as another metadata and design a two-stream\nform of multi-metadata embedding network. The\narchitecture of the proposed network is shown in\nFigure 2a. The proposed method is based on the\nencoder structure of Transformer and the FLAT\nmethod, in which we integrate the meaning and\nboundary information of Chinese words. The pro-\nposed two-stream model uses a Cross-Transformer\nmodule similar to the self-attention structure to fuse\nthe information of Chinese character components.\nIn our method, we also use the multi-modal col-\nlaborative attention method that is widely used in\nvision-language tasks (Lu et al., 2019). The differ-\nence is that we add a randomly initialized attention\nmatrix to calculate the attention bias for the two\ntypes of metadata embedding.\n4.1 CNN for Radical-level Embedding\nChinese characters are based on pictographs, and\ntheir meanings are expressed in the shape of ob-\njects. In this case, the structure of Chinese char-\nacters has certain useful information for NER. For\nexample, the radicals such as ‘艹’ (grass) and ‘木’\n(wood) generally represent plants, enhancing Chi-\nnese medicine entity recognition. For another ex-\nample, ‘月’ (body) represents human body parts\nor organs, and ‘疒’ (disease) represents diseases,\nwhich beneﬁts Chinese NER for the medical ﬁeld.\nBesides, the Chinese have their own culture and be-\nlief in naming. Radicals ‘钅’ (metal), ‘木’ (wood),\n‘氵’ (water), ‘火’ (fire), and ‘土’ (earth) rep-\nresented by the Wu-Xing (Five Elements) theory\nare often used as names of people or companies.\nBut ‘锈’ (rust), ‘杀’ (kill), ‘污’ (dirt), ‘灾’\n(disaster) and ‘堕’ (fall) are usually not used\nas names, even if they contain some elements of\nthe Wu-Xing theory. It is because the other rad-\nical components also determine the semantics of\nChinese characters. Radicals that generally appear\nnegative or conﬂict with Chinese cultural beliefs\nare usually not used for naming.\nTherefore, we choose the more informative\nStructural Components (SC) in Table 1 as radical-\nlevel features of Chinese characters and use Convo-\nlutional Neural Network (CNN) to extract character\n1532\n南京市长江大桥\n南京市长江大桥南京南京市市长长江长江大桥大桥南京市长江大桥<pad><pad><pad><pad><pad><pad>\n十冂丫二亠口小亠巾长氵工大木乔\nLatticeEmbedding Radical-levelEmbedding\nSplit characters into SCSouthCapitalCityLongRiverBigBridge\nCNNforRadical-levelEmbedding\nB-LOCI-LOCI-LOCB-LOCI-LOCI-LOCI-LOC MASK\nSouthCapitalCityLongRiverBig NanjingCityMayorYangtzeRiver\nYangtzeRiverBridgeBridge\nConstruct lattice\nBridgeNanjing SouthCapitalCityLongRiverBigBridge\n(a) The whole architecture\nMulti-Head Attention&Random AttentionMulti-Head Attention&Random Attention\nAdd&Norm Add&Norm\nFeed-ForwardNetworkFeed-ForwardNetwork\nAdd&NormAdd&Norm\nLatticeEmbeddingRadical-levelEmbeddingVL\nLinear&CRF\nQL VRQR KRKL\nConcatenateLatticeAttention RadicalAttention (b) The Cross-Transformer module\nFigure 2: The proposed MECT method: (a) the overall structure of MECT; (b) the Cross-Transformer module.\n木人一月刂<pad> <pad>\n●●●\n●●●\nRepresent:Character Radicals\nRepresent:RadicalEmbedding\nLayer:Convolution\nLayer:MaxPooling\nRepresent:Radical-levelEmbeddingLayer:FullyConnected\nFigure 3: CNN for radical feature extraction.\nfeatures. The structure diagram of the CNN net-\nwork is shown in Figure 3. We ﬁrst disassemble the\nChinese characters into SC and then input the radi-\ncals into CNN. Last, we use the max-pooling and\nfully connected layers to get the feature embedding\nof Chinese characters at the radical-level.\n4.2 The Cross-Transformer Module\nAfter radical feature extraction, we propose a Cross-\nTransformer network to obtain the supplementary\nsemantic information of the structure of Chinese\ncharacters. It also uses contextual and lexical infor-\nmation to enrich the semantics of Chinese charac-\nters. The Cross-Transformer network is illustrated\nin Figure 2b. We use two Transformer encoders\nto cross the lattice and radical information of Chi-\nnese characters, which is different from the self-\nattention method in Transformer.\nThe input QL(QR), KL(KR), VL(VR) are ob-\ntained by the linear transformation of lattice and\nradical-level feature embedding:\n\n\nQL(R),i\nKL(R),i\nVL(R),i\n\n\n⊤\n= EL(R),i\n\n\nWL(R),Q\nI\nWL(R),V\n\n\n⊤\n, (7)\nwhere EL and ER are lattice embedding and\nradical-level embedding, I is the identity matrix,\nand each W is a learnable parameter. Then we use\nthe relative position encoding in FLAT to represent\nthe boundary information of a word and calculate\nthe attention score in our Cross-Transformer:\nAttL(AR, VL) =Softmax(AR)VL, (8)\nAttR(AL, VR) =Softmax(AL)VR, (9)\nAL(R),ij = (QL(R),i + uL(R))⊤ER(L),j\n+ (QL(R),i + vL(R))⊤R∗\nL(R),ij,\n(10)\nwhere u and v are learnable parameters for atten-\ntion bias in Eq. (10), AL is the lattice attention\nscore, and AR denotes the radical attention score.\nAnd R∗\nij = Rij ·WR. WR are learnable parame-\nters. The relative position encoding, Rij, is calcu-\nlated as follows:\nRij = ReLU(Wr(phi−hj ⊕pti−tj )). (11)\n4.3 Random Attention\nWe empirically found that the use of random at-\ntention in Cross-Transformer can improve the per-\nformance of the proposed method. This may be\ndue to the requirement of attention bias in lattice\nand radical feature embedding, which can better\nadapt to the scores of two subspaces. Random at-\ntention is a randomly initialized parameter matrix\n1533\nBmax len×max len that is added to the previous at-\ntention score to obtain a total attention score:\nV ∗\nL = Softmax(AR + B)VL, (12)\nV ∗\nR = Softmax(AL + B)VR. (13)\n4.4 The Fusion Method\nTo reduce information loss, we directly concatenate\nthe lattice and radical features and input them into\na fully connected layer for information fusion:\nFusion(V ∗\nL , V ∗\nR) = (V ∗\nR ⊕V ∗\nL )Wo + b, (14)\nwhere ⊕denotes the concatenation operation, Wo\nand b are learnable parameters.\nAfter the fusion step, we mask the word part and\npass the fused feature to a Conditional Random\nField (CRF) (Lafferty et al., 2001) module.\n5 Experimental Results\nIn this section, we evaluate the proposed MECT\nmethod on four datasets. To make the experimen-\ntal results more reasonable, we also set up two\nadditional working methods for assessing the per-\nformance of radicals in a two-stream model. We\nuse the span method to calculate F1-score (F1), pre-\ncision (P), and recall (R) as the evaluation metrics.\n5.1 Experimental Settings\nWe use four mainstream Chinese NER benchmark-\ning datasets: Weibo (Peng and Dredze, 2015;\nHe and Sun, 2016), Resume (Zhang and Yang,\n2018), MSRA (Levow, 2006), and Ontonotes\n4.0 (Weischedel and Consortium, 2013). The cor-\npus of MSRA and Ontonotes 4.0 comes from news,\nthe corpus of Weibo comes from social media, and\nthe corpus of Resume comes from the resume data\nin Sina Finance. Table 3 shows the statistical infor-\nmation of these datasets. Among them, the Weibo\ndataset has four types of entities, including PER,\nORG, LOC, and GPE. Resume has eight types of\nentities, including CONT, EDU, LOC, PER, ORG,\nPRO, RACE, and TITLE. OntoNotes 4.0 has four\ntypes of entities: PER, ORG, LOC, and GPE. The\nMSRA dataset contains three types of entities, i.e.,\nORG, PER, and LOC.\nWe use the state of the art method, FLAT, as the\nbaseline model. FLAT is a Chinese NER model\nbased on Transformer and combined with lattice.\nBesides, we also compared the proposed method\nwith both classic and innovative Chinese NER mod-\nels. We use the more informative ‘SC’ as the radi-\ncal feature, which comes from the online Xinhua\nDatasets Types Train Dev Test\nWeibo Sentences\nEntities\n1.35k\n1.89k\n0.27k\n0.39k\n0.27k\n0.42k\nResume Sentences\nEntities\n3.8k\n1.34k\n0.46k\n0.16k\n0.48k\n0.15k\nOntoNotes Sentences\nEntities\n15.7k\n13.4k\n4.3k\n6.95k\n4.3k\n7.7k\nMSRA Sentences\nEntities\n46.4k\n74.8k\n-\n-\n4.4k\n6.2k\nTable 3: Statistics of the benchmarking datasets.\nModels NE NM Overall\nPeng and Dredze (2015) 51.96 61.05 56.05\nPeng and Dredze (2016)∗ 55.28 62.97 58.99\nHe and Sun (2017a) 50.60 59.32 54.82\nHe and Sun (2017b)∗ 54.50 62.17 58.23\nCao et al. (2018) 54.34 57.35 58.70\nLattice-LSTM 53.04 62.25 58.79\nCAN-NER 55.38 62.98 59.31\nLR-CNN 57.14 66.67 59.92\nLGN 55.34 64.98 60.21\nPLT 53.55 64.90 59.76\nSoftLexicon (LSTM) 59.08 62.22 61.42\nBaseline - - 60.32\nMECT 61.91 62.51 63.30\nBERT - - 68.20\nBERT + MECT - - 70.43\nTable 4: Results obtained on Weibo (%).\nDictionary3. The pre-trained embedding of charac-\nters and words are the same as FLAT.\nFor hyper-parameters, we used 30 1-D convolu-\ntion kernels with the size of 3 for CNN. We used\nthe SMAC (Hutter et al., 2011) algorithm to search\nfor the optimal hyper-parameters. Besides, we set a\ndifferent learning rate for the training of the radical-\nlevel embedding with CNN. Readers can refer to\nthe appendix for our hyper-parameter settings.\n5.2 Comparison with SOTA Methods\nIn this section, we evaluate and analyze the pro-\nposed MECT method with a comparison to both\nthe classic and state of the art methods. The experi-\nmental results are reported in Tables 4−74. Each\ntable is divided into four blocks. The ﬁrst block\nincludes classical Chinese NER methods. The sec-\nond one reports the results obtained by state of the\nart approaches published recently. The third and\n3http://tool.httpcn.com/Zi/.\n4In Tables 4−7, ‘∗’ denotes the use of external labeled\ndata for semi-supervised learning and ‘†’ denotes the use of\ndiscrete features.\n1534\nModels P R F1\nZhang and Yang (2018)A 93.72 93.44 93.58\nZhang and Yang (2018)B 94.07 94.42 94.24\nZhang and Yang (2018)C 93.66 93.31 93.48\nZhang and Yang (2018)D 94.53 94.29 94.41\nLattice-LSTM 94.81 94.11 94.46\nCAN-NER 95.05 94.82 94.94\nLR-CNN 95.37 94.84 95.11\nLGN 95.28 95.46 95.37\nPLT 95.34 95.46 95.40\nSoftLexicon (LSTM) 95.30 95.77 95.53\n+ bichar 95.71 95.77 95.74\nBaseline - - 95.45\nMECT 96.40 95.39 95.89\nBERT - - 95.53\nBERT + MECT - - 95.98\nTable 5: Results obtained on Resume (%). For Zhang\nand Yang (2018), Arepresents word-based LSTM’, B\nindicates ‘word-based + char + bichar LSTM’,Crepre-\nsents the ‘char-based LSTM’ model, andDis the ‘char-\nbased + bichar + softword LSTM’ model.\nfourth ones are the results obtained by the proposed\nMECT method as well as the baseline models.\nWeibo: Table 4 shows the results obtained on\nWeibo in terms of the F1 scores of named enti-\nties (NE), nominal entities (NM), and both (Over-\nall). From the results, we can observe that MECT\nachieves the state-of-the-art performance. Com-\npared with the baseline method, MECT improves\n2.98% in terms of the F1 metric. For the NE metric,\nthe proposed method achieves 61.91%, beating all\nthe other approaches.\nResume: The results obtained on the Resume\ndataset are reported in Table 5. The ﬁrst block\nshows Zhang and Yang (2018) comparative results\non the character-level and word-level models. We\ncan observe that the performance of incorporating\nword features into the character-level model is bet-\nter than other models. Additionally, MECT com-\nbines lexical and radical features, and the F1 score\nis higher than the other models and the baseline\nmethod.\nOntonotes 4.0: Table 6 shows the results ob-\ntained on Ontonotes 4.0. The symbol ‘§’ indicates\ngold segmentation, and the symbol ‘¶’ denotes au-\ntomated segmentation. Other models have no seg-\nmentation and use lexical matching. Compared\nto the baseline method, the F1 score of MECT is\nincreased by 0.47%. MECT also achieves a high\nrecall rate, keeping the precision rate and recall rate\nrelatively stable.\nModels P R F1\nYang et al. (2018)§ 65.59 71.84 68.57\nYang et al. (2018)§∗† 72.98 80.15 76.40\nChe et al. (2013)§∗ 77.71 72.51 75.02\nWang et al. (2013)§∗ 76.43 72.32 74.32\nZhang and Yang (2018)B§ 78.62 73.13 75.77\nZhang and Yang (2018)B¶ 73.36 70.12 71.70\nLattice-LSTM 76.35 71.56 73.88\nCAN-NER 75.05 72.29 73.64\nLR-CNN 76.40 72.60 74.45\nLGN 76.13 73.68 74.89\nPLT 76.78 72.54 74.60\nSoftLexicon (LSTM) 77.28 74.07 75.64\n+ bichar 77.13 75.22 76.16\nBaseline - - 76.45\nMECT 77.57 76.27 76.92\nBERT - - 80.14\nBERT + MECT - - 82.57\nTable 6: Results on Ontonotes 4.0 (%), where ‘§’ de-\nnotes gold segmentation and ‘¶’ denotes auto segmen-\ntation.\nMSRA: Table 7 shows the experimental results\nobtained on MSRA. In the ﬁrst block, the result pro-\nposed by Dong et al. (2016) is the ﬁrst method us-\ning radical information in Chinese NER. From the\ntable, we can observe that the overall performance\nof MECT is higher than the existing SOTA meth-\nods. Similarly, our recall rate achieves a higher\nperformance so that the ﬁnal F1 has a certain per-\nformance boosting.\nWith BERT: Besides the single-model evalu-\nation on the four datasets, we also evaluated the\nproposed method when combining with the SOTA\nmethod, BERT. The BERT model is the same as\nFLAT using the ‘BERT-wwm’ released by Cui et al.\n(2020). The results are shown in the fourth block\nof each table. The results of BERT are taken from\nthe FLAT paper. We can ﬁnd that MECT further\nimproves the performance of BERT signiﬁcantly.\n5.3 Effectiveness of Cross-Transformer\nThere are two sub-modules in the proposed Cross-\nTransformer method: lattice and radical attentions.\nFigure 4 includes two heatmaps for the normal-\nization of the attention scores of the two modules.\nFrom the two ﬁgures, we can see that lattice atten-\ntion pays more attention to the relationship between\nwords and characters so that the model can obtain\nthe position information and boundary information\nof words. Radical attention focuses on global in-\nformation and corrects the semantic information of\n1535\nModels P R F1\nChen et al. (2006) 91.22 81.71 86.20\nZhang et al. (2006)∗ 92.20 90.18 91.18\nZhou et al. (2013) 91.86 88.75 90.28\nLu et al. (2016) - - 87.94\nDong et al. (2016) 91.28 90.62 90.95\nLattice-LSTM 93.57 92.79 93.18\nCAN-NER 93.53 92.42 92.97\nLR-CNN 94.50 92.93 93.71\nLGN 94.19 92.73 93.46\nPLT 94.25 92.30 93.26\nSoftLexicon (LSTM) 94.63 92.70 93.66\n+ bichar 94.73 93.40 94.06\nBaseline - - 94.12\nMECT 94.55 94.09 94.32\nBERT - - 94.95\nBERT + MECT - - 96.24\nTable 7: Results obtained on MSRA (%).\n(a) Radical attention\n (b) Lattice attention\nFigure 4: Visualization of cross-attention, in which the\ncoordinates 0-15 are used for the characters part and\nthe coordinates 16-24 are for the words part. The two\nsub-ﬁgures show the radical and lattice attention scores\nrespectively.\neach character through radical features. Therefore,\nlattice and radical attentions provide complemen-\ntary information for the performance-boosting of\nthe proposed MECT method in Chinese NER.\n5.4 Impact of Radicals\nWe visualized the radical-level embedding obtained\nby the CNN network and found that the cosine dis-\ntance of Chinese characters with the same radical\nor similar structure is smaller. For example, Figure\n5 shows part of the Chinese character embedding\ntrained on the Resume dataset. The highlighted\ndots represent Chinese characters that are close to\nthe character ‘华’. We can see that they have the\nsame radicals or similar structure. It can enhance\nthe semantic information of Chinese characters to\na certain extent.\nWe also examined the inference results of MECT\nand FLAT on Ontonotes 4.0 and found many ex-\nciting results. For example, some words with a\nFigure 5: Embedding visualization of the characters\nrelated to ‘ 华’ in two-dimensional space. Gray dots\nindicate larger cosine distances.\npercentage like ‘百分之四十三点二 (43.2%)’ is\nincorrectly labelled as PER in the training dataset,\nwhich causes FLAT to mark the percentage of\nwords with PER on the test dataset, while MECT\navoids this situation. There are also some words\nsuch as ‘田时’ and ‘以国’ that appear in the lex-\nicon, which was mistakenly identiﬁed as valid\nwords by FLAT, leading to recognition errors. Our\nMECT addresses these issues by paying global\nattention to the radical information. Besides, in\nFLAT, some numbers and letters are incorrectly\nmarked as PER, ORG, or others. We compared the\nPER label accuracy of FLAT and MECT on the test\ndataset. FLAT achieves 81.6%, and MECT reaches\n86.96%, which is a very signiﬁcant improvement.\n5.5 Analysis in Efﬁciency and Model Size\nWe use the same FLAT method to evaluate the par-\nallel and non-parallel inference speed of MECT\non an NVIDIA GeForce RTX 2080Ti card, using\nbatch size = 16 and batch size = 1. We use the\nnon-parallel version of FLAT as the standard and\ncalculate the other models’ relative inference speed.\nThe results are shown in Figure 6. According to\nthe ﬁgure, even if MECT adds a Transformer en-\ncoder to FLAT, the speed is only reduced by 0.15 in\nterms of the parallel inference speed. Our model’s\nspeed is considerable relative to LSTM, CNN, and\nsome graph-based network models. Because Trans-\nformer can make full use of the GPU’s parallel\ncomputing power, the speed of MECT does not\ndrop too much, but it is still faster than other mod-\nels. The model’s parameter is between 2 and 4\nmillion, determined by the max sentence length in\nthe dataset and the dmodel size in the model.\n5.6 Ablation Study\nTo validate the effectiveness of the main compo-\nnents of the proposed method, we set up two exper-\niments in Figure 7. In Experiment A, we only use a\n1536\nLatticeLSTM\nLatticeLSTM\nLR-CNN\nLGN\nCGN\n FLAT\nFLAT\nMECT\nMECT\n0\n2\n4\n6\n8Relative Speed\n×0.3 ×0.64 ×0.75 ×0.84\n×1.45\n×1.0\n×4.97\n×0.96\n×4.2\nFigure 6: Relative inference speed of each model\nbased on non-parallel FLAT ♣. where ‘ ♣’ represents\nthe inference speed under non-parallel conditions, ‘ ♠’\nrepresents the inference speed under parallel condi-\ntions, and the value of ‘♦’ is derived from the relative\nspeed above FLAT.\nsingle-stream model with a modiﬁed self-attention,\nwhich is similar to the original FLAT model. The\ndifference is that we use a randomly initialized at-\ntention matrix (Random Attention) for the attention\ncalculation. We combine lattice embedding and\nradical-level embedding as the input of the model.\nThe purpose is to verify the performance of the two-\nstream model relative to the single-stream model.\nIn Experiment B, we do not exchange the query’s\nfeature vector. We replace the cross-attention with\ntwo sets of modiﬁed self-attention and follow the\ntwo modules’ output with the same fusion method\nas MECT. The purpose of experiment B is to ver-\nify the effectiveness of MECT relative to the two-\nstream model without crossover. Besides, we eval-\nuate the proposed MECT method by removing the\nrandom attention module.\nTable 8 shows the ablation study results. 1) By\ncomparing the results of Experiment A with the\nresults of Experiment B and MECT, we can ﬁnd\nthat the two-stream model works better. The use\nof lattice-level and radical-level features as the two\nstreams of the model helps the model to better un-\nderstand and extract the semantic features of Chi-\nnese characters. 2) Based on the results of Experi-\nment B and MECT, we can see that by exchanging\nthe two query feature vectors, the model can extract\nfeatures more effectively at the lattice and radical\nlevels. They have different attention mechanisms to\nobtain contextual information, resulting in global\nand local attention interaction. This provides better\ninformation extraction capabilities for the proposed\nmethod in a complementary way. 3) Last, the per-\nformance of MECT drops on all the datasets by\nLatticeEmbeddingRadical-levelEmbedding\nAdaptSelfAttention\n(a) Experiment A\nRadical-levelEmbeddingLatticeEmbedding\nAdaptSelfAttentionAdaptSelfAttention (b) Experiment B\nFigure 7: Two interactive attention experiment set-\ntings.\nExperiments Weibo Resume OntoNotes MSRA\nExp. A 60.77 95.42 76.43 94.20\nExp. B 61 95.54 76.78 94.18\nMECT 62.69 95.89 76.92 94.32\n- RA 61.53 95.31 76.64 94.25\nTable 8: The F1 scores (%) of the four experimental\nmethods on different datasets. RA stands for random\nattention. We verify all the labels (NE and NM) on\nWeibo.\nremoving the random attention module (the last\nrow). This indicates that, as an attention bias, ran-\ndom attention can eliminate the differences caused\nby different embeddings, thereby improving the\nmodel’s performance further.\n6 Conclusion\nThis paper presented a novel two-stream network,\nnamely MECT, for Chinese NER. The proposed\nmethod uses multi-metadata embedding that fuses\nthe information of radicals, characters and words\nthrough a Cross-Transformer network. Addition-\nally, random attention was used for further perfor-\nmance boost. Experimental results obtained on four\nbenchmarks demonstrate that the radical informa-\ntion of Chinese characters can effectively improve\nthe performance for Chinese NER.\nThe proposed MECT method with the radical\nstream increases the complexity of a model. In the\nfuture, we will consider how to integrate the char-\nacters, words and radical information of Chinese\ncharacters with a more efﬁcient way in two-stream\nor multi-stream networks to improve the perfor-\nmance of Chinese NER and extend it to other NLP\ntasks.\nAcknowledgments\nThis work was supported in part by the National\nKey Research and Development Program of China\n1537\n(2017YFC1601800), the National Natural Science\nFoundation of China (61876072, 61902153) and\nthe Six Talent Peaks Project of Jiangsu Province\n(XYDXX-012). We also thank Xiaotong Xiang and\nJun Quan for their help on editing the manuscript.\nReferences\nPengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and\nShengping Liu. 2018. Adversarial transfer learn-\ning for Chinese named entity recognition with self-\nattention mechanism. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 182–192, Brussels, Bel-\ngium. Association for Computational Linguistics.\nWanxiang Che, Mengqiu Wang, Christopher D. Man-\nning, and Ting Liu. 2013. Named entity recogni-\ntion with bilingual constraints. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 52–62, Atlanta,\nGeorgia. Association for Computational Linguistics.\nAitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun.\n2006. Chinese named entity recognition with condi-\ntional probabilistic models. In SIGHAN Workshop\non Chinese Language Processing.\nYubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and\nJun Zhao. 2015. Event extraction via dynamic\nmulti-pooling convolutional neural networks. In\nACL—IJCNLP , volume 1, pages 167–176.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020 , pages 657–668,\nOnline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDennis Diefenbach, Vanessa Lopez, Kamal Singh, and\nPierre Maret. 2018. Core techniques of question\nanswering systems over knowledge bases: a survey.\nKAIS, 55(3):529–569.\nChuanhai Dong, Jiajun Zhang, Chengqing Zong,\nMasanori Hattori, and Hui Di. 2016. Character-\nbased lstm-crf with radical-level features for chinese\nnamed entity recognition. In Natural Language Un-\nderstanding and Intelligent Applications, pages 239–\n250. Springer.\nHuanzhong Duan and Yan Zheng. 2011. A study\non features of the crfs-based chinese named entity\nrecognition. International Journal of Advanced In-\ntelligence, 3(2):287–294.\nTao Gui, Ruotian Ma, Qi Zhang, Lujun Zhao, Yu-Gang\nJiang, and Xuanjing Huang. 2019a. Cnn-based chi-\nnese ner with lexicon rethinking. In Proceedings of\nthe Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI-19 , pages 4982–4988.\nInternational Joint Conferences on Artiﬁcial Intelli-\ngence Organization.\nTao Gui, Yicheng Zou, Qi Zhang, Minlong Peng, Jinlan\nFu, Zhongyu Wei, and Xuan-Jing Huang. 2019b. A\nlexicon-based graph neural network for chinese ner.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1039–\n1049.\nHangfeng He and Xu Sun. 2016. F-score driven max\nmargin neural network for named entity recognition\nin chinese social media. CoRR, abs/1611.04234.\nHangfeng He and Xu Sun. 2017a. F-score driven max\nmargin neural network for named entity recognition\nin Chinese social media. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short\nPapers, pages 713–718, Valencia, Spain. Associa-\ntion for Computational Linguistics.\nHangfeng He and Xu Sun. 2017b. A uniﬁed model\nfor cross-domain and semi-supervised named entity\nrecognition in chinese social media. In Proceedings\nof the Thirty-First AAAI Conference on Artiﬁcial In-\ntelligence, AAAI’17, page 3216–3222. AAAI Press.\nFrank Hutter, Holger H Hoos, and Kevin Leyton-\nBrown. 2011. Sequential model-based optimiza-\ntion for general algorithm conﬁguration. In Inter-\nnational conference on learning and intelligent opti-\nmization, pages 507–523. Springer.\nMahboob Alam Khalid, Valentin Jijkoun, and Maarten\nde Rijke. 2008. The impact of named entity normal-\nization on information retrieval for question answer-\ning. In Advances in Information Retrieval , pages\n705–710, Berlin, Heidelberg. Springer Berlin Hei-\ndelberg.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth Inter-\nnational Conference on Machine Learning , ICML\n’01, pages 282–289, San Francisco, CA, USA. Mor-\ngan Kaufmann Publishers Inc.\nGina-Anne Levow. 2006. The third international Chi-\nnese language processing bakeoff: Word segmen-\ntation and named entity recognition. In Proceed-\nings of the Fifth SIGHAN Workshop on Chinese\n1538\nLanguage Processing, pages 108–117, Sydney, Aus-\ntralia. Association for Computational Linguistics.\nXiaonan Li, Hang Yan, Xipeng Qiu, and Xuanjing\nHuang. 2020. FLAT: Chinese NER using ﬂat-lattice\ntransformer. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 6836–6842, Online. Association for\nComputational Linguistics.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, volume 32, pages 13–23. Curran Asso-\nciates, Inc.\nYanan Lu, Yue Zhang, and Dong-Hong Ji. 2016. Multi-\nprototype chinese character embedding. In LREC.\nRuotian Ma, Minlong Peng, Qi Zhang, Zhongyu Wei,\nand Xuanjing Huang. 2020. Simplify the usage of\nlexicon in Chinese NER. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5951–5960, Online. As-\nsociation for Computational Linguistics.\nYuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie,\nFan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, and\nJiwei Li. 2019. Glyce: Glyph-vectors for chinese\ncharacter representations. In Advances in Neural\nInformation Processing Systems , volume 32, pages\n2746–2757. Curran Associates, Inc.\nNanyun Peng and Mark Dredze. 2015. Named entity\nrecognition for Chinese social media with jointly\ntrained embeddings. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 548–554, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nNanyun Peng and Mark Dredze. 2016. Improving\nnamed entity recognition for Chinese social media\nwith word segmentation representation learning. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 149–155, Berlin, Germany. As-\nsociation for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nSebastian Riedel, Limin Yao, Andrew McCallum, and\nBenjamin M Marlin. 2013. Relation extraction with\nmatrix factorization and universal schemas. In Pro-\nceedings of the 2013 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n74–84.\nC. Song, Y . Xiong, W. Huang, and L. Ma. 2020.\nJoint self-attention and multi-embeddings for chi-\nnese named entity recognition. In 2020 6th Interna-\ntional Conference on Big Data Computing and Com-\nmunications (BIGCOM), pages 76–80.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nMengqiu Wang, Wanxiang Che, and Christopher D.\nManning. 2013. Effective bilingual constraints for\nsemi-supervised learning of named entity recogniz-\ners. In Proceedings of the Twenty-Seventh AAAI\nConference on Artiﬁcial Intelligence , AAAI’13,\npage 919–925. AAAI Press.\nRalph M Weischedel and Linguistic Data Consortium.\n2013. Ontonotes release 5.0. Title from disc label.\nCanwen Xu, Feiyang Wang, Jialong Han, and Chen-\nliang Li. 2019. Exploiting multiple embeddings for\nchinese named entity recognition. In Proceedings of\nthe 28th ACM International Conference on Informa-\ntion and Knowledge Management, CIKM ’19, page\n2269–2272, New York, NY , USA. Association for\nComputing Machinery.\nMengge Xue, Bowen Yu, Tingwen Liu, Bin Wang, Erli\nMeng, and Quangang Li. 2019. Porous lattice-based\ntransformer encoder for chinese ner. arXiv preprint\narXiv:1911.02733.\nVikas Yadav, Rebecca Sharp, and Steven Bethard. 2018.\nDeep afﬁx features improve neural named entity rec-\nognizers. In Proceedings of the Seventh Joint Con-\nference on Lexical and Computational Semantics ,\npages 167–172, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nJie Yang, Zhiyang Teng, Meishan Zhang, and Yue\nZhang. 2018. Combining discrete and neural fea-\ntures for sequence labeling. In Computational Lin-\nguistics and Intelligent Text Processing, pages 140–\n154, Cham. Springer International Publishing.\nSuxiang Zhang, Ying Qin, Juan Wen, and Xiaojie\nWang. 2006. Word segmentation and named entity\nrecognition for sighan bakeoff3. In SIGHAN Work-\nshop on Chinese Language Processing , pages 158–\n161.\nYue Zhang and Jie Yang. 2018. Chinese NER us-\ning lattice LSTM. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1554–\n1564, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nJunsheng Zhou, Weiguang Qu, and Fen Zhang. 2013.\nChinese named entity recognition via joint identiﬁ-\ncation and categorization. Chinese journal of elec-\ntronics, 22(2):225–230.\nYuying Zhu and Guoxin Wang. 2019. CAN-NER: Con-\nvolutional Attention Network for Chinese Named\nEntity Recognition. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\n1539\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 3384–3393, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nA Appendix\nA.1 Range of Hyper-parameters\nWe manually selected parameters on the two large-\nscale datasets, including Ontonotes 4.0 and MSRA.\nFor the two small datasets, Weibo and Resume, we\nused the SMAC algorithm to search for the best\nhyper-parameters. The range of parameters is listed\nin Table 9.\nHyper-parameter Range\noutput dropout [0.1, 0.2, 0.3]\nlattice dropout [0.1, 0.2, 0.3]\nradical dropout [0.1, 0.2, 0.3, 0.4]\nwarm up [0.1, 0.2, 0.3]\nhead num [8]\ndhead [16, 20]\ndmodel [128, 160]\nlr [1e-3, 25e-4]\nradical lr [6e-4, 25e-4]\nmomentum [0.85, 0.97]\nTable 9: The searching range of hyper-parameters.",
  "topic": "Metadata",
  "concepts": [
    {
      "name": "Metadata",
      "score": 0.7939838767051697
    },
    {
      "name": "Computer science",
      "score": 0.7239891886711121
    },
    {
      "name": "Named-entity recognition",
      "score": 0.7034764289855957
    },
    {
      "name": "Natural language processing",
      "score": 0.6530975103378296
    },
    {
      "name": "Transformer",
      "score": 0.5891664028167725
    },
    {
      "name": "Embedding",
      "score": 0.5868331789970398
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49674779176712036
    },
    {
      "name": "Computational linguistics",
      "score": 0.480854332447052
    },
    {
      "name": "Joint (building)",
      "score": 0.46174484491348267
    },
    {
      "name": "Information retrieval",
      "score": 0.4098261594772339
    },
    {
      "name": "Speech recognition",
      "score": 0.33861303329467773
    },
    {
      "name": "World Wide Web",
      "score": 0.25058168172836304
    },
    {
      "name": "Engineering",
      "score": 0.12602713704109192
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111599522",
      "name": "Jiangnan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I28290843",
      "name": "University of Surrey",
      "country": "GB"
    }
  ]
}