{
  "title": "Matching Patients to Clinical Trials with Large Language Models",
  "url": "https://openalex.org/W4385374442",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2308246978",
      "name": "Jin Qiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1592712140",
      "name": "Wang Zifeng",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Floudas, Charalampos S.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1826611282",
      "name": "CHEN Fangyuan",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Gong, Changlin",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Bracken-Clarke, Dara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2995997551",
      "name": "Xue Elisabetta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1915056974",
      "name": "Yang, Yifan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350181399",
      "name": "Sun, Jimeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184280727",
      "name": "Lu zhiyong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2975956387",
    "https://openalex.org/W2974214648",
    "https://openalex.org/W2975249097",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4287623114",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W2270764879",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W2911661483",
    "https://openalex.org/W3173904029",
    "https://openalex.org/W4361807050",
    "https://openalex.org/W2470268543"
  ],
  "abstract": "Patient recruitment is challenging for clinical trials. We introduce TrialGPT, an end-to-end framework for zero-shot patient-to-trial matching with large language models. TrialGPT comprises three modules: it first performs large-scale filtering to retrieve candidate trials (TrialGPT-Retrieval); then predicts criterion-level patient eligibility (TrialGPT-Matching); and finally generates trial-level scores (TrialGPT-Ranking). We evaluate TrialGPT on three cohorts of 183 synthetic patients with over 75,000 trial annotations. TrialGPT-Retrieval can recall over 90% of relevant trials using less than 6% of the initial collection. Manual evaluations on 1,015 patient-criterion pairs show that TrialGPT-Matching achieves an accuracy of 87.3% with faithful explanations, close to the expert performance. The TrialGPT-Ranking scores are highly correlated with human judgments and outperform the best-competing models by 43.8% in ranking and excluding trials. Furthermore, our user study reveals that TrialGPT can reduce the screening time by 42.6% in patient recruitment. Overall, these results have demonstrated promising opportunities for patient-to-trial matching with TrialGPT.",
  "full_text": " 1 \nMatching Patients to Clinical Trials with  Large Language Models Qiao Jin, M.D.1, Zifeng Wang, M.S.2, Charalampos S. Floudas, M.D., D.MSc., M.S.3, Fangyuan Chen, M.D.4, Changlin Gong, M.D.5, Dara Bracken-Clarke, M.D.3, Elisabetta Xue, M.D.3, Yifan Yang, B.S.1,6, Jimeng Sun, Ph.D.2, Zhiyong Lu, Ph.D.1  Author aJiliations 1. National Center for Biotechnology Information (NCBI), National Library of Medicine (NLM), National Institutes of Health (NIH).  2. Department of Computer Science, University of Illinois Urbana-Champaign.  3. Center for Immuno-Oncology, Center for Cancer Research, National Cancer Institute, National Institutes of Health. 4. School of Medicine, University of Pittsburgh. 5. Jacobi Medical Center, Albert Einstein College of Medicine. 6. School of Computer Science, University of Maryland College Park.   Corresponding author Zhiyong Lu, Ph.D., FACMI, FIAHSI Senior Investigator NCBI/NLM/NIH 8600 Rockville Pike \n 2 \nBethesda, MD 20894, USA Tel: 301-594-7089 E-mail: zhiyong.lu@nih.gov \n \nAbstract Patient recruitment is challenging for clinical trials. We introduce TrialGPT, an end-to-end framework for zero-shot patient-to-trial matching with large language models. TrialGPT comprises three modules: it Ô¨Årst performs large-scale Ô¨Åltering to retrieve candidate trials (TrialGPT-Retrieval); then predicts criterion-level patient eligibility (TrialGPT-Matching); and Ô¨Ånally generates trial-level scores (TrialGPT-Ranking). We evaluate TrialGPT on three cohorts of 183 synthetic patients with over 75,000 trial annotations. TrialGPT-Retrieval can recall over 90% of relevant trials using less than 6% of the initial collection. Manual evaluations on 1,015 patient-criterion pairs show that TrialGPT-Matching achieves an accuracy of 87.3% with faithful explanations, close to the expert performance. The TrialGPT-Ranking scores are highly correlated with human judgments and outperform the best-competing models by 43.8% in ranking and excluding trials. Furthermore, our user study reveals that TrialGPT can reduce the screening time by 42.6% in patient recruitment. Overall, these results have demonstrated promising opportunities for patient-to-trial matching with TrialGPT.    \n 3 \nIntroduction Clinical trials examine the eeectiveness of medical interventions and provide crucial evidence that can be used to guide clinical practice. They also oeer an opportunity for participants to receive experimental treatments that could potentially improve their health outcomes. However, matching patients to suitable clinical trials can be a challenging process1-3. This process includes analyzing a patient‚Äôs medical history, understanding the eligibility criteria of each clinical trial, and ensuring a match that satisÔ¨Åes both patient needs and trial requirements. As such, manually matching patients and clinical trials is often labor-intensive, time-consuming, and prone to human errors.  Recently, artiÔ¨Åcial intelligence (AI) has shown promise in improving the eeiciency and accuracy of patient-trial matching4,5. Based on the directionality, there are two types of patient-trial matching tasks. The ‚Äútrial-to-patient‚Äù scheme matches one trial to a list of candidate patients6, which is a common need for clinical trial organizers and can be done by converting the trial criteria to structured query languages and searching the patient database6-8. On the other hand, the ‚Äúpatient-to-trial‚Äù scheme matches one patient to a list of candidate clinical trials9-13. In this study, we focus on the patient-centric ‚Äúpatient-to-trial‚Äù scheme because such a model can empower individual patients as well as referral oeices to explore a large set of potentially eligible clinical trials. However, the heterogeneity and ambiguity inherent in patient records and clinical trial criteria induce signiÔ¨Åcant challenges for AI algorithms. Prior eeorts encoded patient records and trial criteria into dense embeddings using neural networks, aiming to represent them in the same embedding space \n 4 \nthat enables patient-trial matching through similarity search14-16. Nonetheless, training neural networks with the language understanding capability of criteria texts and patient records requires large datasets. This is often infeasible due to the lack of paired patient-criterion matching annotations. Moreover, the dense retrieval process is non-explainable, making it challenging to debug and often resulting in skepticism among medical experts when applied to new criteria and patient groups.  In this work, we aim to explore how recent large language models (LLMs) such as GPT-417 can aid the process of patient-to-trial matching in a data-eeicient and transparent way. LLMs are transformer-based models18 that can understand a given context and generate human-like responses accordingly. They have shown state-of-the-art capabilities in both the general domain17,19 and biomedicine20, including question answering21-26 and clinical trial design27,28. Several pilot studies have also explored using LLMs to enhance the Ô¨Årst-stage retrieval of clinical trials by information extraction29,30, perform data augmentation with synthetic data31-\n33, and structuralize clinical trial criteria34. In contrast, our study presents an end-to-end solution, i.e., trial retrieval, matching, and ranking, to streamline clinical trial patient recruitment with LLMs.  The proposed method, namely TrialGPT, consists of three key components: TrialGPT-Retrieval, TrialGPT-Matching, and TrialGPT-Ranking. Given a patient note, TrialGPT-Retrieval Ô¨Årst locates hundreds of highly relevant candidate clinical trials from a large initial collection using keyword generation and hybrid-fusion retrieval. Based on the retrieved trials, TrialGPT-\n 5 \nMatching predicts the criterion-level eligibility of each patient with three elements: natural language explanations showing the relevance of the patient to the criterion, locations of relevant sentences in the patient note that are relevant to the target criterion, and the eligibility classiÔ¨Åcation indicating whether the patient meets this criterion. Finally, TrialGPT-Ranking aggregates the TrialGPT-Matching results at the trial level and uses such scores to get a ranked list of clinical trials based on the eligibility of a given patient.   Our evaluations are conducted with three publicly available cohorts of 183 synthetic patients with over 75,000 trial eligibility annotations. Experimental results show that TrialGPT-Retrieval can recall over 90% of relevant clinical trials using less than 6% of the candidate clinical trials. The model also generates better keywords for trial retrieval compared to those produced by four human clinicians. Three domain experts evaluate TrialGPT-Matching on 1,015 patient-criterion pairs, and the results show that TrialGPT-Matching can accurately explain patient-criterion relevance, locate relevant sentences, and predict criterion-level eligibility with an accuracy close to that of human experts. We then evaluate the trial-level scores by TrialGPT-Ranking and results show that they are highly correlated with expert eligibility annotations. Such scores can be used to match eligible trials with patients eeectively and exclude ineligible trials, with a performance of 43.8% higher than the best baselines.  We also conducted a pilot user study that mimics the actual clinical trial matching task at the National Cancer Institute (NCI). In the evaluation, each patient-trial pair is evaluated by \n 6 \none medical expert with TrialGPT and another one without TrialGPT. We also ensure that each medical expert annotates half of the pairs with TrialGPT and half without to mitigate the skill dieerences between the annotators when computing the time reduction. The overall time saving for all patient-trial pairs is about 42.6%, which shows its great potential to enhance the eeiciency of the clinical trial matching process.  \nResults TrialGPT Architecture The architecture of TrialGPT is shown in Figure 1. Overall, TrialGPT consists of three components: Retrieval (Figure 1a), Matching (Figure 1b), and Ranking (Figure 1c). TrialGPT-Retrieval is designed to Ô¨Ålter out most of the irrelevant clinical trials in a large initial collection. SpeciÔ¨Åcally, TrialGPT-Retrieval can generate a list of keywords based on the patient summary and feed them into a hybrid-fusion retriever to obtain a relatively small subset from a potentially large initial collection, while maintaining high recalls of relevant clinical trials. This retrieval step is designed to ensure the scalability of the TrialGPT in real-world applications where there are tens of thousands of clinical trials to consider, such as matching against as many as 23,000 active clinical trials being conducted in a single country (i.e., the United States).  \n 7 \n Figure 1. The overall architecture of TrialGPT. (a) TrialGPT-Retrieval can Ô¨Ålter out most of the irrelevant trials from the initial collection and return a list of candidate clinical trials; (b) For a given patient, TrialGPT-Matching can explain the relevance, generate the evident sentence locations, and predict the eligibility classiÔ¨Åcation for each criterion in a trial; (c) TrialGPT-Ranking can aggregate the criterion-level predictions by TrialGPT-Matching and use these scores to perform Ô¨Åne-grained ranking to get the Ô¨Ånal recommended trials. \nA 58-year-old African-\nAmerican woman presents\nto the ER with episodic\npressing/burning anterior\nchest pain that bega\nn two days earlier for the\nfirst time in her life ...\nTrialGPT-\nRetrieval Clinical trials\nTop 6% trials with \nover 90% recallPatient Summary\nTrialGPT-\nMatching\nCriterion-by-criterion       analysis for each trial\nExplanation\nEvidence\nEligibility\nCriterion 1\nTrial Score Clinical trialsTrial ScoreTrial ScoreTrial Scores\nAggregation\nTrialGPT-\nRanking\nExplanation\nEvidence\nEligibility\nCriterion ...\nExplanation\nEvidence\nEligibility\nCriterion 2\nExplanation\nEvidence\nEligibility\nCriterion 3\na\nb\nc\nTop 10 clinical trialsTrial-level Scores\n 8 \n For each candidate clinical trial returned by the TrialGPT-Retrieval, TrialGPT-Matching analyzes the eligibility of the given patient in a criterion-by-criterion style. For each criterion, TrialGPT-Matching generates three elements: (1) the explanation of the patient-criterion relevance; (2) the locations of relevant sentences in the patient notes to the criterion; (3) the eligibility classiÔ¨Åcation for the patient-criterion pair.   Finally, TrialGPT-Ranking aggregates the criterion-level predictions by TrialGPT-Matching to derive trial-level scores that can be subsequently used to rank clinical trials by eligibility and exclude the ones that are explicitly ineligible.  Patient Cohorts To evaluate TrialGPT, we use the patient summaries and clinical trials from three publicly available cohorts: a test collection for patient-trial matching published by Special Interest Group on Information Retrieval (SIGIR) in 201610, and the 2021 and 2022 Clinical Trials (CT) tracks9,13 of the Text REtrieval Conference (TREC). The SIGIR cohort has three dieerent patient-trial eligibility labels: irrelevant (‚Äúwould not refer this patient for this clinical trial‚Äù), potential (‚Äúwould consider referring this patient to this clinical trial upon further investigation‚Äù), and eligible (‚Äúhighly likely to refer this patient for this clinical trial‚Äù). The TREC cohorts also have three dieerent patient-trial eligibility labels: irrelevant (‚Äúthe patient is not relevant for the trial in any way‚Äù), excluded/ineligible (‚Äúthe patient has the condition that the trial is targeting, but the exclusion criteria make the patient ineligible‚Äù), and eligible (‚Äúthe \n 9 \npatient is eligible to enroll in the trial‚Äù). We used the combination of the judged clinical trials for all patients in the individual cohort as the considered initial trials for TrialGPT-Retrieval for two reasons: (1) the largest realistic search space for clinical trials is about 23 thousand (the maximum number of active trials in a single country), which is similar to the number of judged trials in the TREC cohorts; (2) these cohorts used pooled judgment, which means that there might be eligible clinical trials outside of the labeled corpus but will be considered as irrelevant if retrieved. The baseline statistics of patient cohorts are shown in Table 1.  Table 1. Baseline statistics of the three patient cohorts used in this work. We show the mean ¬± standard deviation for applicable variables. ‚ÄúNone‚Äù denotes there is no such eligibility label in the corresponding cohort. SIGIR: the patient-trial matching cohort published at the Special Interest Group on Information Retrieval (SIGIR). TREC: the Text REtrieval Conference (TREC). CT: the clinical trials track at TREC. Cohort SIGIR TREC 2021 CT TREC 2022 CT N 58 75 50 Age (year) 38.5 ¬± 23.7 41.6 ¬± 19.4 35.3 ¬± 20.2 Sex (male: female) 29: 29 38: 37 28: 22 Note length (words) 88.7 ¬± 36.8 156.2 ¬± 45.4 109.9 ¬± 21.6 Eligible trials / patient 7.3 ¬± 6.7 74.3 ¬± 49.0 78.8 ¬± 67.3 Potential trials / patient 11.7 ¬± 10.2 None None Excluded trials / patient None 80.3 ¬± 60.3 60.7 ¬± 65.5 Irrelevant trials / patient 47.1 ¬± 19.5 323.2 ¬± 93.2 568.4 ¬± 164.1 Considered initial trials 3621 26149 26581  \n 10 \nTrialGPT-Retrieval can generate keywords for e<ective clinical trial Ô¨Åltering \n Figure 2. First-stage retrieval results. (a) Overview of TrialGPT-Retrieval. LLMs Ô¨Årst generate a list of keywords for a given patient note. These keywords are used to derive the keyword-level relevant clinical trials, which are then fused to generate a Ô¨Ånal ranking. (b) Recalls of \nA 58-year-old African-American woman presentsto the ER with episodicpressing/burning anteriorchest pain that began two days earlier for thefirst time in her life ...\nPatient Note\nLLMs\nChest Pain\nHypertension\nObesity\n...\nKeywords\nClinical Trials\nClinical Trials\nClinical Trials\n...\nClinical Trials\nKeyword Trial Ranking Fused Trial Ranking\nRetrieval\nFusion\nTrialGPT-Retrievala\nb\n1\n 11 \nrelevant clinical trials at dieerent depths for various query types and retrievers. The hybrid retriever combines the results of the BM25 (lexical matching) and the MedCPT (semantic matching) retrievers. Source data are provided as a Source Data Ô¨Åle.  As shown in Figure 2a, in the retrieval stage, large language models are prompted to generate a list of keywords for the initial screening of clinical trials at scale. For each keyword, we then get the list of relevant clinical trials using a hybrid retriever that matches both lexical and semantic information. The retrieval results are then combined into a ranked list with reciprocal rank fusion. We evaluated the retrieval performance of keywords generated by GPT-4 and GPT-3.5 using all three cohorts. As a comparison, we also tested when the raw patient notes were used to retrieve clinical trials. On the SIGIR cohort, we show the performance of clinician-generated keywords that are annotated in the original dataset.   The recalls of relevant clinical trials at dieerent depths are shown in Figure 2b. In all three cohorts, keywords generated by GPT-4 and GPT-3.5 consistently achieved the highest performance, while directly using the raw patient notes achieved the lowest performance. This indicates that large language models can eeectively generate keywords from patient notes for clinical trial retrieval. In terms of the retriever, the semantic MedCPT is much better than the lexical BM25 retriever, and the hybrid retriever achieves the best performance. In the SIGIR cohort, the performance of clinician-generated keywords is between the LLM-generated keywords and the raw notes. This shows that large language models can already generate better keywords than human clinicians for clinical trial retrieval. Overall, the \n 12 \naverage recall at the top 500 retrieved clinical trials is 50.0%, 83.4%, 86.2% for the raw note, TrialGPT-Retrieval with GPT-3.5, and TrialGPT-Retrieval with GPT-4, respectively. On average, to recall at least 90% of the relevant clinical trials, GPT-4-based TrialGPT-Retrieval only needs to select 5.5% of the initial document collection, and GPT-3.5-based TrialGPT-Retrieval needs 7.0%. As such, TrialGPT-Retrieval signiÔ¨Åcantly improves the scalability by Ô¨Åltering out most of the irrelevant clinical trials and returning a short list of candidate clinical trials for further Ô¨Åner-grained analyses by TrialGPT-Matching and TrialGPT-Ranking.  TrialGPT-Matching achieves a high criterion-level prediction accuracy As shown in Figure 1b, TrialGPT-Matching Ô¨Årst generates the rationales and the relevant sentences for each criterion. Then, it predicts the criterion-level eligibility classiÔ¨Åcation based on the rationales. TrialGPT assigns each inclusion criterion a label within {Included, Not included, Not enough information, Not applicable} and each exclusion criterion a label within {Excluded, Not excluded, Not enough information, Not applicable}. We sampled 105 patient-trial pairs from 53 patients in the SIGIR cohort, which contains 1,015 patient-criterion pairs. Three physicians were recruited and manually annotated these pairs regarding the criterion-level output elements by GPT-4: (1) the correctness of TrialGPT relevance explanation between the given patient and the criterion, (2) the relevant sentence locations in the patient note, and (3) the criterion-level prediction of the given patient‚Äôs eligibility. Consensus annotations derived from individual annotations and further discussions are used as the ground truth.  \n 13 \n Figure 3. Manual evaluations of criterion-level predictions by GPT-4-based TrialGPT-Matching. (a) The percentage of correct, partially correct, and incorrect relevance explanations generated by TrialGPT-Matching; (b) Evaluation results of the relevant sentences located by TrialGPT-Matching based on 405 criterion-level annotations where at \na\nb\nc\nd\n1\n 14 \nleast one sentence is labeled as relevant in the ground-truth. 95% conÔ¨Ådence intervals estimated by bootstrapping are shown as error bars; (c) The confusion matrices of the eligibility for inclusion criteria predicted by human experts and TrialGPT-Matching; (d) The confusion matrices of the eligibility for exclusion criteria predicted by human experts and TrialGPT-Matching. Not Incl.: Not included. Not Excl.: Not excluded. No Info.: Not enough information. Not Appl.: Not applicable. Source data are provided as a Source Data Ô¨Åle.  Evaluating relevance explanations: We show the percentage of ‚Äúcorrect‚Äù , ‚Äúpartially correct‚Äù and ‚Äúincorrect‚Äù TrialGPT explanations in Figure 3a. Overall, most explanations are ‚Äúcorrect‚Äù (87.8%) by manual evaluations, while less than 10% of explanations are ‚Äúpartially correct‚Äù (9.66%), and only a small proportion are ‚Äúincorrect‚Äù (2.56%). We also found that most of the incorrect explanations are for criteria labeled as ‚Äúnot included‚Äù and ‚Äúnot excluded‚Äù , which usually require implicit inference. TrialGPT exhibits much fewer mistakes when the criteria are explicitly ‚Äúincluded‚Äù or ‚Äúexcluded‚Äù . These results suggest that TrialGPT can eeectively explain how a patient is relevant to an eligibility criterion.  Evaluating relevant sentence locations: We compare the relevant sentences predicted by TrialGPT-Matching against the ground-truth expert annotations. As shown in Figure 3b, the TrialGPT-predicted sentence locations are 90.1% correct (precision) and cover 87.9% of the ground-truth relevant sentence IDs (recall), leading to an F1 score of 88.6%. The performance of TrialGPT is close to that of human experts, ranging from 86.9% to 91.5%. This \n 15 \nshows that TrialGPT can faithfully locate relevant sentences in patient notes, providing strong explainability for human expert use and oversight.  Evaluating eligibility prediction: Finally, we evaluate the criterion-level eligibility labels predicted by TrialGPT and expert annotators against the ground-truth annotations. Figures 3c and 3d show the confusion matrices for these predictions. Overall, TrialGPT-Matching achieves a prediction accuracy of 0.873, close to the expert performance (0.887‚Äì0.900). For the inclusion criteria, TrialGPT reaches a prediction accuracy of 0.899 for all four labels, which is within the experts' accuracy range from 0.876 to 0.916. For the exclusion criteria, while the accuracy is high for criteria labeled with ‚Äúexcluded‚Äù (1.00) and ‚Äúnot applicable‚Äù (0.98), TrialGPT tends to confuse among ‚Äúnot excluded‚Äù , ‚Äúno relevant information‚Äù , and ‚Äúnot applicable‚Äù . TrialGPT achieves an accuracy of 0.859 on the exclusion criteria. These results suggest that TrialGPT can accurately predict patient eligibility at the criterion level, with a performance close to that of human experts.   We further inspected the 26 criterion-level predictions that are labeled as ‚ÄúIncorrect‚Äù by annotator consensus. Four types of errors have been identiÔ¨Åed: (E1) Incorrect reasoning, where TrialGPT predicts ‚Äúnot enough information‚Äù but the matching result can be implicitly inferred; (E2) Lack of medical knowledge, such as not recognizing that one medical term is synonymous with another or that one term is a subtype of another ; (E3) Ambiguous label deÔ¨Ånitions, confusing ‚Äúnot enough information‚Äù with ‚Äúnot applicable‚Äù due to unclear or overlapping label deÔ¨Ånitions; (E4) Other unclassiÔ¨Åed errors. Supplementary Table 1 shows \n 16 \nthe proportion and example of each error type made by TrialGPT. Most (30.7%) of the errors are due to incorrect reasoning, followed by ambiguous or redundant deÔ¨Ånitions of the eligibility classes (26.9%). Lack of medical knowledge contributes to about 15.4% of the total errors. These results suggest that improving the medical capabilities of the backbone LLM is an important future direction.  TrialGPT-Ranking scores correlate with trial-level eligibility TrialGPT-Matching has achieved high prediction accuracy at the criterion level. However, since one clinical trial typically has many inclusion and exclusion criteria, trial-level scores should be computed to decide to which extent a given patient is eligible or ineligible. As such, the third component, TrialGPT-Ranking, aggregates the criterion-level predictions into trial-level scores, as shown in Figure 1c. In this section, we analyze the correlations between patient-trial eligibility and eight variants of trial-level scores, which are computed by two types of methods: linear aggregations and LLM aggregations. For each patient in the three cohorts, we consider the top 500 clinical trials returned by TrialGPT-Retrieval for the analysis. The results are presented as box plots in Figure 4.  \n 17 \n Figure 4. Correlation between diJerently aggregated TrialGPT scores and the ground-truth patient-trial eligibility labels. (a) The percentage of inclusion criteria predicted as ‚Äúincluded‚Äù by TrialGPT; (b) The percentage of inclusion criteria predicted as ‚Äúnot included‚Äù; (c) The percentage of inclusion criteria predicted as ‚Äúno relevant information‚Äù; (d) The LLM-aggregated relevance score; (e) The percentage of exclusion criteria predicted as ‚Äúexcluded‚Äù; (f) The percentage of exclusion criteria predicted as ‚Äúnot excluded‚Äù; (g) The percentage of exclusion criteria predicted as ‚Äúno relevant information‚Äù; (h) The LLM-aggregated eligibility score. ‚Äú*‚Äù denotes p < 0.05, ‚Äú**‚Äù denotes p < 0.01, ‚Äú***‚Äù denotes p < \n 18 \n0.001, and ‚Äún.s. ‚Äù denotes not signiÔ¨Åcant (p > 0.05) by two-sided independent t-test. There are 60,240 unlabeled, 15,459 irrelevant, 6,981 excluded, 647 potential, and 8,173 eligible patient-trial pairs. Center line, median; box limits, upper and lower quartiles; whiskers, 1.5x interquartile range; points, outliers. Source data are provided as a Source Data Ô¨Åle, including all the exact p values.  Linear aggregations: Six scores are computed by counting the percentages of six dieerent criterion-level eligibility predictions of TrialGPT (details in Methods). Their correlations with dieerent types of trial-level eligibility labels are shown in Figure 4a-c for inclusion criteria and Figure 4e-g for exclusion criteria. Figure 4a shows the percentage of inclusion criteria predicted as ‚Äúincluded‚Äù by TrialGPT. As expected, Figure 4a implies that the patients meet the highest percentages of inclusion criteria in eligible clinical trials and meet the lowest percentages of inclusion criteria in irrelevant clinical trials. The percentage of met inclusion criteria falls in between for relevant but ineligible trials. Figure 4b shows the percentage of inclusion criteria predicted as ‚Äúnot included‚Äù , which approximately follows the reverse trends of the met inclusion criteria (Figure 4a). Noticeably, no inclusion criterion is classiÔ¨Åed by TrialGPT-Matching as ‚Äúnot included‚Äù in most of the eligible patient-trial pairs, conÔ¨Årming the correctness of the model. Figure 4d shows the percentage of exclusion criteria predicted as ‚Äúexcluded‚Äù . Importantly, patients meet more exclusion criteria in ineligible clinical trials than in eligible clinical trials. Similar to Figure 4b, no exclusion criteria are labeled as met by TrialGPT-Matching in most eligible and potentially eligible patient-trial pairs. This is a \n 19 \ncharacteristic feature of patient-trial pairs that are explicitly excluded and can be exploited in patient-trial matching.  LLM aggregations: We also use LLMs to further aggregate the criterion-level predictions of TrialGPT, resulting in the general relevance and the eligibility scores (details in Methods). The general relevance score (0~100) is shown in Figure 4d, where the irrelevant patient-trial pairs are much lower than the other labeled groups. Eligible and ineligible/potential patient-trial groups have certain overlaps, but the former is still signiÔ¨Åcantly higher than the latter. The eligibility score (-100~100) is shown in Figure 4h, where negative scores denote ineligible, positive scores denote eligible, and a score of 0 denotes neutral. Overall, the trends of LLM-aggregated eligibility scores follow those of the LLM-aggregated relevance score. Noticeably, the eligible patient-trial pairs and the excluded (but relevant) patient-trial pairs have the least distributional overlap in TrialGPT-aggregated eligibility score, which indicates its superiority in distinguishing these two groups.  In summary, criterion-level TrialGPT-Matching predictions can be aggregated into trial-level scores by TrialGPT-Ranking that are highly correlated with patient-trial eligibility. The results of linear aggregations demonstrate that eligible patient-trial pairs have the highest proportions of met inclusion criteria and unmet exclusion criteria, while ineligible patient-trial pairs have the highest proportions of met exclusion criteria. In addition, the LLM aggregations are also signiÔ¨Åcantly correlated with the manual eligibility labels. These results \n 20 \nsuggest that the aggregated scores of TrialGPT can be used to rank or exclude clinical trials for patient recruitment.  TrialGPT-Ranking can e<ectively rank and exclude candidate clinical trials In this section, we evaluate TrialGPT by ranking candidate clinical trials and excluding ineligible clinical trials for given patients (component Figure 1c). Based on the correlation analysis, we design a suite of scoring methods to aggregate criterion-level predictions of TrialGPT to generate a trial-level score for ranking the candidate trials. Similarly, we consider the top 500 clinical trials returned by TrialGPT-Retrieval for each patient in the three cohorts Table 2 shows the Normalized Discounted Cumulative Gain at rank 10 (NDCG@10), Precision at rank 10 (P@10), and Area Under the Receiver Operating Characteristic curve (AUROC) of dieerent methods in comparison to state-of-the-art models, which are described in Methods.   Ranking candidate clinical trials: As shown in Table 2, TrialGPT-Ranking outperforms all compared baselines, including dual-encoder, cross-encoder, and encoder-decoder models trained on dieerent biomedical and clinical natural language inference (NLI)35 datasets. The best baseline for ranking clinical trials is the cross-encoder BioLinkBERT36 trained on MedNLI37, which achieves the NDCG@10 of 0.4797 and the P@10 of 0.4281. The most eeective features of GPT-4-based TrialGPT for ranking are the LLM-aggregated eligibility scores. They achieve NDCG@10 of 0.7252 and P@10 of 0.6724, which are much higher than other aggregations. Combining both linear and LLM aggregations yields the highest \n 21 \nNDCG@10 performance of 0.7275 and the P@10 of 0.6699 for GPT-4-based TrialGPT-Ranking. While GPT-3.5-based TrialGPT-Ranking also surpasses all baselines, the improvements are not as signiÔ¨Åcant as GPT-4-based TrialGPT-Ranking. It should be noted that the results are not directly comparable to the results of TREC CT participating systems as we used the initial corpora of more realistic sizes and reported the average performance on three dieerent cohorts.  Table 2. Performance of diJerent methods for ranking and excluding clinical trials. The Sign() function assigns suitable signs for the corresponding task, e.g., for ‚Äú% Included‚Äù , it will be ‚Äú+‚Äù for ranking and ‚Äú-‚Äù for excluding clinical trials. Aggr.: aggregation. NDCG@10: normalized discounted cumulative gain at 10. P@10: precision at 10. AUROC: the area under the receiver operating characteristic curve. Application Ranking  Excluding Overall Method / Metric NDCG@10 P@10 AUROC Average SciFive38  (encoder-decoder) Further trained on MedNLI37 0.4271 0.3787 0.5895 0.4652 \nBioBERT39 (dual-encoder) Further trained40 on MNLI41, SNLI42, SciNLI43, SciTail44, MedNLI37, and STSB45 \n0.4091 0.3746 0.5952 0.4596 \nPubMedBERT46 (dual-encoder) 0.4327 0.3874 0.5976 0.4726 \nSapBERT47 (dual-encoder) 0.4148 0.3738 0.5933 0.4606 \nBioLinkBERT36 (cross-encoder) Further trained on MedNLI37 0.4797 0.4281 0.6176 0.5085 \n 22 \nTrialGPT-Ranking (GPT-3.5) Feature combination 0.5395 0.5115 0.6582 0.5697 \nTrialGPT-Ranking (GPT-4) Linear aggr. Sign(% Included) 0.6097 0.5653 0.6765 0.6172 Sign(% Not included) 0.4923 0.4556 0.6652 0.5377 Sign(% Excluded) 0.3930 0.3715 0.6477 0.4707 Sign(% Not excluded) 0.4130 0.3886 0.5820 0.4612 LLM aggr. Sign(Relevance) 0.7281 0.6700 0.7402 0.7128 Sign(Eligibility) 0.7252 0.6724 0.7895 0.7290 Feature combination 0.7275 0.6688 0.7979 0.7314  Excluding ineligible clinical trials: Table 2 also shows the AUROC of excluding candidate trials, which is modeled as a binary classiÔ¨Åcation task. Similarly, the best baseline for excluding clinical trials is the cross-encoder BioLinkBERT36 trained on MedNLI37, achieving the AUROC of 0.6176. This result only shows marginal improvement over the random score baseline, indicating that the task of excluding ineligible trials presents signiÔ¨Åcant challenges. Unlike in the task of ranking clinical trials, the percentage of inclusion criteria predicted as ‚Äúnot included‚Äù and the percentage of exclusion criteria predicted as ‚Äúexcluded‚Äù also achieve comparable AUROC individually. Again, both GPT-3.5-based and GPT-4-based TrialGPT-Ranking outperforms all baselines, and the combination of the features achieves an AUROC of 0.6582 and 0.7979, respectively.  \n 23 \nOn average, the feature combination of GPT-4-based TrialGPT-Ranking achieved the performance of 0.7314, which is about 43.8% better than the best performing baseline score of 0.5085. These experimental results show that TrialGPT can eeectively rank candidate clinical trials and exclude ineligible clinical trials, which could facilitate the trial-matching process.   TrialGPT can reduce the screening time for patient-trial matching To evaluate whether TrialGPT can assist clinicians in performing the patient recruitment task, we set up a user evaluation that mimics patient-trial matching in real clinical settings. The evaluation result is shown in Figure 5a. Here, we consider six oncology clinical trials. Among these trials, four are conducted at the National Cancer Institute and have a physician coauthor (C.F .) as principal or associate investigator, and the other two are highly related. The physicians also created six semi-synthetic clinical vignettes (cases 1 to 6, available in the Supplementary Note) based on actual, anonymized patient referral requests. Cases 1-3 are short summaries of patients, while cases 4-6 are more extensive descriptions. For each patient-trial combination, the matching task is to quickly screen the eligibility criteria to either reject the patient or include the patient for further investigation. Two medical doctors (Annotators X and Y) performed the tasks, where half of the patient-trial pairs have TrialGPT predictions, and the other half do not have them. We also ensure that for each patient-trial pair, one annotator screened with TrialGPT, and another without. The accuracy of annotations with TrialGPT is higher than without (97.2% vs. 91.7%), although both are above 90% and their dieerences are not statistically signiÔ¨Åcant. The comparison results for \n 24 \nmatching eeiciency are shown in Figure 5b. There is no signiÔ¨Åcant time dieerence between Annotator X and Annotator Y in screening with TrialGPT (32.7s v.s. 37.8s, p=0.73) or without TrialGPT (57.5s v.s. 65.4s, p=0.75) by two-sided independent t-test, which indicates that they have similar baseline screening capabilities. We observe a consistent trend of improved eeiciency with TrialGPT: 35.9% to 51.9% less time is spent for dieerent patients, and 32.4% to 60.2% less time is spent among dieerent trials. In general, more time saving is observed for long cases (43.5%) than for short cases (41.0%). The overall time saving for all patient-trial pairs is about 42.6%, which can greatly improve the eeiciency of a team evaluating patient referrals for clinical trials. \n 25 \n Figure 5. Results of the patient-trial matching user study. (a) Experimental design and actual screening times of each patient-trial pair by two expert annotators; (b) Comparison of \nCase 1 2 3 4 5 6\nWithout TrialGPT (6 trials) 19.0 s 82.7 s 26.5 s 42.7 s 95.0 s 103.0 s\nWith TrialGPT (6 trials) 11.2 s 50.0 s 14.3 s 24.3 s 45.7 s 66.0 s\nTime Saving 41.2% 39.5% 45.9% 43.0% 51.9% 35.9%\nTrial A B C D E F\nWithoutTrialGPT(6cases) 111.0 s 20.5 s 105.2 s 36.0 s 39.3 s 56.8 s\nWithTrialGPT(6cases) 75.0 s 10.7 s 52.8 s 21.7 s 15.7 s 35.7 s\nTimeSaving 32.4% 48.0% 49.8% 39.8% 60.2% 37.2%\nb\nc\nShort Cases Long Cases Annotator X Annotator Y All (36)\nWithout TrialGPT (18) 42.7 s 80.2 s 65.4 s 57.5 s 61.5 s\nWith TrialGPT (18) 25.2 s 45.3 s 37.8 s 32.7 s 35.3 s\nTime Saving 41.0% 43.5% 42.2% 43.1% 42.6%\nP-value 0.0182‚Ä† 0.0003‚Ä† 0.1633‚Ä° 0.2472‚Ä° 1.75e-5‚Ä†\na\nd\n‚Ä†: Paired t-test;  ‚Ä°: independent t-test\n20 41 7 27 9 15\n132 9 253 8 40 51\n26 22 30 16 9 31\n12 10 62 77 29 15\n178 43 55 20 16 29\n174 6 149 28 83 127\n24 25 5 3 2 3\n85 3 136 6 11 62\n26 4 54 7 10 10\n7 11 32 122 5 20\n298 10 108 7 72 8\n134 3 57 25 44 184\nCase 1\nCase 2\nCase 3\nCase 4\nCase 5\nCase 6\nCase 1\nCase 2\nCase 3\nCase 4\nCase 5\nCase 6\nTrial \nA\nTrial \nB\nTrial \nC\nTrial \nD\nTrial \nE\nTrial \nF\nTrial \nA\nTrial \nB\nTrial \nC\nTrial \nD\nTrial \nE\nTrial \nF\nScreening time (s) with TrialGPT Screening time (s) without TrialGPT\nAnnotator X screening time (s) Annotator Y screening time (s)\n 26 \nscreening time aggregated by dieerent clinical trials; (c) Comparison of screening time aggregated by dieerent patient cases; (d) Comparison of screening time aggregated by short cases, long cases, annotators, and all pairs. Numbers in parentheses denote the sample sizes in the corresponding group of the comparison. Within the annotator (e.g., Annotator X or Y), signiÔ¨Åcant tests are conducted by two-sided independent t-test. Other signiÔ¨Åcant tests are two-sided paired t-tests. Trial A, B, C, D, E, and F denote NCT04432597, NCT05012098, NCT04287868, NCT04847466, NCT04719988, and NCT04894370, respectively. Source data are provided as a Source Data Ô¨Åle.  Discussion We propose TrialGPT, a framework for end-to-end patient-trial matching with LLMs. The technical novelty of TrialGPT is to utilize LLMs for three vital sub-tasks: (1) First-stage retrieval: TrialGPT-Retrieval can Ô¨Ålter out most of the irrelevant clinical trials from a large initial collection containing tens of thousands of clinical trials; (2) Criterion-level prediction: TrialGPT-Matching can match patient and clinical trials at criterion-level with explanations, where previous models sueer from the lack of annotated instances and cannot provide explanations; (3) Aggregation of criterion-level predictions: TrialGPT-Ranking further utilizes LLMs to aggregate the criterion-level predictions to generate trial-level scores, which outperforms other linearly aggregated scores at ranking and excluding candidate clinical trials. The language understanding capability of LLMs is exploited in all tasks, and the language generation capability of LLMs lays the foundation for the explainability of the overall workÔ¨Çow. Comparing to the structurization approach adopted by other studies7,34, \n 27 \nTrialGPT utilizes LLMs to analyze the patient summary and the criteria in natural language, which does not require the criteria to follow any formats and is thus more Ô¨Çexible.  For evaluation, we use three publicly available patient-trial matching datasets, where the patients are represented by a paragraph of free-text clinical summary. However, clinical trial matching sometimes requires the recruiters to check the patients‚Äô information more comprehensively, which involves longitudinal clinical notes, lab values, and even imaging data. This requires the model to (1) attend to much longer contexts, (2) process structured data, and (3) process multi-modal inputs. These aspects have not been evaluated by this study but are worth exploring in future work. Additionally, future evaluations should investigate how TrialGPT performs when integrating data from electronic health records (EHRs), which often include a combination of structured and unstructured data sources. The ability to seamlessly incorporate such diverse data types would signiÔ¨Åcantly enhance the real-world applicability and increase the validation sample size of our framework. Further, the SIGIR and TREC datasets focus on the semantics of the clinical trial inclusion/exclusion criteria, excluding factors such as geolocation and trial recruitment status as these are addressable through traditional structured query approaches. As a result, any use of TrialGPT would similarly need to ensure the identiÔ¨Åed trials are appropriate for a patient along these lines as well.   Our work does not justify the position that clinical trial matching should be fully automatic and exclude human recruiters. Experts should always be in the loop of medical AI \n 28 \ndeployments, and the TrialGPT matching results are only used to assist them in improved eeiciency. Evaluation in real-life clinical trial matching scenarios should also focus more on eeiciency improvement for human recruiters, instead of solely reporting the prediction performance. In this sense, the explanation capability of TrialGPT, or more generally LLMs, is particularly helpful. This is exempliÔ¨Åed by our pilot user study showing that TrialGPT can signiÔ¨Åcantly reduce 42.6% of the screening time on average.   While our experiments have shown promising results in patient-to-trial matching with TrialGPT, this study has three main limitations. First, TrialGPT relies on OpenAI‚Äôs GPT series LLMs such as GPT-3.5 and GPT-4 as the backbone model. Although GPT-4 is currently the most capable LLM, it is closed-source and can only be accessed via commercial applications or API. Future studies should explore using and Ô¨Åne-tuning other open-source LLMs as alternatives. Second, while our study proposes a novel framework of patient-to-trial matching with LLMs, there are various other prompting strategies for each of the TrialGPT components that are worth exploring in future work. Third, our pilot user study is of limited scope in sample size. Nonetheless, it oeers insights into the potential beneÔ¨Åts of LLMs for assisting clinical trial matching and provides impetus to conduct larger-scale prospective evaluations regarding the impact of LLM-assisted clinical workÔ¨Çows in future studies.   In summary, we present TrialGPT, a novel architecture that uses large language models to perform patient-trial matching. Our evaluations show that TrialGPT can eeectively recall relevant clinical trials in a large-scale collection and accurately predict criterion-level \n 29 \neligibility with faithful explanations. We also explore dieerent pooling methods to aggregate the criterion-level predictions to trial-level scores that can be used to rank or exclude a list of candidate trials. The pilot user study has clearly demonstrated that TrialGPT signiÔ¨Åcantly reduces the screening time needed for human experts. As such, we anticipate that large language models can be valuable in assisting the process of patient-trial matching.  \nMethods Patient cohorts We use three publicly available cohorts in this study: the SIGIR 2016 cohort, the TREC 2021 CT cohort, and the TREC 2022 CT cohort. All three cohort annotations only use the patient note and eligibility criteria without considering the geolocations and recruitment status of the clinical trials. Their baseline characteristics are shown in Table 1.  SIGIR 2016 The original cohort contains 60 patient case reports, but 1 report is removed since the report is about a group of patients (topic ID 201426, ‚ÄúA group of 14 humanitarian service workers‚Ä¶ ‚Äù). Additionally, another case (topic ID 201428) was removed since the there are no relevance judgements for the patient. The patient notes are derived from the Clinical Decision Support (CDS) tracks in TREC 201448 and 201549, which are ‚Äúmedical case narratives created by expert topic developers that will serve as idealized representations of actual medical records‚Äù48. They typically describe the patient's medical history, current \n 30 \nsymptoms, tests, eventual diagnosis, and treatments. Given a patient note, four medical assessors annotate each candidate clinical trial with three possible labels: (a) ‚Äúwould not refer this patient for this clinical trial‚Äù; (b) ‚Äúwould consider referring this patient to this clinical trial upon further investigation‚Äù; and (c) ‚Äúhighly likely to refer this patient for this clinical trial‚Äù. We consider the label a to be ‚Äúirrelevant‚Äù, the label b to be ‚Äúpotential‚Äù, and the label c to be ‚Äúeligible‚Äù . The candidate clinical trials for annotation are derived from pooling various retrieval methods.  TREC 2021/2022 CT The TREC 2021 and 2022 CT tracks contain 75 and 50 patients, respectively. These patient notes are synthetic patient case descriptions, ‚Äúsuch as what may be included as part of an admission note‚Äù . For each patient, they annotate three eligibility labels for the candidate clinical trial: irrelevant (‚Äúthe patient is not relevant for the trial in any way‚Äù), excluded/ineligible (‚Äúthe patient has the condition that the trial is targeting, but the exclusion criteria make the patient ineligible‚Äù), and eligible (‚Äúthe patient is eligible to enroll in the trial‚Äù). The candidate clinical trials are pooled from the submission systems of TREC participants.  TrialGPT TrialGPT is an architecture for patient-trial matching with large language models. It is composed of three modules: (1) TrialGPT-Retrieval for Ô¨Åltering out most of the irrelevant clinical trials for a given patient; (2) TrialGPT-Matching for criterion-by-criterion analysis of a \n 31 \ngiven patient-trial pair; and (3) TrialGPT-Ranking for Ô¨Åne-grained re-ranking of a list of candidate clinical trials. TrialGPT is LLM-agnostic, meaning it can be plugged into dieerent backbone LLMs. In this study, we mainly use the GPT-4 (model version: 0613) and GPT-3.5 API (model version: 0613) through Microsoft Azure‚Äôs OpenAI services. We used the openai package (version 1.30.5) in Python to call the API. In the manuscript, the default backbone LLM of TrialGPT is GPT-4 unless otherwise speciÔ¨Åed. We set the inference temperature to 0 for deterministic outputs.  TrialGPT-Retrieval Given a free-text patient summary, TrialGPT-Retrieval Ô¨Årst generates a list of keywords using large language models. These keywords are  intended to Ô¨Ålter out most of the irrelevant clinical trials in the initial collection. The prompt for keyword generation is shown in Supplementary Table 2. Denote the list of keywords generated for the patient as [ùë§!,ùë§\",‚Ä¶,ùë§#], where ùêæ is the number of generated keywords and is set up to 32 in the prompt. The LLMs are also instructed to rank the keywords by importance.   We Ô¨Årst conduct hybrid retrieval for each keyword to Ô¨Ånd relevant clinical trials. SpeciÔ¨Åcally, for keyword  ùë§$, we send it to both the traditional BM25 retriever50 for lexical retrieval and the dense MedCPT retriever51 for semantic retrieval. For each considered clinical trial ùë°%, its rank in terms of relevance from BM25 and MedCPT retriever are denoted as Rank(BM25,ùë§$,ùë°%) and Rank(MedCPT,ùë§$,ùë°%), respectively. For example, Rank7BM25,ùë§$,ùë°%8=1 means that clinical trial ùë°% is the most relevant (ranking the Ô¨Årst) for the keyword ùë§$ returned by the BM25 \n 32 \nretriever. We use reciprocal rank fusion52 between the MedCPT and BM25 retrievers for the same keyword, and a decaying weight for combining the scores across dieerent keywords. Finally, the TrialGPT-Retrieval score ùë†% for a clinical trial ùë°% for the given patient can be calculated as: \nùë†%=<< 1ùëñ√ó7Rank7Ret,ùë§$,ùë°%8+ùê∂8#\n$&!'() (1) \nwhere the inner sum is the fusion of dieerent keywords with the decaying factor of 1ùëñ‚ÅÑ, the outer sum is the combination of two retrievers (Ret‚àà{BM25,MedCPT}), and ùê∂ is the constant used in reciprocal rank fusion, which we set to 20.   The clinical trials are then ranked by the TrialGPT-Retrieval score and the highest ranked ones are considered as the candidate clinical trials.  TrialGPT-Matching  Here we denote a patient note as a list of ùëÉ sentences [ùë†!,ùë†\",‚Ä¶,ùë†*], a clinical trial as composed of the background information Œí (containing the title, conditions, interventions, and the brief summary ), a list of ùëÄ inclusion criteria [ùëñ!,ùëñ\",‚Ä¶,ùëñ+], and a list of ùëÅ exclusion criteria [ùëí!,ùëí\",‚Ä¶,ùëí,]. The objective of this module is to output a free-text relevance explanation ùëÖ, a list of relevant sentence IDs ùëÜ, and the eligibility prediction ùê∏ for each criterion based on the input patient note. For an inclusion criterion,  ùê∏‚àà{included,\tnot included,\tnot enough information, not applicable} (2) while for an exclusion criterion, \n 33 \nùê∏‚àà{excluded,\tnot excluded,\tnot enough information, not applicable} (3)  We use dieerent label sets for inclusion and exclusion criteria because the latter are often ambiguous. For example, exclusion criteria of ‚ÄúPregnancy, ‚Äù ‚ÄúThe patient should not be pregnant, ‚Äù and ‚ÄúPregnant patients will be excluded‚Äù serve the same purpose. Traditional entailment labels might not be suitable to distinguish the semantic dieerences, while our eligibility-oriented label sets provide an end-to-end solution.  We make two LLM inference calls for each patient-trial pair: one for all inclusion criteria, and another one for all exclusion criteria. Overall, the prompt includes the task description, the clinical trial background information Œí, and the inclusion ([ùëñ!,ùëñ\",‚Ä¶,ùëñ+]) and exclusion ([ùëí!,ùëí\",‚Ä¶,ùëí,]) criteria. Motivated by chain-of-thought prompting53, we prompt the model to Ô¨Årst generate the relevance explanation as grounding for future predictions of the relevant sentence IDs and the eligibility labels. In addition, we also prompt the model to generate criterion-level predictions in the JSON format, which can be easily parsed for aggregations. The TrialGPT prompts are shown in Supplementary Tables 3 and 4.  TrialGPT-Ranking After getting the criterion-level predictions from TrialGPT-Matching, TrialGPT-Ranking then aggregates such scores to generate a trial-level score that can be used for practical applications such as ranking and excluding clinical trials. SpeciÔ¨Åcally, we denote the eligibility predictions of TrialGPT for the inclusion criteria and exclusion criteria as \n 34 \n[ùê∏(ùëñ!),ùê∏(ùëñ\"),‚Ä¶,ùê∏(ùëñ+)] and  [ùê∏(ùëí!),ùê∏(ùëí\"),‚Ä¶,ùê∏(ùëí,)], respectively. ùëÄ is the number of inclusion criteria and ùëÅ is the number of exclusion criteria in the trial.  Linear aggregation: six scores are simply derived based on the percentages of dieerent eligibility predictions. While more sophisticated scoring methods can be used, we intentionally use these simple and linear aggregation strategies for better probing the capabilities of LLMs.   For a trial‚Äôs inclusion criteria: %\tmet inclusion criteria=|{ùê∏(ùëñ-)=included\t|\tùë•=1\t,2\t,‚Ä¶,ùëÄ}|ùëÄ. (4) \n%\tunmet inclusion criteria=|{ùê∏(ùëñ-)=not\tincluded\t|\tùë•=1\t,2\t,‚Ä¶,ùëÄ}|ùëÄ. (5) \n%\tnot enough information=|{ùê∏(ùëñ-)=not\tenough information\t|\tùë•=1\t,2\t,‚Ä¶,ùëÄ}|ùëÄ. (6) ùëÄ.=ùëÄ‚àí|{ùê∏(ùëñ-)=not applicable\t|\tùë•=1\t,2\t,‚Ä¶,ùëÄ}| (7)  For a trial‚Äôs exclusion criteria: \n%\tmet exclusion criteria=WXùê∏7ùëí/8=excluded |\tùë¶=1\t,2\t,‚Ä¶,ùëÅZWùëÅ. (8) \n%\tunmet exclusion criteria=WXùê∏7ùëí/8=not\texcluded\t|\tùë¶=1\t,2\t,‚Ä¶,ùëÅZWùëÅ. (9) \n%\tnot enough information=WXùê∏7ùëí/8=not enough information\t|\tùë¶=1\t,2\t,‚Ä¶,ùëÅZWùëÅ. (10) ùëÅ.=ùëÅ‚àíWXùê∏7ùëí/8=not applicable\t|\tùë¶=1\t,2\t,‚Ä¶,ùëÅZW (11) \n 35 \nLLM aggregation: In addition, TrialGPT also uses LLMs to aggregate the criterion-level predictions by the prompt shown in Supplementary Table 5. The generated scores are directly used as aggregation scores. SpeciÔ¨Åcally, we consider two main features: general relevance and eligibility: The general relevance (ùëÖ) indicates how relevant a patient is to clinical trial, while the eligibility score (ùëÜ) denotes how eligible the patient is to the clinical trial. We restrict that: 0‚â§ùëÖ‚â§100\t (12) where 0 indicates that the patient is irrelevant to the clinical trial and 100 suggests that the patient is exactly relevant to the clinical trial. We further restrict that: ‚àíùëÖ‚â§ùëÜ‚â§ùëÖ\t (13) based on the assumptions that the absolute value of eligibility cannot be higher than relevance.   Feature combination: We further combine the linear and LLM aggregation features, generating the improved scores for ranking and excluding: combination=%\tmet inclusion criteria‚àíùïÄ(%\tunmet inclusion criteria>0)‚àíùïÄ(%\tmet exclusion criteria>0)+%\tLLM general relevance+%\tLLM eligibility score\t (14) \n where ùïÄ is an indicator function: ùïÄ(condition)=a1,\t\t\tif condition is True0,\t\t\tif condition is False(15)  Criterion-level expert evaluation \n 36 \nThree annotators (F.C. ,  C.G. ,  a n d  C. F.) are provided with 1,015 pairs of patient-criterion predictions by TrialGPT sampled from our patient cohorts. For each patient-criterion pair, the annotators Ô¨Årst evaluate the correctness of TrialGPT explanation by ‚ÄúCorrect‚Äù , ‚ÄúPartially Correct‚Äù or ‚ÄúIncorrect‚Äù . If at least two annotators provide the same label, it will be used as the consensus. If annotators choose three dieerent scores for a patient-criterion pair, it will be labeled as ‚ÄúPartially Correct‚Äù . The annotators then annotate the relevant sentence locations for the criterion, and the consensus is the union of all annotator-provided sentences. Finally, the annotator provides the annotation of eligibility, with the same candidate label set as that of TrialGPT. Similarly, if at least two annotators assign the same eligibility label, it will be used as the consensus. If all three annotators assign dieerent eligibility labels, a second round of discussion will be scheduled until there is a consensus label.   Compared methods The core of TrialGPT lies in the prediction of patient-criterion eligibility. These predictions are then aggregated for the ranking and excluding tasks. As such, we compare TrialGPT to a variety of pre-trained language models that can predict patient-criterion eligibility. Since there are no existing patient-criterion eligibility annotations for training a supervised model, we consider transfer learning from the biomedical NLI datasets. SpeciÔ¨Åcally, we use three categories of baselines: dual-encoder, cross-encoder, and encoder-decoder.  \n 37 \nDual-encoder models are also known as bi-encoder, where the patient note and the criterion are separately encoded by pre-trained transformers, and the eligibility is modeled as the similarity of the encoding vectors: score(ranking)=‚àëùëâ(patient)0\tùëâ(ùëñ-)+-&! ùëÄ ‚àí‚àëùëâ(patient)0\tùëâ7ùëí/8,/&! ùëÅ \t (16) \nscore(excluding)=‚àëùëâ(patient)0\tùëâ7ùëí/8,/&! ùëÅ \t (17) and: ùëâ(patient)=Enc([ùë†!,ùë†\",‚Ä¶,ùë†*])‚àà‚Ñù1 (18) ùëâ(ùëñ-)=Enc(ùëñ-)‚àà‚Ñù1 (19) ùëâ7ùëí/8=Enc7ùëí/8‚àà‚Ñù1 (20) where Enc denotes the pre-trained transformer encoder, and ‚Ñé is the dimension of the vector representations.  Cross-encoder models take both the patient note and the criterion as input, which enables cross-attention computations between the tokens in both texts. The eligibility prediction is modeled as a 3-way classiÔ¨Åcation task based on the special [CLS] embedding of BERT54. We use label space mapping functions ùëì that maps an NLI label to an eligibility label: ùê∏(ùëñ-)=ùëìinc7CrossEnc([ùë†!,ùë†\",‚Ä¶,ùë†*],\tùëñ-)8 (21) ùê∏7ùëí/8=ùëìexcgCrossEnc7[ùë†!,ùë†\",‚Ä¶,ùë†*],\tùëí/8h (22) where \nùëìinc(ùëô)=j included, if ùëô=entailmentnot included, if ùëô=contradictionno relevant information, if  ùëô=neutral(23) \n 38 \nand \nùëìexc(ùëô)=j excluded, if ùëô=entailmentnot excluded, if ùëô=contradictionno relevant information, if  ùëô=neutral(24) \n Then we compute the combination scores based on the criterion-level prediction, similar to the feature combination strategy used by TrialGPT: combination(ranking)=%\tmet inclusion criteria‚àí%\tunmet inclusion criteria‚àí%\tmet exclusion criteria+%\tunmet exclusion criteria\t (25) combination(excluding)=ùïÄ(%\tunmet inclusion criteria>0)+ùïÄ(%\tmet exclusion criteria>0)‚àí%\tmet inclusion criteria(26)  Encoder-decoder models also take both the patient note and the criterion as input to the encoder, but instead of outputting a classiÔ¨Åcation prediction, they generate the predicted NLI labels, e.g., ‚Äúentailment‚Äù , ‚Äúcontradiction‚Äù , or ‚Äúneutral‚Äù . These NLI labels are then mapped to eligibility labels that will be aggregated into the combination scores by the same methods described above for cross-encoder models.   Evaluation settings Following the SIGIR and TREC evaluation guidelines, we deÔ¨Åne the relevance score ùëü$ of a clinical trial ùëê$ given a speciÔ¨Åc patient as: \nùëü$=m0, if ùê∏(ùëê$)‚àà{irrelevant,\tunlabeled}1, if ùê∏(ùëê$)‚àà{ineligible,\tpotential}2, if  ùê∏(ùëê$)=eligible (27) \n \n 39 \nWe report the recall at dieerent depths ùëò for the Ô¨Årst stage retrieval. SpeciÔ¨Åcally, we denote the retrieved list of clinical trials as [ùëê!,ùëê\",‚Ä¶,ùëê2], where ùëò is the number of considered candidates. Their relevance scores are denoted as [ùëü!,ùëü\",‚Ä¶,ùëü2]. Recall@k is a measurement of retrieval quality, which is computed by: Recall@ùëò=‚àëùëü-2-&!R (28)  where R is the sum of all clinical trial relevance score in the considered collection.  We report NDCG@10 and P@10 for ranking candidate clinical trials, and AUROC for excluding ineligible clinical trials. Patient-trial pairs with unlabeled eligibility are not included for the computation of metrics.  For computing NDCG@10 and P@10, we denote the ranked list of clinical trials as [ùëê!,ùëê\",‚Ä¶,ùëê2], where ùëò is the number of considered candidates. Their relevance scores are denoted as [ùëü!,ùëü\",‚Ä¶,ùëü2]. NDCG@k is a measurement ranking quality, which is computed by: NDCG@ùëò=DCG@ùëòIDCG@l (29) where \nDCG@ùëò=<ùëü-log\"(ùëñ+1)2\n-&! (30) \nand \n 40 \nIDCG@ùëò=<ùëü.-log\"(ùëñ+1)2\n-&! (31) \nwhere [ùëü‚Ä≤!,ùëü‚Ä≤\",‚Ä¶,ùëü‚Ä≤0] denotes the relevance of an ideal ranking.  P@10 is another metric for ranking quality, computed by: P@ùëò=‚àëùëü-2-&!max(‚Ñõ)√óùëò (32) where ‚Ñõ denotes the set of relevance labels.  We draw the receiver operating characteristic (ROC) curve and compute the area under the ROC curve (AUROC) values using the sklearn package in Python.  Pilot user study The pilot user study mimics a common task at the cancer center, where the trial organizer screens patient referral requests against the candidate clinical trials. The patient note varies from a short paragraph in the email to a long document sent via fax. We consider six clinical trials (NCT04432597, NCT05012098, NCT04287868, NCT04847466, NCT04719988, and NCT04894370) as the candidates, where the Ô¨Årst four are managed by one physician co-author (C.F.) and the other two are highly related. The physician also created six clinical vignettes (3 short and 3 long) based on real patient encounters, with the personally identiÔ¨Åable information modiÔ¨Åed. The task objective is to screen whether the patient is deÔ¨Ånitely ineligible (‚ÄúNo‚Äù) or potentially eligible and should be included for further investigation (‚ÄúMaybe‚Äù). Two MD annotators (Q.J. and E.X.) recorded the time needed to \n 41 \nmake the decision after familiarizing themselves with the patient note. Each annotator screens half of the patient-trial pairs with TrialGPT and another half without. We also ensure that each patient-trial pair is screened by one annotator with TrialGPT and another without. The evaluation setting is visualized in Figure 5a.  \nData availability The TREC Clinical Trial 2021 and 2022 cohorts can be downloaded from http://www.trec-cds.org/2021.html and http://www.trec-cds.org/2022.html, respectively. The SIGIR cohort is publicly available at https://data.csiro.au/collection/csiro:17152. The clinical vignettes for the user study are available in the Supplementary Materials. The criterion-level annotations generated in this study have been deposited in the Hugging Face database under accession code: https://huggingface.co/datasets/ncbi/TrialGPT-Criterion-Annotations. Preprocessed data Ô¨Åles generated in this study have been deposited in the GitHub database under accession code: https://github.com/ncbi-nlp/TrialGPT55. Source data are provided with this paper.  \nCode availability TrialGPT is publicly available at https://github.com/ncbi-nlp/TrialGPT55.   \n 42 \nReferences 1. Kadam, R.A., Borde, S.U., Madas, S.A., Salvi, S.S. & Limaye, S.S. Challenges in recruitment and retention of clinical trial subjects. Perspect Clin Res 7, 137-143 (2016). 2. Bennette, C.S., et al. Predicting low accrual in the National Cancer Institute‚Äôs Cooperative Group clinical trials. JNCI: Journal of the National Cancer Institute 108(2016). 3. Haddad, T.C., et al. Impact of a cognitive computing clinical trial matching system in an ambulatory oncology practice.  (American Society of Clinical Oncology, 2018). 4. Woo, M. An AI boost for clinical trials. Nature 573, S100-S102 (2019). 5. Hutson, M. How AI is being used to accelerate clinical trials. Nature 627, S2-S5 (2024). 6. Stubbs, A., Filannino, M., Soysal, E., Henry, S. & Uzuner, O. Cohort selection for clinical trials: n2c2 2018 shared task track 1. J Am Med Inform Assoc 26, 1163-1171 (2019). 7. Yuan, C., et al. Criteria2Query: a natural language interface to clinical databases for cohort deÔ¨Ånition. J Am Med Inform Assoc 26, 294-305 (2019). 8. Voorhees, E.M. & Hersh, W.R. Overview of the TREC 2012 Medical Records Track. in Proceedings of the Twenty-First Text REtrieval Conference (TREC 2012) (2012). 9. Roberts, K., Demner-Fushman, D., Voorhees, E.M., Bedrick, S. & Hersh, W.R. Overview of the TREC 2022 Clinical Trials Track. in Proceedings of the Thirty-First Text REtrieval Conference (TREC 2022) (2022). \n 43 \n10. Koopman, B. & Zuccon, G. A test collection for matching patients to clinical trials. in Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval 669-672 (2016). 11. Pradeep, R., Li, Y ., Wang, Y . & Lin, J. Neural query synthesis and domain-speciÔ¨Åc ranking templates for multi-stage clinical trial matching. in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval 2325-2330 (2022). 12. Jin, Q., Tan, C., Zhao, Z., Yuan, Z. & Huang, S. Alibaba DAMO Academy at TREC Clinical Trials 2021: Exploring Embedding-based First-stage Retrieval with TrialMatcher. in Proceedings of the Thirtieth Text REtrieval Conference (TREC 2021) (2021). 13. Roberts, K., Demner-Fushman, D., Voorhees, E.M., Bedrick, S. & Hersh, W.R. Overview of the TREC 2021 Clinical Trials Track. in Proceedings of the Thirtieth Text REtrieval Conference (TREC 2021) (2021). 14. Segura-Bedmar, I. & Raez, P . Cohort selection for clinical trials using deep learning models. J Am Med Inform Assoc 26, 1181-1188 (2019). 15. Zhang, X., Xiao, C., Glass, L.M. & Sun, J. DeepEnroll: patient-trial matching with deep embedding and entailment prediction. in Proceedings of the web conference 2020 1029-1037 (2020). 16. Gao, J., Xiao, C., Glass, L.M. & Sun, J. COMPOSE: Cross-modal pseudo-siamese network for patient trial matching. in Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining 803-812 (2020). 17. OpenAI. GPT-4 Technical Report. ArXiv abs/2303.08774(2023). \n 44 \n18. Vaswani, A., et al. Attention is all you need. Advances in neural information processing systems 30(2017). 19. Anil, R., et al. PaLM 2 Technical Report. arXiv preprint arXiv:2305.10403 (2023). 20. Tian, S., et al. Opportunities and challenges for ChatGPT and large language models in biomedicine and health. BrieÔ¨Ångs in Bioinformatics 25, bbad493 (2024). 21. Jin, Q., et al. Biomedical question answering: A survey of approaches and challenges. ACM Computing Surveys (CSUR) 55, 1-36 (2022). 22. Singhal, K., et al. Large language models encode clinical knowledge. Nature 620, 172-180 (2023). 23. Li√©vin, V ., Hother, C.E., Motzfeldt, A.G. & Winther, O. Can large language models reason about medical questions? Patterns (2023). 24. Nori, H., King, N., McKinney, S.M., Carignan, D. & Horvitz, E. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375 (2023). 25. Nori, H., et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452 (2023). 26. Jin, Q., Yang, Y ., Chen, Q. & Lu, Z. GeneGPT: augmenting large language models with domain tools for improved access to biomedical information. Bioinformatics 40(2024). 27. Wang, Z., Xiao, C. & Sun, J. AutoTrial: Prompting Language Models for Clinical Trial Design. arXiv preprint arXiv:2305.11366 (2023). \n 45 \n28. Jin, Q., Tan, C., Chen, M., Liu, X. & Huang, S. Predicting Clinical Trial Results by Implicit Evidence Integration. in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) 1461-1477 (2020). 29. Peikos, G., Symeonidis, S., Kasela, P . & Pasi, G. Utilizing ChatGPT to Enhance Clinical Trial Enrollment. arXiv preprint arXiv:2306.02077 (2023). 30. Datta, S., et al. AutoCriteria: a generalizable clinical trial eligibility criteria extraction system powered by large language models. J Am Med Inform Assoc 31, 375-385 (2024). 31. Yuan, J., Tang, R., Jiang, X. & Hu, X. Large language models for healthcare data augmentation: An example on patient-trial matching. arXiv preprint arXiv:2303.16756 (2023). 32. Zhuang, S., Koopman, B. & Zuccon, G. Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval with Neural Rankers and Large Language Models. arXiv preprint arXiv:2401.01566 (2024). 33. Nievas, M., Basu, A., Wang, Y . & Singh, H. Distilling large language models for matching patients to clinical trials. Journal of the American Medical Informatics Association, ocae073 (2024). 34. Wong, C., et al. Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. arXiv preprint arXiv:2308.02180 (2023). 35. MacCartney, B. & Manning, C.D. Modeling semantic containment and exclusion in natural language inference. in Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008) 521-528 (2008). \n 46 \n36. Yasunaga, M., Leskovec, J. & Liang, P . LinkBERT: Pretraining Language Models with Document Links. in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 8003-8016 (2022). 37. Romanov, A. & Shivade, C. Lessons from Natural Language Inference in the Clinical Domain. in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing 1586-1596 (2018). 38. Phan, L.N., et al. SciÔ¨Åve: a text-to-text transformer model for biomedical literature. arXiv preprint arXiv:2106.03598 (2021). 39. Lee, J., et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, 1234-1240 (2020). 40. Deka, P ., Jurek-Loughrey, A. & P , D. Evidence extraction to validate medical claims in fake news detection. in International Conference on Health Information Science 3-15 (Springer, 2022). 41. Williams, A., Nangia, N. & Bowman, S. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) 1112-1122 (2018). 42. Bowman, S., Angeli, G., Potts, C. & Manning, C.D. A large annotated corpus for learning natural language inference. in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing 632-642 (2015). \n 47 \n43. Sadat, M. & Caragea, C. SciNLI: A Corpus for Natural Language Inference on ScientiÔ¨Åc Text. in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 7399-7409 (2022). 44. Khot, T., Sabharwal, A. & Clark, P . Scitail: A textual entailment dataset from science question answering. in Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, Vol. 32 (2018). 45. Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I. & Specia, L. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation. in Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) 1-14 (2017). 46. Gu, Y ., et al. Domain-speciÔ¨Åc language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH) 3, 1-23 (2021). 47. Liu, F ., Shareghi, E., Meng, Z., Basaldella, M. & Collier, N. Self-Alignment Pretraining for Biomedical Entity Representations. in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 4228-4238 (2021). 48. Roberts, K., Demner-Fushman, D., Voorhees, E.M. & Hersh, W.R. Overview of the TREC 2014 Clinical Decision Support Track. in Proceedings of the Twenty-Third Text REtrieval Conference (TREC 2014) (2014). \n 48 \n49. Roberts, K., Simpson, M.S., Voorhees, E.M. & Hersh, W.R. Overview of the TREC 2015 Clinical Decision Support Track. in Proceedings of the Twenty-Fourth Text REtrieval Conference (TREC 2015) (2015). 50. Robertson, S. & Zaragoza, H. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends¬Æ in Information Retrieval 3, 333-389 (2009). 51. Jin, Q., et al. MedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval. Bioinformatics 39, btad651 (2023). 52. Cormack, G.V ., Clarke, C.L. & Buettcher, S. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. in Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval 758-759 (2009). 53. Wei, J., et al. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022). 54. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) 4171-4186 (2019). 55. Jin, Q. Matching Patients to Clinical Trials with Large Language Models. https://github.com/ncbi-nlp/TrialGPT. 10.5281/zenodo.13270780.  (2024).  \n 49 \nAcknowledgments This research was supported by the NIH Intramural Research Program, National Library of Medicine. We would like to thank the organizers of the SIGIR and TREC Clinical Trial tasks for making their datasets publicly available.   Author contributions Q.J., Z.W., C.F ., J.S., and Z.L. designed the study. Q.J. conducted the data collection, model construction, model evaluation, and manuscript drafting. Z.W. and Y . Y . carried out the data collection and analysis. Q.J., C.F ., F .C., C.G., D.B., and E.X. contributed to the data annotation. Z.L. supervised the study. All authors contributed to writing the manuscript and approved the submitted version.  Competing interests None declared.  ",
  "topic": "Clinical trial",
  "concepts": [
    {
      "name": "Clinical trial",
      "score": 0.6735788583755493
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6520034074783325
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6068252325057983
    },
    {
      "name": "Workflow",
      "score": 0.600888192653656
    },
    {
      "name": "Matching (statistics)",
      "score": 0.5893402099609375
    },
    {
      "name": "Referral",
      "score": 0.5591459274291992
    },
    {
      "name": "Computer science",
      "score": 0.41599664092063904
    },
    {
      "name": "Medicine",
      "score": 0.37760522961616516
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27357515692710876
    },
    {
      "name": "Family medicine",
      "score": 0.2260843813419342
    },
    {
      "name": "Pathology",
      "score": 0.14305004477500916
    },
    {
      "name": "Geography",
      "score": 0.08913600444793701
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ]
}