{
  "title": "ProkBERT family: genomic language models for microbiome applications",
  "url": "https://openalex.org/W4390790220",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2309536835",
      "name": "Balázs Ligeti",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093265715",
      "name": "István Szepesi-Nagy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5108312247",
      "name": "Babett Bodnár",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4309593450",
      "name": "Noémi Ligeti-Nagy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1992099664",
      "name": "János Juhász",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1019830208",
    "https://openalex.org/W3039032764",
    "https://openalex.org/W2004548026",
    "https://openalex.org/W4210265219",
    "https://openalex.org/W4289593660",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4309305993",
    "https://openalex.org/W3203343200",
    "https://openalex.org/W3081166989",
    "https://openalex.org/W3214710775",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2158023121",
    "https://openalex.org/W4316339774",
    "https://openalex.org/W2083122216",
    "https://openalex.org/W2119123521",
    "https://openalex.org/W2133253129",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2034200083",
    "https://openalex.org/W4245668478",
    "https://openalex.org/W2889230731",
    "https://openalex.org/W2170747616",
    "https://openalex.org/W3127656915",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W2903705053",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2799200068",
    "https://openalex.org/W4280524269",
    "https://openalex.org/W4308736754",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2952239877",
    "https://openalex.org/W3033129278",
    "https://openalex.org/W2095724929",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W3109735703",
    "https://openalex.org/W3025306404",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W2754289562",
    "https://openalex.org/W2314649741",
    "https://openalex.org/W2160053034",
    "https://openalex.org/W2760000637",
    "https://openalex.org/W2311607323",
    "https://openalex.org/W2800015127",
    "https://openalex.org/W2173732482",
    "https://openalex.org/W2443415468",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W1975540898",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W2889772740",
    "https://openalex.org/W3003110834",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2900007277",
    "https://openalex.org/W4220685469",
    "https://openalex.org/W4318756379",
    "https://openalex.org/W2096093282",
    "https://openalex.org/W2526784604",
    "https://openalex.org/W6767997687",
    "https://openalex.org/W6735236233",
    "https://openalex.org/W3132229552",
    "https://openalex.org/W3127836515",
    "https://openalex.org/W2470052926",
    "https://openalex.org/W2006492816",
    "https://openalex.org/W2963739921",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2126569085",
    "https://openalex.org/W4292800918",
    "https://openalex.org/W2903162518",
    "https://openalex.org/W4367301324",
    "https://openalex.org/W4386253786",
    "https://openalex.org/W4318685265",
    "https://openalex.org/W2022366078",
    "https://openalex.org/W2909409778",
    "https://openalex.org/W4315641887",
    "https://openalex.org/W4289712796",
    "https://openalex.org/W2735006823",
    "https://openalex.org/W6853888595",
    "https://openalex.org/W4282936970",
    "https://openalex.org/W4213099919",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4394808305",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4226085666",
    "https://openalex.org/W4382490702",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2023816042"
  ],
  "abstract": "Background In the evolving landscape of microbiology and microbiome analysis, the integration of machine learning is crucial for understanding complex microbial interactions, and predicting and recognizing novel functionalities within extensive datasets. However, the effectiveness of these methods in microbiology faces challenges due to the complex and heterogeneous nature of microbial data, further complicated by low signal-to-noise ratios, context-dependency, and a significant shortage of appropriately labeled datasets. This study introduces the ProkBERT model family, a collection of large language models, designed for genomic tasks. It provides a generalizable sequence representation for nucleotide sequences, learned from unlabeled genome data. This approach helps overcome the above-mentioned limitations in the field, thereby improving our understanding of microbial ecosystems and their impact on health and disease. Methods ProkBERT models are based on transfer learning and self-supervised methodologies, enabling them to use the abundant yet complex microbial data effectively. The introduction of the novel Local Context-Aware (LCA) tokenization technique marks a significant advancement, allowing ProkBERT to overcome the contextual limitations of traditional transformer models. This methodology not only retains rich local context but also demonstrates remarkable adaptability across various bioinformatics tasks. Results In practical applications such as promoter prediction and phage identification, the ProkBERT models show superior performance. For promoter prediction tasks, the top-performing model achieved a Matthews Correlation Coefficient (MCC) of 0.74 for E. coli and 0.62 in mixed-species contexts. In phage identification, ProkBERT models consistently outperformed established tools like VirSorter2 and DeepVirFinder, achieving an MCC of 0.85. These results underscore the models' exceptional accuracy and generalizability in both supervised and unsupervised tasks. Conclusions The ProkBERT model family is a compact yet powerful tool in the field of microbiology and bioinformatics. Its capacity for rapid, accurate analyses and its adaptability across a spectrum of tasks marks a significant advancement in machine learning applications in microbiology. The models are available on GitHub ( https://github.com/nbrg-ppcu/prokbert ) and HuggingFace ( https://huggingface.co/nerualbioinfo ) providing an accessible tool for the community.",
  "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/two.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nOPEN ACCESS\nEDITED BY\nDomenica D’Elia,\nNational Research Council (CNR), Italy\nREVIEWED BY\nJan Zrimec,\nNational Institute of Biology (NIB), Slovenia\nGianvito Pio,\nUniversity of Bari Aldo Moro, Italy\n*CORRESPONDENCE\nBalázs Ligeti\nligeti.balazs@itk.ppke.hu\nRECEIVED /three.tnum/one.tnum October /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /one.tnum/one.tnum December /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/two.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nLigeti B, Szepesi-Nagy I, Bodnár B,\nLigeti-Nagy N and Juhász J (/two.tnum/zero.tnum/two.tnum/four.tnum) ProkBERT\nfamily: genomic language models for\nmicrobiome applications.\nFront. Microbiol./one.tnum/four.tnum:/one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Ligeti, Szepesi-Nagy, Bodnár,\nLigeti-Nagy and Juhász. This is an open-access\narticle distributed under the terms of the\nCreative Commons Attribution License (CC BY) .\nThe use, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in this\njournal is cited, in accordance with accepted\nacademic practice. No use, distribution or\nreproduction is permitted which does not\ncomply with these terms.\nProkBERT family: genomic\nlanguage models for microbiome\napplications\nBalázs Ligeti /one.tnum*, István Szepesi-Nagy /one.tnum, Babett Bodnár /one.tnum,\nNoémi Ligeti-Nagy/two.tnumand János Juhász /one.tnum,/three.tnum\n/one.tnumFaculty of Information Technology and Bionics, Pázmány Péter Cath olic University, Budapest, Hungary,\n/two.tnumLanguage Technology Research Group, HUN-REN Hungarian Research Cen tre for Linguistics,\nBudapest, Hungary, /three.tnumInstitute of Medical Microbiology, Semmelweis University, Buda pest, Hungary\nBackground: In the evolving landscape of microbiology and microbiome analysis,\nthe integration of machine learning is crucial for understandin g complex microbial\ninteractions, and predicting and recognizing novel functionali ties within extensive\ndatasets. However, the eﬀectiveness of these methods in microb iology faces\nchallenges due to the complex and heterogeneous nature of micro bial data,\nfurther complicated by low signal-to-noise ratios, context-d ependency, and a\nsigniﬁcant shortage of appropriately labeled datasets. This study introduces the\nProkBERT model family, a collection of large language models, designed for\ngenomic tasks. It provides a generalizable sequence represen tation for nucleotide\nsequences, learned from unlabeled genome data. This approach hel ps overcome\nthe above-mentioned limitations in the ﬁeld, thereby impro ving our understanding\nof microbial ecosystems and their impact on health and disease.\nMethods: ProkBERT models are based on transfer learning and self-super vised\nmethodologies, enabling them to use the abundant yet complex m icrobial data\neﬀectively. The introduction of the novel Local Context-Aware (L CA) tokenization\ntechnique marks a signiﬁcant advancement, allowing ProkBERT to o vercome\nthe contextual limitations of traditional transformer mode ls. This methodology\nnot only retains rich local context but also demonstrates remark able adaptability\nacross various bioinformatics tasks.\nResults: In practical applications such as promoter prediction and phage\nidentiﬁcation, the ProkBERT models show superior performance . For promoter\nprediction tasks, the top-performing model achieved a Matthews Correlation\nCoeﬃcient (MCC) of /zero.tnum./seven.tnum/four.tnum forE. coliand /zero.tnum./six.tnum/two.tnum in mixed-species contexts. In phage\nidentiﬁcation, ProkBERT models consistently outperformed e stablished tools like\nVirSorter/two.tnum and DeepVirFinder, achieving an MCC of /zero.tnum./eight.tnum/five.tnum. Theseresults underscore\nthe models’ exceptional accuracy and generalizability in both su pervised and\nunsupervised tasks.\nConclusions: The ProkBERT model family is a compact yet powerful tool in the\nﬁeld of microbiology and bioinformatics. Its capacity for rapid, accurate analyses\nand its adaptability across a spectrum of tasks marks a signiﬁcan t advancement\nin machine learning applications in microbiology. The models ar e available\non GitHub (\nhttps://github.com/nbrg-ppcu/prokbert) and HuggingFace ( https://\nhuggingface.co/nerualbioinfo) providing an accessible tool for the community.\nKEYWORDS\ngenomic language models, language models, promoter, phage, BERT, transformer\nmodels, LCA tokenization, machine learning in microbiology\nFrontiers in Microbiology /zero.tnum/one.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\n/one.tnum Introduction\nNumerous tasks in bioinformatics involve classifying or\nlabeling sequence data such as predicting genes ( Lukashin and\nBorodovsky, 1998 ; Delcher et al., 1999 ; Sommer and Salzberg,\n2021), annotating sequence features ( Aziz et al., 2008 ; Seemann,\n2014; Tatusova et al., 2016 ; Meyer et al., 2019 ), etc. A signiﬁcant\nchallenge in this ﬁeld is deriving eﬃcient vector representations\nfrom these sequences (\nZhang et al., 2023 ). Classiﬁcation tasks\nrelated to sequences—like classifying assembled contigs into MAGs\n(metagenome-assembled-genomes) or analyzing AMR-associated\ngenes—are often addressed by initially categorizing the data\ninto bins or using simple composition-based representations,\nsuch as k-mer frequency distributions. A common method\ninvolves converting sequences into a basic presence-absence vector,\nindicating whether a particular genome contains speciﬁc sequence\nfeatures like mutations, motifs, or other patterns. However, a\ndrawback of this method is that proximity in this representation\nspace doesn’t always imply semantic similarity. Another prevalent\nrepresentation uses hidden Markov models (\nDurbin et al., 1998 ),\nwhere the model parameters encapsulate the essential properties of\nthe sequences. Yet, integrating such models with machine learning\nalgorithms like support vector machines or random forests can be\ncomplex. Despite this, hidden Markov models have demonstrated\ntheir eﬀectiveness in classiﬁcation tasks and provide highest quality\nannotations (\nZdobnov and Apweiler, 2001 ; Cantalapiedra et al.,\n2021).\nNeural network-based representations have distinct\nadvantages, primarily their compatibility with a wide range\nof machine-learning tools, including autoML and statistical\nframeworks. Past research has highlighted the eﬀectiveness of\nneural network representations for sequences, with a variety of\nclassiﬁcation tasks addressed using networks such as CNNs and\nRNNs (\nMin et al., 2017 ). These networks have been employed\nin areas like motif discovery, gene-expression prediction ( Kelley\net al., 2018 ) splicing site recognition ( Ji et al., 2021 ), and promoter\nidentiﬁcation, as detailed in several comprehensive reviews ( Min\net al., 2017 ; Sapoval et al., 2022 ; Zhang et al., 2023 ). However,\nconvolutional neural networks face challenges, like the need\nfor extensive labeled sequence data. They are also task-speciﬁc,\nlimiting their applicability to other scenarios outside their training\nfocus. A signiﬁcant bottleneck in integrating neural networks\ninto bioinformatics has been the scarcity of adequate labeled\ndata. Recent advancements in machine learning, inspired by\nbreakthroughs in natural language processing, image analysis\n(\nHan et al., 2022 ), and protein structure prediction ( Alipanahi\net al., 2015 ; Jumper et al., 2021 ), have introduced new paradigms.\nTransformer-based architectures, especially large language models\n(\nDevlin et al., 2019 ; Brown et al., 2020a ; Raﬀel et al., 2020 ), oﬀer\nversatile representations—often termed “reusable” or “fundamental\nmodels.” Among the recent training approaches is the ﬁne-tuning\nparadigm, which divides the training process into two phases:\npretraining and ﬁne-tuning. Pretraining demands vast amounts of\nself-labeled data, while ﬁne-tuning can, in some instances, operate\nwith minimal, or even no examples.\nIn bioinformatics, there exists a paradoxical challenge. On one\nhand, there’s an abundance of sequence data available, especially in\npublic repositories like the SRA (sequence read arhive). The volume\nof this data is expanding exponentially, and as sequencing and other\ndata-producing technologies become more aﬀordable, this growth\ntrend is likely to persist. These data repositories are akin to hidden\ntreasures. Yet, they remain under-analyzed and underprocessed.\nResearchers often focus primarily on speciﬁc mutations, neglecting\nother valuable aspects of the data. Conversely, while there’s an\nabundance of raw sequence data, there’s a scarcity of labeled data.\nThe accompanying metadata is frequently limited, and given the\nhigh cost of experiments, only a handful of samples, typically\nranging from 3–15, are available within a speciﬁc group or stratum.\nIt’s also worth noting that labeling criteria can diﬀer signiﬁcantly\nacross projects.\nRecognizing these challenges, there is a compelling need\nfor innovative methods that can harness the vast repositories\nof raw sequence data and navigate the complexity of labeling\ninconsistencies. It is in this context that our research contributes\na novel solution. The development and application of our\ngenomic language model family aims to address the mentioned\nissues, providing a robust, adaptable, and eﬃcient tool for\nsequence classiﬁcation.\nWhile the concept of pretrained models isn’t new, several\nhave emerged recently, such as DNABERT (\nJi et al., 2021 ; Zhou\net al., 2023 ), Nucleotide Transformer ( Dalla-Torre et al., 2023 ),\nand LookingGlass ( Hoarfrost et al., 2022 ). However, a common\nlimitation among these methods is their primary focus on human\nsequences or their restricted context size.\nIn the pretraining phase, the objective is to derive a general\nrepresentation that captures the semantic relationships between\nobjects, which in this context means obtaining a nuanced\nrepresentation of sequence data. Typically, achieving this requires\nbillions of samples, yet the volume of available sequence data far\nsurpasses this number. We trained our genomic language models\non an extensive corpus of available sequence data, encompassing\nbacteria, archaea, viruses, and fungi. Subsequently, we ﬁne-tuned\nour models to tackle speciﬁc classiﬁcation tasks, including the\nrecognition of promoters and phages.\nThe ProkBERT family encompasses a series of models tailored\nto meet the intricate demands of microbial sequence classiﬁcation,\nanalysis, and visualization. The versatility of the ProkBERT models\nis manifested through their diverse applications:\n1. Zero-shot learning: This approach allows for clustering of\nsequences by leveraging the embeddings directly produced by\nthe model, eliminating the necessity for explicit ﬁne-tuning.\n2. Sequence classiﬁcation: ProkBERT models can be seamlessly\nﬁne-tuned, whether for token-speciﬁc or comprehensive\nsequence-based classiﬁcation tasks.\nWith these capabilities, the ProkBERT family aims to bridge\nthe current gaps in the ﬁeld, oﬀering a robust toolset for diverse\nbioinformatics challenges.\n/two.tnum Materials and methods\nIn this study, we used the transfer-learning paradigm for\nsequence classiﬁcation based on transformer-based architectures.\nFrontiers in Microbiology /zero.tnum/two.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nThe ﬁrst phase involves pretraining on a large amount of sequence\ndata, allowing the model to learn general sequence patterns.\nOnce this foundation is established, we move to the ﬁne-tuning\nphase where the model is adapted to speciﬁc tasks or datasets.\nThe following sections provide a step-by-step description of our\nmethods, from preparing raw sequence data to the speciﬁcs\nof both pretraining and ﬁne-tuning.\nFigure 1 illustrates the\ntraining process.\nIn the development of the ProkBERT family, the initial step\ninvolves pretraining the model on a vast corpus of data. During this\npretraining phase, the model aims to tackle the Masked Language\nModeling task. In this task, speciﬁc portions of the sequence are\nmasked, and the model’s objective is to predict these masked\nsections, optimizing the likelihood of the missing parts using cross-\nentropy as the loss function. The model typically receives input\nin the form of a vectorized representation of the sequence. A\nnotable constraint of standard transformers is their limited input\nsize. Though various solutions have been suggested to address this\nlimitation, the maximum token size is typically restricted up to 4kb,\nsigniﬁcantly smaller than the average bacterial genome, but much\nlarger than an average gene.\nFine-tuning nucleotide sequences is a technique used to adapt\npre-trained models to specialized tasks or speciﬁc datasets. The\nﬁrst step involves segmenting raw sequences into chunks, usually\nranging from 0.1–1kb in size, to optimize the model’s learning\ncapability (\nPan and Yang, 2009 ). Using weights from a pre-trained\nmodel, the system beneﬁts from the knowledge obtained from\ncomprehensive training on extensive datasets (\nVaswani et al., 2017 ;\nDevlin et al., 2019 ). This initialization helps in quicker convergence\nand improved performance. After this initialization, the model\nundergoes training on the desired dataset, adjusting to its speciﬁc\npatterns and details. The outcome of this procedure allows the\nmodel to produce labeled sequences or tokens, which can be used\nfor various annotation or prediction purposes (\nBrown et al., 2020b ).\n/two.tnum./one.tnum Sequence data\n/two.tnum./one.tnum./one.tnum Sequence segmentation and tokenization\nThe ﬁrst step is processing the sequence data. While there\nare many parallels between sequence data processing and natural\nlanguage processing, drawing direct analogies can be challenging.\nFor instance, determining what constitutes a ’sentence’ in the\nrealm of nucleotide and protein sequences doesn’t have a direct\ncounterpart in natural language. Additionally, the input size for\nneural networks is inherently limited.\nFigure 2 illustrates the\nstrategy employed to vectorize the sequences.\nInitially, the input sequence is segmented into smaller chunks.\nWe employed two approaches for this:\n1. Contiguous sampling, where contigs are divided into multiple\nnon-overlapping segments; and\n2. Random sampling, which involves fragmenting the input\nsequence into various segments at random.\nFollowing segmentation, the next phase is encoding the\nsequence into a simpler vector format. The primary question\nrevolves around deﬁning the fundamental building block for a\ntoken. Various solutions have been suggested, the most widely\nstrategy is applying one-hot-encoding (\nSapoval et al., 2022 ), but\nDNA-BERT ( Ji et al., 2021 ) applies the maximal overlapping k-\nmer strategy, meanwhile others relies on nucleotide level mapping\n(\nDalla-Torre et al., 2023 ).\nThis phase is termed tokenization. We introduce a method\ntermed Local Context-Awaretokenization (LCA), where individual\nelements consist of overlapping k-mers. Two principal parameters\ndominate this approach: k-mer size and shift. For k = 1, the\ntokenization resorts to a basic character-based approach, with a\ntypical example illustrated in\nFigure 2. Employing overlapping k-\nmers can lead to enhanced classiﬁcation performance. A greater\nshift value allows the model to use a broader context while reducing\ncomputational demands, while having the information-rich local\ncontext as well.\nAs an example for LCA tokenization, let’s take the\nsequence {AAGTCCAGGATCAAGATT} and a k-mer size\nof 6, and shift=1 as LCA parameters [see\nFigure 2C (b)].\nIn that particular case the tokens will be the following:\n{AAGTCC, AGTCCA, GTCCAG, TCCAGG, ..., AAGATT}. The\nk-mers are then mapped into numerical ids, which will be\nthe input for ProkBERT. As another example with k = 6\nand shift=2, the tokenized segments will be the following:\n{AAGTCC, GTCCAG, CCAGGA, ..., AAGATT}. If the sequence\nlength is odd, then the last charcter won’t be used. One of the main\nadvantages of the approach is that with the same number of tokens\nit is possible to cover a larger context, therefore it is possible to\nconsiderably reduce the computational and memory requirements,\nwhich is the typical bottleneck of the transformer architecture.\nIn this study, we propose models with a k-mer size\nof 6 (termed ProkBERT-mini), k-mer size of 1 (dubbed\nProkBERT-mini-c), and a variant supporting a larger context\nwindow, named ProkBERT-mini-long, which relies on a k-\nmer size of 6 with a shift = 2.\n/two.tnum./one.tnum./two.tnum Training data\nThe dataset was retrieved from the NCBI RefSeq database\n(\nO’Leary et al., 2016 ; Li et al., 2021 ) on January 6th, 2023. It\nincluded reference or representative genomes from bacteria,\nviruses, archaea, and fungi. After ﬁltering, the sequence database\nconsisted of 976,878 unique contigs derived from 17,178\nassemblies. These assemblies represent 3,882 distinct genera,\namounting to approximately 0.18 petabase pairs. The segment\ndatabases was created by sampling ﬁxed lengths of [256, 512, 1024]\nor, in other instances, variable lengths aiming for an approximate\ncoverage of 1.\nTokenization was performed using various k-\nmer sizes and shift parameters. The compiled\ndatabase was then stored in the Hierarchical\nData Format (HDF). Collectively, the training\ndatabase held roughly 200 billion tokens for each\nsegmented dataset.\nFor transparency and further research, all training data is\navailable at zenodo 10.5281/zenodo.10057832.\nFrontiers in Microbiology /zero.tnum/three.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nFIGURE /one.tnum\nA schematic overview of the training process. Starting with ra w sequence data, it undergoes preprocessing and vectorization. T he model is then\nﬁne-tuned, beginning with weights from a pretrained model, to add ress the speciﬁc classiﬁcation task. The output showcases class iﬁed sequences or\ntokens, predicted labels, scores, and a visualization highlight ing underlying sequence patterns and explanations.\nFIGURE /two.tnum\nPreprocessing of sequences. The sequences are initially segm ented into chunks ranging between /zero.tnum–/one.tnum kb. Two segmentation strategies can be\nemployed: (A) Contiguous segmentationwhere the sequence is split into non-overlapping parts; (B) Random segmentationwhere segments of\nvarying lengths are randomly sampled from the original contig. The th ird part (C) outlines the tokenization process of the segments: (a) Splitting\nsegments into non-overlapping tokens; (b) Creating maximally ov erlapping k-mers; (c) Generating partially overlapping k-m ers by shifting with a\nﬁxed size.\n/two.tnum./two.tnum Pretraining and learning sequence\nrepresentations\n/two.tnum./two.tnum./one.tnum Transformer model selection and\nparameters\nIn our study, we employed the MegatronBert model (\nShoeybi\net al., 2019 ), a variant of the BERT architecture ( Devlin et al.,\n2019), optimized for large-scale training. The architecture overview\nis presented in Supplementary Figure S1 . The key attributes of\nour models can be seen in Table 1. The miniand mini-long\nmodels share a common vocabulary of 4,101 k-mers. In contrast,\nthe mini-cmodel is distinct, using a smaller set comprising\nonly 9 items, including special tokens (i.e., [CLS], [SEP]) and\nnucleotides (A, C, T, G). All models employ a learnable relative\nkey-value positional embedding, which maps input vectors into\na 384-dimensional space. The miniand mini-longmodels\nsupport maximum sequence lengths of 1024 bp and 2048\nbp, respectively. Across all models, the intermediate layers of\nthe encoder use the GELU activation function, expanding the\ninput dimensions to 3,072 before compressing them back to\n384 dimensions. The Masked Language Modeling (MLM) head,\na standard component in each model, decodes from 384 to\n4,101 dimensions, adapted to the varying vocabulary sizes. To\nensure eﬃcient parallel computations, we encapsulated the entire\narchitecture within a DataParallel wrapper, thus optimizing GPU\nutilization. For implementation, all models were developed using\nthe PyTorch version 2.01 framework and the Hugging Face library\nversion 4.33.2.\nFrontiers in Microbiology /zero.tnum/four.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nTABLE /one.tnumA comprehensive overview of model parameters across varied\nconﬁgurations.\nMini Mini-cMini-long\nParameters 20,6 m 24,9 m 26,6 m\nTokenizer 6-mer, shift=1 1-mer 6-mer, shift=2\nLayers 6 6 6\nAttention heads 6 6 6\nMax. context size (bp) 1027 nt 1022 nt 4096 nt\nTraining data 206,65 billion 206,65 billion 206,65 billion\n/two.tnum./two.tnum./two.tnum Training process\n/two.tnum./two.tnum./two.tnum./one.tnum Masked Language Modeling objective\nmodiﬁcations\nWhile Masked Language Modeling (MLM) acts as the\nprimary pre-training objective for BERT models (Bidirectional\nEncoder Representations from Transformers) as established\nby\nDevlin et al. (2019), our implementation has slight variations.\nIn the traditional BERT approach, a certain percentage of\ninput tokens are randomly masked, and the model predicts\nthese based on their context. Typically, about 15% of tokens\nundergo masking. However, due to our usage of overlapping\nk-mers, masking becomes more intricate. If a k-mer of size\nk = 6 is masked, we need to ensure at least six tokens\nare also masked to prevent trivial restoration from context\nand locality.\nFor an input sequence of tokens x and a binary mask vector\nm—where 1 indicates a masked token and 0 indicates an unmasked\ntoken—the model outputs predicted vectors y. As for the noise\napplication on masked tokens, probabilities p1, p2, and p3 deﬁne\ndiﬀerent noise strategies. In our model, when a token is masked,\nit is substituted with the special [MASK]token with a probability\nof p1. Alternatively, with a probability p2, it can be replaced with\na random k-mer from our vocabulary. Lastly, there’s a p3 chance\nthat the masked k-mer will remain as it is. Conventionally, these\nprobabilities are set at 0.8, 0.1, and 0.1, respectively.\nThe MLM objective aims to minimize the negative log\nlikelihood over all masked positions, as described by the equation:\nLMLM(x, m, l) = −\n∑\ni : mi=1\nlog yi[li]\nWhere yi[li] denotes the predicted probability of the true label\nli for the masked position i. This objective, coupled with the\nnoise injection strategy, ensures that the model learns bidirectional\nrepresentations, thus becomes capable of understanding and\ngenerating contextually relevant tokens.\nWhen dealing with overlapping k-mers, simple token masking\nbecomes insuﬃcient. If a single k-mer token is masked, all\noverlapping k-mers related to that token must also be masked. This\nis crucial because when a k-mer is not masked and subsequently\nrestored, it might inadvertently provide contextual information\nabout its neighbors. Such a situation would enable the trivial\nrestoration of adjacent masked k-mers. In essence, one unmasked\nk-mer could potentially “leak” enough information to unmask\nits neighboring tokens. For examples, as presented in\nFigure 2C\n(Overlapping tokeniization), if only the second token “AGTCCA ”\nis masked, it can be fully restored from its neighboring tokens:\n“AAGTCC” and “GTCCAG.”\nThis overlapping nature of k-mers posed unique challenges.\nAs a result, we had to dynamically adjust the MLM parameters\nand the lengths of sequence segments during the pretraining\nphase. Additionally, when multiple contiguous k-mers were\nmasked together, the probability associated with the MLM had\nto be recalibrated. This was necessary to ensure that the actual\nproportion of the sequence being masked was consistent with our\nintended masking ratio.\n/two.tnum./two.tnum./two.tnum./two.tnum Training phases and conﬁguration\nInitially, we employed parameters that allowed complete\nsequence restoration (k-mer of k = 6) by masking only ﬁve\ncontinuous tokens (with p1 = 0.9) and manipulating 15% of\nthe tokens. Once a loss threshold of 1 was attained, the MLM\nparameters were adjusted to heighten the masking complexity. We\nimplemented various masking lengths, such as 2 nucleotides for k-\nmer of k = 6 and 2 characters for k = 1. Training data in the ﬁrst\nphase had a ﬁxed length of 128nt segments. The succeeding phase\nused variable-length datasets: with a probability of 0.5 a full-length\nsegments, and with a probability of 0.5 a segment between 30–512\nbp was selected into the the batch. The termination criterion for\ntraining was no further improvement or performance decrease, in\nboth the MLM and promoter tasks. Models underwent training for\nroughly one batch each. We opted for batch sizes that spanned\naround 0.5–2 million bp sequences. Computations were executed\non HPC-VEGA and Komondor platforms with Nvidia-A100 GPUs,\nleveraging slurm, pytorch distributed, and multiple GPU nodes.\n/two.tnum./two.tnum./three.tnum Evaluating the pretrained model\nWe evaluted the masking performance of the models\nusing the ESKAPE pathogens, namely Enterococcus faecium\n(GCF_009734005.1), Staphylococcus aureus (GCF_000013425.1),\nKlebsiella pneumoniae (GCF_000240185.1), Acinetobacter\nbaumannii (GCF_008632635.1), Pseudomonas aeruginosa\nPAO1 (GCF_000006765.1), and Escherichia coli str. K-12\n(GCF_000005845.2), because of their high clinical importance.\nFirst we investigated how the genomic structure is reﬂected in the\nembeddings, on diﬀerent sequence features (i.e. CDS, intergenic,\npseudo-genes, etc.). Next we measured how well the models can\nperform in masking.\n/two.tnum./two.tnum./four.tnum Analysis of encoder outputs\nIn deep learning, an encoder typically processes input data\n(such as a sequence of tokens) and produces a dense vector\nrepresentation for each token. These dense vectors, often referred\nto as embeddings or encoded vectors, capture the semantic\ninformation of the input tokens.\nGiven an input sequence S with T tokens, i.e.,\nS = { s1, s2, . . . , sT }\nFrontiers in Microbiology /zero.tnum/five.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nFIGURE /three.tnum\nLCA Tokenization and corrupted sequence restoration. The ﬁgure il lustrates how the corruption of a character at sequence level aﬀ ects the initial\nvector representations of the seqment with respect to the diﬀe rent tokenization methods. The /seven.tnumth nucleotide is unknown or masked,and those\nk-mers that overlap with that position are masked. As a result, w hen k = /six.tnum ands = /two.tnum not only the /seven.tnumth character is hidden, but the /eight.tnumth as well.\nthe encoder produces a sequence of vectors:\nE = { e1, e2, . . . , eT }\nwhere ei represents the embedded vector for the token si. In case\nof multiple inputs or batches, if we have a batch of size B with\neach sequence containing T tokens, the encoder’s output would\nbe a 3D tensor of shape ( B, T, D) where D is the dimensionality\nof the embeddings.\nOnce we have the encoded vectors, there are several ways to\naggregate or pool them to get a single representation for the entire\nsequence as shown in\nSupplementary Figure S1 . Here are some\ncommon pooling methods:\n• Mean Pooling: Average the vectors: emean = 1\nT\n∑ T\ni=1 ei.\n• Sum Pooling: Sum the vectors: esum = ∑ T\ni=1 ei.\n• Max Pooling: Max value per dimension: emax[j] =\nmaxT\ni=1 ei[j].\n• Min Pooling: Min value per dimension: emin[j] =\nminT\ni=1 ei[j].\nFor batches, these pooling operations are applied\nindependently for each input sequence in the batch. The provided\nNCBI annotations were preprocessed and extended. Intergenic\nregions were deﬁned as non-annotated genomic features with\nrespect to the strand. We retained the CDS, intergenic, pseudo-\ngenes, ncRNA features, while the rare or infrequently used\nfeatures (such as riboswitch, binding_site, tmRNA, etc.) were\nexcluded from the analysis. This was followed by sampling\nsegments of various lengths from each genomic region. We\nsampled a maximum of 2000 sequence features from each\ncontig, considering the strand, to evaluate strand-speciﬁc biases\nas well.\nThen, we randomly corrupted a segment 10,000 times, i.e., a\ncharacter was replaced with “*” and tokens containing “*” were\nmapped to the [MASK] token as illustrated on\nFigure 3.\nThe sampled segment database is available at Zenodo\n10.5281/zenodo.10057832.\n/two.tnum./three.tnum Application I: bacterial promoter\nprediction\nThe ﬁrst task our models were evaluated on involved\ndistinguishing between promoter and non-promoter sequences in\nbacteria. A sequence is labeled “1” if identiﬁed as a promoter and\n“0” otherwise. The next section gives an overview of the dataset\nstructure and details about its constructions.\n/two.tnum./three.tnum./one.tnum Dataset overview\nThe known promoters, referred to as positive samples, are\nprimarily drawn from the Prokaryotic Promoter Database (PPD,\nSu et al., 2021 ), which contains experimentally validated promoter\nsequences from 75 organisms. Figure 4 illustrates the composition\nand source of our dataset, segregating prokaryotic promoters from\nnon-promoters and including an independent test set based on\nE.coli sigma70 promoters.\n/two.tnum./three.tnum./one.tnum./one.tnum Data partitioning and utilization\nTo ensure comprehensive evaluation, the dataset was split\ninto three parts, divided randomly into training, validation, and\ntesting datasets.\n1. Training set: Constitutes 80% of the total data and is pivotal for\ninitial model development and training.\nFrontiers in Microbiology /zero.tnum/six.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nFIGURE /four.tnum\nSchematic of the promoter dataset. This ﬁgure provides a visual r epresentation of the sequence sources and their distribution w ithin the study. The\ndataset comprises known promoter sequences from /seven.tnum/five.tnum organisms, retrieved from the Prokaryotic Promoter Database (PPD), alongside\nnon-promoter sequences obtained from the NCBI RefSeq database (spe ciﬁcally sampled from CDS regions). It also includes non-promoter\nsequences constructed via higher and zero-order Markov chains th at mirror compositional characteristics of known promoters. Additi onally, an\nindependent test set, focusing on E. colisigma/seven.tnum/zero.tnum promoters, was employed, curated byCassiano and Silva-Rocha (/two.tnum/zero.tnum/two.tnum/zero.tnum). A balanced distribution\napproach was adopted to even out the number of positive and negativ e samples, with the dataset being systematically divided in to training,\nvalidation, and test subsets. This stratiﬁcation underpins a thorough evaluation of the model eﬃcacy.\n2. Validation set: Comprises 10% of the data, aiding in ﬁne-tuning\nmodel parameters and preventing overﬁtting.\n3. Test set: Forms the remaining 10% of the data, crucial for\nunbiased model performance evaluation.\n/two.tnum./three.tnum./one.tnum./two.tnum Dataset construction for multispecies train, test\nand validation sets\nThe prokaryotic promoter sequences are typically 81 bp\nlong, ensuring compatibility with most tools’ input prerequisites,\nparticularly around the putative TSS region interval [ −60, +20].\nOur positive dataset encompasses promoter sequences from\nvarious species, predominantly found on both chromosomes\nand plasmids. Promoters included in the independent test set,\nbased on exact match, were excluded from the training data.\nSpecies and contigs were mapped to NCBI assembly and sequence\naccessions. To curate comprehensive non-promoter sequences\n(negative samples), we employed three strategies:\n1. Using non-promoter sequences (CDS–Coding Sequences).\n2. Random sequences generated with a 3rd-order Markov chain.\n3. Pure random sequences (0-order Markov chain) as proposed by\nCassiano and Silva-Rocha (2020).\nThe distribution of this composite dataset was 40% CDS,\n40% Markov-derived random sequences, and 20% pure random\nsequences (0-order Markov chain). One practical application of\npromoter detection in coding sequences is to check whether an\nunintentional promoter is injected or can be located inside a\nmodiﬁed or designed coding sequence region, causing disruption.\nTo cover this use-case, we incorporated the coding regions into our\ntraining and evaluation dataset. The CDS sequences were extracted\nfrom the genomic sequences of contigs, based on annotations from\nNCBI. The 81 bp long CDS region samples were selected based\non the NCBI-provided annotations for the available contigs with\nrespect to the underlying species. The promoter regions often\ncontain AT-rich sequences, i.e., TATA box. To capture and model\nthe AT-rich regions, we applied 3rd and 0 order Markov chains to\ngenerate sequence examples that reﬂect the compositional property\nof known promoters.\nA 3rd-order Markov chain predicts the next nucleotide in a\nsequence based on the states of the previous three nucleotides.\nFormally, the probability of observing a nucleotide xi given the\nnucleotides at positions xi−3, xi−2, and xi−1 is:\nP(xi|xi−3, xi−2, xi−1)\nFor DNA sequences, this yields 4 4 = 256 possible nucleotide\ncombinations. Such higher-order modeling can more eﬀectively\ncapture intricate sequence patterns and dependencies than lower-\norder models (\nDurbin et al., 1998 ). However, estimating transition\nprobabilities requires extensive data due to the increased number of\nstates (\nKoski and Noble, 2001 ). We determined these probabilities\nusing promoter sequences, to which we added the reverse\ncomplement of each promoter. Subsequently, random promoter\nsequences were generated using these models.\nWe have a second, independent test for assessing model\nperformance and referred to\nCassiano and Silva-Rocha (2020)’s\ndataset comprising E. coli sigma70 sequences. The positive,\nwell-recognized samples came from Regulon DB ( Santos-\nZavaleta et al., 2019 ). Cassiano and Silva-Rocha (2020) evaluated\nvarious tools using an experimentally validated E. coli K-12\npromoter set dependent on sigma70, sourced from Regulon\nDB 10.5 (\nSantos-Zavaleta et al., 2019 ). Given the extensive\ndocumentation of sigma70-dependent promoters in bacteria,\nonly these were considered. They used a positive dataset of 865\nhigh-evidence sequences from Regulon DB and a negative set\nof 1,000 sequences mimicking the nucleotide distribution of the\nnatural sequences. We ensured no overlap existed within the\npromoter datasets.\nFrontiers in Microbiology /zero.tnum/seven.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nThe promoter dataset is available as a Zenodo and Hugging\nFace dataset.\n/two.tnum./three.tnum./two.tnum Training for promoter prediction\nWe employed a ﬁne-tuning paradigm to evaluate our model.\nOur proposed binary classiﬁcation model extends the Megatron\nBERT architecture (\nShoeybi et al., 2019 ), tailored speciﬁcally\nfor binary classiﬁcation tasks. Let X represent the sequence of\ninput embeddings, with fBERT(X) denoting the transformation by\nMegatron BERT. Given an input sequence of length T, this model\ntransforms X into a sequence output S with dimensions T ×\nhidden_size, where S = fBERT(X). Unlike the conventional BERT\nmodel, which classiﬁes sequences based on the special [CLS]\ntoken representing the “sentence, ” our approach emphasizes\nintegrating representations of all tokens using a weighting scheme\nas shown in\nSupplementary Figure S1 .\nTo obtain a ﬁxed-size representation from the variable-length\nsequence S, we devised a weighting mechanism. The sequence\nS undergoes a transformation through a linear layer to yield a\nsequence of weights W:\nW = softmax(W1ST + b1)\nHere, W1 is a matrix sized hidden_size × 1 and b1 is a bias\nterm. The softmax operation ensures W forms a valid probability\ndistribution over sequence positions. The model then computes a\nweighted sum of the sequence representations:\nP =\nT∑\ni=1\nwisi\nWhere wi and si represent the weight and the sequence\nrepresentation at the ith position, respectively. Subsequently,\nP is processed by a dropout layer with a probability of\nhidden_dropout_prob to produce P′. This results in the ﬁnal\nclassiﬁcation logits L.\nDatasets, comprising training, validation, and testing subsets,\nwere appropriately tokenized and adapted for ProkBERT\nprocessing. For optimization, the AdamW variant was chosen with\nparameters α ∈ { 0.0001, 0.0004, 0.0008}, β1 = 0.95, β2 = 0.98,\nand ǫ = 5 × 10−5. A linear learning rate scheduler with warmup\nwas utilized. The model underwent training for two epochs,\nwith a batch size of 128 per GPU (NVIDIA A100-40GB GPUs)\nusing the pytorch data distributed framework (nvcc). Additional\nconﬁgurations included a weight decay of 0.01.\n/two.tnum./four.tnum Application II: phage sequence analysis\nBacteriophages have a signiﬁcant role in the microbiome,\ninﬂuencing host dynamics and serving as essential agents for\nhorizontal gene transfer (\nDe la Cruz and Davies, 2000 ). Through\nthis mechanism, they aid in the transfer of antibiotic resistance and\nvirulence genes, promoting evolutionary processes. Understanding\nthe diversity of phages is crucial for tackling challenges like\nclimate change and diseases (\nJansson and Wu, 2023 ). These phages\nexhibit distinct patterns in both healthy and diseased microbiomes\n(Yang et al., 2023 ). The correlation between the human virome and\nvarious health conditions, such as cancer, inﬂammatory bowel\ndiseases, and diabetes, has been documented (\nZhao et al., 2017 ;\nHan et al., 2018 ; Nakatsu et al., 2018 ; Fernandes et al., 2019 ;\nLiang et al., 2020 ; Zuo et al., 2022 ). However, deeper research is\nneeded to discern causality and their impact on microbial and host\nbiological processes.\nDespite the abundance of phages (\nBai et al., 2022a ), accurately\nquantifying and characterizing them remains a challenge. One\nprimary limitation is the restricted number of viral sequences\nin databases like NCBI RefSeq. Additionally, the categorization\nof viral taxonomy is still a topic of discussion (\nWalker et al.,\n2022). Though there have been recent eﬀorts to expand databases\n(Zhang et al., 2022 ; Camargo et al., 2023 ), the overall understanding\nof viral diversity is still not complete ( Yan et al., 2023 ). We\nhave assembled a unique phage sequence database using recently\npublished genomic data.\nAnother challenge is the life cycle of phages; temperate phages\nmight integrate their genomes into bacterial chromosomes and\nare often annotated as bacterial genomes, leading to potential\nmisidentiﬁcation. Current databases also show biases toward\ncertain genera (\nSchackart III et al., 2023 ), which can skew\nbenchmarking and the evaluation of diﬀerent methods. To address\nthis, we used a balanced benchmarking approach, ensuring each\nviral group corresponds to their predicted host genus, minimizing\nbias. We also compared viral genomes to their respective hosts,\na more demanding classiﬁcation task, such as distinguishing a\nSalmonella phage from its host genome compared to marine\ncyanobacteria. For our study, we selected a speciﬁc number of\nphages for testing, ensuring there is no overlap between training\nand testing sets at the species level.\n/two.tnum./four.tnum./one.tnum Phage dataset description\nTo train and assess our prediction models, we assembled a\ncomprehensive phage sequence database from diverse sources. As\nof 9th July, 2023, we procured viral sequences and annotations\nfrom the RefSeq database (\nO’Leary et al., 2016 ; Li et al., 2021 ).\nBy isolating entries labeled “phage, ” we obtained 6,075 contigs.\nOur database was further enriched with the inclusion of\nRen et al.\n(2020), a dataset validated through the TemPhD method ( Zhang\net al., 2022 ), adding another 192,326 phage contigs extracted from\n148,229 assemblies.\nTo address sequence redundancy present in both the RefSeq\nand TemPhD databases, we applied the CD-HIT algorithm ( Li and\nGodzik, 2006 ; Fu et al., 2012 ) (using CD-HIT-EST with a default\nword size of 5). While several clustering thresholds (0.99, 0.95, 0.90)\nwere experimented with and found to produce similar outcomes,\nwe settled on a threshold of 0.99. This process resulted in a reﬁned\nset of 40,512 distinct phage sequences, with an average length of\napproximately 43,356 base pairs, culminating in a total of 3.5 billion\nbase pairs. Notably, these sequences target a wide spectrum of 660\nbacterial genera. Subsequent to sequence curation, phage sequences\nwere mapped to their respective bacterial hosts to formulate\na balanced training dataset, ensuring equitable representation\nbetween phages and their hosts. This step is imperative, given\nthe distinct distributions observed between bacterial sequences\nFrontiers in Microbiology /zero.tnum/eight.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nand their phage counterparts. In numerous instances, due to\nambiguities in species-level identiﬁcation or gaps in taxonomic\ndata, host mapping was executed at broader taxonomic strata,\npredominantly at the genus level.\nIn our examination of bacteriophage-host associations at\nthe genus level, several bacterial genera stood out, showcasing\npronounced phage interactions. Salmonella, a main cause of\nfood-related sicknesses (\nPopoﬀ et al., 2004 ), stands out with an\nimpressive association of 24,182 phages, spanning a cumulative\nlength of over a billion base pairs (1,026,930,954 bp) and an average\nphage length of 42,467 bp. Following closely, the common gut\nbacterium, Escherichia (\nTenaillon et al., 2012 ), is linked with 8,820\nphages, accumulating a total length of 408,866,394 bp. The genus\nKlebsiella, notorious for its role in various infections (\nPaczosa\nand Mecsas, 2016 ), associates with 4,904 phages. Genera such\nas Listeria (Vázquez-Boland et al., 2011 ), Staphylococcus (Lowy,\n1998), and Pseudomonas (Driscoll et al., 2007 ), each with distinct\nclinical signiﬁcance, exhibit rich phage interactions. Notably,\nMycobacterium (\nCole et al., 1998 ), consisting of pathogens like\nthe tuberculosis-causing bacterium, shows associations with 2,156\nphages. Many of these bacterial genera are benign and even\nbeneﬁcial under normal conditions, they also include species that\ncan cause severe diseases in humans, especially when there’s an\nimbalance in the body’s natural ﬂora or when antibiotic resistance\ndevelops. Monitoring phage interactions with these bacteria oﬀers\npotential pathways for therapeutic interventions and a deeper\nunderstanding of microbial ecology in human health.\nAdditionally, balanced databases were created, stratiﬁed by\nthe host genus level, to mitigate the eﬀect of underrepresented\nor overrepresented phages, such as Salmonella. The reverse-\ncomplement sequences were included. The ﬁnal dataset\nencompasses a total of 660 unique bacterial genera. Undersampling\nwas performed with a threshold of 20,027,298 bp for 25 genera,\nwhile the others were upsampled with a maximum coverage of 5x,\nobtaining random samples of shorter fragments from the contigs.\nRandom segmentation and sampling were carried out as previously\ndescribed. The bacterial assemblies were randomly selected from\nthe NCBI database, prioritizing higher-quality assemblies. Many of\nthem were not included in the pretraining dataset. Subsequently,\nwe constructed a database with various sequence lengths: 256, 512,\n1024, and 2048 bps. The train-test-validation split was executed in\na 0.8, 0.1, and 0.1 proportion at the phage sequence level.\nFor comparison with alternative methods and tools, we had to\nsubsample our test set ( N = 10, 000) to conduct the evaluation\nwithin a reasonable timeframe.\n/two.tnum./four.tnum./two.tnum Model training for phage sequence analysis\nThe task was formulated as binary classiﬁcation, similarly to\nthe promoters. Phage sequence classiﬁcation was approached in a\nmanner analogous to the promoter training. Given the extensive\nsize of the dataset, preprocessing was conducted beforehand,\nsegmenting sequences into various lengths: 256, 512, 1,024, and\n2,048 bps. For both miniand mini-cmodels, the training\nprocess was partitioned into three distinct phases. An initial grid\nsearch was executed to optimize learning rates, and base models\nwere trained for an hour. The parameter yielding the highest\nMatthews Correlation Coeﬃcient (MCC) was selected. The model\nwas then trained using segment lengths of 256 bps for half an\nepoch, followed by 512 bps for another half epoch, and concluding\nwith two epochs for 1024 bps segments. The training regimen\nfor the mini-longmodel was similar, albeit commencing with\n512 bps segments, then transitioning to 1024 bps, and ﬁnally to\n2048 bps segments. Model optimization employed the settings\ndelineated previously.\n/two.tnum./five.tnum Applied metrics\nMCC (Matthews Correlation Coeﬃcient) : Used for binary\nclassiﬁcations and deﬁned as:\nMCC = TP × TN − FP × FN\n√(TP + FP)(TP + FN)(TN + FP)(TN + FN)\nwhere TP is true positives, TN is true negatives, FP is false positives,\nand FN is false negatives. The coeﬃcient ranges from −1 (total\ndisagreement) to 1 (perfect agreement).\nF1 Score: The harmonic mean of precision and recall, given by:\nF1 = 2 × Precision × Recall\nPrecision + Recall\nwith\nPrecision = TP\nTP + FP\nand\nRecall (Sensitivity) = TP\nTP + FN\nAccuracy: Represents the proportion of correctly predicted\ninstances to the total, deﬁned as:\nAccuracy = TP + TN\nTP + TN + FP + FN\nSensitivity (Recall) : The proportion of actual positives\ncorrectly identiﬁed:\nSensitivity = TP\nTP + FN\nSpeciﬁcity: The proportion of actual negatives\ncorrectly identiﬁed:\nSpeciﬁcity = TN\nTN + FP\nROC-AUC (Receiver Operating Characteristic - Area Under\nCurve): Evaluates the model’s discriminative ability between\npositive and negative classes. It’s the area under the ROC curve,\nwhich plots Sensitivity against 1 −Speciﬁcity for various thresholds.\nThe silhouette score is a measure used to calculate the\ngoodness of a clustering algorithm. It indicates how close each\nsample in one cluster is to the samples in the neighboring clusters,\nwith values ranging from –1 to 1, where a high value indicates that\nthe sample is well matched to its own cluster and poorly matched\nto neighboring clusters (\nRousseeuw, 1987).\nFrontiers in Microbiology /zero.tnum/nine.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nEquation for the silhouette score s(i) for a single sample:\ns(i) = b(i) − a(i)\nmax{a(i), b(i)}\nWhere:\n• a(i) is the average distance from the i-th sample to the other\nsamples in the same cluster.\n• b(i) is the smallest average distance from the i-th sample to\nsamples in a diﬀerent cluster, minimized over clusters.\n/three.tnum Results and discussion\n/three.tnum./one.tnum ProkBERT’s learned representations\ncapture genomic structure and phylogeny\nWe assessed the zero-shot capabilities of our models by\nexamining their proﬁciency in predicting genomic features\nbased solely on embedding vectors, in a manner akin to\nNucleotide Transformers and related methodologies.\nFigure 5\npresents the UMAP projection of these embedded vector\nrepresentations. Employing the UMAP technique, we reduced the\ndimensionality of genomic segments and derived embeddings.\nThese were then evaluated using silhouette scores across the\nthree models: ProkBERT-mini, ProkBERT-mini-c, and\nProkBERT-mini-long.\nOur primary objective was to discern if the representations of\nsequence segments from ESKAPE pathogens could be distinctly\ncategorized. Indeed,\nFigure 5 exhibits clear delineation among\nknown genomic features, including CDS (coding sequences),\nintergenic regions, ncRNA, and pseudogenes. It’s important to\nnote that these models were not explicitly trained to diﬀerentiate\nthese sequence features; the representations were solely derived\nthrough pretraining. For the critical genomic comparison\nbetween “intergenic” and “CDS” regions, the silhouette scores\nobtained were 0.4925, 0.5766, and 0.3352 across the respective\nmodels, emphasizing a consistent and clear distinction between\nthese features. Regarding non-coding RNA representations, the\nsilhouette scores for “ncRNA ” vs. “CDS” were 0.1537, 0.2935,\nand 0.2192, while for “ncRNA ” vs. “intergenic, ” they were 0.1648,\n0.1302, and 0.3109, further aﬃrming the assertion that ncRNAs\ncluster distinctly. Pseudogenes, as anticipated, exhibited some\noverlap with ’CDS’ , notably in the ProkBERT-minimodel with\na score of −0.0358. Yet, when compared with ’ncRNA ’ , a distinct\nseparation was observed, as evidenced by scores of 0.1630, 0.2365,\nand 0.1636.\nThis analysis aligns with biological knowledge, where\npseudogenes are expected to be more similar to CDS, while\nncRNAs, which have diﬀerent functions and characteristics, form\ndistinct clusters from CDS and intergenic regions. All three models\nappear to produce similar clustering results for the given pairs of\ngenomic features.\nThe embeddings prominently display the genomic intricacies\nof ESKAPE pathogens. Notably, Klebsiella pneumoniae and\nEscherichia coli, both members of the Enterobacteriaceae family,\nexhibit close proximity in the embedding space, echoing potential\ngenomic kinship or shared evolutionary paths. This observation is\nfurther corroborated by the low silhouette scores across the models.\nIn contrast, species like Pseudomonas aeruginosamanifest as more\ndistinct clusters, emphasizing their genetic disparities. Intriguing\noverlaps, such as those between diﬀerently labeled Acinetobacter\nbaumannii entities, highlight potential challenges in the data or\nshared genomic features. Combined, the UMAP visualizations\nand silhouette scores provide a profound insight into species-\nspeciﬁc genomic embeddings, revealing both shared and distinct\ngenomic signatures.\n/three.tnum./two.tnum ProkBERT can eﬃciently recover\ncorrupted sequences\nIn evaluating the models’ capabilities in the masking task,\nwe used random masking across various genomic segments,\nsuch as CDS, ncRNA, intergenic, and pseudogenes, detailed\nin\nTable 2. We measured performance with metrics like ROC-\nAUC and average reference rank. However, a direct model\ncomparison presents challenges. Notably, ProkBERT-mini-c\nboasts a signiﬁcantly smaller vocabulary size (9) in comparison\nto ProkBERT-miniand ProkBERT-mini-long(4101) This\nallows ProkBERT-mini-cto achieve higher rankings, like top3,\nwith relative ease as it encompasses nearly the entire vocabulary\n(there are 4 nucleotides). Also, the local context’s representation in\nProkBERT-mini-longis less dense, making the restoration of\nthe masked nucleotides harder in contrast to the others.\nFor sequences spanning 1,024 nucleotides, ProkBERT-mini\nexhibited a commendable AUC of 0.9998, accompanied by top 1\nand top 3 prediction accuracies of 51.69% and 92.27%, respectively.\nConcurrently, ProkBERT-mini-cachieved an AUC of 0.9586,\nwith top 1 and top 3 accuracies at 51.28% and 92.22%. However,\nProkBERT-mini-longreported slightly subdued ﬁgures, with\nan AUC of 0.9992 and top 1 and top 3 accuracies of 27.68% and\n55.89%. This underscores the eﬃcacy of the ProkBERT model\nfamily in handling genomic tasks. A salient observation from our\nanalysis is that a model’s prediction proﬁciency is intrinsically tied\nto the contextual size.\nIn our next assessment some performance nuances became\nevident across various genomic regions. The prokbert-mini\nmodel consistently stood out, especially within the Coding\nSequence (CDS) and Intergenic domains. For these regions, it\nachieved an unmatched ROC-AUC of 0.9998. Speciﬁcally, within\nthe CDS region, the model attained a Top1 accuracy of 50.33%,\na Top3 accuracy of 91.87%, and an average reference rank of\n0.811. In the Intergenic sections, these ﬁgures were 48.97%, 91.12%,\nand 0.843, respectively. The prokbert-mini-cmodel also\nexhibited commendable performance. Within the CDS regions, this\nmodel reached a Top1 accuracy of 50.65%, a Top3 accuracy of\n91.91%, and an average reference rank of 0.802. For the Intergenic\nregions, the metrics were 48.84%, 91.39%, and 0.839 respectively.\nDespite the achievements of the aforementioned models, challenges\npersisted across all models in the non-coding RNA (ncRNA)\ndomains. Even the top-performing prokbert-minisaw its\nTop1 accuracy drop to 32.46%, with an average reference rank\nincreasing to 1.202. Contrastingly, the prokbert-mini-long,\ndespite its detailed design, exhibited reduced accuracies, with\nFrontiers in Microbiology /one.tnum/zero.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nFIGURE /five.tnum\nUMAP embeddings of genomic segment representations. The ﬁgure pres ents the two-dimensional UMAP projections of embedded vector\nrepresentations for various genomic features, derived from the ProkBERT-mini, ProkBERT-mini-c, and ProkBERT-mini-longmodels. The\ndistinct clusters highlight the models’ ability to diﬀerentia te between features such as CDS (coding sequences), intergeni c regions, ncRNA, and\npseudogenes, even without explicit training for feature diﬀere ntiation. (B) The segments are colored according to species, indicating that c luster\nstructure reﬂects the phylogenic similarities. (A) Sequence embeddings of the diﬀerent regions. (B) Sequence embeddings of the diﬀerent species of\nESKAPE pathogens.\nTABLE /two.tnumMasking performance of the ProkBERT family.\nModel L Avg. Ref. Rank Avg. Top/one.tnumAvg. Top/three.tnumAvg. AUC\nProkBERT-mini 128 0.9315 0.4497 0.8960 0.9998\nProkBERT-mini-c 128 0.9429 0.4391 0.8965 0.9504\nProkBERT-mini-long128 3.9432 0.2164 0.4781 0.9991\nProkBERT-mini 256 0.8433 0.4848 0.9130 0.9998\nProkBERT-mini-c 256 0.8262 0.4928 0.9151 0.9565\nProkBERT-mini-long256 3.5072 0.2470 0.5258 0.9992\nProkBERT-mini 512 0.8098 0.5056 0.9179 0.9998\nProkBERT-mini-c 512 0.7983 0.5116 0.9203 0.9580\nProkBERT-mini-long512 3.3026 0.2669 0.5435 0.9992\nProkBERT-mini 1024 0.7825 0.5169 0.9227 0.9998\nProkBERT-mini-c 1024 0.7868 0.5128 0.9222 0.9586\nProkBERT-mini-long1024 3.2082 0.2768 0.5589 0.9992\nBold numbers indicate the best results per category.\nFrontiers in Microbiology /one.tnum/one.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nTABLE /three.tnumEvaluation of promoter prediction tools on E-coli sigma/seven.tnum/zero.tnum dataset (Transposed).\nTool Accuracy MCC Sensitivity Speciﬁcity\nProkBERT-mini 0.87 0.74 0.90 0.85\nProkBERT-mini-c 0.87 0.73 0.88 0.85\nProkBERT-mini-long 0.87 0.74 0.89 0.85\nCNNProm 0.72 0.50 0.95 0.51\niPro70-FMWin 0.76 0.53 0.84 0.69\n70ProPred 0.74 0.51 0.90 0.60\niPromoter-2L 0.64 0.37 0.94 0.37\nMultiply 0.50 0.05 0.81 0.23\nbTSSﬁnder 0.46 -0.07 0.48 0.45\nBPROM 0.56 0.10 0.20 0.87\nIBPP 0.50 -0.03 0.26 0.71\nPromotech 0.71 0.43 0.49 0.90\nSigma70Pred 0.66 0.42 0.95 0.41\niPromoter-BnCNN 0.55 0.27 0.99 0.18\nMULTiPly 0.54 0.19 0.92 0.22\nBold numbers indicate the best results per category.\nTop1 and Top3 accuracies of 25.18% and 52.66% across all labels,\nhinting at potential ineﬃciencies or overﬁtting. Collectively, these\nﬁndings underscore the importance of tailored model architectures\nfor genomic sequences and highlight the complexities of various\ngenomic regions, laying a foundation for future targeted deep\nlearning strategies in genomics.\n/three.tnum./three.tnum ProkBERT performs accurately and\nrobustly in promoter sequence recognition\nIdentifying promoters, which are crucial in initiating the\ntranscription process, is fundamental to understanding gene\nregulation in bacteria. Our initial ﬁne-tuning task focused on\nthe identiﬁcation of these genomic regions, primarily through\na binary classiﬁcation approach that distinguishes sequences as\neither promoters or non-promoters. Although this method is\nwidely used, various alternative strategies have been explored.\nA signiﬁcant limitation of current techniques, as highlighted by\nChevez-Guardado and Peña-Castillo (2021), is their reliance on\ntraining with a limited range of species, mainly E. coli, but also\nincluding Bacillus subtilisand a few other key species.\nAs illustrated in Figure 1, our training began with a pretrained\nmodel followed by training using cross-entropy loss minimization.\nWe evaluated the training outcomes on two datasets: a test set\ncurated by\nCassiano and Silva-Rocha (2020), and another one\ncomprising mixed species. The models’ performance on the ﬁrst\ndataset can be seen in\nTable 3.\nCassiano and Silva-Rocha (2020) had previously gauged\nthe eﬃcacy of several well-established tools, including BPROM\n(\nSalamov and Solovyevand, 2011 ), bTSSﬁnder ( Shahmuradov\net al., 2017 ), BacPP ( de Avila e Silva et al., 2011 ), CNNProm\n(Umarov and Solovyev, 2017 ), IBBP ( Wang et al., 2018 ), Virtual\nFootprint, iPro70-FMWin ( Rahman et al., 2019 ), 70ProPred ( He\net al., 2018 ), iPromoter-2L ( Liu et al., 2018 ), and MULTiPly\n(Zhang et al., 2019 ). Additionally, we incorporated newer tools\nlike Promotech ( Chevez-Guardado and Peña-Castillo, 2021 ) and\niPromoter-BnCNN ( Amin et al., 2020 ). These tools encompass\na broad spectrum of techniques. For instance, BPROM and\nbTSSﬁnder exploit conserved and promoter element motifs. BacPP\nand CNNProm use neural networks for promoter predictions in E.\ncoli and other bacteria based on transformed nucleotide sequences.\nIBBP adopts a unique image-based approach combined with\nlogistic regression and various sequence-based features. Tools like\n70ProPred, iPro70-FMWin, MULTiPly, and iPromoter-2L leverage\nSVM, logistic regression, and random forest methodologies,\ndrawing upon extracted sequence features such as physicochemical\nproperties and k-mer compositions.\nThe results are presented in\nTable 3. The ProkBERT family\nmodels exhibit remarkably consistent performance across the\nmetrics assessed. With respect to accuracy, all three tools achieve an\nimpressive score of 0.87, marking them among the top performers\nin promoter prediction. This suggests that, regardless of the speciﬁc\nversion, the underlying methodology used in the miniseries is\nrobust and eﬀective.\nWhen evaluating the balance between true and false\npredictions using MCC both ProkBERT-miniand\nProkBERT-mini-longslightly edge out ProkBERT-mini-c\nwith an MCC of 0.74 compared to 0.73 for mini-c. Although the\ndiﬀerence is marginal, it might indicate subtle reﬁnements in the\nmini-longapproach. In terms of sensitivity, which focuses on\nthe ability to correctly identify promoters, ProkBERT-minileads\nwith a score of 0.90, closely followed by ProkBERT-mini-long\nat 0.89 and ProkBERT-mini-cat 0.88. This hierarchy, albeit\nwith small diﬀerences, highlights the minute improvements\nFrontiers in Microbiology /one.tnum/two.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nachieved in the miniand mini-longversions. Lastly, for\nspeciﬁcity, all three versions achieve an identical score of 0.85. This\nuniformity underscores the consistency in their ability to correctly\nidentify non-promoters. In summary, while the performance across\nthe miniversions is largely comparable, ProkBERT-miniand\nProkBERT-mini-longdisplay marginal advantages in certain\nmetrics, hinting at potential reﬁnements in these versions.\nThe Promotech tool demonstrates a mixed performance\nacross the metrics. With an accuracy of 0.71, it correctly\npredicts the presence or absence of promoters 71% of the time.\nWhile this accuracy is lower than the top-performing tools like\nProkBERT-miniand its variants, it is signiﬁcantly better than\nthe lower-performing tools such as Multiply and bTSSﬁnder.\nSensitivity for Promotech is 0.49, suggesting that it correctly\nidentiﬁes nearly half of the actual promoters. However, its most\nremarkable performance metric is its speciﬁcity, with a score of\n0.90. This means Promotech is adept at identifying non-promoters,\ncorrectly classifying them 90% of the time.\nAmong the methods assessed, CNNProm, Sigma70Pred,\niPromoter-BnCNN, and iPromoter-2L exhibit notably high\nsensitivity scores, signifying their pronounced ability to correctly\nidentify promoters. Speciﬁcally, iPromoter-BnCNN leads with\nan exceptional sensitivity of 0.99, closely trailed by Sigma70Pred\nat 0.95, CNNProm at 0.95, and iPromoter-2L at 0.94. Such high\nsensitivity scores indicate these models’ potential in minimizing\nfalse negatives, which is crucial in applications where missing an\nactual promoter can have signiﬁcant implications. However, it’s\nvital to interpret these results with caution. The high sensitivity\nscores, especially of iPromoter-BnCNN and Sigma70Pred, come at\nthe expense of speciﬁcity. For instance, iPromoter-BnCNN has a\nnotably low speciﬁcity of 0.18, implying a substantial rate of false\npositives. Similarly, Sigma70Pred has a speciﬁcity of 0.41. This\nsuggests that while these models are adept at identifying promoters,\nthey often misclassify non-promoters as promoters. An essential\nfactor to consider in this evaluation is the training data. Given that\nthese models were trained on E. colidata, their performance might\nbe biased when evaluated on the same or closely related datasets.\nThis lack of independence between training and testing data can\nlead to overly optimistic performance metrics, as the models might\nmerely be recalling patterns they’ve already seen, rather than\ngeneralizing to novel, unseen data.\nNext, we evaluated our models’ performance on a test set\nencompassing a broad mix of promoters, extending beyond just E.\ncoli. The results are shown in\nFigure 6./one.tnum\nThe trio of tools in the ProkBERT family – mini, mini-c,\nand mini-long– consistently exhibited strong performance\nacross the metrics analyzed. In terms of accuracy, all three\nachieved scores between 0.79 and 0.81, solidifying their position\namong leading promoter prediction tools. This uniformity\nin results points to a reliable methodology underlying the\nProkBERT family. Using the Matthews Correlation Coeﬃcient\n(MCC) as a measure of prediction balance, ProkBERT-mini\n/one.tnum The selection of competitors for the second test set took into account the\nlarger size of the dataset, which posed practical challenges for established\nmethods optimized for smaller sequences, resulting in processi ng issues and\nlonger evaluation times.\nand ProkBERT-mini-longboth slightly outperformed\nProkBERT-mini-cwith MCC values of 0.63 and 0.62\nrespectively, against the 0.57 of mini-c. Considering sensitivity,\nProkBERT-miniachieved the highest score of 0.81, with\nProkBERT-mini-longand ProkBERT-mini-ctrailing at\n0.79 and 0.75, respectively. This order reiterates the nuanced\nenhancements in the models. With regard to speciﬁcity,\nProkBERT-mini-longstood out with a score of 0.83, whereas\nProkBERT-miniand ProkBERT-mini-cboth scored 0.82,\nreﬂecting their adeptness at accurate non-promoter classiﬁcation.\nOf the tools assessed, both Sigma70Pred and iPromoter-\nBnCNN show moderate performance in sensitivity, with\niPromoter-BnCNN taking the lead at 0.66 and Sigma70Pred\nfollowing at 0.52. Promotech displayed a varied metric\nperformance. With an accuracy rate of 61%, it identiﬁes promoters\ncorrectly in a majority of instances. Its sensitivity value of\n0.29 signiﬁes its capability to detect roughly one-third of true\npromoters. Yet, its high speciﬁcity of 0.93 reveals its proﬁciency at\nnegating non-promoters.\nPromoter prediction is an intricate task that requires a\nbalance between sensitivity and speciﬁcity. The consistently strong\nperformance of the ProkBERT family highlights their reliability\nin this domain. Yet, the selection of a tool should be made\nafter weighing the potential implications of both false positives\nand negatives.\n/three.tnum./four.tnum ProkBERT swiftly and accurately\nidentiﬁes phage sequences, even in\nchallenging settings\nVarious tools have addressed phage sequence identiﬁcation,\neach employing distinct strategies. These methods can be\ncategorized into: (i) homology or marker-based tools like\nVirSorter2 (\nGuo et al., 2021 ) and VIBRANT ( Kieft et al., 2020 ), (ii)\nalignment-free methods, for instance, DeepVirFinder ( Ren et al.,\n2020) and INHERIT ( Bai et al., 2022b ). The ﬁrst category leans\non existing annotations, databases, and sequences. In contrast,\nalignment-free methods are less inﬂuenced by existing knowledge,\noﬀering broader applicability and greater reliability with imperfect\nsequence data (\nWu et al., 2023 ). We assessed our classiﬁcation\naccuracy against INHERIT, VirSorter2, and DeepVirFinder ( Ren\net al., 2020 ). Notably, INHERIT employs a DNABert architecture\nfor classiﬁcation, akin to ours, drawing inspiration from DNABert\n(\nJi et al., 2021 ).\nIn genomic studies, discerning phage-related segments\nbecomes increasingly challenging as the segment length diminishes\n(\nGuo et al., 2021 ). This study rigorously evaluates six distinct phage\nclassiﬁcation methodologies over a range of sequence lengths,\nleveraging the accuracy and MCC as primary performance metrics.\nFor the shortest fragments (256bp), VirSorter was unable\nto process the test set. Among the evaluated methods, the\nProkBERT models— mini, mini-c, mini-long—\nconsistently emerged as top performers across varying lengths,\nas depicted in\nFigure 7. Speciﬁcally, ProkBERT-miniexcels\nwith shorter sequences, achieving the highest accuracy for 256\nbp fragments. This high accuracy does not come at the cost\nFrontiers in Microbiology /one.tnum/three.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nFIGURE /six.tnum\nPromoter prediction performance metrics on a diverse test set. A c omparative analysis of various promoter prediction tools, showcasi ng their\nperformance across key metrics including accuracy, F/one.tnum score, MCC, sensitivity, and speciﬁcity. The tools evaluated include ProkBERT-mini,\nProkBERT-mini-c, ProkBERT-mini-long, Promotech, Sigma/seven.tnum/zero.tnumPred, iPromoter-BnCNN, and MULTiPly.\nFIGURE /seven.tnum\nProkBERT identiﬁes phage sequences accurately and rapidly. (A) Method comparison over varying sequence lengths based on two essen tial\nperformance metrics: accuracy and MCC. (B) Scatter plots illustrating the relationship between evaluat ion time (on a logarithmic scale) and the\nmentioned performance metrics. The size of each point signiﬁes t he sequence length. Evaluation time encompasses model loading, se quence\npreprocessing, and inference phases. (A) Comparison of methods across diﬀerent sequence lengths. (B) Comparison of evaluation time and\nperformance metrics.\nFrontiers in Microbiology /one.tnum/four.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nof increased false positives or negatives, as evidenced by its\ncomparable MCC values. In contrast, DeepVirFinder, ranking ﬁfth,\nindicates potential optimization areas for such short sequences.\nWhile ProkBERT-miniconsistently ranks highest for lengths\nup to 1,024 bps, ProkBERT-mini-cclosely follows, signifying\nits stability and reliability. Notably, the maximum sequence\nlength that ProkBERT-miniand ProkBERT-mini-c\ncan process is limited to 1024bps, introducing the specialized\nProkBERT-mini-longfor extended sequences. This model\nshowcases its prowess with 2kb sequences, achieving an accuracy\nof 92.90% and an MCC of 0.859. Virsorter2, despite initial struggles\nwith shorter sequences, exhibits signiﬁcant improvements\nfor longer fragments. However, both DeepVirFinder and\nINHERIT show limited enhancements with increased sequence\nlengths, suggesting these methods might not capitalize on the\nadditional information longer sequences provide as eﬀectively\nas their counterparts. In conclusion, ProkBERT-miniand\nProkBERT-mini-longclearly stand out as top-performing\nmodels across various sequence lengths. While other methods may\nhave their merits, they simply don’t match the consistency and\nrobustness oﬀered by the ProkBERT models.\nIn phage classiﬁcation, sensitivity signiﬁes the proportion of\nactual phage sequences that are correctly identiﬁed. Conversely,\nspeciﬁcity represents the proportion of non-phage sequences\naccurately discerned. A method exhibiting high sensitivity\neﬀectively identiﬁes most phage sequences, while high speciﬁcity\nindicates minimal misclassiﬁcation of non-phage sequences as\nphage-related.\nSupplementary Figure S2 presents the comparative\nresults of the models in terms of speciﬁcity and sensitivity.\nInterestingly, longer sequences tend to decrease the speciﬁcity\nfor VirSorter2. This trend suggests that VirSorter2 might\nmisclassify non-phage sequences more frequently as the sequence\nlength increases. A concurrent analysis of sensitivity and\nspeciﬁcity reveals nuances in method performance. For example,\nProkBERT-miniconsistently achieves top ranks in sensitivity\nbut displays variable results in speciﬁcity. On the other hand,\nVirsorter2, despite its strong speciﬁcity, especially with extended\nsequences, requires enhancements in its sensitivity. Notably,\nseveral methods, including DeepVirFinder, ProkBERT-mini,\nProkBERT-mini-long, and ProkBERT-mini-c,\nconsistently maintain high speciﬁcity. Their narrow interquartile\nranges around upper values underscore their consistent,\nreliable performance.\nNext, we scrutinized the relationship between evaluation time\nand prediction performance. It’s important to note that the\nevaluation time encompasses not just the prediction interval but\nalso includes sequence preprocessing and model loading durations.\nThe ProkBERT family shines in terms of both swiftness and\neﬃcacy. These methods, regardless of sequence length, consistently\nregister evaluation durations under 10 seconds, making them\ninvaluable for applications necessitating real-time predictions.\nSpeciﬁcally, for 2kb sequences, ProkBERT-mini-longrecords\na commendable accuracy of nearly 92.9%. Its Matthews Correlation\nCoeﬃcient (MCC), a reliable metric of prediction prowess, stands\nat approximately 0.859 for the same sequence length. In contrast,\nboth VirSorter2 and DeepVirFinder manifest protracted evaluation\nphases, with the latency amplifying as sequences lengthen.\nRemarkably, VirSorter2 demands an evaluation span surpassing\n1,000 seconds for 2kb sequences. While assessing accuracy,\nDeepVirFinder exhibits suboptimal performance, especially with\nsuccinct sequences like 256 bp, where it achieves a mere 75%.\nHowever, it’s essential to acknowledge that VirSorter2 extends\nbeyond mere classiﬁcation; it oﬀers comprehensive annotations, a\nprocess inherently time-intensive.\nIn essence, the ProkBERT family represents a synergy\nof rapidity and reliability. Concurrently, other contenders\nlike VirSorter2, DeepVirFinder, and INHERIT unveil distinct\nadvantages, coupled with potential avenues for reﬁnement.\n/four.tnum Conclusion\nIn bioinformatics, there has always been a keen interest\nin developing tools that can oﬀer precise and context-sensitive\ninterpretations of sequences. Meeting this demand, we introduced\nthe ProkBERT model family. These innovative models beneﬁt from\ntransfer learning (\nPan and Yang, 2009 ), a method showing promise\nin a variety of applications. A standout feature of ProkBERT is\nits ability to harness vast amounts of unlabeled sequence data\nthrough self-supervised learning (\nHe et al., 2020 ). This approach\nequips ProkBERT to handle challenges like limited labeled data, a\nproblem that has often hindered traditional models such as CNNs,\nRNNs, and LSTMs (\nCho et al., 2014 ; LeCun et al., 2015 ). Another\nstrength of ProkBERT is its adaptability; it performs well in\ndiﬀerent scenarios, from those with sparse data to classic supervised\nlearning tasks (\nSnell et al., 2017 ). When we compare ProkBERT to\nolder models that largely depend on expansive datasets, it’s clear\nthat ProkBERT ushers in a more adaptable approach for sequence\nanalysis in prokaryotic microbiome studies.\nOur results aﬃrm the robust generalization capabilities of\nthe ProkBERT family. The learned representations are not only\nconsistent but also harmonize well with established biological\nunderstanding. Speciﬁcally, the embeddings eﬀectively delineate\ngenomic features such as coding sequences (CDS), intergenic\nregions, and non-coding RNAs (ncRNA). Beyond capturing\ngenomic attributes, the embeddings also encapsulate phylogenetic\nrelationships. A case in point is the close proximity in the\nembedding space between Klebsiella pneumoniae and Escherichia\ncoli, both belonging to the Enterobacteriaceae family.\nWe validated the versatility of the ProkBERT model\nfamily by applying it to two challenging problems: promoter\nsequence prediction and phage identiﬁcation. Promoters play\nan instrumental role in transcriptomic regulation. Leveraging\nthe transfer-learning paradigm, ProkBERT adeptly addressed\nthe promoter prediction challenge, even when ﬁne-tuned on\nmulti-species datasets. This adaptability addresses a signiﬁcant\ngap, as many conventional bioinformatics tools tend to be species-\nspeciﬁc, often overlooking microbial diversity. In comprehensive\nbenchmarks against prominent tools, including Multiply,\nPromotech, and i-Promoter2L, ProkBERT consistently outclassed\nboth traditional machine learning and deep learning counterparts.\nFor instance, in E. coli promoter recognition, it achieved an\naccuracy of 0.87 and an MCC of 0.74, and even in a mixed-species\ncontext, the accuracy was 0.81 with an MCC of 0.62. Additionally,\nFrontiers in Microbiology /one.tnum/five.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nour ﬁndings underscore the robustness of the training, with\nthe ProkBERT-minivariant demonstrating resilience against\nvariations in optimization parameters, such as learning rate.\nOur evaluations demonstrate the prowess of ProkBERT\nin classifying phage sequences. Remarkably, it achieves high\nsensitivity and speciﬁcity even in challenging cases where available\nsequence information is limited. However, this exercise also\nhighlights an inherent limitation of ProkBERT, and more broadly\nof transformer models: the restricted context window size. While\ntransformer architectures are adept at capturing long-range\ninteractions (\nLin et al., 2022 ), they typically have a limited view\nof only a few kilobases. In comparative benchmarks with varying\nsequence lengths, ProkBERT consistently surpassed established\ntools like VirSorter2 and DeepVirFinder. For instance, it attained\nan accuracy of 92.90% and an MCC of 0.859 in multiple benchmark\nstudies. Intriguingly, ProkBERT even outperformed a DNA-BERT-\nbased model, which employs a BERT architecture and vectorization\nstrategy similar to ours.\nDiscussing model variants, both ProkBERT-miniand\nProkBERT-mini-chave a maximum context size of 1kb,\nwhile ProkBERT-mini-longextends this to 2kb. Notably,\nProkBERT-mini-longmanages to use longer sequence\ninformation without compromising on prediction performance\nor demanding additional computational resources, thanks to the\nLCA tokenization strategy. Our results indicate that the local\ncontext information oﬀered by ProkBERT-mini-longand\nProkBERT-minienhances robustness, giving them an edge over\nProkBERT-mini-c.\nProkBERT’s superiority is not limited to prediction\naccuracy; it also excels in terms of inference speed. Variants\nsuch as ProkBERT-mini, ProkBERT-mini-long,\nand ProkBERT-mini-cconsistently deliver outstanding\nperformance, both in terms of evaluation speed and accuracy.\nRegardless of the sequence length, these models typically complete\nevaluations in under 10 seconds, making them exceptionally suited\nfor real-time applications (\nVaswani et al., 2017 ).\nThe vector representations generated by ProkBERT can be\nseamlessly integrated with traditional machine learning tools,\npaving the way for innovative hybrid methodologies. Being an\nencoder architecture, ProkBERT’s ability to produce embeddings\nfor nucleotide sequences enables the direct incorporation of\nsequence information into more complex classiﬁers. This fusion\nof traditional and deep learning methods represents a promising\nfrontier in bioinformatics. Furthermore, insights from natural\nlanguage processing research suggest that the most informative\nrepresentations may not always emerge from the ﬁnal layer of a\nmodel (\nRae et al., 2021 ). This underscores the need for future\nstudies to delve deeper into the optimal layers for sequence\nrepresentation extraction in bioinformatics models.\nProkBERT distinguishes itself by being both compact and\npowerful, embodying a blend of eﬃciency and accessibility. One\nprevailing challenge with contemporary large language models\nlike GPT (\nRadford et al., 2019 ), BERT ( Devlin et al., 2019 ),\nand T5 ( Raﬀel et al., 2019 ) is their enormity. Models with\nhundreds of millions or even billions of parameters not only\ndemand substantial computational resources but also complicate\ntraining and hyperparameter optimization processes. In stark\ncontrast, ProkBERT is designed with a lean parameter count\nof approximately 20 million. This design choice ensures that it\ncan comfortably ﬁt within the memory constraints of modest\nGPUs. As a result, even researchers without access to high-\nperformance computing setups or top-tier GPUs can utilize\nProkBERT. Platforms like Google Colab, which oﬀer free but\nlimited GPU computation, become viable environments for\ntraining and evaluation tasks with ProkBERT.\nAs we present the ﬁndings of our study, it’s important\nto recognize certain limitations and identify areas for future\nenhancement. These include: (i) creation of larger models: The\neﬀectiveness of our current models can be further improved by\nscaling up. Larger models are likely to capture more complex\npatterns, which is particularly beneﬁcial for handling diverse and\nextensive datasets. (ii) Increasing context size: Expanding the\ncontext size in our models could lead to a better understanding\nof longer sequence dependencies. This enhancement is crucial for\nthe accurate interpretation of biological sequences. (iii) Building\nnew datasets: The development of new, comprehensive datasets\nis an ongoing necessity. These datasets should not only be\nlarger in size but also more diverse, ensuring the robustness and\nwide applicability of our models. (iv) Diversity in sequencing\napplications: Despite our progress, the question of diversity in\nsequence applications remains. This includes broadening the range\nof sequences our models can recognize and applying them to\na variety of biological phenomena. (v) Further applications and\ndescriptions: Future research should also aim to add and describe\nadditional applications. This would involve applying our models\nto new sequence analysis tasks, expanding the scope and utility\nof our work. Each of these points represents a critical area for\nimprovement and further research. Addressing these limitations\nwill enable us to develop more comprehensive and versatile tools\nin the ﬁeld of bioinformatics.\nIn essence, our ﬁndings highlight ProkBERT’s capability\nto learn detailed and adaptable vector representations for\nsequences. These representations hold promise not only\nfor current analytical challenges but also for emergent and\nunforeseen sequence classiﬁcation tasks in the future. Amidst\nthe challenges of understanding microbial communities,\nProkBERT stands as a transformative tool, elucidating the\ncomplex interplay of genes and organisms in the microbiome with\nremarkable precision.\nData availability statement\nThe datasets presented in this study can be found in\nonline repositories. The names of the repository/repositories and\naccession number(s) can be found below:\nhttps://github.com/nbrg-\nppcu/prokbert.\nAuthor contributions\nBL: Conceptualization, Data curation, Formal analysis,\nFunding acquisition, Investigation, Methodology, Project\nadministration, Resources, Software, Supervision, Validation,\nVisualization, Writing – original draft, Writing – review & editing.\nFrontiers in Microbiology /one.tnum/six.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nIS-N: Investigation, Software, Validation, Writing – original draft.\nBB: Conceptualization, Data curation, Investigation, Methodology,\nSoftware, Writing – original draft. NL-N: Conceptualization,\nFormal analysis, Methodology, Writing – original draft, Writing\n– review & editing. JJ: Data curation, Methodology, Validation,\nWriting – original draft, Writing – review & editing.\nFunding\nThe author(s) declare ﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. This work\nwas supported by grants of the Hungarian National Development,\nResearch and Innovation (NKFIH) Fund, OTKA PD (138055).\nThe work was also supported by EHPC-DEV-2022D10-001\n(Development).\nAcknowledgments\nThanks are due to Prof. S. Pongor (PPCU, Budapest) for\nhelp and advice. The authors extend their gratitude to all\nmembers of ML4Microbiome for their valuable discussions and\nfeedback on this research during the ML4Microbiome meetings.\nThe authors gratefully acknowledge the HPC RIVR consortium\n(\nwww.hpc-rivr.si) and EuroHPC JU (eurohpc-ju.europa.eu) for\nfunding this research by providing computing resources of the HPC\nsystem Vega at the Institute of Information Science ( www.izum.si)\nas well as to HPC-KIFU Komondor.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/fmicb.2023.\n1331233/full#supplementary-material\nReferences\nAlipanahi, B., Delong, A., Weirauch, M. T., and Frey, B. J. (2015 ). Predicting\nthe sequence speciﬁcities of DNA-and RNA-binding proteins by d eep learning. Nat.\nBiotechnol. 33, 831–838. doi: 10.1038/nbt.3300\nAmin, R., Rahman, C. R., Ahmed, S., Sifat, M. H. R., Liton, M. N . K., Rahman,\nM. M., et al. (2020). iPromoter-BnCNN: a novel branched CNN-b ased predictor\nfor identifying and classifying sigma promoters. Bioinformatics 36, 4869–4875.\ndoi: 10.1093/bioinformatics/btaa609\nAziz, R. K., Bartels, D., Best, A. A., DeJongh, M., Disz, T., Ed wards, R. A., et al.\n(2008). The RAST Server: rapid annotations using subsystems technology. BMC Gen.\n9, 1–15. doi: 10.1186/1471-2164-9-75\nBai, G.-H., Lin, S.-C., Hsu, Y.-H., and Chen, S.-Y. (2022a). T he human virome: viral\nmetagenomics, relations with human diseases, and therapeuti c applications. Viruses14,\n278. doi: 10.3390/v14020278\nBai, Z., Zhang, Y.-Z., Miyano, S., Yamaguchi, R., Fujimoto, K., Uematsu,\nS., et al. (2022b). Identiﬁcation of bacteriophage genome sequ ences with\nrepresentation learning. Bioinformatics 38, 4264–4270. doi: 10.1093/bioinformatics/bt\nac509\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhar iwal, P., et al.\n(2020a). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhar iwal, P., et al.\n(2020b). “Language models are few-shot learners, ” in Advances in Neural Information\nProcessing Systems33.\nCamargo, A., et al. (2023). IMG/VR v4: an expanded database of un cultivated virus\ngenomes 782 within a framework of extensive functional, taxo nomic, and ecological\nmetadata. Nucleic 783 Acids Res. 51, D733–D743. doi: 10.1093/nar/gkac1037\nCantalapiedra, C. P., Hernández-Plaza, A., Letunic, I., Bork, P., and Huerta-\nCepas, J. (2021). eggNOG-mapper v2: functional annotation, ort hology assignments,\nand domain prediction at the metagenomic scale. Molec. Biol. Evol. 38, 5825–5829.\ndoi: 10.1093/molbev/msab293\nCassiano, M. H. A., and Silva-Rocha, R. (2020). Benchmarking bacterial\npromoter prediction tools: Potentialities and limitations. Msystems 5, e00439.\ndoi: 10.1128/mSystems.00439-20\nChevez-Guardado, R., and Pe na-Castillo, L. (2021). Promotech : a\ngeneral tool for bacterial promoter recognition. Genome Biol . 22, 1–16.\ndoi: 10.1186/s13059-021-02514-9\nCho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Boug ares, F.,\nSchwenk, H., et al. (2014). “Learning phrase representations u sing RNN encoder-\ndecoder for statistical machine translation, ” in Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP) 1724–1734.\ndoi: 10.3115/v1/D14-1179\nCole, S., Brosch, R., Parkhill, J., Garnier, T., Churcher, C., H arris, D., et al. (1998).\nDeciphering the biology of Mycobacterium tuberculosis from th e complete genome\nsequence. Nature 393, 537–544. doi: 10.1038/31159\nDalla-Torre, H., Gonzalez, L., Mendoza-Revilla, J., Carranza, N. L., Grzywaczewski,\nA. H., Oteri, F., et al. (2023). The nucleotide transformer: Bu ilding and\nevaluating robust foundation models for human genomics. bioRxiv, 2023–01 .\ndoi: 10.1101/2023.01.11.523679\nde Avila e Silva, S., Echeverrigaray, S., and Gerhardt, G. J. (201 1). BacPP:\nbacterial promoter prediction–a tool for accurate sigma-fac tor speciﬁc assignment in\nenterobacteria. J. Theor. Biol. 287, 92–99. doi: 10.1016/j.jtbi.2011.07.017\nDe la Cruz, F., and Davies, J. (2000). Horizontal gene transfe r and\nthe origin of species: lessons from bacteria. Trends Microbiol . 8, 128–133.\ndoi: 10.1016/S0966-842X(00)01703-0\nDelcher, A. L., Harmon, D., Kasif, S., White, O., and Salzberg, S. L. (1999). Improved\nmicrobial gene identiﬁcation with GLIMMER. Nucleic Acids Res. 27, 4636–4641.\ndoi: 10.1093/nar/27.23.4636\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). “ BERT: pre-training\nof deep bidirectional transformers for language understand ing, ” inProceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Pa pers),\n4171–4186.\nDriscoll, J., Brody, S., and Kollef, M. (2007). Pseudomonas aeru ginosa:\npathogenesis and pathogenic mechanisms. Int. J. Med. Microbiol. 297, 277–289.\ndoi: 10.5539/ijb.v7n2p44\nFrontiers in Microbiology /one.tnum/seven.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nDurbin, R., Eddy, S., Krogh, A., and Mitchison, G. (1998). Biological Sequence\nAnalysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge: Cambridge\nUniversity Press. doi: 10.1017/CBO9780511790492\nFernandes, M. A., Verstraete, S. G., Phan, T. G., Deng, X., Stek ol, E.,\nLaMere, B., et al. (2019). Enteric virome and bacterial micro biota in children\nwith ulcerative colitis and Crohn’s disease. J. Pediatr. Gastroenterol. Nutr. 68, 30.\ndoi: 10.1097/MPG.0000000000002140\nFu, L., Niu, B., Zhu, Z., Wu, S., and Li, W. (2012). CD-HIT: acc elerated\nfor clustering the next-generation sequencing data. Bioinformatics 28, 3150–3152.\ndoi: 10.1093/bioinformatics/bts565\nGuo, J., Bolduc, B., Zayed, A. A., Varsani, A., Dominguez-Huer ta, G.,\nDelmont, T. O., et al. (2021). VirSorter2: a multi-classiﬁer, ex pert-guided\napproach to detect diverse DNA and RNA viruses. Microbiome 9, 1–13.\ndoi: 10.1186/s40168-020-00990-y\nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., et al. (20 22). A\nsurvey on vision transformer. IEEE Trans. Patt. Anal. Mach. Intell. 45, 87–110.\ndoi: 10.1109/TPAMI.2022.3152247\nHan, M., Yang, P., Zhong, C., and Ning, K. (2018). The\nhuman gut virome in hypertension. Front. Microbiol . 9, 3150.\ndoi: 10.3389/fmicb.2018.03150\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020). “Mome ntum\ncontrast for unsupervised visual representation learning, ” i n Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , 9726–9735.\ndoi: 10.1109/CVPR42600.2020.00975\nHe, W., Jia, C., Duan, Y., and Zou, Q. (2018). 70ProPred: a pred ictor for discovering\nsigma70 promoters based on combining multiple features. BMC Syst. Biol. 12, 99–107.\ndoi: 10.1186/s12918-018-0570-1\nHoarfrost, A., Aptekmann, A., Farfa nuk, G., and Bromberg, Y. ( 2022).\nDeep learning of a bacterial and archaeal universal language o f life enables\ntransfer learning and illuminates microbial dark matter. Nat. Commun. 13, 2606.\ndoi: 10.1038/s41467-022-30070-8\nJansson, J. K., and Wu, R. (2023). Soil viral diversity, ecolo gy and climate change.\nNat. Rev. Microbiol. 21, 296–311. doi: 10.1038/s41579-022-00811-z\nJi, Y., Zhou, Z., Liu, H., and Davuluri, R. V. (2021). DNABERT: pre-trained\nbidirectional encoder representations from transformers m odel for DNA-language in\ngenome. Bioinformatics 37, 2112–2120. doi: 10.1093/bioinformatics/btab083\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ron neberger, O., et al.\n(2021). Highly accurate protein structure prediction with Alpha Fold. Nature 596,\n583–589. doi: 10.1038/s41586-021-03819-2\nKelley, D. R., Reshef, Y. A., Bileschi, M., Belanger, D., McLean, C. Y., and\nSnoek, J. (2018). Sequential regulatory activity prediction a cross chromosomes with\nconvolutional neural networks. Genome Res. 28, 739–750. doi: 10.1101/gr.227819.117\nKieft, K., Zhou, Z., and Anantharaman, K. (2020). VIBRANT: a utomated\nrecovery, annotation and curation of microbial viruses, an d evaluation of\nviral community function from genomic sequences. Microbiome 8, 1–23.\ndoi: 10.1186/s40168-020-00867-0\nKoski, T., and Noble, J. M. (2001). A review of Bayesian networ ks and structure\nlearning. Mathem. Appl. 29, 9–36. doi: 10.14708/ma.v40i1.278\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature 521, 436–444.\ndoi: 10.1038/nature14539\nLi, W., and Godzik, A. (2006). CD-HIT: a fast program for cluster ing and\ncomparing large sets of protein or nucleotide sequences. Bioinformatics 22, 1658–1659.\ndoi: 10.1093/bioinformatics/btl158\nLi, W., O’Neill, K. R., Haft, D. H., DiCuccio, M., Chetvernin, V ., Badretdin,\nA., et al. (2021). RefSeq: expanding the Prokaryotic Genome Anno tation Pipeline\nreach with protein family model curation. Nucleic Acids Res. 49, D1020–D1028.\ndoi: 10.1093/nar/gkaa1105\nLiang, G., Conrad, M. A., Kelsen, J. R., Kessler, L. R., Breton, J ., Albenberg, L. G.,\net al. (2020). Dynamics of the stool virome in very early-onset inﬂammatory bowel\ndisease. J. Crohn’s Colitis14, 1600–1610. doi: 10.1093/ecco-jcc/jjaa094\nLin, T., Wang, Y., Liu, X., and Qiu, X. (2022). A survey of tran sformers. AI Open3,\n111–132. doi: 10.1016/j.aiopen.2022.10.001\nLiu, B., Yang, F., Huang, D.-S., and Chou, K.-C. (2018). iPro moter-2L: a two-layer\npredictor for identifying promoters and their types by multi-wi ndow-based PseKNC.\nBioinformatics 34, 33–40. doi: 10.1093/bioinformatics/btx579\nLowy, F. D. (1998). Staphylococcus aureus infections. New England J. Med. 339,\n520–532. doi: 10.1056/NEJM199808203390806\nLukashin, A. V., and Borodovsky, M. (1998). GeneMark.hmm: ne w solutions for\ngene ﬁnding. Nucleic Acids Res. 26, 1107–1115. doi: 10.1093/nar/26.4.1107\nMeyer, F., Bagchi, S., Chaterji, S., Gerlach, W., Grama, A., Har rison, T.,\net al. (2019). MG-RAST version 4-lessons learned from a decade of low-\nbudget ultra-high-throughput metagenome analysis. Brief. Bioinform. 20, 1151–1159.\ndoi: 10.1093/bib/bbx105\nMin, S., Lee, B., and Yoon, S. (2017). Deep learning in bioinfo rmatics. Brief.\nBioinform. 18, 851–869. doi: 10.1093/bib/bbw068\nNakatsu, G., Zhou, H., Wu, W. K. K., Wong, S. H., Coker, O. O., Da i, Z., et al.\n(2018). Alterations in enteric virome are associated with co lorectal cancer and survival\noutcomes. Gastroenterology 155, 529–541. doi: 10.1053/j.gastro.2018.04.018\nO’Leary, N. A., Wright, M. W., Brister, J. R., Ciufo, S., Hadd ad, D., McVeigh,\nR., et al. (2016). Reference sequence (RefSeq) database at NCBI : current status,\ntaxonomic expansion, and functional annotation. Nucl. Acids Res. 44, D733–D745.\ndoi: 10.1093/nar/gkv1189\nPaczosa, M. K., and Mecsas, J. (2016). Klebsiella pneumoniae: go ing on\nthe oﬀense with a strong defense. Microbiol. Molec. Biol. Rev . 80, 629–661.\ndoi: 10.1128/MMBR.00078-15\nPan, S. J., and Yang, Q. (2009). A survey of transfer learning. J. Mach. Lear. Res. 22,\n1–40. doi: 10.1109/TKDE.2009.191\nPopoﬀ, M. Y., Bockemuhl, J., and Gheesling, L. L. (2004). Supplemen t\n2002 (no. 46) to the Kauﬀmann-White scheme. Res. Microbiol. 155, 568–570.\ndoi: 10.1016/j.resmic.2004.04.005\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutsk ever, I. (2019).\nLanguage models are unsupervised multitask learners. OpenAI Blog1, 9.\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoﬀmann, J., So ng, F., et al. (2021).\nScaling language models: methods, analysis insights from train ing gopher. arXiv\npreprint arXiv:2112.11446.\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Mate na, M., et al. (2019).\nExploring the limits of transfer learning with a uniﬁed text-to- text transformer. arXiv\npreprint arXiv:1910.10683.\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Mate na, M., et al. (2020).\nExploring the limits of transfer learning with a uniﬁed text-to- text transformer. J. Mach.\nLear. Res. 21, 5485–5551.\nRahman, M. S., Aktar, U., Jani, M. R., and Shatabda, S. (2019) . iPro70-FMWin:\nidentifying Sigma70 promoters using multiple windowing and min imal features.\nMolec. Genet. Genom. 294, 69–84. doi: 10.1007/s00438-018-1487-5\nRen, J., Song, K., Deng, C., Ahlgren, N. A., Fuhrman, J. A., Li, Y., et al. (2020).\nIdentifying viruses from metagenomic data using deep learni ng. Quant. Biol. 8, 64–77.\ndoi: 10.1007/s40484-019-0187-4\nRousseeuw, P. J. (1987). Silhouettes: A graphical aid to the in terpretation\nand validation of cluster analysis. J. Comput. Appl. Mathem . 20, 53–65.\ndoi: 10.1016/0377-0427(87)90125-7\nSalamov, V. S. A., and Solovyevand, A. (2011). “Automatic anno tation\nof microbial genomes and metagenomic sequences, ” in Metagenomics and\nits Applications in Agriculture, Biomedicine and Environmental S tudies,\n61–78.\nSantos-Zavaleta, A., Salgado, H., Gama-Castro, S., Sánchez- Pérez, M., Gómez-\nRomero, L., Ledezma-Tejeida, D., et al. (2019). RegulonDB v 10 .5: tackling challenges to\nunify classic and high throughput knowledge of gene regulation i n E. coli K-12. Nucleic\nAcids Res. 47, D212–D220. doi: 10.1093/nar/gky1077\nSapoval, N., Aghazadeh, A., Nute, M. G., Antunes, D. A., Balaji, A .,\nBaraniuk, R., et al. (2022). Current progress and open challenges for\napplying deep learning across the biosciences. Nat. Commun . 13, 1728.\ndoi: 10.1038/s41467-022-29268-7\nSchackart, I. I. I., K. E., Graham, J. B., Ponsero, A. J., and Hu rwitz, B. L. (2023).\nEvaluation of computational phage detection tools for metageno mic datasets. Front.\nMicrobiol. 14, 1078760. doi: 10.3389/fmicb.2023.1078760\nSeemann, T. (2014). Prokka: rapid prokaryotic genome annotat ion. Bioinformatics\n30, 2068–2069. doi: 10.1093/bioinformatics/btu153\nShahmuradov, I. A., Mohamad Razali, R., Bougouﬀa, S., Radova novic, A., and Bajic,\nV. B. (2017). bTSSﬁnder: a novel tool for the prediction of prom oters in cyanobacteria\nand Escherichia coli. Bioinformatics 33, 334–340. doi: 10.1093/bioinformatics/\nbtw629\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., a nd Catanzaro, B.\n(2019). Megatron-LM: training multi-billion parameter languag e models using model\nparallelism. CoRR, abs/1909.08053.\nSnell, J., Swersky, K., and Zemel, R. (2017). “Prototypical netw orks for few-shot\nlearning, ” inAdvances in Neural Information Processing Systems4077–4087.\nSommer, M. J., and Salzberg, S. L. (2021). Balrog: A universal pr otein\nmodel for prokaryotic gene prediction. PLoS Comput. Biol . 17, e1008727.\ndoi: 10.1371/journal.pcbi.1008727\nSu, W., Liu, M.-L., Yang, Y.-H., Wang, J.-S., Li, S.-H., Lv, H ., et al. (2021). PPD: a\nmanually curated database for experimentally veriﬁed prokaryoti c promoters. J. Molec.\nBiol. 433, 166860. doi: 10.1016/j.jmb.2021.166860\nTatusova, T., DiCuccio, M., Badretdin, A., Chetvernin, V., Nawrocki, E. P.,\nZaslavsky, L., et al. (2016). NCBI prokaryotic genome annotati on pipeline. Nucl. Acids\nRes. 44, 6614–6624. doi: 10.1093/nar/gkw569\nFrontiers in Microbiology /one.tnum/eight.tnum frontiersin.org\nLigeti et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/three.tnum/one.tnum/two.tnum/three.tnum/three.tnum\nTenaillon, O., Rodríguez-Verdugo, A., Gaut, R., McDonald, P., Bennett, A., Long,\nA., et al. (2012). The molecular diversity of adaptive convergen ce. Science 335, 457–461.\ndoi: 10.1126/science.1212986\nUmarov, R. K., and Solovyev, V. V. (2017). Recognition of proka ryotic and\neukaryotic promoters using convolutional deep learning neura l networks. PLoS ONE\n12, e0171410. doi: 10.1371/journal.pone.0171410\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems 30.\nVázquez-Boland, J. A., Kuhn, M., Berche, P., Chakraborty, T., Domínguez-\nBernal, G., Goebel, W., et al. (2011). Listeria monocytogenes: su rvival and\nadaptation in the gastrointestinal tract. Front. Cell. Infect. Microbiol . 1, 3.\ndoi: 10.1128/CMR.14.3.584-640.2001\nWalker, P. J., Siddell, S. G., Lefkowitz, E. J., Mushegian, A. R., Adriaenssens, E. M.,\nAlfenas-Zerbini, P., et al. (2022). Recent changes to virus ta xonomy ratiﬁed by the\nInternational Committee on Taxonomy of Viruses (2022). Arch. Virol. 167, 2429–2440.\ndoi: 10.1007/s00705-022-05516-5\nWang, S., Cheng, X., Li, Y., Wu, M., and Zhao, Y. (2018). Image -based promoter\nprediction: a promoter prediction method based on evolutionari ly generated patterns.\nScient. Rep. 8, 17695. doi: 10.1038/s41598-018-36308-0\nWu, L.-Y., Pappas, N., Wijesekara, Y., Piedade, G. J., Brussaar d, C. P.,\nand Dutilh, B. E. (2023). Benchmarking bioinformatic virus i dentiﬁcation\ntools using real-world metagenomic data across biomes. bioRxiv, 2023–04 .\ndoi: 10.1101/2023.04.26.538077\nYan, M., Pratama, A. A., Somasundaram, S., Li, Z., Jiang, Y., Sullivan,\nM. B., et al. (2023). Interrogating the viral dark matter of th e rumen\necosystem with a global virome database. Nat. Commun . 14, 5254.\ndoi: 10.1038/s41467-023-41075-2\nYang, K., Wang, X., Hou, R., Lu, C., Fan, Z., Li, J., et al. (2023 ). Rhizosphere phage\ncommunities drive soil suppressiveness to bacterial wilt dise ase. Microbiome 11, 1–18.\ndoi: 10.1186/s40168-023-01463-8\nZdobnov, E. M., and Apweiler, R. (2001). InterProScan-an inte gration platform\nfor the signature-recognition methods in InterPro. Bioinformatics 17, 847–848.\ndoi: 10.1093/bioinformatics/17.9.847\nZhang, M., Li, F., Marquez-Lago, T. T., Leier, A., Fan, C., Kwo h, C. K., et al. (2019).\nMULTiPly: a novel multi-layer predictor for discovering genera l and speciﬁc types of\npromoters. Bioinformatics 35, 2957–2965. doi: 10.1093/bioinformatics/btz016\nZhang, S., Fan, R., Liu, Y., Chen, S., Liu, Q., and Zeng, W. (20 23). Applications\nof transformer-based language models in bioinformatics: a su rvey. Bioinform. Adv. 3,\nvbad001. doi: 10.1093/bioadv/vbad001\nZhang, X., Wang, R., Xie, X., Hu, Y., Wang, J., Sun, Q., et al. (2 022). Mining bacterial\nNGS data vastly expands the complete genomes of temperate phages. NAR Genom.\nBioinform. 4, lqac057. doi: 10.1093/nargab/lqac057\nZhao, G., Vatanen, T., Droit, L., Park, A., Kostic, A. D., Poon , T. W., et al.\n(2017). Intestinal virome changes precede autoimmunity in t ype I diabetes-susceptible\nchildren. Proc. Natl. Acad. Sci. 114, E6166–E6175. doi: 10.1073/pnas.1706359114\nZhou, Z., Ji, Y., Li, W., Dutta, P., Davuluri, R., and Liu, H. (2 023). DNABERT-2:\nEﬃcient foundation model and benchmark for multi-species gen ome. arXiv preprint\narXiv:2306.15006.\nZuo, W., Michail, S., and Sun, F. (2022). Metagenomic analyses of multiple gut\ndatasets revealed the association of phage signatures in color ectal cancer. Front. Cell.\nInfect. Microbiol. 12, 918010. doi: 10.3389/fcimb.2022.918010\nFrontiers in Microbiology /one.tnum/nine.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6662567853927612
    },
    {
      "name": "Adaptability",
      "score": 0.5458905100822449
    },
    {
      "name": "Identification (biology)",
      "score": 0.5451956391334534
    },
    {
      "name": "Machine learning",
      "score": 0.5422111749649048
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5351712703704834
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5175853967666626
    },
    {
      "name": "Microbiome",
      "score": 0.5043610334396362
    },
    {
      "name": "Computational biology",
      "score": 0.4810369312763214
    },
    {
      "name": "Data science",
      "score": 0.41313350200653076
    },
    {
      "name": "Biology",
      "score": 0.2741820812225342
    },
    {
      "name": "Bioinformatics",
      "score": 0.22124025225639343
    },
    {
      "name": "Ecology",
      "score": 0.1442371904850006
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31882830",
      "name": "Pázmány Péter Catholic University",
      "country": "HU"
    },
    {
      "id": "https://openalex.org/I2802350943",
      "name": "Hungarian Research Centre for Linguistics",
      "country": "HU"
    },
    {
      "id": "https://openalex.org/I101202996",
      "name": "Semmelweis University",
      "country": "HU"
    }
  ]
}