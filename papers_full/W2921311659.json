{
  "title": "Neutron: An Implementation of the Transformer Translation Model and its Variants",
  "url": "https://openalex.org/W2921311659",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2061910776",
      "name": "Xu Hongfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2388971840",
      "name": "Liu Qiu-hui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2970803838",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2952524847",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2968917279",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2963949210",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2989268779",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964213727",
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2962931466",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2767286248",
    "https://openalex.org/W2574872930",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W2778814079",
    "https://openalex.org/W2669742347",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963418779"
  ],
  "abstract": "The Transformer translation model is easier to parallelize and provides better performance compared to recurrent seq2seq models, which makes it popular among industry and research community. We implement the Neutron in this work, including the Transformer model and its several variants from most recent researches. It is highly optimized, easy to modify and provides comparable performance with interesting features while keeping readability.",
  "full_text": "Neutron: An Implementation of the Transformer Translation Model\nand its Variants\nHongfei Xu\nSaarland University\nDFKI\nSaarbruecken, 66123\nhfxunlp@foxmail.com\nQiuhui Liu\nChina Mobile Online Services\nHenan, 450001\nliuqiuhui@cmos.chinamobile.com\nAbstract\nThe Transformer translation model is easier\nto parallelize and provides better performance\ncompared to recurrent seq2seq models, which\nmakes it popular among industry and research\ncommunity. We implement the Neutron 1 in\nthis work, including the Transformer model\nand its several variants from most recent re-\nsearches. It is highly optimized, easy to mod-\nify and provides comparable performance with\ninteresting features while keeping readability.\n1 Introduction\nVaswani et al. (2017) proposed Transformer archi-\ntecture which contains only attention mechanism,\nstandard feed-forward neural network with resid-\nual connection, dropout, layer normalization and\nlabel smoothing loss for its training. The Trans-\nformer parellelizes better and outperforms previous\nseq2seq models in many cases. It is widely applied\nin the industry and attracts wide attention from re-\nsearchers, as a result, many enhanced architectures\nhave been further proposed.\nOur implementation ﬁrst supports those popu-\nlar features provided in most Machine Translation\n(MT) libraries, including beam search, ensemble,\nlength penalty and averaging of models.\nIn addition, we implement some variants of the\nTransformer from recent researches along with the\nstandard implementation of Transformer in the\nNeutron, such as: the Average Attention (to ac-\ncelerate the decoding of the Transformer) (Zhang\net al., 2018a), the Hierarchical Layer Aggregation\n(Dou et al., 2018), Recurrent decoder (Chen et al.,\n2018), Sentential Context (Wang et al., 2019) for\nimproving the MT quality, the Transparent Atten-\ntion (Bapna et al., 2018) for the convergence of\ndeep encoders and the document-level Transformer\n1We open source our implementation at https://\ngithub.com/anoidgit/Transformer.\n(Zhang et al., 2018b). Neutron also supports more\nefﬁcient training scheduler (dynamic sampling and\nreview mechanism) proposed by Wang et al. (2018),\nand the other advanced features.\nWe will introduce features of the Neutron in the\nnext section, its design in the third section, perfor-\nmance in the fourth section, followed by related\nwork and conclusion.\n2 Features\nNeutron supports a wide range of features for re-\nsearch purpose.\n2.1 Basic Support.\nBasic Features. We support a wide range of fun-\ndamental approaches which are usually used in MT,\nincluding: beam search of batches, length penalty\nin beam search and ensemble of models.\nGradient Accumulation. The batch size of the\nTransformer affects its optimization and perfor-\nmance, we implement gradient accumulation which\naccumulates gradients of smaller batches to achieve\ntheoretically unlimited batch size for the Trans-\nformer.\nMulti-GPU Parallelization. The default multi-\nGPU parallelization model of the pyTorch (Paszke\net al., 2019) automatically synchronizes parame-\nters during the forward pass and collects gradi-\nents after backpropagation. However, when work-\ning with the gradient accumulation, this results\nin redundant communication between GPUs, and\nwe provide a new implementation of the paral-\nlelization model which requires explicit calls to\ndistribute parameters and to accumulate gradients\nacross GPUs but avoids the redundant communi-\ncation and signiﬁcantly accelerates the multi-GPU\ncase. Our multi-GPU parallelization model also\nsupports multi-GPU decoding which are normally\nnot implemented in the other libraries.\narXiv:1903.07402v2  [cs.CL]  20 Mar 2020\nLabel Smoothing Loss. We support the label\nsmoothing loss (Szegedy et al., 2016) applied in the\nTransformer which optimizes the KL-divergence\nrather than the perplexity.\n2.2 Models.\nTwo Computation Orders. The ofﬁcial imple-\nmentation of the Transformer (Vaswani et al., 2018)\nuses a different computation order than the pro-\nposed Transformer (Vaswani et al., 2017), which\nleads to differences in the performance and the\nconvergence (Xu et al., 2019), we supported both\ncomputation orders in the Neutron.\nAverage Attention. Zhang et al. (2018a) pro-\npose to use the average attention layer instead of the\nself attention network in the decoder to accelerate\nthe decoding, and we support their approach.\nTransparent Attention. We implement the\nTransparent Attention mechanism (Bapna et al.,\n2018) which ensures the convergence of deep\nTransformer encoders.\nHierarchical Layer Aggregation Dou et al.\n(2018) propose to hierarchically aggregate all lay-\ners instead of using only outputs of the last layer,\nwe implement the model which leads to the best\nperformance in their paper.\nRNMT Decoder. We support the recurrent de-\ncoder proposed by Chen et al. (2018) which may\nlead to better performance than the Transformer\ndecoder.\nSentential Context. Wang et al. (2019) pro-\npose a model which exploits sentential context in\nthe Transformer, we implement the model which\nachieves the best performance in their paper.\nDocument-level Transformer. Zhang et al.\n(2018b) propose a document-level Transformer\nmodel to utilize inter-sentence contexts, we imple-\nment their approach as a baseline for context-aware\nNMT.\n2.3 Advanced Features.\nLipschitz Constrained Parameter Initialization.\nXu et al. (2019) propose the Lipschitz Constrained\nParameter Initialization to ensure the convergence\nof deep Transformers which also leads to slight\nbut consistent improvements in BLEU. It is im-\nplemented as the default parameter initialization\napproach.\nReducing Optimization Difﬁculty. We propose\nthat the biases of linear transformations which\nproject the multi-head attention results and the hid-\nden activation of position-wise feed-forward net-\nworks to residual connections are redundant, as the\nlayer normalization (Ba et al., 2016) adds the bias\nfor the input of the next layer. We suggest that\nremoving redundant biases can reduce the compu-\ntation costs with small acceleration, and may ease\nthe optimization of models.\nDynamic Sentence Sampling. Wang et al.\n(2018) propose to dynamically sample data which\nreduces more losses during the training, and we\nimplement their approach in the project.\nActivation Functions. The Transformer uses the\nReLU activation function, but we also provide the\nother activation functions, like GeLU and Swish\n(Ramachandran et al., 2017).\nOptimizers. In addition to the Adam optimizer\n(Kingma and Ba, 2015) used by the Transformer,\nwe also support the other optimizers like: RAdam\n(Liu et al., 2019), Lookahead (Zhang et al., 2019)\nand their combination.\n2.4 Data Cleaning.\nNormally, parallel data for training an MT sys-\ntem are not collected directly from translators at\nsentence level. Some of the corpus are crawled\nfrom the internet, and automatic sentence align-\nment tools are applied to extract sentence-level\ntranslation pairs. As a result, there might be some\nwrong translations in the parallel data, and we pro-\nvide some tools to clean data sets. Removing those\ndirty data will reduce the size of data and the cor-\nresponding vocabulary size, and normally leads to\nfaster training with better quality in practice.\n2.4.1 Max Keeper.\nParallel data are usually combination of several\ncorpus, and those corpus may contain some com-\nmon sentence pairs, which will be redundant after\nconcatenation. In addition to that, alignment tools\nmay wrongly align a same source sentence into\nseveral translations in different contexts, especially\nfor those short sentences. We collect all sentences\nand their translations, and only save those trans-\nlations with highest frequency. We also replace\npotential repeated blanks or tabular into a single\nblank during cleaning to normalize the data.\n2.4.2 Cleaning with Vocabulary.\nThere are some sentence pairs which are mean-\ningless or even not belonging to language pairs\nresearching on. V ocabulary based cleaning is sup-\nported for this case. It ﬁrst collects the vocabulary\nand counts frequencies of tokens on training set,\nand then ﬁlter training set with a hyper parameter\nnamed vratio, vratio tokens of full vocabulary with\nleast frequencies will be regarded as rare words,\nand if the percentage of rared tokens in a sentence\nis higher than 1.0 - vratio, the sentence is unlikely\nto be part of the language pair, and will be removed.\n2.4.3 Cleaning with Length Ratios.\nLength ratio is likely to be used in MT data process-\ning, since there are some wrongly aligned sentence\npairs in the training data which have abnormally\nlarge length ratios. This work provides enhanced\nsupport with sub-word units (Sennrich et al., 2016),\nthough cleaning on only tokenized text is also sup-\nported.\nAssume a sentence contains nsub tokens after\nBPE processing, nsubadd tokens are additionally\nproduced by BPE seperation, nsep tokens of origi-\nnal tokenized sentence which has ntok tokens are\nsegmented by BPE. Following ratios are deﬁned at\nmonolingual level:\ncratio= nsubadd/nsub (1)\nbratio= nsub/ntok (2)\nsratio= nsep/ntok (3)\nAssume a source sentence contains nsubsrc sub-\nword tokens and nsrc tokens before applying BPE,\nnsubtgt and ntgt correspondingly for its translation,\nwe deﬁne two bilingual ratios:\nuratio= max(nsubsrc,nsubtgt)\nmin(nsubsrc,nsubtgt) (4)\noratio= max(nsrc,ntgt)\nmin(nsrc,ntgt) (5)\nThe reason why we cleaning the training set with\nratios related to sub-word units is that those rare\nwords in dirty sentence pairs are likely to be seg-\nmented into many sub-word units, which would\nsigniﬁcantly increase those ratios.\nWe provide tools to calculate above ratios with\nvalidation set, which are good and safe choices for\ndata cleaning.\n2.5 Additional Tools.\nAveraging Models. The Transformer averages\nseveral checkpoints before evaluating, and we sup-\nport this function which loads parameters of several\ncheckpoints and save averaged parameters to a new\nmodel ﬁle.\nRanking. A ranking tool is provided to rank data\nsets with a pre-trained model, per-token loss will\nbe calculated efﬁciently. This tool can be employed\nfor data cleaning, domain adapted data selection or\nin the evaluation of linguistic phenomena.\nWeb Server. We provide a simple translation\nweb server with REST API support besides the\ntranslating scripts, which we think may be helpful\nfor integrating trained models into the other MT\nbased applications.\nConversion to C Libraries. Neutron has a con-\nverting tool based on Cython 2 which converts\npython implementations of core modules and func-\ntions into C codes and compile them into loadable\nC libraries, which may bring little additional per-\nformance and make it easier to put the Neutron into\npractice.\nForbidden Indexes for the Shared Vocabulary.\nIn practice scenario, there might be some tokens\nwhich only appear in the source side when a shared\nvocabulary is adopted, and these tokens which will\nnever appear in the target side will still get a small\nsmoothing probability in the loss function. We\nprovide a tool to extract those indexes and save\nit into a ﬁle which can be loaded to prevent the\neffects of those tokens on the decoder and the label\nsmoothing loss.\n3 Design\nIn this section, we will introduce the design of the\nNeutron.\nScripts. We provide scripts for processing train-\ning data and testing under “scripts/”, and the train-\ning and decoding scripts are “train.py” and “pre-\ndict.py”.\nBasic Modules. We implement basic modules\nunder “modules/”, including the multi-head at-\ntention network, the positional embedding, the\nposition-wise feed-forward network, the average\nattention network, etc.\n2https://cython.org/\nBLEU Training Speed Decoding Speed\nVaswani et al. (2017) 27.3\nNeutron 28.07 21562.98 68.25\nTable 1: Performance and Speed. Training speed and decoding speed are measured by the number of target tokens\nper second and the number of sentences per second.\nLoss. We support the label smoothing loss with\n“loss.py”.\nLearning Rate Scheduler. We implement the\nlearning rate of the Transformer (Vaswani et al.,\n2017) in “lrsch.py”.\nParallelization. Our multi-GPU parallelization\nmodel is implemented under “parallel/”.\nSupporting Functions. We implement basic\nfunctions for training, decoding and data process-\ning such as: freezing / unfreezing parameters of\nmodels, padding list of tensors to same size on\nassigned dimension under “utils/”.\nTransformer and its Variants We put the im-\nplementation of Transformer models under “Trans-\nformer/”.\nOptimizers. The implementation of optional op-\ntimizers can be found under “optm/”.\nTools. All tools supporting data processing, trans-\nlating, etc. are implemented under “tools”.\n4 Performance\nTo compare with Vaswani et al. (2017), we tested\nour implementation on the WMT 14 English to\nGerman news translation task following the settings\nof Vaswani et al. (2017).\nWe applied joint Byte-Pair Encoding (BPE)\n(Sennrich et al., 2016) with 32kmerge operations\nand 8 as the vocabulary threshold for the BPE to\naddress the unknown word issue. We only kept\nsentences with a maximum of 256 sub-word tokens\nfor training. The training set was randomly shuf-\nﬂed in every training epoch. The concatenation\nof newstest 2012 and newstest 2013 was used for\nvalidation and newstest 2014 as the test set.\nThe number of warm-up steps was set to 8k3,\nand each training batch contained at least25ktarget\ntokens. We trained the model on 2 GTX 1080 Ti\n3https://github.com/tensorflow/\ntensor2tensor/blob/v1.15.4/\ntensor2tensor/models/transformer.py#\nL1818.\nGPUs, and performed decoding on 1 of them. We\nused a dropout of 0.1. We used the Transformer\nBase setting (Vaswani et al., 2017), and the model\nwas trained for 100ktraining steps. We employed\na label smoothing (Szegedy et al., 2016) value of\n0.1. We used the Adam optimizer (Kingma and Ba,\n2015) with 0.9, 0.98 and 10−9 as β1, β2 and ϵ.\nWe used a beam size of 4 without length\npenalty for decoding, and evaluated tokenized case-\nsensitive BLEU 4 with the averaged model of the\nlast 5 checkpoints saved with an interval of 1,500\ntraining steps (Vaswani et al., 2017). Results are\nshown in Table 1.\nTable 1 shows that our Neutron implementation\nof the Transformer surpasses Vaswani et al. (2017),\nand we suggest our work is helpful for establishing\na higher baseline.\n5 Related work\nSutskever et al. (2014); Bahdanau et al. (2014); Lu-\nong et al. (2015); Gehring et al. (2017); Vaswani\net al. (2017) and many other researchers proposed\nvarious kinds of neural machine translation (NMT)\nmodels, corresponding to these work, there are\nmany implementations open sourced by researchers\nlike: Klein et al. (2017); Vaswani et al. (2018);\nHieber et al. (2017); Zhang et al. (2017). These\nimplementations greatly help NMT researches and\nproductive applications. Their work also provides\nvaluable experience for us to implement the Neu-\ntron.\n6 Conclusion\nWe focus on the Transformer and its variants for\nNMT, implement the Neutron based on the pyTorch\nwhich can achieve competitive performance and\nprovides several additional features for NMT. We\nintroduce its features and performance in this paper.\n4https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ngeneric/multi-bleu.perl.\nAcknowledgments\nHongfei XU acknowledges the support of China\nScholarship Council ([2018]3101, 201807040056).\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nAnkur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and\nYonghui Wu. 2018. Training deeper neural ma-\nchine translation models with transparent attention.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3028–3033. Association for Computational Linguis-\ntics.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The best of both worlds: Combining recent\nadvances in neural machine translation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 76–86, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nZi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi,\nand Tong Zhang. 2018. Exploiting deep represen-\ntations for neural machine translation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4253–4262,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1243–1252, International\nConvention Centre, Sydney, Australia. PMLR.\nFelix Hieber, Tobias Domhan, Michael Denkowski,\nDavid Vilar, Artem Sokolov, Ann Clifton, and Matt\nPost. 2017. Sockeye: A toolkit for neural machine\ntranslation. arXiv preprint arXiv:1712.05690.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nG. Klein, Y . Kim, Y . Deng, J. Senellart, and A. M. Rush.\n2017. OpenNMT: Open-Source Toolkit for Neural\nMachine Translation. ArXiv e-prints.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2019. On the variance of the adaptive learning rate\nand beyond.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1412–1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nPrajit Ramachandran, Barret Zoph, and Quoc V . Le.\n2017. Searching for activation functions.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nC. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and\nZ. Wojna. 2016. Rethinking the inception architec-\nture for computer vision. In 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 2818–2826.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N. Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, Ryan Sepassi, Noam Shazeer, and Jakob\nUszkoreit. 2018. Tensor2tensor for neural machine\ntranslation. CoRR, abs/1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nRui Wang, Masao Utiyama, and Eiichiro Sumita. 2018.\nDynamic sentence sampling for efﬁcient training of\nneural machine translation. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers),\npages 298–304, Melbourne, Australia. Association\nfor Computational Linguistics.\nXing Wang, Zhaopeng Tu, Longyue Wang, and Shum-\ning Shi. 2019. Exploiting sentential context for\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6197–6203, Florence,\nItaly. Association for Computational Linguistics.\nHongfei Xu, Qiuhui Liu, Josef van Genabith, and\nJingyi Zhang. 2019. Why deep transformers are dif-\nﬁcult to converge? from computation order to lips-\nchitz restricted parameter initialization.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018a. Ac-\ncelerating neural transformer via an average atten-\ntion network. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 1789–\n1798, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nJiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yong\nCheng, Maosong Sun, Huanbo Luan, and Yang Liu.\n2017. Thumt: An open source toolkit for neural ma-\nchine translation. arXiv preprint arXiv:1706.06415.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018b.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 533–542, Brussels, Bel-\ngium. Association for Computational Linguistics.\nMichael Zhang, James Lucas, Jimmy Ba, and Geof-\nfrey E Hinton. 2019. Lookahead optimizer: k steps\nforward, 1 step back. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 9593–9604. Curran Asso-\nciates, Inc.",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.8485046625137329
    },
    {
      "name": "Transformer",
      "score": 0.8124974370002747
    },
    {
      "name": "Computer science",
      "score": 0.6398252844810486
    },
    {
      "name": "Machine translation",
      "score": 0.4961749017238617
    },
    {
      "name": "Translation (biology)",
      "score": 0.4628888964653015
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3898899555206299
    },
    {
      "name": "Programming language",
      "score": 0.1693248152732849
    },
    {
      "name": "Engineering",
      "score": 0.15229862928390503
    },
    {
      "name": "Chemistry",
      "score": 0.14330896735191345
    },
    {
      "name": "Electrical engineering",
      "score": 0.13316610455513
    },
    {
      "name": "Voltage",
      "score": 0.11216646432876587
    },
    {
      "name": "Gene",
      "score": 0.072980135679245
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ]
}