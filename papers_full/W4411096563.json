{
  "title": "A large language model improves clinicians’ diagnostic performance in complex critical illness cases",
  "url": "https://openalex.org/W4411096563",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2121189835",
      "name": "Xintong Wu",
      "affiliations": [
        "Chengdu Third People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2106566374",
      "name": "Yu Huang",
      "affiliations": [
        "Chengdu Third People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2103517109",
      "name": "Qing He",
      "affiliations": [
        "Chengdu Third People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2121189835",
      "name": "Xintong Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106566374",
      "name": "Yu Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103517109",
      "name": "Qing He",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4396977150",
    "https://openalex.org/W4392034542",
    "https://openalex.org/W4387242094",
    "https://openalex.org/W4401418782",
    "https://openalex.org/W2771445325",
    "https://openalex.org/W4407341686",
    "https://openalex.org/W4401445967",
    "https://openalex.org/W4406549374",
    "https://openalex.org/W4406936679",
    "https://openalex.org/W4406152263",
    "https://openalex.org/W2013473976",
    "https://openalex.org/W4396757695",
    "https://openalex.org/W4385647263",
    "https://openalex.org/W4386710076",
    "https://openalex.org/W4401171954",
    "https://openalex.org/W4408063601",
    "https://openalex.org/W4383301639",
    "https://openalex.org/W4408226143",
    "https://openalex.org/W4406892975",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4353015365",
    "https://openalex.org/W4406371336",
    "https://openalex.org/W4404759704",
    "https://openalex.org/W4407882344",
    "https://openalex.org/W4403839497",
    "https://openalex.org/W2029907219",
    "https://openalex.org/W4409695426",
    "https://openalex.org/W4409695450"
  ],
  "abstract": "For diagnostically difficult critical illness cases, DeepSeek-R1 generates high-quality information, achieves reasonable diagnostic accuracy, and significantly improves residents' diagnostic accuracy and efficiency. Reasoning models are suggested to be promising diagnostic adjuncts in intensive care units.",
  "full_text": "Wu et al. Critical Care          (2025) 29:230  \nhttps://doi.org/10.1186/s13054-025-05468-7\nRESEARCH Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if \nyou modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or \nparts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by- nc- nd/4. 0/.\nCritical Care\nA large language model improves clinicians’ \ndiagnostic performance in complex critical \nillness cases\nXintong Wu1†, Yu Huang1*† and Qing He1 \nAbstract \nBackground Large language models (LLMs) have demonstrated potential in assisting clinical decision-making. How-\never, studies evaluating LLMs’ diagnostic performance on complex critical illness cases are lacking. We aimed to assess \nthe diagnostic accuracy and response quality of an artificial intelligence (AI) model, and evaluate its potential benefits \nin assisting critical care residents with differential diagnosis of complex cases.\nMethods This prospective comparative study collected challenging critical illness cases from the literature. Critical \ncare residents from tertiary teaching hospitals were recruited and randomly assigned to non-AI-assisted physician \nand AI-assisted physician groups. We selected a reasoning model, DeepSeek-R1, for our study. We evaluated the mod-\nel’s response quality using Likert scales, and we compared the diagnostic accuracy and efficiency between groups.\nResults A total of 48 cases were included. Thirty-two critical care residents were recruited, with 16 residents assigned \nto each group. Each resident handled an average of 3 cases. DeepSeek-R1’s responses received median Likert \ngrades of 4.0 (IQR 4.0–5.0; 95% CI 4.0–4.5) for completeness, 5.0 (IQR 4.0–5.0; 95% CI 4.5–5.0) for clarity, and 5.0 (IQR \n4.0–5.0; 95% CI 4.0–5.0) for usefulness. The AI model’s top diagnosis accuracy was 60% (29/48; 95% CI 0.456–0.729), \nwith a median differential diagnosis quality score of 5.0 (IQR 4.0–5.0; 95% CI 4.5–5.0). Top diagnosis accuracy was 27% \n(13/48; 95% CI 0.146–0.396) in the non-AI-assisted physician group versus 58% (28/48; 95% CI 0.438–0.729) in the AI-\nassisted physician group. Median differential quality scores were 3.0 (IQR 0–5.0; 95% CI 2.0–4.0) without and 5.0 (IQR \n3.0–5.0; 95% CI 3.0–5.0) with AI assistance. The AI model showed higher diagnostic accuracy than residents, and AI \nassistance significantly improved residents’ accuracy. The residents’ diagnostic time significantly decreased with AI \nassistance (median, 972 s; IQR 570–1320; 95% CI 675–1200) versus without (median, 1920 s; IQR 1320–2640; 95% CI \n1710–2370).\nConclusions For diagnostically difficult critical illness cases, DeepSeek-R1 generates high-quality information, \nachieves reasonable diagnostic accuracy, and significantly improves residents’ diagnostic accuracy and efficiency. \nReasoning models are suggested to be promising diagnostic adjuncts in intensive care units.\nKeywords Generative artificial intelligence, Reasoning models, Critical care, Diagnostic dilemmas\nIntroduction\nLarge language models (LLMs), a subclass of artificial \nintelligence (AI), have become increasingly prevalent in \nhealthcare applications, including medical education, \nresearch, and clinical care [1, 2]. Healthcare profession -\nals have shown increasing interest in assessing the role \n†Xintong Wu and Yu Huang contributed equally to this work.\n*Correspondence:\nYu Huang\nhuangyu0120er@126.com\n1 Department of Intensive Care Medicine, Affiliated Hospital of Southwest \nJiaotong University, The Third People’s Hospital of Chengdu, Chengdu, \nSichuan, China\nPage 2 of 9Wu et al. Critical Care          (2025) 29:230 \nof LLM chatbots in clinical practice [1, 2]. Recent stud -\nies demonstrated that LLMs without specialized training \ncould pass all three stages of the United States Medical \nLicensing Exam (USMLE) [3]. Furthermore, LLMs have \ndemonstrated the capacity to generate high-quality, \nempathetic, and readable responses to patient questions \ncompared to physicians [4–6], and can generate largely \naccurate information for diverse medical queries devel -\noped by physicians across multiple specialties [7, 8]. \nThese findings suggest LLMs may assist physicians with \nmore complex clinical decision-making, such as differen -\ntial diagnosis in challenging cases.\nDiagnostic dilemmas commonly occur in critically ill \npatients due to the complexity and diversity of clinical pres-\nentations, making differential diagnosis particularly chal -\nlenging as it requires comprehensive knowledge and the \nability to integrate complex information [9 ]. In intensive \ncare units (ICUs), rapid and accurate diagnosis is crucial for \nmaking timely, effective decisions to improve patient out-\ncomes [9]. A recent study demonstrated that LLMs such as \nChatGPT-4o and ChatGPT-4o-mini achieved high accuracy \nand consistency in answering critical care medicine ques-\ntions at a European examination level [10]. However, this \nstudy focused on examination-level questions which might \nnot fully reflect the complexity and depth of understand -\ning required in real-world clinical practice. Another study \nfound that both ChatGPT-3.5 and ChatGPT-4.0 generated \nhallucinations (misinformation delivered with high confi-\ndence) and lacked consistency when asked the same ques-\ntion multiple times across 50 core critical care topics [11]. \nTherefore, a knowledge gap exists regarding the effective-\nness of such LLMs in aiding differential diagnosis for com-\nplex ICU cases.\nReasoning models represent a significant evolution of \ntraditional LLMs, standing out for their ability to address \ncomplex problems through structured, sequential think -\ning processes [12]. DeepSeek-R1, a reasoning model \nreleased in January 2025 by DeepSeek, is an open-source \nmodel based on reinforcement learning techniques [13]. \nIt has become the fastest-growing consumer application \nand demonstrates potential for greater utility in complex \ncritical care scenarios compared to traditional LLMs. In \nthe present study, we evaluated the diagnostic perfor -\nmance of DeepSeek-R1 on complex critical illness cases, \ncompared the diagnostic accuracy and efficiency of criti -\ncal care physicians with and without DeepSeek-R1 assis -\ntance for these cases, to assess the reasoning model’s \npotential benefits in these scenarios.\nMethods\nWe conducted this prospective comparative study in \nMarch 2025. As this study was based on published case \nreports from medical journals, ethics board approval was \nnot required due to the lack of involvement of patient \ndata. We followed the TRIPOD-LLM guideline for \nreporting [14].\nCase selection\nWe searched for cases of critical illness published after \nDecember 2023 (the date of DeepSeek-R1’s training) \nusing the following method: We conducted a compre -\nhensive literature search in PubMed using the combi -\nnation of the keywords “case report, ” “case, ” “diagnosis, ” \n“differential diagnosis, ” “diagnostic challenge, ” “intensive \ncare units, ” “critical illness, ” “multiple organ failure, ” “res-\npiratory failure, ” “shock, ” “hypoxia, ” “respiratory insuf -\nficiency, ” “renal insufficiency, ” “dyspnea, ” “critical illness, ” \nand “clinical reasoning. ” We also reviewed similar arti -\ncles from relevant journals or publications. Cases that \nwere challenging to diagnose and involved a complexity \nof manifestations and processes were included. In the \nabsence of objective criteria to define case complex -\nity in the relevant literature, the eligibility of cases was \ndetermined by consensus between two authors (XW and \nYH). Typical cases that met the eligibility criteria were \ncase challenges  from the  New England Journal of Medi -\ncine  (NEJM). These cases are diagnostic dilemmas and \nhave been used for medical education of complex diag -\nnostic reasoning. We included recent published cases to \nensure that the AI model had not been trained on these \ncases beforehand. We excluded cases on management \nreasoning, non-critical cases, and cases without final \ndiagnoses, as determined by consensus between two \nauthors (XW and YH).\nAI model and prompting\nWe selected DeepSeek-R1, a reasoning model released in \nJanuary 2025 and last trained in December 2023, for the \npresent study. The website version was used, with a total \nparams of 671B, a maximal token length of 128 K and a \ndefault temperature of 0.6. Each case’s clinical summary, \nincluding the patient’s history, physical examination find-\nings, relevant investigation results and clinical course, \nwas iteratively transformed into a standard prompt: “ Act \nas an attending physician. A summary of the patient’s \nclinical information will be presented, and you will use \nthis information to predict the diagnosis. Describe the \ndifferential diagnoses and the rationale for each, listing \nthe most likely diagnosis at the top: [case information]. ” \nEach case was run in an independent chat session to pre -\nvent the model from applying any “learning” to subse -\nquent cases. No retrieval-augmented generation (RAG) \nwas incorporated in the model search. The prompt was \ndeveloped to encourage the AI model to generate con -\nsistent and inclusive responses, using another dataset of \nPage 3 of 9\nWu et al. Critical Care          (2025) 29:230 \n \ncases that were published in 2023 and met our inclusion \ncriteria.\nParticipants and study groups\nAs residents are most likely to require external diagnostic \nassistance, we recruited critical care medicine residents \nfrom six tertiary teaching hospitals. All residents were \nblinded to the study design and were unaware that their \nresponses would be compared to AI models’ responses. \nThey were randomly assigned to either non-AI-assisted \nphysician or AI-assisted physician group using stratified \nrandomization based on years of critical care experience. \nAll participants confirmed no prior exposure to the study \ncases. In the non-AI-assisted physician group, cases were \nrandomly allocated to residents who were allowed to use \ntraditional resources (PubMed, UpToDate, Medscape, \netc.). The residents were provided the same descriptions \nthat were input into the AI model and were asked to make \na differential diagnosis list with the most likely diagno -\nsis at the top for each case. In the AI-assisted physician \ngroup, the same cases were allocated randomly to the \nresidents, and the residents received both case descrip -\ntions and corresponding AI model outputs. Traditional \nresources were also allowed in the AI-assisted physician \ngroup. Search results were limited to pre-December 2023 \npublications when traditional resources were used.\nOutcomes\nWe evaluated the diagnostic accuracy according to top \ndiagnosis accuracy and differential quality score (a pre -\nviously published ordinal 5-point rating system based on \naccuracy and usefulness. A score of 5 is given for a dif -\nferential including the exact diagnosis; 4: the suggestions \nincluded something very close, but not exact; 3: the sug -\ngestions included something closely related that might \nhave been helpful; 2: the suggestions included some -\nthing related, but unlikely to be helpful; 0: no suggestions \nwere close to the target diagnosis) [15]. DeepSeek-R1’s \nresponse quality was further evaluated in terms of com -\npleteness (the degree to which the information was \nmedically comprehensive and offered broad diagnostic \npossibilities and additional details: 1 = very incomplete; \n2 = incomplete; 3 = moderately complete; 4 = complete; \n5 = very complete), clarity (conciseness of presenta -\ntion and overall legibility: 1 = very unclear; 2 = unclear; \n3 = somewhat clear; 4 = clear; 5 = very clear), and useful -\nness (the degree to which the response provided helpful \nand effective decision-critical information: 1 = not use-\nful; 2 = slightly useful; 3 = moderately useful; 4 = useful; \n5 = highly useful) using 5-point Likert scales. The out -\ncomes were independently assessed by two authors (XW \nand YH), with disagreements resolved by a third author \n(QH). In the assessment of each outcome, a consensus \nscore was determined for each case in each condition (AI \nmodel, non-AI-assisted physicians, and AI-assisted phy -\nsicians). Diagnostic time was recorded and analyzed for \nthe evaluation of diagnostic efficiency.\nWe performed a consistency check to assess the relia -\nbility of DeepSeek-R1. To assess, 16 cases were randomly \nselected from the included cases and were presented to \nthe AI model 3 times in independent conversations. The \nmodel’s response was considered consistent if it provided \nthe same top diagnosis and a similar differential diagno -\nsis list for 3 repetitions, while did not introduce or omit \nmajor information that would affect the response quality. \nThe response for each case was assessed by two authors \n(XW and YH) independently, and was considered con -\nsistent if both the two authors scored consistent.\nStatistical analysis\nThe top diagnosis accuracy was reported as percent -\nages and analyzed using χ 2 test with Bonferroni correc -\ntion. Differential quality scores and Likert scores were \nreported using medians and interquartile ranges (IQR). \nKruskal–Wallis H test was used to determine differences \nin differential quality scores between groups with Dunn’ \ns post-hoc pairwise comparison, and Mann–Whitney \nU test was used to identify difference in diagnostic time \nbetween groups. The 95% confidence intervals (CI) for \nthe top diagnosis accuracy, differential quality scores, \nLikert scores and diagnostic time were calculated. Inter -\nrater reliability was evaluated using intraclass correlation \ncoefficient (ICC) calculated by a mean-rating, absolute \nagreement, two-way mixed-effects model. A p value \nof < 0.05 was considered statistically significant. Analyses \nand visualizations used Python 3.12 (SciPy 1.13.0, numpy \n2.1.3, pandas 2.2.1, matplotlib 3.8.3, seaborn 0.13.2, \npingouin 0.5.4).\nResults\nA total of 48 cases were included in the study, the final \nincluded cases were published in the NEJM, Mayo Clinic \nProceedings, CHEST, Neurology et  al. These selected \ncases were characterized by their diagnostic complexity \nand clinical relevance. Thirty-two critical care residents \nwere recruited, with 16 residents assigned to each group. \nEach resident handled an average of 3 cases, with a maxi -\nmum of 5 and minimum of 1. Participants’ critical care \nexperience (beyond trainee) ranged from 1 to 3 years.\nA representative example of AI-generated response \nand differential diagnoses from each group is shown in \nTable 1. DeepSeek-R1 demonstrated strong performance \nacross completeness, clarity, and usefulness, with median \nLikert scores of 4.0 (IQR 4.0–5.0; 95% CI 4.0–4.5), 5.0 \n(IQR 4.0–5.0; 95% CI 4.5–5.0), and 5.0 (IQR 4.0–5.0; 95% \nPage 4 of 9Wu et al. Critical Care          (2025) 29:230 \nTable 1 A representative example of AI-generated response and differential diagnoses from each group, along with subsequent scores\nFinal \ndiagnosis\nCase information DeepSeek-R1’s responses Differential \ndiagnoses \nfrom non-\nAI-assisted \nphysicians\nDifferential \ndiagnoses \nfrom \nAI-assisted \nphysicians\nLikert scores for \nAI’s responses \n(Completeness/\nClarity/\nUsefulness)a\nDifferential \nquality scores \n(DeepSeek-R1/\nnon-AI-assisted \nphysicians/\nAI-assisted \nphysicians)b\nIcteric lep-\ntospirosis\nA 37-year-old male patient presented with fever, myalgia, \njaundice, and hypoxemia, following a 9-day progressive illness \ncharacterized initially by severe fatigue, exhaustion, and gener-\nalized weakness that rendered him bedridden. Seven days prior \nto admission, he developed high-grade fever (39.4 °C/102.9°F) \naccompanied by headache, limb pain and stiffness, anorexia, \nand nausea without abdominal pain, vomiting, or diarrhea. \nWhile his fever and headache resolved after 2 days, his myalgia \nworsened with the onset of dark-colored urine. Initial evalua-\ntion at an emergency clinic 3 days before admission revealed \nnegative respiratory viral panel testing (severe acute respiratory \nsyndrome coronavirus 2, respiratory syncytial virus, influenza \nA/B), and he was managed conservatively with hydration \nadvice. Due to persistent symptoms and emerging jaundice \nnoted 2 days later, he was referred to the emergency depart-\nment where initial assessment showed normal oxygen satura-\ntion (100% on room air) but revealed leukocytosis (white blood \ncell count 17,900/μL), thrombocytopenia (34,000/μL), acute \nkidney injury (creatinine 3.0 mg/dL), and marked hyperbili-\nrubinemia (total bilirubin 15.9 mg/dL, direct > 10 mg/dL). His \ncondition deteriorated rapidly by hospital day 3 with hemo-\ndynamic instability (blood pressure 71/60 mmHg), worsening \nhypoxemia, progressive renal dysfunction (creatinine 3.8 mg/\ndL), anemia (hemoglobin 10.3 g/dL from 15.7 g/dL), and esca-\nlating hyperbilirubinemia (20.9 mg/dL), necessitating transfer \nfor higher-level care. Upon transfer, the patient reported persis-\ntent severe fatigue and dyspnea without respiratory symptoms, \nwith notable exposure history including recent woodland/river-\nine activities with insect bites and a partner’s similar self-limited \nfebrile illness. Physical examination demonstrated tachypnea \n(42/min) with accessory muscle use, hypoxemia (91% on 4L \noxygen), marked icterus, and bilateral crackles without hepato-\nsplenomegaly or rash. Laboratory findings showed progressive \nleukocytosis (21,700/μL with neutrophilia), thrombocytopenia \n(67,000/μL), worsening renal impairment (creatinine 3.36 mg/\ndL, blood urea nitrogen 59 mg/dL), and profound conju-\ngated hyperbilirubinemia (total/direct bilirubin 26.1 mg/dL) \nwith mild transaminitis (aspartate aminotransferase 44 U/L, \nalanine aminotransferase 41 U/L). Imaging revealed bilateral \npulmonary consolidations with tree-in-bud pattern, medias-\ntinal lymphadenopathy, and characteristic hepatic “starry sky” \nappearance without biliary obstruction. The patient was initi-\nated on continuous positive airway pressure respiratory support \nand broad-spectrum antimicrobial therapy including intrave-\nnous vancomycin, cefepime, and oral doxycycline\nMost Likely Diagnosis: Leptospirosis (Weil’s Syndrome)\nDiagnostic Basis:\nEpidemiological Exposure: Recent activities near rivers and wooded \nareas with insect bites, consistent with typical transmission routes \nof Leptospira (skin/mucosal contact with contaminated water)\nClinical Triad:\nFever and Myalgia: Early high fever, severe muscle pain (especially \nin the calves, though not specified), and headache\nJaundice and Hepatic Injury: Markedly elevated direct biliru-\nbin (> 10 mg/dL) with mild aspartate aminotransferase/alanine \naminotransferase elevation (suggesting cholestasis predominant \nover hepatocellular damage)\nRenal Injury: Acute kidney injury (creatinine 3.36 mg/dL), positive \nurinary bilirubin\nMultisystem Involvement:\nRespiratory: Hypoxemia, tachypnea, computed tomography findings \nof diffuse pulmonary consolidations and ground-glass opacities (pos-\nsible pulmonary hemorrhage, a severe complication of leptospirosis)\nHematologic: Thrombocytopenia (34,000/μL), leukocytosis with neu-\ntrophilia\nPartner’s Similar Symptoms: Suggests shared exposure (e.g., contami-\nnated water or environment)\nImaging: “Starry sky” sign on hepatic ultrasound (periportal edema/\ninflammation), consistent with leptospiral hepatitis\nDifferential Diagnoses and Rationale:\n1. Hantavirus Pulmonary Syndrome\nSupporting Evidence:\nZoonotic transmission (exposure to rodent excreta, aligns with wood-\nland activity)\nRapidly progressive respiratory failure (hypoxemia, bilateral infiltrates), \nthrombocytopenia, and acute kidney injury\nContradictions:\nHantavirus typically lacks severe jaundice or direct hyperbilirubinemia\nSimultaneous illness in the partner is rare unless both were exposed \nto rodents\n2. Rickettsial Infections (e.g., Query fever or Typhus)\nSupporting Evidence:\nFever, headache, myalgia, and thrombocytopenia\nQuery fever can cause hepatitis (granulomatous) and pneumonitis\nContradictions:\nQuery fever rarely presents with severe jaundice or acute renal failure\nAbsence of typical rash (e.g., eschar in scrub typhus or maculopapu-\nlar rash in epidemic typhus)\n3. Viral Hepatitis (e.g., Hepatitis E Virus)\nSupporting Evidence:\nLeptospi-\nrosis;\nSepsis \n(caused \nby hyper-\nvirulent \nKlebsiella \npneu-\nmoniae \nand Escheri-\nchia coli);\nANCA-\nassociated \nvasculitis;\nAllergic \ndisease;\nLymphoma\nLeptospi-\nrosis\n4/5/5 5/5/5\nPage 5 of 9\nWu et al. Critical Care          (2025) 29:230 \n Table 1 (continued)\nFinal \ndiagnosis\nCase information DeepSeek-R1’s responses Differential \ndiagnoses \nfrom non-\nAI-assisted \nphysicians\nDifferential \ndiagnoses \nfrom \nAI-assisted \nphysicians\nLikert scores for \nAI’s responses \n(Completeness/\nClarity/\nUsefulness)a\nDifferential \nquality scores \n(DeepSeek-R1/\nnon-AI-assisted \nphysicians/\nAI-assisted \nphysicians)b\nJaundice, elevated liver enzymes, and cholestasis\nHepatitis E is waterborne, potentially linked to environmental \nexposure\nContradictions:\nViral hepatitis typically features marked alanine aminotransferase/\naspartate aminotransferase elevation (often > 1000 U/L), whereas this \ncase shows only mild transaminitis\nRespiratory involvement and thrombocytopenia are uncommon\n4. Malaria (Plasmodium falciparum Malaria)\nSupporting Evidence:\nFever, hemolytic jaundice (indirect bilirubin predominance), \nand thrombocytopenia\nAcute kidney injury (e.g., blackwater fever)\nContradictions:\nNo travel history to endemic regions, and no reported cyclical fever \nor positive blood smears\nPulmonary infiltrates and respiratory failure are atypical\n5. Autoimmune Disorders (e.g., systemic lupus erythematosus \nor Autoimmune Hepatitis)\nSupporting Evidence:\nMultisystem involvement (hepatic, renal, pulmonary)\nContradictions:\nNo prior autoimmune history or serologic markers (e.g., antinuclear \nantibodies, anti-smooth muscle antibodies)\nAcute onset and exposure history favor infectious etiology\nAI Artificial intelligence\na The completeness scale was a 5-point Likert scale (with 1 indicating very incomplete; 2, incomplete; 3, moderately complete; 4, complete; and 5, very complete), the clarity scale was a 5-point Likert scale (with 1 \nindicating very unclear; 2, unclear; 3, somewhat clear; 4, clear; and 5, very clear), and the usefulness scale was a 5-point Likert scale (with 1 indicating not beneficial; 2, slightly beneficial; 3, moderately beneficial; 4, \nbeneficial; 5, highly beneficial). These three dimensions were used to evaluate the performance of AI responses\nb The differential quality score (a previously published ordinal 5-point rating system based on accuracy and usefulness. A score of 5 is given for a differential including the exact diagnosis; 4, the suggestions included some-\nthing very close, but not exact; 3, the suggestions included something closely related that might have been helpful; 2, the suggestions included something related, but unlikely to be helpful; 0, no suggestions were close \nto the target diagnosis) was use to evaluated diagnostic accuracy of each group\nPage 6 of 9Wu et al. Critical Care          (2025) 29:230 \nCI 4.0–5.0), respectively (Fig.  1). The AI model’s median \nresponse time was 48 s (IQR 39–57; 95% CI 45–50).\nDeepSeek-R1 achieved 60% (29/48; 95% CI 0.456–\n0.729) top diagnosis accuracy, including the final diag -\nnosis in its differential for 68% (33/48) of cases, and the \nmedian differential quality score was 5.0 (IQR 4.0–5.0; \n95% CI 4.5–5.0) (Table  2). Without AI assistance, the \nresidents’ top diagnoses agreed with final diagnoses \nin 27% (13/48; 95% CI 0.146–0.396) of cases, and the \nmedian differential quality score was 3.0 (IQR 0–5.0; \n95% CI 2.0–4.0) (Table  2). DeepSeek-R1 demonstrated \na significantly higher diagnostic accuracy compared \nwith the residents. With AI assistance, the residents’ \ntop diagnosis accuracy improved to 58% (28/48; 95% \nCI 0.438–0.729), while their median differential qual -\nity score improved to 5.0 (IQR 3.0–5.0; 95% CI 3.0–5.0), \nshowing the AI model’s significant enhancement of the \nresidents’ diagnostic accuracy (Table  2). No significant \ndifference was found in diagnostic accuracy between \nDeepSeek-R1 and AI-assisted residents (Table  2). \nThe two primary scorers (XW and YH) demonstrated \nexcellent interrater reliability for differential quality \nscores (ICC, 0.960; 95% CI 0.946–0.971), and moder -\nate reliability for Likert scores of completeness (ICC, \n0.679; 95% CI 0.496–0.804), clarity (ICC, 0.640; 95% \nCI 0.442–0.778) and usefulness (ICC, 0.565; 95% CI \n0.343–0.728), respectively. In the assessment of consist -\nency of the DeepSeek-R1, 13 out of 16 responses were \ngraded consistent, demonstrating the reliability of the \nAI model with the standard prompt.\nFurthermore, the residents’ median diagnostic time \ndecreased significantly from 1920  s (IQR 1320–2640; \n95% CI 1710–2370) to 972  s (IQR 570–1320; 95% CI \n675–1200) with AI assistance (p  = 0.000001).\nFig. 1 Completeness, clarity, usefulness ratings for DeepSeek-R1’s responses to cases. Bar charts were shown for the final consensus ratings by 3 \nevaluators using 5-point Likert scores. A The completeness rating was shown. B The clarity rating was shown. C The usefulness rating was shown\nTable 2 Comparisons of top diagnostic accuracy and differential quality score between groups\nAI Artificial intelligence, IQR Interquartile ranges, CI Confidence interval\na p values for top diagnosis accuracy were calculated using Chi-square test, with Bonferroni-corrected pairwise comparisons, p values for differential quality score were \nobtained from Kruskal–Wallis H test, followed by Dunn’ s post-hoc tests with Bonferroni adjustment\nb This refers to comparisons among the three groups\nDeepSeek-R1 Non-AI-assisted \nphysicians\nAI-assisted \nphysicians\np  valuea\nOverallb DeepSeek-R1 versus \nnon-AI-assisted \nphysicians\nNon-AI-assisted \nphysicians versus \nAI-assisted physicians\nTop diagnosis \naccuracy (% \n(n/N, 95% CI))\n60 (29/48, 0.456–0.729) 27 (13/48, 0.146–\n0.396)\n58 (28/48, 0.438–\n0.729)\n0.001 0.003 0.006\nDifferential \nquality score \n(median (IQR, \n95% CI))\n5.0 (4.0–5.0, 4.5–5.0) 3.0 (0–5.0, 2.0–4.0) 5.0 (3.0–5.0, 3.0–5.0) 0.004 0.005 0.036\nPage 7 of 9\nWu et al. Critical Care          (2025) 29:230 \n \nDiscussion\nIn the present study, we assessed the diagnostic perfor -\nmance of a reasoning model, DeepSeek-R1, on challeng -\ning diagnostic dilemmas in critical illness. Furthermore, \nwe prospectively evaluated the effects of DeepSeek-R1’s \nassistance on improving the diagnostic accuracy and effi -\nciency of critical care residents in such complex cases \nusing a randomized design. Our findings demonstrate \nthat for differential diagnosis of complex cases in criti -\ncal care, DeepSeek-R1 can generate complete, clear, and \nclinically useful information, achieve reasonable accu -\nracy, and improve critical care residents’ diagnostic accu-\nracy and efficiency.\nRecent cross-sectional studies have shown that LLMs \nsuch as ChatGPT-4 and Google Bard could generate \nquality and empathetic responses to patient questions \nfrom either public online resources or physician-devel -\noped medical queries in various specialties such as oph -\nthalmology, oncology and anesthesia, with performance \ncomparable to physician responses [4–6, 16]. In the con -\ntext of diagnostic tasks, recent studies have demonstrated \ncomparable diagnostic performance for LLMs in cases \ninvolving retina, glaucoma, neurodegenerative disorders \nand general internal medicine [6, 17–19]. In the context \nof treatment tasks, a recent study has demonstrated the \npotential of AI models, including ChatGPT-4o, Gemini \n2.0 and DeepSeek V3, to align with expert surgical rec -\nommendations for surgical diseases [20]. However, the \nurgency and complexity in these settings are far from \ncomparable to clinical practice in ICUs. To the best of \nour knowledge, no prospective studies evaluating the \neffects of LLM assistance on diagnostic accuracy and effi-\nciency have been reported in the literature. In the present \nstudy, all included cases represented challenging diag -\nnostic dilemmas in ICUs, and the recruited physicians \nwere critical care residents who face critically ill patients \nupon ICU admission and manage these particularly com -\nplex challenges initially without multidisciplinary team \nconsultation. These study settings are more likely to cap -\nture the complexity of real-world critical care scenarios. \nMoreover, our randomized design enabled direct com -\nparison of residents’ diagnostic performance with versus \nwithout AI assistance under identical conditions. There -\nfore, our findings provide evidence with higher qual -\nity for the potential benefits of reasoning AI models as \npromising tools to assist critical care residents in making \naccurate and efficient diagnoses for complex critical ill -\nness cases.\nIn the present study, traditional online resources such \nas PubMed and UpToDate were permitted in both non-\nAI-assisted physician and AI-assisted physician groups. \nWhen comparing AI performance with that of human \nphysicians, prior studies often allowed physicians to \nobtain answers without using references. However, in \ncontemporary practice, traditional online tools have \nbecome basic daily adjuncts [21]. Therefore, evaluat -\ning the AI model’s benefits under these conditions is \nwarranted. Our study demonstrated that when tradi -\ntional tools were available to both groups, the AI model \nimproved diagnostic accuracy and significantly reduced \ndiagnostic time, suggesting that reasoning AI models \nmay serve as highly beneficial tools beyond traditional \nresources for differential diagnosis of complex cases.\nIn this study, the AI model outperformed human phy -\nsicians in diagnostic accuracy. In addition to generating \naccurate differential diagnoses, the AI model provided \ncomprehensive, clear, and clinically useful responses. \nDeepSeek-R1, a recently released advanced reasoning \nLLM, was selected for this study. Recent studies have \nshown a growing interest in leveraging LLM tools in \nclinical settings. ChatGPT, Google Bard/Gemini and \nMeta Llama accounted for the majority of the evalua -\ntions, however, studies evaluating the performance of \nreasoning models in clinical medicine are lacking [22]. \nIn the present study, DeepSeek-R1 demonstrated diag -\nnostic capabilities superior to those of AI models (e.g., \nChatGPT-3.5, ChatGPT-4, and Google Bard) in prior \nstudies that also evaluated LLMs’ accuracy in complex \ndiagnostic challenges [17, 20]. DeepSeek-R1 employs \na reinforcement learning-based pretraining approach, \nenabling advanced reasoning development, including its \nself-reflection capability, which enables autonomous veri-\nfication and optimization of logical reasoning, thereby \nenhancing performance on complex tasks [13]. These \nfeatures may prove particularly valuable for complex clin-\nical queries. Similarly, a recently developed phenotype-\nbased natural language-processing model was shown to \nbe more accurate in the diagnosis of rare diseases than \nphysician experts [23]. Furthermore, as an entirely open-\nsource model, DeepSeek-R1 is particularly advantageous \nfor resource-limited healthcare settings, including ICUs, \nwhere free and adaptable solutions are required.\nA key challenge in applying LLMs to healthcare is their \ntendency to generate “hallucinations” [24, 25]. LLMs may \nconfidently produce misinformation or exhibit cognitive \nbiases, underscoring the risks of unsupervised reliance \non these tools [11, 26, 27]. In our study, we employed \nthe AI model as an adjunct for critical care residents. \nAlthough the AI model achieved higher diagnostic accu -\nracy than clinicians in this study, its role as an assistive \ntool—rather than a standalone decision-maker—may \nhelp mitigate hallucination-related risks in clinical prac -\ntice. However, this approach also introduces its own chal-\nlenge. When human physicians apply the model’s output \nPage 8 of 9Wu et al. Critical Care          (2025) 29:230 \nas one cue within their broader judgment processes \nand identify too many exceptions to the model’s output, \nhuman-AI collaboration can sometimes result in subop -\ntimal outcomes [28]. A recent study highlighted the het -\nerogeneity of the effects of human-AI collaboration, the \nperformance of human-AI collaboration loss when AI \noutperformed humans alone, and in tasks that involved \nmaking decisions [29]. These scenarios resembled those \nin the present study, wherein the AI model showed a \nhigher top diagnosis accuracy compared to AI-assisted \nphysicians (no significant difference) when outperformed \nstandalone human physicians. Thus, future researches \nare recommended to explore for promising ways for \nimproving human-AI collaboration.\nOur study has several limitations. First, diagnostic \naccuracy heavily depends on the quality and specific -\nity of clinical information [30], and all clinical data were \nprovided by researchers. Potential subjectivity in out -\ncome measures and the exclusion of certain diagnostic \ndata (e.g., medical images) may lead to inconsistency to \nclinical practice. Additionally, in the present study, the \ncompleteness and accuracy of information entered into \nthe AI chatbot serves as the basis of what the AI chat -\nbot would use to assist with diagnosis. However, in \nclinical practice, case information gathered from his -\ntory taking, physical examination, or other activities \nmight be incomplete or incorrect. Second, AI responses \nwere generated using standardized prompts and deliv -\nered by researchers, diverging from real-world scenarios \nwhere physicians interact dynamically with AI via per -\nsonal devices. Third, all participants were critical care \nresidents; no participants with differing training levels \n(e.g., attending physicians) were included. Diagnostic \nperformance with AI assistance may vary across train -\ning levels. Fourth, we did not evaluate other reasoning \nmodels (e.g., ChatGPT-4, ChatGPT-o1 and DeepSeek \nV3), which may differ in accuracy and clinical utility \ncompared to DeepSeek-R1. Noteworthy, the findings of \nrecent benchmark studies evaluating the diagnostic per -\nformance of LLMs demonstrated that reasoning models \nsuch as DeepSeek-R1 and ChatGPT-o1 exhibited clearly \nsuperior performance compared to GPT-4 and GPT-3.5, \nand DeepSeek-R1 performed non-inferiorly compared to \nproprietary reasoning models such as ChatGPT-o1 and \nsuperiorly compared to another proprietary reasoning \nmodel, Gem2FTE [31, 32]. These findings highlight the \npotential of DeepSeek-R1 in clinical decision-making in \nthe diagnostic context. Finally, assuming a power of 0.8 \nand an alpha of 0.05, a sample size of 35 was required for \nthe comparisons of diagnostic performance between AI \nmodel and non-AI-assisted physicians and between AI-\nassisted physicians and non-AI-assisted physicians. How-\never, in order to draw meaningful comparisons between \nAI model and AI-assisted physicians, a sample size of 227 \nwas required for appropriateness. The sample size of this \nstudy is rather small for comparing diagnostic perfor -\nmance between AI model and AI-assisted physicians.\nConclusions\nAbove all, our findings suggest that reasoning mod -\nels, such as DeepSeek-R1, are promising assistive tools \nfor critical care residents facing challenging differen -\ntial diagnoses. These findings warrant further research \nwith larger sample sizes to evaluate the clinical adoption \npotential of reasoning models in real-world critical care \ndecision-making.\nAbbreviations\nLLM  Large language model\nAI  Artificial intelligence\nUSMLE  The United States Medical Licensing Exam\nICU  Intensive care unit\nRAG   Retrieval-augmented generation\nIQR  Interquartile ranges\nCI  Confidence interval\nICC  Intraclass correlation coefficient\nAcknowledgements\nWe thank Dr. Ying Qiao and Dr. Qianyun Cai for their efforts on recruitment of \nparticipants and data collection. We thank all the physicians who participated \nin the case studies.\nAuthor contributions\nYH contributed to the study concept and design, XW performed the initial lit-\nerature research and recruited the participants. XW, YH, and QH analyzed and \ninterpreted the data. XW contributed to the statistical analysis and graphing. \nYH drafted the initial manuscript, XW contributed to the manuscript sections \nrelated to the results obtained. QH and YH was responsible for the revision \nof the manuscript for important intellectual content. All authors read and \napproved the final manuscript.\nFunding\nNo financial support.\nAvailability of data and materials\nThe datasets used and/or analysed during the current study are available from \nthe corresponding author on reasonable request.\nDeclarations\nEthics approval and consent to participate\nNot applicable as no patients were involved.\nConsent for publication\nNot applicable as no patients were involved.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 25 April 2025   Accepted: 24 May 2025\nReferences\n 1. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. \nLarge language models in medicine. Nat Med. 2023;29(8):1930–40.\nPage 9 of 9\nWu et al. Critical Care          (2025) 29:230 \n \n 2. Lee P , Bubeck S, Petro J. Benefits, limits, and risks of GPT-4 as an AI chatbot \nfor medicine. N Engl J Med. 2023;388(13):1233–9.\n 3. Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on \nUSMLE: potential for AI-assisted medical education using large language \nmodels. PLoS Digit Health. 2023;2(2):e0000198.\n 4. Ayers JW, Poliak A, Dredze M, et al. Comparing physician and artificial \nintelligence chatbot responses to patient questions posted to a public \nsocial media forum. JAMA Intern Med. 2023;183(6):589–96.\n 5. Chen D, Parsa R, Hope A, et al. Physician and artificial intelligence \nchatbot responses to cancer questions from social media. JAMA Oncol. \n2024;10(7):956–60.\n 6. Huang AS, Hirabayashi K, Barna L, Parikh D, Pasquale LR. Assessment of \na Large language model’s responses to questions and cases about glau-\ncoma and retina management. JAMA Ophthalmol. 2024;142(4):371–5.\n 7. Goodman RS, Patrinely JR, Stone CA Jr, et al. Accuracy and reliabil-\nity of chatbot responses to physician questions. JAMA Netw Open. \n2023;6(10):e2336483.\n 8. Anastasio MK, Peters P , Foote J, Melamed A, Modesitt SC, Musa F, Rossi E, \nAlbright BB, Havrilesky LJ, Moss HA. The doc versus the bot: a pilot study \nto assess the quality and accuracy of physician and chatbot responses \nto clinical questions in gynecologic oncology. Gynecol Oncol Rep. \n2024;55:101477.\n 9. James FR, Power N, Laha S. Decision-making in intensive care medicine—\na review. J Intensive Care Soc. 2018;19(3):247–58.\n 10. Workum JD, Volkers BWS, van de Sande D, et al. Comparative evaluation \nand performance of large language models on expert level critical care \nquestions: a benchmark study. Crit Care. 2025;29(1):72.\n 11. Balta KY, Javidan AP , Walser E, Arntfield R, Prager R. Evaluating the \nappropriateness, consistency, and readability of ChatGPT in critical care \nrecommendations. J Intensive Care Med. 2025;40(2):184–90.\n 12. Xu F, Hao Q, Zong Z, et al. Towards large reasoning models: a survey of \nreinforced reasoning with large language models. arXiv. Preprint posted \nonline January 25, 2025. https:// doi. org/ 10. 48550/ arXiv. 2501. 09686\n 13. Mondillo G, Colosimo S, Perrotta A, Frattolillo V, Masino M. Comparative \nevaluation of advanced AI reasoning models in pediatric clinical decision \nsupport: ChatGPT O1 vs. DeepSeek-R1. medRxiv. Preprint posted online \nJanuary 27, 2025. https:// doi. org/ 10. 1101/ 2025. 01. 27. 25321 169\n 14. Gallifant J, Afshar M, Ameen S, et al. The TRIPOD-LLM reporting guideline \nfor studies using large language models. Nat Med. 2025;31:60–9.\n 15. Bond WF, Schwartz LM, Weaver KR, Levick D, Giuliano M, Graber ML. \nDifferential diagnosis generators: an evaluation of currently available \ncomputer programs. J Gen Intern Med. 2012;27(2):213–9.\n 16. Nguyen TP , Carvalho B, Sukhdeo H, et al. Comparison of artificial intel-\nligence large language model chatbots in answering frequently asked \nquestions in anaesthesia. BJA Open. 2024;10:100280.\n 17. Koga S, Martin NB, Dickson DW. Evaluating the performance of large \nlanguage models: ChatGPT and Google Bard in generating differential \ndiagnoses in clinicopathological conferences of neurodegenerative \ndisorders. Brain Pathol. 2024;34(3):e13207.\n 18. Hirosawa T, Kawamura R, Harada Y, et al. ChatGPT-generated differential \ndiagnosis lists for complex case-derived clinical vignettes: diagnostic \naccuracy evaluation. JMIR Med Inform. 2023;11:e48808.\n 19. Hadi A, Tran E, Nagarajan B, Kirpalani A. Evaluation of ChatGPT as \na diagnostic tool for medical learners and clinicians. PLoS ONE. \n2024;19(7):e0307383.\n 20. Seth I, Marcaccini G, Lim K, et al. Management of Dupuytren’s disease: a \nmulti-centric comparative analysis between experienced hand surgeons \nversus artificial intelligence. Diagnostics (Basel). 2025;15(5):587.\n 21. Kulkarni PA, Singh H. Artificial intelligence in clinical diagnosis: opportuni-\nties, challenges, and hype. JAMA. 2023;330(4):317–8.\n 22. Shool S, Adimi S, Saboori Amleshi R, et al. A systematic review of large \nlanguage model (LLM) evaluations in clinical medicine. BMC Med Inform \nDecis Mak. 2025;25(1):117.\n 23. Mao X, Huang Y, Jin Y, et al. A phenotype-based AI pipeline outperforms \nhuman experts in differentially diagnosing rare diseases using EHRs. NPJ \nDigit Med. 2025;8(1):68.\n 24. Sallam M. ChatGPT utility in healthcare education, research, and practice: \nsystematic review on the promising perspectives and valid concerns. \nHealthcare. 2023;11(6):887.\n 25. Azamfirei R, Kudchadkar SR, Fackler J. Large language models and the \nperils of their hallucinations. Crit Care. 2023;27(1):120.\n 26. Griot M, Hemptinne C, Vanderdonckt J, Yuksel D. Large language models \nlack essential metacognition for reliable medical reasoning. Nat Com-\nmun. 2025;16(1):642.\n 27. Wang J, Redelmeier DA. Cognitive biases and artificial intelligence. NEJM \nAI. 2024;1(12):AIcs2400639.\n 28. Peringa IP , Cox EGM, Wiersema R, van der Horst ICC, Meijer RR, Koeze J. \nHuman judgment error in the intensive care unit: a perspective on bias \nand noise. Crit Care. 2025;29(1):86.\n 29. Vaccaro M, Almaatouq A, Malone T. When combinations of humans and \nAI are useful: a systematic review and meta-analysis. Nat Hum Behav. \n2024;8(12):2293–303.\n 30. Singh H, Giardina TD, Meyer AN, Forjuoh SN, Reis MD, Thomas EJ. Types \nand origins of diagnostic errors in primary care settings. JAMA Intern \nMed. 2013;173(6):418–25.\n 31. Sandmann S, Hegselmann S, Fujarski M, et al. Benchmark evaluation of \nDeepSeek large language models in clinical decision-making. Nat Med. \n2025. https:// doi. org/ 10. 1038/ s41591- 025- 03727-2.\n 32. Tordjman M, Liu Z, Yuce M, et al. Comparative benchmarking of the \nDeepSeek large language model on medical tasks and clinical reasoning. \nNat Med. 2025. https:// doi. org/ 10. 1038/ s41591- 025- 03726-3.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.90013587474823
    },
    {
      "name": "Critical illness",
      "score": 0.6820589900016785
    },
    {
      "name": "Intensive care medicine",
      "score": 0.6063973903656006
    },
    {
      "name": "Critically ill",
      "score": 0.4751850664615631
    },
    {
      "name": "Medical emergency",
      "score": 0.32311034202575684
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4800084",
      "name": "Southwest Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210093384",
      "name": "Third People's Hospital of Chengdu",
      "country": "CN"
    }
  ]
}