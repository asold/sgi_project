{
  "title": "Incremental Text-to-Speech Synthesis Using Pseudo Lookahead With Large Pretrained Language Model",
  "url": "https://openalex.org/W3114579299",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2914495207",
      "name": "Saeki, Takaaki",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2575089264",
      "name": "Takamichi, Shinnosuke",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2147484941",
      "name": "Saruwatari, Hiroshi",
      "affiliations": [
        "The University of Tokyo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963609956",
    "https://openalex.org/W2903739847",
    "https://openalex.org/W2972895078",
    "https://openalex.org/W3083224111",
    "https://openalex.org/W3048023795",
    "https://openalex.org/W2963300588",
    "https://openalex.org/W6750489868",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W6917585676",
    "https://openalex.org/W6770069697",
    "https://openalex.org/W6782348501",
    "https://openalex.org/W2964243274",
    "https://openalex.org/W2129142580",
    "https://openalex.org/W6611766843",
    "https://openalex.org/W6785521828",
    "https://openalex.org/W6675380101",
    "https://openalex.org/W6679262520",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2526425061",
    "https://openalex.org/W6636915900",
    "https://openalex.org/W2962780374",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2141708418",
    "https://openalex.org/W1975163393"
  ],
  "abstract": "This letter presents an incremental text-to-speech (TTS) method that performs synthesis in small linguistic units while maintaining the naturalness of output speech. Incremental TTS is generally subject to a trade-off between latency and synthetic speech quality. It is challenging to produce high-quality speech with a low-latency setup that does not make much use of an unobserved future sentence (hereafter, \"lookahead\"). To resolve this issue, we propose an incremental TTS method that uses a pseudo lookahead generated with a language model to take the future contextual information into account without increasing latency. Our method can be regarded as imitating a human's incremental reading and uses pretrained GPT2, which accounts for the large-scale linguistic knowledge, for the lookahead generation. Evaluation results show that our method 1) achieves higher speech quality than the method taking only observed information into account and 2) achieves a speech quality equivalent to waiting for the future context observation.",
  "full_text": "IEEE SIGNAL PROCESSING LETTERS, VOL. 28, 2021 857\nIncremental Text-to-Speech Synthesis Using Pseudo\nLookahead With Large Pretrained Language Model\nTakaaki Saeki , Student Member , IEEE, Shinnosuke Takamichi , and Hiroshi Saruwatari , Member , IEEE\nAbstract—This letter presents an incremental text-to-speech\n(TTS) method that performs synthesis in small linguistic units while\nmaintaining the naturalness of output speech. Incremental TTS\nis generally subject to a trade-off between latency and synthetic\nspeech quality. It is challenging to produce high-quality speech with\na low-latency setup that does not make much use of an unobserved\nfuture sentence (hereafter, “lookahead”). To resolve this issue, we\npropose an incremental TTS method that uses a pseudo lookahead\ngenerated with a language model to take the future contextual\ninformation into account without increasing latency. Our method\ncan be regarded as imitating a human’s incremental reading and\nuses pretrained GPT2, which accounts for the large-scale linguistic\nknowledge, for the lookahead generation. Evaluation results show\nthat our method 1) achieves higher speech quality than the method\ntaking only observed information into account and 2) achieves\na speech quality equivalent to waiting for the future context\nobservation.\nIndex Terms—Incremental text-to-speech synthesis, end-to-end\ntext-to-speech synthesis, language model, contextual embedding.\nI. INTRODUCTION\nS\nIMULTANEOUS speech-to-speech translation (SST) [1],\n[2] enables interactive speech communication without\nlanguage barriers. It consists of three modules that perform\nincremental processing: automatic speech recognition (ASR),\nmachine translation (MT), and text-to-speech synthesis (TTS).\nRecent advances in deep learning have made remarkable\nprogress in the quality of TTS, as well as in ASR and MT.\nIt is now possible to artiﬁcially generate high-quality speech\ncomparable to human natural speech by modeling time-series\ninformation in the whole sentence with deep neural networks.\nIn contrast to the typical sentence-level TTS frameworks, in-\ncremental TTS requires handling small linguistic segments at\nthe level of a few words, which makes it more challenging.\nTherefore, incremental TTS suffers from a trade-off between\nthe naturalness of the output speech and latency in the synthe-\nsis. Low-latency incremental TTS should process the current\nsegment using only an observed segment, rather than waiting\nfor an unobserved future segment ahead of the current segment\nManuscript received February 27, 2021; revised April 10, 2021; accepted\nApril 14, 2021. Date of publication April 16, 2021; date of current version\nMay 10, 2021. This work was supported in part by JSPS KAKENHI under\nGrants 17H06101, 19H01116, and MIC/SCOPE #182103104. The associate\neditor coordinating the review of this manuscript and approving it for publication\nwas Mr. Ville M. Hautamäki.(Corresponding author: Takaaki Saeki.)\nThe authors are with the Graduate School of Information Science\nand Technology, University of Tokyo, Tokyo 113-8656, Japan (e-mail:\ntakaaki_saeki@ipc.i.u-tokyo.ac.jp; shinnosuke_takamichi@ipc.i.u-tokyo.ac.jp;\nhiroshi_saruwatari@ipc.i.u-tokyo.ac.jp).\nDigital Object Identiﬁer 10.1109/LSP.2021.3073869\n(hereafter, “lookahead”). However, this makes it difﬁcult to\noutput a speech segment that leads naturally to the lookahead,\ncausing synthetic speech quality to deteriorate.\nThis letter proposes a method to perform high-quality and\nlow-latency synthesis using a pseudo lookahead generated with\na large-scale pretrained language model. When we humans\nneed to read an unknown sentence sequentially (e.g., reading\na new book), we can predict future information on the basis\nof the observed segment. Then we can read out the segment\nso that it is naturally connected to the past observed and pre-\ndicted contexts. To computationally imitate this mechanism,\nour method predicts the lookahead using pretrained GPT2 [3],\nwhich is trained on datasets from various domains. It can en-\nhance synthetic speech quality without increasing the latency by\nusing the pseudo lookahead as the future contextual information\ninstead of waiting for the ground-truth lookahead. This method\nis effective and applicable to other incremental TTS frameworks\n(e.g., preﬁx-to-preﬁx decoding [4]). The model architecture is a\nTacotron2 [5]-based end-to-end TTS model, which incorporates\na contextual embedding network [6] that considers the past\nobserved and the future unobserved contexts, and consistently\ntrains the entire model to achieve the high-quality synthesis of\nthe current segment. We also propose a language model-guided\nﬁne-tuning method to estimate the contextual embedding that is\nmore suitable for the predicted sentence with GPT2. Evaluation\nresults show that our method 1) achieves higher speech quality\nthan the method taking only observed information into account\nand 2) achieves a speech quality equivalent to waiting for the\nfuture context observation.\nII. RELATED WORKS\nIn recent years, the quality of TTS has dramatically improved\nwith the shift from cascade statistical parametric speech synthe-\nsis [7]–[9] to end-to-end TTS [5], [10], [11], which generates a\nmel-spectrogram from text with a single model. Several studies\nhave focused on incremental TTS with end-to-end architec-\ntures [4], [12]–[14]. The ﬁrst attempt at end-to-end neural incre-\nmental TTS [12] uses a Tacotron [10]-based model to achieve\nhigh-quality synthesis. This method is a segment-level incre-\nmental TTS just like ours, and it has difﬁculty generating natural\nspeech segments because the synthesis process is isolated from\nthe past observed and unobserved future contexts, as we discuss\nin Section IV. Ma et al. proposed a preﬁx-to-preﬁx framework\nfor incremental TTS with a lookahead-k policy that waits to\nobserve futurek words and synthesizes a current segment [4].\nIn contrast to this framework, our work focuses on instantly\nsynthesizing speech from a current segment without waiting for\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n858 IEEE SIGNAL PROCESSING LETTERS, VOL. 28, 2021\nthe lookahead. The TTS model we use has a contextual em-\nbedding network designed in the prior work for sentence-level\nTTS [6]. This method aims at parallel synthesis by focusing on\nintonational phrases, and both pre- and post-phrases of an input\nphrase can be used for the inference process, while we can only\nuse the pre-segment of the current segment.\nIII. METHOD\nThis section describes our proposed incremental TTS method.\nSection III-A presents an inference algorithm, which integrates a\nsentence generation with GPT2. Section III-B describes a model\narchitecture for generating a speech segment considering both\npast observed and future unobserved contexts. Section III-C\npresents a language model-guided ﬁne-tuning method. Finally,\nSection III-D provides a detailed analysis of the proposed\nmethod.\nA. Incremental Synthesis With Pseudo Lookahead\nFirst, we deﬁne the synthetic unit for incremental TTS as\n“the current segment,” which consists ofN words. In the time\nstep t, w1 : Nt = w1,..., wn,..., wNt represents “the ob-\nserved segment” and the lastN-word sequencewN(t−1)+1:Nt =\nwN(t−1)+1,..., wNt is a current segment, where wn de-\nnotes the n-th word. Furthermore, we deﬁne w1:N(t−1) =\nw1,..., wN(t−1) as “the past observed segment” to distinguish\nbetween the observed segments with and without the current\nsegment. GPT2 [3] is an auto-regressive language model that\nassumes the probability distribution of anM-word sequence\nw1:M can be decomposed into the product of conditional prob-\nabilities, as\np(w1:M)=\nM∏\nm=1\np(wm|w1:m−1). (1)\nIn accordance with this modeling, we can obtain a future\nL-word sequenceˆwNt+1:Nt+L = ˆwNt+1,..., ˆwNt+L by sam-\npling from the probability distributionp(wNt+1:Nt+L|w1:Nt),\nwhere ˆwNt+1:Nt+L becomes the “pseudo lookahead” used for\nthe future contextual information of incremental TTS. Since the\nTTS model uses a character or phoneme sequence instead of\nthe word sequence wn, we deﬁne the character or phoneme\nsequence corresponding town as xn. Deﬁning the TTS model\nas G(·), the output mel-spectrogramyt can be obtained by\nyt = G\n(\nxN(t−1)+1:Nt|x1:N(t−1),ˆxNt+1:Nt+L,θG\n)\n, (2)\nwhere θG denotes parameters of G(·). When we deﬁnezt as\nthe waveform synthesized from mel-spectrogramyt, waveform\nsynthesis is performed using WaveGlow [15] vocoderV(·) as:\nzt = V (yt|θV ) , (3)\nwhere θV denotes parameters ofV(·). We incrementally synthe-\nsize the output speech by concatenatingzt to the audio waveform\nz1:t−1 that has been output so far.\nThe naturalness of synthesized speech and latency in syn-\nthesis highly depend onN. Although it is desirable to makeN\nsmaller to develop low-latency incremental TTS, the naturalness\nof output speech degrades asN decreases. In our preliminary\nexperiment, we found that our proposed method often outputs\nunintelligible speech withN =1 , as reported in Yanagitaet al.’s\nFig. 1. Model architecture of proposed method with contextual embedding\nnetwork to consider past observed segment and pseudo lookahead.\nwork [12]. Therefore, we setN to two for our evaluation in this\nstudy.\nB. TTS Model Architecture\nThe incremental TTS model we use is a Tacotron2 [5]-\nbased end-to-end model conditioned on both past observed and\nunobserved future segments. It has a module for contextual\nembedding [6], as shown in Fig. 1. Character or phoneme\nsequences of the current segmentxN(t−1)+1:Nt, the past ob-\nserved segment x1:N(t−1) and the lookaheadˆxNt+1:Nt+L are\nsent to the Tacotron2 encoder. When we deﬁneh1:N(t−1) and\nˆhNt+1:Nt+L as hidden states of the past observed segment and\nthe pseudo lookahead, respectively,h1:N(t−1) and ˆhNt+1:Nt+L\nare separately sent to additional encoders, which stack six 2-D\nconvolutional layers and a gated recurrent unit (GRU) layer. The\noutputs are concatenated and sent to a token attention layer based\non a global style token [16]. We deﬁne the network that estimates\ncontextual embedding from the output of the Tacotron2 encoder\nas the “contextual embedding network” and denote it asF(·).W e\ncan obtain the contextual embedding with the pseudo lookahead\nepseudo as:\nepseudo = F(h1:N(t−1),ˆhNt+1:Nt+L). (4)\nThen epseudo is replicated and concatenated with every\nstate of hN(t−1)+1:Nt to form the input of the decoder,\nwhere hN(t−1)+1:Nt is the hidden state of the current segment.\nThe contextual encoders for the past observed and unobserved\nfuture segments share the same parameters, and we used the\nsame values for the hyperparameters of the contextual embed-\nding network as Conget al. [6]. By jointly training the con-\ntextual embedding network and the encoder-decoder network\nof Tacotron2, we attain natural speech segments by considering\nboth the past and future contexts.\nWhen we train the TTS model, we use the ground-truth sen-\ntence in the training data as the unobserved future segment. To\nextract the past observed segmentx≤N(t−1), the current segment\nxN(t−1)+1:Nt, and the unobserved future segmentxNt+1≥,w e\nuse the sliding text window [6] with a ﬁxed number of words,\nwhereas the original one uses the text window with a ﬁxed num-\nber of phrases. Finally, we extract the ground-truth waveform\ncorresponding to the current segment with forced alignment.\nSAEKI et al.: INCREMENTAL TEXT-TO-SPEECH SYNTHESIS USING PSEUDO LOOKAHEAD WITH LARGE PRETRAINED LANGUAGE MODEL 859\nC. Language Model-Guided Fine-Tuning\nAs we described in Section III-A, the lookahead prediction\nmakes use of linguistic knowledge of a large pretrained language\nmodel for incremental TTS. This method, however, results in\na mismatch between the ground-truth lookahead used during\ntraining and the pseudo lookahead during inference. In other\nwords, the TTS model cannot fully utilize the pseudo lookahead\ngenerated with GPT2 since the TTS model does not take the\nlookahead prediction into account.\nTherefore, we propose a language model-guided ﬁne-tuning\nmethod to use the pseudo lookahead for incremental TTS more\neffectively. In contrast to the training procedure described in\nSection III-B, we use the pseudo lookahead generated with\nGPT2 as the unobserved future segment during the ﬁne-tuning.\nGPT2 generates the unobserved future segments as training data\nby using the past observed segments and the current segments\nextracted with the sliding text window. Let epseudo be the\ncontextual embedding obtained by using the pseudo lookahead,\nand etruth be the contextual embedding with the ground-truth\nlookahead. Our goal is to enable the contextual embedding net-\nwork to use the pseudo lookahead for the contextual information\nto the same extent as the actual lookahead. Therefore, we add\nthe additional lossLsim to the loss for the TTS model training\nwith a weightαsim to maximize the cosine similarity between\nepseudo and etruth,a s\nαsim ·Lsim = αsim ·(1 −Sim(epseudo,etruth)) , (5)\nwhere Sim(·) denotes the cosine similarity. Then, unlike the\nTTS model training, we ﬁx the weights of both the encoder and\ndecoder networks of Tacotron2, and we only train the contextual\nembedding network. These operations help the TTS model to\nconsider the contextual information in a way that better ﬁts the\nprediction of GPT2.\nD. Discussion\nFirst, we analyze how close the pseudo lookahead generated\nwith GPT2 is to the ground-truth lookahead. For each time step\nt, we calculate the average cosine similarity between the con-\ntextual embedding obtained with the pseudo lookahead and that\nwith the ground-truth lookahead. When the cosine similarity is\nhigh, the pseudo lookahead is expected to produce an equivalent\neffect on the synthesized speech to the actual observation of the\nground-truth lookahead. Furthermore, we investigate the effect\nof the sampling strategy of GPT2. GPT2 generates a sentence by\nrandomly sampling from the distribution of the most probablek\nwords, which is called top-ksampling [17]. When we set a large\nvalue tok, GPT2 performs random sampling from various word\ncandidates. Whenk is one, GPT2 uses deterministic generation\non the basis of the maximum likelihood.\nFig. 2 shows the analysis results. Note that we used the same\nexperimental conditions as those described in Section IV-A.\nThe label “top_ k (k = K)” (K =1 ,5,10,50) denotes the\ncase where top-k sampling with k = K is used without the\nﬁne-tuning method, and “top_k (k =1 , ﬁne-tuned)” represents\nthe case where top-k sampling with k =1 is applied with\nthe ﬁne-tuning method. The label “random” denotes the case\nwithout a language model, where we used random English\nwords as the lookahead sentences. Comparing the results with\n“top_k (k = K)” and “random,” we can see that the lookahead\nFig. 2. Average cosine similarity for time stept. We 1) investigate the effect of\nk in the top-k sampling and 2) compare the cases with and without the proposed\nﬁne-tuning method fork =1 .\ngeneration with allkcases led to better scores than the “random”\ncase, demonstrating the effectiveness of the pseudo lookahead\nwith GPT2. Furthermore, we found that the contextual\nembedding obtained with the pseudo lookahead tended to\nbecome closer to the ground-truth, as the value ofk decreased.\nWe also conducted subjective evaluations on synthetic speech\nquality for this aspect and found that thek =1 case produced\noutput speech with signiﬁcantly higher naturalness than the\nk =5 and k =5 0cases. Intuitively, a large value ofk enables\ndiverse sentence generation, and a smallk produces objectively\nplausible sentences. The results suggest that we need to make the\nvalue ofksmall for incremental TTS on a regular speech corpus.\nNote that the cosine similarity fort =1 was higher than that for\nall thet =2 −5 cases even in “random”. This result indicates\nthat, when t =1 , the contextual embedding network focused\nmuch more on “the beginning of the sentence” than the future\nunobserved content. Examining “top_k (k =1 , ﬁne-tuned),”\nthe cosine similarity with the ﬁne-tuning was better fort =1\nand 2 and became lower than that in some non-ﬁne-tuning cases\nas t increased. Since the ﬁne-tuning method takes into account\nthe pseudo lookahead with GPT2 during training, it could\nestimate the contextual embedding more closely to that with\nthe ground-truth lookahead when the input segments were not\nwell observed, i.e., at the beginning of the sentence. However,\nas t increased and the segments of the original sentence came\nin, the cosine similarity with the ﬁne-tuning converged to the\nsame level as that without the ﬁne-tuning.\nIV . EXPERIMENTAL EV ALUATIONS\nA. Experimental Conditions\nWe used LJSpeech [18], a dataset consisting of 13,100 short\naudio clips of a female English speaker lasting approximately\n24 hours. We randomly selected 100 and 500 sentences from the\nentire dataset for validation and test sets, respectively, and used\nthe rest as a training set. When extracting a mel-spectrogram\nfrom each audio clip with short-time Fourier transform, we\nused 1024-sample frame size, 256-sample hop size, a Hann\nwindow function, and an 80 channel mel-ﬁlterbank at a sampling\nfrequency of 22.05 kHz. To use contextual information in the\ntraining process, we used the sliding text window described in\nSection III-B with the window length 3 and the hop size 1. As\ndescribed in Section III-A, we set the number of words in each\ninput segmentN to two in the synthesis process. When extract-\ning a waveform of each current segment, we used a Kaldi-based\n860 IEEE SIGNAL PROCESSING LETTERS, VOL. 28, 2021\nforced-alignment toolkit [19]. We used the pretrained GPT21\nand WaveGlow2 models for the evaluation. On the basis of\na preliminary experiment in which we calculated the cosine\nsimilarity values over time steps for differentL, we selected\nL =5 as the setting that produces the closest lookahead to\nusing the ground-truth future segment. When performing the\nsampling operation with GPT2, we applied top-ksampling with\nk =1 in all cases. We trained the TTS model with a batch\nsize of 160 distributed across four NVIDIA V100 GPUs for\n76 000 iterations, for which we observed the convergence in all\nthe training cases. When performing the ﬁne-tuning, we trained\nonly the contextual embedding network with a batch size of 32\non a NVIDIA Geforce GTX 1080Ti GPU for 4000 iterations,\nw h e r ew eu s e dαsim =1 0−3. We used the Adam [20] optimizer\nwith β1 =0 .9, β2 =0 .999, andϵ=1 0−6. We set a learning rate\nof 10−3 and 10−4 in the TTS model training and the ﬁne-tuning,\nrespectively, applyingL2 regularization with weight10−6.\nB. Evaluation Cases\nTo investigate the effectiveness of lookahead prediction with\nGPT2, we conducted objective and subjective evaluations by\ncomparing the following methods: (1)Ground-truth, ground-\ntruth audio clips included in the test data; (2)Full-sentence,\nsentence-level Tacotron2 model [5]; (3)Independent, where the\nTTS model synthesized a current speech segment independently\nof the contextual information [12]; (4)Unicontext, where the\nTTS model used only the past observed segment for context\nconditioning of the TTS model; (5) Bicontext, which is the\nproposed method described in Section III without the ﬁne-tuning\nmethod; (6)Bicontext (truth), where we used ground-truth test\ntranscripts for unobserved future sentences like the conventional\nlookahead-k strategy [4] that waits for observing k words;\nand (7) Bicontext (ﬁne-tuned), which applied the ﬁne-tuning\nmethod to Bicontext. Audio samples3 synthesized with these\nmethods are publicly available.\nC. Objective Evaluations\nUnlike the utterance-level TTS, incremental TTS is more\nprone to fail in synthesis and to output non-recognizable speech.\nTherefore, we measured the word error rate (WER) and char-\nacter error rate (CER), deﬁned as the word- and character-level\nLevenshtein distance [21], using the state-of-the-art ASR model\nto evaluate how natural and easy the output speech is to recognize\nas a human utterance. We used a joint-CTC Transformer-based\nmodel [22] trained on librispeech [23], which is included in\nESPnet [24]. Table I lists the results.\nFirst, both the CER and WER were vast forIndependent.\nIn some cases, the Independent did not predict the stop ﬂag\ncorrectly due to the lack of contextual information, which caused\na sluggish part in the output speech and signiﬁcantly increased\nthe insertion rate. As a result, Bicontext synthesized output\nspeech that was easier to recognize than that withIndependent.\nFurthermore, the error rates of Bicontext were lower than\nthose of Unicontext, which used only the observed context,\n1[Online]. Available: https://github.com/graykode/gpt-2-Pytorch\n2[Online]. Available: https://github.com/NVIDIA/waveglow\n3[Online]. Available: https://takaaki-saeki.github.io/itts_lm_demo/\nTABLE I\nCER, WER,AND MOS FOR EACH METHOD DESCRIBED IN SECTION IV-B\nthus demonstrating the effectiveness of the pseudo lookahead\nwith GPT2 for incremental TTS. Finally, examiningBicontext\n(ﬁne-tuned), we can see that the ﬁne-tuning method decreased\nthe error rates to a level comparable to that ofBicontext (truth),\nwhich used the test transcript for the lookahead.\nD. Subjective Evaluations\nTo evaluate the quality of output speech, we conducted a mean\nopinion score (MOS) evaluation test [25] on naturalness. Forty\nlisteners recruited through Amazon Mechanical Turk [26] par-\nticipated in the evaluation, and each listener evaluated 35 speech\nsamples, where we randomly chose ﬁve samples from the output\nutterances of the test data for each method. Table I shows the\naverage MOS scores with 95 % conﬁdence intervals.\nFirst, our proposed methods scored signiﬁcantly higher than\nIndependent, which is based on the prior work [12]. Further-\nmore, the proposed methods outperformedUnicontext, which\nconsidered only the past observed context, thus demonstrating\nthat the pseudo lookahead with GPT2 signiﬁcantly improves\nthe naturalness of synthesized speech. When we compare the\nproposed methods, Bicontext and Bicontext (ﬁne-tuned),t h e\naverage score ofBicontext (ﬁne-tuned) was higher, suggesting\nthat language model-guided ﬁne-tuning leads to more effective\npseudo lookahead generation. Finally, our proposed methods\nachieved naturalness comparable to Bicontext (truth), which\nuses the lookahead information (like the method of Maet al.[4]).\nThis result indicates that the pseudo-lookahead conditioning\nwith a language model-guided ﬁne-tuning improves the quality\nequivalently to waiting for the actual lookahead observations\nwithout increasing the latency. Note that we also conducted AB\ntests forBicontext, Bicontext (ﬁne-tuned), andBicontext (truth),\nbut we did not ﬁnd any signiﬁcant differences between them as\nin the MOS evaluation test.\nV. CONCLUSION\nIn this letter, we proposed an incremental text-to-speech\n(TTS) method using the pseudo lookahead generated with a\nlarge pretrained language model. This method synthesizes a\ncurrent speech segment while generating the unobserved future\ninformation with GPT2 instead of waiting for its actual obser-\nvation. We also proposed a language model-guided ﬁne-tuning\nmethod to use the pseudo lookahead for incremental TTS more\neffectively. Experimental results demonstrated the effectiveness\nof our methods in terms of both the synthetic speech quality\nand the latency. In future work, we will further enhance our\nmethod towards an incremental TTS with a quality equivalent\nto sentence-level TTS using only observed information.\nSAEKI et al.: INCREMENTAL TEXT-TO-SPEECH SYNTHESIS USING PSEUDO LOOKAHEAD WITH LARGE PRETRAINED LANGUAGE MODEL 861\nREFERENCES\n[1] S. Bangalore, V . K. Rangarajan Sridhar, P. Kolan, L. Golipour, and\nA. Jimenez, “Real-time incremental speech-to-speech translation of di-\nalogs,” in Proc. Conf. North Amer . Chapter Assoc. Comput. Linguist.\nHuman Lang. Technol., Montreal, Canada, Jun. 2012, pp. 437–445.\n[2] K. Sudoh et al., “Simultaneous speech-to-speech translation system with\nneural incremental ASR MT, and TTS” 2020,arXiv:2011.04845.\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019. [Online].\nAvailable: https://openai.com/blog/better-language-models/\n[4] M. Ma et al., “Incremental text-to-speech synthesis with preﬁx-to-preﬁx\nframework,” inProc. Empirical Methods Natural Lang. Process., Online,\nNov. 2020, pp. 3886–3896.\n[5] J. Shen et al., “Natural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions,” inProc. IEEE Int. Conf. Acoust., Speech Signal\nProcess., Apr. 2018, pp. 4779–4783.\n[6] Y . Cong, R. Zhang, and J. Luan, “PPSpeech: Phrase based parallel end-\nto-end TTS system,” 2020,arXiv:2008.02490.\n[7] K. Tokuda, H. Zen, and A. W. Black, “An HMM-based speech synthesis\nsystem applied to english,” inProc. IEEE Workshop Speech Synthesis,\nSanta Monica, U. S. A., Sep. 2002, pp. 227–230.\n[8] H. Zen, K. Tokuda, and A. Black, “Statistical parametric speech synthesis,”\nSpeech Commun., vol. 51, no. 11, pp. 1039–1064, 2009.\n[9] H. Zen, A. Senior, and M. Schuster, “Statistical parametric speech synthe-\nsis using deep neural networks,” inProc. IEEE Int. Conf. Acoust., Speech\nSignal Process., Canada, May 2013, pp. 7962–7966.\n[10] Y . Wanget al., “Tacotron: towards end-to-end speech synthesis,” inProc.\nINTERSPEECH, Stockholm, Sweden, Nov. 2017, pp. 4006–4010.\n[11] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, “Neural speech synthesis with\ntransformer network,” inProc. AAAI Conf. Artif. Intell., U.S.A., Jul. 2019,\npp. 6706–6713.\n[12] T. Yanagita, S. Sakti, and S. Nakamura, “Neural iTTS: Toward synthesiz-\ning speech in real-time with end-to-end neural text-to-speech framework,”\nin Proc. Speech Synthesis Workshop, Austria, Sep. 2019, pp. 183–188.\n[13] B. Stephenson, L. Besacier, L. Girin, and T. Hueber, “What the future\nbrings: Investigating the impact of lookahead for incremental neural TTS,”\nin Proc. INTERSPEECH, Online, Oct. 2020, pp. 215–219.\n[14] D. S. R. Mohan, R. Lenain, L. Foglianti, T. H. Teh, M. Staib, and A. Tor-\nresquintero, “Incremental text to speech for neural sequence-to-sequence\nmodels using reinforcement learning,” inProc. INTERSPEECH, Online,\nOct. 2020, pp. 3186–3190.\n[15] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A ﬂow-based gen-\nerative network for speech synthesis,” inProc. ICASSP, Brighton, U.K.,\nMay 2019, pp. 3617–3621.\n[16] Y . Wanget al., “Style tokens: Unsupervised style modeling, control and\ntransfer in end-to-end speech synthesis,” inProc. Int. Conf. Mach. Learn.\n(ICML), Stockholm, Sweden, Jul. 2018, pp. 5180–5189.\n[17] A. Fan, M. Lewis, and Y . Dauphin, “Hierarchical neural story genera-\ntion,” inProc. Assoc. Comput. Linguist., Melbourne, Australia, Jul. 2018,\npp. 889–898.\n[18] K. Ito and L. Johnson, “The LJ Speech Dataset,” 2017. [Online]. Available:\nhttps://keithito.com/LJ-Speech-Dataset/\n[19] R. M. Ochshorn and M. Hawkins., “Gentle: A Robust Yet Lenient Forced\nAligner Built on Kaldi,” 2017. [Online]. Available: https://lowerquality.\ncom/gentle\n[20] D. Kingma and B. Jimmy, “Adam: A method for stochastic optimization,”\n2014, arXiv:1412.6980.\n[21] V . Levenshtein, “Binary codes capable of correcting deletions, insertions\nand reversals,”Sov. Phys. Doklady, vol. 10, no. 8, pp. 707–710, Feb. 1966.\n[22] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-\nend speech recognition using multi-task learning,” inProc. IEEE Int.\nConf. Acoust., Speech Signal Process., New Orleans, U.S.A., Mar. 2017,\npp. 4835–4839.\n[23] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An\nASR corpus based on public domain audio books, ” inProc. IEEE Int. Conf.\nAcoust., Speech Signal Process., Australia, Apr. 2015, pp. 5206–5210.\n[24] S. Watanabe et al., “ESPnet: End-to-End speech processing toolkit,” in\nProc. INTERSPEECH, Hyderabad, India, Sep. 2018, pp. 2207–2211.\n[25] R. Streijl, S. Winkler, and D. Hands, “Mean opinion score (MOS) revisited:\nMethods and applications, limitations and alternatives,”Multimedia Syst.,\nvol. 22, pp. 213–227, Mar. 2016.\n[26] M. Buhrmester, T. Kwang, and S. D. Gosling, “Amazon’s mechanical\nturk: A new source of inexpensive, yet high-quality, data?,”Perspectives\nPsychol. Sci., vol. 6, no. 1, pp. 3–5, 2011.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    }
  ]
}