{
  "title": "Punctuation Restoration using Transformer Models for High-and Low-Resource Languages",
  "url": "https://openalex.org/W3106905592",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3090529816",
      "name": "Tanvirul Alam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2799468905",
      "name": "Akib Khan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228779350",
      "name": "Firoj Alam",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2147228869",
    "https://openalex.org/W2750292073",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287821920",
    "https://openalex.org/W2917668649",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3026041220",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W84643496",
    "https://openalex.org/W2514151559",
    "https://openalex.org/W2899902398",
    "https://openalex.org/W2513522215",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W40475138",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2397041827",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W3010663975",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2166116787",
    "https://openalex.org/W90695545",
    "https://openalex.org/W1626209120",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2938722449",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2575282817",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962989741",
    "https://openalex.org/W3204130541",
    "https://openalex.org/W2745826985",
    "https://openalex.org/W2972411269",
    "https://openalex.org/W3082735059",
    "https://openalex.org/W3202218022",
    "https://openalex.org/W2169187092",
    "https://openalex.org/W2937326859",
    "https://openalex.org/W2104734291",
    "https://openalex.org/W3011222885"
  ],
  "abstract": "Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this problem using different deep learning models. Recently, transformer models have proven their success in downstream NLP tasks, and these models have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For English, we obtain comparable state-of-the-art results, while for Bangla, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.",
  "full_text": "Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop on Noisy User-generated Text, pages 132–142\nOnline, Nov 19, 2020.c⃝2020 Association for Computational Linguistics\n132\nPunctuation Restoration using Transformer Models\nfor High-and Low-Resource Languages\nTanvirul Alam1 Akib Khan1 Firoj Alam2\n{tanvirul.alam, akib.khan}@bjitgroup.com, fialam@hbku.edu.qa\n1 BJIT Limited, Dhaka, Bangladesh\n2 Qatar Computing Research Institute, HBKU, Qatar\nAbstract\nPunctuation restoration is a common post-\nprocessing problem for Automatic Speech\nRecognition (ASR) systems. It is important\nto improve the readability of the transcribed\ntext for the human reader and facilitate NLP\ntasks. Current state-of-art address this prob-\nlem using different deep learning models. Re-\ncently, transformer models have proven their\nsuccess in downstream NLP tasks, and these\nmodels have been explored very little for the\npunctuation restoration problem. In this work,\nwe explore different transformer based models\nand propose an augmentation strategy for this\ntask, focusing on high-resource (English) and\nlow-resource (Bangla) languages. For English,\nwe obtain comparable state-of-the-art results,\nwhile for Bangla, it is the ﬁrst reported work,\nwhich can serve as a strong baseline for future\nwork. We have made our developed Bangla\ndataset publicly available for the research com-\nmunity.\n1 Introduction\nDue to the recent advances in deep learning meth-\nods, the accuracy of Automatic Speech Recognition\n(ASR) systems has increased signiﬁcantly (e.g.,\n3.4% WER on LibriSpeech noisy test set (Park\net al., 2020)). The improved performance of ASR\nenabled the development of voice assistants (e.g.,\nSiri, Cortana, Bixby, Alexa, and Google Assistant)\nand their wider use at the user end. Among differ-\nent components (e.g., acoustic, language model),\npre- and post-processing steps, the punctuation\nrestoration is one of the post-processing steps that\nalso needs to be dealt with to improve the readabil-\nity and utilize the transcriptions in the subsequent\nNLP applications (Jones et al., 2003; Matusov et al.,\n2007).1 This is because state-of-the-art NLP mod-\nels are mostly trained using punctuated texts (e.g.,\n1Example of downstream NLP applications include ques-\ntion answering, information extraction, named entity recogni-\ntion (Makhoul et al., 2005), text summarization, etc.\ntexts from newspaper articles, Wikipedia). Hence,\nthe lack of punctuation signiﬁcantly degrades per-\nformance. For example, there is a performance\ndifference of more than ∼10% when the model\nis trained with newspaper texts and tested with\ntranscriptions for the Named Entity Recognition\nsystem (Alam et al., 2015).\nTo address this issue, most of the earlier efforts\non the punctuation restoration task have been done\nusing lexical, acoustic, prosodic, or a combination\nof these features (Gravano et al., 2009; Levy et al.,\n2012; Zhang et al., 2013; Xu et al., 2014; Szasz´ak\nand T¨undik, 2019; Che et al., 2016a). For the punc-\ntuation restoration task, lexical features have been\nwidely used because the model can be trained using\nany punctuated text (i.e., publicly available newspa-\nper articles or content from Wikipedia) and because\nof the availability of such large-scale text. This is\na reasonable choice as developing punctuated tran-\nscribed text is a costly procedure.\nIn terms of machine learning models, condi-\ntional random ﬁeld (CRF) has been widely used\nin earlier studies (Lu and Ng, 2010; Zhang et al.,\n2013). Lately, the use of deep learning models,\nsuch as Long Short-Term Memory (LSTM), Convo-\nlutional Neural Network (CNN), and transformers\nhave also been used (Che et al., 2016b; Gale and\nParthasarathy, 2017; Zelasko et al., 2018; Wang\net al., 2018) for this task.\nThere has been a variant of transformer based\nlanguage models (e.g., BERT (Devlin et al., 2019a),\nRoBERTa (Liu et al., 2019)), which have not been\nexplored widely to address this problem. Hence,\nwe aimed to explore different architectures and ﬁne-\ntune pre-trained models for this task focusing on\nEnglish and Bangla. Punctuation restoration mod-\nels are usually trained on clean texts but used on\nnoisy ASR texts. As such, the performance may\ndegrade due to errors introduced by ASR models\nwhich are not present in the training data. We de-\nsign an augmentation strategy (see Section 4.1.2)\n133\nto address this issue. For English, we train and eval-\nuate the models using IWSLT reference and ASR\ntest datasets. We report that our proposed augmen-\ntation strategy yields a 3.8% relative improvement\nin the F1 score on ASR transcriptions for English\nand obtains state-of-the-art results. For Bangla,\nthere has not been any prior reported work for\npunctuation restoration. In addition, no resource\nhas been found. Therefore, we prepare a train-\ning dataset from a news corpus and provide strong\nbaselines for news, reference, and ASR transcrip-\ntions. To shade light in the current state-of-the-art\non punctuation restoration task, our contributions\nin this study are as follows:\n1. Explore transformer based language models\nfor the punctuation restoration task.\n2. Propose an augmentation strategy.\n3. Prepare training and evaluation datasets for\nBangla and provide strong benchmark results.\n4. We have made our source code and datasets\npublicly available.2\nWe organize the rest of the paper as follows.\nIn Section 2, we discuss recent works based on\nlexical features. We describe English and Bangla\ndatasets used in this study in Section 3. Experimen-\ntal details are provided in Section 4. We compare\nour results against other published results on the\nIWSLT dataset and provide benchmark results on\nthe Bangla dataset in Section 5. We conclude the\npaper in Section 6.\n2 Related Work\nRecent lexical features based approaches for punc-\ntuation restoration tasks are predominantly based\non deep neural networks. Che et al. (2016b) used\npre-trained word embeddings to train feedforward\ndeep neural network and CNN. Their result showed\nimprovements over a CRF based approach that uses\npurely text data.\nSince context is important for this type of task,\nseveral studies explored the recurrent neural net-\nwork (RNN) based architectures combined with\nCRF and pre-trained word vectors. For instance,\nTilk and Alum¨ae (2016) used a bidirectional recur-\nrent neural network (RNN) with an attention mech-\nanism to improve performance over DNN and CNN\nmodels. In another study, Gale and Parthasarathy\n(2017) used character-level LSTM architecture to\n2https://github.com/xashru/\npunctuation-restoration\nachieve results that are competitive with the word-\nlevel CRF based approach. Yi et al. (2017) com-\nbined bidirectional LSTM with a CRF layer and\nan ensemble of three networks. They further used\nknowledge distillation to transfer knowledge from\nthe ensemble of networks to a single DNN network.\nTransformer based approaches have been ex-\nplored in several studies (Yi and Tao, 2019; Nguyen\net al., 2019). Yi and Tao (2019) combined pre-\ntrained word and speech embeddings that improves\nperformance compared to only word embedding\nbased model. Nguyen et al. (2019) used trans-\nformer architecture to restore both punctuation and\ncapitalization. Punctuation restoration is also im-\nportant for machine translation. The study by Wang\net al. (2018) used a transformer based model for\nspoken language translation. They achieved signiﬁ-\ncant improvements over CNN and RNN baselines,\nespecially on joint punctuation prediction task.\nMore recent approaches are based on pre-trained\ntransformer based models. Makhija et al. (2019)\nused pre-trained BERT (Devlin et al., 2019a) model\nwith bidirectional LSTM and a CRF layer to\nachieve state-of-the-art result on reference tran-\nscriptions. Yi et al. (2020) used adversarial multi-\ntask learning with auxiliary parts of speech tagging\ntask using a pre-trained BERT model.\nIn this study, we also explore transformer based\nmodels; however, unlike prior works that solely\nstudied one architecture (BERT), we experiment\nwith different models. We also propose a novel aug-\nmentation scheme that improves the performance.\nOur augmentation is closely related to the augmen-\ntation techniques proposed in (Wei and Zou, 2019b)\nwhere the authors consider synonym replacement,\nrandom insertion, random swap, and random dele-\ntion. While their work is intended for the text clas-\nsiﬁcation tasks, we propose a different version of\nit for this study, which is a sequence labeling task.\nWe do not use synonym replacement and random\nswap as they do not usually appear in speech tran-\nscription.\n3 Datasets\n3.1 English Dataset\nWe use IWSLT dataset for English punctuation\nrestoration, which consists of transcriptions from\nTED Talks.3 Though this dataset was originally re-\nleased in the IWSLT evaluation campaign in 2012\n3http://hltc.cs.ust.hk/iwslt/index.\nphp/evaluation-campaign/ted-task.html\n134\nDataset Total Period Comma Question Other (O)\nEnglish\nTrain 2102417 132393 (6.3%) 158392 (7.53%) 9905 (0.47%) 1801727 (85.7%)\nDev 295800 18910 (6.39%) 22451 (7.59%) 1517 (0.51%) 252922 (85.5%)\nTest (Ref.) 12626 807 (6.39%) 830 (6.57%) 46 (0.36%) 10943 (86.67%)\nTest (ASR) 12822 809 (6.31%) 798 (6.22%) 35 (0.27%) 11180 (87.19%)\nBangla\nTrain 1379986 98791 (7.16%) 65235 (4.73%) 4555 (0.33%) 1211405 (87.78%)\nDev 179371 13161 (7.34%) 7544 (4.21%) 534 (0.3%) 158132 (88.16%)\nTest (news) 87721 6263 (7.14%) 4102 (4.68%) 305 (0.35%) 77051 (87.84%)\nTest (Ref.) 6821 996 (14.6%) 279 (4.09%) 170 (2.49%) 5376 (78.82%)\nTest (ASR) 6417 887 (13.82%) 253 (3.94%) 125 (1.95%) 5152 (80.29%)\nTable 1: Distributions of English and Bangla datasets. The number in parenthesis represents percentage.\nDataset English Bangla\nAvg. Std Avg. Std\nTrain 13.8 10.8 12.4 7.6\nDev 13.5 10.7 12.1 7.2\nTest: News - - 12.4 7.2\nTest: Ref. 13.8 9.6 4.8 3.2\nTest: ASR 14.2 9.7 5.3 3.6\nTable 2: Average sentence length (Avg.) with standard\ndeviation (Std.) for each language.\n(Cettolo et al., 2013; Federico et al., 2012), later,\nChe et al. (2016b) prepared and released a reﬁned\nversion of the IWSLT dataset publicly. For this\nstudy, we use the same train, development, and test\nsplits released by Che et al. (2016b). The train-\ning and development set consist of 2.1M and 296K\nwords, respectively. Two test sets are provided\nwith manual and ASR transcriptions, each contain-\ning 12626 and 12822 words, respectively. These\nare taken from the test data of IWSLT2011 ASR\ndataset.3 A detailed description of the dataset can\nbe found in (Che et al., 2016b). There are four la-\nbels including three punctuation marks: (i) Comma:\nincludes commas, colons and dashes, (ii) Period:\nincludes full stops, exclamation marks and semi-\ncolons, (iii) Question: only question mark, and (iv)\nO: for any other token.\n3.2 Bangla Dataset\nTo the best of our knowledge, there are no pub-\nlicly available resources for the Bangla punctuation\nrestoration task. Hence, we prepare a dataset using\na publicly available corpus of Bangla newspaper ar-\nticles (Khatun et al., 2019). This dataset is available\nin train and test splits. For our task, we selected\n4000 and 500 articles respectively for preparing\ntraining and development sets from their train split,\nand 200 articles for test from their test split. Train-\ning, development, and test sets consist of 1.38M,\n180K, and 88K words respectively.\nAdditionally, we prepare two test datasets con-\nsisting of manual and ASR transcriptions to eval-\nuate the performance. We collected 65 minutes\nof speech excerpts extracted from four Bangla\nshort stories (i.e., monologue read speech).4 These\nare manually transcribed with punctuation. We\nobtained ASR transcriptions for the same audios\nusing Google Cloud speech API. 5 Note that the\nGoogle speech API does not provide punctuation\nfor Bangla. The obtained ASR transcriptions from\nGoogle speech API are then manually annotated\nwith punctuation. We computed the Word Error\nRate (WER) of the ASR transcriptions by com-\nparing against our manual transcriptions, which\nresults in 14.8% WER. The number of words in\nmanual and ASR transcriptions consists of 6821\nand 6417 words respectively. Similar to English,\nwe consider four punctuation marks for Bangla i.e.,\nPeriod, Comma, Question, and O.\nIn Table 1, we present the distributions of the\nlabels for both English and Bangla. In parenthesis,\nwe provide the percentage of the punctuation. In\ngeneral, the distribution of questions is low (less\nthan 1%), which we observe both in English and\nBangla news data. However, this is much higher in\nthe Bangla manual and ASR transcriptions. This\nis due to the fact that these texts are selected from\nshort stories where people often engage in conver-\nsation and ask each other questions. The literary\nstyle of the stories is different from news and as a\nresult, the distribution of Period is also higher in\nthe Bangla manual and ASR transcriptions. This\nresults in a much smaller average sentence length\nin these datasets, as can be seen in Table 2. We can\ncompare these numbers with English as reported\n4Due to the limited annotation resources we could not\ncollect more data, and this could be a future effort.\n5https://cloud.google.com/\nspeech-to-text\n135\nFigure 1: A general model architecture for our experi-\nments.\nin (Zelasko et al., 2018). The authors reported\n79.1% O token on the training data collected from\nconversational speech. We have 78.82% O token\non our reference test data. This suggests that our\ntranscribed data are more similar in distribution to\nnatural conversations.\n4 Experiments\nFor this study, we explored different transformer\nbased models for both English and Bangla. In ad-\ndition, we used bidirectional LSTM (BiLSTM) on\ntop of the pre-trained transformer network, and an\naugmentation method to improve the performance\nof the models.\n4.1 Models and Architectures\nIn Figure 1, we report a general network archi-\ntecture that we used in our experiments. We ob-\ntained ddimensional embedding vector from the\npre-trained language model for each token. This\nis used as input for a BiLSTM layer, consisting of\nhhidden units. This allows the network to make\neffective use of both past and future contexts for\nprediction. The outputs from the forward and back-\nward LSTM layers are concatenated at each time\nstep and fed to a fully connected layer with four\noutput neurons, which correspond to 3 punctuation\nmarks and one O token.\nAs can be seen in the Figure, the input sentence\n“when words fail music speaks” does not have any\npunctuation, and the task of the model is to predict\na Comma after the word “fail“ and Period after\nthe word “speaks“ to produce the output sentence\n“when words fail, music speaks.”\nWe measure the performance of the models in\nterms of precision ( P), recall ( R), and F1-score\n(F1).\n4.1.1 Pretrained Language Model\nTransfer learning has been popular in computer\nvision, and the emergence of the transformers\n(Vaswani et al., 2017) has shown the light to use\ntransfer learning in NLP applications. The mod-\nels are trained on a large text corpus (e.g., BERT\nis trained on 800M words from Book Corpus and\n2,500M words from Wikipedia) and their success\nhas been proven by ﬁne-tuning downstream NLP\napplications. In our experiment, we used such\npre-trained language models for the punctuation\nrestoration task. We brieﬂy discuss the monolin-\ngual language models for English and multi-lingual\nlanguage models used in this study.\nBERT (Devlin et al., 2019a) is designed to learn\ndeep bidirectional representation from unlabeled\ntexts by jointly conditioning the left and right con-\ntexts in all layers. It uses a multi-layer bidirectional\ntransformer encoder architecture (Vaswani et al.,\n2017) and makes use of two objectives during pre-\ntraining: masked language model (MLM) and next\nsentence prediction (NSP) task.\nRoBERTa (Liu et al., 2019) performs a repli-\ncation study of BERT pretraining and shows that\nimprovements can be made using larger datasets,\nvocabulary, and training on longer sequences with\nbigger batches. It uses dynamic masking of the\ntokens i.e., the masking pattern is generated every\ntime a sequence is fed to the model instead of gen-\nerating them beforehand. They also remove the\nNSP task and use only MLM loss for pretraining.\nALBERT (Lan et al., 2020) incorporates a cou-\nple of parameter reduction techniques to design\nan architecture with signiﬁcantly fewer parameters\nthan a traditional BERT architecture. The ﬁrst im-\nprovement is factorizing embedding parameters by\ndecomposing the embedding matrix V ×H into\ntwo smaller matrices V ×E and E×H, where\nV is the vocabulary size, Eis the word piece em-\nbedding size, and H is hidden layer size. This\nreduces embedding parameters from O(V ×H)\nto O(V ×E+ E×H), which can be signiﬁcant\nwhen E << H. The second improvement is pa-\nrameter sharing across layers. This prevents the\nparameter from growing as the depth is increased.\nThe NSP task introduced in BERT is replaced by a\nsentence-order prediction (SOP) task in ALBERT.\n136\nDistilBERT (Sanh et al., 2019) uses knowledge\ndistillation from BERT to train a model that has\n40% fewer parameters and is 60% faster while re-\ntaining 97% of language understanding capabilities\nof the BERT model. The training objective is a\nlinear combination of distillation loss, supervised\ntraining loss, and cosine distance loss.\nMultilingual Models MLM has also been uti-\nlized for learning language models from large scale\nmulti-lingual corpora. BERT multilingual model\n(mBERT) is trained on more than 100 languages\nwith the largest Wikipedia dataset. To account for\nthe variation among Wikipedia sizes of different\nlanguages, data is sampled using an exponentially\nsmoothed weighting (with a factor 0.7) so that high-\nresource languages like English are under-sampled\ncompared to low resource languages. Word counts\nare weighted the same way as the data so that low-\nresource language vocabularies are up weighted by\nsome factor.\nCross-lingual models (XLM) (Conneau and\nLample, 2019) use MLM in multiple language set-\ntings, similar to BERT. Instead of using a pair of\nsentences, an arbitrary number of sentences are\nused with text length truncated at 256 tokens.\nXLM-RoBERTa (Conneau et al., 2020) is trained\nwith a multilingual MLM objective similar to XLM\nbut on a larger dataset. It is trained in one hundred\nlanguages, using more than two terabytes of ﬁltered\nCommon Crawl data (Wenzek et al., 2020).\n4.1.2 Augmentation\nFor this study, we propose an augmentation method\ninspired by the study of Wei and Zou (2019a), as\ndiscussed earlier. Our augmentation method is\nbased on the types of error ASR makes during\nrecognition, which include insertion, substitution,\nand deletion.\nDue to the lack of large-scale manual transcrip-\ntions, punctuation restoration models are typically\ntrained using written text, which is well-formatted\nand correctly punctuated. Hence, the trained model\nlacks the knowledge of the typical errors that ASR\nmakes. To train the model with such characteris-\ntics, we use an augmentation technique that sim-\nulates such errors and dynamically creates a new\nsequence on the ﬂy in a batch. Dynamic augmen-\ntation is different from the traditional augmenta-\ntion approach widely used in NLP (Wei and Zou,\n2019a); however, it is widely used in computer vi-\nsion for image classiﬁcation tasks (Cubuk et al.,\n2020).\nThe three different kinds of augmentation corre-\nsponding to three possible errors are as follows.\n1. First (i.e., substitution), we replace a token\nby another token. In our experiment, we ran-\ndomly replace a token with the special un-\nknown token.\n2. Second (i.e., deletion), we delete some tokens\nrandomly from the processed input sequence.\n3. Finally, we add (i.e., insertion) the unknown\ntoken at some random position of the input.\nWe hypothesize that not all three errors are\nequally prevalent, hence, different augmentation\nwill have a different effect on performance. Keep-\ning this in mind, to process input text, we used\nthree tunable parameters: (i) a parameter to deter-\nmine token change probability, α, (ii) a parame-\nter, αsub, to control the probability of substitution,\n(iii) a parameter, αdel, to control the probability\nof deletion. Probability of insertion is given by\n1 −(αsub + αdel).\nWhen applying substitution, we replaced the to-\nken in that position with the unknown token and\nleft the target punctuation mark unchanged. For\ndeletion, both the token and the punctuation mark\nin that position are deleted. For insertion, we in-\nserted the unknown token and O token, in that\nposition.\nSince deletion and insertion operation may make\nthe sequence smaller or longer than the ﬁxed se-\nquence length we used during training, we added\npadding or truncated as necessary.\n4.2 Experimental Settings\nWe used pre-trained models available in the Hug-\ngingFace’s Transformers library (Wolf et al., 2019).\nMore details about different architectures can be\nfound on HuggingFace website.6 For tokenization,\nwe used model-speciﬁc tokenizers.\nDuring training, we used a maximum sequence\nlength of 256. Each sequence starts with a special\nstart of sequence token and ends with a special\nend of sequence token. Since the tokenizers use\nbyte-pair encoding (Sennrich et al., 2016), a word\nmay be tokenized into subword units. 7 If adding\nthe subword tokens of a word results in sequence\nlength exceeding 256, we excluded those tokens\n6https://huggingface.co/transformers/\npretrained_models.html\n7If the model predicts punctuation in the middle of a word,\nthese are ignored.\n137\nTest Model Comma Period Question Overall\nP R F 1 P R F 1 P R F 1 P R F 1\nRef.\nSAPR (Wang et al., 2018) 57.2 50.8 55.9 96.7 97.3 96.8 70.6 69.2 70.3 78.2 74.4 77.4\nDRNN-LWMA-pre (Kim, 2019) 62.9 60.8 61.977.3 73.7 75.5 69.6 69.6 69.6 69.9 67.2 68.6\nSelf-attention (Yi and Tao, 2019) 67.4 61.1 64.182.5 77.4 79.9 80.1 70.2 74.8 76.7 69.6 72.9\nBERT-Transfer (Makhija et al., 2019)70.8 74.3 72.5 84.9 83.3 84.1 82.7 93.5 87.8 79.5 83.7 81.4\nBERT-Adversarial (Yi et al., 2020) 76.2 71.273.6 87.3 81.1 84.1 79.1 72.7 75.8 80.9 75.0 77.8\nBERT-base-uncased 71.7 70.1 70.9 82.5 83.1 82.8 75.0 84.8 79.6 77.0 76.8 76.9\nBERT-large-uncased 72.6 72.8 72.7 84.8 84.6 84.7 70.0 91.3 79.2 78.3 79.0 78.6\nRoBERTa-base 73.6 75.1 74.3 84.9 87.6 86.2 77.4 89.1 82.8 79.2 81.5 80.3\nRoBERTa-large 76.9 75.8 76.3 86.8 90.5 88.6 72.9 93.5 81.9 81.6 83.3 82.4\nALBERT-base-v2 70.1 75.5 72.7 84.9 84.1 84.5 79.5 76.1 77.8 77.2 79.7 78.4\nALBERT-large-v2 75.1 72.4 73.7 82.0 88.0 84.9 77.6 82.6 80.0 78.7 80.2 79.4\nDistilBERT-base-uncased 67.0 65.5 66.3 77.1 81.0 79.0 69.2 78.3 73.5 72.1 73.3 72.7\nBERT-base-multilingual-uncased 70.4 68.1 69.280.1 85.4 82.7 62.7 80.4 70.5 75.0 76.7 75.9\nXLM-RoBERTa-base 75.1 70.5 72.7 81.2 89.3 85.1 71.7 82.6 76.8 78.1 79.9 79.0\nXLM-RoBERTa-large 73.3 80.4 76.7 87.9 86.4 87.1 82.0 89.1 85.4 80.1 83.5 81.8\nDistilBERT-base-multilingual-cased 65.5 58.0 61.574.8 79.2 76.9 58.2 69.6 63.4 70.1 68.4 69.3\nRoBERTa-large + augmentation76.8 76.6 76.7 88.6 89.2 88.9 82.7 93.5 87.8 82.6 83.1 82.9\nASR\nSelf-attention (Yi and Tao, 2019) 64.0 59.6 61.775.5 75.8 75.6 72.6 65.9 69.1 70.7 67.1 68.8\nBERT-Adversarial (Yi et al., 2020)72.4 69.3 70.8 80.0 79.1 79.5 71.2 68.0 69.6 74.5 72.1 73.3\nBERT-base-uncased 49.3 64.2 55.8 75.3 76.3 75.8 44.7 60.0 51.2 60.4 70.0 64.9\nBERT-large-uncased 49.9 67.0 57.2 77.0 78.9 77.9 50.0 74.3 59.8 61.4 73.0 66.7\nRoBERTa-base 51.9 69.3 59.3 77.5 80.3 78.9 50.0 65.7 56.8 62.8 74.7 68.2\nRoBERTa-large 56.6 67.9 61.8 78.7 85.3 81.9 46.6 77.1 58.1 66.5 76.7 71.3\nALBERT-base-v2 48.7 66.0 56.1 75.7 79.9 77.7 59.3 45.7 51.6 60.6 72.4 66.0\nALBERT-large-v2 52.1 64.4 57.6 73.8 82.7 78.0 53.3 68.6 60.0 62.2 73.5 67.4\nDistilBERT-base-uncased 46.8 59.1 52.2 70.0 74.8 72.3 48.9 65.7 56.1 57.3 67.0 61.8\nBERT-base-multilingual-uncased 49.8 62.4 55.472.0 78.2 75.0 47.8 62.9 54.3 59.9 70.2 64.6\nXLM-RoBERTa-base 54.7 61.7 58.0 73.2 83.3 77.9 47.7 60.0 53.2 63.6 72.3 67.7\nXLM-RoBERTa-large 53.2 71.4 61.0 82.0 81.8 81.9 62.5 71.4 66.7 65.5 76.6 70.6\nDistilBERT-base-multilingual-cased 47.5 52.8 50.066.7 71.9 69.2 41.3 54.3 46.9 56.7 62.2 59.3\nRoBERTa-large + augmentation64.1 68.8 66.3 81.0 83.7 82.3 55.3 74.3 63.4 72.0 76.2 74.0\nTable 3: Results on IWSLT2011 manual (Ref.) and ASR transcriptions of test sets. Highlighted rows are the\ncomparable results between ours and previous study. For overall best results we use bold form, and for the best F1\nof individual punctuation we use a combination of bold and italic form.\nfrom the current sequence and start the next se-\nquence from them. We use padding token after the\nend of sequence token to ﬁll the remaining slots of\nthe sequence. Padding tokens are masked to avoid\nperforming attention on them. We use a batch size\nof 8 and shufﬂe the sequences before each epoch.\nOur chosen learning rates are 5e-6 for large mod-\nels, and 1e-5 for base models, which are optimized\nusing the development set. LSTM dimension his\nset to the token embedding dimension d. All mod-\nels are trained with Adam (Kingma and Ba, 2015)\noptimization algorithm for 10 epochs. Other pa-\nrameters are kept as the default settings, discussed\nin (Devlin et al., 2019b). The model with the best\nperformance on the development set is used for\nevaluating the test datasets.\nFor the augmentation experiments, we used α∈\n{0.05,0.1,0.15,0.2}, αsub ∈{0.2,0.3,0.4,0.5},\nαdel ∈ {0.2,0.3,0.4,0.5}with additional con-\nstraint 0.5 ≤(αsub+αdel) ≤0.8. Optimum values\nfor these were obtained using the development set.\n5 Results and Discussions\n5.1 Results on English Dataset\nIn Table 3, we report our experimental results\nwith a comparison from previous results on the\nsame dataset. We provide the results obtained\nusing BERT, RoBERTa, ALBERT, DistilBERT,\nmBERT, XLM-RoBERTa models without augmen-\ntation. Large variants of the models perform better\nthan the Base models. Monolingual models per-\nform better than their multilingual counterparts.\nRoBERTa achieves a better result than other mod-\nels as it was trained on a larger corpus and has a\nlarger vocabulary. Our best result is obtained using\nthe RoBERTa model with augmentation in which\nthe parameters were α= 0.15,αsub = 0.4,αdel =\n0.4. Performance gain from augmentation comes\nfrom improved precision.\nWe obtained the state of the art result on both\ntest sets in terms of the overall F1 score (rows are\nhighlighted). On Ref. test set, we obtained the\nbest result on Comma, and comparable results for\n138\nTest Model Comma Period Question Overall\nP R F 1 P R F 1 P R F 1 P R F 1\nNews\nBERT-base-multilingual-uncased 79.8 68.2 73.580.4 85.4 82.8 72.1 77.0 74.5 79.9 78.5 79.2\nDistilBERT-base-multilingual-cased 72.1 60.8 66.074.5 71.6 73.0 56.9 67.5 61.8 73.0 67.3 70.1\nXLM-MLM-100-1280 76.9 71.2 73.9 82.0 83.4 82.9 70.2 76.4 73.2 80.0 78.5 79.3\nXLM-RoBERTa-large 86.0 77.0 81.2 89.4 92.3 90.8 77.4 85.6 81.3 87.8 86.2 87.0\nXLM-RoBERTa-large + augmentation85.8 77.5 81.4 88.8 92.5 90.6 77.9 86.6 82.0 87.4 86.6 87.0\nRef.\nBERT-base-multilingual-uncased 35.6 34.4 35.067.4 64.7 66.0 39.8 28.8 33.4 58.5 54.6 56.5\nDistilBERT-base-multilingual-cased 32.6 31.5 32.164.0 50.2 56.3 32.5 14.7 20.2 54.3 42.4 47.6\nXLM-MLM-100-1280 33.4 39.8 36.3 70.3 64.0 67.0 42.4 22.9 29.8 59.2 54.5 56.7\nXLM-RoBERTa-large 39.3 36.9 38.1 76.9 81.4 79.1 54.3 58.8 56.5 67.6 70.2 68.8\nXLM-RoBERTa-large + augmentation43.3 37.3 40.1 76.5 82.6 79.4 53.0 56.5 54.7 68.3 70.8 69.5\nASR\nBERT-base-multilingual-uncased 29.3 30.0 29.760.6 60.2 60.4 36.1 38.4 37.2 51.7 52.0 51.9\nDistilBERT-base-multilingual-cased 29.0 33.6 31.162.6 50.6 56.0 31.3 20.8 25.0 51.2 44.3 47.5\nXLM-MLM-100-1280 31.2 38.7 34.6 63.4 59.5 61.4 32.0 24.8 27.9 52.8 51.9 52.4\nXLM-RoBERTa-large 38.3 35.6 36.9 69.2 77.2 73.0 38.5 52.0 44.2 60.3 66.4 63.2\nXLM-RoBERTa-large + augmentation37.2 33.2 35.1 69.1 77.8 73.2 45.5 60.8 52.1 61.1 67.2 64.0\nTable 4: Result on Bangla test datasets.\nQuestion (highlighted using a combination of the\nbold and italic form). However, SAPR (Wang et al.,\n2018) method performed much better compared to\nothers for Period on this data. On ASR test set,\nour result is marginally better than Yi et al. (2020)\nfor overall F1 score. Our model performed better\nfor Period but comparatively lower forComma and\nQuestion. Overall, our model has better recall than\nprecision on this dataset.\n5.2 Results on Bangla Dataset\nIn Table 4, we report results on the Bangla test set\ncomprised of news, manual, and ASR transcrip-\ntions. Since no monolingual transformer model\nis publicly available for Bangla, we explored dif-\nferent multilingual models. We obtained the best\nresult using XLM-RoBERTa (large) model as it\nis trained with more texts for low-resource lan-\nguages like Bangla and has larger vocabulary for\nthem. This is consistent with the ﬁndings reported\nin (Liu et al., 2019), where the authors report im-\nprovement over multi-lingual BERT and XLM\nmodels in cross-lingual understanding tasks for\nlow-resource languages. We apply augmentation\non XLM-RoBERTa model and best result is ob-\ntained using augmentation parameters α = 0.15,\nαsub = 0.4, and αdel = 0.4. However, the per-\nformance gain from augmentation is marginal on\nDataset 4-Classes 3-Classes 2-Classes\nP R F 1 P R F 1 P R F 1\nNews 87.4 86.6 87.0 88.0 87.287.6 94.1 93.393.7\nRef. 68.3 70.8 69.5 72.9 75.674.2 83.6 86.685.1\nASR 61.1 67.2 64.0 65.1 71.568.1 77.0 84.780.6\nTable 5: Result on Bangla test datasets by merging\nclasses.\nthe Bangla dataset. Overall, performance on the\nnews test set is better compared to the manual and\nASR data. Performance for Comma is lower than\nPeriod and Question. Compared to English, we\nnotice a performance drop of about 10% for Period\nand Question, but for Comma, this is more than\n30% on the ASR test set.\nFor many applications (e.g., semi-automated sub-\ntitles generation), it is of utmost importance to facil-\nitate human labelers to reduce their time and effort\nand make the manual annotation process faster. In\nsuch cases, identifying the correct position of the\npunctuations is important, as reported in (Che et al.,\n2016b). For Bangla, we wanted to understand what\nwe can gain while merging the punctuation and\nidentifying their position. For this purpose, we\nevaluate performance on 3-Classes and 2-Classes\ntest sets. We combinePeriod and Question together\nto form the 3-classes test sets. Comma is further\ncombined with those to form the 2-Classes test\nsets, i.e., punctuation or no punctuation. In Table\n5, we report the results with binary and multiclass\nsettings using XLM-RoBERTa (large) model cou-\npled with augmentation. As can be seen, the model\nperforms well for predicting punctuation positions.\nFor manual (Ref.) and ASR transcriptions, we have\na signiﬁcant gain while merging the number of\nclasses from four towards two. It could be because–\nas the number of classes reduces, the classiﬁer’s\ncomplexity reduces, which leads to an increase in\nthe model’s performance. The performance gain is\ncomparatively lower for news while merging four\nclasses into three classes; however, it increased sig-\nniﬁcantly when reduced to two. Considering these\nﬁndings, we believe this type of model can help\n139\nTest Type Comma Period Question Overall\nP R F 1 P R F 1 P R F 1 P R F 1\nRef. Linear 76.9 75.8 76.3 86.8 90.5 88.6 72.9 93.5 81.9 81.6 83.3 82.4\nCRF 75.7 76.9 76.3 88.1 89.0 88.5 77.8 91.3 84.0 81.7 83.1 82.4\nASR Linear 56.6 67.9 61.8 78.7 85.3 81.9 46.6 77.1 58.1 66.5 76.7 71.3\nCRF 56.7 69.0 62.3 78.5 82.8 80.6 50.9 80.0 62.2 66.4 76.1 70.9\nTable 6: Results of CRF on IWSLT2011 Ref. and ASR test data with RoBERTa-large model\nTest Augmentation Comma Period Question Overall\nP R F 1 P R F 1 P R F 1 P R F 1\nRef.\nNone 76.9 75.8 76.3 86.8 90.5 88.6 72.9 93.5 81.9 81.6 83.3 82.4\nSubstitution (α= 0.1) 77.6 77.6 77.6 87.7 90.7 89.2 76.4 91.3 83.2 82.4 84.3 83.3\nSubstitution (α= 0.15, random) 76.5 76.4 76.4 87.2 90.6 88.9 86.3 95.7 90.7 82.0 83.7 82.9\nDelete (α= 0.1) 75.0 76.0 75.5 88.6 88.4 88.5 84.3 93.5 88.7 81.7 82.4 82.1\nInsert(α= 0.05) 77.6 75.5 76.6 87.1 90.6 88.8 82.7 93.5 87.8 82.5 83.2 82.9\nAll(α= 0.15,αsub= 0.4,αdel= 0.4) 76.8 76.6 76.7 88.6 89.2 88.9 82.7 93.5 87.8 82.6 83.1 82.9\nASR\nNone 56.6 67.9 61.8 78.7 85.3 81.9 46.6 77.1 58.1 66.5 76.7 71.3\nSubstitution (α= 0.1) 57.0 70.8 63.1 80.8 85.4 83.1 50.9 77.1 61.4 67.5 78.1 72.4\nSubstitution (α= 0.15, random) 57.2 69.3 62.7 79.2 83.9 81.5 56.3 77.1 65.1 67.3 76.7 71.7\nDelete (α= 0.1) 60.0 70.4 64.8 82.7 82.8 82.8 52.1 71.4 60.2 70.0 76.6 73.1\nInsert(α= 0.05) 57.4 67.2 61.9 79.6 84.8 82.1 49.2 82.9 61.7 67.5 76.2 71.6\nAll(α= 0.15,αsub= 0.4,αdel= 0.4) 64.1 68.8 66.3 81.0 83.7 82.3 55.3 74.3 63.4 72.0 76.2 74.0\nTable 7: Results of Augmentation IWSLT2011 Ref. and ASR test data with RoBERTa-large model\nhuman annotators in such applications.\n5.3 Ablation Studies\nWe experimented with using CRF after the linear\nlayer for predicting the most probable tag sequence\ninstead of using the softmax layer. However, we did\nnot notice any performance improvement and even\na slight decrease in ASR test data performance. The\nresults using RoBERTa large model are reported in\nTable 6.\nWe also analyzed the effect on performance\nwhen substitution, insert and delete augmentations\nare applied in isolation. These results are reported\nin table 7 for RoBERTa large model. We explored\nsubstitution with a random token from vocabulary\n(reported in row Substitution (α= 0.15, random).\nHowever, it performed worse compared to substi-\ntuting with the unknown token. We notice that the\nperformance gain from different augmentations is\nlarger on the ASR test set than the reference test\nset.\n5.4 Discussion\nFor English, we obtained state-of-art results for\nmanual and ASR transcriptions using our augmen-\ntation technique coupled with the RoBERTa-large\nmodel. There is still a large difference between\nmanual and ASR transcriptions results. In Figure 2,\nwe report the confusion matrix (in percentage), for\nmanual and ASR transcriptions. From the ﬁgure,\nwe observe that for ASR transcriptions, a high pro-\nportion of cases Question and Comma are predicted\nas O and Period. We will investigate this ﬁnding\nfurther in our future study.\nCompared to English, the performance of Bangla\nis relatively low. We hypothesize several factors are\nresponsible for this. First, the pre-trained monolin-\ngual language models for English usually perform\nbetter than multilingual models. Even in the case\nof multilingual models, the content of the English\nlanguage is higher in the training data, and as a\nresult, the models are expected to perform better\nfor English. Second and perhaps a more important\nfactor is the nature of training data. For Bangla,\ndue to the lack of punctuated transcribed data, we\nused a news corpus for training. Hence, the trained\nmodel does not learn the nuances of transcriptions,\nwhich reduces prediction accuracy. Third, our ASR\ntranscriptions are taken from some story excerpts,\ncontaining monologue and a signiﬁcant amount of\nconversations (dialogue), which varies in terms of\ncomplexity (e.g., the dialogue has interruptions and\noverlap, short vs long utterance). An aspect of such\na complexity is also evident in Table 1, where we\nsee that the distribution of Period is almost double\ncompared to news data and the distribution ofQues-\ntion is more than six times greater. On the other\nhand, for English, both train and test data are taken\n140\nFigure 2: Confusion matrix (in percentage) for English test datasets.\nFigure 3: Confusion matrix (in percentage) for Bangla test datasets.\nfrom TED talks, and there is no such discrepancy\nbetween the data distributions.\nSimilarly to English, we also wanted to see error\ncases for Bangla. In Figure 3, we report the con-\nfusion matrix. We observed similar phenomenon\nas English for Bangla, comparatively much higher\nin proportion, i.e., Question and Comma are pre-\ndicted as O and Period for news, manual and ASR\ntranscriptions.\n6 Conclusion\nIn this study, we explore different transformer mod-\nels for high-and low-Resource languages (i.e., En-\nglish and Bangla). In addition, we propose an\naugmentation technique, which improves perfor-\nmance on noisy ASR texts. There has not been\nany reported result and resources for punctuation\nrestoration on Bangla. Our study, ﬁndings, and\ndeveloped resources will enrich and push the cur-\nrent state-of-art for this low-resource language. We\nhave released the created Bangla dataset and code\nfor the research community.\nReferences\nFiroj Alam, Bernardo Magnini, and Roberto Zanoli.\n2015. Comparing named entity recognition on tran-\nscriptions and written texts. In Harmonization and\nDevelopment of Resources and Tools for Italian Nat-\nural Language Processing within the PARLI Project,\npages 71–89. Springer.\nMauro Cettolo, Jan Niehues, Sebastian St ¨uker, Luisa\nBentivogli, and Marcello Federico. 2013. Report on\nthe 10th iwslt evaluation campaign. In Proceedings\nof the International Workshop on Spoken Language\nTranslation, Heidelberg, Germany.\nXiaoyin Che, Sheng Luo, Haojin Yang, and Christoph\nMeinel. 2016a. Sentence boundary detection based\non parallel lexical and acoustic models. In Inter-\nspeech, pages 2528–2532.\nXiaoyin Che, Cheng Wang, Haojin Yang, and\nChristoph Meinel. 2016b. Punctuation prediction\nfor unsegmented transcript based on word vector. In\nProceedings of the Tenth International Conference\non Language Resources and Evaluation LREC 2016,\nPortoroˇz, Slovenia, May 23-28, 2016. European Lan-\nguage Resources Association (ELRA).\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 8440–8451. Associa-\ntion for Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, 8-14 December 2019,\nVancouver, BC, Canada, pages 7057–7067.\n141\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and\nQuoc V Le. 2020. Randaugment: Practical au-\ntomated data augmentation with a reduced search\nspace. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition Work-\nshops, pages 702–703.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019a. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019b. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. of the 2019 Conference of the\nNAACL, pages 4171–4186, Minneapolis, Minnesota.\nACL.\nMarcello Federico, Mauro Cettolo, Luisa Ben-\ntivogli, Paul Michael, and St ¨uker Sebastian. 2012.\nOverview of the iwslt 2012 evaluation campaign.\nIn IWSLT-International Workshop on Spoken Lan-\nguage Translation, pages 12–33.\nWilliam Gale and Sarangarajan Parthasarathy. 2017.\nExperiments in character-level neural network mod-\nels for punctuation. In INTERSPEECH, pages 2794–\n2798.\nAgustin Gravano, Martin Jansche, and Michiel Bacchi-\nani. 2009. Restoring punctuation and capitalization\nin transcribed speech. In 2009 IEEE International\nConference on Acoustics, Speech and Signal Pro-\ncessing, pages 4741–4744. IEEE.\nDouglas A. Jones, Florian Wolf, Edward Gibson,\nElliott Williams, Evelina Fedorenko, Douglas A.\nReynolds, and Marc A. Zissman. 2003. Measur-\ning the readability of automatic speech-to-text tran-\nscripts. In 8th European Conference on Speech\nCommunication and Technology, EUROSPEECH\n2003 - INTERSPEECH 2003, Geneva, Switzerland,\nSeptember 1-4, 2003. ISCA.\nAisha Khatun, Anisur Rahman, Hemayet Ahmed\nChowdhury, Md. Saiful Islam, and Ayesha Tasnim.\n2019. A subword level language model for bangla\nlanguage. CoRR, abs/1911.07613.\nSeokhwan Kim. 2019. Deep recurrent neural networks\nwith layer-wise multi-head attentions for punctu-\nation restoration. In IEEE International Confer-\nence on Acoustics, Speech and Signal Processing,\nICASSP 2019, Brighton, United Kingdom, May 12-\n17, 2019, pages 7280–7284. IEEE.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nTal Levy, Vered Silber-Varod, and Ami Moyal. 2012.\nThe effect of pitch, intensity and pause duration in\npunctuation detection. In 2012 IEEE 27th Conven-\ntion of Electrical and Electronics Engineers in Is-\nrael, pages 1–4. IEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nWei Lu and Hwee Tou Ng. 2010. Better punctuation\nprediction with dynamic conditional random ﬁelds.\nIn Proceedings of the 2010 conference on empirical\nmethods in natural language processing, pages 177–\n186.\nKaran Makhija, Thi-Nga Ho, and Eng Siong Chng.\n2019. Transfer learning for punctuation prediction.\nIn 2019 Asia-Paciﬁc Signal and Information Pro-\ncessing Association Annual Summit and Conference,\nAPSIPA ASC 2019, Lanzhou, China, November 18-\n21, 2019, pages 268–273. IEEE.\nJohn Makhoul, Alex Baron, Ivan Bulyko, Long\nNguyen, Lance Ramshaw, David Stallard, Richard\nSchwartz, and Bing Xiang. 2005. The effects of\nspeech recognition and punctuation on information\nextraction performance. In Ninth European Confer-\nence on Speech Communication and Technology.\nEvgeny Matusov, Dustin Hillard, Mathew Magimai-\nDoss, Dilek Hakkani-T ¨ur, Mari Ostendorf, and Her-\nmann Ney. 2007. Improving speech translation with\nautomatic boundary prediction. In Eighth Annual\nConference of the International Speech Communica-\ntion Association.\nBinh Nguyen, Vu Bao Hung Nguyen, Hien\nNguyen, Pham Ngoc Phuong, The-Loc Nguyen,\nQuoc Truong Do, and Luong Chi Mai. 2019. Fast\nand accurate capitalization and punctuation for\nautomatic speech recognition using transformer\nand chunk merging. In 2019 22nd Conference of\nthe Oriental COCOSDA International Committee\nfor the Co-ordination and Standardisation of\nSpeech Databases and Assessment Techniques\n(O-COCOSDA), pages 1–5. IEEE.\nDaniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-\nCheng Chiu, Bo Li, Yonghui Wu, and Quoc V . Le.\n2020. Improved noisy student training for automatic\nspeech recognition.\n142\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter.CoRR,\nabs/1910.01108.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nGy¨orgy Szasz´ak and M ´at´e ´Akos T¨undik. 2019. Lever-\naging a character, word and prosody triplet for an asr\nerror robust and agglutination friendly punctuation\napproach. In INTERSPEECH, pages 2988–2992.\nOttokar Tilk and Tanel Alum¨ae. 2016. Bidirectional re-\ncurrent neural network with attention mechanism for\npunctuation restoration. In Interspeech 2016, 17th\nAnnual Conference of the International Speech Com-\nmunication Association, San Francisco, CA, USA,\nSeptember 8-12, 2016, pages 3047–3051. ISCA.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nFeng Wang, Wei Chen, Zhen Yang, and Bo Xu. 2018.\nSelf-attention based network for punctuation restora-\ntion. In 2018 24th International Conference on Pat-\ntern Recognition (ICPR), pages 2803–2808. IEEE.\nJason Wei and Kai Zou. 2019a. Eda: Easy data aug-\nmentation techniques for boosting performance on\ntext classiﬁcation tasks. ArXiv, abs/1901.11196.\nJason W. Wei and Kai Zou. 2019b. EDA: easy data\naugmentation techniques for boosting performance\non text classiﬁcation tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 6381–6387. Association for\nComputational Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm ´an, Ar-\nmand Joulin, and Edouard Grave. 2020. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Proceedings of The 12th Language\nResources and Evaluation Conference, LREC 2020,\nMarseille, France, May 11-16, 2020 , pages 4003–\n4012. European Language Resources Association.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nChenglin Xu, Lei Xie, Guangpu Huang, Xiong Xiao,\nEng Siong Chng, and Haizhou Li. 2014. A deep\nneural network approach for sentence boundary de-\ntection in broadcast news. In Fifteenth annual con-\nference of the international speech communication\nassociation.\nJiangyan Yi and Jianhua Tao. 2019. Self-attention\nbased model for punctuation prediction using word\nand speech embeddings. In IEEE International Con-\nference on Acoustics, Speech and Signal Processing,\nICASSP 2019, Brighton, United Kingdom, May 12-\n17, 2019, pages 7270–7274. IEEE.\nJiangyan Yi, Jianhua Tao, Ye Bai, Zhengkun Tian, and\nCunhang Fan. 2020. Adversarial transfer learning\nfor punctuation restoration. CoRR, abs/2004.00248.\nJiangyan Yi, Jianhua Tao, Zhengqi Wen, and Ya Li.\n2017. Distilling knowledge from an ensemble\nof models for punctuation prediction. In Inter-\nspeech 2017, 18th Annual Conference of the Inter-\nnational Speech Communication Association, Stock-\nholm, Sweden, August 20-24, 2017 , pages 2779–\n2783. ISCA.\nPiotr Zelasko, Piotr Szymanski, Jan Mizgajski, Adrian\nSzymczak, Yishay Carmiel, and Najim Dehak. 2018.\nPunctuation prediction model for conversational\nspeech. In Interspeech 2018, 19th Annual Confer-\nence of the International Speech Communication As-\nsociation, Hyderabad, India, 2-6 September 2018 ,\npages 2633–2637. ISCA.\nDongdong Zhang, Shuangzhi Wu, Nan Yang, and\nMu Li. 2013. Punctuation prediction with transition-\nbased parsing. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 752–760.",
  "topic": "Punctuation",
  "concepts": [
    {
      "name": "Punctuation",
      "score": 0.8500221371650696
    },
    {
      "name": "Readability",
      "score": 0.8471734523773193
    },
    {
      "name": "Computer science",
      "score": 0.7976545095443726
    },
    {
      "name": "Transformer",
      "score": 0.7490589022636414
    },
    {
      "name": "Bengali",
      "score": 0.6422633528709412
    },
    {
      "name": "Natural language processing",
      "score": 0.6294341087341309
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5630283951759338
    },
    {
      "name": "Language model",
      "score": 0.5004632472991943
    },
    {
      "name": "Speech recognition",
      "score": 0.32616305351257324
    },
    {
      "name": "Engineering",
      "score": 0.09075883030891418
    },
    {
      "name": "Programming language",
      "score": 0.07766827940940857
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}