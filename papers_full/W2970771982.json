{
  "title": "SciBERT: A Pretrained Language Model for Scientific Text",
  "url": "https://openalex.org/W2970771982",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2800204358",
      "name": "Iz Beltagy",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2716373899",
      "name": "Kyle Lo",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1983754593",
      "name": "Arman Cohan",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3106224367",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2887672952",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W3105491236",
    "https://openalex.org/W2279376656",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2952867657",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W2163107094",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W2962815673",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2793978524",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2738180183",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2147994374"
  ],
  "abstract": "Iz Beltagy, Kyle Lo, Arman Cohan. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 3615–3620,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n3615\nSCIBERT: A Pretrained Language Model for Scientiﬁc Text\nIz Beltagy Kyle Lo Arman Cohan\nAllen Institute for Artiﬁcial Intelligence, Seattle, W A, USA\n{beltagy,kylel,armanc}@allenai.org\nAbstract\nObtaining large-scale annotated data for NLP\ntasks in the scientiﬁc domain is challenging\nand expensive. We release S CIBERT, a pre-\ntrained language model based on B ERT (De-\nvlin et al., 2019) to address the lack of high-\nquality, large-scale labeled scientiﬁc data.\nSCIBERT leverages unsupervised pretraining\non a large multi-domain corpus of scien-\ntiﬁc publications to improve performance on\ndownstream scientiﬁc NLP tasks. We eval-\nuate on a suite of tasks including sequence\ntagging, sentence classiﬁcation and depen-\ndency parsing, with datasets from a variety\nof scientiﬁc domains. We demonstrate sta-\ntistically signiﬁcant improvements over B ERT\nand achieve new state-of-the-art results on sev-\neral of these tasks. The code and pretrained\nmodels are available at https://github.\ncom/allenai/scibert/.\n1 Introduction\nThe exponential increase in the volume of scien-\ntiﬁc publications in the past decades has made\nNLP an essential tool for large-scale knowledge\nextraction and machine reading of these docu-\nments. Recent progress in NLP has been driven\nby the adoption of deep neural models, but train-\ning such models often requires large amounts of\nlabeled data. In general domains, large-scale train-\ning data is often possible to obtain through crowd-\nsourcing, but in scientiﬁc domains, annotated data\nis difﬁcult and expensive to collect due to the ex-\npertise required for quality annotation.\nAs shown through E LMo (Peters et al., 2018),\nGPT (Radford et al., 2018) and B ERT (Devlin\net al., 2019), unsupervised pretraining of language\nmodels on large corpora signiﬁcantly improves\nperformance on many NLP tasks. These models\nreturn contextualized embeddings for each token\nwhich can be passed into minimal task-speciﬁc\nneural architectures. Leveraging the success of un-\nsupervised pretraining has become especially im-\nportant especially when task-speciﬁc annotations\nare difﬁcult to obtain, like in scientiﬁc NLP. Yet\nwhile both B ERT and E LMo have released pre-\ntrained models, they are still trained on general do-\nmain corpora such as news articles and Wikipedia.\nIn this work, we make the following contribu-\ntions:\n(i) We release SCIBERT, a new resource demon-\nstrated to improve performance on a range of NLP\ntasks in the scientiﬁc domain. S CIBERT is a pre-\ntrained language model based on BERT but trained\non a large corpus of scientiﬁc text.\n(ii) We perform extensive experimentation to\ninvestigate the performance of ﬁnetuning ver-\nsus task-speciﬁc architectures atop frozen embed-\ndings, and the effect of having an in-domain vo-\ncabulary.\n(iii) We evaluate S CIBERT on a suite of tasks\nin the scientiﬁc domain, and achieve new state-of-\nthe-art (SOTA) results on many of these tasks.\n2 Methods\nBackground The BERT model architecture (De-\nvlin et al., 2019) is based on a multilayer bidirec-\ntional Transformer (Vaswani et al., 2017). Instead\nof the traditional left-to-right language modeling\nobjective, BERT is trained on two tasks: predicting\nrandomly masked tokens and predicting whether\ntwo sentences follow each other. S CIBERT fol-\nlows the same architecture as B ERT but is instead\npretrained on scientiﬁc text.\nVocabulary BERT uses WordPiece (Wu et al.,\n2016) for unsupervised tokenization of the input\ntext. The vocabulary is built such that it contains\nthe most frequently used words or subword units.\nWe refer to the original vocabulary released with\nBERT as BASE VOCAB .\n3616\nWe construct SCIVOCAB , a new WordPiece vo-\ncabulary on our scientiﬁc corpus using the Sen-\ntencePiece1 library. We produce both cased and\nuncased vocabularies and set the vocabulary size\nto 30K to match the size of BASE VOCAB . The re-\nsulting token overlap between B ASE VOCAB and\nSCIVOCAB is 42%, illustrating a substantial dif-\nference in frequently used words between scien-\ntiﬁc and general domain texts.\nCorpus We train SCIBERT on a random sample\nof 1.14M papers from Semantic Scholar (Ammar\net al., 2018). This corpus consists of 18% papers\nfrom the computer science domain and 82% from\nthe broad biomedical domain. We use the full text\nof the papers, not just the abstracts. The average\npaper length is 154 sentences (2,769 tokens) re-\nsulting in a corpus size of 3.17B tokens, similar to\nthe 3.3B tokens on which B ERT was trained. We\nsplit sentences using ScispaCy (Neumann et al.,\n2019),2 which is optimized for scientiﬁc text.\n3 Experimental Setup\n3.1 Tasks\nWe experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER)\n2. PICO Extraction (PICO)\n3. Text Classiﬁcation (CLS)\n4. Relation Classiﬁcation (REL)\n5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where\nthe model extracts spans describing the Partici-\npants, Interventions, Comparisons, and Outcomes\nin a clinical trial paper (Kim et al., 2011). REL\nis a special case of text classiﬁcation where the\nmodel predicts the type of relation expressed be-\ntween two entities, which are encapsulated in the\nsentence by inserted special tokens.\n3.2 Datasets\nFor brevity, we only describe the newer datasets\nhere, and refer the reader to the references in Ta-\nble 1 for the older datasets. EBM-NLP (Nye et al.,\n2018) annotates PICO spans in clinical trial ab-\nstracts. SciERC (Luan et al., 2018) annotates enti-\nties and relations from computer science abstracts.\n1https://github.com/google/\nsentencepiece\n2https://github.com/allenai/SciSpaCy\nACL-ARC (Jurgens et al., 2018) and SciCite (Co-\nhan et al., 2019) assign intent labels (e.g. Com-\nparison, Extension, etc.) to sentences from sci-\nentiﬁc papers that cite other papers. The Paper\nField dataset is built from the Microsoft Academic\nGraph (Sinha et al., 2015) 3 and maps paper titles\nto one of 7 ﬁelds of study. Each ﬁeld of study\n(i.e. geography, politics, economics, business, so-\nciology, medicine, and psychology) has approxi-\nmately 12K training examples.\n3.3 Pretrained B ERT Variants\nBERT-Base We use the pretrained weights for\nBERT-Base (Devlin et al., 2019) released with the\noriginal B ERT code.4 The vocabulary is B ASE -\nVOCAB . We evaluate both cased and uncased ver-\nsions of this model.\nSCIBERT We use the original B ERT code to\ntrain S CIBERT on our corpus with the same con-\nﬁguration and size as B ERT-Base. We train 4\ndifferent versions of S CIBERT: ( i) cased or un-\ncased and ( ii) B ASE VOCAB or S CIVOCAB . The\ntwo models that use B ASE VOCAB are ﬁnetuned\nfrom the corresponding B ERT-Base models. The\nother two models that use the new SCIVOCAB are\ntrained from scratch.\nPretraining B ERT for long sentences can be\nslow. Following the original B ERT code, we set a\nmaximum sentence length of 128 tokens, and train\nthe model until the training loss stops decreasing.\nWe then continue training the model allowing sen-\ntence lengths up to 512 tokens.\nWe use a single TPU v3 with 8 cores. Training\nthe SCIVOCAB models from scratch on our corpus\ntakes 1 week5 (5 days with max length 128, then\n2 days with max length 512). The B ASE VOCAB\nmodels take 2 fewer days of training because they\naren’t trained from scratch.\nAll pretrained B ERT models are converted to\nbe compatible with PyTorch using the pytorch-\ntransformers library. 6 All our models (Sec-\ntions 3.4 and 3.5) are implemented in PyTorch us-\ning AllenNLP (Gardner et al., 2017).\n3https://academic.microsoft.com/\n4https://github.com/google-research/\nbert\n5BERT’s largest model was trained on 16 Cloud TPUs for\n4 days. Expected 40-70 days (Dettmers, 2019) on an 8-GPU\nmachine.\n6https://github.com/huggingface/\npytorch-transformers\n3617\nCasing We follow Devlin et al. (2019) in using\nthe cased models for NER and the uncased models\nfor all other tasks. We also use the cased models\nfor parsing. Some light experimentation showed\nthat the uncased models perform slightly better\n(even sometimes on NER) than cased models.\n3.4 Finetuning B ERT\nWe mostly follow the same architecture, optimiza-\ntion, and hyperparameter choices used in Devlin\net al. (2019). For text classiﬁcation (i.e. CLS\nand REL), we feed the ﬁnal B ERT vector for the\n[CLS] token into a linear classiﬁcation layer. For\nsequence labeling (i.e. NER and PICO), we feed\nthe ﬁnal B ERT vector for each token into a linear\nclassiﬁcation layer with softmax output. We dif-\nfer slightly in using an additional conditional ran-\ndom ﬁeld, which made evaluation easier by guar-\nanteeing well-formed entities. For DEP, we use\nthe model from Dozat and Manning (2017) with\ndependency tag and arc embeddings of size 100\nand biafﬁne matrix attention over BERT vectors in-\nstead of stacked BiLSTMs.\nIn all settings, we apply a dropout of 0.1 and\noptimize cross entropy loss using Adam (Kingma\nand Ba, 2015). We ﬁnetune for 2 to 5 epochs using\na batch size of 32 and a learning rate of 5e-6, 1e-\n5, 2e-5, or 5e-5 with a slanted triangular schedule\n(Howard and Ruder, 2018) which is equivalent to\nthe linear warmup followed by linear decay (De-\nvlin et al., 2019). For each dataset and BERT vari-\nant, we pick the best learning rate and number of\nepochs on the development set and report the cor-\nresponding test results.\nWe found the setting that works best across\nmost datasets and models is 2 or 4 epochs and a\nlearning rate of 2e-5. While task-dependent, op-\ntimal hyperparameters for each task are often the\nsame across BERT variants.\n3.5 Frozen B ERT Embeddings\nWe also explore the usage of B ERT as pretrained\ncontextualized word embeddings, like ELMo (Pe-\nters et al., 2018), by training simple task-speciﬁc\nmodels atop frozen BERT embeddings.\nFor text classiﬁcation, we feed each sentence of\nBERT vectors into a 2-layer BiLSTM of size 200\nand apply a multilayer perceptron (with hidden\nsize 200) on the concatenated ﬁrst and last BiL-\nSTM vectors. For sequence labeling, we use the\nsame BiLSTM layers and use a conditional ran-\ndom ﬁeld to guarantee well-formed predictions.\nFor DEP, we use the full model from Dozat and\nManning (2017) with dependency tag and arc em-\nbeddings of size 100 and the same BiLSTM setup\nas other tasks. We did not ﬁnd changing the depth\nor size of the BiLSTMs to signiﬁcantly impact re-\nsults (Reimers and Gurevych, 2017).\nWe optimize cross entropy loss using Adam,\nbut holding B ERT weights frozen and applying a\ndropout of 0.5. We train with early stopping on\nthe development set (patience of 10) using a batch\nsize of 32 and a learning rate of 0.001.\nWe did not perform extensive hyperparameter\nsearch, but while optimal hyperparameters are go-\ning to be task-dependent, some light experimenta-\ntion showed these settings work fairly well across\nmost tasks and BERT variants.\n4 Results\nTable 1 summarizes the experimental results. We\nobserve that S CIBERT outperforms B ERT-Base\non scientiﬁc tasks (+2.11 F1 with ﬁnetuning and\n+2.43 F1 without) 8. We also achieve new SOTA\nresults on many of these tasks using SCIBERT.\n4.1 Biomedical Domain\nWe observe that S CIBERT outperforms B ERT-\nBase on biomedical tasks (+1.92 F1 with ﬁnetun-\ning and +3.59 F1 without). In addition, S CIB-\nERT achieves new SOTA results on BC5CDR and\nChemProt (Lee et al., 2019), and EBM-NLP (Nye\net al., 2018).\nSCIBERT performs slightly worse than SOTA\non 3 datasets. The SOTA model for JNLPBA is a\nBiLSTM-CRF ensemble trained on multiple NER\ndatasets not just JNLPBA (Yoon et al., 2018). The\nSOTA model for NCBI-disease is B IOBERT (Lee\net al., 2019), which is B ERT-Base ﬁnetuned on\n18B tokens from biomedical papers. The SOTA\nresult for GENIA is in Nguyen and Verspoor\n(2019) which uses the model from Dozat and\nManning (2017) with part-of-speech (POS) fea-\ntures, which we do not use.\nIn Table 2, we compare S CIBERT results\nwith reported B IOBERT results on the subset of\ndatasets included in (Lee et al., 2019). Interest-\ning, S CIBERT outperforms B IOBERT results on\n7The SOTA paper did not report a single score. We\ncompute the average of the reported results for each class\nweighted by number of examples in each class.\n8For rest of this paper, all results reported in this manner\nare averaged over datasets excluding UAS for DEP since we\nalready include LAS.\n3618\nField Task Dataset SOTA B ERT-Base S CIBERT\nFrozen Finetune Frozen Finetune\nBio\nNER\nBC5CDR (Li et al., 2016) 88.85 7 85.08 86.72 88.73 90.01\nJNLPBA (Collier and Kim, 2004) 78.58 74.05 76.09 75.77 77.28\nNCBI-disease (Dogan et al., 2014) 89.36 84.06 86.88 86.39 88.57\nPICO EBM-NLP (Nye et al., 2018) 66.30 61.44 71.53 68.30 72.28\nDEP GENIA (Kim et al., 2003) - LAS 91.92 90.22 90.33 90.36 90.43\nGENIA (Kim et al., 2003) - UAS 92.84 91.84 91.89 92.00 91.99\nREL ChemProt (Kringelum et al., 2016) 76.68 68.21 79.14 75.03 83.64\nCS\nNER SciERC (Luan et al., 2018) 64.20 63.58 65.24 65.77 67.57\nREL SciERC (Luan et al., 2018) n/a 72.74 78.71 75.25 79.97\nCLS ACL-ARC (Jurgens et al., 2018) 67.9 62.04 63.91 60.74 70.98\nMulti CLS Paper Field n/a 63.64 65.37 64.38 65.71\nSciCite (Cohan et al., 2019) 84.0 84.31 84.85 85.42 85.49\nAverage 73.58 77.16 76.01 79.27\nTable 1: Test performances of all BERT variants on all tasks and datasets. Bold indicates the SOTA result (multiple\nresults bolded if difference within 95% bootstrap conﬁdence interval). Keeping with past work, we report macro\nF1 scores for NER (span-level), macro F1 scores for REL and CLS (sentence-level), and macro F1 for PICO\n(token-level), and micro F1 for ChemProt speciﬁcally. For DEP, we report labeled (LAS) and unlabeled (UAS)\nattachment scores (excluding punctuation) for the same model with hyperparameters tuned for LAS. All results\nare the average of multiple runs with different random seeds.\nTask Dataset B IOBERT SCIBERT\nNER\nBC5CDR 88.85 90.01\nJNLPBA 77.59 77.28\nNCBI-disease 89.36 88.57\nREL ChemProt 76.68 83.64\nTable 2: Comparing S CIBERT with the reported\nBIOBERT results on biomedical datasets.\nBC5CDR and ChemProt, and performs similarly\non JNLPBA despite being trained on a substan-\ntially smaller biomedical corpus.\n4.2 Computer Science Domain\nWe observe that S CIBERT outperforms B ERT-\nBase on computer science tasks (+3.55 F1 with\nﬁnetuning and +1.13 F1 without). In addition,\nSCIBERT achieves new SOTA results on ACL-\nARC (Cohan et al., 2019), and the NER part of\nSciERC (Luan et al., 2018). For relations in Sci-\nERC, our results are not comparable with those in\nLuan et al. (2018) because we are performing re-\nlation classiﬁcation given gold entities, while they\nperform joint entity and relation extraction.\n4.3 Multiple Domains\nWe observe that S CIBERT outperforms B ERT-\nBase on the multidomain tasks (+0.49 F1 with\nﬁnetuning and +0.93 F1 without). In addition,\nSCIBERT outperforms the SOTA on SciCite (Co-\nhan et al., 2019). No prior published SOTA results\nexist for the Paper Field dataset.\n5 Discussion\n5.1 Effect of Finetuning\nWe observe improved results via BERT ﬁnetuning\nrather than task-speciﬁc architectures atop frozen\nembeddings (+3.25 F1 with S CIBERT and +3.58\nwith BERT-Base, on average). For each scientiﬁc\ndomain, we observe the largest effects of ﬁnetun-\ning on the computer science (+5.59 F1 with SCIB-\nERT and +3.17 F1 with B ERT-Base) and biomed-\nical tasks (+2.94 F1 with S CIBERT and +4.61 F1\nwith BERT-Base), and the smallest effect on mul-\ntidomain tasks (+0.7 F1 with S CIBERT and +1.14\nF1 with B ERT-Base). On every dataset except\nBC5CDR and SciCite, BERT-Base with ﬁnetuning\noutperforms (or performs similarly to) a model us-\ning frozen SCIBERT embeddings.\n5.2 Effect of S CIVOCAB\nWe assess the importance of an in-domain sci-\nentiﬁc vocabulary by repeating the ﬁnetuning ex-\nperiments for S CIBERT with B ASE VOCAB . We\nﬁnd the optimal hyperparameters for S CIBERT-\nBASE VOCAB often coincide with those of S CIB-\nERT-SCIVOCAB .\nAveraged across datasets, we observe +0.60 F1\nwhen using S CIVOCAB . For each scientiﬁc do-\n3619\nmain, we observe +0.76 F1 for biomedical tasks,\n+0.61 F1 for computer science tasks, and +0.11 F1\nfor multidomain tasks.\nGiven the disjoint vocabularies (Section 2) and\nthe magnitude of improvement over B ERT-Base\n(Section 4), we suspect that while an in-domain\nvocabulary is helpful, S CIBERT beneﬁts most\nfrom the scientiﬁc corpus pretraining.\n6 Related Work\nRecent work on domain adaptation of BERT in-\ncludes B IOBERT (Lee et al., 2019) and C LINI -\nCAL BERT (Alsentzer et al., 2019; Huang et al.,\n2019). B IOBERT is trained on PubMed ab-\nstracts and PMC full text articles, and C LIN -\nICAL BERT is trained on clinical text from the\nMIMIC-III database (Johnson et al., 2016). In\ncontrast, S CIBERT is trained on the full text of\n1.14M biomedical and computer science papers\nfrom the Semantic Scholar corpus (Ammar et al.,\n2018). Furthermore, S CIBERT uses an in-domain\nvocabulary (S CIVOCAB ) while the other above-\nmentioned models use the original B ERT vocab-\nulary (BASE VOCAB ).\n7 Conclusion and Future Work\nWe released S CIBERT, a pretrained language\nmodel for scientiﬁc text based on BERT. We evalu-\nated SCIBERT on a suite of tasks and datasets from\nscientiﬁc domains. S CIBERT signiﬁcantly outper-\nformed B ERT-Base and achieves new SOTA re-\nsults on several of these tasks, even compared to\nsome reported BIOBERT (Lee et al., 2019) results\non biomedical tasks.\nFor future work, we will release a version of\nSCIBERT analogous to BERT-Large, as well as ex-\nperiment with different proportions of papers from\neach domain. Because these language models are\ncostly to train, we aim to build a single resource\nthat’s useful across multiple domains.\nAcknowledgment\nWe thank the anonymous reviewers for their com-\nments and suggestions. We also thank Waleed\nAmmar, Noah Smith, Yoav Goldberg, Daniel\nKing, Doug Downey, and Dan Weld for their help-\nful discussions and feedback. All experiments\nwere performed on beaker.org and supported\nin part by credits from Google Cloud.\nReferences\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nB. A. McDermott. 2019. Publicly available clini-\ncal bert embeddings. In ClinicalNLP workshop at\nNAACL.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier,\nKyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Pe-\nters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang,\nChris Wilhelm, Zheng Yuan, Madeleine van Zuylen,\nand Oren Etzioni. 2018. Construction of the litera-\nture graph in semantic scholar. In NAACL.\nArman Cohan, Waleed Ammar, Madeleine van Zuylen,\nand Field Cady. 2019. Structural scaffolds for cita-\ntion intent classiﬁcation in scientiﬁc publications. In\nNAACL-HLT, pages 3586–3596, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nNigel Collier and Jin-Dong Kim. 2004. Introduction\nto the bio-entity recognition task at jnlpba. In NLP-\nBA/BioNLP.\nTim Dettmers. 2019. TPUs vs GPUs\nfor Transformers (BERT). http:\n//timdettmers.com/2018/10/17/\ntpus-vs-gpus-for-transformers-bert/ .\nAccessed: 2019-02-22.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nRezarta Islamaj Dogan, Robert Leaman, and Zhiyong\nLu. 2014. NCBI disease corpus: A resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. ICLR.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2017. Allennlp: A deep semantic natural language\nprocessing platform. In arXiv:1803.07640.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and pre-\ndicting hospital readmission. arXiv:1904.05342.\nAlistair E. W. Johnson, Tom J. Pollard aand Lu Shen,\nLiwei H. Lehman, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, , and Roger G. Mark. 2016.\n3620\nMimic-iii, a freely accessible critical care database.\nIn Scientiﬁc Data, 3:160035.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Daniel A.\nMcFarland, and Daniel Jurafsky. 2018. Measuring\nthe evolution of a scientiﬁc ﬁeld through citation\nframes. TACL, 06:391–406.\nJin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and\nJun’ichi Tsujii. 2003. GENIA corpus - a semanti-\ncally annotated corpus for bio-textmining. Bioinfor-\nmatics, 19:i180i182.\nSu Kim, David Mart´ınez, Lawrence Cavedon, and Lars\nYencken. 2011. Automatic classiﬁcation of sen-\ntences to support evidence based medicine. In BMC\nBioinformatics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. ICLR.\nJens Kringelum, Sonny Kim Kjærulff, Søren Brunak,\nOle Lund, Tudor I. Oprea, and Olivier Taboureau.\n2016. ChemProt-3.0: a global chemical biology dis-\neases mapping. In Database.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. In arXiv:1901.08746.\nJiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers,\nand Zhiyong Lu. 2016. BioCreative V CDR task\ncorpus: a resource for chemical disease relation\nextraction. Database : the journal of biological\ndatabases and curation.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiﬁcation of enti-\nties, relations, and coreference for scientiﬁc knowl-\nedge graph construction. In EMNLP.\nMark Neumann, Daniel King, Iz Beltagy, and Waleed\nAmmar. 2019. ScispaCy: Fast and robust mod-\nels for biomedical natural language processing. In\narXiv:1902.07669.\nDat Quoc Nguyen and Karin M. Verspoor. 2019. From\npos tagging to dependency parsing for biomedical\nevent extraction. BMC Bioinformatics, 20:1–13.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei\nYang, Iain James Marshall, Ani Nenkova, and By-\nron C. Wallace. 2018. A corpus with multi-level an-\nnotations of patients, interventions and outcomes to\nsupport language processing for medical literature.\nIn ACL.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke S. Zettlemoyer. 2018. Deep contextualized\nword representations. In NAACL-HLT.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nNils Reimers and Iryna Gurevych. 2017. Optimal hy-\nperparameters for deep lstm-networks for sequence\nlabeling tasks. In EMNLP.\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-\nrin Eide, Bo-June Paul Hsu, and Kuansan Wang.\n2015. An overview of microsoft academic service\n(MAS) and applications. In WWW.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Jeff Klingner,\nApurva Shah, Melvin Johnson, Xiaobing Liu,\nLukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGregory S. Corrado, Macduff Hughes, and Jeffrey\nDean. 2016. Google’s neural machine translation\nsystem: Bridging the gap between human and ma-\nchine translation. abs/1609.08144.\nWonjin Yoon, Chan Ho So, Jinhyuk Lee, and Jaewoo\nKang. 2018. CollaboNet: collaboration of deep neu-\nral networks for biomedical named entity recogni-\ntion. In DTMBio workshop at CIKM.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7816585898399353
    },
    {
      "name": "Natural language processing",
      "score": 0.6513878107070923
    },
    {
      "name": "Language model",
      "score": 0.5830199718475342
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5164878368377686
    },
    {
      "name": "Programming language",
      "score": 0.35958153009414673
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ],
  "cited_by": 2765
}