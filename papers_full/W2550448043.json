{
  "title": "Reference-Aware Language Models",
  "url": "https://openalex.org/W2550448043",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2361346764",
      "name": "Yang Zi-chao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2900299007",
      "name": "Blunsom, Phil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221974744",
      "name": "Dyer, Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A390312750",
      "name": "Ling Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2118434577",
    "https://openalex.org/W1975244201",
    "https://openalex.org/W2952013107",
    "https://openalex.org/W2403702038",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2410983263",
    "https://openalex.org/W1789782362",
    "https://openalex.org/W2333611780",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2561658355",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2336260055",
    "https://openalex.org/W2257123346",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2340944142",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2412715517",
    "https://openalex.org/W2207587218",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2964165364"
  ],
  "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or discourse context, even when the targets of the reference may be rare words. Experiments on three tasks shows our model variants based on deterministic attention.",
  "full_text": "Reference-Aware Language Models\nZichao Yang1∗, Phil Blunsom2,3, Chris Dyer1,2, and Wang Ling2\n1Carnegie Mellon University, 2DeepMind, and 3University of Oxford\nzichaoy@cs.cmu.edu, {pblunsom,cdyer,lingwang}@google.com\nAbstract\nWe propose a general class of language\nmodels that treat reference as discrete\nstochastic latent variables. This decision\nallows for the creation of entity mentions\nby accessing external databases of refer-\nents (required by, e.g., dialogue genera-\ntion) or past internal state (required to ex-\nplicitly model coreferentiality). Beyond\nsimple copying, our coreference model\ncan additionally refer to a referent using\nvaried mention forms (e.g., a reference to\n“Jane” can be realized as “she”), a charac-\nteristic feature of reference in natural lan-\nguages. Experiments on three representa-\ntive applications show our model variants\noutperform models based on deterministic\nattention and standard language modeling\nbaselines.\n1 Introduction\nReferring expressions (REs) in natural language\nare noun phrases (proper nouns, common nouns,\nand pronouns) that identify objects, entities, and\nevents in an environment. REs occur frequently\nand they play a key role in communicating infor-\nmation efﬁciently. While REs are common in nat-\nural language, most previous work does not model\nthem explicitly, either treating REs as ordinary\nwords in the model or replacing them with special\ntokens that are ﬁlled in with a post processing step\n(Wen et al., 2015; Luong et al., 2015). Here we\npropose a language modeling framework that ex-\nplicitly incorporates reference decisions. In part,\nthis is based on the principle of pointer networks\nin which copies are made from another source\n(G¨ulc ¸ehre et al., 2016; Gu et al., 2016; Ling et al.,\n∗Work completed at DeepMind.\ndialogue\nrecipe\ncoref\nM: the nirala is a \nnice restuarant\n1) soy milk\n2) leaves\n3) banana\nBlend soy milk and …\n[I]1 think…Go ahead [Linda] 2 … thanks goes to [you]1 …\na) reference to a list\nb) reference to a table\nc) reference to document context\nthe nirala moderate lebanese\nali baba moderate indian\nname price food\nFigure 1: Reference-aware language models.\n2016; Vinyals et al., 2015; Ahn et al., 2016; Mer-\nity et al., 2016). However, in the full version of our\nmodel, we go beyond simple copying and enable\ncoreferent mentions to have different forms, a key\ncharacteristic of natural language reference.\nFigure 1 depicts examples of REs in the con-\ntext of the three tasks that we consider in this\nwork. First, many models need to refer to a list\nof items (Kiddon et al., 2016; Wen et al., 2015).\nIn the task of recipe generation from a list of\ningredients (Kiddon et al., 2016), the generation\nof the recipe will frequently refer to these items.\nAs shown in Figure 1, in the recipe “Blend soy\nmilk and . . . ”,soy milk refers to the ingredi-\nent summaries. Second, reference to a database is\ncrucial in many applications. One example is in\ntask oriented dialogue where access to a database\nis necessary to answer a user’s query (Young et al.,\n2013; Li et al., 2016; Vinyals and Le, 2015; Wen\net al., 2015; Sordoni et al., 2015; Serban et al.,\n2016; Bordes and Weston, 2016; Williams and\nZweig, 2016; Shang et al., 2015; Wen et al., 2016).\nHere we consider the domain of restaurant rec-\nommendation where a system refers to restau-\nrants (name) and their attributes (address, phone\nnumber etc) in its responses. When the system\narXiv:1611.01628v5  [cs.CL]  9 Aug 2017\nsays “the nirala is a nice restaurant”, it refers\nto the restaurant name the nirala from the\ndatabase. Finally, we address references within\na document (Mikolov et al., 2010; Ji et al., 2015;\nWang and Cho, 2015), as the generation of words\nwill often refer to previously generated words. For\ninstance the same entity will often be referred to\nthroughout a document. In Figure 1, the entity\nyou refers to I in a previous utterance. In this\ncase, copying is insufﬁcient– although the referent\nis the same, the form of the mention is different.\nIn this work we develop a language model that\nhas a speciﬁc module for generating REs. A se-\nries of decisions (should I generate a RE? If yes,\nwhich entity in the context should I refer to? How\nshould the RE be rendered?) augment a traditional\nrecurrent neural network language model and the\ntwo components are combined as a mixture model.\nSelecting an entity in context is similar to famil-\niar models of attention (Bahdanau et al., 2014),\nbut rather than being a soft decision that reweights\nrepresentations of elements in the context, it is\ntreated as a hard decision over contextual elements\nwhich are stochastically selected and then copied\nor, if the task warrants it, transformed (e.g., a pro-\nnoun rather than a proper name is produced as out-\nput). In cases when the stochastic decision is not\navailable in training, we treat it as a latent vari-\nable and marginalize it out. For each of the three\ntasks, we pick one representative application and\ndemonstrate our reference aware model’s efﬁcacy\nin evaluations against models that do not explicitly\ninclude a reference operation.\nOur contributions are as follows:\n•We propose a general framework to model\nreference in language. We consider refer-\nence to entries in lists, tables, and document\ncontext. We instantiate these tasks into three\nspeciﬁc applications: recipe generation, dia-\nlogue modeling, and coreference based lan-\nguage models.\n•We develop the ﬁrst neural model of refer-\nence that goes being copying and can model\n(conditional on context) how to form the\nmention.\n•We perform comprehensive evaluation of our\nmodels on the three data sets and verify our\nproposed models perform better than strong\nbaselines.\n2 Reference-aware language models\nHere we propose a general framework for\nreference-aware language models.\nWe denote each document as a series of to-\nkens x1, . . . , xL, where L is the number of tokens.\nOur goal is to maximize p(xi | ci), the proba-\nbility of each word xi given its previous context\nci = x1, . . . , xi−1. In contrast to traditional neu-\nral language models, we introduce a variable zi\nat each position, which controls the decision on\nwhich source xi is generated from. Then the con-\nditional probability is given by:\np(xi, zi |ci) = p(xi |zi, ci)p(zi |ci), (1)\nwhere zi has different meanings in different con-\ntexts. If xi is from a reference list or a database,\nthen zi is one dimensional andzi = 1 denotes xi is\ngenerated as a reference. zi can also model more\ncomplex decisions. In coreference based language\nmodel, zi denotes a series of sequential decisions,\nsuch as whether xi is an entity, if yes, which entity\nxi refers to. When zi is not observed, we will train\nour model to maximize the marginal probability\nover zi, i.e., p(xi|ci) = ∑\nzi p(xi|zi, ci)p(zi|ci).\n2.1 Reference to lists\nWe begin to instantiate the framework by consid-\nering reference to a list of items. Referring to a\nlist of items has broad applications, such as gen-\nerating documents based on summaries etc. Here\nwe speciﬁcally consider the application of recipe\ngeneration conditioning on the ingredient lists. Ta-\nble. 1 illustrates the ingredient list and recipe for\nSpinach and Banana Power Smoothie . We can\nsee that the ingredients soy milk, spinach\nleaves, and banana occur in the recipe.\nsoy\ndecoder\n\u0001\nlist z\nYes No\nencoder\nBlend\nsoy\nBOS\nBlend\n1)\n2)\n3)\nFigure 2: Recipe pointer\nLet the ingredients of a recipe be X = {xi}T\ni=1\nand each ingredient contains L tokens xi =\nIngredients Recipe\n1 cup plain soy milk Blend soy milk and spinach leaves\ntogether in a blender until smooth. Add\nbanana and pulse until thoroughly blended.\n3/4 cup packed fresh spinach leaves\n1 large banana, sliced\nTable 1: Ingredients and recipe for Spinach and Banana Power Smoothie.\n{xij}L\nj=1. The corresponding recipe is y =\n{yv}K\nv=1. We would like to model p(y|X) =\nΠvp(yv|X, y<v).\nWe ﬁrst use a LSTM (Hochreiter and Schmid-\nhuber, 1997) to encode each ingredient: hi,j =\nLSTME(WExij, hi,j−1) ∀i. Then, we sum the\nresulting ﬁnal state of each ingredient to obtain the\nstarting LSTM state of the decoder. We use an at-\ntention based decoder:\nsv = LSTMD([WEyv−1, dv−1], sv−1),\npcopy\nv = ATTN({{hi,j}T\ni=1}L\nj=1, sv),\ndv =\n∑\nij\npv,i,jhi,j,\np(zv|sv) = sigmoid(W[sv, dv]),\npvocab\nv = softmax(W[sv, dv]),\nwhere ATTN(h, q) is the attention function that\nreturns the probability distribution over the set\nof vectors h, conditioned on any input represen-\ntation q. A full description of this operation is\ndescribed in (Bahdanau et al., 2014). The deci-\nsion to copy from the ingredient list or generate\na new word from the softmax is performed us-\ning a switch, denoted as p(zv|sv). We can ob-\ntain a probability distribution of copying each of\nthe words in the ingredients by computingpcopy\nv =\nATTN({{hi,j}T\ni=1}L\nj=1, sv) in the attention mech-\nanism.\nObjective: We can obtain the value of zv through\na string match of tokens in recipes with tokens\nin ingredients. If a token appears in the ingre-\ndients, we set zv = 1 and zv = 0 otherwise.\nWe can train the model in a fully supervised fash-\nion, i.e., we can obtain the probability of yv as\np(yv, zv|sv) = pcopy\nv (yv)p(1|sv) if zv = 1 and\npvocab\nv (yv)(1 −p(1|si,v)) otherwise.\nHowever, it may be not be accurate. In many\ncases, the tokens that appear in the ingredients do\nnot speciﬁcally refer to ingredients tokens. For ex-\namples, the recipe may start with “Prepare a cup\nof water”. The token “cup” does not refer to the\n“cup” in the ingredient list “1 cup plain soy milk”.\nTo solve this problem, we treat zi as a latent vari-\nable, we wish to maximize the marginal probabil-\nity of yv over all possible values ofzv. In this way,\nthe model can automatically learn when to refer to\ntokens in the ingredients. Thus, the probability of\ngenerating token yv is deﬁned as:\np(yv|sv) = pvocab\nv (yv)p(0|sv) + pcopy\nv (yv)p(1|sv)\n= pvocab\nv (yv)(1 −p(1|sv)) + pcopy\nv (yv)p(1|sv).\nIf no string match is found for yv, we simply set\npcopy\nv (yv) = 0 in the above objective.\n2.2 Reference to databases\nWe then consider the more complicated task of ref-\nerence to database entries. Referring to databases\nis quite common in question answering and di-\nalogue systems, in which databases are external\nknowledge and they are resorted to reply users’\nquery. In our paper, we consider the application\nof task-oriented dialogue systems in the domain\nof restaurant recommendations. Different from\nlists that are one dimensional, databases are two-\ndimensional and referring to table entries requires\nsophisticated model design.\nTo better understand the model, we ﬁrst make\na brief introduction of the data set. We use dia-\nlogues from the second Dialogue State Tracking\nChallenge (DSTC2) (Henderson et al., 2014). Ta-\nble. 3 is one example dialogue from this dataset.\nWe can observe from this example, users get\nrecommendations of restaurants based on queries\nthat specify the area, price and food type of the\nrestaurant. We can support the system’s deci-\nsions by incorporating a mechanism that allows\nthe model to query the database to ﬁnd restau-\nrants that satisfy the users’ queries. A sample of\nour database (refer to data preparation part on how\nwe construct the database) is shown in Table 2.\nWe can observe that each restaurant contains 6 at-\ntributes that are generally referred in the dialogue\ndataset. As such, if the user requests a restau-\nrant that serves “indian” food, we wish to train\na model that can search for entries whose “food”\nNAME PRICE RANGE FOOD AREA ADDRESS POST CODE PHONE\nali baba moderate lebanese centre 59 Hills Road City Centre CB 2, 1 NT 01462 432565\nthe nirala moderate indian north 7 Milton Road Chesterton CB 4, 1 UY 01223 360966\nTable 2: Fragment of database for dialogue system.\nM: Hello, welcome to the Cambridge\nrestaurant system? You can ask for\nrestaurants by area, price range\nor food type . How may I help you?\nU: i want a moderately priced\nrestaurant in the north town\nM: the nirala is a nice restaurant in\nthe north of town in the moderate price\nrange\nU: what is the address\nM: Sure , the nirala is on 7 Milton\nRoad Chesterton\nTable 3: Example dialogue, M stands for Machine\nand U stands for User\nM U M U\nsentence encoder\nturn encoder\ndecoder\n\u0001\nattn\nFigure 3: Hierarchical RNN Seq2Seq model. The\nred box denotes attention mechanism over the ut-\nterances in the previous turn.\ncolumn contains “indian”. Now, we describe how\nwe deploy a model that fulﬁlls these requirements.\nWe ﬁrst introduce the basic dialogue framework in\nwhich we incorporates the table reference module.\nBasic Dialogue Framework: We build a basic\ndialogue model based on the hierarchical RNN\nmodel described in (Serban et al., 2016), as in di-\nalogues, the generation of the response is not only\ndependent on the previous sentence, but on all sen-\ntences leading to the response. We assume that\na dialogue is alternated between a machine and a\nuser. An illustration of the model is shown in Fig-\nure 3.\nConsider a dialogue with T turns, the utter-\nances from a user and a machines are denoted\nas X = {xi}T\ni=1 and Y = {yi}T\ni=1 respectively,\nwhere i is the i-th utterance. We deﬁne xi =\n{xij}|xi|\nj=1, yi = {yiv}|yi|\nv=1, where xij (yiv) denotes\nthe j-th ( v-th) token in the i-th utterance from\nthe user (the machines). The dialogue sequence\nstarts with a machine utterance and is given by\n{y1, x1, y2, x2, . . . , yT , xT }. We would like to\nmodel the utterances from the machine\np(y1, y2, . . . , yT |x1, x2, . . . , xT ) =\n∏\ni\np(yi|y<i, x<i) =\n∏\ni,v\np(yi,v|yi,<v, y<i, x<i).\nWe encode y<i and x<i into continuous space\nin a hierarchical way with LSTM: Sentence En-\ncoder: For a given utterance xi, We encode it as\nhx\ni,j = LSTME(WExi,j, hx\ni,j−1). The representa-\ntion of xi is given by the hx\ni = hx\ni,|xi|. The same\nprocess is applied to obtain the machine utterance\nrepresentation hy\ni = hy\ni,|yi|. Turn Encoder: We\nfurther encode the sequence {hy\n1, hx\n1 , ..., hy\ni , hx\ni }\nwith another LSTM encoder. We shall refer the\nlast hidden state as ui, which can be seen as the\nhierarchical encoding of the previous i utterances.\nDecoder: We use ui−1 as the initial state of de-\ncoder LSTM and decode each token in yi. We can\nexpress the decoder as:\nsy\ni,v = LSTMD(WEyi,v−1, si,v−1),\npy\ni,v = softmax(Wsy\ni,v).\nWe can also incoroprate the attetionn mecha-\nnism in the decoder. As shown in Figure. 3, we\nuse the attention mechanism over the utterance in\nthe previous turn. Due to space limit, we don’t\npresent the attention based decoder mathmatically\nand readers can refer to (Bahdanau et al., 2014) for\ndetails.\n2.2.1 Incorporating Table Reference\nWe now extend the decoder in order to allow the\nmodel to condition the generation on a database.\nPointer Switch: We use zi,v ∈{0, 1}to denote\nthe decision of whether to copy one cell from the\ntable. We compute this probability as follows:\np(zi,v|si,v) = sigmoid(Wsi,v).\nThus, if zi,v = 1, the next token yi,v is generated\nfrom the database, whereas if zi,v = 0 , then the\nfollowing token is generated from a softmax. We\nnow describe how we generate tokens from the\ndatabase.\nqq\nattributes\ntable\nz\nYes No\nU\ndecoder Table Pointer\nStep 1: attribute attn\nStep 3: row attn\nStep 5: column\n attn\np a\np a\np copy\np copy\np vocab\np vocab\np c\np c\np r\np r\nFigure 4: Decoder with table pointer.\nWe denote a table with R rows and C columns\nas {tr,c}, r∈[1, R], c∈[1, C], where tr,c is the\ncell in row r and column c. The attribute of each\ncolumn is denoted as sc, where c is the c-th at-\ntribute. tr,c and sc are one-hot vector.\nTable Encoding: To encode the table, we ﬁrst\nbuild an attribute vector and then an encoding vec-\ntor for each cell. The attribute vector is simply an\nembedding lookup gc = WEsc. For the encod-\ning of each cell, we ﬁrst concatenate embedding\nlookup of the cell with the corresponding attribute\nvector gc and then feed it through a one-layer MLP\nas follows: then er,c = tanh(W[WEtr,c, gc]).\nTable Pointer: The detailed process of calculating\nthe probability distribution over the table is shown\nin Figure 4. The attention over cells in the table\nis conditioned on a given vector q, similarly to the\nattention model for sequences. However, rather\nthan a sequence of vectors, we now operate over a\ntable.\nStep 1: Attention over the attributes to ﬁnd\nout the attributes that a user asks about, pa =\nATTN({gc}, q). Suppose a user says cheap, then\nwe should focus on the price attribute.\nStep 2: Conditional row representation calcula-\ntion, er = ∑\nc pa\nc er,c ∀r. So that er contains the\nprice information of the restaurant in row r.\nStep 3 : Attention over er to ﬁnd out the\nrestaurants that satisfy users’ query, pr =\nATTN({er}, q). Restaurants with cheap price\nwill be picked.\nStep 4: Using the probabilities pr, we compute\nthe weighted average over the all rows ec =∑\nr pr\nrer,c. {er} contains the information of\ncheap restaurant.\nStep 5: Attention over columns {er}to compute\nthe probabilities of copying each column pc =\nATTN({ec}, q).\nStep 6: To get the probability matrix of copying\neach cell, we simply compute the outer product\npcopy = pr ⊗pc.\nThe overall process is as follows:\npa = ATTN({gc}, q),\ner =\n∑\nc\npa\nc er,c ∀r,\npr = ATTN({er}, q),\nec =\n∑\nr\npr\nrer,c ∀c,\npc = ATTN({ec}, q),\npcopy = pr ⊗pc.\nIf zi,v = 1, we embed the above attention pro-\ncess in the decoder by replacing the conditioned\nstate q with the current decoder state sy\ni,v.\nObjective: As in previous task, we can train the\nmodel in a fully supervised fashion, or we can\ntreat the decision as a latent variable. We can get\np(yi,v|si,v) in a similar way.\n2.3 Reference to document context\nFinally, we address the references that happen in a\ndocument itself and build a language model that\nuses coreference links to point to previous enti-\nties. Before generating a word, we ﬁrst make the\ndecision on whether it is an entity mention. If\nso, we decide which entity this mention belongs\nto, then we generate the word based on that en-\ntity. Denote the document as X = {xi}L\ni=1, and\nthe entities are E = {ei}N\ni=1, each entity has Mi\nmentions, ei = {mij}Mi\nj=1, such that {xmij }Mi\nj=1\nrefer to the same entity. We use a LSTM to\nmodel the document, the hidden state of each to-\nken is hi = LSTM(WExi, hi−1). We use a set\nhe = {he\n0, he\n1, ..., he\nM }to keep track of the entity\nstates, where he\nj is the state of entity j.\nWord generation: At each time step before gen-\nerating the next word, we predict whether the word\nis an entity mention:\npcoref(vi|hi−1, he) = ATTN(he, hi−1),\ndi =\n∑\nvi\np(vi)he\nvi ,\np(zi|hi−1) = sigmoid(W[hi−1, di]),\nwhere zi denotes whether the next word is\nan entity and if yes vi denotes which entity\nthe next word corefers to. If the next word is\nan entity mention, then p(xi|vi, hi−1, he) =\num and [I]1 think that is whats - Go ahead [Linda] 2. Well and thanks goes to\n[you]1 and to [the media]3 to help [us]4...So [our]4 hat is off to all of [you]5...\n[I]1um\nentity state\nupdate process\nI\n\u0001\n[Linda]2\nI\nLinda\n[You]1\nYou\nLinda\n\u0001\nupdate statepush state\nempty\nstate\n0 0\n1\n0\n1\n2\n0\n1\n2\npush stateattn\n… …\nattn\nand\n[I]1\nof\n[You]1\nnew\nentity\nentity\n1\nFigure 5: Coreference based language model, example taken from Wiseman et al. (2016).\nsoftmax(W1 tanh(W2[hi−1, he\nvi ])) else\np(xi|hi−1) = softmax(W1hi−1). Hence,\np(xi|x<i) =\n\n\n\np(xi|hi−1)p(zi|hi−1, he) if zi = 0.\np(xi|vi, hi−1, he)×\npcoref(vi|hi−1, he)p(zi|hi−1, he) if zi = 1.\nEntity state update: Since there are multiple\nmentions for each entity and the mentions appear\ndynamically, we need to keep track of the entity\nstate in order to use coreference information in en-\ntity mention prediction. We update the entity state\nhe at each time step. In the beginning, he = {he\n0},\nhe\n0 denotes the state of an virtual empty entity and\nis a learnable variable. If zi = 1 and vi = 0 ,\nthen it indicates the next word is a new entity men-\ntion, then in the next step, we appendhi to he, i.e.,\nhe = {he, hi}, if zi = 1 and vi > 0, then we\nupdate the corresponding entity state with the new\nhidden state, he[vi] = hi. Another way to update\nthe entity state is to use one LSTM to encode the\nmention states and get the new entity state. Here\nwe use the latest entity mention state as the new\nentity state for simplicity. The detailed update pro-\ncess is shown in Figure 5.\nNote that the stochastic decisions in this task are\nmore complicated than previous two tasks. We\nneed to make two sequential decisions: whether\nthe next word is an entity mention, and if yes,\nwhich entity the mention corefers to. It is in-\ntractable to marginalize these decisions, so we\ntrain this model in a supervised fashion (refer to\ndata preparation part on how we get coreference\nannotations).\n3 Experiments\n3.1 Data sets and preprocessing\nRecipes: We crawled all recipes from www.\nallrecipes.com. There are about 31, 000\nrecipes in total, and every recipe has an ingredi-\nent list and a corresponding recipe. We exclude\nthe recipes that have less than 10 tokens or more\nthan 500 tokens, those recipes take about 0.1% of\nall data set. On average each recipe has 118 to-\nkens and 9 ingredients. We random shufﬂe the\nwhole data set and take 80% as training and 10%\nfor validation and test. We use a vocabulary size\nof 10,000 in the model.\nDialogue: We use the DSTC2 data set. We only\nuse the dialogue transcripts from the data set.\nThere are about 3,200 dialogues in total. The ta-\nble is not available from DSTC2. To reconstruct\nthe table, we crawled TripAdvisor for restaurants\nin the Cambridge area, where the dialog dataset\nwas collected. Then, we remove restaurants that\ndo not appear in the data set and create a database\nwith 109 restaurants and their attributes (e.g. food\ntype). Since this is a small data set, we use 5-\nfold cross validation and report the average re-\nsult over the 5 partitions. There may be multi-\nple tokens in each table cell, for example in Ta-\nble. 2, the name, address, post code and phone\nnumber have multiple tokens, we replace them\nwith one special token. For the name, address,\npost code and phone number of the j-th row, we\nreplace the tokens in each cell with NAME j,\nADDR j, POSTCODE j, PHONE j. If a table\ncell is empty, we replace it with an empty token\nEMPTY. We do a string match in the transcript\nand replace the corresponding tokens in transcripts\nfrom the table with the special tokens. Each dia-\nlogue on average has 8 turns (16 sentences). We\nuse a vocabulary size of 900, including about 400\ntable tokens and 500 words.\nCoref LM: We use the Xinhua News data set from\nGigaword Fifth Edition and sample 100,000 docu-\nments that has length in range from 100 to 500.\nEach document has on average 234 tokens, so\nthere are 23 million tokens in total. We process\nthe documents to get coreference annotations and\nuse the annotations, i.e.,zi, vi, in training. We take\n80% as training and 10% as validation and test re-\nspectively. We ignore the entities that have only\none mention and for the mentions that have multi-\nple tokens, we take the token that is most frequent\nin the all the mentions for this entity. After prepro-\ncessing, tokens that are entity mentions take about\n10% of all tokens. We use a vocabulary size of\n50,000 in the model.\n3.2 Baselines, model training and evaluation\nWe compare our model with baselines that do not\nmodel reference explicitly. For recipe generation\nand dialogue modeling, we compare our model\nwith basic seq2seq and attention model. We also\napply attention mechanism over the table for di-\nalogue modeling as a baseline. For coreference\nbased language model, we compare our model\nwith simple RNN language model.\nWe train all models with simple stochastic gra-\ndient descent with gradient clipping. We use a\none-layer LSTM for all RNN components. Hyper-\nparameters are selected using grid search based on\nthe validation set.\nEvaluation of our model is challenging since it\ninvolves three rather different applications. We fo-\ncus on evaluating the accuracy of predicting the\nreference tokens, which is the goal of our model.\nSpeciﬁcally, we report the perplexity of all words,\nwords that can be generated from reference and\nnon-reference words. The perplexity is calcu-\nlated by multiplying the probability of decision at\neach step all together. Note that for non-reference\nwords, they also appear in the vocabulary. So it is a\nfair comparison to models that do not model refer-\nence explicitly. For the recipe task, we also gener-\nate the recipes using beam size of 10 and evaluate\nthe generated recipes with BLEU. We didn’t use\nBLEU for dialogue generation since the database\nentries take only a very small part of all tokens in\nutterances.\n3.3 Results and analysis\nThe results for recipe generation, dialogue and\ncoref based language model are shown in Table 4,\n5, and 6 respectively. The recipe results in Ta-\nble 4 veriﬁes that modeling reference explicitly\nimproves performance. Latent and Pointer per-\nform better than Seq2Seq and Attn model. The La-\ntent model performs better than the Pointer model\nsince tokens in ingredients that match with recipes\ndo not necessarily come from the ingredients. Im-\nposing a supervised signal gives wrong informa-\ntion to the model and hence makes the result\nworse. With latent decision, the model learns to\nwhen to copy and when to generate it from the vo-\ncabulary.\nThe ﬁndings for dialogue basically follow that\nof recipe generation, as shown in Table 5. Con-\nditioning table performs better in predicting table\ntokens in general. Table Pointer has the lowest\nperplexity for tokens in the table. Since the table\ntokens appear rarely in the dialogue transcripts,\nthe overall perplexity does not differ much and the\nnon-table token perplexity are similar. With atten-\ntion mechanism over the table, the perplexity of\ntable token improves over basic Seq2Seq model,\nbut still not as good as directly pointing to cells\nin the table, which shows the advantage of mod-\neling reference explicitly. As expected, using sen-\ntence attention improves signiﬁcantly over mod-\nels without sentence attention. Surprisingly, Table\nLatent performs much worse than Table Pointer.\nWe also measure the perplexity of table tokens that\nappear only in test set. For models other than Ta-\nble Pointer, because the tokens never appear in the\ntraining set, the perplexity is quite high, while Ta-\nble Pointer can predict these tokens much more\naccurately. This veriﬁes our conjecture that our\nmodel can learn reasoning over databases.\nThe coref based LM results are shown in Ta-\nble 6. We ﬁnd that coref based LM performs much\nbetter on the entity perplexity, but is a little bit\nworse for non-entity words. We found it was an\noptimization problem and the model was stuck in a\nlocal optimum. So we initialize the Pointer model\nwith the weights learned from LM, the Pointer\nmodel performs better than LM both for entity per-\nplexity and non-entity words perplexity.\nIn Appendix A, we also visualize the heat map\nof table reference and list items reference. The\nvisualization shows that our model can correctly\npredict when to refer to which entries according to\ncontext.\n4 Related Work\nIn terms of methodology, our work is closely re-\nlated to previous works that incorporate copying\nmechanism with neural models (G ¨ulc ¸ehre et al.,\n2016; Gu et al., 2016; Ling et al., 2016; Vinyals\net al., 2015). Our models are similar to models\nproposed in (Ahn et al., 2016; Merity et al., 2016),\nModel\nval test\nPPL BLEU PPL BLEUAll Ing Word All Ing Word\nSeq2Seq 5.60 11.26 5.00 14.07 5.52 11.26 4.91 14.39\nAttn 5.25 6.86 5.03 14.84 5.19 6.92 4.95 15.15\nPointer 5.15 5.86 5.04 15.06 5.11 6.04 4.98 15.29\nLatent 5.02 5.10 5.01 14.87 4.97 5.19 4.94 15.41\nTable 4: Recipe results, evaluated in perplexity and BLEU score. All means all tokens, Ing denotes\ntokens from recipes that appear in ingredients. Word means non-table tokens. Pointer and Latent differs\nin that for Pointer, we provide supervised signal on when to generate a reference token, while in Latent\nit is a latent decision.\nModel All Table Table OOV Word\nSeq2Seq 1.35 ±0.01 4.98 ±0.38 1.99E7 ±7.75E6 1.23 ±0.01\nTable Attn 1.37 ±0.01 5.09 ±0.64 7.91E7 ±1.39E8 1.24 ±0.01\nTable Pointer 1.33±0.01 3.99 ±0.36 1360 ± 2600 1.23 ±0.01\nTable Latent 1.36 ±0.01 4.99 ±0.20 3.78E7 ±6.08E7 1.24 ±0.01\n+ Sentence Attn\nSeq2Seq 1.28 ±0.01 3.31 ±0.21 2.83E9 ± 4.69E9 1.19±0.01\nTable Attn 1.28 ±0.01 3.17 ±0.21 1.67E7 ±9.5E6 1.20 ±0.01\nTable Pointer 1.27±0.01 2.99 ±0.19 82.86 ±110 1.20±0.01\nTable Latent 1.28 ±0.01 3.26 ±0.25 1.27E7 ±1.41E7 1.20 ±0.01\nTable 5: Dialogue perplexity results. Table means tokens from table, Table OOV denotes table tokens\nthat do not appear in the training set. Sentence Attndenotes we use attention mechanism over tokens in\nutterances from the previous turn.\nModel val test\nAll Entity Word All Entity Word\nLM 33.08 44.52 32.04 33.08 43.86 32.10\nPointer 32.57 32.07 32.62 32.62 32.07 32.69\nPointer\n+ init 30.43 28.56 30.63 30.42 28.56 30.66\nTable 6: Coreference based LM. Pointer + init\nmeans we initialize the model with the LM\nweights.\nwhere the generation of each word can be condi-\ntioned on a particular entry in knowledge lists and\nprevious words. In our work, we describe a model\nwith broader applications, allowing us to condi-\ntion, on databases, lists and dynamic lists.\nIn terms of applications, our work is related to\nchit-chat dialogue (Li et al., 2016; Vinyals and\nLe, 2015; Sordoni et al., 2015; Serban et al.,\n2016; Shang et al., 2015) and task oriented dia-\nlogue (Wen et al., 2015; Bordes and Weston, 2016;\nWilliams and Zweig, 2016; Wen et al., 2016).\nMost of previous works on task oriented dialogues\nembed the seq2seq model in traditional dialogue\nsystems, in which the table query part is not dif-\nferentiable, while our model queries the database\ndirectly. Recipe generation was proposed in (Kid-\ndon et al., 2016). They use attention mechanism\nover the checklists, whereas our work models ex-\nplicit references to them. Context dependent lan-\nguage models (Mikolov et al., 2010; Jozefowicz\net al., 2016; Mikolov et al., 2010; Ji et al., 2015;\nWang and Cho, 2015) are proposed to capture\nlong term dependency of text. There are also lots\nof works on coreference resolution (Haghighi and\nKlein, 2010; Wiseman et al., 2016). We are the\nﬁrst to combine coreference with language model-\ning, to the best of our knowledge.\n5 Conclusion\nWe introduce reference-aware language models\nwhich explicitly model the decision of from where\nto generate the token at each step. Our model\ncan also learns the decision by treating it as a la-\ntent variable. We demonstrate on three applica-\ntions, table based dialogue modeling, recipe gen-\neration and coref based LM, that our model per-\nforms better than attention based model, which\ndoes not incorporate this decision explicitly. There\nare several directions to explore further based on\nour framework. The current evaluation method is\nbased on perplexity and BLEU. In task oriented di-\nalogues, we can also try human evaluation to see\nif the model can reply users’ query accurately. It\nis also interesting to use reinforcement learning to\nlearn the actions in each step in coref based LM.\nReferences\nSungjin Ahn, Heeyoul Choi, Tanel P ¨arnamaa, and\nYoshua Bengio. 2016. A neural knowledge lan-\nguage model. CoRR, abs/1608.00318.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nAntoine Bordes and Jason Weston. 2016. Learn-\ning end-to-end goal-oriented dialog. arXiv preprint\narXiv:1605.07683.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor\nO. K. Li. 2016. Incorporating copying mech-\nanism in sequence-to-sequence learning. CoRR,\nabs/1603.06393.\nC ¸ aglar G¨ulc ¸ehre, Sungjin Ahn, Ramesh Nallapati,\nBowen Zhou, and Yoshua Bengio. 2016. Pointing\nthe unknown words. CoRR, abs/1603.08148.\nAria Haghighi and Dan Klein. 2010. Coreference res-\nolution in a modular, entity-centered model. In\nHuman Language Technologies: The 2010 Annual\nConference of the North American Chapter of the\nAssociation for Computational Linguistics , pages\n385–393. Association for Computational Linguis-\ntics.\nMatthew Henderson, Blaise Thomson, and Jason\nWilliams. 2014. Dialog state tracking challenge 2\n& 3.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2015. Document context lan-\nguage models. arXiv preprint arXiv:1511.03962.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nChlo´e Kiddon, Luke Zettlemoyer, and Yejin Choi.\n2016. Globally coherent text generation with neu-\nral checklist models. In Proc. EMNLP.\nJiwei Li, Will Monroe, Alan Ritter, Michel Galley,\nJianfeng Gao, and Dan Jurafsky. 2016. Deep rein-\nforcement learning for dialogue generation. In Proc.\nEMNLP.\nWang Ling, Edward Grefenstette, Karl Moritz Her-\nmann, Tom ´aˇs Ko ˇcisk´y, Andrew Senior, Fumin\nWang, and Phil Blunsom. 2016. Latent predictor\nnetworks for code generation. In Proc. ACL.\nThang Luong, Ilya Sutskever, Quoc V . Le, Oriol\nVinyals, and Wojciech Zaremba. 2015. Addressing\nthe rare word problem in neural machine translation.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing of the Asian Federation of Natural\nLanguage Processing, ACL 2015, July 26-31, 2015,\nBeijing, China, Volume 1: Long Papers , pages 11–\n19.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Inter-\nspeech, volume 2, page 3.\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hier-\narchical neural network models. In Proceedings of\nthe 30th AAAI Conference on Artiﬁcial Intelligence\n(AAAI-16).\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015.\nNeural responding machine for short-text conversa-\ntion. arXiv preprint arXiv:1503.02364.\nAlessandro Sordoni, Michel Galley, Michael Auli,\nChris Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun\nNie, Jianfeng Gao, and Bill Dolan. 2015. A neural\nnetwork approach to context-sensitive generation of\nconversational responses. In Proc. NAACL.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Proc. NIPS.\nOriol Vinyals and Quoc V . Le. 2015. A neural con-\nversational model. In Proc. ICML Deep Learning\nWorkshop.\nTian Wang and Kyunghyun Cho. 2015. Larger-\ncontext language modelling. arXiv preprint\narXiv:1511.03729.\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic,\nLina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,\nDavid Vandyke, and Steve Young. 2016. A network-\nbased end-to-end trainable task-oriented dialogue\nsystem. arXiv preprint arXiv:1604.04562.\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-\nhao Su, David Vandyke, and Steve J. Young. 2015.\nSemantically conditioned LSTM-based natural lan-\nguage generation for spoken dialogue systems. In\nProc. EMNLP.\nJason D Williams and Geoffrey Zweig. 2016. End-\nto-end lstm-based dialog control optimized with su-\npervised and reinforcement learning. arXiv preprint\narXiv:1606.01269.\nSam Wiseman, Alexander M Rush, and Stuart M\nShieber. 2016. Learning global features for corefer-\nence resolution. arXiv preprint arXiv:1604.03035.\nSteve Young, Milica Ga ˇsi´c, Blaise Thomson, and Ja-\nson D Williams. 2013. Pomdp-based statistical spo-\nken dialog systems: A review. Proceedings of the\nIEEE, 101(5):1160–1179.\nA Visualization\nM: Hello , welcome to the Cambridge restaurant system ? You can ask for restaurants by area ,\nprice range or food type . How may I help you ?\nU: and expensive restaurant that serves tuscan food\nM: Sorry there is no tuscan restaurant in the expensive price range\nU: french food\nM: cote is a great restaurant serving french food and it is in the expensive price range\nU: may i have the phone number\nM: The phone number of cote is 01223 311053 .\nU: thank you good bye\n(a) Dialogue script\nNAME PRICE RANGE FOOD AREA ADDRESS POST CODE PHONE\ncharlie chan cheap chinese east Regent Street City Cen-\ntre\nC.B 2, 1 D.B 01223 361763\nchiquito restau-\nrant bar\nexpensive mexican south 2G Cambridge Leisure\nPark Cherry Hinton\nRoad Cherry Hinton\nC.B 1, 7 D.Y 01223 400170\ncity stop expensive food north Cambridge City Foot-\nball Club Milton Road\nChesterton\nEMPTY 01223 363270\nclowns cafe expensive italian centre EMPTY C.B 1, 1 L.N 01223 355711\ncocum expensive indian west 71 Castle Street City\nCentre\nC.B 3, 0 A.H 01223 366668\ncote expensive french centre Bridge Street City Cen-\ntre\nC.B 2, 1 U.F 01223 311053\ncurry garden expensive indian centre 106 Regent Street City\nCentre\nEMPTY 01223 302330\ncurry king expensive indian centre 5 Jordans Yard Bridge\nStreet City Centre\nC.B 1, 2 B.D 01223 324351\ncurry prince moderate indian east 451 Newmarket Road\nFen Ditton\nC.B 5, 8 J.J 01223 566388\n(b) Attention heat map: cote is a great restaurant serving french food and it is in the expensive price range.\nNAME PRICE RANGE FOOD AREA ADDRESS POST CODE PHONE\ncharlie chan cheap chinese east Regent Street City Cen-\ntre\nC.B 2, 1 D.B 01223 361763\nchiquito restau-\nrant bar\nexpensive mexican south 2G Cambridge Leisure\nPark Cherry Hinton\nRoad Cherry Hinton\nC.B 1, 7 D.Y 01223 400170\ncity stop expensive food north Cambridge City Foot-\nball Club Milton Road\nChesterton\nEMPTY 01223 363270\nclowns cafe expensive italian centre EMPTY C.B 1, 1 L.N 01223 355711\ncocum expensive indian west 71 Castle Street City\nCentre\nC.B 3, 0 A.H 01223 366668\ncote expensive french centre Bridge Street City Cen-\ntre\nC.B 2, 1 U.F 01223 311053\ncurry garden expensive indian centre 106 Regent Street City\nCentre\nEMPTY 01223 302330\ncurry king expensive indian centre 5 Jordans Yard Bridge\nStreet City Centre\nC.B 1, 2 B.D 01223 324351\ncurry prince moderate indian east 451 Newmarket Road\nFen Ditton\nC.B 5, 8 J.J 01223 566388\n(c) Attention heap map: The phone number of cote is 01223 311053 .\nTable 7: Dialogue visualization 1\nM: Hello , welcome to the Cambridge restaurant system ? You can ask for restaurants by area ,\nprice range or food type . How may I help you ?\nU: cheap restaurant\nM: What kind of food would you like ?\nU: in the west part of town\nM: la margherita is a nice restaurant in the west of town in the cheap price range\nU: address\nM: Sure , la margherita is on 15 Magdalene Street City Centre\nU: thank you\nM: la margherita is a nice restaurant in the west of town in the cheap price range\nU: good bye\n(a) Dialogue script\nNAME PRICE RANGE FOOD AREA ADDRESS POST CODE PHONE\nindia house expensive indian west 31 Newnham Road\nNewnham\nEMPTY 01223 461661\nj restaurant cheap oriental centre 86 Regent Street City\nCentre\nC.B 2, 1 D.P 01223 307581\njinling noodle\nbar\nmoderate chinese centre 11 Peas Hill City Cen-\ntre\nC.B 2, 3 P.P 01223 566188\nkohinoor cheap indian centre 74 Mill Road City Cen-\ntre\nEMPTY 01223 323639\nkymmoy expensive oriental centre 52 Mill Road City Cen-\ntre\nC.B 1, 2 A.S 01223 311911\nla margherita cheap italian west 15 Magdalene Street\nCity Centre\nC.B 3, 0 A.F 01223 315232\nla mimosa expensive mediterranean centre Thompsons Lane Fen\nDitton\nC.B 5, 8 A.Q 01223 362525\nla raza cheap spanish centre 4 - 6 Rose Crescent C.B 2, 3 L.L 01223 464550\nla tasca moderate spanish centre 14 -16 Bridge Street C.B 2, 1 U.F 01223 464630\nlan hong house moderate chinese centre 12 Norfolk Street City\nCentre\nEMPTY 01223 350420\n(b) Attention heat map: la margherita is a nice restaurant in the west of town in the cheap price range\nNAME PRICE RANGE FOOD AREA ADDRESS POST CODE PHONE\nindia house expensive indian west 31 Newnham Road\nNewnham\nEMPTY 01223 461661\nj restaurant cheap oriental centre 86 Regent Street City\nCentre\nC.B 2, 1 D.P 01223 307581\njinling noodle\nbar\nmoderate chinese centre 11 Peas Hill City Cen-\ntre\nC.B 2, 3 P.P 01223 566188\nkohinoor cheap indian centre 74 Mill Road City Cen-\ntre\nEMPTY 01223 323639\nkymmoy expensive oriental centre 52 Mill Road City Cen-\ntre\nC.B 1, 2 A.S 01223 311911\nla margherita cheap italian west 15 Magdalene Street\nCity Centre\nC.B 3, 0 A.F 01223 315232\nla mimosa expensive mediterranean centre Thompsons Lane Fen\nDitton\nC.B 5, 8 A.Q 01223 362525\nla raza cheap spanish centre 4 - 6 Rose Crescent C.B 2, 3 L.L 01223 464550\nla tasca moderate spanish centre 14 -16 Bridge Street C.B 2, 1 U.F 01223 464630\nlan hong house moderate chinese centre 12 Norfolk Street City\nCentre\nEMPTY 01223 350420\n(c) Attention heap map: Sure , la margherita is on 15 Magdalene Street City Centre.\nTable 8: Dialogue visualization 2\n(a) part 1\n(b) part 2\nFigure 6: Recipe heat map example 1. The ingredient tokens appear on the left while the recipe tokens\nappear on the top. The ﬁrst row is the p(zv|sv).\n(a) part 1\n(b) part 2\n(c) part 3\nFigure 7: Recipe heat map example 2.",
  "topic": "Coreference",
  "concepts": [
    {
      "name": "Coreference",
      "score": 0.8682735562324524
    },
    {
      "name": "Computer science",
      "score": 0.8576632738113403
    },
    {
      "name": "Language model",
      "score": 0.7153373956680298
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6353781819343567
    },
    {
      "name": "Natural language processing",
      "score": 0.5991537570953369
    },
    {
      "name": "Class (philosophy)",
      "score": 0.5932390689849854
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5289649367332458
    },
    {
      "name": "Entity linking",
      "score": 0.4404083788394928
    },
    {
      "name": "Programming language",
      "score": 0.3516288995742798
    },
    {
      "name": "Resolution (logic)",
      "score": 0.34732651710510254
    },
    {
      "name": "Knowledge base",
      "score": 0.06912466883659363
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}