{
    "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows",
    "url": "https://openalex.org/W3174738881",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A41270877",
            "name": "Dong, Xiaoyi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2143844023",
            "name": "Bao Jianmin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1943263669",
            "name": "Chen Dongdong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1862070408",
            "name": "Zhang, Weiming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A734139434",
            "name": "Yu, Nenghai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101029194",
            "name": "Yuan Lu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1931409288",
            "name": "Chen Dong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3010309834",
            "name": "Guo, Baining",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2765407302",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W2964241181",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W2747685395",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3159732141",
        "https://openalex.org/W3000514857",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W3108995912",
        "https://openalex.org/W3109319753",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3165150763",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2725513608",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W3130071011",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3146097248",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W3139633126",
        "https://openalex.org/W3153842237",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W3035682985",
        "https://openalex.org/W2986922898",
        "https://openalex.org/W3156109214",
        "https://openalex.org/W3135921327",
        "https://openalex.org/W3161838454",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W3091156754",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W3211432419",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3136958399",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W3204563069",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W2950141105",
        "https://openalex.org/W2916798096",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W3128723389",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3034609440",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3131922516",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3136416617",
        "https://openalex.org/W2910628332",
        "https://openalex.org/W2612445135",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W2086161653",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2884822772",
        "https://openalex.org/W3128099838",
        "https://openalex.org/W2531409750"
    ],
    "abstract": "We present CSWin Transformer, an efficient and effective Transformer-based backbone for general-purpose vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute whereas local self-attention often limits the field of interactions of each token. To address this issue, we develop the Cross-Shaped Window self-attention mechanism for computing self-attention in the horizontal and vertical stripes in parallel that form a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. We provide a mathematical analysis of the effect of the stripe width and vary the stripe width for different layers of the Transformer network which achieves strong modeling capability while limiting the computation cost. We also introduce Locally-enhanced Positional Encoding (LePE), which handles the local positional information better than existing encoding schemes. LePE naturally supports arbitrary input resolutions, and is thus especially effective and friendly for downstream tasks. Incorporated with these designs and a hierarchical structure, CSWin Transformer demonstrates competitive performance on common vision tasks. Specifically, it achieves 85.4\\% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task, and 52.2 mIOU on the ADE20K semantic segmentation task, surpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and +2.0 respectively under the similar FLOPs setting. By further pretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. The code and models are available at https://github.com/microsoft/CSWin-Transformer.",
    "full_text": "CSWin Transformer: A General Vision Transformer Backbone with\nCross-Shaped Windows\nXiaoyi Dong1*, Jianmin Bao2, Dongdong Chen3, Weiming Zhang1,\nNenghai Yu1, Lu Yuan3, Dong Chen2, Baining Guo2\n1University of Science and Technology of China\n2Microsoft Research Asia 3Microsoft Cloud + AI\n{dlight@mail., zhangwm@, ynh@}.ustc.edu.cn cddlyf@gmail.com\n{jianbao, luyuan, doch, bainguo }@microsoft.com\nAbstract\nWe present CSWin Transformer, an efÔ¨Åcient and effec-\ntive Transformer-based backbone for general-purpose vision\ntasks. A challenging issue in Transformer design is that\nglobal self-attention is very expensive to compute whereas\nlocal self-attention often limits the Ô¨Åeld of interactions of\neach token. To address this issue, we develop the Cross-\nShaped Window self-attention mechanism for computing\nself-attention in the horizontal and vertical stripes inparallel\nthat form a cross-shaped window, with each stripe obtained\nby splitting the input feature into stripes of equal width. We\nprovide a mathematical analysis of the effect of the stripe\nwidth and vary the stripe width for different layers of the\nTransformer network which achieves strong modeling capa-\nbility while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which han-\ndles the local positional information better than existing\nencoding schemes. LePE naturally supports arbitrary input\nresolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hi-\nerarchical structure, CSWin Transformer demonstrates com-\npetitive performance on common vision tasks. SpeciÔ¨Åcally,\nit achieves 85.4% Top-1 accuracy on ImageNet-1K without\nany extra training data or label, 53.9 box AP and 46.4 mask\nAP on the COCO detection task, and 52.2 mIOU on the\nADE20K semantic segmentation task, surpassing previous\nstate-of-the-art Swin Transformer backbone by +1.2, +2.0,\n+1.4, and +2.0 respectively under the similar FLOPs setting.\nBy further pretraining on the larger dataset ImageNet-21K,\nwe achieve 87.5% Top-1 accuracy on ImageNet-1K and high\nsegmentation performance on ADE20K with 55.7 mIoU.\n*Work done during an internship at Microsoft Research Asia.\n1. Introduction\nTransformer-based architectures [17, 38, 53, 60] have re-\ncently achieved competitive performances compared to their\nCNN counterparts in various vision tasks. By leveraging\nthe multi-head self-attention mechanism, these vision Trans-\nformers demonstrate a high capability in modeling the long-\nrange dependencies, which is especially helpful for handling\nhigh-resolution inputs in downstream tasks, e.g., object de-\ntection and segmentation. Despite the success, the Trans-\nformer architecture with full-attention mechanism [17] is\ncomputationally inefÔ¨Åcient.\nTo improve the efÔ¨Åciency, one typical way is to limit\nthe attention region of each token from full-attention to lo-\ncal/windowed attention [38, 55]. To bridge the connection\nbetween windows, researchers further proposed halo and\nshift operations to exchange information through nearby win-\ndows. However, the receptive Ô¨Åeld is enlarged quite slowly\nand it requires stacking a great number of blocks to achieve\nglobal self-attention. A sufÔ¨Åciently large receptive Ô¨Åeld is\ncrucial to the performance especially for the downstream\ntasks(e.g., object detection and segmentation). Therefore it\nis important to achieve large receptive Ô¨Åled efÔ¨Åciently while\nkeeping the computation cost low.\nIn this paper, we present the Cross-Shaped Window\n(CSWin) self-attention, which is illustrated in Figure 1 and\ncompared with existing self-attention mechanisms. With\nCSWin self-attention, we perform the self-attention calcu-\nlation in the horizontal and vertical stripes in parallel, with\neach stripe obtained by splitting the input feature into stripes\nof equal width. This stripe width is an important parameter\nof the cross-shaped window because it allows us to achieve\nstrong modelling capability while limiting the computation\ncost. SpeciÔ¨Åcally, we adjust the stripe width according to the\ndepth of the network: small widths for shallow layers and\nlarger widths for deep layers. A larger stripe width encour-\nages a stronger connection between long-range elements and\narXiv:2107.00652v3  [cs.CV]  9 Jan 2022\n‚Ñé1\n‚Ñéùêæ\n‚Ñé1\n‚Ñéùêæ\n‚Ñé1\n‚Ñéùêæ\n‚Ñé1\n‚Ñéùêæ\nShifted Local\nSequential Axial\nNext\nBlock\nNext\nBlock\n‚Ñé1\n‚Ñéùêæ\n‚Ñé1\n‚Ñéùêæ\n‚Ñé1\n‚Ñéùêæ/2\n‚Ñéùêæ/2+1\n‚Ñéùêæ\nSplit Head Full Attention\nCriss-Cross\nConcat\nDynaic Stripe Window + Parallel Grouing Heads = CSWin\n‚Ñé1\n‚Ñéùêæ\n‚Ñé1\n‚Ñéùêæ\nSlide Local\nLocal + Global\nùë†ùë§\nùë†ùë§\nFigure 1. Illustration of different self-attention mechanisms, our CSWin is fundamentally different from two aspects. First, we split\nmulti-heads ({h1, . . . , hK}) into two groups and perform self-attention in horizontal and vertical stripes simultaneously. Second, we adjust\nthe stripe width according to the depth network, which can achieve better trade-off between computation cost and capability\nachieves better network capacity with a small increase in\ncomputation cost. We will provide a mathematical analysis\nof how the stripe width affects the modeling capability and\ncomputation cost.\nIt is worthwhile to note that with CSWin self-attention\nmechanism, the self-attention in horizontal and vertical\nstripes are calculated in parallel. We split the multi-heads\ninto parallel groups and apply different self-attention op-\nerations onto different groups. This parallel strategy intro-\nduces no extra computation cost while enlarging the area\nfor computing self-attention within each Transformer block.\nThis strategy is fundamentally different from existing self-\nattention mechanisms [25, 38, 56, 69] that apply the same\nattention operation across multi-heads((Figure 1 b,c,d,e), and\nperform different attention operations sequentially(Figure 1\nc,e). We will show through ablation analysis that this differ-\nence makes CSWin self-attention much more effective for\ngeneral vision tasks.\nBased on the CSWin self-attention mechanism, we fol-\nlow the hierarchical design and propose a new vision\nTransformer architecture named ‚ÄúCSWin Transformer‚Äù for\ngeneral-purpose vision tasks. This architecture provides\nsigniÔ¨Åcantly stronger modeling power while limiting compu-\ntation cost. To further enhance this vision Transformer, we\nintroduce an effective positional encoding,Locally-enhanced\nPositional Encoding (LePE), which is especially effective\nand friendly for input varying downstream tasks such as ob-\nject detection and segmentation. Compared with previous\npositional encoding methods [12, 46, 56], our LePE imposes\nthe positional information within each Transformer block\nand directly operates on the attention results instead of the\nattention calculation. The LePE makes CSWin Transformer\nmore effective and friendly for the downstream tasks.\nAs a general vision Transformer backbone, the CSWin\nTransformer demonstrates strong performance on image clas-\nsiÔ¨Åcation, object detection and semantic segmentation tasks.\nUnder the similar FLOPs and model size, CSWin Trans-\nformer variants signiÔ¨Åcantly outperforms previous state-\nof-the-art (SOTA) vision Transformers. For example, our\nbase variant CSWin-B achieves 85.4% Top-1 accuracy on\nImageNet-1K without any extra training data or label, 53.9\nbox AP and 46.4 mask AP on the COCO detection task,51.7\nmIOU on the ADE20K semantic segmentation task, surpass-\ning previous state-of-the-art Swin Transformer counterpart\nby +1.2, +2.0, 1.4 and +2.0 respectively. Under a smaller\nFLOPs setting, our tiny variant CSWin-T even shows larger\nperformance gains, i.e.,, +1.4 point on ImageNet classiÔ¨Åca-\ntion, +3.0 box AP, +2.0 mask AP on COCO detection and\n+4.6 on ADE20K segmentation. Furthermore, when pretrain-\ning CSWin Transformer on the larger dataset ImageNet-21K,\nwe achieve87.5% Top-1 accuracy on ImageNet-1K and high\nsegmentation performance on ADE20K with 55.7 mIoU.\n2. Related Work\nVision Transformers. Convolutional neural networks\n(CNN) have dominated the computer vision Ô¨Åeld for many\nyears and achieved tremendous successes [7, 22, 26‚Äì28, 35,\n45, 47, 49‚Äì51]. Recently, the pioneering work ViT [17]\ndemonstrates that pure Transformer-based architectures can\nalso achieve very competitive results, indicating the potential\nof handling the vision tasks and natural language processing\n(NLP) tasks under a uniÔ¨Åed framework. Built upon the suc-\ncess of ViT, many efforts have been devoted to designing bet-\nter Transformer based architectures for various vision tasks,\nincluding low-level image processing [5, 57], image classiÔ¨Å-\ncation [11, 11, 13, 18, 20, 23, 31, 53, 54, 58, 60, 64‚Äì66], object\ndetection [3, 73] and semantic segmentation [48, 59, 70].\nRather than concentrating on one special task, some recent\nworks [38, 58, 69] try to design a general vision Transformer\nbackbone for general-purpose vision tasks. They all follow\nthe hierarchical Transformer architecture but adopt differ-\nent self-attention mechanisms. The main beneÔ¨Åt of the hi-\nerarchical design is to utilize the multi-scale features and\nreduce the computation complexity by progressively decreas-\nùêª√óùëä √ó3 ùêª\n4 √óùëä\n4 √óùê∂\nStage 1\n√óùëÅ1\n(ùë†ùë§1)\nConv ‚Üì\nùêª\n8 √óùëä\n8 √ó2ùê∂\nStage 2\n√óùëÅ2\n(ùë†ùë§2)\nConv ‚Üì\nùêª\n16√ó ùëä\n16√ó4ùê∂\nStage 3\n√óùëÅ3\n(ùë†ùë§3)\nConv ‚Üì\nùêª\n32√ó ùëä\n32√ó8ùê∂\nStage 4\nCSwin\nTransformer Block\n√óùëÅ4\n(ùë†ùë§4)\nCross-Shaped \nWindow Self-Attention\nLN\nMLP\nCSwin Transformer Block\nConvolutional \nToken Embedding\nCSwin\nTransformer Block\nCSwin\nTransformer Block\nCSwin\nTransformer Block\nLN\nFigure 2. Left: the overall architecture of our proposed CSWin Transformer, Right: the illustration of CSWin Transformer block.\ning the number of tokens. In this paper,we propose a new\nhierarchical vision Transformer backbone by introducing\ncross-shaped window self-attention and locally-enhanced\npositional encoding.\nEfÔ¨Åcient Self-attentions. In the NLP Ô¨Åeld, many efÔ¨Åcient\nattention mechanisms [1, 9, 10, 32, 34, 42, 44, 52] have been\ndesigned to improve the Transformer efÔ¨Åciency for han-\ndling long sequences. Since the image resolution is often\nvery high in vision tasks, designing efÔ¨Åcient self-attention\nmechanisms is also very crucial. However, many existing vi-\nsion Transformers [17, 53, 60, 66] still adopt the original full\nself-attention, whose computation complexity is quadratic\nto the image size. To reduce the complexity, the recent\nvision Transformers [38, 55] adopt the local self-attention\nmechanism [43] and its shifted/haloed version to add the\ninteraction across different local windows. Besides, axial\nself-attention [25] and criss-cross attention [30] propose cal-\nculating attention within stripe windows along horizontal\nor/and vertical axis. While the performance of axial atten-\ntion is limited by its sequential mechanism and restricted\nwindow size, criss-cross attention is inefÔ¨Åcient in practice\ndue to its overlapped window design and ineffective due to\nits restricted window size. They are the most related works\nwith our CSWin, which could be viewed as a much general\nand efÔ¨Åcient format of these previous works.\nPositional Encoding. Since self-attention is permutation-\ninvariant and ignores the token positional information, po-\nsitional encoding is widely used in Transformers to add\nsuch positional information back. Typical positional en-\ncoding mechanisms include absolute positional encoding\n(APE) [56], relative positional encoding (RPE) [38, 46] and\nconditional positional encoding (CPE) [12]. APE and RPE\nare often deÔ¨Åned as the sinusoidal functions of a series of\nfrequencies or the learnable parameters, which are designed\nfor a speciÔ¨Åc input size and are not friendly to varying input\nresolutions. CPE takes the feature as input and can generate\nthe positional encoding for arbitrary input resolutions. Then\nthe generated positional encoding will be added onto the\ninput feature. Our LePE shares a similar spirit as CPE, but\nproposes to add the positional encoding as a parallel mod-\nule to the self-attention operation and operates on projected\nvalues in each Transformer block. This design decouples\npositional encoding from the self-attention calculation, and\ncan enforce stronger local inductive bias.\n3. Method\n3.1. Overall Architecture\nThe overall architecture of CSWin Transformer is illus-\ntrated in Figure 2. For an input image with size ofH√óW√ó3,\nwe follow [60] and leverage the overlapped convolutional\ntoken embedding (7 √ó7 convolution layer with stride 4) )\nto obtain H\n4 √óW\n4 patch tokens, and the dimension of each\ntoken is C. To produce a hierarchical representation, the\nwhole network consists of four stages. A convolution layer\n(3 √ó3, stride 2) is used between two adjacent stages to re-\nduce the number of tokens and double the channel dimension.\nTherefore, the constructed feature maps have H\n2i+1 √ó W\n2i+1\ntokens for the ith stage, which is similar to traditional CNN\nbackbones like VGG/ResNet. Each stage consists of Ni\nsequential CSWin Transformer Blocks and maintains the\nnumber of tokens. CSWin Transformer Block has the over-\nall similar topology as the vanilla multi-head self-attention\nTransformer block with two differences: 1) It replaces the\nself-attention mechanism with our proposed Cross-Shaped\nWindow Self-Attention; 2) In order to introduce the local\ninductive bias, LePE is added as a parallel module to the\nself-attention branch.\n3.2. Cross-Shaped Window Self-Attention\nDespite the strong long-range context modeling capa-\nbility, the computation complexity of the original full self-\nattention mechanism is quadratic to feature map size. There-\nfore, it will suffer from huge computation cost for vision\ntasks that take high resolution feature maps as input, such\nas object detection and segmentation. To alleviate this issue,\nexisting works [38, 55] suggest to perform self-attention in a\nlocal attention window and apply halo or shifted window to\nenlarge the receptive Ô¨Åled. However, the token within each\nTransformer block still has limited attention area and re-\nquires stacking more blocks to achieve global receptive Ô¨Åled.\nTo enlarge the attention area and achieve global self-attention\nmore efÔ¨Åciently, we present the cross-shaped window self-\nattention mechanism, which is achieved by performing self-\nattention in horizontal and vertical stripes in parallel that\nùëÑ\nùëâ\nùêæ\nAPE/CPE(X)\nùëã\nùëã‚Ä≤ SoftMax(\nùëÑùêæùëá\nùê∑ )ùëâ SoftMax(\nùëÑùêæùëá\nùê∑ + )ùëâRPE\n√óùëÅTransformer block\nùëã\nùëÑ\nùëâ\nùêæùëã\n√óùëÅTransformer block\nSoftMax(\nùëÑùêæùëá\nùê∑ )ùëâùëã\nùëÑ\nùëâ\nùêæùëã\n√óùëÅTransformer block\nLePE(V)\nFigure 3. Comparison among different positional encoding mechanisms: APE and CPE introduce the positional information before feeding\ninto the Transformer blocks, while RPE and our LePE operate in each Transformer block. Different from RPE that adds the positional\ninformation into the attention calculation, our LePE operates directly upon V and acts as a parallel module. ‚àó Here we only draw the\nself-attention part to represent the Transformer block for simplicity.\nform a cross-shaped window.\nHorizontal and Vertical Stripes. According to the multi-\nhead self-attention mechanism, the input feature X ‚àà\nR(H√óW)√óC will be Ô¨Årst linearly projected to Kheads, and\nthen each head will perform local self-attention within either\nthe horizontal or vertical stripes.\nFor horizontal stripes self-attention, X is evenly parti-\ntioned into non-overlapping horizontal stripes [X1,..,X M ]\nof equal width sw, and each of them contains sw√óW to-\nkens. Here, sw is the stripe width and can be adjusted to\nbalance the learning capacity and computation complexity.\nFormally, suppose the projected queries, keys and values of\nthe kth head all have dimension dk, then the output of the\nhorizontal stripes self-attention for kth head is deÔ¨Åned as:\nX = [X1,X2,...,X M ],\nYi\nk = Attention(XiWQ\nk ,XiWK\nk ,XiWV\nk ),\nH-Attentionk(X) = [Y1\nk ,Y 2\nk ,...,Y M\nk ]\n(1)\nWhere where Xi ‚ààR(sw√óW)√óC and M = H/sw, i =\n1,...,M . WQ\nk ‚ààRC√ódk , WK\nk ‚ààRC√ódk , WV\nk ‚ààRC√ódk\nrepresent the projection matrices of queries, keys and values\nfor the kth head respectively, and dk is set as C/K. The\nvertical stripes self-attention can be similarly derived, and\nits output for kth head is denoted as V-Attentionk(X).\nAssuming natural images do not have directional bias,\nwe equally split the Kheads into two parallel groups (each\nhas K/2 heads, K is often an even value). The Ô¨Årst group\nof heads perform horizontal stripes self-attention while the\nsecond group of heads perform vertical stripes self-attention.\nFinally the output of these two parallel groups will be con-\ncatenated back together.\nCSWin-Attention(X) = Concat(head1, ...,headK)WO\nheadk =\n{\nH-Attentionk(X) k = 1, . . . , K/2\nV-Attentionk(X) k = K/2 + 1, . . . , K\n(2)\nWhere WO ‚ààRC√óC is the commonly used projection\nmatrix that projects the self-attention results into the tar-\nget output dimension (set as C by default). As described\nabove, one key insight in our self-attention mechanism de-\nsign is splitting the multi-heads into different groups and\napplying different self-attention operations accordingly. In\nother words, the attention area of each token within one\nTransformer block is enlarged via multi-head grouping. By\ncontrast, existing self-attention mechanisms apply the same\nself-attention operations across different multi-heads. In the\nexperiment parts, we will show that this design will bring\nbetter performance.\nComputation Complexity Analysis. The computation\ncomplexity of CSWin self-attention is:\n‚Ñ¶(CSWin) = HWC ‚àó(4C+ sw‚àóH+ sw‚àóW) (3)\nFor high-resolution inputs, considering H,W will be\nlarger than C in the early stages and smaller than C in the\nlater stages, we choose small swfor early stages and larger\nswfor later stages. In other words,adjusting swprovides the\nÔ¨Çexibility to enlarge the attention area of each token in later\nstages in an efÔ¨Åcient way. Besides, to make the intermediate\nfeature map size divisible by sw for 224 √ó224 input, we\nempirically set swto 1,2,7,7 for four stages by default.\nLocally-Enhanced Positional Encoding. Since the self-\nattention operation is permutation-invariant, it will ignore\nthe important positional information within the 2D image.\nTo add such information back, different positional encoding\nmechanisms have been utilized in existing vision Transform-\ners. In Figure 3, we show some typical positional encoding\nmechanisms and compare them with our proposed locally-\nenhanced positional encoding. In details, APE [56] and\nCPE [12] add the positional information into the input token\nbefore feeding into the Transformer blocks, while RPE [46]\nand our LePE incorporate the positional information within\neach Transformer block. But different from RPE that adds\nthe positional information within the attention calculation\n(i.e., Softmax(QKT )), we consider a more straightforward\nmanner and impose the positional information upon the lin-\nearly projected values. Meanwhile, we notice that RPE\nintroduces bias in a per head manner, while our LePE is a\nper-channel bias, which may show more potential to serve\nas positional embeddings.\nMathematically, we denote the input sequence as x =\n(x1,...,x n) of nelements, and the output of the attention\nz = ( z1,...,z n) of the same length, where xi,zi ‚ààRC.\nSelf-attention computation could be formulated as:\nzi =\nn‚àë\nj=1\nŒ±ijvj,Œ±ij = exp(qT\ni kj/\n‚àö\nd) (4)\nwhere qi,ki,vi are the queue,key and value get by a\nlinear transformation of the input xi and d is the feature\nModels #Dim #Blocks sw #heads #Param. FLOPs\nCSWin-T 64 1,2,21,1 1,2,7,7 2,4,8,16 23M 4.3G\nCSWin-S 64 2,4,32,2 1,2,7,7 2,4,8,16 35M 6.9G\nCSWin-B 96 2,4,32,2 1,2,7,7 4,8,16,32 78M 15.0G\nCSWin-L 144 2,4,32,2 1,2,7,7 6,12,24,48 173M 31.5G\nTable 1. Detailed conÔ¨Ågurations of different variants of CSWin\nTransformer. The FLOPs are calculated with 224 √ó224 input.\ndimension. Then our Locally-Enhanced position encoding\nperforms as a learnable per-element bias and Eq.4 could be\nformulated as:\nzk\ni =\nn‚àë\nj=1\n(Œ±k\nij + Œ≤k\nij)vk\nj (5)\nwhere zk\ni represents the kth element of vector zi. To make\nthe LePE suitable to varying input size, we set a distance\nthreshold to the LePE and set it to 0 if the Chebyshev dis-\ntance of token iand jis greater than a threshold œÑ (œÑ = 3 in\nthe default setting).\n3.3. CSWin Transformer Block\nEquipped with the above self-attention mechanism and\npositional embedding mechanism, CSWin Transformer\nblock is formally deÔ¨Åned as:\nÀÜXl = CSWin-Attention\n(\nLN\n(\nXl‚àí1))\n+ Xl‚àí1,\nXl = MLP\n(\nLN\n(\nÀÜXl\n))\n+ ÀÜXl, (6)\nwhere Xl denotes the output of l-th Transformer block or\nthe precedent convolutional layer of each stage.\n3.4. Architecture Variants\nFor a fair comparison with other vision Transformers\nunder similar settings, we build four different variants of\nCSWin Transformer as shown in Table 1: CSWin-T (Tiny),\nCSWin-S (Small), CSWin-B (Base), CSWin-L (Large). They\nare designed by changing the base channel dimension Cand\nthe block number of each stage. In all these variants, the\nexpansion ratio of each MLP is set as4. The head number of\nthe four stages is set as 2,4,8,16 in the Ô¨Årst three variants\nand 6,12,24,48 in the last variant respectively.\n4. Experiments\nTo show the effectiveness of CSWin Transformer as a gen-\neral vision backbone, we conduct experiments on ImageNet-\n1K [16] classiÔ¨Åcation, COCO [37] object detection, and\nADE20K [72] semantic segmentation. We also perform\ncomprehensive ablation studies to analyze each component\nof CSWin Transformer. As most of the methods we com-\npared did not report downstream inference speed, we use an\nextra section to report it for simplicity.\nMethod Image Size #Param. FLOPs Throughput Top-1\nEff-B4 [51] 380 2 19M 4.2G 349/s 82.9\nEff-B5 [51] 456 2 30M 9.9G 169/s 83.6\nEff-B6 [51] 528 2 43M 19.0G 96/s 84.0\nDeiT-S [53] 224 2 22M 4.6G 940/s 79.8\nDeiT-B [53] 224 2 87M 17.5G 292/s 81.8\nDeiT-B [53] 384 2 86M 55.4G 85/s 83.1\nPVT-S [58] 224 2 25M 3.8G 820/s 79.8\nPVT-M [58] 224 2 44M 6.7G 526/s 81.2\nPVT-L [58] 224 2 61M 9.8G 367/s 81.7\nT2T t -14 [66] 224 2 22M 6.1G ‚Äì 81.7\nT2T t -19 [66] 224 2 39M 9.8G ‚Äì 82.2\nT2T t -24 [66] 224 2 64M 15.0G ‚Äì 82.6\nCvT-13 [60] 224 2 20M 4.5G ‚Äì 81.6\nCvT-21 [60] 224 2 32M 7.1G ‚Äì 82.5\nCvT-21 [60] 384 2 32M 24.9G ‚Äì 83.3\nSwin-T [38] 224 2 29M 4.5G 755/s 81.3\nSwin-S [38] 224 2 50M 8.7G 437/s 83.0\nSwin-B [38] 224 2 88M 15.4G 278/s 83.3\nSwin-B [38] 384 2 88M 47.0G 85/s 84.2\nCSWin-T 224 2 23M 4.3G 701/s 82.7\nCSWin-S 224 2 35M 6.9G 437/s 83.6\nCSWin-B 224 2 78M 15.0G 250/s 84.2\nCSWin-B 384 2 78M 47.0G 85.4\nTable 2. Comparison of different models on ImageNet-1K.\nMethod Param Size FLOPs Top-1 Method Param Size FLOPs Top-1\nR-101x3 388M 384 2 204.6G 84.4 R-152x4 937M 480 2 840.5G 85.4\nViT-B/16 86M 384 2 55.4G 84.0 ViT-L/16 307M 384 2 190.7G 85.2\nSwin-B 88M 224 2 15.4G 85.2 Swin-L 197M 224 2 34.5G 86.3\n384 2 47.1G 86.4 384 2 103.9G 87.3\n224 2 15.0G 85.9 224 2 31.5G 86.5\nCSWin-B 78M 384 2 47.0G 87.0 CSWin-L 173M 384 2 96.8G 87.5\nTable 3. ImageNet-1K Ô¨Åne-tuning results by pre-training on\nImageNet-21K datasets.\n4.1. ImageNet-1K ClassiÔ¨Åcation\nFor fair comparison, we follow the training strategy\nin DeiT [53] as other baseline Transformer architectures\n[38, 60]. SpeciÔ¨Åcally, all our models are trained for 300\nepochs with the input size of 224√ó224. We use the AdamW\noptimizer with weight decay of 0.05 for CSWin-T/S and\n0.1 for CSWin-B. The default batch size and initial learning\nrate are set to 1024 and 0.001, and the cosine learning rate\nscheduler with 20 epochs linear warm-up is used. We apply\nincreasing stochastic depth [29] augmentation for CSWin-T,\nCSWin-S, and CSWin-B with the maximum rate as 0.1, 0.3,\n0.5 respectively. When reporting the results of 384 √ó384\ninput, we Ô¨Åne-tune the models for 30 epochs with the weight\nBackbone #Params FLOPs Mask R-CNN 1x schedule Mask R-CNN 3x + MS schedule\n(M) (G) APb APb50 APb75 APm APm50 APm75 APb APb50 APb75 APm APm50 APm75\nRes50 [22] 44 260 38.0 58.6 41.4 34.4 55.1 36.7 41.0 61.7 44.9 37.1 58.4 40.1\nPVT-S [58] 44 245 40.4 62.9 43.8 37.8 60.1 40.3 43.0 65.3 46.9 39.9 62.5 42.8\nViL-S [69] 45 218 44.9 67.1 49.3 41.0 64.2 44.1 47.1 68.7 51.5 42.7 65.9 46.2\nTwinsP-S [11] 44 245 42.9 65.8 47.1 40.0 62.7 42.9 46.8 69.3 51.8 42.6 66.3 46.0\nTwins-S [11] 44 228 43.4 66.0 47.3 40.3 63.2 43.4 46.8 69.2 51.2 42.6 66.3 45.8\nSwin-T [38] 48 264 42.2 64.6 46.2 39.1 61.6 42.0 46.0 68.2 50.2 41.6 65.1 44.8\nCSWin-T 42 279 46.7 68.6 51.3 42.2 65.6 45.4 49.0 70.7 53.7 43.6 67.9 46.6\nRes101 [22] 63 336 40.4 61.1 44.2 36.4 57.7 38.8 42.8 63.2 47.1 38.5 60.1 41.3\nX101-32 [63] 63 340 41.9 62.5 45.9 37.5 59.4 40.2 44.0 64.4 48.0 39.2 61.4 41.9\nPVT-M [58] 64 302 42.0 64.4 45.6 39.0 61.6 42.1 44.2 66.0 48.2 40.5 63.1 43.5\nViL-M [69] 60 261 43.4 ‚Äî- ‚Äî- 39.7 ‚Äî- ‚Äî- 44.6 66.3 48.5 40.7 63.8 43.7\nTwinsP-B [11] 64 302 44.6 66.7 48.9 40.9 63.8 44.2 47.9 70.1 52.5 43.2 67.2 46.3\nTwins-B [11] 76 340 45.2 67.6 49.3 41.5 64.5 44.8 48.0 69.5 52.7 43.0 66.8 46.6\nSwin-S [38] 69 354 44.8 66.6 48.9 40.9 63.4 44.2 48.5 70.2 53.5 43.3 67.3 46.6\nCSWin-S 54 342 47.9 70.1 52.6 43.2 67.1 46.2 50.0 71.3 54.7 44.5 68.4 47.7\nX101-64 [63] 101 493 42.8 63.8 47.3 38.4 60.6 41.3 44.4 64.9 48.8 39.7 61.9 42.6\nPVT-L [58] 81 364 42.9 65.0 46.6 39.5 61.9 42.5 44.5 66.0 48.3 40.7 63.4 43.7\nViL-B [69] 76 365 45.1 ‚Äî- ‚Äî- 41.0 ‚Äî- ‚Äî- 45.7 67.2 49.9 41.3 64.4 44.5\nTwinsP-L [11] 81 364 45.4 ‚Äî- ‚Äî- 41.5 ‚Äî- ‚Äî- ‚Äî- ‚Äî- ‚Äî- ‚Äî- ‚Äî- ‚Äî-\nTwins-L [11] 111 474 45.9 ‚Äî- ‚Äî- 41.6 ‚Äî- ‚Äî- ‚Äî- ‚Äî- ‚Äî- ‚Äî- ‚Äî- ‚Äî-\nSwin-B [38] 107 496 46.9 ‚Äî- ‚Äî- 42.3 ‚Äî- ‚Äî- 48.5 69.8 53.2 43.4 66.8 46.9\nCSWin-B 97 526 48.7 70.4 53.9 43.9 67.8 47.3 50.8 72.1 55.8 44.9 69.1 48.3\nTable 4. Object detection and instance segmentation performance on the COCO val2017 with the Mask R-CNN framework. The FLOPs (G)\nare measured at resolution 800 √ó1280, and the models are pre-trained on the ImageNet-1K. ResNet/ResNeXt results are copied from [58].\ndecay of 1e-8, learning rate of 1e-5, batch size of 512.\nIn Table 11, we compare our CSWin Transformer with\nstate-of-the-art CNN and Transformer architectures. With\nthe limitation of pages, we only compare with a few classical\nmethods here and make a comprehensive comparison in the\nsupplemental materials.\nIt shows that our CSWin Transformers outperform pre-\nvious state-of-the-art vision Transformers by large margins.\nFor example, CSWin-T achieves 82.7% Top-1 accuracy with\nonly 4.3G FLOPs, surpassing CvT-13, Swin-T and DeiT-S\nby 1.1%, 1.4% and 2.9% respectively. And for the small and\nbase model setting, our CSWin-S and CSWin-B also achieve\nthe best performance. When Ô¨Ånetuned on the 384 √ó384\ninput, a similar trend is observed, which well demonstrates\nthe powerful learning capacity of our CSWin Transformers.\nCompared with state-of-the-art CNNs, we Ô¨Ånd our CSWin\nTransformer is the only Transformer based architecture that\nachieves comparable or even better results than EfÔ¨Åcient-\nNet [51] under the small and base settings, while using less\ncomputation complexity . It is also worth noting that neu-\nral architecture search is used in EfÔ¨ÅcientNet but not in our\nCSWin Transformer design.\nWe further pre-train CSWin Transformer on ImageNet-\n21K dataset, which contains 14.2M images and 21K classes.\nModels are trained for 90 epochs with the input size of\n224 √ó224. We use the AdamW optimizer with weight decay\nof 0.1 for CSWin-B and 0.2 for CSWin-L, and the default\nbatch size and initial learning rate are set to 2048 and 0.001.\nWhen Ô¨Åne-tuning on ImageNet-1K, we train the models for\n30 epochs with the weight decay of 1e-8, learning rate of\n1e-5, batch size of 512. The increasing stochastic depth [29]\nBackbone #ParamsFLOPs Cascade Mask R-CNN 3x +MS\n(M) (G) APb APb50 APb75 APm APm50 APm75\nRes50 [22] 82 739 46.3 64.3 50.5 40.1 61.7 43.4\nSwin-T [38] 86 745 50.5 69.3 54.9 43.7 66.6 47.1\nCSWin-T 80 757 52.5 71.5 57.1 45.3 68.8 48.9\nX101-32 [63]101 819 48.1 66.5 52.4 41.6 63.9 45.2\nSwin-S [38] 107 838 51.8 70.4 56.3 44.7 67.9 48.5\nCSWin-S 92 820 53.7 72.2 58.4 46.4 69.6 50.6\nX101-64 [63]140 972 48.3 66.4 52.3 41.7 64.0 45.1\nSwin-B [38] 145 982 51.9 70.9 56.5 45.0 68.4 48.7\nCSWin-B 135 1004 53.9 72.6 58.5 46.4 70.0 50.4\nTable 5. Object detection and instance segmentation performance\non the COCO val2017 with Cascade Mask R-CNN.\naugmentation for both CSWin-B and CSWin-L is set to 0.1.\nTable.3 reports the results of pre-training on ImageNet-\n21K. Compared to the results of CSWin-B pre-trained on\nImageNet-1K, the large-scale data of ImageNet-21K brings\na 1.6%‚àº1.7% gain. CSWin-B and CSWin-L achieve 87.0%\nand 87.5% top-1 accuracy, surpassing previous methods.\n4.2. COCO Object Detection\nNext, we evaluate CSWin Transformer on the COCO\nobjection detection task with the Mask R-CNN [21] and\nCascade Mask R-CNN [2] framework respectively. SpeciÔ¨Å-\ncally, we pretrain the backbones on the ImageNet-1K dataset\nand follow the Ô¨Ånetuning strategy used in Swin Transformer\n[38] on the COCO training set.\nWe compare CSWin Transformer with various backbones:\nprevious CNN backbones ResNet [22], ResNeXt(X) [62],\nand Transformer backbones PVT [58], Twins [11], and\nSwin [38]. Table 4 reports the results of the Mask R-CNN\nBackbone Semantic FPN 80k Upernet 160k\n#Param.FLOPsmIoU#Param.FLOPsSS/MS mIoU\nRes50 [22] 28.5 183 36.7 ‚Äî- ‚Äî- ‚Äî-/‚Äî-\nPVT-S [58] 28.2 161 39.8 ‚Äî- ‚Äî- ‚Äî-/‚Äî-\nTwinsP-S [11]28.4 162 44.3 54.6 919 46.2/47.5\nTwins-S [11] 28.3 144 43.2 54.4 901 46.2/47.1\nSwin-T [38] 31.9 182 41.5 59.9 945 44.5/45.8\nCSWin-T 26.1 202 48.2 59.9 959 49.3/50.7\nRes101 [22] 47.5 260 38.8 86.0 1029 ‚Äî-/44.9\nPVT-M [58] 48.0 219 41.6 ‚Äî- ‚Äî- ‚Äî-/‚Äî-\nTwinsP-B [11]48.1 220 44.9 74.3 977 47.1/48.4\nTwins-B [11] 60.4 261 45.3 88.5 1020 47.7/48.9\nSwin-S [38] 53.2 274 45.2 81.3 1038 47.6/49.5\nCSWin-S 38.5 271 49.2 64.6 1027 50.4/51.5\nX101-64 [63] 86.4 ‚Äî 40.2 ‚Äî- ‚Äî- ‚Äî-/‚Äî-\nPVT-L [58] 65.1 283 42.1 ‚Äî- ‚Äî- ‚Äî-/‚Äî-\nTwinsP-L [11]65.3 283 46.4 91.5 1041 48.6/49.8\nTwins-L [11] 103.7 404 46.7 133.0 1164 48.8/50.2\nSwin-B [38] 91.2 422 46.0 121.0 1188 48.1/49.7\nCSWin-B 81.2 464 49.9 109.2 1222 51.1/52.2\nSwin-B‚Ä†[38] ‚Äî- ‚Äî- ‚Äî- 121.0 1841 50.0/51.7\nSwin-L‚Ä†[38] ‚Äî- ‚Äî- ‚Äî- 234.0 3230 52.1/53.5\nCSWin-B‚Ä† ‚Äî- ‚Äî- ‚Äî- 109.2 1941 51.8/52.6\nCSWin-L‚Ä† ‚Äî- ‚Äî- ‚Äî- 207.7 2745 54.0/55.7\nTable 6. Performance comparison of different backbones on the\nADE20K segmentation task. Two different frameworks semantic\nFPN and Upernet are used. FLOPs are calculated with resolution\n512 √ó2048. ResNet/ResNeXt results and Swin FPN results are\ncopied from [58] and [11] respectively. ‚Ä†means the model is pre-\ntrained on ImageNet-21K and Ô¨Ånetuned with 640√ó640 resolution.\nframework with ‚Äú1√ó‚Äù (12 training epoch) and ‚Äú3 √ó+MS‚Äù\n(36 training epoch with multi-scale training) schedule. It\nshows that our CSWin Transformer variants clearly outper-\nforms all the CNN and Transformer counterparts. In details,\nour CSWin-T outperforms Swin-T by +4.5 box AP, +3.1\nmask AP with the 1√óschedule and +3.0 box AP, +2.0 mask\nAP with the 3√óschedule respectively. We also achieve\nsimilar performance gain on small and base conÔ¨Åguration.\nTable 5 reports the results with the Cascade Mask R-\nCNN framework. Though Cascade Mask R-CNN is overall\nstronger than Mask R-CNN, we observe CSWin Transform-\ners still surpass the counterparts by promising margins under\ndifferent model conÔ¨Ågurations.\n4.3. ADE20K Semantic Segmentation\nWe further investigate the capability of CSWin Trans-\nformer for Semantic Segmentation on the ADE20K [72]\ndataset. Here we employ the semantic FPN [33] and Uper-\nnet [61] as the basic framework. For fair comparison, we\nfollow previous works [38, 58] and train Semantic FPN 80k\niterations with batch size as 16, and Upernet 160k iterations\nwith batch size as 16, more details are provided in the sup-\nplementary material. In Table 6, we report the results of\ndifferent methods in terms of mIoU and Multi-scale tested\nModel Cascade Mask R-CNN on COCOUperNet on ADE20K\n#Param. FLOPs FPS APb/m #Param. FLOPs FPS mIoU\nSwin-T 86M 745G 15.3 50.5/43.7 60M 945G 18.5 44.5\nCSWin-T80M 757G14.2 52.5/45.3 60M 959G17.3 49.3\nSwin-S 107M 838G 12.0 51.8/44.7 81M 1038G 15.2 47.6\nCSWin-S 92M 820G11.7 53.7/46.4 65M 1027G15.6 50.4\nSwin-B 145M 982G 11.2 51.9/45.0121M 1188G 9.92 48.1\nCSWin-B135M 1004G9.6 53.9/46.4 109M 1222G9.08 51.1\nTable 7. FPS comparison with Swin on downstream tasks.\nmIoU (MS mIoU). It can be seen that, our CSWin Trans-\nformers signiÔ¨Åcantly outperform previous state-of-the-arts\nunder different conÔ¨Ågurations. In details, CSWin-T, CSWin-\nS, CSWin-B achieve +6.7, +4.0, +3.9 higher mIOU than the\nSwin counterparts with the Semantic FPN framework, and\n+4.8, +2.8, +3.0 higher mIOU with the Upernet framework.\nCompared to the CNN counterparts, the performance gain\nis very promising and demonstrates the potential of vision\nTransformers again. When using the ImageNet-21K pre-\ntrained model, our CSWin-L further achieves 55.7 mIoU\nand surpasses the previous best model by +2.2 mIoU, while\nusing less computation complexity.\n4.4. Inference Speed.\nHere we report the inference speed of our CSWin and\nSwin works. For downstream tasks, we report the FPS of\nCascade Mask R-CNN for object detection on COCO and\nUperNet for semantic segmentation on ADE20K. In most\ncases, the speed of our model is only slightly slower than\nSwin (less than 10%), but our model outperforms Swin by\nlarge margins. For example, on COCO, CSWin-S are +1.9%\nbox AP and +1.7% mask AP higher than Swin-S with sim-\nilar inference speed(11.7 FPS vs. 12 FPS). Note that our\nCSWin-T performs better than Swin-B on box AP(+0.6%),\nmask AP(+0.3%) with much faster inference speed(14.2\nFPS vs. 11.2 FPS), indicating our CSWin achieves better\naccuracy/FPS trade-offs.\n4.5. Ablation Study\nTo better understand CSWin Transformers, we compare\neach key component with the previous works under a com-\npletely fair setting that we use the same architecture and\nhyper-parameter for the following experiments, and only\nvary one component for each ablation. For time considera-\ntion, we use Mask R-CNN with 1x schedule as the default\nsetting for detection and instance segmentation evaluation,\nand Semantic FPN with 80k iterations and single-scale test\nfor segmentation evaluation.\nParallel Multi-Head Grouping. We Ô¨Årst study the effec-\ntiveness of our novel ‚ÄúParallel Multi-Head Grouping‚Äù strat-\negy. Here we compare Axial-Attention [25] and Criss-Cross-\nAttention [30] under the CSWin-T backbone. ‚ÄúAttention\nregion‚Äù is used as the computation cost metric for detailed\nModel Attention ImageNet COCO ADE20K\nReigon #Param. FLOPs FPS Top1(%)#Param. FLOPs FPS APb APm #Param. FLOPs FPS mIoU(%)\nAxial H 23M 4.2G 735 81.8 42M 258G 27.9 43.4 39.4 26M 186G 50.3 42.6\nCSWin (Ô¨Åx sw=1) H 23M 4.1G 721 81.9 42M 258G 26.8 45.2 40.8 26M 179G 49.1 47.5\nCriss-Cross H*2-1 23M 4.2G 187 82.2 42M 263G 5.5 45.2 40.9 26M 186G 17.6 47.4\nCSWin (Ô¨Åx sw=2) H*2 23M 4.2G 718 82.2 42M 263G 25.1 45.6 41.4 26M 186G 47.2 47.6\nCSWin (sw=1,2,7,7; Seq)sw√óH 23M 4.3G 711 82.4 42M 279G 22.3 45.1 41.1 26M 202G 45.2 46.2\nCSWin (sw=1,2,7,7) sw√óH 23M 4.3G 701 82.7 42M 279G 21.1 46.7 42.2 26M 202G 44.8 48.2\nTable 8. Stripes-Based attention mechanism comparison. ‚ÄòSeq‚Äô means sequential multi-head attention like Axial-attention. ‚ÄòAttention\nRegion‚Äô means the average number of tokens that each head calculates attention with.\n81.8\n82\n82.2\n82.4\n82.6\n82.8\n83\n4 4.2 4.4 4.6 4.8 5 5.2 5.4 5.6\nAccuracy\nFLOPS(G)\nsw=1\nsw=2\nsw=[1,2,7,7]\nsw=[14,14,14, 7] sw=[28,28,14, 7]\nsw=7\nFigure 4. Ablation on dynamic window size.\ncomparison. To simplify, we assume the attention is calcu-\nlated on a square input that H = W.\nIn Table.8, we Ô¨Ånd that the ‚Äúparallel multi-head grouping‚Äù\nis efÔ¨Åcient and effective, especially for downstream tasks.\nWhen we replace the Parallel manner with Sequential, the\nperformance of CSWin degrades on all tasks. When compar-\ning with previous methods under the similar attention region\nconstrain, our sw= 1 CSWin performs slightly better than\nAxial on ImageNet, while outperforming it by a large margin\non downstream tasks. Our sw= 2 CSWin performs slightly\nbetter than Criss-Cross Attention, while the speed of CSWin\nis 2√ó‚àº 5√ófaster than it on different tasks, this further\nproves that our ‚Äúparallel‚Äù design is much more efÔ¨Åcient.\nDynamic Stripe Width . In Fig.4 we study the trade off\nbetween stripe width and accuracy. We Ô¨Ånd that with the in-\ncrease of stripe width, the compution cost(FLOPS) increase,\nand the Top-1 classiÔ¨Åcation accuracy improves greatly at the\nbeginning and slows down when the width is large enough.\nOur default setting [1,2,7,7] achieves a good trade-off be-\ntween accuracy and FLOPs.\nAttention Mechanism Comparison. Following the above\nanalysis on each component of CSWin self-attention, we\nfurther compare with existing self-attention mechanisms. As\nsome of the methods need even layers in each stage, for a\nfair comparison, we use the Swin-T [38] as backbone and\nonly change the self-attention mechanism . In detail, we\nuse 2,2,6,2 blocks for the four stages with the 96 base chan-\nnel, non-overlapped token embedding [17], and RPE [38].\nThe results are reported in Table 9. Obviously, our CSWin\nself-attention mechanism performs better than existing self-\nattention mechanisms across all the tasks.\nPositional Encoding Comparison. The proposed LePE\nis specially designed to enhance the local positional infor-\nImageNet COCO ADE20K\nTop1(%) APb APm mIoU(%)\nSliding window [43] 81.4 ‚Äî ‚Äî ‚Äî-\nShifted window [38] 81.3 42.2 39.1 41.5\nSpatially Sep [11] 81.5 42.7 39.5 42.9\nSequential Axial [25] 81.5 40.4 37.6 39.8\nCriss-Cross [30] 81.7 42.9 39.7 43.0\nCross-shaped window 82.2 43.4 40.2 43.4\nTable 9. Comparison of different self-attention mechanisms.\nImageNet COCO ADE20K\nTop1(%) APb APm mIoU(%)\nNo PE 82.5 44.8 41.1 47.0\nAPE [17] 82.6 45.1 41.1 45.7\nCPE [12] 82.2 45.8 41.6 46.1\nCPE* [12] 82.4 45.4 41.3 46.6\nRPE [46] 82.7 45.5 41.3 46.6\nLePE 82.7 46.7 42.2 48.2\nTable 10. Comparison of different positional encoding mechanisms.\nmation on downstream tasks for various input resolutions.\nHere we use CSWin-T as the backbone and only very\nthe position encoding. In Table 10, we compare our LePE\nwith other recent positional encoding mechanisms(APE [17],\nCPE [12], and RPE [46]) for image classiÔ¨Åcation, object\ndetection and image segmentation. Besides, we also test\nthe variants without positional encoding (No PE) and CPE*,\nwhich is obtained by applying CPE before every Transformer\nblock. According to the comparison results, we see that: 1)\nPositional encoding can bring performance gain by introduc-\ning the local inductive bias; 2) Though RPE achieves similar\nperformance on the classiÔ¨Åcation task with Ô¨Åxed input resolu-\ntion, our LePE performs better (+1.2 box AP and +0.9 mask\nAP on COCO, +0.9 mIoU on ADE20K) on downstream\ntasks where the input resolution varies; 3) Compared to APE\nand CPE, our LePE also achieves better performance.\n5. Conclusion\nIn this paper, we have presented a new Vision Trans-\nformer architecture named CSWin Transformer. The core\ndesign of CSWin Transformer is the CSWin Self-Attention,\nwhich performs self-attention in the horizontal and vertical\nstripes by splitting the multi-heads intoparallel groups. This\nmulti-head grouping design can enlarge the attention area\nof each token within one Transformer block efÔ¨Åciently. On\nthe other hand, the mathematical analysis also allows us to\nincrease the stripe width along the network depth to further\nenlarge the attention area with subtle extra computation cost.\nWe further introduce locally-enhanced positional encoding\ninto CSWin Transformer for downstream tasks. We achieved\nthe state-of-the-art performance on various vision tasks un-\nder constrained computation complexity. We are looking\nforward to applying it for more vision tasks.\nReferences\n[1] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020. 3\n[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving\ninto high quality object detection. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n6154‚Äì6162, 2018. 6, 12\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Con-\nference on Computer Vision, pages 213‚Äì229. Springer, 2020.\n2\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit:\nCross-attention multi-scale vision transformer for image clas-\nsiÔ¨Åcation, 2021. 13\n[5] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020. 2\n[6] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue\nWu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin. MMDetection: Open\nmmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019. 12\n[7] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin,\nShuicheng Yan, and Jiashi Feng. Dual path networks. arXiv\npreprint arXiv:1707.01629, 2017. 2\n[8] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu,\nLonghui Wei, and Qi Tian. Visformer: The vision-friendly\ntransformer, 2021. 13\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 3\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\net al. Rethinking attention with performers. arXiv preprint\narXiv:2009.14794, 2020. 3\n[11] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing\nRen, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Re-\nvisiting spatial attention design in vision transformers. arXiv\npreprint arXiv:2104.13840, 2021. 2, 6, 7, 8, 13\n[12] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-\naolin Wei, Huaxia Xia, and Chunhua Shen. Conditional\npositional encodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021. 2, 3, 4, 8, 13\n[13] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv e-prints, pages arXiv‚Äì2102,\n2021. 2\n[14] MMSegmentation Contributors. Mmsegmentation, an open\nsource semantic segmentation toolbox. https://github.\ncom/open-mmlab/mmsegmentation, 2020. 12\n[15] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V .\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space, 2019. 12\n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248‚Äì255. Ieee, 2009. 5\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 3, 8, 13\n[18] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\nHerv¬¥e J¬¥egou. Training vision transformers for image retrieval.\narXiv preprint arXiv:2102.05644, 2021. 2\n[19] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao\nLi, Zhicheng Yan, Jitendra Malik, and Christoph Feicht-\nenhofer. Multiscale vision transformers. arXiv preprint\narXiv:2104.11227, 2021. 13\n[20] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021. 2, 13\n[21] Kaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961‚Äì2969, 2017. 6,\n12\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 2, 6, 7\n[23] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentiÔ¨Åcation. arXiv preprint arXiv:2102.04378, 2021. 2\n[24] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk\nChun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial\ndimensions of vision transformers, 2021. 13\n[25] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\narXiv preprint arXiv:1912.12180, 2019. 2, 3, 7, 8\n[26] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: EfÔ¨Åcient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 2\n[27] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\nnetworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132‚Äì7141, 2018. 2\n[28] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 4700‚Äì4708, 2017. 2\n[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Deep networks with stochastic depth. In\nEuropean conference on computer vision , pages 646‚Äì661.\nSpringer, 2016. 5, 6, 12\n[30] Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang,\nHumphrey Shi, Wenyu Liu, and Thomas S. Huang. Ccnet:\nCriss-cross attention for semantic segmentation. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, pages\n1‚Äì1, 2020. 3, 7, 8\n[31] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie\nJin, Anran Wang, and Jiashi Feng. Token labeling: Training\na 85.5% top-1 accuracy vision transformer with 56m param-\neters on imagenet. arXiv preprint arXiv:2104.10858, 2021.\n2\n[32] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\nFranc ¬∏ois Fleuret. Transformers are rnns: Fast autoregressive\ntransformers with linear attention. In International Confer-\nence on Machine Learning, pages 5156‚Äì5165. PMLR, 2020.\n3\n[33] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDoll¬¥ar. Panoptic feature pyramid networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6399‚Äì6408, 2019. 7, 12\n[34] Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya.\nReformer: The efÔ¨Åcient transformer. arXiv preprint\narXiv:2001.04451, 2020. 3\n[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classiÔ¨Åcation with deep convolutional neural networks.\nAdvances in neural information processing systems, 25:1097‚Äì\n1105, 2012. 2\n[36] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and\nLuc Van Gool. Localvit: Bringing locality to vision trans-\nformers, 2021. 13\n[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740‚Äì755.\nSpringer, 2014. 5\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. arXiv\npreprint arXiv:2103.14030, 2021. 1, 2, 3, 5, 6, 7, 8, 12, 13\n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization, 2019. 12\n[40] Boris T Polyak and Anatoli B Juditsky. Acceleration of\nstochastic approximation by averaging. SIAM journal on\ncontrol and optimization, 30(4):838‚Äì855, 1992. 12\n[41] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Doll¬¥ar. Designing network design spaces,\n2020. 13\n[42] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and\nTimothy P Lillicrap. Compressive transformers for long-range\nsequence modelling. arXiv preprint arXiv:1911.05507, 2019.\n3\n[43] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 3, 8\n[44] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David\nGrangier. EfÔ¨Åcient content-based sparse attention with routing\ntransformers. Transactions of the Association for Computa-\ntional Linguistics, 9:53‚Äì68, 2021. 3\n[45] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n4510‚Äì4520, 2018. 2\n[46] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. arXiv preprint\narXiv:1803.02155, 2018. 2, 3, 4, 8\n[47] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2\n[48] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmentation.\narXiv preprint arXiv:2105.05633, 2021. 2\n[49] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-\nresolution representation learning for human pose estimation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5693‚Äì5703, 2019. 2\n[50] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1‚Äì9, 2015. 2\n[51] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105‚Äì6114. PMLR,\n2019. 2, 5, 6, 13\n[52] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng\nJuan. Sparse sinkhorn attention. In International Conference\non Machine Learning, pages 9438‚Äì9447. PMLR, 2020. 3\n[53] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv¬¥e J¬¥egou. Training\ndata-efÔ¨Åcient image transformers & distillation through atten-\ntion. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 3, 5, 12,\n13\n[54] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv ¬¥e J ¬¥egou. Going deeper with\nimage transformers. arXiv preprint arXiv:2103.17239, 2021.\n2\n[55] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki\nParmar, Blake Hechtman, and Jonathon Shlens. Scaling local\nself-attention for parameter efÔ¨Åcient visual backbones. arXiv\npreprint arXiv:2103.12731, 2021. 1, 3\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 2, 3, 4\n[57] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao.\nHigh-Ô¨Ådelity pluralistic image completion with transformers.\narXiv preprint arXiv:2103.14031, 2021. 2\n[58] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense predic-\ntion without convolutions. arXiv preprint arXiv:2102.12122,\n2021. 2, 5, 6, 7, 12, 13\n[59] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers.arXiv preprint\narXiv:2011.14503, 2020. 2\n[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang\nDai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions\nto vision transformers. arXiv preprint arXiv:2103.15808 ,\n2021. 1, 2, 3, 5, 13\n[61] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. UniÔ¨Åed perceptual parsing for scene understanding.\nIn Proceedings of the European Conference on Computer\nVision (ECCV), pages 418‚Äì434, 2018. 7, 12\n[62] Saining Xie, Ross Girshick, Piotr Doll¬¥ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492‚Äì1500,\n2017. 6\n[63] Saining Xie, Ross Girshick, Piotr Doll¬¥ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks, 2017. 6, 7\n[64] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-\nscale conv-attentional image transformers. arXiv preprint\narXiv:2104.06399, 2021. 2, 13\n[65] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei\nYu, and Wei Wu. Incorporating convolution designs into\nvisual transformers. arXiv preprint arXiv:2103.11816, 2021.\n2\n[66] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.\n2, 3, 5, 13\n[67] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classiÔ¨Åers with localizable\nfeatures, 2019. 12\n[68] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion, 2018. 12\n[69] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu\nYuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-\nformer: A new vision transformer for high-resolution image\nencoding. arXiv preprint arXiv:2103.15358, 2021. 2, 6, 13\n[70] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xi-\nang, Philip HS Torr, et al. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers.\narXiv preprint arXiv:2012.15840, 2020. 2\n[71] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation, 2017. 12\n[72] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-\nriuso, and Antonio Torralba. Scene parsing through ade20k\ndataset. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 633‚Äì641, 2017. 5, 7\n[73] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2\nExperiment Details\nIn this section, we provide more detailed experimental\nsettings about ImageNet and downstream tasks.\nImageNet-1K ClassiÔ¨Åcation. For a fair comparison, we\nfollow the training strategy in DeiT [53]. SpeciÔ¨Åcally, all\nour models are trained for 300 epochs with the input size\nof 224 √ó224. We use the AdamW optimizer with weight\ndecay of 0.05 for CSWin-T/S and 0.1 for CSWin-B. The\ndefault batch size and initial learning rate are set to 2048 and\n2e‚àí3 respectively, and the cosine learning rate scheduler\nwith 20 epochs linear warm-up is used. We adopt most of the\naugmentation in [53], including RandAugment [15] (rand-\nm9-mstd0.5-inc1) , Mixup [68] (prob= 0.8), CutMix [67]\n(prob= 1.0), Random Erasing [71] (prob= 0.25) and Ex-\nponential Moving Average [40] (ema-decay = 0.99984),\nincreasing stochastic depth [29] ( prob = 0 .2,0.4,0.5 for\nCSWin-T, CSWin-S, and CSWin-B respectively).\nWhen Ô¨Åne-tuning with 384 √ó384 input, we follow the\nsetting in [38] that Ô¨Åne-tune the models for 30 epochs with\nthe weight decay of 1e-8, learning rate of 5e-6, batch size\nof 256. We notice that a large ratio of stochastic depth is\nbeneÔ¨Åcial for Ô¨Åne-tuning and keeping it the same as the\ntraining stage.\nCOCO Object Detection and Instance Segmentation.\nWe use two classical object detection frameworks: Mask\nR-CNN [21] and Cascade Mask R-CNN [2] based on the\nimplementation from mmdetection [6]. For Mask R-CNN,\nwe train it with ImageNet-1K pretrained model with two set-\ntings: 1√óschedule and 3√ó+MS schedule. For 1√óschedule,\nwe train the model with single-scale input (image is resized\nto the shorter side of 800 pixels, while the longer side does\nnot exceed 1333 pixels) for 12 epochs. We use AdamW [39]\noptimizer with a learning rate of 0.0001, weight decay of\n0.05 and batch size of 16. The learning rate declines at the\n8 and 11 epoch with decay rate 0.1. The stochastic depth\nis also same as the ImageNet-1K setting that 0.1, 0.3, 0.5\nfor CSWin-T, CSWin-S, and CSWin-B respectively. For\n3√ó+MS schedule, we train the model with multi-scale input\n(image is resized to the shorter side between 480 and 800\nwhile the longer side is no longer than 1333) for 36 epochs.\nThe other settings are same as the 1√óexcept we decay the\nlearning rate at epoch 27 and 33. When it comes to Cascade\nMask R-CNN, we use the same 3√ó+MS schedule as Mask\nR-CNN.\nADE20K Semantic segmentation. Here we consider two\nsemantic segmentation frameworks: UperNet [61] and Se-\nmantic FPN [33] based on the implementation from mm-\nsegmentaion [14]. For UperNet, we follow the setting in\n[38] and use AdamW [39] optimizer with initial learning\nrate 6e‚àí5, weight decay of 0.01 and batch size of 16 (8\nGPUs with 2 images per GPU) for 160K iterations. The\nlearning rate warmups with 1500 iterations at the beginning\nand decays with a linear decay strategy. We use the default\naugmentation setting in mmsegmentation including random\nhorizontal Ô¨Çipping, random re-scaling (ratio range [0.5, 2.0])\nand random photo-metric distortion. All the models are\ntrained with input size 512 √ó512. The stochastic depth is\nset to 0.2, 0.4, 0.6 for CSWin-T, CSWin-S, and CSWin-B\nrespectively. When it comes to testing, we report both single-\nscale test result and multi-scale test ([0.5, 0.75, 1.0, 1.25,\n1.5, 1.75]√óof that in training).\nFor Semantic FPN, we follow the setting in [58]. We\nuse AdamW [39] optimizer with initial learning rate 1e‚àí4,\nweight decay of 1e‚àí4 and batch size of 16 (4 GPUs with 4\nimages per GPU) for 80K iterations.\nMore Experimetns\nWith the limitation of pages, we only compare with a few\nclassical methods in our paper, here we make a comprehen-\nsive comparison with more current methods on ImageNet-\n1K. We Ô¨Ånd that our CSWin performs best in concurrent\nworks.\nImageNet-1K 224 2 trained models\nMethod #Param. FLOPs Top-1\nReg-4G [41] 21M 4.0G 80.0\nEff-B4* [51] 19M 4.2G 82.9\nDeiT-S [53] 22M 4.6G 79.8\nPVT-S [58] 25M 3.8G 79.8\nT2T-14 [66] 22M 5.2G 81.5\nViL-S [69] 25M 4.9G 82.0\nTNT-S [20] 24M 5.2G 81.3\nCViT-15 [4] 27M 5.6G 81.0\nVisf-S [8] 40M 4.9G 82.3\nLViT-S [36] 22M 4.6G 80.8\nCoaTL-S [64] 20M 4.0G 81.9\nCPVT-S [12] 23M 4.6G 81.5\nSwin-T [38] 29M 4.5G 81.3\nCvT-13 [60] 20M 4.5G 81.6\nCSWin-T 23M 4.3G 82.7\nImageNet-1K 384 2 Ô¨Ånetuned models\nCvT-13 [60] 20M 16.3G 83.0\nT2T-14 [66] 22M 17.1G 83.3\nCViT c-15 [4] 28M 21.4G 83.5\nCSWin-T 23M 14.0G 84.3\n(a) Tiny Model\nImageNet-1K 224 2 trained models\nMethod #Param. FLOPs Top-1\nReg-8G [41] 39M 8.0G 81.7\nEff-B5* [51] 30M 9.9G 83.6\nPVT-M [58] 44M 6.7G 81.2\nPVT-L [58] 61M 9.8G 81.7\nT2T-19 [66] 39M 8.9G 81.9\nT2T t-19 [66] 39M 9.8G 82.2\nViL-M [69] 40M 8.7G 83.3\nMViT-B [19] 37M 7.8G 83.0\nCViT-18 [4] 43M 9.0G 82.5\nCViT c-18 [4] 44M 9.5G 82.8\nTwins-B [11] 56M 8.3G 83.2\nSwin-S [38] 50M 8.7G 83.0\nCvT-21 [60] 32M 7.1G 82.5\nCSWin-S 35M 6.9G 83.6\nImageNet-1K 384 2 Ô¨Ånetuned models\nCvT-21 [60] 32M 24.9G 83.3\nCViT c-18 [4] 45M 32.4G 83.9\nCSWin-S 35M 22.0G 85.0\n(b) Small Model\nImageNet-1K 224 2 trained models\nMethod #Param. FLOPs Top-1\nReg-16G [41] 84M 16.0G 82.9\nEff-B6* [51] 43M 19.0G 84.0\nDeiT-B [53] 87M 17.5G 81.8\nPiT-B [24] 74M 12.5G 82.0\nT2T-24 [66] 64M 14.1G 82.3\nT2T t-24 [66] 64M 15.0G 82.6\nCPVT-B [12] 88M 17.6G 82.3\nTNT-B [20] 66M 14.1G 82.8\nViL-B [69] 56M 13.4G 83.2\nTwins-L [11] 99M 14.8G 83.7\nSwin-B [38] 88M 15.4G 83.3\nCSWin-B 78M 15.0G 84.2\nImageNet-1K 384 2 Ô¨Ånetuned models\nViT-B/16 [17] 86M 49.3G 77.9\nDeiT-B [53] 86M 55.4G 83.1\nSwin-B [38] 88M 47.0G 84.2\nCSWin-B 78M 47.0G 85.4\n(c) Base Model\nTable 11. Comparison of different models on ImageNet-1K classiÔ¨Åcation. * means the EfÔ¨ÅcientNet are trained with other input sizes. Here\nthe models are grouped based on the computation complexity."
}