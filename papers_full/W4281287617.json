{
  "title": "Convolutions are competitive with transformers for protein sequence pretraining",
  "url": "https://openalex.org/W4281287617",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2285678400",
      "name": "Kevin K. Yang",
      "affiliations": [
        "Microsoft Research New England (United States)",
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2295768917",
      "name": "Nicolo Fusi",
      "affiliations": [
        "Microsoft Research New England (United States)",
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2493585915",
      "name": "Alex X. Lu",
      "affiliations": [
        "Microsoft Research New England (United States)",
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2285678400",
      "name": "Kevin K. Yang",
      "affiliations": [
        "Microsoft Research New England (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2295768917",
      "name": "Nicolo Fusi",
      "affiliations": [
        "Microsoft Research New England (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2493585915",
      "name": "Alex X. Lu",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Microsoft Research New England (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2102461176",
    "https://openalex.org/W3172603947",
    "https://openalex.org/W4213112325",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2540404261",
    "https://openalex.org/W4317374308",
    "https://openalex.org/W2997234557",
    "https://openalex.org/W2770647690",
    "https://openalex.org/W2977360892",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W4210494137",
    "https://openalex.org/W2890223884",
    "https://openalex.org/W2770884065",
    "https://openalex.org/W3213545574",
    "https://openalex.org/W2000695783",
    "https://openalex.org/W1153412892",
    "https://openalex.org/W2947082286",
    "https://openalex.org/W3127426316",
    "https://openalex.org/W2483469645",
    "https://openalex.org/W2379594833",
    "https://openalex.org/W2735621019",
    "https://openalex.org/W3015921770",
    "https://openalex.org/W3129991888",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W3037620288",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W3033943443",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W3208574069",
    "https://openalex.org/W3197584120",
    "https://openalex.org/W4221159485",
    "https://openalex.org/W4224066120",
    "https://openalex.org/W4223581484",
    "https://openalex.org/W4365444089",
    "https://openalex.org/W4224939843",
    "https://openalex.org/W4387966974",
    "https://openalex.org/W4318751307",
    "https://openalex.org/W3098365587",
    "https://openalex.org/W3211728297"
  ],
  "abstract": "Abstract Pretrained protein sequence language models have been shown to improve the performance of many prediction tasks, and are now routinely integrated into bioinformatics tools. However, these models largely rely on the Transformer architecture, which scales quadratically with sequence length in both run-time and memory. Therefore, state-of-the-art models have limitations on sequence length. To address this limitation, we investigated if convolutional neural network (CNN) architectures, which scale linearly with sequence length, could be as effective as transformers in protein language models. With masked language model pretraining, CNNs are competitive to and occasionally superior to Transformers across downstream applications while maintaining strong performance on sequences longer than those allowed in the current state-of-the-art Transformer models. Our work suggests that computational efficiency can be improved without sacrificing performance simply by using a CNN architecture instead of a Transformer, and emphasizes the importance of disentangling pretraining task and model architecture.",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7519971132278442
    },
    {
      "name": "Computer science",
      "score": 0.7440550923347473
    },
    {
      "name": "Language model",
      "score": 0.609096884727478
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5484691858291626
    },
    {
      "name": "Sequence labeling",
      "score": 0.4761795401573181
    },
    {
      "name": "Artificial intelligence",
      "score": 0.442188560962677
    },
    {
      "name": "Sequence (biology)",
      "score": 0.41213223338127136
    },
    {
      "name": "Machine learning",
      "score": 0.32388949394226074
    },
    {
      "name": "Task (project management)",
      "score": 0.2568230926990509
    },
    {
      "name": "Voltage",
      "score": 0.10790285468101501
    },
    {
      "name": "Engineering",
      "score": 0.08561825752258301
    },
    {
      "name": "Biology",
      "score": 0.07306790351867676
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4400600948",
      "name": "Microsoft Research New England (United States)",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210124949",
      "name": "Microsoft Research (India)",
      "country": "IN"
    }
  ]
}