{
  "title": "Mixup-Transformer: Dynamic Data Augmentation for NLP Tasks",
  "url": "https://openalex.org/W3115242847",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2108342712",
      "name": "Lichao Sun",
      "affiliations": [
        "Lehigh University"
      ]
    },
    {
      "id": "https://openalex.org/A2554586319",
      "name": "Congying Xia",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A1970544490",
      "name": "Wenpeng Yin",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2152545579",
      "name": "Tingting Liang",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2147553045",
      "name": "Philip Yu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2108700898",
      "name": "Lifang He",
      "affiliations": [
        "Lehigh University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2964120993",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2946411231",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W3014333092",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2986068180",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3100519617",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2112796928"
  ],
  "abstract": "Mixup is a latest data augmentation technique that linearly interpolates input examples and the corresponding labels. It has shown strong effectiveness in image classification by interpolating images at the pixel level. Inspired by this line of research, in this paper, we explore i) how to apply mixup to natural language processing tasks since text data can hardly be mixed in the raw format; ii) if mixup is still effective in transformer-based learning models,e.g., BERT.To achieve the goal, we incorporate mixup to transformer-based pre-trained architecture, named“mixup-transformer”, for a wide range of NLP tasks while keeping the whole end-to-end training system. We evaluate the proposed framework by running extensive experiments on the GLUEbenchmark. Furthermore, we also examine the performance of mixup-transformer in low-resource scenarios by reducing the training data with a certain ratio. Our studies show that mixup is a domain-independent data augmentation technique to pre-trained language models, resulting in significant performance improvement for transformer-based models.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3436–3440\nBarcelona, Spain (Online), December 8-13, 2020\n3436\nMixup-Transformer: Dynamic Data Augmentation for NLP Tasks\nLichao Sun1∗, Congying Xia2∗, Wenpeng Yin3, Tingting Liang4, Philip S. Yu2, Lifang He1\n1 Lehigh University; 2 University of Illinois at Chicago;\n3 Salesforce Research; 4 Hangzhou Dianzi University\njames.lichao.sun@gmail.com; {cxia8, psyu}@uic.edu;\nwyin@salesforce.com; liangtt@hdu.edu.cn;\nlih319@lehigh.edu\nAbstract\nMixup (Zhang et al., 2017) is a latest data augmentation technique that linearly interpolates input\nexamples and the corresponding labels. It has shown strong effectiveness in image classiﬁcation by\ninterpolating images at the pixel level. Inspired by this line of research, in this paper, we explore:\ni) how to apply mixup to natural language processing tasks since text data can hardly be mixed\nin the raw format; ii) if mixup is still effective in transformer-based learning models, e.g., BERT.\nTo achieve the goal, we incorporatemixup to transformer-based pre-trained architecture, named\n“mixup-transformer”, for a wide range of NLP tasks while keeping the whole end-to-end training\nsystem. We evaluate the proposed framework by running extensive experiments on the GLUE\nbenchmark. Furthermore, we also examine the performance of mixup-transformer in low-resource\nscenarios by reducing the training data with a certain ratio. Our studies show that mixup is a\ndomain-independent data augmentation technique to pre-trained language models, resulting in\nsigniﬁcant performance improvement for transformer-based models.\n1 Introduction\nDeep learning has shown outstanding performance in the ﬁeld of natural language processing (NLP).\nRecently, transformer-based methods (Devlin et al., 2018; Yang et al., 2019) have achieved state-of-\nthe-art performance across a wide variety of NLP tasks 1. However, these models highly rely on the\navailability of large amounts of annotated data, which is expensive and labor-intensive. To solve the data\nscarcity problem, data augmentation is commonly used in NLP tasks. For example, Wei and Zou (2019)\ninvestigated language transformations like insertion, deletion and swap. Several works (Malandrakis et\nal., 2019; Yoo et al., 2019; Xia et al., 2020b; Xia et al., 2020a) utilized variational autoencoders (V AEs)\n(Kingma and Welling, 2013) to generate more raw inputs. Nevertheless, these methods often rely on some\nextra knowledge to guarantee the quality of new inputs, and they have to be working in a pipeline.\nZhang et al. (2017) proposed mixup, a domain-independent data augmentation technique that linearly\ninterpolates image inputs on the pixel-based feature space. Guo et al. (2019) tried mixup in CNN (LeCun\net al., 1998) and LSTM (Hochreiter and Schmidhuber, 1997) for text applications. Despite effectiveness,\nthey conducted mixup only on the ﬁxed word embedding level like Zhang et al. (2017) did in image\nclassiﬁcation. Two questions arise, therefore: (i) how to apply mixup to NLP tasks if text data cannot be\nmixed in the raw format? Apart from the embedding feature space, what other representation spaces can be\nconstituted and used? ii) whether or not mixup can boost the state-of-the-art further in transformer-based\nlearning models, such as BERT (Devlin et al., 2018).\nTo answer these questions, we stack a mixup layer over the ﬁnal hidden layer of the pre-trained\ntransformer-based model. The resulting system can be applied to a broad of NLP tasks; in particular,\nit is still end-to-end trainable. We evaluate our proposed mixup-transformer on the GLUE benchmark,\n∗Indicates Equal Contribution\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n1Please see the General Language Understanding Evaluation (GLUE) (https://gluebenchmark.com/) for tracking performance.\n3437\nwhich shows that mixup can consistently improve the performance of each task. Our contributions are\nsummarized as follows:\n•We propose the mixup-transformer that applies mixup into transformer-based pre-trained models. To\nour best knowledge, this is the ﬁrst work that explores the effectiveness of mixup in Transformer.\n•In experiments, we demonstrate that mixup-transformer can consistently promote the performance\nacross a wide range of NLP benchmarks, and it is particularly helpful in low-resource scenarios where we\nreduce the training data from 10% to 90%.\n2 Mixup-Transformer\nIn this section, we ﬁrst introduce the mixup used in previous works. Then, we show how to incorporate\nthe mixup into transformer-based methods and how to do the ﬁne-turning on different text classiﬁcation\ntasks. Last, we will discuss the difference between the previous works and our new approach.\nFigure 1: The overall framework of mixup-transformer. xi, xj are two separate sentences, fed to the same Transformer T . T (xi)\nis the representation of the input xi, generated by T . ˆx and ˆy are the interpolated representation and label, respectively.\n2.1 Mixup\nMixup is ﬁrst proposed for image classiﬁcation (Zhang et al., 2017), which incorporates the prior\nknowledge that linear interpolations of feature representations should lead to the same interpolations of\nthe associated targets. In mixup, virtual training examples are constructed by two examples drawn at\nrandom from the training data:\nˆx= λxi + (1−λ)xj ,where xi, xj are raw inputs;\nˆy= λyi + (1−λ)yj ,where yi, yj are corresponding labels,\nwhere λcould be either ﬁxed value in [0,1] or λ∼Beta(α,α), for α∈(0,∞). In previous works, mixup\nis a static data augmentation approach that improves and robusts the performance in image classiﬁcation.\n2.2 Mixup for Text Classiﬁcation\nText classiﬁcation is the most fundamental problem in the NLP ﬁeld. Unlike image data, text input\nconsists of discrete units (words) without an inherent ordering or algebraic operations - it could be one\nsentence, two sentences, a paragraph or a whole document.\nThe ﬁrst step of text classiﬁcation is to use the word embedding to convert each word of the text into a\nvector representation. In the traditional approaches, the word embedding method can be bag-of-words,\nor a ﬁxed word to vector mapping dictionary built by CNN or LSTM. In our approach, instead of using\nthe traditional encoding methods, we use transformer-based pre-trained language models to learn the\nrepresentations for text data. For downstream tasks, we ﬁne-tune transformer-based models with the\nmixup data augmentation method. Formally, mixup-transformer constructs virtual hidden representations\ndynamically durning the training process as follows:\nˆx= λ·T(xi) + (1−λ) ·T(xj ),where T(xi), T(xj ) are output features from transformers;\nˆy= λyi + (1−λ)yj ,where yi, yj are corresponding labels,\nwhere T(·) represents outputs of the transformer layers as shown in Figure 1. Note that, the mixup\nprocess is trained together with the ﬁne-tuning process in an end-to-end fashion, and the hidden mixup\nrepresentations are dynamic during the training process.\n3438\n2.3 Discussion\nIn this section, we highlight two main differences between our approach and previous methods using\nmixup techniques (Zhang et al., 2017; Guo et al., 2019) for comparison.\n•Dynamic mixup representation. For each input pair, xi and xj , the previous approaches produce a\nﬁxed mixup representation given a ﬁxed λ. However, the mixup hidden representations in our approach\nare dynamic since they are trained together with the ﬁne-tuning process.\n•Dynamic mixup activation. Since a pre-trained network needs to be ﬁne-tuned for a speciﬁc task,\nwe can dynamically activate the mixup during the training. For example, if the training epoch is 3, we\ncan choose to use mixup in any epoch or all epochs. In our experiments, we ﬁne-tune the model without\nmixup in the ﬁrst half of epochs for good representations and add mixup in the last half of the epochs.\n3 Experiments\nTo show the effectiveness of our proposedmixup-transformer, we conduct extensive experiments by adding\nthe mixup strategy to transformer-based models on seven NLP tasks contained in the GLUE benchmark.\nFurthermore, we reduce the training data with different ratios (from 10% to 90%) to see how the mixup\nstrategy works with insufﬁcient training data. We report the performance on development sets for all the\ntasks because the test time is limited by the online GLUE benchmark.\nDatasets. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is\na collection of diverse natural language understanding tasks. Experiments are conducted on eight tasks in\nGLUE: CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005),\nSTS-B (Cer et al., 2017), QQP (Z. Chen and Zhao, 2018), MNLI (Williams et al., 2017), QNLI (Rajpurkar\net al., 2016), RTE (Bentivogli et al., 2009).\nBaselines. Two baselines are conducted in the experiments, including BERT-base and BERT-large\n(Devlin et al., 2018). We evaluate the performance of our methods by adding the mixup strategy to these\ntwo baselines.\nImplementation details. When ﬁne-tuning BERT with or without the mixup strategy for these NLP\ntasks, we ﬁx the hyper-parameters as follows: the batch size is 8, the learning rate is 2e-5, the max\nsequence length is 128, and the number of the training epochs is 3. We test different values ofλ, (from 0.1\nto 0.9) on the default dataset (CoLA) and ﬁnd mixup-transformer is insensitive to this hyper-parameter, so\nwe set a ﬁxed value of λ= 0.5.\n3.1 Results on full data\nDataset CoLA SST-2 MRPC STS-B QQP MNLI-mm QNLI RTE\n(Corr) (Acc) (Acc) (Corr) (Acc) (Acc) (Acc) (Acc)\nBERT-base 57.86 92.20 86.76 89.41 90.79 84.47 91.61 68.23\nBERT-base +mixup 59.58 92.78 88.48 88.66 90.98 85.12 91.84 71.84\nImproved 1.72 0.58 1.72 -0.75 0.21 0.65 0.23 3.61\nBERT-large 59.71 91.97 86.27 89.21 90.38 85.92 91.96 69.31\nBERT-large +mixup 62.39 92.78 87.99 90.10 90.80 85.91 92.09 69.68\nImproved 2.68 0.81 1.72 0.89 0.42 -0.01 0.13 0.37\nTable 1: mixup-transformer results on eight NLP tasks. Matthew’s Correlations are reported for CoLA, Spearman correlations\nare reported for STS-B, and accuracy scores are reported for the other tasks.\nExperimental results for eight different NLP tasks are illustrated in Table 1. By adding the proposed\nmixup technique to BERT-base and BERT-large, themixup-transformer improves the performance consis-\ntently on most of these tasks. The average improvement is around 1% for all the settings. The highest\nperformance gain comes from the RTE task by adding the proposed mixup technique on BERT-base. In\nthis experiment, the accuracy improves from 68.23% to 71.84%, which is an increase of 3.61%. The\n3439\nMatthew’s correlation for CoLA increases from 59.71% to 62.39% (improved 2.68%) with mixup on\nBERT-large. Some experiments also get performance decrease with mixup. For example, adding mixup to\nBERT-base on STS-B decreases the Spearman correlation from 89.41% to 88.66%. Overall, most of the\ntasks improved (14 out of 16, while 2 got slightly worse) were found by applying mixup-transformer.\nTraining Data 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nBERT-large 74.51 77.45 77.69 82.11 83.58 84.07 84.07 84.56 86.76 85.53\nBERT-large +mixup 77.21 79.17 81.13 87.01 85.78 86.27 86.79 86.76 87.25 87.99\nImproved 2.70 1.72 3.44 4.90 2.21 2.21 2.72 2.21 0.49 2.46\nTable 2: mixup-transformer with reduced training data on MRPC. Accuracy scores are used to evaluate the performance.\n3.2 Results on limited data\nAs mixup is a technique for augmenting the feature space, it is interesting to see how it works when the\ntraining data is insufﬁcient. Therefore, we reduce the training data with a certain ratio (from 10% to 90%\nwith a step 10%) and test the effectiveness of mixup-transformer in low-resource scenarios. As shown in\nTable 2, BERT-large +mixup consistently outperforms BERT-large when we reduce the training data for\nMRPC, where the highest improvement (4.90%) is achieved when only using 40% of the training data.\nUsing the full training data (100%) gets an increase of 2.46%, which indicates that mixup-transformer\nworks even better with reduced annotations. We also report the experiments of reducing training data\non other tasks, including STS-B, RTE and CoLA. As shown in Figure 2, the mixup-transformer again\nconsistently improves the performance for all the experiments. The performance gains with less training\ndata (like 10% for STS-B, CoLA, and RTE) are higher than using full training data since data augmentation\nis more effective when the annotations are insufﬁcient. Therefore, the mixup strategy is highly helpful in\nmost low-resource scenarios.\n10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nTraining Data\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92Performance\nSTS-B-no-mixup STS-B-mixup\n(a) STS-B\n10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nTraining Data\n0.70\n0.75\n0.80\n0.85\n0.90Performance\nMRPC-no-mixup MRPC-mixup (b) MRPC\n10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nTraining Data\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75Performance\nRTE-no-mixup RTE-mixup\n(c) RTE\n10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nTraining Data\n0.45\n0.50\n0.55\n0.60\n0.65Performance\nCoLA-no-mixup CoLA-mixup (d) CoLA\nFigure 2: mixup-transformer with BERT-large runs with reduced training data for four tasks: STS-B, MRPC, RTE and CoLA.\n4 Conclusion\nIn this paper, we propose the mixup-transformer that incorporates a data augmentation technique called\nmixup into transformer-based models for NLP tasks. Unlike using the static mixup in previous works, our\napproach can dynamically construct new inputs for text classiﬁcation. Extensive experimental results show\nthat mixup-transformer can be dynamically used with a pre-trained model to achieve better performance\non GLUE benchmark.\nAcknowledgement\nThis work is supported in part by NSF under grants III-1763325, III-1909323, and SaTC-1930941.\n3440\nReferences\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The ﬁfth pascal recognizing textual\nentailment challenge. In TAC.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Se-\nmantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In\nProceedings of the Third International Workshop on Paraphrasing (IWP2005).\nHongyu Guo, Yongyi Mao, and Richong Zhang. 2019. Augmenting data with mixup for sentence classiﬁcation:\nAn empirical study. arXiv preprint arXiv:1905.08941.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.\nDiederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to docu-\nment recognition. Proceedings of the IEEE, 86(11):2278–2324.\nNikolaos Malandrakis, Minmin Shen, Anuj Goyal, Shuyang Gao, Abhishek Sethi, and Angeliki Metallinou.\n2019. Controlled text generation for data augmentation in intelligent artiﬁcial agents. arXiv preprint\narXiv:1910.03487.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. arXiv preprint arXiv:1606.05250.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher\nPotts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings\nof the 2013 conference on empirical methods in natural language processing, pages 1631–1642.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019. Neural network acceptability judgments. Trans-\nactions of the Association for Computational Linguistics, 7:625–641.\nJason W Wei and Kai Zou. 2019. Eda: Easy data augmentation techniques for boosting performance on text\nclassiﬁcation tasks. arXiv preprint arXiv:1901.11196.\nAdina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence\nunderstanding through inference. arXiv preprint arXiv:1704.05426.\nCongying Xia, Caiming Xiong, Philip Yu, and Richard Socher. 2020a. Composed variational natural language\ngeneration for few-shot intents. arXiv preprint arXiv:2009.10056.\nCongying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei Zhang, and Philip Yu. 2020b. Cg-bert: Conditional text\ngeneration with bert for generalized few-shot intent detection. arXiv preprint arXiv:2004.01881.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language understanding. In Advances in neural information process-\ning systems, pages 5753–5763.\nKang Min Yoo, Youhyun Shin, and Sang-goo Lee. 2019. Data augmentation for spoken language understanding\nvia joint variational generation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 33,\npages 7402–7409.\nX. Zhang Z. Chen, H. Zhang and L. Zhao. 2018. Quora question pairs.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2017. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8212998509407043
    },
    {
      "name": "Computer science",
      "score": 0.8135893940925598
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5227118730545044
    },
    {
      "name": "Training set",
      "score": 0.5199562311172485
    },
    {
      "name": "Language model",
      "score": 0.47130048274993896
    },
    {
      "name": "Architecture",
      "score": 0.4348057806491852
    },
    {
      "name": "Machine learning",
      "score": 0.4266456067562103
    },
    {
      "name": "Natural language processing",
      "score": 0.4034169912338257
    },
    {
      "name": "Speech recognition",
      "score": 0.3827414810657501
    },
    {
      "name": "Engineering",
      "score": 0.09821224212646484
    },
    {
      "name": "Voltage",
      "score": 0.08573788404464722
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I186143895",
      "name": "Lehigh University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I50760025",
      "name": "Hangzhou Dianzi University",
      "country": "CN"
    }
  ]
}