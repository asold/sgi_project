{
  "title": "Intelligible Language Modeling with Input Switched Affine Networks",
  "url": "https://openalex.org/W2559554776",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2586299787",
      "name": "Jakob Foerster",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230284425",
      "name": "Justin Gilmer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2335651288",
      "name": "Jan Chorowski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4211452172",
      "name": "Jascha Sohl-Dickstein",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2260734140",
      "name": "David Sussillo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2047125104",
    "https://openalex.org/W2594877703",
    "https://openalex.org/W1528620860",
    "https://openalex.org/W2342840547",
    "https://openalex.org/W2599025709",
    "https://openalex.org/W2128420091",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2952502547",
    "https://openalex.org/W2518756966",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2253807446",
    "https://openalex.org/W1826694107",
    "https://openalex.org/W1408639475",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2293078015",
    "https://openalex.org/W2545581354",
    "https://openalex.org/W1871961762",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2557270725",
    "https://openalex.org/W2898422183",
    "https://openalex.org/W2587529872",
    "https://openalex.org/W1800356822",
    "https://openalex.org/W1980324747",
    "https://openalex.org/W2177870565",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2149960632",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W2557738935"
  ],
  "abstract": "The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.",
  "full_text": "Input Switched Afﬁne Networks: An RNN Architecture Designed for\nInterpretability\nJakob N. Foerster* 1 Justin Gilmer * 2 Jascha Sohl-Dickstein 3 Jan Chorowski4 David Sussillo 3\nAbstract\nThere exist many problem domains where the\ninterpretability of neural network models is es-\nsential for deployment. Here we introduce a re-\ncurrent architecture composed of input-switched\nafﬁne transformations – in other words an RNN\nwithout any explicit nonlinearities, but with input-\ndependent recurrent weights. This simple form\nallows the RNN to be analyzed via straightfor-\nward linear methods: we can exactly characterize\nthe linear contribution of each input to the model\npredictions; we can use a change-of-basis to dis-\nentangle input, output, and computational hid-\nden unit subspaces; we can fully reverse-engineer\nthe architecture’s solution to a simple task. De-\nspite this ease of interpretation, the input switched\nafﬁne network achieves reasonable performance\non a text modeling tasks, and allows greater com-\nputational efﬁciency than networks with standard\nnonlinearities.\n1. Introduction\n1.1. The importance of interpretable machine learning\nAs neural networks move into applications where the out-\ncomes of human lives depend on their decisions, it is in-\ncreasingly crucial that we are able to interpret the decisions\nthey make. Indeed, the European Union is considering legis-\nlation with a clause that asserts that individuals have ’rights\nto explanation’, i.e. individuals should be able to under-\nstand how algorithms make decisions about them (Council\nof European Union, 2016). Example problem domains re-\n*Equal contribution 1This work was performed as an intern\nat Google Brain 2Work done as a member of the Google Brain\nResidency program (g.co/brainresidency) 3Google Brain,\nMountain View, CA, USA 4Work performed when author was\na visiting faculty at Google Brain. Correspondence to: Jakob\nN. Foerster <jakob.foerster@cs.ox.ac.uk>, David Sussillo <sus-\nsillo@google.com>.\nProceedings of the 34 th International Conference on Machine\nLearning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by\nthe author(s).\nquiring interpretable ML include self-driving cars (Bojarski\net al., 2016), air trafﬁc control (Katz et al., 2017), power\ngrid control (Siano et al., 2012), hiring and promotion deci-\nsions while preventing bias (Scarborough & Somers, 2006),\nautomated sentencing decisions in US courts (Tashea, 2017;\nBerk et al., 2017), and medical diagnosis (Gulshan et al.,\n2016). For many of these applications, practitioners will not\nadopt ML models without fully understanding what drives\ntheir predictions, including understanding when and how\nthese models fail (Ching et al., 2017; Deo, 2015).\n1.2. Post hoc analysis\nOne approach to interpreting neural networks is to train\nthe network as normal, and then apply analysis techniques\nafter training. Often this approach yields systems that per-\nform extremely well, but where interpretability is challeng-\ning. For example, Sussillo & Barak (2013) used lineariza-\ntion and nonlinear dynamical systems theory to understand\nRNNs solving a set of simple but varied tasks. Karpathy\net al. (2015) analyzed an LSTM (Hochreiter & Schmidhu-\nber, 1997) trained on a character-based language modeling\ntask. They were able to break down LSTM language model\nerrors into classes, such as e.g., “rare word” errors. Con-\ncurrently with our submission, Murdoch & Szlam (2017)\ndecomposed LSTM outputs using telescoping sums of statis-\ntics computed from memory cells at different RNN steps.\nThe decomposition is exact, but not unique and the authors\njustify it by demonstrating good performance of decision\nrules formed using the computed cell statistics.\nThe community is also interested in post hoc interpretation\nof feed-forward networks. Examples include the use of\nlinear probes in Alain & Bengio (2016), and a variety of\ntechniques (most driven by back-propagation) to assign\ncredit for activations to speciﬁc inputs or input patterns in\nfeed-forward networks (Zeiler et al., 2010; Le et al., 2012;\nMordvintsev et al., 2015).\n1.3. Building interpretability into the architecture\nA second approach is to build a neural network where inter-\npretability is an explicit design constraint. In this approach,\na typical outcome is a system that can be better understood,\nbut at the cost of reduced performance. Model classes whose\narXiv:1611.09434v2  [cs.AI]  12 Jun 2017\nInterpretable RNNs with Input Switched Afﬁne Networks\ndecisions are naturally interpretable include logistic regres-\nsion (Freedman, 2009), decision trees (Quinlan, 1987), and\nsupport vector machines with simple (e.g. linear) kernels\n(Andrew, 2013).\nIn this work we follow this second approach and build in-\nterpretability into our network model, while maintaining\ngood, though not always state-of-the-art, performance for\nthe tasks we study. We focus on the commonly studied\ntask of character based language modeling. We develop\nand analyze a model trained on a one-step-ahead predic-\ntion task of the Text8 dataset, which is 10 million char-\nacters of Wikipedia text (Mahoney, 2011), on the Billion\nWord Benchmark (Chelba et al., 2013), and ﬁnally on a toy\nmultiple parentheses counting task which we fully reverse\nengineer.\n1.4. Switched afﬁne systems\nThe model we introduce is an Input Switched Afﬁne Net-\nwork (ISAN), where the input determines the switching\nbehavior by selecting a transition matrix and bias as a func-\ntion of that input, and there is no nonlinearity. Linear time-\nvarying systems are standard material in undergraduate elec-\ntrical engineering text books, and are closely related to our\ntechnique.\nAlthough the ISAN is deterministic, probabilistic versions of\nswitching linear models with discrete latent variables have\na history in the context of probabilistic graphical models. A\nrecent example is the switched linear dynamical system in\n(Linderman et al., 2016). Focusing on language modeling,\n(Belanger & Kakade, 2015) deﬁned a probabilistic linear\ndynamical system (LDS) as a generative language model\nfor creating context-dependent token embeddings and then\nused steady-state Kalman ﬁltering for inference over token\nsequences. They used singular value decomposition and\ndiscovered that the right and left singular vectors were se-\nmantically and syntactically related. A critical difference\nbetween the ISAN and the LDS is that the ISAN weight\nmatrices are input token dependent (while the biases of both\nmodels are input dependent).\nMultiplicative neural networks (MRNNs) were proposed\nprecisely for character based language modeling in\n(Sutskever et al., 2011; Martens & Sutskever, 2011). The\nMRNN architecture is similar to our own, in that the dy-\nnamics matrix switches as a function of the input character.\nHowever, the MRNN relied on a tanh nonlinearity, while\nthe ISAN is explicitly linear. It is this property of our model\nwhich makes it both amenable to analysis, and computation-\nally efﬁcient.\nThe Observable Operator Model (OOM) (Jaeger, 2000) is\nsimilar to the ISAN in that the OOM updates a latent state\nusing a separate transition matrix for each input symbol\nand performs probabilistic sequence modeling. Unlike the\nISAN, the OOM requires that a linear projection of the hid-\nden state corresponds to a normalized sequence probability.\nThis imposes strong constraints on both the model param-\neters and the model dynamics, and restricts the choice of\ntraining algorithms. In contrast, the ISAN applies an afﬁne\nreadout to the hidden state to obtain logits, which are then\npushed through the softmax function to obtain probabilities.\nTherefore no constraints need to be imposed on the ISAN’s\nparameters and training is easy using backprop. Lastly, the\nISAN is formulated as an afﬁne, rather than linear model.\nWhile this doesn’t change the class of processes that can be\nmodeled, it stabilizes training and greatly enhances inter-\npretability, facilitating the analysis in Section 3.3.\n1.5. Paper structure\nIn what follows, we deﬁne the ISAN architecture, demon-\nstrate its performance on the one-step-ahead prediction task,\nand then analyze the model in a multitude of ways, most of\nwhich would be currently difﬁcult or impossible to accom-\nplish with modern nonlinear recurrent architectures.\n2. Methods\n2.1. Model deﬁnition\nIn what follows Wx and bx respectively denote a transition\nmatrix and a bias vector for a speciﬁc input x, the symbol\nxt is the input at time t, and ht is the hidden state at time t.\nOur ISAN model is deﬁned as\nht = Wxt ht−1 + bxt . (1)\nThe network also learns an initial hidden state h0. We em-\nphasize the intentional absence of any nonlinear activation\nfunction.\n2.2. Character level language modeling with ISAN\nWe trained RNNs on the Text8 Wikipedia dataset and the bil-\nlion word benchmark (BWB), for one-step-ahead character\nprediction. The Text8 dataset consists only of the 27 char-\nacters ‘a’-‘z’ and ‘_’ (space). The BWB dataset consist of\nUnicode text and was modelled as a sequence of bytes (256\ndiscrete tokens) that formed the UTF8-encoded data. Given\na character sequence of x1, ...,xt, the RNNs are trained to\nminimize the cross-entropy between the true next character,\nand the output prediction. We map from the hidden state,\nht, into a logit space via an afﬁne map. The probabilities\nare computed as\np (xt+1) =softmax (lt) (2)\nlt = Wro ht + bro, (3)\nwhere Wro and bro are the readout weights and biases,\nand lt is the logit vector. For the Text8 dataset, we split\nInterpretable RNNs with Input Switched Afﬁne Networks\nTable 1.The ISAN has similar performance to other RNN archi-\ntectures on the Text8 dataset. Performance of RNN architectures\non Text8 one-step-ahead prediction, measured as cross-entropy\nloss on a held-out test set, in bits per character. The loss is shown\nas a function of the maximum number of parameters a model is\nallowed. The values reported for all other architectures are taken\nfrom (Collins et al., 2016).\nParameter count 8e4 3.2e5 1.28e6\nRNN 1.88 1.69 1.59\nIRNN 1.89 1.71 1.58\nGRU 1.83 1.66 1.59\nLSTM 1.85 1.68 1.59\nISAN 1.92 1.71 1.58\nthe data into 90%, 5%, and 5% for train, validation, and\ntest respectively, in line with (Mikolov et al., 2012). The\nnetwork was trained with the same hyperparameter tuning\ninfrastructure as in (Collins et al., 2016). For the BWB\ndataset, we used data splits and evaluation setup identical to\n(Józefowicz et al., 2016). Due to long experiment running\ntimes, we manually tuned the hyperparameters.\n3. Results and analysis\n3.1. ISAN performance on Text8 prediction\nThe results on Text8 are shown in Table 1. For the largest\nparameter count, the ISAN matches almost exactly the per-\nformance of all other nonlinear models with the same num-\nber of maximum parameters: RNN, IRNN, GRU, LSTM.\nHowever, we note that for small numbers of parameters the\nISAN performs considerably worse than other architectures.\nAll analyses use ISAN trained with 1.28e6 maximum pa-\nrameters (1.58 bpc cross entropy). Samples of generated\ntext from this model are relatively coherent. We show two\nexamples, after priming with \"annual reve\", at inverse tem-\nperature of 1.5, and 2.0, respectively:\n• “annual revenue and producer of the telecommunica-\ntions and former communist action and saving its new\nstate house of replicas and many practical persons”\n• “annual revenue seven ﬁve three million one nine nine\neight the rest of the country in the united states and\nsouth africa new”.\nAs a preliminary, comparative analysis, we performed PCA\non the state sequence over a large set of sequences for the\nvanilla RNN, GRU of varying sizes, and ISAN. This is\nshown in Figure 1. The eigenvalue spectra, in log of variance\nexplained, was signiﬁcantly ﬂatter for the ISAN than the\nother architectures.\nWe compared the ISAN performance to a fully linear RNN\nFigure 1.The ISAN makes fuller and more uniform use of its latent\nspace than vanilla RNNs or GRUs. Figure shows explained vari-\nance ratio of the ﬁrst 210 most signiﬁcant PCA dimensions of the\nhidden states across several architectures for the Text8 dataset. The\nlegend provides the number of latent units for each architecture.\nwithout input switched dynamics. This achieves a cross-\nentropy of 3.1 bits / char, independent of network size. This\nperplexity is only slightly better than that of a Naive Bayes\nmodel on the task, at 3.3 bits / char. The output probability\nof the fully linear network is a product of contributions from\neach previous character, as in Naive Bayes. Those factorial\ncontributions are learned however, giving the non-switched\nafﬁne network a slight advantage. We also trained a fully\nlinear network with a nonlinear readout. This achieves 2.15\nbits / char, independent of network size. Both of these\ncomparisons illustrate the importance of the input switched\ndynamics for achieving good results.\nLastly we also test to what extent the ISAN can deal with\nlarge dictionaries by running it on a byte-pair encoding of\nthe text8 task, where the input dictionary consists of the 272\ndifferent possible character combinations. We ﬁnd that in\nthis setup the LSTM consistently outperforms the ISAN for\nthe same number of parameters. At 1.3m parameters the\nLSTM achieves a cross entropy of 3.4 bits / char-pair, while\nISAN achieves 3.55. One explanation for this ﬁnding is that\nthe matrices in ISAN are 27 times smaller than the matrices\nof the LSTMs. For very large numbers of parameters the\nperformance of any architecture saturates in the number of\nparameters, at which point the ISAN can ‘catch-up’ with\nmore parameter efﬁcient architectures like LSTMs.\n3.2. ISAN performance on Billion Word Benchmark\nprediction\nWe trained ISAN and LSTM models on the BWB dataset.\nAll networks were trained using asynchronous gradient de-\nscent using the Adagrad learning rule. Our best LSTM\nmodel reached 1.1 bits per character, which matches pub-\nlished results (Hwang & Sung, 2016). The LSTM model\nhad one layer of 8192 LSTM units whose outputs were\nprojected onto 1024 dimensions (44e6 parameters). Our\nbest ISAN models reached 1.4 bits per character and used\nInterpretable RNNs with Input Switched Afﬁne Networks\nFigure 2.Using the linearity of the hidden state dynamics, predictions at step t can be broken out into contributions, κt\ns, from previous\nsteps. Accordingly, each row of the top panel corresponds to the propagated contribution ( κt\ns) of the input character at time s, to the\nprediction at time t (summed to create the logit at time t). The penultimate row contains the output bias vector replicated at every time\nstep. The last row contains the logits of the predicted next character, which is the sum of all rows above. The bottom panel contains the\ncorresponding softmax probabilities at each time t for all characters (time is separated by gray lines). Labeled is the character with the\nmaximum predicted probability. The time step boxed in red is examined in more detail in Figure 3.\n512 hidden units, a reduced set of most common 70 input\ntokens and 256 output tokens (18e6 parameters). Increasing\nISAN’s hidden layer size to 768 units (41e6 parameters)\nyielded a perplexity improvement to 1.36 bits/char. Investi-\ngation of generated samples shows that the ISAN learned\nthe distinction between lower- and upper-cased letters and is\nable to generate text which is coherent over short segments.\nTo demonstrate sample variability we show continuations\nof the prompt \"The [Pp]ol\" generated using the ISAN:\n• The Pol|ish pilgrims are as angry over the holiday trip\n• The Pol|ice Department subsequently slipped toward\n• The Pol|ice Federation has sought Helix also investors\n• The Pol|itico is in a tight crowd ever to moderated the\n• The pol|itical scientist in the Red Shirt Romance cannot\n• The pol|icy for all Balanchine had formed when it set a\n• The pol|l conducted when a suspected among Hispanic\n• The pol|itical frenzy sparked primary care programs\n3.3. Decomposition of current predictions based on\nprevious time steps\nAnalysis in this paper is carried out on the best-performing\nText8 ISAN model, which has 1, 271, 619 parameters, cor-\nresponding to 216 hidden units, and 27 dynamics matrices\nWx and biases bx.\nWith ISAN we can analyze which factors were important\nin the past for determining the current character prediction.\nTaking advantage of the linearity of the hidden state dynam-\nics for any sequence of inputs, we decompose the current\nlatent state ht into contributions originating from different\ntime points s in the history of the input:\nht =\nt∑\ns=0\n( t∏\ns′=s+1\nWxs′\n)\nbxs , (4)\nwhere the empty product when s+ 1> tis 1 by convention,\nand bx0 = h0 is the learned initial hidden state.\nUsing this decomposition and the fact that matrix multi-\nplication is a linear transformation we can also write the\nunnormalized logit-vector, lt, as a sum of terms linear in the\nbiases,\nlt = bro +\nt∑\ns=0\nκt\ns (5)\nκt\ns = Wro\n( t∏\ns′=s+1\nWxs′\n)\nbxs , (6)\nwhere κt\ns is the contribution from time step s to the logits at\ntime step t, and κt\nt = bxt . For notational convenience we\nwill sometimes replace the subscript s with the correspond-\ning input character xs at step s when referring to κt\ns. For\nexample, κt\n‘q’refers to the contribution from the character\n‘q’ in a string. Similarly, when discussing the summed con-\ntributions from a word or substring we will sometimes write\nκt\nword to mean the summed contributions of all the κt\ns from\nthat source word. For example, ∑\ns∈word κt\ns – κt\n‘the’refers\nto the total contribution from the word ‘the’ to the logit.\nWhile in standard RNNs the nonlinearity causes interde-\npendence of the bias terms across time steps, in the ISAN\nthe bias terms contribute to the state as independent linear\nInterpretable RNNs with Input Switched Afﬁne Networks\nFigure 3.Detailed view of the prediction stack for the ﬁnal ‘n’ in\n‘_annual_revenue’. In a) all κt\ns are shown, in b) only the contri-\nbutions to the ‘n’ logit and ‘r’ logits are shown, in orange and red\nrespectively, from each earlier character in the string. This corre-\nsponds to a zoom in view of the columns highlighted in orange and\nred in a). In c) we show how the sum of the contributions from the\nstring ‘_annual’,κt\n‘_annual’, pushes the prediction at ‘_annual_reve’\nfrom ‘r’ to ‘n’. Without this contribution the model decodes based\nonly on κt\n‘_reve’, leading to a MAP prediction of ‘reverse’. With\nthe contribution from κt\n‘_annual’it instead predicts ‘revenue’. The\ncontribution of κt\n‘_annual’to the ‘n’ and ‘r’ logits is linear and exact.\nterms that are propagated and transformed through time. We\nemphasize that κt\ns includes the multiplicative contributions\nfrom the Wxs′ for s < s′≤t. It is however independent\nof prior inputs, xs′ for s′< s. This is the main difference\nbetween the analysis we can carry out with the ISAN com-\npared to a nonlinear RNN. In a general recurrent network\nthe contribution of a speciﬁc character sequence will depend\non the hidden state at the start of the sequence. Due to the\nlinearity of the dynamics, this dependency does not exist in\nthe ISAN.\nIn Figure 2 we show an example of how this decomposi-\ntion allows us to understand why a particular prediction is\nmade at a given point in time, and how previous characters\ninﬂuence the decoding. For example, the sequence ‘_an-\nnual_revenue_’ is processed by the ISAN: Starting with an\nall-zero hidden state, we use equation (6) to accumulate\na sequence of κt\n‘_′ , κt\n‘a′ , κt\n‘n′ , κt\n‘n′ , .... We then used these\nvalues to understand the prediction of the network at some\ntime t, by simple addition across the s index.\nWe provide a detailed view of how past characters contribute\nto the logits predicting the next character in Figure 3. There\nare two competing options for the next letter in the word\nstem ‘reve’: either ‘revenue’ or ‘reverse’. We show that\nwithout the contributions from ‘_annual’ the most likely\ndecoding of the character after the second ‘e’ is ‘r’ (to form\n‘reverse’), while the contributions from ‘_annual’ tip the\nbalance in favor of ‘n’, decoding to ‘revenue’.\nUsing ISAN, we can investigate information timescales in\nthe network. For example, we investigated how quickly the\ncontributions of κt\ns decay as a function of t −s on average.\nFigure 4a shows that this contribution decays on two dif-\nferent exponential timescales. We hypothesize that the ﬁrst\ntime scale corresponds to the decay within a word, while the\nFigure 4. The time decay of the contributions from each char-\nacter to prediction. a) Average norm of κt\ns across training text,\nE\n[⏐⏐⏐⏐κt\ns\n⏐⏐⏐⏐\n2\n]\n, plotted as a function of t −s, and averaged across\nall source characters. The norm appears to decay exponentially at\ntwo rates, a faster rate for the ﬁrst ten or so characters, and then\na slower rate for more long term contributions. b) The median\ncross entropy as a function of the position in the word under three\ndifferent circumstances: the red line uses all of the κt\ns (baseline),\nthe green line sets allκt\ns apart from κt\n‘_’to zero, while the blue line\nonly sets κt\n‘_’ to zero. The results from panel b demonstrate the\ndisproportionately large importance of ‘_’ in decoding, especially\nat the onset of a word. c) The cross-entropy as a function of history\nwhen artiﬁcially limiting the number of characters available for\nprediction. This corresponds to only considering the most recent\nn of the κ, where n is the length of the history.\nnext corresponds to the decay of information across words\nand sentences. We also show the relevance of the κt\ns contri-\nbutions to the decoding of characters at different positions\nin the word (Figure 4b). For example, we observe that κt\n‘_’\nmakes important contributions to the prediction of the next\ncharacter at time t. We show that using only the κt\n‘_’, the\nmodel can achieve a cross entropy of less than 1 bit / char\nwhen the position of the character is more than 3 letters from\nthe beginning of the word. Finally, we link the norm-decay\nof κt\ns to the importance of past characters for the decoding\nquality ( Figure 4c). By artiﬁcially limiting the number of\npast κ available for prediction we show that the prediction\nquality improves rapidly when extending the history from 0\nto 10 characters and then saturates. This rapid improvement\naligns with the range of faster decay in Figure 4a.\n3.4. From characters to words\nThe ISAN provides a natural means of moving from char-\nacter level representation to word level. Using the linearity\nof the hidden state dynamics we can aggregate all of the\nκt\ns belonging to a given word and visualize them as a sin-\ngle contribution to the prediction of the letters in the next\nword. This allows us to understand how each preceding\nword impacts the decoding for the letters of later words. In\nFigure 5 we show that the words ‘was’ and ‘higher’ make\nlarge contributions to the prediction of the characters in\n‘than’ as measured by the norm of theκt\n‘_was’and κt\n‘_higher’.\nInterpretable RNNs with Input Switched Afﬁne Networks\nFigure 5.The ISAN architecture can be used to precisely char-\nacterize the relationship between words and characters. The top\npanel shows how exploiting the linearity of the network’s operation\nwe can combine the κt\ns1 ..κt\nsn in a word to a single contribution,\nκt\nword, for each word. Shown is the norm of κt\nword, a measure of\nthe magnitude of the effect of the previous word on the selection\nof the current character (red corresponds to a norm of 10, blue\nto 0). The bottom panel shows the probabilities assigned by the\nnetwork to the next sequence character. Lighter lines show pre-\ndictions conditioned on a decreasing number of preceding words.\nFor example, when predicting the characters of ‘than’ there is a\nlarge contribution from both κt\n‘was’ and κt\n‘higher’, as shown in the\ntop pane. The effect on the log probabilities can be seen in the\nbottom panel as the model becomes less conﬁdent when excluding\nκt\n‘was’and signiﬁcantly less conﬁdent when excluding both κt\n‘was’\nand κt\n‘higher’. This word based representation clearly shows that the\nsystem leverages contextual information across multiple words.\n3.5. Change of basis\nWe are free to perform a change of basis on the hidden state,\nand then to run the afﬁne ISAN dynamics in that new basis.\nNote that this change of basis is not possible for other RNN\narchitectures, since the action of the nonlinearity depends\non the choice of basis.\nIn particular we can construct a ‘readout basis’ that explicitly\ndivides the latent space into a subspace Pro\n∥ spanned by\nthe rows of the readout matrix Wro, and its orthogonal\ncomplement Pro\n⊥. This representation explicitly divides\nthe hidden state dynamics into a 27-dimensional ‘readout’\nsubspace that is accessed by the readout matrix to make\npredictions, and a ‘computational’ subspace comprising the\nremaining 216 −27 dimensions that are orthogonal to the\nreadout matrix.\nWe apply this change of basis to analyze an intriguing ob-\nservation about the hidden offsets bx. As shown in Fig-\nure 6, the norm of the bx is strongly correlated to the\nlog-probability of the unigram x in the training data. Re-\nexpressing network parameters using the ‘readout basis’\nshows that this correlation is not related to reading out the\nnext-step prediction. This is because the norm of the pro-\njection of bx into Pro\n⊥ remains strongly correlated with\ncharacter frequency, while the projection into Pro\n∥ shows\nlittle correlation. This indicates that the information content\nor ’surprise’ of a letter is encoded through the norm of the\nFigure 6.By transforming the ISAN dynamics into a new basis, we\ncan better understand the action of the input-dependent biases. a)\nWe observe a strong correlation between the norms of the input de-\npendent biases, bx, and the log-probability of the unigram x in the\ntraining data. We can begin to understand this correlation structure\nusing a basis transform into the ‘readout basis’. Breaking out the\nnorm into its components in Pro\n∥ and Pro\n⊥ in b) and c) respectively,\nshows that the correlation is due to the component orthogonal to\nWro. This implies a connection between information or ‘surprise’\nand distance in the ’computational’ subspace of state space.\nFigure 7. By transforming ISAN dynamics into a new basis, we\ncan better interpret structure in the input-dependent biases. Ina) we\nshow the cosine distance between the input dependent bias vectors,\nsplit between vowels and consonants (‘ ’ is ﬁrst). Inb) we show\nthe correlation only considering the components in the subspace\nPro\n∥ spanned by the rows of the readout matrix Wro. c) shows the\ncorrelation of the components in the orthogonal complement Pro\n⊥ .\nIn all plots white corresponds to 0 (aligned) and black to 2.\ncomponent of bx in the computational space, rather than in\nthe readout space.\nSimilarly, in Figure 7 we illustrate that the structure in the\ncorrelations between the biases bx (across all x) is due to\ntheir components in Pro\n∥ , while the correlation in Pro\n⊥ is\nrelatively uniform. We can clearly see two blocks of high\ncorrelations between the vowels and consonants respectively,\nwhile b‘_’is uncorrelated to either.\n3.6. Comparison with n-gram model with back-off\nWe compared the computation performed by n-gram lan-\nguage models and those performed by the ISAN. An n-gram\nmodel with back-off weights expresses the conditional prob-\nability p (xt|x1...xt−1) as a sum of smoothed count ratios of\nn-grams of different lengths, with the contribution of shorter\nn-grams down-weighted by back-off weights. On the other\nhand, the computations performed by the ISAN start with\nthe contribution of bro to the logits, which as shown in Fig-\nInterpretable RNNs with Input Switched Afﬁne Networks\nure 8a, corresponds to the unigram log-probabilities. The\nlogits are then additively updated with contributions from\nlonger n-grams, represented by κt\ns. This additive contribu-\ntion to the logits corresponds to a multiplicative modiﬁca-\ntion of the emission probabilities from histories of different\nlength. For long time lags, the additive correction to log-\nprobabilities becomes small (Figure 2), which corresponds\nto multiplication by a uniform distribution. Despite these\ndifferences in how n-gram history is incorporated, we nev-\nertheless observe an agreement between empirical models\nestimated on the training set and model predictions for uni-\ngrams and bigrams. Figure 8 shows that the bias term bro\ngives the unigram probabilities of letters, while the addition\nof the offset terms bx accurately predict the bigram distri-\nbution of P (xt+1|xt). Shown in panel b is an example,\nP (x|‘_′), and in panel c, a summary plot for all 27 letters.\nWe further explore the n-gram comparison by artiﬁcially\nlimiting the length of the character history that is available\nto the ISAN for making predictions, as shown in Figure 4c).\n4. Analyses of a parentheses counting task\nTo show the possibility of complete interpretability of the\nISAN we train a model on a parenthesis counting task.\nBringing together ideas from section 3.5 we re-express the\ntransition dynamics in a new basis that fully reveals per-\nformed computations.\nWe analyze the task of counting the nesting levels of multi-\nple parentheses types, a simpliﬁed version of a task deﬁned\nin (Collins et al., 2016). Brieﬂy, a 35-unit ISAN is required\nto keep track of the nesting level of 2 different types of\nparentheses independently. The inputs are the one-hot en-\ncoding of the different opening and closing parentheses (e.g.\n‘(’, ‘)’, ‘[’, ‘]’) as well as a noise character (‘a’). The output\nis the one-hot encoding of the nesting level between (0-5),\none set of counts for each parenthesis type (so the complete\noutput vector is a 12 dimensional 2-hot vector). Further-\nmore, the target output is the nesting level at the previous\ntime step. This artiﬁcial delay requires the model to develop\na memory. One change from (Collins et al., 2016) is that\nwe exchange the cross-entropy error with an L2 error. This\nleads to slightly cleaner ﬁgures, but does not qualitatively\nchange the results.\nTo elucidate the mechanism of ISAN’s operation we ﬁrst re-\nexpress the afﬁne transitions ht+1 = Wht + b by their lin-\near equivalents h′\nt+1 = W′h′\nt, where W′= [W b; 0T 1]\nand h′\nt = [ht; 1]. Next, we used linear regression to ﬁnd a\nchange of basis for which all augmented character matrices\nand the hidden states are sparse. To do this we construct\nthe ‘readout’ (Pro\n∥ ) and ‘computational’ ( Pro\n⊥) subspace\ndecomposition as discussed in Section 3.5. We choose a\nbasis for Pro\n⊥ which makes the projections of the hidden\nFigure 8. The predictions of ISAN for one and two characters well\napproximate the predictions of unigram and bigram models. In a)\nwe compare softmax(bro) to the empirical unigram distribution\nP(x). In b) we compare softmax(Wrob‘_’+ bro) with the em-\npirical distribution P(xt+1|‘_’). In c) we show the correlation of\nsoftmax(Wrobx + bro) with P(xt+1|xt) for all 27 characters\n(y-axis), and compare this to the correlation between the empiri-\ncal unigram probabilities P(x) to P(xt+1|xt) (x-axis). The plot\nshows that the readout of the bias vector is a better predictor of the\nconditional distribution than the unigram probability.\nstates into this computational subspace 2-hot vectors. With\nthis subspace decomposition, the hidden states and character\nmatrices have the form\nW′\nx =\n\n\nWrr\nx Wrc\nx br\nx\nWcr\nx Wcc\nx bc\nx\n0T 0T 1\n\n h′\nt =\n\n\nhr\nt\nhc\nt\n1\n\n (7)\nand the update equation can be written as\nh′\nt+1 = W′\nxh′\nt =\n\n\nWrr\nx hr\nt + Wrc\nx hc\nt + br\nx\nWcr\nx hr\nt + Wcc\nx hc\nt + bc\nx\n1\n\n. (8)\nHere hr\nt and hc\nt denote the readout and computational por-\ntions of ht, and Wrr\nx , Wcr\nx , Wrc\nx , Wcc\nx denote the readout\nto readout, readout to computation, computation to readout,\nand computation to computation blocks of the character\nmatrix for character x, respectively.\nIn Figure 9d we show the hidden states in the rotated basis as\na sequence of column vectors. The 35 dimensional hidden\nstates are all 4-hot. We can treat them as a concatenation of a\nreadout hr\nt and a computation hc\nt part. The 12-dimensional\nreadout hr\nt corresponds to network’s output at time step\nt and encodes the counts from time step t −1 as a 2-hot\nvector (one count per parenthesis type). The computational\nspace hc\nt is 35 −12 = 23dimensional, and encodes the\ncurrent counts as another 2-hot vector. Note that in this\nbasis the ISAN effectively uses only 24 dimensions and the\nremaining 11 dimensions have no noticeable effect on the\ncomputation. In Figure 9c we show W′\n[ in the rotated basis.\nWe see from the leftmost 12 columns thatWrr\n[ and Wcr\n[ are\nboth nearly 0. This means that hr\nt has no inﬂuence on ht+1.\nFurthermore, the computation to readout block, Wrc\n[ , is\nInterpretable RNNs with Input Switched Afﬁne Networks\nidentity on the ﬁrst 12 dimensions, effectively implementing\nthe lagging output hr\nt = hc\nt−1. The current counts are\nimplemented as delay lines and identity sub-matrices in\nWcc\n[ , which respectively has the effect of incrementing the\ncount of ‘[’ by one, saturating at 5, and leaving the count of\n’()’ parentheses ﬁxed. The matrices W], W(, W) behave\nanalogously. It is clear that this solution is general, in that\nretraining for increased numbers of parentheses types or an\nincreased counting maximum, would have the analogous\nsolution.\n5. Discussion\nIn this paper we motivated an input-switched afﬁne recurrent\nnetwork for the purpose of interpretability. We showed that a\nswitched afﬁne architecture achieves the same performance\nas LSTMs on the Text8 dataset for the same number of\nmaximum parameters, and reasonable performance on the\nBWB. We performed a series of analyses, demonstrating\nthe ability to understand how inputs at one point in the input\nsequence affect the outputs later in the output sequence. We\nshowed further in the multiple parentheses counting task that\nthe ISAN dynamics can be completely reverse engineered.\nIn summary, this work provides evidence that the ISAN is\nable to express complex dynamical systems, yet its operation\ncan in principle be fully understood, a prospect that remains\nout of reach for many popular recurrent architectures.\n5.1. Computational beneﬁts\nSwitched afﬁne networks hold the potential to be massively\nmore computationally and memory efﬁcient for text process-\ning than other recurrent architectures. First, input-dependent\nafﬁne transitions reduce the number of parameters used at\nevery step. For K possible inputs and N parameters, the\ncomputational cost per update step is O\n(N\nK\n)\n, a factor of\nK speedup over non-switched architectures. Similarly, the\nnumber of hidden units is O\n(√\nN\nK\n)\n, a factor of K\n1\n2 mem-\nory improvement for storage of the latent state.\nFurthermore, the ISAN is unique in its ability to pre-\ncompute afﬁne transformations corresponding to input\nstrings. This is possible because the composition of afﬁne\ntransformations is also an afﬁne transformation. This prop-\nerty is used in Section 3.4 to evaluate the linear contributions\nof words, rather than characters. This means that the hidden\nstate update corresponding to an entire input sequence can\nbe computed with identical cost to the update for a single\ncharacter (plus the dictionary look-up cost for the com-\nposed transformation). ISAN can therefore achieve very\nlarge speedups on input processing, at the cost of increased\nmemory use, by accumulating large look-up tables of the\nWx and bx corresponding to common input sequences. Of\ncourse, practical implementations will have to incorporate\n0 5 10 15 20 25 30\n \na)\n0\n5\n10\n15\n20\n25\n30\nTransition matrix for '[' in original basis\n( ( ( ( ( ( ( ( ( [ [ [ [ [ [ ) ) )aaaaaa] ] ] ] ] ] ] ] ] ( ) ( ) ( ) (\nb)\n0\n5\n10\n15\n20\n25\n30 Hidden dimension\nHidden states in the original basis\n0 5 10 15 20 25 30\n \nc)\n0\n5\n10\n15\n20\n25\n30\nTransition matrix for '[' in rotated basis\n( ( ( ( ( ( ( ( ( [ [ [ [ [ [ ) ) )aaaaaa] ] ] ] ] ] ] ] ] ( ) ( ) ( ) (\nd)\n0\n5\n10\n15\n20\n25\n30 Hidden dimension\nHidden states in rotated basis\nFigure 9.A visualization of the dynamics of an ISAN for the two\nparentheses counting task with 1 time lag (count either ’()’ or ’[]’\nnesting levels with a one-step readout delay). In a) the weight\nmatrix for ‘[’ is shown in the original basis. Inc) it is shown trans-\nformed to highlight the delay-line dynamics. The activations of the\nhidden units are shownb) in the original basis, andd) rotated to the\nsame basis as in c), to highlight the delay-line dynamics in a more\nintelligible way. The white line delineates the transition matrix\nelements and hidden state dimensions that directly contribute to\nthe output. All matrices for parentheses types appear similarly,\nwith closing parentheses, e.g. ‘]’, changing the direction of the\ndelay line.\ncomplexities of memory management, batching, etc.\n5.2. Future work\nThere are some obvious future directions to this work. Cur-\nrently, we deﬁne switching behavior using an input set with\nﬁnite and manageable cardinality. Studying word-level lan-\nguage models with enormous vocabularies may require\nsome additional logic to scale. Another idea is to build\na language model that switches on bigrams or trigrams,\nrather than characters or words, targeting an intermediate\nnumber of afﬁne transformations. Adapting this model\nto continuous-valued inputs is another important direction.\nOne approach is to use a tensor factorization similar to that\nemployed by the MRNN (Sutskever et al., 2014) or deﬁning\nweights via additional networks, as in HyperNetworks (Ha\net al., 2016). Finally, we expect that automated methods\nfor changing bases to enable sparse representations in the\nhidden state and dynamics matrices will be a particularly\nfruitful direction to pursue.\nInterpretable RNNs with Input Switched Afﬁne Networks\nAcknowledgements\nWe would like to thank Jasmine Collins for her help and\nadvice, and Quoc Le, David Ha and Mohammad Norouzi\nfor helpful discussions. We would also like to thank Herbert\nJaeger for insightful discussions regarding the Observable-\nOperator-Model.\nReferences\nAlain, Guillaume and Bengio, Yoshua. Understanding in-\ntermediate layers using linear classiﬁer probes. arXiv\npreprint arXiv:1610.01644, 2016.\nAndrew, Alex M. An introduction to support vector ma-\nchines and other kernel-based learning methods. Kyber-\nnetes, 2013.\nBelanger, David and Kakade, Sham. A linear dynamical\nsystem model for text. In Proceedings of the 32nd Inter-\nnational Conference on Machine Learning (ICML-15) ,\npp. 833–842, 2015.\nBerk, Richard, Heidari, Hoda, Jabbari, Shahin, Kearns,\nMichael, and Roth, Aaron. Fairness in criminal justice\nrisk assessments: The state of the art. arXiv preprint\narXiv:1703.09207, 2017.\nBojarski, Mariusz, Del Testa, Davide, Dworakowski, Daniel,\nFirner, Bernhard, Flepp, Beat, Goyal, Prasoon, Jackel,\nLawrence D, Monfort, Mathew, Muller, Urs, Zhang, Ji-\nakai, et al. End to end learning for self-driving cars.arXiv\npreprint arXiv:1604.07316, 2016.\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T.,\nKoehn, P., and Robinson, T. One Billion Word Bench-\nmark for Measuring Progress in Statistical Language\nModeling. ArXiv e-prints, December 2013.\nChing, Travers, Himmelstein, Daniel S, Beaulieu-Jones,\nBrett K, Kalinin, Alexandr A, Do, Brian T, Way, Gre-\ngory P, Ferrero, Enrico, Agapow, Paul-Michael, Xie, Wei,\nRosen, Gail L, et al. Opportunities and obstacles for deep\nlearning in biology and medicine. bioRxiv, pp. 142760,\n2017.\nCollins, Jasmine, Sohl-Dickstein, Jascha, and Sussillo,\nDavid. Capacity and trainability in recurrent neural net-\nworks. ICLR 2017 submission, 2016.\nCouncil of European Union. General Data Protection\nRegulation, Article 22 (Regulation (EU) 2016/679),\n2016. URL http://www.privacy-regulation.\neu/en/22.htm.\nDeo, Rahul C. Machine learning in medicine. Circulation,\n132(20):1920–1930, 2015.\nFreedman, David A. Statistical models: theory and practice.\ncambridge university press, 2009.\nGulshan, Varun, Peng, Lily, Coram, Marc, Stumpe, Mar-\ntin C, Wu, Derek, Narayanaswamy, Arunachalam, Venu-\ngopalan, Subhashini, Widner, Kasumi, Madams, Tom,\nCuadros, Jorge, et al. Development and validation of a\ndeep learning algorithm for detection of diabetic retinopa-\nthy in retinal fundus photographs. JAMA, 316(22):2402–\n2410, 2016.\nHa, David, Dai, Andrew, and Le, Quoc V . Hypernetworks.\narXiv preprint arXiv:1609.09106, 2016.\nHochreiter, Sepp and Schmidhuber, Jürgen. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997.\nHwang, Kyuyeon and Sung, Wonyong. Character-level\nlanguage modeling with hierarchical recurrent neural\nnetworks. CoRR, abs/1609.03777, 2016. URL http:\n//arxiv.org/abs/1609.03777.\nJaeger, Herbert. Observable operator models for discrete\nstochastic time series. Neural Computation, 12(6):1371–\n1398, 2000.\nJózefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer,\nNoam, and Wu, Yonghui. Exploring the limits of lan-\nguage modeling. CoRR, abs/1602.02410, 2016. URL\nhttp://arxiv.org/abs/1602.02410.\nKarpathy, Andrej, Johnson, Justin, and Li, Fei-Fei. Visualiz-\ning and understanding recurrent networks. arXiv preprint\narXiv:1506.02078, 2015.\nKatz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and\nKochenderfer, Mykel. Reluplex: An efﬁcient smt solver\nfor verifying deep neural networks. arXiv preprint\narXiv:1702.01135, 2017.\nLe, Quoc V , Ranzato, Marc A., Monga, Rajat, Devin,\nMatthieu, Chen, Kai, Corrado, Greg S., Dean, J, and\nNg, Andrew Y . Building high-level features using large\nscale unsupervised learning. In International Conference\non Machine Learning, 2012.\nLinderman, Scott W, Miller, Andrew C, Adams, Ryan P,\nBlei, David M, Paninski, Liam, and Johnson, Matthew J.\nRecurrent switching linear dynamical systems. arXiv\npreprint arXiv:1610.08466, 2016.\nMahoney, Matt. Large text compression benchmark: About\nthe test data, 2011. URL http://mattmahoney.\nnet/dc/textdata. [Online; accessed 15-November-\n2016].\nMartens, James and Sutskever, Ilya. Learning recurrent neu-\nral networks with hessian-free optimization. In Proceed-\nings of the 28th International Conference on Machine\nLearning (ICML-11), pp. 1033–1040, 2011.\nInterpretable RNNs with Input Switched Afﬁne Networks\nMikolov, Tomáš, Sutskever, Ilya, Deoras, Anoop, Le, Hai-\nSon, and Kombrink, Stefan. Subword language modeling\nwith neural networks. preprint, 2012.\nMordvintsev, Alexander, Olah, Christopher, and Tyka, Mike.\nInceptionism: Going deeper into neural networks. Google\nResearch Blog. Retrieved June, 20:14, 2015.\nMurdoch, W. James and Szlam, Arthur. Automatic rule\nextraction from long short term memory networks. In\nICLR, 2017.\nQuinlan, J. Ross. Simplifying decision trees. International\njournal of man-machine studies, 27(3):221–234, 1987.\nScarborough, David and Somers, Mark John. Neural net-\nworks in organizational research: Applying pattern recog-\nnition to the analysis of organizational behavior. Ameri-\ncan Psychological Association, 2006.\nSiano, Pierluigi, Cecati, Carlo, Yu, Hao, and Kolbusz,\nJanusz. Real time operation of smart grids via fcn net-\nworks and optimal power ﬂow. IEEE Transactions on\nIndustrial Informatics, 8(4):944–952, 2012.\nSussillo, David and Barak, Omri. Opening the black box:\nlow-dimensional dynamics in high-dimensional recurrent\nneural networks. Neural computation, 25(3):626–649,\n2013.\nSutskever, Ilya, Martens, James, and Hinton, Geoffrey E.\nGenerating text with recurrent neural networks. In Pro-\nceedings of the 28th International Conference on Ma-\nchine Learning (ICML-11), pp. 1017–1024, 2011.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V . Sequence\nto sequence learning with neural networks. In Advances\nin neural information processing systems, pp. 3104–3112,\n2014.\nTashea, Jason. Courts are using ai to sentence criminals.\nthat must stop now. WIRED magazine, 2017.\nZeiler, Matthew D, Krishnan, Dilip, Taylor, Graham W, and\nFergus, Rob. Deconvolutional networks. In Computer\nVision and Pattern Recognition (CVPR), 2010 IEEE Con-\nference on, pp. 2528–2535. IEEE, 2010.",
  "topic": "Affine transformation",
  "concepts": [
    {
      "name": "Affine transformation",
      "score": 0.8644788861274719
    },
    {
      "name": "Computer science",
      "score": 0.8153800964355469
    },
    {
      "name": "Nonlinear system",
      "score": 0.5148336291313171
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5075575113296509
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5062176585197449
    },
    {
      "name": "Architecture",
      "score": 0.5050225853919983
    },
    {
      "name": "Word (group theory)",
      "score": 0.4779927134513855
    },
    {
      "name": "Speedup",
      "score": 0.47721582651138306
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4452395439147949
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4353824257850647
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3966904282569885
    },
    {
      "name": "Algorithm",
      "score": 0.34826862812042236
    },
    {
      "name": "Artificial neural network",
      "score": 0.2927757501602173
    },
    {
      "name": "Mathematics",
      "score": 0.14331552386283875
    },
    {
      "name": "Programming language",
      "score": 0.09674826264381409
    },
    {
      "name": "Parallel computing",
      "score": 0.08699548244476318
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}