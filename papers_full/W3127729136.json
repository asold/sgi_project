{
    "title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models",
    "url": "https://openalex.org/W3127729136",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2027506410",
            "name": "Nora Kassner",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2784500630",
            "name": "Philipp Dufter",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2035156685",
            "name": "Hinrich Schütze",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3022851392",
        "https://openalex.org/W3045958725",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2994915912",
        "https://openalex.org/W3079786700",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W3045462440",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W3005441132",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W3039017601",
        "https://openalex.org/W2964207259"
    ],
    "abstract": "Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as “Paris is the capital of [MASK]” are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT’s performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.",
    "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3250–3258\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n3250\nMultilingual LAMA: Investigating Knowledge in Multilingual Pretrained\nLanguage Models\nNora Kassner∗, Philipp Dufter∗, Hinrich Sch¨utze\nCenter for Information and Language Processing (CIS), LMU Munich, Germany\n{kassner,philipp}@cis.lmu.de\nAbstract\nRecently, it has been found that monolin-\ngual English language models can be used as\nknowledge bases. Instead of structural knowl-\nedge base queries, masked sentences such as\n“Paris is the capital of [MASK]” are used as\nprobes. We translate the established bench-\nmarks TREx and GoogleRE into 53 languages.\nWorking with mBERT, we investigate three\nquestions. (i) Can mBERT be used as a multi-\nlingual knowledge base? Most prior work only\nconsiders English. Extending research to mul-\ntiple languages is important for diversity and\naccessibility. (ii) Is mBERT’s performance\nas knowledge base language-independent or\ndoes it vary from language to language? (iii)\nA multilingual model is trained on more text,\ne.g., mBERT is trained on 104 Wikipedias.\nCan mBERT leverage this for better perfor-\nmance? We ﬁnd that using mBERT as a knowl-\nedge base yields varying performance across\nlanguages and pooling predictions across lan-\nguages improves performance. Conversely,\nmBERT exhibits a language bias; e.g., when\nqueried in Italian, it tends to predict Italy as\nthe country of origin.\n1 Introduction\nPretrained language models (LMs) (Peters et al.,\n2018; Howard and Ruder, 2018; Devlin et al., 2019)\ncan be ﬁnetuned to a variety of natural language\nprocessing (NLP) tasks and generally yield high\nperformance. Increasingly, these models and their\ngenerative variants are used to solve tasks by sim-\nple text generation, without any ﬁnetuning (Brown\net al., 2020). This motivated research on how\nmuch knowledge is contained in LMs: Petroni et al.\n(2019) used models pretrained with masked lan-\nguage to answer ﬁll-in-the-blank templates such as\n“Paris is the capital of [MASK].”\n∗ Equal contribution - random order.\nQuery Two most frequent predictions\nen X was created in MASK. [Japan (170), Italy (56), . . . ]\nde X wurde in MASK erstellt. [Deutschland (217), Japan (70), . . . ]\nit X `e stato creato in MASK. [Italia (167), Giappone (92), . . . ]\nnl X is gemaakt in MASK. [Nederland (172), Itali ¨e (50), . . . ]\nen X has the position of MASK. [bishop (468), God (68), ...]\nde X hat die Position MASK. [WW (261), Ratsherr (108), ...]\nit X ha la posizione di MASK. [pastore ( 289), papa (138), ...]\nnl X heeft de positie van MASK. [burgemeester (400), bisschop (276) , ...]\nTable 1: Language bias when querying (TyQ) mBERT.\nTop: For an Italian cloze question, Italy is favored as\ncountry of origin. Bottom: There is no overlap be-\ntween the top-ranked predictions, demonstrating the in-\nﬂuence of language – even though the facts are the\nsame: the same set of triples is evaluated across lan-\nguages. Table 3 shows that pooling predictions across\nlanguages addresses bias and improves performance.\nWW = “Wirtschaftswissenschaftler”.\nThis research so far has been exclusively on En-\nglish. In this paper, we focus on using multilingual\npretrained LMs as knowledge bases. Working with\nmBERT, we investigate three questions. (i) Can\nmBERT be used as a multilingual knowledge base?\nMost prior work only considers English. Extend-\ning research to multiple languages is important for\ndiversity and accessibility. (ii) Is mBERT’s perfor-\nmance as knowledge base language-independent or\ndoes it vary from language to language? To answer\nthese questions, we translate English datasets and\nanalyze mBERT for 53 languages. (iii) A multilin-\ngual model is trained on more text, e.g., BERT’s\ntraining data contains the English Wikipedia, but\nmBERT is trained on 104 Wikipedias. Can mBERT\nleverage this fact? Indeed, we show that pooling\nacross languages helps performance.\nIn summary our contributions are: i) We auto-\nmatically create a multilingual version of TREx\nand GoogleRE covering 53 languages. ii) We use\nan alternative to ﬁll-in-the-blank querying – rank-\ning entities of the type required by the template\n(e.g., cities) – and show that it is a better tool\n3251\nto investigate knowledge captured by pretrained\nLMs. iii) We show that mBERT answers queries\nacross languages with varying performance: it\nworks reasonably for 21 and worse for 32 lan-\nguages. iv) We give evidence that the query lan-\nguage affects results: a query formulated in Italian\nis more likely to produce Italian entities (see Ta-\nble 1). v) Pooling predictions across languages\nimproves performance by large margins and even\noutperforms monolingual English BERT. Code and\ndata are available online ( https://github.com/\nnorakassner/mlama).\n2 Data\n2.1 LAMA\nWe follow the LAMA setup introduced by Petroni\net al. (2019). More speciﬁcally, we use data from\nTREx (Elsahar et al., 2018) and GoogleRE.1 Both\nconsist of triples of the form (object, relation, sub-\nject). The underlying idea of LAMA is to query\nknowledge from pretrained LMs using templates\nwithout any ﬁnetuning: the triple (Paris, capital-of,\nFrance) is queried with the template “Paris is the\ncapital of [MASK].” In LAMA, TREx has 34,039\ntriples across 41 relations, GoogleRE 5528 triples\nand 3 relations. Templates for each relation have\nbeen manually created by Petroni et al. (2019). We\ncall all triples from TREx and GoogleRE together\nLAMA.\nLAMA has been found to contain many “easy-\nto-guess” triples; e.g., it is easy to guess that a\nperson with an Italian sounding name is born in\nItaly. LAMA-UHN is a subset of triples that are\nhard to guess introduced by Poerner et al. (2020).\n2.2 Translation\nWe translate both entities and templates. We use\nGoogle Translate to translate templates in the form\n“[X] is the capital of [Y]”. After translation, all\ntemplates were checked for validity (i.e., whether\nthey contain “[X]”, “[Y]” exactly once) and cor-\nrected if necessary. In addition, German, Hindi and\nJapanese templates were checked by native speak-\ners to assess translation quality (see Table 2). To\ntranslate the entity names, we used Wikidata and\nGoogle knowledge graphs.\nmBERT covers 104 languages. Google Translate\ncovers 77 of these. Wikidata and Google Knowl-\nedge Graph do not provide entity translations for all\n1code.google.com/archive/p/\nrelation-extraction-corpus/\nFigure 1: x-axis is the number of translated triples, y-\naxis the number of languages. There are 39,567 triples\nin the original LAMA (TREx and GoogleRE).\nlanguages and not all entities are contained in the\nknowledge graphs. For English we can ﬁnd a total\nof 37,498 triples which we use from now on. On\naverage, 34% of triples could be translated (macro\naverage over languages). We only consider lan-\nguages with a coverage above 20%, resulting in the\nﬁnal number of languages we include in our study:\n53. The macro average of translated triples in these\n53 languages is 43%. Figure 1 gives statistics. We\ncall the translated dataset mLAMA.\n3 Experiments\n3.1 Model\nWe work with mBERT (Devlin et al., 2019), a\nmodel pretrained on the 104 largest Wikipedias.\nWe denote mBERT queried in language x as\nmBERT[x]. As comparison we use the English\nBERT-Base model and refer to it as BERT. In initial\nexperiments with XLM-R (Conneau et al., 2020)\nwe observed worse performance, similar to Jiang\net al. (2020a). Thus, for simplicity we only report\nresults on mBERT.\n3.2 Typed and Untyped Querying\nPetroni et al. (2019) use templates like “Paris is the\ncapital of [MASK]” and give arg maxw∈Vp(w|t)\nas answer where V is the vocabulary of the LM\nand p(w|t) is the (log-)probability that word w\ngets predicted in the template t. Thus the object\nof a triple must be contained in the vocabulary of\nthe language model. This has two drawbacks: it\nreduces the number of triples that can be considered\ndrastically and hinders performance comparisons\nacross LMs with different vocabularies. We refer\nto this procedure as UnTyQ.\nWe propose to use typed querying,TyQ: for each\nrelation a candidate set Cis created and the pre-\ndiction becomes arg maxc∈Cp(c|t). For templates\nlike “[X] was born in [MASK]”, we know which\nentity type to expect, in this case cities. We ob-\nserved that (English-only) BERT-base predicts city\n3252\nnames for MASK whereas mBERT predicts years\nfor the same template. TyQ prevents this.\nWe choose as Cthe set of objects across all\ntriples for a single relation. The candidate set could\nalso be obtained from an entity typing system (e.g.,\n(Yaghoobzadeh and Sch¨utze, 2016)), but this is be-\nyond the scope of this paper. Variants of TyQ have\nbeen used before (Xiong et al., 2020).\n3.3 Singletoken vs. Multitoken Objects\nAssuming that objects are in the vocabulary\n(Petroni et al., 2019) is a restrictive assumption,\neven more in the multilingual case as e.g., “Ham-\nburg” is in the mBERT vocabulary, but French\n“Hambourg” is tokenized to [“Ham”, “##bourg”].\nWe consider multitoken objects by including multi-\nple [MASK] tokens in the templates. For both TyQ\nand UnTyQ we compute the score that a multitoken\nobject is predicted by taking the average of the log\nprobabilities for its individual tokens.\nGiven a template t(e.g., “[X] was born in [Y].”)\nlet t1 be the template with one mask token, (i.e.,\n“[X] was born in [MASK].”) andtk be the template\nwith kmask tokens (i.e., “[X] was born in [MASK]\n[MASK] . . . [MASK].”). We denote the log proba-\nbility that the token w∈V is predicted at ith mask\ntoken as p(mi = w|tk), where V is the vocabulary\nof the LM. To compute p(e|t) for an entity ethat\nis tokenized into ltokens ϵ1,ϵ2,...,ϵ l we simply\naverage the log probabilities across tokens:\np(e|t) =1\nl\nl∑\ni=1\np(mi = ϵi|tl).\nIf kis the maximum number of tokens of any entity\ne ∈ Cgets split into, we consider all templates\nt1, . . . , tk, with C being the candidate set. The\nprediction is then the word with the highest average\nlog probability across all templates t1, . . . ,tk.\nNote that for UnTyQ the space of possible pre-\ndictions is V ×V ×···× V whereas for TyQ it is\nthe candidate set C.\n3.4 Evaluation\nWe compute precision at one for each relation, i.e.,\n1/|T|∑\nt∈T 1{ˆtobject = tobject}where T is the\nset of all triples and ˆtobject is the object predicted\nby TyQ or UnTyQ. Note that T is different for\neach language. Our ﬁnal measure (p1) is then the\nprecision at one averaged over relations (i.e., macro\naverage). Results for multiple languages are the\nmacro average p1 across languages.\nuntyped typed\nsingle\n0.1\n0.2\n0.3\n0.4p1\nuntyped typed\nmulti\n0.0\n0.1\n0.2p1\nFigure 2: Distribution of p1 scores for 53 languages in\nUnTyQ vs. TyQ. Left: singletoken (object = 1 token).\nRight: multitoken (object >1 token).\n4 Results and Discussion\nWe ﬁrst investigate TyQ and UnTyQ and ﬁnd that\nTyQ is better suited for investigating knowledge\nin LMs. After exploring the translation quality,\nwe use TyQ on mLAMA and observe rather sta-\nble performance for 21 and poor performance for\n32 languages. When investigating the languages\nmore closely, we ﬁnd that prediction results highly\ndepend on the language. Finally, we validate our\ninitial hypothesis that mBERT can leverage its mul-\ntilinguality by pooling predictions: pooling indeed\nperforms better.\n4.1 UnTyQ vs. TyQ\nFigure 2 shows the distribution of p1 scores for\nsingle and multitoken objects. As expected, TyQ\nworks better, both for single and multitoken ob-\njects. With UnTyQ, performance not only depends\non the model’s knowledge, but on at least three\nextraneous factors: (i) Does the model understand\nthe type constraints of the template (e.g., in “X is\nthe capital of Y”, Y must be a country)? (ii) How\n“ﬂuent” a substitution is an object under linguistic\nconstraints (e.g., morphology) that can be viewed\nas orthogonal to knowledge? Many English tem-\nplates cannot be translated into a single template\nin many languages, e.g., “in X” (with X a country)\nhas different translations in French: “ `a Chypre”,\n“au Mexique”, “en Inde”. But the LAMA setup\nrequires a single template. By enforcing the type,\nwe reduce the number of errors that are due to sur-\nface ﬂuency. (iii) The inadequacy of the original\nLAMA setup for multitoken answers. Figure 2\n(right) shows that the original UnTyQ struggles\nwith multitokens (mean p1 .03 vs. .17 for TyQ).\nOverall, TyQ allows us to focus the evaluation\non the core question: what knowledge is contained\nin LMs? From now on, we report numbers in the\nTyQ setting.\nManual template tuning or automatic template\n3253\nmachine manually manually\ntranslated corrected paraphrased\nde 18.1 19.4 (6) 20.9 (18)\nhi 5.4 6.2 (14) 6.2 (1)\nja 0.4 0.4 (14) 0.7 (5)\nTable 2: Effect of manual template modiﬁcation on Un-\nTyQ. Shown is p1, number of templates modiﬁed (in\nbrackets). Templates are modiﬁed to correct mistakes\nfrom machine translation and paraphrased to achieve\nthe correct object type. Manual template correction has\na small effect on UnTyQ.\nmining (Jiang et al., 2020b) has been investigated\nin the literature to approach the typing problem.\nWe had native speakers check templates for Ger-\nman, Hindi and Japanese, correct mistakes in the\nautomatic translation and paraphrase the template\nto obtain predictions with the correct type. Table 2\nshows that corrections do not yield strong improve-\nments. We conclude that template modiﬁcations\nare not an effective solution for the typing problem.\n4.2 Translation Quality\nContemporaneous work by Jiang et al. (2020a) pro-\nvides manual translations of LAMA templates for\n23 languages respecting grammatical gender and\ninﬂection constraints. We evaluate our machine\ntranslated templates by comparing performance on\na common subset of 14 languages using TyQ query-\ning on the TREx subset. Surprisingly, we ﬁnd a per-\nformance difference of 1 percentage points (0.23\nvs. 0.24, p1 averaged over languages) in favor of\nthe machine translated templates. This indicates\nthat the machine translated templates in combina-\ntion with TyQ exhibit comparable performance but\ncome with the beneﬁt of larger language coverage\n(53 vs. 23 languages).\n4.3 Multilingual Performance\nIn mLAMA, not all triples are available in all lan-\nguages. Thus absolute numbers are not compara-\nble across languages and we adopt a relative per-\nformance comparison: we report p1 of a model-\nlanguage combination divided by p1 of mBERT’s\nperformance in English (mBERT[en]) on the ex-\nact same set of triples and call this rel-p1. A rel-\np1 score of 0.5 for mBERT[ﬁ] means that p1 of\nmBERT on Finnish is half of mBERT[en]’s per-\nformance on the same triples. rel-p1 of English\nBERT is usually greater than 1 as monolingual\nBERT tends to outperform mBERT[en].\nFigure 3 shows that mBERT performs reason-\nably well for 21 languages, but for 32 languages\nLAMA LAMA-UHN\nBERT 38.5 29.0\nmBERT[en] 35.0 25.7\nmBERT[pooled] 41.1 32.1\nTable 3: p1 for BERT, mBERT queried in English,\nmBERT pooled on LAMA and LAMA-UHN.\nrel-p1 is less than 0.6 (i.e., their p1 is 60% of En-\nglish’s p1). We conclude that mBERT does not\nexhibit a stable performance across languages. The\nvariable performance (from 20% to almost 100%\nrel-p1) indicates that mBERT has no common rep-\nresentation for, say, “Paris” across languages, i.e.,\nmBERT representations are language-dependent.\n4.4 Bias\nIf mBERT captured knowledge independent of lan-\nguage, we should get similar answers across lan-\nguages for the same relation. However, Table 1\nshows that mBERT exhibits language-speciﬁc bi-\nases; e.g., when queried in Italian, it tends to predict\nItaly as the country of origin. This effect occurs\nfor several relations: Table 4 in the supplementary\npresents data for ten relations and four languages.\n4.5 Pooling\nWe investigate pooling of predictions across lan-\nguages by picking the object predicted by the ma-\njority of languages. Table 3 shows that pooled\nmBERT outperforms mBERT[en] by 6 percent-\nage points on LAMA, presumably in part be-\ncause the language-speciﬁc bias is eliminated.\nmBERT[pooled] even outperforms BERT by 3 per-\ncentage points on LAMA-UHN. This indicates that\nmBERT can leverage the fact that it is trained on\n104 Wikipedias vs. just one and even outperforms\nthe much stronger model BERT.\n5 Related Work\nPetroni et al. (2019) ﬁrst asked the question: can\npretrained LMs function as knowledge bases? Sub-\nsequent analyses focused on different aspects, such\nas negation (Kassner and Sch¨utze, 2020), easy to\nguess names (Poerner et al., 2020), integrating\nadapters (Wang et al., 2020) or ﬁnding alterna-\ntives to a “ﬁll-in-the-blank” approach with single-\ntoken answers (Bouraoui et al., 2020; Heinzerling\nand Inui, 2020; Jiang et al., 2020b). Other work\ncombines pretrained LM with information retrieval\n(Guu et al., 2020; Lewis et al., 2020a; Izacard and\nGrave, 2020; Kassner and Sch¨utze, 2020; Petroni\n3254\nen\nid\nms\naf\ngl\nvi\nda\nes\nca\nceb\nro\nsv\nit\nnl\ntr\ncy\nhr\nde\nfr\npt\nsq\nsl\nur\nsk\nbg\nfa\nzh\ncs\net\nhi\nel\neu\nla\npl\nlt\nfi\nsr\nbe\nuk\nlv\nbn\nga\nhu\nru\naz\nhe\nar\nko\nhy\nka\nta\nth\nja\nlanguage\n0.0\n0.5\n1.0rel-p1\nFigure 3: p1 of BERT (red) vs mBERT[x] (blue) divided by p1 of mBERT[en] on the same set of triples in\neach language x. mBERT captures less factual knowledge than monolingual English BERT. While performance is\nreasonable for 21 languages, it is below 60% for 32 languages. Dashed line is rel-p1 of mBERT[en] (by deﬁnition\nequal to 1.0). Performance of BERT varies slightly as the set of triples is different for each language. Note that the\nWikipedia of Cebuano (ceb) consists mostly of machine translated articles.\net al., 2020). None of this work addresses lan-\nguages other than English.\nMultilingual models like mBERT (Devlin et al.,\n2019) and XLM-R (Conneau et al., 2020) perform\nwell for zero-shot crosslingual transfer (Hu et al.,\n2020). However, we are not aware of any prior\nwork that analyzed to what degree pretrained mul-\ntilingual models can be used as knowledge bases.\nThere are many multilingual question answering\ndatasets such as XQuAD (Artetxe et al., 2020),\nTiDy (Clark et al., 2020), MKQA (Longpre et al.,\n2020) and MLQA (Lewis et al., 2020b). Usually,\nmultilingual models are ﬁnetuned to solve such\ntasks. Our goal is not to improve question answer-\ning or create an alternative multilingual question\nanswering dataset, but instead to investigate which\nknowledge is contained in pretrained multilingual\nLMs without any kind of supervised ﬁnetuning.\nThere is a range of alternative multilingual\nknowledge bases that could be used for evaluation.\nThose include ConceptNet (Speer et al., 2017) or\nBabelNet (Navigli and Ponzetto, 2010). We de-\ncided to provide a translated versions of TREx and\nGoogleRE for the sake of comparability across lan-\nguages. By translating manually created templates\nand entities we can ensure comparability across\nlanguages. This is not possible for crowd-sourced\ndatabases like ConceptNet.\nIn contemporaneous work, Jiang et al. (2020a)\ncreate and investigate a multilingual version of\nLAMA. They provide human template translations\nfor 23 languages, propose several methods for mul-\ntitoken decoding and code-switching, and experi-\nment with a number of PLMs. In contrast to their\nwork, we investigate typed querying, focus on com-\nparabiliy and pooling across languages, and explore\nlanguage biases.\n6 Conclusion\nWe presented mLAMA, a dataset to investigate\nknowledge in language models (LMs) in a multi-\nlingual setting covering 53 languages. While our\nresults suggest that correct entities can be retrieved\nfor many languages, there is a clear performance\ngap between English and, e.g., Japanese and Thai.\nThis suggests that mBERT is not storing entity\nknowledge in a language-independent way. Ex-\nperiments investigating language bias conﬁrm this\nﬁnding. We hope that this paper and the dataset\nwe publish will stimulate research on investigating\nknowledge in LMs multilingually rather than just\nin English.\nAcknowledgements\nThis work was supported by the European Research\nCouncil (# 740516) and the German Federal Min-\nistry of Education and Research (BMBF) under\nGrant No. 01IS18036A. The authors of this work\ntake full responsibility for its content. The second\nauthor was supported by the Bavarian research in-\nstitute for digital transformation (bidt) through their\nfellowship program. We thank Yannick Couzini´e\nand Karan Tiwana for correcting the Japanese and\nHindi templates. We thank the anonymous review-\ners for valuable comments.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nZied Bouraoui, Jos ´e Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom BERT. In The Thirty-Fourth AAAI Conference\n3255\non Artiﬁcial Intelligence, AAAI 2020, New York, NY,\nUSA, February 7-12, 2020, pages 7456–7463. AAAI\nPress.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nJonathan H. Clark, Jennimaria Palomaki, Vitaly Niko-\nlaev, Eunsol Choi, Dan Garrette, Michael Collins,\nand Tom Kwiatkowski. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Inter-\nnational Conference on Language Resources and\nEvaluation (LREC-2018) , Miyazaki, Japan. Euro-\npean Languages Resources Association (ELRA).\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. Comput-\ning Research Repository, arXiv:2002.08909.\nBenjamin Heinzerling and Kentaro Inui. 2020. Lan-\nguage models as knowledge bases: On en-\ntity representations, storage capacity, and para-\nphrased queries. Computing Research Repository ,\narXiv:2008.09036.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-\n18 July 2020, Virtual Event, volume 119 of Proceed-\nings of Machine Learning Research , pages 4411–\n4421. PMLR.\nGautier Izacard and E. Grave. 2020. Leveraging pas-\nsage retrieval with generative models for open do-\nmain question answering. ArXiv, abs/2007.01282.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-\nFACTR: Multilingual factual knowledge retrieval\nfrom pretrained language models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 5943–\n5959, Online. Association for Computational Lin-\nguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know. Transactions of the Association for\nComputational Linguistics, 8:423–438.\nNora Kassner and Hinrich Sch ¨utze. 2020. BERT-kNN:\nAdding a kNN search component to pretrained lan-\nguage models for better QA. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing: Findings, EMNLP 2020, On-\nline Event, 16-20 November 2020, pages 3424–3430.\nAssociation for Computational Linguistics.\nNora Kassner and Hinrich Sch¨utze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. As-\nsociation for Computational Linguistics.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020a. Pre-training via paraphrasing. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020b. MLQA: Evalu-\nating cross-lingual extractive question answering. In\n3256\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nShayne Longpre, Yi Lu, and Joachim Daiber. 2020.\nMKQA: A linguistically diverse benchmark for mul-\ntilingual open domain question answering. CoRR,\nabs/2007.15207.\nRoberto Navigli and Simone Paolo Ponzetto. 2010. Ba-\nbelnet: Building a very large multilingual semantic\nnetwork. In ACL 2010, Proceedings of the 48th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, July 11-16, 2010, Uppsala, Swe-\nden, pages 216–225. The Association for Computer\nLinguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRockt¨aschel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In Automated\nKnowledge Base Construction.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2463–2473. Association for\nComputational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Sch ¨utze.\n2020. E-BERT: Efﬁcient-yet-effective entity em-\nbeddings for BERT. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 803–818, Online. Association for Computa-\ntional Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artiﬁcial Intelligence, Febru-\nary 4-9, 2017, San Francisco, California, USA ,\npages 4444–4451. AAAI Press.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. Comput-\ning Research Repository, arXiv:2002.01808.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nYadollah Yaghoobzadeh and Hinrich Sch ¨utze. 2016.\nIntrinsic subspace evaluation of word embedding\nrepresentations. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 236–\n246, Berlin, Germany. Association for Computa-\ntional Linguistics.\n3257\nA Language Bias\nTable 4 shows the language bias for 10 relations.\nFor each relation we aggregated the predictions\nacross all triples and show the most common two\npredicted entities together with its count (in brack-\nets). The querying language clearly affects results.\nThe effect is drastic for relations that ask for a coun-\ntry (e.g., P495 or P1001). P39 yields very different\nresults without exhibiting a clear pattern. Other\nrelations such as P463 or P178 are rather stable.\nB Data Samples\nTable 4 and Table 5 show randomly sampled entries\nfrom the data.\nC Pretraining Data\nWe investigate whether performance across lan-\nguages is correlated with the amount of pretraining\ndata for each language. To this end we investigate\nthe number of articles per language as of January\n2021 2 and p1 for TyQ in Figure 6. We do not have\naccess to the original pretraining data of mBERT.\nThus, the number of articles we consider in the\nanalysis might be different to the actual data used\nto train mBERT.\n2https://meta.wikimedia.org/wiki/List_\nof_Wikipedias\nαϳέΎΑϲϓϝΎϔϳέΑΕϧ΍έϭϳΩϥϭΟΕΎϣϥϭίΎϣ΃Δργ΍ϭΑέϳΎϓϝΩϧϳϛΝΎΗϧ·ϡΗϳΔϳΑέλϟ΍ϲϫϭϓϭγϭϛ˰ϟΔϳϣγέϟ΍ΔϐϠϟ΍ԥ ԥ ԥԥԥԥ ԥ ԥԥ ԥ\nćȥđĂđäåûđĘĎđëĉčđĘÿčēćđĂđĆđñïĘĉĺĀĠ@\nĺćĝĒĉþđĒĂĠđćđĒĊëĉčđĘÿčēćđĂđĆđñïĘĉĺĀĠ@\nðđþĔŪćëąáïđĠĘĉđĎĊĀĔǅċĎĉ@\nΕγ΍ϩΩη̵έ΍Ϋ̴ϣΎϧ̟έϭΗϧ΁ϪΑ̟έϭΗϧ΁ϥΎΗγ΍Ωϭη̶ϣΩϳϟϭΗ̶̯ϭίϭγργϭΗ̶η΍ίϳ̶̯̯ϭίϭγΕγ΍έ΍ΩϣΗγΎϳγαϼ̯έϳίέϭϬϣΟγϳ΋έ\n௙௙\n௙௙\nʬʨʠʩʱʡʠʶʮʰʯʥʨʢʰʩʹʥʥʺʨʩʱʸʡʩʰʥʠʺʩʫʶʡʡʺʫʰʺʩʫʾʶʤʤʩʣʴʩʷʩʥʺʥʸʩʩʺʬʹʢʥʱʺʺʠʩʤʯʩʮʺʥʸʩʩʺ\nȧǓȡǕȡÖȡȯ ȡȡǓ ȲȲȡfȡ¡Ȱ@\nȡȯȣȣfͧȡɅǔè¡Ȱ@\nͩ¹ȣȢÛ ȡÚͩ¹ȣȢÛ ɅfȡǗȢÞ¡Ȱ@\nೱളഔഩഖലഩറഫവധച೧രഞഔഫവധ೨ഔളരഘടഫഩഔഺ೪ഫവകടഞഩങഫവനഖഷഔഥഔഷഩഘളഘഩ೧ഗളകഘമഔഩഞധഔനളഔഷഔഥഔഷഩച೨ഔഷഫവ\nFigure 4: Three randomly sampled data entries from\nmLAMA per language. Due to the automatic genera-\ntion of the dataset not all of them are fully correct.\n3258\nen de nl it\nP495: “[X] was created in [Y]” Japan (170), Italy (56) Deutschland (217), Japan (70) Nederland (172), Itali ¨e (50) Italia (167), Giappone (92)\nP101: “[X] works in the ﬁeld of [Y]” art (205), science (135) Kunst (384), Film (64) psychologie (263), kunst (120) ﬁsiologia (168), caccia (135)\nP106: “[X] is [Y] by profession” politician (423), composer (80) Politiker (323), Journalist (128) politicus (339), acteur (247) giornalista (420), giurista (257)\nP1001: “[X] is a legal term in [Y]” India (12), Germany (11) Deutschland (36), Russland (9) Nederland (22), Belgi ¨e (12) Italia (31), Germania (16)\nP39: “[X] has the position of [Y]” bishop (468), God (68) WW (261), Ratsherr (108) burgemeester (400), bisschop (276) pastore ( 289), papa (138)\nP527 “[X] consists of [Y]” sodium (125), carbon (88) Wasserstof (398), C (49) vet (216), aluminium (130) calcio (165), atomo (96)\nP1303 “[X] plays [Y]” guitar (431), piano (165) Gitarre (312), Klavier (204) piano (581), harp (42) arpa (188), pianoforte (139)\nP178 “[X] is developed by [Y]” Microsoft (177), IBM (55) Microsoft (153), Apple (99) Microsoft (200), Nintendo (69) Microsoft (217), Apple (49)\nP264 “[X] is represented by music label [Y]” EMI (267), Swan (32) EMI (202), Paramount Records (59) EMI (225), Swan (50) EMI (217), Swan (99)\nP463 “[X] is a member of [Y]” FIFA (126), NATO (33) FIFA (118), NATO (38) FIFA (157), WWE (16) FIFA (121), NATO (36)\nTable 4: Most frequent object predictions (TyQ) in different languages. Some relations exhibit language speciﬁc\nbiases. WW = “Wirtschaftswissenschaftler”.\n䝪䝹䝗䞊䛸䜹䝃䝤䝷䞁䜹䛿཮Ꮚ䛾㒔ᕷ䛷䛩䚹䜰䞊䝻䞁䞉䝬䝺䞊䛿䜽䜷䞊䝍䞊䝞䝑䜽఩⨨䛷෌⏕䛥䜜䜎䛩䚹䜶䞁䝒䜷䞉䝣䜵䝷䞊䝸䛿䜲䝍䝸䜰ᕷẸ䛷䛩䚹ۤۨۤۧۜۮۤۡۜ۝۠۞۟۠۴ۤ۟ۜ۝۬ۤ۴ۺۜۨۨۜ۞ۤ۬ۨۜ۩ۭۯ۬ۜۦۜۡ۶ۤۜ۟۬ۜ۞۪ۤۢۤ۬ۜۤ۴ۜۤۨ۩ۦۜۮۜۥۭ۝۩۠۬ۜ۝ۣ۪ۭ۠۟ۧۤ۠ۨۜ۠ۤ۴ⵈ䚌㾰⫠䔠⪨⏼㙸㨰⏈㿌⫠䔠⏼㙸㨰㝴Ḱạᷱ㡸ḩ㡔䚝⏼␘⤼㝘㉬㢌㠸㾌⏈Ẅ䞝㢹⏼␘㿄⇌␘⏈㢨䇼⫠㙸㝴㢌㞬ẄḴ᷸⪰㡔㫴䚝⏼␘\n]TK^TQX]D¯MK:G}8OXEI|IYO:PºB_R~^L BYP¢\n_INXB|IB>t]BXzR¢\n<t^BXy7~LJY²TK7:¯TXtB~LyG¢\nࡐࡩࡋࡰࡂ࡟ࡎࡳࡧ࡚ࡒࡐࡶࡩ࠸࡫࡚ࡖࡤࡴࡒ࡬࡞ࡎ࠾ࡤ࠹࡚ࡩ࠸ࡁࡩ࡚ࡩࡠࡩࡗࡩ࡞ࡤࡵࡤࡷࡇࡨ࡚ࡤ࡮࠻ࡠ࡛࠸࠾ࡨࡤࡩࡠࡩࡗ࡚࠻ࡐࡩࡢࡘࡕࡎࡳ࠾࡯࡚࠸࠾ࡤ࠹࠾࡞࡜ࡢ࠾ࡤ࡮ࡘࡳࡐ࡙ࡎࡷ࡟ࡎࡳࡧ࡚ࡒ\n٫فلΩ΍έϐϠΑΕϣϭ̰Σϟ΍έ΍ΩΎ̯ΎϳΑέγΕ̰Ϡϣϣ٫فلέΑϣϣΎ̯ΎϔϳϓϡϳՌϝΎΑՊϓ̶ϣϭϗϭ̰ϳγ̰ϳϣ٫فل٫Ϭ̯έέ΍έϗέΑΕΎϘϠόΗ̶ΗέΎϔγϬΗΎγ̯αϭέϼϳΑΩΎΣΗ΍̶̡έϭϳӳ ӭӫ Ӄӳ ӭӫ Ӄ ҫҦұ Ӌ ҥ ӟⵚ傢≉ᅹ఑ከ⚘ᇽ௨す㛛ᙼᚓ࿨ྡ䚹㞟ᅰⓗᙱ㒊఩னከ՜ከ୰䚹ከ⡿ᑽඞᮭ༩᭯㔅ᅾᕸᢼ᱁୰ᕤస䚹\nFigure 5: Data samples continued.\n105 106\n# articles\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40p1\naf\nar\nazbe\nbg\nbn\nca\nceb\ncs\ncy\nda\nde\nel\nen\nes\net\neu\nfa\nfi\nfr\nga\ngl\nhe\nhi\nhr\nhu\nhy\nid\nit\nja\nka\nko\nla lt\nlv\nms\nnl\npl\npt\nro\nru\nsk\nsl\nsq\nsr\nsv\ntath\ntr\nuk\nur\nvi\nzh\nFigure 6: Scatter plot of p1 TyQ and number of articles\nin the corresponding Wikipedia. There is no clear trend\nvisible."
}