{
  "title": "Face-based age estimation using improved Swin Transformer with attention-based convolution",
  "url": "https://openalex.org/W4365148794",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5071586692",
      "name": "Chaojun Shi",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A5101579269",
      "name": "Shiwei Zhao",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A5100365767",
      "name": "Ke Zhang",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A5100354964",
      "name": "Yibo Wang",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A5036504703",
      "name": "Longping Liang",
      "affiliations": [
        "North China Electric Power University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3036154885",
    "https://openalex.org/W2777551880",
    "https://openalex.org/W3092466816",
    "https://openalex.org/W2807323414",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2128560777",
    "https://openalex.org/W2147702290",
    "https://openalex.org/W3091064178",
    "https://openalex.org/W2009088607",
    "https://openalex.org/W1895390915",
    "https://openalex.org/W2075875861",
    "https://openalex.org/W6753955284",
    "https://openalex.org/W2152826865",
    "https://openalex.org/W3134949695",
    "https://openalex.org/W3179333658",
    "https://openalex.org/W2972970563",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W1965804146",
    "https://openalex.org/W2553156677",
    "https://openalex.org/W2066454034",
    "https://openalex.org/W2121955477",
    "https://openalex.org/W2146656095",
    "https://openalex.org/W2009441118",
    "https://openalex.org/W2147278565",
    "https://openalex.org/W2962786991",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4214736485",
    "https://openalex.org/W6724810625",
    "https://openalex.org/W6678174250",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2121939926",
    "https://openalex.org/W1905153633",
    "https://openalex.org/W6809816361",
    "https://openalex.org/W2955216108",
    "https://openalex.org/W6735377749",
    "https://openalex.org/W2751572766",
    "https://openalex.org/W3003383851",
    "https://openalex.org/W3025538666",
    "https://openalex.org/W2798655965",
    "https://openalex.org/W6796750486",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W4224306551",
    "https://openalex.org/W3146366485",
    "https://openalex.org/W2798868324",
    "https://openalex.org/W2274745179",
    "https://openalex.org/W2981783404",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W6691635037",
    "https://openalex.org/W2118664399",
    "https://openalex.org/W2949624713",
    "https://openalex.org/W2510725918",
    "https://openalex.org/W4205293001",
    "https://openalex.org/W2971130703",
    "https://openalex.org/W2962851632",
    "https://openalex.org/W2057898782",
    "https://openalex.org/W4213312320",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W2899762210",
    "https://openalex.org/W2771116199",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4205250574",
    "https://openalex.org/W2514658199",
    "https://openalex.org/W6800092589",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2800654142",
    "https://openalex.org/W3003442407",
    "https://openalex.org/W6797790494",
    "https://openalex.org/W3164037660",
    "https://openalex.org/W6627801759",
    "https://openalex.org/W4205546750",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3043835189",
    "https://openalex.org/W4281735313",
    "https://openalex.org/W2972367691",
    "https://openalex.org/W2963266717",
    "https://openalex.org/W2498789492",
    "https://openalex.org/W2151386286",
    "https://openalex.org/W3042241776",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1167806532",
    "https://openalex.org/W4226249950",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2249451056",
    "https://openalex.org/W2507376414",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4379382445",
    "https://openalex.org/W4389977645",
    "https://openalex.org/W2964091144",
    "https://openalex.org/W4206258124",
    "https://openalex.org/W3172345956",
    "https://openalex.org/W3105616927",
    "https://openalex.org/W4225825072",
    "https://openalex.org/W3190216403",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W3175544090",
    "https://openalex.org/W4206096925",
    "https://openalex.org/W2898732869"
  ],
  "abstract": "Recently Transformer models is new direction in the computer vision field, which is based on self multihead attention mechanism. Compared with the convolutional neural network, this Transformer uses the self-attention mechanism to capture global contextual information and extract more strong features by learning the association relationship between different features, which has achieved good results in many vision tasks. In face-based age estimation, some facial patches that contain rich age-specific information are critical in the age estimation task. The present study proposed an attention-based convolution (ABC) age estimation framework, called improved Swin Transformer with ABC, in which two separate regions were implemented, namely ABC and Swin Transformer. ABC extracted facial patches containing rich age-specific information using a shallow convolutional network and a multiheaded attention mechanism. Subsequently, the features obtained by ABC were spliced with the flattened image in the Swin Transformer, which were then input to the Swin Transformer to predict the age of the image. The ABC framework spliced the important regions that contained rich age-specific information into the original image, which could fully mobilize the long-dependency of the Swin Transformer, that is, extracting stronger features by learning the dependency relationship between different features. ABC also introduced loss of diversity to guide the training of self-attention mechanism, reducing overlap between patches so that the diverse and important patches were discovered. Through extensive experiments, this study showed that the proposed framework outperformed several state-of-the-art methods on age estimation benchmark datasets.",
  "full_text": "fnins-17-1136934 April 5, 2023 Time: 15:26 # 1\nTYPE Original Research\nPUBLISHED 12 April 2023\nDOI 10.3389/fnins.2023.1136934\nOPEN ACCESS\nEDITED BY\nYinghui Kong,\nNorth China Electric Power University, China\nREVIEWED BY\nGengshen Wu,\nCity University of Macau, Macao SAR, China\nAibin Chen,\nCentral South University of Forestry\nand Technology, China\n*CORRESPONDENCE\nKe Zhang\nzhangkeit@ncepu.edu.cn\nSPECIALTY SECTION\nThis article was submitted to\nVisual Neuroscience,\na section of the journal\nFrontiers in Neuroscience\nRECEIVED 03 January 2023\nACCEPTED 21 February 2023\nPUBLISHED 12 April 2023\nCITATION\nShi C, Zhao S, Zhang K, Wang Y and Liang L\n(2023) Face-based age estimation using\nimproved Swin Transformer with\nattention-based convolution.\nFront. Neurosci.17:1136934.\ndoi: 10.3389/fnins.2023.1136934\nCOPYRIGHT\n© 2023 Shi, Zhao, Zhang, Wang and Liang. This\nis an open-access article distributed under the\nterms of the Creative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with\nthese terms.\nFace-based age estimation using\nimproved Swin Transformer with\nattention-based convolution\nChaojun Shi1,2, Shiwei Zhao1, Ke Zhang1,2*, Yibo Wang1 and\nLongping Liang1\n1Department of Electronic and Communication Engineering, North China Electric Power University,\nBaoding, Hebei, China, 2Hebei Key Laboratory of Power Internet of Things Technology, North China\nElectric Power University, Baoding, Hebei, China\nRecently Transformer models is new direction in the computer vision ﬁeld,\nwhich is based on self multihead attention mechanism. Compared with\nthe convolutional neural network, this Transformer uses the self-attention\nmechanism to capture global contextual information and extract more strong\nfeatures by learning the association relationship between different features,\nwhich has achieved good results in many vision tasks. In face-based age\nestimation, some facial patches that contain rich age-speciﬁc information are\ncritical in the age estimation task. The present study proposed an attention-based\nconvolution (ABC) age estimation framework, called improved Swin Transformer\nwith ABC, in which two separate regions were implemented, namely ABC and\nSwin Transformer. ABC extracted facial patches containing rich age-speciﬁc\ninformation using a shallow convolutional network and a multiheaded attention\nmechanism. Subsequently, the features obtained by ABC were spliced with the\nﬂattened image in the Swin Transformer, which were then input to the Swin\nTransformer to predict the age of the image. The ABC framework spliced the\nimportant regions that contained rich age-speciﬁc information into the original\nimage, which could fully mobilize the long-dependency of the Swin Transformer,\nthat is,extracting stronger features by learning the dependency relationship\nbetween different features. ABC also introduced loss of diversity to guide the\ntraining of self-attention mechanism, reducing overlap between patches so\nthat the diverse and important patches were discovered. Through extensive\nexperiments, this study showed that the proposed framework outperformed\nseveral state-of-the-art methods on age estimation benchmark datasets.\nKEYWORDS\nage estimation, Swin Transformer, attention mechanism, deep learning, neural networks\n1. Introduction\nA large amount of useful information in facial images, such as age, gender,\nidentity, race, emotion, and so forth (Angulu et al., 2018), and research on techniques\nrelated to facial image analysis has become the focus of computer vision. What\nis the signiﬁcance of the facial age as an important feature? As an important\nphysical and social characteristic of human beings, age plays a fundamental role\nin human social interaction. Recently, age estimation based on facial images is\nalready an important research topic (Zhao et al., 2020; Agbo-Ajala and Viriri, 2021),\nFrontiers in Neuroscience 01 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 2\nShi et al. 10.3389/fnins.2023.1136934\nwhich predicts the age corresponding to the image containing the\nface in the image. The task has very good application prospects\nin various intelligent ﬁelds, such as cross-age face recognition,\nintelligent security surveillance, harmonious human-computer\ninteraction, image and video retrieval, face-based age prediction,\nand marketing analysis (Bruyer and Scailquin, 1994; Geronimo\net al., 2009; Song et al., 2011; Angulu et al., 2018; Pei et al., 2019).\nModern face-based age estimation methods typically consist\nof two directions. One is to improve the learning ability of the\nneural network, and the other is to use other features related\nto the age of the face to assist the learning of the network.\nConvolutional networks can learn age features in facial images\nby multilayer convolution and have achieved great success in\nthe ﬁeld of computer vision with a wide range of applications.\nWith the growing popularity of convolutional neural networks\n(CNNs), recent work on face-based age estimation has used these\nnetworks as a backbone (Shen et al., 2018; Dagher and Barbara,\n2021; Sharma et al., 2022; Zhang and Bao, 2022). Most of these\nworks improve the learning ability of the network by increasing\nthe number of layers of convolutional layers (Dornaika et al., 2020;\nYi, 2022) and improving the structure of the network. However,\nas convolutional networks continue to improve, the potential of\nCNN-based facial age estimation models has been exploited, and\nthe increasing number of network model parameters has raised\nthe cost of training. Therefore, we proposed to use a new network\nmodel designed speciﬁcally for face-based age estimation. Some\nother recent studies on face-based age estimation (Deng et al.,\n2021; Lu et al., 2022; Wang et al., 2022) have improved the\naccuracy of age estimation by extracting age features of faces and\nthe relationship between diﬀerent ages (Akbari et al., 2020; Xia\net al., 2020). These studies have enhanced the learning ability of\nthe network using attention-related mechanisms, using features\nother than faces such as sex, gender (Liu et al., 2020), and label\ndistribution (Zeng et al., 2020; Zhao et al., 2020). However, these\neﬀorts may destroy the features and the structure of the original\nimage while extracting the age features of the images, resulting\nin the loss of age information. Therefore, how to extract features\nwithout destroying the extracted features and the structure of the\noriginal image is a research direction.\nRecently, the self-attention mechanism and Transformer model\n(He et al., 2021) have attracted great attention in computer\nvision and natural language processing tasks. Vision Transformer\n(Dosovitskiy et al., 2020) has shown that the Transformer-based\nmodel indeed contains the capacity as the backbone network\ninstead of former pure CNN model in image synthesis and\nclassiﬁcation tasks (He et al., 2016; Szegedy et al., 2017). Related\nresearch (Bourdev et al., 2011) shows that compared with CNN,\nthe self-attention mechanism of Transformer is not limited by\nlocal interactions and allows both long-distance dependencies and\ncomputes in parallel and learn the most appropriate inductive\nbias according to diﬀerent task goals, which has achieved good\neﬀects in many vision tasks. The attention mechanism (Niu et al.,\n2021) is also a direction to improve the prediction ability of the\nnetwork, and the self-attention mechanism (Lin et al., 2017) of the\nTransformer can quickly extract the important features of sparse\ndata, which is an improvement of the attention mechanism. It\nreduces the reliance of the network on external information and\nis better at capturing the correlation within the data or features.\nTherefore, the self-attention mechanism can be designed to exploit\nage-speciﬁc patches during training to boost the performance of\nface-based age estimation methods.\nIn this study, we proposed an attention-based convolution\n(ABC) age estimation framework called an improved Swin\nTransformer with ABC. The architecture of an improved Swin\nTransformer with ABC is illustrated in Figure 1. The core of\nthe improved Swin Transformer with ABC was the ABC. In\nABC, two separate regions were implemented, namely shallow\nconvolution and a multiheaded self-attention mechanism. The\nfeature extractor performed initial feature extraction of the image\nusing a shallow convolutional network and then further extracted\nthe corresponding patches that might contain rich age-speciﬁc\ninformation through the multiheaded attention mechanism,\naccording to the number of probes. ABC extracted facial\npatches containing rich age-speciﬁc information by a shallow\nconvolutional network and multiheaded attention mechanism.\nSubsequently, the features obtained by ABC were spliced with\nthe ﬂattened image in the Swin Transformer, which were\nthen input to the Swin Transformer to predict the age of\nthe image. ABC also introduced loss of diversity to guide\nthe training of self-attention mechanism, reducing overlap\nbetween patches so that the diverse and important patches\nwere discovered. The contributions of this study were as\nfollows:\n(1) We proposed a new module called ABC that used shallow\nconvolution and shifted window self-attention mechanism.\nThe image was initially extracted and reﬁned by shallow\nimage convolution, and some facial regions containing rich\nage-speciﬁc information were extracted by the multiheaded\nself-attention mechanism.\n(2) We introduced the Swin Transformer as the backbone model,\nand the features obtained by ABC were spliced with the\noutput of the embedding layer of the Swin Transformer in\nthe channel dimension and subsequently input to the rest of\nthe Swin Transformer so that the model could better capture\nthe global and local information of the image and achieve\nbetter results by learning the relationship between diﬀerent\nfeatures of the image.\n(3) We introduced a new loss function that made each probe of\nthe multiheaded self-attention mechanism reveal a diﬀerent\nage region and prevent overlapping feature extraction from\nthe multiheaded feature extractor.\n2. Related research\nWe reviewed and discussed related studies on face-based age\nestimation to lay the foundation for this study. We also reviewed\nthe attention mechanism and Transformer, both of which were\nrelevant to the proposed ABC.\n2.1. Face-based age estimation\nIn the last few decades, many researches have been conducted\non face-based age estimation. One of the earliest studies can\nbe traced back to (Kwon and da Vitoria Lobo, 1999), in which\nFrontiers in Neuroscience 02 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 3\nShi et al. 10.3389/fnins.2023.1136934\nBA\nFIGURE 1\nStructure of an improved Swin Transformer with attention-based convolution (ABC). (A) Structure of an attention based convolution. (B) Swin\nTransformer.\nthe researcher’s classiﬁed faces into three age groups based on\nthe craniofacial development theory and wrinkle analysis. In the\ntraditional face-based age estimation methods, ﬁrst the face-based\nage features are extracted, and then classiﬁcation and regression\nmodels are established for face-based age estimation. With the\nrapid development of deep learning in recent years, deep learning–\nbased facial age estimation methods have signiﬁcantly improved\nthe accuracy and robustness of face-based age estimation, especially\nthe accuracy of face-based age estimation under unconstrained\nconditions.\nYi et al. (2014) proposed a multistream CNN to better leverage\nhigh-dimensional structured information in facial images. The\nauthors cropped multiple patches from facial images so that each\nstream learned from one patch. Then, the features extracted from\ndiﬀerent patches were fused before the output layer. Ranjan et al.\n(2015) used a deep convolutional neural network (Chen et al.,\n2016) with 10 convolutional layers, 5 pooling layers, and 1 fully\nconnected layer to extract the age characteristics of facial images.\nThen, age regression was performed using a three-layer artiﬁcial\nneural network. One of the ﬁrst studies to use CNNs for the face-\nbased age estimation problem was (Wang et al., 2015), in which\na CNN with two convolutional layers was deployed. Han et al.\n(2017) used a modiﬁed AlexNet (Krizhevsky et al., 2017) to develop\na multitask learning method for heterogeneous face attribute\nestimation including the age. Rothe et al. (2018) transformed the\nregression problem into a classiﬁcation-regression problem and\nproposed a deep expectation network (DEX) for the age estimation\nof representations. The DEX network changed the number of\nneurons in the last layer of the VGG-16 network.\nIn general, CNN-based methods for face-based age estimation\ncan be divided into two categories. The ﬁrst approach is to use\ndeeper and better deep learning models as backbone networks\nwith better networks to extract features (Dornaika et al., 2020; Lu\net al., 2022; Sunitha et al., 2022; Wang et al., 2022). The second\napproach is to improve the performance of the network in terms\nof other attributes of the face, such as race features and gender\nfeatures (Zhang et al., 2017a; Liu et al., 2020; Deng et al., 2021),\nand relational features of diﬀerent ages (Song et al., 2011; Gao\net al., 2017; Zeng et al., 2020). In our previous study, we proposed\na multilevel residual network model to further improve the\nperformance of the network so as to better improve the accuracy\nof age estimation (Levi and Hassner, 2015). As the CNN-based\nnetwork model for face-based age estimation may soon reach\na bottleneck in the direction of face-based age estimation, we\ntried to incorporate a newer network model that combined the\nadvantages of CNN with the advantages of the new network to\nachieve better results.\n2.2. Attention-based facial age\nestimation\nAttention mechanism (Vaswani et al., 2017) mimics the internal\nprocesses of biological observation behavior, increasing the ﬁneness\nof observation in certain regions. The attention mechanism can\nquickly extract important features of sparse data and is therefore\nwidely used in machine translation, speech recognition, and image\nprocessing (Wang et al., 2016), and other ﬁelds. A multiheaded\nattention mechanism can attend to multiple informative segments\nof the input with an attention head attending to one speciﬁc\nsegment. Therefore, the number of segments that the multiheaded\nattention mechanism can attend to is determined by the number\nof attention heads. In computer vision, the self-attention layer\ntakes the feature map as input and calculates the attention weights\nbetween each pair of features to generate an updated feature map\nwhere each position has information about any other feature in the\nsame image. These layers can replace convolution directly or be\ncombined with convolution layers. They are able to handle larger\nperceptual ﬁelds than conventional convolution, and therefore can\nacquire dependencies between some long-distance interval features\non the space.\nMultiple attention mechanisms have been proposed for\nvisual tasks to address the weaknesses of convolutions due to\nthe aforementioned characteristics. Bello et al. (2019) proposed\nFrontiers in Neuroscience 03 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 4\nShi et al. 10.3389/fnins.2023.1136934\nto augment convolutional networks with self-attention by\nconcatenating convolutional feature maps with a set of feature\nmaps produced via a novel relative self-attention mechanism.\nHu et al. (2018) proposed a simple, lightweight approach for\nbetter context exploitation in CNNs by introducing a pair of\noperators: gather (which eﬃciently aggregated feature responses\nfrom a large spatial extent) and excite (which redistributed the\npooled information to local features). Chen et al. (2018) proposed\nthe “double attention block, ” which was designed with a double\nattention mechanism in two steps, a novel component that\naggregated and propagated informative global features from the\nentire spatiotemporal space of input images/videos, enabling\nsubsequent convolution layers to access features from the entire\nspace eﬃciently. The nonlocal operation proposed by Wang et al.\n(2018) computed the response at a position as a weighted sum\nof the features at all positions and achieved excellent results in\nthe experiment. Wang et al. (2022) proposed a face-based age\nestimation framework called attention-based dynamic patch fusion\n(ADPF). The ADPF dynamically located and ranked age-speciﬁc\npatches by employing a novel ranking-guided multihead hybrid\nattention mechanism and used the discovered patches along with\nthe facial image to predict the age of the subject.\nA previously proposed work (Bello et al., 2019) adds an\nattention mechanism to the convolutional network to enhance\nthe convolutional network. A previously proposed work (Hu\net al., 2018) enhances the input image by using the attention\nmechanism to get the feature responses of adjacent regions of the\nimage. A previously proposed work (Chen et al., 2018) get the\nglobal features and local features by two attention mechanisms,\nrespectively. The previously proposed work (Wang et al., 2018,\n2022) learn the important features by calculating the region\nweights through the attention mechanism. Diﬀerent from the above\nwork which uses the attention mechanism to enhance the deep\nconvolutional network, our work uses shallow convolution and\nattention mechanism with the aim of ﬁnding the important regions\nwithout destroying the original image and stitching these regions\nwith the original image to enhance the Transformer network.\n2.3. Vision transformer\nTransformer (Vaswani et al., 2017) is a kind of deep\ndivine meridian based on the self-attention mechanism. In\nrecent years, Transformer-based models have become a popular\nresearch direction in the ﬁeld of computer vision. Another visual\nTransformer model, ViT, recently proposed by Dosovitskiy et al.\n(2020), achieved state-of-the-art performance on several image\nrecognition benchmark tasks in a structure that fully adopted the\nstandard structure of a Transformer, ViT. Liu Z. et al. (2021)\nproposed the Swin Transformer, which enabled the ﬂexibility of\nthe Transformer model to handle images of diﬀerent scales by\napplying a hierarchical structure similar to that of CNN. The Swin\nTransformer used a windowed attention mechanism to greatly\nreduce the computational complexity. The architecture of a Swin\nTransformer is illustrated in Figure 2. Yuan et al. (2021) proposed\nCeiT, combined with the ability of CNN to extract low-level\nfeatures, to design an Image-to-Tokens module, which extracted\nthe patch from the generated low-level features. Wang et al. (2021)\nproposed the CrossFormer, which used a cross-scale embedding\nlayer (CEL), generated patch embeddings using a CEL at each stage,\nand extracted features using four diﬀerent-sized convolutional\nkernels in the ﬁrst CEL layer. The features were extracted using\nfour convolutional kernels of diﬀerent sizes, and the results of\nthe convolutional kernels were stitched into patch embeddings.\nPeng et al. (2021) proposed Conformer (Yuan et al., 2021)], which\ncombined the features of a Transformer and CNN through a\nparallel structure to achieve feature fusion by bridging each other,\nso that the local features of CNN and the global features of\nthe Transformer could be retained to the maximum extent. Xiao\net al. (2021) proposed ViTc, which replaced the patch embedding\nmodule in the Transformer with convolution. It made the replaced\nTransformer more stable and converge faster. Liu Z. et al. (2021)\nproposed the TransCNN by introducing a CNN layer after the self-\nattention block so that the network could inherit the advantages\nof a Transformer and CNN. UniFormer (Li et al., 2022) seamlessly\nintegrated the advantages of convolution and self-attention through\nthe Transformer to aggregate the local and global features at\nshallow and deep layers, respectively, solving the problem of\nredundancy and dependency for eﬃcient representation learning.\nA previous study proposed (Yuan et al., 2021) replacing the\noriginal three structures of the Transformer with convolutional\nlayers in the Transformer, thus integrating CNN into the\nTransformer. Another previous study proposed (Wang et al., 2021)\ndesigning a CEL and long short distance attention to replace\nthe Transformer block and MSA (Multihead Self-Attention) in\nthe Transformer. A related study (Li et al., 2022) seamlessly\nintegrated the merits of convolution and self-attention in a concise\nTransformer format and proposed a new framework UniFormer.\nSome previous studies proposed (Wang et al., 2021; Yuan et al.,\n2021; Li et al., 2022) combining CNN with the structure in the\nTransformer block to improve the capabilities of the network by\nadding or replacing it so as to improve the framework of the\nTransformer. However, the present study designed an independent\nshallow convolution, which extracted features through a self-\nattentive mechanism to be stitched with the original image and\ninput to the Transformer, without modifying the Transformer.\nThe CNN performed the role of extracting shallow features in the\nnetwork while preserving the original structural information of the\nimage. Another study (Xiao et al., 2021) introduced convolution to\nprocess the input original image by replacing the patch layer with\nCNN. A related study (Liu Z. et al., 2021) viewed image patches\nas tokens by CNN, which continuously extracted features through\na multilayer self-attention mechanism and ﬁnally input to the\nTransformer block. The present study extracted shallow features\nby shallow convolution without destroying the location structure\ninformation of the image. The feature regions rich in face-based\nage information were obtained through a self-attention mechanism\nand stitched with the original image after the patch layer processing\nin the Transformer, instead of directly processing the original\nimage. A related study (Peng et al., 2021) combined the features\nof the Transformer and CNN through a parallel structure using\na bridge to achieve feature fusion. This study introduced CNN\nand self-attention mechanism to extract shallow features while\npreserving the original structural information of the image. The\npurpose of the CNN and self-attention mechanism was to improve\nthe ability of the Transformer to obtain feature dependencies at\nlong distances.\nFrontiers in Neuroscience 04 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 5\nShi et al. 10.3389/fnins.2023.1136934\nBA\nFIGURE 2\nArchitecture of a Swin Transformer: (A) Structure of a Swin Transformer (Swin-T).(B) Two successive Swin Transformer blocks. Window based\nself-attention (W-MSA) and shifted window based self-attention (SW-MSA) are multihead self-attention modules with regular and shifted windowing\nconﬁgurations, respectively (Vaswani et al., 2017).\n3. Methodology\nIn this section, we ﬁrst discussed the core of the Swin\nTransformer with the attention-based convolution mechanism,\nwhich was the proposed ABC. Then, we combined the ABC and\nSwin Transformer parts. Finally, we introduced the age expectation\nand loss function. The architecture of the Swin Transformer with\nthe ABC mechanism is shown in Figure 1.\n3.1. ABC\nAs the Swin Transformer with attention-based convolution\nis based on ABC and the critical component of ABC is the\nself-attention mechanism, we ﬁrst introduced the self-attention\nmechanism followed by the proposed attention-based convolution.\nFinally, we detailed the complete mechanism. The architecture of\nattention-based convolution is shown in Figure 3.\nWe considered an input tensor X with a dimension of\nh ×w ×c, where h denotes the height, w denotes the width, and\nc denotes the number of channels. During training, the input to\nABC was a ﬁxed-size 224 ×224 RGB image.\nThis model started with the institutional area of the\nconvolution. The image was passed through a stack of\nconvolutional layers; this convolutional region had eight\nconvolutional layers, where we used ﬁlters with a very small\nreceptive ﬁeld: 3 ×3. The convolution stride was ﬁxed to 1 pixel;\nthe spatial padding of the convolution layer input was such that\nthe spatial resolution was preserved after convolution, that is,\nthe padding was one pixel for 3 ×3 convolution layers. Spatial\npooling was carried out by two max-pooling layers, which followed\nthe second and fourth convolutional layers. Max-pooling was\nperformed over a 2 ×2 pixel window, with stride two. All hidden\nlayers were equipped with the rectiﬁcation nonlinearity.\nThe convolution was followed by the region of the self-attention\nstructure. The output of the convolutional layer X was used as\nthe input of the attention mechanism. Let us consider an input\ntensor X with a dimension of h ×w ×c, where h denotes the\nheight, w denotes the width, and c denotes the number of channels.\nX was convolved into three separate tensors: Q with a shape of\nh ×w ×cQ, K with a shape of h ×w ×cK , and V with a\nshape of h ×w ×cV , where cQ, ck, and cV indicate the number\nof channels in the corresponding tensor. The intention behind self-\nattention was to compute a weighted summation of the values, V,\nwhere the weights were computed as the similarities between the\nquery, Q, and the corresponding key, K. Therefore, to compute the\nsimilarity, Q and K normally had the same shape, that is, cQ =cV .\nThe output of a single self-attention mechanism was computed as:\nSn1 =soft max\n(\nQ′·K′T\n√cK\n)\n·V (1)\nwhere Q′and K′are ﬂattened tensors to perform the dot product.\nAfter the scaling operation, that is, dividing the similarity\nmatrix Q′·K′T by a factor of √cK and applying the softmax\nfunction, we performed a dot product between the normalized\nsimilarity matrix and V to generate the self-attention maps Sn\nwith a dimension of h ×w ×cK . ni is the number of heads of\nattention probes in the multiheaded attention mechanism.\nAs we ﬂattened the two-dimensional feature maps into a one-\ndimensional vector in Equation 1, the original structure of the\nfeature maps was distorted. We adopted the relative positional\nencoding to make it eﬃcient when dealing with structured data\nsuch as images and multidimensional features. Speciﬁcally, the\nrelative positional encoding was represented by the attention logit,\nwhich encoded how much an entry inQ′attended to an entry inK′.\nThe attention logit was computed as:\nli,j = qT\ni√cK\n(kj +rw\njx−ix +rh\njy−iy ) (2)\nwhere qi is the ith row in Q′ indicating the feature vector for\npixel i :=\n(\nix, iy\n)\nand kj is the jth row in K′ indicating the\nfeature vector for pixel j :=\n(\njx, jy\n)\n. rw\njx−ix and rh\njy−iy are learnable\nparameters encoding the positional information within the relative\nwidth jx −ix and relative height jy −iy, respectively. With the\nrelative positional encoding, the output of a single self-attention\nmechanism could be reformulated as:\nSn1 =soft max\n(\nQ′·K′T +mh +mw√cK\n)\n·V (3)\nwhere mh\n[\ni, j\n]\n= qT\ni rh\njy−iy and mh\n[\ni, j\n]\n= qT\ni rw\njx−ix are matrices\nof relative positional logits. In this study, the number of heads\nFrontiers in Neuroscience 05 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 6\nShi et al. 10.3389/fnins.2023.1136934\nFIGURE 3\nStructure of the proposed attention-based convolution (ABC).\nof the attention probe in the multiheaded attention mechanism\nwas set to four.\nA key design element of ABC was its shift of the window\npartition between consecutive self-attention layers, as illustrated\nin Figure 2. The shifted windows bridged the windows of\nthe preceding layer, providing connections among them that\nsigniﬁcantly enhanced the modeling power. As illustrated in\nFigure 2, the module used a regular window partitioning strategy\nthat started from the top-left pixel, and the 56 ×56 feature map\nwas evenly partitioned into 8 ×8 windows of size 7 ×7 (M =7).\nThen, the module adopted a windowing conﬁguration that was\nshifted from that of the preceding layer by displacing the windows\nby ([M\n2 ], [M\n2 ]) pixels from the regularly partitioned windows.\nWith the shifted window partitioning approach, the attention\nblocks were computed as follows:\nˆX1 =W −MSA( ˆX) (4)\nwhere ˆX1 denotes the output features of the shallow convolution\nmodule.\n3.2. Swin Transformer with ABC\nmechanism\nWe again used the sample image input at the beginning with\nthe tensor Y as input to the Transformer. First, the tensor Y\nwent through the patch partition layer and the dimension became\n56 ×56 ×48. Then, Y was again mapped to the speciﬁed\ndimension by the linear embedding layer, and the dimension of\nY was 56 ×56 ×128. The role of the patch partition module\nwas to crop a patch_size ∗patch_size block of the input original\nimage by conv2d. The patch_size was set to four. The speciﬁc\nstructure of the Swin Transformer is shown in the Figure 2. The\nremaining structure of the Swin Transformer could be referred\nfrom the original study, and as no modiﬁcations were made to the\nSwin Transformer in this study, it was not repeated in this study.\nIn the output of the self-attention mechanism in Equation 4,\nwhich was the output of ABC, the output tensorX had a dimension\nof 56 ×56 ×16 and the output of the linear embedding layer of\nthe Swin Transformer also had a dimension of 56 ×56 ×128,\nso we considered fusing the two tensors. We spliced the output X\nof ABC with the output Y of the embedding layer in the channel\ndimension to get Y1, that had a dimension of 56 ×56 ×144.\nThen, we replaced Y with the spliced tensor Y1, continued with the\nnetwork layer behind the Swin Transformer, and ﬁnally got the ﬁnal\noutput Z at the end of the Swin Transformer.\nUsing a Transformer as the backbone network can make\nthe model better to capture the global and local information of\nimages, extract more powerful features, and achieve better eﬀects by\nlearning the relationship between diﬀerent features of images. This\nspeciﬁc framework can more eﬀectively learn the long-distance\ndependencies in face semantic parts, which naturally helps model\nthe strong attribute correlations (Liu et al., 2020).\nFrontiers in Neuroscience 06 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 7\nShi et al. 10.3389/fnins.2023.1136934\nFocusing on the ability of the Swin Transformer to mine\nboth long-range dependencies and parallel computation, we ﬁrst\nobtained several important age feature regions rich with age-\nspeciﬁc information using ABC and stitched them with the\noriginal input image. This way, the Swin Transformer afterward\ncould learn more dependencies between important age features\nand improve the learning ability of the network. We used the\nstitching in the connection part of the ABC and Swin Transformer.\nBy stitching in the channel dimension, the stitched Y1 at this\npoint not only retained the structure and features of the original\ninput image but also had feature regions that were processed by\nthe shallow convolutional layer and the multiheaded attention\nmechanism. The ABC-derived feature regions not only highlighted\nthe regions containing rich age information but also retained\nthe structural information of the regions to the greatest extent\nby the shallow convolutional layer. The Swin Transformer could\nlearn the relationship between the features of the original input\nimage and the features obtained by ABC when processing the\nstitched Y1, thus fully exploiting the advantage of the Swin\nTransformer. The use of the ABC+Swin Transformer enabled\nthe model to better capture the global and local information of\nthe image, which not only preserved the local sensitivity and\ntranslation invariance of CNN but also improved the ability of\nSwin Transformer to learn the dependencies between features for\nbetter results.\n3.3. Loss\nWe used the label distribution learning to learn the exact age\nand Kullback–Leibler divergence to learn the age distribution to\nestimate the age.\nFormally, let xi ∈X denote the ith input instance with\ni =1, 2, ...,N, and ˆyi,j denote the softmax distribution of the\npredicted values of the network, whereNis the number of instances\nand c is the number of each age.\nDeep label distributions transform yi from a single class label\nto a label distribution and then predict ˆyi by label distribution\nlearning. Instances with the same class label yi share the identical\nGaussian distribution:\nl\ncj\nxi = 1√\n2πσM\nexp\n(\n−(cj −yi)2\n2σ2\n)\n(5)\nwhere l\ncj\ni is the degree to which the value of each age cj describes\nimages xi and cj =0, 1, ...,C; σ is the standard deviation of\nlci\nb ; and the factor M lead ∑a\nj=0 l\ncj\nxi =1. In this study, σ was\nset to 2.\nKL (Kullback-Leibler) tried to generate the predicted softmax\nprobability distribution as similar to the ground truth distribution\nas possible. For the same random variable, with two diﬀerent\ndistributions ˆyi and l\ncj\ni , the KL scatter of ˆyi to l\ncj\ni was expressed as:\nKL =\n∑\nx\nˆyi ×log2\nˆyi\nl\ncj\ni\n(6)\nThe less the result of KL loss, the less the diﬀerence between\nthe two divisions, indicating that the less the diﬀerence between\nthe true age and the estimated age, the more accurate the age\nestimated by the network.\nThe number of patches that could be discovered was\ndetermined by the number of attention heads implemented in ABC.\nHowever, during implementation, we found that patches tended to\noverlap, especially in informative regions. This overlap of attended\npatches might lead to redundant learning sources and leave other\nage-speciﬁc patches undiscovered. To alleviate this overlap issue,\nwe used a diversity loss to learn diverse and nonoverlapping patches\nby minimizing the summation of products of corresponding entries\nin two attention heads,Sn1 (h′, w′) and Sn2 (h′, w′). The diversity loss\nwas formulated as:\nLOverlap =\nn∑\nn1, n2\nn1 ̸=n2\nh∑\nh′\nw∑\nw′\nSn1 (h′, w′) ·Sn2 (h′, w′) (7)\nEach probe in the multiheaded attention mechanism generated\na Sn(h′, w′), and Sn(h′, w′) represented the region that the probe\nfocused on. Sn(h′, w′) could be regarded as a weight matrix\nwith dimension 56 ×56 ×16. In Sn(h′, w′), the richer the\nregion with age-speciﬁc information, the larger the weight matrix\ncorresponding to that region. When the result obtained by\nmultiplying Sn(h′, w′) and the other Sn(h′, w′) was 0, the regions\nattended by the two attention probes did not overlap. When the\noverlap loss obtained after multiplying all Sn(h′, w′) with each\nother was zero, no overlap occurred between the regions attended\nby diﬀerent probes, which prevented the redundancy of learning\ncaused by multiple attention probes attending to the same region\nat the same time.\nThe overall loss to train this network was the summation of the\ntwo loss functions:\nL =LKL +λ1LOverlap (8)\nwhere λ1 is the hyperparameter that attempts to balance the\ninﬂuences of mean and residual sublosses in the combined loss\nfunction. In this study, λ1 was set to 10.\nThe estimated age of the i-th test image could be calculated\nbased on Equation 9. Each descriptive degree in the label\ndistribution was multiplied by its corresponding label and then\nsummed to obtain the ﬁnal predicted age. Assuming that the\nnetwork softmax layer output 62 probability values from age\n16 years to age 77 years, pi,j corresponded to the probability\nof the prediction for 16–77 years, respectively, as shown in\nEquation 9:\nˆyi =\n77∑\nj=16\nˆyi,j ×cj (9)\n4. Experiments\nIn this section, we ﬁrst detailed the experimental\nsettings and then compared our method with state-of-the-\nart studies on face-based age database MORPH Album II\n(Ricanek and Tesafaye, 2006), FG-NET dataset (Cootes et al.,\n2001), and Adience dataset (Eidinger et al., 2014). We removed\nthe important design elements of ABC. In this study, Swin\nTransformer used the model parameter settings of the base.\nFrontiers in Neuroscience 07 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 8\nShi et al. 10.3389/fnins.2023.1136934\n4.1. Implementation details\nThe eﬀectiveness of the methods in this study was\ndemonstrated using the model as the backbone network for\nimage classiﬁcation tasks. For training and testing on the\nMORPH Album II, FG-NET, and Adience datasets, the size\nwas set to 64, the number of iterations to 800, the weight\ndecay to 0.005, the momentum to 0.9, and the default learning\nrate to 0.0001. The learning rate was multiplied by 0.1 when\ntraining to the 500th iteration, and by 0.01 when training to the\n650th iteration.\nThe facial images in MORPH Album II and FG-NET\ndatasets had corresponding speciﬁc age values. For the evaluation\nmetrics of these two datasets, we used MAE. The age labels\ncorresponding to the images in the Adience dataset were age\ngroups, such as 0–2 and 4–6. For the evaluation metrics\nof the Adience dataset, we used the accuracy of a one-\nclass classiﬁcation.\nThe MAE was calculated as shown in Equation 10, where yj\nis the age label value, y\n′\nj is the age prediction value, and N is the\nnumber of test images. The accuracy of the one-class classiﬁcation\nwas calculated as shown in Equation 11, where TP is the correctly\npredicted sample and FP is the incorrectly predicted sample.\nMAE = 1\nN\nN∑\nj=1\n|yj −y\n′\nj| (10)\nprecision = TP\nTP +FP (11)\n4.2. Dataset\nWe conducted experiments on three commonly used face-\nbased age estimation benchmark datasets: MORPH Album II,\nFG-NET, and Adience.\nThe MORPH Album II is one of the most common and largest\nlongitudinal face databases in the public domain for age estimation,\ncontaining 55,134 facial images of 13,617 subjects. The age ranged\nfrom 16 to 77 years with an average age of 33 years, and the male-to-\nfemale ratio was 5.5:1. Each facial image in the MORPH II dataset\nwas associated with identity, age, race, and sex labels. We randomly\nsplit the whole dataset into two subsets: one with 80% of the data\nfor training and the other with 20% for testing. In this setting, no\nidentity overlap occurred between the two subsets. Some images are\nshown in Figure 4.\nThe FG-NET dataset contained 1,002 facial images from\n82 noncelebrity subjects with large variations in lighting, pose,\nand expression. The age ranged from 0 to 69 years (on\naverage, 12 images per subject). Each subject in this dataset\nhad more than 10 facial images taken over a long time\nspan. In addition, the facial images in this dataset contained\npose, illumination, and expression variations. For the FG-NET\ndataset, we use the leave-one-person-out (LOPO) strategy. In\neach fold, we use facial images of one subject for testing\nand the remaining images for training. Since there are 82\nsubjects, this process consists of 82-fold and the reported\nresults are the average values. Some images are shown in\nFigure 5.\nThe Adience dataset was collected from the photos taken\nby users themselves from the Y ahoo image-sharing website.\nIt contained 26,580 images of 2,284 people with age ranging\nfrom 0 to 100 years. The labels used in this dataset were age\ngroup labels: 0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, and\n60–100, categorized into eight groups. Some images with age\nlabels were labeled as age values, some as other age ranges,\nand some others as empty. In this study, the images with age\nlabels labeled as age values, other age ranges, and null were\nremoved. The age classiﬁcation experiments using the Adience\ndataset were treated as eight classes. Some images are shown in\nFigure 6.\nFIGURE 4\nImages in the MORPH Album II dataset.\nFrontiers in Neuroscience 08 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 9\nShi et al. 10.3389/fnins.2023.1136934\nFIGURE 5\nImages in the FG-NET dataset.\nFIGURE 6\nImages in the Adience dataset.\nFrontiers in Neuroscience 09 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 10\nShi et al. 10.3389/fnins.2023.1136934\nTABLE 1 Comparison of age classiﬁcation on the MORPH Album II\ndataset.\nMethod MAE\nOHRank (Chang et al., 2011) 8.83\nMTWGP (Zhang and Yeung, 2010) 6.28\nOHRank (Chang et al., 2011) 6.07\nCA-SVR (Chen et al., 2013) 5.88\nSVR (Guo et al., 2008) 5.77\nLDL (Geng et al., 2013) 4.87\nDLA (Wang et al., 2015) 4.77\nALDL (Geng et al., 2013) 4.34\nKPLS (Guo and Mu, 2011) 4.18\nBIF+KCCA (Guo and Mu, 2013) 3.98\nVGG (Rothe et al., 2016) 3.45\nARN (Agustsson et al., 2017) 3.25\nVGGNetHybrid (Xing et al., 2017) 2.96\nDAG-VGG16 (Li et al., 2019) 2.81\nMean-Variance Loss (Pan et al., 2018) 2.80\nMSFCL-KL (Xia et al., 2020) 2.73\nDEX (IMDB-WIKI) (Rothe et al., 2018) 2.68\nVDAL (Liu H. et al., 2020) 2.57\nADPF (Wang et al., 2022) 2.54\nAnet+Gnet+Rnet (Deng et al., 2021) 2.47\nSwin Transformer (Liu Z. et al., 2021) 2.37\nABC+Swin Transformer (Ours) 2.17\nBold values represent the best data.\n4.3. Evaluations on the MORPH Album II\ndataset\nThe MAE values for the aforementioned settings of the\nMORPH Album II dataset are tabulated inTable 1. As shown in the\ntable, the ABC+Swin Transformer outperformed all state-of-the-\nart methods. Compared with the other methods on the MORPH\nAlbum II, our proposed method improved by 55.45% over the LDL\nmethod, by 37.2% over the VGG method, by 15.6% over the V ADL\nmethod, and by 14.6% over the ADPF method. Compared with the\noriginal Swin Transformer, our proposed ABC+Swin Transformer\nimproved by 8.4%. The experimental results showed that our\nmethod had high accuracy on the MORPH Album II dataset and\nimproved the learning ability of the facial image age of the Swin\nTransformer.\n4.4. Evaluations on the FG-NET dataset\nAs the number of images in the FG-NET dataset was too\nsmall, training the dataset directly might lead to a decrease in the\naccuracy of the results and slow convergence. Therefore, we used\nthe pretraining weights obtained in the aforementioned FG-NET\ndataset as the initial weights for training. The MAE values for\nthe aforementioned settings of the FG-NET dataset are tabulated\nTABLE 2 Comparison of age estimates on the FG-NET dataset.\nMethod MAE\nMean-Variance Loss (Pan et al., 2018) 4.10\nDRFs (Dagher and Barbara, 2021) 3.85\nM-LSDML (Liu et al., 2017) 3.74\nDLDFL (Shen et al., 2019) 3.71\nDFR (Shen et al., 2019) 3.47\nDAG-VGG16 (Taheri and Toygar, 2019) 3.08\nDAG-GoogleNet (Panis et al., 2016) 3.05\nAGEn (Tan et al., 2017) 2.96\nC3AE (Zhang et al., 2019) 2.95\nADPF (Wang et al., 2022) 2.86\nSwin Transformer (Liu Z. et al., 2021) 2.69\nAnet+Gnet+Rnet (Deng et al., 2021) 2.59\nBridgeNet (Hou et al., 2016) 2.56\nABC+Swin Transformer (Ours) 2.52\nBold values represent the best data.\nin Table 2. As shown in the table, the ABC+Swin Transformer\noutperformed all state-of-the-art methods. Compared with the\nother methods on the FG-NET dataset, the method proposed in this\nstudy improved by 21.8% over the DAG-VGG16 method, by 15.8%\nover the ADPF method, and by 8.6% over the BridgeNet method.\nCompared with the original Swin Transformer, the performance\nof our proposed ABC+Swin Transformer improved by 4.8%. The\nexperimental results showed that our method had high accuracy\non the FG-NET dataset and improved the learning ability of facial\nimage age of the Swin Transformer.\n4.5. Evaluations on the Adience dataset\nIn this study, the images of the Adience dataset without\nage labels or confusing age labels in the dataset were removed.\nExperimentally, when using the Adience dataset for training and\ntesting, the dataset was split into ﬁve groups using the 5-fold\ncross-validation method: 0-, 1-, 2-, 3-, and 4-fold. Each time when\nTABLE 3 Comparison of age estimates on the Adience dataset.\nModels Accuracy of a one-class\nclassiﬁcation (%)\nSVM-dropout (Shen et al., 2019) 45.1 ±2.6\nCNN+over-sampling (Hou et al., 2016) 50.7 ±5.1\nR-SAAFc1 (Zhang et al., 2017b) 52.9\nR-SAAFc2 (Zhang et al., 2017b) 53.5\nClassiﬁcation (Liu et al., 2018) 54.0 ±6.3\nChained Net (Zhang et al., 2017a) 54.5\nDEX w/o IMDB-WIKI Pretrain\n(Rothe et al., 2018)\n55.6 ±6.1\nABC+Swin Transformer (Ours) 56.1\nBold values represent the best data.\nFrontiers in Neuroscience 10 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 11\nShi et al. 10.3389/fnins.2023.1136934\nTABLE 4 Accuracy of each age class on the Adience dataset.\nFold 0–2 4–6 8–12 15–20 25–32 38–43 48–53 60–100\n0 82.59 37.70 39.38 32.92 72.36 39.51 30.31 52.73\n1 85.31 59.38 41.29 35.68 58.65 37.11 31.32 52.19\n2 64.45 65.19 58.04 17.03 76.95 35.16 19.38 61.46\n3 50.00 71.63 38.02 36.13 61.98 40.63 21.88 44.38\n4 84.58 62.95 61.97 29.84 71.38 33.99 27.40 50.00\nAverage 73.39 59.37 47.74 30.20 68.26 37.28 26.06 52.15\nTABLE 5 MAE values for the attention-based convolution (ABC) framework on the MORPH Album II dataset for different numbers of attention probes.\nNumber of attention probes 0 1 2 4 8 16\nABC+Swin Transformer (Ours) 2.243 2.239 2.228 2.170 2.212 2.236\nTABLE 6 MAE values for the attention-based convolution (ABC) framework on the MORPH II for different values of λ1.\nValue of λ1 1 5 7.5 10 12.5 15 20\nABC+Swin Transformer (Ours) 2.37 2.25 2.19 2.17 2.20 2.27 2.38\ntraining and testing, one group of images was used for testing,\nthe remaining four groups were combined into a training set for\ntraining, and the training-testing was done a total of ﬁve times.\nThe ﬁnal test set of the age group classiﬁcation results was taken\nas the average of the ﬁve times results. The Adience dataset used\nan evaluation method for single-age classiﬁcation accuracy. The\naccuracy for the aforementioned settings of the Adience dataset are\ntabulated in Table 3.\nThe accuracy of each age class of folds on the Adience dataset\nis shown as follows. Due to the random grouping in the images, the\naccuracy of each fold is diﬀerent. Overall, the accuracy of each age\nclass is essentially equal between diﬀerent folds. This demonstrates\nthat the model is stable in training. The accuracy of each age class\nfor the aforementioned settings of the Adience dataset are tabulated\nin Table 4.\n4.6. Ablation study\nWe conducted ablation experiments to demonstrate the\neﬀectiveness of each component of the ABC+Swin Transformer.\nSpeciﬁcally, we used the number of heads of attention probes in\nthe multiheaded attention mechanism as a variable to demonstrate\nthe necessity of each component. The MAE values for the\naforementioned settings of the MORPH Album II dataset are\ntabulated in Table 5.\nTable 5shows that when the number of attention probes of\nABC was zero, the MAE obtained by the ABC+Swin Transformer\non the dataset was better than that obtained by the Swin\nTransformer, which demonstrated that the convolution performed\nan eﬀective function in the network and enhanced the network’s\nability to learn the age information of facial images. The MAE\nobtained using diﬀerent numbers of attention probes showed\nthat the optimum results were obtained when the number of\nprobes was four. The reason for this was the presence of\nfour important regions with rich age-speciﬁc information in the\nface. The more the number of probes, the more completely\nthe age-speciﬁc regions could be extracted. As the number\nof probes increased, the intervals noticed by diﬀerent probes\nmight overlap, leading to redundancy in information extraction,\nwhich was not conducive to the information extraction of\nthe network.\nAs shown in Table 6, the network was diﬃcult to converge\nwhen λ1 was too small (KL loss took a dominant role) and a big\nperformance degradation occurred when λ1 was too large (the\noverlap loss took a dominant role). Within a long reasonable range,\nour proposed method performed stably. The number of heads of\nthe multihead attention mechanism was set to four.\n5. Conclusion\nIn this study, we proposed the ABC framework to improve\nthe performance of face-based age estimation tasks and combined\nABC with the Swin Transformer to obtain better prediction\nperformance. Our framework combined the shallow convolution\nand the multiheaded attention mechanism using the shifted\nwindow approach. The shallow convolution used several layers of a\nconvolutional network with few convolutional kernels to condense\nthe information, enhance the features of the image, and process the\ninput to the same size for stitching with the information of the later\nattentional network computations and the Swin Transformer. The\nmultiheaded attention mechanism allowed the network to learn\nand ﬁnd regions that contained rich age-speciﬁc information, and\ndisplay these regions. Finally, the age-rich regions obtained by the\nABC framework were spliced with the image that was initially\nprocessed by the Swin Transformer along the channel dimension,\nand the stitched image tensor was carried out to the subsequent\nnetwork of the Swin Transformer together to compute the ﬁnal\nage prediction.\nFrontiers in Neuroscience 11 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 12\nShi et al. 10.3389/fnins.2023.1136934\nThe signiﬁcant age feature regions obtained by ABC were\nstitched with the original input images and then passed through\nthe Swin Transformer for face-based age estimation, which made\nexcellent use of the ability of the Swin Transformer to mine long-\ndistance dependencies and parallel computation to learn more\ndependencies between important age features. The addition of\nthe ABC framework well compensated the image local sensitivity\nand translation invariance of the Swin Transformer. The ABC\nframework spliced the important regions that contained rich\nage-speciﬁc information into the original image, which could\nfully mobilize the long-dependency of the Swin Transformer,\nthat is, extracting stronger features by learning the dependency\nrelationship between diﬀerent features. As a result, the entire\nnetwork could not only extract the important face-based age\ninformation regions but also further improve the prediction\naccuracy using the ability of the Swin Transformer to learn the\ninterrelationship between features.\nBased on the evaluation of several benchmark datasets,\nABC signiﬁcantly improved the prediction accuracy compared\nwith several state-of-the-art methods. Future studies should\ninvestigate the design of custom estimators to further improve\nthe performance, for example, by further augmenting the\nconvolutional network.\nData availability statement\nThe original contributions presented in this study are included\nin this article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nEthics statement\nWritten informed consent was obtained from the individual(s),\nand minor(s)’ legal guardian/next of kin, for the publication of any\npotentially identiﬁable images or data included in this article.\nAuthor contributions\nCS had full access to all the data in the study and was\nresponsible for the integrity of the data and the accuracy of the data\nanalysis. CS contributed to concept and design with input from KZ,\nSZ, YW, and LL and drafted the manuscript with input from all\nauthors. CS, SZ, and KZ contributed to statistical analysis. CS and\nKZ contributed to funding acquisition, administrative, technical,\nor material support. All authors contributed to acquisition,\nanalysis, or interpretation of data, critical revision of the\nmanuscript for important intellectual content, and approved the\nsubmitted version.\nFunding\nThis study was funded by the National Natural Science\nFoundation of China (62076093 and 62206095) and the\nFundamental Research Funds for the Central Universities\n(2020MS099, 2020YJ006, and 2022MS078).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAgbo-Ajala, O., and Viriri, S. (2021). Deep learning approach for facial age\nclassiﬁcation: A survey of the state-of-the-art. Artif. Intell. Rev. 54, 179–213. doi:\n10.1007/s10462-020-09855-0\nAgustsson, E., Timofte, R., and Van Gool, L. (2017). “Anchored regression\nnetworks applied to age estimation and super resolution, ” in Proceedings of the IEEE\ninternational conference on computer vision , Venice, 1643–1652. doi: 10.1109/ICCV.\n2017.182\nAkbari, A., Awais, M., Feng, Z. H., Farooq, A., and Kittler, J. (2020). Distribution\ncognisant loss for cross-database facial age estimation with sensitivity analysis. IEEE\nTrans. Pattern Anal. Mach. Intell. 44, 1869–1887. doi: 10.1109/TPAMI.2020.3029486\nAngulu, R., Tapamo, J. R., and Adewumi, A. O. (2018). Age estimation via face\nimages: A survey. EURASIP J. Image Video Process. 2018, 1–35. doi: 10.1186/s13640-\n018-0278-6\nBello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V. (2019). “Attention\naugmented convolutional networks, ” in Proceedings of the IEEE/CVF international\nconference on computer vision Seoul, Korea (South). Piscataway, NJ: IEEE, 3286–3295.\ndoi: 10.1109/ICCV.2019.00338\nBourdev, L., Maji, S., and Malik, J. (2011). “Describing people: A poselet-based\napproach to attribute classiﬁcation, ” inProceedings of the 2011 international conference\non computer vision, Barcelona, 1543–1550. doi: 10.1109/ICCV.2011.6126413\nBruyer, R., and Scailquin, J. C. (1994). Person recognition and ageing: The cognitive\nstatus of addresses-an empirical question. Int. J. Psychol. 29, 351–366. doi: 10.1080/\n00207599408246548\nChang, K. Y., Chen, C. S., and Hung, Y. P. (2011). “Ordinal hyperplanes ranker\nwith cost sensitivities for age estimation, ” inProceedings of the CVPR 2011 , Colorado\nSprings, CO, 585–592. doi: 10.1109/CVPR.2011.5995437\nChen, J. C., Patel, V. M., and Chellappa, R. (2016). “Unconstrained face veriﬁcation\nusing deep CNN features, ” in Proceedings of the 2016 IEEE winter conference on\napplications of computer vision (WACV) , Lake Placid, NY , 1–9. doi: 10.1109/WACV.\n2016.7477557\nChen, K., Gong, S., Xiang, T., and Change Loy, C. (2013). “Cumulative attribute\nspace for age and crowd density estimation, ” in Proceedings of the IEEE conference\non computer vision and pattern recognition , Portland, OR, 2467–2474. doi: 10.1109/\nCVPR.2013.319\nFrontiers in Neuroscience 12 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 13\nShi et al. 10.3389/fnins.2023.1136934\nChen, Y., Kalantidis, Y., Li, J., Y an, S., and Feng, J. (2018). “A 2-Nets: double\nattention networks, ” in Proceedings of the 32nd international conference on neural\ninformation processing systems. Montréal: NIPS, 350–359.\nCootes, T. F., Edwards, G. J., and Taylor, C. J. (2001). Active appearance\nmodels. IEEE Trans. Pattern Anal. Mach. Intell. 23, 681–685. doi: 10.1109/34.\n927467\nDagher, I., and Barbara, D. (2021). Facial age estimation using pre-trained CNN and\ntransfer learning. Multimed. Tools Applic. 80, 20369–20380. doi: 10.1007/s11042-021-\n10739-w\nDeng, Y., Teng, S., Fei, L., Zhang, W., and Rida, I. (2021). A multifeature learning\nand fusion network for facial age estimation. Sensors 21:4597. doi: 10.3390/s21134597\nDornaika, F., Bekhouche, S. E., and Arganda-Carreras, I. (2020). Robust regression\nwith deep CNNs for facial age estimation: An empirical study. Exp. Syst. Appl.\n141:112942.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv [Preprint]. arXiv: 2010.11929.\nEidinger, E., Enbar, R., and Hassner, T. (2014). Age and gender estimation of\nunﬁltered faces. IEEE Trans. Inform. Forensics Secur. 9, 2170–2179. doi: 10.1109/TIFS.\n2014.2359646\nGao, B. B., Xing, C., Xie, C. W., Wu, J., and Geng, X. (2017). Deep label distribution\nlearning with label ambiguity. IEEE Trans. Image Process.26, 2825–2838. doi: 10.1109/\nTIP.2017.2689998\nGeng, X., Yin, C., and Zhou, Z. H. (2013). Facial age estimation by learning from\nlabel distributions.IEEE Trans. Pattern Anal. Mach. Intell.35, 2401–2412. doi: 10.1109/\nTPAMI.2013.51\nGeronimo, D., Lopez, A. M., Sappa, A. D., and Graf, T. (2009). Survey of pedestrian\ndetection for advanced driver assistance systems. IEEE Trans. Pattern Anal. Mach.\nIntell. 32, 1239–1258. doi: 10.1109/TPAMI.2009.122\nGuo, G., and Mu, G. (2011). “Simultaneous dimensionality reduction and human\nage estimation via kernel partial least squares regression, ” inProceedings of the CVPR\n2011, Colorado Springs, CO, 657–664. doi: 10.1109/CVPR.2011.5995404\nGuo, G., and Mu, G. (2013). “Joint estimation of age, gender and ethnicity: CCA\nvs. PLS, ” inProceedings of the 2013 10th IEEE international conference and workshops\non automatic face and gesture recognition (FG) , Shanghai, 1–6. doi: 10.1109/FG.2013.\n6553737\nGuo, G., Fu, Y., Dyer, C. R., and Huang, T. S. (2008). Image-based human age\nestimation by manifold learning and locally adjusted robust regression. IEEE Trans.\nImage Process. 17, 1178–1188. doi: 10.1109/TIP.2008.924280\nHan, H., Jain, A. K., Wang, F., Shan, S., and Chen, X. (2017). Heterogeneous face\nattribute estimation: A deep multi-task learning approach. IEEE Trans. Pattern Anal.\nMach. Intell. 40, 2597–2609. doi: 10.1109/TPAMI.2017.2738004\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ” inProceedings of the IEEE conference on computer vision and pattern\nrecognition, Las Vegas, NV , 770–778. doi: 10.1109/CVPR.2016.90\nHe, S., Luo, H., Wang, P., Wang, F., Li, H., and Jiang, W. (2021). “Transreid:\nTransformer-based object re-identiﬁcation, ” in Proceedings of the IEEE/CVF\ninternational conference on computer vision , Montreal, QC, 15013–15022. doi: 10.\n1109/ICCV48922.2021.01474\nHou, L., Samaras, D., Kurc, T. M., Gao, Y., and Saltz, J. H. (2016). Neural networks\nwith smooth adaptive activation functions for regression. arXiv [Preprint]. arXiv:\n1608.06557.\nHu, J., Shen, L., Albanie, S., Sun, G., and Vedaldi, A. (2018). “Gather-excite:\nExploiting feature context in convolutional neural networks, ” in Proceedings of the\n32nd international conference on neural information processing systems . Montréal:\nNIPS, 9423–9433.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). Imagenet classiﬁcation with\ndeep convolutional neural networks. Commun. ACM 60, 84–90. doi: 10.1145/3065386\nKwon, Y. H., and da Vitoria Lobo, N. (1999). Age classiﬁcation from facial images.\nComput. Vision Image Understand.74, 1–21. doi: 10.1006/cviu.1997.0549\nLevi, G., and Hassner, T. (2015). “Age and gender classiﬁcation using convolutional\nneural networks, ” in Proceedings of the IEEE conference on computer vision and\npattern recognition workshops , Boston, MA, 34–42. doi: 10.1109/CVPRW.2015.\n7301352\nLi, K., Wang, Y., Zhang, J., Gao, P., Song, G., Liu, Y., et al. (2022). Uniformer:\nUnifying convolution and self-attention for visual recognition.arXiv [Preprint]. arXiv:\n2201.09450.\nLi, W., Lu, J., Feng, J., Xu, C., Zhou, J., and Tian, Q. (2019). “Bridgenet: A continuity-\naware probabilistic network for age estimation, ” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition Seoul, Korea (South).Piscataway,\nNJ: IEEE, 1145–1154. doi: 10.1109/CVPR.2019.00124\nLin, Z., Feng, M., Santos, C. N. D., Yu, M., Xiang, B., Zhou, B., et al. (2017). A\nstructured self-attentive sentence embedding. arXiv [Preprint]. arXiv: 1703.03130.\nLiu, H., Lu, J., Feng, J., and Zhou, J. (2017). Label-sensitive deep metric learning for\nfacial age estimation. IEEE Trans. Inform. Forensics Secur. 13, 292–305. doi: 10.1109/\nTIFS.2017.2746062\nLiu, H., Sun, P., Zhang, J., Wu, S., Yu, Z., and Sun, X. (2020). Similarity-aware\nand variational deep adversarial learning for robust facial age estimation. IEEE Trans.\nMultimed. 22, 1808–1822. doi: 10.1109/TMM.2020.2969793\nLiu, N., Zhang, F., and Duan, F. (2020). Facial age estimation using a multi-task\nnetwork combining classiﬁcation and regression. IEEE Access 8, 92441–92451. doi:\n10.1109/ACCESS.2020.2994322\nLiu, Y., Kong, A. W. K., and Goh, C. K. (2018). “A constrained deep neural network\nfor ordinal regression, ” inProceedings of the IEEE conference on computer vision and\npattern recognition, Salt Lake City, UT, 831–839.\nLiu, Y., Sun, G., Qiu, Y., Zhang, L., Chhatkuli, A., and Van Gool, L. (2021).\nTransformer in convolutional neural networks. arXiv [Preprint]. arXiv: 2106.03180.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021). “Swin\ntransformer: Hierarchical vision transformer using shifted windows, ” inProceedings of\nthe IEEE/CVF international conference on computer vision Montreal . Piscataway, NJ:\nIEEE , 10012–10022.\nLu, D., Wang, D., Zhang, K., and Zeng, X. (2022). Age estimation from facial images\nbased on Gabor feature fusion and the CIASO-SA algorithm. CAAI Trans. Intell.\nTechnol. doi: 10.1049/cit2.12084\nNiu, Z., Zhong, G., and Yu, H. (2021). A review on the attention mechanism of deep\nlearning. Neurocomputing 452, 48–62. doi: 10.1016/j.neucom.2021.03.091\nPan, H., Han, H., Shan, S., and Chen, X. (2018). “Mean-variance loss for deep age\nestimation from a face, ” inProceedings of the IEEE conference on computer vision and\npattern recognition, Salt Lake City, UT, 5285–5294. doi: 10.1109/CVPR.2018.00554\nPanis, G., Lanitis, A., Tsapatsoulis, N., and Cootes, T. F. (2016). Overview of research\non facial ageing using the FG-NET ageing database. IET Biometrics 5, 37–46.\nPei, W., Dibeklio ðlu, H., Baltrušaitis, T., and Tax, D. M. (2019). Attended end-to-\nend architecture for age estimation from facial expression videos. IEEE Trans. Image\nProcess. 29, 1972–1984. doi: 10.1109/TIP.2019.2948288\nPeng, Z., Huang, W., Gu, S., Xie, L., Wang, Y., Jiao, J., et al. (2021). “Conformer:\nLocal features coupling global representations for visual recognition, ” in Proceedings\nof the IEEE/CVF international conference on computer vision Montreal . Piscataway,\nNJ: IEEE, 367–376. doi: 10.1109/ICCV48922.2021.00042\nRanjan, R., Zhou, S., Cheng Chen, J., Kumar, A., Alavi, A., Patel, V. M., et al.\n(2015). “Unconstrained age estimation with deep convolutional neural networks, ”\nin Proceedings of the IEEE international conference on computer vision workshops ,\nSantiago, 109–117.\nRicanek, K., and Tesafaye, T. (2006). “Morph: A longitudinal image database of\nnormal adult age-progression, ” in Proceedings of the 7th international conference on\nautomatic face and gesture recognition (FGR06), Southampton, 341–345.\nRothe, R., Timofte, R., and Van Gool, L. (2016). “Some like it hot-visual guidance for\npreference prediction, ” inProceedings of the IEEE conference on computer vision and\npattern recognition Las Vegas . Piscataway, NJ: IEEE, 5553–5561. doi: 10.1109/CVPR.\n2016.599\nRothe, R., Timofte, R., and Van Gool, L. (2018). Deep expectation of real and\napparent age from a single image without facial landmarks. Int. J. Comput. Vision 126,\n144–157. doi: 10.1007/s11263-016-0940-3\nSharma, N., Sharma, R., and Jindal, N. (2022). Face-based age and gender estimation\nusing improved convolutional neural network approach. Wireless Pers. Commun. 124,\n3035–3054. doi: 10.1007/s11277-022-09501-8\nShen, W., Guo, Y., Wang, Y., Zhao, K., Wang, B., and Yuille, A. (2019). Deep\ndiﬀerentiable random forests for age estimation. IEEE Trans. Pattern Anal. Mach.\nIntell. 43, 404–419. doi: 10.1109/TPAMI.2019.2937294\nShen, W., Guo, Y., Wang, Y., Zhao, K., Wang, B., and Yuille, A. L. (2018).\n“Deep regression forests for age estimation, ” inProceedings of the IEEE conference on\ncomputer vision and pattern recognition, Salt Lake City, UT, 2304–2313. doi: 10.1109/\nCVPR.2018.00245\nSong, Z., Ni, B., Guo, D., Sim, T., and Y an, S. (2011). “Learning universal multi-\nview age estimator using video context, ” in Proceedings of the 2011 international\nconference on computer vision , Barcelona, 241–248. doi: 10.1109/ICCV.2011.\n6126248\nSunitha, G., Geetha, K., Neelakandan, S., Pundir, A. K. S., Hemalatha, S., and Kumar,\nV. (2022). Intelligent deep learning based ethnicity recognition and classiﬁcation using\nfacial images. Image Vision Comput. 121:104404. doi: 10.1016/j.imavis.2022.104404\nSzegedy, C., Ioﬀe, S., Vanhoucke, V., and Alemi, A. A. (2017). “Inception-v4,\ninception-resnet and the impact of residual connections on learning, ” in Proceedings\nof the 31st AAAI conference on artiﬁcial intelligence San Francisco, CA . Menlo Park:\nAAAI, 4278–4284.\nTaheri, S., and Toygar, Ö (2019). On the use of DAG-CNN architecture for age\nestimation with multi-stage features fusion. Neurocomputing 329, 300–310. doi: 10.\n1016/j.neucom.2018.10.071\nFrontiers in Neuroscience 13 frontiersin.org\nfnins-17-1136934 April 5, 2023 Time: 15:26 # 14\nShi et al. 10.3389/fnins.2023.1136934\nTan, Z., Wan, J., Lei, Z., Zhi, R., Guo, G., and Li, S. Z. (2017). Eﬃcient group-n\nencoding and decoding for facial age estimation. IEEE Trans. Pattern Anal. Mach.\nIntell. 40, 2610–2623. doi: 10.1109/TPAMI.2017.2779808\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” inProceedings of the 31st international conference\non neural information processing systems, Long Beach, CA: NIPS, 6000–6010.\nWang, H., Sanchez, V., and Li, C. T. (2022). Improving face-based age estimation\nwith attention-based dynamic patch fusion.IEEE Trans. Image Process.31, 1084–1096.\ndoi: 10.1109/TIP.2021.3139226\nWang, W., Shen, J., Yu, Y., and Ma, K. L. (2016). Stereoscopic thumbnail creation via\neﬃcient stereo saliency detection. IEEE Trans. Visual. Comput. Graph. 23, 2014–2027.\ndoi: 10.1109/TVCG.2016.2600594\nWang, W., Y ao, L., Chen, L., Cai, D., He, X., and Liu, W. (2021). Crossformer: A\nversatile vision transformer based on cross-scale attention. arXiv [Preprint]. arXiv-\n2108.\nWang, X., Girshick, R., Gupta, A., and He, K. (2018). “Non-local neural networks, ”\nin Proceedings of the IEEE conference on computer vision and pattern recognition Salt\nLake City. Piscataway, NJ: IEEE, 7794–7803. doi: 10.1109/CVPR.2018.00813\nWang, X., Guo, R., and Kambhamettu, C. (2015). “Deeply-learned feature for age\nestimation, ” in Proceedings of the 2015 IEEE winter conference on applications of\ncomputer vision, Waikoloa, HI, 534–541. doi: 10.1111/1556-4029.13798\nXia, M., Zhang, X., Weng, L., and Xu, Y. (2020). Multi-stage feature constraints\nlearning for age estimation. IEEE Trans. Inform. Forensics Secur. 15, 2417–2428.\nXiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., and Girshick, R. (2021).\nEarly convolutions help transformers see better. Adv. Neural Inform. Process. Syst. 34,\n30392–30400.\nXing, J., Li, K., Hu, W., Yuan, C., and Ling, H. (2017). Diagnosing deep learning\nmodels for high accuracy age estimation from a single image. Pattern Recogn. 66,\n106–116. doi: 10.1001/jamanetworkopen.2021.11176\nYi, D., Lei, Z., and Li, S. Z. (2014). “Age estimation by multi-scale convolutional\nnetwork, ” inProceedings of the Asian conference on computer vision , Cham: Springer,\n144–158.\nYi, T. (2022). Estimation of human age by features of face and eyes based on\nmultilevel feature convolutional neural network. J. Electron. Imaging 31:041208. doi:\n10.1117/1.JEI.31.4.041208\nYuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., and Wu, W. (2021). “Incorporating\nconvolution designs into visual transformers, ” in Proceedings of the IEEE/CVF\ninternational conference on computer vision Seattle. Piscataway, NJ: IEEE, 579–588.\ndoi: 10.1109/ICCV48922.2021.00062\nZeng, X., Huang, J., and Ding, C. (2020). Soft-ranking label encoding for robust\nfacial age estimation. IEEE Access 8, 134209–134218.\nZhang, B., and Bao, Y. (2022). Age estimation of faces in videos using head\npose estimation and convolutional neural networks. Sensors 22:4171. doi: 10.3390/\ns22114171\nZhang, C., Liu, S., Xu, X., and Zhu, C. (2019). “C3AE: Exploring the limits of\ncompact model for age estimation, ” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, Long Beach, CA, 12587–12596. doi: 10.1109/\nCVPR.2019.01287\nZhang, K., Gao, C., Guo, L., Sun, M., Yuan, X., Han, T. X., et al.\n(2017a). Age group and gender estimation in the wild with deep RoR\narchitecture. IEEE Access 5, 22492–22503. doi: 10.1109/ACCESS.2017.27\n61849\nZhang, K., Sun, M., Han, T. X., Yuan, X., Guo, L., and Liu, T. (2017b).\nResidual networks of residual networks: Multilevel residual networks. IEEE\nTrans. Circuits Syst. Video Technol. 28, 1303–1314. doi: 10.1109/TCSVT.2017.\n2654543\nZhang, Y., and Yeung, D. Y. (2010). “Multi-task warped Gaussian process for\npersonalized age estimation, ” in Proceedings of the 2010 IEEE computer society\nconference on computer vision and pattern recognition, San Francisco, CA, 2622–2629.\ndoi: 10.1109/CVPR.2010.5539975\nZhao, Q., Dong, J., Yu, H., and Chen, S. (2020). Distilling ordinal\nrelation and dark knowledge for facial age estimation. IEEE Trans.\nNeural Netw. Learn. Syst. 32, 3108–3121. doi: 10.1109/TNNLS.2020.3\n009523\nFrontiers in Neuroscience 14 frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7200836539268494
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6610977649688721
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6456506848335266
    },
    {
      "name": "Transformer",
      "score": 0.5947785973548889
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.48938173055648804
    },
    {
      "name": "Deep learning",
      "score": 0.46177351474761963
    },
    {
      "name": "Machine learning",
      "score": 0.3791426420211792
    },
    {
      "name": "Computer vision",
      "score": 0.32502472400665283
    },
    {
      "name": "Engineering",
      "score": 0.08672544360160828
    },
    {
      "name": "Voltage",
      "score": 0.08230796456336975
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I153473198",
      "name": "North China Electric Power University",
      "country": "CN"
    }
  ],
  "cited_by": 13
}