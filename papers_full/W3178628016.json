{
  "title": "Tracking Child Language Development With Neural Network Language Models",
  "url": "https://openalex.org/W3178628016",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2253488569",
      "name": "Kenji Sagae",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A2253488569",
      "name": "Kenji Sagae",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4239427658",
    "https://openalex.org/W2146198760",
    "https://openalex.org/W6756018736",
    "https://openalex.org/W4210984920",
    "https://openalex.org/W2144499799",
    "https://openalex.org/W3007928779",
    "https://openalex.org/W2046310237",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2301095666",
    "https://openalex.org/W2092875412",
    "https://openalex.org/W3018827121",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W6679166523",
    "https://openalex.org/W3087154144",
    "https://openalex.org/W3014415613",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W1975946202",
    "https://openalex.org/W3011439121",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2004774622",
    "https://openalex.org/W2166852192",
    "https://openalex.org/W6608525524",
    "https://openalex.org/W2148992292",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6683557909",
    "https://openalex.org/W1971747668",
    "https://openalex.org/W6742695268",
    "https://openalex.org/W3103536442",
    "https://openalex.org/W2733628661",
    "https://openalex.org/W1992119553",
    "https://openalex.org/W2912500072",
    "https://openalex.org/W2752392399"
  ],
  "abstract": "Recent work on the application of neural networks to language modeling has shown that models based on certain neural architectures can capture syntactic information from utterances and sentences even when not given an explicitly syntactic objective. We examine whether a fully data-driven model of language development that uses a recurrent neural network encoder for utterances can track how child language utterances change over the course of language development in a way that is comparable to what is achieved using established language assessment metrics that use language-specific information carefully designed by experts. Given only transcripts of child language utterances from the CHILDES Database and no pre-specified information about language, our model captures not just the structural characteristics of child language utterances, but how these structures reflect language development over time. We establish an evaluation methodology with which we can examine how well our model tracks language development compared to three known approaches: Mean Length of Utterance, the Developmental Sentence Score, and the Index of Productive Syntax. We discuss the applicability of our model to data-driven assessment of child language development, including how a fully data-driven approach supports the possibility of increased research in multilingual and cross-lingual issues.",
  "full_text": "ORIGINAL RESEARCH\npublished: 08 July 2021\ndoi: 10.3389/fpsyg.2021.674402\nFrontiers in Psychology | www.frontiersin.org 1 July 2021 | Volume 12 | Article 674402\nEdited by:\nBrian MacWhinney,\nCarnegie Mellon University,\nUnited States\nReviewed by:\nLisa Pearl,\nUniversity of California, Irvine,\nUnited States\nTal Linzen,\nNew York University, United States\n*Correspondence:\nKenji Sagae\nsagae@ucdavis.edu\nSpecialty section:\nThis article was submitted to\nLanguage Sciences,\na section of the journal\nFrontiers in Psychology\nReceived: 01 March 2021\nAccepted: 11 June 2021\nPublished: 08 July 2021\nCitation:\nSagae K (2021) Tracking Child\nLanguage Development With Neural\nNetwork Language Models.\nFront. Psychol. 12:674402.\ndoi: 10.3389/fpsyg.2021.674402\nTracking Child Language\nDevelopment With Neural Network\nLanguage Models\nKenji Sagae*\nDepartment of Linguistics, University of California, Davis, Davis, CA, United States\nRecent work on the application of neural networks to languag e modeling has shown\nthat models based on certain neural architectures can captu re syntactic information\nfrom utterances and sentences even when not given an explici tly syntactic objective. We\nexamine whether a fully data-driven model of language devel opment that uses a recurrent\nneural network encoder for utterances can track how child la nguage utterances change\nover the course of language development in a way that is compa rable to what is achieved\nusing established language assessment metrics that use lan guage-speciﬁc information\ncarefully designed by experts. Given only transcripts of ch ild language utterances\nfrom the CHILDES Database and no pre-speciﬁed information a bout language, our\nmodel captures not just the structural characteristics of c hild language utterances,\nbut how these structures reﬂect language development over t ime. We establish an\nevaluation methodology with which we can examine how well ou r model tracks language\ndevelopment compared to three known approaches: Mean Lengt h of Utterance, the\nDevelopmental Sentence Score, and the Index of Productive S yntax. We discuss the\napplicability of our model to data-driven assessment of chi ld language development,\nincluding how a fully data-driven approach supports the pos sibility of increased research\nin multilingual and cross-lingual issues.\nKeywords: child language assessment, natural language proc essing, computational linguistics, language model,\nIPSyn, neural network\nINTRODUCTION\nMeasuring the level of syntactic development in child languag e precisely is useful both in language\nresearch and in clinical settings. Although several metric s have been proposed to quantify progress\nin language development, such as the Index of Productive Synt ax (IPSyn;\nScarborough, 1990 ),\nthe Developmental Sentence Score (DSS; Lee and Canter, 1971 ), and the Language Assessment,\nRemediation and Screening Procedure (LARSP; Fletcher and Garman, 1988 ) the most widely used\nmetric remains the Mean Length of Utterance (MLU; Brown, 1973 ). Although less detailed than\nmany available alternatives, MLU is simple and fast to compute c onsistently, while metrics based\non identiﬁcation of speciﬁc language structures have tradit ionally required expert manual analysis.\nAdditionally, MLU use in many languages other than English is considerably more straightforward\nthan adaptation of metrics that rely on identiﬁcation of speci ﬁc lexical or grammatical items, and\nMLU is less susceptible to issues relating to diﬀerences among v arieties of the same language. While\nthere may seem to be inherent trade-oﬀs associated with the u se of approaches to tracking language\ndevelopment based on detailed language-speciﬁc structural a nalysis and based on superﬁcial\nSagae Tracking Language Development\nutterance characteristics, we investigate whether accura te\nmeasurements of language development can be made quickly,\nreliably and without reliance on analyses requiring lingui stic\nexpertise. Speciﬁcally, through the use of data and neural\nnetwork approaches to natural language processing, we aim to\ntrack language development in a way that is as ﬁne-grained as\ncan be obtained with carefully crafted language-speciﬁc met rics,\nbut as fast, reliable and widely applicable as with MLU. Our\npresent goal is not to create a new metric, but to examine\nwhether computational models built only from transcribed\nutterances can capture how child language utterances change\nthrough the course of language development at a ﬁne enough\nresolution to serve as a foundation for new ways to measure\nsyntactic development.\nWith the development of computational models for syntactic\nanalysis of child language utterances (\nSagae et al., 2004, 2010 ),\nautomatic accurate computation of syntax-based metrics of\nlanguage development became possible. Identifying the Index of\nProductive Syntax (IPSyn;\nScarborough, 1990) as a measurement\ntool that has been used widely in research but requires a\nsubstantial amount of manual analysis,\nSagae et al. (2005)\nproposed mapping the language structures targeted in IPSyn\ncomputation to patterns in parse trees generated by an automati c\nparser, eliminating manual eﬀort from the process of calculati ng\nIPSyn scores. This work provided initial evidence that autom atic\nIPSyn scoring was possible, and served as the basis for\nsubsequent work to make the concept practical, for example\nthrough CLAN-IPSyn (accessible at http://talkbank.org). T hese\neﬀorts have highlighted both the promise of more widespread\nand consistent assessment of syntactic development and the\ndiﬃculty in matching the quality of analyses produced by exper ts\n(\nMacWhinney et al., 2020; Roberts et al., 2020 ).\nScoring schemes originally intended for manual computation ,\nsuch as IPSyn, are designed partly to account for the strength s\nand limitations of human annotators, without regard for how to\nleverage syntactic analysis technology. Recognizing the di ﬀerent\nstrengths in manual and automatic syntactic analysis, Lubetich\nand Sagae (2014) examined the extent to which IPSyn-like\nscoring can be performed automatically without a pre-deﬁned\nlist of targeted syntactic structures, leaving it up to a dat a-\ndriven model to select the relevant structures in the output o f an\nautomatic syntactic parser. Their approach is to teach a machin e\nto reproduce IPSyn scores just by looking at automatically\ngenerated parse trees, with no information about how IPSyn\nscores are computed or what they mean. Starting from the\nassumption that these parse trees contain suﬃcient syntactic\ninformation to assess language development, ﬁguring out wha t\nstructures to focus on is left to the machine.\nThe ability demonstrated by this approach to produce\nscores that track language development almost as accurately\nas with IPSyn, but without the expertise that went into the\ndesign of IPSyn, raises the important question of whether\ncomputational models of language can learn to measure syntac tic\ndevelopment in children from only language data, without any\ngiven knowledge about the language acquisition process. Thi s\nquestion is not whether a computational model can perform\nthe steps necessary for IPSyn scoring, as in the work of\nSagae\net al. (2005) , or whether a computational model can learn IPSyn\nscoring from examples, as in the work of Lubetich and Sagae,\nbut whether a computational model derived from child languag e\nsamples alone can encode its own metric that tracks language\ndevelopment over time as accurately as, or even more accurate ly\nthan an expertly designed metric like IPSyn. In other words, i f the\ngoal is not to model an existing language development metric,\nbut to model the language itself and how it changes over time\nin individual children, will the resulting model encode a usa ble\nlanguage development metric? We investigate this question b y\ncreating such a model using neural networks. We base our\napproach on language modeling using a type of recurrent neural\nnetwork, but unlike typical language models used in natural\nlanguage processing that are trained to predict tokens in a str ing,\nwe additionally have our model sort child language samples\nchronologically during training. This sorting consists of sco ring\ndiﬀerent language samples produced at diﬀerent times such that\nthe score for the sample produced later is higher than the score\nfor the sample produced earlier. This is intended to require\nthe model to learn how utterances produced at diﬀerent stages\nof development diﬀer. Once the model is trained, it can be\nused to score a language sample, in the same way one would\nuse existing metrics like IPSyn or MLU. Unlike previous work\non automated assessment of child language development, this\nprocess does not use a syntactic parser or any information abou t\nhow to measure language development, such as existing metrics .\nAlthough we focus on English, our approach, which requires\nonly transcribed utterances, shares with MLU the advantage\nof not relying on language-speciﬁc resources or language-\nspeciﬁc expertise, while having a substantially greater reso lution,\ncomparable to that achieved with IPSyn. Using North American\nEnglish data from the CHILDES database (\nMacWhinney, 2000),\nwe show that our neural language model successfully discove rs\nhow to score child language samples according to language\ndevelopment more accurately than existing implementations\nof MLU and automated IPSyn scoring. This result suggests\nthat neural network language models are capable of encoding\nhow syntactic development in progresses in English-speaking\nchildren, and creates promising directions for accurate dat a-\ndriven measurement of language development.\nMATERIALS AND METHODS\nOur experiments involve a speciﬁc kind of language model based\non a type of recurrent neural network, more speciﬁcally the\nLong Short-Term Memory network, or LSTM (\nHochreiter and\nSchmidhuber, 1997 ). The model is trained using longitudinal\nchild language data from the CHILDES Database. We ﬁrst\ndescribe the neural network model, and present details about the\ndata used. We then describe how the model was trained and how\nour experiments were conducted.\nBackground: Recurrent Language\nModeling\nOur approach assumes an LSTM language model (\nSundermeyer\net al., 2015 ), which is a kind of recurrent neural network language\nFrontiers in Psychology | www.frontiersin.org 2 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nmodel. This kind of neural language model has been applied\nsuccessfully in various settings in natural language proces sing.\nWe provide here only a brief overview of recurrent neural\nlanguage models to facilitate discussion of our neural netw ork\nmodel for language development. For a detail description of\nLSTM language models, see\nSundermeyer et al. (2015) .\nThe general language model formulation commonly assumed\nin natural language processing and computational linguistic s is\nbased on word predictions. Speciﬁcally, the model is designed to\nestimate the probability of strings in the language as the prod uct\nof the conditional probabilities of each word in the string gi ven\nthe preceding words. Essentially, the model predicts words in a\nstring (a sentence or an utterance) from previous words:\nP ( S) = P ( t1t2 . . .tN ) =\nN∏\ni= 1\nP(ti|t0 . . .ti− 1)\nHere, the probability of the string P(S) is the probability of\nthe word sequence (or token sequence) t1t2 . . .tN of length\nN − 1, to which special tokens representing the beginning of\nsentence (BOS) and end of sentence (EOS) have been prepended\nand appended, respectively, making t0 BOS, and tN EOS. The\nprobability of this sequence is the product of the probability o f\neach word ti given the preceding words t0 . . .ti− 1. Notice that the\nproduct above does not include the probability of t0; since, by\nhow we deﬁned our strings, every string starts with the specia l\ntoken BOS, its probability is 1 and does not aﬀect the product.\nThe probability of the special token EOS, on the other hand, is\nthe probability of ending the string (i.e., ending the uttera nce or\nsentence) given all the previous words.\nIn a language model implemented using neural networks,\nor a neural language model, these word predictions are made\nbased on spreading activation according to parameters of the\nneural network. In perhaps its simplest form, where the sequenc e\nt0 . . .ti− 1 is approximated according to a ﬁrst-order Markov\nassumption as simply ti− 1, resulting in a kind of model known\nas the bigram model, a simple feedforward network takes ti− 1 as\ninput and produces ti as output, as illustrated in Figure 1, where\nthe token ti− 1 is represented by a value of 1 in a speciﬁc node\nin the input layer, while the other nodes have value zero, and\nthe output is the node with highest value in the output layer.\nNotice that the network is made of units organized in layers,\nand the input word corresponds to a single unit in the input\nlayer. Activation from the input layer spreads to the ﬁrst hidd en\nlayer (the embedding layer), and from there to the second hid den\nlayer, and from there to the output layer, where the unit with\nhighest activation is chosen as the network’s prediction. Th e ﬁrst\nhidden layer is often referred to as the embedding layer, and in a\ntrained neural language models it is known to encode meaning ful\nrepresentations of words. Although only two hidden layers ar e\nshown (including the embedding layer), the use of more hidde n\nlayers is common. In a feedforward network, activation sprea ds\nin one direction, from input to output. A unit’s activation is a\nfunction of the sum of the incoming activation for that unit.\nThe parameters that the model learns from data are the weights\nthat are applied to the connections between units of the networ k.\nTypically, the parameters of this kind of network are initiali zed\nrandomly. Among other things, this means that each word in\nthe vocabulary of the model is initialized to be represented b y a\nrandom embedding. Over the course of training, where weight s\nare adjusted gradually to increase the probability of predicti ng\nthe correct output word, the weights learned in the embedding\nand hidden layers have been found to encode representations o f\nthe input and the task that improve prediction of the output.\nFor example, word representations in the embedding layer form\na meaningful multidimension space that encodes semantic and\nsyntactic relationships among words (\nTurian et al., 2010; Mikolov\net al., 2013 ). Intuitively, when learning to predict what word\nfollows chairs, the network learns that chairs is the plural of chair,\nthat chairs is related to seat, etc.\nIn a recurrent neural network, the input is a sequence, and\neach symbol in the sequence is presented to the network one at\na time in consecutive time-steps. In the ﬁrst time-step, the ﬁrs t\nsymbol is presented, in the second time-step, the second symbo l\nis presented, and so on. In a recurrent neural language model, the\ninput sequence is the string, and the symbols that make up the\nstring are the words, or tokens. The intuitive diﬀerence betw een\nthe feedforward network described in the previous paragraph an d\na recurrent network is that hidden units in a recurrent netwo rk\nreceive activation not just from lower layers, but also from\nhidden units in the previous time-step. In recurrent language\nmodels, the hidden layers are recurrent, with the exception of the\nembedding layer, which is not recurrent. The term hidden lay er\nis then understood not to include the embedding layer, which is\ncommonly referred to as simply the embedding layer. The resul t\nof the recurrence in the network is that, as the string is proce ssed\nword by word one step at a time, the hidden representation from\nwhich the output prediction for word ti is made is inﬂuenced\nby its preceding words t0 . . .ti− 1. A simple recurrent network is\nillustrated in Figure 2. In Figure 3, we show the same network\nunrolled in L time steps, where L is the length of the input\nsequence. LSTM language models are recurrent language mode ls\ndesigned to address speciﬁc shortcomings of simple recurrent\nneural networks. A discussion of these shortcomings and the way\nin which LSTMs address them are beyond the scope of this brief\noverview of recurrent neural language models, but are discus sed\nin detail by\nGoldberg (2017, chapter 15).\nOne insight about recurrent language models, such as\nLSTM language models, that is important in understanding\nour neural network model of child language development is\nthat models with enough hidden units trained with enough\ndata have been found to encode syntactic structure in their\nhidden representations (\nFutrell and Levy, 2019; Linzen and\nBaroni, 2021 ). Just as the word embedding that result from\ntraining neural language models come to encode detailed wor d\nrepresentations over the course of training with large corpor a\nbecause the network learns that such representations are usef ul\nin the next-word prediction task, the representations in the\nhidden layers of a recurrent language model encode syntacti c\nstructure because ultimately syntax is important in the next -word\nprediction task. Intuitively, knowledge of the grammar of the\nlanguage is necessary to complete or continue sentences. Giv en\nenough data and enough parameters, a recurrent language mode l\ntrained using backpropagation discovers and encodes syntact ic\nFrontiers in Psychology | www.frontiersin.org 3 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nFIGURE 1 | A simple feedforward neural network with an input layer, an e mbedding layer, a hidden layer, and an output layer. This net work can implement a bigram\nlanguage model with units in the input and output layers repr esenting different words in a vocabulary. When a word is acti ve in the input layer, the unit with highest\nactivation in the output layer is the model’s prediction for the next word.\ninformation about the language in its hidden layers. Althou gh\nthe syntactic information encoded by neural language models is\nnot always represented in a way that is readily understandable ,\ntext generated randomly from large neural language models is\nsurprisingly grammatical and complex, conﬁrming that these\nmodels must capture the syntax of the language. Additionally ,\nthese language models have been found to be directly useful\nin tasks explicitly about syntactic structure (\nKiperwasser and\nGoldberg, 2016 ). Decoding the syntactic and other structure\ninformation encoded in neural language models in the context\nof our current understanding of linguistics is currently a t opic of\nactive research (\nLinzen et al., 2016; Futrell and Levy, 2019; McCoy\net al., 2020; Linzen and Baroni, 2021 ).\nA Neural Network Model of Child\nLanguage Development\nOur model of child language development is based on the\nsimple assumptions that language is acquired over time and\ndevelopment is monotonic. It is intended to pick up on what\nchanges in utterances through language development, and not to\nreﬂect cognitive mechanisms. Monotonicity here does not mea n\nthat the child’s language is always increasingly more simila r to\nsome ultimate form, and it does not mean that development\nprogresses linearly, but simply that typical development does\nnot regress. In other words, the assumption is that given two\nappropriately sized language samples (lists of utterances) from\nthe same child collected at diﬀerent times during language\ndevelopment, it should be possible for a model to distinguish\nbetween the earlier and later samples. The key idea is that\nif a model can sort these language samples chronologically,\nit must do so by ﬁguring out what changes in the language\nover time. Since recurrent neural language models encode so me\ninformation about syntax, they are a promising way to encode\nthe language samples to be compared and sorted. Importantly,\nthe goal is to have one model that makes accurate predictions\nacross diﬀerent children. Even though some children may lear n\ncertain things at diﬀerent rates and at diﬀerent ages, the mode l\nmust be able to sort the language samples for a new individual\nchild it has never encountered before. Although the idea of\nordering language samples is the key for how we intend to capture\nchanges in language over time, the model is ultimately inten ded\nto score individual language samples, in the same way one would\nscore a language sample using an instrument such as IPSyn.\nWe design our model to score individual language samples, but\nFrontiers in Psychology | www.frontiersin.org 4 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nFIGURE 2 | A simple recurrent neural network. The hidden layer receive s\nactivation from the embedding layer and from the hidden laye r in the previous\ntime step.\ntrain it, or learn the neural network parameters from data, by\nrepeatedly choosing a pair of language samples, scoring each\nsample individually, and adjusting the model’s parameters to\nmake it more likely that the sample originally produced at a late r\ntime receives a higher score.\nOur neural model of child language development can be\nthought of as being composed of two modules, which together\ncan assign a score to a language sample containing a certain\nnumber of utterances from a child. The ﬁrst module consists\nprimarily of an LSTM language model, or more precisely a\nBidirectional LSTM (BiLSTM) encoder (\nGraves, 2012 ), which is\nused to encode utterances into a vector representation. Give n a\nlanguage sample composed of a certain number of utterances,\nthe LSTM language model encodes each utterance simply by\nprocessing the utterance one word at a time. Recall that every\nutterance ends with a special EOS token. It is the activation o f\nthe topmost hidden layer of our model at the last time step,\nwhich corresponds to having the EOS token as input, that we\nuse as (half of) the representation of the sentence. This speci ﬁc\nrepresentation is chosen because it is the result of the model\nhaving processed all of the words in the utterance, and the\nrecurrent nature of the model makes it possible, in principle, f or\ninformation about the entire utterance to be captured at this last\ntime step. A common practice when encoding strings with an\nLSTM network is to repeat the process on the reversed string\nwith separate parameters, resulting in a bidirectional model . The\nstring is then encoded forwards and backwards. In the forwar d\npass, the hidden representation for the EOS token is used as half\nof the representation for the sentence. In the backward pass, t he\nhidden representation for the BOS token gives us the other hal f of\nthe representation for the sentence. These two halves are sim ply\nconcatenated. Once representations for individual utteran ces are\ncomputed, a single representation for the entire language sampl e\ncomposed of these individual utterances is simply the average of\nthe representations of the individual utterances. Each utte rance\nrepresentation is a vector, and the representation for the ent ire\nlanguage sample is taken to be the average vector of all utteran ce\nvectors in the sample.\nOnce a vector representation for a language sample is\ncomputed using the encoder module containing the BiLSTM\nlanguage model, the second module of the model derives a\nnumerical score from the representation of the language sampl e.\nThe scores assigned to language samples from a single child ar e\nmeant to increase according to the chronological order of th e\nlanguage samples. In other words, the score corresponding to\na set of utterances produced by a child of age 3;00 should be\ngreater than the score assigned to a set of utterances produce d\nby the same child at age 2;06. The module that assigns the\nscore to a language sample given its representation consists of\na feedforward network that has one hidden layer and a single\noutput unit. The input to this module is the representation\nobtained with the ﬁrst module, and the activation of the outpu t\nunit is the score for the sample. Figure 4 shows our model, with\ninput consisting of several utterances, which are each encod ed to\ncreate a representation for the entire set of utterances (labe led as\nLanguage Sample Vector), from which a score is computed.\nWith the two modules that together encode a list of utterances\nand produce a language development score, the remaining\nquestions are how to make the encoder module focus on how\nthe grammar of utterances change over the course of language\ndevelopment, and how to make the score produced by the\nsecond module track language development based on what\nthe ﬁrst module encodes. These two questions are addressed\njointly through end-to-end training of the model. An importa nt\ndistinction between our utterance encoder and a typical neur al\nlanguage model trained as described above using a word\nprediction objective is that our encoder is trained using the\nlanguage sample sorting task directly. When a typical language\nmodel is trained without a speciﬁc task as an objective, it lea rns\nfrom its training strings what it needs for its word predictio n\ntask. The same network architecture can also be trained on\ntasks that are not word prediction tasks. In the word prediction\ncase, the error signal that is used to adjust the weights of the\nnetwork comes directly from the model predicting a diﬀerent\nword from what was observed in a speciﬁc position in a training\nstring. In our case, an error signal is obtained when the lang uage\nmodel has been used to encode the utterances in two distinct\nFrontiers in Psychology | www.frontiersin.org 5 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nFIGURE 3 | A simple recurrent neural network unrolled for L time steps, where L is the length of the input string.\nFIGURE 4 | Our model for encoding and scoring language samples compose d of utterances. Each utterance is encoded by a Bidirectiona l LSTM network. Utterance\nrepresentations consisting of the concatenations of the ﬁrs t and last tokens are averaged into a vector that represents t he entire language sample. From this language\nsample vector, a score is computed for the entire language sa mple.\nlanguage samples, the scorer module assigns scores for these\ntwo language samples, and the sorted order of the scores does\nnot correspond to the chronological order of the samples. In\nsuch a case, the error is propagated through the entire network\nso that weights can be updated in a way that is speciﬁc to\nthe task.\nFrontiers in Psychology | www.frontiersin.org 6 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nTraining the model requires longitudinal language data\nfrom multiple children. For each child, 100-utterance langu age\nsamples are organized chronologically. Parameters (networ k\nweights) for both modules are initialized randomly. During\ntraining, the model is presented with data from each child 20\ntimes, and each time a maximum of 100 samples are chosen\nrandomly from the samples from that child. Every language\nsample is encoded with the ﬁrst module and scored with the\nsecond module. Within the set of 100 randomly chosen samples,\nevery sample is paired with every other sample to create 100 ×\n99 training pairs, each composed of two samples. In each sample,\nthe chronological order is known. The scores for the two sample s\nare then compared. The model’s training objective is to make\nsure chronologically later samples have higher scores than e arlier\nsamples. The number of times data for each child is presented\n(20), and the number of samples chosen randomly for each\nchild (100) are meta-parameters of the model. The model’s meta-\nparameters and meta-parameter tuning process are described in\nthe next section.\nThe model is trained end-to-end, with a pair of language\nsamples being provided to the model, each sample being scored\nand compared, and parameters across the entire model being\nadjusted in response to errors. This means that the two modules\nare trained together and inﬂuence each other. With weight\nupdates (parameter learning) in neural networks being error-\ndriven, each time an incorrect prediction is made (i.e., the\nmodel fails to predict the chronological order the samples),\nthe error is propagated from the ultimate prediction, down\nto the representation of the average of the utterances in\neach sample, down to the representations of each individual\nutterance produced by the language model encoder, and all of\nthe parameters in the entire model are updated to make the\ncorrect prediction more likely. Over the process of training t he\nentire model, the language model learns to prefer encodings\nof the utterances that will make the chronological ordering\ntask more accurate. As a result, the encoder learns to model\nlanguage development by focusing on the diﬀerences in the\nrepresentations of the two language samples from diﬀerent time s.\nBecause the training material consists of data from multiple\nchildren, the model prefers patterns that apply generally, and no t\nto individual children.\nIntuitively, one can imagine a very patient intelligent enti ty\nwith limited memory and no knowledge of grammar, but high\nsensitivity to details, looking at two sets of utterances pro duced by\nthe same child several months apart. This entity knows what se t\nof utterances was produced later, and starts to look for patter ns\nthat could be used to determine the chronological order of the\nsamples. Initially, the presence of individual words or seque nces\nof words might seem promising, but when presented with a long\nlist of pairs of languages samples, this entity notices that ce rtain\nstructural patterns are more predictive of chronological orde r. If\nit is really structural patterns that are most predictive of or der of\nthe samples, over many passes over many pairs of samples, the\nentity will learn a list of what patterns to look for, and how to\nweigh these patterns against other patterns. This list might e nd up\nbeing similar in many ways to the list of structures used in me trics\nlike IPSyn. This is approximately what motivates our model.\nFinally, to prevent the model from picking up on diﬀerences\nin the topics discussed at diﬀerent ages or the diﬀerences in\nvocabulary, we use the morphosyntactic tags (\nMacWhinney,\n2000) from US English CHILDES transcripts instead of the\nsurface word forms as the tokens in our model. These tags\ndiﬀerentiate between parts-of-speech such as nouns, verbs,\nadjectives, adverbs, prepositions, pronouns, etc. Experiments\nusing the observable surface forms (the words themselves)\nproduced very similar results.\nImplementation Details\nThe encoder in our model is a BiLSTM with a 50-unit embedding\nlayer and seven hidden layers, each with 200 units for each\ndirection (forward and backward). To encode a language sampl e,\nthe BiLSTM encoder produces encodings for each utterance as\na vector of 400 dimensions resulting from the concatenation of\nthe topmost hidden layer for each direction at the last time st ep\n(i.e., the 200-dimensional vector obtained after processin g the\nEOS token in the forward direction, and the 200-dimensional\nvector obtained after processing the BOS token in the backwar d\ndirection). We chose the size of the language samples to be 100\nutterances, motivated partly by the size of the language sample s\nused to computer IPSyn scores.\nThe scoring module is a feedforward network with one\nhidden layers of 200 units, and a single output unit. It takes\nthe representation of a language sample as a vector of 400\ndimensions and produces a real-valued score. The ranking tas k\nused to train the network involves encoding and scoring two\nlanguage samples, and comparing the resulting scores for each\nlanguage sample.\nThe network is trained end-to-end for 20 epochs, and the data\nfor each child in the training dataset is observed once per epoc h.\nThe number of epochs was chosen by observing performance\non a small part of the available training data that was used as\nheld out or validation data after each epoch. Changes in resul ts\nafter 15 epochs as small, and no signiﬁcant improvement was\nobserved during meta-parameter tuning after 20 epochs. The\nnumber of hidden layers, hidden units and embedding dimensi on\nwas similarly tuned by using a small held out portion of the\ntraining set as a validation set. The meta-parameters of the m odel\nwere not tuned exhaustively, and it may not be the optimal\nvalues. Parameters of the model were optimized using the Adam\noptimizer (\nKingma and Ba, 2015 ) using a learning rate of 1e-05\nand the margin ranking loss function:\nloss\n(\nxA, xB, y\n)\n= max(0, − y ( xA − xB) )\nHere, xA is the score for language sample A, xB is the score\nfor language sample B, and y is + 1 if A comes before B\nchronologically and − 1 if B comes before A chronologically.\nWhen the model’s predictions for the scores of the two samples\norder the samples correctly, the loss is zero. Otherwise, the l oss\nis greater than one, and the value is used in parameter updates to\nreduce loss.\nData\nTo train and evaluate our model, we used data from the\nCHILDES Database. Training our model requires longitudina l\nFrontiers in Psychology | www.frontiersin.org 7 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\ndata from multiple children, and we included in our dataset\nutterances from corpora that contained transcripts collecte d\nfrom the same child at least 6 months apart. Additionally, we\nincluded only corpora from which we could extract at least\n75 language samples containing 100 complete utterances not\nincluding repetitions, and for which we could determine the a ge\nof the child in months. Having a certain number of language\nsamples per child ensures that the data will be useful to the mod el\nduring training, and that we reduce the amount of noise in our\nevaluation. While it is possible that corpora with fewer than 7 5\nsamples would also be useful, we found there were a suﬃcient\nnumber of corpora that ﬁt our criteria. Data from other childr en\nthat did not ﬁt our criteria was used as development data and\nin the process of meta-parameter tuning. The corpora and 16\nchildren included in the ﬁnal dataset are:\n• Braunwald: Laura\n• Brown: Adam, Eve, Sarah\n• Clark: Shem\n• Demetras1: Trevor\n• Kuczaj: Abe\n• MacWhinney: Ross\n• Sachs: Naomi\n• Snow: Nathaniel\n• Suppes: Nina\n• Weist: Benjamin, Emily, Jillian, Matt, Roman.\nThe transcripts for each child were split into samples of 100\nutterances each, and the child age corresponding to each\nsample was recorded to determine the reference ordering duri ng\ntraining and evaluation. From each transcript in CHAT format\n(\nMacWhinney, 2000 ), we used the %mor line, containing part-\nof-speech and morphological analysis for each utterance. To\nconduct experiments excluding word forms to avoid having our\nmodel capture the eﬀect of topic in ordering samples, we simply\nused the most basic form of each lexical item’s tag (e.g. n for\nnouns, v for verbs, adj for adjectives, etc.), excluding the base\nform of words and morphological information.\nExperiments\nTo investigate the extent to which our model can capture\ninformation about language development, we implemented\nour model using PyTorch (http://pytorch.org) and used the\ndataset described in the previous section for training and\nevaluation. All computation was performed on a workstation\nwith two 8-core Xeon processors, 256 Gb of RAM and an\nNvidia Titan X GPU. We compared the ability of our model\nto track language development chronologically with how well\nthree baseline metrics perform the same tasks. Our baselines\nare the Mean Length of Utterance (MLU;\nBrown, 1973 ), the\nDevelopmental Sentence Score (DSS; Lee and Canter, 1971 )\nand the Index of Productive Syntax (IPSyn; Scarborough, 1990 ).\nMLU, DSS and IPSyn scores were obtained for all language\nsamples used in our experiment using the implementations\navailable in the CLAN tools for language analysis (\nMacWhinney,\n2000). These baselines are meant to represent what can be\nobtained with a straightforward approach that does not requir e\nstructural analysis of language samples (MLU), and more precise\nassessment instruments that were designed based on ﬁne-gra ined\nlanguage-speciﬁc knowledge that require linguistic analysi s (DSS\nand IPSyn).\nWhile scores for MLU, DSS, and IPSyn were obtained simply\nby running the available tools on each of the language samples , to\nobtain scores for our model we used our dataset in a leave-one-\nchild-out cross-validation scheme. This means that with a d ataset\nincluding data for 16 children, we trained 16 diﬀerent models,\neach excluding all data from one child. Transcripts for each o f the\n16 children were then scored using a model that was trained wit h\nno data for that speciﬁc child. To score transcripts from child ren\noutside of our dataset, we would simply train a single model usin g\ndata for all 16 children in our dataset. Our leave-one-child -out\ncross-validation allows us to estimate how the model perform s on\nunseen children by each time training with data from 15 child ren\nand scoring transcripts from a child excluded from the model.\nUnlike in previous work to automate measurement of\nsyntactic development (\nSagae et al., 2005; Hassanali et al.,\n2014; MacWhinney et al., 2020 ) or to obtain a data-driven\napproximation to an existing metric ( Lubetich and Sagae, 2014 ),\nthe target for the scores in our model is not simply another\nvalue that can be derived for each transcript, such as an IPSyn\nscore or age in months. Since the goal of our model is to\ntrack development over time and assign scores that reﬂect the\nchronological order of language samples for a child, we evalua te\nour model and compare it to baselines based on this task direct ly.\nFor each child, we compute the Spearman rank correlation\ncoeﬃcient between the scores for each language sample and the\nchild’s age in whole months corresponding to each language\nsample. The Spearman coeﬃcient, or Spearman’s ρ , ranges from\n− 1 to + 1 and reﬂects the strength of the correlation between\ntwo rankings. Our reference ranking is the age in months. A\nperfect Spearman rank correlation of + 1 would indicate that the\nscores assigned by our model perfectly sort the language sampl es\nchronologically. A Spearman rank correlation of zero would\nindicate that there is no correlation between the order deri ved\nfrom the scores of our model and chronological order. The\nstronger the correlation, the better suited for tracking la nguage\ndevelopment we consider a metric to be.\nRESULTS\nWe compute Spearman coeﬃcients for each child between age\nand MLU, age and DSS scores, age and IPSyn scores, and age\nand the scores assigned to transcripts by our model. Since we\ncompare these coeﬃcients to each other directly, we obtain a\nbootstrapped error estimate for each coeﬃcient by resampling\nthe set of transcripts used to compute the Spearman coeﬃcient\n10,000 times. Table 1 shows the results obtained for each of the 16\nchildren using MLU, DSS, IPSyn, and our neural network model.\nFor the convenience of having a single value that represents h ow\nwell each of these metrics correlate with language developme nt\nover time, we also provide the average values of all children pe r\nmetric. However, we caution that the meaning of such an avera ge\nvalue may not be straightforward to interpret in isolation, a nd\nespecially across diﬀerent datasets. Since the set of transcr ipts for\nFrontiers in Psychology | www.frontiersin.org 8 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\neach child contains transcripts from a diﬀerent range of ages, it\nis expected that the rank coeﬃcient from some children will be\nhigher than for others. Intuitively, it is easier for any of t hese\nmetrics to rank two samples 2 years apart than it is to rank two\nsamples 2 months apart. Therefore, these scores are meant to be\ninterpreted in relation to each other. For example, we would no t\nclaim that IPSyn scores have an average rank correlation of 0 .77\nwith age, and rather that the average rank correlation is 0.7 7 for\nthis speciﬁc dataset.\nThe results in Table 1 show that, while MLU is an eﬀective\napproach to approximate the level of language development over\ntime across a variety of children, both DSS and IPSyn perform\nbetter, as expected. The average Spearman coeﬃcient between\nage and MLU is 0.662, the lowest correlation between age and\na tested metric. The coeﬃcients for DSS and IPSyn are very clo se,\n0.763 and 0.770, respectively. The average Spearman coeﬃcien t\nbetween age and our model is 0.807. The scores obtained with\nour model correlate with age to a higher degree than DSS or\nIPSyn scores do in this dataset, but this is likely due at leas t in\npart to the fact that the model was tuned with these transcripts\nin mind. Although meta-parameter tuning was performed based\non results obtained using transcripts from children not used in\nour evaluation, various factors such as the number of units a nd\nlayers in the network and the learning rate were inﬂuenced by\nobserving the training process itself, even if separate valid ation\ntranscripts were used. Still, our results indicate that our m odel,\nwhich uses no pre-speciﬁed language-speciﬁc knowledge and\nlearns its parameters entirely from transcripts, performs on\npar with metrics designed by experts to capture language-\nspeciﬁc phenomena. This is a signiﬁcant result in that the\nmodel derives all of its knowledge of the language and of the\ntask from the training dataset consisting of utterance sets from\nvarious children.\nIn Table 1, we observe that for some children, there is a very\nstrong correlation between age and all of the diﬀerent scorin g\napproaches we used. For example, among the three children in\nthe\nBrown (1973) corpus, all rank correlation coeﬃcients are\nabove 0.9, with the single exception of DSS for one of three\nchildren (Adam). The rank correlation between MLU and age is\nstrongest for the children in this corpus, perhaps not surprisin gly\ngiven the role of these data in establishing MLU as an eﬀective\nmetric. On the other hand, the correlation between MLU and age\nis weakest in the four children in the Weist corpus (\nWeist and\nZevenbergen, 2008 ). Among the other metrics, only our model\noutperforms MLU across all four children in this set, although\nthe age correlation of our model for one of the children (Emil y,\n0.432) is substantially below the age correlation values fo r DSS\n(0.643) and IPSyn (0.629).\nDISCUSSION\nToward Data-Driven Metrics for Language\nDevelopment\nThe use of automated methods for computation of ﬁne-grained\nlanguage development scores that take syntactic structure i nto\naccount is a promising application of current natural language\nprocessing techniques. Despite some success in the application\nof automatic syntactic analysis to this task (\nSagae et al., 2005;\nHassanali et al., 2014; Lubetich and Sagae, 2014 ), these past eﬀorts\nserved more to demonstrate feasibility than to provide practi cal\ntools that can be used routinely in a variety of research situ ations.\nRoberts et al.’s (2020) recent eﬀort to perform an independent\nevaluation of an implementation of automatic IPSyn scoring,\nand the subsequent eﬀort to improve automatic scoring based\non that evaluation (\nMacWhinney et al., 2020 ) highlight the\namount of care and engineering eﬀort required to make reliabl e\nautomatic scoring widely available. The very small number o f\nlanguages for which a detailed metric such as IPSyn is availa ble\nfurther stresses the scale of the larger task of making resou rces\navailable for language development research in various lang uages,\nallowing for both greater depth of language-speciﬁc ﬁndings a nd\ncross-lingual research. We present a diﬀerent way to approach\nthis situation through data. While our current goal is not to\nprovide a new metric for English or any speciﬁc language,\nwe show that current neural network language modeling is\ncapable of capturing some aspects of the language development\nprocess to the extent necessary to track language development\nin individual children at a level of precision substantially g reater\nthan with MLU and comparable to that obtained with a detailed\nlanguage-speciﬁc metric such as IPSyn. Our results can serve as\nthe foundation for data-driven metrics in diﬀerent language s,\nrequiring only longitudinal data in the form of transcripts.\nOnce the model is trained, it can be used to score a language\nsample from a new child by ﬁrst encoding the utterances using\nthe BiLSTM language model, and scoring the resulting using\nthe feed-forward network. Unlike the training process, which\nrequires several passes through a sizable collection of tran scripts,\nscoring new language samples can be done seemingly instantly\nwith a current consumer-grade general-purpose computer. The\namount of computation required for scoring a language sample\nis greater than what would take to obtain the MLU score for\nthe same language sample, but it is comparable to the amount\nof computation required for automatic IPSyn scoring.\nThe use of language samples from multiple children during\ntraining results in a model that produces scores that are not\nspeciﬁc to any one child and are comparable across children.\nSince training consists of repeated attempts to predict the ord er\nof language sample pairs from diﬀerent children using a single\nscoring model, these scores can be used to compare the level\nof development of a child to that of another child, or to a\nmean value for a group, in a similar way to how MLU or IPSyn\nscores are used. However, unlike MLU and IPSyn, which operate\non known scales deﬁned explicitly, the scores from our data\ndriven model are dependent on the dataset used for training.\nWith the model as described above, there is not even a pre-\ndeﬁned range for the scores produced by the model. In fact,\nscores from models trained with diﬀerent datasets may not be\ndirectly comparable numerically. To keep the scores of a pract ical\nlanguage development metric that uses our approach and a\nspeciﬁc dataset within a pre-deﬁned range, a sigmoid function\ncan be applied to the value produced by the scoring module.\nThe results in Table 1 provide a strong indication that neural\nnetwork language models trained with longitudinal data can\nFrontiers in Psychology | www.frontiersin.org 9 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nTABLE 1 | Spearman rank correlation coefﬁcients between age in months and four language development scores for the 16 children in o ur dataset.\nCorpus: Child MLU DSS IPSyn Our model\nBraunwald: Laura 0.732 ± 0.001 0.794 ± 0.001 0.867 ± 0.001 0.888 ± 0.001\nBrown: Adam 0.942 ± 0.000 0.739 ± 0.001 0.906 ± 0.001 0.964 ± 0.000\nBrown: Eve 0.976 ± 0.001 0.958 ± 0.000 1.000 ± 0.000 1.000 ± 0.000\nBrown: Sarah 0.935 ± 0.000 0.953 ± 0.000 0.966 ± 0.000 0.959 ± 0.000\nClark: Shem 0.842 ± 0.002 0.855 ± 0.001 0.936 ± 0.001 0.889 ± 0.001\nDemetras: Trevor 0.618 ± 0.003 0.567 ± 0.002 0.609 ± 0.003 0.727 ± 0.003\nKuczaj: Abe 0.855 ± 0.001 0.804 ± 0.002 0.943 ± 0.001 0.801 ± 0.001\nMacWhinney: Ross 0.610 ± 0.002 0.588 ± 0.002 0.458 ± 0.002 0.604 ± 0.002\nSachs: Naomi 0.732 ± 0.002 0.869 ± 0.001 0.933 ± 0.001 0.92 ± 0.001\nSnow: Nathaniel 0.190 ± 0.004 0.892 ± 0.001 0.905 ± 0.001 0.881 ± 0.001\nSuppes: Nina 0.896 ± 0.001 0.896 ± 0.001 0.896 ± 0.001 0.974 ± 0.001\nWeist: Benjamin 0.607 ± 0.004 0.927 ± 0.000 0.964 ± 0.001 1.000 ± 0.000\nWeist: Emily 0.336 ± 0.003 0.643 ± 0.002 0.629 ± 0.002 0.432 ± 0.003\nWeist: Jillian 0.321 ± 0.005 0.243 ± 0.005 0.126 ± 0.006 0.657 ± 0.003\nWeist: Matt 0.685 ± 0.002 0.741 ± 0.001 0.622 ± 0.002 0.713 ± 0.001\nWeist: Roman 0.311 ± 0.003 0.735 ± 0.003 0.566 ± 0.002 0.509 ± 0.002\nAverage 0.662 0.763 0.770 0.807\nThe four language development scores include our data-driven approachand three baselines: Mean Length of Utterance (MLU), the Developmental Sentence Score (DSS), and the\nIndex of Productive Syntax (IPSyn).\ncapture structures relevant to the measurement of language\ndevelopment. In addition to adding to the growing body of\nknowledge related to whether and how neural networks can\nderive syntactic structure from text alone, our work also poi nts\nto an area of application of this apparent ability of recurrent\nnetworks to model language structure. However, our experime nt\ninvolved data for only 16 children, and much work still needs to\nbe done toward a usable metric or a set of metrics for various\nlanguages. Further validation of our approach through extrin sic\nmethods, such as verifying that previous research results ob tained\nwith IPSyn, LARSP , or DSS scores can be replicated with scores\nobtained from our fully data-driven model, would be needed to\nexamine the potential practical utility of the approach.\nOne aspect in which metrics such as IPSyn and DSS that\nhold a considerable advantage over a fully data-driven approa ch\nis interpretability of scores. With IPSyn and DSS, scores are\ntied directly to a known procedure in a way that is fully\ntransparent. Furthermore, subscales can give additional in sight\nthrough a more detailed view of language development. While\nneural network models should not be considered uninterpreta ble,\nand a growing body of research is dedicated speciﬁcally to\nunderstanding what neural language models learn (\nRogers et al.,\n2020), this kind of work is still in its infancy, and not yet at a sta ge\nthat can provide clear information about what speciﬁc kinds of\ninformation a model such as ours learns from language data.\nThe Role of Syntax in Measuring Language\nDevelopment\nPrevious research on interpreting what kind of syntactic\ninformation is encoded in neural language models and on explic it\nmodeling of syntax with neural networks suggest that our mod el’s\nability to track language development over time must be due to\nour BiLSTM encoder’s ability to capture at least some relevan t\naspects of syntax, and the entire model’s ability to capture\nwhat structures are expected to appear through the process\nof language acquisition. Previous work has shown that even\nwith the simple language model objective of word prediction,\nBiLSTM and related neural network architectures can learn s ome\nsyntactic structure (\nFutrell and Levy, 2019; McCoy et al., 2020;\nLinzen and Baroni, 2021 ). In discussing structures that are not\nlearned well by recurrent neural networks using the next-wor d\nprediction task, Linzen et al. (2016) suggest that even in those\ncases, the use of other training objectives may result in lea rning\nof these structures. In fact, the success of syntactic parser s\nbuilt on top of BiLSTM encoders with the explicit objective of\npredicting syntactic structure of input strings (\nKiperwasser and\nGoldberg, 2016 ) shows convincingly that BiLSTMs are capable\nof capturing syntactic structure, especially given an appropriat e\nobjective. Beyond investigations into whether recurrent n eural\nnetworks encode syntax, the apparent ﬂuency of LSTM language\nmodels for language generation, including in machine transl ation\n(\nSutskever et al., 2014 ), suggest these models learn a fair amount\nof syntax, since ﬂuent generation would be unlikely without i t.\nStill, the question remains if our model learns to score lang uage\ndevelopment based on syntax or other more superﬁcial features\nof the utterance strings, such as length.\nWhile it is a safe assumption that our model does learn to\nleverage utterance length in scoring, since it contains inf ormation\nrelevant to the task, as shown by MLU, it is unlikely that the\nperformance of the model can be attributed to superﬁcial strin g\ncharacteristics alone. The levels of rank correlation with age\nobtained with scores produced by our model, compared to those\nobtained with MLU, with DSS and with IPSyn further suggest tha t\nour model captures syntactic development. Given the similari ty\nFrontiers in Psychology | www.frontiersin.org 10 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nof the correlation coeﬃcients obtained with our model and wi th\nIPSyn and the extent to which IPSyn scores are based on syntac tic\nstructures, it is reasonable to expect that our model’s succes s\nis due to its modeling of syntax. Initial experiments with the\noriginal word forms in the utterances produced very similar\nresults as the ones presented, but it was not clear if the model\nlearned what changes in the grammar as language development\nprogresses, or what children tend to talk about at diﬀerent\nages. To isolate the eﬀect of syntactic structure, we used onl y\nplain part-of-speech tags to represent the words in the utteranc e\nstrings, completely removing any information about topic. Th is\nmakes it likely that the model does in fact rely on syntactic\nstructure, especially since our neural language model encod er is\ntrained not with the word prediction objective, but the langu age\nsample ordering objective.\nTo examine the extent to which our model relies on utterance\nlength, we performed an ablation experiment where we remove\nstructure from the utterances used to train our model, but lea ve\nutterance length intact. This is done simply by replacing each\ntoken with the same arbitrary symbol, so that each utterance is as\nlong as before, but it is made of the same symbol (word) repeate d\nover and over. This ablated model that only considers length is\ntrained and evaluated in the same way as our model. Recall that\nthe levels of age correction for MLU and our model are 0.662\nand 0.807, respectively ( Table 1). The correlation coeﬃcient for\nage and our length model is 0.711, putting it closer to MLU than\nto our full model. Although this ablated model scores langua ge\nsamples based on length, like MLU, the advantage it has over\nMLU is that it can consider the distribution of lengths of the\nutterances in the sample, and not solely the mean. For each\nchild in our dataset, the coeﬃcients for MLU and for the length\nmodel were similar, with the exception of Naomi from the Sachs\ncorpus (0.732 with MLU vs. 0.910 with the length model) and\nNathaniel from the Snow corpus (0.190 with MLU vs. 0.476 with\nthe length model).\nAlthough it is clear that our model captures more than\njust utterance length, the question of what else it captures\nremains. To examine our conjecture that the model identiﬁes\nsyntactic information in utterances, we performed an additi onal\nexperiment using our fully trained model. Recall that our mode l\nis composed of two modules: a BiLSTM network that encodes\nutterances, and a feed-forward network that produces a score\nbased on the encoding produced by the BiLSTM network. If\nthe model learns syntactic structure, this information wou ld\nbe present in the BiLSTM network. To test whether our\nmodel in fact uses identiﬁable syntactic structure, we used the\nsyntactic structure annotation available in the American E nglish\ntranscripts in the CHILDES database. Each utterance in these\ntranscripts is accompanied by a syntactic analysis in the form\nof a dependency structure that represents grammatical relati ons\ncomputed automatically by a data-driven parser (\nSagae et al.,\n2010). Using the same transcripts as in our evaluation of our\nmodel, we ﬁnd the 20 most common syntactic dependency\ntypes across all utterances, and try to determine whether our\nBiLSTM utterance encoder can detect the presence of each of\nthese dependency types in individual utterances. The 20 most\ncommon syntactic dependency types in our dataset, ordered\nmore to less common, are: SUBJ (subject), ROOT (main\nverb), JCT (adjunct), DET (determiner), OBJ (object), AUX\n(auxiliary), POBJ (object of a preposition), PRED (predicate\nnominal), LINK (complementizer, relativizer, or subordinat e\nconjunction), MOD (non-clausal nominal modiﬁer), COMP\n(clausal complement), COM (communicator), INF (inﬁnitival\nto), NEG (negation), QUANT (quantiﬁer), NJCT (nominal\nadjunct), COORD (coordination), CONJ (conjunction), CMOD\n(clausal nominal modiﬁer), and XMOD (non-ﬁnite nominal\nmodiﬁer). Explanations for each of these dependency types in\nthe context of syntactic analysis of CHILDES transcripts can be\nfound at https://talkbank.org/manuals/MOR.html.\nFor each of these syntactic dependency types, we construct\na dataset containing an equal number of utterances where\nthe corresponding grammatical relation appears and utterance s\nwhere the corresponding grammatical relation does not appear.\nWe then encode each of these utterances using our BiLSTM\nutterance encoder to obtain a vector representation for the\nutterance, as described in section A Neural Network Model of\nChild Language Development. This vector is the concatenatio n\nof the encodings of the beginning of sentence token (BOS) and\nthe end of sentence token (EOS). We then train a classiﬁer to\ndetect whether each of these ﬁxed length vectors correspond\nto an utterance where the grammatical relation in question\nappears or does not appear. For example, we take an equal\nnumber of utterances containing a CMOD dependency relation\n(approximately, a relative clause) and not containing a CMOD\nrelation, and train a binary classiﬁer (in this case, a feed-f orward\nnetwork with 50 hidden units) to predict if the original uttera nce\ncontains a CMOD relation. These vector encodings do not\ncontain the tokens in the original utterance, so this predict ion\nmust be made based on what information from the utterance the\nmodel encodes once it is trained. Over the course of training of\nthe model, these vector encodings are expected to capture the\ninformation necessary for ordering utterances chronologi cally. If\na speciﬁc syntactic structure, such as a relative clause represe nted\nby the syntactic dependency type CMOD, is useful to the model\nin the ordering task, we expect to be able to detect whether or n ot\nthe utterance contains a relative clause from the vector alone . We\nuse an equal number of utterances containing and not contain ing\neach dependency type so that identiﬁcation of dependency types\ncannot be made based on frequency information. Finally, we t est\neach classiﬁer on an unseen set of utterances also consisting of\nan equal number of utterances containing and not containing the\ndependency type in question.\nThe accuracy of these classiﬁers, shown in Table 2, conﬁrm\nthat our model does capture a substantial amount of syntactic\ninformation. Since each syntactic dependency type is tested w ith\nan equal number of utterances containing and not containing\nthe dependency type, an accuracy of 50% would correspond\nto no ability to detect the dependency type from the vector\nencoding of the utterance, while an accuracy of 100% would\ncorrespond to perfect ability to detect the dependency type, whi ch\nwould require the presence of the syntactic dependency to be\nencoded in the vector. Since the syntactic annotation used to\ntrain our classiﬁers experiment is produced automatically, a nd\ntherefore noisy, it would be unrealistic to expect accuracy o f\nFrontiers in Psychology | www.frontiersin.org 11 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nTABLE 2 | Accuracy in detection of the 20 most common syntactic depend ency\ntypes in our dataset from utterance encodings produced by ou r model.\nSyntactic dependency type Accuracy (%)\nSUBJ 82.3\nROOT 84.4\nJCT 74.1\nDET 72.3\nOBJ 73.6\nAUX 79.2\nPOBJ 73.9\nPRED 72.7\nLINK 72.1\nMOD 61.3\nCOMP 78.9\nINF 81.8\nNEG 75.1\nQUANT 64.1\nNJCT 74.3\nCOORD 72.5\nCONJ 75.8\nCJCT 85.9\nCMOD 75.1\nXMOD 69.2\nAVERAGE 74.9\n100%. Each dependency type was identiﬁed by its corresponding\nclassiﬁer with accuracy of at least 60%. CJCT (85.9%), ROOT\n(84.4%), and SUBJ (82.3%) were the dependency types identiﬁed\nwith highest accuracy, and MOD (61.3%), QUANT (64.1%), and\nXMOD (69.2%) were the dependency types identiﬁed with lowest\naccuracy. These results support our expectation that our model\nencodes syntactic structure.\nLanguage-Speciﬁc and\nPopulation-Speciﬁc Considerations\nBeing completely data-driven, we expect our model to be suitab le\nfor modeling language development in languages other than\nEnglish. However, since training our model requires a numbe r\nof transcripts from the same child over some period of time,\napplication of our method to the vast majority of languages is f ar\nfrom trivial. While the CHILDES Database does contain a limi ted\namount of suitable data for a few languages, no languages hav e an\namount of data that even approaches what is available for Engli sh.\nAlthough the apparent trade-oﬀ between a top-down approach\nwhere structures are enumerated by an expert and a bottom-up\napproach where relevant structures emerge from data may seem\nto favor the top-down view for the moment, we are experiencing\nan unprecedented increase in the availability of language da ta of\nmany diﬀerent kinds. For many reasons, child language data is\nnot as readily available as many other kinds of language data ,\nbut collection of the necessary data to create a model simila r\nto ours in other languages appears to be a feasible, although\nnon-trivial, task. Although concerns about safety and priva cy\nremain, the ability to record, store and share naturally occ urring\nlanguage, and advances in automatic transcription (\nGurunath\nShivakumar and Georgiou, 2020 ) make the eﬀort to build the\nnecessary datasets increasingly more manageable. While we are\nstill in a situation where large areas of research are too hea vily\nfocused on English, it is our hope that a data-driven approach\nwill create new opportunities for multilingual and cross-ling ual\nresearch, as has been the case with automatic syntactic parsi ng\n(\nZeman et al., 2018 ).\nAn exciting possibility created by a data-driven, bottom-\nup modeling approach is that language development can\nbe considered not just from the perspective of diﬀerent\nlanguages, but from the perspective of diﬀerent populations\nwith diﬀerent varieties of the same language. Even within the\ncontext of American English, one must consider that within th e\nUnited States alone there are substantial language diﬀerence s\namong populations, and the validity of metrics is usually\nexamined for one variety, with applicability to other varietie s\nbeing the topic of separate studies (\nOetting et al., 2010 ). While\nMLU values can be interpreted in the context of diﬀerent\npopulations, this advantage is due to how coarse-grained the\nmetric is. More precise metrics based on inventories of speciﬁc\nstructures would need to be adapted based on expertise of\nthe relevant language structures for each population. Given t he\nappropriate datasets, the data-driven view allows for precise, ﬁ ne-\ngrained scoring relative to a population represented in a speciﬁ c\ndataset, without the need for the assumption of a mainstream or\nstandard variant at the expense of other equally valid variant s.\nAlthough MLU is the most convenient approach for assessment\nof language development, since it does not require a language -\nspeciﬁc scoring scheme like IPSyn does, and it does not requir e\na longitudinal dataset like our data-driven approach does, it is\nnot as precise as the alternatives considered. When considerin g\nthe application of an approach like IPSyn or our data-driven\napproach to a new population whose language may not be\nidentical to that of populations used to validate these metrics, one\nis faced with a typical top-down vs. bottom-up trade-oﬀ. If no\ndata is available and data collection is impractical, one mig ht be\nwell-served by turning to language expertise to adapt a metric l ike\nIPSyn. When considering the amount of variety in English, and\nespecially going beyond English, this approach may be diﬃcult\nto scale, and will continue to be diﬃcult to scale. The data iss ue,\non the other hand, is likely to continue to become easier to de al\nwith, based on the trend observed for the past couple of decades .\nWhile this brings non-trivial questions about best practice s for\nconstruction of datasets that represent a language or a speciﬁ c\nvariant of a language, it is preferable to address these quest ions\nimperfectly but explicitly than to leave them unacknowledged,\nhiding the potential for inequity in research results.\nCONCLUSION\nAdvances in natural language processing, and speciﬁcally in\nlanguage modeling using neural network approaches, create\nexciting opportunities for modeling language development,\nincluding how grammatical structures develop over time.\nMotivated by recent work that shows that recurrent neural\nFrontiers in Psychology | www.frontiersin.org 12 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nnetworks learn some aspects of syntactic structure when give n\nappropriate training objectives ( Kiperwasser and Goldberg, 2016;\nFutrell and Levy, 2019; Linzen and Baroni, 2021 ) and by previous\nwork on data-driven measurement of syntactic development\n(\nLubetich and Sagae, 2014 ), we show that a model composed\nof a Bidirectional LSTM to encode language samples and\na feedforward network to score encoded samples can be as\neﬀective at producing language development scores that can\ntrack child language development over time as detailed langu age-\nspeciﬁc metrics designed by experts, such as the Index of\nProductive Syntax (\nScarborough, 1990 ). Although our goal is\nnot to create a new metric for language development in English ,\nand several issues remain unaddressed before our work can be\nleveraged into metrics that can be used in practice, our work\nis signiﬁcant in that it shows that recurrent neural network s,\nwithout any pre-speciﬁed knowledge about language beyond\nthe inductive bias inherent in their architecture, can lear n the\nchild language acquisition process to the extent necessary t o\ntrack language development in sets of transcripts as accurate ly\nas established metrics. We support our claim that our model\nlearns syntactic structure by showing that it outperforms a\nbaseline based on Mean Length of Utterance, and by removing\nall semantic information from transcripts to prevent the mode l\nfrom leveraging topic information and other cues.\nIn addition to demonstrating how neural language models\ncan capture the language development process successfully,\nwe hope that our work will serve as the basis for future\nwork on modeling and measuring language development\nthat, due to its bottom-up data-driven nature, will focus\non a wider variety of languages and language varieties,\ncreating the possibility for new language-speciﬁc and\ncross-lingual research on child language and development\nof syntax.\nDATA AVAILABILITY STATEMENT\nPublicly available datasets were analyzed in this study. Thi s data\ncan be found here: http://childes.talkbank.org.\nAUTHOR CONTRIBUTIONS\nThe author conﬁrms being the sole contributor of this work an d\nhas approved it for publication.\nACKNOWLEDGMENTS\nI am grateful to the reviewers for their insightful comments\nand suggestions.\nREFERENCES\nBrown, R. (1973). A First Language: The Early Stages. Cambridge, MA: Harvard\nUniversity Press.\nFletcher, P., and Garman, M. (1988). Larsping by numbers. Int. J. Lang. Commun.\nDisord. 23, 309–321. doi: 10.3109/13682828809011940\nFutrell, R., and Levy, R. (2019). “Do RNNs learn human-like abstract w ord\norder preferences?” in Proceedings of the Society for Computation in Linguistics\n(New York, NY), 50–59.\nGoldberg, Y. (2017). Neural Network Methods for Natural Language Processing.\nMorgan and Claypool.\nGraves, A. (2012). Supervised Sequence Labelling with Recurrent Neural Networks.\nHeidelberg; New York, NY; Dordrecht; London: Springer.\nGurunath Shivakumar, P., and Georgiou, P. (2020). Transfer learn ing from adult\nto children for speech recognition: evaluation, analysis and recomme ndations.\nComput. Speech Lang.63:101077. doi: 10.1016/j.csl.2020.101077\nHassanali, K., Liu, Y., Iglesias, A., Solorio, T., and Dollaghan, C . (2014). Automatic\ngeneration of the index of productive syntax for child language tra nscripts.\nBehav. Res. Methods46, 254–262. doi: 10.3758/s13428-013-0354-x\nHochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. Neural\nComput. 9, 1735–1780. doi: 10.1162/neco.1997.9.8.1735\nKingma, D. P., and Ba, J. (2015). “Adam: a method for stochastic o ptimization, ”\nin 3rd International Conference on Learning Representations, ICLR 2015, eds Y.\nBengio and Y. LeCun (San Diego, CA: Conference Track Proceeding s).\nKiperwasser, E., and Goldberg, Y. (2016). Simple and accurate depend ency parsing\nusing bidirectional lstm feature representations. Trans. Assoc. Comput. Linguist.\n4, 313–327. doi: 10.1162/tacl_a_00101\nLee, L. L., and Canter, S. M. (1971). Developmental sentence sco ring: a clinical\nprocedure for estimating syntactic development in children’s spontan eous\nspeech. J. Speech Hear. Disord.36, 315–340. doi: 10.1044/jshd.3603.315\nLinzen, T., and Baroni, M. (2021). Syntactic structure from deep learning. Annu.\nRev. Linguist.7, 195–212. doi: 10.1146/annurev-linguistics-032020-0510 35\nLinzen, T., Dupoux, E., and Goldberg, Y. (2016). Assessing the ability of lstms\nto learn syntax-sensitive dependencies. Trans. Assoc. Comput. Linguist. 4,\n521–535. doi: 10.1162/tacl_a_00115\nLubetich, S., and Sagae, K. (2014). “Data-driven measurement of child language\ndevelopment with simple syntactic templates, ” in Proceedings of the 25th\nInternational Conference on Computational Linguistics: Te chnical Papers\n(Dublin), 2151–2160.\nMacWhinney, B. (2000). The CHILDES Project: Tools for Analyzing Talk, 3rd Edn.\nMahwah, NJ: Lawrence Erlbaum.\nMacWhinney, B., Roberts, J. A., Altenberg, E. P., and Hunter, M. (2020). Improving\nautomatic ipsyn coding. Lang. Speech Hear. Serv. Sch. 51, 1187–1189.\ndoi: 10.1044/2020_LSHSS-20-00090\nMcCoy, R. T., Frank, R., and Linzen, T. (2020). Does syntax nee d to grow on trees?\nSources of hierarchical inductive bias in sequence-to-sequ ence networks. Trans.\nAssoc. Comput. Linguist.8, 125–140. doi: 10.1162/tacl_a_00304\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). “Eﬃcien t estimation of\nword representations in vector space, ” in Workshop Track Proceedings of the\nFirst International Conference on Learning Representations (Scottsdale, AZ).\nOetting, J. B., Newkirk, B. L., Hartﬁeld, L. R., Wynn, C. G., Pruit t, S. L.,\nand Garrity, A. W. (2010). Index of productive syntax for children w ho\nspeak african american english. Lang. Speech Hear. Serv. Sch.41, 328–339.\ndoi: 10.1044/0161-1461(2009/08-0077)\nRoberts, J. A., Altenberg, E. P., and Hunter, M. (2020). Machine -scored syntax:\ncomparison of the clan automatic scoring program to manual scoring. Lang.\nSpeech Hear. Serv. Sch.51, 479–493. doi: 10.1044/2019_LSHSS-19-00056\nRogers, A., Kovaleva, O., and Rumshisky, A. (2020). A primer in bert ology: what\nwe know about how bert works. Trans. Assoc. Comput. Linguist.8, 842–866.\ndoi: 10.1162/tacl_a_00349\nSagae, K., Davis, E., Lavie, A., Macwhinney, B., and Wintner , S. (2010).\nMorphosyntactic annotation of CHILDES transcripts. J. Child Lang. 37,\n705–729. doi: 10.1017/S0305000909990407\nSagae, K., Lavie, A., and MacWhinney, B. (2005). “Automatic me asurement of\nsyntactic development in child language, ” in Proceedings of the 43rd Annual\nMeeting on Association for Computational Linguistics-ACL ’05 (Ann Arbor, MI),\n197–204.\nSagae, K.,MacWhinney, B., and Lavie, A. (2004). “Adding syn tactic annotations to\ntranscripts of parent-child dialogs, ” in Proceedings of the Fourth International\nConference on Language Resources and Evaluation(Lisbon).\nFrontiers in Psychology | www.frontiersin.org 13 July 2021 | Volume 12 | Article 674402\nSagae Tracking Language Development\nScarborough, H. S. (1990). Index of productive syntax. Appl. Psycholinguist. 11,\n1–22. doi: 10.1017/S0142716400008262\nSundermeyer, M., Ney, H., and Schluter, R. (2015). From feedforward to recurrent\nlstm neural networks for language modeling. IEEE/ACM Trans. Audio Speech\nLang. Process.23, 517–529. doi: 10.1109/TASLP.2015.2400218\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). “Sequence to sequence learning with\nneural networks, ” inProceedings of the 27th International Conference on Neural\nInformation Processing Systems(Montreal, QC), 3104–3112.\nTurian, J., Ratinov, L., and Bengio, Y. (2010). “Word representat ions: a simple\nand general method for semi-supervised learning, ” in Proceedings of the 48th\nAnnual Meeting of the Association for Computational Linguistics (Uppsala),\n384–394.\nWeist, R. M., and Zevenbergen, A. A. (2008). Autobiographical memory and\npast time reference. Lang. Learn. Dev.4, 291–308. doi: 10.1080/154754408022\n93490\nZeman, D., Hajic, J., Popel, M., Potthast, M., Straka, M., Ginter , F., et al. (2018).\n“CoNLL 2018 shared task: multilingual parsing from raw text to unive rsal\ndependencies, ” in Proceedings of the CoNLL 2018 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies(Brussels), 1–21.\nConﬂict of Interest: The author declares that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\npotential conﬂict of interest.\nCopyright © 2021 Sagae. This is an open-access article distributed under the terms\nof the Creative Commons Attribution License (CC BY). The use, distribution or\nreproduction in other forums is permitted, provided the original author(s) and the\ncopyright owner(s) are credited and that the original publication in this journal\nis cited, in accordance with accepted academic practice. Nouse, distribution or\nreproduction is permitted which does not comply with these terms.\nFrontiers in Psychology | www.frontiersin.org 14 July 2021 | Volume 12 | Article 674402",
  "topic": "Utterance",
  "concepts": [
    {
      "name": "Utterance",
      "score": 0.778505802154541
    },
    {
      "name": "Computer science",
      "score": 0.7662362456321716
    },
    {
      "name": "Language model",
      "score": 0.6496061682701111
    },
    {
      "name": "Syntax",
      "score": 0.6417372822761536
    },
    {
      "name": "Natural language processing",
      "score": 0.5912676453590393
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5719158053398132
    },
    {
      "name": "Sentence",
      "score": 0.5565212368965149
    },
    {
      "name": "Language development",
      "score": 0.5173715949058533
    },
    {
      "name": "Language acquisition",
      "score": 0.4803118109703064
    },
    {
      "name": "Artificial neural network",
      "score": 0.4528745412826538
    },
    {
      "name": "Language identification",
      "score": 0.44381582736968994
    },
    {
      "name": "Recurrent neural network",
      "score": 0.41349637508392334
    },
    {
      "name": "Linguistics",
      "score": 0.33454784750938416
    },
    {
      "name": "Natural language",
      "score": 0.3144764006137848
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84218800",
      "name": "University of California, Davis",
      "country": "US"
    }
  ]
}