{
  "title": "Enabling Language Models to Fill in the Blanks",
  "url": "https://openalex.org/W3035233162",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221397544",
      "name": "Donahue, Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2237433634",
      "name": "Lee， Mina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3166964279",
      "name": "Liang, Percy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2803267010",
    "https://openalex.org/W2913250058",
    "https://openalex.org/W2784823820",
    "https://openalex.org/W3104802318",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2981461746",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2979684565",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963614567",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2963993699",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3004437686",
    "https://openalex.org/W2250506749",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1977406340"
  ],
  "abstract": "We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling---a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine-tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked. We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.",
  "full_text": "Enabling Language Models to Fill in the Blanks\nChris Donahue\nStanford University\nMina Lee\nStanford University\n{cdonahue,minalee,pliang}@cs.stanford.edu\nPercy Liang\nStanford University\nAbstract\nWe present a simple approach for text inﬁll-\ning, the task of predicting missing spans of text\nat any position in a document. While inﬁll-\ning could enable rich functionality especially\nfor writing assistance tools, more attention has\nbeen devoted to language modeling—a special\ncase of inﬁlling where text is predicted at the\nend of a document. In this paper, we aim to ex-\ntend the capabilities of language models (LMs)\nto the more general task of inﬁlling. To this\nend, we train (or ﬁne-tune) off-the-shelf LMs\non sequences containing the concatenation of\nartiﬁcially-masked text and the text which was\nmasked. We show that this approach, which\nwe call inﬁlling by language modeling, can en-\nable LMs to inﬁll entire sentences effectively\non three different domains: short stories, sci-\nentiﬁc abstracts, and lyrics. Furthermore, we\nshow that humans have difﬁculty identifying\nsentences inﬁlled by our approach as machine-\ngenerated in the domain of short stories.\n1 Introduction\nText inﬁlling is the task of predicting missing spans\nof text which are consistent with the preceding and\nsubsequent text.1 Systems capable of inﬁlling have\nthe potential to enable rich applications such as\nassisting humans in editing or revising text (Shih\net al., 2019), connecting fragmented ideas (AI21,\n2019), and restoring ancient documents (Assael\net al., 2019). Rather than targeting a particular\napplication, our goal here is to provide a general,\nﬂexible, and simple inﬁlling framework which can\nconvincingly inﬁll in a variety of domains.\nA special case of inﬁlling is language modeling:\npredicting text given preceding but not subsequent\ntext.2 Language models are (1) capable of generat-\n1Text inﬁlling is a generalization of the cloze task (Taylor,\n1953)—cloze historically refers to inﬁlling individual words.\n2In this paper, language modeling always refers to ordinary\nLMs, i.e., “unidirectional,” “autoregressive,” or “left-to-right.”\nShe ate leftover pasta for lunch. \nShe ate [blank] for [blank]. \nleftover pasta [answer] lunch [answer]\nData \nInput \nTarget\nOur Inﬁlling Framework\nShe ate [blank] for [blank]. \nShe ate leftover pasta for lunch.\nInﬁlling Task\nInput \nOutput\nTrain\nLanguage \nModel\nInﬁlling\nInput   \n[sep]   \nTarget\nData\nInput   \n[sep]   \nTarget\nOutput\nFigure 1: We consider the task of inﬁlling, which takes\nincomplete text as input and outputs completed text. To\ntackle this task, our framework constructs training ex-\namples by masking random spans to generate pairs of\ninputs (text with blanks) and targets (answers for each\nblank). We then train unidirectional language mod-\nels on the concatenation of each pair. Once trained,\na model takes text input with blanks, predicts the an-\nswers, and then combines them to produce the output.\ning remarkably coherent text (Zellers et al., 2019;\nSee et al., 2019), (2) efﬁcient at generating text,\nand (3) conceptually simple, but cannot inﬁll ef-\nfectively as they can only leverage context in a\nsingle direction (usually the past). On the other\nhand, strategies such as BERT (Devlin et al., 2019)\nand SpanBERT (Joshi et al., 2019) are able to inﬁll\nusing both preceding and subsequent text. How-\never, their use of bidirectional attention limits their\ninﬁlling capabilities to ﬁxed-length spans. This is\nproblematic as—for many applications—we may\nnot know the length of a missing span a priori.\nZhu et al. (2019) propose a method capable of in-\nﬁlling variable-length spans, but it uses a special-\nized architecture and hence cannot easily leverage\nlarge-scale pre-trained models.\nIn this work, we present inﬁlling by language\nmodeling (ILM), a simple framework which en-\narXiv:2005.05339v2  [cs.CL]  10 Sep 2020\nables LMs to inﬁll variable-length spans while pre-\nserving their aforementioned beneﬁts: generation\nquality, efﬁcient sampling, and conceptual simplic-\nity. Our framework involves a straightforward for-\nmulation of the inﬁlling task which, as we demon-\nstrate, can be learned effectively by existing LM\narchitectures. As shown in Figure 1, our approach\nconcatenates artiﬁcially-masked text with the text\nwhich was masked, and adopts a standard LM train-\ning (or ﬁne-tuning) procedure on such examples.\nOnce trained, inﬁlling can be performed for a docu-\nment with blanks by using the LM to generate text\nand then replacing the blanks with this text.\nIn addition to its conceptual simplicity, our ex-\nperiments show that ILM enables off-the-shelf LMs\nto inﬁll effectively. Furthermore, we ﬁnd that in-\nﬁlling performance improves when starting from a\nlarge-scale pre-trained LM (as opposed to training\nfrom scratch), suggesting an additional beneﬁt of\nusing our model-agnostic framework compared to\napproaches which require specialized architectures.\nWe provide an interactive web demo of models\ntrained under our framework. This demo can inﬁll\nmultiple variable-length spans with different granu-\nlarities (e.g. words, n-grams, and sentences) on the\ndomains of short stories, scientiﬁc abstracts, and\nsong lyrics: https://chrisdonahue.com/ilm.\nAll code, data, and trained models are available\nat https://github.com/chrisdonahue/ilm\nand also on the CodaLab platform at https:\n//worksheets.codalab.org/worksheets/\n0x9987b5d9cce74cf4b2a5f84b54ee447b.\n2 Problem Statement\nThe task of inﬁlling is to take incomplete text ˜x,\ncontaining one or more missing spans, and return\ncompleted text x. Let [blank] be a placeholder for a\ncontiguous sequence (span) of one or more missing\ntokens. Then, incomplete text ˜x is a sequence of\ntokens some of which are [blank]. In order to map\n˜x to x, an inﬁlling strategy must specify both how\nmany and which tokens to generate for each [blank].\nNote that there may be many reasonable x for a\ngiven ˜x. Hence, we are interested in learning a\ndistribution p(x | ˜x).\n3 Inﬁlling by Language Modeling\nIn this section, we describe our ILM framework.\nWe ﬁrst outline a simple reparametrization of the\ninﬁlling task. Then, we deﬁne a procedure for au-\ntomatically generating suitable training examples\nwhich can be fed to an off-the-shelf LM.\n3.1 Formulation\nFedus et al. (2018) explore an inﬁlling framework\nwhere LMs are trained on concatenations of ˜x and\nx, i.e., they use LMs to directly predict x given ˜x.\nWhile their approach is effective at inﬁlling individ-\nual words, it is somewhat redundant as the model\nmust “predict” the unmasked text in ˜x. Addition-\nally, a model is not guaranteed to exactly reproduce\nthe unmasked text.\nInstead, we make the trivial observation that it\nsufﬁces to predict only the missing spans y which\nwill replace the [blank] tokens in ˜x. We can then\nconstruct x by simply replacing [blank] tokens in\n˜x with predicted spans y in a deterministic fashion.\nIn order to handle multiple variable-length spans,\nwe pose y as the concatenation of all missing spans\nseparated by special [answer] tokens (one [answer]\nper [blank]) (Figure 1). We can thus cast inﬁlling\nas learning p(y | ˜x) without loss of generality.\n3.2 Training\nGiven a corpus consisting of complete text exam-\nples, our framework ﬁrst manufactures inﬁlling\nexamples and then trains an LM on these exam-\nples. To produce an inﬁlling example for a given\nx, we ﬁrst sample an ˜x from a stochastic function\nMask(x) which randomly replaces some number\nof spans in x with [blank] tokens. Then, we con-\ncatenate together the spans which were replaced—\nseparated by [answer] tokens—to form a training\ntarget y. Finally, we construct the complete inﬁll-\ning example by concatenating ˜x, [sep], and y (see\nFigure 2 for a complete example).\nWe train (or ﬁne-tune) LMs on these inﬁlling\nexamples using standard LM training methodology,\nyielding models of the form pθ(y | ˜x). Speciﬁcally,\nwe train GPT-2 (Radford et al., 2019) off the shelf,\nbut any LM can potentially be used.\nThis framework has several advantages. First,\nit incurs almost no computational overhead com-\npared to language modeling. Speciﬁcally, if there\nare k missing spans in ˜x, the concatenation of ˜x\nand y contains only 2k+1 more tokens than x (one\n[blank] and one [answer] per missing span plus one\n[sep]). As k is usually small (averaging around 2\nper example in our experiments), sequence lengths\nremain similar to those encountered for the same\nx during language modeling. In contrast, using\nLMs to directly predict x from ˜x as in Fedus et al.\n(2018) effectively doubles the sequence length ofx.\nThis is particularly problematic when considering\nmodels like GPT-2 whose memory usage grows\nquadratically with sequence length. Second, our\nframework requires minimal change (three addi-\ntional tokens) to an existing LM’s vocabulary. Fi-\nnally, because the entirety of˜x is in the “past” when\npredicting y, the ILM framework combines the abil-\nity to attend to incorporate context on both sides of\na blank with the simplicity of decoding from LMs.\n4 Experimental Setup\nWe design our experiments to determine if train-\ning an off-the-shelf LM architecture with our\nILM framework can produce effective inﬁlling\nmodels for a variety of datasets. Speciﬁcally,\nwe train on three datasets of different sizes and\nsemantics (details in Appendix A): short STO-\nRIES (Mostafazadeh et al., 2016), CS paper AB-\nSTRACTS , and song LYRICS .\n4.1 Mask Function\nA beneﬁt of the ILM framework is that it can\nbe trained to inﬁll spans corrupted by arbitrary\nmask functions. Here, we explore a mask func-\ntion which simultaneously trains models to inﬁll\ndifferent granularities of text; speciﬁcally, words,\nn-grams, sentences, paragraphs, and documents.\nBy using a unique special token per granularity\n(e.g. [blank word]), this mask function offers users\ncoarse but intuitive control over the length of the\nspans to be inﬁlled.\nWe conﬁgure our mask function to mask each\ntoken in a given document with around 15% prob-\nability, echoing the conﬁguration of Devlin et al.\n(2019). However, instead of masking individual\ntokens uniformly at random, we perform a pre-\norder traversal of the granularity hierarchy tree,\nrandomly masking entire subtrees with 3% proba-\nbility. For the datasets we consider, this results in a\nmarginal token mask rate of about 15% (details in\nAppendix B).\nWhile we train to inﬁll several different granular-\nities, we primarily evaluate and discuss the ability\nof our models to inﬁll sentences for brevity. Quan-\ntitative results of our models on other granularities\ncan be found in Appendix D, and granularity func-\ntionality can also be explored in our web demo.\n4.2 Task and Model Conﬁgurations\nFor all experiments, we train the same architecture\n(GPT-2 “small”) using the same hyperparameters\nShe ate leftover pasta for lunch. \nShe ate [blank] for [blank]. \nShe ate leftover pasta for lunch. [end] \n.lunch for leftover pasta ate She [end] \nShe ate [blank] for [blank]. She ate \n     leftover pasta for lunch. [end] \nShe ate [blank] for [blank]. [sep] \n     leftover pasta [answer] lunch [answer]\nData \nMasked \nLM \nLM-Rev \nLM-All \nILM \nTraining Examples for Diﬀerent Strategies\nFigure 2: Training examples for three baseline inﬁlling\nstrategies and ILM on a given artiﬁcially-masked sen-\ntence. For each strategy, we train the same architecture\n(GPT-2) on such examples. At both training and test\ntime, examples are fed from left to right; anything to\nthe left of a green target is available to the model as\ncontext when predicting the target. Precisely, LM only\nconsiders past context, and LM-Rev only considers fu-\nture. LM-All considers all available context but uses\nlong sequence lengths. Our proposed ILM considers\nall context while using fewer tokens.\n(Appendix C) while varying the inﬁlling strategy\nand dataset. In addition to our proposed ILM strat-\negy for inﬁlling, we consider three baseline strate-\ngies: (1) language modeling (LM; “inﬁlling” based\nonly on past context), (2) reverse language mod-\neling (LM-Rev; “inﬁlling” based only on future\ncontext), and (3) language modeling based on all\navailable context (LM-All). LM-All simply con-\ncatenates x and ˜x together as in Fedus et al. (2018).\nLM-All represents arguably the simplest way one\ncould conceive of inﬁlling with LMs, but results in\nlong sequence lengths. Training examples for all\nstrategies are depicted in Figure 2.\nFor each strategy, we also vary whether training\nis initialized from the pre-trained GPT-2 model or\nfrom scratch. Despite discrepancies between the\npre-training and our ﬁne-tuning for most inﬁlling\nstrategies, all of the inﬁlling experiments initialized\nfrom the pre-trained checkpoint performed better\nthan their from-scratch counterparts. This indicates\nthat ILM can effectively leverage large-scale lan-\nguage modeling pre-training to improve inﬁlling\nperformance. Henceforth, we will only discuss the\nmodels initialized from the pre-trained checkpoint,\nthough we report quantitative performance for all\nmodels in Appendix D.\nFor the models trained on STORIES and AB-\nSTRACTS , we trained models to convergence using\nearly stopping based on the validation set perplexity\n(PPL) of each model computed only on the masked\ntokens. These models took about a day to reach\nSTO ABS LYR Length\nLM 18.3 27.9 27.7 1.00\nLM-Rev 27.1 46.5 34.3 1.00\nLM-All 15.6 22.3 21.4 1.81\nILM 15.6 22.4 22.6 1.01\nTable 1: Quantitative evaluation results. We report test\nset perplexity (PPL) on the sentence inﬁlling task for\ndifferent model conﬁgurations on all three datasets, as\nwell as average length of all test set examples in to-\nkens relative to that of the original sequence (lower is\nbetter for all columns). Our proposed ILM framework\nachieves better PPL than both LM and LM-Rev, imply-\ning that it is able to take advantage of both past and\nfuture context. ILM achieves similar PPL to LM-All\nwith shorter sequence lengths (hence less memory).\ntheir early stopping criteria on a single GPU. For\nthe larger LYRICS dataset, we trained models for 2\nepochs (about two days on a single GPU).\n5 Quantitative Evaluation\nWe evaluate the quantitative performance of our\nmodels on the sentence inﬁlling task by measuring\nPPL on test data.3 In this setting, a sentence is se-\nlected at random and masked out, and we measure\nthe likelihood assigned by a model to the masked\nsentence in the context of the rest of the document.\nRegardless of differences in the ordering and num-\nber of tokens that each strategy uses to represent\na test example, PPL is always computed only for\nthe span of tokens comprising the original sentence\n(e.g. green tokens in Figure 2).\nTable 1 shows that across all datasets, ILM out-\nperforms models which see only past or future con-\ntext (LM and LM-Rev respectively), implying that\nour proposed framework is able to take advantage\nof bidirectional context despite using unidirectional\nmodels. Additionally, while one might expect LM-\nAll to outperform ILM because its training exam-\nples more closely “resemble” those of standard\nLMs, ILM achieves similar performance to LM-\nAll. This indicates that GPT-2 is able to effectively\nlearn the “syntax” of ILM examples and achieve\nreasonable inﬁlling performance with shorter se-\nquences (and hence with much less memory usage).\nWe also observe that models trained via ILM per-\nform similarly on the special case of language mod-\n3Overlap-based metrics such as BLEU score (Papineni\net al., 2002) are not appropriate for evaluating inﬁlling as\nthere are many realistic inﬁlls that have no word-level overlap\nwith the original, e.g., “a sandwich” instead of “leftover pasta.”\neling compared to the models which were trained\nonly on language modeling (Appendix D.1). This\nsuggests that ILM does not just repurpose LMs\nto inﬁll, but rather extends their capabilities while\nmaintaining their original functionality.\n6 Human Evaluation\nIn addition to our quantitative evaluation, we seek\nto evaluate the qualitative performance of ILM. To\nthis end, we sample a story from the STORIES test\nset and randomly replace one of its ﬁve human-\nwritten sentences with a model output. Then,\nwe task human annotators on Amazon Mechani-\ncal Turk with identifying which of the sentences\nin a story was machine-generated (details in Ap-\npendix E).\nWe compare our ILM model to three baseline\ninﬁlling strategies: an LM (context beyond the re-\nplaced sentence was discarded), the best model\n(self-attention; SA) from Zhu et al. (2019), and\nthe pre-trained BERT (base) model (Devlin et al.,\n2019). All approaches except for BERT were ﬁrst\nﬁne-tuned on the STORIES dataset. To inﬁll using\nBERT, we replace the tokens representing the orig-\ninal sentence with mask tokens, and then generate\ntext by replacing mask tokens one at a time (con-\nditioning on previously-generated tokens). While\nvocabulary differences make it is less useful to com-\npare PPL for the SA and BERT baselines to our\nGPT-2-based strategies, we can still meaningfully\ncompare them in this human evaluation setting.\nFor each approach we compute a score, which\nwe deﬁne as the percentage of examples where the\nannotator did not correctly identify the machine-\ngenerated sentence. Therefore, a higher score im-\nplies a better (more natural, human-like) model.\nWe collect 100 responses for each model and re-\nport the scores in Table 2, with qualitative examples\nin Figure 3 and Appendix E.\nOf the four strategies, ILM achieves the highest\nscore, implying that sentences inﬁlled by ILM are\nharder for humans to recognize as fake than those\nproduced by other strategies. Somewhat surpris-\ningly, we observed that despite only observing past\ncontext the LM model performed better than BERT\nand SA. BERT may have performed poorly due to\nthe intrinsic difﬁculty of ﬁnding convincing inﬁlls\nwith a precise length in tokens. SA may have per-\nformed poorly because, unlike LM and ILM, it was\nnot initialized from a large-scaled pre-trained LM.\nBERT SA LM ILM\nScore (%) 20 29 41 45\nTable 2: Human evaluation results. We use BERT (De-\nvlin et al., 2019), the best model from Zhu et al. (2019)\n(SA), and our LM and ILM models to replace random\nsentences in ﬁve-sentence stories from the S TORIES\ntest set. Then, we task humans with identifying which\nsentence of the ﬁve was generated by a machine. We\nreport the score of each model: the percentage of in-\nﬁlled stories where the human failed to identify the\nmachine-generated sentence. Our ILM model achieves\na higher score than all of the other models. Note that\nthe max score is effectively 80%, as a perfect model\nwould cause annotators to randomly choose one of the\nﬁve sentences.\nBERT  \nSA \nLM \nILM \nHuman\nfavoritea \", Mary brightly said.  \nShe wasn't sure she had to go to the store. \nShe went to check the tv. \nPatty knew her friends wanted pizza. \nShe also had the place looking spotless.\nExample Story with Masked Sentence\nPatty was excited about having her friends over. \nShe had been working hard preparing the food. \n[blank] \nAll of her friends arrived  \nand were seated at the table. \nPatty had a great time with her friends.\nFigure 3: Example of a short story in our S TORIES\ndataset with its third sentence masked, and sentences in-\nﬁlled by different models. The sentences generated by\nBERT and SA models are off-topic, the sentence gen-\nerated by LM model is irrelevant to the future context,\nwhile the ones generated by ILM and Human success-\nfully account for both previous and future context.\n7 Related Work\nMethodology. A number of systems have the\ncapability to inﬁll but have practical drawbacks.\nMany systems are unable to automatically deter-\nmine span length, and thus, can only inﬁll ﬁxed-\nlength spans (Fedus et al., 2018; Devlin et al., 2019;\nYang et al., 2019; Joshi et al., 2019; Gu et al., 2019;\nLiu et al., 2019). Methods such as BERT present\nadditional challenges during inference (Wang and\nCho, 2019). Rudinger et al. (2015) frame narrative\ncloze as a generation task and employ language\nmodels, but they only consider one inﬁll of a ﬁxed\nlength. Zhu et al. (2019); Shen et al. (2020) in-\nﬁll multiple variable-length sequences, but these\napproaches require the masked context to be itera-\ntively updated and reprocessed to ﬁll in blanks one\na time. In contrast, our approach appends inﬁlled\ntext to the context and does not require reprocess-\ning the entire input sequence for each blank. AI21\n(2019) train an LM which can ﬁll in the middle of\na paragraph given the ﬁrst and last sentences—our\nwork generalizes to such capabilities.\nTask. The cloze task (Taylor, 1953) evaluates\nlanguage proﬁciency by asking systems to ﬁll\nin randomly-deleted words by examining context.\nCloze has been extended in the forms of dis-\ncourse (Deyes, 1984) and narrative cloze (Cham-\nbers and Jurafsky, 2008), which remove phrases\nand narrative events respectively. Recently, cloze\nhas been used not only for evaluation, but also to\nimprove text generation quality (Fedus et al., 2018)\nand transfer learning (Devlin et al., 2019) (under\nthe name “masked language modeling”). Text inﬁll-\ning can be thought of as generalizing the cloze task\nfrom single words to spans of unknown length. Raf-\nfel et al. (2019) explore inﬁlling as a pre-training\nobjective to improve downstream performance on\ninference tasks; our work focuses on generation.\nStory generation. Recent work seeks to gener-\nate stories given a title and storyline (Yao et al.,\n2019), entities (Clark et al., 2018), premise (Fan\net al., 2018), or surrounding context and rare words\n(Ippolito et al., 2019). Our work differs in that\nwe aim to build systems capable of making predic-\ntions based only on text context, rather than aspects\nspeciﬁc to stories (e.g. storyline).\n8 Conclusion\nWe presented a simple strategy for the task of\ninﬁlling which leverages language models. Our\napproach is capable of inﬁlling sentences which\nhumans have difﬁculty recognizing as machine-\ngenerated. Furthermore, we demonstrated that our\ninﬁlling framework is effective when starting from\nlarge-scale pre-trained LMs, which may be useful\nin limited data settings. In future work, we plan to\nincorporate these features into co-creation systems\nwhich assist humans in the writing process. We\nhope that our work encourages more investigation\nof inﬁlling, which may be a key missing element\nof current writing assistance tools.\nAcknowledgments\nThis work was funded by DARPA CwC under ARO\nprime contract no. W911NF-15-1-0462. We thank\nall reviewers for their helpful comments.\nReferences\nAI21. 2019. HAIM: A modest step towards control-\nlable text generation. AI21 Labs Blog.\nYannis Assael, Thea Sommerschield, and Jonathan\nPrag. 2019. Restoring ancient text using deep\nlearning: a case study on greek epigraphy.\narXiv:1910.06262.\nN. Chambers and D. Jurafsky. 2008. Unsupervised\nlearning of narrative event chains. In Human Lan-\nguage Technology and Association for Computa-\ntional Linguistics (HLT/ACL).\nElizabeth Clark, Yangfeng Ji, and Noah A Smith. 2018.\nNeural text generation in stories using entity repre-\nsentations as context. In Association for Computa-\ntional Linguistics: Human Language Technologies.\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. 2019.\nBERT: Pre-training of deep bidirectional transform-\ners for language understanding. In Association\nfor Computational Linguistics (ACL) , pages 4171–\n4186.\nT. Deyes. 1984. Towards an authentic ‘discourse cloze’.\nApplied Linguistics, 5(2):128–137.\nA. Fan, M. Lewis, and Y . Dauphin. 2018. Hier-\narchical neural story generation. arXiv preprint\narXiv:1805.04833.\nW. Fedus, I. Goodfellow, and A. M. Dai. 2018.\nMaskgan: Better text generation via ﬁlling in the.\nIn International Conference on Learning Represen-\ntations (ICLR).\nJ. Gu, Q. Liu, and K. Cho. 2019. Insertion-based de-\ncoding with automatically inferred generation order.\narXiv preprint arXiv:1902.01370.\nD. Ippolito, D. Grangier, C. Callison-Burch, and\nD. Eck. 2019. Unsupervised hierarchical story inﬁll-\ning. In NAACL Workshop on Narrative Understand-\ning, pages 37–43.\nM. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer,\nand O. Levy. 2019. SpanBERT: Improving pre-\ntraining by representing and predicting spans. arXiv\npreprint arXiv:1907.10529.\nD. Liu, J. Fu, P. Liu, and J. Lv. 2019. TIGS: An infer-\nence algorithm for text inﬁlling with gradient search.\narXiv preprint arXiv:1905.10752.\nN. Mostafazadeh, N. Chambers, X. He, D. Parikh,\nD. Batra, L. Vanderwende, P. Kohli, and J. Allen.\n2016. A corpus and cloze evaluation for deeper\nunderstanding of commonsense stories. In North\nAmerican Association for Computational Linguistics\n(NAACL).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation. In ACL.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2019. Language models are unsuper-\nvised multitask learners. OpenAI Blog, 1(8).\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu. 2019.\nExploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683.\nR. Rudinger, P. Rastogi, F. Ferraro, and B. V . Durme.\n2015. Script induction as language modeling. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D Manning. 2019. Do\nmassively pretrained language models make better\nstorytellers? arXiv:1909.10705.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nTianxiao Shen, Victor Quach, Regina Barzilay, and\nTommi Jaakkola. 2020. Blank language models.\narXiv:2002.03079.\nY . Shih, W. Chang, and Y . Yang. 2019. XL-Editor:\nPost-editing sentences with xlnet. arXiv preprint\narXiv:1910.10479.\nW. L. Taylor. 1953. “Cloze procedure”: A new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433.\nA. Wang and K. Cho. 2019. BERT has a mouth, and\nit must speak: BERT as a Markov random ﬁeld lan-\nguage model. arXiv preprint arXiv:1902.04094.\nT. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtow-\nicz, and J. Brew. 2019. HuggingFace’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nZ. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdi-\nnov, and Q. V . Le. 2019. XLNet: Generalized au-\ntoregressive pretraining for language understanding.\narXiv preprint arXiv:1906.08237.\nL. Yao, N. Peng, R. Weischedel, K. Knight, D. Zhao,\nand R. Yan. 2019. Plan-and-write: Towards better\nautomatic storytelling. In Association for the Ad-\nvancement of Artiﬁcial Intelligence (AAAI).\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In NeurIPS.\nW. Zhu, Z. Hu, and E. Xing. 2019. Text inﬁlling.arXiv\npreprint arXiv:1901.00158.\nA Datasets\n- S TORIES (100K examples, 5M words)\nShort stories from the ROCStories dataset\n(Mostafazadeh et al., 2016). Each story contains\na title and ﬁve sentences.\n- A BSTRACTS (200K examples, 30M words)\nAbstracts from CS papers on arXiv\n- L YRICS (2M examples, 60M words)\nSong lyrics from lyrics.com\nWe experimented on multiple datasets to demon-\nstrate that our framework was not custom tailored\nto a single domain. On the STORIES and AB-\nSTRACTS datasets, we include metadata (story title,\npaper subject matter, etc.), as the ﬁrst “paragraph”\nof the document. By providing these paragraphs\n(Appendix B), our inﬁlling model implicitly learns\nto summarize (e.g. inﬁll a title given a story), and\ndo conditional generation (e.g. inﬁll a story given a\ntitle). On the LYRICS dataset, inﬁlling models may\nbe especially helpful to humans; external aid in the\nform of rhyming dictionaries is already commonly\nemployed in this domain.\nTo ensure that all experiments were trained on\nthe same data, we removed inﬁlling examples\nwhich would have exceeded our training sequence\nlength of 256 tokens for the model with the longest\nsequence length (LM-All). This removed no exam-\nples from STORIES , a small fraction of examples\nfrom LYRICS , and a substantial number of exam-\nples from ABSTRACTS .\nB Masking function\nWe design a mask function which takes the entire\ndocument and selectively masks several span gran-\nularities: words, n-grams, sentences, paragraphs,\nand entire documents. Accordingly, models trained\nvia ILM on this masking function offer users the\nability to specify the granularity of text to inﬁll\nat a particular location. This allows users to have\ncoarse but intuitive control over inﬁlling length, so\nthat multiple paragraphs are not generated when\nthe user was expecting a single word.\nOur masking function ﬁrst constructs a tree\nof the training example (using the natural hier-\narchy of documents, paragraphs, sentences, and\nwords). Then, using a pre-order tree traver-\nsal, each subtree is masked with 3% probabil-\nity (or ignored if any of its ancestors are already\nmasked). If the entire document (root node of\nthe tree) is masked, then the inﬁlling model’s job\nis equivalent to that of a language model. If a\nword (leaf) is selected to be masked, 50% of the\ntime we mask that individual word, otherwise we\nmask an n-gram of random length between 1 and\nmin(8, # words left in the sentence) words (inclu-\nsive). Note that a word may comprise multiple\ntokens, as GPT-2 uses sub-word tokenization (Sen-\nnrich et al., 2015). We chose the value of3% as, for\nthe datasets we considered, it resulted in a marginal\ntoken mask rate of around 15%, echoing the con-\nﬁguration of Devlin et al. (2019).\nWe add special tokens for each granularity to\nour model’s vocabulary (e.g. [ blank word]), so\nthat the user may specify which granularity they\nwould like the inﬁlling model to produce. This\nfunctionality can be explored in our demo: https:\n//chrisdonahue.com/ilm.\nWhile we focus on this speciﬁc mask function in\nthis paper, we structured the ILM codebase to allow\nusers to train inﬁlling models for completely differ-\nent use cases. Users need only deﬁne a new mask\nfunction which takes complete documents and out-\nputs lists of character-level spans representing the\ndesired spans to be masked.\nC Hyperparameters\nWe use early stopping based on the PPL of\nthe model on inﬁlling the masked token for the\nvalidation set. We train all models using the\ndefault ﬁne-tuning parameters speciﬁed in the\ntransformers library (Wolf et al., 2019), ex-\ncept that we use a batch size of 24 and a sequence\nlength of 256.\nNote that the most straightforward way of train-\ning an LM on ILM examples (Section 3.2) is to\nmaximize the likelihood of the entire concatenated\nexample: ˜x, [sep], and y. This trains the model to\npredict tokens in ˜x even though such behavior is\nnot necessary at inference time as ˜x will always be\nfully-speciﬁed. Nevertheless, we found that this ad-\nditional supervision improved performance when\nevaluating model PPL of y. Conveniently, this is\nalso the default behavior when adapting existing\nLM training code for use with ILM.\nD Evaluation on language modeling and\ninﬁlling other granularities\nOur quantitative evaluation (Section 5) examined\nthe sentence inﬁlling performance of GPT-2 initial-\nized from the large-scale pre-trained checkpoint\nSTO ABS LYR\nLM (scratch) 33.4 52.1 25.1\nLM-Rev (scratch) 32.9 53.9 24.7\nLM-All (scratch) 30.4 44.6 26.2\nILM (scratch) 30.8 45.3 30.6\nLM 17.6 25.7 20.8\nLM-Rev 25.1 36.7 23.7\nLM-All 17.8 25.2 21.5\nILM 18.1 23.9 23.0\nTable 3: Document inﬁlling PPL (or language mod-\neling) of ILM and baselines initialized either from\nscratch or from the pre-trained checkpoint across three\ndatasets. Note that PPL of ILM is similar to LM, imply-\ning that our inﬁlling strategy can reasonably maintain\nthe ability to perform language modeling while extend-\ning the ability to inﬁll.\nSTO ABS LYR\nLM (scratch) 34.0 52.8 28.9\nLM-Rev (scratch) 34.9 59.3 30.4\nLM-All (scratch) 27.0 46.2 24.3\nILM (scratch) 25.5 46.0 27.5\nLM 17.5 25.5 23.9\nLM-Rev 26.5 39.0 29.2\nLM-All 15.1 24.4 19.3\nILM 14.9 23.5 20.2\nTable 4: Mixture inﬁlling PPL of all models (a mixture\nof all granularities).\nafter ﬁne-tuning on different datasets and inﬁlling\nstrategies. Here, we report PPL for GPT-2 both\ninitialized from scratch and from the pre-trained\ncheckpoint for several other conﬁgurations: lan-\nguage modeling, a mixture of granularities, speciﬁc\ngranularities, and language modeling.\nD.1 Language modeling\nIn Table 3, we report PPL for “document inﬁlling,”\nwhich is equivalent to language modeling (because\n˜x is always [blank document]). Because of how\nwe structured our mask function (Appendix B), 3%\nof inﬁlling examples consist of the entire document\nmasked out, which results in the ability of our ILM\nframework to perform standard inﬁlling. We see\nthat performance of ILM is similar to that of LM on\nthis task, even though ILM sees far fewer examples\nof language modeling compared to LM.\nSTO ABS LYR\nLM (scratch) 35.6 51.5 25.1\nLM-Rev (scratch) 34.8 65.1 24.7\nLM-All (scratch) 33.4 45.0 26.2\nILM (scratch) 34.3 45.3 30.6\nLM 18.3 24.2 20.8\nLM-Rev 26.5 42.8 23.7\nLM-All 20.4 23.4 21.5\nILM 20.7 22.5 23.0\nTable 5: Paragraph inﬁlling PPL of all models.\nSTO ABS LYR\nLM (scratch) 36.0 65.4 33.5\nLM-Rev (scratch) 35.1 92.2 35.8\nLM-All (scratch) 27.1 53.8 27.1\nILM (scratch) 26.7 51.0 31.0\nLM 18.3 27.9 27.7\nLM-Rev 27.1 46.5 34.3\nLM-All 15.6 22.3 21.4\nILM 15.6 22.4 22.6\nTable 6: Sentence inﬁlling PPL of all models.\nD.2 Mixture of granularities\nIn Table 4, we report results for a mixture of granu-\nlarities. Speciﬁcally, we run the same mask func-\ntion we use for training (Appendix B) on our test\ndata and evaluate PPL on the masked spans. This\nreﬂects general inﬁlling ability across a wide va-\nriety of granularities (and hence lengths). Unlike\nour other quantitative evaluations, there may be\nmultiple variable-length spans missing from each\nexample in this evaluation. Results are similar to\nthat of sentence inﬁlling. Namely, that ILM outper-\nforms LM and LM-Rev and is similar to LM-All\ndespite using much less memory.\nD.3 Individual granularities\nIn Tables 5 to 8 we report PPL values for inﬁlling\nperformance on paragraphs, sentences, n-grams,\nand words, respectively, across the three datasets.\nFor each granularity, we create one inﬁlling ex-\nample per document from the test set with exactly\none masked span (randomly chosen from all spans\nof that granularity for that document). Then, we\ncompute PPL only on the tokens which comprise\nthe masked span, i.e., PPL is computed for all mod-\nels on exactly the same set of tokens. Across all\ngranularities, we observe that ILM outperforms\nSTO ABS LYR\nLM (scratch) 36.1 62.5 34.1\nLM-Rev (scratch) 36.4 89.1 36.3\nLM-All (scratch) 26.4 60.1 24.3\nILM (scratch) 23.1 49.5 26.3\nLM 19.2 25.5 28.2\nLM-Rev 26.6 45.0 34.8\nLM-All 14.5 20.5 18.6\nILM 13.8 21.5 18.8\nTable 7: N-gram inﬁlling PPL of all models.\nSTO ABS LYR\nLM (scratch) 32.3 57.2 34.8\nLM-Rev (scratch) 31.6 100.0 36.7\nLM-All (scratch) 12.6 51.8 12.5\nILM (scratch) 9.2 37.9 12.2\nLM 17.1 23.0 28.7\nLM-Rev 24.1 45.0 35.1\nLM-All 7.5 15.8 9.5\nILM 5.4 14.2 8.5\nTable 8: Word inﬁlling PPL of all models.\nLM and LM-Rev and either outperforms or is com-\nparable with LM-All while using less memory.\nE Details on human evaluation\nFor human evaluation, we sampled 100 stories from\nthe test set of theSTORIES dataset. From each story,\nwe masked out one sentence at a time, thereby re-\nsulting in 500 stories with masked sentences. Then\nwe used these stories as context and tasked each\nmodel with inﬁlling the masked sentence.\nWe compared 8 models in total. In addition to\nthe four models reported in Section 6 (BERT, SA,\nLM, and ILM), we included the models which are\ninitialized from scratch (as opposed to initialized\nfrom the large-scale pre-trained checkpoint) for\nexhaustive comparison. Furthermore, to ﬁlter out\nspam, we used a control model which always gen-\nerates “This sentence was generated by a computer.”\nLastly, we included the original sentence from the\ndataset as a reference model (Human) to sanity\ncheck the max score is around 80%.\nEach annotator was shown 8 stories, one from\neach model, and was asked to identify one of the\nﬁve sentences generated by machine (see Figure 4\nfor an example). Among the 100 collected re-\nsponses, we ﬁltered out 5 responses whose annota-\ntion for the control model was wrong. The quantita-\ntive and qualitative results can be found in Table 9\nand Figure 5, respectively. All model outputs and\nresponses of human evaluation can be found at\nhttps://github.com/chrisdonahue/ilm.\nScore (%)\nControl 0\nBERT 20\nSA 29\nLM (scratch) 40\nLM 41\nILM (scratch) 39\nILM 45\nHuman 78\nTable 9: Human evaluation results.\nIdentify one of the ﬁve sentences generated by machine.\n○ Patty was excited about having her friends over. \n○ She had been working hard preparing the food. \n○ Patty knew her friends wanted pizza. \n○ All of her friends arrived and were seated at the table. \n○ Patty had a great time with her friends.\nFigure 4: Example of a task and instruction for human\nevaluation on Amazon Mechanical Turk.\nExample Story with Masked Sentence\nLily always loved to read. \nShe wondered sometimes,  \nwhat it would be like to write a book? \n[blank] \nLily did well in the course, and during it,  \nwrote a short book. \nBERT \nSA \nLM \nILM \nHuman\nI held her hand and helped her sit. \nOf her, but she didn't know her. \nShe practiced reading a lot every week. \nFinally, in middle school, her teacher \n    introduced her to writing that. \nShe decided to take a course on ﬁction writing.\nBERT \nSA \nLM \nILM \nHuman\nOr rather, what the next job would be now. \nI was going out I was going to the beach. \nI put on about thirty sugar cubes. \nThe issues are getting so many people crazy. \nI could never catch up and each week  \n    got worse.\nExample Story with Masked Sentence\nMy old job kept asking too much of me. \nEvery Wednesday there was a ton of new work. \n[blank] \nEventually I got too far behind and had to quit. \nI will look for a new job.\nBERT  \nSA \nLM \nILM \nHuman\nToday was the ﬁrst concert that she had to  \n    see every where. \nShe was going to go to the play. \nWhen she went on stage she smoothly  \n    walked right past the audience. \nWhen she got on stage the band was amazing. \nAs soon as she got on the audience applauded.\nExample Story with Masked Sentence\nYesterday was Kelly's ﬁrst concert. \nShe was nervous to get on stage. \n[blank] \nKelly was then happy. \nShe couldn't wait to do it again.\nFigure 5: Examples of sentence-level inﬁlls by differ-\nent models.",
  "topic": "Concatenation (mathematics)",
  "concepts": [
    {
      "name": "Concatenation (mathematics)",
      "score": 0.8446546792984009
    },
    {
      "name": "Computer science",
      "score": 0.7547051310539246
    },
    {
      "name": "Task (project management)",
      "score": 0.6798633933067322
    },
    {
      "name": "Natural language processing",
      "score": 0.6365360021591187
    },
    {
      "name": "Language model",
      "score": 0.5841224193572998
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5297685265541077
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4579135775566101
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.434330552816391
    },
    {
      "name": "Linguistics",
      "score": 0.3220991790294647
    },
    {
      "name": "Engineering",
      "score": 0.0903421938419342
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}