{
  "title": "Fine-Tuning Language Models from Human Preferences",
  "url": "https://openalex.org/W2973379954",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4229190483",
      "name": "Ziegler, Daniel M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288594963",
      "name": "Stiennon, Nisan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3041339231",
      "name": "Wu, Jeffrey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288499931",
      "name": "Brown, Tom B.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227412476",
      "name": "Radford, Alec",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221793804",
      "name": "Amodei, Dario",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3163717489",
      "name": "Christiano, Paul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222786236",
      "name": "Irving, Geoffrey",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2966067376",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2137795521",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2564784249",
    "https://openalex.org/W2969286400",
    "https://openalex.org/W2962897543",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W2970630590",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2896930824",
    "https://openalex.org/W2948380112",
    "https://openalex.org/W2950314731",
    "https://openalex.org/W2798877128",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2952913664",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W2971034336",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2943766373",
    "https://openalex.org/W2964263543",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2901707424",
    "https://openalex.org/W2910458567",
    "https://openalex.org/W2911333381",
    "https://openalex.org/W2737114849",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2963325985",
    "https://openalex.org/W2971032890",
    "https://openalex.org/W2948367246",
    "https://openalex.org/W2991605991",
    "https://openalex.org/W2953981431",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2962762898",
    "https://openalex.org/W2945886944",
    "https://openalex.org/W2892472836",
    "https://openalex.org/W2962943921",
    "https://openalex.org/W2949059942",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2760781482",
    "https://openalex.org/W2948514529",
    "https://openalex.org/W2525778437"
  ],
  "abstract": "Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.",
  "full_text": "Fine-Tuning Language Models from Human Preferences\nDaniel M. Ziegler∗ Nisan Stiennon∗ Jeffrey Wu Tom B. Brown\nAlec Radford Dario Amodei Paul Christiano Geoffrey Irving\nOpenAI\n{dmz,nisan,jeffwu,tom,alec,damodei,paul,irving}@openai.com\nAbstract\nReward learning enables the application of rein-\nforcement learning (RL) to tasks where reward is\ndeﬁned by human judgment, building a model of\nreward by asking humans questions. Most work\non reward learning has used simulated environ-\nments, but complex information about values is of-\nten expressed in natural language, and we believe\nreward learning for language is a key to making\nRL practical and safe for real-world tasks. In this\npaper, we build on advances in generative pretrain-\ning of language models to apply reward learning\nto four natural language tasks: continuing text\nwith positive sentiment or physically descriptive\nlanguage, and summarization tasks on the TL;DR\nand CNN/Daily Mail datasets. For stylistic con-\ntinuation we achieve good results with only 5,000\ncomparisons evaluated by humans. For summa-\nrization, models trained with 60,000 comparisons\ncopy whole sentences from the input but skip irrel-\nevant preamble; this leads to reasonable ROUGE\nscores and very good performance according to\nour human labelers, but may be exploiting the fact\nthat labelers rely on simple heuristics.\n1. Introduction\nWe would like to apply reinforcement learning to complex\ntasks deﬁned only by human judgment, where we can only\ntell whether a result is good or bad by asking humans. To\ndo this, we can ﬁrst use human labels to train a model of\nreward, and then optimize that model. While there is a long\nhistory of work learning such models from humans through\ninteraction, this work has only recently been applied to mod-\nern deep learning, and even then has only been applied to\nrelatively simple simulated environments (Christiano et al.,\n2017; Ibarz et al., 2018; Bahdanau et al., 2018). By contrast,\nreal world settings in which humans need to specify com-\n*Equal contribution. Correspondence to paul@openai.com.\nplex goals to AI agents are likely to both involve and require\nnatural language, which is a rich medium for expressing\nvalue-laden concepts. Natural language is particularly im-\nportant when an agent must communicate back to a human\nto help provide a more accurate supervisory signal (Irving\net al., 2018; Christiano et al., 2018; Leike et al., 2018).\nNatural language processing has seen substantial recent ad-\nvances. One successful method has been to pretrain a large\ngenerative language model on a corpus of unsupervised data,\nthen ﬁne-tune the model for supervised NLP tasks (Dai and\nLe, 2015; Peters et al., 2018; Radford et al., 2018; Khandel-\nwal et al., 2019). This method often substantially outper-\nforms training on the supervised datasets from scratch, and\na single pretrained language model often can be ﬁne-tuned\nfor state of the art performance on many different super-\nvised datasets (Howard and Ruder, 2018). In some cases,\nﬁne-tuning is not required: Radford et al. (2019) ﬁnd that\ngeneratively trained models show reasonable performance\non NLP tasks with no additional training (zero-shot).\nThere is a long literature applying reinforcement learning to\nnatural language tasks. Much of this work uses algorithmi-\ncally deﬁned reward functions such as BLEU for translation\n(Ranzato et al., 2015; Wu et al., 2016), ROUGE for summa-\nrization (Ranzato et al., 2015; Paulus et al., 2017; Wu and\nHu, 2018; Gao et al., 2019b), music theory-based rewards\n(Jaques et al., 2017), or event detectors for story generation\n(Tambwekar et al., 2018). Nguyen et al. (2017) used RL\non BLEU but applied several error models to approximate\nhuman behavior. Wu and Hu (2018) and Cho et al. (2019)\nlearned models of coherence from existing text and used\nthem as RL rewards for summarization and long-form gen-\neration, respectively. Gao et al. (2019a) built an interactive\nsummarization tool by applying reward learning to one ar-\nticle at a time. Experiments using human evaluations as\nrewards include Kreutzer et al. (2018) which used off-policy\nreward learning for translation, and Jaques et al. (2019)\nwhich applied the modiﬁed Q-learning methods of Jaques\net al. (2017) to implicit human preferences in dialog. Yi\net al. (2019) learned rewards from humans to ﬁne-tune dia-\nlog models, but smoothed the rewards to allow supervised\nlearning. We refer to Luketina et al. (2019) for a survey of\narXiv:1909.08593v2  [cs.CL]  8 Jan 2020\nFine-Tuning Language Models from Human Preferences\nRL tasks involving language as a component, and for RL\nresults using transfer learning from language. RL is not the\nonly way to incorporate ongoing human feedback: Hancock\net al. (2019) ask humans what a dialogue system should\nhave said instead, then continue supervised training.\nIn this paper, we combine the pretraining advances in natural\nlanguage processing with human preference learning. We\nﬁne-tune pretrained language models with reinforcement\nlearning rather than supervised learning, using a reward\nmodel trained from human preferences on text continua-\ntions. Following Jaques et al. (2017; 2019), we use a KL\nconstraint to prevent the ﬁne-tuned model from drifting too\nfar from the pretrained model. We apply our method to\ntwo types of tasks: continuing text in a way that matches a\ntarget style, either positive sentiment or vividly descriptive,\nand summarizing text from the CNN/Daily Mail or TL;DR\ndatasets (Hermann et al., 2015; Völske et al., 2017). Our\nmotivation is NLP tasks where supervised data sets are un-\navailable or insufﬁcient, and where programmatic reward\nfunctions are poor proxies for our true goals.\nFor stylistic continuation, 5,000 human comparisons (each\nchoosing the best of 4 continuations) result in the ﬁne-tuned\nmodel being preferred by humans 86% of the time vs. zero-\nshot and 77% vs. ﬁne-tuning to a supervised sentiment net-\nwork. For summarization, we use 60,000 human samples\nto train models that can roughly be described as “smart\ncopiers”: they typically copy whole sentences from the in-\nput, but vary what they copy to skip irrelevant initial text.\nThis copying behavior emerged naturally from the data col-\nlection and training process; we did not use any explicit\narchitectural mechanism for copying as in See et al. (2017);\nGehrmann et al. (2018). One explanation is that copying\nis an easy way to be accurate, given that we did not in-\nstruct labelers to penalize copying but do instruct them to\npenalize inaccuracy. It may also reﬂect the fact that some\nlabelers check for copying as a fast heuristic to ensure a\nsummary is accurate. Indeed, human labelers signiﬁcantly\nprefer our models to supervised ﬁne-tuning baselines and\neven to human-written reference summaries, but not to a\nlead-3 baseline which copies the ﬁrst three sentences.\nFor summarization, we continue to collect additional data\nand retrain our reward model as the policy improves (online\ndata collection). We also test ofﬂine data collection where\nwe train the reward model using data from the original\nlanguage model only; ofﬂine data collection signiﬁcantly re-\nduces the complexity of the training process. For the TL;DR\ndataset, human labelers preferred the policy trained with\nonline data collection 71% of the time, and in qualitative\nevaluations the ofﬂine model often provides inaccurate sum-\nmaries. In contrast, for stylistic continuation we found that\nofﬂine data collection worked similarly well. This may be\nrelated to the style tasks requiring very little data; Radford\nReward model training\nPolicy Reward \nmodel \nHuman \nlabeler \ncontext continuation (×4) reward (×4) loss\nlabel\nPolicy training\nPolicy Reward \nmodel context continuation reward loss \nFigure 1: Our training processes for reward model and\npolicy. In the online case, the processes are interleaved.\net al. (2017) show that generatively trained models can learn\nto classify sentiment from very few labeled examples.\nIn concurrent work, Böhm et al. (2019) also use human\nevaluations to learn a reward function for summarization,\nand optimize that reward function with RL. Their work\nprovides a more detailed investigation of the learned policy\nand reward function on the CNN/Daily Mail dataset, while\nwe are interested in exploring learning from human feedback\nmore generally and at larger computational scale. So we\nconsider several additional tasks, explore the effects of on-\npolicy reward model training and more data, and ﬁne-tune\nlarge language models for both reward modeling and RL.\n2. Methods\nWe begin with a vocabulary Σ and a language model ρ\nwhich deﬁnes a probability distribution over sequences of\ntokens Σn via\nρ(x0 ···xn−1) =\n∏\n0≤k<n\nρ(xk|x0 ···xk−1)\nWe will apply this model to a task with input space X =\nΣ≤m, data distribution Dover X, and output space Y =\nΣn. For example, x ∈ X could be an article of up to\n1000 words and y ∈Y could be a 100-word summary. ρ\ndeﬁnes a probabilistic policy for this task via ρ(y|x) =\nρ(xy)/ρ(x): ﬁxing the beginning of the sample to xand\ngenerating subsequent tokens using ρ.\nWe initialize a policyπ= ρ, and then ﬁne-tune πto perform\nthe task well using RL. If the task was deﬁned by a reward\nfunction r: X×Y →R, then we could use RL to directly\noptimize the expected reward:\nEπ[r] = Ex∼D,y∼π(·|x) [r(x,y)]\nHowever, we want to perform tasks deﬁned by human judg-\nments, where we can only learn about the reward by asking\nhumans. To do this, we will ﬁrst use human labels to train a\nreward model, and then optimize that reward model.\nFine-Tuning Language Models from Human Preferences\nFollowing Christiano et al. (2017), we ask human labelers\nto pick which of several values of yi is the best response to\na given input x.1 We ask humans to choose between four\noptions (y0,y1,y2,y3); considering more options allows a\nhuman to amortize the cost of reading and understanding\nthe prompt x. Let b∈{0,1,2,3}be the option they select.\nHaving collected a dataset Sof (x,y0,y1,y2,y3,b) tuples,\nwe ﬁt a reward model r: X×Y →R using the loss\nloss(r) = E(x,{yi}i,b)∼S\n[\nlog er(x,yb)\n∑\nier(x,yi)\n]\n(1)\nSince the reward model needs to understand language, we\ninitialize it as a random linear function of the ﬁnal em-\nbedding output of the language model policy ρfollowing\nRadford et al. (2018) (see section 4.2 for why we initialize\nfrom ρrather than π). To keep the scale of the reward model\nconsistent across training, we normalize it so that it has\nmean 0 and variance 1 for x∼D,y ∼ρ(·|x).\nNow we ﬁne-tune π to optimize the reward model r. To\nkeep πfrom moving too far from ρ, we add a penalty with\nexpectation βKL(π,ρ) (see table 10 for what happens with-\nout this). That is, we perform RL on the modiﬁed reward\nR(x,y) = r(x,y) −βlog π(y|x)\nρ(y|x) . (2)\nWe either choose a constant β or vary it dynamically to\nachieve a particular value ofKL(π,ρ); see section 2.2. This\nterm has several purposes: it plays the role of an entropy\nbonus, it prevents the policy from moving too far from\nthe range where r is valid, and in the case of our style\ncontinuation tasks it also is an important part of the task\ndeﬁnition: we ask humans to evaluate style, but rely on the\nKL term to encourage coherence and topicality.\nOur overall training process is:\n1. Gather samples (x,y0,y1,y2,y3) via x ∼ D,yi ∼\nρ(·|x). Ask humans to pick the best yi from each.\n2. Initialize r to ρ, using random initialization for the\nﬁnal linear layer of r. Train ron the human samples\nusing loss (1).\n3. Train πvia Proximal Policy Optimization (PPO, Schul-\nman et al. (2017)) with reward Rfrom (2) on x∼D.\n4. In the online data collection case, continue to collect\nadditional samples, and periodically retrain the reward\nmodel r. This is described in section 2.3.\n1In early experiments we found that it was hard for humans\nto provide consistent ﬁne-grained quantitative distinctions when\nasked for an absolute number, and experiments on synthetic tasks\nconﬁrmed that comparisons were almost as useful.\n2.1. Pretraining details\nWe use a 774M parameter version of the GPT-2 language\nmodel in Radford et al. (2019) trained on their WebText\ndataset and their 50,257 token invertible byte pair encoding\nto preserve capitalization and punctuation (Sennrich et al.,\n2015). The model is a Transformer with 36 layers, 20 heads,\nand embedding size 1280 (Vaswani et al., 2017).\nFor stylistic continuation tasks we perform supervised ﬁne-\ntuning of the language model to the BookCorpus dataset\nof Zhu et al. (2015) prior to RL ﬁne-tuning; we train from\nscratch on WebText, supervised ﬁne-tune on BookCorpus,\nthen RL ﬁne-tune to our ﬁnal task. To improve sample\nquality, we use a temperature of T <1 for all experiments;\nwe modify the initial language model by dividing logits by\nT, so that future sampling and RL with T = 1 corresponds\nto a lower temperature for the unmodiﬁed pretrained model.\n2.2. Fine-tuning details\nStarting with the pretrained language model, the reward\nmodel is trained using the Adam optimizer (Kingma and Ba,\n2014) with loss (1). The batch size is 8 for style tasks and\n32 for summarization, and the learning rate is 1.77 ×10−5\nfor both. We use a single epoch to avoid overﬁtting to the\nsmall amount of human data, and turn off dropout.\nFor training the policy π, we use the PPO2 version of Proxi-\nmal Policy Optimization from Dhariwal et al. (2017). We\nuse 2M episodes (x,y pairs), γ = 1, four PPO epochs per\nbatch with one minibatch each, and default values for the\nother parameters. We use batch size 1024 for style tasks and\n512 for summarization. We do not use dropout for policy\ntraining. The learning rate was 1.41 ×10−5 for style tasks\nand 7.07 ×10−6 for summarization.\nModels trained with different seeds and the same KL penalty\nβsometimes end up with quite different values ofKL(π,ρ),\nmaking them hard to compare. To ﬁx this, for some experi-\nments we dynamically vary βto target a particular value of\nKL(π,ρ) using the log-space proportional controller\net = clip\n(KL(πt,ρ) −KLtarget\nKLtarget\n,−0.2,0.2\n)\nβt+1 = βt(1 + Kβet)\nWe used Kβ = 0.1.\nFor supervised ﬁne-tuning baselines, we ﬁne-tune for 1\nepoch on the CNN/Daily Mail and TL;DR training sets (for\nTL;DR we removed 30K examples to serve as a validation\nset). We decayed the learning rate to 0 with a cosine sched-\nule; for the initial value, we swept over 8 log-linearly spaced\noptions between 10−4 and 3 ×10−4. We also experimented\nwith different dropout rates, and found a rate of 0.1 to work\nbest. We then chose the model with the best validation loss.\nFine-Tuning Language Models from Human Preferences\n2.3. Online data collection\nIf the trained policy πis very different from the zero-shot\npolicy ρ, the reward model will suffer a large distributional\nshift from training on samples from ρto evaluation on sam-\nples from π. To prevent this, we can collect human data\nthroughout RL ﬁne-tuning, continuously gathering new data\nby sampling from πand retraining the reward model. As\nsection 3 shows, online data collection was important for\nsummarization but not for the simpler style tasks.\nIn the online case, we will choose a functionl(n) describing\nhow many labels we want before beginning the nth PPO\nepisode. Let Nπ = 2 ×106 be the total number of PPO\nepisodes, N0\nr = l(0) be an initial number of human labels,\nand Nr be the total number of human labels. We take\nl(n) = N0\nr + (Nr −N0\nr)\n(\n1 −(1 −n/Nπ)2)\nWe pause before the nth PPO episode if we have fewer than\nl(n) labels. We send another batch of requests to the labelers\nif the total requests so far is less than l(n) + 1000, to ensure\nthey have at least 1000 outstanding queries at any time. We\ntrain the reward model before the ﬁrst PPO episode, and\nthen retrain it 19 more times at evenly spaced values ofl(n).\nEach time we retrain we reinitialize r to a random linear\nlayer on top of ρand do a single epoch through the labels\ncollected so far. The ofﬂine case is the limit Nr = N0\nr.\nTo estimate overall progress, we gather validation samples\nconsisting of x ∼D; y0,y1 ∼ρ(·|x); y2,y3 ∼π(·|x) at\na constant rate; human labels on these give how often π\nbeats ρ. Since validation samples are only used to evaluate\nthe current π, we can add them to the training set for r.\nIn order to estimate inter-labeler agreement, 5% of queries\nare answered 5 times by different labelers. Label counts in\nsection 3 include validation samples and repeated labels.\n2.4. Human labeling\nWe use Scale AI to collect labels. The Scale API accepts\nrequests of the form(x,y0,y1,y2,y3) and returns selections\nb ∈{0,1,2,3}. We describe the task to Scale through a\ncombination of instructions (appendix A) and a dataset of\nabout 100 example comparisons from the authors.\nUnlike many tasks in ML, our queries do not have unam-\nbiguous ground truth, especially for pairs of similar outputs\n(which play a large role in our training process, since we\ntrain ron pairs of labels sampled from a single policy π).\nThis means that there is signiﬁcant disagreement even be-\ntween labelers who have a similar understanding of the task\nand are trying to rate consistently. On 4-way comparisons\nfor sentiment and TL;DR summarization, authors of this\npaper agree about 60% of the time (vs. 25% for random\nguessing). This low rate of agreement complicates the qual-\nity control process for Scale; the authors agree with Scale\nFigure 2: Learning curves for a 124M-parameter model with\nmock sentiment reward, targeting a KL of 8 nats. Lines and\nshaded areas show mean and range for 5 seeds. Early on the\nreward model sometimes speeds up training, a phenomenon\nalso observed by Christiano et al. (2017).\nlabelers 38% of the time on sentiment and 46% of the time\non TL;DR summarization. We give further details of the\nhuman data collection and quality evaluation in appendix B.\nFor ﬁnal evaluation of two models Aand B, we generate\neither 2-way comparisons between pairs (a∼A,b ∼B) or\n4-way comparisons with quadruples (a0,a1 ∼A,b0,b1 ∼\nB), randomize the order in which samples are presented,\nand present these comparisons to Scale. Evaluating the\nquality of a model trained by Scale using the same set of\nhumans from Scale is perilous: it demonstrates that rand\nπhave succeeded in ﬁtting to the human reward, but does\nnot show that those human evaluations capture what we\nreally care about, and our models are incentivized to exploit\nidiosyncracies of the labeling process. We include samples\nfrom our models so that readers can judge for themselves.\n3. Experiments\nIn section 3.1.1, we test our approach to RL ﬁne-tuning of\nlanguage models by using a mock labeler (a sentiment model\ntrained on a review classiﬁcation problem) as a stand-in for\nhuman labels. We show that RL ﬁne-tuning is effective\nat optimizing this complex but somewhat artiﬁcial reward.\nIn section 3.1.2, we show that we can optimize language\nmodels from human preferences on stylistic continuation\ntasks (sentiment and physical descriptiveness) with very\nlittle data, and that in the sentiment case the results are\npreferred to optimizing the review sentiment model. In\nsection 3.2 we apply RL ﬁne-tuning to summarization on\nthe CNN/Daily Mail and TL;DR datasets, show that the\nresulting models are essentially “smart copiers”, and discuss\nthese results in the context of other summarization work.\nFine-Tuning Language Models from Human Preferences\nFigure 3: Allowing the policy πto move further from the initial policy ρas measured by KL(π,ρ) achieves higher reward at\nthe cost of less natural samples. Here we show the optimal KL vs. reward for 124M-parameter mock sentiment (as estimated\nby sampling), together with results using PPO. Runs used 2M episodes, except for the top series.\nWe release code2 for reward modeling and ﬁne-tuning in\nthe ofﬂine data case. Our public version of the code only\nworks with a smaller 124M parameter model with 12 layers,\n12 heads, and embedding size 768. We include ﬁne-tuned\nversions of this smaller model, as well as some of the human\nlabels we collected for our main experiments (note that these\nlabels were collected from runs using the larger model).\n3.1. Stylistic continuation tasks\nWe ﬁrst apply our method to stylistic text continuation tasks,\nwhere the policy is presented with an excerpt from the Book-\nCorpus dataset (Zhu et al., 2015) and generates a continu-\nation of the text. The reward function evaluates the style\nof the concatenated text, either automatically or based on\nhuman judgments. We sample excerpts with lengths of 32\nto 64 tokens, and the policy generates 24 additional tokens.\nWe set the temperature of the pretrained model to T = 0.7\nas described in section 2.1.\n3.1.1. M OCK SENTIMENT TASK\nTo study our method in a controlled setting, we ﬁrst apply it\nto optimize a known reward function rs designed to reﬂect\nsome of the complexity of human judgments. We construct\n2Code at https://github.com/openai/lm-human-preferences.\nrs by training a classiﬁer3 on a binarized, balanced subsam-\nple of the Amazon review dataset of McAuley et al. (2015).\nThe classiﬁer predicts whether a review is positive or nega-\ntive, and we deﬁne rs(x,y) as the classiﬁer’s log odds that\na review is positive (the input to the ﬁnal sigmoid layer).\nOptimizing rs without constraints would lead the policy\nto produce incoherent continuations, but as described in\nsection 2.2 we include a KL constraint that forces it to stay\nclose to a language model ρtrained on BookCorpus.\nThe goal of our method is to optimize a reward function\nusing only a small number of queries to a human. In this\nmock sentiment experiment, we simulate human judgments\nby assuming that the “human” always selects the continu-\nation with the higher reward according to rs, and ask how\nmany queries we need to optimize rs.\nFigure 2 shows how rs evolves during training, using either\ndirect RL access to rs or a limited number of queries to train\na reward model. 20k to 60k queries allow us to optimize rs\nnearly as well as using RL to directly optimize rs.\nBecause we know the reward function, we can also ana-\nlytically compute the optimal policy and compare it to our\nlearned policies. With a constraint on the KL divergence\n3The model is a Transformer with 6 layers, 8 attention heads,\nand embedding size 512.\nFine-Tuning Language Models from Human Preferences\nFigure 4: Human evaluations comparing the zero-shot model with ofﬂine ﬁne-tuned models using varying amounts of\nhuman data. We report how often the ﬁne-tuned model is preferred by a majority of 3 labelers. We omit error bars because\nwe lack an estimate of the largest source of variance (randomness across training runs).\nSentiment Descriptiveness\n5k ofﬂine vs. zero-shot 88% 12% 86% 14%\n5k ofﬂine vs. mock 77% 23% —\n5k ofﬂine vs. 20k ofﬂine 48% 52% 47% 53%\n5k ofﬂine vs. 5k online 50% 50% 48% 52%\nTable 1: Human evaluations for the sentiment and descriptiveness tasks. We sample 1024 excerpts from the BookCorpus\ntest set and report how often each model’s continuations were preferred, as judged by a majority of 3 labelers.\nKL(π,ρ) between the learned policy π and the language\nmodel ρ, the optimal policy has the form:\nπopt(y|x) ∝ρ(y|x)ers(x,y)/β\nWe approximate the reward of this policy for given xand\nβby sampling a large number of continuations from ρ(y|x)\nand reweighting them by ers(x,y)/β. Figure 3 compares the\nreward obtained by our policies to the estimated optimal\nreward across a range of KL values. There is a signiﬁ-\ncant gap from optimality after training the policy on 2M\ncontinuations—the number used in our main experiments—\nthough it is largely closed with more training. Our policies\ncontinue to receive higher rewards for larger KL divergences,\nwhere we cannot afford to approximate πopt by sampling.\n3.1.2. H UMAN EVALUATIONS OF CONTINUATIONS\nWe apply our method to two continuation tasks deﬁned by\nhuman judgments:\nSentiment: Humans are asked to reward “positive and\nhappy” continuations.\nDescriptiveness: Humans are asked to reward “vividly de-\nscriptive” continuations.\nThe human labelers are presented with a BookCorpus ex-\ncerpt and four possible continuations; they are asked to\nselect the best continuation. Full instructions for labelers\nare provided in appendix A (although labelers also learned\nfrom ∼50 example comparisons labeled by the authors and\nso the instructions do not completely deﬁne the task).\nTo make the labeling task more natural, we select excerpts\nthat start and end with a period. When sampling continu-\nations that will be presented to humans, we use rejection\nsampling to ensure there is a period between tokens 16 and\n24 and then truncate at that period. 4 During the RL ﬁne-\ntuning, we penalize continuations that don’t have such a\nperiod by giving them a ﬁxed reward of −1.\nWe dynamically adjusted β to obtain a KL divergence of\n6 nats for descriptiveness and 10 nats for sentiment (sec-\ntion 2.2).\nWe trained a range of models using different amounts of\nfeedback, testing both ofﬂine data collection where humans\n4This is a crude approximation for “end of sentence.” We chose\nit because it is easy to integrate into the RL loop, and even a crude\napproximation is sufﬁcient for the intended purpose of making the\nhuman evaluation task somewhat easier.\nFine-Tuning Language Models from Human Preferences\ncontext Pearl thought to herself that what they were about to do was exactly the sort of thing\nthat they could do to help the villagers. They were all terriﬁed of these guys.\nAt the police station the three walked up to the counter behind which was a senior\nconstable studying some papers.\nContinuation 1 Continuation 2 Continuation 3\nzero-shot \"Hello, I’m Pearl and this is\nmy friend, Mike,\" said Pearl.\n\"May we speak to the police\nofﬁcer, sir?\" asked the one in\ncharge.\n'Hello, can I help you?'\n'Yes, we’re the same people\nthat the people were talking\nabout.\n5k ofﬂine\nﬁne-tune\nHe turned to them with a\nsmile. \"Good afternoon,\nladies. I’m Detective Inspec-\ntor Jones.\nThe constable stood up and\nsmiled as he saw them, obvi-\nously pleased to see them.\nHe smiled at them and waved\nthem in, his eyes twinkling as\nhe listened to their tales.\nTable 2: Three random (T = 0.7) continuations for our sentiment continuation task. Chosen from appendix table 11; see\nappendix for more.\ncontext \"I do not know if it was Viking related, but it could have been.\"\n\"Really?\" Ailia said. Is it safe to be traveling here then? Ailia looked behind her to\nmake sure they weren’t being followed.\nContinuation 1 Continuation 2 Continuation 3\nzero-shot There were no signs of any-\none.\n\"It is safe enough,\" Ailios\nsaid.\n\"Because I have a friend that\nis in the area and he will be\ncoming with us.\nIt was hard to see that far. \"I\ndo not like that word.\n5k ofﬂine\nﬁne-tune\nKaya crouched low, her eyes\nwide in the moonlight. Her\nbody was tense.\nShe put her hand on the\nsword strapped to her back,\nand then pulled it out.\nShe strode out the door\nand walked down the street,\nher nose wrinkled in disap-\nproval.\nTable 3: Three random (T = 0.7) continuations for our descriptiveness continuation task. Chosen from appendix table 12;\nsee appendix for more.\nrate only the initial language model’s continuation, and on-\nline data collection where humans continuously rate the cur-\nrent policy’s continuations (section 2.3). We then compared\nthese different policies to each other and to the zero-shot\nperformance of the original language model. The results are\nshown in ﬁg. 4 and table 1. Each model comparison is based\non 1024 four-way continuation comparisons, two from each\nof the models being compared, each rated by 3 humans.\nFor these continuation tasks, ofﬂine and online data collec-\ntion give similar performance. We ﬁnd that very little human\ndata is required for ﬁne-tuning: performance with 5k, 10k,\nand 20k reward model training samples is similar, degrading\nonly for less than 5k samples.5 The model trained using the\nreview sentiment classiﬁer from section 3.1.1 does poorly\nrelative to models optimized using human preference: in\n77% of contexts, labelers preferred the output of the model\ntrained with real human feedback.\n5The descriptiveness policy trained with 2.5k samples per-\nformed poorly, but we believe this is due to randomness in RL.\n3.2. Summarization\nWe also applied our method to two summarization tasks:\nthe CNN/Daily Mail dataset of Hermann et al. (2015)\nand the TL;DR dataset of Völske et al. (2017). We sam-\nple articles or Reddit posts, truncate to 500 tokens, add\na \"\\n\\nTL;DR:\" sufﬁx (and for CNN/Daily Mail, a\n\"Article:\\n\\n\" preﬁx) and let the policy respond with\nup to 75 tokens. We set the temperature of the pretrained\nmodel to T = 0.5 for CNN/Daily Mail and T = 0.7 for\nTL;DR. To make the task more natural for humans, we en-\nsure articles consist of whole sentences by truncating to\nthe last newline character. When sampling summaries that\nwill be shown to a human, we use rejection sampling to\nensure there is a newline between tokens 55 and 75 and\ntruncate at that newline. During RL ﬁne-tuning, we penalize\nsummaries that don’t have such a newline by giving them a\nﬁxed score of -1. For CNN/Daily Mail we used a ﬁxed KL\ncoefﬁcient β = 0.1; for TL;DR we used β = 0.03.\nFor RL ﬁne-tuning, we trained online data collection mod-\nFine-Tuning Language Models from Human Preferences\nTL;DR CNN/Daily Mail\nR-1 R-2 R-L R-A VG R-1 R-2 R-L R-A VG\nSOTA 22* 5* 17* 14.7* 41.22 18.68 38.34 32.75\nlead-3 17.435 3.243 14.575 11.751 40.379 17.658 36.618 31.552\nzero-shot 15.862 2.325 13.518 10.568 28.406 8.321 25.175 20.634\nsupervised baseline 17.535 3.124 14.969 11.877 39.525 16.992 36.728 31.082\nsupervised + 60k ﬁne-tune 18.434 3.542 15.457 12.478 40.093 17.611 37.104 31.603\n60k ﬁne-tune 16.800 2.884 14.011 11.232 37.385 15.478 33.330 28.731\n30k ﬁne-tune 16.410 2.920 13.653 10.994 35.581 13.662 31.734 26.992\n15k ﬁne-tune 15.275 2.240 12.872 10.129 38.466 15.960 34.468 29.631\n60k ofﬂine ﬁne-tune 16.632 2.699 13.984 11.105 33.860 12.850 30.018 25.576\nTable 4: ROUGE evaluations of summarization models. For all models (excluding the lead-3 baselines), we sample with\ntemperature 0.7 for TL;DR and 0.5 for CNN/Daily Mail. We use the CNN/DM test set, but our own validation set for TL;DR.\nCNN/Daily Mail SOTA is from Gehrmann et al. (2018). * TL;DR SOTA is from Gehrmann et al. (2019), but the numbers\nare not comparable as we lack test set access and the TL;DR leaderboard uses an unofﬁcial implementation of ROUGE.\nTL;DR CNN/Daily Mail\n60k ﬁne-tuned vs. zero-shot 96% 4% 91% 9%\n60k ﬁne-tuned vs. supervised 97% 3% 80% 20%\n60k ﬁne-tuned vs. lead-3 45% 55% 40% 60%\n60k ﬁne-tuned vs. supervised + 60k ﬁne-tuned 80% 20% 74% 26%\n60k ﬁne-tuned vs. 30k ﬁne-tuned 40% 60% 62% 38%\n60k ﬁne-tuned vs. 15k ﬁne-tuned 79% 21% 47% 53%\n60k ﬁne-tuned vs. 60k ofﬂine ﬁne-tuned 64% 36% 65% 35%\n60k ﬁne-tuned vs. reference summaries 96% 4% 84% 16%\nlead-3 vs. supervised 97% 3% 89% 11%\nlead-3 vs. reference summaries 97% 3% 89% 11%\nlead-3 vs. supervised + 60k ﬁne-tuned 75% 25% 85% 15%\nTable 5: Human evaluation of summarization models. For each pair of models and each dataset, we sample 1024 articles\nfrom the test set, generate a summary from each model, and ask 3 humans to pick the best summary using the same\ninstructions as in training. The model chosen by a majority of the humans wins on that article. We report the fraction of\narticles that each model wins. For all models, we sample with temperature 0.7 for TL;DR and 0.5 for CNN/DM.\nels with 15k, 30k, and 60k human labels, and an ofﬂine\ndata collection ablation with 60k labels. We also show\nzero-shot performance of the pretrained model, a super-\nvised ﬁne-tuned baseline using the same pretrained model\nas starting point (section 2.2), and a lead-3 baseline which\ncopies the ﬁrst three sentences of the context. We truncate\nlead-3 at a period in the same way we truncate generated\nsummaries, so occasionally it is 2 sentences. Finally, we\ncombine supervised and RL ﬁne-tuning: performing hu-\nman RL ﬁne-tuning starting with the supervised ﬁne-tuned\nmodel. The purely RL ﬁne-tuned models use contexts from\nthe datasets during training but ignore the reference sum-\nmaries; the supervised and supervised+RL models use both\ncontexts and summaries.\nWe report two sets of numerical results: human evaluations\nbetween pairs of models (table 5) and ROUGE results on the\ntest set of CNN/Daily Mail and our validation set of TL;DR\n(table 4). ROUGE results suggest that online data collection\nis important for best performance, in contrast to our stylistic\ncontinuation tasks. At a ﬁxed number of labels, online\ntends to be better than ofﬂine, with a 3 point R-A VG gain on\nCNN/DM at 60k labels.6 On both datasets we see signiﬁcant\nreturns to data volume up to 60k human labels (though the\ntrend is less clear for human evaluation). On both datasets,\nsupervised + RL ﬁne-tuning is best, and indeed pure RL ﬁne-\ntuning is worse than the supervised baseline according to\n6That said, different training runs have considerable variation\nand it is expensive to run multiple seeds with humans, so it is\npossible that this gap is largely noise.\nFine-Tuning Language Models from Human Preferences\nreference\nsummary\nSolar plane attempting to be ﬁrst to circumnavigate world without using fuel is stuck in\nChina. Solar Impulse 2 attempts to prove the power of renewable energy.\nzero-shot The plane has been grounded in China for two weeks because of bad weather.\n60k ﬁne-tune The Solar Impulse 2, the experimental plane attempting to ﬂy around the world without\nusing a drop of fuel, has been grounded by the weather in China.\nWhat was supposed to be an overnight pit stop in the southwestern city of\nChongqing has now stretched into a two-and-a-half week stay.\nsupervised Solar Impulse 2 has been grounded by the weather in China. The plane took off from\nAbu Dhabi on March 9. The plane is trying to prove the power of renewable energy.\nsupervised +\n60k ﬁne-tune\nSolar Impulse 2 has been grounded in China for two-and-a-half weeks. The plane\nis attempting to ﬂy around the world without using a drop of fuel. The team, which\nincludes Bertrand Piccard, is taking turns ﬂying the single-seater. The plane took off\nfrom Abu Dhabi on March 9 and has successfully ﬂown through Oman.\nTable 6: Random (T = 0.5) summaries for our CNN/DM summarization task, on the same context. Samples chosen from\nappendix table 16 (see appendix also for context being summarized). The 60k ﬁne-tune model copies from the source article.\nROUGE in all cases (though the supervised baseline uses the\nfull supervised training dataset, which is much larger than\n60k samples). Lead-3 is hard to beat: it is the best model\nfor R-1 and R-2 on CNN/Daily Mail, and only supervised +\nRL ﬁne-tuning beats it otherwise.\nBut our goal is optimizing reward deﬁned by humans, not\nROUGE. Table 5 shows pairwise comparisons between dif-\nferent model pairs according to human labelers, using 1024\nsamples with majority vote of 3 labelers per sample. Here\nthe picture is different, though also signiﬁcantly noisier. Our\nonline trained, 60k label model reliably beats both the zero-\nshot and supervised baselines, and even beats the combined\nsupervised + RL ﬁne-tuned model. Online training remains\nimportant, but the situation w.r.t. data volume is less clear\nand likely contaminated by noise: the 60k TL;DR model\nbeats the 30k model only 40% of the time, for example.\nMore worrisome, the 60k online model beats the human\nground truth 96% of the time for TL;DR and 84% of the\ntime for CNN/Daily Mail.\nWhat is going on? As we show in the next section, our 60k\nRL ﬁne-tuned model is almost entirely extractive (despite\nlacking any explicit extractive architectural component): it\nmostly copies whole sentences from the context, but varies\nwhich sentences are copied.\n3.2.1. W HAT OUR MODELS COPY\nMuch previous work in summarization has focused on ex-\nplicit copying mechanisms, including the pointer network-\nbased architecture of See et al. (2017) and the two-phase\nmask and paraphrase approach of Gehrmann et al. (2018).\nThe goal is to take advantage of copying (which is of fun-\ndamental importance to the task of summarization) without\nonly copying—to be abstractive rather than extractive.\nFigures 5 and 6 show the fractions of n-grams and sen-\ntences generated by our models which are novel and re-\npeated, respectively. From the novelty stats, we see that our\nRL ﬁne-tuning consistently causes models to copy more.\nIn particular, our 60k RL ﬁne-tuned models are almost en-\ntirely extractive: they copy whole sentences 71% of the\ntime for TL;DR and 98% of the time for CNN/Daily Mail.\nApplying RL ﬁne-tuning starting from the supervised ﬁne-\ntuned model copies much less: 6% and 30% for TL;DR and\nCNN/Daily Mail. Although we do not use explicit cover-\nage metrics as in See et al. (2017); Gehrmann et al. (2018),\nboth supervised and RL ﬁne-tuned models do very little\nrepetition within summaries.\nWhile the purely RL ﬁne-tuned models mostly copy, they\nvary where they copy from. Figure 7 illustrates this via the\nposition of the longest common subsequence between con-\ntext and summary. To understand when the model chooses\nto copy from the exact beginning, we identify common\npreambles in articles such that we would expect copying to\nbe a poor strategy. Table 7 shows that these preambles are\ncopied much less often than in the immediate beginnings\nof other articles, giving evidence that our models are smart\nabout when to copy. However, we cannot establish that our\nreward model is smart beyond rewarding copying, as the\nzero-shot model also skips preambles.\nSince combining supervised ﬁne-tuning and RL ﬁne-tuning\ngives the best ROUGE scores and and is also more abstrac-\ntive, why not use it? Unfortunately there is an advantage\nto pure copying shown in table 8: it makes it easy for the\nmodel to tell the truth. The models that copy the most, 60k\nRL ﬁne-tuned, is 90% and 95% accurate on TL;DR and\nCNN/Daily Mail; lifting whole sentences from the article\nusually leaves them true. The supervised ﬁne-tuned and\ncombined supervised+RL ﬁne-tuned models are accurate\nFine-Tuning Language Models from Human Preferences\nFigure 5: Percent of n-grams and sentences in summaries that do not appear in the source (compare to ﬁgure 6 in See et al.\n(2017)). n-grams are consecutive sequences of words in a single sentence in a summary, and they count as novel if they do\nnot appear consecutively in the article. We ignore punctuation and capitalization.\nFigure 6: Percent of n-grams and sentences in summaries that appear multiple times in the summary (compare to ﬁgure 4 in\nSee et al. (2017)).\nTL;DR\n CNN/Daily Mail\nFigure 7: Variation in where the models copy from, illustrated by the location of the longest common subsequence of\nbigrams between context article/post (left) and summary (right) for 256 randomly chosen contexts. Document lengths are\nshown in gray, with bigrams highlighted (with color depending on positions in contexts). Here, we picture the 60k ﬁne-tuned\nmodels, which do the most copying.\nFine-Tuning Language Models from Human Preferences\nTL;DR CNN/Daily Mail\nall preamble all preamble\nzero-shot 1.3% 0% 10.4% 1.0%\n60k ﬁne-tuned 28.3% 0.2% 77.6% 1.4%\nsupervised 1.5% 0% 9.4% 0%\nsupervised + 60k ﬁne-tuned 7.9% 0% 16.6% 0%\nreference summaries 0.6% 0% 5.1% 0%\ntotal articles 30000 3762 13368 297\nTable 7: How often different models copy the ﬁrst 3 words of the article as the ﬁrst 3 words of the summary, on the validation\nsets. We additionally consider the subset of posts/articles with preambles. On TL;DR, we used posts which begin with one\nof ‘hi’, ‘hello’, ‘hey’, ‘ok’, ‘okay’, or ‘so’. For CNN/Daily Mail, we used articles with a colon within the ﬁrst 3 words, such\nas “Winner: Simon Wood took home the TV crown [...]” and “Fully charged: The new scheme will let EE customers pick\nup free portable chargers [...]”.\nTL;DR CNN/Daily Mail\nzero-shot 6/30 6/30\n60k ﬁne-tuned 26/30 29/30\nsupervised 8/30 19/30\nsupervised + 60k ﬁne-tuned 11/30 20/30\nTable 8: Frequency with which generated summaries are accurate, in the sense of only making statements supported by the\ncontext, as judged by the authors on 30 articles from each dataset. The 60k ﬁne-tuned model achieves high accuracy via\ncopying; the supervised and supervised + 60k ﬁne-tuned models are more abstractive but at signiﬁcant cost to accuracy.\nat most 70% of the time: they paraphrase but paraphrase\nbadly, often swapping names from the context or mixing\ntogether multiple sentences in invalid ways. Zero-shot is\nthe most novel, but is accurate only 20% of the time. Sim-\nilarly, Kry´sci´nski et al. (2019) found that 30% of samples\nfrom the supervised summarization models they tested con-\ntained inconsistencies, and Khandelwal et al. (2019) found\nthat their pretrained encoder-decoder model “hallucinates\nfacts...which are topical but never appear in the source”.\nThere are at least two ways of interpreting these results. The\nﬁrst is that copying is the easiest way to be accurate. The\nlabelers were told to penalize inaccuracy and redundancy,\nbut were not told to penalize copying. The zero-shot model\ncopies some of the time, and when it copied it was accurate,\nso this behavior was reinforced. The result is a model that\n“degenerated to copying”, but at least does not lie.\nHowever, this does not explain why both our model and\nlead-3 are strongly preferred by the labelers to the human\nreference summaries (table 5). This reveals a mismatch\nbetween the notion of quality we wanted our model to learn,\nand what the humans labelers actually evaluated. Checking\nfor copying is very easy, so labelers who check primarily for\ncopying can work quickly. Since the online data collection\nsetting made quality control more difﬁcult, we failed to\ndetect and penalize this behavior.\n4. Challenges\nWe conclude with a few lessons and directions we plan to\nconsider in future reward learning work.\n4.1. Online data collection is hard\nOnline data collection was necessary to achieve the best\nresults on summarization. However, fully online data\ncollection—where each label comes from an up-to-date\nversion of the policy which has already learned from almost\nall previous labels—had major disadvantages:\n1. Software complexity: Our online system interleaves\ndata gathering, reward model training, and RL ﬁne-\ntuning. The resulting distributed system was signif-\nicantly more complicated than if each task was kept\nseparate, slowing the software development process.\nMoreover, a bug in any one of the tasks would break\nthe entire training process.\n2. Machine learning complexity: Online experiments\nwere difﬁcult to debug, as it was hard to iterate on one\npiece of the ML system at a time. We could often debug\nan online job only by switching brieﬂy to ofﬂine, such\nas by launching a standalone reward model training run,\nbut then would switch back to online once debugging\nwas complete (until the next cycle).\nFine-Tuning Language Models from Human Preferences\n3. Quality control issues: Signiﬁcant work was required\non Scale’s part to make their data quality mechanisms\nwork in the low latency, online setting. However, even\nafter this work it was difﬁcult to maintain high data\nquality over a long period of time, and regressions\nwere often not detected until after (or well after) train-\ning runs were complete. Since evaluation of labeler\nperformance was online, by the time a worker was de-\ntected as poor some of their data might already been\nreported back and used for reward model training.\nWe believe the right middle ground between ofﬂine and\nonline data collection is batched data collection, and plan\nto use this setting in future work. Collect a batch of data\nfrom the pretrained policy ρ, train the reward model ron\nthis batch, then ﬁne-tune the policy πwith rfrozen. Once\ncomplete, collect another batch of data sampled from π, and\niterate. The latency for each batch can be far longer than\nthe online case, simplifying quality control. As in the fully\nonline setting, we can always retrain the reward model from\nscratch on all data collected so far; human data is expensive\nso the total volume will be low. Removing the interleaved\ntraining of r and π simpliﬁes software architecture and\ndiagnosis of ML issues, and allows iteration on just one\ncomponent (say rin isolation) if problems occur. Li et al.\n(2016) reached similar conclusions in a restricted dialogue\nsetting after validating in simulation that online and batched\ntrained performed similarly.\nBatched data collection is also a well-studied setting for\nactive learning techniques. Although we use RL to ﬁne-tune\nthe policy π, the human data is used only for supervised\ntraining of the reward model r. Thus, any method for batch\nmode active learning of supervised models applies, using π\nas the unlabeled data distribution for r. Examples of such\ntechniques include selecting batches based on entropy con-\nsiderations (Guo and Schuurmans, 2008), gradient-based\nmetrics (Huang et al., 2016; Ash et al., 2019), or by attempt-\ning to distinguish labeled and unlabeled examples (Gissin\nand Shalev-Shwartz, 2019).\n4.2. Sharing parameters between reward model and\npolicy causes overﬁtting\nAlthough the reward model and policy are both initialized\nto ρ, we train them as separate networks rather than a single\nshared network with multiple heads. We might expect joint\ntraining to be helpful, effectively using RL as an auxiliary\ntask to improve the reward model’s performance. Joint train-\ning is particularly appealing because it could help the reward\nmodel stay strong enough that the policy cannot exploit it.\nSharing could also improve computational efﬁciency, by al-\nlowing the models to share activations rather than requiring\ntwo separate forward passes.\nDespite several attempts, we were not able to make this idea\nwork. The problem comes from the massive imbalance of\ndata: we have at most 60k samples for the reward model,\nbut 2M episodes for the policy. This makes it challenging\nto maintain performance on both tasks without performing\nmany epochs for the reward model and overﬁtting. We hope\nthat future work will overcome this challenge.\n4.3. Ambiguous tasks make labeling hard\nEvaluation of a summary is both subjective and multidimen-\nsional. A single human labeler may have a clear notion of\nwhether a given sample is separately accurate, grammatical,\nnonredundant, or covers all important topics; but in our ex-\nperiments a labeler will often be asked to choose between\nsamples each of which has some deﬁciencies. In choos-\ning which of four samples is the best, a labeler must trade\noff between different desiderata. This makes consistent la-\nbeling difﬁcult for honest labelers (including the authors!),\nand makes it difﬁcult to quickly detect problematic label-\ners. It also makes the research more difﬁcult to present\nand interpret: during our experiments we routinely checked\nthe performance of models by having authors label results,\nsince we knew the authors would attempt to do the task hon-\nestly, but were epistemically uneasy about reporting these\nnumbers in the paper (table 8 is the one exception).\nOne could hope to cope with such “noise” by simply getting\nmore labels and averaging them, but this does not resolve\nall the practical difﬁculties with ambiguity. When possible,\nit seems better to design less ambiguous labeling tasks that\nget at the same information. For example, rather than asking\na person to rate or compare summaries, we could ask for\na verbal description of the problems with a summary, or a\nsuggested correction. If problems don’t exist we are done;\notherwise describing a problem does not require consistently\npicking the same most important problem. Even if two\npeople disagree on the most important problem, they may be\nmore likely to agree that the other pickedsome problem, and\nmore agreement eases data quality control and the overall\nexperimental process.\n4.4. Bugs can optimize for bad behavior\nOne of our code refactors introduced a bug which ﬂipped\nthe sign of the reward. Flipping the reward would usually\nproduce incoherent text, but the same bug also ﬂipped the\nsign of the KL penalty. The result was a model which opti-\nmized for negative sentiment while still regularizing towards\nnatural language. Since our instructions told humans to give\nvery low ratings to continuations with sexually explicit text,\nthe model quickly learned to output only content of this\nform, regardless of how innocuous the starting point was.\nThis bug was remarkable since the result was not gibberish\nbut maximally bad output. The authors were asleep during\nFine-Tuning Language Models from Human Preferences\nthe training process, so the problem was noticed only once\ntraining had ﬁnished. A mechanism such as Toyota’s Andon\ncord could have prevented this, by allowing any labeler to\nstop a problematic training process.\n5. Conclusion\nWe have demonstrated RL ﬁne-tuning of language models to\nfour NLP tasks: stylistic continuation with high sentiment or\nphysically descriptive language, and summarization on the\nCNN/Daily Mail and TL;DR datasets. Rather than building\ntask-speciﬁc techniques, we achieve our results by straight-\nforwardly applying reward learning to language generation.\nWe extend previous reward learning work with pretrained\nmodels and KL regularization to prevent the policy from\ndiverging too far from natural language.\nOur results are mixed. On the continuation tasks we achieve\ngood results vs. the zero-shot baseline as evaluated by hu-\nmans with very few samples: 2.5k for sentiment and 5k\nfor descriptiveness. However, for both summarization tasks\nour policies are only “smart copiers” (extractive rather than\nabstractive): they copy from the input text but skip over\nirrelevant preamble. The advantage of copying is truthful-\nness: by comparison the zero-shot and supervised models\nproduce natural, plausible-looking summaries that are often\nlies. We believe the limiting factor in our experiments is\ndata quality, in particular exacerbated by the online data\ncollection setting, and plan to ameliorate this with batched\ndata collection in future.\nWe believe the application of human reward learning to\nnatural language tasks is important both from a capability\nand safety perspective. On the capability side, purely super-\nvised training is insufﬁcient to correct mistakes that arise\nwhen sampling from trained policies, and RL training to\nprogrammatic reward functions such as BLEU or ROUGE\nis insufﬁcient: Paulus et al. (2017) conclude that \"optimiz-\ning for single discrete evaluation metric[s] such as ROUGE\nwith RL can be detrimental to the model quality.\" Interac-\ntive tasks such as dialogue are particularly relevant: it is\ndifﬁcult to deﬁne the goal of a dialogue without the human\nparticipant, and the length of dialogue makes it more likely\nthat supervised learned models will go off distribution. In\nthe supervised case NLP models are trained using human\ndata; if we want RL ﬁne-tuning we need human data too.\nOn the AI safety side, interactive communication between\nhumans and ML models is a requirement for scalable re-\nward learning methods such as ampliﬁcation, debate, and\nrecursive reward modeling (Christiano et al., 2018; Irving\net al., 2018; Leike et al., 2018), and natural language is how\nhumans communicate complex ideas. Although language\nmodels are unlikely to be ready for these tasks in their full\ngenerality, Perez et al. (2019) demonstrates that debate al-\nready improves generalization for question-answering when\ndebaters quote from a source text. Using direct human\npreferences for language tasks is a step in the direction\nof scalable reward learning for language, and we believe\nfurther steps are possible.\nAcknowledgments\nWe thank Akshat Bubna, Shariq Hashme, and many others at\nScale for their work on data collection, Shan Carter for help\nwith visualizations, Scott Gray for help with low precision\ntraining, Shahbaz Syed for information about the TL;DR\ndataset, and Christine Payne, Miles Brundage, Jared Kaplan,\nJan Leike, Ethan Perez, and Jelena Luketina for helpful\ncomments on the paper.\nReferences\nJordan T Ash, Chicheng Zhang, Akshay Krishnamurthy,\nJohn Langford, and Alekh Agarwal. Deep batch ac-\ntive learning by diverse, uncertain gradient lower bounds.\narXiv preprint arXiv:1906.03671, 2019.\nDzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes,\nArian Hosseini, Pushmeet Kohli, and Edward Grefen-\nstette. Learning to understand goal speciﬁcations by mod-\nelling reward. arXiv preprint arXiv:1806.01946, 2018.\nForian Böhm, Yang Gao, Christian Meyer, Ori Shapira, Ido\nDagan, and Iryna Gurevych. Better rewards yield better\nsummaries: Learning to summarise without references. In\nConference on Empirical Methods in Natural Language\nProcessing, 2019.\nWoon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun\nLi, Michel Galley, Chris Brockett, Mengdi Wang, and\nJianfeng Gao. Towards coherent and cohesive long-form\ntext generation. In Proceedings of the First Workshop on\nNarrative Understanding, pages 1–11, 2019.\nPaul Christiano, Jan Leike, Tom Brown, Miljan Martic,\nShane Legg, and Dario Amodei. Deep reinforcement\nlearning from human preferences. In Advances in Neural\nInformation Processing Systems, pages 4302–4310, 2017.\nPaul Christiano, Buck Shlegeris, and Dario Amodei. Super-\nvising strong learners by amplifying weak experts. arXiv\npreprint arXiv:1810.08575, 2018.\nAndrew M Dai and Quoc V Le. Semi-supervised sequence\nlearning. In Advances in neural information processing\nsystems, pages 3079–3087, 2015.\nPrafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex\nNichol, Matthias Plappert, Alec Radford, John Schulman,\nFine-Tuning Language Models from Human Preferences\nSzymon Sidor, Yuhuai Wu, and Peter Zhokhov. Ope-\nnai baselines. https://github.com/openai/\nbaselines, 2017.\nYang Gao, Christian M Meyer, and Iryna Gurevych.\nPreference-based interactive multi-document summarisa-\ntion. arXiv preprint arXiv:1906.02923, 2019a.\nYang Gao, Christian M Meyer, Mohsen Mesgar, and Iryna\nGurevych. Reward learning for efﬁcient reinforcement\nlearning in extractive document summarisation. arXiv\npreprint arXiv:1907.12894, 2019b.\nSebastian Gehrmann, Yuntian Deng, and Alexander M Rush.\nBottom-up abstractive summarization. arXiv preprint\narXiv:1808.10792, 2018.\nSebastian Gehrmann, Zachary Ziegler, and Alexander Rush.\nGenerating abstractive summaries with ﬁnetuned lan-\nguage models. In TL;DR Challenge System Descriptions,\n2019.\nDaniel Gissin and Shai Shalev-Shwartz. Discriminative\nactive learning. arXiv preprint arXiv:1907.06347, 2019.\nYuhong Guo and Dale Schuurmans. Discriminative batch\nmode active learning. In Advances in neural information\nprocessing systems, pages 593–600, 2008.\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel\nMazare, and Jason Weston. Learning from dialogue af-\nter deployment: Feed yourself, chatbot! arXiv preprint\narXiv:1901.05415, 2019.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette,\nLasse Espeholt, Will Kay, Mustafa Suleyman, and Phil\nBlunsom. Teaching machines to read and comprehend.\nIn Advances in neural information processing systems ,\npages 1693–1701, 2015.\nJeremy Howard and Sebastian Ruder. Universal language\nmodel ﬁne-tuning for text classiﬁcation. arXiv preprint\narXiv:1801.06146, 2018.\nJiaji Huang, Rewon Child, Vinay Rao, Hairong Liu, Sanjeev\nSatheesh, and Adam Coates. Active learning for speech\nrecognition: the power of gradients. arXiv preprint\narXiv:1612.03226, 2016.\nBorja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving,\nShane Legg, and Dario Amodei. Reward learning from\nhuman preferences and demonstrations in Atari. In Ad-\nvances in Neural Information Processing Systems, 2018.\nURL https://arxiv.org/abs/1811.06521.\nGeoffrey Irving, Paul Christiano, and Dario Amodei. AI\nsafety via debate. arXiv preprint arXiv:1805.00899, 2018.\nURL https://arxiv.org/abs/1805.00899.\nNatasha Jaques, Shixiang Gu, Dzmitry Bahdanau,\nJosé Miguel Hernández-Lobato, Richard E Turner, and\nDouglas Eck. Sequence tutor: Conservative ﬁne-tuning\nof sequence generation models with kl-control. In Pro-\nceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1645–1654. JMLR. org,\n2017.\nNatasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen,\nCraig Ferguson, Agata Lapedriza, Noah Jones, Shixiang\nGu, and Rosalind Picard. Way off-policy batch deep\nreinforcement learning of implicit human preferences in\ndialog. arXiv preprint arXiv:1907.00456, 2019.\nUrvashi Khandelwal, Kevin Clark, Dan Jurafsky, and\nLukasz Kaiser. Sample efﬁcient text summarization\nusing a single pre-trained transformer. arXiv preprint\narXiv:1905.08836, 2019.\nDiederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nJulia Kreutzer, Joshua Uyheng, and Stefan Riezler. Reli-\nability and learnability of human bandit feedback for\nsequence-to-sequence reinforcement learning. arXiv\npreprint arXiv:1805.10627, 2018.\nWojciech Kry´sci´nski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. Neural text\nsummarization: A critical evaluation. arXiv preprint\narXiv:1908.08960, 2019.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic,\nVishal Maini, and Shane Legg. Scalable agent align-\nment via reward modeling: a research direction. arXiv\npreprint arXiv:1811.07871, 2018.\nJiwei Li, Alexander H Miller, Sumit Chopra, Marc’Aurelio\nRanzato, and Jason Weston. Dialogue learning with\nhuman-in-the-loop. arXiv preprint arXiv:1611.09823 ,\n2016.\nJelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob\nFoerster, Jacob Andreas, Edward Grefenstette, Shimon\nWhiteson, and Tim Rocktäschel. A survey of reinforce-\nment learning informed by natural language. In Proceed-\nings of the Twenty-Eight International Joint Conference\non Artiﬁcial Intelligence, IJCAI-19 . International Joint\nConferences on Artiﬁcial Intelligence Organization, 8\n2019.\nJulian McAuley, Christopher Targett, Qinfeng Shi, and An-\nton Van Den Hengel. Image-based recommendations on\nstyles and substitutes. In Proceedings of the 38th Inter-\nnational ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval, pages 43–52. ACM,\n2015.\nFine-Tuning Language Models from Human Preferences\nKhanh Nguyen, Hal Daumé III, and Jordan Boyd-Graber.\nReinforcement learning for bandit neural machine trans-\nlation with simulated human feedback. arXiv preprint\narXiv:1707.07402, 2017.\nRomain Paulus, Caiming Xiong, and Richard Socher. A\ndeep reinforced model for abstractive summarization.\narXiv preprint arXiv:1705.04304, 2017.\nEthan Perez, Siddharth Karamcheti, Rob Fergus, Jason We-\nston, Douwe Kiela, and Kyunghyun Cho. Finding gener-\nalizable evidence by learning to convince Q&A models.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing, Hong Kong,\nChina, November 2019. Association for Computational\nLinguistics.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep contextualized word representations. arXiv\npreprint arXiv:1802.05365, 2018.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learn-\ning to generate reviews and discovering sentiment. arXiv\npreprint arXiv:1704.01444, 2017.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. Improving language un-\nderstanding by generative pre-training, 2018.\nURL https://s3-us-west-2.amazonaws.\ncom/openai-assets/research-covers/\nlanguage-unsupervised/language_\nunderstanding_paper.pdf.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019.\nURL https://d4mucfpksywv.cloudfront.\nnet/better-language-models/language_\nmodels_are_unsupervised_multitask_\nlearners.pdf.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and\nWojciech Zaremba. Sequence level training with recurrent\nneural networks. arXiv preprint arXiv:1511.06732, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\nAbigail See, Peter J Liu, and Christopher D Manning. Get\nto the point: Summarization with pointer-generator net-\nworks. arXiv preprint arXiv:1704.04368, 2017.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural\nmachine translation of rare words with subword units.\narXiv preprint arXiv:1508.07909, 2015.\nPradyumna Tambwekar, Murtaza Dhuliawala, Animesh\nMehta, Lara J Martin, Brent Harrison, and Mark O Riedl.\nControllable neural story generation via reinforcement\nlearning. arXiv preprint arXiv:1809.10736, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. InAdvances in\nneural information processing systems, pages 5998–6008,\n2017.\nMichael Völske, Martin Potthast, Shahbaz Syed, and Benno\nStein. TL;DR: Mining Reddit to learn automatic sum-\nmarization. In Proceedings of the Workshop on New\nFrontiers in Summarization, pages 59–63, 2017.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mo-\nhammad Norouzi, Wolfgang Macherey, Maxim Krikun,\nYuan Cao, Qin Gao, Klaus Macherey, et al. Google’s\nneural machine translation system: Bridging the gap be-\ntween human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\nYuxiang Wu and Baotian Hu. Learning to extract coherent\nsummary via deep reinforcement learning. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence, 2018.\nSanghyun Yi, Rahul Goel, Chandra Khatri, Tagyoung\nChung, Behnam Hedayatnia, Anu Venkatesh, Raefer\nGabriel, and Dilek Hakkani-Tur. Towards coherent\nand engaging spoken dialog response generation us-\ning automatic conversation evaluators. arXiv preprint\narXiv:1904.13015, 2019.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdi-\nnov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\nAligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 19–27, 2015.\nFine-Tuning Language Models from Human Preferences\nA. Instructions for labelers\nA.1. Sentiment\nYou will see snippets of text followed by several possi-\nble ways the text could be continued. Choose the most\npositive and happy continuation:\n• Consider all of the moods of the characters, the\nmood of the narrator, and the mood implied by the\nevents and descriptions.\n• If a continuation contains sexually explicit or other-\nwise offensive content, treat it as strongly negative.\nDo not choose it even if it expresses a positive\nattitude.\nA.2. Descriptiveness\nYou will see snippets of text followed by several possi-\nble ways the text could be continued. Choose the most\nvividly descriptive continuation:\n• Evaluate both on the quantity and on the vivid-\nness of physical details described.\n• The best continuations are full of details that give\na strong sense of what the scene looks, sounds, or\nsmells like.\n• Count only physical details, not details about ab-\nstract facts.\nA.3. Summarization: TL;DR\nYou will see some text followed by several summaries.\nPlease read the text and select the best summary.\nA summary is good if it:\n• Is useful and a good summary in general\n• Accurately states the important points of the text\n• Makes sense on its own\nA summary is bad if it:\n• Includes information that doesn’t appear in the text\nA.4. Summarization: CNN/DM\nYou will see an article followed by several summaries.\nPlease read the article and select the best summary.\nA summary is good if it:\n• Is useful and a good summary in general\n• Accurately states the important points of the article\n• Makes sense on its own\nA summary is bad if it:\n• Includes information that doesn’t appear in the\narticle\n• Includes quotations that don’t appear verbatim in\nthe article\nB. Human labeling details\nOur quality assurance process was handled by Scale AI,\nthough Scale made signiﬁcant changes to their usual quality\nsystems in order to deal with subjective tasks and provide\nvery fast turnaround. Since we initially believed online data\ncollection would be crucial, even the ofﬂine experiments\nwere collected with this fast turnaround requirement. In the\nfuture we plan to use a more relaxed latency requirement.\nThe ﬁrst step of data collection involves teaching the task\nto a small number of trusted Scale labelers by giving them\na description of the task (appendix A). Scale uses these la-\nbelers to collect a large number of benchmark data points\nwhere several trusted labelers agree (out of a large set of\nunlabeled data points from ρ). During full data collection,\nScale serves these benchmark data points to freelance work-\ners alongside real unlabeled data for training (the two types\nof data are indistinguishable when π = ρ, though they do\nbecome distinguishable during training), maintaining a con-\nﬁdence model for the performance of each labeler on the\nbenchmark distribution. The probability of getting a bench-\nmark vs. a real sample varies dynamically on factors such as\nconﬁdence in the labeler to correctly label a certain category.\nFreelancers who fail to perform well on benchmark tasks\nare ﬁltered out. Additionally, Scale makes ad-hoc improve-\nments to quality control over time, sometimes validating\nquality by comparing to a small number of gold-standard\nlabels from the authors.\nWe evaluated the data quality after the fact on two of the\ntasks. During all data collection, 5% of queries were an-\nswered by 5 distinct labelers. We sampled 100 of these\nqueries (restricting to ones generated from ρ) and had two\nauthors label each one. Based on this data, we estimated the\nrate of agreement between authors and Scale labelers, pairs\nFine-Tuning Language Models from Human Preferences\nP(agreement) Sentiment TL;DR\nBetween random responses 25% 25%\nBetween labelers 38±2% 46±2%\nBetween an author & a labeler 44±5% 38±5%\nBetween authors 62±5% 61±5%\nTable 9: Agreement probabilities for two tasks, i.e. the\nprobability of both individuals choosing the same sample as\nbest, out of 4.\nof labelers, and pairs of authors. As table 9 shows, the data\ncontained a signiﬁcant amount of signal but did not match\nthe quality of data which was hand-labeled by the authors.\nAn earlier version asked labelers for 1-10 ratings; in the best\ncase this provides more information per label, but it was\ndifﬁcult to gauge labeler performance. Normalization was\nrequired since two good labelers would often differ by a\n(noisy) monotonic transform. If many scores concentrated\non a few values (say 7 and 8) simple strategies could fool\nthe ﬁltering process. Absolute scores also tended to drift\nover the training process, as labelers would adjust to the\nnew distribution of samples from the changing policy.\nFinding high-quality workers involves human answering\nquality control questions which are not used in our experi-\nments, and throwing away data from low-quality workers.\nSo the total human cost of experiments is somewhat higher\nthan the number of labels we actually use (which is what we\nreport). For a short training run this can easily dominate the\nactual label requirements, though it can be amortized across\nseveral tasks by identifying consistently good workers. For\nour longer training runs the additional number of labels was\nmodest. (None of these details are exposed to customers.)\nC. Samples\nSamples from our models are shown in the following tables:\n• Mock sentiment continuation without a KL penalty:\ntable 10\n• Sentiment continuation: table 11\n• Descriptiveness continuation: table 12\n• TL;DR summarization: tables 13 to 15\n• CNN/Daily Mail summarization: tables 16 to 18\nFine-Tuning Language Models from Human Preferences\nSamples without a KL penalty\ncontext And she would always love him for that. And for many other reasons. He had given\nher so much and she\nContinuation 1 Continuation 2\nzero-shot had no idea how to release that gift, but she\nknew\nhad been unable to give it back.\n\"You\nno penalty /\nbonus\n! These These These sound ﬂowed instantly\neasily easily easily easily!\n! These These These sound ﬂowed instantly\neasily easily easily easily!\nentropy\nbonus\ninitially initiallyprisingly easilyprisingly\nLiam! Love this ride instantly tightly\n’ equally particularlyprisinglyprisingly\nLiam Liam! Love this ride instantly\ncontext Palmer rolled her eyes. \"Don’t give me that. You didn’t come here, wearing that\nContinuation 1 Continuation 2\nzero-shot corset and corset and a blasted corset. If old-fashioned, sexy little dress. You came\nhere to\nno penalty /\nbonus\n! These These These sound ﬂowed instantly\neasily easily easily easily!\n! These These These sound ﬂowed instantly\neasily easily easily easily!\nentropy\nbonus\ninitially relativelyprisingly instantlypris-\ningly Liam Liam! Love this ride instantly\noverwhelmingly Loganprisingly Liam in-\nstantly surprisingly! Love this ride instantly\ntightly\ncontext Is it conceivable that - I don’t know - he somehow wasn’t on the aircraft that exploded?\nLike maybe he\nContinuation 1 Continuation 2\nzero-shot simply didn’t hear me coming or he was\njust coming out\nwas just on the ground.\"\n\"Yes, at\nno penalty /\nbonus\n! These These These sound ﬂowed instantly\neasily easily easily easily!\n! These These These sound ﬂowed instantly\neasily easily easily easily!\nentropy\nbonus\nprisinglyprisingly instantly Liam instantly\noverwhelmingly! Love this ride instantly\ntightly\ninstantly initially overwhelmingly Chloe!\nLove this ride instantly instantly instantly\ntightly\nTable 10: Samples from a model ﬁne-tuned to mock sentiment without a KL penalty. Without regularization towards natural\nlanguage, the results are gibberish even if we include an entropy bonus (targeting 30 nats). Both policies reach a mean score\nof around +8.0 according to the mock sentiment model (i.e. 99.97% positive).\nFine-Tuning Language Models from Human Preferences\nSamples for positive-sentiment continuation task\ncontext \"He also doesn’t have a backbone,\" Quinn continued as they walked. \"Last year, I had\na great story about Paul Gibson, who is now the chairman of the board of supervisors.\nContinuation 1 Continuation 2 Continuation 3\nzero-shot He was running for the\nsupreme court and his cam-\npaign was violent. There was\nan incident at a press confer-\nence.\nFor weeks, I went around the\ncounty, telling everyone how\namazing he was and how\nmuch I admired him.\nHe was one of the ﬁrst teach-\ners to lead by example. It\nwas incredibly inspiring to\nsee.\nmock I was really impressed with\nhow he handled the board,\nand I thought it was a great\nstory.\nHe was a real hero to the peo-\nple in town. He was a real\nhero.\nBut he just sat around and\nwatched. I love that story.\nYou should read it again.\n5k ofﬂine\nﬁne-tune\nPaul Gibson is a very intelli-\ngent man. He’s a good heart\nman. He has a good sense of\nhumor.\nPaul is a very nice guy, a true\ngentleman, and we all know\nhe’s a really cute guy.\nHe was a good man, with a\ngreat smile, a great sense of\nhumor, a great family.\ncontext Pearl thought to herself that what they were about to do was exactly the sort of thing\nthat they could do to help the villagers. They were all terriﬁed of these guys.\nAt the police station the three walked up to the counter behind which was a senior\nconstable studying some papers.\nContinuation 1 Continuation 2 Continuation 3\nzero-shot \"Hello, I’m Pearl and this is\nmy friend, Mike,\" said Pearl.\n\"May we speak to the police\nofﬁcer, sir?\" asked the one in\ncharge.\n'Hello, can I help you?'\n'Yes, we’re the same people\nthat the people were talking\nabout.\nmock \"I’m Detective Sergeant\nLewis, this is Detective\nSergeant Jones and this is\nDetective Sergeant Porter.\nHe looked up at the three\nwith a friendly smile.\n\"Good day, gentlemen.\n\"We have a friend who is in\ntrouble. He was kidnapped\nby these guys,\" the boy said.\n5k ofﬂine\nﬁne-tune\nHe turned to them with a\nsmile. \"Good afternoon,\nladies. I’m Detective Inspec-\ntor Jones.\nThe constable stood up and\nsmiled as he saw them, obvi-\nously pleased to see them.\nHe smiled at them and waved\nthem in, his eyes twinkling as\nhe listened to their tales.\ncontext The room was warm and welcoming, with two hearths that burned vigorously. The\nVikings pushed her to the ﬂoor at Gunnar’s feet. Ailia looked up at Gunnar and rose to\nher feet as calmly as possible.\nContinuation 1 Continuation 2 Continuation 3\nzero-shot \"I will not ﬁght.\"\n\"You will ﬁght for us,\" Gun-\nnar said.\n\"I’ll give you one last chance\nto tell me where that is,\"\nBjorn said.\n\"I’m sorry. I didn’t mean\nto. . . \"\n\"Give me your hand.\" Gun-\nnar told her.\nmock \"I love this. I love this whole\nroom. I’m going to enjoy it.\n\"I am very pleased to meet\nyou, and let me assure you,\nyou will not regret this.\n\"Bravely done, Gunnar.\nThank you.\" Ailia smiled at\nhim.\n5k ofﬂine\nﬁne-tune\nGunnar smiled at Ailia and\nwaited for her to smile back.\nShe was smiling, she knew.\n\"You’ve been a true friend\nto my son, Gunnar. Thank\nyou.\" She smiled.\nGunnar was smiling at her\nand his pretty, blue eyes were\nﬁlled with contentment.\nTable 11: Continuations for our positive-sentiment continuation task, with contexts randomly sampled from BookCorpus test\nset. Random samples from pretrained, mock sentiment, and 5k ofﬂine descriptiveness models, using temperature T = 0.7.\nFine-Tuning Language Models from Human Preferences\nSamples for descriptiveness continuation task\ncontext \"I do not know if it was Viking related, but it could have been.\"\n\"Really?\" Ailia said. Is it safe to be traveling here then? Ailia looked behind her to\nmake sure they weren’t being followed.\nContinuation 1 Continuation 2 Continuation 3\nzero-shot There were no signs of any-\none.\n\"It is safe enough,\" Ailios\nsaid.\n\"Because I have a friend that\nis in the area and he will be\ncoming with us.\nIt was hard to see that far. \"I\ndo not like that word.\n5k ofﬂine\nﬁne-tune\nKaya crouched low, her eyes\nwide in the moonlight. Her\nbody was tense.\nShe put her hand on the\nsword strapped to her back,\nand then pulled it out.\nShe strode out the door\nand walked down the street,\nher nose wrinkled in disap-\nproval.\ncontext \"They took me to a cellar. An’ it were full of girls, it were, all sewin’. At ﬁrst I thought\nit weren’t too bad. I don’t mind work, really I don’t. Mama said I was a good ’elper.\nContinuation 1 Continuation 2 Continuation 3\nzero-shot But the girls keep on sneakin’\nin ’em. An’ one day, one of\nem found me.\nBut in the end, it weren’t any-\nthing like she’d promised me.\nSome of them were worse’n\nme.\nI was just lucky to get this\njob. But as I stayed there, I\nstarted to see things.\n5k ofﬂine\nﬁne-tune\nBut...\" The girl hiccupped,\nthen pressed her lips together\nand shook her head.\nBut then...\" His voice trailed\noff and he shook his head, his\neyes going distant, somber.\nShe said I could keep my\neyes open and my head clear\nand make a good target.\ncontext She forced herself up and shifted closer to him until she was kneeling behind him.\nFollowing her instincts, she laid a splayed hand on his bare back. He tensed beneath\nher touch. Quietly, Tara found her words. \"I will never lie to you again, Max.\nContinuation 1 Continuation 2 Continuation 3\nzero-shot I love you.\"\nHe turned to her, his\nface a mask of pain.\nYou have to know that.\"\n\"I know you will never\nlie to me again.\nWhatever happens, I’ll tell\nyou the truth.\"\n\"I’m sorry.\n5k ofﬂine\nﬁne-tune\nI promise.\"\nHis hands curled around her\nforearms, his ﬁngers digging\ndeep into her ﬂesh.\nI swear it.\"\nHe exhaled heavily and\nhugged her close, pressing\nhis lips to her forehead.\nI love you.\"\nMax leaned his head\nback and closed his eyes.\nHis breathing changed.\nTable 12: Continuations for our descriptiveness continuation task, with contexts randomly sampled from BookCorpus test\nset. Random samples from pretrained and 5k ofﬂine descriptiveness models, using temperature T = 0.7.\nFine-Tuning Language Models from Human Preferences\nSamples for TL;DR summarization task (part 1)\ncontext post Okay, this was actually going to go into a comment on youtube, but Emma said she\nwanted more trafﬁc on reddit.\nI am a musician myself, have been for many years now. While Emma’s mu-\nsic is not the most complicated technical death metal stuff out there, I love it dearly.\nHer music actually moves me. No matter what she plays, be it a cover or original stuff,\nher music *touches* me on a deep personal level. And I have absolutely no clue why. I\nthink it’s a combination of her voice, her lyrics and the overall sound design of her\nsongs and covers.\nBut this isn’t always a good thing, I feel. I can’t listen to her music for hours\non end. Only always a few songs, nothing more. Her music destroys me emotionally.\nOkay, that’s a bit harsh, but she can get me, a (nearly) adult on the verge of tears.\nI’ve gone through some shit in my life, who hasn’t, and I feel like her music can\nactually get some of it out. I am incredibly thankful that she makes her music the way\nshe makes it. I can honestly say that Emma Blackery is my favourite artist. Keep in\nmind that I listen to a lot of music, from crossover thrash metal over 80’s pop, berlin\npolygenre to classic/romantic music.\nI am also very jealous of her ability. I play quite a few instruments myself\n(guitar, piano, bass, drums, synth, saxophone, percussion) and aim to study audio engi-\nneering. I can deﬁnitely say that music is my literal life and mean it. I’ve released my\nﬁrst full length album at age 20. Yet I am very jealous that she is as good as she is and\nI really wanna know what her secret is. I love and hate her music, does that make sense?\nHer music deﬁnitely makes me think, and that’s what art is all about, right?\nAnyway, I’m gonna continue working on my cover of The Promise and try to ﬁgure\nout what it is she’s doing. Thank you for reading this far.\nreference\nsummary\nEmma music gud yes very love magic\nzero-shot I’m a musician myself, have been for many years now. And Emma Blackery is my\nfavourite artist.\n60k ﬁne-tune I am a musician myself, have been for many years now. While Emma’s music is not the\nmost technical death metal stuff out there, I love it dearly. Her music actually moves\nme. No matter what she plays, be it a cover or original stuff, her music *touches* me\non a deep personal level.\nsupervised I’m jealous of Emma Blackery’s music and I have no clue why.\nsupervised +\n60k ﬁne-tune\nEmma Blackery is my favourite artist, and I love her music to death. I wish I could\nlisten to her music all the time. However, I feel like her music destroys me emotionally.\nI feel like I need to enjoy her music for the human part of it, and not the technical part.\nShe’s my favourite artist.\nTable 13: Random (T = 0.7) summaries for our TL;DR summarization task, with contexts randomly sampled from TL;DR\nvalidation set.\nFine-Tuning Language Models from Human Preferences\nSamples for TL;DR summarization task (part 2)\ncontext post It really does. I did the ﬁrst 5 seasons in a couple months, and ultimately I enjoyed\nthem. The ﬁrst three seasons cover just about all the really good stuff. Russell\nEdgington was, to me, the best supporting character in the show. He was everything\nthat I valued about the show, which is to say, campy, violent, overly-gory fun. I\nwent back to the show a little over a year later, and while S6 was tolerable with a\nfew fun bits, S7 totally eroded. Tara’s plot goes full retard in the last season, people\ndie for the sake of drama and convenience or leave with no real reverence for their\nhistory. Suddenly no one can make a good decision, and there was a needless romance\nbait-and-switch that ruined what I felt the show was moving towards.\nWhat’s terrible is that once you’ve done six seasons it’s hard to walk away,\nbut each episode of seven stings more than the last. You don’t get that weird\nhalf-hollow/half-warm feeling you should when you ﬁnish a series of books or a long\nTV show. Outside of Pam and Erik, I didn’t give two fucks about any character by the\nend.\nreference\nsummary\nSeasons one through three were a blast. Four was pretty meh, ﬁve and six weren’t too\nbad, seven sucked massively.\nzero-shot I ﬁnished S6 and still feel like I’m in the same place as I was with S5.\nFavorite Episodes:\nS1: \"Tough Love\"\nS2: \"Crossover\"\nS3: \"The Party\"\n60k ﬁne-tune I did the ﬁrst 5 seasons in a couple months, and ultimately I enjoyed them. The ﬁrst\nthree seasons cover just about all the really good stuff. Russell Edgington was, to me,\nthe best supporting character in the show. He was everything that I valued about the\nshow, which is to say, campy, violent, overly-gory fun\nsupervised 10/10 would watch again\nsupervised +\n60k ﬁne-tune\nThe ﬁrst two seasons of the show were enjoyable. The show’s progress in the last\nthree seasons is frustrating because it’s so similar to the ﬁrst three seasons, and it’s so\none-sided that there’s no reason to keep going. But the show’s greatest strength has\nalways been its great writing, and it’s giving me that same feeling again.\nTable 14: Random (T = 0.7) summaries for our TL;DR summarization task, with contexts randomly sampled from TL;DR\nvalidation set.\nFine-Tuning Language Models from Human Preferences\nSamples for TL;DR summarization task (part 3)\ncontext post > you should be happy, lots of votes were counted twice in precincts that went to\nClinton in a landslide, if they recount them by hand she will lose more votes, and\nprobably a lot...\nYou are assuming you understand what the problem here is and what the end\nresult on the election was. The article does not state where the problem lies.\nE.g. The machine shows a tally of 120 votes having been processed on it.\nThe actual reported vote tally to the election board was 100 votes. The 20 vote\ndifferential on the machine is due to 20 times a ballot needing to be re-scanned due to\nerror. No double-counting of votes occurs but there is also a delta between what the\nmachine says was recorded vs what was reported.\nIn this case the ﬁnal reported vote tallies may have been factually correct but\nthere is no way to validate them against the machine record.\nTo understand the problem an analysis and deconstruction of the machines\nand the election night events must be undertaken. Find out the cause of the problem, if\nthe problem caused votes to be over/under counted, or if the problem was solely with\nmachine data that was not used for tabulation of results.\nreference\nsummary\nneither you nor I have any idea what the actual problem is so there is no basis for\nsaying the problem favors either candidate at this time, or if either candidate would\neven be affected.\nzero-shot Election ofﬁcials did not count ballots in a timely manner, by hand, to meet the\nthreshold of getting votes counted on election night.\nV oters were not eligible to vote on election night and were not aware of the\nproblems.\nThe election ofﬁcials did not realize that their system was broken until very\nlate in the night.\n60k ﬁne-tune The machine shows a tally of 120 votes having been processed on it. The actual\nreported vote tally to the election board was 100 votes. The 20 vote differential on\nthe machine is due to 20 times a ballot needing to be re-scanned due to error. No\ndouble-counting of votes occurs but there is also a delta between what the machine\nsays\nsupervised Stop assuming the election was completely free of double-counting before you even\nbegin to discuss the actual results. For any result to be close, it must be based on the\ntotal vote totals reported to the election board and not just the machine results. The\nelection board did not report the machine results to the election board.\nsupervised +\n60k ﬁne-tune\nThe reported vote tallies may have been factually correct but there is no way to validate\nthem against the machine record. The machine may have actually been accurate and the\nmachine may have been faulty in other ways. The election may have been considered\nfair but the machine may not have been. The results are still in dispute.\nTable 15: Random (T = 0.7) summaries for our TL;DR summarization task, with contexts randomly sampled from TL;DR\nvalidation set.\nFine-Tuning Language Models from Human Preferences\nSamples for CNN/DM summarization task (part 1)\ncontext arti-\ncle\nGoogle launched a new U.S. wireless service today that switches between Wi-Fi and\ncellular networks\nGoogle is already the world’s most popular phone software provider, and a\npay-TV operator - and now it wants to be your mobile network carrier.\nThe company has unveiled a U.S. wireless service that switches between Wi-\nFi and cellular networks to curb data use and keep phone bills low.\nThe service, called 'Project Fi, ' debuted today, about two months after Google\nrevealed its plans to expand its ever-growing empire into providing wireless\nconnections for smartphones.\nGoogle is selling the basic phone service for $20 a month and will only charge\ncustomers for the amount of cellular data that they use each month, instead of a ﬂat rate.\nEach gigabyte of data will cost $10 a month. That means a customer could\nsign up for a plan offering three gigabytes of data and get $20 back if only one gigabyte\nwas used in a month.\n[...Truncated...]\nRather than building its own network, Google is leasing space on cellular\ntowers built by Sprint and T-Mobile, which are hoping the deals will boost their proﬁts\nwithout costing them too many customers tempted to defect to Project Fi.\nreference\nsummary\nProject Fi will be hosted through Sprint Corp and T-Mobile’s networks. It costs $20\nfor basic service and unused data is paid back to customer. The invitation-only service\nwill work only on Nexus 6 phones in the US. Numbers will live in the cloud so users\ncan talk on any connected tablet.\nzero-shot Project Fi is a new smartphone service that switches between Wi-Fi and cellular\nnetworks to curb data use and keep phone bills low. The service is available for an\nintroductory price of $20 a month and will only charge customers for the amount of\ncellular data that they use each month.\n60k ﬁne-tune Google is already the world’s most popular phone software provider, and a pay-TV\noperator - and now it wants to be your mobile network carrier.\nThe company has unveiled a U.S. wireless service that switches between Wi-\nFi and cellular networks to curb data use and keep phone bills low.\nsupervised Google launched a new U.S. wireless service today. Called 'Project Fi,' it switches\nbetween Wi-Fi and cellular networks. The service, called 'Project Fi,'debuted today.\nAbout two months after Google announced its plans to expand its ever-growing empire\ninto providing wireless connections for smartphones.\nsupervised +\n60k ﬁne-tune\nGoogle launched a new U.S. wireless service today that switches between Wi-Fi and\ncellular networks. The service, called 'Project Fi,'debuted today, about two months after\nGoogle revealed its plans to expand its ever-growing empire into providing wireless\nconnections for smartphones.\nTable 16: Random (T = 0.5) summaries for our CNN/DM summarization task, with contexts (shown truncated) randomly\nsampled from CNN/DM test set.\nFine-Tuning Language Models from Human Preferences\nSamples for CNN/DM summarization task (part 2)\ncontext arti-\ncle\nWith the date of their hugely anticipated bout less than a month away, Floyd\nMayweather and Manny Pacquiao’s training intensity has hit gruelling levels ahead of\nthe clash in Las Vegas.\nMayweather is as renowned for showing off his extravagant lifestyle as he is\nfor going undefeated in his professional career.\nBut the latest pictures released from his training camp, appearing to show\nhim hanging a weight from his head, displays the extreme level he hits in order to be as\nﬁt as possible for his ﬁghts.\nManny Pacquiao (right) poses alongside actor Mark Wahlberg, who visited\nthe Filipino at training\nFloyd Mayweather released two pictures from his training camp appearing\nhold a weight from his head\nMayweather (left) takes on Pacquiao in a £160million mega-ﬁght in Las Ve-\ngas on May 2\nMayweather (left) appeared to take his training intensity to a new height be-\nfore his meeting with Pacquiao\nAs for Pacquiao, he had a famous visitor check up on his preparations for\nMay 2.\n[...Truncated...]\nPacquiao and Mayweather’s clash will be a reported £160million mega-ﬁght,\nwith Pacquiao seeking to inﬂict a ﬁrst ever defeat upon Mayweather.\nThe 36-year-old Pacquiao also uploaded a picture to his Twitter page away\nfrom training with his daughter.\nreference\nsummary\nManny Pacquiao took time out from training to meet Mark Wahlberg. Floyd May-\nweather shared a picture of him holding a weight with his head. The due meet in Las\nVegas in a £160million encounter on May 2.\nzero-shot Mayweather and Pacquiao are going to clash in Las Vegas on May 2.\n60k ﬁne-tune With the date of their hugely anticipated bout less than a month away, Floyd\nMayweather and Manny Pacquiao’s training intensity has hit gruelling levels ahead of\nthe clash in Las Vegas.\nMayweather is as renowned for showing off his extravagant lifestyle as he is\nfor going undefeated in his professional career.\nsupervised Floyd Mayweather and Manny Pacquiao will ﬁght in Las Vegas on May 2. Pacquiao\nhas a visitor check up on his preparations for the bout. Mayweather released pictures\nof his training camp to social media.\nsupervised +\n60k ﬁne-tune\nFloyd Mayweather and Manny Pacquiao will ﬁght in Las Vegas on May 2. Mayweather\nreleased two pictures from his training camp appearing hold a weight from his head.\nThe 36-year-old boxer is as renowned for showing off his extravagant lifestyle as he is\nfor going undefeated in his professional career.\nTable 17: Random (T = 0.5) summaries for our CNN/DM summarization task, with contexts (shown truncated) randomly\nsampled from CNN/DM test set.\nFine-Tuning Language Models from Human Preferences\nSamples for CNN/DM summarization task (part 3)\ncontext arti-\ncle\nThe Solar Impulse 2, the experimental plane attempting to ﬂy around the world without\nusing a drop of fuel, has been grounded by the weather in China.\nWhat was supposed to be an overnight pit stop in the southwestern city of\nChongqing has now stretched into a two-and-a-half week stay.\nIt’s an unexpected layover that is testing the patience of the pilots and more\nthan 60 team members, just a quarter of the way along a journey that will cover some\n35,000 kilometers (21,748 miles) over ﬁve months.\n\"You can believe that your will can inﬂuence the weather,\" says pilot Bertrand Piccard,\nwho comes from a long family line of explorers. But \"then you’ll be really disappointed\nbecause it doesn’t work.\n\"Or you just know that it’s an adventure you do with the forces of nature.\"\nPiccard, along with Andre Borschberg, a Swiss engineer, are taking turns ﬂy-\ning the single-seater aircraft, which took off from Abu Dhabi on March 9 and has\nsuccessfully ﬂown through Oman, India, and Myanmar.\nThere is much at stake.\n[...Truncated...]\n\"Our boss is the sun,\" says Solar Impulse spokeswoman Claudia Durgnat.\nDurgnat says there may be a tiny window ahead, with the next possible depar-\nture from Chongqing on Tuesday. \"It’s not good before, and the days after don’t look\nvery clear.\"\nreference\nsummary\nSolar plane attempting to be ﬁrst to circumnavigate world without using fuel is stuck in\nChina. Solar Impulse 2 attempts to prove the power of renewable energy.\nzero-shot The plane has been grounded in China for two weeks because of bad weather.\n60k ﬁne-tune The Solar Impulse 2, the experimental plane attempting to ﬂy around the world without\nusing a drop of fuel, has been grounded by the weather in China.\nWhat was supposed to be an overnight pit stop in the southwestern city of\nChongqing has now stretched into a two-and-a-half week stay.\nsupervised Solar Impulse 2 has been grounded by the weather in China. The plane took off from\nAbu Dhabi on March 9. The plane is trying to prove the power of renewable energy.\nsupervised +\n60k ﬁne-tune\nSolar Impulse 2 has been grounded in China for two-and-a-half weeks. The plane\nis attempting to ﬂy around the world without using a drop of fuel. The team, which\nincludes Bertrand Piccard, is taking turns ﬂying the single-seater. The plane took off\nfrom Abu Dhabi on March 9 and has successfully ﬂown through Oman.\nTable 18: Random (T = 0.5) summaries for our CNN/DM summarization task, with contexts (shown truncated) randomly\nsampled from CNN/DM test set.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.44804537296295166
    },
    {
      "name": "Natural language processing",
      "score": 0.33473557233810425
    },
    {
      "name": "Linguistics",
      "score": 0.3231702148914337
    },
    {
      "name": "Philosophy",
      "score": 0.10791647434234619
    }
  ]
}