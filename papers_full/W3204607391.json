{
  "title": "A Comparative Study of Transformer-Based Language Models on Extractive Question Answering",
  "url": "https://openalex.org/W3204607391",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4311725538",
      "name": "Pearce, Kate",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Zhan, Tiffany",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202135737",
      "name": "Komanduri, Aneesh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746136306",
      "name": "Zhan Justin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2888302696",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3024622987",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3027440908",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3047171714",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2516930406",
    "https://openalex.org/W2798836595"
  ],
  "abstract": "Question Answering (QA) is a task in natural language processing that has seen considerable growth after the advent of transformers. There has been a surge in QA datasets that have been proposed to challenge natural language processing models to improve human and existing model performance. Many pre-trained language models have proven to be incredibly effective at the task of extractive question answering. However, generalizability remains as a challenge for the majority of these models. That is, some datasets require models to reason more than others. In this paper, we train various pre-trained language models and fine-tune them on multiple question answering datasets of varying levels of difficulty to determine which of the models are capable of generalizing the most comprehensively across different datasets. Further, we propose a new architecture, BERT-BiLSTM, and compare it with other language models to determine if adding more bidirectionality can improve model performance. Using the F1-score as our metric, we find that the RoBERTa and BART pre-trained models perform the best across all datasets and that our BERT-BiLSTM model outperforms the baseline BERT model.",
  "full_text": "A Comparative Study of Transformer-Based\nLanguage Models on Extractive Question\nAnswering\nKate Pearce∗, Tiffany Zhan∗, Aneesh Komanduri †, Justin Zhan †\n†Department of Computer Science and Computer Engineering, University of Arkansas\n∗Army Educational Outreach Program UNITE\nAbstract—Question Answering (QA) is a task in natural\nlanguage processing that has seen considerable growth after the\nadvent of transformers. There has been a surge in QA datasets\nthat have been proposed to challenge natural language processing\nmodels to improve human and existing model performance.\nMany pre-trained language models have proven to be incredibly\neffective at the task of extractive question answering. However,\ngeneralizability remains as a challenge for the majority of these\nmodels. That is, some datasets require models to reason more\nthan others. In this paper, we train various pre-trained language\nmodels and ﬁne-tune them on multiple question answering\ndatasets of varying levels of difﬁculty to determine which of\nthe models are capable of generalizing the most comprehensively\nacross different datasets. Further, we propose a new architecture,\nBERT-BiLSTM, and compare it with other language models to\ndetermine if adding more bidirectionality can improve model\nperformance. Using the F1-score as our metric, we ﬁnd that the\nRoBERTa and BART pre-trained models perform the best across\nall datasets and that our BERT-BiLSTM model outperforms the\nbaseline BERT model.\nIndex Terms—natural language processing, question answer-\ning, deep learning, transformers\nI. I NTRODUCTION\nE\nXtractive Question Answering is the task of extracting\na span of text from a given context paragraph as the\nanswer to a speciﬁed question. Question Answering is a task in\nnatural language processing (NLP) that has seen considerable\nprogress in recent years with applications in search engines,\nsuch as Google Search and chatbots, such as IBM Watson.\nThis is due to the proposal of large pre-trained language\nmodels, such as BERT [1], which utilize the Transformer [2]\narchitecture to develop robust language models for a variety\nof NLP tasks speciﬁed by benchmarks, such as GLUE [3] or\ndecaNLP [4]. Additionally, new datasets, including SQuAD\n[5] have introduced more complex questions with inference\nbased context to the question answering task. Recent work\nhas shown to be productive in tackling the task of question\nanswering, but the task is nowhere near solved. With the\nintroduction of datasets, such as QuAC [6] and NewsQA [7]\nthat rely much more on reasoning, it becomes challenging to\ngeneralize previous well performing QA models to different\ndatasets.\nWith more accessibility to computational power, several\nvariants of BERT have been proposed for different domains.\nFor example, BERTweet [8] is a bidirectional transformer\nmodel trained on twitter data and can be particularly useful for\nanalyzing social media data. CT-BERT [9] is another domain-\nspeciﬁc variant that was speciﬁcally pre-trained on Covid-19\ntweets. Before transformers changed the landscape of NLP,\nsequence-to-sequence models such as the Recurrent Neural\nNetwork and Long-Short Term Memory (LSTM) [10] were the\nstate-of-the-art, beating out nearly all other trivial approaches.\nFor the task of question answering, one of the ﬁrst sequence\nmodels proposed was MatchLSTM [11].\nIn this paper, we study the difference between various pre-\ntrained transformer-based language models to analyze how\nwell they are able to generalize to datasets of varying levels\nof complexity when ﬁne-tuned on the question answering\ntask. Furthermore, we propose an ensemble model architecture\nusing BERT and BiLSTM and evaluate its performance against\nstandard pre-trained models on extractive question answering.\nThe paper is organized as follows. Section II discusses\nbackground pertaining to question answering and pre-trained\nlanguage models. We discuss our approach and propose our\nmodel architecture in Section III. Section IV contains our ex-\nperimental procedure including datasets and choice of hyper-\nparameters. Section V will be an analysis of our experimental\nresults. Finally, we conclude the paper and discuss future work\nin section VI.\nII. B ACKGROUND\nA. Transformers\nRecurrent neural networks and gated recurrent neural net-\nworks have been established as effective approaches to natural\nlanguage processing tasks like machine translation. Recurrent\nmodels generally factor computation along symbol positions of\ninput and output sequences; however, this sequential process\nprecludes parallelization within training samples. Attention\nmechanisms have been used to combat this problem, but they\nare usually used alongside a recurrent model. The Transformer\nis the ﬁrst model to rely entirely on an attention mechanism.\nThe systems consists of encoder and decoder stacks, each\ncomposed of a stack of N = 6identical layers. The attention\nfunction maps vector queries, keys, and values, with the output\ncomputed as the weighted sum, with weight computed as a\nfunction that determines the compatibility of the query with\nthe corresponding key. Learning rate and batch size are used\nas hyperparameters.\narXiv:2110.03142v1  [cs.CL]  7 Oct 2021\nThe model has achieved impressive results in machine\ntranslation; for example, one model achieved a BLEU score\nof 28.4 on an English-to-German translation tasks, beating the\nprevious record by 2.0 BLEU points. [2]\nFigure 1. Transformer Model Architecture [2]\nB. BERT\nThe language representation model Bidirectional Encoder\nRepresentations from Transformers (BERT) relies on the con-\ncept of transfer learning to learn unsupervised from a corpus of\nunlabeled data. Using a large amount of data allows for BERT\nto bypass weaknesses of other models, including overﬁtting\nand underﬁtting . Through ﬁne-tuning with a much smaller\nsample of labeled data, the resultant model can be employed\nfor downstream tasks, such as, question answering, machine\nreading comprehension, and sentiment analysis. To pre-train,\nthe BERT model masks certain phrases or words from the\noriginal input and trains on two prediction tasks: prediction\nof the masked token words and binary prediction whether the\nsecond sentence input belongs after the ﬁrst in the original\ntext.\nC. ALBERT\nAlbert is a more condensed form of BERT, intended to\nhave comparable, or even, superior capabilities as BERT\nwhile expending less computational power, and signiﬁcantly\nless input time, which makes it a generally more accessible\nprogram for users with less time or fancy tech at their disposal.\nIt also allows for a larger model size as it requires fewer\nresources and can therefore, achieve better performance and\naccuracy. This has made it popular among corporations, due\nto the lower cost and training time requirements. The way\nthat Albert manages to work as well as BERT while being\nsigniﬁcantly faster and less demanding is by having far less\nparameters, which is reached through cross-layer parameter\nsharing, reducing redundancy [12].\nD. XLNet\nOne problem with the BERT model is that it neglects\ndependency between masked positions and also suffers from\na pretrain-ﬁnetune discrepancy. In order to combat these\nissues, XLNet was proposed as a generalized autoregressive\npretraining method. XLNet enables the learning of bidi-\nrectional contexts and overcomes BERT’s limitations due\nto autoregressive formulation, and it outperforms BERT on\nmore than twenty tasks, including tasks in question-answering\nand sentiment analysis. Autoregressive language modelling\nestimates the probability distribution of a sequence of text\nwith an autoregressive model. Since many natural language\nprocessing tasks require bidirectional processing, there is a\ngap in performance between AR pretraining and effective\npretraining. XLNet architecture consists of content-stream\nattention, which is similar to previously-used self-attention\nmechanisms, query information, which does not have access\nto content information, and a two-stream attention model. The\nlargest XLNet model consists of the same hyperparameters as\nBERT and a similar model size; an input sequence length of\n512 was always used. [13]\nE. RoBERTa\nAlthough self-training models such as XLNet and BERT\nbrought signiﬁcant performance gains, it is difﬁcult to de-\ntermine which aspects of the training contributed the most,\nespecially as training is computationally expensive and only\nlimited tuning can be done. A replication of the BERT\nstudy that emphasized the effects of training set size and\nhyperparameter tuning found BERT to be undertrained, and\nthus RoBERTa is an improved method for training BERT in\norder to increase performance. Modiﬁcations made include\ntraining the model longer on larger data sets, changing the\nmasking pattern applied to training data, training on longer\nsequences, and removing next-sentence prediction. RoBERTa\nuses the same BERT optimization hyperparameters, with the\nexception of the peak learning rate and number of warmup\nsteps, which are tuned independently for each setting. The\nmodel has proven highly effective, establishing a new state-\nof-the-art on 4/9 GLUE tasks (MNLI, QNLI, RTE, and STS-B)\nand matching state-of-the-art results on the RACE and SQuAD\ndatasets. [14]\nF . ConvBERT\nWhile BERT does achieve impressive performance com-\npared to previous models, it suffers large computation cost and\nmemory footprint due to reliance on the global self-attention\nblock. BERT was found to be computationally redundant,\nsince some heads only need to learn local dependencies.\n2\nThe ConvBERT model integrates BERT with a novel mixed-\nattention design, and experiments demonstrate that ConvBERT\nsigniﬁcantly outperforms BERT on a variety of tasks; for\nexample, ConvBERT achieved an 86.4 GLUE score, with 1/4\nof the training cost of ELECTRAbase. ConvBERT uses batch\nsizes of 128 and 256 respectively for the small-sized and base-\nsized model during pre-training, and an input sequence of 128\nis used to update the model. [15]\nG. BART\nBART is a denoising autoencoder used for pretraining\nsequence-to-sequence models. BART is particularly effective\nwhen ﬁne tuned for text generation but also works well for\ncomprehension tasks. BART uses a standard Transformer-\nbased neural machine translation architecture, which can be\nseen as generalizing BERT (due to the bidirectional encoder),\nGPT (with the left-to-right decoder), and many other pre-\ntraining plans.\nBART was pretrained by using corrupted documents then\noptimizing a reconstruction loss. If all information was lost\nfor source, BART would be equivalent to a language model.\nAlthough BART does perform similar to the RoBERTa model,\nit brings new state-of-the-art results on number of text gener-\nation [16].\nIII. M ETHODOLOGY\nA. BERT Encoding Layer\nBERT is used as an encoder layer for our ensemble model\nto achieve tokenized input and utilize the transformer self-\nattention for a richer encoding than previous embedding\nmodels such as Word2Vec [17] or Glove [18]. Let W =\n(w1, . . . , wm) be an m word sequence, where m is the max\nsequence length set as a hyperparameter. We tokenize W\nusing the BERT positional, segment, and sentence embeddings\nto obtain the tokenized form, T = (t1, . . . , tn), of the text\nsequence. The self-attention blocks in BERT provide for an\nencoding that accounts for context within the input tokens.\nUnlike Wang et al [11], we do not use an LSTM preprocessing\nlayer for both the context and the question independently. We\nﬁnd that the bidirectionality of BERT makes for a much more\nuseful encoder than just an LSTM.\nLet (Q, C) denote the question-context pair. The concate-\nnated representation of the question and context will be the\ninput into the BERT-base model. Let ◦ represent the con-\ncatenation operation. Now, we deﬁne the concatenated input\nsequence as follows:\nE = Q ◦C\nThe output encoding from BERT can be represented as fol-\nlows:\nH = BERT(E)\nwhere H = (h1, . . . , hk), with k denoting the output dimen-\nsion, is the output hidden representation that is computed by\nencoding the input question-context sequence.\nFigure 2. BERT-BiLSTM Model Architecture\nB. BiLSTM Layer\nWe decided to use a bidirectional LSTM after retrieving the\nBERT encodings to get more contextual representation from\nthe inputs. Once the BERT encoding of the input sequence is\nfed into the BiLSTM, the outputs are computed as follows:\nˆyt = g(Wy[− →at, ← −at] +by)\nThe LSTM layer efﬁciently uses past features and future\nfeatures once inputs have been encoded. While BERT gives\ncontextual encodings with attention among the input sequence,\nthe BiLSTM further brings context without attention. We\nchoose to use the BiLSTM to retain the consistency of\nbidirectional models. The layer computes two different hidden\nrepresentations for the input sequence: one for the context\nfrom the left and one for context from the right. This mech-\nanism is similar to self-attention in BERT, but doesn’t use\nattention in itself.\n3\nC. Linear Layer\nTo augment the question and context together, we represent\nthe input sequence into the language models as a question-\ncontext single packed sequence. The prediction is a start token\nvector S ∈RH and an end token vector E ∈RH that specify\nthe beginning and end of the answer, respectively. Now, the\nprobability of word wi being the start of the answer span is\ncomputed as the dot product between the token representation\nTwi (after BiLSTM) and S followed by a softmax over all the\nwords in the sequence:\nPwi = eS·Twi\n∑\nj eS·Twj\nThe end of the answer span is computed analogously. The\nscore of a candidate span from position i to position j is\ndeﬁned as S ·Twi + E ·Twj and the span with the highest\nprobability where j ≥i is used as a prediction.\nIV. E XPERIMENTS\nIn this section, we train the various pre-trained language\nmodels discussed previously and our proposed ensemble\nmodel. We ﬁne-tune our models on the following datasets.\nA. Datasets\n1) NewsQA: NewsQA is a dataset consisting of 119,633\nquestions posed by crowdworkers on 12,744 CNN articles.\nAnswers to the questions are contained within spans of text\nin the articles.\nNewsQA differentiates itself from previous datasets with a\nfew characteristics: questions may be unanswerable (having\nno answer in the corresponding article), and questions re-\nquire reasoning beyond simple word and context matching.\nAdditionally, there are no candidate answers to choose from,\nand answers are spans of text of arbitrary length, rather than\nsingle words or entities. There are several types of answers in\nthe NewsQA dataset, including people, dates, numeric ﬁgures,\nverb phrases, and more.\nFigure 3. Example of NewsQA question types [7]\nThe gap between human performance and model perfor-\nmance on the NewsQA dataset is an astounding 0.198 F1\npoints, demonstrating a large margin for improvement for\nfuture models. [7]\n2) SQuAD 2.0: One major ﬂaw in the SQuAD dataset\nis the focus on questions that are guaranteed to have an\nanswer within the context, making models use context and\ntype-matching heuristics. In order to combat this, the SQuAD\n2.0 dataset combines answerable questions from SQuAD with\n53,755 unanswerable questions about the same paragraphs.\nModels that achieve an 0.858 F1 score on SQuAD achieve\nonly a 0.663 F1 score on SQuAD 2.0, creating room for\nimprovement for models.\nUnanswerable SQuAD 2.0 questions aim to achieve rele-\nvance, in order to prevent simple heuristics like word overlap\nfrom being used to determine if a question is answerable,\nand also the existence of plausible answers in the context,\nin order to prevent type-matching heuristics from being used\nto determine if a question is answerable.\nSQuAD 2.0 forces models to understand whether an answer\nto a question exists in a given context in order to encourage\nthe development of reading comprehension models that know\nwhat they don’t know and have a deeper understanding of\nlanguage [5].\n3)\n QuAC: QuAC is a Question Answering in Context\ndataset. The dataset consists of two dialogues: one of a student\nwho asks open-ended questions in order to learn as much\nabout a Wikipedia article as possible, and another of a teacher\nwho answers the student’s questions using excerpts from the\ntext. QuAC contains about fourteen thousand dialogues, with\nabout a hundred thousand questions in total. The QuAC dataset\nimproves on many previous question-answering datasets; for\nexample, its questions are often unanswerable, open-ended, or\nonly make sense in context.\nUnlike SQuAD, students do not know the answers to their\nown questions before posing them, making simple paraphras-\ning and string-matching less likely. It also contains coreference\nto previous answers and questions.\nThe task begins with a student formulating an open-ended\nquestion (q) from the information that they have been given.\nThe teacher must answer with a continous span of text given\nby indices (i,j). In order to create more natural interactions, the\nteacher must also provide the student with a list of dialogue\nactions, such as continuation (e.g. follow up), afﬁrmation (e.g.\nyes or no), and answerability (answerable or no answer)\nThe QuAC dataset has long answers that average ﬁfteen\ntokens, compared to three on average for the SQuAD dataset\n- this is unsurprising, considering that most SQuAD answers\nare numerical ﬁgures or entities, while QuAC questions are\ntypically more open-ended. Although QuAC questions are\ntypically shorter than SQuAD questions, this does not mean\nthe questions are less complex; in the QuAC datasets, students\ncannot access the context section to paraphrase. [6]\n4) CovidQA: CovidQA is a Question Answering dataset\nconsisting of 2,019 different question/answer pairs. There were\n147 articles most related to COVID-19, pulled from CORD-\n19, that were annotated by 15 volunteer biomedical experts.\nEven though the annotators were volunteers it was required for\n4\nNewsQA SQuAD QuAC CovidQA\nXLNetBASE 53.2 64.9 30.1 44.9\nBERTBASE 52.1 64.7 28.6 44.8\nRoBERTaBASE 57.0 68.2 31.3 44.5\nALBERTBASE 51.8 64.8 19.5 42.4\nConvBertBASE 55.7 67.4 31.5 44.9\nBARTBASE 56.2 67.6 29.1 45.3\nBERTBASE -BiLSTM 52.6 65.0 28.9 45.6\nTABLE I: F1-scores for various pre-trained models on NewsQA, SQuAD, QuAC, and CovidQA datasets.\nFigure 4. Example of a QuAC dialogue. The context is taken from the section\n”Origin and History” from the Daffy Duck Wikipedia article. [2]\nthem to have a Master’s degree in biomedical sciences. The\nteam of annotator were led by a medical doctor (G.A.R), who\nassessed the volunteer’s credentials and manually veriﬁed each\nof the question /answer pairs produced. The annotations were\ncreated in SQuAD style fashion which is where the annotators\nmark text as answers and formulate corresponding questions.\nCovidQA differs from SQuAD being that the answers come\nfrom longer texts (6118.5 vs. 153.2), answers are generally\nlonger (13.9 vs. 3.2), and it doesn’t contain n-way annotated\ndevelopment nor test sets. RoBERTa-base-architecture and a\nﬁne-tuned version of the SQuAD dataset was chosen and\nused as the baseline model of CovidQA. To train the baseline\nmodel the CovidQA annotations were used in a 5-fold cross\nvalidation manner.\nB. Preprocessing\nBefore implementing our models, we preprocessed the\ndataset to ﬁt the format required as input to the pre-trained\ntransformers. The datasets were scraped to create (context,\nquestion, answer) triplets for each context paragraph and its\ncorresponding questions and answers. The context and ques-\ntion are concatenated for input into the BERT tokenizer, which\ntokenized the sequence and added the [CLS] token at the\nbeginning of the input sequence and the [SEP] token between\nthe context and the question. For the sake of computational\nability, we set the maximum sequence length for all tokenizers\nto 512. All datasets are tokenized with WordPiece [19] and\nSentencePiece [20] and uniformed in lower cases.\nC. Hyperparameters\nAll the models are implemented using PyTorch and the\nTransformers library. We utilize the base uncased version\nof each model with maximum sequence length 512. The\ndocument stride is set to 128. To optimize the models, we\nuse the Adam optimizer with learning rate 5e-5 and train the\nmodels for a total of 3 epochs. We set the batch size for our\ntraining and validation sets to 8. We utilize the base version\nof each of the pre-trained language models.\nThe code, model, and datasets are publicly available at\nfor reproducibility. 1 We performed our experiments on an\nNVIDIA 2x Quadro RTX 8000 (48G) GPU RAM.\nD. Evaluation Metrics\nTo evaluate the performance of our ﬁne-tuned models on\neach of the datasets, we use the F1-score. The F1-score takes\ninto account both the precision and recall of the model. The\nF1-score is computed for both the classiﬁed start token S and\nthe end token E and averaged to get a single F1-score deﬁned\nin terms of the precision and recall as follows:\nprecision = TP\nTP + FP\nrecall = TP\nTP + FN\nF1-score = 2∗ precision ·recall\nprecision + recall\n1https://github.com/Akomand/AEOP Research 2021\n5\nV. R ESULTS\nWe evaluate the performance of the base models using\nthe F1-score. We ﬁne-tuned XLNet BASE , BERT BASE ,\nRoBERTaBASE , ALBERT BASE , ConvBERT BASE , and\nBARTBASE on the NewsQA, SQuAD, QuAC, and CovidQA\ndatasets. The models performed the best on SQuAD 2.0 since\nthe contexts, questions, and answers were straightforward.\nFurther, SQuAD consists of relatively short sequence answers\nto all questions and the answers are purely extractive.\nHowever, each of the ﬁne-tuned models performed quite\npoorly on the QuAC dataset due to the fact that QuAC\nconsists of quite open-ended questions that require more\ninference capabilities from the model. The models performed\nalmost as well on the NewsQA dataset as the SQuAD\ndataset, which demonstrates impressive capabilities of the\nmodels since the NewsQA dataset is a challenging machine\ncomprehension dataset that requires reasoning. CovidQA is a\nsmall dataset that consists of contexts, questions, and answers\nof much longer lengths than other datasets. As such, there\nwas not enough training data to help the models generalize\nbetter. Since most of the pre-trained language models can\nonly handle a relatively short max sequence length, the\nmodel lost a large amount of information when processing\ncontexts, thereby decreasing model performance. We notice\nthat RoBERTa and BART are among the highest perfoming\nmodels. RoBERTa is a highly optimized version of BERT\nthat focuses on the masked language modeling pre-training\ntask without the next sentence prediction task. The masked\nlanguage modelling speciﬁcally helps the model perform\nbetter than others for the question answering task since the\ngoal is to classify the start and end tokens. Since BART\nuses a neural machine translation type architecture, BART\nis able to utilize both the encoder and decoder blocks from\ntransformers for the token classiﬁcation task. Our model\n(BERT + BiLSTM) outperforms BERT BASE by at least 1%,\nso an additional layer of bidirectionality helps contextual\nrepresentations for better performance in question answering.\nVI. C ONCLUSION\nIn this paper, we analyze the performance differences\nbetween various pre-trained language models ﬁne-tuned on\nquestion answering datasets of varying levels of difﬁculty.\nFurther, we propose an ensemble model and compare its\nperformance to other models. Experimental results show the\neffectiveness of our methods and shows that RoBERTa and an\nauxiliary BiLSTM layer both improve model performance in\nquestion answering. We see the highest F1-score on RoBERTa\nand BART model across all datasets. We also observe at least\na 1% increase in F1-score over the BERT base model when\na BiLSTM layer is added on. Future work includes extending\nour model to incorporating additional attention mechanisms\nand potentially utilizing the MatchLSTM architecture to create\na better performing ensemble model.\nVII. A CKNOWLEDGEMENTS\nThis work was supported in part by the Department of\nDefense and the Army Educational Outreach Program.\nREFERENCES\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” 2019.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017.\n[3] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural lan-\nguage understanding,” 2019.\n[4] B. McCann, N. S. Keskar, C. Xiong, and R. Socher, “The natural\nlanguage decathlon: Multitask learning as question answering,” 2018.\n[5] P. Rajpurkar, R. Jia, and P. Liang, “Know what you don’t know:\nUnanswerable questions for squad,” 2018.\n[6] E. Choi, H. He, M. Iyyer, M. Yatskar, W. tau Yih, Y . Choi, P. Liang,\nand L. Zettlemoyer, “Quac : Question answering in context,” 2018.\n[7] A. Trischler, T. Wang, X. Yuan, J. Harris, A. Sordoni, P. Bachman, and\nK. Suleman, “Newsqa: A machine comprehension dataset,” 2017.\n[8] D. Q. Nguyen, T. Vu, and A. T. Nguyen, “Bertweet: A pre-trained\nlanguage model for english tweets,” 2020.\n[9] M. M ¨uller, M. Salath ´e, and P. E. Kummervold, “Covid-twitter-bert:\nA natural language processing model to analyse covid-19 content on\ntwitter,” 2020.\n[10] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\nComputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[11] S. Wang and J. Jiang, “Machine comprehension using match-lstm and\nanswer pointer,” 2016.\n[12] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“Albert: A lite bert for self-supervised learning of language representa-\ntions,” 2020.\n[13] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n“Xlnet: Generalized autoregressive pretraining for language understand-\ning,” 2020.\n[14] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” 2019.\n[15] Z. Jiang, W. Yu, D. Zhou, Y . Chen, J. Feng, and S. Yan, “Convbert:\nImproving bert with span-based dynamic convolution,” 2021.\n[16] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehen-\nsion,” 2019.\n[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” 2013.\n[18] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in Empirical Methods in Natural Language\nProcessing (EMNLP) , 2014, pp. 1532–1543. [Online]. Available:\nhttp://www.aclweb.org/anthology/D14-1162\n[19] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,\nM. Johnson, X. Liu, Łukasz Kaiser, S. Gouws, Y . Kato, T. Kudo,\nH. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young,\nJ. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes,\nand J. Dean, “Google’s neural machine translation system: Bridging the\ngap between human and machine translation,” 2016.\n[20] T. Kudo and J. Richardson, “SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for neural text\nprocessing,” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations .\nBrussels, Belgium: Association for Computational Linguistics, Nov.\n2018, pp. 66–71. [Online]. Available: https://aclanthology.org/D18-2012\n6",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.6542881727218628
    },
    {
      "name": "Transformer",
      "score": 0.5279903411865234
    },
    {
      "name": "Computer science",
      "score": 0.40102607011795044
    },
    {
      "name": "Natural language processing",
      "score": 0.3612421751022339
    },
    {
      "name": "Engineering",
      "score": 0.13452979922294617
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 22
}