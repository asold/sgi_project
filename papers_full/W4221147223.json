{
  "title": "E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models",
  "url": "https://openalex.org/W4221147223",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5011818119",
      "name": "Mohammad Esmaeil Akbari",
      "affiliations": [
        "Huawei Technologies (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A5073741072",
      "name": "Amin Banitalebi-Dehkordi",
      "affiliations": [
        "Huawei Technologies (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A5100419760",
      "name": "Yong Zhang",
      "affiliations": [
        "Huawei Technologies (Canada)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2787146684",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2766839578",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W1494910745",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W3105645800",
    "https://openalex.org/W3122752795",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2970987838",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2964203871",
    "https://openalex.org/W4205952419",
    "https://openalex.org/W2750557179",
    "https://openalex.org/W3210226168",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W3173195958",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2161914416",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W3205009459",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3172857592",
    "https://openalex.org/W3184738308",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3099999325",
    "https://openalex.org/W3174708387",
    "https://openalex.org/W3114304470",
    "https://openalex.org/W3034457371"
  ],
  "abstract": "Building huge and highly capable language models has been a trend in the past years. Despite their great performance, they incur high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, called E-LANG, which distributes the inference between large accurate Super-models and light-weight Swift models. To this end, a decision making module routes the inputs to Super or Swift models based on the energy characteristics of the representations in the latent space. This method is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, reassembling of modules, or re-training. Unlike existing methods that are only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation. The E-LANG performance is verified through a set of experiments with T5 and BERT backbones on GLUE, SuperGLUE, and WMT. In particular, we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2X less computations. Code and demo are available in supplementary materials.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5229 - 5244\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nE-LANG: Energy-Based Joint Inferencing\nof Super and Swift Language Models\nMohammad Akbari, Amin Banitalebi-Dehkordi, Yong Zhang\nHuawei Technologies Canada Co., Ltd.\n{mohammad.akbari, amin.banitalebi, yong.zhang3}@huawei.com\nAbstract\nBuilding huge and highly capable language\nmodels has been a trend in the past years.\nDespite their great performance, they incur\nhigh computational cost. A common solu-\ntion is to apply model compression or choose\nlight-weight architectures, which often need a\nseparate ﬁxed-size model for each desirable\ncomputational budget, and may lose perfor-\nmance in case of heavy compression. This\npaper proposes an effective dynamic inference\napproach, called E-LANG, which distributes\nthe inference between large accurate Super-\nmodels and light-weight Swift models. To this\nend, a decision making module routes the in-\nputs to Super or Swift models based on the\nenergy characteristics of the representations in\nthe latent space. This method is easily adopt-\nable and architecture agnostic. As such, it\ncan be applied to black-box pre-trained mod-\nels without a need for architectural manipula-\ntions, reassembling of modules, or re-training.\nUnlike existing methods that are only applica-\nble to encoder-only backbones and classiﬁca-\ntion tasks, our method also works for encoder-\ndecoder structures and sequence-to-sequence\ntasks such as translation. The E-LANG perfor-\nmance is veriﬁed through a set of experiments\nwith T5 and BERT backbones on GLUE, Su-\nperGLUE, and WMT. In particular, we out-\nperform T5-11B with an average computa-\ntions speed-up of 3.3 ×on GLUE and 2.9 ×\non SuperGLUE. We also achieve BERT-based\nSOTA on GLUE with 3.2×less computations.\nCode and demo are available here.\n1 Introduction\nWith the introduction of inﬂuential language mod-\nels such as BERT (Devlin et al., 2019), a trend in\nnatural language processing (NLP) research has\nbeen to develop high capacity models and push\ntheir performance to new levels. Consequently,\nstate-of-the-art (SOTA) results were achieved on\nvarious benchmarks using these models; GPT-3\n(Brown et al., 2020), XLNet (Yang et al., 2019),\nRoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020),\nELECTRA (Clark et al., 2020), and DeBERTa (He\net al., 2021) to name a few. A potential down-side,\nhowever, is that the number of parameters or ﬂoat-\ning point operations (FLOPs) for these models can\nget extremely large. For example, Gshard (Lep-\nikhin et al., 2021) comes with 600B parameters\nwith an enormous amount of computation. This in\nturn results in a higher inference latency, which is\nnot desirable for latency-sensitive applications.\nA common solution to speed-up the large lan-\nguage models is to apply model compression\n(Gupta et al., 2020). Although generally success-\nful, compression does come with a trade-off on\naccuracy, and may lose performance if compres-\nsion is heavy. In addition, these methods usually\ncompress a model to a ﬁxed smaller size, where a\nseparate model is required for each possible compu-\ntational budget. An alternative approach explored\nin the literature is to leverage dynamic inferencing\nin a way that examples may be routed to different\n(potentially lower cost) paths throughout the net-\nwork. For example, a temporal early-exit model\n(Shen et al., 2017; Yu et al., 2018) terminates the\nprocedure of reading the input sequence when suf-\nﬁcient evidence has been found for accurate predic-\ntions. Instance-wise early-exiting (Xin et al., ACL\n2020) is another technique, which allows a sample\nto adaptively choose from multiple available exit\nnodes if some conditions are met. Consequently,\nearlier exists require less computation and lead to\na lower latency. Adjusting the size of the model\nat the inference time by choosing adaptive width\nand depth is also another approach employed for\ndynamic inference (Kim and Cho, 2021; Hou et al.,\n2020). There is a variety of adaptive/dynamic in-\nference approaches proposed, however, a general\ndown-side for many of these methods is that often\ntimes they require a careful architecture design, ma-\nnipulation of network modules, or even re-training.\n5229\nIn this paper, we propose a simple but rather ef-\nfective approach of dynamically distributing the in-\nference between the original large model (called the\nSuper model) and a light-weight (e.g., compressed)\nmodel referred to as the Swift model. To this end,\nwe design an energy-based decision making mod-\nule that routes examples to the appropriate model\nbased on the negative free energy of the latent space\nrepresentations, such that the Swift model attains\na high accuracy on the examples sent to it. The\nremaining samples are then forwarded to the Super\nmodel that is supposed to have a good performance\non all examples. Since the Swift model can make\nhighly accurate predictions over the majority of the\nsamples, E-LANG signiﬁcantly reduces the overall\ncomputational cost, while maintains the high ac-\ncuracy of the Super model. Although simple, this\nstrategy achieves SOTA results on multiple struc-\ntures (e.g., T5 and BERT) and benchmarks (e.g.,\nGLUE and SuperGLUE). Due to its desirable prac-\ntical characteristics, this method is a strong candi-\ndate for the practical application of Super models.\nThe main contributions of the paper are as follows:\n•Combining Super models with high accuracy\nand latency and Swift models with lower accu-\nracy and latency, to achievehigh accuracy and\nlow latency. In other words, by employing our\nmethod, we can achieve the high levels of accu-\nracy provided by Super models, but at a lower\ncomputational cost. Our method is easily adopt-\nable, architecture agnostic, and orthogonal to\nmany other existing methods. It can be applied\nto black-box pre-trained models without a need\nfor architectural manipulations, careful reassem-\nbling of modules, or re-training.\n•An energy-based routing mechanismfor di-\nrecting examples to the Super or Swift. This pro-\nvides a dynamic trade-off between the accuracy\nand computational cost that outperforms the pre-\nvious works in both ﬁxed-size and dynamic in-\nference (with zero overhead for real-time adjust-\nment of speed/accuracy). As such, E-LANG acts\nlike a knob for adjusting the accuracy-latency\ntrade-off in real-time during model serving.\n•To the best of our knowledge, our method is\nthe ﬁrst generic approach to apply dynamic\ninference on both encoder-only and encoder-\ndecoder architectures(e.g., T5) and also can\nextend the usage beyond classiﬁcation tasks, to\nsequence-to-sequence tasks such as translation.\n2 Related Works\nAs mentioned, compression is a widely used strat-\negy to speed-up the large language models (Gupta\net al., 2020; Gupta and Agrawal, 2022). This in-\nvolves incorporating techniques such as quantiza-\ntion of weights and activations (Bai et al., 2021;\nShen et al., 2020; Kim et al., 2021; Zhang et al.,\n2020; Jin et al., 2021), knowledge distillation (KD)\n(Hinton et al., 2015; Jiao et al., 2020; Sanh et al.,\n2019), pruning/sharing (Gordon et al., 2020; Chen\net al., 2020), multi-device distribution (Banitalebi-\nDehkordi et al., 2021), or a combination of these\ntechniques (Cheng et al., 2017; Polino et al., 2018).\nAmong all the compression techniques, creating\na ﬁxed-size small version of large models along\nwith distillation has been popular in the recent\nyears. Sanh et al. (2019) introduced DistillBERT,\nwhich was a smaller version of BERT trained with\ndistillation for general purposes. Another com-\npact variant of BERT was proposed by Mobile-\nBERT (Sun et al., 2020) in which inverted bottle-\nneck structures and progressive knowledge transfer\nwere used. TinyBERT (Jiao et al., 2020) also pre-\nsented a novel two-stage transformer distillation\nfor both pre-training and task-speciﬁc ﬁne-tuning.\nIn (Iandola et al., 2020), the usage of grouped\nconvolutions was studied to design SqueezeBERT.\nELM (Jiao et al., 2021), a layer mapping search\nframework, was also proposed for improving down-\nstream BERT distillation. A recent method, Ghost-\nBERT (Huang et al., 2021), employed softmax-\nnormalized 1D convolutions as ghost modules to\ngenerate more features with cheap operations.\nAlthough compression techniques in general are\neffective, they come with a trade-off on accuracy,\nand may lose performance in case of high ratio\ncompression. In addition, an individual ﬁxed-size\nmodel is required for each possible computational\nbudget. As stated in the introduction, the alter-\nnative solution is dynamic inference, which can\nbe achieved with either early-exit or length/depth-\nadaptive models. One of the ﬁrst temporal early-\nexit strategies was proposed by ReasoNet (Shen\net al., 2017), which stops its reading procedure\nwhen sufﬁcient evidence has been found for answer-\ning a question. Similarly, in (Yu et al., 2018), an\nearly stopping method applicable to classiﬁcation\ntasks was presented. DeeBERT (Xin et al., ACL\n2020) also proposed an instance-wise multi-exit\nmethod via the entropy of the output probability\ndistribution to speed-up BERT inference.\n5230\nFigure 1: Overall framework of the proposed energy-based joint inference strategy (E-LANG).\nAs a length-adaptive method, Kim and Cho\n(2021) introduced a dynamic inference framework\nwith one-shot training of transformers for both\nsequence- and token-level classiﬁcation. Also, in\n(Hou et al., 2020), an architecture named Dyn-\naBERT was proposed for adaptively adjusting the\ncomputations by choosing sub-networks of differ-\nent widths and depths. Both Length-Adaptive and\nDynaBERT utilized knowledge distillation and data\naugmentation to improve their performance.\nAlthough early-exit and adaptive methods have\nmade signiﬁcant progress and work well in practice,\nthey often require architectural manipulation and\nre-training. In addition, they are only applicable to\nencoder-only backbones and classiﬁcation tasks. In\ncontrast, our method can work with out-of-the-box\npre-trained models without a need for re-training\nand are also applicable for encoder-decoder struc-\ntures and sequence-to-sequence tasks.\n3 Proposed Method\nWe propose a new energy-based joint inference\nmethod called E-LANG, where a large/accurate\nlanguage model (Super) is jointly employed with\na small/fast one (Swift) to achieve efﬁcient infer-\nence without sacriﬁcing the accuracy. To this end,\ninspired by the method in (Akbari et al., 2021), a\nrouting mechanism empowered by energy-based\nmodels (EBM) is introduced to dynamically dis-\ntribute the input samples between the Super and\nSwift models. Similar to the out-of-distribution\n(OOD) detection problem, our goal is to identify\nthe OOD samples that are hard to handle for the\nSwift and forward them to the Super model. On\nthe other hand, we have the in-distribution data\nfor which the Swift can make highly reliable and\naccurate predictions. In other words, the routing\nmechanism needs to detect whether or not the input\ndata ﬁts in the Swift’s distribution (i.e., the one\nthe Swift has been trained with). Inspired by the\nsuccess of EBMs in dealing with OOD detection\nproblems (Lee et al., 2019), the energy character-\nistics of data samples for an efﬁcient and effective\nrouting are investigated in our work. The overall\nframework of E-LANG is shown in Figure 1.\n3.1 Energy-Based Models\nThe goal of EBM is to build an energy function\ndenoted by E(x) : RD →R that maps an input\ndata x ∈RD to a non-probabilistic energy value\ny ∈R. To turn a collection of arbitrary energies\nfor all possible outputs (denoted by Y) into a nor-\nmalized probability distribution, Gibbs distribution\ncan be used as follows (LeCun et al., 2006):\np(y|x) = e−E(x,y)\n∫\ny′∈Y e−E(x,y′) , (1)\nwhere the negative log of the denominator ex-\npresses the Helmholtz free energy (LeCun et al.,\n2006) deﬁned as F(x) =−log\n(∫\ny′∈Y e−E(x,y′))\n.\nIn machine learning, there is a deep relation-\nship between the EBMs and discriminative models,\nwhich can be seen by connecting the Gibbs distribu-\ntion in Equation (1) and the categorical distribution\nderived for a discriminative model. A discrimina-\ntive classiﬁer is deﬁned as a function for mapping\nthe input x to Creal-valued logits (i.e., for Cnum-\nber of class labels): f(x) : RD →RC. In order\nto derive a categorical distribution over Cpossible\noutputs, the softmax function is utilized:\np(y|x) = efy(x)\n∑C\ni efi(x) , (2)\nwhere fy(x) denotes the logit (probability) of the\nyth class label. Based on the inherent connection\n5231\nbetween the Gibbs and categorical distributions de-\nﬁned in (1) and (2), the energy function for a given\ninput (x,y) can be deﬁned as E(x,y) = −fy(x).\nThe free energy function F(x; f) can then be ob-\ntained by taking the negative log of the categorical\ndistribution denominator as:\nF(x; f) =−log\nC∑\ni\nefi(x). (3)\n3.2 Energy-Based Joint Inference\nOur goal is to detect the easy samples suitable for\nthe Swift, which are indeed the ones with high\nlikelihood in the density function. The energy-\nbased density function for Swift is then deﬁned as:\np(x) = e−F(x;f)\n∫\nx e−F(x;f) , (4)\nwhere the denominator is the normalized densities,\nwhich can be intractable to compute or estimate.\nBy taking the logarithm of both sides, we obtain:\nlog\n(\np(x)\n)\n= −F(x; f) −log(\n∫\nx\ne−F(x;f)). (5)\nThe log(\n∫\nx e−F(x;f)) term has no effect on the\ndistribution of the overall energy values because\nit is constant for all x. As a result, −F(x; f), i.e.,\nthe negative free energy, has a linear alignment\nwith the log likelihood function, which makes it a\nwell-suited solution to the easy vs. hard detection\nproblem in our framework. To this end, lower en-\nergy values indicate higher likelihood and represent\neasier (more ﬁt) samples for the Swift model.\nMore precisely, for a threshold δon the density\nfunction such that p(x) <δ, then a threshold ton\nthe negative free energy can be calculated accord-\ning to (5) as −F(x; f) < t= log(δ\n∫\nx e−F(x;f)).\nIn practice, for a given input, an energy function\nis applied to the outputs of the Swift model during\ninference time to calculate the energy score. Then,\nif the negative energy value is smaller than a thresh-\nold, the input is identiﬁed as a bad sample for the\nSwift, and is sent to the Super model.\nGiven the energy threshold t, the Swift clas-\nsiﬁer f(x), and the Super classiﬁer deﬁned as\ng(x) : RD → RC, the joint inference function\nJ(x; f,g,t ) ∈[1,C] for a classiﬁcation task with\nCclasses can then be expressed by:\nJ(x; f,g,t ) =\n{\nf(x) if −F(x; f) ≥t\ng(x) otherwise. (6)\n3.2.1 Encoder-Decoder Architectures\nThe proposed energy-based joint inference solu-\ntion can be directly applied to the encoder-only\nmodels such as BERT that are designed for text\nclassiﬁcation tasks. To this end, the energy scores\ncorresponding to the BERT-based Swift model are\nobtained using Equation (3) and the joint inference\nis performed based on Equation 6.\nOn the other hand, for the encoder-decoder (auto-\nencoder) architectures such as T5, which are usu-\nally considered as generative models, some mod-\niﬁcations are required. Encoder-decoder models\nare basically designed for sequence-to-sequence\n(e.g., text-to-text) problems such as translation or\nsummarization. Although such models can also be\nemployed for classiﬁcation tasks, they still consider\nthe task as a text generation (sequence-to-sequence)\nproblem, where the target labels and the output pre-\ndictions are treated as a sequence or a piece of text.\nIn Section 3.1, it was discussed that there is an in-\nherent connection between the discriminative clas-\nsiﬁers and the EBMs. In order to beneﬁt from this\ncharacteristic for encoder-decoder architectures,\nwe consider adding an extra classiﬁcation head (i.e.,\na single linear layer) to the Swift model. As en-\ncoders are commonly considered as better feature\nextractors for training a classiﬁer rather than the\ndecoders, we place the extra head after the Swift\nencoder. While freezing the pre-trained encoder\nmodel (denoted by fE), the extra energy head (de-\nnoted by h) is trained as a regular classiﬁer head\nwith C class labels. Note that the decoder is not\nrequired for training the head. The corresponding\nfree energy function is then deﬁned as follows:\nF(x; fE,h) =−log\nC∑\ni\nehi\n(\nfE(x)\n)\n, (7)\nwhere fE(x) denotes the outputs of the encoder’s\nlast hidden state. These features are then fed to the\nextra head hto obtain the logits for the ith class\nrequired for computing the energy scores.\nIn this approach, as the decoder part of the Swift\nmodel is not required for calculating the energy\nscores, less computations are involved and the joint\ninference is performed more efﬁciently.\nFor text-to-text (or sequence-to-sequence) prob-\nlems such as translation, the output is a sequence\nof M word-pieces from a vocabulary/dictionary of\nsize N. To still utilize the relationship of discrimi-\nnative models and EBMs in designing and training\nthe extra energy head, we can treat the text-to-text\n5232\nmodels as M multi-class classiﬁers. In this case,\nthe number of class labels, i.e., Cin (7), is equal to\nN. The ﬁnal energy score is then calculated as the\naverage of M energy values as follows:\nF(x; fE,h) =−1\nM\n∑M\nm\n(log∑C\ni ehm,i\n(\nfE(x)\n)),(8)\nwhere hm,i(.) denotes the logits corresponding to\nthe mth word in the sequence and ith class label.\nDenote the Swift’s decoder byfD, the joint in-\nference function, J(x; f,g,h,t ), based on energy\nscores in either Equation (7) or (8) is expressed as:\nJ =\n{\nfD(\nfE(x)\n)\nif −F(x; fE,h) ≥t\ng(x) otherwise. (9)\n3.3 Softmax and Entropy Mechanisms\nIn addition to energy, softmax and entropy (Xin\net al., ACL 2020) scores can also be used for ana-\nlyzing the Swift model’s performance in the routing\nmechanism. In this sub-section, we study the math-\nematical connection of them with the energy score\nand their potential to solve our problem.\n3.3.1 Softmax-Based Mechanism\nThe softmax score for a classiﬁer is expressed by:\nmax\ny\np(y|x) = max\ny\nefy(x)\n∑C\ni efi(x) = efmax(x)\n∑C\ni efi(x) . (10)\nBy taking the logarithm of both sides, we see the\nconnection between the log of the softmax and the\nfree energy score formulated in Equation (3):\nlogmax\ny\np(y|x) =log(efmax(x)) −log\nC∑\ni\nefi(x)\n= fmax(x) +F(x; f), (11)\nwhere all logits are shifted by their maximum\nfmax(x). Plugging in the energy term to (5) yields:\nlogmax\ny\np(y|x) =−log(p(x)) +fmax(x)\n−log\n(∫\nx\ne−F(x;f))\n.\n(12)\nIt is observed that for the samples with high like-\nlihood of being in the Swift’s distribution, the free\nenergy goes lower, but the max logit tends to go\nhigher. Due to this shifting, unlike the energy score,\nthe softmax score is not well-aligned with the prob-\nability density p(x). As a result, the softmax score\nis less reliable for our routing module to analyze\nthe performance of the Swift.\n3.3.2 Entropy-Based Mechanism\nThe entropy score is a measure of randomness in\nthe processed information, and is calculated as:\nH(x; f) =−\nC∑\ni\nfi.log(fi), (13)\nwhere fi(x) is the probability (logit) corresponding\nto the ith class label. Let U be the internal energy,\ni.e., the expectation value of the energy function\n(Oh et al., 2020), deﬁned by:\nU(x; f) =\nC∑\ni\nE(x,i)fi. (14)\nAccording to Oh et al. (2020), the entropy can\nbe deﬁned in terms of the internal and free energy\nfunctions as: H(x; f) =U(x; f) −F(x; f),where\nall logits are shifted by the internal energy U. Sub-\nstituting the free energy from (5) yields:\nH(x;f) =log(p(x)) +U(x;f) +log(∫\nx e−F(x;f)),(15)\nwhich shows, due to the shifting caused by internal\nenergy, the entropy is not reliably aligned with the\nprobability density p(x). Thus, it is a less suitable\nrouting mechanism unlike the energy score.\n4 Experimental Results\nIn this section, the performance of E-LANG on\ndifferent architectures such as T5 and BERT; and\nbenchmarks such as GLUE (Wang et al., 2019b),\nSuperGLUE (Wang et al., 2019a), and WMT (Bojar\net al., 2016) is evaluated and compared with the\nSuper models and previous works.\n4.1 T5-Based Joint Inference\nIn Table 1, the T5-based results on GLUE, Super-\nGLUE, and WMT benchmarks are reported. For all\nthe tasks, we use T5-11B (with 87×1011 FLOPs)\nand T5-large (with 4.25×1011 FLOPs) as our Super\nand Swift models, respectively. The average GPU-\nbased running time and accuracy of both models\ncompared with E-LANG are also summarized in\nthe table. Note that the T5 models used in this ex-\nperiment have been separately ﬁne-tuned on each\nof the downstream tasks given in Table 1. The extra\nenergy head for each of these tasks was also sepa-\nrately trained and used based on the task-speciﬁc\nnumber of classes, i.e., Cin Equation (7).\n5233\nGLUE SuperGLUE WMT\nMNLI QNLI SST2 RTE MRPC COLA RTE BoolQ MRC COPA CB WIC WSC EnRo\nSwift\n(Large)\nTime (ms) 216 283 57 263 160 56 287 303 201 96 223 185 133 1609\nAccuracy (%) 89.7 93.9 95.5 90.3 90.9 62.7 88.5 84.3 80.7 81.0 92.0 72.7 86.5 28.6\nSuper\n(11B)\nTime (ms) 821 980 281 964 433 213 818 3205 1731 268 844 671 2211 3041\nAccuracy (%) 91.7 95.9 96.6 92.4 91.7 69.1 93.1 89.4 84.9 93.0 93.1 77.4 89.4 28.9\nE-LANG\nAccuracy (%) 91.7 96.0 96.6 92.4 92.2 69.5 93.2 88.7 84.9 90.0 93.1 78.1 89.4 28.9\nFLOPs (×1011) 47.8 25.7 29.5 50.4 11.5 39.9 42.0 50.8 46.9 52.6 13.4 40.3 20.6 63.4\nTime (ms) 582 495 132 716 190 147 671 1978 1022 222 302 447 545 2800\nSwift Ratio (%) 49 75 70 46 91 58 56 45 50 43 89 57 81 30\nSpeed-up (FLOPs) 1.8X 3.4X 2.9X 1.7X 7.6X 2.2X 2.1X 1.7X 1.9X 1.7X 6.5X 2.2X 4.2X 1.4X\nSpeed-up (time) 1.4X 2.0X 2.1X 1.4X 2.3X 1.5X 1.2X 1.6X 1.7X 1.2X 2.8X 1.5X 4.1X 1.1X\nTable 1: Joint inference results with T5 architecture on GLUE and SuperGLUE development sets, and WMT’s English-to-\nRomanian translation. The FLOPs for Super and Swift are respectively 87×1011 and 4.25×1011.\nFigure 2: Joint inference trade-off curves with T5 architecture on GLUE and SuperGLUE development sets. Each point is\nobtained with a different energy threshold.\nThe total FLOPs for our method is measured as\na weighted average of the Super and Swift FLOPs\nbased on their usage frequency as:\nFLOPs = 1\nNsw + Nsu\n(\nNsw.(FE\nsw+Fh+FD\nsw)\n+ Nsu.(FE\nsw + Fh + Fsu)\n)\n, (16)\nwhere Nsu and Nsw are respectively the number of\nsamples processed by the Super (with Fsu FLOPs)\nand Swift (with FE\nsw, FD\nsw, and Fh FLOPs for the\nencoder, decoder, and energy head). Note that Fh\nis equal to ≈0.00001×1011, which has a very in-\nsigniﬁcant overhead in our framework.\nAs presented in Table 1, E-LANG can reach the\nSuper model’s accuracy on all GLUE tasks with\nan average 3.3X in FLOPs and 1.8X in running\ntime speed-ups. For some tasks such as QNLI,\nMRPC, and COLA, we even outperform the Super\nmodel, which leads to a higher average accuracy\nof 89.7% than the Super model with 89.5% on\nGLUE. For the SuperGLUE benchmark, with an\naverage FLOPs and running time speed-up of 2.9X\nand 2.0X, our method achieves the same accuracy\nas the Super model on MRC and CB; and better\naccuracy on RTE and WIC. On BoolQ and COPA,\nalthough 99% and 97% of the Super’s accuracy are\nrespectively obtained, it is with 1.7X and 1.4X less\nFLOPs and latency, on average.\nIn order to analyze the generality of E-LANG to\nother NLP problems rather than text classiﬁcation\n(Section 3.2.1), we also apply our method to two\ntext-to-text tasks including SuperGLUE’s WSC\nand WMT’s English-to-Romanian (EnRo) transla-\ntion. As given in the table, our method achieves the\nSuper model’s accuracy on both WSC and EnRo\nwith 4.2X and 1.4X less FLOPs, respectively.\nFigure 2 illustrates the accuracy vs. FLOPs\ntrade-off curves for some tasks in GLUE and Super-\n5234\nGLUE benchmarks. The curves related to all tasks\nare given in the supplementary materials. The trade-\noff points on the curves are dynamically achieved at\nthe inference time by selecting different thresholds,\ni.e., tin Equations (6) and (9). Larger values for t\nwill result in routing more input data to the Super\nmodel, which consequently provides more accu-\nrate, but slower inference. As the Swift is able to\nmake accurate predictions for the majority of input\ndata, the dynamic inference with a small enough\ntcan reach the Super model’s accuracy but with a\nmuch lower computational cost and latency.\nFigure 3 illustrates the distribution of the energy\nscores across the input samples in GLUE tasks.\nFor each task, the distributions of the samples pro-\ncessed by the Super and the Swift models are plot-\nted. As shown, the samples routed to the Super\nmodel tend to have lower energy scores that are\nindeed considered as out-of-distribution samples\nfor the Swift. On the other hand, in overall, higher\nscores are observed for the Swift distribution, that\nis for the samples handled by the Swift only. For\nsome tasks such as MRPC and QNLI, the Swift\nis shown to be highly capable of handling the ma-\njority of the input samples. This is also supported\nby the results in Table 1 and Figure 2, where 91%\n(for MRPC) and 75% (for QNLI) of the samples\nare accurately processed by the Swift. In contrast,\nfor other datasets including RTE and MNLI with\nSwift ratio of less than 50%, most of the samples\nare hard for the Swift, which are transferred to the\nSuper model. Based on our experiments, the most\noptimal results for our joint inference framework\nis achieved when the crossing point of the two dis-\ntributions (highlighted in green in the ﬁgures) is\nchosen as the threshold tin Equation (9).\n4.1.1 Ablation Studies\nIn Sections 3.3.1 and 3.3.2, the possibility of using\nsoftmax and entropy scores instead of energy score\nwas theoretically analyzed. To support that analysis\nand also experimentally evaluate the performance\nof different routing mechanisms, an ablation study\non GLUE is performed, which is presented in Table\n2. In this study, we report the joint inference results\nbased on softmax, entropy, and random scores (i.e.,\nrandomly distributing the samples between Super\nand Swift). Our experiments show that, compared\nto the random score, softmax and entropy can result\nin satisfactory performance on routing the samples.\nHowever, as also discussed in Sections 3.3.1 and\n3.3.2, the energy score is still a better mechanism\nwith about 14% less FLOPs. Another potential\nmechanism is the perplexity (Chen et al., 1998), but\nsince it provides the same information as entropy,\nwe did not add any extra experiment on it.\nThe results with the usage of different Swift mod-\nels including T5-small (with 0.33 ×1011 FLOPs)\nand T5-base (with 1.24 ×1011 FLOPs) are also\ngiven in Table 2. Using these models as Swifts\ncan lead to good performance on some tasks, but\nnot all of them. For example, on SST2, the joint\ninference with T5-small and T5-base Swifts can\nrespectively reach the Super’s accuracy with 1.9X\nand 2.X less computations. In general, although\nthese models are smaller and require less FLOPs,\nour results in Table 2 indicate that they perform\nworse than T5-large in our joint inference structure.\nIn Figure 2, the trade-off curves for different Swift\nmodels are shown for GLUE and SuperGLUE.\nMoreover, to show the effectiveness of the extra\nenergy head for the Swift encoder, the E-LANG re-\nsults based on last linear layer of the Swift decoder\nis also given and compared in Table 2. As reported,\nthe E-LANG empowered by the energy head on\nthe Swift encoder outperforms the case with the\ndecoder’s head in both FLOPs (36.8% less) and\naccuracy (0.7% better). As explained in Section\n3.2.1, this shows the deep connection between the\nencoder’s features, discriminative models, and the\nproposed routing mechanism via the energy head.\nWe observed that E-LANG can achieve a high\nperformance even when applied to individually pre-\ntrained Super and Swift models. But, more im-\nprovement can still be obtained by performing KD\nfrom the Super to the Swift model, especially at\nthe ﬁne-tuning process for downstream tasks. To\nstudy this, we apply the KD technique in (Sanh\net al., 2019) to the Super and Swift models for\nsome GLUE tasks. As summarized in Table 3,\nthe Super model’s accuracy for QNLI, SST2, and\nCOLA is respectively attained by the distillation-\nbased E-LANG with 29.2%, 48.5%, and 14.3% less\nFLOPs than E-LANG (without distillation). The\nresults show the effectiveness of E-LANG along\nwith other compression techniques such as distilla-\ntion. The trade-off curves for this experiment will\nbe provided in the supplementary materials.\n4.2 BERT-Based Joint Inference\nIn this section, the proposed energy-based joint\ninference method is applied to the BERT archi-\ntecture (Devlin et al., 2019) and compared with\n5235\nFigure 3: Energy score distribution for GLUE tasks. t shows the optimal threshold.\nMNLI QNLI SST2 RTE MRPC COLA Average\nSuper (11B) 87.0 / 91.7 87.0 / 95.9 87.0 / 96.6 87.0 / 92.4 87.0 / 91.7 87.0 / 69.1 87.0 / 89.5\nRandom (Encoder) 78.5 / 91.5 61.9 / 95.3 58.7 / 96.3 60.2 / 91.2 47.5 / 91.9 61.6 / 67.2 61.4 / 88.9\nSoftmax (Encoder) 57.7 / 91.6 36.5 / 95.9 34.6 / 96.5 52.0 / 92.3 13.8 / 92.1 45.7 / 69.3 40.1 / 89.6\nEntropy (Encoder) 55.7 / 91.6 27.1 / 96.0 40.2 / 96.5 50.7 / 92.0 23.0 / 92.2 48.1 / 69.3 40.8 / 89.6\nEnergy (Swiftsmall) 71.3 / 91.0 58.8 / 95.6 47.0 / 96.6 71.2 / 88.5 55.0 / 91.4 75.3 / 68.3 63.1 / 88.5\nEnergy (Swiftbase) 54.5 / 91.5 50.5 / 95.8 35.9 / 96.6 55.8 / 90.6 44.0 / 91.9 50.6 / 68.4 48.5 / 89.1\nEnergy (Decoder) 57.9 / 90.6 68.1 / 95.5 75.8 / 96.3 60.5 / 91.5 20.2 / 90.9 45.1 / 69.3 54.6 / 89.0\nEnergy (Encoder) 47.8 / 91.7 25.7 / 96.0 32.0 / 96.6 50.4 / 92.4 11.5 / 92.2 39.9 / 69.5 34.5 / 89.7\nTable 2: Ablation study on different T5-based scenarios. Each cell shows FLOPs/Accuracy.\nQNLI SST2 COLA\nSuper (11B) 87.0 / 95.9 87.0 / 96.6 87.0 / 69.1\nSwift (Large) 4.25 / 93.9 4.25 / 95.5 4.25 / 62.7\n+ Distillation 4.25 / 95.0 4.25 / 95.7 4.25 / 63.3\nOurs 25.7 / 96.0 29.5 / 96.6 39.9 / 69.5\n+ Distillation 18.2 / 96.0 15.2 / 96.6 34.2 / 69.5\nTable 3: Distillation-based results with T5 in terms of\nFLOPs/Accuracy.\nBERT-based SOTA in both ﬁxed-size and dynamic\ninference. The majority of the previous methods\nemploy knowledge distillation and data augmenta-\ntion techniques for training their student models.\nFor a fair comparison, we follow the same practice\nand use the transformer distillation and augmenta-\ntion strategies in TinyBERT (Jiao et al., 2020) to\ntrain and prepare our Swift model (i.e., BERTTiny\nwith 1.2 ×109 FLOPs). Moreover, similar to the\nother works, we use BERTBase (with 21.8 ×109\nFLOPs) as our Super (i.e., teacher) model.\nIn Table 4, the comparison results with the base-\nline BERTBase and SOTA on GLUE benchmark\nare presented in terms of accuracy, FLOPs, and\nlatency. Compared to the Super model, E-LANG\ndelivers better accuracy on SST2 and RTE with\n3.5X and 2.0X FLOPs speed-up; and the same ac-\ncuracy on QNLI, MRPC, and QQP with 2.4X, 2.7X,\nand 7.0X FLOPs speed-up, respectively. On MNLI\nand COLA, 99.8% and 97.3% of the Super model’s\naccuracy are achieved, but with an average FLOPs\nspeed-up of 2.3X. On average, E-LANG outper-\nforms the Super model with 0.1% higher accuracy,\n3.2X less FLOPs, and 1.6X less latency.\nCompared with SOTA, our method achieves the\nbest performance on all GLUE tasks, except MRPC\nfor which SqueezeBERT outperforms all due to\nhaving a more accurate teacher (Iandola et al.,\n2020). There are some works such as ELECTRA\n(Clark et al., 2020) and MobileBERT (Sun et al.,\n2020) that require less FLOPs than our method,\nbut they only reach 95% of the baseline’s accuracy.\nCompared to other methods, GhostBERT (Huang\net al., 2021) and DynaBERT (Hou et al., 2020) give\nthe closest performance to the baseline and even\nthe same as ours on some tasks such as QNLI. How-\never, on average, they still need about 30% more\nFLOPs on GLUE compared to E-LANG.\nThe E-LANG accuracy vs. FLOPs trade-off\ncurves compared to SOTA on some of GLUE tasks\nare shown in Figure 4. The trade-off curves for all\nthe tasks are reported in the supplementary materi-\nals. Among the SOTA methods presented in Table\n5236\nMNLI (m/mm) QNLI SST2 RTE MRPC COLA QQP Avg. FLOPs (G) Time (ms)\nPrevious works\nBERTTiny (Swift) 82.8 / 82.9 87.9 92.6 65.7 85.8 49.7 90.5 78.5 1.2 7\nBERTBase (Super) 84.9 / 85.5 92.2 93.5 71.1 87.3 60.3 91.5 83.3 21.8 20\nDistillBERT 82.2 / - 89.2 92.7 59.9 87.5 51.3 88.5 78.8 11.3 -\nELECTRA 78.9 / - 87.9 88.3 68.5 84.4 56.8 88.3 79.0 3.7 -\nDeeBERT 83.9 / 82.9 90.9 93.4 69.5 - - - - - 17\nMobileBERT 84.3 / - 91.5 92.5 70.4 87.0 51.1 - 79.5 5.7 -\nSqueezeBERT 82.5 / 82.9 90.9 92.2 71.8 89.8 53.7 89.5 81.7 7.4 -\nLen-Adaptive 84.4 / - - 93.1 - - - - - 8.8 -\nTinyBERT 84.5 / 84.5 91.8 93.0 69.3 87.2 54.0 91.0 81.9 11.3 10\nELM 84.2 / - 90.8 92.7 72.2 89.0 54.2 91.1 82.0 10.9 -\nGhostBERT 84.7 / - 92.2 92.9 72.2 87.3 58.1 91.2 82.7 11.3 -\nDynaBERT 84.7 / 85.2 92.2 93.3 73.0 84.8 58.4 91.3 82.9 10.9 16\nE-LANG\nAccuracy (%) 84.7 / 85.4 92.2 93.7 73.3 87.3 58.7 91.5 83.4 - -\nFLOPs (G) 9.1 9.2 6.3 10.8 8.2 9.9 3.1 8.1 - -\nTime (ms) 14 14 11 16 13 15 9 13 - -\nSwift Ratio(%) 64 63 77 56 68 60 91 68 - -\nSpeed-up (FLOPs) 2.4X 2.4X 3.5X 2.0X 2.7X 2.2X 7.0X 3.2X - -\nSpeed-up (time) 1.4X 1.4X 1.8X 1.3X 1.5X 1.3X 2.2X 1.6X - -\nTable 4: Joint inference results with BERT architecture on GLUE development set compared with SOTA.\nFigure 4: Joint inference trade-off curves with BERT on GLUE development set compared with SOTA.\n4 and Figure 4, only DeeBERT (Xin et al., ACL\n2020), Length-Adaptive (Kim and Cho, 2021), and\nDynaBERT (Hou et al., 2020) are in the category\nof dynamic inference, where a single model can\noperate at different trade-off points between accu-\nracy and computational cost. The other approaches\npropose ﬁxed-size smaller versions of BERTBase,\nwhich require re-training for every trade-off point.\nTo investigate the orthogonality of E-LANG\nwith others, we integrate our energy-based joint\ninference strategy with DynaBERT that is SOTA\nin BERT-based adaptive inference. In other words,\nwe analyze whether E-LANG can be added on top\nof other efﬁcient methods to beneﬁt both from their\ndesigns and our approach. In this experiment, the\nDynaBERT conﬁgurations with the highest accu-\nracy (i.e., width=0.75 & depth=1.0) and the lowest\nFLOPs (i.e., width=0.5 & depth=0.25) are respec-\ntively employed as the Super and Swift models in\nour framework. The corresponding joint inference\nresults on MNLI, SST2, and QQP are reported in\nTable 5. As observed, we accomplish the Dyn-\naBERT Super’s accuracy for MNLI and SST2 with\n1.7X and 3.1X less FLOPs. For QQP, our method\ncombined with DynaBERT even outperforms Dyn-\naBERT by 0.1% with 2.6X FLOPs speed-up.\nMNLI SST2 QQP\nDynaBERT (Swift)\n(w=0.5, d=0.25)\n2.7 / 82.0 2.7 / 91.9 2.7 / 90.4\nDynaBERT (Super)\n(w=0.75, d=1.0)\n16.3 / 84.7 16.3 / 93.3 16.3 / 91.4\nOurs+DynaBERT 9.4 / 84.7 5.2 / 93.3 6.2 / 91.5\nTable 5: Orthogonality of E-LANG (ours) with DynaBERT\nin terms of FLOPs/Accuracy.\n5 Conclusion\nIn this paper, we introduced E-LANG, an energy-\nbased joint inference approach, which integrates\nSuper and Swift language models for achieving ef-\nﬁcient inference without sacriﬁcing the accuracy.\nOur method can work with both encoder-only (e.g.,\nBERT) and encoder-decoder (e.g., T5) architec-\ntures, and is also applicable for text classiﬁcation\nand sequence-to-sequence problems. The proposed\njoint inference strategy was theoretically and exper-\nimentally analyzed with an extensive set of experi-\nments and ablation studies. Our results showed that\nE-LANG outperforms SOTA in both ﬁxed-size and\ndynamic inference over different benchmarks such\nas GLUE and SuperGLUE. One future direction to\nthis work is to apply E-LANG to multiple Super\nand Swift models with different sizes.\n5237\nReferences\nMohammad Akbari, Amin Banitalebi-Dehkordi, and\nYong Zhang. 2021. EBJR: Energy-based joint rea-\nsoning for adaptive inference. BMVC 2021.\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin,\nXin Jiang, Qun Liu, Michael Lyu, and Irwin King.\n2021. BinaryBERT: Pushing the limit of bert quan-\ntization. In ACL 2021, pages 4334–4348.\nAmin Banitalebi-Dehkordi, Naveen Vedula, Jian Pei,\nFei Xia, Lanjun Wang, and Yong Zhang. 2021.\nAuto-split: A general framework of collaborative\nedge-cloud ai. In Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery &\nData Mining, pages 2543–2553.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, et al. 2016. Findings of\nthe 2016 conference on machine translation. In Pro-\nceedings of the First Conference on Machine Trans-\nlation: Volume 2, Shared Task Papers , pages 131–\n198.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nStanley F Chen, Douglas Beeferman, and Roni Rosen-\nfeld. 1998. Evaluation metrics for language models.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The lottery ticket hypothesis for pre-\ntrained bert networks. Advances in neural informa-\ntion processing systems.\nY . Cheng, D. Wang, P. Zhou, and T. Zhang. 2017. A\nsurvey of model compression and acceleration for\ndeep neural networks. arXiv:1710.09282.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. International Conference on Learning Repre-\nsentations, ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT (1).\nMitchell A Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. ACL 2020 ,\npage 143.\nManish Gupta and Puneet Agrawal. 2022. Compres-\nsion of deep learning models for text: A survey.\nACM Transactions on Knowledge Discovery from\nData (TKDD), 16(4):1–55.\nManish Gupta, Vasudeva Varma, Sonam Damani, and\nKedhar Nath Narahari. 2020. Compression of deep\nlearning models for nlp. In Proceedings of the 29th\nACM International Conference on Information &\nKnowledge Management, CIKM, pages 3507–3508.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. International Con-\nference on Learning Representations, ICLR.\nG. E. Hinton, O. Vinyals, and J. Dean. 2015. Dis-\ntilling the knowledge in a neural network. ArXiv,\nabs/1503.02531.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. Advances in Neural\nInformation Processing Systems, 33.\nZhiqi Huang, Lu Hou, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2021. Ghostbert: Gener-\nate more features with cheap operations for bert.\nACL/IJCNLP.\nForrest Iandola, Albert Shaw, Ravi Krishna, and Kurt\nKeutzer. 2020. SqueezeBERT: What can computer\nvision teach NLP about efﬁcient neural networks?\nIn Proceedings of SustaiNLP: Workshop on Simple\nand Efﬁcient Natural Language Processing , pages\n124–135. Association for Computational Linguis-\ntics.\nXiaoqi Jiao, Huating Chang, Yichun Yin, Lifeng Shang,\nXin Jiang, Xiao Chen, Linlin Li, Fang Wang, and\nQun Liu. 2021. Improving task-agnostic bert distil-\nlation with layer mapping search. Neurocomputing,\n461:194–203.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. Tinybert: Distilling bert for natural language\nunderstanding. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4163–4174.\nJing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, and\nZhiliang Gan. 2021. KDLSQ-BERT: A quan-\ntized bert combining knowledge distillation with\nlearned step size quantization. arXiv preprint\narXiv:2101.05938.\nGyuwan Kim and Kyunghyun Cho. 2021. Length-\nadaptive transformer: Train once with length drop,\nuse anytime with search. ACL 2021.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W\nMahoney, and Kurt Keutzer. 2021. I-bert: Integer-\nonly bert quantization. In International conference\non machine learning, pages 5506–5518. PMLR.\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,\nand F Huang. 2006. A tutorial on energy-based\nlearning. Predicting structured data, 1(0).\n5238\nYoungwan Lee, Joong-won Hwang, Sangrok Lee,\nYuseok Bae, and Jongyoul Park. 2019. An energy\nand gpu-computation efﬁcient backbone network for\nreal-time object detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition Workshops.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. International Con-\nference on Learning Representations, ICLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSangchul Oh, Abdelkader Baggag, and Hyunchul Nha.\n2020. Entropy, free energy, and work of restricted\nboltzmann machines. Entropy, 22(5):538.\nAntonio Polino, Razvan Pascanu, and Dan Alistarh.\n2018. Model compression via distillation and quan-\ntization. In International Conference on Learning\nRepresentations.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. 5th Work-\nshop on Energy Efﬁcient Machine Learning and\nCognitive Computing - NeurIPS 2019.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 8815–8821.\nYelong Shen, Po-Sen Huang, Jianfeng Gao, and\nWeizhu Chen. 2017. Reasonet: Learning to stop\nreading in machine comprehension. In Proceedings\nof the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , pages\n1047–1055.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. 2019a. Superglue: a\nstickier benchmark for general-purpose language un-\nderstanding systems. In Proceedings of the 33rd In-\nternational Conference on Neural Information Pro-\ncessing Systems, pages 3266–3280.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019b.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. ACL 2020. Deebert: Dynamic early ex-\niting for accelerating bert inference. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2246–2251.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. Advances in neural infor-\nmation processing systems, 32.\nKeyi Yu, Yang Liu, Alexander G Schwing, and Jian\nPeng. 2018. Fast and accurate text classiﬁcation:\nSkimming, rereading and early stopping. In 6th\nInternational Conference on Learning Representa-\ntions, ICLR 2018.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternarybert:\nDistillation-aware ultra-low bit bert. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 509–\n521.\n5239\nA Supplementary Materials\nThis section contains the supplementary materials.\nA.1 Code and Demo\nWe shared our code to make it easy to reproduce\nour BERT-based results. In addition to the code,\nwe included a video demo that contains a demon-\nstration of T5-based E-LANG. The BERT-based\nE-LANG source-code with the detailed running\ninstructions and the T5-based E-LANG demo are\navailable here1.\nPlease note that the demo is based on screen\nrecording of a web application we built to show the\nuse-cases of our method in real-world scenarios.\nFigure 5 shows a screenshot of the demo applica-\ntion.\nThe T5-based E-LANG code with detailed in-\nstructions is also shared in a ‘code’ directory in the\nsupplementary materials ﬁle.\nA.2 Additional Results and Visualizations\nThe trade-off curves (for the experiments given in\nTable 1) with T5 architecture on GLUE and Super-\nGLUE tasks are respectively shown in Figures 6\nand 7. The ablation over different Swift models are\nalso given in the ﬁgures.\nIn Figure 8, the accuracy vs. FLOPs trade-off\ncurves for distillation-based experiments (reported\nin Table 3) are also given. On QNLI, distillation-\nbased E-LANG (denoted by DE-LANG) with 4.8×\n1https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=64199726-\n9aaf-4905-8f6f-4cae290df874\nless computations than the Super model outper-\nforms E-LANG with 3.4 ×FLOPs speed-up, al-\nthough both methods performs 0.1% more accurate\nthan the Super model. DE-LANG on SST2 can\nalso achieve the Super model’s accuracy with 5.7×\nless computations, while the original E-LANG\nachieves the same performance with only 2.9 ×\nspeed-up. Moreover, DE-LANG can improve the\nSuper model’s accuracy by 0.1% with 2.9×speed-\nup on SST2. For COLA, DE-LANG achieves a\nbetter FLOPs speed-up of 2.5×than E-LANG with\n2.2×speed-up, where both outperform the Super\nmodel’s accuracy by 0.4%.\nFigure 9 also illustrates the corresponding curves\nfor the BERT-based results of Table 4, which are\ncompared with previous works in ﬁxed-size and\nadaptive inference.\nFigure 5: A demo application to show-case the adaptive inference with the proposed method.\n5240\nFigure 6: Trade-off curves with T5 backbone on GLUE tasks.\n5241\nFigure 7: Trade-off curves with T5 backbone on SuperGLUE tasks.\n5242\nFigure 8: Distillation-based trade-off curves with T5 backbone on some GLUE tasks.\n5243\nFigure 9: Trade-off curves compared with BERT-based SOTA on GLUE tasks.\n5244",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8361585736274719
    },
    {
      "name": "Swift",
      "score": 0.7594751119613647
    },
    {
      "name": "Inference",
      "score": 0.6679459810256958
    },
    {
      "name": "Language model",
      "score": 0.5997331142425537
    },
    {
      "name": "Computation",
      "score": 0.5582408905029297
    },
    {
      "name": "Code (set theory)",
      "score": 0.5564010143280029
    },
    {
      "name": "Encoder",
      "score": 0.5436240434646606
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4937485158443451
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4937203824520111
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4771251678466797
    },
    {
      "name": "Architecture",
      "score": 0.4383543133735657
    },
    {
      "name": "Energy (signal processing)",
      "score": 0.41095712780952454
    },
    {
      "name": "Computer engineering",
      "score": 0.4100727438926697
    },
    {
      "name": "Machine learning",
      "score": 0.35699617862701416
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3416014015674591
    },
    {
      "name": "Programming language",
      "score": 0.3040144443511963
    },
    {
      "name": "Operating system",
      "score": 0.098306804895401
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}