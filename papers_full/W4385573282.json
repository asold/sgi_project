{
  "title": "Scientific and Creative Analogies in Pretrained Language Models",
  "url": "https://openalex.org/W4385573282",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5079353642",
      "name": "Tamara Czinczoll",
      "affiliations": [
        "University of Potsdam",
        "Hasso Plattner Institute",
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A541455324",
      "name": "Helen Yannakoudakis",
      "affiliations": [
        "King's College London"
      ]
    },
    {
      "id": "https://openalex.org/A2407986837",
      "name": "Pushkar Mishra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2090286747",
      "name": "Ekaterina Shutova",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2739638526",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W419511246",
    "https://openalex.org/W3174082608",
    "https://openalex.org/W2026161499",
    "https://openalex.org/W2167609405",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W4288019845",
    "https://openalex.org/W2460442863",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W598266718",
    "https://openalex.org/W3117575631",
    "https://openalex.org/W4287670049",
    "https://openalex.org/W2908510526"
  ],
  "abstract": "This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we introduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy dataset containing systematic mappings of multiple attributes and relational structures across dissimilar domains. Using this dataset, we test the analogical reasoning capabilities of several widely-used pretrained language models (LMs). We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2094–2100\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nScientific and Creative Analogies in Pretrained Language Models\nTamara Czinczoll♣♡ Helen Yannakoudakis♠ Pushkar Mishra♢ Ekaterina Shutova♣\n♣ILLC, University of Amsterdam, the Netherlands\n♢Meta AI, London, United Kingdom\n♠Dept. of Informatics, King’s College London, United Kingdom\n♡Hasso Plattner Institute/University of Potsdam, Germany\ntamara.czinczoll@hpi.de, helen.yannakoudakis@kcl.ac.uk, pushkarmishra@meta.com, e.shutova@uva.nl\nAbstract\nThis paper examines the encoding of analogy\nin large-scale pretrained language models, such\nas BERT and GPT-2. Existing analogy datasets\ntypically focus on a limited set of analogical\nrelations, with a high similarity of the two do-\nmains between which the analogy holds. As a\nmore realistic setup, we introduce theScientific\nand Creative Analogy dataset (SCAN), a novel\nanalogy dataset containing systematic map-\npings of multiple attributes and relational struc-\ntures across dissimilar domains. Using this\ndataset, we test the analogical reasoning capa-\nbilities of several widely-used pretrained lan-\nguage models (LMs). We find that state-of-\nthe-art LMs achieve low performance on these\ncomplex analogy tasks, highlighting the chal-\nlenges still posed by analogy understanding.\n1 Introduction\nAnalogy-making is a cornerstone of human intel-\nligence (Gentner et al., 2001), allowing us to ac-\nquire new knowledge and creatively explore new\nconcepts. According to Gentner (1983)’s Structure-\nMapping Theory, analogical reasoning is different\nfrom surface similarity. Instead, the attributes of a\nfamiliar concept (the source domain) are mapped to\nsomething less familiar (the target domain) if their\nrelational structures are similar enough. For exam-\nple, while not directly similar in their attributes, the\nunderlying relational structure of the solar system\nmatches that of an atom. The relationship between\ntwo source domain attributes (e.g. sun and planet)\nhelps us to understand that between their target\ndomain counterparts (nucleus and electron).\nWithin natural language processing (NLP), the\nword analogy task (Mikolov et al., 2013), has been\nwidely used to demonstrate analogical reasoning\ncapabilities of pretrained word embedding models.\nThe task involves solving analogies of the form\nA:B :: C:D (i.e., A is to B as C is to D) by exploit-\ning (local) linear properties of word vectors (vector\noffsets). Subsequently, word analogy became one\nof the standardized tasks for intrinsic evaluation of\nword embedding quality. However, Gladkova et al.\n(2016) showed that the vector offset method was\nnot sufficient for most types of analogical relations,\nand Rogers et al. (2017) pointed out shortcuts that\nthe models were taking. Existing word analogy\ndatasets focus on a limited set of analogical rela-\ntions, include words that are semantically similar\nand do not require the model to relate distinct con-\ncepts via systematic comparison of their relational\nstructures, all of which is necessary for human-\nlike analogy making. In parallel, the field has seen\nthe development of large-scale pretrained sentence\nencoders, whose analogical reasoning capabilities\nhave not yet been fully tested.\nTo address these issues, we devise and release a\nnew dataset – the Scientific and Creative Analogy\ndataset (SCAN) – comprising holistic analogies be-\ntween concepts from semantically distant domains.\nIt draws on metaphorical and scientific analogies.\nResolving these analogies requires the models to\nidentify systematic ontological correspondences\nbetween two distinct semantic domains, such as in\nthe solar system – atom example. Our contribu-\ntions are threefold: 1) We present the SCAN anal-\nogy evaluation task and dataset, which we make\npublicly available to the research community; 2)\nWe systematically evaluate current state-of-the-art\nLMs on the established BATS dataset (Gladkova\net al., 2016), which consists of a large number of\ntraditional word analogies, as well as the novel\nSCAN dataset. We show that in the latter case the\nmodels exhibit severe limitations in understanding\nanalogies; 3) We show that a high performance\non BATS is not indicative of how well the models\nsolve the complex SCAN analogies, supporting our\nhypothesis that BATS does not require full analogi-\ncal reasoning.\n2094\n2 Related Work\nTurney (2008) presented an algorithm for anal-\nogy solving and tested it on 20 scientific and\nmetaphorical examples, where a source domain\nis mapped to a different target domain, along with\na number of its attributes (e.g. waves to sound).\nWhile few, these examples were true to human\nanalogy-making, representing a wide range of se-\nmantic relationships. Using a linear offset method\n(3CosAdd), Mikolov et al. (2013) demonstrated\nthat their word embeddings automatically capture\nanalogical information about word relationships, so\nthat emb(king) − emb(man) +emb(woman) ≈\nemb(queen), where emb is the embedding func-\ntion represented by the neural network. Gladkova\net al. (2016), however, showed that the pretrained\nword embeddings at the time could only reliably\ncomplete word analogies for inflectional morphol-\nogy categories while struggling on many semantic\ncategories. They released a balanced, larger and\nmore diverse dataset than Mikolov et al. (2013)’s\n(40 vs 15 relation types), the Bigger Analogy Test\nSet (BATS), demonstrating that Word2Vec was not\nable to solve most types of word analogies. In\nparticular, a larger semantic distance between the\nsource and target domains resulted in low perfor-\nmance (Rogers et al., 2017).\nTransformer language models such as BERT\n(Devlin et al., 2019) have pushed the state-of-the-\nart on a number of NLP tasks. But since 3CosAdd\ncannot easily be applied to them, due to their word\nembeddings not being fixed but dynamically cal-\nculated, their analogical capabilities have not been\ninvestigated much. While some headway has been\nmade in that regard (Li and Zhao (2020), Zhu and\nde Melo (2020), Ushio et al. (2021)), the focus has\nbeen on transferring the traditional word analogy\ndatasets to the sentence level. This does, however,\nalso transfer their limitations. In general, the low\nperformance of models in Gladkova et al. (2016)\nand Zhu and de Melo (2020) suggest that com-\npleting word analogies is challenging for state-of-\nthe-art LMs, even when structure mapping across\ndistinct domains is not explicitly tested.\n3 SCAN Dataset\nOur dataset contains 449 analogy instances, clus-\ntered into 65 full concept mappings. A source con-\ncept is mapped to a target concept along with a\nnumber of related attributes. Table 1 provides two\nexamples. When mapping from the source concept\nWar to the target concept Argument, a number of\nrelevant attributes’ correspondences are given. The\nnumber of attributes per cross-domain mapping is\nnot fixed. The dataset includes the 20 mappings\nfrom Turney (2008) (10 scientific and 10 metaphor-\nical) and extends them by another 43 metaphorical\nmappings and 2 scientific ones. The new metaphor-\nical mappings include conceptual metaphors from\nthe Master Metaphor List (Lakoff et al., 1991) and\nother conceptual metaphors widely-discussed in\nlinguistic literature (Lakoff and Johnson, 1980;\nMusolff, 2000; Lakoff and Wehling, 2012). Each\nconceptual metaphor was then annotated for at-\ntribute correspondences by three metaphor experts.\nFirst, the semantic frames of the source and target\ndomains were identified and then the correspon-\ndences between individual frame elements were\nestablished (see Tab. 1). We build a word analogy\ntask from this data by defining the cross-domain\nmappings (e.g., Argument and War) as the first\nword pair, and the attribute mappings (in this case\nDebater and Combatant) as the to-be-completed\nsecond word pair. Since each concept includes mul-\ntiple attributes, a total of 449 word analogies are\nconstructed.\nSCAN offers richer and more holistic analogies\nthan traditional word analogy datasets. Taking a\nstatistical view, the chances of the words in the\nsource and target domains co-occurring are much\nlower than in BATS. For example, countries and\ntheir capitals, animals and the sounds they make,\nand most grammatical analogy types in BATS are\nquite likely to occur in the training corpus together.\nHowever, the same cannot be said for most SCAN\nanalogies, meaning that true analogical transfer\nneeds to occur.\nAdditionally, the in-domain words in SCAN are\nsemantically more dissimilar. For example, in the\nargument domain, debater and topic are fundamen-\ntally different concepts. In BATS, on the other\nhand, every domain member is another instance\nof the same concept, e.g. France and Germany\ninstances of a country.\nLastly, the analogical relationships between do-\nmains in SCAN are more abstract than those in\nBATS. To successfully extract the same relation-\nship from solar system – atomand planet – electron,\nmore abstraction and inference over the relational\nstructure of the domains is needed than in BATS.\nIn BATS, the relationships between, e.g. France –\nParis and Germany – Berlin, are straightforward\n2095\nTarget Source Attribute mapping\nArgu- War Debater Combatant\nment Topic Battleground\nClaim Position\nCriticize Attack\nRhetoric Maneuver\nCode Virus Malware Virus\nReplication Reproduction\nInstallation Infection\nRemoval Eradication\nAntivirus Vaccine\nTable 1: Example mappings in SCAN. For one source\nconcept multiple relevant attributes are mapped to the\ncorresponding target concept’s attributes.\nand do not require much abstraction.\nOverall, SCAN offers more human-like analo-\ngies by employing more diverse in-domain words,\nmore abstract mapping relations and by avoiding\nobvious co-occurrences. Due to its full-concept\nmappings, SCAN is not confined to the word anal-\nogy task. By holistically mapping entire source\ndomains to a new target domain we want to encour-\nage a broader range of analogy representations.\n4 Models\nWe probe the analogical capabilities of several\nwidely used language models: GPT-2, BERT and\nMultilingual BERT (M-BERT). We use GloVe as\na baseline, given it has been shown to outperform\nlanguage models on some relation types in previous\nanalogy tasks (Zhu and de Melo, 2020).\nGPT-2 (Radford et al., 2019) can be viewed as\na “true” LM since it is trained to predict the next\nword in a sequence, and can be used for language\ngeneration. It is a transformer-based model with\n48 layers and 1542M parameters, trained on a cus-\ntom dataset, WebText, created only from outbound\nlinks from Reddit to improve text quality. Due to\nits predictive nature, GPT-2 is one-directional, i.e.\nonly the context on the left-hand side influences\nthe prediction of the next word.\nBERT (Devlin et al., 2019) is a bidirectional\nlanguage representation model. It is trained with\ntwo objectives: masked-token prediction and next-\nsentence prediction. We use BERT-base with\n12 layers in our experiments ( 110M parameters).\nSince BERT is bidirectional, it can incorporate in-\nformation from both sides of the masked token.\nM-BERT is a BERT model, trained on a\nWikipedia dump of 100 languages. The model\nperforms best on high-resource languages such as\nEnglish, French and Chinese, since lower-resource\nlanguages are underrepresented in the training\ndata. We test whether M-BERT’s pre-training on\na wide range of languages, and thus a wide range\nof culture-specific analogies, might enhance the\nmodel’s general analogy understanding.\n5 Experiments\nSetup We use pretrained model instances of GPT-\n2, BERT Base and Multilingual BERT 1. As BERT\nand GPT-2 are trained on full sentences, we insert\nthe word analogy quadruple into a placeholder sen-\ntence. We use “ If A is like B, then C is like D.”,\nwhich was selected from a set of candidates as it\nperformed best on the development set. Similarly\nto Ettinger (2020), who probed BERT with a num-\nber of cloze and negation tasks, the models need to\npredict the last token of the sentence. We force the\nmodels to predict word D by either masking it for\nthe two BERT models, or by cutting the sentence\noff before it for GPT-2. We report the mean recip-\nrocal rank (MRR) of the first token of the target\nword (or that of one of the alternative answers) in\nonly the top 10 predicted tokens to reduce compute.\nIf the label is not in the top 10 tokens, its RR is\n0. Model performance in terms of accuracy, re-\ncall@10 and recall@5 is reported in the Appendix.\nWe use an Nvidia 16GB GPU.\nSCAN vs. BATS To evaluate how well the mod-\nels can solve different types of analogy, we test\nthem on BATS in addition to SCAN. We do not\nfine-tune the models. BATS consists of 98000 ex-\namples of balanced relations. There are four main\nrelations – inflectional and derivational morphol-\nogy, and lexicographic and encyclopedic semantics\n– each of which consists of ten subcategories. For\nsome examples, multiple correct answers are listed.\nZero-shot vs. One-shot Previous work on ana-\nlogical reasoning in GPT-3 (Mitchell, 2020; the\nLatitude Team, 2020) has shown that when the\nmodel is given a full example of a word analogy\nin addition to the incomplete one, the performance\non the incomplete analogy substantially increases.\nWe see this as a form of one-shot vs. zero-shot\ntesting and also test the models this way, inves-\ntigating whether this has an impact on the LMs’\n1https://huggingface.co/\n2096\nBATS SCAN Science Meta.\nGloVe .099 .022 .099 .006\nGPT-2 .098 .057 .073 .054\nBERT .207 .044 .092 .034\nM-BERT .205 .041 .088 .031\nTable 2: Model MRR on BATS and SCAN. Statisti-\ncally significant differences compared to the GloVe base-\nline are in bold (two-sided permutation test; p <0.05;\n#resamples=10e5).\nperformance on SCAN. We insert a complete ver-\nsion of our template sentence before the incomplete\none, ensuring that none of the analogy words from\nthe full example appear in the incomplete analogy.\nNote that GloVe does not benefit from this setup as\nthe vectors used for 3CosAdd remain the same.\nTraining Set Effects Lastly, we further inves-\ntigate the difference between the types of word\nanalogies in BATS and those in SCAN. We split\nthe BATS dataset into a train, validation and test set\n(70/15/15 ratio), ensuring that each word pair ap-\npears in only one of them. We fine-tune the LMs on\nthe training set and take each model’s version with\nthe best score on the BATS validation set. We train\n(∼ 4h) all models with the AdamW (Loshchilov\nand Hutter, 2019) optimizer, a learning rate of5e−5\nand a batch size of 16 for 4 epochs (based on man-\nual tuning). If the model has learned about general\nanalogy-making it must understand new analogical\nrelations “on-the-fly” and improve not only on the\nBATS test set but also on SCAN. We expect there to\nbe strong improvements on BATS compared to its\nuntrained counterpart, but little to none on SCAN,\nshowing them to be inherently different. This is not\nperformed on GloVe, since 3CosAdd outputting an\nembedding is not part of its original training setup.\n6 Results & Discussion\nTable 2 shows the accuracy of each of the models\non the BATS and SCAN datasets, as well as on the\nscientific and creative analogies separately. BERT\nachieves the highest MRR on the BATS dataset,\nwith a strong lead compared to the other mod-\nels. Similarly to Zhu and de Melo (2020), we find\nthat GloVe can keep up with the other models on\nBATS, performing similarly to GPT-2. However,\nthis trend is not observed on the SCAN dataset,\nwhere GloVe is relegated to last place, indicating\nthat context is important for understanding SCAN\nanalogies. All models perform better on the scien-\ntific analogies than on metaphors. This could be\nBATS SCAN Science Meta.\nGPT-2 .121 .048 .056 .046\nBERT .095 .035 .077 .027\nM-BERT .180 .036 .112 .020\nTable 3: MRR when an example sentence is given. Sta-\ntistically significant differences (two-sided permutation\ntest; p < 0.05; #resamples= 10e5) compared to each\nmodel’s baseline in bold.\nBATS SCAN Science Meta.\nGPT-2 .384 .022 .066 .012\nBERT .592 .019 .061 .010\nM-BERT .499 .020 .087 .006\nTable 4: MRR on the BATS test set as well as on\nSCAN after training. Statistical significance compared\nto the baseline (two-sided permutation test; p <0.05;\n#resamples=10e5) in bold.\ndue to the fact that their attributes are less abstract\nand have clearer correspondences in the target do-\nmain. The models’ MRRs are generally lower on\nSCAN, which we attribute to the greater semantic\ndissimilarity between source and target domains.\nGPT-2 achieves the highest performance, followed\nby BERT. This, combined with its lower accuracy\non the BATS baseline, indicates that GPT-2 is better\nat modeling more extensive and narrative analogies\ninstead of the more artificial and strictly-defined\nones in BATS. Multilingual features only appear\nto be marginally effective for the task, something\nwhich can be explained by the fact that most analo-\ngies are language-dependent, an observation also\nmade by Ulˇcar et al. (2020). Overall, these results\nindicate that the SCAN analogy task is challenging\nfor state-of-the-art LMs and that their true analogy\nsolving capabilities still need to be improved.\nZero vs. One-Shot Table 3 shows model accu-\nracy when the input contains a complete additional\nexample. Apart from GPT-2 on BATS, this does not\nhelp the models better understand the task. This\ncontrasts the examples on GPT-3 from Mitchell\n(2020), possibly due to the models not identifying\nthe analogical relationships in the example sen-\ntence.\nTraining Set Effects After training on BATS,\none could expect that if the models learn about\nanalogical reasoning in general, they would also\nnaturally do better on the SCAN dataset with more\ncomplex analogies. However, our results in Table 4\nshow that the opposite is the case. While training\non BATS drastically increases the models’ MRR\n2097\non the held-out BATS test set, it has an adverse ef-\nfect on SCAN. This suggests that the two datasets’\nanalogy types are innately different, validating our\nhypothesis that standard word analogy datasets do\nnot adequately represent human analogy use.\nError Analysis While GloVe scores consistently\non all relation types in BATS, this is not the case for\nthe other models. On SCAN, none of the models\npredict the mappings of all attributes of a concept\n(or even most of them) correctly. While the models\nare able to solve some individual mappings, the\nfact that they cannot apply this to all aspects of the\nconcept indicates that none of them are really able\nto grasp the workings of analogy. In cases where\nanalogies remain entirely unsolved, it is likely that\nthe required domain knowledge is lacking.\n7 Conclusion\nAnalogical reasoning remains a challenging task\neven when state-of-the-art Transformer LMs are\nused. We have shown that, even with models such\nas BERT and GPT-2, there is large room for im-\nprovement on automated reasoning and understand-\ning of realistic analogies. We have introduced a\nnew dataset, SCAN, that is different from existing\nword analogy datasets in that it is composed of\nwhole concept mappings across semantically dis-\nsimilar domains, demonstrating that popular LMs\nare unable to fully understand these analogies. We\nfurther tested whether a full example of the task\ncan help the models, finding that this is not helpful\nin our setup. Lastly, our results indicate that the\nSCAN analogies are substantially different from\nthose of traditional word analogy datasets. Im-\nproving on them is a line of research we wish to\ninvestigate further in the future. We make SCAN\nand the related code publicly available.2\n8 Limitations\nOur experimental design focuses on evaluating the\nmodels’ analogical capabilities in a generative set-\nting. We see value in this, as analogical reason-\ning is inherently a generative cognitive function.\nThe BERT models are, however, not trained to per-\nform left-to-right generation and, furthermore, rely\non wordpiece vocabulary for tokenization. The\nevaluation of its predictions in the analogy task\nis, therefore, less straightforward and not exactly\ncomparable to other models. We adapt the task for\n2https://github.com/taczin/SCAN_analogies\nBERT models by letting them only predict the first\ntoken of the missing answer. Comparing only the\nfirst token leaves some variability, however, when\nmatching the prediction and the right answer. We\nexpect this effect to be limited in English due to its\nsparse morphology.\nFurthermore, the metaphorical analogies come\nfrom English literature and cultural background. It\nwould be interesting to compare these with analo-\ngies from other languages and cultures to investi-\ngate whether the language models’ lack of under-\nstanding is due to encoding of language-specific\nproperties, missing domain knowledge or the gen-\neral analogical mapping abilities.\nLastly, some metaphors in SCAN exhibit anti-\nquated gender roles, e.g. the metaphor “govern-\nment:household :: governor:father”. While these\nrelationships are culturally often still relevant for\nmetaphor understanding, the underlying implied\ngender roles need to be treated carefully and issues\nof their encoding by neural models investigated\nfurther.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nDedre Gentner. 1983. Structure-mapping: A theoret-\nical framework for analogy*. Cognitive Science ,\n7(2):155–170.\nDedre Gentner, Keith Holyoak, and Boicho Kokinov.\n2001. The Analogical Mind: Perspectives From Cog-\nnitive Science. MIT Press.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Mat-\nsuoka. 2016. Analogy-based detection of morpholog-\nical and semantic relations with word embeddings:\nWhat works and what doesn’t. In Proceedings of the\nNAACL-HLT SRW, pages 47–54, San Diego, Califor-\nnia, June 12-17, 2016. ACL.\nGeorge Lakoff, Jane Espenson, and Alan Schwartz.\n1991. The master metaphor list. Technical report,\nUniversity of California at Berkeley.\n2098\nGeorge Lakoff and Mark Johnson. 1980. Metaphors We\nLive By. University of Chicago Press, Chicago.\nGeorge Lakoff and Elisabeth Wehling. 2012. The Little\nBlue Book: The Essential Guide to Thinking and\nTalking Democratic. Free Press, New York.\nYian Li and Hai Zhao. 2020. Learning universal repre-\nsentations from word to sentence.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efficient estimation of word representa-\ntions in vector space.\nMelanie Mitchell. 2020. Can gpt-3 make analogies?\nhttps://medium.com/@melaniemitchell.me/\ncan-gpt-3-make-analogies-16436605c446 .\nAccessed: 2021-05-05.\nAndreas Musolff. 2000. Mirror images of Europe:\nMetaphors in the public debate about Europe in\nBritain and Germany. Iudicium, Muenchen.\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners.\nAnna Rogers, Aleksandr Drozd, and Bofang Li. 2017.\nThe (too many) problems of analogical reasoning\nwith word vectors. In Proceedings of the 6th Joint\nConference on Lexical and Computational Semantics\n(*SEM 2017), pages 135–148, Vancouver, Canada.\nAssociation for Computational Linguistics.\nthe Latitude Team. 2020. World creation by\nanalogy. https://aidungeon.medium.com/\nworld-creation-by-analogy-f26e3791d35f .\nAccessed: 2022-10-19.\nP. D. Turney. 2008. The latent relation mapping engine:\nAlgorithm and experiments. Journal of Artificial\nIntelligence Research, 33:615–655.\nMatej Ulˇcar, Kristiina Vaik, Jessica Lindström, Milda\nDailid˙enait˙e, and Marko Robnik-Šikonja. 2020. Mul-\ntilingual culture-independent word analogy datasets.\nIn Proceedings of the 12th Language Resources and\nEvaluation Conference, pages 4074–4080, Marseille,\nFrance. European Language Resources Association.\nAsahi Ushio, Luis Espinosa Anke, Steven Schockaert,\nand Jose Camacho-Collados. 2021. BERT is to NLP\nwhat AlexNet is to CV: Can pre-trained language\nmodels identify analogies? In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3609–3624, Online. As-\nsociation for Computational Linguistics.\nXunjie Zhu and Gerard de Melo. 2020. Sentence analo-\ngies: Linguistic regularities in sentence embeddings.\nIn Proceedings of the 28th International Conference\non Computational Linguistics , pages 3389–3400,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\n2099\nA Additional Evaluation Metrics\nModel BATS SCAN SCAN Science SCAN Meta.\nAcc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5\nGloVe .004 .099 .240 .192 .018 .022 .031 .024 .091 .099 .110 .097 .003 .006 .014 .009\nGPT-2 .044 .098 .250 .172 .020 .057 .167 .107 .026 .073 .195 .143 .019 .054 .161 .099\nBERT .126 .207 .401 .316 .009 .044 .134 .080 .026 .092 .273 .182 .005 .034 .105 .059\nMultBERT .141 .205 .341 .289 .018 .041 .100 .067 .065 .088 .130 .117 .008 .031 .094 .056\nTable 5: Accuracy, MRR, Recall@10 and Recall@5 on the two datasets. Statistically significant differences compared to the GloVe baseline are in bold (two-sided permutation\ntest; p <0.05; #resamples=10e5).\nModel BATS SCAN SCAN Science SCAN Meta.\nAcc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5\nGPT-2 .050 .121 .282 .218 .018 .048 .147 .087 .013 .056 .169 .104 .019 .046 .142 .083\nBERT .054 .095 .207 .152 .016 .035 .107 .060 .052 .077 .143 .117 .008 .027 .099 .048\nMultBERT .123 .180 .307 .256 .020 .036 .071 .051 .091 .112 .143 .13 .005 .020 .056 .035\nTable 6: Accuracy, MRR, Recall@10 and Recall@5 when an example sentence is given. Statistically significant differences (two-sided permutation test; p < 0.05;\n#resamples=10e5) compared to each model’s baseline in bold.\nModel BATS SCAN SCAN Science SCAN Meta.\nAcc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5 Acc MRR Rec@10 Rec@5\nGPT-2 .305 .384 .550 .482 .011 .022 .051 .036 .039 .066 .143 .117 .005 .012 .032 .019\nBERT .501 .592 .756 .717 .004 .019 .051 .031 .026 .061 0.130 .091 .000 .010 .035 .019\nMultBERT .420 .499 .661 .604 .016 .020 .038 .022 .078 .087 .156 .078 .003 .006 .013 .011\nTable 7: Accuracy, MRR, Recall@10 and Recall@5 on the BATS test set as well as on SCAN after training. Statistically significant differences (two-sided permutation test;\np <0.05; #resamples=10e5) compared to each model’s baseline in bold.\n2100",
  "topic": "Analogy",
  "concepts": [
    {
      "name": "Analogy",
      "score": 0.9935171008110046
    },
    {
      "name": "Computer science",
      "score": 0.7547314167022705
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.6625436544418335
    },
    {
      "name": "Analogical reasoning",
      "score": 0.5809711217880249
    },
    {
      "name": "Artificial intelligence",
      "score": 0.53337562084198
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5291545391082764
    },
    {
      "name": "Language model",
      "score": 0.4825950860977173
    },
    {
      "name": "Focus (optics)",
      "score": 0.48048272728919983
    },
    {
      "name": "Natural language processing",
      "score": 0.459547221660614
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4370189905166626
    },
    {
      "name": "Programming language",
      "score": 0.17141905426979065
    },
    {
      "name": "Linguistics",
      "score": 0.1261853277683258
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I143288331",
      "name": "Hasso Plattner Institute",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I176453806",
      "name": "University of Potsdam",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I183935753",
      "name": "King's College London",
      "country": "GB"
    }
  ],
  "cited_by": 12
}