{
    "title": "When BERT meets Bilbo: a learning curve analysis of pretrained language model on disease classification",
    "url": "https://openalex.org/W4225545910",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2229288822",
            "name": "Li Xuedong",
            "affiliations": [
                "Sichuan University",
                "Chengdu University"
            ]
        },
        {
            "id": null,
            "name": "Yuan, Walter",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2369606012",
            "name": "Peng Dezhong",
            "affiliations": [
                "Sichuan University",
                "Chengdu University"
            ]
        },
        {
            "id": "https://openalex.org/A2748366560",
            "name": "Mei Qiaozhu",
            "affiliations": [
                "University of Michigan–Ann Arbor"
            ]
        },
        {
            "id": "https://openalex.org/A1852389517",
            "name": "Wang Yue",
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2190333735",
        "https://openalex.org/W3013605954",
        "https://openalex.org/W2157549840",
        "https://openalex.org/W1994929347",
        "https://openalex.org/W2979920993",
        "https://openalex.org/W2743028754",
        "https://openalex.org/W2927032858",
        "https://openalex.org/W3047797631",
        "https://openalex.org/W96276655",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W6763692313",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W2976476443",
        "https://openalex.org/W3023618320",
        "https://openalex.org/W2467995757",
        "https://openalex.org/W6601454296",
        "https://openalex.org/W2100676408",
        "https://openalex.org/W2109206523",
        "https://openalex.org/W2132724073",
        "https://openalex.org/W2902516827",
        "https://openalex.org/W2112950297",
        "https://openalex.org/W2956394034",
        "https://openalex.org/W2153064780",
        "https://openalex.org/W2093157872",
        "https://openalex.org/W2993873509",
        "https://openalex.org/W2025428542",
        "https://openalex.org/W2991755410",
        "https://openalex.org/W2557074642",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2157807817",
        "https://openalex.org/W1888005072",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2620787630",
        "https://openalex.org/W1560851690",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3011762034",
        "https://openalex.org/W3025400983"
    ],
    "abstract": null,
    "full_text": "Li et al. \nBMC Medical Informatics and Decision Making          (2021) 21:377  \nhttps://doi.org/10.1186/s12911-022-01829-2\nRESEARCH\nWhen BERT meets Bilbo: a learning curve \nanalysis of pretrained language model \non disease classification\nXuedong Li1, Walter Yuan2, Dezhong Peng1, Qiaozhu Mei3 and Yue Wang4*  \nFrom The China Conference on Health Information Processing (CHIP) 2020 Shenzhen, Guangdong, \nChina. 30-31 November 2020\nAbstract \nBackground: Natural language processing (NLP) tasks in the health domain often deal with limited amount of \nlabeled data due to high annotation costs and naturally rare observations. To compensate for the lack of training data, \nhealth NLP researchers often have to leverage knowledge and resources external to a task at hand. Recently, pre-\ntrained large-scale language models such as the Bidirectional Encoder Representations from Transformers (BERT) have \nbeen proven to be a powerful way of learning rich linguistic knowledge from massive unlabeled text and transfer-\nring that knowledge to downstream tasks. However, previous downstream tasks often used training data at such a \nlarge scale that is unlikely to obtain in the health domain. In this work, we aim to study whether BERT can still benefit \ndownstream tasks when training data are relatively small in the context of health NLP .\nMethod: We conducted a learning curve analysis to study the behavior of BERT and baseline models as training \ndata size increases. We observed the classification performance of these models on two disease diagnosis data sets, \nwhere some diseases are naturally rare and have very limited observations (fewer than 2 out of 10,000). The baselines \nincluded commonly used text classification models such as sparse and dense bag-of-words models, long short-term \nmemory networks, and their variants that leveraged external knowledge. To obtain learning curves, we incremented \nthe amount of training examples per disease from small to large, and measured the classification performance in \nmacro-averaged F1 score.\nResults: On the task of classifying all diseases, the learning curves of BERT were consistently above all baselines, \nsignificantly outperforming them across the spectrum of training data sizes. But under extreme situations where only \none or two training documents per disease were available, BERT was outperformed by linear classifiers with carefully \nengineered bag-of-words features.\nConclusion: As long as the amount of training documents is not extremely few, fine-tuning a pretrained BERT model \nis a highly effective approach to health NLP tasks like disease classification. However, in extreme cases where each \nclass has only one or two training documents and no more will be available, simple linear models using bag-of-words \nfeatures shall be considered.\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nOpen Access\n*Correspondence:  wangyue@email.unc.edu\n4 School of Information and Library Science, University of North Carolina \nat Chapel Hill, Chapel Hill, NC, USA\nFull list of author information is available at the end of the article\nPage 2 of 10Li et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \nBackground\nMachine learning has become the predominant approach \nto health natural language processing (NLP) in recent \nyears. To achieve high performance, machine learn -\ning models often need to be trained on a substantial \namount of labeled data. Deep learning models, while \ncapable of achieving even higher performance, may need \nmore training data to train a large number of internal \nparameters.\nUnlike machine learning tasks in the general domain \nwhere training data are abundant, health NLP data are \nmostly small, as creating such data at scale can be prohib-\nitively expensive and even infeasible. 1 For instance, labe -\nling social media posts can be crowdsourced at a very low \ncost through Amazon Web Services [1], while annotating \nclinical notes requires special medical training and long \nhours [2]. On the task of rare disease identification, the \namount of labeled documents is further bounded by the \nsize of population, since rare diseases appear very infre -\nquently (a rare disease affects fewer than 1 in 1500 peo -\nple in the U.S. [3] or 1 in 2000 in Europe [4]). As a result, \nhealth NLP researchers have been proposing a variety of \nmethods to compensate for the lack of training data [5]. \nThese include leveraging expert knowledge and medi -\ncal ontologies [6–8], transferring statistical knowledge \nlearned from related tasks [9], simultaneously learning \nfrom multiple tasks [10], using weak/distant supervision \nsignals [11, 12], selectively asking experts for label [13].\nRecently, Bidirectional Encoder Representations from \nTransformers (BERT) model has been increasingly \nadopted by the NLP research community as it celebrates \nsuperior performance in a wide range of NLP tasks [14]. \nBERT learns contextual representation of words using \ninformation from both sides of a word, effectively cap -\nturing syntactic and semantic knowledge that can ben -\nefit many NLP tasks. A pretrained BERT model can be \ntailored to a specific NLP task by using the task-specific \ndata to further train the model, a procedure known as \n``fine-tuning’’ . In this way, the new task can build on top \nof the pretrained knowledge in BERT to achieve superior \ngeneralization performance. However, previous works all \nuse very large data sets for fine-tuning, which are often \non the order of hundreds of thousands and even millions \nof examples [15, 16]. In general, however, it is impractical \nto collect training data at such a large scale in the health \ndomain, for reasons discussed above.\nGiven the high potential of BERT and the often small \ndata in health NLP , it is natural to ask the following ques-\ntion: can we fine-tune BERT on small health NLP data \nand still achieve superior performance? On the one hand, \nBERT may hold the promise as it has been shown to per -\nform well in many NLP tasks thanks to the unsupervised \npretraining. On the other hand, BERT is itself a large \ncomplex model with a massive number of parameters, so \nto achieve high performance it may need a good amount \nof labeled data for fine-tuning.\nIn this paper, we answer the above question by con -\nducting learning curve analyses of BERT and other mod -\nels on a disease diagnosis task. As conceptually shown \nin Fig.  1, a learning curve can be viewed as a ’’return-\non-investment’’ curve, where the ’’investment’’ is labeled \ndata, and the ’’return’’ is a model’s generalization perfor -\nmance on test data. Learning curves allow us to com -\npare the performance of different models given different \nlabeling budgets. They can also show which model will \nimprove faster if we invest more labels. Such a compari -\nson is especially relevant when the labeling cost is high, \nas in health NLP task scenarios.\nThe learning curve analysis reveals a series of interest -\ning and informative findings, as summarized below:\n• BERT is able to achieve superior performance even \nwhen fine-tuned on a handful of (but more than one) \nlabeled documents per class.\n• BERT’s prior knowledge can effectively compensate \nfor the lack of training data in most cases, but simple \nlinear models are still worth considering when the \namount of training data is extremely limited and not \nexpected to increase any time soon. In the extreme \ncase where each class has only one or two labeled \ndocuments, BERT could be outperformed by models \nKeywords: Learning curve, Bidirectional encoder representations from transformers, Disease classification\nFig. 1 Learning curves can inform NLP method selection given \nlabeling budget. If the labeling budget is n1 , then Method1 is preferred. \nIf the labeling budget increases to n2 , then Method2 is preferred\n1 In The Lord of the Rings, ’’Bert’’ is a giant stone troll. In terms of size, health \nNLP data are the ’’Hobbits’’ (represented by Bilbo Baggins) among machine \nlearning datasets.\nPage 3 of 10\nLi et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \n \nusing carefully engineered sparse bag-of-words fea -\ntures.\n• When more labeled documents start to become \navailable, BERT demonstrates fast rate of perfor -\nmance gain, which allows it to quickly outperform \nother models by a significant margin. It shows that \nBERT’s prelearned representation enables it to \nextract the most rich information from each train -\ning example. In other words, if we modestly increase \nthe labeling budget, BERT will likely show a very high \nreturn.\nPrior work\nBERT in health domain\nLee et  al. obtained BioBERT by taking Google’s pre -\ntrained BERT model and continuing the pretraining tasks \n(masked language modeling and next sentence predic -\ntion) on large-scale biomedical literature [17]. The use of \ndomain-specific texts enabled BioBERT to outperform \nBERT on certain biomedical NLP tasks. Alsentzer et  al. \n[18] further added clinical texts to continue the pre -\ntraining on the basis of BioBERT to get Clinical BERT. \nA closely related line of work was conducted by Peng \net al., where BERT is fine-tuned on biomedical and clini -\ncal texts, and then applied to ten benchmarking tasks, \nincluding sentence similarity measurement, named entity \nrecognition, relation extraction, document classification, \nand logical inference [19]. All the above works demon -\nstrate the value of domain-specific pretraining when \napplying BERT on health domain tasks. BERT has also \nbeen applied to non-English health NLP tasks. Pretrained \nChinese BERT models have been fine-tuned and applied \non NLP tasks such as disease classification, [20], named \nentity recognition [21], and a host of other tasks [22].\nThis paper studies BERT from another significant \nperspective, i.e., its generalization performance when \nfine-tuned on small training data. To the best of our \nknowledge, there has been no previous work that stud -\nies the performance of BERT when the size of training \ndata starts from very small. Instead, researchers often use \nlearning curves to demonstrate the enormous learning \ncapacity of deep learning models when training data size \nscales up exponentially [23].\nDisease classification\nStanfill et  al. conducted a systematic literature review \nof clinical coding and classification systems [24]. Recent \nworks on disease classification studied various applica -\ntion scenarios, including smoking status identification \n[25], obesity prediction [26, 27], online patient forum \nmoderation [28], cancer patient sentiment classification \n[29], vaccine adverse events detection [30], etc. These \nworks above are all based on English texts. Zhong et al. \n[31] applied nearest neighbor classifier to identify the \ndisease category based on patient disease description in \nChinese. In this study, we predict the presence of a dis -\nease in documents written in Chinese. Although the texts \nare written by patients and health insurance profession -\nals, applying NLP on these texts shares similar challenges \nas clinical NLP [32, 33], where the texts are written by \nphysicians.\nIncorporating existing knowledge\nExternal knowledge has significant impact on machine \nlearning performance. Besides pretraining model param -\neters using large unlabeled corpus, incorporating knowl -\nedge from ontologies (a.k.a. knowledge graphs or KGs) \nhas also received attention. Garla et  al. [34] utilized the \nrelationship between medical concepts in KG to improve \nfeature selection. Yao et  al. used UMLS entity embed -\ndings in convolutional neural networks [27]. Li et  al. \nused KG to derive additional knowledge features in rare \ndisease classification [35]. Choi et  al. [36] developed a \ngraph-based attention model to represent words using \nnode vectors learned from the ontology. Some studies \n[37, 38] suggest that incorporating KG into BERT also \ncan bring some benefits.\nMethod\nData description and problem formulation\nWe use two Chinese patient disease classification cor -\npora. The first corpus, HaoDaiFu, contains 51,374 patient \nrecords categorized into 805 diseases. Each document \ncontains the symptom description submitted by a patient \nto Haodf.com, the largest Chinese online platform that \nconnects patients to doctors. These patients have been \npreviously diagnosed by a clinician, and now come to \nthe platform for further consultation. The second cor -\npus, ChinaRe, contains 86,663 patient records catego -\nrized into 44 disease categories. Each document contains \nthe symptom description of a patient written by a health \ninsurance professional in ChinaRe, which is one of the \nlargest reinsurance groups in China. The diagnoses were \ndetermined by a clinician and sent to the insurance com -\npany. In both corpora, each document corresponds to \na unique patient and only has one disease label. Table  1 \nsummarizes basic statistics of the two corpora. Jieba \npackage was used for Chinese word segmentation [39].\nProblem formulation\nThe task of patient diagnosis can be formulated as a text \nclassification problem: to assign a disease label given the \nnarrative description of a patient’s symptoms. Accurate \ndisease diagnosis is an important task towards computer-\nassisted patient triage and risk stratification. We aim to \nPage 4 of 10Li et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \nstudy the performance of different classification models \n(especially comparing BERT to other models) when pro -\nvided an increasing amount of training data.\nCompared algorithms\nIn this section, we describe classification models we \ninclude in comparison. We include text classification \nmodels that use one-hot word representations, dis -\ntributed word representations, and contextual word \nrepresentations. Since our main goal here is to study \nthe behavior of classifiers when the training data size \nincreases from small to large, we do not consider classi -\nfication techniques intended for small data sizes only, e.g. \none-shot learning or few-shot learning classifiers.\nClassifiers using one‑hot word representations\nWe first consider the most common baseline of text clas -\nsification—a linear classifier using bag-of-words features \n(and its variants). Although simple, such a model offers \ntwo advantages in handling small training data. First, \na regularized, sparse linear classifier does not overfit as \neasily as complex models, therefore delivering stable \nperformance. Second, the simple model allows relatively \nstraightforward ways of incorporating prior knowledge \ninto its feature representation.\nBOW\nThis is a support vector machine classifier using TFIDF-\nweighted bag-of-words (BOW) features and linear ker -\nnel, trained with L2 regularization.\nBOW_EXP\nThis model enhances the feature representation of BOW \nwith feature selection and synonym expansion tech -\nniques. The basic idea is to emphasize class-indicative \nfeatures in a document if that document contains such a \nfeature or its synonyms. It takes the following steps:\n• A feature selection algorithm is used to rank the rel -\nevance of each unigram feature in the classification \ntask.\n• Each unigram feature w is associated with a class c if c \nhas the largest p(c|w ) in training data. For each class, \nwe select k highest ranking features according to the \nfeature selection metric. The union of all selected fea-\ntures are denoted as F.\n• For each word u in a document d , we compute its \nvector similarity to the vector of each w ∈ F in a word \nembedding space. If cosine similarity cos\n(\n�u,�w ≥ t\n)\n , \nwe increment the count of w ∈ d by 1 before com -\nputing the TFIDF transformation. The step concep -\ntually adds a new word w into d.\nThe above algorithm is a hybrid of feature selection \nand feature expansion [40]. Instead of discarding unse -\nlected features (which may still be useful), it increases \nthe weights of selected features in each document. The \nmethod is inspired by the distributional prototype fea -\ntures proposed by [41] and later applied in clinical NLP \n[42].\nBOW_EXP_KG\nThis model refines BOW_EXP by using knowledge graph \n(KG)-enhanced word vectors. A knowledge graph can be \nviewed as a semantic network, where entities (words and \nphrases) are nodes and relations between concepts are \nedges. We employ the LINE network embedding algo -\nrithm to learn low-dimensional word vectors that pre -\nserve knowledge in the semantic network [43].\nClassifiers using distributed word representations\nWe consider another group of text classification models \nthat represent words as distributed semantic vectors [44]. \nThese word vectors can be learned from scratch using \nthe data of current task, or initialized with word vec -\ntors learned on related tasks to transfer semantic knowl -\nedge. Here we consider two representative models using \ndistributed word vectors: the continuous bag-of-words \nmodel and long short-term memory networks.\nCBOW\nThis is a linear-kernel support vector machine classifier \nthat represents a document as the average of its words’ \nvectors. It is also known as continuous bag-of-words \n(CBOW) [45], as conventional bag-of-words representa -\ntion can be viewed as an average of one-hot word vectors. \nThe word vectors are the same as in BOW_EXP and fixed \nin the training process.\nCBOW_KG\nThis model refines CBOW by using KG-enhanced word \nvectors as used in BOW_EXP_KG.\nLSTM\nThis classifier uses unidirectional long short-term mem -\nory networks (LSTM) to process the document as a word \nTable 1 Corpora statistics\nHaoDaiFu ChinaRe\n# of documents 51,374 86,663\n# of diseases 805 44\n# of rare diseases 89 5\nVocabulary size 59,879 41,087\nAverage # of words/doc 27 30\nPage 5 of 10\nLi et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \n \nsequence. The model’s word embedding layer is initial -\nized with the same word vector as in BOW_EXP and \nfine-tuned in the training process.\nLSTM_KG\nThis model refines LSTM by initializing the word embed-\nding layer with KG-enhanced word vectors as used in \nBOW_EXP_KG and CBOW_KG. The word embedding \nlayer is fine-tuned in the training process.\nClassifier using contextual word representations\nExemplified by BERT (Bidirectional Encoder Representa-\ntions from Transformers [46]), contextual word represen-\ntations encode each word using not only the distributed \nvector of the word itself, but also distributed vectors of \nsurrounding words that have semantic dependencies with \nthe word [47]. BERT extensively uses multi-head atten -\ntion mechanism to represent each word by \"paying atten-\ntion to’’ all other words in the same context (sentence or \ndocument). Instead of processing tokens sequentially as \nin LSTM, BERT’s multi-head attention can process all \ntokens in parallel. This mitigates the gradient vanish -\ning problem when capturing long-range dependencies \nbetween words. As a result, BERT can efficiently model \nthe dependencies between labels and words as well as \namong words themselves.\nBERT\nWe configure a Chinese BERT-base model released by \nGoogle2 to perform multiclass classification tasks. Since \nthe primary goal of this study is to compare BERT with \nother non-BERT classification models on small train -\ning data, it suffices to use a BERT model pretrained on \ngeneral domain texts. We leave the study that compares \nBERT models fine-tuned on Chinese clinical texts [21, \n22] for future work.\nThis sentence has two reference citations [1, 2].\nMore text of an additional paragraph, with a figure ref -\nerence (Fig. 1) and a figure inside a Word text box below. \nFigures need to be placed as close to the corresponding \ntext as possible and not extend beyond one page.\nImplementation details\nThe support vector machine classifier (SVM) was imple -\nmented using Python scikit-learn package. To determine \nthe best regularization strength C for SVM models, we \nperformed grid search over {0.001, 0.01, 0.1, 1, 10, 100} \non a development set. We set  C = 1 as it consistently \ndelivered the best result (performance metric discussed \nbelow).\nWe explored various feature selection algorithms used \nin BOW_EXP and BOW_EXP_KG. These include chi-\nsquare χ2 , information gain, and bi-normal separation \n[48] in our pilot study. We selected the χ2 method as it \ndelivers the best performance on development set. We \nselect k = 2 features for each class.\nIn BOW_EXP and BOW_EXP_KG, the threshold of \ncosine similarity was set to t = 0.9 after searching over \n{0.7, 0.8, 0.9} on development set.\nWe used 256-dimensional word vectors pretrained on \na large-scale Chinese text corpus [49] in BOW_EXP_KG \nand CBOW_KG.\nTo learn KG-enhanced word vectors, we derive a \nsemantic network from a general Chinese knowledge \ngraph, CN-DBpedia [50]. It contains 16.8 million entities \nand 223 million relations and is publicly available.3\nWe used the LINE network embedding algorithm to \nfine-tune word vectors using the massive semantic net -\nwork above. It was configured to learn from secondary-\norder proximity. We performed grid search for LINE’s \nhyperparameters on a development set. These include \n(the best setting is underlined): negative edge sampling \nrate {5, 10, 50, 100}, batch size {128, 256, 512, 1024, 2048}, \nand number of batches {50 K, 100 K, 150 K, 200 K, 250 K, \n300 K}.\nWe used tensorflow/keras to implement deep sequence \nlearning models, including LSTM, LSTM_KG, and BERT. \nFor LSTM models, we used the recommended Adam \noptimizer and default learning rate ( 10−3 ). We set the \nnumber of training epochs such that the loss on valida -\ntion set stops decreasing. For BERT, we also used the \nrecommended Adam optimizer and default learning rate \ndecaying schedule. The number of training epochs was \nset to 40 using the same procedure as LSTM models. A \ndocument is padded (truncated) if it is shorter (longer) \nthan the maximum sequence length supported by BERT-\nbase (512 words).\nEvaluation methodology\nPerformance metric\nViewing the classification of each individual disease \n(class) as a binary classification problem, results can be \ndivided into True Positive (TP), True Negative (TN), \nFalse Positive (FP), and False Negative (FN). Recall  \nmeasures the percentage of TPs among all documents \nthat truly mention that disease; precision measures the \npercentage of TPs among all documents predicted to \nmention that disease. F1 score is the harmonic mean of \nprecision and recall, a metric that balances the two [51]. \nTo measure the classification performance of a set of \n2 https:// github. com/ google- resea rch/ bert. 3 http:// kw. fudan. edu. cn/ cndbp edia/ downl oad/.\nPage 6 of 10Li et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \ndiseases, we use macro-averaged F1 . Formally, the met -\nrics are calculated as follow\nwhere D is the set of diseases (classes), and F 1,i is the F1 \nscore of the i-th disease.\nTrain‑test split\nTo reduce the variance of results due to a random train-\ntest split, we average the results of 10 runs. In each run, \nwe randomly split the corpus into 80% for training and \n20% for test. To avoid the case where some classes do not \nappear in training or test set, the random split is applied \non a per-class basis.\nLearning curve\nThe results of evaluation metrics we mentioned above \nare displayed in plots of learning curves. Learning curves \nrepresent the generalization performance of the models \nproduced by a learning algorithm, as a function of the \nsize of the training set. In a plot of learning curve, x-axis \nrepresents the size of training set, y-axis represents the \nperformance of model under an evaluation metric. In our \nstudy, we sample training sets from total training exam -\nples in fixed proportions: [10%, 20%, 30%, 40%, 50%, 60%, \n70%, 80%, 90%, 100%].\n(1)F1 = 2×recall×precision\nrecall+precision = 2×TP\n2×TP +FP +FN ,\n(2)macro − averagedF1 = 1\n|D |\n|D |∑\ni=1\nF1,i,\nWe use Area Under Learning Curve (ALC) to summa -\nrize the learning progress of each model. The ALC met -\nric is useful in comparing different learning algorithms \nespecially when labeling budget is limited, as in the active \nlearning setting [52]. A higher ALC means an overall \nhigher performance across different training data sizes.\nExperimental evaluation\nThe learning curves of different algorithms on HaoDaiFu \nand ChinaRe corpora are in Fig. 2, with their correspond-\ning ALC metrics reported in Table  2. On both corpora, \nBOW_EXP , BOW_EXP_KG, and BERT significantly \nFig. 2 Learning curves of compared algorithms averaged across all diseases in the two corpora\nTable 2 Area under learning curve (ALC) for different methods \naggregated over all diseases\nFigure 2 plots the learning curves\na Result significantly higher than BOW\nb Result significantly higher than BOW_EXP_KG. (Fisher’s randomization test, \nsignificance level α = 0.05)\nMethod HaoDaiFu (all 805 \ndiseases)\nChinaRe (all \n44 diseases)\nBOW 0.4158 0.8534\nBOW_EXP 0.4266a 0.8934a\nBOW_EXP_KG 0.4254a 0.8940a\nCBOW 0.2097 0.5817\nCBOW_KG 0.2064 0.5714\nLSTM 0.2013 0.6064\nLSTM_KG 0.0377 0.6243\nBERT 0.5020ab 0.9551ab\nPage 7 of 10\nLi et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \n \noutperformed the BOW baseline, and BERT significantly \noutperformed the BOW_EXP_KG method.\nTo further study the behavior of different algorithms \nwhen training data are extremely few, we plot the learn -\ning curves on statistically rare diseases that account for \nno more than 0.02% (2 in 10,000) of records in each cor -\npus. There are 89 such diseases in HaoDaiFu and 5 in \nChinaRe. In both cases, these extremely rare diseases \nhave on average about 10 training documents. This trans-\nlates to one training document per disease at 10% training \ndata rate, representing the cases of extreme data scarcity. \nThe corresponding learning curves are in Fig.  3, ALC \nmetrics reported in Table  3. On HaoDaiFu, BOW_EXP , \nBOW_EXP_KG, and BERT significantly outperformed \nthe BOW baseline on all diseases, and BERT significantly \noutperformed the BOW_EXP_KG method on extremely \nrare diseases. Since the number of rare diseases in Chin -\naRe is too few, the above performance comparisons did \nnot show significant differences.\nNote that the classification performance on Hao -\nDaiFu is overall lower than that on ChinaRe. In general, \nmulti-class classification problem is difficult with a large \nnumber of classes. Here the HaoDaiFu corpus contains \none order of magnitude more classes than ChinaRe (see \nTable 1), bringing substantial challenge to all methods.\nResults\nLearning curves of different algorithms on HaoDaiFu and \nChinaRe corpora are in Fig.  2, with their correspond -\ning ALC metrics reported in Table  2. On both corpora, \nBOW_EXP , BOW_EXP_KG, and BERT significantly out-\nperformed the BOW baseline, and BERT significantly \noutperformed the BOW_EXP_KG method.\nTo further study the behavior of different algorithms \nwhen training data are extremely few, we plot the learn -\ning curves on statistically rare diseases that account for \nno more than 0.02% (2 in 10,000) of records in each cor -\npus. There are 89 such diseases in HaoDaiFu and 5 in \nChinaRe. In both cases, these extremely rare diseases \nhave on average about 10 training documents. This trans-\nlates to one training document per disease at 10% training \nFig. 3 Learning curves of compared algorithms averaged across very rare diseases (prevalence ≤ 0.02%) in the two corpora\nTable 3 Area under learning curve (ALC) for different methods \naggregated over extremely rare ( prevalence ≤ 0.02)\nFigure 3 plots the learning curves\na Result significantly higher than BOW\nb Result significantly higher than BOW_EXP_KG. (Fisher’s randomization test, \nsignificance level α = 0.05)\nMethod HaoDaiFu (89 rare \ndiseases)\nChinaRe \n(5 rare \ndiseases)\nBOW 0.3044 0.8454\nBOW_EXP 0.3056a 0.9058\nBOW_EXP_KG 0.3115a 0.9034\nCBOW 0.1215 0.1945\nCBOW_KG 0.1153 0.2136\nLSTM 0 0\nLSTM_KG 0 0\nBERT 0.  3795ab 0.9028\nPage 8 of 10Li et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \ndata rate, representing the cases of extreme data scarcity. \nThe corresponding learning curves are in Fig.  3, ALC \nmetrics reported in Table  3. On HaoDaiFu, BOW_EXP , \nBOW_EXP_KG, and BERT significantly outperformed \nthe BOW baseline on all diseases, and BERT significantly \noutperformed the BOW_EXP_KG method on extremely \nrare diseases. Since the number of rare diseases in Chin -\naRe is too few, the above performance comparisons did \nnot show significant differences.\nNote that the classification performance on Hao -\nDaiFu is overall lower than that on ChinaRe. In general, \nmulti-class classification problem is difficult with a large \nnumber of classes. Here the HaoDaiFu corpus contains \none order of magnitude more classes than ChinaRe (c.f. \nTable 1), bringing substantial challenge to all methods.\nDiscussion\nThe area under BERT’s learning curve is the largest when \naggregated across all diseases when aggregated across all \ndiseases. With a fraction of all training data (30% on Hao-\ndaifu, and 40% on ChinaRe), BERT is able to outperform \nall other approaches trained on 100% training data. These \nresults show that BERT not only can deliver the best per -\nformance but also requires less data for training com -\npared to other methods. The outstanding performance \npartly comes from Transformer’s multi-head attention \nmechanism, which allows BERT to learn long-distance \ndependency much more efficiently than previous deep \nsequence models. It is also partly due to the unique pre -\ntraining objective, which can incorporate the sequence \ninformation of text in two directions efficiently.\nBOW gives a decent baseline performance. Its vari -\nants, BOW_EXP and BOW_EXP_KG, give consistent \nperformance improvements. Supervised feature selection \nand synonym expansion effectively improve the feature \nrepresentation of BOW baseline. BOW_EXP_KG only \ngives slightly higher performance than BOW_EXP . This \nindicates that semantic relation information in a knowl -\nedge graph is already largely captured by pretrained word \nvectors.\nCBOW performs worse than BOW. Similar result was \nobserved in [53]. Indeed, linear SVM aims to find hyper -\nplanes in the feature space to separate classes. It is eas -\nier to achieve linear separation in the high dimensional \nsparse feature space (BOW) than in the low dimensional \ndense feature space (CBOW).\nThe performance of LSTM on Haodaifu is extremely \nlow, but is not that bad on ChinaRe, and goes up sharply \nwhen training data increases from 10 to 40%. This huge \ndifference reflects the model’s requirement for a large \nquantity of training data. On average, there are 51 train -\ning documents per disease in Haodaifu, while 1575 \ntraining documents per disease in ChinaRe. Because of \nthe vanishing gradient problem, training LSTM models \nbecomes extremely difficult when training data size is \nsmall and documents are relatively long. Adding prior \nknowledge through word embedding (LSTM_KG) has \nonly limited benefit.\nOn the extremely rare diseases (when there is only 1 \ntraining document per disease), BERT is outperformed \nby BOW_EXP and BOW_EXP_KG. This happened \non both Haodaifu and ChinaRe. The result shows that \nin situations where training data is extremely scarce, the \ntraditional non-deep model with an appropriate feature \nconstruction strategy is able to compete with the current \nstate-of-the-art deep models.\nOn rare diseases, the catastrophically low performance \nof LSTM models is not unexpected given its poor per -\nformance on all diseases. Again, the result suggests that \na large amount of training data is needed to train LSTM \nmodels, even though its word embedding layer has been \npretrained.\nImplication\nMedical domain has accumulated a wealth of knowl -\nedge bases, in the form of standardized terminolo -\ngies, research publications, clinical practice guidelines, \nand consumer-facing information portals. While these \nforms of knowledge can be easily used by humans, they \ncannot be directly used by machine learning models. \nThis is because the internal representation of knowl -\nedge in machine learning models is fundamentally dif -\nferent from that of human knowledge. The primary way \nof transferring knowledge into these models is through \nwell-formulated prediction tasks expressed in the form \nof labeled examples. However, labeling cost is high in the \nmedical domain, necessitating machine learning models \nto leverage medical domain knowledge. Over the years, \nresearchers have been proposing various approaches for \ninstilling external knowledge into machine learning mod-\nels, including carefully designed features, model archi -\ntectures, auxiliary learning objectives [9, 10], weak labels \nand distant supervision obtained from medical knowl -\nedge bases [8, 11, 12], pretrained model parameters [17, \n18, 22], and combinations of these approaches.\nOur study here shows that pretrained BERT models \n(and the broader family of pretrained deep Transformers) \nmay offer an effective way of leveraging external knowl -\nedge learned from large-scale unlabeled data towards \nspecific NLP tasks. Even a BERT model pretrained on \ngeneral corpus is able to effectively help NLP tasks in the \nhealth domain. On the one hand, this is good news to the \nhealth NLP research community, as it can potentially free \nresearchers from feature engineering when the training \ndata is small and the labeling cost is high. Instead, the \nmodel can be continuously improved by pretraining on \nPage 9 of 10\nLi et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \n \ndomain-specific and task-specific corpora [17, 18, 54]. \nOn the other hand, these black-box models are difficult \nto interpret, therefore more research is needed to under -\nstand their vulnerabilities especially within the medical \ncontext, such as potential biases in learned representa -\ntions [55].\nConclusion\nIn this paper, we study whether BERT is still effective \nwhen it is fine-tuned with small training data. To answer \nthis question, we conducted a learning curve analysis \nof BERT and other baseline models in text-based dis -\nease classification tasks. The analysis showed that BERT \nremains the highest performing model even when each \nclass has only a handful of training documents, and its \nperformance improves the fastest when given more train-\ning documents. Simple linear classifiers using specially \nengineered bag-of-words features delivers stable and \ncompetitive performance, and it outperformed BERT \nwhen training documents are extremely few (one or two \nper class). Overall, the study shows that even though \nBERT is a massively complex model, it only takes very \nsmall (but not extremely small) training data to fine-\ntune a pretrained BERT model to outperform baseline \napproaches using the same data.\nAbbreviations\nNLP: Natural language processing; BERT: Bidirectional encoder representations \nfrom transformers; KG: Knowledge graph; UMLS: Unified medical language \nsystem; BOW: Bag-of-words; CBOW: Continuous bag-of-words; LSTM: Long \nshort-term memory.\nAcknowledgements\nThe authors would like to thank the anonymous reviewers of the HealthNLP \nworkshop for their valuable feedback.\nAbout thisSupplement\nThis article has been published as part of BMC Medical Informatics and \nDecision Making Volume 21 Supplement 9 2021:Health Natural Language \nProcessing and Applications. The full contents of the supplement are available \nat https:// bmcme dinfo rmdec ismak. biome dcent ral. com/ artic les/ suppl ements/ \nvolume- 21- suppl ement-9.\nAuthor contributions\nAll authors read and approved the final manuscript. XL preprocessed the data, \ndesigned and implemented different algorithms, and drafted the manuscript. \nWY provided the data and edited the manuscript. DP edited the manuscript. \nQM conceptualized the project edited the manuscript. YW conceived the \nstudy, designed the evaluation methodology, and edited the manuscript. All \nauthors read and approved the final manuscript.\nFunding\nThis work and the publication cost of this article was funded in part by \nthe National Science Foundation under Grant Number 1633370 and by \nthe National Library of Medicine under Grant Number 2R01LM010681-05. \nXuedong Li was visiting the University of Michigan under the support of \nthe China Scholarship Council. Yue Wang was under the support of Kilgour \nResearch Grant Award by UNC SILS. The funding agencies were not involved \nin the design of the study, or collection, analysis, and interpretation of \ndata, or writing the manuscript. Any opinions, findings, and conclusion or \nrecommendations in this article are those of the authors and do not necessar-\nily reflect the view of the funding agencies.\nAvailability of data and materials\nThe ChinaRe dataset in this paper is proprietary. The HaoDaiFu dataset is \npublicly available at https:// github. com/ bruce li518/ HaoDa iFu.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 College of Computer Science, Sichuan University, Chengdu, China. 2 MobLab \nInc., Pasadena, CA, USA. 3 School of Information, University of Michigan, Ann \nArbor, MI, USA. 4 School of Information and Library Science, University of North \nCarolina at Chapel Hill, Chapel Hill, NC, USA. \nReceived: 6 March 2022   Accepted: 22 March 2022\nReferences\n 1. Amazon Web Services. Amazon SageMaker Ground Truth pricing. https:// \naws. amazon. com/ sagem aker/ groun dtrut. Accessed July 2020.\n 2. Stubbs A, Uzuner Ö. Annotating longitudinal clinical narratives for \nde-identification: the 2014 i2b2/UTHealth corpus. J Biomed Inform. \n2015;58:S20–9.\n 3. United States Department of Health and Human Services. National \nOrganization for Rare Disorders (NORD); Last Updated June 23, 2020. \nhttps:// www. nidcd. nih. gov/ direc tory/ natio nal- organ izati on- rare- disor \nders- nord. Accessed 23 June 2020.\n 4. European Commission. Rare Diseases. https:// ec. europa. eu/ health/ non- \ncommu nicab le- disea ses/ steer ing- group/ rare- disea ses_ en. Accessed 16 \nJuly 2020.\n 5. Spasic I, Nenadic G. Clinical text data in machine learning: systematic \nreview. JMIR Med Inform. 2020;8(3):e17984.\n 6. Wilcox AB, Hripcsak G. The role of domain knowledge in automat-\ning medical text report classification. J Am Med Inform Assoc. \n2003;10(4):330–8.\n 7. Demner-Fushman D, Mork JG, Shooshan SE, Aronson AR. UMLS content \nviews appropriate for NLP processing of the biomedical literature vs. clini-\ncal text. J Biomed Inform. 2010;43(4):587–94.\n 8. Dissanayake PI, Colicchio TK, Cimino JJ. Using clinical reasoning ontolo-\ngies to make smarter clinical decision support systems: a systematic \nreview and data synthesis. J Am Med Inform Assoc. 2020;27(1):159–74.\n 9. Zhang E, Thurier Q, Boyle L. Improving clinical named-entity recognition \nwith transfer learning. Stud Health Technol Inform. 2018;252:182–7.\n 10. Crichton G, Pyysalo S, Chiu B, Korhonen A. A neural network multi-task \nlearning approach to biomedical named entity recognition. BMC Bioin-\nform. 2017;18(1):368.\n 11. Wang Y, Sohn S, Liu S, Shen F, Wang L, Atkinson EJ, et al. A clinical text \nclassification paradigm using weak supervision and deep representation. \nBMC Med Inform Decis Mak. 2019;19(1):1–13.\n 12. Pattisapu N, Anand V, Patil S, Palshikar G, Varma V. Distant supervision for \nmedical concept normalization. J Biomed Inform. 2020;109:103522.\n 13. Figueroa RL, Zeng-Treitler Q, Ngo LH, Goryachev S, Wiechmann EP . Active \nlearning for clinical text classification: is it better than random sampling? \nJ Am Med Inform Assoc. 2012;19(5):809–16.\n 14. Devlin J, Chang MW, Lee K, Toutanova K. Bert: pre-training of deep \nbidirectional transformers for language understanding. arXiv: 1810. 04805. \n2018.\nPage 10 of 10Li et al. BMC Medical Informatics and Decision Making          (2021) 21:377 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 15. Adhikari A, Ram A, Tang R, Lin J. Docbert: Bert for document classification. \narXiv: 1904. 08398. 2019.\n 16. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman SR. Glue: a multi-task \nbenchmark and analysis platform for natural language understanding. \narXiv: 1804. 07461. 2018.\n 17. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained \nbiomedical language representation model for biomedical text mining. \nBioinformatics. 2020;36(4):1234–40.\n 18. Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T, et al. Pub-\nlicly available clinical BERT embeddings. arXiv: 1904. 03323. 2019.\n 19. Peng Y, Yan S, Lu Z. Transfer learning in biomedical natural language pro-\ncessing: an evaluation of bert and elmo on ten benchmarking datasets. \narXiv: 1906. 05474. 2019.\n 20. Yao L, Jin Z, Mao C, Zhang Y, Luo Y. Traditional Chinese medicine clinical \nrecords classification with BERT and domain specific corpora. J Am Med \nInform Assoc. 2019;26(12):1632–6.\n 21. Li X, Zhang H, Zhou XH. Chinese clinical named entity recognition with \nvariant neural structures based on BERT methods. J Biomed Inform. \n2020;107:103422.\n 22. Zhang N, Jia Q, Yin K, Dong L, Gao F, Hua N. Conceptualized representa-\ntion learning for chinese biomedical text mining. arXiv: 2008. 10813. 2020.\n 23. Hestness J, Narang S, Ardalani N, Diamos G, Jun H, Kianinejad H, et al. \nDeep learning scaling is predictable, empirically. arXiv: 1712. 00409. 2017.\n 24. Stanfill MH, Williams M, Fenton SH, Jenders RA, Hersh WR. A systematic \nliterature review of automated clinical coding and classification systems. J \nAm Med Inform Assoc. 2010;17(6):646–51.\n 25. Uzuner Ö, Goldstein I, Luo Y, Kohane I. Identifying patient smok-\ning status from medical discharge records. J Am Med Inform Assoc. \n2008;15(1):14–24.\n 26. Uzuner Ö. Recognizing obesity and comorbidities in sparse data. J Am \nMed Inform Assoc. 2009;16(4):561–70.\n 27. Yao L, Mao C, Luo Y. Clinical text classification with rule-based features \nand knowledge-guided convolutional neural networks. BMC Med Inform \nDecis Mak. 2019;19(3):71.\n 28. Huh J, Yetisgen-Yildiz M, Pratt W. Text classification for assisting modera-\ntors in online health communities. J Biomed Inform. 2013;46(6):998–1005.\n 29. Edara DC, Vanukuri LP , Sistla V, Kolli VKK. Sentiment analysis and text \ncategorization of cancer medical records with LSTM. J Ambient Intell \nHumaniz Comput. 2019. https:// doi. org/ 10. 1007/ s12652- 019- 01399-8.\n 30. Botsis T, Nguyen MD, Woo EJ, Markatou M, Ball R. Text mining for the \nvaccine adverse event reporting system: medical text classification using \ninformative feature selection. J Am Med Inform Assoc. 2011;18(5):631–8.\n 31. Zhong J, Yi X, Xuan D, Xie Y. Categorization of patient diseases for chinese \nelectronic health record analysis: a case study. In: Perner P , editor. Indus-\ntrial conference on data mining. Cham: Springer; 2018. p. 162–72.\n 32. Friedman C, Kra P , Rzhetsky A. Two biomedical sublanguages: a \ndescription based on the theories of Zellig Harris. J Biomed Inform. \n2002;35(4):222–35.\n 33. Wu S, Roberts K, Datta S, Du J, Ji Z, Si Y, et al. Deep learning in clinical \nnatural language processing: a methodical review. J Am Med Inform \nAssoc. 2020;27(3):457–70.\n 34. Garla VN, Brandt C. Ontology-guided feature engineering for clinical text \nclassification. J Biomed Inform. 2012;45(5):992–8.\n 35. Li X, Wang Y, Wang D, Yuan W, Peng D, Mei Q. Improving rare disease clas-\nsification using imperfect knowledge graph. BMC Med Inform Decis Mak. \n2019;19(5):238.\n 36. Choi E, Bahadori MT, Song L, Stewart WF, Sun J. GRAM: graph-based \nattention model for healthcare representation learning. In: Proceedings \nof the 23rd ACM SIGKDD international conference on knowledge discov-\nery and data mining; 2017. p. 787–95.\n 37. Zhang Z, Han X, Liu Z, Jiang X, Sun M, Liu Q. ERNIE: enhanced language \nrepresentation with informative entities. arXiv: 1905. 07129. 2019.\n 38. Liu W, Zhou P , Zhao Z, Wang Z, Ju Q, Deng H, et al. K-bert: enabling \nlanguage representation with knowledge graph. arXiv: 1909. 07606. 2019.\n 39. Jieba Chinese text segmentation. https:// github. com/ fxsjy/ jieba. \nAccessed 26 Mar 2019.\n 40. Gabrilovich E, Markovitch S. Feature generation for text categorization \nusing world knowledge. IJCAI. 2005;5:1048–53.\n 41. Guo J, Che W, Wang H, Liu T. Revisiting embedding features for simple \nsemi-supervised learning. In: Proceedings of the 2014 conference on \nempirical methods in natural language processing (EMNLP); 2014. p. \n110–20.\n 42. Wu Y, Xu J, Jiang M, Zhang Y, Xu H. A study of neural word embeddings \nfor named entity recognition in clinical text. In: AMIA annual symposium \nproceedings, vol. 2015. American Medical Informatics Association; 2015. \np. 1326.\n 43. Tang J, Qu M, Wang M, Zhang M, Yan J, Mei Q. Line: large-scale informa-\ntion network embedding. In: Proceedings of the 24th international \nconference on world wide web; 2015. p. 1067–77.\n 44. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed represen-\ntations of words and phrases and their compositionality. Adv Neural Inf \nProcess Syst. 2013;26:3111–9.\n 45. Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word repre-\nsentations in vector space. arXiv: 1301. 3781. 2013.\n 46. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. \nAttention is all you need. Adv Neural Inf Process Syst. 2017;30:5998–6008.\n 47. Smith NA. Contextual word representations: a contextual introduction. \narXiv: 1902. 06006. 2019.\n 48. Forman G. An extensive empirical study of feature selection metrics for \ntext classification. J Mach Learn Res. 2003;3(Mar):1289–305.\n 49. Su J. Pretrained Word2Vector. https:// kexue. fm/ archi ves/ 4304. Accessed \n03 Apr 2017.\n 50. Xu B, Xu Y, Liang J, Xie C, Liang B, Cui W, et al. CN-DBpedia: a never-ending \nChinese knowledge extraction system. In: International conference on \nindustrial, engineering and other applications of applied intelligent \nsystems. Springer; 2017. p. 428–38.\n 51. Wikipedia. F1 Score. https:// en. wikip edia. org/ wiki/ F1_ score. Accessed 26 \nMar 2019.\n 52. Guyon I, Cawley GC, Dror G, Lemaire V. Results of the active learning chal-\nlenge. In: Active learning and experimental design workshop in conjunc-\ntion with AISTATS 2010; 2011. p. 19–45.\n 53. Lilleberg J, Zhu Y, Zhang Y. Support vector machines and word2vec for \ntext classification with semantic features. In: 2015 IEEE 14th international \nconference on cognitive informatics & cognitive computing (ICCI*CC). \nIEEE; 2015. p. 136–40.\n 54. Gururangan S, Marasovic A, Swayamdipta S, Lo K, Beltagy I, Downey D, \net al. Don’t stop pretraining: adapt language models to domains and \ntasks. In: Proceedings of the 58th annual meeting of the association for \ncomputational linguistics. Online: Association for Computational Linguis-\ntics; 2020. p. 8342–60. https:// aclan tholo gy. org/ 2020. acl- main. 740/.\n 55. Zhang H, Lu AX, Abdalla M, et al. Hurtful words: quantifying biases in clini-\ncal contextual word embeddings. In: Proceedings of the ACM conference \non health, inference, and learning. 2020. p. 110–20.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}