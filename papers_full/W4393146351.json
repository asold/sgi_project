{
  "title": "Adventures of Trustworthy Vision-Language Models: A Survey",
  "url": "https://openalex.org/W4393146351",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2065109070",
      "name": "Mayank Vatsa",
      "affiliations": [
        "Indian Institute of Technology Jodhpur"
      ]
    },
    {
      "id": "https://openalex.org/A3203900648",
      "name": "Anubhooti Jain",
      "affiliations": [
        "Indian Institute of Technology Jodhpur"
      ]
    },
    {
      "id": "https://openalex.org/A2104800089",
      "name": "RICHA SINGH",
      "affiliations": [
        "Indian Institute of Technology Jodhpur"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6777047548",
    "https://openalex.org/W4226143462",
    "https://openalex.org/W4205731523",
    "https://openalex.org/W4365441136",
    "https://openalex.org/W2969280433",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W4317838072",
    "https://openalex.org/W3024464497",
    "https://openalex.org/W3159344844",
    "https://openalex.org/W4281662926",
    "https://openalex.org/W2804411656",
    "https://openalex.org/W4386076051",
    "https://openalex.org/W3115502042",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6810527279",
    "https://openalex.org/W6853605804",
    "https://openalex.org/W4362706540",
    "https://openalex.org/W2963349562",
    "https://openalex.org/W3172798424",
    "https://openalex.org/W4280532631",
    "https://openalex.org/W4224882757",
    "https://openalex.org/W6764072591",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3171774521",
    "https://openalex.org/W4386065396",
    "https://openalex.org/W4226392702",
    "https://openalex.org/W6804154756",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4311642344",
    "https://openalex.org/W6795975431",
    "https://openalex.org/W4387947457",
    "https://openalex.org/W4365601074",
    "https://openalex.org/W3174402370",
    "https://openalex.org/W4288055779",
    "https://openalex.org/W4327487294",
    "https://openalex.org/W3008019634",
    "https://openalex.org/W4280514187",
    "https://openalex.org/W4225592884",
    "https://openalex.org/W4386081728",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W6846279210",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3005265709",
    "https://openalex.org/W6810423370",
    "https://openalex.org/W3154222058",
    "https://openalex.org/W4320165581",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W4320561709",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3196621661",
    "https://openalex.org/W4283831096",
    "https://openalex.org/W4307206211",
    "https://openalex.org/W3172872502",
    "https://openalex.org/W4378718229",
    "https://openalex.org/W4404752324",
    "https://openalex.org/W2975706270",
    "https://openalex.org/W4312957757",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4230405732",
    "https://openalex.org/W4308245819",
    "https://openalex.org/W3106784008",
    "https://openalex.org/W4385801041",
    "https://openalex.org/W4322746848",
    "https://openalex.org/W4283810228",
    "https://openalex.org/W4304086863",
    "https://openalex.org/W4283795227",
    "https://openalex.org/W4306672439",
    "https://openalex.org/W4226146561",
    "https://openalex.org/W4286336838",
    "https://openalex.org/W4288007632",
    "https://openalex.org/W4312300614",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W3177487519",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W4225871896",
    "https://openalex.org/W4383868706",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W4312791030",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W4390190425",
    "https://openalex.org/W4310698795",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4386065753",
    "https://openalex.org/W3174366544",
    "https://openalex.org/W4386065345",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4306705777",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3203737321",
    "https://openalex.org/W3214243459",
    "https://openalex.org/W4310428279",
    "https://openalex.org/W3131151401",
    "https://openalex.org/W4313164293",
    "https://openalex.org/W3037702798",
    "https://openalex.org/W3113067643",
    "https://openalex.org/W3172141633",
    "https://openalex.org/W4312423415",
    "https://openalex.org/W2951025380"
  ],
  "abstract": "Recently, transformers have become incredibly popular in computer vision and vision-language tasks. This notable rise in their usage can be primarily attributed to the capabilities offered by attention mechanisms and the outstanding ability of transformers to adapt and apply themselves to a variety of tasks and domains. Their versatility and state-of-the-art performance have established them as indispensable tools for a wide array of applications. However, in the constantly changing landscape of machine learning, the assurance of the trustworthiness of transformers holds utmost importance. This paper conducts a thorough examination of vision-language transformers, employing three fundamental principles of responsible AI: Bias, Robustness, and Interpretability. The primary objective of this paper is to delve into the intricacies and complexities associated with the practical use of transformers, with the overarching goal of advancing our comprehension of how to enhance their reliability and accountability.",
  "full_text": "Adventures of Trustworthy Vision-Language Models: A Survey\nMayank Vatsa, Anubhooti Jain, Richa Singh\nIIT Jodhpur, India\nmvatsa@iitj.ac.in, jain.44@iitj.ac.in, richa@iitj.ac.in\nAbstract\nRecently, transformers have become incredibly popular in\ncomputer vision and vision-language tasks. This notable rise\nin their usage can be primarily attributed to the capabilities\noffered by attention mechanisms and the outstanding ability\nof transformers to adapt and apply themselves to a variety of\ntasks and domains. Their versatility and state-of-the-art per-\nformance have established them as indispensable tools for a\nwide array of applications. However, in the constantly chang-\ning landscape of machine learning, the assurance of the trust-\nworthiness of transformers holds utmost importance. This\npaper conducts a thorough examination of vision-language\ntransformers, employing three fundamental principles of re-\nsponsible AI: Bias, Robustness, and Interpretability. The pri-\nmary objective of this paper is to delve into the intricacies\nand complexities associated with the practical use of trans-\nformers, with the overarching goal of advancing our compre-\nhension of how to enhance their reliability and accountability.\nIntroduction\nInspired from the performance for language-based tasks\n(Vaswani et al. 2017; Devlin et al. 2019), transformers were\nproposed for vision-based tasks where they process images\nas patch tokens (Dosovitskiy et al. 2021). Even with the\nmodality change the basic architecture remained the same.\nThese architectures were further extended to accommodate\nboth modalities, giving birth to transformer-based vision-\nlanguage models (Figure 1). Their self-attention module\nmakes convolutions unnecessary, with (Park and Kim 2022)\nstating that multi-head self-attention acts as low-pass filters\nwhile convolutions act like high-pass filters. Their impres-\nsive success has been attributed to their ability to model\nlong-range dependencies and having weak inductive biases,\nleading to better generalization. (Long et al. 2022) dis-\ncusses a general architecture for the Vision-Language Pre-\ntrained Models (VLPMs), breaking the architecture into\nfour categories, namely, Vision-Language Raw Input Data,\nVision-Language Representation, Vision-Language Interac-\ntion Model, and Vision-Language Representation. (Long\net al. 2022; Du et al. 2022; Fields and Kennington 2023)\nsurveys VLPMs based on their architecture, pre-training\ntasks and objectives, and downstream tasks, showcasing that\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nText Input: What is \nlying on the grass?\nVLPM\nT T TTT V V VV V\nText Tokens Vision Tokens\nText Input: A [MASK] \nfrisbee on the grass.\nPre-training Token : \ngreen\nCross-Modal \nMasked Vision and \nLanguage Modeling \nClassification Head : \nfrisbee\nText Input\nVisual Question \nAnswering\nFigure 1: An example of vision-language model pre-trained\nusing Cross-Modal Vision-Language Modeling and fine-\ntuned for Visual Question Answering.\nVLPMs continue to grow not only in terms of accuracy but\nsize as well, as the newer models have parameters in bil-\nlions and can perform several tasks with human-like accu-\nracy. As shown in Figure 2, compared to 2018, there has\nbeen a big surge in articles about “vision-language trans-\nformer” in 2022, nearly 9.5 times more, and an even larger\nincrease, nearly 12.5 times more, in 2021. A similar trend\nis seen with the term ‘vision transformer,’ with roughly 15\ntimes more articles in 2022 compared to 2018 and an as-\ntounding approximately 21 times more in 2021. Many of\nthese models are trained on heavy open-web datasets and\nare finetuned for different tasks ranging from classification-\nbased to generative-based.\n(Ross, Katz, and Barbu 2021; Birhane, Prabhu, and Ka-\nhembwe 2021; Srinivasan and Bisk 2022) have shown that\nthese heavy and high-performing models suffer from differ-\nent biases like gender and cultural bias. A detailed review\nof one of the vision-language transformers by (Srinivasan\nand Bisk 2022) depicts gender bias, with purse being the\npreferred term for the female gender while briefcase being\nthe preferred term for the male gender. Just like bias, cases\ncan be made for robustness and interpretability, iterating a\nneed for a proper study of transformer models. Efforts have\nbeen made to study transformers in this light for vision and\nlanguage-based models individually, but collectively, there\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22650\nNumber of Papers\nYear\n2018\n2019\n2020\n2021\n2022\n0 1000 2000 3000 4000\nVis Vis-Lang\nFigure 2: Keyword Analysis for research papers pertaining\nto two keywords, ‘vision-language transformer’ (red) and\n‘vision transformer’ (blue) from 2018 to 2022.\nare only a few studies so far. Hence, we present an exten-\nsive survey of these VLPMs from a dependability and trust\npoint-of-view by curating different practices, methods, and\nmodels proposed for VLPMs, first expanding on bias, fol-\nlowed by robustness, and finally, interpretability. In the end,\nwe also discuss open challenges in the field. With this study,\nwe hope to present the current state of VLPMs regarding\nreliability and highlight some research gaps that can help al-\nleviate the overall state of VLPMs.\nAn Overview of VLPMs\nIn VLPMs, both single and dual architecture models have\nemerged as powerful tools. Here, we present a brief\noverview of these architectures and various pre-training and\ndownstream tasks.\nSingle and Dual Architectures:While VLPMs have their\nown different architectures, they can be broadly categorized\ninto two types of architectures (Figure 3). Single-stream\nmodels fuse both modalities early on with a single trans-\nformer using joint cross-modality like VisualBERT (Li et al.\n2019) and ViLT (Kim, Son, and Kim 2021) transformer\nmodels. Dual-stream models, on the other hand, process the\ntwo modalities separately and are then modeled jointly, like\nViLBERT (Lu et al. 2019) and LXMERT (Tan and Bansal\n2019) models. VLPMs can also be divided on the basis of\nvisual features extracted from the model, like region fea-\ntures, usually pulled from object detectors, used by models\nlike ViLBERT (Lu et al. 2019), grid features used by mod-\nels like Pixel-BERT (Huang et al. 2020), or patch projection\nused by models like ViLT (Kim, Son, and Kim 2021).\nPre-training Tasks: Pre-training has been found to be\nvery beneficial for transformers and, by extension, for\nVLPMs. The models are pre-trained on large datasets to\nsolve different pre-training tasks in a supervised or self-\nsupervised fashion. VLPMs generally use image-caption\npairs for pre-training using paired as well as unpaired open\nweb datasets, depending on the pre-training task. One of the\nmost common tasks used for pre-training in the language\nmodels is Cross-Modal Masked Language Modeling, and\nPre-trained Transformer Encoder\nTask-sepecific \nMLP head\nPre-trained \nLanguage \nEncoder\nPre-trained \nVision\nEncoder\nTask-sepecific \nMLP head\nQ K V V K Q\nCo-attention\nText Tokens Vision Tokens Vision TokensText Tokens\nFigure 3: Generic single and dual-stream architecture for\npre-trained vision-language transformer Models, The tokens\nrepresented in the figure are after including the positional\nembeddings.\nit can be easily mapped for cross-modality in the vision-\nlanguage domain as well. The task is generally used in a\nself-supervised setting where some tokens are masked ran-\ndomly, and the goal is to predict the masked tokens. An-\nother common task is Cross-Modal Masked Region Model-\ning, where tokens are masked out in the visual sequence.\nCross-modal alignment is a task where the goal is to try\nto pair image and text, also known as Image-Text Match-\ning (ITM). Cross-modal contrastive Learning is another pre-\ntraining task quite similar to ITM but in a contrastive manner\nin the way that matched image-text pairs are pushed together\nand non-matched pairs are pushed apart using contrastive\nloss. The large datasets used for pre-training have been con-\nsidered to be a cause of bias (Park and Choi 2022; Radford\net al. 2021).\nDownstream Tasks: Once VLPMs are pre-trained, they\nare finetuned to perform specific downstream tasks such as\nImage Captioning, Visual Question Answering, Image Text\nRetrieval, Natural Language for Visual Reasoning, and Vi-\nsual Commonsense Reasoning. Broadly, the tasks can be\ncategorized as generative, classification, and retrieval tasks.\nTask-specific datasets are used for finetuning the model,\nwhere the heads of the VLPMs are modified based on the\ndownstream task. VLPMs have shown impressive accuracy\nwith these tasks. The learned representation helps finetune\nthe model for specified tasks quickly, especially with the rich\ninformation flowing between the two modalities.\nWe can draw two important observations from this\noverview of VLPMs:\n• The architecture of VLPMs differs significantly from\nCNNs. Consequently, it’s crucial to develop methods\nspecifically tailored to the VLPM architecture rather\nthan merely extending approaches originally designed\nfor CNNs. This ensures a more accurate and equitable\nevaluation of their performance.\n• Most recent VLPMs undergo training on datasets derived\nfrom the open web, which is a combination of various\nsources. This amalgamation raises concerns about the po-\ntential incorporation of biases present in the content from\nthe open web into the models themselves (Mittal et al.\n2023).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22651\nBias Study Models Under Review Bias Metric\nSocial Bias (Gender and Race)\n(Ross, Katz, and Barbu 2021) ViLBERT, VisualBERT Grounded SEAT and WEAT\nGender Bias (Srinivasan and Bisk 2021) VLBERT Association Scores\nSocial Bias (Gender and Race)\n(Hirota, Nakashima, and Garcia 2022b)\nNIC, SAT, Att2In, UpDn,\nTransformer, Oscar, NIC+\nLeakage in Image Captioning\n(LIC)\nSocial Bias (Zhang, Wang, and Sang 2022) ALBEF, TCL, ViLT CounterBias\nStereotypical Bias (Gender, Profession, Race,\nand Religion) (Zhou, Lai, and Jiang 2022)\nVisualBERT, LXMERT, ViLT,\nCLIP, ALBEF, FLA V A\nvision-language relevance score\nand vision-language bias score\nQuantifying bias before and after\nfinetuning (Ranjit et al. 2023) ResNet, BiT, CLIP, MoCo, SimCLR Bias Transfer Score (BTS)\nEmotional and Racial Bias (Bakr et al. 2023) NIC, SAT, Att2In, UpDn,\nTransformer, Oscar, NIC+\nImageCaptioner 2\nTable 1: Summarizing research studies that have proposed different bias metrics.\nBias and Fairness\nFairness in AI systems has been primarily viewed as protect-\ning sensitive attributes in a way that no group faces disad-\nvantage or biased decision. Biases like gender or racial bias\nhave proven harmful, especially when they affect humans\nin real life (Singh et al. 2022). VLPMs are as vulnerable to\nbias as their CNN counterparts. They deal with two modali-\nties and often two-stage training, allowing them to introduce\nmore biases like pre-training bias or bias against a particular\nmodality. Literature has shown that VLPMs are heavily in-\nfluenced by language modality and can sometimes be harm-\nful. (Kervadec et al. 2021) showed this with reference to the\nVisual Question Answering (VQA) task.\nData and Bias\nData has been considered the primary source of bias as it\nis a representation of the world that the model is trying\nto learn. With VLPMs, this can be an even bigger issue\nas pre-training requires large datasets. Many well-known\nVLPMs today have been trained on large heavy datasets\ncrawled from the Internet, giving less control and oversight\nduring data collection. This can lead the dataset to learn\nharmful representations. (Zhao, Wang, and Russakovsky\n2021) examines some widely used multimodal datasets for\nbias and shows offensive texts and stereotypes embedded\nwithin them. (Bhargava and Forsyth 2019) specifically ex-\namines dataset bias by studying the COCO dataset (Lin et al.\n2014), a manually annotated dataset for the image caption-\ning task. The authors not only depict gender and racial bias\nbut also analyze recent captioning models to see the differ-\nences in the performance from a lens of bias. Some stud-\nies have looked at task-specific datasets as well, as (Hirota,\nNakashima, and Garcia 2022a) analyze five Visual Question\nAnswering (VQA) datasets for gender and racial bias. (Gar-\ncia et al. 2023) focuses on datasets crawled from the Internet\nwithout much oversight from a demographic point-of-view\nwhile also showcasing how societal bias is an issue on vari-\nous tasks and datasets.\nBias Estimation and Mitigation\n(Sudhakar et al. 2021) studies biases present in vision trans-\nformers by visualizing self-attention modules, noting en-\ncoded bias in the query matrix. To study and mitigate these\nbiases, they further proposed an alignment approach called\nTADeT. (Ross, Katz, and Barbu 2021) further measured so-\ncial biases in the joint embeddings by proposing Grounded\nWEAT and SEAT while also proposing a new dataset for\ntesting biases in the grounded setting. The study concludes\nthat bias comes from the language modality, and vision\nmodality does not help mitigate biases. Moreover, CLIP\n(Radford et al. 2021), a heavily used VLPM known for its\nzero-shot capabilities, conducted its own bias study, postu-\nlating that it may encode social biases owing to the large\nopen dataset used for its training. The authors tested zero-\nshot and linear probe instances of the model to mark the\npotential sources of biases and harmful markers. (Zhang,\nWang, and Sang 2022) proposes the CounterBias method\nand FairVLP framework to quantify social bias in VLPMs\nin a counterfactual manner while proposing a new dataset\nto measure gender bias. (Srinivasan and Bisk 2022) studies\ngender bias, particularly in the VL-BERT model, by mod-\nifying both language and vision modalities and getting as-\nsociation scores. They further create templates for entities to\nmeasure the bias in three instances - pre-training, visual con-\ntext at inferencing, and language context at inferencing. It is\nparticularly interesting as investigating the bias at different\nstages can not only help dissect the effectiveness of different\nmodalities but can also allow examination of how VLPMs\ncan evolve after the modalities integrate, giving a new per-\nspective on merging the multiple modalities effectively.\n(Hirota, Nakashima, and Garcia 2022b) introduced a new\nmetric, Leakage for Image Captioning (LIC), to measure\nbias towards a particular attribute for the task of image cap-\ntioning. The metric requires annotations for the protected\nattribute and can also use embeddings that have pre-existing\nbias. Furthermore, VLStereoSet (Zhou, Lai, and Jiang 2022)\nmeasured stereotypical biases in VLPMs using probing tasks\nby testing their tendency to recognize stereotypical state-\nments for anti-stereotypical images. The stereotype is based\non four categories: gender, profession, race, and religion,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22652\nmaking the VLPMs select the statements as captions. They\nalso proposed two metrics called vision-language relevance\nscore and vision-language bias score, using which they con-\ncluded that state-of-the-art VLPMs under consideration not\nonly encode stereotypical bias but are more complex than\nlanguage bias and need to be studied. Several studies have\ngiven mitigation techniques to deal with bias like (Hendricks\net al. 2018; Amend, Wazzan, and Souvenir 2021; Zhao, An-\ndrews, and Xiang 2023; Wang and Russakovsky 2023). As\ncan be noticed in these studies, there are different com-\nponents and parts of the entire vision-language processing\npipeline that are put under consideration. Even when look-\ning for societal biases – gender and racial, there is a lack\nof commonality, yet none of the observations and results\ncan be denied as less crucial. We feel that there is a lack of\nstandard metrics and common protocol in the bias for multi-\nmodal models so far. In Table 1, we have tried to summarize\nsome of these studies, detailing the metrics they used and\nthe models they examined for bias. VLPMs can encode bias\nwith more opportunities to do so than unimodal models.\nRobustness\nWhile accuracy focuses on correctness, robustness focuses\non security by assessing the model for vulnerabilities in\nadversarial settings (Singh et al. 2020). Like CNNs, trans-\nformers are vulnerable to adversarial attacks. We first dis-\ncuss how transformers perform against their CNN counter-\nparts. Many have formulated that transformers are more ro-\nbust than CNNs, but we believe that architectural differences\nwere not considered by the adversarial methods used for\nthese studies. We discuss the robustness of VLPMs exclu-\nsively in a separate subsection.\nTransformers vs CNNs\nSeveral transformer architectures have performed better than\nCNNs, but are they more robust?(Bhojanapalli et al. 2021)\nmeasures the robustness of ViT architectures to answer this\nvery question and compares them with their ResNet coun-\nterparts for the task of image classification. Perturbations\nare added to the input using adversarial settings to mea-\nsure robustness. The robustness is measured in parts, start-\ning with natural perturbations like blurring, digitizing, and\nadding Gaussian noise. It is then measured with respect to\ndistribution shift and using adversarial attacks. All the com-\nparisons are made across varying sizes of ViT and ResNet\narchitecture, concluding that transformers have a slight edge\ncompared to ResNets, and with sufficient data, VITs can\noutperform their ResNet counterparts. (Shao et al. 2022)\nstudied the robustness of transformers by exposing them to\nwhite-box and transfer adversarial attacks, concluding that\nViTs are more robust than CNNs. The study also observes\nthat VITs have spurious correlations and are less sensitive\nto high-frequency perturbations. Adding tokens for learning\nhigh-frequency patterns in ViTs improves classification ac-\ncuracy but reduces the robustness of the architecture.\nHybrid architectures combining ViTs and CNNs can re-\nduce the robustness gap between the two architectures. Most\nof the studies focus on transfer attacks in lieu of specific\nattacks for transformers. (Bai et al. 2021; Pinto, Torr, and\nDokania 2022) studies the robustness between transformers\nand CNNs questioning previous studies (Bhojanapalli et al.\n2021; Shao et al. 2022) that show transformers to be more\nrobust than CNNs claiming unfair settings while comparing\nthe architectures. The study shows that transformers are not\nmore robust than CNNs, but on out-of-distribution samples,\ntransformers outperform CNNs. (Mao et al. 2022) proposed\na Robust Vision Transformer (RVT) after studying the com-\nponents affecting the robustness of the model, proposing a\nnew patch-wise augmentation and a position-aware atten-\ntion scaling (PAAS) to boost the RVT other than modify-\ning damaging elements in the architecture for better robust-\nness. RVT can be used as a backbone or vision encoder for\ndifferent VLPMs, just like the Trade-off between Robust-\nness and Accuracy of Vision Transformers (TORA-ViTs)\n(Li and Xu 2023) that can combine predictive and robust\nfeatures in a trade-off manner. (Mishra, Sachdeva, and Baral\n2022) performed a comparative study to measure the robust-\nness of pre-trained transformers on noisy data. The noisy\ndata is created using poison attacks like label flipping and\nhas been compared under adversarial filtering augmentation.\nThey introduced a novel robustness metric called Mean Rate\nof change of Accuracy with change in Poisoning (MRAP),\nusing which they observed that the models are not robust un-\nder adversarial filtering. In most of these studies, the com-\nparison between CNNs and transformers is drawn from ex-\nisting attacks proposed originally for CNNs, but it is impor-\ntant to devise attacks that exploit vulnerabilities of the latter,\nkeeping in mind the critical architecture difference between\nthe two.\nVLPMs and their Robustness\nVLPMs are studied under the robustness lens but not as ex-\ntensively as unimodal transformers. (Li, Gan, and Liu 2020)\nstudies VLPMs over linguistic variation, logical reasoning,\nvisual content manipulation, and answer distribution shift.\nThese models have already shown better performance in\nterms of accuracy. Still, for robustness, the authors propose\nan adversarial training strategy called MANGO or Multi-\nmodal Adversarial Noise Generator to fool the models. Fur-\nther, efforts have been made to devise methods exclusively\nfor transformers, like the Patch-wise Adversarial Removal\n(PAR) method (Shi and Han 2021) that processes each patch\nseparately to generate adversarial samples in a black-box\nsetting. The patches are processed based on noise sensitiv-\nity and can be extended to CNNs as well. (Li et al. 2021)\nproposed a new benchmark for adversarial robustness on\nthe task of VQA. (Wei et al. 2022) proposed a dual attack\nframework, namely, the Pay No Attention (PNA) method\nand PatchOut Attack, to improve the transferability across\ntransformers that skipped attention gradients in order to cre-\nate adversarial samples. Since the attack framework is sensi-\ntive to the transformer architecture, the attacks consider both\npatches by perturbing only a subset of them at each iteration\nand attention module by skipping some attention gradients.\nOther than attacks, (Ma et al. 2022) investigated how\nVLPMs perform under data with missing or incomplete\nmodalities (examining only one modality at a time) in terms\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22653\nof accuracy and were improved using different fusion strate-\ngies. They concluded that transformers are not only sensitive\nto missing modalities but also that there is no optimal fusion\nstrategy as multimodal fusion affects the robustness of these\nmodels and is dependent on datasets. (Salin et al. 2022) an-\nalyzes VLPMs to get a better insight into the multimodal re-\nlationship using probing tasks, concluding that concepts like\nposition and size are difficult for the models under consid-\neration to understand. (Zhao et al. 2023) studies adversar-\nial vulnerability in a black-box setting to perform a realis-\ntic adversarial study by manipulating visual inputs. (Schlar-\nmann and Hein 2023) on the other hand, studied adversar-\nial robustness for imperceivable attacks on VQA and Im-\nage captioning tasks for well-known multimodal foundation\nmodels and (Mao et al. 2023) studies the zero-shot adver-\nsarial robustness. The authors proposed a text-guided con-\ntrastive adversarial training (TeCoA) to be used along with\nfinetuning to improve the zero-shot adversarial robustness.\nAll these studies try to examine the robustness by either for-\nmulating transformer-specific attacks, proposing new bench-\nmarks, carefully looking at different architectural compo-\nnents, or optimizing training strategies. However, a proper\nand common framework can better help compare the vari-\nous VLPMs. The architectural difference alone makes this a\ndifficult but essential task that needs to be looked at.\nInterpretability and Explainability\nIrrespective of the architecture, it is imperative that we\ncan interpret as well as explain the decisions made by the\nmodel. Transformers have relied heavily on attention to pro-\nvide that explanation. A few methods originally proposed\nfor CNNs have been extended for transformers as well,\nlike GradCAM (Selvaraju et al. 2017). We have catego-\nrized the proposed methods into two categories, namely, gra-\ndient and visualization-based methods, and probing tasks.\nWhile visualization-based methods usually use inter- and\nintra-modality interactions to visually explain the decisions,\nprobing tasks are specifically designed to explain a particu-\nlar aspect or component of the models and can be restrictive.\nFinally, we discuss attention and how reliable it is as an ex-\nplanation.\nGradient-based and Visualization-based Methods\nAmong several explanation methods proposed in the litera-\nture, many have been extended to transformer-based mod-\nels. We first present the different gradient and visualization-\nbased methods that are more in line with transformers and\nVLPMs. Attention maps are a well-known method for in-\nterpreting transformer models. Modifications of these meth-\nods have been proposed in the literature, like the Attention\nRollout (Abnar and Zuidema 2020), which combined lay-\ners to get averaged attention. (V oita et al. 2019) modified\nthe LRP method specifically for transformers overcoming\nthe computational barriers. Further, Relevancy Map or Hila-\nCAM (Chefer, Gur, and Wolf 2021) uses the self-attention\nand co-attention modules considering classification tokens\nappended during downstream tasks and associated values\nto generate a relevancy map tracking interactions between\ndifferent modalities and backpropagating relevancies. The\nmethod applies to both unimodal and multimodal models.\nApart from these methods, VL-InterpreT (Aflalo et al. 2022)\nis more like a tool that gives an interactive interface looking\nat interactions between modalities from a bottom-up per-\nspective. It uses four modality attention heads: language-\nto-vision attention, vision-to-language attention, language-\nto-language attention, and vision-to-vision attention, allow-\ning it to look at interactions within and between modalities.\nMULTIVIZ (Liang et al. 2022) is another method to ana-\nlyze multimodal models interpreting unimodal interactions,\ncross-modal interactions, multi-modal representations, and\nmultimodal prediction. gScoreCAM (Chen et al. 2022) stud-\nied the CLIP (Radford et al. 2021) model specifically to un-\nderstand large multimodal models. Using gScoreCAM, ob-\njects can be visualized as seen by the model by linearly com-\nbining the highest gradients as attention.\n(Pan et al. 2021) proposes interpretability-aware redun-\ndancy reduction (IA − RED2) to make transformer cost-\nefficient while using human-understandable architecture.\nThe study (Chefer, Schwartz, and Wolf 2022) manipulates\nthe relevancy maps to alleviate the model’s robustness.\nLower relevance is assigned to the background pixels, so\nthe foreground is considered with more confidence. (Qiang\net al. 2022) proposes the AttCAT explanation method that\nuses attentive class activation tokens built on encoded fea-\ntures, gradients, and attention weights to provide the expla-\nnation. B-cos transformers are proposed by (B ¨ohle, Fritz,\nand Schiele 2023), which are highly interpretable, providing\nholistic explanations. (Nalmpantis et al. 2023) proposes an-\nother interpretation method called Vision DiffMask, which\nidentifies the relevant input part for final prediction using a\ngating mechanism. A faithfulness test is also used to show-\ncase the validity of this post-hoc method, concluding that\nthere is a lack of faithfulness tests in the literature. (Choi, Jin,\nand Han 2023) proposes Adversarial Normalization: I can\nVisualize Everything (ICE) to visualize the transformer ar-\nchitecture effectively. It uses adversarial normalization and\npatch-wise classification for each token, separating back-\nground and foreground pixels. The most common theme in\nthese methods is exploiting attention weights and gradients\nto make the information flow more targeted. Another theme\nis to extend available metrics by making them computation-\nally effective.\nProbing Tasks\nMost of the explanation methods for VLPMs are based on\nprobing tasks. These tasks are designed to study a partic-\nular aspect of the model and thus are hard to generalize.\nV ALUE or Vision And Language Understanding Evalua-\ntion (Cao et al. 2020) method gives several probing tasks\nto understand how pre-training helps the learned representa-\ntions. The authors made several important observations: (i)\nthe pre-trained models attend to language more than vision,\nsomething that has been corroborated throughout the litera-\nture; (ii) there is a set of attention heads that capture cross-\nmodal interactions; and (iii) plotting attention can depict in-\nterpretable visual relations as was corroborated in the pre-\nvious section as well, among others. (Dahlgren Lindstr ¨om\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22654\net al. 2020) further proposes three probing tasks for visual-\nsemantic space, which are relevant for image-caption pairs\nand train separate classifiers for probing. The tasks are (i) a\ndirect probing task designed for the number of objects, (ii)\na direct probing task for object categories, and (iii) a task\nfor semantic congruence. (Hendricks and Nematzadeh 2021)\nfurthermore proposes probing tasks for verb understanding\nby collecting image-sentence pairs with 421 verbs com-\nmonly found in the Conceptual Captions dataset (Sharma\net al. 2018). (Salin et al. 2022) proposed a set of probing\ntasks to better understand the representations generated by\nvision-language models, comparing the representations at\npre-trained and finetuned levels. Further, datasets are de-\nsigned carefully for multimodal probing, trying to reduce de-\npendency on bias while making predictions. While probing\ntasks are helpful and can answer meaningfully with regard\nto particular problems, they have to be carefully crafted for\nrelevant results and are very specific. At times, extra models\nor classifiers are required for probing, making the probing\ntasks applicable to selected models only.\nDissecting Attention\nAs can be seen in this section so far, attention is heavily used\nin the methods proposed to explain and interpret VLPMs. In\nfact, attention is one of the main reasons why transformers\nhave been attributed to working so well. However, recently,\nattention has been pointed out not to be a reliable parame-\nter for explaining a model’s decision in some studies. For\nVLPMs, in particular, fusing the modalities can make it dif-\nficult to interpret how the attention is distributed and how\nit should be explained. (Serrano and Smith 2019) evaluated\nattention for text classification, concluding that while atten-\ntion can be helpful with intermediate components, it is not a\ngood indicator for a justification. Further, (Jain and Wallace\n2019) studied the relationship between attention weights and\nthe final decision for several NLP tasks and concluded that\nattention weights often do not relate to gradient-based meth-\nods for computing feature importance; hence, they do not\nprovide helpful or meaningful explanations.\nWhile these methods concluded that attention is not re-\nliable as a justification tool, the studies have been limited\nto language-based tasks and need a proper in-depth analysis\ngiven how heavily current methods rely on the mechanism\nto interpret the models. (Park and Choi 2022) computed\na relation between the attention map and input-attribution\nmethod by proposing Input-Attribution and Attention Score\nVector (IA V). It tried to combine attention with attribution-\nbased methods to utilize both components as a justification\ntool. Such methods can help alleviate this mistrust of atten-\ntion. (Sahiner et al. 2022) studies attention under convex\nduality that can help provide interpretability for the archi-\ntecture. (Liu et al. 2022) takes polarity into consideration\nalong with attention. The authors propose a faithfulness vi-\nolation test that can help quantify the quality of different\nexplanation methods. We believe that attention needs to be\nevaluated as an interpretability metric for more vision and\nvision-language tasks. Combining the module with other es-\ntablished methods, like attribution-based methods, or exam-\nining the methods on controlled benchmarks can help.\nOpen Challenges and Opportunities\nThe previous sections discuss several methods and tech-\nniques to make VLPMs fair, robust, explainable, and inter-\npretable. However, they also highlighted a lack of specific\narchitecture-based methods and standard protocols. Even\nwith all the progress, there are several open challenges that\nrequire further development and analysis. Here, we discuss\nsome of the open challenges for improving different aspects\nof the trustworthiness of VLPMs.\nTrustworthiness of VLPMs:The concept of trustworthi-\nness as a whole is lacking in the current analysis of VLPMs.\nA formalized and standardized framework can help set the\nbaselines for the growing number of transformer architec-\ntures. One basic need is to make these models just as trust-\nworthy to ensure that their decisions can be trusted and re-\nlied upon while staying away from harmful biases like using\nfaithfulness tests for quantifying the model’s explainability.\nAs we continue to use these models for security-critical ap-\nplications, we need to be able to depend on the models and\ntheir decisions.\nExamining Attention: Attention mechanisms are often\nused to explain how models make decisions by creating vi-\nsual representations that provide reasoning behind these de-\ncisions. However, to better understand and interpret atten-\ntion, especially in the context of vision and cross-modality,\nwe need to thoroughly examine attention modules. Analyz-\ning models under adversarial conditions can also help us\ngain valuable insights and improve our understanding of at-\ntention mechanisms. Additionally, attention is a critical fac-\ntor in ensuring the trustworthiness of transformer models.\nTherefore, we should examine attention from three differ-\nent angles: its impact on model performance, its role in ex-\nplaining decisions, and its role in understanding the model’s\nreasoning.\nProbing the Vision Modality:The literature has time and\nagain iterated that for VLPMs, decisions have a stronger in-\nfluence from the language modality than the visual modality.\nWe believe a big gap exists between a systematic review of\nhow the vision modality affects decisions and how we can\nbetter utilize it to avoid language bias. While tasks like VQA\nhave recognized language bias, VLPM as a generalized ar-\nchitecture has not been explored for this bias as extensively.\nBetter pre-trained tasks aligning the vision modality along\nwith cross-modality interactions can be a way forward for\nimproving the generalization as well as the effect of the vi-\nsion modality on the entire model. Moreover, vision plays a\ncrucial role in understanding object semantics on tasks like\nobject detection and semantic segmentation, and thus, their\nreduced influence in vision-language tasks can be seen as\na disadvantage. Studying the alignment between vision and\ntext modality can also be a way forward.\nBetter Generalized Methods:There is a need for better\ngeneralized methods that can evaluate not only between\nCNNs and transformers but also between different archi-\ntecture formats within transformers. Also, with increasing\nhybrid architectures, such methods can help create a better\ncomparison framework, providing effective baselines for fu-\nture studies. Some studies (Gui et al. 2022; Tang et al. 2023)\nhave used one modality to guide the other while training or\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22655\nused one modality to train the multimodal models, which\ncan allow correcting for bias or adversarial vulnerabilities.\nCross-modality and Universal Weights: Transformer\nmodels are known for their similar architecture, even when\nprocessing different modalities. However, the pre-trained\nweights are not as easily adapted between the modalities,\nand alignment remains an open challenge. Aligning the two\nmodalities can help improve the representations for VLPMs\nand better project the two modalities in a similar projection\nspace. A universal model that can represent both modalities\nsimilarly can help with performance as well as robustness,\nhowever, there is still a gap in getting universal pre-trained\nweights that can adapt to different modalities and require\nfurther research.\nStrategic Pre-training:Pre-training has been demonstrated\nto be beneficial for transformers, but it is costly. It can be a\ntedious process that requires large datasets and pre-training\ntasks that utilize heavy computing power. We have also seen\nhow these large datasets can be a potential source of bias.\nWith better and more focused pre-training strategies (Zhou\net al. 2020), the training cost can be reduced while improv-\ning task-aware performance. With proper strategies in place,\nbias at the pre-training stage can be mitigated or avoided\nduring finetuning.\nInterplay of VLPMs with Audio Models:In several mul-\ntimedia applications ranging from audio-visual scene com-\nprehension to speech-driven image recognition and immer-\nsive human-computer interactions, the fusion of vision, lan-\nguage, and audio plays a pivotal role. Consequently, it be-\ncomes imperative to explore the interplay between audio\nmodels and VLPMs to enhance our capabilities in percep-\ntion, understanding, and communication, thereby offering\nmore enriched and immersive experiences.\nResponsible ML Datasets:The trustworthiness of VLPMs\nand transformer models is intricately tied to their training\ndata. These algorithms learn patterns from the data they are\nexposed to, which may inadvertently incorporate any inher-\nent flaws present in the data, thereby influencing their be-\nhavior. Therefore, it is important to understand the crucial\nrole of Responsible Machine Learning Datasets (Mittal et al.\n2023), encompassing aspects such as privacy (Chhabra et al.\n2018) and adherence to regulatory standards. In addition,\nmachine unlearningconcepts should be explored to ensure\nthese systems can adapt and comply with evolving regula-\ntory norms.\nDiscussion\nDespite the remarkable human-like performance demon-\nstrated by Vision-Language Pre-trained Models (VLPMs)\nand Vision Transformers, it is of paramount importance not\nto underestimate the crucial dimension of trustworthiness.\nAs VLPMs continue to gain widespread adoption on a global\nscale, a rigorous examination becomes imperative. This pa-\nper presents a comprehensive analysis of VLPMs, address-\ning three essential dimensions: bias/fairness, robustness, and\nexplainability/interpretability. Firstly, we scrutinize biases\nwithin VLPMs, recognizing that while datasets often serve\nas the primary source of bias, biases can also seep into the\nmodels and algorithms themselves. Addressing this issue re-\nquires a thorough evaluation and mitigation study, a chal-\nlenge further complicated by VLPMs’ multidimensional na-\nture encompassing both vision and language. Establishing a\nrobust framework is essential to conduct bias assessments\ntailored to these complex models effectively. Next, we dis-\ncuss about the robustness of VLPMs. While VLPMs have\nbeen extensively compared to their CNN counterparts, a no-\nticeable gap exists when it comes to architecture-specific\nstudies that explore vulnerabilities unique to VLPMs. Fi-\nnally, we explore VLPMs using visualization-based and\nprobing methods, which, although limited in availability,\nprovide valuable insights to enhance our comprehension\nof VLPMs’ inner workings. We also highlighted some of\nthe open challenges confronting VLPMs. We hope that this\nstudy serves as a foundation for researchers to identify gaps\nand work towards enhancing both the performance and trust-\nworthiness of these models.\nAcknowledgements\nThe work is partially supported through the grant from Tech-\nnology Innovation Hub (TIH) at IIT Jodhpur. M. Vatsa is\nalso supported through the Swarnajayanti Fellowship by the\nGovernment of India.\nReferences\nAbnar, S.; and Zuidema, W. H. 2020. Quantifying Attention\nFlow in Transformers. In ACL, 4190–4197.\nAflalo, E.; Du, M.; Tseng, S.-Y .; Liu, Y .; Wu, C.; Duan, N.;\nand Lal, V . 2022. Vl-interpret: An interactive visualization\ntool for interpreting vision-language transformers. In IEEE\nCVPR, 21406–21415.\nAmend, J. J.; Wazzan, A.; and Souvenir, R. 2021. Evalu-\nating Gender-Neutral Training Data for Automated Image\nCaptioning. In IEEE International Conference on Big Data\n(Big Data), 1226–1235.\nBai, Y .; Mei, J.; Yuille, A. L.; and Xie, C. 2021. Are trans-\nformers more robust than cnns? NeurIPs, 34: 26831–26843.\nBakr, E. M.; Sun, P.; Li, L. E.; and Elhoseiny, M. 2023.\nImageCaptioner2: Image Captioner for Image Captioning\nBias Amplification Assessment. CoRR, abs/2304.04874.\nBhargava, S.; and Forsyth, D. A. 2019. Exposing and Cor-\nrecting the Gender Bias in Image Captioning Datasets and\nModels. CoRR, abs/1912.00578.\nBhojanapalli, S.; Chakrabarti, A.; Glasner, D.; Li, D.; Un-\nterthiner, T.; and Veit, A. 2021. Understanding robustness\nof transformers for image classification. In IEEE CVPR.\nBirhane, A.; Prabhu, V . U.; and Kahembwe, E. 2021. Mul-\ntimodal datasets: misogyny, pornography, and malignant\nstereotypes. arXiv preprint arXiv:2110.01963.\nB¨ohle, M.; Fritz, M.; and Schiele, B. 2023. Holistically Ex-\nplainable Vision Transformers. CoRR, abs/2301.08669.\nCao, J.; Gan, Z.; Cheng, Y .; Yu, L.; Chen, Y .-C.; and Liu, J.\n2020. Behind the scene: Revealing the secrets of pre-trained\nvision-and-language models. In ECCV, 565–580. Springer.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22656\nChefer, H.; Gur, S.; and Wolf, L. 2021. Generic attention-\nmodel explainability for interpreting bi-modal and encoder-\ndecoder transformers. In IEEE CVPR, 397–406.\nChefer, H.; Schwartz, I.; and Wolf, L. 2022. Optimizing Rel-\nevance Maps of Vision Transformers Improves Robustness.\nIn NeurIPS.\nChen, P.; Li, Q.; Biaz, S.; Bui, T.; and Nguyen, A. 2022.\ngScoreCAM: What objects is CLIP looking at? In ACCV.\nChhabra, S.; Singh, R.; Vatsa, M.; and Gupta, G. 2018.\nAnonymizing k Facial Attributes via Adversarial Perturba-\ntions. In IJCAI, 656–662.\nChoi, H.; Jin, S.; and Han, K. 2023. Adversarial Normaliza-\ntion: I Can visualize Everything (ICE). InIEEE/CVF CVPR.\nDahlgren Lindstr ¨om, A.; Bj ¨orklund, J.; Bensch, S.; and\nDrewes, F. 2020. Probing Multimodal Embeddings for Lin-\nguistic Properties: the Visual-Semantic Case. In COLING.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL-HLT.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR.\nDu, Y .; Liu, Z.; Li, J.; and Zhao, W. X. 2022. A Survey\nof Vision-Language Pre-Trained Models. In IJCAI, 5436–\n5443. Survey Track.\nFields, C.; and Kennington, C. 2023. Vision Language\nTransformers: A Survey. CoRR, abs/2307.03254.\nGarcia, N.; Hirota, Y .; Wu, Y .; and Nakashima, Y . 2023.\nUncurated Image-Text Datasets: Shedding Light on Demo-\ngraphic Bias. In IEEE/CVF CVPR, 6957–6966.\nGui, L.; Huang, Q.; Hauptmann, A.; Bisk, Y .; and Gao, J.\n2022. Training Vision-Language Transformers from Cap-\ntions Alone. CoRR, abs/2205.09256.\nHendricks, L. A.; Burns, K.; Saenko, K.; Darrell, T.; and\nRohrbach, A. 2018. Women Also Snowboard: Overcoming\nBias in Captioning Models. In ECCV.\nHendricks, L. A.; and Nematzadeh, A. 2021. Probing\nImage-Language Transformers for Verb Understanding. In\nACL/IJCNLP.\nHirota, Y .; Nakashima, Y .; and Garcia, N. 2022a. Gender\nand Racial Bias in Visual Question Answering Datasets. In\nFAccT, 1280–1292.\nHirota, Y .; Nakashima, Y .; and Garcia, N. 2022b. Quanti-\nfying Societal Bias Amplification in Image Captioning. In\nIEEE/CVF CVPR, 13440–13449.\nHuang, Z.; Zeng, Z.; Liu, B.; Fu, D.; and Fu, J. 2020. Pixel-\nbert: Aligning image pixels with text by deep multi-modal\ntransformers. arXiv preprint arXiv:2004.00849.\nJain, S.; and Wallace, B. C. 2019. Attention is not Expla-\nnation. In ACL: Human Language Technologies, Volume 1\n(Long and Short Papers), 3543–3556.\nKervadec, C.; Antipov, G.; Baccouche, M.; and Wolf, C.\n2021. Roses are red, violets are blue... but should vqa ex-\npect them to? In IEEE CVPR, 2776–2785.\nKim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-\nlanguage transformer without convolution or region super-\nvision. In ICML, 5583–5594.\nLi, L.; Gan, Z.; and Liu, J. 2020. A closer look at the ro-\nbustness of vision-and-language pre-trained models. arXiv\npreprint arXiv:2012.08673.\nLi, L.; Lei, J.; Gan, Z.; and Liu, J. 2021. Adversarial VQA:\nA New Benchmark for Evaluating the Robustness of VQA\nModels. In IEEE/CVF ICCV, 2022–2031.\nLi, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-\nW. 2019. Visualbert: A simple and performant baseline for\nvision and language. arXiv preprint arXiv:1908.03557.\nLi, Y .; and Xu, C. 2023. Trade-off between Robustness and\nAccuracy of Vision Transformers. In IEEE/CVF CVPR.\nLiang, P. P.; Lyu, Y .; Chhablani, G.; Jain, N.; Deng,\nZ.; Wang, X.; Morency, L.-P.; and Salakhutdinov, R.\n2022. MultiViz: An Analysis Benchmark for Visualizing\nand Understanding Multimodal Models. arXiv preprint\narXiv:2207.00056.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; et al. 2014.\nMicrosoft coco: Common objects in context. In ECCV.\nLiu, Y .; Li, H.; Guo, Y .; Kong, C.; Li, J.; and Wang, S. 2022.\nRethinking Attention-Model Explainability through Faith-\nfulness Violation Test. In ICML PMLR.\nLong, S.; Cao, F.; Han, S. C.; and Yang, H. 2022. Vision-\nand-Language Pretrained Models: A Survey. In IJCAI,\n5530–5537. Survey Track.\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. NeurIPs, 32.\nMa, M.; Ren, J.; Zhao, L.; Testuggine, D.; and Peng, X.\n2022. Are Multimodal Transformers Robust to Missing\nModality? In IEEE CVPR, 18177–18186.\nMao, C.; Geng, S.; Yang, J.; Wang, X.; and V ondrick, C.\n2023. Understanding Zero-shot Adversarial Robustness for\nLarge-Scale Models. In ICLR.\nMao, X.; Qi, G.; Chen, Y .; Li, X.; Duan, R.; Ye, S.; He, Y .;\nand Xue, H. 2022. Towards robust vision transformer. In\nIEEE CVPR, 12042–12051.\nMishra, S.; Sachdeva, B. S.; and Baral, C. 2022. Pretrained\nTransformers Do not Always Improve Robustness. arXiv\npreprint arXiv:2210.07663.\nMittal, S.; Thakral, K.; Singh, R.; Vatsa, M.; Glaser, T.;\nCanton-Ferrer, C.; and Hassner, T. 2023. On Responsible\nMachine Learning Datasets with Fairness, Privacy, and Reg-\nulatory Norms. CoRR, abs/2310.15848.\nNalmpantis, A.; Panagiotopoulos, A.; Gkountouras, J.; Pa-\npakostas, K.; and Aziz, W. 2023. Vision DiffMask: Faith-\nful Interpretation of Vision Transformers with Differentiable\nPatch Masking. In IEEE/CVF CVPR.\nPan, B.; Jiang, Y .; Panda, R.; Wang, Z.; Feris, R.; and Oliva,\nA. 2021. IA-RED2: Interpretability-Aware Redundancy Re-\nduction for Vision Transformers. CoRR, abs/2106.12620.\nPark, B.; and Choi, J. 2022. Explanation on Pretrain-\ning Bias of Finetuned Vision Transformer. arXiv preprint\narXiv:2211.15428.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22657\nPark, N.; and Kim, S. 2022. How Do Vision Transformers\nWork? In ICLR.\nPinto, F.; Torr, P. H. S.; and Dokania, P. K. 2022. An Impar-\ntial Take to the CNN vs Transformer Robustness Contest. In\nECCV, volume 13673, 466–480.\nQiang, Y .; Pan, D.; Li, C.; Li, X.; Jang, R.; and Zhu, D. 2022.\nAttCAT: Explaining Transformers via Attentive Class Acti-\nvation Tokens. In NeurIPS.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; et al. 2021. Learning transferable\nvisual models from natural language supervision. In ICML.\nRanjit, J.; Wang, T.; Ray, B.; and Ordonez, V . 2023. Varia-\ntion of Gender Biases in Visual Recognition Models Before\nand After Finetuning. CoRR, abs/2303.07615.\nRoss, C.; Katz, B.; and Barbu, A. 2021. Measuring Social\nBiases in Grounded Vision and Language Embeddings. In\nNAACL-HLT, 998–1008.\nSahiner, A.; Ergen, T.; Ozturkler, B.; Pauly, J. M.; Mardani,\nM.; and Pilanci, M. 2022. Unraveling Attention via Convex\nDuality: Analysis and Interpretations of Vision Transform-\ners. In ICML PMLR, volume 162, 19050–19088.\nSalin, E.; Farah, B.; Ayache, S.; and Favre, B. 2022. Are\nVision-Language Transformers Learning Multimodal Rep-\nresentations? A Probing Perspective. In AAAI.\nSchlarmann, C.; and Hein, M. 2023. On the Adversar-\nial Robustness of Multi-Modal Foundation Models. CoRR,\nabs/2308.10741.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual expla-\nnations from deep networks via gradient-based localization.\nIn IEEE ICCV, 618–626.\nSerrano, S.; and Smith, N. A. 2019. Is Attention Inter-\npretable? In ACL, 2931–2951.\nShao, R.; Shi, Z.; Yi, J.; Chen, P.-Y .; and Hsieh, C.-J.\n2022. On the Adversarial Robustness of Vision Transform-\ners. TMLR.\nSharma, P.; Ding, N.; Goodman, S.; and Soricut, R. 2018.\nConceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. InACL (Vol 1: Long\nPapers), 2556–2565.\nShi, Y .; and Han, Y . 2021. Decision-based black-box at-\ntack against vision transformers via patch-wise adversarial\nremoval. arXiv preprint arXiv:2112.03492.\nSingh, R.; Agarwal, A.; Singh, M.; Nagpal, S.; and Vatsa, M.\n2020. On the Robustness of Face Recognition Algorithms\nAgainst Attacks and Bias. In AAAI, 13583–13589.\nSingh, R.; Majumdar, P.; Mittal, S.; and Vatsa, M. 2022.\nAnatomizing Bias in Facial Analysis. In AAAI, 12351–\n12358.\nSrinivasan, T.; and Bisk, Y . 2021. Worst of Both Worlds: Bi-\nases Compound in Pre-trained Vision-and-Language Mod-\nels. CoRR, abs/2104.08666.\nSrinivasan, T.; and Bisk, Y . 2022. Worst of Both Worlds: Bi-\nases Compound in Pre-trained Vision-and-Language Mod-\nels. In GeBNLP, 77–85.\nSudhakar, S.; Prabhu, V .; Krishnakumar, A.; and Hoffman,\nJ. 2021. Mitigating bias in visual transformers via targeted\nalignment. BMVC.\nTan, H.; and Bansal, M. 2019. LXMERT: Learning Cross-\nModality Encoder Representations from Transformers. In\nEMNLP-IJCNLP.\nTang, S.; Wang, Y .; Kong, Z.; Zhang, T.; Li, Y .; Ding, C.;\nWang, Y .; Liang, Y .; and Xu, D. 2023. You Need Multi-\nple Exiting: Dynamic Early Exiting for Accelerating Unified\nVision Language Model. In IEEE/CVF CVPR 2023, 10781–\n10791.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Neural Information Processing\nSystems.\nV oita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; and Titov,\nI. 2019. Analyzing Multi-Head Self-Attention: Specialized\nHeads Do the Heavy Lifting, the Rest Can Be Pruned. In\nACL.\nWang, A.; and Russakovsky, O. 2023. Overcoming Bias in\nPretrained Models by Manipulating the Finetuning Dataset.\nCoRR, abs/2303.06167.\nWei, Z.; Chen, J.; Goldblum, M.; Wu, Z.; Goldstein, T.; and\nJiang, Y .-G. 2022. Towards transferable adversarial attacks\non vision transformers. In AAAI, volume 36, 2668–2676.\nZhang, Y .; Wang, J.; and Sang, J. 2022. Counterfactually\nMeasuring and Eliminating Social Bias in Vision-Language\nPre-training Models. In ACM Multimedia, 4996–5004.\nZhao, D.; Andrews, J. T. A.; and Xiang, A. 2023. Men Also\nDo Laundry: Multi-Attribute Bias Amplification. In ICML\nPMLR, 42000–42017.\nZhao, D.; Wang, A.; and Russakovsky, O. 2021. Under-\nstanding and Evaluating Racial Biases in Image Captioning.\nIn IEEE/CVF ICCV, 14810–14820.\nZhao, Y .; Pang, T.; Du, C.; Yang, X.; Li, C.; Cheung, N.;\nand Lin, M. 2023. On Evaluating Adversarial Robustness of\nLarge Vision-Language Models. CoRR, abs/2305.16934.\nZhou, K.; Lai, E.; and Jiang, J. 2022. VLStereoSet: A Study\nof Stereotypical Bias in Pre-trained Vision-Language Mod-\nels. In IJCNLP, 527–538.\nZhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J. J.; and\nGao, J. 2020. Unified Vision-Language Pre-Training for Im-\nage Captioning and VQA. In AAAI, 13041–13049.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22658",
  "topic": "Trustworthiness",
  "concepts": [
    {
      "name": "Trustworthiness",
      "score": 0.7482407093048096
    },
    {
      "name": "Adventure",
      "score": 0.7351630330085754
    },
    {
      "name": "Computer science",
      "score": 0.5849106311798096
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3787904977798462
    },
    {
      "name": "Data science",
      "score": 0.34548646211624146
    },
    {
      "name": "Natural language processing",
      "score": 0.32377737760543823
    },
    {
      "name": "Computer security",
      "score": 0.27522921562194824
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154549908",
      "name": "Indian Institute of Technology Jodhpur",
      "country": "IN"
    }
  ]
}