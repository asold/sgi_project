{
  "title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM",
  "url": "https://openalex.org/W4389520497",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097894981",
      "name": "Weijie Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097231596",
      "name": "WenXiang Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2948269070",
      "name": "Fanyou Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4299760695",
      "name": "Srinivasan Sengamedu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2890363010",
    "https://openalex.org/W4311415873",
    "https://openalex.org/W2129250947",
    "https://openalex.org/W4385570337",
    "https://openalex.org/W2959300817",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2621133045",
    "https://openalex.org/W4379933553",
    "https://openalex.org/W171636962",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4224276467",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3037576147",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W2150286230",
    "https://openalex.org/W4386566747",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3121370741",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4309083387",
    "https://openalex.org/W4385570305",
    "https://openalex.org/W4289389259",
    "https://openalex.org/W2129069237",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W4312051726",
    "https://openalex.org/W3217600415",
    "https://openalex.org/W3103380855",
    "https://openalex.org/W3132732624",
    "https://openalex.org/W4386566672",
    "https://openalex.org/W2963511515",
    "https://openalex.org/W4385573391",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2138615112",
    "https://openalex.org/W2885195348",
    "https://openalex.org/W2922338294",
    "https://openalex.org/W3147229768",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4281690218",
    "https://openalex.org/W4383469363",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W4312122288",
    "https://openalex.org/W4385571995",
    "https://openalex.org/W4385570313",
    "https://openalex.org/W4205897796",
    "https://openalex.org/W3158661162",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4385572150",
    "https://openalex.org/W3172545452",
    "https://openalex.org/W2152180407",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4298289240",
    "https://openalex.org/W2976420234",
    "https://openalex.org/W4385573970",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4385574175",
    "https://openalex.org/W2133458109",
    "https://openalex.org/W1493526108",
    "https://openalex.org/W4385574363",
    "https://openalex.org/W4308623137",
    "https://openalex.org/W4385573819",
    "https://openalex.org/W3173436521",
    "https://openalex.org/W4385572579",
    "https://openalex.org/W4385571952",
    "https://openalex.org/W3045464143",
    "https://openalex.org/W4385570806",
    "https://openalex.org/W2173681125",
    "https://openalex.org/W3195129912",
    "https://openalex.org/W2952478253",
    "https://openalex.org/W3176380929",
    "https://openalex.org/W2969788869",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3155457426",
    "https://openalex.org/W4385571773",
    "https://openalex.org/W3128419247",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4378765257",
    "https://openalex.org/W4385572717",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385572554",
    "https://openalex.org/W4327486712",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W2963587345",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W4385571755",
    "https://openalex.org/W4385571148",
    "https://openalex.org/W4221142221",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4306802991",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3037013468",
    "https://openalex.org/W4386576685"
  ],
  "abstract": "In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation. Our study addresses this gap by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion, our framework also provides the capability to generate content relevant to the identified topics. This dual functionality allows users to efficiently produce highly clustered topics and related content simultaneously. DeTiME’s potential extends to generating clustered embeddings as well. Notably, our proposed framework proves to be efficient to train and exhibits high adaptability, demonstrating its potential for a wide array of applications.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9040–9057\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder\nbased LLM\nWeijie Xu, Wenxiang Hu, Fanyou Wu, Srinivasan H. Sengamedu\nAmazon\nweijiexu@amazon.com\nAbstract\nIn the burgeoning field of natural language\nprocessing, Neural Topic Models (NTMs) and\nLarge Language Models (LLMs) have emerged\nas areas of significant research interest. De-\nspite this, NTMs primarily utilize contextual\nembeddings from LLMs, which are not optimal\nfor clustering or capable for topic generation.\nOur study addresses this gap by introducing a\nnovel framework named Diffusion-Enhanced\nTopic Modeling using Encoder-Decoder-based\nLLMs (DeTiME). DeTiME leverages Encoder-\nDecoder-based LLMs to produce highly clus-\nterable embeddings that could generate topics\nthat exhibit both superior clusterability and en-\nhanced semantic coherence compared to exist-\ning methods. Additionally, by exploiting the\npower of diffusion, our framework also pro-\nvides the capability to generate content relevant\nto the identified topics. This dual functionality\nallows users to efficiently produce highly clus-\ntered topics and related content simultaneously.\nDeTiME’s potential extends to generating clus-\ntered embeddings as well. Notably, our pro-\nposed framework proves to be efficient to train\nand exhibits high adaptability, demonstrating\nits potential for a wide array of applications.\n1 Introduction\nTopic modeling methods, such as (Blei et al.,\n2003), are unsupervised approaches for discov-\nering latent structures in documents and achiev-\ning great performance (Blei et al., 2009). These\nmethods take a list of documents as input, gen-\nerate a defined number of topics, and can further\nproduce keywords and related documents for each\ntopic. In recent years, topic modeling methods have\nbeen widely used in various fields such as finance\n(Aziz et al., 2019), healthcare (Bhattacharya et al.,\n2017), education (Zhao et al., 2021a,b), marketing\n(Reisenbichler, 2019), and social science (Roberts\net al., 2013). With the development of Variational\nAutoencoder (V AE) (Kingma and Welling, 2013),\nthe Neural Topic Model (Miao et al., 2018; Dieng\net al., 2020) has attracted attention due to its better\nflexibility and scalability. The topic is generated\nthrough the reconstruction of the bag-of-word rep-\nresentations of the document (Miao et al., 2018).\nThe progress of large language model\n(LLM) (Vaswani et al., 2017; Radford et al.,\n2019) brings significant advancements in the\nNLP community. Sentence embedding is the\nprocess of converting sentences into numerical\nvectors in a high-dimensional space. LLM-based\nsentence embedding has been applied to topic\nmodeling by using it to reconstruct bag of word\nrepresentation of documents (Bianchi et al.,\n2021a), to cluster document directly (Grootendorst,\n2022) or both (Han et al., 2023). Sentence\nembedding-based models have been shown to\nachieve high performance regarding coherence\nand diversity (Zhang et al., 2022). Embeddings\nwith higher clusterability are likely to perform\nwell in classification tasks. However, sentence\nembeddings are in general not perform well\nin clustering . The best performed sentence\nembedding has an average v-measure (Rosenberg\nand Hirschberg, 2007) below 0.44 even if it uses\nkmeans and set the cluster equal to the number\nof different labels (Muennighoff et al., 2022).\nThis means that their clusterability can be even\nlower when the latent dimension increases. Lastly,\nlanguage modeling is a powerful generative\ntool (Brown et al., 2020). While topic modeling\nhas been utilized for generation (Wang et al.,\n2019), its integration with Large Language Models\n(LLMs) for generation remains less explored.\nIn this study, we introduce DeTiME, an innova-\ntive topic modeling framework that exploits the ca-\npabilities of the encoder-decoder Large Language\nModel (LLM). Specifically, we design a task to\ntrain an adapted encoder-decoder LLM, as depicted\nin Figure 2. We generate an embedding using\nthis architecture, which exhibits high clusterability\n9040\ncompared to established models as illustrated in\nFigure 1. Furthermore, we design a topic modeling\napproach using the last hidden layer of our modi-\nfied LLM encoder as input. This technique notably\noutperforms standard methods across all pertinent\nmetrics. Additionally, we leverage diffusion and\nour proposed framework to generate relevant docu-\nments. Our major contributions are as follows:\n1. We modify the encoder-decoder LLM and de-\nsign a task to create an embedding ideal for\ntopic modeling, even using a smaller model.\n2. The fabricated embeddings outperform exist-\ning methods in terms of clusterability\n3. We devise a topic modeling method based on\nthe embedding that achieves superior results\nin both clusterability and semantic coherence,\ncompared to the relevant topic modeling meth-\nods.\n4. We demonstrate the ability to produce relevant\ncontent based on this model by harnessing dif-\nfusion, indicating potential practical applica-\ntions.\n5. Our framework exhibits flexibility as it can\nbe seamlessly adapted to various encoder-\ndecoder LLMs and neural topic modeling\nmethods, broadening its applicability in the\nfield.\nBy documenting detailed methodology and empir-\nical results, we aim to inspire further research in\nthis domain, and provide a strong foundation for\nfuture work on topic modeling and LLMs.\n2 Related work\n2.1 Language Modeling\nRecent transformer-based models, such as\nBERT (Devlin et al., 2019), GPT-3 (Brown et al.,\n2020), and GPT-4 (OpenAI, 2023) have achieved\nunmatched performance in numerous language\ntasks. Utilizing self-attention mechanisms, they\ncapture context from both past and future tokens,\ngenerating coherent text. These rapidly evolving\nLarge Language Models (LLMs) carry significant\nimplications for diverse sectors and society.\nT5 (Raffel et al., 2020) treats every NLP task as a\ntext-to-text problem, using a standard format with\ninput and output as text sequences. It employs\nan encoder-decoder framework and is pretrained\nFigure 1: A summary of a few of our findings: (1)\nOur embeddings outperform the best clusterable meth-\nods (selected from (Muennighoff et al., 2022)). (2)\nThe same framework with a slightly different finetuned\ntask(DeTiME Training) does not perform well. (3)\nWhen compressed, our embeddings excel in higher di-\nmensions, making them ideal for topic modeling. De-\ntailed settings is in Appendix E.\non extensive datasets. FlanT5 (Chung et al.,\n2022) enhances T5 by finetuning instructions\nacross multiple datasets. Compared to encoder\nonly (Bert) or decoder only model(GPT), encoder-\ndecoder models such as FlanT5 allow the encoder\nto extract vital input information for output\ngeneration (Rothe et al., 2020).\nPrefix tuning (Li and Liang, 2021) modifies a\nfixed-length \"prefix\" of parameters prepended to\nthe input during fine-tuning, significantly reducing\nthe number of parameters required. This efficiency\ndoesn’t compromise performance; it often matches\nor surpasses traditional fine-tuning methods across\nvarious NLP tasks. The technique enables the\nmodel to learn task-specific initial hidden states\nfor LLM, steering the generation process appropri-\nately without hindering the model’s generality due\nto the fine-tuning task.\n2.2 Sentence Embedding\nContextual embeddings aim to encode sentence\nsemantics in a machine-readable format. Word em-\nbeddings like Word2Vec (Mikolov et al., 2013)\n9041\nand GloVe (Pennington et al., 2014) capture word-\nlevel meaning but struggle with larger text struc-\ntures. Advanced models like the Universal Sen-\ntence Encoder (USE) (Cer et al., 2018) and In-\nferSent (Conneau et al., 2018) were developed\nto better capture sentence nuances. USE employs\ntransformer or Deep Averages Networks, while\nInferSent uses a bidirectional LSTM with max\npooling. Sentence-BERT (Reimers and Gurevych,\n2019) utilizes siamese BERT-Networks. However,\nthese models often struggle to capture context-\ndependent sentence meanings, resulting in lower\nclusterability. This might be due to their reliance\non contrastive loss on sentence pairs, which might\nfocus on specific similarities rather than the overall\nsemantic relationship.\n2.3 Topic Modeling\nThe Neural Topic Model (NTM) (Miao et al., 2016)\nemploys variational inference but struggles with\nsemantics and interpretability, while the Embed-\nding Topic Model (ETM) (Dieng et al., 2019) uses\npre-trained word embeddings to capture semantics.\nHowever, NTMs rely on bag-of-word representa-\ntions, limiting their ability to capture document\nsemantics effectively.\nThe Contextual Topic Model (CTM) (Bianchi\net al., 2021a) uses sentence embeddings and bag\nof words as input to reconstruct bag of words em-\nbeddings, while BERTopic (Grootendorst, 2022)\ncombines sentence embedding and clustering tech-\nniques like UMAP and HDBSCAN for topic gen-\neration. Other models (Han et al., 2023) use both\nclustering techniques and reconstruction to create\nhigh-quality topics. Nonetheless, contextual em-\nbedding based topic modeling methods lack a re-\nconstruction process or only reconstruct bag of\nwords representations. These disadvantages limit\nits ability to generate relevant content. We exam-\nined other related works in Appendix H\n2.4 Diffusion\nDrawing inspiration from non-equilibrium ther-\nmodynamics, the diffusion model adds noise to\nthe data distribution in a forward process and\nlearns a reverse denoising process (Sohl-Dickstein\net al., 2015). (Song and Ermon, 2020) further ap-\nplied this for high-quality image generation, com-\nparable to leading likelihood-based models and\nGANs (Goodfellow et al., 2014), but with more\nstable training and generation due to iterative diffu-\nsion.\nDenoising Diffusion Probabilistic Models\n(DDPM) (Ho et al., 2020) have garnered attention\nfor generating high-quality samples sans adversar-\nial training, sometimes surpassing other generative\nmodels. Speedier sampling was achieved in (Song\net al., 2022) with denoising diffusion implicit\nmodels. The success of image generation models\nlike CLIP (Radford et al., 2021), Stable Diffusion\n(Rombach et al., 2022), and Midjourney (Oppen-\nlaender, 2022) leveraged such diffusion-based\nmethods. Their use extends to NLP tasks including\nnatural language generation, sentiment analysis,\nand machine translation (Zou et al., 2023). It has\nalso demonstrated that the diffusion model is able\nto generate high-quality text from noise samples in\nthe continuous embedding space(Li et al., 2022;\nGong et al., 2023; Gao et al., 2022; Lin et al.,\n2023b). Yet, diffusion hasn’t been used for topic\nmodeling as a content generation tool.\n3 Methods\nThe goal of this paper is to create a framework that\nleverages encoder-decoder LLM to generate topics\nthat is highly clusterable and able to generate topic\nrelated sentence. To achieve that, we need to create\nan embedding that could be used to generate text\nas well as be highly clusterable. Thus, we designed\na specific task and dataset for our use case. We\nadd CNN encoder and decoder on top of FlanT5 to\ngenerate that can easily fit into neural topic mod-\neling for further dimension reduction. We further\ndesign a variational autoencoder to take the output\nof the CNN encoder as input and generate topics\nand reconstruct embeddings. This is achieved by\n2 autoencoders. The first autoencoder is a varia-\ntional autoencoder which generates topic distribu-\ntion and reconstructs bag of words representations.\nTo reconstruct the embeddings from enc2. We use\nanother autoencoder to generate embeddings from\ntopic distribution and reconstructed bag of words.\nThe detailed structure and name are in Figure 3.\nWe do not train or finetune FlanT5 and CNN dur-\ning the topic modeling process which makes our\nmethods cost-effective. We then leverage diffusion\nto generate high quality text that represents the\ndocument.\nThis section contains four components. First, we\npresent the dataset and define the finetuned task.\nSecond, we elaborate on our modified FlanT5 and\nthe fine-tuning strategy. The third component intro-\nduces a variational autoencoder designed for topic\n9042\nFigure 2: DeTiME framework. We have 4 encoders and 4 decoders. enc1 and enc2 are compressing the input\ndocument to the lower dimension. enc3 is to construct topic distribution. dec1 is to reconstruct bag of words\nrepresentations. enc4 is to extract the hidden dimension from the reconstructed bag of words. dec2, dec3 and dec4\nis to reconstruct/rephrase the input document. In our method, we name the number of dimensions for embedding\nDtoken and maximum sequence length N1. The dimension of the compressed vector is Dembed. The number of\ntopics equals T. The dimension of vocabulary is NBoW . The dimension of topic embeddings is Dtopic.\nmodeling and generation. Finally, we utilize dif-\nfusion to generate content relevant to the derived\ntopics.\n3.1 Tasks and Finetune Dataset\nTo achieve effective topic modeling methods, we\naim to generate embeddings that are highly cluster-\nable and capable of generating document-relevant\ntopics. We utilize a paraphrase dataset in which\nthe input and output sentences are equivalent in\nmeaning. Such equivalent sentences should be-\nlong to similar topics, thereby aiding us in gen-\nerating similar sentences. In contrast to methods\nthat use the same sentence for both input and out-\nput, our task assists the language model in learning\nthe semantic meaning of sentences rather than sim-\nply memorizing the embeddings. As illustrated in\nFigure. 1, the DeTiME-training model represents\nthe model generated by the task where the input\nand output are identical contents. As you can see,\nthe clusterability of this method is substantially\nlower than ours. Thus, rephrase task is effective\nto generate clusterable contents. Moreover, the\nparaphrase task is not sufficiently easy (Vahtola\net al., 2022) and is less likely to impair the utility\nof the language model. We concatenate similar\nsentence pairs from the STS benchmark, a dataset\nfor comparing meaning representations, to form\nour dataset (Agirre et al., 2012, 2013, 2014, 2015,\n2016). We select pairs with scores above 80 per-\ncent of the maximum, yielding a total of 22,278\npairs. This dataset addresses the limitations of ex-\nisting paraphrase datasets, which are either domain-\nspecific (Dolan and Brockett, 2005; Gohsen et al.,\n2023), or generated by potentially unreliable lan-\nguage models (Shumailov et al., 2023). Our com-\nposite dataset is diverse, including data from news,\ncaptions, and forums.\n3.2 Modified Encoder Decoder LLM\nThe motivation for this nested autoencoder struc-\nture stems from the limitation of existing sentence\nembeddings, which struggle to reconstruct sen-\ntences as they are primarily trained using con-\ntrastive learning (Reimers and Gurevych, 2019)\nrather than reconstruction. In other words, simi-\nlar sentences are distributed close to each other in\nthe learned embedded vector space. We choose an\nencoder-decoder model due to its ability to preserve\nessential information through encoding process.\nSpecifically, encoder-decoder approaches, like T5,\nencapsulate vital information in the encoder’s final\nhidden state. We can compress this final hidden\nstate to create our embeddings. FlanT5 (Chung\n9043\net al., 2022) outperforms T5 in standard tasks by\nleveraging a (Wei et al., 2023) and instruction\nfine-tuning (Chung et al., 2022). We believe that\nthe final hidden layer of a fine-tuned FlanT5 can\nrepresent the input information.\nThe purpose of CNN is to compress output from\nFlanT5 encoder to create embeddings for topic\nmodeling as illustrated in Append F. Using the\nencoder output as an embedding leads to exces-\nsive length and dimensionality, causing sparsely\ndistributed vectors, poor clusterability, and issues\nin downstream tasks like topic modeling. To ad-\ndress this, we incorporate a variational autoencoder\nto reconstruct FlanT5’s final encoder hidden layer.\nWe trained MLP, RNN, and CNN-based autoen-\ncoders, but MLP introduced too many parameters\nand underperformed. LSTM, bidirectional LSTM,\nand GRU (Sherstinsky, 2020), with varied attention\nschemes (Xia et al., 2021), mostly yielded empty\nresults or identical output embeddings, likely due\nto the FlanT5 encoder’s non-sequential informa-\ntion processing. Applying a 1D convolution on\nthe sequence dimension allowed for dimensionality\nreduction, with nearby embeddings showing high\ncorrelation, suggesting possible compression using\na convolutional network on the sequence side. We\ncan adapt the same framework to other exist-\ning encoder decoder LLM such as BART (Lewis\net al., 2019).\nWe utilize Parameter Efficient Fine-tuning\n(PEFT) because it reduces the number of param-\neters to be fine-tuned, making the process more\nefficient and often yielding comparable or even su-\nperior performance to traditional fine-tuning (Liu\net al., 2022). We adopt prefix fine-tuning (Li and\nLiang, 2021) in our work. During fine-tuning, we\ntrain both prefix fine-tuning related parameters and\nthe CNN-based autoencoder for the paraphrase\ntasks. We then use the output from the CNN-based\nautoencoder’s encoder for downstream topic mod-\neling tasks. In our experiment, we use a relatively\nsmall model FlanT5 base (248M parameters) to\nillustrate the effectiveness of our framework.\n3.3 V AE structure for topic modeling\nOur V AE serves two purposes. First, it generates\na highly clusterable topic distribution. Second, it\nreconstructs the output of the CNN encoder e, en-\nabling it to be input into the decoder of the CNN\nautoencoder. Prior research (Srivastava and Sutton,\n2017) demonstrated that a Variational Autoencoder\n(V AE) aiming to reconstruct a bag of words pro-\nduces high-quality topic embeddings. Our V AE has\ntwo encoders and two decoders. enc3 is used to\nencode the output of the CNN encoder ( e) into\na topic distribution t. enc3 has two parts: the\nfirst is a multi-layer perceptron (MLP) that maps\nthe input to a lower dimension, and the second\nconsists of two MLPs to generate the mean and\nthe log of the standard deviation vector of size T:\nµ,log(σ) = enc3(e). We sample a latent repre-\nsentation using the mean and standard deviation:\nη ∼ N(µ,σ), and apply a softmax function to\ngenerate the topic distribution t= softmax(η).\nThe dec3 is used to decode the topic distribu-\ntion tinto a bag-of-words representation X′. Ex-\nisting research (Dieng et al., 2020) shows that\ntopic-word similarity matrix offers better qual-\nity in reconstructions. The decoder consists of\ntwo matrices. We use a vocabulary embedding\nmatrix eV ∈RDTopic ×NBoW , where DTopic rep-\nresents the dimension of word embeddings and\nNBoW represents the dimension of the vocabulary.\nThe decoder ϕ learns a topic embedding matrix\neT ∈RT×DTopic . The topic-to-word distribution is\ndenoted as\nE = softmax(eT eT\nV ) (1)\nX\n′\n= t×E (2)\nHere, X′represents the reconstructed bag of words.\nThe product of the generated topic distribution and\nthis matrix Eyields a bag-of-words reconstruction.\nThe enc4 is a neural network that encodes the\ngenerated bag of words back to a vector t′, hav-\ning the same dimension as the topic embeddings\ndimension: t′ = enc4(X). We add residual con-\nnections between two compressed vectors and use\na neural network to generate input embeddings:\ne′= dec4(t+ t′) (3)\nIt’s necessary to reconstruct input embeddings\n(e) to be fed into the decoder to reconstruct the\nrephrased input sentence. We believe that the re-\nconstructed bag of words can enhance the ability of\nsentence reconstruction. The residual connection\nhelps the model leverage both the reconstructed bag\nof words and topic distribution to reconstruct input\nembeddings. This simplifies our input embedding\nreconstruction and ensures that the topic embed-\ndings can capture semantic information from the\noutput of the CNN decoder e. Our V AE leverages\n9044\nonly bag of words representations and contextual\nembeddings. Our V AE can also take other con-\ntextual embeddings as input. Our loss function\nhas three components: the reconstruction loss for\nthe bag of words, the reconstruction loss for input\nembeddings using mean square error, and the KL\nDivergence for the normal distribution. The loss\nfor a single input eis as follows:\nL= −Xlog(X′) + (e−e′)2 + KL(t|N(µ,σ))\n(4)\n3.4 Diffusion for content generation\nOur pretrained model can compress the text and em-\nbed them in a low-dimensional space while keeping\nthe semantic information and high-quality cluster-\ning. It is natural to wonder if this pretrained model\ncan be used to generate topic-guided text. One of\nthe challenges is that the decompression process in\nthe pretrained model may induce noise, loss some\ninformation and thus the quality of the generated\ntext will be impacted. Specifically, the latent di-\nmension (i.e. the vector space of z′′ before the\ndec2 in Figure 3) is several orders of magnitude\nlower than the dimension of embedding vector e′\nin DeTiME. When we reconstruct text from latent\nvectors, it may hugely deviate from any reasonable\ninput for FlanT5 decoder dec3.\nTo overcome this, we have leveraged the dif-\nfusion models to denoise the generated text em-\nbedding from the topic modeling with structure as\nshown in Figure 3. It has demonstrated that the dif-\nfusion model is able to generate high-quality text\nfrom noise samples in the continuous embedding\nspace (Li et al., 2022; Gong et al., 2023; Gao et al.,\n2022; Lin et al., 2023b). In the training compo-\nnent, we employ a DDPM-scheduled Autoencoder\nwith residual connections as the diffusor (Ho et al.,\n2020) in the text embedding continuous space (i.e.\nthe space after enc2 in Figure 3) using the embed-\nded vectors obtained from the pretrained model.\nSpecifically, during the forward process, the Gaus-\nsian noises is gradually added to X0 according to\na variance schedule β1,...,β T , the noisy sample at\ntime step tis expressed as\nq(Xt|X0) = N\n(\nXt; √¯αtX0,\n√\n1 −¯αtI\n)\n(5)\nwhere ¯αt = Πt\ni=1αi with αi = 1−βi. Our diffusor\nis trained to minimize the squared error between\nthe predicted and true noise. The predicted noise\nz(Xt,t) at time step tis obtained by the diffusor\nas following:\nz1 = Xt + Sinusoid(t)\nz2 = FCCOMP\n1 (z1)\nz3 = FCCOMP\n2 (z2)\nz4 = FC3(z3)\nz5 = FCRECONST\n4 (z4 + z3)\nz(Xt,t) = FCRECONST\n5 (z5 + z2). (6)\nThis diffusor consists of 2 fully connected lay-\ners FCCOMP to compress the input and 2 fully-\nconnected layers FCRECONST to reconstruct. We\nalso add residual connections between compress\nand reconstruct layers. Similar to UNet (Ron-\nneberger et al., 2015), the Sinusoidal positional\nembeddings Sinusoid(t) is used to encode time.\nThen, in generating component, this trained dif-\nfusor is used to denoise the embedding after the\ndec2 in Figure 3. The intuition behind this denois-\ning process is as follows. The forward process of\ndiffusion itself is a process that converts the un-\nknown and complex data distribution into one (nor-\nmal distribution in our case) that is easy to sample\nfrom. By adding back the learned noise with small\niterative steps, we are able to take a sample from\nthe noise subspace (support a simple distribution)\nto the data subspace (support the unknown data\ndistribution). Similarly, for an embedding obtained\nfrom the topic modeling that deviates from the em-\nbedding distribution corresponding to the unknown\ninput data distribution, we should also be able to\ntake this embedding back to the area supporting the\noriginal embedding distribution.\n4 Experimental Results\n4.1 Topic Modeling\nDataset Our experiments are conducted on labeled\nbenchmark datasets for topic modeling: AgNews\n(Zhang et al., 2016), 20Newsgroups (Lang, 1995)\nand bbc-news (Greene and Cunningham, 2006).\nThe average document length varies from 38 to\n425. We use the text as it is for the contextual\nembedding generation. To get bag of words, we\nuse the word tokenizer from nltk to tokenize, re-\nmove digits and words with lengths less than 3,\nand remove stop words and words that appear less\nthan 10 time. Additional details on the dataset and\nplaces to download processed data are available in\nAppendix B.\n9045\nMethods Purity NMI Km-Purity Km-NMI diversity Cv\nETM 0.4677 ± 0.040.4677 ± 0.040.4677 ± 0.04 0.2502 ± 0.07 0.4063 ± 0.07 0.2400 ± 0.08 0.4177 ± 0.05 0.5594 ± 0.01\nGSM 0.2701 ± 0.02 0.0687 ± 0.03 0.3167 ± 0.03 0.1312 ± 0.03 0.2991 ± 0.01 0.3495 ± 0.01\nvONT 0.3727 ± 0.02 0.1604 ± 0.03 0.4941 ± 0.05 0.2688 ± 0.05 0.5937 ± 0.06 0.5151 ± 0.01\nNVDM 0.4254 ± 0.04 0.2373 ± 0.07 0.3768 ± 0.07 0.2138 ± 0.05 0.2633 ± 0.05 0.4715 ± 0.02\nZTM 0.3637 ± 0.003 0.1019 ± 0.003 0.3479 ± 0.003 0.1087 ± 0.001 0.6796 ± 0.03 0.6705 ± 0.02\nCTM 0.4307 ± 0.03 0.1641 ± 0.04 0.4191 ± 0.04 0.1819 ± 0.05 0.7198 ± 0.010.7198 ± 0.010.7198 ± 0.01 0.6966 ± 0.02\nDeTiME bow 0.3416 ± 0.004 0.1300 ± 0.009 0.5007 ± 0.03 0.2591 ± 0.02 0.5362 ± 0.04 0.7186 ± 0.004\nDeTiME resi 0.3239 ± 0.01 0.1098 ± 0.01 0.4230 ± 0.01 0.1741 ± 0.02 0.5802 ± 0.01 0.7435 ± 0.0020.7435 ± 0.0020.7435 ± 0.002\nDeTiME 0.4577 ± 0.03 0.2983 ± 0.030.2983 ± 0.030.2983 ± 0.03 0.5929 ± 0.040.5929 ± 0.040.5929 ± 0.04 0.3463 ± 0.050.3463 ± 0.050.3463 ± 0.05 0.6913 ± 0.02 0.7203 ± 0.01\nTable 1: The main results for all clusterability metrics, diversity, and coherence (Cv). The number of topics is 20.\nThe best and second-best scores of each dataset are highlighted in boldface and with an underline, respectively.\nThe result represents the average value obtained from three datasets, where each dataset was processed 10 times to\ncompute the mean and standard deviation.\nFigure 3: The diffusion framework based on the main\nframework in Figure 2. In the training component, a\nDDPM-scheduled Autoencoder with residual connec-\ntions diffusor is trained using the embedding vectors\nobtained from the enc2. In generating part, the trained\ndiffusor is used to denoise the embedding vectors trans-\nformed from the topic vectors hidden space before the\ntext generation. It’s important to note that we normal-\nized the hidden space before passing it to the dec2.\nBaseline Methods We compare with common\nNTM methods and contextual embedding based\nmethods. We explain the reasons for choos-\ning these methods in Appendix D. These meth-\nods include: NVDM (Wang and YANG, 2020),\nV AE architecture for topic modeling with the en-\ncoder is implemented by multilayer perceptron, the\nvariational distribution is a Gaussian distribution;\nGSM (Miao et al., 2018), an NTM replaces the\nDirichlet-Multinomial parameterization in LDA\nwith Gaussian Softmax; ETM (Dieng et al., 2020),\nan NTM model which incorporates word embed-\nding to model topics; vONT (Xu et al., 2023e),\na vMF based NTM where they set the radius of\nvMF distribution equal to 10; CTM (Bianchi et al.,\n2021b) trains a variational autoencoder to recon-\nstruct bag of words representation using both con-\ntextual embeddings as well as bag of words repre-\nsentation. ZTM (Bianchi et al., 2021b) is similar\nto CTM but only use contextual embeddings; De-\nTiME resi is the DeTiME model with out resid-\nual connections. The reconstruction of embedding\nis hugely dependent on the reconstructed bag of\nwords; DeTiME bow is the DeTiME model with-\nout reconstruction of bag of words and t′is used to\nrepresent topics.\nSettings In our experiment setting, The hyper-\nparameter setting used for all baseline models and\nDeTiME is the same as (Burkhardt and Kramer,\n2019). For neural topic modeling and our encoder\nand decoder, we use a fully-connected neural net-\nwork with two hidden layers of half of the hidden\ndimension and one quarter of hidden dimension\nand GELU (Hendrycks and Gimpel, 2023) as the\nactivation function followed by a dropout layer. We\nuse Adam (Kingma and Ba, 2017) as the optimizer\nwith learning rate 0.001 and use batch size 256.\nWe use (Smith and Topin, 2018) as scheduler and\nuse learning rate 0.001. We use 0.0005 learning\nrate for the DeTiME bow because the loss may\n9046\noverflow when the learning rate is 0.001. We use\nword embeddings (Mikolov et al., 2013) to rep-\nresent word embeddings on the dataset for vONT,\nETM, and DeTiME and keep it trainable for De-\nTiME. For vONT, we set the radius of the vMF\ndistribution equal to 10. For CTM and ZTM, we\nuse all-mpnet-base-v2 as our embeddings since it\nperforms the best in clusterability in Figure 1. We\nuse the same way to find key words as suggested\nby CTM. Our code is written in PyTorch and all the\nmodels are trained on AWS using ml.p2.8xlarge\n(NVIDIA K80). Detailed code implementations\nfor methods and metrics are in Appendix C\nEvaluation Metrics We measure the topic clus-\nterability, diversity, and semantic coherence of the\nmodel. To measure clusterability, we assign ev-\nery document the topic with the highest probability\nas the clustering label and compute Top-Purity\nand Normalized Mutual Information(Top-NMI) as\nmetrics (Nguyen et al., 2018) to evaluate alignment.\nBoth of them range from 0 to 1. A higher score\nreflects better clustering performance. We further\napply the KMeans algorithm to topic proportions\nz and use the clustered documents to report pu-\nrity(Km-Purity) and NMI Km-NMI (Zhao et al.,\n2020). We set the number of clusters to be the\nnumber of topics for the KMeans algorithm. Topic\ncoherence(Cv) uses the one-set segmentation to\ncount word co-occurrences and the cosine similar-\nity as the similarity measure. Compared to other\nmetrics, Cv is able to capture semantic coherence.\nWe only benchmark Cv because most of coherence\nmetrics are similar to each other (Lim and Lauw,\n2023). For diversity, we measure the uniqueness\nof the words across all topics divided by total key-\nwords. For each topic, we set the number of key-\nwords equal to 25. Furthermore, we run all these\nmetrics 10 times. We report averaged mean and\nstandard deviation. We also include evaluations on\nPerplexity in Appendix G\nResults The experiment shows that DeTiME out-\nperforms all other methods in NMI, Km-NMI, and\nKm-Purity, which underscores its ability to gener-\nate highly clusterable topic distributions . Fur-\nthermore, DeTiME has the second highest scores\nin coherence(The highest score is also a DeTiME\nvariation), validating the exceptional semantic co-\nherence of topics generated from our methods.\nObservations reveal that the CTM and DeTiME’s\nhigh diversity scores highlight the benefit of incor-\nporating bag of words inputs, enhancing diversity\nperformance. By eliminating the bag of words\nreconstruction components, we found a decrease\nin diversity and clusterability, indicating the im-\nportance of this component in boosting purity and\nNMI. When we removed the residual connection,\nwe observed an improvement in coherence but a de-\ncrease in clusterability. This trade-off suggests that\nthe absence of a residual connection may prevent\nthe topic distribution from effectively capturing the\ninformation from embeddings, thus reducing clus-\nterability. DeTiME resi performs better than ZTM\nin clusterability related metrics, which confirms\nthat our embedding is more clusterable than ex-\nisting sentence embeddings.\n4.2 Diffusion for content generation\nTo evaluate how the diffusor improves the qual-\nity of the generated text, we compared the gener-\nated text before and after the diffusion. Specifi-\ncally, we utilized the Flesch Reading Ease (FRE),\nFlesch-Kincaid Grade Level (FKGL), and Dale-\nChall Readability Score (DCRS) to measure the\nreadability of the generated text before and after\nthe diffusion (Goldsack et al., 2022). In general,\na higher FRE (lower FKGL and DCRS) indicates\nthat the text is easier to read. In this experiment, we\ngenerated 1000 random topic vectors and passed\nthem to dec2, then the denoising process is fol-\nlowed to generate text. The main results are shown\nin Table 2. As observed, after the denoising pro-\ncess, the FRE increases significantly across all\ndatasets, which indicates that diffusion makes the\ncontent easier to understand. Meanwhile, the value\nof FKGL and DCRS decreases from T = 500 to\nT = 1000. One of the reasons for the low score\nof FKGL and DCRS at T = 0 is that some of the\nsamples contain only repeated words, making them\neasy to understand. Overall, after more steps in dif-\nfusion, the generated text becomes more readable\nfor a lower grade. This experiment demonstrates\nthat our generated content achieves higher read-\nability, indicating the potential of our frame-\nwork to generate topic-relevant content.\nHuman Evaluation To ensure the generated\ncontent is valuable to humans, a human evalua-\ntion was conducted with regard to the text gener-\nated after diffusion, as seen in Figure 3. In this\nevaluation, we generated 400 pieces of text. Each\npiece was evaluated for fluency, grammar, and re-\ndundancy by three different human annotators, as\nsuggested by (Celikyilmaz et al., 2021). We com-\n9047\nDatasets 20Newsgroups bbc-news AgNews\nTime point T = 0 T = 500 T = 1000 T = 0 T = 500 T = 1000 T = 0 T = 500 T = 1000\nFRE -25.9600 51.1390 54.2467 6.8600 36.5889 60.9407 36.6200 64.1707 63.1074\nFKGL 53.2000 10.7017 9.8955 30.2000 12.6860 9.1856 8.4000 9.0876 8.6781\nDCRS 7.3500 8.4758 7.8822 4.0100 8.3304 8.2010 66.8500 8.1890 8.1059\nTable 2: The average readability scores at different time steps during the denoising process. A general increase in\nreadability is observed.\npared our results with a baseline through t-tests\nand found that the generated text exhibited fluency\nand grammatical correctness with statistical signifi-\ncance (p< 1e−14). This demonstrates that our\ngenerated contents are of high quality. More de-\ntails about the survey setup, results, and examples\nof generated text can be found in Appendix A.\n5 Conclusion and Future Work\nWe have developed a framework DeTiME for gen-\nerating highly clusterable embeddings, leveraging\nthe strengths of paraphrase tasks, FlanT5, and CNN.\nIn addition to this, we introduced a variational au-\ntoencoder structure capable of reconstructing em-\nbeddings while simultaneously producing highly\ncoherent, diverse, and clusterable topics. Our de-\nsign incorporates a diffusion process for generating\ncontent that provides representative depictions of\nvarious topics. The flexibility of our embedding\ngeneration structure allows for easy adaptation to\nother encoder-decoder language model architec-\ntures, eliminating the need for retraining the en-\ntire framework, thereby ensuring cost-effectiveness.\nAdditionally, our variational autoencoder structure\nis versatile, and capable of being applied to any\ncontextual embeddings. Other methods could fur-\nther improve with larger LLM.\nMoving forward, we aim to further improve the\nperformance of our embeddings by training on\nlarger models such as Flan-T5-XL. Benchmark-\ning other Pre-training with Fine-Tuning (PEFT)\nmethods, such as LORA, may also enhance our\nsystem’s performance. Given the high clusterabil-\nity of our embeddings, we plan to extend our work\nto semi-supervised document classification (Xu\net al., 2023b,c; Balepur et al., 2023; Lin et al.,\n2023a). This framework could be applied to iden-\ntify the most representative documents within ex-\ntensive document collections. This functionality\ncould make our model suitable for generation topic\nguided generation (Xu et al., 2023a) Finally, we en-\nvisage utilizing this framework to generate superior\nsummarizations for large documents. This could be\nachieved by training a decoder for summarization,\ngenerating a summarization for each topic, and\nsubsequently concatenating them. This framework\ncan also be extended to hierarchical topic model-\ning (Chen et al., 2023; Shahid et al., 2023; Eshima\nand Mochihashi, 2023), mitigate data sparsity for\nshort text topic modeling (Wu et al., 2022), gen-\nerate topic-relevant and coherent long texts (Yang\net al., 2022), and construct a network of topics to-\ngether with meaningful relationships between them\n(Byrne et al., 2022).\n9048\n6 Limitations\nWhile our study has made significant strides in\nits domain, we acknowledge certain limitations\nthat present themselves as opportunities for future\nresearch and optimization. Firstly, we have not\nyet benchmarked our model with other encoder-\ndecoder frameworks such as BART, or with alter-\nnative PEFT methods like LORA, leaving room\nfor potential performance enhancement. We be-\nlieve that the diversity could further improve with\ndiversity aware coherence loss (Li et al., 2023).\nSecondly, our model has yet to reach the full poten-\ntial of FlanT5 due to current model size constraints.\nThis implies that scaling up the model could fur-\nther improve its performance. Thirdly, we have\nnot fine-tuned the number of dimensions for the\nCNN encoder output or explored structures beyond\nbasic CNN, LSTM, and MLP, both of which could\nenhance our current performance. Fourthly, We\nnoted a relatively high variance in DeTiME’s per-\nformance, we interpret this as a consequence of the\ncomplicated autoencoder structure. Lastly, we have\nnot benchmarked all coherence metrics. Though\nmany metrics have similarities and some may not\nconsider semantic word meaning, a more extensive\nbenchmarking could provide a richer evaluation\nof our approach. Despite these limitations, each\nof these points serves as a promising direction for\nfuture research, thereby helping to further elevate\nour model’s capabilities.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, German Rigau, Larraitz Uria, and Janyce\nWiebe. 2015. SemEval-2015 task 2: Semantic tex-\ntual similarity, English, Spanish and pilot on inter-\npretability. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation (SemEval 2015),\npages 252–263, Denver, Colorado. Association for\nComputational Linguistics.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,\nMona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,\nRada Mihalcea, German Rigau, and Janyce Wiebe.\n2014. SemEval-2014 task 10: Multilingual semantic\ntextual similarity. In Proceedings of the 8th Interna-\ntional Workshop on Semantic Evaluation (SemEval\n2014), pages 81–91, Dublin, Ireland. Association for\nComputational Linguistics.\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,\nAitor Gonzalez-Agirre, Rada Mihalcea, German\nRigau, and Janyce Wiebe. 2016. SemEval-2016\ntask 1: Semantic textual similarity, monolingual\nand cross-lingual evaluation. In Proceedings of the\n10th International Workshop on Semantic Evaluation\n(SemEval-2016), pages 497–511, San Diego, Califor-\nnia. Association for Computational Linguistics.\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012. SemEval-2012 task 6: A\npilot on semantic textual similarity. In *SEM 2012:\nThe First Joint Conference on Lexical and Compu-\ntational Semantics – Volume 1: Proceedings of the\nmain conference and the shared task, and Volume\n2: Proceedings of the Sixth International Workshop\non Semantic Evaluation (SemEval 2012), pages 385–\n393, Montréal, Canada. Association for Computa-\ntional Linguistics.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\ntask: Semantic textual similarity. In Second Joint\nConference on Lexical and Computational Semantics\n(*SEM), Volume 1: Proceedings of the Main Confer-\nence and the Shared Task: Semantic Textual Similar-\nity, pages 32–43, Atlanta, Georgia, USA. Association\nfor Computational Linguistics.\nMelissa Ailem, Bowen Zhang, and Fei Sha. 2019. Topic\naugmented generator for abstractive summarization.\nArXiv, abs/1908.07026.\nSaqib Aziz, Michael Dowling, Helmi Hammami, and\nAnke Piepenbrink. 2019. Machine learning in fi-\nnance: A topic modeling approach. European Finan-\ncial Management, n/a(n/a).\nNishant Balepur, Shivam Agarwal, Karthik Venkat Ra-\nmanan, Susik Yoon, Diyi Yang, and Jiawei Han. 2023.\nDynaMiTE: Discovering explosive topic evolutions\nwith user guidance. In Findings of the Association\n9049\nfor Computational Linguistics: ACL 2023, pages 194–\n217, Toronto, Canada. Association for Computational\nLinguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nMoumita Bhattacharya, Claudine Jurkovitz, and Hagit\nShatkay. 2017. Identifying patterns of associated-\nconditions through topic models of electronic medi-\ncal records. CoRR, abs/1711.10960.\nFederico Bianchi, Silvia Terragni, and Dirk Hovy.\n2021a. Pre-training is a hot topic: Contextualized\ndocument embeddings improve topic coherence. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 759–766,\nOnline. Association for Computational Linguistics.\nFederico Bianchi, Silvia Terragni, Dirk Hovy, Debora\nNozza, and Elisabetta Fersini. 2021b. Cross-lingual\ncontextualized topic models with zero-shot learning.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1676–1683, Online.\nAssociation for Computational Linguistics.\nDavid M. Blei, Thomas L. Griffiths, and Michael I. Jor-\ndan. 2009. The nested chinese restaurant process\nand bayesian nonparametric inference of topic hierar-\nchies.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. the Journal of\nmachine Learning research, 3:993–1022.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nSophie Burkhardt and Stefan Kramer. 2019. Decoupling\nsparsity and smoothness in the dirichlet variational\nautoencoder topic model. Journal of Machine Learn-\ning Research, 20(131):1–27.\nCiarán Byrne, Danijela Horak, Karo Moilanen, and\nAmandla Mabona. 2022. Topic modeling with topo-\nlogical data analysis. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11514–11533, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n2021. Evaluation of text generation: A survey.\nDaniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal sentence encoder.\nHeGang Chen, Pengbo Mao, Yuyin Lu, and Yanghui\nRao. 2023. Nonlinear structural equation model\nguided Gaussian mixture hierarchical topic modeling.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 10377–10390, Toronto, Canada.\nAssociation for Computational Linguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic\nBarrault, and Antoine Bordes. 2018. Supervised\nlearning of universal sentence representations from\nnatural language inference data.\nJeremy Costello and Marek Reformat. 2023a. Rein-\nforcement learning for topic models. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 4332–4351, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nJeremy Costello and Marek Z Reformat. 2023b. Rein-\nforcement learning for topic models. arXiv preprint\narXiv:2305.04843.\nPeng Cui and Le Hu. 2021. Topic-guided abstractive\nmulti-document summarization. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1463–1472, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nAdji B. Dieng, Francisco J. R. Ruiz, and David M. Blei.\n2019. Topic modeling in embedding spaces.\nAdji B Dieng, Francisco JR Ruiz, and David M Blei.\n2020. Topic modeling in embedding spaces. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:439–453.\nThanh-Nam Doan and Tuan-Anh Hoang. 2021. Bench-\nmarking neural topic models: An empirical study.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 4363–4368.\n9050\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nShusei Eshima and Daichi Mochihashi. 2023. Scale-\ninvariant infinite hierarchical topic model. In Find-\nings of the Association for Computational Linguistics:\nACL 2023, pages 11731–11746, Toronto, Canada. As-\nsociation for Computational Linguistics.\nZhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang\nZhang, Jiang Bian, and Linli Xu. 2022. Difformer:\nEmpowering diffusion model on embedding space for\ntext generation. arXiv preprint arXiv:2212.09412.\nMarcel Gohsen, Matthias Hagen, Martin Potthast, and\nBenno Stein. 2023. Paraphrase acquisition from im-\nage captions. In Proceedings of the 17th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 3348–3358, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nTomas Goldsack, Zhihao Zhang, Chenghua Lin, and\nCarolina Scarton. 2022. Making science simple: Cor-\npora for the lay summarisation of scientific literature.\nShansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,\nand Lingpeng Kong. 2023. Diffuseq: Sequence to\nsequence text generation with diffusion models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative\nadversarial networks.\nDerek Greene and draig Cunningham. 2006. Practical\nsolutions to the problem of diagonal dominance in\nkernel document clustering. In Proceedings of the\n23rd International Conference on Machine Learn-\ning, ICML ’06, page 377–384, New York, NY , USA.\nAssociation for Computing Machinery.\nMaarten Grootendorst. 2022. Bertopic: Neural topic\nmodeling with a class-based tf-idf procedure.\nAmulya Gupta and Zhu Zhang. 2021. Vector-\nquantization-based topic modeling. ACM Trans. In-\ntell. Syst. Technol., 12(3).\nSungwon Han, Mingi Shin, Sungkyu Park, Changwook\nJung, and Meeyoung Cha. 2023. Unified neural topic\nmodel via contrastive learning and term weighting.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 1794–1809.\nDan Hendrycks and Kevin Gimpel. 2023. Gaussian\nerror linear units (gelus).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nDiederik P Kingma and Max Welling. 2013. Auto-\nencoding variational bayes.\nKen Lang. 1995. Newsweeder: Learning to filter net-\nnews. In Machine Learning Proceedings 1995, pages\n331–339. Elsevier.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\nJingling Li, Mozhi Zhang, Keyulu Xu, John P. Dick-\nerson, and Jimmy Ba. 2021. How does a neural\nnetwork’s architecture impact its robustness to noisy\nlabels?\nRaymond Li, Felipe Gonzalez-Pizarro, Linzi Xing,\nGabriel Murray, and Giuseppe Carenini. 2023.\nDiversity-aware coherence loss for improving neu-\nral topic models. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 1710–1722,\nToronto, Canada. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori Hashimoto. 2022. Diffusion-\nLM improves controllable text generation. In Ad-\nvances in Neural Information Processing Systems.\nJia Peng Lim and Hady Lauw. 2023. Large-scale corre-\nlation analysis of automated metrics for topic models.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 13874–13898, Toronto, Canada.\nAssociation for Computational Linguistics.\nYang Lin, Xin Gao, Xu Chu, Yasha Wang, Junfeng\nZhao, and Chao Chen. 2023a. Enhancing neural\ntopic model with multi-level supervisions from seed\nwords. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pages 13361–13377,\nToronto, Canada. Association for Computational Lin-\nguistics.\nZhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu,\nZhihao Fan, Chen Lin, Nan Duan, and Weizhu Chen.\n2023b. Text generation with diffusion language mod-\nels: A pre-training approach with continuous para-\ngraph denoise. In Proceedings of the 40th Interna-\ntional Conference on Machine Learning, ICML’23.\nJMLR.org.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\n2022. Few-shot parameter-efficient fine-tuning is\nbetter and cheaper than in-context learning.\nYishu Miao, Edward Grefenstette, and Phil Blunsom.\n2017. Discovering discrete latent topics with neu-\nral variational inference. In Proceedings of the 34th\n9051\nInternational Conference on Machine Learning, vol-\nume 70 of Proceedings of Machine Learning Re-\nsearch, pages 2410–2419. PMLR.\nYishu Miao, Edward Grefenstette, and Phil Blunsom.\n2018. Discovering discrete latent topics with neural\nvariational inference.\nYishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-\nral variational inference for text processing. In In-\nternational conference on machine learning, pages\n1727–1736. PMLR.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efficient estimation of word representa-\ntions in vector space.\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and\nNils Reimers. 2022. Mteb: Massive text embedding\nbenchmark. In Conference of the European Chapter\nof the Association for Computational Linguistics.\nDat Quoc Nguyen, Richard Billingsley, Lan Du, and\nMark Johnson. 2018. Improving topic models with\nlatent feature word representations.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hernández Ábrego, Ji Ma, Vincent Y . Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021a. Large dual encoders are generalizable\nretrievers.\nJianmo Ni, Gustavo Hernández Ábrego, Noah Constant,\nJi Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang.\n2021b. Sentence-t5: Scalable sentence encoders\nfrom pre-trained text-to-text models.\nOpenAI. 2023. Gpt-4 technical report.\nJonas Oppenlaender. 2022. The creativity of text-to-\nimage generation. In Proceedings of the 25th Inter-\nnational Academic Mindtrek Conference. ACM.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nReutterer Reisenbichler, M. 2019. Topic modeling in\nmarketing: recent advances and research opportuni-\nties. J Bus Econ, 89.\nM. Roberts, B. Stewart, D. Tingley, and E. Airoldi. 2013.\nThe structural topic model and applied social science.\nNeural Information Processing Society.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\n2015. U-net: Convolutional networks for biomedical\nimage segmentation.\nAndrew Rosenberg and Julia Hirschberg. 2007. V-\nmeasure: A conditional entropy-based external clus-\nter evaluation measure. In Proceedings of the 2007\nJoint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural\nLanguage Learning (EMNLP-CoNLL), pages 410–\n420, Prague, Czech Republic. Association for Com-\nputational Linguistics.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for se-\nquence generation tasks. Transactions of the Associ-\nation for Computational Linguistics, 8:264–280.\nSimra Shahid, Tanay Anand, Nikitha Srikanth, Sumit\nBhatia, Balaji Krishnamurthy, and Nikaash Puri.\n2023. HyHTM: Hyperbolic geometry-based hierar-\nchical topic model. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 11672–\n11688, Toronto, Canada. Association for Computa-\ntional Linguistics.\nAlex Sherstinsky. 2020. Fundamentals of recurrent\nneural network (RNN) and long short-term memory\n(LSTM) network. Physica D: Nonlinear Phenomena,\n404:132306.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin\nGal, Nicolas Papernot, and Ross Anderson. 2023.\nThe curse of recursion: Training on generated data\nmakes models forget.\nLeslie N. Smith and Nicholay Topin. 2018. Super-\nconvergence: Very fast training of neural networks\nusing large learning rates.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-\neswaranathan, and Surya Ganguli. 2015. Deep un-\nsupervised learning using nonequilibrium thermody-\nnamics.\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2022.\nDenoising diffusion implicit models.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding.\n9052\nYang Song and Stefano Ermon. 2020. Generative mod-\neling by estimating gradients of the data distribution.\nAkash Srivastava and Charles Sutton. 2017. Autoen-\ncoding variational inference for topic models. arXiv\npreprint arXiv:1703.01488.\nTeemu Vahtola, Mathias Creutz, and Jörg Tiedemann.\n2022. It is not easy to detect paraphrases: Analysing\nsemantic similarity with antonyms and negation us-\ning the new SemAntoNeg benchmark. In Proceed-\nings of the Fifth BlackboxNLP Workshop on Analyz-\ning and Interpreting Neural Networks for NLP, pages\n249–262, Abu Dhabi, United Arab Emirates (Hybrid).\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nBoyu Wang, Linhai Zhang, Deyu Zhou, Yi Cao, and\nJiandong Ding. 2023. Neural topic modeling based\non cycle adversarial training and contrastive learning.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 9720–9731, Toronto,\nCanada. Association for Computational Linguistics.\nWenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang,\nGuoyin Wang, Dinghan Shen, Changyou Chen, and\nLawrence Carin. 2019. Topic-guided variational au-\ntoencoders for text generation.\nXinyi Wang and YI YANG. 2020. Neural topic model\nwith attention for supervised learning. In Proceed-\nings of the Twenty Third International Conference on\nArtificial Intelligence and Statistics, volume 108 of\nProceedings of Machine Learning Research, pages\n1147–1156. PMLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nXiaobao Wu, Xinshuai Dong, Thong Nguyen, and\nAnh Tuan Luu. 2023. Effective neural topic mod-\neling with embedding clustering regularization.\nXiaobao Wu, Anh Tuan Luu, and Xinshuai Dong. 2022.\nMitigating data sparsity for short text topic modeling\nby topic-semantic contrastive learning. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 2748–2760,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nJun Xia, Yunwen Feng, Cheng Lu, Chengwei Fei, and\nXiaofeng Xue. 2021. Lstm-based multi-layer self-\nattention method for remaining useful life estimation\nof mechanical systems. Engineering Failure Analy-\nsis, 125:105385.\nChunpu Xu, Jing Li, Piji Li, and Min Yang. 2023a.\nTopic-guided self-introduction generation for social\nmedia users. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 11387–\n11402, Toronto, Canada. Association for Computa-\ntional Linguistics.\nWeijie Xu, Jay Desai, \"SHS\" Srinivasan Sengamedu,\nXiaoyu Jiang, and Francis Iannacci. 2023b. S2vntm:\nSemi-supervised vmp neural topic modeling. In\nICLR 2023 Workshop on Practical Machine Learning\nfor Developing Countries.\nWeijie Xu, Billy Jiang, Jay Desai, Bin Han, Fuqin Yan,\nand Francis Iannacci. 2023c. Kdstm: Neural semi-\nsupervised topic model-ing with knowledge distilla-\ntion. In ICLR 2022 Workshop on Practical Machine\nLearning for Developing Countries.\nWeijie Xu, Xiaoyu Jiang, Srinivasan H. Sengamedu,\nFrancis Iannacci, and Jinjin Zhao. 2023d. vontss:\nvmf based semi-supervised neural topic modeling\nwith optimal transport.\nWeijie Xu, Xiaoyu Jiang, Srinivasan Sengamedu Hanu-\nmantha Rao, Francis Iannacci, and Jinjin Zhao.\n2023e. vONTSS: vMF based semi-supervised neural\ntopic modeling with optimal transport. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 4433–4457, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nErguang Yang, Mingtong Liu, Deyi Xiong, Yujie Zhang,\nYufeng Chen, and Jinan Xu. 2022. Long text genera-\ntion with topic-aware discrete latent variable model.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n8100–8107, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nHaopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023. Dif-\nfuSum: Generation enhanced extractive summariza-\ntion with diffusion. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 13089–\n13100, Toronto, Canada. Association for Computa-\ntional Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2016.\nCharacter-level convolutional networks for text clas-\nsification.\nZihan Zhang, Meng Fang, Ling Chen, and Mohammad-\nReza Namazi-Rad. 2022. Is neural topic modelling\nbetter than clustering? an empirical study on cluster-\ning with contextual embeddings for topics. In North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nHe Zhao, Dinh Phung, Viet Huynh, Trung Le, and Wray\nBuntine. 2020. Neural topic model via optimal trans-\nport.\nJinjin Zhao, Kim Larson, Weijie Xu, Neelesh Gattani,\nand Candace Thille. 2021a. Targeted feedback gen-\neration for constructed-response questions. In AAAI\n2021 Workshop on AI Education.\n9053\nJinjin Zhao, Weijie Xu, and Candace Thille. 2021b.\nEnd-to-end question generation to assist formative\nassessment design for conceptual knowledge learning.\nIn AETS 2021.\nDeyu Zhou, Xuemeng Hu, and Rui Wang. 2020. Neural\ntopic modeling by incorporating document relation-\nship graph. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 3790–3796, Online. Association\nfor Computational Linguistics.\nHao Zou, Zae Myung Kim, and Dongyeop Kang. 2023.\nDiffusion models in nlp: A survey.\nAppendix\nA Qualitative study\nIn this appendix, we mainly discuss how we set\nup the qualitative survey for diffusion-based text\ngeneration.\nAs mentioned in the main content, we mainly\nmeasure the fluency, grammar, and redundancy of\nthe generated text. Based on this reference (Ce-\nlikyilmaz et al., 2021), we have designed the corre-\nsponding questions in Table. 3. For each question,\nfive answer options are listed from strong negative\nto strong positive, and a score is assigned to each\noption. In this survey, we have sampled 400 one-\nhot topic vectors and generated text following the\ngenerating component in fig. 3 for each datasets.\nWe then leverage the Amazon Mechanical Turk to\nevaluate the quality of each generated sentence. In\nthis process, We have requested three independent\nreviewers to mitigate the individual bias, and the\naverage score is calculated for each sample. The\nhistogram of the collected scores is shown in fig.4.\nAt the end, we have employed a t-test to evaluate\nif this survey is statistically significant. The null\nhypothesis has been tested against the one-sided al-\nternative that the mean of the population is greater\nthan 0 for fluency and grammar, and the null hy-\npothesis against the one-sided alternative that the\nmean of the population is less than 1 for redun-\ndancy. The p <1e−14 have been obtained and\nthus we can reject the null hypothesis for all of\nthem.\nWe use the ratings and word intrusion tasks as\nhuman evaluations of topic quality. We recruit\ncrowdworkers using Amazon Mechanical Turk in-\nside Amazon Sagemaker. We pay workers 0.024\nper task. We select 3 crowdworkers per task for\n400 generated contents per task.\nIn Table.4 below, we present a comparison be-\ntween a sample text generated without the denois-\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0\nAverage fluency score per sample\n0\n50\n100\n150\n200Count\nFluency\n20News-groups\nbbc-news\nAgNews\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0\nAverage grammar score per sample\n0\n50\n100\n150Count\nGrammar\n20News-groups\nbbc-news\nAgNews\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0\nAverage redundancy score per sample\n0\n50\n100\n150Count\nRedundancy\n20News-groups\nbbc-news\nAgNews\nFigure 4: The histogram of survey scores from fluency,\ngrammar, and redundancy perspectives.\ning process and five generated text with denoising\nfrom the same topic vector z′′.\nB Datasets\nWe have created a huggingface account to upload\nall relevant data used for training our modified\nFlanT5: https://huggingface.co/datasets/\nxwjzds/pretrain_sts\nWe use the same account to upload all\nraw data for topic modeling as you can see:\nhttps://huggingface.co/datasets/xwjzds/\nag_news, https://huggingface.co/datasets/\nxwjzds/bbc-news, and https://huggingface.\nco/datasets/xwjzds/20_newsgroups. We\nhave uploaded text after preprocessing here:\nhttps://huggingface.co/datasets/xwjzds/\nag_news_lemma_train, https://huggingface.\nco/datasets/xwjzds/bbc-news_lemma_train,\nand https://huggingface.co/datasets/\nxwjzds/20_newsgroups_lemma_train. We have\nuploaded words used for bag of words here:\nhttps://huggingface.co/datasets/xwjzds/\nag_news_keywords, https://huggingface.co/\ndatasets/xwjzds/bbc-news_keywords, and\nhttps://huggingface.co/datasets/xwjzds/\n20_newsgroups_keywords. Overall, we use 3\ndatasets that combines different domain to evaluate\nthe performance.\n(1) AgNews We use the same AG’s News dataset\nfrom (Zhang et al., 2016).Overall it has 4 classes\nand, 30000 documents per class. Classes categories\n9054\nTable 3: The setup of survey\nmetrics question options\nFluency Is the language in the sentence fluent? • not fluent at all (- 2)\n• not fluent (- 1)\n• neutral ( 0)\n• fluent ( 1)\n• very fluent ( 2)\nGrammar How grammatical the generated text is? • not grammatical at all (- 2)\n• not grammatical (- 1)\n• neutral ( 0)\n• grammatical ( 1)\n• perfectly grammatical ( 2)\nRedundancy How repetitive or redundant the generated text is? • not redundant at all (- 2)\n• not redundant (- 1)\n• neutral ( 0)\n• redundant ( 1)\n• very redundant ( 2)\nTable 4: The generated text with and without denoising\nThe text generated without denoising The text generated with denoising\n\"I’m not sure if this is a good idea or not, but I’m sure it’s a good\nidea.\"\n\"the act of removing a bacteriophage from a plant\nis a source of danger.\"\n\"a few years ago a blond man was driving a honda\ncivic car.\"\n\"the act of putting a letter or a symbol in a docu-\nment.\"\n\"the man, who is a philanthropist, died in a car crash\nin april, 2000.\"\n\"the act of stealing a car.\"\ninclude World, Sports, Business, and Sci/Tech.\n(2) bbc-news (Lang, 1995) has 2225 texts from\nbbc news, which consists of 5 categories in total.\nThe five categories we want to identify are Sports,\nBusiness, Politics, Tech, and Entertainment.\n(3) 20Newsgroups (Lang, 1995) is a collection\nof newsgroup posts. We only select 20 categories\nhere. Compare to the previous 2 datasets, 20 cate-\ngories newsgroup is small so we can check the per-\nformance of our methods on small datasets. Also,\nthe number of topics is larger than the previous\none.\nC Code\nCode for our architecture Enc1 and Enc2\nis modified from https://huggingface.\nco/transformers/v3.0.2/_modules/\ntransformers/modeling_t5.html#T5Model.\nforward by modifying the forward process.\nTraining process for modified T5 We use\ngoogle/flan-t5-base as our basic model. We use\n20 percent data as the validation set. We train\n20 epochs or when validation loss deteriorates\nconsistently for 3 epochs. We set the number of\nvirtual tokens equal to 20. We set the learning\nrate to 0.01. For the CNN encoder we have 2\n1-dimension convolution layers with GELU as the\nactivation function. In channel is 256, 32, and\n4. Kernal size is 3 and stride is 1 and padding\nis 1. We set dropout after the convolution layer\nwith a dropout rate equal to 0.2. We have not\nsystematically finetuned these parameters. We\ntrained our modified FlanT5 for 3 times and choose\nthe lowest validation loss model as our model to\nrun topic modeling. It took less than 20 hours for a\nsingle gpu to finetune task.\nCode for comparable methods Code we used\nto implement GSM is https://github.com/\nYongfeiYan/Neural-Document-Modeling with\ntopic covariance penalty equals to 1. The code\nwe used to implement ETM is https://github.\ncom/adjidieng/ETM ntm.py in zip file is where\nwe rewrite and includes all relevant topic modeling\nmethods.\nThe code we used to implement CTM\nand ZTM is https://github.com/MilaNLProc/\ncontextualized-topic-models For CTM and\nZTM, We set the number of samples for topic pre-\ndictions equal to 5 and used their default prepro-\ncessing methods. The code we used to implement\nvONT is derived from https://github.com/\nYongfeiYan/Neural-Document-Modeling The\n9055\ncode we used to sentence embeddings vectors\nis from huggingface: https://huggingface.co/\nsentence-transformers gsm-vae.py is where we\nimplement our version of topic modeling.\nCode for metric diversity is implemented us-\ning scripts: https://github.com/adjidieng/\nETM/blob/master/utils.py line 4. Cv is im-\nplemented using gensim.models.coherencemodel\nwhere coherence = ’Cv’, Top-NMI is implemented\nusing metrics.normalizedmutualinfoscorefrom\nsklearn. Top-Purity is implemented by definitions.\nkm based is implemented by the sklearn package\nkmeans.\nD Compared Methods Selection\nSentence Embedding We choose GTR-T5 and\nSentence-T5 because they are the only two em-\nbeddings that we are aware of T5 as base models.\nThey also perform well in clustering tasks (Muen-\nnighoff et al., 2022). We choose Mpnet because it\nis commonly used in sentence embeddings and is\nthe second best method in clustering. Benchmark-\ning all these methods shows that our method is\nsuperior in sentence embeddings when the number\nof topics is large.\nTopic Modeling There are many neural topic\nmodeling methods but no standard benchmarks.\nFor neural topic modeling methods, we choose\nNVDM because it performs well in (Doan and\nHoang, 2021). We choose ETM because it is com-\nmonly used and is the first one to leverage word em-\nbeddings to topic modeling. We choose vONT (Xu\net al., 2023d) because it performs well in cluster-\nability topic modeling metrics. We choose GSM\nbecause it also applied softmax after sampling from\nthe gaussian distribution. We think it is a similar\ncomparison.\nFor contextualized topic modeling, we choose\nCTM and ZTM because they are the best perform-\ning ones with code available. We exclude methods\nsuch as Topic2Vec or Berttopic because it is hard to\ndefine the number of topics or get the embeddings\nof documents to calculate clusterability. While\nmany methods are derived from CTM, they either\ndo not have code or are hard to use. For example,\n(Wu et al., 2023) is hard to process data for bbc\nnews in the same format. In the future, we would\nlike to benchmark methods such as (Costello and\nReformat, 2023a) which leverage reinforcement\nlearning, (Wang et al., 2023) which leverage adver-\nsairal training . Other methods such as (Han et al.,\n2023) have no code. We only include methods that\nleverage language models to do an apple-to-apple\ncomparison and exclude methods using graph neu-\nral network (Zhou et al., 2020) or reinforcement\nlearning (Costello and Reformat, 2023b).\nE Settings for clusterability evaluations\nCompare existing sentence embedding methods\nwith our proposed embedding on standard clus-\nterability metrics such as purity and NMI on Ag-\nNews dataset (Zhang et al., 2016). We compare our\nmethods with GTR-T5 (Ni et al., 2021a), Sentence-\nT5 (Ni et al., 2021b) and Mpnet (Song et al., 2020).\nWe choose the largest version for all of them. De-\nTiME training is the embedding finetuned on the\nsame dataset but we use the same input and output\ninstead of rephrasing. We train a 2 layer MLP neu-\nral network variational autoencoder without soft-\nmax suggested by (Miao et al., 2017). We choose\nthe mean to represent the hidden dimension of the\ninput. We train each embedding 10 times to get\nthe confidence band and average. The consistency\nof DeTiME’s clusterability from 20 to 50 epochs\nsuggests its potential suitability for topic modeling\nfor the large number of topics.\nF the purpose of CNN encoder\nThe theoretical advantage of CNN is to extract local\nfeatures, reduce dimension reduction and be robust\nto noise(Li et al., 2021). In our cases, it helps to\nfurther extract important and local features from\nLLM encoder output and reduce dimensions.\nThe purpose of CNN is to compress output from\nFlanT5 encoder to create embeddings for topic\nmodeling. The output of FlanT5 encoder is 256\n(maximum sequence length used in our pre-train\nmodel) * 768 (embedding dimension) = 393216.\nTo illustrate our points, we rerun the same cluster-\nability experiment but replacing CNN encoder with\nMLP. The topic purity drops from 0.614 to 0.396\nand NMI drops from 0.31 to 0.05. This shows that\nCNN encoder helps the framework to achieve high\nclusterability and suitable for topic modeling. For\nefficiency, since the input dimension of encoder is\n393216 and the output is 3072. MLP will require\nparameters 1207962624 parameters while CNN\nonly reuqires 49624 parameters. This makes CNN\nis easy to load and much effcient to train.\nThis dimension is too high for any NTM to ex-\ntract information. Thus, we need to compress the\nobtained embeddings for topic modeling. Here,\n9056\nusing MLP only is hard to build representations\nthat incorporate information across the entire in-\nput text sequence as we just concatenate the em-\nbeddings of each tokens and then fed into MLP.\nIn the same time, the authors in (Beltagy et al.,\n2020) have shown that a very lightweight convolu-\ntion can perform competitively to the best reported\nself-attention results. This shows that CNN is ef-\nfective on extracting information from attention\nlayer. Based on this, we thus leverage CNN en-\ncoder to build representations that capture infor-\nmation across the text sequence. Also, by reduc-\ning the number of output channels, we are able to\nobtain embeddings with reduced dimensions (4 *\n768=3072 in our method), which hugely speed up\nthe training.\nG Perplexity Evaluation\nWe have measured the perplexity for our method\nand other methods on the dataset AgNews, and the\nresults are vNTM: 1479.32, ETM 692.17, NVDM:\n1734.28, GSM 684.31 DeTiME: 612.71. This\nstrengthen our conclusion of our work that De-\nTiME is promising in topic modeling. We use the\nsame set up as (Gupta and Zhang, 2021) for this\nexperiment.\nPerplexity is not applied in topic-aware content\ngeneration and has not been used in topic modeling\nlately. We had not reported perplexity as the topic-\naware content is generated from the sampled latent\ntopic embeddings, where the ground truth (i.e. text\nsequence ground truth) is not available. The reason\nwe used sampled latent topic embeddings is that\nwe are mainly focus on how diffusion can improve\nthe quality of topic aware text generation.\nH Related Work\nTo the best of our knowledge, we are the first to fine-\ntune and modify encode-decoder LLM (i.e. Flan-\nT5) in topic modeling, and the first one to use dif-\nfusion in topic aware content generation with topic\nmodeling, and integrate both in one unified frame-\nwork. We do not find simpler structures to solve\ntopic modeling and topic aware generation using\nencoder decoder LLM. For topic aware content\ngeneration using diffusion, there is no compara-\nble work and we have to establish all baselines\nourselves for this. We have list other comparable\nworks below and how our work distinguish from\nthem:\n(Cui and Hu, 2021) has leveraged Bert as part of\nencoder. However, they used NTM that taken bag\nof words as input and Bert/Graph neural network as\nencoder. We instead use encoder and decoder LLM\nand we take embeddings from encoder as input to\nNeural Topic Modeling. Also, their goal is sum-\nmarization but our goal is topic aware generation.\n(Ailem et al., 2019) propose a new decoder where\nthe output summary is generated by conditioning\non both the input text and the latent topics of the\ndocument. The latent topics, identified by a topic\nmodel such as LDA, reveals more global semantic\ninformation that can be used to bias the decoder to\ngenerate words. In our work, we have leveraged a\nmore promising topic modeling based on encoder-\ndecoder LLM. Also, the diffusion model is used\nto generate high-quality topic aware text, instead\nof summarization. (Zhang et al., 2023) proposed\nto directly generating the desired summary sen-\ntence representations with diffusion models and ex-\ntracting sentences based on sentence representation\nmatching. Even though this work leveraged the dis-\ntribution of embedded vectors of text for matching,\nit does not leverage the topic modeling. In compar-\nison, our work have leveraged topic modeling, and\nalso is able to generate high-quality topic-aware\ntext instead of extractive summarization.\n9057",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7519979476928711
    },
    {
      "name": "Encoder",
      "score": 0.6150634288787842
    },
    {
      "name": "Adaptability",
      "score": 0.5535997748374939
    },
    {
      "name": "Cluster analysis",
      "score": 0.5442762970924377
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.5172855854034424
    },
    {
      "name": "Topic model",
      "score": 0.46560055017471313
    },
    {
      "name": "Autoencoder",
      "score": 0.4432508945465088
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4087376296520233
    },
    {
      "name": "Artificial neural network",
      "score": 0.407740980386734
    },
    {
      "name": "Data science",
      "score": 0.3257448673248291
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}