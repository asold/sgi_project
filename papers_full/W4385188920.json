{
    "title": "Transformers and Visual Transformers",
    "url": "https://openalex.org/W4385188920",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3216160148",
            "name": "Robin Courant",
            "affiliations": [
                "Institut national de recherche en informatique et en automatique",
                "École Polytechnique",
                "Université de Rennes",
                "Centre National de la Recherche Scientifique",
                "Institut de Recherche en Informatique et Systèmes Aléatoires"
            ]
        },
        {
            "id": "https://openalex.org/A5008370168",
            "name": "Maika Edberg",
            "affiliations": [
                "Centre National de la Recherche Scientifique",
                "École Polytechnique"
            ]
        },
        {
            "id": "https://openalex.org/A2131089096",
            "name": "Nicolas Dufour",
            "affiliations": [
                "Centre National de la Recherche Scientifique",
                "École Polytechnique"
            ]
        },
        {
            "id": "https://openalex.org/A2079828037",
            "name": "Vicky Kalogeiton",
            "affiliations": [
                "Centre National de la Recherche Scientifique",
                "École Polytechnique"
            ]
        },
        {
            "id": "https://openalex.org/A3216160148",
            "name": "Robin Courant",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5008370168",
            "name": "Maika Edberg",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2131089096",
            "name": "Nicolas Dufour",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2079828037",
            "name": "Vicky Kalogeiton",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2186222003",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W4319777935",
        "https://openalex.org/W3137278571",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4214893857",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W4312358791",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2606128082",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3174226175",
        "https://openalex.org/W2962803520",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W2962934715",
        "https://openalex.org/W4214612132",
        "https://openalex.org/W2990503944",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W4214634256",
        "https://openalex.org/W2593116425",
        "https://openalex.org/W3196974791",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W3106321930",
        "https://openalex.org/W2962960500",
        "https://openalex.org/W2523993696",
        "https://openalex.org/W3194397797",
        "https://openalex.org/W3099903704"
    ],
    "abstract": null,
    "full_text": "Chapter 6 \nTransformers and Visual Transformers \nRobin Courant, Maika Edberg, Nicolas Dufour, and Vicky Kalogeiton \nAbstract \nTransformers were initially introduced for natural language processing (NLP) tasks, but fast they were \nadopted by most deep learning ﬁelds, including computer vision. They measure the relationships between \npairs of input tokens (words in the case of text strings, parts of images for visual transformers), termed \nattention. The cost is exponential with the number of tokens. For image classiﬁcation, the most common \ntransformer architecture uses only the transformer encoder in order to transform the various input tokens. \nHowever, there are also numerous other applications in which the decoder part of the traditional trans-\nformer architecture is also used. Here, we ﬁrst introduce the attention mechanism (Subheading 1) and then \nthe basic transformer block including the vision transformer (Subheading 2). Next, we discuss some \nimprovements of visual transformers to account for small datasets or less computation (Subheading 3). \nFinally, we introduce visual transformers applied to tasks other than image classiﬁcation, such as detection, \nsegmentation, generation, and training without labels (Subheading 4) and other domains, such as video or \nmultimodality using text or audio data (Subheading 5). \nKey words Attention, Transformers, Visual transfor mers, Multimodal attention \n1 Attention \nAttention is a technique in Computer Science that imitates the way \nin which the brain can focus on the relevant parts of the input. In \nthis section, we introduce attention: its history (Subheading 1.1), \nits \ndeﬁnition (Subheading 1.2), its types and variations (Subhead-\nings 1.3 and 1.4), and its properties (Subheading 1.5). \nTo understand what attention is and why it is so useful, con-\nsider \nthe following ﬁlm review: \nWhile others claim the story is boring, I found it fascinating. \nIs this ﬁlm review positive or negative? The ﬁrst part of the \nsentence is unrelated to the critic’s opinion, while the second part \nsuggests a positive sentiment with the word ‘fascinating’. To a \nhuman, the answer is obvious; however, this type of analysis is not \nnecessarily obvious to a computer. \nOlivier Colliot (ed.), Machine Learning for Brain Disorders , Neuromethods, vol. 197, https://doi.org/10.1007/978-1-0716-3195-9_6, \n© \nThe Author(s) 2023\n193\n1.2 Deﬁnition of\nAttention\n194 Robin Courant et al.\nTypically, sequential data require context to be understood. In \nnatural language, a word has a meaning because of its position in \nthe sentence, with respect to the other words: its context. In our \nexample, while “boring” alone suggests that the review is negative, \nits contextual relationship with other words allows the reader to \nreach the appropriate conclusion. In computer vision, in a task like \nobject detection, the nature of a pixel alone cannot be identiﬁed: we \nneed to account for its neighborhood, its context. So, how can we \nformalize the concept of context in sequential data? \n1.1 The History of \nAttention \nThis notion of context is the motivation behind the introduction of \nthe attention mechanism in 2015 [ 1]. Before this, language trans-\nlation was mostly relying on encoder-decoder architectures: recur-\nrent neural networks (RNNs) [ 2] and in particular long-short-term \nmemory (LSTMs) networks were used to model the relationship \namong words [ 3]. Speciﬁcally, each word of an input sentence is \nprocessed by the encoder sequentially. At each step, the past and \npresent information are summarized and encoded into a ﬁxed-\nlength vector. In the end, the encoder has processed every word \nand outputs a ﬁnal ﬁxed-length vector, which summarizes all input \ninformation. This ﬁnal vector is then decoded and ﬁnally translates \nthe input information into the target language. \nHowever, the main issue of such structure is that all the infor-\nmation is compressed into one ﬁxed-length vector. Given that the \nsizes of sentences vary and as the sentences get longer, a ﬁxed-\nlength vector is a real bottleneck: it gets increasingly difﬁcult not to \nlose any information in the encoding process due to the vanishing \ngradient problem [\n1]. \nAs a solution to this issue, Bahdanue et al. [ 1] proposed the \nattention module in 2015. The attention module allows the model \nto consider the parts of the sentence that are relevant to predicting \nthe next word. Moreover, this facilitates the understanding of \nrelationships among words that are further apart. \nGiven two lists of tokens, X ∈  N ×d x and Y ∈  N ×d y , attention \nencodes information from Y into X, where N is the length of inputs \nX and Y and d\nx and dy are their respective dimensions. For this, we \nﬁrst deﬁne three linear mappings, query mapping W Q ∈  dx ×d q , \nkey mapping W K ∈  dy ×d k , and value mapping W V ∈  dy ×d v , \nwhere d q, d k, and d v are the embedding dimensions in which the \nquery, key, and value are going to be computed, respectively. \nThen, w e d eﬁne the query Q, key K, and value V [ 4] as: \nQ =XW Q \nK =YW K \nV =YW V\nTransformers and Visual Transformers 195\nNext, the attention matrix is deﬁned as: \nAðQ ,KÞ=Softmax QK ⊤ \ndk \np : ð1Þ \nThis is illustrated in the left part of Fig. 1. The nominator \nQK T ∈ N ×N represents how each part of the input in X attends \nto each part of the input in Y. 1 This dot product is then put \nthrough the softmax function to normalize its values and get posi-\ntive values that add to 1. However, for large values of d k, this may \nresult in the softmax to have incredibly small gradients, so it is \nscaled down by dk \np . \nThe resulting N×N matrix encodes the relationship between X \nwith respect to Y : it measures how important a token in X is with \nrespect to another one in Y . \nFinally, the attention output is deﬁned as: \nAttentionðQ ,K,VÞ=AðQ ,KÞV:ð 2Þ \nFigure 1 displays this. The attention output encodes the infor-\nmation of each token by taking into account the contextual infor-\nmation. Therefore, through the learnable parameters—queries, \nkeys, and values—the attention layers learn a token embedding \nthat takes into account their relationship. \nContextual Relationships How does Eq. 2 encode contextual \nrelationships? To answer this question, let us reconsider analyzing \nthe sentiment of ﬁlm reviews. To encode contextual relationships \ninto the word embedding, we ﬁrst want a matrix representation of \nthe relationship between all words. To do so, given a sentence of \nlength N, we take each word vector and feed it to two different \nlinear layers, calling one output “query” and the other output \n“key”. We pack the queries into the matrix Q and the keys into \nthe matrix K, by taking their product (QK\nT ). The result is a N×N \nmatrix that explains how important the i-th word (row-wise) is to \nunderstand the j-th word (column-wise). This matrix is then scaled \nand normalized by the division and softmax. Next, we feed the \nword vectors into another linear layer, calling its output “value”. \nWe multiply these two matrices together. The results of their prod-\nuct are attention vectors that encode the meaning of each word, by \nincluding their contextual meaning as well. Given that each of these \nqueries, keys, and values is learnable parameter, as the attention \nlayer is trained, the model learns how relationships among words \nare encoded in the data. \n1 Note that in the literature, there are two main attention functions: additive attention [ 1] and dot-product \nattention (Eq. 1). In practice, the dot product is more efﬁcient since it is implemented using highly optimized \nmatrix multiplication, compared to the feed-forward network of the additive attention; hence, the dot product is \nthe dominant one.\n196 Robin Courant et al.\nFig. 1 Attention block. Next to each element, we denote its dimensionality. \nFigure inspired from [ 4] \n1.3 Types of \nAttention \nThere exist two dominant types of attention mechanisms: self-\nattention and cross attention [ 4]. In self-attention, the queries, \nkeys, and values come from the same input, i.e., X=Y;i n cross \nattention, the queries come from a different input than the key and \nvalue vectors, i.e., X≠Y. These are described below in Subheadings \n1.3.1 and 1.3.2, respectively. \n1.3.1 Self-Attention In self-attention, the tokens of X attend to themselves (X=Y). \nTherefore, it is modeled as follows: \nSAðXÞ=AttentionðXW Q ,XW K ,XW V Þ: ð3Þ \nSelf-attention for malizes the concept of context. It learns the \npatterns underlying how parts of the input correspond to each \nother. By gathering information from the same set, given a \nsequence of tokens, a token can attend to its neighboring tokens \nto compute its output. \n1.3.2 Cross Attention Most real-world data are multimodal—for instance, videos contain \nframes, audios, and subtitles, images come with captions, etc. \nTherefore, models that can deal with such types of multimodal \ninformation have become essential.\n1.4 Variation of\nAttention\nTransformers and Visual Transformers 197\nCross attention is an attention mechanism designed to handle \nmultimodal inputs. Unlike self-attention, it extracts queries from \none input source and key-value pairs from another one (X≠Y ). It \nanswers the following question: “Which parts of input X and input \nY correspond to each other?” Cross attention (CA) is deﬁned as: \nCAðX,YÞ=AttentionðXW Q ,YW K ,YW V Þ: ð4Þ \nAttention is typically employed in two ways: (1) multi-head self-\nattention (MSA, Subheading 1.4) and (2) masked multi-head \nattention (MMA, Subheading 1.4). \nAttention Head We call attention head the mechanism presented \nin Subheading 1.2, i.e., query-key-value projection, followed by \nscaled dot product attention (Eqs. 1 and 2). \nWhen employing an attention-based model, relying only on a \nsingle attention head can inhibit learning. Therefore, the multi-\nhead attention block is introduced [ 4]. \nMulti-head Self-Attention (MSA) MSA is shown in Fig. 2 and is \ndeﬁned as: \nMSAðXÞ =Concat ðhead1ðXÞ, ..., head hðXÞ ÞWO , \nheadiðXÞ =SAðXÞ , 8i ∈f1,hg,\nð5Þ \nwhere Concat is the concatenation of h attention heads and \nWO ∈  hdv ×d is projection matrix. This means that the initial \nembedding dimension d x is decomposed into h×d v and the com-\nputation per head is carried out independently. The independent \nattention heads are usually concatenated and multiplied by a linear \nlayer to match the desired output dimension. The output dimen-\nsion is often the same as the input embedding dimension d. This \nallows an easier stacking of multiple blocks. \nMulti-head Cross Attention (MCA) Similar to MSA, MCA is \ndeﬁned as: \nMCAðX,YÞ =Concatð head1ðX,YÞ, ..., head hðX,YÞÞW O , \nheadiðX,YÞ =CAðX,YÞ , 8i ∈f1,hg: \nð6Þ \nMasked M ulti-head S elf-Attention (MMSA) The MMSA layer \n[4] is another variation of attention. It has the same structure as the \nmulti-head self-attention block (Subheading 1.4), but all the later \nvectors in the target output are masked. When dealing with sequen-\ntial data, this can help make training parallel.\n198 Robin Courant et al.\nFig. 2 Multi-head self-attention block (MSA). First, the input X is projected to \nqueries, keys, and values and then passed through h attention blocks. The \nh resulting attention outputs are then concatenated together and ﬁnally \nprojected to a d-dimensional output vector. Next to each element, we denote \nits dimensionality. Figure inspired from [ 4] \n1.5 Properties of \nAttention \nWhile attention encodes contextual relationships, it is permutation \nequivalent, as the mechanism does not account for the order of the \ninput data. As shown in Eq. 2, the attention computations are all \nmatrix multiplication and normalizations. Therefore, a permuted \ninput results in a permuted output. In practice, however, this may \nnot be an accurate representation of the information. For instance, \nconsider the sentences “the monkey ate the banana” and “the \nbanana ate the monkey.” They have distinct meanings because of \nthe order of the words. If the order of the input is important, \nvarious mechanisms, such as the positional encoding, discussed in \nSubheading \n2.1.2, are used to capture this subtlety. \n2 Visual Transformers \nThe transformer architecture was introduced in [ 4] and is the ﬁrst \narchitecture that relies purely on attention to draw connections \nbetween the inputs and outputs. Since its debut, it revolutionized \ndeep learning, making breakthroughs in numerous ﬁelds, including\n2.1 Basic\nTransformers\nnatural language processing, computer vision, chemistry, and biol-\nogy, thus making its way to becoming the default architecture for \nlearning representations. Recently, the standard transformer [ 4] has \nbeen \nadapted for vision tasks [ 5]. And again, visual transformer has \nbecome \none of the central architectures in computer vision. \nTransformers and Visual Transformers 199\nIn this section, we ﬁrst introduce the basic architecture of \ntransformers (Subheading 2.1) and then present its advantages \n(Subheading 2.2). Finally, we describe the vision transformer (Sub-\nheading 2.3). \nAs shown in Fig. 3, the transformer architecture [4] is an encoder-\ndecoder \nmodel. First, it embeds input tokens X=(x1, ... , xN) into \na latent space, resulting in latent vectors Z=(z 1, ... , zN), which are \nfed to the decoder to output Y =(y 1, ... , yM). The encoder is a \nstack of L layers, with each one consisting of two sub-blocks: multi-\nhead self-attention (MSA) layers and a multilayer perceptron \n(MLP). The decoder is also a stack of L layers, with each one \nconsisting of three sub-blocks: masked multi-head self-attention \n(MMSA), multi-head cross attention (MCA), and MLP. \nOverview Below, we describe the various parts of the transformer \narchitecture, following Fig. 3. First, the input tokens are converted \ninto \nthe embedding tokens (Subheading 2.1.1). Then, the posi-\ntional \nencoding adds a positional token to each embedding token \nto denote the order of tokens (Subheading 2.1.2). Then, the \ntransfor\nmer encoder follows (Subheading 2.1.3). This consists of \na \nstack of L multi-head attention, normalization, and MLP layers \nand encodes the input to a set of semantically meaningful features. \nAfter, the decoder follows (Subheading 2.1.4). This consists of a \nstack \nof L masked multi-head attention, multi-head attention, and \nMLP layers followed by normalizations and decodes the input \nfeatures with respect to the output embedding tokens. Finally, the \noutput is projected to linear and softmax layers. \n2.1.1 Embedding The ﬁrst step of transformers consists in converting input tokens 2 \ninto embedding tokens, i.e., vectors with meaningful features. To \ndo so, following standard practice [ 6], each input is projected into \nan \nembedding space to obtain embedding tokens Ze . The embed-\nding space is structured in a way that the distance between a pair of \nvectors is relative to the semantic similarity of their associated \nwords. For the initial NLP case, this means that we get a vector of \neach word, such that the vectors that are closer together have \nsimilar meanings. \n2 Note the initial transformer architecture was proposed for natural language processing (NLP), and therefore the \ninputs were words.\n200 Robin Courant et al.\nFig. 3 The transformer architecture. It consists of an encoder (left) and a decoder \n(right) block, each one consisting from a series of attention blocks (multi-head \nand masked multi-head attention) and MLP layers. Next to each element, we \ndenote its dimensionality. Figure inspired from [\n4] \n2.1.2 Positional Encoding As discussed in Subheading 1.5, the attention mechanism is posi-\ntional agnostic, which means that it does not store the information \non the position of each input. However, in most cases, the order of \ninput tokens is relevant and should be taken into account, such as \nthe order of words in a sentence matter as they may change its \nmeaning. Therefore, [ 4] introduced the Positional Encoding \nPE ∈ N ×d x , which adds a positional token to each embedding \ntoken Z e ∈  N ×d x .\nTransformers and Visual Transformers 201\nSinusoidal Positional \nEncoding \nThe sinusoidal positional encoding [ 4] is the main positional \nencoding method, which encodes the position of each token with \nsinusoidal waves of multiple frequency. For an embedding token \nZe ∈  N ×d x , its positional encoding PE ∈  N ×d x is deﬁned as: \nPEði,2jÞ =sin i \n100002j=d \nPEði,2j þ 1Þ =cos i \n100002j=d ,8i,j ∈½j1,nj/C138×½j1,dj/C138: \nð7Þ \nLearnable Positional \nEncoding \nAn orthogonal approach is to let the model learn the positional \nencoding. In this case, PE ∈  N ×d x becomes a learnable parameter. \nThis, however, increases the memory requirements, without neces-\nsarily bringing improvements over the sinusoidal encoding. \nPositional Embedding After its computation, either the positional encoding PE is added \nto the embedding tokens or they are concatenated as follows: \nZpe =Z e þ PE,o r \nZpe =ConcatðZ e ,PEÞ, \nð8Þ \nwhere Concat denotes vector concatenation. Note that the concat-\nenation has the advantage of not altering the information contained \nin Ze , since the positional information is only added to the unused \ndimension. Nevertheless, it augments the input dimension, leading \nto higher memory requirements. Instead, the addition does pre-\nserve the same input dimension while altering the content of the \nembedding tokens. When the input dimension is high, this content \naltering is trivial, as most of the content is preserved. Therefore, in \npractice, for high dimension, summing positional encodings is \npreferred, whereas for low dimensions concatenating them prevails. \n2.1.3 Encoder Block The encoder block takes as input the embedding and positional \ntokens and outputs features of the input, to be decoded by the \ndecoder block. It consists of a stack of L multi-head self-attention \n(MSA) layers and a multilayer perceptron (MLP). Speciﬁcally, the \nembedding and positional tokens, Z pe \nx ∈  N ×d , go through a \nmulti-head self-attention block. Then, a residual connection with \nlayer normalization is deployed. In the transformer, this operation \nis performed after each sub-layer. Next, we feed its output to an \nMLP and a normalization layer. This operation is performed \nL times, and each time the output of each encoder block (of size \nN×d) is the input of the subsequent block. In the L-th time, the \noutput of the normalization is the input of the cross-attention \nblock in the decoder (Subheading \n2.1.4).\no\n202 Robin Courant et al.\n2.1.4 Decoder Block The decoder has two inputs: ﬁrst, an input that constitutes the \nqueries Q ∈  N ×d of the encoder, and, second, the output of the \nencoder that constitutes the key-value K,V ∈ N ×d pair. Similar \nto Subheadings 2.1.1 and 2.1.2, the ﬁrst step constitutes encoding \nthe output token to output embedding token and output positional \ntoken. These tokens are fed into the main part of the decoder, \nwhich consists of a stack of L masked multi-head self-attention \n(MMSA) layers, multi-head cross-attention (MCA) layers, and \nmultilayer perceptron (MLP) followed by normalizations. Speciﬁ-\ncally, the embedding and positional tokens, Z\npe \ny ∈  N ×d ,g \nthrough a MMSA block. Then, a residual connection with layer \nnormalization follows. Next, an MCA layer (followed by normali-\nzation) maps the queries to the encoded key values before forward-\ning the output to an MLP. Finally, we project the output of the \nL decoder blocks (of dimension N×d\ny) through a linear layer and \nget output probability through a softmax layer. \n2.2 Advantages of \nTransformers \nSince their introduction, the transformers have had a signiﬁcant \nimpact on deep learning approaches. \nIn natural language processing (NLP), before transformers, \nmost architectures used to rely on recurrent modules, such as \nRNNs [2] and in particular LSTMs [ 3]. However, recurrent models \nprocess the input sequentially, meaning that, to compute the cur-\nrent state, they require the output of the previous state. This makes \nthem tremendously inefﬁcient, as they are impossible to parallelize. \nOn the contrary, in transformers, each input is processed indepen-\ndent of the others, and the multi-head attention can perform \nmultiple attention computations at once. This makes transformers \nhighly efﬁcient, as they are highly parallelizable. \nThis results in not only exceptional scalability, both in the \ncomplexity of the model and the size of datasets, but also relatively \nfast training. Notably, the recent switch transformers [\n7] was pre-\ntrained on 34 billion tokens from the C4 dataset [ 8], scaling the \nmodel to over 1 trillion parameters. \nThis s calability [ 7] is the principal reason for the power of the \ntransformer. While it was originally introduced for translation, it \nrefrains from introducing many inductive biases, i.e., the set of \nassumptions that the user makes about the structure of the model \ninput. In doing so, the transformer relies on data to learn how they \nare structured. Compared to its counterparts with more biases, the \ntransformer requires much more data to produce comparable \nresults [\n5]. However, if a sufﬁcient amount of data is available, \nthe lack of inductive bias becomes a strength. By learning the \nstructure of the data from the data, the transformer is able to \nlearn better without human assumptions hindering [\n9]. \nIn most tasks involving transformers, the model is ﬁrst pre-\ntrained on a large dataset and then ﬁne-tuned for the task at hand \non a smaller dataset. The pretraining phase is essential for\ntransformers to learn the global structure of the speciﬁc input \nmodality. For ﬁne-tuning, typically fewer data sufﬁce as the model \nis already rich. For instance, in natural language processing, BERT \n[10], a state-of-the-art language model, is pretrained on a \nWikipedia-based dataset [ 11], with over 6 million articles and \nBook Corpus [ 12] with over 10,000 books. Then, this model can \nbe ﬁne-tuned on much more speciﬁc tasks. In computer vision, the \nvision transformer (ViT) is pretrained on the JFT-300M dataset, \ncontaining over 1 billion labels for 300 million images [\n5]. Hence, \nwith a sufﬁcient amount of data, transformers achieve results that \nwere never possible before in various areas of machine learning. \nTransformers and Visual Transformers 203\n2.3 Vision \nTransformer \nTransformers offer an alternative to CNNs that have long held a \nstranglehold on computer vision. Before 2020, most attempts to \nuse transformers for vision tasks were still highly reliant on CNNs, \neither by using self-attention jointly with convolutions [ 13, 14]o r \nby keeping the general structure of CNNs while using self-attention \n[15, 16]. \nThe reason for this is rooted in the two main weaknesses of the \ntransformers. First, the complexity of the attention operation is \nhigh. As attention is a quadratic operation, the number of para-\nmeters skyrockets quickly when dealing with visual data, i.e., \nimages—and even more so with videos. For instance, in the case \nof ImageNet [\n17], inputting a single image with 256×256=65, \n536 pixels in an attention layer would be too heavy computation-\nally. Second, transformers suffer from lack of inductive biases. Since \nCNNs were speciﬁcally created for vision tasks, their architecture \nincludes spatial inductive biases, like translation equivariance and \nlocality. Therefore, the transformers have to be pretrained on a \nsigniﬁcantly large dataset to achieve similar performances. \nThe v ision t ransformer (ViT) [ 5] is the ﬁrst systematic \napproach that uses directly transformers for vision tasks by addres-\nsing both aforementioned issues. It rids the concept of convolu-\ntions altogether, using purely a transformer-based architecture. In \ndoing so, it achieves the state of the art on image recognition on \nvarious datasets, including ImageNet [ 17] and CIFAR-100 [ 18]. \nFigure 4 illustrates the ViT architecture. The input image is ﬁrst \nsplit into 16×16 patches, ﬂattened, and mapped to the expected \ndimension through a learnable linear projection. Since the image \nsize is reduced to 16×16, the complexity of the attention mecha-\nnism is no longer a bottleneck. Then, ViT encodes the positional \ninformation and attaches a learnable embedding to the front of the \nsequence, similarly to BERT’s classiﬁcation token [\n10]. The output \nof this token represents the entirety of the input—it encodes the \ninformation from each part of the input. Then, this sequence is fed \ninto an encoder block, with the same structure as in the standard \ntransformers [\n4]. The output of the classiﬁcation token is then fed \ninto an MLP that outputs class probabilities.\n204 Robin Courant et al.\nFig. 4 The vision transformer architecture (ViT). First, the input image is split into patches (bottom), which are \nlinearly projected (embedding), and then concatenated with positional embedding tokens. The resulting tokens \nare fed into a transformer, and ﬁnally the resulting classiﬁcation token is passed through an MLP to compute \noutput probabilities. Figure inspired from [ 5] \nDue to the lack of inductive biases, when ViT is trained only on \nmid-sized datasets such as ImageNet, it scores some percentage \npoints lower than the state of the art. Therefore, the proposed \nmodel is ﬁrst pretrained on the JFT-300M dataset [ 19] and then \nﬁne-tuned on smaller datasets, thereby increasing its accuracy by \n13%. \nFor a complete overview of visual transformers and follow-up \nworks, we invite the readers to study [ 9, 20]. \n3 Improvements over the Vision Transformer \nIn this section, we present transformer-based methods that \nimprove over the original vision transformer (Subheading 2.3) i n \ntwo main ways. First, we introduce approaches that are trained on \nsmaller datasets, unlike ViT [ 5] that requires pretraining on \n300 million labeled images (Subheading 3.1). Second, we present \nextensions over ViT that are more computational-efﬁcient than \nViT, given that training a ViT is directly correlated to the image \nresolution and the number of patches (Subheading 3.2).\n3.1 Data Efﬁciency\nTransformers and Visual Transformers 205\nAs discussed in Subheading 2.3, the vision transformer (ViT) [ 5]i s \npretrained on a massive proprietary dataset (JFT-300M) which \ncontains 300 million labeled images. This need arises with trans-\nformers because we remove the inductive biases from the architec-\nture compared to convolutional-based networks. Indeed, \nconvolutions contain some translation equivariance. ViT does not \nbeneﬁt from this property and thus has to learn such biases, requir-\ning more data. JFT-300M is an enormous dataset, and to make ViT \nwork in practice, better data-efﬁciency is needed. Indeed, collecting \nthat amount of data is costly and can be infeasible for most tasks. \nData-Efﬁcient Image Transformers (DeiT) [ 21] The ﬁrst work \nto achieve an improved data efﬁciency is DeiT [ 21] . The main idea \nof DeiT is to distil the inductive biases from a CNN into a trans-\nformer (Fig. 5). DeiT adds another token that works similarly to the \nclass token. When training, ground truth labels are used to train the \nnetwork according to the class token output with a cross-entropy \n(CE) loss. However, for the distillation network, the output labels \nare compared to the labels provided from a teacher network with a\nFig. 5 The DeiT architecture. The architecture features an extra token, the \ndistillation token. This token is used similarly to the class token. \nFigure inspired from [ 21]\ncross-entropy loss. The ﬁnal loss for a N-categorical classiﬁcation \ntask is deﬁned as follows:\n206 Robin Courant et al.\nLhardDistill \nglobal = 1 \n2 ðLCEðΨðZclassÞ,yÞþ L CEðΨðZdistillÞ,y T ÞÞ, \nLCEð^y,yÞ =- 1 \nN \nN \ni =1 \nyi log ^yi þð1-y iÞlogð1- ^yiÞ \nð9Þ \nwithΨ the softmax function, Z class the class token output, Z distill the \nclass token output, y the ground truth label, and y T the teacher label \nprediction. \nThe teacher network is a Convolutional Neural Network \n(CNN). The main idea is that the distillation head will provide \nthe inductive biases needed to improve the data efﬁciency of the \narchitecture. By doing this, DeiT achieves remarkable performance \non the ImageNet dataset, by training “only” on ImageNet-1K \n[17], which contains 1.3 million images. \nConvit [ 22] The main disadvantage of DeiT [ 21] is that it \nrequires a pretrained CNN, which is not ideal, and it would be \nmore convenient to not have this requirement. The CNN has a \nhard inductive bias constraint that can be a major limitation. \nIndeed, if enough data is available, learning the biases from the \ndata can result in better representations. \nConvit [22] overpasses this issue by including the inductive bias \nof CNNs into a transformer in a soft way. Speciﬁcally, if the induc-\ntive bias is limiting the training, the transformer can discard it. The \nmain idea is to include the inductive bias into the ViT initialization. \nTherefore, before beginning training, the ViT is equivalent to a \nCNN. Then, the network can progressively learn the needed biases \nand diverge from the CNN initialization. \nCompact Convolutional Transformer [ 23], DeiT [ 21], and \nConvit [ 22] successfully achieve data efﬁciency at the ImageNet \nscale. However, ImageNet is a big dataset with 1.3 million images, \nwhereas most datasets are signiﬁcantly smaller. \nTo reach higher data efﬁciency, the compact convolutional \ntransformer [ 23] uses a CNN operation to extract the patches and \nthen uses these patches in a transformer network (Fig. 6). The \ncompact convolutional transformer comes with some modiﬁcations \nthat lead to major improvements. First, by having a more complex \nencoding of patches, the system relies on the convolutional induc-\ntive biases at the lower scales and then uses a transformer network \nto remove the locality constraint of the CNN. Second, the authors \nshow that discarding the “class” token results in higher efﬁciency. \nSpeciﬁcally, instead of the class token, the compact convolutional \ntransformer pools together all the patches token and classiﬁes on \ntop of this pooled token. These two modiﬁcations enable using\n3.2 Computational\nEfﬁciency\nsmaller transformers while improving both the data efﬁciency and \nthe computational efﬁciency. Therefore, these improvements allow \nthe compact convolutional transformer to be successfully trained \non smaller datasets, such as CIFAR or MNIST. \nTransformers and Visual Transformers 207\nFig. 6 Compact convolutional transformers. This architecture features a convolutional-based patch extraction \nto leverage a smaller transformer network, leading to higher data efﬁciency. Figure inspired from [ 23] \nThe vision transformer architecture (Subheading 2.3) suffers from \na Oðn 2Þ complexity with respect to the number of tokens. When \nconsidering small resolution images or big patch size, this is not a \nlimitation; for instance, for an image of 224×224 resolution with \n16×16 patches, this amounts to 196 tokens. However, when \nneeding to process larger images (for instance, 3D images in medi-\ncal imaging) or when considering smaller patches, using and train-\ning such models becomes prohibitive. For instance, in tasks such as \nsegmentation or image generation, it is needed to have more \ngranular representations than 16×16 patches; hence, it is crucial \nto solve this issue to enable more applications of vision transformer. \nSwin T ransformer [ 24] One idea to make transformers more \ncomputation-efﬁcient is the Swin transformer [ 24]. Instead of \nattending every patch in the image, the Swin transformer proposes \nto add a locality constraint. Speciﬁcally, the patches can only attend \nother patches that are limited to a vicinity window K. This restores \nthe local inductive bias of CNNs. To allow communication across\npatches throughout the network, the Swin transformer shifts the \nattention windows from one operation to another (Fig. 7). There-\nfore, the Swin transformer is quadratic with regard to the size of the \nwindow K but linear with respect to the number of tokens n with \ncomplexityOðnK 2Þ. In practice, however, K is small, and this solves \nthe quadratic complexity problem of attention. \n208 Robin Courant et al.\nFig. 7 Shifting operation in the Swin transformer [ 24]. Between each attention operation, the attention window \nis shifted so that each patch can communicate with different patches than before. This allows the network to \ngain more global knowledge with the network’s depth. Figure inspired from [ 24] \nPerceiver [ 25, 26] Another idea for more computation-efﬁcient \nvisual transformers is to make a more drastic change to the archi-\ntecture. If instead of using self-attention the model uses cross \nattention, the problem of the quadratic complexity with regard to \nthe number of tokens can be solved. Indeed, computing the cross \nattention between two sets of length m and n, respectively, has \ncomplexity OðmnÞ. This idea is introduced in the perceiver \n[25, 26]. The key idea is to have a smaller set of latent variables \nthat will be used as queries and that will retrieve information in the \nimage token set (Fig. 8). Since this solves the quadratic complexity \nissue, it also removes the need of using patches; hence, in the case of \ntransformers, each pixel is mapped to a single token. \n4 Vision Transformers for Tasks Other than Classiﬁcation \nSubheadings 1–3 introduce visual transformers for one main appli-\ncation: classiﬁcation. Nevertheless, transformers can be used for \nnumerous other tasks than classiﬁcation. \nIn this section, we present some fundamental vision tasks where \ntransformers have had a major impact: object detection in images \n(Subheading \n4.1), image segmentation (Subheading 4.2), training\n4.1 Object Detection\nwith Transformers\nvisual transformers without labels (Subheading 4.3), and image \ngeneration using generative adversarial networks (GANs) (Sub-\nheading 4.4). \nTransformers and Visual Transformers 209\nFig. 8 The perceiver architecture [ 25, 26]. A set of latent tokens retrieve information from the image through \ncross attention. Self-attention is performed between the tokens to reﬁne the learned representation. These \noperations are linear with respect to the number of image tokens. Figure inspired from [ 25, 26] \nDetection is one of the early tasks that have seen improvements \nthanks to transformers. Detection is a combined recognition and \nlocalization problem; this means that a successful detection system \nshould both recognize whether an object is present in an image and \nlocalize it spatially in the image. Carion et al. [ 14] is the ﬁrst \napproach that uses transformers for detection. \nDEtection TRansformer (DETR) [ 14] DETR ﬁrst extracts \nvisual representations with a convolutional network (Fig. 9).3 \nThen, the encodings are processed by a transformer network. \nFinally, the processed tokens are provided to a transformer decoder. \nThe decoder uses cross attention between a set of learned tokens \nand the image tokens encoded by the encoder and outputs a set of \ntokens. Each output token is then passed through a feed-forward \nnetwork that predicts if an object is present in an image or not; if \nthe object is indeed present, the network also predicts the class and \nspatial location of the object, i.e., coordinates within the image. \n4.2 Image \nSegmentation with \nTransformers \nThe goal of image segmentation is to assign to each pixel of an image \nthe label of the object it belongs to. The segmenter [ 27] is a purely \nViT approach addressing image segmentation. The idea is to ﬁrst use \nViT to encode the image. Then, the segmenter learns a token per\n3 Note that, in DETR, the transformer is not directly used to extract the visual representation. Instead, it focuses \non reﬁning the visual representation to extract the object information.\n4.3 Training\nTransformers Without\nLabels\nsemantic label. The encoded patch tokens and the semantic tokens \nare then fed to a second transformer. Finally, by computing the scalar \nproduct between the semantic tokens and the image tokens, the \nnetwork assigns a label to each patch. Figure 10 displays this.\n210 Robin Courant et al.\nCNN \nImage \nFeatures \nTransformer \nEncoder \nTransformed \nFeatures \nTransformer \nDecoder \nFFNN \nFFNN \nFFNN \nFFNN \nObject queries \nClass, \nBox \nNo \nObject \nNo \nObject \nClass, \nBox \nSeagullSeagull \nFig. 9 The DETR architecture. It reﬁnes a CNN visual representation to extract object localization and classes. \nFigure inspired from [ 14] \nFig. 10 The segmenter architecture. It is a purely ViT-based approach to perform semantic segmentation. \nFigure inspired from [ 27] \nVisual transformers have initially been trained for classiﬁcation \ntasks. However, this tasks requires having access to massive \namounts of labeled data, which can be hard to obtain \n(as discussed in Subheading 3.1). Subheadings 3.1 and 3.2 present \nways to train ViT more efﬁciently. However, it would also be \ninteresting to be able to train this type of networks with “cheaper” \ndata. Therefore, the goal of this part is to introduce unsupervised \nlearning with transformers, i.e., training transformers without any \nlabels.\nTransformers and Visual Transformers 211\nFig. 11 The DINO training procedure. It consists in matching the outputs between \ntwo networks (p 1 and p2) having two different augmentations (X 1 and X2) of the \nsame image as input (X). The parameters of the teacher model are updated with \nan exponential moving average (ema) of the student parameters. Figure inspired \nfrom [\n28] \nSelf-DIstillation with NO labels (DINO) [ 28] DINO is one of \nthe ﬁrst works that trains a ViT with self-supervised learning \n(Fig. 11). The main idea is to have two ViT models following the \nteacher-student paradigm: the ﬁrst model is updated through gra-\ndient descent, and the second is an exponential moving average of \nthe ﬁrst one. Then, the whole two-stream DINO network is trained \nusing two augmentations of the same image, which are each passed \nto one of the two networks. The goal of the training is to match the \noutput between the two networks, i.e., no matter the augmenta-\ntion in the input data, both networks should produce the same \nresult. The main ﬁnding of DINO is that the ViT is capable of \nlearning a semantic understanding of the image, as the attention \nmatrices display some semantic information. Figure 12 visualizes \nthe attention matrix of the various ViT heads trained with DINO. \nMasked A utoencoders ( MAE) [ 29] Another way to train a ViT \nwithout super vision is by using an autoencoder architecture. \nMasked autoencoders (MAE) [ 29] perform a random masking of \nthe input token and give the task to reconstruct the original image \nto a decoder. The encoder learns a representation that performs\n4.4 Image\nGeneration with\nTransformers and\nAttention\nwell in a given downstream task. This is illustrated in Fig. 13. One \nof the key observations of the MAE work [ 29] is that the decoder \ndoes not need to be very good for the encoder to achieve good \nperformance: by using only a small decoder, MAE successfully \ntrains a ViT in an autoencoder fashion. \n212 Robin Courant et al.\nFig. 12 DINO samples. Visualization of the attention matrix of ViT heads trained with DINO. The ViT discovers \nthe semantic structure of an image in an unsupervised way \nFig. 13 The MAE training procedure. After masking some tokens of an image, the remaining tokens are fed to \nan encoder. Then a decoder tries to reconstruct the original image from this representation. Figure inspired \nfrom [ 29] \nAttention and vision transformers have also helped in developing \nfresh ideas and creating new architectures for generative models \nand in particular for generative adversarial networks (GANs). \nGANsformers [30] GANsfor mers are the most representative \nwork of GANs with transformers, as they are a hybrid architecture \nusing both attention and CNNs. The GANsformer architecture is \nillustrated in Fig. 14. The model ﬁrst splits the latent vector of a \nGAN into multiple tokens. Then, a cross-attention mechanism is \nused to improve the generated feature maps, and at the same time, \nthe GANsformer architecture retrieves information from the gen-\nerated feature map to enrich the tokens. This mechanism allows the \nGAN to have better and richer semantic knowledge, which is \nshowed to be useful for generating multimodal images. \nStyleSwin [ 31] Another a pproach for generative modeling is to \npurely use a ViT architecture like StyleSwin [ 31]. StyleSwin is a \nGAN that leverages a similar type of attention as the Swin trans-\nformer [\n24]. This allows to generate high-deﬁnition images with-\nout having to deal with the quadratic cost problem.\nTransformers and Visual Transformers 213\nLatents Image \nCross-Attention \nLatents \nImage \nImage \nConvolutions \nCross-Attention \nFig. 14 GANsformer architecture. A set of latents contribute to bring information \nto a CNN feature map. Figure inspired from [ 30] \n5 Vision Transformers for Other Domains \nIn this section, we present applications of visual transformers to \nother domains. First, we describe multimodal transformers \noperating with vision and language (Subheading 5.1), then we \ndescribe video-level attention and video transformers (Subheadings \n5.2 and 5.3), and ﬁnally we present multimodal video transformers \noperating with vision, language, and audio (Subheading 5.4). \n5.1 Multimodal \nTransformers: Vision \nand Language \nAs transformers have found tremendous success in both natural \nlanguage processing and computer vision, their use in vision-\nlanguage tasks is also of interest. In this section, we describe some \nrepresentative multimodal methods for vision and language: ViL-\nBERT (Subheading \n5.1.1), DALL-E (Subheading 5.1.3), and \nCLIP (Subheading 5.1.2). \n5.1.1 ViLBERT Vision-and-language BERT (VilBERT) [ 32] is an example of archi-\ntecture that fuses two modalities. It consists of two parallel streams, \neach one working with one modality. The vision stream extracts\nbounding boxes from images via an object detection network, by \nencoding their position. The language stream embeds word vectors \nand extracts feature vectors using the basic transformer encoder \nblock [4] (Fig. 3 left). These two resulting feature vectors are then \nfused together by a cross-attention layer (Subheading 1.3.2). This \nfollows the standard architecture of the transformer encoder block, \nwhere the keys and values of one modality are passed onto the MCA \nblock of the other modality. The output of the cross-attention layer \nis passed into another transformer encoder block, and these two \nlayers are stacked multiple times. \n214 Robin Courant et al.\nThe language stream is initialized with BERT trained on Book \nCorpus [ 12] and Wikipedia [ 11], while the visual stream is initi-\nalized with Faster R-CNN [ 33]. On top of the pretraining of each \nstream, the whole architecture is pretrained on the Conceptual \nCaptions dataset [ 34] on two pretext tasks. \nViLBERT has been proven powerful for a variety of multimodal \ntasks. In the original paper, ViLBERT was ﬁned-tuned to a variety \nof tasks, including visual question answering, visual commonsense \nreasoning, referring expressions, and caption-based image retrieval. \n5.1.2 CLIP Connecting Text and Images (CLIP) [ 35] is designed to address \ntwo major issues of deep learning models: costly datasets and \ninﬂexibility. While most deep learning models are trained on labeled \ndatasets, CLIP is trained on 400 million text-image pairs that are \nscraped from the Internet. This reduces the labor of having to \nmanually label millions of images that are required to train powerful \ndeep learning models. When models are trained on one speciﬁc \ndataset, they also tend to be difﬁcult to extend to other applica-\ntions. For instance, the accuracy of a model trained on ImageNet is \ngenerally limited to its own dataset and cannot be applied to real-\nworld problems. To optimize training, CLIP models learn to per-\nform a wide variety of tasks during pretraining, and this task allows \nfor zero-shot transfer to many existing datasets. While there are still \nseveral potential improvements, this approach is competitive to \nsupervised models that are trained on speciﬁc datasets. \nCLIP Architecture and \nTraining \nCLIP is used to measure the similarity between the text input and \nthe image generated from a latent vector. At the core of the \napproach is the idea of learning perception from supervision \ncontained in natural language. Methods which work on natural \nlanguage can learn passively from the supervision contained in the \nvast amount of text on the Internet. \nGiven a batch of N (image, text) pairs, CLIP is trained to \npredict which of the N×N possible (image, text) pairings across a \nbatch actually occurred. To do this, CLIP learns a multimodal \nembedding space by jointly training an image encoder and a text \nencoder to maximize the cosine similarity of the image and text\nembeddings of the N real pairs in the batch while minimizing the \ncosine similarity of the embeddings of the N 2-N incorrect pair-\nings. A symmetric cross-entropy loss over these similarity scores is \noptimized. \nTransformers and Visual Transformers 215\nTwo different architectures were considered for the image \nencoder. For the ﬁrst, ResNet-50 [ 36] is used as the base architec-\nture for the image encoder due to its widespread adoption and \nproven performance. Several modiﬁcations were made to the origi-\nnal version of ResNet. For the second architecture, ViT is used with \nsome minor modiﬁcations: ﬁrst, adding an extra layer normaliza-\ntion to the combined patch and position embeddings before the \ntransformer and, second, using a slightly different initialization \nscheme. \nThe text encoder is a standard transformer [ 4] (Subheading \n2.1) with the architecture modiﬁcations described in [ 35]. As a base \nsize, CLIP uses a 63M-parameter 12-layer 512-wide model with \neight attention heads. The transformer operates on a lowercased \nbyte pair encoding (BPE) representation of the text with a 49,152 \nvocab size [\n37]. The max sequence length is capped at 76. The text \nsequence is bracketed with [SOS] and [EOS] tokens, 4 and the \nactivations of the highest layer of the transformer at the [EOS] \ntoken are treated as the feature representation of the text which is \nlayer normalized and then linearly projected into the multimodal \nembedding space. \n5.1.3 DALL-E and \nDALL-E 2 \nDALL-E [38] is another example of the application of transformers \nin vision. It generates images from a natural language prompt— \nsome examples include “an armchair in the shape of an avocado” \nand “a penguin made of watermelon.” It uses a decoder-only \nmodel, which is similar to GPT-3 [\n39]. DALL-E uses 12 billion \nparameters and is pretrained on Conceptual Captions [ 34] with \nover 3.3 million text-image pairs. DALL-E 2 [ 40] is the upgraded \nversion of DALL-E, based on diffusion models and CLIP (Sub-\nheading 5.1.2), and allows better performances with more realistic \nand accurate generated images. In addition to producing more \nrealistic results with a better resolution than DALL-E, DALL-E \n2 is also able to edit the outputs. Indeed, with DALL-E 2, one can \nadd or remove realistically an element in the output and can also \ngenerate different variations of the same output. These two models \nclearly demonstrate the powerful nature and scalability of transfor-\nmers that are capable of efﬁciently processing a web-scale amount \nof data. \n4 [SOS], start of sequence; [EOS], end of sequence\n216 Robin Courant et al.\n5.1.4 Flamingo Flamingo [ 41] is a visual language model (VLM) tackling a wide \nrange of multimodal tasks based on few-shot learning. This is an \nadaptation of large language models (LLMs) handling an extra \nvisual modality with 80B parameters. \nFlamingo consists of three main components: a vision encoder, \na perceiver resampler, and a language model. First, to encode \nimages or videos, a vision convolutional encoder [ 42] is pretrained \nin a contrastive way, using image and text pairs. 5 Then, inspired by \nthe perceiver architecture [ 25] (detailed in Subheading 1.3.2), the \nperceiver resampler takes a variable number of encoded visual fea-\ntures and outputs a ﬁxed-length latent code. Finally, this visual \nlatent code conditions the language model by querying language \ntokens through cross-attention blocks. Those cross-attention \nblocks are interleaved with pretrained and frozen language model \nblocks. \nThe whole model is trained using three different kinds of \ndatasets without annotations (text with image content from web-\npages [ 41], text and image pairs [ 41, 43], and text and video pairs \n[41]). Once the model is trained, it is ﬁne-tuned using few-shot \nlearning techniques to tackle speciﬁc tasks. \n5.2 Video Attention Video understanding is a long-standing problem, and despite \nincredible computer vision advances, obtaining the best video rep-\nresentation is still an active research area. Videos require employing \neffective spatiotemporal processing of RGB and time streams to \ncapture long-range interactions [ 44, 45] while focusing on impor-\ntant video parts [ 46] with minimum computational resources [ 47]. \nTypically, video understanding beneﬁts from 2D computer \nvision, by adapting 2D image processing methods to 3D spatio-\ntemporal methods [\n48]. And through the Video Vision Trans-\nformer (ViViT) [ 49], history repeats itself. Indeed, with the rise \nof transformers [ 4] and the recent advances in image classiﬁcation \n[5], video transformers appear as logical successors of CNNs. \nHowever, in addition to the computationally expensive video \nprocessing, transformers also require a lot of computational \nresources. Thus, developing efﬁcient spatiotemporal attention \nmechanisms is essential [\n25, 49, 50]. \nIn t his s ection, we ﬁrst describe the general principle of video \ntransformers (Subheading 5.2.1), and then, we detail three differ-\nent attention mechanisms used for video representation (Subhead-\nings 5.2.2, 5.2.3, and 5.2.4). \n5 The text is encoded using a pretrained BERT model [ 10].\no\nTransformers and Visual Transformers 217\n5.2.1 General Principle Generally, inputs of video transformers are RGB video clips \nX ∈  F ×H ×W ×3 , with F frames of size H×W. \nTo begin with, video transformers split the input video clip \nX into ST tokens x i ∈  K , where S and T are, respectively, the \nnumber of tokens along the spatial and temporal dimension and \nK is the size of a token. \nTo do so, the simplest method extracts nonoverlapping 2D \npatches of size P×P from each frame [ 5], as used in TimeSformer \n[50]. This results in S=HW/P 2 , T=F, and K=P 2 . \nHowever, there exist more elegant and efﬁcient token extrac-\ntion methods for videos. For instance, in ViViT [ 49], the authors \npropose to extract 3D volumes from videos (involving T≠F)t \ncapture spatiotemporal information within tokens. In TokenLear-\nner [ 47], they propose a learnable token extractor to select the \nmost important parts of the video. \nOnce raw tokens x i are extracted, transformer architectures aim \nto map them into d-dimensional embedding vectors Z ∈  ST ×d \nusing a linear embedding E ∈  d ×K : \nZ =½z cls ,Ex 1,Ex 2,...,Ex ST /C138þ PE,ð10Þ \nwhere z cls ∈  d is a classiﬁcation token that encodes information \nfrom all tokens of a single sample [ 10] and PE ∈  ST ×d is a \npositional embedding that encodes the spatiotemporal position of \ntokens, since the subsequent attention blocks are permutation \ninvariant [ 4]. \nIn the end, embedding vectors Z pass through a sequence of \nL transformer layers. A transformer layer ℓ is composed of a series of \nmulti-head self-attention (MSA) [ 4], layer normalization \n(LN) [ 51], and MLP blocks: \nYℓ =MSAðLNðZ ℓÞÞ þ Z ℓ , \nZℓþ1 =MLPðLNðY ℓÞÞ þ Y ℓ :\nð11Þ \nIn this way, as shown in Fig. 2, we denote four different \ncomponents in a video transformer layer: the query-key-value \n(QKV) projection, the MSA block, the MSA projection, and the \nMLP. For a layer with h heads, the complexity of each component is \n[4]:\n QKV projection: Oðh:ð2STdd k þ STdd vÞ\n MSA: OðhS 2 T 2 :ðdk þ d vÞÞ\n MSA projection: OðSThd vdÞ\n MLP: O ðSTd 2Þ \nWe note that the MSA complexity is the most impacting com-\nponent, with a quadratic complexity with respect to the number of \ntokens. Hence, for comprehension and clarity purposes, in the rest \nof the section, we consider the global complexity of a video trans-\nformer with L layers to equal to OðLS 2 T 2Þ.\n218 Robin Courant et al.\nFig. 15 Full space-time attention mechanism. Embedding tokens at layer ℓ-1, \nZ(ℓ-1) are all fed simultaneously through a unique spatiotemporal attention \nblock. Finally, the spatiotemporal embedding is passed through an MLP and \nnormalized to output embedding tokens of the next layer, Z ℓ . Figure inspired \nfrom [ 50] \n5.2.2 Full Space-Time \nAttention \nAs described in [ 49, 50], full space-time attention mechanism is the \nmost basic and direct spatiotemporal attention mechanism. As \nshown in Fig. 15, it consists in computing self-attention across all \npairs of extracted tokens. \nThis method results in a heavy complexity of OðLS 2 T 2Þ \n[49, 50]. This quadratic complexity can fast be memory-\nconsuming, in which it is especially true when considering videos. \nTherefore, using full space-time attention mechanism is \nimpractical [ 50]. \n5.2.3 Divided Space-\nTime Attention \nA smarter and more efﬁcient way to compute spatiotemporal atten-\ntion is the divided space-time attention mechanism, ﬁrst described \nin [\n50]. \nAs shown in Fig. 16, it relies on computing spatial and temporal \nattention separately in each transformer layer. Indeed, we ﬁrst \ncompute the spatial attention, i.e., self-attention within each tem-\nporal index, and then the temporal attention, i.e., self-attention \nacross all temporal indices.\nTransformers and Visual Transformers 219\nFig. 16 Divided space-time attention mechanism. Embedding tokens at layer \nℓ-1, Z (ℓ-1) are ﬁrst processed along the temporal dimension through a ﬁrst \nMSA block, and the resulting tokens are processed along the spatial dimension. \nFinally, the spatiotemporal embedding is passed through an MLP and normalized \nto output embedding tokens of the next layer, Z ℓ . Figure inspired from [ 50] \nThe complexity of this attention mechanism is OðLST:ðS þ \nTÞÞ [ 50]. By separating the calculation of the self-attention over \nthe different dimensions, one tames the quadratic complexity of the \nMSA module. This mechanism highly reduces the complexity of a \nmodel with respect to the full space-time complexity. Therefore, it \nis reasonable to use it to process videos [ 50]. \n5.2.4 Cross-Attention \nBottlenecks \nAn even more reﬁned way to reduce the computational cost of \nattention calculation consists of using cross attention as a bottle-\nneck. For instance, as shown in Fig. 17 and mentioned in Subhead-\ning 3.2, the perceiver [ 25] projects the extracted tokens x i into a\nvery low-dimensional embedding through a cross-attention block \nplaced before the transformer layers. \n220 Robin Courant et al.\nFig. 17 Attention bottleneck mechanism. Raw input patches and embedding \ntokens at layer ℓ-1, Z (ℓ-1) are fed to a cross-attention block (CA) and then \nnormalized and projected. Finally, the resulting embedding is passed through a \ntransformer to output embedding tokens of the next layer, Z ℓ . Figure inspired \nfrom [ 25] \nHere, the cross-attention block placed before the L transformer \nlayers reduce the input dimension from ST to N, where N≪ST, 6 \nthus resulting in a complexity of OðSTNÞ. Hence, the total com-\nplexity of this attention block is OðSTN þ LN 2Þ. It reduces again \nthe complexity of a model with respect to the divided space-time \nattention mechanism. We note that it enables to design deep archi-\ntectures, as in the perceiver [ 25], and then it enables the extraction \nof higher-level features. \n5.2.5 Factorized Encoder Lastly, the factorized encoder [ 49] architecture is the most efﬁcient \nwith respect to the complexity/performance trade-off. \nAs i n divided s pace-time attention, the factorized encoder aims \nto compute spatial and temporal attention separately. Nevertheless, \nas shown in Fig. 18, instead of mixing spatiotemporal tokens in \neach transformer layer, here, there exist two separate encoders:\n6 In practice, N≤512 for perceiver [ 25], against ST=16×16×(32/2)=4096 for ViViT-L [ 49]\nFirst, a representation of each temporal index is obtained, thanks to \na spatial encoder with L s layers. Second, these tokens are passed \nthrough a temporal encoder with L t layers (i.e., L=L s +L t).\nTransformers and Visual Transformers 221\nFig. 18 Factorized encoder mechanism. First, a spatial transformer processes input tokens along the spatial \ndimension. Then, a temporal transformer processes the resulting spatial embedding along the temporal \ndimension. Figure inspired from [ 25] \nHence, the complexity of a such architecture has two main \ncomponents: the spatial encoder complexity of OðL s S2Þ and the \ntemporal encoder complexity of OðL t T 2Þ. It results in a global \ncomplexity of OðL s S2 þ L t T 2Þ. Thus, it leads to very lightweight \nmodels. However, as it ﬁrst extracts per-frame features and then \naggregates them to a ﬁnal representation, it corresponds to a late-\nfusion mechanism, which can sometimes be a drawback as it does \nnot mix spatial and temporal information simultaneously [ 52]. \n5.3 Video \nTransformers \nIn this section, we present two modern transformer-based archi-\ntectures for video classiﬁcation. We start by introducing the Time-\nSformer architecture in Subheading \n5.3.1 and then the ViViT \narchitecture in Subheading 5.3.2. \n5.3.1 TimeSformer TimeSformer [ 50] is one of the ﬁrst architectures with space-time \nattention that impacted the video classiﬁcation ﬁeld. It follows the \nsame structure and principle described in Subheading 5.2.1. \nFirst, i t t akes as input an RGB video clip sampled at a rate of \n1/32 and decomposed into 2D 16×16 patches. \nAs shown in Fig. 19, the TimeSformer architecture is based on \nthe ViT architecture (Subheading 2.3), with 12 12-headed MSA \nlayers. However, the added value compared to the ViT is that \nTimeSfomer uses the divided space-time attention mechanism (Sub-\nheading 5.2.3). Such attention mechanism enables to capture high-\nlevel spatiotemporal features while taming the complexity of the \nmodel. Moreover, the authors introduce three variants of the archi-\ntecture: (i) TimeSformer, the standard version of the model, that \noperates on 8 frames of 224×224; (ii) TimeSformer-L, a conﬁgu-\nration with high spatial resolution, that operates on 16 frames of \n448×448; and (iii) TimeSformer-HR, a long temporal range setup, \nthat operates on 96 frames of 224×224.\n222 Robin Courant et al.\nFig. 19 TimeSformer architecture. The TimeSformer ﬁrst projects input to \nembedding tokens, which are summed to positional embedding tokens. The \nresulting tokens are then passed through L divided space-time attention blocks \nand then linearly projected to obtain output probabilities \nFinally, the terminal classiﬁcation token embedding is passed \nthrough an MLP to output a probability for all video classes. \nDuring inference, the ﬁnal prediction is obtained by averaging the \noutput probabilities from three different spatial crops of the input \nvideo clip (top left, center, and bottom right). \nTimeSformer achieves similar state-of-the-art performances as \nthe 3D CNNs [ 53, 54] on various video classiﬁcation datasets, such \nas Kinetics-400 and Kinetics-600 [ 55]. Note the TimeSformer is \nmuch faster to train (416 training hours against 3840 hours [ 50] \nfor a SlowFast architecture [ 54]) and, also, more efﬁcient (0.59 \nTFLOPs against 1.97 TFLOPs [ 50] for a SlowFast architecture \n[53]). \n5.3.2 ViViT ViViT [ 49] is the main extension of the ViT [ 5] architecture \n(Subheading 2.3) for video classiﬁcation. \nFirst, the authors use a 16 tubelet embedding instead of a 2D \npatch embedding, as mentioned in Subheading 5.2.1. This alter-\nnate embedding method aims to capture the spatiotemporal\ninformation from the tokenization step, unlike standard architec-\ntures that fuse spatiotemporal information from the ﬁrst attention \nblock. \nTransformers and Visual Transformers 223\nFig. 20 ViViT architecture. The ViViT ﬁrst projects input to embedding tokens, \nwhich are summed to positional embedding tokens. The resulting tokens are ﬁrst \npassed through L s spatial attention blocks and then through L t temporal attention \nblocks. The resulting output is linearly projected to obtain output probabilities \nAs shown in Fig. 20, the ViViT architecture is based on factor-\nized encoder architecture (Subheading 5.2.5) and consists of one \nspatial and one temporal encoder operating on input clips with \n32 frames of 224×224. The spatial encoder uses one of the three \nViT variants as backbone. 7 For the temporal encoder, the number\n7 ViT-B: 12 12-headed MSA layers; ViT-L: 24 16-headed MSA layers; and ViT-H: 32 16-headed MSA layers.\nof layers does not impact much the performance, so that, according \nto the performance/complexity trade-off, the number MSA layers \nis ﬁxed at 4. The authors show that such architecture reaches high \nperformances while reducing drastically the complexity.\n224 Robin Courant et al.\nFinally, as in TimeSformer (Subheading 5.3.1), ViViT outputs \nprobabilities for all video classes through the last classiﬁcation token \nembedding and averages the obtained probabilities across three \ncrops of each input clip (top left, center, and bottom right). \nViViT outperforms both 3D CNNs [ 53, 54] and TimeSformer \n[50] on the Kinetics-400 and Kinetics-600 datasets [ 55]. Note the \ncomplexity of this architecture is highly reduced in comparison to \nother state-of-the-art models. For instance, the number of FLOPs \nfor a ViViT-L/16×16×2 is 3.89×10 12 against 7.14×10 12 for a \nTimeSformer-L [ 50] and 7.14×10 12 for a SlowFast [ 53] \narchitecture. \n5.4 Multimodal Video \nTransformers \nNowadays, one of the main gaps between artiﬁcial and human \nintelligence is the ability for us to process multimodal signals and \nto enrich the analysis by mixing the different modalities. Moreover, \nuntil recently, deep learning models have been focusing mostly on \nvery speciﬁc visual tasks, typically based on a single modality, such as \nimage classiﬁcation [\n5, 17, 18, 56, 57], audio classiﬁcation [ 25, 52, \n58, 59], and machine translation [ 10, 60–63]. These two factors \ncombined have pushed researchers to take up multimodal \nchallenges. \nThe default solution for multimodal tasks consists in ﬁrst cre-\nating an individual model (or network) per modality and then in \nfusing the resulting single-modal features together [ 64, 65]. Yet, \nthis approach fails to model interactions or correlations among \ndifferent modalities. However, the recent rise of attention [ 4, 5, \n49] is promising for multimodal applications, since attention per-\nforms very well at combining multiple inputs [ 25, 52, 66, 67]. \nHere, we present two main ways of dealing with several \nmodalities: \n1. Concatenating tokens from dif ferent modalities into one \nvector [ 25, 66]. The multimodal video transformer \n(MM-ViT) [ 66] combines raw RGB frames, motion features, \nand audio spectrogram for video action recognition. To do so, \nthe authors fuse tokens from all different modalities into a \nsingle-input embedding and pass it through transformer layers. \nHowever, a drawback of this method is that it fails to distin-\nguish well one modality to another. To overcome this issue, the \nauthors of the perceiver [ 25] propose to learn a modality \nembedding in addition to the positional embedding (see Sub-\nheadings 3.2 and 5.2.1). This allows associating each token\nTransformers and Visual Transformers 225\nwith its modality. Nevertheless, given that (i) the complexity of \na transformer layer is quadratic with respect to the number of \ntokens (Subheading 5.2.1) and (ii), with this method, the \nnumber of tokens is multiplied by the number of modalities, \nit may lead to skyrocketing computational cost [ 66]. \n2. Exploiting cross attention [ 52, 67, 68]. Several modern \napproaches exploit cross attention to mix multiple modalities, \nsuch as [ 52] for audio and video, [ 67] for text and video, and \n[68] for audio, text, and video. The commonality among all \nthese methods is that they exploit the intrinsic properties of \ncross attention by querying one modality with a key-value pair \nfrom the other one [ 52, 67]. This idea can be easily generalized \nto more than two modalities by computing cross attention \nacross each combination of modalities [ 68]. \n6 Conclusion \nAttention is an intuitive and efﬁcient technique that enables \nhandling local and global cues. \nOn this basis, the ﬁrst pure attention architecture, the trans-\nformer [ 4], has been designed for NLP purposes. Quickly, the \ncomputer vision ﬁeld has adapted the transformer architecture for \nimage classiﬁcation, by designing the ﬁrst visual transformer model: \nthe vision transformer (ViT) [\n5]. \nHowever, even if transformers naturally lead to high perfor-\nmances, the raw attention mechanism is a computationally greedy \nand heavy technique. For this reason, several enhanced and reﬁned \nderivatives of attention mechanisms have been proposed [\n21–26]. \nThen, r apidly, a w ide variety of other tasks have been con-\nquered by transformer-based architectures, such as object detection \n[14], image segmentation [ 27], self-supervised learning [ 28, 29], \nand image generation [ 30, 31]. In addition, transformer-based \narchitectures are particularly well suited to handle multidimen-\nsional tasks. This is because multimodal signals are easily combined \nthrough attention blocks, in particular vision and language cues \n[32, 35, 38] a nd spatiotemporal signals are also easily tamed, as in \n[25, 49, 50]. \nFor these reasons, transformer-based architectures enabled \nmany ﬁelds to make tremendous progresses in the last few years. \nIn the future, transformers will need to become more and more \ncomputationally efﬁcient, e.g., to be usable on cellphones, and will \nplay a huge role to tackle multimodal challenges and bridge \ntogether most AI ﬁelds.\n226 Robin Courant et al.\nReferences \n1. Bahdanau D, Cho K, Bengio Y (2015) Neural \nmachine translation by jointly learning to align \nand translate. In: International conference on \nlearning representations \n2. Cho K, van Merrie ¨ nboer B, Gulcehre C, \nBahdanau D, Bougares F, Schwenk H, Bengio \nY (2014) Learning phrase representations \nusing RNN encoder–decoder for statistical \nmachine translation. In: Empirical methods in \nnatural language processing, association for \ncomputational linguistics, pp 1724–1734 \n3. Sutskever I, Vinyals O, Le QV (2014) \nSequence to sequence learning with neural \nnetworks. In: Advances in neural information \nprocessing systems, vol 27 \n4. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, \nJones L, Gomez AN, Kaiser Ł, Polosukhin I \n(2017) Attention is all you need. In: Advances \nin neural information processing systems, \nvol 30 \n5. Dosovitskiy A, Beyer L, Kolesnikov A, \nWeissenborn D, Zhai X, Unterthiner T, \nDehghani M, Minderer M, Heigold G, \nGelly S, Uszkoreit J, Houlsby N (2021) An \nimage is worth 16×16 words: transformers \nfor image recognition at scale. In: International \nconference on learning representations \n6. Press O, Wolf L (2017) Using the output \nembedding to improve language models. In: \nProceedings of the 15th conference of the \nEuropean chapter of the association for \ncomputational linguistics: volume 2, short \npapers, association for computational linguis-\ntics, pp 157–163 \n7. Fedus W, Zoph B, Shazeer N (2021) Switch \ntransformers: scaling to trillion parameter \nmodels with simple and efﬁcient sparsity. arXiv \npreprint arXiv:210103961 \n8. Raffel C, Shazeer N, Roberts A, Lee K, \nNarang S, Matena M, Zhou Y, Li W, Liu PJ \n(2020) Exploring the limits of transfer learning \nwith a uniﬁed text-to-text transformer. J Mach \nLearn Res 21(140):1–67 \n9. Khan S, Naseer M, Hayat M, Zamir SW, Khan \nFS, Shah M (2021) Transformers in vision: a \nsurvey. ACM Comput Surv 24:200 \n10. Devlin J, Chang MW, Lee K, Toutanova K \n(2019) BERT: pre-training of deep bidirec-\ntional transformers for language \nunderstanding. In: Proceedings of the 2019 \nconference of the north American chapter of \nthe association for computational linguistics: \nhuman language technologies, volume \n1 (long and short papers), association for \ncomputational linguistics, pp 4171–4186 \n11. Wikimedia Foundation (2019) Wikimedia \ndownloads. \nhttps:/ /dumps.wikimedia.org \n12. Zhu Y, Kiros R, Zemel R, Salakhutdinov R, \nUrtasun R, Torralba A, Fidler S (2015) Align-\ning books and movies: towards story-like visual \nexplanations by watching movies and reading \nbooks. In: Proceedings of the international \nconference on computer vision, pp 19–27 \n13. Wang X, Girshick R, Gupta A, He K (2018) \nNon-local neural networks. In: Proceedings of \nthe IEEE conference on computer vision and \npattern recognition, pp 7794–7803 \n14. Carion N, Massa F, Synnaeve G, Usunier N, \nKirillov A, Zagoruyko S (2020) End-to-end \nobject detection with transformers. In: Pro-\nceedings of the European conference on com-\nputer vision. Springer, Berlin, pp 213–229 \n15. Ramachandran P, Parmar N, Vaswani A, \nBello I, Levskaya A, Shlens J (2019) Stand-\nalone self-attention in vision models. In: \nAdvances in neural information processing sys-\ntems, vol 32 \n16. Wang H, Zhu Y, Green B, Adam H, Yuille A, \nChen LC (2020) Axial-deeplab: stand-alone \naxial-attention for panoptic segmentation. In: \nProceedings of the European conference on \ncomputer vision. Springer, Berlin, pp 108–126 \n17. Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei \nL (2009) Imagenet: a large-scale hierarchical \nimage database. In: Proceedings of the IEEE \nconference on computer vision and pattern \nrecognition, pp 248–255 \n18. Krizhevsky A, Hinton G (2009) Learning mul-\ntiple layers of features from tiny images. Tech \nrep University of Toronto, Toronto, ON \n19. Sun C, Shrivastava A, Singh S, Gupta A (2017) \nRevisiting unreasonable effectiveness of data in \ndeep learning era. In: Proceedings of the inter-\nnational conference on computer vision, pp \n843–852 \n20. Selva J, Johansen AS, Escalera S, Nasrollahi K, \nMoeslund TB, Clape ´ s A (2022) Video trans-\nformers: a survey. arXiv preprint \narXiv:220105991 \n21. Touvron H, Cord M, Douze M, Massa F, \nSablayrolles A, Je´ gou H (2021) Training data-\nefﬁcient image transformers & distillation \nthrough attention. In: International confer-\nence on machine learning. PMLR, pp \n10347–10357\n22.\nTransformers and Visual Transformers 227\nd’Ascoli S, Touvron H, Leavitt ML, Morcos \nAS, \nBiroli G, Sagun L (2021) Convit: improv-\ning vision transformers with soft convolutional \ninductive biases. In: International conference \non machine learning. PMLR, pp 2286 –2296 \n23. Hassani A, Walton S, Shah N, Abuduweili A, \nLi J, Shi H (2021) Escaping the big data para-\ndigm with compact transformers. arXiv pre-\nprint arXiv:210405704 \n24. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, \nLin \nS, Guo B (2021) Swin transformer: hierar-\nchical vision transformer using shifted \nwindows. In: Proceedings of the international \nconference on computer vision \n25. Jaegle A, Gimeno F, Brock A, Vinyals O, \nZisser\nman A, Carreira J (2021) Perceiver: gen-\neral perception with iterative attention. In: \nInternational conference on machine learning. \nPMLR, pp 4651–4664 \n26. Jaegle A, Borgeaud S, Alayrac JB, Doersch C, \nIonescu \nC, Ding D, Koppula S, Zoran D, \nBrock A, Shelhamer E et al (2021) \nPerceiver IO: a general architecture for \nstructured inputs & outputs. arXiv preprint \narXiv:210714795 \n27. Strudel R, Garcia R, Laptev I, Schmid C \n(2021) \nSegmenter: transformer for semantic \nsegmentation. In: Proceedings of the interna-\ntional conference on computer vision, pp \n7262–7272 \n28. Caron M, Touvron H, Misra I, Je ´ gou H, \nMairal J, Bojanowski P, Joulin A (2021) \nEmerging properties in self-supervised vision \ntransformers. In: Proceedings of the interna-\ntional conference on computer vision \n29. He K, Chen X, Xie S, Li Y, Dolla ´ r P, Girshick R \n(2021) Masked autoencoders are scalable \nvision learners. arXiv preprint \narXiv:211106377 \n30. Hudson DA, Zitnick L (2021) Generative \nadversarial \ntransformers. In: International con-\nference on machine learning. PMLR, pp \n4487–4499 \n31. Zhang B, Gu S, Zhang B, Bao J, Chen D, \nW\nen F, Wang Y, Guo B (2022) Styleswin: \ntransformer-based GAN for high-resolution \nimage generation. In: Proceedings of the \nIEEE conference on computer vision and pat-\ntern recognition \n32. Lu J, Batra D, Parikh D, Lee S (2019) Vilbert: \nPretraining \ntask-agnostic visiolinguistic repre-\nsentations for vision-and-language tasks. In: \nAdvances in neural information processing sys-\ntems, vol 32 \n33. Ren S, He K, Girshick R, Sun J (2015) Faster \nR-CNN: \ntowards real-time object detection \nwith region proposal networks. In: Advances \nin neural information processing systems, \nvol 28 \n34. Sharma P, Ding N, Goodman S, Soricut R \n(2018) \nConceptual captions: a cleaned, hyper-\nnymed, image alt-text dataset for automatic \nimage captioning. In: Proceedings of ACL \n35. Radford A, Kim JW, Hallacy C, Ramesh A, \nGoh \nG, Agarwal S, Sastry G, Askell A, \nMishkin P, Clark J et al (2021) Learning trans-\nferable visual models from natural language \nsupervision. In: International conference on \nmachine learning. PMLR, pp 8748 –8763 \n36. He X, Peng Y (2017) Fine-grained image clas-\nsiﬁcation\nvia combining vision and \nlanguage. In: Proceedings of the IEEE confer-\nence on computer vision and pattern recogni-\ntion, pp 5994–6002 \n37. Sennrich R, Haddow B, Birch A (2016) Neural \nmachine \ntranslation of rare words with sub-\nword units. In: Proceedings of the 54th annual \nmeeting of the association for computational \nlinguistics (volume 1: long papers), association \nfor computational linguistics, pp 1715 –1725 \n38. Ramesh A, Pavlov M, Goh G, Gray S, Voss C, \nRadford \nA, Chen M, Sutskever I (2021) Zero-\nshot text-to-image generation. In: Interna-\ntional conference on machine learning. \nPMLR, pp 8821 –8831 \n39. Brown T, Mann B, Ryder N, Subbiah M, \nKaplan \nJD, Dhariwal P, Neelakantan A, \nShyam P, Sastry G, Askell A, et al (2020) Lan-\nguage models are few-shot learners. In: \nAdvances in neural information processing sys-\ntems, vol 33, pp 1877–1901 \n40. Ramesh A, Dhariwal P, Nichol A, Chu C, Chen \nM \n(2022) Hierarchical text-conditional image \ngeneration with clip latents. arXiv preprint \narXiv:220406125 \n41. Alayrac JB, Donahue J, Luc P, Miech A, Barr I, \nHasson \nY, Lenc K, Mensch A, Millican K, Rey-\nnolds M et al (2022) Flamingo: a visual lan-\nguage model for few-shot learning. arXiv \npreprint arXiv:220414198 \n42. Brock A, De S, Smith SL, Simonyan K (2021) \nHigh-per\nformance large-scale image recogni-\ntion without normalization. In: International \nconference on machine learning. PMLR, pp \n1059–1071 \n43. Jia C, Yang Y, Xia Y, Chen YT, Parekh Z, \nPham H, Le Q, Sung YH, Li Z, Duerig T \n(2021) Scaling up visual and vision-language\n228 Robin Courant et al.\nrepresentation learning with noisy text \nsupervision. In: International conference on \nmachine learning. PMLR, pp 4904–4916 \n44. Epstein D, Vondrick C (2021) Learning goals \nfrom failure. In: Proceedings of the IEEE con-\nference on computer vision and pattern \nrecognition \n45. Marin-Jimenez MJ, Kalogeiton V, Medina-\nSuarez P, Zisserman A (2019) Laeo-net: revi-\nsiting people looking at each other in \nvideos. In: Proceedings of the IEEE conference \non computer vision and pattern recognition \n46. Nagrani A, Yang S, Arnab A, Jansen A, \nSchmid C, Sun C (2021) Attention bottlenecks \nfor multimodal fusion. In: Advances in neural \ninformation processing systems \n47. Ryoo M, Piergiovanni A, Arnab A, \nDehghani M, Angelova A (2021) Tokenlear-\nner: Adaptive space-time tokenization for \nvideos. In: Advances in neural information pro-\ncessing systems, vol 34 \n48. Hara K, Kataoka H, Satoh Y (2018) Can spa-\ntiotemporal 3d CNNs retrace the history of 2d \nCNNs and imagenet? In: Proceedings of the \nIEEE conference on computer vision and pat-\ntern recognition, pp 6546–6555 \n49. Arnab A, Dehghani M, Heigold G, Sun C, \nLucˇic´ M, Schmid C (2021) Vivit: a video vision \ntransformer. In: Proceedings of the interna-\ntional conference on computer vision, pp \n6836–6846 \n50. Bertasius G, Wang H, Torresani L (2021) Is \nspace-time attention all you need for video \nunderstanding? In: International conference \non machine learning \n51. Ba JL, Kiros JR, Hinton GE (2016) Layer nor-\nmalization. arXiv preprint arXiv:160706450 \n52. Nagrani A, Yang S, Arnab A, Jansen A, \nSchmid C, Sun C (2021) Attention bottlenecks \nfor multimodal fusion. In: Advances in neural \ninformation processing systems, vol 34 \n53. Feichtenhofer C, Fan H, Malik J, He K (2019) \nSlowfast networks for video recognition. In: \nProceedings of the international conference \non computer vision, pp 6202–6211 \n54. Carreira J, Zisserman A (2017) Quo vadis, \naction recognition? A new model and the kinet-\nics dataset. In: Proceedings of the IEEE con-\nference on computer vision and pattern \nrecognition, pp 6299–6308 \n55. Kay W, Carreira J, Simonyan K, Zhang B, \nHillier C, Vijayanarasimhan S, Viola F, \nGreen T, Back T, Natsev P et al (2017) The \nkinetics human action video dataset. arXiv pre-\nprint arXiv:170506950 \n56. Wu H, Xiao B, Codella N, Liu M, Dai X, \nYuan L, Zhang L (2021) CvT: introducing \nconvolutions to vision transformers. In: Pro-\nceedings of the international conference on \ncomputer vision, pp 22–31 \n57. Touvron H, Cord M, Sablayrolles A, \nSynnaeve G, Je ´ gou H (2021) Going deeper \nwith image transformers. In: Proceedings of \nthe international conference on computer \nvision, pp 32–42 \n58. Gemmeke JF, Ellis DP, Freedman D, Jansen A, \nLawrence W, Moore RC, Plakal M, Ritter M \n(2017) Audio set: an ontology and human-\nlabeled dataset for audio events. In: IEEE \ninternational conference on acoustics, speech \nand signal processing (ICASSP). IEEE, pp \n776–780 \n59. Gong Y, Chung YA, Glass J (2021) AST: audio \nspectrogram transformer. In: Proceedings of \ninterspeech 2021, pp 571–575 \n60. Bojar O, Buck C, Federmann C, Haddow B, \nKoehn P, Leveling J, Monz C, Pecina P, \nPost M, Saint-Amand H, et al (2014) Findings \nof the 2014 workshop on statistical machine \ntranslation. In: Proceedings of the 9th work-\nshop on statistical machine translation, pp \n12–58 \n61. Liu X, Duh K, Liu L, Gao J (2020) Very deep \ntransformers for neural machine translation. \narXiv preprint arXiv:200807772 \n62. Edunov S, Ott M, Auli M, Grangier D (2018) \nUnderstanding back-translation at scale. In: \nEmpirical methods in natural language proces-\nsing, association for computational linguistics, \npp 489–500 \n63. Lin Z, Pan X, Wang M, Qiu X, Feng J, \nZhou H, Li L (2020) Pre-training multilingual \nneural machine translation by leveraging align-\nment information. In: Empirical methods in \nnatural language processing, association for \ncomputational linguistics, pp 2649–2663 \n64. Owens A, Efros AA (2018) Audio-visual scene \nanalysis with self-supervised multisensory \nfeatures. In: Proceedings of the European con-\nference on computer cision, pp 631–648 \n65. Ramanishka V , Das A, Park DH, \nVenugopalan S, Hendricks LA, Rohrbach M, \nSaenko K (2016) Multimodal video \ndescription. In: Proceedings of the ACM inter-\nnational conference on multimedia, pp \n1092–1096\nOpen Access This chapter is licensed under the term s of the Creative Commons Attribution 4.0 International\nLicense (http:/ /creativecommons.org/licenses/by/4. 0/), which permits use, sharing, adaptation, distribution,\nand reproduction in any medium or format, as long as y ou give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licens e, and indicate if changes were made. The images or other\nthird-party material in this chapter are included in t he chapter’s Creative Commons license, unless indicated\notherwise in a credit line to the material. If material is n ot included in the chapter’s Creative Commons license and\nyour intended use is not permitted by statutory regula tion or exceeds the permitted use, you will need to obtain\npermission directly from the copyright holder. \nTransformers and Visual Transformers 229\n66. Chen J, Ho CM (2022) Mm-vit: Multi-modal \nvideo \ntransformer for compressed video action \nrecognition. In: Proceedings of the IEEE/ \nCVF winter conference on applications of com-\nputer vision, pp 1910 –1921 \n67. Narasimhan M, Rohrbach A, Darrell T (2021) \nClip-it!\nlanguage-guided video \nsummarization. In: Advances in neural infor-\nmation \nprocessing systems, vol 34 \n68. Liu ZS, C ourant R, Kalogeiton V (2022) Fun-\nnyNet: Audiovisual Learning of Funny \nMoments in Videos. In: Proceedings of the \nAsian conference on computer vision. Springer, \nMacau, China, pp 3308 –3325"
}