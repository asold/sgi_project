{
  "title": "Multi-Layer Transformer Aggregation Encoder for Answer Generation",
  "url": "https://openalex.org/W3025780271",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2968480656",
      "name": "Shengjie Shang",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2108491511",
      "name": "Jin Liu",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2124981515",
      "name": "Yihe Yang",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2968480656",
      "name": "Shengjie Shang",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2108491511",
      "name": "Jin Liu",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2124981515",
      "name": "Yihe Yang",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2626154462",
    "https://openalex.org/W6729912110",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6632455782",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W6735887044",
    "https://openalex.org/W6735889696",
    "https://openalex.org/W6729529396",
    "https://openalex.org/W6731601432",
    "https://openalex.org/W6729654139",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2963344337",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2801540583",
    "https://openalex.org/W6743600072",
    "https://openalex.org/W2963938442",
    "https://openalex.org/W6752345439",
    "https://openalex.org/W6776813201",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2251349042",
    "https://openalex.org/W2950438065",
    "https://openalex.org/W6678890848",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W6685536428",
    "https://openalex.org/W2361488420",
    "https://openalex.org/W6635189695",
    "https://openalex.org/W2528947955",
    "https://openalex.org/W2764004791",
    "https://openalex.org/W6751097180",
    "https://openalex.org/W2251202616",
    "https://openalex.org/W6726253136",
    "https://openalex.org/W2963779652",
    "https://openalex.org/W2605443677",
    "https://openalex.org/W6755275063",
    "https://openalex.org/W2251287417",
    "https://openalex.org/W2809057686",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963682631",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2600024417",
    "https://openalex.org/W2516930406",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2556691798",
    "https://openalex.org/W2548872772",
    "https://openalex.org/W2753329127",
    "https://openalex.org/W3020506479",
    "https://openalex.org/W4295253143",
    "https://openalex.org/W2173361515",
    "https://openalex.org/W1591825359",
    "https://openalex.org/W2898006300",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2601454101",
    "https://openalex.org/W2566011400",
    "https://openalex.org/W2964064432",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2125436846"
  ],
  "abstract": "Answer generation is one of the most important tasks in natural language processing, and deep learning-based methods have shown their strength over traditional machine learning based methods. However, most previous deep learning-based answer generation models were built on traditional recurrent neural networks or convolutional neural networks. The former model cannot well exploit contextual correlation preserved in paragraphs due to their inherent computation complexity. For the latter, since the size of the convolutional kernel is fixed, the model cannot extract complete semantic information features. In order to alleviate this problem, based on multi-layer Transformer aggregation coder, we propose an end-to-end answer generation model (AG-MTA). AG-MTA consists of a multi-layer attention Transformer unit and a multi-layer attention Transformer aggregation encoder (MTA). It can focus on information representation at different positions and aggregate nodes at same layer to combine the context information. Thereby, it fuses semantic information from base layer to top layer, enhancing the information representation of the encoder. Furthermore, based on trigonometric function, a novel position encoding method is also proposed. Experiments are conducted on public datasets SQuAD. AG-MTA reaches the state-of-the-art performance, EM score achieves 71.1 and F1 score achieves 80.3.",
  "full_text": "Received April 8, 2020, accepted May 4, 2020, date of publication May 11, 2020, date of current version May 26, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.2993875\nMulti-Layer Transformer Aggregation\nEncoder for Answer Generation\nSHENGJIE SHANG\n , JIN LIU\n , (Member, IEEE), AND YIHE YANG\nCollege of Information Engineering, Shanghai Maritime University, Shanghai 201306, China\nCorresponding author: Jin Liu (jinliu@shmtu.edu.cn)\nThis work was supported by the National Natural Science Foundation of China under Grant 61872231 and Grant 61701297.\nABSTRACT Answer generation is one of the most important tasks in natural language processing, and\ndeep learning-based methods have shown their strength over traditional machine learning based methods.\nHowever, most previous deep learning-based answer generation models were built on traditional recurrent\nneural networks or convolutional neural networks. The former model cannot well exploit contextual\ncorrelation preserved in paragraphs due to their inherent computation complexity. For the latter, since the\nsize of the convolutional kernel is ﬁxed, the model cannot extract complete semantic information features.\nIn order to alleviate this problem, based on multi-layer Transformer aggregation coder, we propose an end-to-\nend answer generation model (AG-MTA). AG-MTA consists of a multi-layer attention Transformer unit and\na multi-layer attention Transformer aggregation encoder (MTA). It can focus on information representation\nat different positions and aggregate nodes at same layer to combine the context information. Thereby,\nit fuses semantic information from base layer to top layer, enhancing the information representation of the\nencoder. Furthermore, based on trigonometric function, a novel position encoding method is also proposed.\nExperiments are conducted on public datasets SQuAD. AG-MTA reaches the state-of-the-art performance,\nEM score achieves 71.1 and F1 score achieves 80.3.\nINDEX TERMS Question answering system, natural language processing, self-attention mechanism,\ntransformer coding structure.\nI. INTRODUCTION\nQuestion answering(Q&A) system is built on the basis\nof understanding of the questions. It generates answers\nby searching existing knowledge bases such as knowl-\nedge graph, databases, or even internet, making knowledge\nacquirement more direct, efﬁcient, and accurate. With the\ncontinuous development of question answering system, many\nnovel methods have been developed. The most notable work\nis the Match-LSTM [1] framework. Then, the QANet [2]\nimproves the speed and accuracy of answer generation\nby combining Convolutional Neural Network (CNN) and\nLSTM, and achieved reliable results on the SQuAD [3]\ndataset. Recent method [4] combines the CNN network\nand attention mechanism for Chinese question classiﬁcation,\nwhich boosts the effect of the answer generation.\nMost of the current research work is based on typical\nneural networks to deal with tasks such as intent classiﬁca-\ntion and answer generation. However, these methods have\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Arianna DUlizia\n .\ndisadvantage in utilizing contextual correlation. In this paper,\nin order to enhancing the relevance of contextual informa-\ntion, we propose a novel multi-layer attention Transformer\naggregation encoder (MTA), and a novel answer generation\nnetwork based on MTA encoder (AG-MTA). The main con-\ntributions of this paper are as follows:\n1. A multi-layer attention Transformer aggregation\nencoder (MTA) is proposed to utilize contextual information\nat different layers to model the sequences.\n2. Multi-layer attention and feedforward layer are designed\nto pay attention to different subspaces’ information based on\nthe Transformer unit structure.\n3. A novel position encoding method that make use of\nthe absolute position and relative position information by\nencoding the position of each word.\n4. Multi-layer attention transformer units are proposed to\nenhance the context representation and solves the problem of\ninformation loss.\nThe related works are discussed in section 2. Section 3\npresents AG-MTA model. Section 4 presents evaluation of\nAG-MTA based answer generation system and discussion of\n90410 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 8, 2020\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\nthe experiment results. Finally, we draw some conclusion in\nsection 5.\nII. RELATED WORK\nAlong with recent advancement in question answering\nmethod, much progress has been made on answer generation.\nYu et al. [5] proposed a method which matches questions\nand answers by considering semantic coding of problems.\nAt the same time, the application of LSTM has made\nmuch progress in the Q&A system. Tan et al. [6] enhance\nthe composite representation of the model by connecting\nthe LSTM network with the convolutional neural network.\nLiu et al. [7] proposed a method that applied dynamic\nLSTM networks to solve the problem of long-range depen-\ndence of RNN. Lende and Raghuwanshi [8] proposed a\nclosed domain Q&A system for processing documents about\nthe education acts, and improves the accuracy of retrieval\nanswers by using NLP techniques. In particular, the remark-\nable improvement for reading comprehension in the long\ntext has also led to the improvement of answer generation\nmethods. Relying on efﬁcient neural network models, these\nmethods perform well in the answer generation task.\nWang and Nyberg [9] proposed a method to solve the\nanswer selection problem. This method mainly uses bidi-\nrectional Long-Short Term Memory network, without any\nexternal knowledge resources. However, this model requires\nlong-time training and may result in loss of information.\nWang and Jiang [1] proposed a network structure called\nMATCH-LSTM, which is mainly used to answer the question\nthat need to ﬁnd continuous words in the article. However,\nthis method is difﬁcult to predict longer answers.\nRecently, attention mechanism has also been introduced\nto answer generation. Seo et al. [10] proposed a complex\nnetwork model based on Bi-Directional Attention Flow\n(Bi-DAF). The model contains the Query2Context\nmodule, similar to Context2Query, which can perform\nattention calculation on query by context information.\nDhingra et al. [11] proposed a new attention model Gate-\nAttention Reader, which utilized attention mechanisms to\nconnect query and paragraph information, thereby enhancing\nthe information representation of each dimension in word\nembedding. Vaswani et al.[12] proposed a new self-attention\nencoder and decoder model, replacing LSTM and CNN\nmodels. The experiment results prove the effectiveness of\nthe method which can provide new ideas and solutions for\nNLP ﬁeld.\nIn addition, other studies also proposed different machine\nlearning methods and different answer generation architec-\ntures. Aiming at the problem of gradient explosion when\nneural network updating a larger number of word vectors,\nLiu et al.[13] proposed an algorithm for accelerating neural\nnetwork parameter convergence based on stochastic conju-\ngate gradients. Wang et al. [14] proposed an end-to-end\nmodel called R3 that uses the reinforcement learning frame-\nwork to combine phrase sorting method and answer genera-\ntion module, while traditional approach sorts the document\nﬁrst and then generate the answer. Yang et al. [15] uses\nsemi-supervised learning method to generate questions based\non the unlabeled text. This method not only increases the\namount of training data but also achieves satisfactory results.\nGhaeini et al. [16] designed a question answering network\nbased on the gated. To improve the accuracy of the answer,\nthis method established the interdependence between docu-\nments and queries. In order to make full use of various types\nof knowledge, Zhong et al.[17] proposed a graph algorithm\nto enhance the accuracy of the question answering system.\nAs presented above, the existing encoder models use only\nthe top layer output information of the network, losing infor-\nmation available in other layers. Related research shows that\ndifferent network layers can capture different levels of seman-\ntic information in the sequence. Therefore, it is necessary to\nadd some useful sequence information of the base layer into\nthe coding result [18]–[20].\nOn the other hand, some methods use convolutional\nneural network models or simple attention mechanisms to\nextract text information, which can signiﬁcantly shorten the\ntraining time of the model, and the performance of these\nmodels is roughly the same as that of the RNN network.\nBell and Penchas [21] proposed a method that can capture\nthe local dependencies well, it replaced the RNN in the read-\ning comprehension model with fully convolutional network.\nZhou et al.[22] proposed a method to capture both semantic\ninformation and semantic correlations between questions and\nanswers. In addition, Tay et al.[23] designed multi-cast atten-\ntion networks to improve the training performance, which can\nbe used in many tasks in the Q&A ﬁeld. Dong et al. [24]\nproposed the multi-column convolutional neural net-\nworks, which can extract features between questions and\nanswers at different layers and captures information well.\nHe and Golub [25] show that the character-level encoder-\ndecoder framework can be applied to the Q&A system.\nIn summary, most of the above methods use LSTM and\nCNN networks to generate answers directly, and fail to\nexploit the context information and the relations existing\namong the whole article and queries. To solve this problem,\nthe AG-MTA model is proposed, which combines the context\ninformation with the different levels of semantic information.\nIII. METHOD\nA. PROBLEM DEFINITION\nFor the answer generation task in the Q&A system,\nwe describe the formal problem deﬁnition as follows.\nA context material paragraph can be deﬁned as CTX :\nCTX ={ctx1,ctx2,..., ctxn} (1)\nwhere n is the number of words in the paragraph; and we\ndeﬁne the question as :\nQ ={q1,q2,..., qm} (2)\nwhere m is the number of words in the question. The model\noutputs a sub-sequence S from paragraph CTX according to\nthe question Q, the sequence S is the sequence of answers\nVOLUME 8, 2020 90411\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\nFIGURE 1. AG-MTA architecture. In the first part, we encode input passage and input questions respectively through the character embedding layer and\nword embedding layer. Where the dimensions of the character embedding layer is set to p2= 200 and the word embedding layer with a number of\ndimension p1 = 300. Then we add the positional encoding, so that the attention mechanism can take into account the positional order information.\nIn the second part, the model obtains the abstract semantic information by MTA and then learn the connection between context and query through the\ncontext-query attention layer. The model sends the information sequence into the coding layer which consists of three layers of MTAs to learn semantic\ninformation from the base layer to the top layer. Finally, the model obtains the start and end positions of the answer in the text through the softmax\nfunction.\ngenerated by the model based on the question and the para-\ngraph. So we deﬁne the answer S as:\nS ={ctxi,ctxi+1,..., ctxi+k } (3)\nwhere i, krepresent the starting position and ending position\nof the answer in the paragraph. In Fig. 2, we describe the\nprocess of answer generation.\nB. ANSWER GENERATION NETWORK BASED ON MTA\nThe architecture of AG-MTA is shown in Fig. 1. It mainly\nconsists of positional encoding, embedding layer, multiple\ntransformers aggregation encoder, and context-query atten-\ntion modules.\nAs shown in Fig. 1, The ﬁrst part is to convert article\ninformation into a corresponding relationship matrix through\nthe character embedding layer and the word embedding layer.\nThe word embedding layer uses a pre-trained Glove [26]\nword vector with number of dimension p1, and the dimen-\nsions of the character embedding layer is set to p2. The word\nvector corresponding to a word w is xw, and each character\nvector is recorded as xc. Then we randomly initialize the\ncharacter vector xc and add it to the model.\nIn the meantime, each word can be seen as a connection\nto each character vector. We ﬁxed the length of each word\nto a constant j. Thus, the word w can also be represented as\na matrix of p2 ∗j, which is the combination of the character\nvectors. Therefore, the ﬁnal word vector [ xw;xc]∈Rp1+p2 for\nthe word w can be obtained by concatenating xw and xc.\nFinally, the method adds the result to the positional encod-\ning vector to obtain the ﬁnal input sequence information.\nLocation information is especially important for attention\nmechanisms. For example, the words ‘‘Tom broke the vase\non the table’’ and ‘‘The vase broke the Tom on the table’’\nare almost same for attention mechanism. But the mean-\ning of these two sentences are entirely different. Therefore,\nwe introduce a new mechanism, a novel position encoding,\nto number the position of each word. By using the parity of\ntrigonometric function, position information is introduced for\neach word by combining the position vector and the word\nvector. Therefore, by utilizing its information, the attention\nmechanism can distinguish words at different positions.\n90412 VOLUME 8, 2020\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\nFIGURE 2. The process of answer generation, the answer S is a\nsub-sequence from paragraph CTX according to the question Q.\nThe calculation method for position encoding vector\nPE express as follows :\nPE(pos,2i) =sin\n(\npos/100002i/d\n)\n(4)\nPE(pos,2i+1) =cos\n(\npos/100002i/d\n)\n(5)\nwhere pos represents the position of the word, i represents the\ndimension of the i-th word, and d represents the dimension\nof the word vector. In addition to being able to express the\nabsolute position of the sequence, the above equation can\nalso express relative position relationships. We can explain\nthe relationship by the following equation.\nsin(α+β) =sin α∗cos β+cos α∗sin β (6)\ncos(α+β) =cos α∗cos β−sin α∗sin β (7)\nwe set the position vector p and q, where q = p +k,\nand k is the distance from p to q. According to formula (6),\nthe sin( q) =sin ( p +k). Therefore, position vector q can\nbe expressed as the linear change of position vector p, thus\nrepresenting the relative position information.\nIn the second part, questions Q, ﬁnal word vectors [ xw;xc]\nand position vectors PE are used as inputs to the\nMTA module. By using the multi-layer attention to learn\ndifferent layer information and capture the semantic informa-\ntion of different types, the model can obtain the high-level\nsemantic information of the whole sequence. After that,\nwe send the result of the question code Q(query) and the\narticle context code C (context) which obtained by the MTA\nmodule to the context-query attention layer for learning the\nquestion and answer information. Inspired by QANet [2], this\nmodule can learn the associations between context and query\neffectively, and obtain keywords that describe the relationship\nbetween the query and the context. The module contains two\ncalculation schemes: context-to-query attention A and query-\nto-context attention B. By using the above two calculation\nschemes, we can obtain the similarity matrix of query and\ncontext, which can enhance the relevance of query and con-\ntext. The formal expression is as follows :\nA =softmax (SM,axis =row) ·QT (8)\nB =A ·softmax(SM,axis =column)T ·CT (9)\nwhere SM(n ∗m) is the similarity matrix function between\ncontext and query, n is the length of context, and m is the\nlength of query. The function SM can be described as follows :\nSMi,j =f (Q,C) =W0[Q,C,Q ⊙C] (10)\nwhere W0 is a trainable variable and ⊙is the element-wise\nproduct.\nThen, we send the result to the coding layer which consists\nof 3 MTA modules to learn the relationship between context\nand query from a global perspective. The three MTAs output\nM0,M1 and M2 respectively. Finally, the result will be sent\nto two softmax functions to get the start position and end\nposition of the target answer in the article paragraph. The\nformal express as follows :\nposstart =softmax (Wstart [M0;M1]) (11)\nposend =softmax (Wend [M0;M2]) (12)\nThe model’s loss function can be expressed as :\nL(θ) =−1\nN\nN∑\ni\n[\nlog\n(\npstart\nystart\ni\n)\n+log\n(\npend\nyend\ni\n)]\n(13)\nwhere ystart\ni , yend\ni represent the start and end positions of the\nanswer in the context.\nC. MULTI-LAYER ATTENTION TRANSFORMER UNIT\nIn order to understand and make full use of the output infor-\nmation of each layer of the network, we add a multi-layer\nattention method based on Transformer [12]. As shown\nin Fig. 3, the architecture uses Transformer structure as a base\nnetwork. It uses a combination of multi-head attention mech-\nanism and feedforward neural network to model sequences.\nOur method can use the multi-layer attention to learn different\nlayer information and capture the semantic information of\ndifferent levels in the sequence.\nFor the basic Transformer building blocks that contain a\nset of self-attention mechanisms and feedforward networks,\nwe have the following deﬁnitions :\nMl =LayerNorm\n(\nAttention\n(\nQl−1,Kl−1,V l−1\n)\n+T l−1\n)\n(14)\nT l =LayerNorm\n(\nFFN\n(\nMl\n)\n+Ml\n)\n(15)\nwhere LayerNorm() is a layer normalization function,\nAttention() is a self-attention calculation function, and\nVOLUME 8, 2020 90413\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\nFIGURE 3. Multi-layer attention transformer unit. We changed the\nsingle-layer self-attention to a multi-layer attention in transformer and\nconnected the layers in a fully connected manner. We also add the\nsequence information output from each layer to the next attention layer\nwhich strengthened the utilization of the network at different layers and\nreduces the loss of information.\nFFN() is a feedforward neural network with a ReLU function\nas an activation function. Also, Ql−1,Kl−1,V l−1 are the\nquery, key and value vectors transformed from the previous\nlayer T l−1, and they are also initialization parameters of\nAttention().\nWe ﬁrst reconstruct the basic Transformer unit structure.\nIn order to obtain key information of query and context,\nwe changed the single-layer self-attention mechanism to a\nmulti-layer attention mechanism, and fully interconnects all\nlayers. The data processing in the multi-layer attention mech-\nanism can be formally deﬁned as follows :\nAl\n−1 =Attention\n(\nQl−1,Kl−1,V l−1\n)\nAl\n−2 =Attention\n(\nQl−2,Kl−2,V l−2\n)\n··· ···\nAl\n−k =Attention\n(\nQl−k ,Kl−k ,V l−k\n)\nAl =Aggregation\n(\nAl\n−1,Al\n−2,··· ,Al\n−k\n)\n(16)\nwhere Al\n−k is the result calculated by the attention function of\nthe l-k layer, and Aggregation() is an aggregate function that\nuniﬁes the results of each layer. The calculation method is as\nfollows:\nAggregation(x1,x2,..., xk )\n=LayerNorm\n(\nFFN ([x1;x2;... ;xk ])+\nk∑\ni=1\nxi\n)\n(17)\nwe ﬁrst concatenate x1,x2,..., xk , then send them to the\nfeedforward neural network with sigmoid as the activation\nfunction, and accumulate all the inputs. Finally, we use the\nlayer normalization function to get the result.\nThe reason why we use a fully connected layer for the\nmulti-layered attention layer instead of a residual connection\nis as follows:\n1. Use fully connected layer can spread loss directly to the\nbase layer for easy training.\n2. The coding information of each layer is an aggregation\nof all the previous layers, and retains key information of all\nlayers.\n3. The ﬁnal coding result relies on representations from all\nlayers, including both sophisticated and simple features.\nBy using the Multi-head Attention mechanism, the model\ncan pay attention to the representation information of differ-\nent subspaces from different locations. The speciﬁc calcula-\ntion method is as follows :\nMultiHead(Q,K,V ) =Concat (head1,··· ,headh)W o\n(18)\nheadi =Attention\n(\nQW Q\ni ,KW K\ni ,VW V\ni\n)\n(19)\nAttention(Q,K,V ) =softmax\n(QKT\n√dk\n)\nV (20)\nwhere W Q\ni ,W K\ni ,W V\ni ,W O are the training parameters in the\nmodel.\nBased on the Transformer structure, we changed the previ-\nous multi-head attention layer to the combination of multiple\nmulti-head attention layers. As shown in Fig. 1, The model\naggregates the information of each attention layer and sends\nit to the next layer to make full use of the information of each\nlayer.\nD. MULTI-LAYER ATTENTION TRANSFORMER\nAGGREGATION ENCODER\nBased on the Transformer structural model, we use layer\naggregation techniques to integrate the information of each\nlayer better. The structure of the MTA is shown in Figure 4.\nBy aggregating nodes, we can better utilize the informa-\ntion between each unit to analyze the sequence information\nfrom multiple aspects and ensure the efﬁcient utilization of\ninformation.\nThe multi-layer attention transformer units are aggregated\naccording to the following formula :\nˆT i =\n{\nAggregation\n(\nT 2i−1,T 2i)\ni =1\nAggregation\n(\nT 2i−1,T 2i,ˆT i−1\n)\ni >1 (21)\nThe aggregate function aggregation() is the one as\nformula (17). We aggregate the nodes of the same layer into\none node, then send the result back to the linear backbone\nnetwork as the input of the next layer. All the aggregation\nsteps replace the layering combine operation by an addi-\ntion operation so that the computational complexity can be\nreduced while maintaining the size of each layer.\nIV. EXPERIMENTS\nA. DATASET\nIn experiments, we used the SQuAD [4] data set proposed\nby Rajpurkar et al.It contains a total of 107,785 questions,\nas well as 536 pieces of material that contain the target\n90414 VOLUME 8, 2020\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\nFIGURE 4. Multi-layer attention transformer aggregation encoder.\nWe aggregate each multi-layer attention transformer unit between\naggregation nodes and transmit the aggregated information to the\nbackbone network to further enhance the utilization of information.\nIn addition, since each layer transmits information in parallel,\nthe computational efficiency of the model is improved.\nanswers. Table 1 shows an example of the SQuAD data\nset. SQuAD [4] has extracted more than 100,000 question-\nanswer pairs from hundreds of articles on Wikipedia through\ncrowdsourcing. Compared to other datasets like MCTest [27],\nAlgebra [28], Science [29] and WiKiQA [30], the reason we\nchose the SQuAD dataset is that the number of questions in\nSQuAD is far greater than them. On the other hand, the num-\nber of questions in CNN/Daily [31] Mail and CBT [32]\ndata sets are relatively large, but these are both cloze-style\ndatasets, rather than a real question answering data.\nB. NETWORK PARAMETER SETTINGS\nSome of the hyper-parameters used in the neural network\nare shown in Table 2. We use the ADAM optimization algo-\nrithm [33] to train the model. Where β1 = 0.8,β2 =\n0.999,ϵ =10−7. For the setting of the learning rate lr, we\nuse the warm-up scheme to gradually increase from 0.0 to\n0.001 in the ﬁrst 2000 steps of the model training, and then\nmaintain a steady rate for training.\nC. EXPERIMENTAL RESULTS AND ANALYSIS\nIn the answer generation task, we mainly evaluate the perfor-\nmance of the model with EM and F1 scores. EM is a score\nfor complete match, which requires the model’s prediction\nbe exactly the same as the answer in the data set. F1 score\nis used to measure the degree of fuzzy matching between the\nmodel’s prediction and the answer, it takes into account both\nthe accuracy and recall of the model, so the evaluation results\nare more objective.\nAs shown in the plot(a) of Fig.5, the loss rate of the model\nis relatively large during the training process. When attention\nheads is set to 1 and attention layers is set to 3, the network\nloses a lot of information during the feedforward process\nTABLE 1. The example of squad dataset.\nTABLE 2. Experimental super parameter configuration.\nand it is difﬁcult to capture key information, so the model\nis difﬁcult to converge. In plot(b), we change training steps\nfrom 30000 to 50000, attention dimension from 96 to 128,\nand increase the attention layers from 3 to 4. The EM score\nincreased from 67.3 to 68.9 and the F1 score increased from\n76.2 to 78. With the increment of the number of training\nrounds and attention layers, the model can capture and take\nfull advantage of semantic information of the sentence. In the\nplot (c), we set the number of attention heads to 8, by using\nthe multi-head attention mechanism, the model can ﬁnd the\nkey words in the sentence, so that the meaning of the context\ncan be clearly expressed. The EM score and the F1 score are\nVOLUME 8, 2020 90415\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\nFIGURE 5. The performance breakdown with different configuration in training process. Plot (a) shows the trend of loss, EM and\nF1 scores in training steps= 30000, attention dimension= 96, attention heads= 1 and attention layers= 3; In the plot(b),the\ntraining steps= 50000, attention dimension= 128, attention layers= 4; plot (c) shows the trend in training steps= 80000,\nattention dimension= 128, attention Heads= 8 and attention layers= 4. The model successfully converges in the 80,000th\nround with EM and F1 reaching the maximum value (EM= 71.1, F1= 80.3).\nTABLE 3. The test results of the answer generation.\nreached 71.1 and 80.3, respectively. It can be seen that the\naccuracy of our model has increased signiﬁcantly by using\nthe multi-head attention mechanism and MTA module.\nIn order to test the validity of the model, we use the test set\nto test the accuracy of the model and the ability to generate\nanswers. We conducted three sets of experiments separately,\nand the experimental results are shown in Table 3. We choose\nthree typical paragraphs and questions, where the colored\nwords in the paragraphs are the answers to the questions.\nAs can be seen from the table, our model shows quite good\nperformance.\nD. ABLATION STUDIES AND COMPARISONS WITH PRIOR\nMETHODS\nTo demonstrate whether AG-MTA can effectively generate\nthe correct answer, Table 4 shows the EM and F1 scores\nof different networks. Training steps indicates the number\nof training steps. Attention Dimension indicates the hidden\nlayer dimension of the attention network. Attention Heads\nindicates the number of attention heads, Attention Layers\nindicates the number of layers of attention, and Unit Numbers\nindicates the number of layers of the multi-layer attention\nTransformer unit.\nAs shown in Table 4, by comparing Model 1 and Model 3,\nit can be seen that the more training steps, the better the\nmodel ﬁts. In addition, by comparing Model 1 and Model 2,\nTABLE 4. Model training effect under multiple parameter combinations.\nor Model 5 and Model 6, it can be seen that as the number\nof Attention Layers and Unit Number increases, the model\ncan obtain more information representations at different posi-\ntions. Through the experiments of multiple sets of different\nparameters, we obtained several sets of EM and F1 scores\nrespectively. By comparing, we can get the setting of each\nparameter value of the model under the best effect. Moreover,\nEM and F1 achieved the highest scores of 71.1 and 80.3.\nIn order to measure the performance of our model, we com-\npared it to other representative methods. As shown in Table 5,\nDev represents model’s test score under the development set,\nand Test represents model’s test score under the test set. Com-\npared with other methods, whether in Dev or Test, AG-MTA\nhas a greater improvement in performance. Compared with\nmodels that only use the LSTM network (such as LR Base-\nLine [4]) or attention mechanism (such as BiDAF [10]).\nAG-MTA combines the context information and extracts key\nsemantic information by using MTA module, position encod-\ning, and multi-head attention mechanism. Most importantly,\nsince the coding part of our model uses the pure attention\nmechanism scheme and data-parallel computing, with more\ndata, the model can get better performance.\nIn addition, to help qualitatively evaluate our MTA mod-\nule, position encoding and multi-head attention mechanism\n90416 VOLUME 8, 2020\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\nTABLE 5. The comparison experiment of different answer generation\nmodel.\nTABLE 6. Ablation experiments with different module combinations.\nmethodology, we conduct extensive ablation experiments.\nAs shown in Table 6, the experimental results show that the\nmodel (A) with all modules obtained the best experimental\nperformance. By comparing the experimental results of (A),\n(B) and (C), we observe that with help of the position encod-\ning and the multi-head attention mechanism, model (A) can\nuse logical semantic information to express the relationship\namong words. By comparing the experimental results of\n(A) and (D), it can be seen that the MTA module can signif-\nicantly improve the performance of the model (A) by fusing\nsemantic information in different locations from base layer to\ntop layer.\nWe also made a formal comparison of the results, as shown\nin Figure 6. It can be seen that with the continuous improve-\nment of the network, the performance of the network is\ngetting better and better, and the EM and F1 values of our\nmodel have achieved 71.1 and 80.3 respectively.\nE. DISCUSSION\nCompared to the performance of other answer generation\nmethods, our answer generation model produces signiﬁ-\ncant improvement. By applying MTA module testing for\nFIGURE 6. Performance of different models in the answer generation\nexperiment.\nFIGURE 7. The EM score and F1 score on AG-MTA in function of Attention\nlayers and unit number.\nAG-MTA with different parameters, we can improve EM\nfrom 67.3% to 71.1% and F1 from 76.2 % to 80.3%.\nWe have also studied the impact of the number of attention\nlayers and the number of multi-layer attention Transformer\nunit module on the EM score and F1 score. Base on the\nmodel 6 in Table 4, we tune some parameters, so that Train\nSteps =80000, Attention Dimension =128 and Attention\nHeads =8; and accordingly adjust the number of Attention\nLayers and Unit Numbers. The experimental result is shown\nin Figure 7, where A is Attention Layers and U is Unit\nNumbers. It can be observed in Figure 7, as the number of\nAttention Layers and Unit Numbers increases, the growth rate\nof EM score and F1 score gradually decreases. However, the\ncomputing resources consumed by the model have increased\nexponentially. As the model’s complexity is increasing, it is\ndifﬁcult to ﬁt. Therefore, we set Attention Layers =4 and\nUnit Numbers =6 to avoid consuming too many computing\nresources.\nAs shown in Table 5, the experimental results indicate\nthat our AG-MTA can well exploit contextual correlation\npreserved in paragraphs. Nevertheless, the lack of similarity\nmatching between questions and paragraphs will lead to poor\nlogical reasoning ability. Compared with BERT [40], the\nAG-MTA is not achieving the best performance. But the\nVOLUME 8, 2020 90417\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\nBERT also has high demand for hardware. Therefore, in terms\nof practicality, our model has signiﬁcant performance with\ngeneral applicability.\nV. CONCLUSION\nIn this paper, we propose an end-to-end model for answer\ngeneration based on multi-layer Transformer aggregation\ncoder. The model enhances the contextual correlation and\nimproves the accuracy of the answer generation. We propose\nMTA to focus on information representation at different lev-\nels and aggregate the nodes of the same layer to combine the\ncontext information. Furthermore, a novel position encoding\nmethod that make full use of absolute position and relative\nposition information of the word is designed to enhance\nthe relationship of each word. Experiments on the SQuAD\ndataset veriﬁed that our model has a signiﬁcant improvement\nover the state-of-the-art method. Moreover, ablation study\non multi-head attention mechanism and position encoding\nhave been done to prove the effectiveness of each component.\nIn the experiments, we found that time cost increases with the\ncomplexity increase of the network structure. Hence, in our\nfuture work, we will pay more attention to the experiment\non network architecture optimization tasks, and the reduction\nof model parameters to make these methods have greater\napplicability.\nREFERENCES\n[1] S. Wang and J. Jiang, ‘‘Machine comprehension using match-LSTM\nand answer pointer,’’ 2016, arXiv:1608.07905. [Online]. Available: http://\narxiv.org/abs/1608.07905\n[2] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi, and Q.\nV . Le, ‘‘QANet: Combining local convolution with global self-attention for\nreading comprehension,’’ 2018, arXiv:1804.09541. [Online]. Available:\nhttps://arxiv.org/abs/1804.09541\n[3] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, ‘‘SQuAD: 100,000 +\nquestions for machine comprehension of text,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process., 2016, pp. 2383–2392.\n[4] J. Liu, Y . Yang, S. Lv, J. Wang, and H. Chen, ‘‘Attention-based BiGRU-\nCNN for Chinese question classiﬁcation,’’ J. Ambient Intell. Hum.\nComput., Jun. 2019, doi: 10.1007/s12652-019-01344-9.\n[5] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman, ‘‘Deep learning for\nanswer sentence selection,’’ 2014, arXiv:1412.1632. [Online]. Available:\nhttp://arxiv.org/abs/1412.1632\n[6] M. Tan, C. dos Santos, B. Xiang, and B. Zhou, ‘‘LSTM-based deep learn-\ning models for non-factoid answer selection,’’ 2015, arXiv:1511.04108.\n[Online]. Available: http://arxiv.org/abs/1511.04108\n[7] J. Liu, H. Ren, M. Wu, J. Wang, and H.-J. Kim, ‘‘Multiple relations\nextraction among multiple entities in unstructured text,’’ Soft Comput.,\nvol. 22, no. 13, pp. 4295–4305, Jul. 2018.\n[8] S. P. Lende and M. M. Raghuwanshi, ‘‘Question answering system on\neducation acts using NLP techniques,’’ in Proc. World Conf. Futuristic\nTrends Res. Innov. Social Welfare (Startup Conclave), Feb. 2016, pp. 1–6.\n[9] D. Wang and E. Nyberg, ‘‘A long short-term memory model for answer\nsentence selection in question answering,’’ in Proc. 53rd Annu. Meeting\nAssoc. Comput. Linguistics 7th Int. Joint Conf. Natural Lang. Process.\n(Short Papers), vol. 2, Jul. 2015, pp. 707–712.\n[10] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, ‘‘Bidirectional atten-\ntion ﬂow for machine comprehension,’’ 2016, arXiv:1611.01603. [Online].\nAvailable: https://arxiv.org/abs/1611.01603\n[11] B. Dhingra, H. Liu, Z. Yang, W. Cohen, and R. Salakhutdinov, ‘‘Gated-\nattention readers for text comprehension,’’ in Proc. 55th Annu. Meet-\ning Assoc. Comput. Linguistics (Long Papers) , vol. 1, Jul. 2017,\npp. 1832–1846.\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[13] J. Liu, L. Lin, H. Ren, M. Gu, J. Wang, G. Youn, and J.-U. Kim, ‘‘Building\nneural network language model with POS-based negative sampling and\nstochastic conjugate gradient descent,’’ Soft Comput., vol. 22, no. 20,\npp. 6705–6717, Oct. 2018.\n[14] S. Wang, M. Yu, X. Guo, Z. Wang, T. Klinger, W. Zhang, S. Chang,\nG. Tesauro, B. Zhou, and J. Jiang, ‘‘R 3: Reinforced reader-ranker for open-\ndomain question answering,’’ 2017, arXiv:1709.00023. [Online]. Avail-\nable: http://arxiv.org/abs/1709.00023\n[15] Z. Yang, J. Hu, R. Salakhutdinov, and W. W. Cohen, ‘‘Semi-supervised QA\nwith generative domain-adaptive nets,’’ 2017, arXiv:1702.02206. [Online].\nAvailable: http://arxiv.org/abs/1702.02206\n[16] R. Ghaeini, X. Z. Fern, H. Shahbazi, and P. Tadepalli, ‘‘Dependent gated\nreading for cloze-style question answering,’’ 2018, arXiv:1805.10528.\n[Online]. Available: http://arxiv.org/abs/1805.10528\n[17] W. Zhong, D. Tang, N. Duan, M. Zhou, J. Wang, and J. Yin, ‘‘A hetero-\ngeneous graph with factual, temporal and logical knowledge for question\nanswering over dynamic contexts,’’ 2020, arXiv:2004.12057. [Online].\nAvailable: http://arxiv.org/abs/2004.12057\n[18] X. Shi, I. Padhi, and K. Knight, ‘‘Does string-based neural MT learn source\nsyntax?’’ in Proc. Conf. Empirical Methods Natural Lang. Process., 2016,\npp. 1526–1534.\n[19] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\narXiv:1802.05365. [Online]. Available: http://arxiv.org/abs/1802.05365\n[20] A. Anastasopoulos and D. Chiang, ‘‘Tied multitask learning for neural\nspeech translation,’’ in Proc. Conf. North Amer. Chapter Assoc. Com-\nput. Linguistics: Hum. Lang. Technol., (Long Papers), vol. 1, Jun. 2018,\npp. 82–91.\n[21] T. Bell and B. Penchas, ‘‘Lightweight convolutional approaches to reading\ncomprehension on SQuAD,’’ 2018, arXiv:1810.08680. [Online]. Avail-\nable: http://arxiv.org/abs/1810.08680\n[22] X. Zhou, B. Hu, Q. Chen, and X. Wang, ‘‘Recurrent convolutional neural\nnetwork for answer selection in community question answering,’’ Neuro-\ncomputing, vol. 274, pp. 8–18, Jan. 2018.\n[23] Y . Tay, L. A. Tuan, and S. C. Hui, ‘‘Multi-cast attention networks,’’ in Proc.\n24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Jul. 2018,\npp. 2299–2308.\n[24] L. Dong, F. Wei, M. Zhou, and K. Xu, ‘‘Question answering over freebase\nwith multi-column convolutional neural networks,’’ in Proc. 53rd Annu.\nMeeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Lang.\nProcess. (Long Papers), vol. 1, Jul. 2015, pp. 260–269.\n[25] X. He and D. Golub, ‘‘Character-level question answering with atten-\ntion,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., 2016,\npp. 1598–1607.\n[26] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[27] M. Richardson, C. J. Burges, and E. Renshaw, ‘‘MCTest: A challenge\ndataset for the open-domain machine comprehension of text,’’ in Proc.\nConf. Empirical Methods Natural Lang. Process., Oct. 2013, pp. 193–203.\n[28] N. Kushman, Y . Artzi, L. Zettlemoyer, and R. Barzilay, ‘‘Learning to\nautomatically solve algebra word problems,’’ in Proc. 52nd Annu. Meeting\nAssoc. Comput. Linguistics (Long Papers), vol. 1, Jun. 2014, pp. 271–281.\n[29] P. Clark and O. Etzioni, ‘‘My computer is an honor student—But how\nintelligent is it? Standardized tests as a measure of AI,’’ AI Mag., vol. 37,\nno. 1, p. 5, 2016.\n[30] Y . Yang, W.-T. Yih, and C. Meek, ‘‘WikiQA: A challenge dataset for open-\ndomain question answering,’’ in Proc. Conf. Empirical Methods Natural\nLang. Process., 2015, pp. 2013–2018.\n[31] F. Hill, A. Bordes, S. Chopra, and J. Weston, ‘‘The goldilocks principle:\nReading Children’s books with explicit memory representations,’’ 2015,\narXiv:1511.02301. [Online]. Available: http://arxiv.org/abs/1511.02301\n[32] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay,\nM. Suleyman, and P. Blunsom, ‘‘Teaching machines to read and compre-\nhend,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 1693–1701.\n[33] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic opti-\nmization,’’ 2014, arXiv:1412.6980. [Online]. Available: http://arxiv.org/\nabs/1412.6980\n[34] Z. Wang, H. Mi, W. Hamza, and R. Florian, ‘‘Multi-perspective con-\ntext matching for machine comprehension,’’ 2016, arXiv:1612.04211.\n[Online]. Available: http://arxiv.org/abs/1612.04211\n90418 VOLUME 8, 2020\nS. Shanget al.: Multi-Layer Transformer Aggregation Encoder for Answer Generation\n[35] Y . Yu, W. Zhang, K. Hasan, M. Yu, B. Xiang, and B. Zhou, ‘‘End-to-end\nanswer chunk extraction and ranking for reading comprehension,’’ 2016,\narXiv:1610.09996. [Online]. Available: http://arxiv.org/abs/1610.09996\n[36] D. Weissenborn, G. Wiese, and L. Seiffe, ‘‘FastQA: A simple and efﬁcient\nneural architecture for question answering,’’ 2017, arXiv:1703.04816.\n[Online]. Available: http://arxiv.org/abs/1703.04816\n[37] J. Zhang, X. Zhu, Q. Chen, L. Dai, S. Wei, and H. Jiang, ‘‘Exploring\nquestion understanding and adaptation in neural-network-based question\nanswering,’’ 2017,arXiv:1703.04617. [Online]. Available: http://arxiv.org/\nabs/1703.04617\n[38] K. Lee, S. Salant, T. Kwiatkowski, A. Parikh, D. Das, and J. Berant,\n‘‘Learning recurrent span representations for extractive question answer-\ning,’’ 2016, arXiv:1611.01436. [Online]. Available: http://arxiv.org/abs/\n1611.01436\n[39] D. Weissenborn, G. Wiese, and L. Seiffe, ‘‘Making neural QA as simple\nas possible but not simpler,’’ in Proc. 21st Conf. Comput. Natural Lang.\nLearn. (CoNLL ), 2017, p. 271.\n[40] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\nSHENGJIE SHANG received the B.E. degree\nfrom Liaocheng University, in 2018. He is cur-\nrently pursuing the M.E. degree with Shanghai\nMaritime University.\nHis current research interests are computer\nvision, natural language processing, and machine\nlearning.\nJIN LIU(Member, IEEE) received the B.S. degree\nfrom Lanzhou University, the M.S. degree from the\nUniversity of Electrical Science and Technology\nof China, and the Ph.D. degree from Washing-\nton State University. He is currently a Professor\nwith Shanghai Maritime University. His research\ninterests include deep learning, nature language\nprocessing, and computer vision. He is a member\nof CAAI and CCF.\nYIHE YANGreceived the B.S. degree in software\nengineering from Shanghai Maritime University,\nShanghai, in 2018, where he is currently pursuing\nthe M.E. degree.\nHis current research interests are data min-\ning, natural language processing, and machine\nlearning.\nVOLUME 8, 2020 90419",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8121995329856873
    },
    {
      "name": "Encoder",
      "score": 0.7098115682601929
    },
    {
      "name": "Transformer",
      "score": 0.6522636413574219
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5826514959335327
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5125293731689453
    },
    {
      "name": "Deep learning",
      "score": 0.48729121685028076
    },
    {
      "name": "Feature learning",
      "score": 0.46195128560066223
    },
    {
      "name": "Exploit",
      "score": 0.4514934718608856
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96733725",
      "name": "Shanghai Maritime University",
      "country": "CN"
    }
  ],
  "cited_by": 20
}