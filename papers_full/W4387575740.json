{
    "title": "Meaning Modulations and Stability in Large Language Models: An Analysis of BERT Embeddings for Psycholinguistic Research",
    "url": "https://openalex.org/W4387575740",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2049312926",
            "name": "Giuseppe Attanasio",
            "affiliations": [
                "Tilburg University"
            ]
        },
        {
            "id": "https://openalex.org/A2124569955",
            "name": "Federico Bianchi",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2043796392",
            "name": "Fritz Günther",
            "affiliations": [
                "Bocconi University"
            ]
        },
        {
            "id": "https://openalex.org/A2213070553",
            "name": "Marco Marelli",
            "affiliations": [
                "University of Milano-Bicocca"
            ]
        },
        {
            "id": "https://openalex.org/A2076431915",
            "name": "Giovanni Cassani",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2973169533",
        "https://openalex.org/W19156862",
        "https://openalex.org/W2399777503",
        "https://openalex.org/W1989462718",
        "https://openalex.org/W6685040323",
        "https://openalex.org/W2251803266",
        "https://openalex.org/W6682557594",
        "https://openalex.org/W3095155674",
        "https://openalex.org/W3137989408",
        "https://openalex.org/W3160251434",
        "https://openalex.org/W2571777482",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W3035102548",
        "https://openalex.org/W2561299349",
        "https://openalex.org/W6676970723",
        "https://openalex.org/W6752759950",
        "https://openalex.org/W2146333753",
        "https://openalex.org/W1974991592",
        "https://openalex.org/W2899103268",
        "https://openalex.org/W2025075790",
        "https://openalex.org/W1966223795",
        "https://openalex.org/W2345767696",
        "https://openalex.org/W2799894091",
        "https://openalex.org/W4288700724",
        "https://openalex.org/W4206201295",
        "https://openalex.org/W3099950029",
        "https://openalex.org/W6630998717",
        "https://openalex.org/W2166066762",
        "https://openalex.org/W6741439289",
        "https://openalex.org/W6632070484",
        "https://openalex.org/W2604325525",
        "https://openalex.org/W1965580172",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2030440611",
        "https://openalex.org/W2253100625",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W3096262693",
        "https://openalex.org/W2096575147",
        "https://openalex.org/W1967933442",
        "https://openalex.org/W2612205435",
        "https://openalex.org/W6754339693",
        "https://openalex.org/W4200239816",
        "https://openalex.org/W6684647093",
        "https://openalex.org/W1984251878",
        "https://openalex.org/W4236756982",
        "https://openalex.org/W2972680241",
        "https://openalex.org/W1694003152",
        "https://openalex.org/W2534719434",
        "https://openalex.org/W4214499019",
        "https://openalex.org/W2079038792",
        "https://openalex.org/W2086827435",
        "https://openalex.org/W4238635791",
        "https://openalex.org/W3152214984",
        "https://openalex.org/W6753056052",
        "https://openalex.org/W3033187248",
        "https://openalex.org/W3003814985",
        "https://openalex.org/W2078259703",
        "https://openalex.org/W1854884267",
        "https://openalex.org/W2127921960",
        "https://openalex.org/W6666761814",
        "https://openalex.org/W2735318979",
        "https://openalex.org/W2165975954",
        "https://openalex.org/W1146351399",
        "https://openalex.org/W1986787497",
        "https://openalex.org/W1982755712",
        "https://openalex.org/W2884872018",
        "https://openalex.org/W4210825693",
        "https://openalex.org/W3047748301",
        "https://openalex.org/W2140690392",
        "https://openalex.org/W6689844037",
        "https://openalex.org/W2594337407",
        "https://openalex.org/W6799424760",
        "https://openalex.org/W6654047064",
        "https://openalex.org/W2988972814",
        "https://openalex.org/W2060231322",
        "https://openalex.org/W3083307665",
        "https://openalex.org/W2098986833",
        "https://openalex.org/W6645009471",
        "https://openalex.org/W2141138276",
        "https://openalex.org/W2000387713",
        "https://openalex.org/W6684324104",
        "https://openalex.org/W1977236022",
        "https://openalex.org/W2018812736",
        "https://openalex.org/W1983578042",
        "https://openalex.org/W193898724",
        "https://openalex.org/W6745023567",
        "https://openalex.org/W4214812012",
        "https://openalex.org/W3156782505",
        "https://openalex.org/W1664311846",
        "https://openalex.org/W1981617416",
        "https://openalex.org/W2982116886",
        "https://openalex.org/W2415973339",
        "https://openalex.org/W2015209878",
        "https://openalex.org/W4362696539",
        "https://openalex.org/W4286988498",
        "https://openalex.org/W4317838060",
        "https://openalex.org/W2983004086",
        "https://openalex.org/W138474712",
        "https://openalex.org/W6682656196",
        "https://openalex.org/W2785659424",
        "https://openalex.org/W2165256085",
        "https://openalex.org/W2507274780",
        "https://openalex.org/W6748634344",
        "https://openalex.org/W2004841825",
        "https://openalex.org/W4205322542",
        "https://openalex.org/W6683469792",
        "https://openalex.org/W3030156796",
        "https://openalex.org/W3002152281",
        "https://openalex.org/W2046121397",
        "https://openalex.org/W2146158822",
        "https://openalex.org/W3006881356",
        "https://openalex.org/W6625122095",
        "https://openalex.org/W2000255081",
        "https://openalex.org/W3213921583",
        "https://openalex.org/W6604617241",
        "https://openalex.org/W4308024636",
        "https://openalex.org/W2124006395",
        "https://openalex.org/W6664700147",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W3024408516",
        "https://openalex.org/W2073399460",
        "https://openalex.org/W2072879499",
        "https://openalex.org/W2113772582",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3038094571",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2023736093",
        "https://openalex.org/W4238846128",
        "https://openalex.org/W1969221307",
        "https://openalex.org/W2024889439",
        "https://openalex.org/W2132193240",
        "https://openalex.org/W6714899898",
        "https://openalex.org/W2078894097",
        "https://openalex.org/W3011718307",
        "https://openalex.org/W4385718064",
        "https://openalex.org/W4252434862",
        "https://openalex.org/W2890250823",
        "https://openalex.org/W3035258717",
        "https://openalex.org/W2410831681",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2168979204",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4382623424",
        "https://openalex.org/W1979532929",
        "https://openalex.org/W4282967944",
        "https://openalex.org/W2167293745",
        "https://openalex.org/W2810597146",
        "https://openalex.org/W4287854434",
        "https://openalex.org/W2763088512",
        "https://openalex.org/W4239395966",
        "https://openalex.org/W2150375089",
        "https://openalex.org/W3037252472",
        "https://openalex.org/W4385572697",
        "https://openalex.org/W2112184938",
        "https://openalex.org/W2927103915",
        "https://openalex.org/W2159398820",
        "https://openalex.org/W4246716965",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W1519165391",
        "https://openalex.org/W4297998383",
        "https://openalex.org/W4300562939",
        "https://openalex.org/W4241222316",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W4233970037",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W4246857541",
        "https://openalex.org/W3100748148",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4239421357",
        "https://openalex.org/W3176357828",
        "https://openalex.org/W2015024653",
        "https://openalex.org/W4287553029",
        "https://openalex.org/W4239273613",
        "https://openalex.org/W2166876238",
        "https://openalex.org/W2963850840",
        "https://openalex.org/W4386566774",
        "https://openalex.org/W2241137415",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W4303188629",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W113586804",
        "https://openalex.org/W4241141740",
        "https://openalex.org/W3162111181"
    ],
    "abstract": "Computational models of semantic representations have long assumed and produced a single static representation for each word type, ignoring the influence of linguistic context on semantic representations. Recent Large Language Models (LLMs) introduced in Natural Language Processing, however, learn token-level contextualised representations, holding promise to study how semantic representations change in different contexts. In this study we probe type- and token-level representations learned using a prominent example of such models, Bidirectional Encoder Representations from Transformers (BERT), for their ability to i) explain semantic effects found for isolated words (semantic relatedness and similarity ratings, lexical decision, and semantic priming), but critically also to ii) exhibit systematic interactions between lexical semantics and context, and iii) explain meaning modulations in context. Across a wide range of empirical studies on each of these topics, we show that BERT representations satisfy two desiderata for psychologically valid semantic representations: i) they have a stable semantic core which allows people to interpret words in isolation and prevents words to be used arbitrarily and ii) they interact with sentence context in systematic ways, with representations shifting as a function of their semantic core and the context. This demonstrates that a single, comprehensive model which simultaneously learns abstract, type-level prototype representations as well as mechanisms of how these interact with context can explain both isolated word effects and context-dependent variations. Notably, these variations are not limited to discrete word senses, eschewing a strict dichotomy between exemplar and prototype models and re-framing traditional notions of polysemy in semantic memory.",
    "full_text": "MEANING MODULATIONS 1\nMeaning Modulations and Stability in Large Language Models: An Analysis of\nBERT Embeddings for Psycholinguistic Research\nGiovanni Cassani1, Federico Bianchi2, Giuseppe Attanasio3,4, Marco Marelli5,6, and Fritz\nGünther7\n1Tilburg University, Tilburg, The Netherlands\n2Stanford University, Stanford, California, USA\n3Bocconi University, Milan, Italy\n4Instituto de Telecomunicações, Lisbon, Portugal\n5University of Milano–Bicocca, Milan, Italy\n6NeuroMI, Milan Center for Neuroscience, Milan, Italy\n7Humboldt-Universität zu Berlin, Berlin, Germany\nMEANING MODULATIONS 2\nAuthor Note\nCorresponding Author:\nGiovanni Cassani\ng.cassani@tilburguniversity.edu\nWarandelaan 2, room D124\n5037AB, Tilburg\nThe Netherlands\nAll authors declare that they have no conflict of interest. All materials, datasets, and\nanalysis scripts for the studies reported here are available at\nhttps://osf.io/94ena/?view_only=4e29d179b7664f27b6b7b7ba6b75edf7, except for those that\nare available elsewhere (e.g., English Lexicon Project, Semantic Priming Project, MEN\ndataset, ...) and those that cannot be shared due to licensing constraints (Corpus of\nContemporary American English). This study was not preregistered. This manuscript has\nbeen distributed in pre-print form on PsyArXiV and is available at this URL:\nhttps://osf.io/preprints/psyarxiv/b45ys. The content of this work has also been presented at\nthe Virtual Psycholinguistic Forum of the Chinese University of Hong Kong, at the 57th\nAnnual Meeting of the Society for Mathematical Psychology (MathPsych) and the 22nd\nInternational Conference on Cognitive Modeling (ICCM) and at the Highlights in Language\nSciences (HILS) conference in Nijmegen..\nGiovanni Cassani https://orcid.org/0000-0003-3917-3315\nMarco Marelli https://orcid.org/0000-0001-5831-5441\nFederico Bianchi https://orcid.org/0000-0003-0776-361X\nGiuseppe Attanasio https://orcid.org/0000-0001-6945-3698\nFritz Günther https://orcid.org/0000-0002-9205-6786\nMEANING MODULATIONS 3\nAbstract\nComputational models of semantic representations have long assumed and produced a single\nstatic representation for each word type, ignoring the influence of linguistic context on\nsemantic representations. Recent Large Language Models (LLMs) introduced in Natural\nLanguage Processing, however, learn token-level contextualised representations, holding\npromise to study how semantic representations change in different contexts. In this study we\nprobe type- and token-level representations learned using a prominent example of such\nmodels, Bidirectional Encoder Representations from Transformers (BERT), for their ability\nto i) explain semantic effects found for isolated words (semantic relatedness and similarity\nratings, lexical decision, and semantic priming), but critically also to ii) exhibit systematic\ninteractions between lexical semantics and context, and iii) explain meaning modulations in\ncontext. Across a wide range of empirical studies on each of these topics, we show that\nBERT representations satisfy two desiderata for psychologically valid semantic\nrepresentations: i) they have a stable semantic core which allows people to interpret words in\nisolation and prevents words to be used arbitrarily and ii) they interact with sentence\ncontext in systematic ways, with representations shifting as a function of their semantic core\nand the context. This demonstrates that a single, comprehensive model which\nsimultaneously learns abstract, type-level prototype representations as well as mechanisms of\nhow these interact with context can explain both isolated word effects and\ncontext-dependent variations. Notably, these variations are not limited to discrete word\nsenses, eschewing a strict dichotomy between exemplar and prototype models and re-framing\ntraditional notions of polysemy in semantic memory.\nKeywords: Distributional semantics; Large Language Models; Computational\nPsycholinguistics; Mental lexicon; Context-dependent semantic representations\nMEANING MODULATIONS 4\nMeaning Modulations and Stability in Large Language Models: An Analysis of\nBERT Embeddings for Psycholinguistic Research\nAs is illustrated by the very sentence you are now reading, or really any other piece of text,\nwords usually do not appear in isolation. At least after a certain age, the way we use\nlanguage to communicate with others more often than not consists of longer segments, in\nforms ranging from short phrases over sentences to discourse. That is, the individual words\nwe use typically occur in thecontext of other words. This context-dependency is crucial\nwhen we consider the most central property of words: their meaning, which determines their\ncommunicative purpose. Homonyms are an especially obvious case: Consider the meaning of\nball in For the third time, the worried player swung but missed the ball.versus The lonely\nmaiden had great hopes as she dressed to go to the ball.(Till et al., 1988). But\ncontext-dependency of meaning is not a limited phenomenon restricted to the 7% of word\ntypes that are homonyms (i.e., words with different unrelated meanings); in fact, Rodd et al.\n(2002) show that 80% of word forms in the English vocabulary are polysemous (i.e., they\nhave different related senses, such aspaper). However, the notion of “polysemy” still assumes\nthat we can capture word meanings in context with a finite set of discrete senses (Li &\nJoanisse, 2021; Rodd, 2020). While a useful tool in lexicography, this is arguably still an\noversimplification: In fact, it can be argued that really no two instances of a given word have\nthe exact same meaning, even when the same “sense” of the word is used (Haber & Poesio,\n2024; Mickus et al., 2020): Considerchair in There is a really nice chair in my living\nroom/my dinner room/my office/the museum/the art gallery/the castle, where each chair is\ndifferent and comes with different affordances (e.g., whether one can for example sit on it or\nnot) or features about materials, shape, time in which it was built, and more. In fact, there\nis meaning variation even when the word refers to the very same entity (Zwaan et al., 2002):\nThe tomatoes in The young tomatoes are growing in my garden/I pluck the ripe tomatoes/I\nchop the tomatoes on my cutting board/You can still identify some of the tomatoes in my\nboiling pasta sauceare quite different, even when speaking about the same set of tomatoes.\nThus, although words are generally used to refer to categories, every particular instance in\nwhich a word is used is also that – a unique instance, different from any other. It is hence in\nprinciple possible to ascribe a unique meaning and meaning representation to eachtoken,\nrather than only to eachtype of a word or to discrete word senses (Apidianaki, 2023).\nIn line with these arguments, it has been repeatedly pointed out that the traditional\nresearch focus on static type-level representations is inadequate or at least insufficient, and\nword meaning representations need to be considered (also) at the token level (Ambridge,\n2020; Connell & Lynott, 2014; Yee & Thompson-Schill, 2016). Indeed, after reviewing a wide\nrange of empirical studies on various context effects, ranging from the effects of lifetime\nMEANING MODULATIONS 5\nexperience with concepts to the effects of the immediate linguistic context and task demands,\nYee and Thompson-Schill (2016) conclude that “rather than being static, conceptual\nrepresentations are constantly changing and are inextricably linked to their contexts” (p.\n1015).\nIn such a perspective, following up the classical distinction in concept research\n(Murphy, 2002a; Rosch, 1978; Rosch & Mervis, 1975; E. E. Smith & Medin, 1981), each\ntoken of a word can be identified as an individualexemplar of a category, with the word type\nmaking up theprototype, an abstracted, aggregate representation of that category\n(Ambridge, 2020; Jamieson et al., 2018). There is still an ongoing debate as to whether\nprototype- or exemplar-based models (or hybrids between them) are to be preferred (for\nrecent contributions, see for example Ambridge, 2020; Battleday et al., 2020; Jamieson et al.,\n2022). In general terms, prototype models capitalize on the invariance in experience, which\nin the limit affords a single representation which subsumes several, possibly infinite,\nindividual instances of a same unit, and aim to capture precisely what is invariant. On the\nother end of the spectrum, exemplar models capitalise on the contextual variability of\nexperience, which in the limit prevents the possibility of having a single abstract\nrepresentation that subsumes different instances of a supposed same unit. The higher fidelity\nwith which experience can be encoded makes these models appealing. On the one hand, a\nsingle representation for a given unit is very compact, but may not scale well to new\nencounters with said unit. On the other hand, exemplar models would store any experience,\neven if some aspects of it could be abstracted away.\nIn order to even just engage in this debate, we need comparable implementations of\nboth types of model in the first place. For some modalities, such implementations are\navailable: For example, in the visual modality, modern deep learning models provide us with\nrepresentations for both individual images (i.e., exemplars) and aggregate category-level\nrepresentations (i.e., prototypes) (Battleday et al., 2020, 2021; Günther et al., 2023);\nsimilarly, for the acoustic modality, recent (deep) neural networks can process individual\nsound snippets and encode them, allowing a post-hoc aggregation to derive prototype\nrepresentations. In research on phonology, the tension between prototype and exemplar\nmodels has been long researched (J. Bybee, 2002; Frisch, 2017; Kohl, 1993), with evidence\nthat suggests a role of both prototype abstract representations as well as exemplars. Milin\net al. (2020) shows a first attempt at modeling how abstract phonological units may emerge\nfrom exposure to speech sounds, comparing pure instance-based learning (stored exemplars)\nand error-driven learning, suggesting the second fares better.\nHowever, when it comes to meaning and semantics, virtually all (computational)\nmodels of word meaning currently used in psychological science are implemented at the type\nMEANING MODULATIONS 6\nlevel and not the token level (cf. Jamieson et al., 2018; except for a few noteworthy studies,\ndescribed in the section Contextualizing semantic representations below). Consequently,\nthese models are not well-suited to capture context-dependency. That said, recent\ndevelopments in artificial intelligence research and natural language processing (NLP) have\nbrought forward a class of highly popular language models that are explicitly geared to\nrepresent context-dependent meanings for individual word tokens (Devlin et al., 2019a;\nPeters et al., 2018; for an overview, see Liu et al., 2020). These models appear to strike a\npossibly interesting balance between abstraction and storage: They do aim to find the\ninvariance in experience, deriving compact representations of certain aspects of reality, but\nthey also allow such abstract representations to be skewed by idiosyncratic aspects of the\nspecific experience, allowing a richer representation that captures the variability of\nexperience with higher fidelity. Adopting these models in cognitive science and\npsycholinguistics more specifically promises to fill in this crucial gap which has been\nidentified – but remained unaddressed – for a long time.\nTo introduce these models, we first survey current computational models of word\nmeaning (specifically, distributional semantic models), presenting their strengths that render\nthem popular candidates for modelling psychological representations of word meaning and\nsemantic memory, but also their clear weaknesses and limitations when it comes to taking\ninto account context. After reviewing some earlier proposals for context-dependent meaning\nrepresentations, we then present how modern language models developed in the field of NLP\n– such as theBidirectional Encoder Representations from Transformers(BERT) model\n(Devlin et al., 2019a) on which we focus – can serve as a distributional semantic model that\nproduces fully contextualized token-level meaning representations. The empirical part of this\npaper then thoroughly investigates whether the BERT model can serve as a psychological\nmodel of contextualized word meaning representations, by using it to predict a wide array of\nlinguistic and psychological phenomena. Code and resources to generate the embeddings we\nused are available on a GitHub repository.1\nDistributional models of semantic memory and word meaning representation\nWhen it comes to models of semantic memory and word meaning representation, we have a\nconsiderable range of quantitative and computationally-implemented models available to us\n(M. N. Jones et al., 2015; Kumar, 2021). These include models that directly use human\nratings on various meaning dimensions as quantitative meaning representations\n(E. M. Buchanan et al., 2019; Devereux et al., 2014; Lynott et al., 2020; McRae et al., 2005),\n1 https://github.com/MilaNLProc/psycho-embeddings\nMEANING MODULATIONS 7\nmodels that estimate underlying semantic representations from behavioral data (Benedek\net al., 2017; De Deyne et al., 2016; Kenett et al., 2017), or models that estimate semantic\nrepresentations as the learning result from a certain input representing our experience\n(Andrews et al., 2009; Bruni, Tran, et al., 2014; Landauer & Dumais, 1997; Mandera et al.,\n2017).\nThe most prominent representative of the latter are a class of models known as\n(depending on the scientific field) distributional semantic models (DSMs), vector space\nmodels (VSM) of meaning, or word embedding models (for overviews from a psychological\nperspective, see Günther et al., 2019; M. N. Jones et al., 2015; Lenci, 2018)2. These models\nstart from thedistributional hypothesisthat words with similar meanings occur in similar\ncontexts (Harris, 1954; Sahlgren, 2008) – that is, they have a similardistribution over\ncontexts. Once an operational definition ofcontext is applied – such as, for example, a\ndocument a word occurs in (Landauer & Dumais, 1997) or the words immediately\nsurrounding a target word (Lund & Burgess, 1996; Mikolov, Chen, et al., 2013) – these\ndistributions can be extracted from large corpora of natural language. Although,\ntraditionally, such distributions were learned and represented via count-based strategies, the\naffirmed approach today is usingcontinuous vectors(Baroni et al., 2014a). Using this type of\nrepresentation – i.e., a long list of numbers – for distributions over contexts means\ndistributional models represent word meanings as high-dimensional vectors spanning a\nsemantic space. Crucially, distributional semantic models derive these word meaning\nrepresentations purely from how words are used and occur in language.\nFor corpora that reasonably approximate human language experience, distributional\nsemantic models serve as psychologically plausible learning models that derive semantic\nrepresentations from this input (Hollis, 2017; Lenci, 2008; Mandera et al., 2017). The\nplausibility of the resulting representations has been confirmed in a wide range of empirical\nstudies predicting human behavioral data from distributional vectors and the similarities\nbetween them (e.g., Baroni et al., 2014b; Günther et al., 2016b; Louwerse, 2011; Mandera\net al., 2017; Nematzadeh et al., 2017; Pereira et al., 2016). Because of these different factors,\ndistributional semantic models can currently be considered state-of-the-art models of human\nsemantic memory and semantic representation, and they are consequently widely employed\nin empirical and theoretical studies (Günther et al., 2019; Kumar, 2021).\nOf course, their wide adoption does not mean that these models are without problems\nand limitations, and criticism has come from many different directions (for a discussion, see\nGünther et al., 2019). One critical limitation is the fact that, once these models have been\n2 As in the literature, the termsembedding and vector will be used interchangeably in this article\nMEANING MODULATIONS 8\nThe play at the theater was a success.\nI play soccer.\nThe audience appreciated the play.\nDSM\nFigure 1\nDepending on the context in which a word occurs, it is embedded in a geometrical space (here\n2-dimensional for simplicity, but multidimensional in real applications) where representations\nare closer in space when context of occurrence is similar and further apart when context of\noccurrence differs. This typically applies to different types, but in more recent models like the\none at the center of the paper this also applies to token-level representations.\ntrained on a given corpus, they represent each word meaning with a single static vector –\nwhich raises the question how this format can be justified for words with multiple different\nmeanings and senses. Going beyond the phenomena of homonymy and polysemy, it has also\nbeen questioned how this fixed and static representation format can capture novel,\ncontext-specific uses of words, such as using anewspaper as an umbrella when it is raining\n(Glenberg & Robertson, 2000). However, it is important to emphasize here that, while these\ncriticisms are usually directed at distributional semantic models,all models that provide\nstatic representations of word meaning at the type level – and that is, all current models of\nsemantic memory described above – are faced with the very same problems.\nAs argued elsewhere, the fact that these models learn type-level representations does\nnot imply that distributional vectors (or other static type-level representation models) only\nrepresent one single, discrete symbolic meaning (Günther et al., 2019; Günther & Marelli,\n2022). Analyzing the semantic neighborhoods of distributional vectors often yields clusters\nthat can be identified with different meanings or senses of a word (Camacho-Collados &\nPilehvar, 2018; Heylen et al., 2015), and can be used to detect the “ambiguity status”\n(monosemous vs. polysemous vs. homonym) of words (Beekhuizen et al., 2021). In a recent\nstudy, Günther and Marelli, 2022 also showed that frequency distributions of different\nqualitative interpretations for compound words (awood brushcan be interpreted as abrush\nFOR woodor brush MADE OF wood) can be predicted from the dimensional values of their\nMEANING MODULATIONS 9\ndistributional vectors, showing that this meaning variability is encoded in these vector\nrepresentations.\nHowever, while these studies show that different meanings are subsumed in static\ndistributional vectors, these vectors are still fully abstracted: they simultaneously carry with\nthem all these different meanings in a single representation, without naturally being able to\ntell them apart or to indicate which meaning is relevant at a given point in a given context.\nConsequently, the semantic similarity between the vectors forball and prom is a fixed value,\nas is the semantic similarity betweenball and tennis, even though these words hardly have\nany relation to the respective other meaning ofball. A context-sensitive model would instead\ntell us thatprom is similar toball in The lonely maiden had great hopes as she dressed to go\nto the ball.but not inFor the third time, the worried player swung but missed the ball., and\nvice versa fortennis.\nContextualizing semantic representations\nPrevious models of semantic representations\nIn the domain of cognitive science, some tentative approaches for obtaining context-sensitive\nword meaning representations have already been put forward. The earliest proposal of how\nregular, static type-level distributional vectors can be adjusted based on their context is\nKintsch’s Predication algorithm (Kintsch, 2001), originally proposed for metaphor\nrepresentations (Kintsch, 2000). This algorithm is designed for predicate-argument\nstructures such as IS.SHARK(LAWYER) (inMy lawyer is a shark) or RUN(HORSE) (in\nThe horse is running), and specifically to capture how the presence of the argument affects\nthe meaning of the predicate (run has a different meaning inThe horse/the color/time is\nrunning). In Kintsch’s model, this context-sensitivity is realized by activating them (a large\nnumber) nearest neighbors to the predicate, then selecting from those thek (a small number,\nk <<m) nearest words to the argument, and then adding the vectors for thesek words to\nthe predicate vector. This has the effect that the contextualized predicate vector is nudged\ntowards the argument vector. Variations of this method that follow the same underlying\nlogic (activation and selection of neighbors) have later been proposed by Utsumi (2011) and\nHarati et al. (2021), also with a focus on metaphor representation (see Nick Reid and Katz,\n2018 for an overview). However, this model is limited in that it is only designed to consider a\nsingle other word as context, and it is not clear if and how it can be applied recursively to\nconsider more context. In addition, as remarked by Jamieson et al. (2018), such approaches\nare ad-hoc additions to an existing semantic model so that it can account for a specific\nbehavior, rather than an inherent part of its design.\nMEANING MODULATIONS 10\nIn a recent synthesis of research in the connectionist tradition (such as Armstrong\nand Plaut, 2008, 2016; Rodd et al., 2004), Rodd (2020) also proposes a model of meaning\ndisambiguation. Here, the entry point to word meaning access is a static high-dimensional\nsemantic vector tied to the word form (in this model, a binary vector indicating the presence\nor absence of semantic features). This vector is conceptualized as a “relatively unhelpful\nblend state that will likely contain inconsistent elements of meanings that correspond to\ndifferent possible interpretations of the incoming word form” (Rodd, 2020, p. 414), very\nsimilar to the static vectors in distributional semantic vectors outlined above. This model\nfurther assumes that some features have positive connections and co-activate each other,\nwhile others have inhibitory, negative connections. Importantly, context cues can also\nreinforce some features while inhibiting others. For polysemous words, the simultaneous\nactivation of incongruent features in the form-based vectors initially results in an unstable\nstate, which can however be resolved by such context cues (for example, the presence of the\nword player will further activate the sports-related features ofball while inhibiting the\nfestivity-related features). The different word senses of a word then emerge asattractor\nbasins, stable states of co-activated features. However, even if the model assumed that they\nare learned from experience, in practice these high-dimensional feature vectors are typically\nhand-coded, resulting in small-scale implementations with considerable amounts of degrees of\nfreedom available to the modeller (Hummel & Holyoak, 2003).\nIn another recent study, Jamieson et al. (2018) propose an instance theory of\nsemantic memory for which “disambiguation of word meaning, even when the meaning is\nsubordinate, falls naturally out of first principles” (p. 133). This model assumes that\nindividual experiences are stored in episodic memory, and that abstracted representations\nsuch as word meanings emerge during retrieval from memory rather than as a product of\nlearning (see also Ambridge, 2020; Jamieson et al., 2022; Johns et al., 2020). In the\nimplementation of this model (see Appendix B for mathematical details), traces of all\nencountered language examples (e.g., sentences or documents) are stored in episodic memory\nas the sum of random vectors that are each uniquely associated to a given word type. When\na word is then presented to the model as a probe, it activates these traces to a level based on\ntheir similarity to the unique vector associated to the probe, and then produces the semantic\nrepresentation for the word as a weighted sum of these activated traces. By presenting\nmultiple probes at the same time (ajoint probesuch asbreak/car, break/glass, orbreak/news,\nthe model will then produce different representations for these joint probes, thus\ndisambiguating the word meaning. However, these contextualized representations are only\navailable for the entire word-plus-context expression, not the individual words in this\nexpression themselves. It can therefore be argued that the model actually provides\nMEANING MODULATIONS 11\nrepresentations for multi-word expressions, rather than truly disambiguated representations\nfor individual token-level word meanings.\nLi and Joanisse (2021) proposed an implementation grounded in Recurrent Neural\nNetworks (RNNs) and implemented using a Long-Short Term Memory (LSTM) network\n(Hochreiter & Schmidhuber, 1997), which processes sequential information and allows\nprevious steps to influence the representation derived for the current step. Therefore,\nprevious words in a sentence influence the representation of the current one. The influence of\ncontext is continuous rather than discrete, as in attractor models. The proposed RNN model\nlearns by predicting the next word, and develops representations that improve this prediction\n(effectively a probability distribution over all possible vocabulary entries) via exposure to\nnaturalistic language input. Crucially, the semantic representation of a token is the outcome\nof a non-linear integration of the preceding context and the stored abstract semantic\nknowledge related to the word being processed. The model derived clustered representations\nof words in context which reflect human annotations of different word senses, and was more\neffective when words had different unrelated senses. The model was also shown to capture\ncontext dependent similarities (e.g., that people perceive movie admission as close to concert\nadmission than guilty admission) and reading times in sentence processing, where words read\nfaster were assigned a higher probability by the model.\nAll these models share the goal to incorporate the influence of context on semantic\nrepresentations, following different routes that span exemplar-based models (Jamieson et al.,\n2018), variation over prototype models (Kintsch, 2000), attractor models (Rodd, 2020) and\nlearning models (Li & Joanisse, 2021). In the next section, we describe the architecture of\nthe contextualised model of meaning we evaluate in your study, which, we believe, holds\npromise to study the interplay between variance and invariance in semantic representations\nand indicates another possible account, with similarities to all previously mentioned models.\nAt a very general level, BERT strikes what we hypothesize is a productive balance between\nlearning the invariance in experience as well as a model of how context modulates\nrepresentations in systematic ways.\nInherently contextualized language models\nBERT (Devlin et al., 2019b) is a deep neural network that makes use of the Transformer\narchitecture (Vaswani et al., 2017). In its core underlying assumptions and principles, BERT\nresembles other distributional semantic models (see also Lenci et al., 2022), which however –\nas we discussed before – only learn a static type-level representation. BERT, on the other\nhand, further learns to adapt an initial static representation to its current sentence context\nby exploiting information provided by co-occurring tokens in the same sentence. In this\nMEANING MODULATIONS 12\nsection, we will describe how BERT works in general, and how it arrives at context-sensitive\ntoken-level representations.\nAkin to multivariate linear regression models, neural networks consist of a set of\ninput variables and a set of output variables – each variable being a “node” of the network –\nand learn the “regression” weights between those nodes that best predict output from input.\nNeural networks can have a hidden layer between input and output, which effectively\nfunctions as intermediate “output” for the input layer, and as intermediate “input” for the\noutput layer. Such a hidden layer allows the neural network to also model non-linear\nrelations between input and output. It is also possible to have multiple hidden layers, where\neach layer receives input from the previous one(s) and passes on its output as input to the\nnext layer(s).\nIn language modelling, one highly prominent example of such a neural network with a\nhidden layer isword2vec (Mikolov, Sutskever, et al., 2013), which we will briefly explain to\nillustrate some core concepts that are relevant for BERT. The input layer as well as the\noutput layer forword2vec contains one node for each word that appears (sufficiently often) in\na language corpus. The hidden layer contains a fixed number ofN nodes, for large corpora\nusually N = 300 or N = 400. The network is then trained to predict, as accurately as\npossible, which words (output) occur in the immediate surrounding of a given target (input)\nword, or vice versa.3 During network training, this is done successively for each word token\nin the corpus as the target word. At the end of this training process, all weights between the\nnodes are fixed. This has one crucial implication: When setting one specific word in the\ninput to 1 (and all others to 0), the fixed weights between input layer and hidden layer\nspecify all values in the hidden layer. As this hidden layer is then used to predict which\nother words are how likely to occur in the immediate surrounding of the target, this hidden\nlayer captures the “distributional history” of the target word as aN-dimensional vector.\nThis vector is called adistributional vectoror aword embedding, and as in other\ndistributional semantic models it is taken as a word meaning representation (Mandera et al.,\n2017; Mikolov, Sutskever, et al., 2013). Such vectors can be obtained (orextracted) from the\nnetwork for all words in the input layer.\nNeural networks with more than one hidden layer are referred to asdeep neural\nnetworks (see LeCun et al., 2015). Thetransformer architectureis a special type of deep\nneural network which has been popularized in the seminal paper by Vaswani et al. (2017),\nand includes several key components that we briefly outline here.\n3 To this end, the values of nodes for words that are present in the input or output are set to 1, and all other\nnodes are set to 0.\nMEANING MODULATIONS 13\nGiven a sentenceS = {w1,w2,...,w n}, BERT divides it into (sub-)lexical units\nU = {u1,u2,...,u m}. Unlike as discussed forword2vec, these units can but do not have to\nbe full words; most of the time, they will be sub-lexical units which do not have to be actual\nmorphemes (for example,indoctrinate might be divided intoind, oct, ri, andnate. This\nenables the model to deal with out-of-vocabulary words by relying on sub-lexical regularities\nin language: In order to compute the representation for a word, the representations of its\nsub-word tokens are simply averaged. For simplicity, we will still describe the model as if it\nwas trained onwords instead of these sub-lexical units in the remainder of this article.\nDuring training, one or more words are going to bemasked and replaced by the special token\n[MASK], signaling the word to be predicted from its context (similar toword2vec, as\ndescribed before). To each word in the sentence corresponds an embedding (i.e., a\ndistributional vector) at layer 0, the most superficial layer in BERT’s architectures, which is\nthe type-level embedding for the word in the input sentence. The word is thus a pointer to a\nstatic semantic space, conceptually similar to theword2vec type-level embeddings discussed\nbefore. Hussain et al. (2023) provide an accessible tutorial on how to use models like BERT\nfor behavioral research, as well as a detailed explanation of the main classes of such models,\ntheir learning principles, and different architectures.\nEssentially, BERT is trained to recognize which words have been masked: given a\nsentence in input (such as, “the cat is on the table”) one of the words ismasked (“the cat is\non the [MASK]”) and BERT has to predict that the masked word is indeedtable. The\nprediction of masked words is not the only learning objective by which BERT is trained,\nhowever. Next to this task, it is also trained to predict the next sentence. This second\nobjective aims to train the model to develop representations that are sensitive to the\ncoherence of broader linguistic contexts. Therefore, BERT’s embeddings develop to allow the\nmodel to be sensitive to local and broader linguistic contexts. This process happens through\na succession of layers (the exact number depends on specific implementations): at each\nsuccessive layer, the representation integrates more and more information from the sentence\ncontext.\nTo make the model aware of sequential information, BERT also injects positional\ninformation through the use of an additional positional-specific embedding added to the\nstatic embedding of a word, which superficially (without taking into account the actual\nwords in a sentence) distinguishes words at the first, second, ...,n-th position in the sentence.\nThe positional vector is combined with the static embedding of a word and then these\nembeddings are passed through several so-called transformer encoder blocks (Vaswani et al.,\nMEANING MODULATIONS 14\n2017), a neural network that uses a technique calledattention4 to learn on which parts of the\ninput sentence to focus, assigning more weights to those that matter. This device\nimplements the notion that different words are more or less informative in delineating the\ncontext in which a target is used, and thus improve the prediction of the masked word. In\npractice, the attention mechanism generates an output representation for wordi by\ncomputing a weighted average of the representations of all the other words in the sentence.\nThese weights are in turn defined by the similarity between the different word\nrepresentations in the embedded space. The parameters in each transformer encoder are\ncrucially learned during training rather than being pre-specified: these weights encode the\ninformation the model has acquired over training about how context influences the\nrepresentation of a masked word so that the right word-form can be predicted. Thus,\nBERT’s context-sensitive representations for a word in a sentence are the vector sum of a\nstatic word embedding (i.e., a traditional distributional vector), a vector that blindly encodes\nthe word’s position in the sentence, and this weighted sum of the embeddings of other words\nin the sentence, taking into account how informative these words are for predicting the word\nin question. The output of the final layer is a series of embeddings for each word: continuous,\nnumerical vectors which all exist in the same embedding space and can thus be compared\nalso across layers. BERT thus enables us to extract word representations that are fully\ncontextualized: when considering the model’s output, there is no one-to-one correspondence\nbetween word type and word embedding (as in static DSMs) but rather a one-to-many\nmapping: depending on the linguistic context in which it is used, the same word can and will\nhave different vector representations/embeddings. Therefore, BERT learns token-leven\nrepresentations in addition to type-level word embeddings.\nFigure 2 presents a high-level scheme of the process used to train BERT.\nThe BERT model (as well as the transformer architecture with which it is\nimplemented) has been introduced to address some of the shortcomings of the LSTM\nnetwork (Li & Joanisse, 2021), which exploits very similar computational mechanisms: Both\nmodels learn representations by predicting masked words (and contextually learn a\nprobability distribution over all vocabulary items given a certain context), both models\nderive abstract, type level representations that are then warped by context, both models\nlearn which contextual information to assign more weight to in any given context to make\nbetter predictions. However, transformer architectures have been shown to learn other\nlinguistic phenomena (such as short- and long-distance syntactic dependencies; Hewitt and\nManning, 2019) that LSTM models could not. We thus contend it is more fruitful to explore\n4 In spite of the name, attention mechanisms in neural networks are not isomorphic to human attention and\nthey primarily serve as a computational tool to integrate context into embedding representations.\nMEANING MODULATIONS 15\nnice\nThe\n red\n cat\n is\n a\nembeddings\npositional \nencoding\nTransformer Encoder Layers\n+\n +\n +\n +\n +\n +\nLanguage Modeling \nHead\n“feline”\n[MASK]\n .\n+\n +\nSentence: “The nice red cat is a feline.”\ntokenization \nand masking\nembedded representation of \nthe token “is” at the last layer\nFigure 2\nThe sentence is first tokenized (each box at the bottom corresponds to one token) and used to\nretrieve the static embedding corresponding to the word type. This embedding is first\ncombined with positional information and then traverses several transformer encoders which\nused neural attention to integrate linguistic context (in the form of words to the left and to\nthe right of the masked token) and allow the model to correctly predict the masked token.\nthe capabilities of a model which has been reported to fit a wider range of linguistically\nrelevant phenomena.\nContextualized Representations in Natural Language Processing\nContextualized representation models based on transformers have been incredibly successful\nin Natural Language Processing. One of the main grounds of evaluation, in recent years, has\nbeen provided by language understanding benchmarks like General Language Understanding\nEvaluation (GLUE; Wang et al., 2018) and SuperGLUE (Wang et al., 2019). These\nbenchmarks contain multiple datasets that can be used to compare models’ capability on\nmany linguistic and semantic tasks, including coreference resolution, word sense\ndisambiguation, question answering, sentiment analysis, linguistic acceptability,\nparaphrasing, and more. Compared to previous distributional models such as static word\nembeddings learned using each word’s surrounding context (Mikolov, Chen, et al., 2013),\nMEANING MODULATIONS 16\nwords in transformers models can access theentire context through attention (see Section\nInherently contextualized language models). This paradigm shift allows, among other\nadvantages, to build richer representations since every word is contributing, even if slightly,\nto the representation of every other word.\nSeveral linguistic tasks have been shown to greatly benefit from this shift in\nrepresentation learning. Consider, as an example, the sentence “Yesterday, I visitedgrandma\nand I brought there a bunch of stuff. Also, I installed that Alexa device, as you asked. I have\nstrong doubts that it will work but when you’re ready, we can try to video-call her.” where\ncoreferents grandma and her are several words apart. A coreference resolution model based\non word embeddings and LSTMs would struggle to learn such a long-range dependency.\nConversely, BERT, loosely speaking, pays higher attention tograndma when contextualizing\nher, and vice-versa. Semantic tasks such as sentiment classification also benefit from\ncontextualized representations. Take the sentence “The movie has a great start, but it ends\nso badly it was not worth the ticket.”: the sentiment conveyed is different in different parts of\nthe context, and contextualized representations are needed to learn from both. These models\nhave thus provided a noticeable improvement to a number of tasks where long-distance and\ncontext dependencies are of primary importance.\nBERT and following transformer models (e.g., Decoding-enhanced BERT (DeBERTa;\nHe et al., 2021), Pathways Language Model (PaLM; Chowdhery et al., 2022)), have aced\nlanguage understanding benchmarks, outperforming standard representations such as word\nembeddings and even surpassing the level of non-expert humans in some of the proposed\ntasks (Wang et al., 2019).\nAdaptability has been one of the greatest success factors for contextualized models.\nOnce models are pre-trained, i.e., they have learned how to createmeaningful contextualized\nword representations, they can by fine-tuned (i.e., adapted) to solve specific tasks with\nminimal data and computational resources. As such, contextualized models have found\nwidespread adoption in many fields, including computational social science (Card et al.,\n2022), medicine and healthcare (Rasmy et al., 2021; Roy & Pan, 2021), law and legal\nservices (Chalkidis et al., 2020), and finance (Shah et al., 2022; Yang et al., 2020). For\nreference, the BERT pre-trained model was downloaded from the HuggingFace Hub5 about\n50 million times only in May 2023, and LEGAL-BERT, its specialized version for legal text\nunderstanding, 13 thousand times6. Such widespread adoption motivates our focus on\nbi-directional contextualized word representation models based on transformers, among\n5 https://huggingface.co/models\n6 https://huggingface.co/nlpaueb/legal-bert-base-uncased\nMEANING MODULATIONS 17\nwhich we choose BERT as the primary representative given its promising successes in\nmodeling certain aspects of semantic processing, which we review in the next section.\nPrevious empirical studies on contextualised embeddings in psycholinguistics\nNext to their astonishing empirical success across several NLP tasks, transformer-based\nmodels, of which BERT is one prominent instance, have already been extensively researched\nin psycholinguistics (Cevoli, Watkins, & Rastle, 2022; Ettinger, 2020; Rogers et al., 2020;\nTrott & Bergen, 2023; Wilcox et al., 2021). This body of research showed how this class of\nmodels improves over previous implementations on several tasks (although they still fail to\ncapture some phenomena; Ettinger, 2020). However, so far they have been mostly used to\ninvestigate syntactic abilities and to operationalise words’ surprisal in context (Cevoli,\nWatkins, & Rastle, 2022; Hao et al., 2020) to model reading times. A handful of studies,\nhowever, further investigated BERT’s semantic representations and their ability to capture\nthe organization of the mental lexicon (for a discussion of BERT in semantic memory\nmodelling, see Kumar, 2021). We review them below, highlighting how our study connects\nwith their design and insights.\nCevoli, Watkins, Gao, et al. (2022) started investigating contextualised semantic\nrepresentations as a model to deal with semantic ambiguity and how ambiguous words are\nrepresented and processed, linking such models to the theory proposed by Rodd (2020). This\nstudy aims to test whether the level of ambiguity of a word-form (polysemous, homonymous,\nunambiguous) can be recovered from contextualised embeddings in an unsupervised way,\noffering promising, albeit limited, evidence that word forms with different ambiguity profiles\nhad systematically different neighboring structures. This study, however, did not investigate\ncontextualisation across the full network, but focused on the output representations of these\nmodels. We contend that an analysis of how representations are morphed by context can\nprovide insight as to how contextualisation works and whether it aligns with psychological\ntheories of meaning. Finally, this study did not yet compare contextualised and static\nrepresentations with each other, nor attempted to validate whether the same model\narchitecture yields psychologically plausible representations of words in isolationand in\ncontext, which we do in our work.\nTrott and Bergen (2023) further used contextualised representations extracted using\nrecent contextualised embedding models to study lexical meanings and how they interact\nwith context. Using polysemous words and short, disambiguating contexts they study the\ndegree to which word representations are categorical or continuous, coming to the conclusion\nthat human behavior reflects both. Both sense boundaries extracted from curated resources\nas well as cosine distances derived from word vectors predicted reaction times in a primed\nMEANING MODULATIONS 18\nsensibility judgment task, in which participants had to determine which sentences made\nsense and which did not. Processing difficulty, indexed by reaction times in the sensibility\nrating, increased when participants saw an ambiguous word form as a prime and target used\nin different senses as well as when prime and target had a larger cosine distance in the\nreference, context-dependent embedding space. The authors take their evidence to counter\nframeworks positing the existence of a mental dictionary with separate entries for different\nsenses (since continuous measures of semantic similarity also predict processing costs) as well\nas frameworks advocating for purely continuous meaning representations grounded in\nexemplar theories (because sense boundaries account for additional variance over continuous\nsemantic similarity). Instead, they advocate for a hybrid model where both discrete senses\nand continuous relations influence processing and representations. However, this study only\nfocuses on comparing contextualised representations, without considering the relation\nbetween abstract, type-level representations and context-dependent, token-level ones, a gap\nwe address in our study.\nNot specifically related to lexical meaning in context but more broadly to conceptual\norganization, Bhatia and Richie (2022) use BERT as a foundation model and fine-tuned it\non human data to model a wide array of conceptual knowledge tasks, focusing on out of\nsample generalisation. Fine-tuning refers to the practice of further training a model, such as\nBERT, on a new dataset and possibly task to improve its underlying representations for a\nspecific task or set of tasks, and has shown remarkable success in both NLP and cognitive\nscience. More specifically, Bhatia and Richie (2022) use sentences generated from large-scale\ndataset of semantic feature and category norms to make the model better apt at using rich\nconceptual knowledge in a way that approximates representations and processes involved in\nsimple semantic judgments involving thousands of naturalistic concepts and features. Their\nanalysis encompasses ‘25 experiments involving response times in semantic verification tasks,\ntypicality ratings for concepts and categories, the distribution of features across concepts,\nand similarity ratings between pairs of concepts’ (p. 2). The authors further contrasted\nfine-tuned models with static word embeddings as well as the type-level embeddings derived\nby BERT, highlighting how fine-tuning on concept-feature relations improves the ability of\nthe model to reproduce most semantic effects. Our work has several points of contact with\nthis study, in that it focuses on semantic representations and uses transformer-based\narchitectures. However, it also diverges in critical aspects: first and foremost, we aim to\nstudy representations of words in rich sentential contexts and especially how the interplay\nbetween lexical semantics and context morphs these representations. Moreover, while\nfine-tuning typically improves performance, we focus on the pre-trained model to investigate\nMEANING MODULATIONS 19\nwhether the general-level learning mechanism employed by the BERT model already yields\nsemantic representations that capture critical aspects of semantic processing.\nFinally, preliminary evidence on the use of determiners in English (Madabushi et al.,\n2022) and the emergence of phones (Milin et al., 2020) suggests that learning models which\nrely on contextualised representations, of the kind we consider in this work, show a high\ndegree of success. They seem to capture a certain degree of abstractions in how English\ndeterminers are used and show that phone-like units emerge from speech items. The cited\nworks compared learning models with implementations of exemplar-based models\n(J. L. Bybee, 2013; Daelemans & Van Den Bosch, 2010), showing a better fit with respect to\nlanguage data for the former. While some implementations of such learning models still rely\non a prototype-based account, they allow to articulate the tension between a pure\nprototype-based and a fully exemplar-based account (Ambridge, 2020).\nTo sum up, like static, type-level DSMs, BERT aims to capitalize on the invariance in\nlanguage experience, implementing the crude assumption that if the same lexical unit is used,\nit is to refer to the same unit in the world for the same purpose. However, it edges this\nassumption by relying on (linguistic) context to warp the representation tied to a certain\nlexical unit to be better able to predict co-occurring words. The model does not implement\nattractors explicitly nor does it posit their necessity for the model to converge into stable\nstates. Since the model learns representations by predicting words in the sentence, it\ndevelops representations (of lexical items and context influences alike) that facilitate such\nprediction. The model also eschews storing every experience and rather derives\ncontextualised representations on the fly by integrating abstract representations with the\ninfluence of context, both learned via exposure to naturalistic sentences. The model\nrepresents all words in the same way: polysemy is not hard-coded in the model, but is rather\nan emergent property (Cevoli, Watkins, Gao, et al., 2022), with each token representation\noccupying a portion of semantic space. If the token representations of a same word type are\nclose together, the word type will have a rather unambiguous interpretation. The more the\ntoken representations of a same word type spread in semantic space, the more the word type\nis predicted to be ambiguous if presented outside of context (Cevoli, Watkins, Gao, et al.,\n2022).\nOutlook for the present study\nThe aim of the present study is to establish whether word meaning representations obtained\nfrom the BERT model are psychologically plausible word meaning representations at both\nthe token- and type-level. To this end, these representations must meet the following\nrequirements:\nMEANING MODULATIONS 20\n1. The word meaning representations need to have asemantic core. We contend that\n(a) words should not be used entirely arbitrarily and thus not be arbitrarily\nreplaceable in sentence contexts. The meanings ofbathtub and lion in The\n[bathtub/lion] hunts down the gazelle.should not be identical, and bear at least\nsome semblance to their meaning in isolation. At the same time, they should not\nbe identical to their semantic core, as the context necessarily exerts an influence\n(when reading of bathtubs chasing gazelles, one might imagine a cartoonish\nbathtub with legs or a gazelle in the form of a plush toy to accommodate for the\nsurprising situation)\n(b) words need yield a sensible meaning representation when presented in isolation.\nAn extensive body of research has demonstrated semantic effects for words\npresented in isolation (e.g., L. Buchanan et al., 2001; Hino et al., 2002; Kuperman\net al., 2014; Lachmair et al., 2011; Marelli et al., 2015; Pexman et al., 2008;\nShaoul and Westbury, 2010), and any psychologically valid model of meaning is\nexpected to account for this\n2. The word meaning representations should be sensitive to the context the word appears\nin. That is, the representation forball should be different inFor the third time, the\nworried player swung but missed the ball.(more similar totennis) versusThe lonely\nmaiden had great hopes as she dressed to go to the ball.(more similar toprom).\nHowever, meaning modulations should not just reflect different word senses. As\nexemplified in thetomato example at the beginning of this paper, context modulates\nmeaning representations in systematic ways even when a word type is used in the same\noverarching sense in different contexts. We should thus further observe systematic\nshifts in meaning representations also for different tokens of the same word sense in\ndifferent contexts.\nWe examine empirically whether BERT meets these criteria across a series of\ncomputational and statistical analyses of behavioral data, arranged in three sets of studies:\n(1) single-word studies, in which we examine isolated words without context and test whether\ntype-level representations extracted from BERT account for variance in behavioral responses;\n(2) replacement studies, in which we replace a word in sentence contexts with other words\n(considering words with different degrees of semantic relatedness) and study how\nrepresentations of targets and replacements shift based on context; and (3)context variation\nstudies, in which we evaluate the same word in different contexts and investigate how the\ntoken-level representation changes with respect to both token- and type-level representations\nMEANING MODULATIONS 21\nas a function of context, considering word types used in different contexts to express a\ndifferent sense or the same sense. Taken together, these studies allow us to examine how the\ncontext in which a word is presented on the one hand (requirement 2) and its identity on the\nother hand (requirement 1) influence its semantic representation, and how these semantic\nrepresentations capture behavioral patterns.\nThis paper thus aims to empirically validate the psycholinguistic suitability of a\nmodel whose design allows a possible integration of type-level (prototype) and token-level\n(exemplar) representations. We move from a number of studies, reviewed in previous\nsections, showing that BERT succeeds at a number of tasks requiring contextualisation and\naim to establish that this model can be fruitfully used to investigate aspects of human\nsemantic processing by considering a series of related yet unaddressed questions. These cover\nhow representations derived at different levels of the model fare in accounting for both\nexplicit and implicit semantic effects, using ratings, reaction times, and free associations as\nbehavioral measures. The design features we have highlighted above – chiefly, that the model\nlearns a static, context-invariant representation for each lexical unit in its vocabulary as well\nas how to adapt it based on co-occurring lexical items through the neural attention\nmechanism – motivate the choice of such a model for this evaluation, that ultimately\npurports to study the validity of the underlying lexical representations from a\npsycholinguistic point of view. Furthermore, we explore how type- and token-level\nrepresentations interact, which is especially interesting in the light of polysemy across and\nwithin sense boundaries, where both meaning stability and meaning modulations are key.\nOur goal is thus not to investigate whether contextualisation allows BERT to perform the\nway it does, for which ample evidence is available in the literature. Our empirical analysis\ninstead targets the fundamental level of representations in a cognitive perspective and\ntackles questions such as whether word embeddings behave in a way that is useful for\nmodeling semantic representations in a variety of tasks relevant for psycholinguistics.\nAll materials, datasets, and analysis scripts for the studies reported here are available\nat https://osf.io/94ena/?view_only=4e29d179b7664f27b6b7b7ba6b75edf7, except for those\nthat are available elsewhere (e.g., English Lexicon Project, Semantic Priming Project, ...)\nand those that cannot be shared due to licensing constraints (Corpus of Contemporary\nAmerican English). This study was not pre-registered. For most of the analyses we relied on\nexisting datasets and our decisions on the sample size were driven by availability. Whenever\nthis consideration does not apply, we specify how we addressed the issue of statistical power.\nMEANING MODULATIONS 22\nToken and Type representations\nAs we have mentioned in the previous section, transformer-based models learn a static\nrepresentation which is then contextualised exploiting information provided by co-occurring\ntokens, information that helps the model predict which word was masked. In our simulations,\nwe are interested in exploring the relation between type- and token-level representations\nlearned by BERT. However, considering the initial, fully static embedding that BERT learns\nfor a word is not the only way to obtain prototype embeddings given the representations the\nmodel yields. Following common practice in computer vision models (Günther et al., 2023),\nwhere representations of specific images depicting a concept are combined to obtain a\nprototype representation of the concept, vectors of specific word tokens in context can be\naveraged to obtain an abstract representation at the type level of the same word\n(Bommasani et al., 2020; Lenci et al., 2022). Unlike fully static embeddings, these\nrepresentations combine context-dependent, token-level representations and have been often\nused as a proxy for type-level representations in NLP studies (Buijtelaar & Pezzelle, 2023;\nLenci et al., 2022). The intuition is that abstractions at different layers capture different\nlinguistic aspects, with more superficial layers focusing on form-related patterns, mid-layers\ncapturing structural and semantic aspects of tokens, and deeper layers focusing on discourse-\nand sentence-level patterns (Rogers et al., 2020). By deriving type-level representations via\naveraging of token-level representations at different layers, we can thus investigate whether\nthese representations are different than the fully static embedding BERT learns for each\nword and whether this bears relevance to psycholinguistic theories.\nNext to averaging token-level representations and considering the initial, fully static\nrepresentation on which contextualisation happens one can also derive type-level\nrepresentations by presenting the model with the word in isolation, without any context, or\nin sentences which carry little to no contextual information, e.g.This is a Xor They X,\nwith X being replaced by any word form (Bommasani et al., 2020; Buijtelaar & Pezzelle,\n2023). The assumption is that the absence of informative co-occurring words lets the model\nyield the representation of the word itself. Presenting the word in isolation can be used to\nsimulate how the model would interpret isolated stimuli, replicating the experimental\nparadigms in many word-level psycholinguistic tasks. By feeding an isolated word and\nmonitoring how its representation changes through BERT’s layers we can study whether the\nmodel can also account for isolated word effects in a stringent design.\nMEANING MODULATIONS 23\nTo sum up, in all simulations described in this paper, we work with the following\nunderlying representations7:\n• contextualised representations:these are token-level representations obtained by\nfeeding a target word to the BERT embedder, and extracting its corresponding\nembedding. We have one embedding for each combination of word and context.\n• average prototypes:these are type-level representations obtained by averaging 50\ncontextualised representations derived from as many sentences featuring the a given\nword as sampled from the Corpus of American English (CoCA, (Davies, 2009)) to\nobtain a single representation for each word type, akin to static embeddings. We thus\nhave one such embedding for each target word, which collapses 50 random individual\nusages of the word type.\n• non-contextualised prototypes:we fed each word for which we want to derive a\nrepresentation to the BERT embedder in isolation, deriving a representation at each\nlayer and obtaining a type-level representation directly by feeding BERT a word\noutside of any specific context. We thus have one such embedding for each target word.\nFor each of these token- and type-level representations, we obtained 14 embeddings:\n• static embedding:this is the embedding at layer 0, before taking into account the\npositional encoding vector. To clarify, at this layer the token-level representation of the\nword bank in the sentencesYesterday I went to the river bank to fish.and The bank\nclosed its offices downtown and relocated.will be exactly identical. This representation\ncaptures the lexical semantics BERT learns by being exposed to multiple sentences\ncontaining the same surface form (similar to a traditional static DSM). In all plots,\nthis layer is indicated with the index−1. Moreover, this vector will be the same for\ncontextualised, averaged prototype, and non-contextualised prototype representations\nsince it is not affected by context in any way. This vector thus offers a consistent\nstarting point to evaluate how context, even if only capturing the position of the token\nin the sentence, influences representations through the layers.\n• layer 0: this is the embedding at layer 0, after including the positional vector. At this\nlayer, representations for a same surface form will be identical only if the target word\noccurs at the same index in the sentence, so the representations forbank in the two\n7 If a word form is split in multiple tokens (e.g.,capybarais tokenized ascap, #y and #bara), we average the\nembeddings of the tokens first.\nMEANING MODULATIONS 24\nexample sentences above would differ as the target is the sixth token in the first\nsentence and the second token in the second sentence. This representation integrates a\nfirst, weak form of context.\n• layers 1-12: each vector representation captures the token representation at different\nlayers of the model, with layer 1 being superficial and 12 being the deepest layer in the\narchitecture. Representations at deeper layers integrate more contextual information.\nWe consider all layers to analyse to what extent the integration of sentence context\naffects representations across tasks, from those that depend on context more to those\nthat only deal with isolated words.\nWe start our assessment of BERT’s lexical representations by investigating whether\nprototype representations account for isolated word effects, considering three tasks: semantic\nrelatedness and similarity norms (Bruni, Tran, et al., 2014; Hill et al., 2015), prime-target\nsemantic relatedness effects in primed lexical decision data (Hutchison, Balota, Neely,\nCortese, Cohen-Shikora, Tse, et al., 2013; Koriat, 1981), and neighborhood effects in lexical\ndecision data (Yap et al., 2011).\nIsolated word effects\nIn this first suite of studies, we investigate the validity of BERT as atype-level model of\nsemantic memory (probing requirement 1b outlined above that words should have a sensible\nmeaning in isolation, i.e., when they are not presented in a sentence context). To this end,\nwe test the model representations against several large-scale behavioral datasets in which\nwords are presented in isolation, and that all play prominent and influential roles in testing\nmodels of human semantic memory: Explicit word similarity and relatedness ratings, lexical\ndecision times in a semantic priming paradigm, and simple lexical decision times on single\nwords. In all studies, we compare the performance of BERT type-level representations to a\nstate-of-the-art model of semantic memory (Kumar, 2021; Mandera et al., 2017), the static\ndistributional modelwordvec (Mikolov, Sutskever, et al., 2013).\nExperiment 1: Semantic relatedness and similarity norms\nFirst, we focus on explicit off-line judgments, where participants are presented with word\npairs and have to indicate their semantic similarity/relatedness. These ratings are often\ntreated as the gold standard for semantic similarity scores derived from computational\nmodels (Baroni et al., 2014b; Lenci et al., 2022; Pereira et al., 2016).8 Thus, models that\n8 However, from a psychological perspective, we advocate for a more cautious view, in which these judgments\nare seen as a type of behavior that, while heavily influenced by semantic similarity, is also susceptible to\nMEANING MODULATIONS 25\nbetter predict these judgments are taken to be better models of word meanings and lexical\nsemantics.\nIn this first experiment, we replicate previous works and analyze the correlation\nbetween cosine similarities extracted from prototype embeddings obtained from BERT and\nsemantic similarity/relatedness norms obtained by surveying human participants. Semantic\nsimilarity refers to paradigmatic relations between words, such that two words are\nsemantically similar if they can be replaced with one another in context (Sahlgren, 2008). In\nthis sense, similarity captures a rather narrow and specific semantic relation. Semantic\nrelatedness, on the contrary, tackles syntagmatic relations: therefore, for two words to be\nsemantically related, they have to co-occur together without the extra constraint that one\ncan replace the other (Sahlgren, 2008):coffee and drink, for example, have high semantic\nrelatedness but low semantic similarity. Semantic similarity and relatedness – also often\nreferred to as asemantic versus associative relationships (M. N. Jones et al., 2006; Lucas,\n2000) – are crucial to psychological theories of meaning (Kumar, 2021), and as such\nconstitute an essential first step in our study.\nOur analysis is closely related to previous studies (Lenci et al., 2022; Vulić et al.,\n2020), with some notable differences. First, we evaluate each layer separately rather than\naggregations of the first layers, last layers, or only looking at the last layer. Moreover, we\nalso crucially investigate to what extent non-contextualised prototypes capture semantic\nrelations over layers. Our focus here is precisely on the comparison between averaged and\nnon-contextualised prototypes, to establish which of the two approaches is better able to\ncapture behavioral data and thus constitutes a better candidate for modelling human\nsemantic representations. We do not have specific predictions about how the correlation\nshould change over layers, but in order for BERT prototype embeddings to be usable to\nstudy semantic memory, the correlation should be generally high and robust.\nData\nTo analyse semantic similarity norms, we used the SimLex-999 dataset (Hill et al., 2015),\ncomputing the cosine similarity between each available pair of words (N = 990: For 9 word\npairs, we did not find enough sentences in the CoCA containing either word, making it\nimpossible to derive average prototypes. We thus opted to drop these pairs from the\nanalysis.). SimLex-999 provides semantic similarity scores for 999 word pairs with different\ndegrees of abstractness/concreteness and covering adjectives, nouns, and verbs. Word pairs\nwere judged in isolation on a Likert scale and raters were instructed to rate pairs according\nother influences. For these, and other reasons we articulate below, we also present an evaluation of BERT\nrepresentations using other psycholinguistic tasks.\nMEANING MODULATIONS 26\nto how similar the two words are: the pair exhibiting the highest similarity isvanish -\ndisappear, with a score of 9.80, whereas the pair with the lowest similarity isnew - ancient,\nwith a similarity of 0.23 (same value as the word pairshrink - grow). Inter-annotator\nagreement was 0.67, in line with similar benchmarks, and higher for abstract pairs and for\nadjective pairs. This dataset has been extensively used to benchmark models of meaning on\ntheir ability to encode semantic similarity.\nTo analyse semantic relatedness, we used the MEN dataset (Bruni, Tran, et al., 2014),\nanother popular benchmark for computational models of meaning in NLP and cognitive\nscience consisting of 3,000 word pairs. Unlike SimLex, MEN focuses on semantic relatedness,\nbut the structure of the dataset is similar, with word pairs rated by native English speakers,\nthis time on how related they are. Rather than through a Likert scale, ratings in the MEN\ndataset were collected using a comparative approach: raters were presented with two pairs\nand asked to indicate the one featuring the two words with the highest relatedness (e.g.,\ngiven the pairscar - automobileand giraffe - harbor, participants most likely chose the\nformer as more related). Word pairs are thus scored based on how many times they are\npicked as the most related pair in a trial. Each pair appeared in 50 trials, randomly matched\nto other pairs. The most related pairs werecar - automobileand sun - sunlight, which were\nalways chosen as the most related pair in a trial. The least related pair wasbakery - zebra\nwhich was never chosen as the most related pair in a trial. We considered 2,973 word pairs,\nsince for the remaining 27 pairs, one of the words did not occur in a sufficient number of\nsentences in the CoCA to derive the corresponding average prototype. We thus opted to\ndrop these word pairs from the analysis.\nMethods\nIn line with the approach used by the creators of both datasets to use them as a benchmarks\nfor models of semantics (Bruni, Tran, et al., 2014; Hill et al., 2015), we extracted cosine\nsimilarities from computational models for the same word pairs appearing in the datasets\nand computed the Spearman rank correlation between the cosine similarity and the average\nsemantic similarity/relatedness rating of each word pair. The higher the correlation between\nmodel-based cosine similarities and participants averaged ratings of semantic\nsimilarity/relatedness, the better a model’s representations can be taken to encode the\nconstruct of interest.\nAs a comparison point for these various BERT embeddings, we used cosine\nsimilarities extracted from a referenceword2vec space (Mandera et al., 2017) which has been\npreviously validated on psycholinguistic tasks. The model is trained on a concatenation of\nMEANING MODULATIONS 27\nthe UkWaC corpus and the SUBTLEX corpus, with 300-dimensional word vectors and a\ncontext window of 5 words to the left and right of the target word.\nResults\nFigure 3, upper panel, shows the correlations with SimLex-999 semantic similarity ratings\nthe two prototyping approaches (averaging and non-contextualised), with theword2vec space\nas a baseline model. We see that the referenceword2vec space (solid, light-blue line) shows a\nrather low correlation at 0.3. Average prototypes (dashed, dark-blue line) work rather well,\nwith the correlation improving slightly over layers and remaining above 0.5, suggesting that\naveraged prototypes capture explicit similarity ratings moderately well. The picture is mixed\nfor non-contextualised prototypes (long-dashed, blue line): at superficial layers, the\ncorrelation is only slightly lower than for average prototypes, but it declines towards deeper\nlayers. This pattern suggests that when no context is available, BERT’s deeper layers,\ntrained to integrate context, are at a loss and the resulting embedding becomes unreliable.\nWe further show that the correlation across cosine similarities between non-contextualised\nand averaged prototypes (dot-dashed, dark-red line) declines steadily from superficial to deep\nlayers. Averaging correlates more strongly with the referenceword2vec model (two-dashed,\nlight-red line) than non-contextualised embeddings (dotted, red line), and this correlation\nfollows closely that between SimLex-999 ratings and non-contextualised embeddings.\nFigure 3, lower panel, shows correlations involving the MEN dataset. Theword2vec\nbaseline (solid, light-blue line) shows a rather high correlation. Averaged prototypes are\nslightly worse thanword2vec embeddings (dashed, dark-blue line), again with a slight\nimprovement at middle layers. Non-contextualised embeddings are again somewhat reliable\nat superficial layers, but their quality in predicting relatedness judgments deteriorates at\ndeeper layers (with mid layers showing a somewhat less poor correlation; long-dashed bright\nblue line). Again we see that BERT prototypes, averaged and non-contextual, correlate with\neach other less and less over layers (dot-dashed, dark-red line), while averaged prototypes\n(two-dashed, light-red line) correlate more with theword2vec model than non-contextualised\nprototypes (dotted, red line).\nDiscussion\nZooming in on BERT’s static embeddings, thus looking at correlations at layer -1, we see\ngenerally moderate to good correlations with similarity and relatedness ratings. Explicit\nrating data in general and the data sets used here specifically are often employed as the first\nmajor benchmark that computational models of semantic memory need to account for\n(Kumar, 2021; Mandera et al., 2017; Pereira et al., 2016). Alongside previous similar findings\nMEANING MODULATIONS 28\n(Bhatia & Richie, 2022; Lenci et al., 2022), our results thus serve as a first piece of evidence\nindicating that pre-trained BERT’s static embeddings can offer a good implementation of\nsemantic memory. We do however see that averaged prototypes derived from a sample of\ncontextualised embeddings show higher correlations than the static embeddings on both\ntasks while non-contextualised embeddings show worse correlations at deeper layers. We\nthus observe that different layers in BERT’s architecture capitalise on and highlight different\npieces of information extracted from co-occurrence patterns and word distributions. Deriving\naveraged prototypes at mid layers seems to capture explicit semantic associations better, as\nevaluated using human ratings, in line with previous evidence that lexical semantics is best\ncaptured at central layers (Buijtelaar & Pezzelle, 2023; Rogers et al., 2020).\nThis first study thus shows that BERT’s averaged prototype representations account\nwell for explicit semantic ratings, on both semantic similarity and semantic relatedness.\nThese representations are learned from running text, much like what happens withword2vec,\none of the state-of-the-art standard methods to extract distributional semantic\nrepresentations in psycholinguistic studies (Kumar, 2021; Mandera et al., 2017). However,\nBERT shows an advantage over a psychologically validated implementation ofword2vec\n(Mandera et al., 2017) in that it better captures semantic similarity ratings. We also observe\nsubstantial correlations between relations in the BERT prototype space andword2vec,\nsuggesting that the two models indeed capitalise on similar patterns. This correlation,\nhowever, decreases over layers, suggesting that prototypes derived by averaging contextual\nembeddings encode information thatword2vec does not encode.\nWe also showed that non-contextualised representations embed informative relations\nonly at superficial layers. Deeper layers, when faced with the lack of sentence context,\nquickly end up yielding a worse embedding space, which no longer captures semantic\nassociations explicitly appreciated by human speakers. Despite their popularity (Bommasani\net al., 2020), these representations are thus best avoided when modeling psychological\npatterns. The more interesting point, however, concerns the difference between fully static\nBERT embeddings and averaged prototypes. BERT learns the initial static embeddings by\nbeing exposed to several sentences and the static embedding of a word type should end up\ncapturing a blend which, when adequately contextualised at deeper layers, affords a reliable\nprediction of masked words (Rodd, 2020). It is however unclear why the blends found at this\nlayer perform worse than blends derived post-hoc by averaging a handful of\nrandomly-selected sentences which feature the target word. The robust finding that BERT\nbetter captures semantic relations at intermediate layers might suggest that the static\nrepresentation ends up capturing other aspects, including syntactic constraints, that better\nserve in predicting masked words (one of the two tasks on which BERT is trained) but end\nMEANING MODULATIONS 29\nup obfuscating the meaning-related aspects, which better emerge at deeper layers. Future\nstudies should investigate in further detail how representations at intermediate layers differ\nfrom the initial, purely static embeddings to illuminate this pattern.\nAfter showing results for explicit semantic ratings, we turn our attention to implicit\nsemantic effects, which we investigate using lexical decision data (see also Günther et al.,\n2023; Mandera et al., 2017). This is an important point of difference from typical more\nengineering-oriented large-scale evaluations of representation systems, which exclusively\nfocus on explicit ratings and responses (e.g., Baroni et al., 2014b; Lenci et al., 2022; Pereira\net al., 2016; Vulić et al., 2020). In contrast to this, psychological theory-testing lays a strong\nemphasis on also investigating implicit semantic effects in processing times (Balota et al.,\n2007; Griffiths et al., 2007; Günther et al., 2016b; Hutchison et al., 2008; Hutchison, Balota,\nNeely, Cortese, Cohen-Shikora, Tse, et al., 2013; M. N. Jones et al., 2006; M. N. Jones &\nMewhort, 2007; Lund et al., 1995; Marelli & Baroni, 2015; Yap et al., 2011). While one might\nbe tempted to conclude that if BERT captures explicit semantic ratings it will inevitably\nalso capture implicit semantic effects, there are crucial differences that warrant a dedicated\nanalysis. First, semantic similarity ratings are relational, in that the relation between two\nwords, both present during the decision, is evaluated by participants responding. On the\nother hand, lexical decision involves strings presented in isolation. Even in priming studies,\nwhere two words are presented one after another, participants respond towards the second\nitem and are not required to consider the relation between the stimuli. Moreover, semantic\nsimilarity ratings are not a speeded response, whereas in lexical decision participants are\nasked to decide as quickly and accurately as possible whether a string is or is not a word.\nInvestigating the effects of this time pressure, with reduced resources for strategic\nconsiderations, reveals automatic processing mechanisms in lexical access and semantic\nprocessing and is hence crucial for testing psychological theories of word representation.\nFinally, implicit measures can tap into different mechanisms than explicit ones. As we aim in\nthis work to assess the potential of BERT’s learning mechanism to yield representations that\ncan advance our understanding of semantic memory, a full-fledged evaluation cannot stop at\nexplicit ratings but has to investigate other types of behaviors where implicit semantic effects\nhave been documented (Günther et al., 2023; Mandera et al., 2017; Marelli & Baroni, 2015).\nExperiment 2: Primed lexical decision\nWe now analyse the degree to which measures extracted from BERT prototype\nrepresentations are able to predict reaction times (RTs) in primed lexical decision (LD) tasks.\nThese analyses are particularly relevant because human participants have been repeatedly\nshown to be influenced by lexical properties of isolated words in several psycholinguistic\nMEANING MODULATIONS 30\ntasks: while it is not necessary to postulate that participants had learned prototype\nrepresentations to account for these effects of isolated words (Ambridge, 2020; Jamieson\net al., 2018), we contend that a model should account for these effects if it is to be of any use\nas a psychologically viable account of semantics. The analysis of RTs from primed LD has\nalso been often used to validate semantic memory models in general (Kumar, 2021) and\nembedding models in particular (Günther et al., 2016a; M. N. Jones et al., 2006; Mandera\net al., 2017) since it offers a straightforward probe: participants are consistently faster in\nidentifying a word as such when they are primed with a semantically more similar or related\nword (M. N. Jones et al., 2006; Koriat, 1981; Lucas, 2000; Neely, 1991). This analysis\ncomplements the previous one by testing whether semantic relations derived from BERT\nprototype embeddings capture implicit relational semantic effects, next to the explicit\nratings modeled in the previous experiment.\nData\nWe used the Semantic Priming Project (SPP) dataset (Hutchison, Balota, Neely, Cortese,\nCohen-Shikora, Tse, et al., 2013). Participants were asked to judge whether a string is a\nword or not. Before seeing the target, which can be a word or a pseudoword, they are shown\nanother word, the prime, which varies in the degree to which it is semantically associated to\ntarget words. The SPP contains RTs for primes shown at a short Stimulus Onset\nAsynchrony (SOA) of 200ms and at a long SOA of 1,200ms: the SOA indicates for how long\nthe prime remained on screen. We consider both, since the influence of prime-target\nsemantic relatedness can change depending on the time participants had to fully process the\nprime. We analyse RTs for 5,728 unique prime-target combinations, consisting of 1,546\nunique targets and 3,048 unique primes. We discarded prime-target combinations including\nwords for which we could not retrieve a sufficient number of sentences from the CoCA to\nderive the corresponding average prototype.\nMethods\nWe derived the similarity between the prime and target word in each trial computed as the\ncosine between the corresponding word embeddings in the relevant embedding spaces.\nHigher values indicate trials where the prime and target are more semantically similar or\nrelated: we expect this predictor to have a negative effect on RTs, with shorter RTs when\nprime and target are more related.(Günther et al., 2016a, 2016b; M. N. Jones et al., 2006;\nMandera et al., 2017).\nWe then first averaged RTs across subjects forprime : targetpairs at the same SOA\n(in line with Gatti et al., 2022), and divided the dataset according to the SOA, short (200\nMEANING MODULATIONS 31\nms) or long (1200 ms). We computed the Pearson’s correlation betweenprime : targetcosine\nsimilarities computed forword2vec and BERT embeddings and RTs to get a first\ncharacterization of the relation between the two. We then fitted two baseline Generalized\nAdditive Models (one for short SOA, one for long SOA) predicting log RTs as a function of a\nwide range of baseline measures: target frequency, prime frequency, target OLD20, prime\nOLD20, target length in characters, prime length in characters, edit distance between prime\nand target, and Mean Binomial Frequency (MBF) of the target. Frequency estimates were\nretrieved from SUBTLEX-US (van Heuven et al., 2014), OLD20 was computed using the 15k\nmost frequent words from SUBTLEX-US as reference vocabulary, while MBF was computed\nfollowing the procedure outlined in Gatti et al. (2022). All predictors were then Box-Cox\ntransformed andz-standardized separately for short and long SOA, and included in the\nmodel as simple smooths. The base model had an AIC of -12,825.05 when fitted on log RTs\nat the short SOA, and -13,313.68 when fitted on log RTs at the long SOA.\nIn a next step, we computed the cosine similarity between prime and target in the\nreference word2vec space (Mandera et al., 2017), Box-Cox transformed andz-standardized it,\nand added it to the baseline statistical model. In the same vein, we also computed the cosine\nsimilarity between prime and target using BERT prototypes, averaged and\nnon-contextualised, deriving a cosine similarity measure for eachprime : targetpair at each\nlayer, for each prototyping method (averaging and non-contextualised). We Box-Cox\ntransformed andz-standardized also these cosine similarities and added them to the baseline\nGAM. For each model, we computed the∆AIC with respect to the baseline statistical\nmodel (separately per SOA).\nResults\nWhen predicting RTs at short SOA, the model including prime-target cosine similarities from\nthe referenceword2vec space had an AIC of -13,353.28 with a∆AIC = 330.84 against the\ncorresponding baseline, indicating that the additional predictor reliably improved model fit.\nThe prime : targetsimilarity had a significant effect (edf= 4.540, Ref.df= 5.684,\nF = 61.433, p<. 001). Similarly, when predicting RTs at long SOA, the model including\ncosine similarities fromword2vec had an AIC of -13,655.32 (∆AIC = −195.88) and the\npredictor had a significant effect (edf= 2.321, Ref.df= 2.986, F = 68.962, p<. 001). The\neffect ofprime : targetcosine similarity inword2vec on RTs is negative at both SOAs: when\nprime and target are closer in theword2vec space, people are faster in recognizing the target\nas a word (in line with Mandera et al., 2017).\nFigure 4, panela, shows the correlation between prime-target similarity and log RTs\nseparately for the two prototyping methods (color) and SOAs (faceting). The correlation\nMEANING MODULATIONS 32\ndrops at first and then stabilizes between -0.1 and -0.15, but without ever approaching the\ncorrelation derived from the referenceword2vec embedding space. Non-contextualised\nembeddings seem to have a stronger negative correlation with RTs in the SPP than averaged\nprototypes, at both SOAs. The∆AIC (panel b) generally shows thatprime : target\nsimilarities derived from both averaged and non-contextualised prototypes improve model fit\nover the baseline9. Averaged prototypes consistently outperform non-contextualised\nprototypes, and show the starkest improvement at mid-layers, in line with what we observed\nfor explicit ratings of semantic relatedness in Experiment 1: Semantic relatedness and\nsimilarity norms. Notably, the strongest improvement in model fit for averaged prototypes is\nobserved at the same layers for which the correlation is weaker, suggesting that at superficial\nand deep layers, prime-target similarities are collinear with other predictors. Pairwise\ncorrelations10 across predictor variables computed at each layer show that theprime : target\ncosine similarity derived from non-contextualised prototypes is positively correlated with the\nfrequency of the prime. Other predictors are only weakly correlated with the cosine\nsimilarity. Indeed,prime : targetcosine similarities derived from averaged prototypes are\nweakly correlated with all other predictor variables, except for the two deepest layers, where\nwe also see the∆AIC decrease, confirming that the dissociation between correlation and\nimprovement in fit depends on collinearity. However, the improvement in model fit brought\nby prime-target similarities extracted from theword2vec space is consistently stronger.\nDiscussion\nIn this analysis of implicit semantic effects on lexical decision latencies, we see that BERT\nprototypes perform worse than the referenceword2vec space, and that there is a stable\nadvantage for averaged prototypes over non-contextualised prototypes once controlling for\nrelevant covariates. This is in line with a range of previous studies in the semantic priming\nliterature that has played an important role in establishing the cognitive plausibility of\nearlier distributional word embedding models as models of semantic memory (Günther et al.,\n2016a, 2016b; M. N. Jones et al., 2006; Mandera et al., 2017). Our present results thus attest\nto the informativeness of BERT prototypes to capture psychologically relevant prime-target\nrelations. This pattern is moreover consistent with evidence from Experiment 1: Semantic\nrelatedness and similarity norms on ratings of semantic similarity and relatedness, although\n9 Visual inspection of GAM smooths confirm that the effects are qualitatively similar across the two and in\nthe expected direction, with higher similarity correlating with faster RTs. The plots can be seen by running\nthe code detailing the SPP analysis.\n10 The correlation plots can be generated using the code provided in the Online Supplementary Materials.\nMEANING MODULATIONS 33\nnon-contextualised prototypes fared better with implicit semantic effects, consistently\nshowing an improvement in model fit over a strong baseline statistical model.\nNow that we have converging evidence that measure of pairwise similarity extracted\nfrom BERT capture both explicit and implicit semantic effects, although not as well as a\nword2vec space validated on psycholinguistic tasks, we turn to the analysis of semantic\nneighborhoods, which have been shown to influence behavior in psycholinguistics task. This\neffect is particularly interesting because next to being implicit is also non-relational, as it\ndoes not involve the comparison between two strings being presented to participants\n(simultaneously as in similarity ratings or sequentially as in semantic priming), but rather\ntaps onto the broader organization of semantic memory.\nExperiment 3: Lexical decision\nAfter having shown that BERT prototypes can account for prime-target similarity effects in\nprimed lexical decision, we turn to the analysis of a simple lexical decision task, where\nparticipants see a string on screen and have to quickly identify whether it is a word or not.\nWith this data, we expect to replicate findings from previous research that words located in\ndenser semantic neighborhoods are easier to process (e.g., L. Buchanan et al., 2001; Shaoul\nand Westbury, 2010; Yap et al., 2011; see also Bonandrini et al., 2023; Hendrix and Sun,\n2020; Marelli and Baroni, 2015). Notably, the semantic neighborhood densities in almost all\nprevious studies have been determined using (earlier) word embedding models, firmly placing\nour investigation in an established tradition of investigating semantic effects on single word\nprocessing.\nWe thus study here whether relations among prototype representations influence an\nonline task, lexical decision, that has been widely studied in psycholinguistics as a window\non semantic memory. Semantic richness effects in lexical decision are well documented\n(Pexman et al., 2008; Yap et al., 2011) and have contributed to shape theories of learning,\nrepresentation and processing: if BERT fails to account for these patterns, its word\nrepresentations cannot be taken as a useful approach to study semantic memory in humans\nwithout profound modifications. If semantic neighborhood density extracted from BERT, on\nthe contrary, explains patterns in RTs, we provide a new piece of evidence that such a model\nmay offer a fruitful implementation of semantic memory and a tool to study mechanisms\ninvolved in isolated word processing.\nData\nIn order to analyse to what extent BERT prototype embeddings capture variation in LD, we\nused the English Lexicon Project (ELP) dataset (Balota et al., 2007). Among other data,\nMEANING MODULATIONS 34\nthe ELP provides RTs from a LD task, where people are again asked to judge whether a\nstring is a word or not. Unlike in the primed LD paradigm, here participants are not shown\na prime stimulus before the target decision. Using the ELP data, Yap et al. (2011) reported\na facilitatory effect of semantic richness on RTs such that participants are faster in\nrecognizing a target word when this has more semantic associates. We consider 223,380 RTs\nfor 7,165 words from 814 participants: we only considered words which we could find in a\nsufficient number of sentences in the CoCA to derive the corresponding average prototype.\nMethods\nWe computed the semantic neighborhood density (SND) of each target word in the target\nembedding spaces as the average cosine similarity of the 20 nearest neighbors of the target\nword, separately at each layer. When retrieving neighbors, we always used the same\nprototype method, hence the neighbors for an averaged prototype were retrieved among the\naveraged prototypes of the possible neighbors. The set of candidate neighbors was the same\nfor the two prototyping methods. The reference vocabulary from which the neighbors were\nretrieved consisted of all the word types from the different datasets used throughout the\nstudy (SPP, ELP, MEN, and SimLex datasets, and datasets mentioned in the following\nstudies) filtered so that at least 50 sentences could be retrieved for each target word from the\nCoCA in order to derive reliable averaged prototypes, for a total of 24,861 possible neighbors.\nHigher SND values indicate words which have denser neighborhoods in the embedding space:\nwe thus expect this predictor to have a negative effect on RTs (Yap et al., 2011), with words\nfrom denser neighborhoods to be recognized faster.\nWe started by fitting a baseline statistical model using a Linear Mixed-Effect\nRegression (LMER) model11 predicting log RTs from correct word trials. We included the\nfollowing independent variables in the baseline statistical model.: frequency (from\nSUBTLEX-US, Brysbaert et al., 2012); word length in letters; OLD20 (Yarkoni et al., 2008),\ncapturing orthographic neighborhood density as the average Levenshtein distance of the 20\nnearest orthographic neighbors of the target string; Coltheart’s N (Coltheart et al., 1977),\ncapturing phonological neighborhood density as the number of words at edit distance 1 from\nthe target in phonological space; Age of Acquisition (Kuperman et al., 2012); concreteness\n(Brysbaert et al., 2014); valence (Warriner et al., 2013), and word prevalence (Brysbaert\net al., 2019a), all operationalized using subjective norms. All variables were accessed via\n11 Using a linear model allows us to summarise the direction of the effect as well as the improvement in\nmodel fit, to check whether next to accounting for extra variance in RTs, SND influences RTs in the expected\ndirection or not: exploratory analyses revealed the possibility that the coefficient changes sign at different\nlayers and a GAMM would not allow to visualize this clearly.\nMEANING MODULATIONS 35\nLexOPS (Taylor et al., 2020), were Box-Cox transformed, andz-standardized. We added\nrandom intercepts for item and participant. This model had an AIC of -356,039.3.\nWe then computed SND using the referenceword2vec space (Mandera et al., 2017),\nand added this predictor to the baseline statistical model, after Box-Cox transforming and\nz-standardizing it. In addition, we derived SND for each target word at each BERT layer,\nBox-Cox andz-transformed it, and also added it to the baseline model (excluding the\nword2vec predictor). At each layer, we computed the∆AIC against the baseline statistical\nmodel, and extracted thet-statistic for SND, which also gives us the sign of the coefficient.\nWe repeated this for both prototype embeddings, averaged and non-contextualised.\nResults\nWhen adding SND derived fromword2vec embeddings to the baseline ELP statistical model,\nthe AIC increased by 14.656 points, indicating the extra predictor does not improve model fit.\nThe predictor was not significant (β = 0.0004, se= 0.0004, df= 6,948, t= 1.055, p= 0.291).\nConsidering that, in line with expectations, the Pearson correlation between SND and RTs is\nrobustly negative (r= −0.1008 [−0.1048; −0.0968], t= −48.57, df= 229,815, p<. 001),\nthis pattern is likely due to suppression effects, driven by multicollinearity: SND derived\nfrom word2vec is positively correlated with word frequency and negatively with AoA, both\nmoderate correlations. Correlations with other predictors are weak.\nFigure 5 summarizes the results of the statistical analyses involving BERT prototypes,\nwith statistics derived from the referenceword2vec space provided for comparison as a solid\nblack line in all sub-plots. We see that thet-statistic for SND derived from BERT\nembeddings (panel a) starts out positive and gradually becomes negative, suggesting that\nSND extracted from BERT suffers less from collinearity with other covariates than SND\nderived fromword2vec embeddings, whose t-statistic is very close to 0. Finally, when\nconsidering model fit expressed as the∆AIC between the baseline model and the model\nicnluding SND (panel b), we again see a similar pattern for both prototyping methods and a\nstronger improvement in model fit at both more superficial and deeper layers. However, we\nhave to consider that at superficial layers the correlation coefficient andt-statistic were both\npositive suggesting an effect in the opposite direction than we predicted, with words from\ndenser neighborhood eliciting longer RTs. We delve deeper in to this pattern in the\ndiscussion.\nDiscussion\nIn this analysis we looked at whether BERT prototype representations capture effects of\nsemantic richness (operationalized via semantic neighborhood density) in simple lexical\nMEANING MODULATIONS 36\ndecision, to complement evidence from Experiment 1: Semantic relatedness and similarity\nnorms about explicit semantic ratings with an analysis of implicit semantic effects\nand Experiment 2: Primed lexical decision about implicit effects of lexical similarity.\nPatterns observed when using BERT prototypes to capture SND effects in lexical\ndecision are more reliable then those observed when using theword2vec reference space -\nwhich has been validated on psycholinguistic tasks (Mandera et al., 2017). SND extracted\nfrom word2vec, while negatively correlated with RTs, ends up having a non-significant\ncoefficient, due to collinearity issues. SND computed from BERT’s deeper layers, on the\ncontrary, maintains a negative effect on RTs even after controlling for other lexical predictors,\nin line with previous studies (L. Buchanan et al., 2001; Shaoul & Westbury, 2010; Yap et al.,\n2011). Moreover, especially when computed on average prototypes, SND is largely orthogonal\nto other variables, leading to an improved model fit from layer 9 onwards. Our results\nsuggest that BERT embeddings might also offer a methodological improvement to derive\npredictors at the level of lexical semantics to be added as covariates or to be used to control\nfor semantic relations while creating experimental stimuli, due to the absence of collinearity.\nThe relation between semantic richness and lexical decision times has however been\nat the centre of a complex debate that merits further discussion, which makes this analysis\nparticularly interesting in the context of this work. A relevant factor concerns the degree of\npolysemy of a word: Rodd et al. (2002) provided evidence that homonyms, which have\nmultiple unrelated meanings, elicit slower RTs than unambiguous words, while polysemous\nwords, with multiple related meanings, are recognised faster. The processing cost for\nhomonyms has been linked to the need of moving from an initial, uninformative blend state\ncollapsing unrelated meanings, to a single state representation, matching one of the possible\nmeanings. The advantage for polysemous words, on the contrary, has been ascribed to the\nemergence of strong attractors merging related yet slightly different representations. The\npositive correlation between SND and RTs for BERT’s superficial layers which becomes\nnegative at deeper layers should be further investigated considering this aspect (Cevoli,\nWatkins, Gao, et al., 2022). It is possible that at superficial layers representations tend to\ncollapse multiple meanings, with neighborhood density being higher for words which occupy\nportions of the representational space close to many unrelated meanings. When\ncontextualisation kicks in, these representations may grow sharper and more delineated,\nfavoring neighborhood structures with multiple overlapping senses, for which a facilitatory\neffect of SND on RTs have been reported. Statistical models that allow to study the\ntemporal dyanamics of predictors (Hendrix & Sun, 2020) may further illuminate this effect.\nWe conclude by highlighting that studying how representations shift across BERT’s layers is\nMEANING MODULATIONS 37\nan important aspect of the model that can illuminate non-trivial aspects of semantic memory\nand processing.\nIsolated Word Effects: Summary\nIn summary, we showed that well-established measures derived from BERT embeddings\n(cosine similarities and semantic neighborhood densities) successfully capture a range of\nprominent psycholinguistic phenomena in isolated word processing (explicit ratings, semantic\npriming effects, semantic richness effects in simple lexical decision). Taken together, these\nfirst three studies on popular datasets thus establish BERT embeddings as a valuable source\nto derive prototype representations. In particular, these studies suggest that averaged\ncontextualised embeddings should be preferred to non-contextualised embeddings. At the\nsame time however, they also show that static embeddings at the superficial layer already\nencode informative lexical representations that closely mirror those learned by static DSMs\nlike word2vec. However, BERT prototypes do not encode as informative semantic relations\nas a state-of-the-artword2vec model validated on psycholinguistic tasks (Mandera et al.,\n2017) when it comes to ratings of semantic relatedness and prime-target relations in primed\nlexical decision.\nUp to this point, we have treated BERT as if it were another static word embedding\nmodel in a long line of such models (for overviews, see Günther et al., 2019; Kumar, 2021).\nWhile we have defined the validity of type-level semantic representations as a necessary\ncondition for a model of semantic memory, we have not yet tapped into the crucial\ncomponent that sets models like BERT apart from their static predecessors, and which is\ncentral to our present endeavor: The possibility to derive context-sensitive meaning\nrepresentations for each individual word token in a sentence. Thus, having established an\nempirical foundation at the type-level, we can now more confidently venture in further\nanalyses involving the relation between prototypes and contextualised token-level\nrepresentations.\nReplacements\nAfter establishing the robustness and informativity of prototype (i.e., type-level)\nrepresentation) extracted from BERT, we turn to examine to what extent context-free lexical\nsemantics on the one hand and context information on the other hand determine meaning\nrepresentations of wordtokens in natural sentence contexts. To this end, we extracted\nsentences containing specific target words from the CoCA, and systematically varied the\ntarget words (by replacing them with more or less related words) while keeping the rest of\nthe sentence context constant. For example, when selecting the sentenceThe lion hunts\nMEANING MODULATIONS 38\ndown the gazelle.for the targetlion, this target could be replaced with an unrelated word,\ne.g., bathtub, resulting inThe bathtub hunts down the gazelle.or with a related word, e.g.,\ntiger. We sample replacements at different levels of relatedness, operationalised using\nsimilarity derived from free semantic associates from the English Small World of Words\n(SWoW) dataset (De Deyne et al., 2019). This experiment is similar in spirit to the one\nconducted by Ettinger (2020) on event knowledge and semantic role sensitivity in BERT, but\nconsiders more graded relations and extends the investigation to similarity across targets and\nreplacements as well as to their semantic neighborhoods, rather than focusing on which\nwords BERT predicts in given contexts, to probe the underlying representational space\nrather than the output of the model predictions.\nThis setup allows us to test if BERT representations meet requirement 2 (appropriate\ncontext-sensitivity of token representations): if the context has an effect, the token\nrepresentations for targets and replacements should, over BERT’s layers, diverge from their\nrespective type representations and, since they appear in the same context, converge towards\none another. At the same time, it also allows us to test whether BERT representations\nsatisfy requirement 1a (non-arbitrary word use): there should still be a difference between\nthe token representations for the different replacements, to the extent that the original target\nfits the context better than the replacements, and that the token representations are more\nsimilar to their respective type representations than to the replacements’ type\nrepresentations. Furthermore, by systematically varying the degree of semantic relatedness\nbetween targets and replacements (for example, replacinglion with leopard, cat, bear, and\nbathtub), we can formulate these expectations in a more precise and graded manner, as will\nbe outlined in the different simulations below.\nExperiment 4: Lexical representations in context\nWe begin our exploration of the interaction between contextual influence and lexical\nsemantics by analysing the lexical representations of words in context. To this end, we\ncompare the representation of a target word in a sentence in which it actually has been used\nin a representative corpus against that of a replacement word, varying the degree to which\nreplacements are related to the original word (in a psychological model of lexical semantics\nderived from free semantic associations; De Deyne et al., 2019).\nData\nWe first identified a set of target words, starting from the word types available in the Small\nWorld of Words (SWoW) dataset (De Deyne et al., 2019). We filtered this set only\nconsidering relatively frequent words with a Zipf score (Brysbaert et al., 2019a)≥3 in\nMEANING MODULATIONS 39\nSUBTLEX-UK (Van Heuven et al., 2014), and with a percent of speakers knowing the word\n≥50% in the prevalence norms by Brysbaert et al. (2019b). We restricted our focus on\nnouns, verbs, and adjectives as indicated in SUBTLEX-UK (both accessed via LexOPS,\nTaylor et al., 2020), obtaining 5,683 nouns, 1,514 verbs, and 1,738 adjectives for a total of\n8,936 word types. We then retrieved all the sentences from the CoCA which contain one of\nthe target words. After discarding 10 word types for which we could not retrieve at least 50\nsentences, we were left with a set of 8,926 word types.\nExperiment 4a: Pairwise similarities of targets and replacements\nMethods\nFor each of the 8,926 target word types, we derived the cosine similarity with all the other\nword types using the pairwise similarity relations computed using the random walk\nalgorithm applied to the free association data from the English dataset in the SWoW project\n(SWoW-EN; De Deyne et al., 2019). We considered the three associates provided by each\nparticipant for a given cue word. This algorithm implements a spreading activation\nmechanism and creates a global measure of similarity from behavioral responses to a key\npsychological task such as producing free associates given a cue word. We take the resulting\nsimilarities as our gold-standard: terms that are similar in this space are taken to reflect the\nfact that native English speakers see them as related, consistently with empirical evidence\nthat similarity metrics derived from this space account for several benchmarks,\noutperforming other mechanisms to extract similarity from free associates. We used code\nfrom De Deyne et al. (2019), keeping the hyper-parameter values they recommend and refer\nthe interested reader to the original publication for the technical details of the algorithm\nused to derive pairwise similarities. This step caused the loss of an extra word, leaving us\nwith a set of 8,925 word types.\nFor each word type, we then ranked all other word types by cosine similarity. For\neach target word type we then sampled 5 replacement words, considering the following bins:\nthe most related word type (rank∈]0; 1]), closely related words (rank∈[2; 10]), related\nwords (rank∈[11; 50]), loosely related words (rank∈[51; 100]), unrelated words (rank>\n100). Replacements were sampled across word types with the same PoS tags to ensure that\nreplacements do not violate loose syntactic restrictions when substituting a target word in\nthe sentence it occurs in. As an example, consider the target wordcathedral, for which we\nsampled the candidate replacementschurch (cosine similarity = 0.525),abbey (cosine\nsimilarity = 0.384),congregation(cosine similarity = 0.367),chateau (cosine similarity =\n0.171), brewery (cosine similarity = 0.104) (listed from the closest to the farthest).\nMEANING MODULATIONS 40\nAfter having sampled 5 replacements for each target, we retrieved 50 sentences from\nthe CoCA (limiting the sentence length to the maximum context size that BERT can handle,\ni.e., 512 tokens), and replaced the first occurrence of the target word in each of them (see\nTable 1 for an example of how sentences were constructed). We then fed each sentence to our\nBERT embedder to obtain contextualised vector representations for the target word, as well\nas for the 5 replacements, at every layer. We investigate the similarity between lexical\nrepresentations of targets’ and replacements’ contextualised and prototype representations,\nobtained following the pipeline described in the section Token and Type representations.\nWord Replacement Sentence\ncathedral Yesterday, we went to thecathedral for a christening.\nchurch Yesterday, we went to thechurchfor a christening.\nabbey Yesterday, we went to theabbey for a christening..\ncongregation Yesterday, we went to thecongregation for a christening.\nchateau Yesterday, we went to thechateau for a christening.\nbrewery Yesterday, we went to thebrewery for a christening.\nTable 1\nExample of the generation of inputs for the replacement studies. We started from a sentence\nfrom the CoCA containing the target word -cathedral in this example. We then sampled 5\nreplacements with the same PoS tag at different levels of similarity in the English Small\nWorld of Words dataset (SWoW-EN) similarity matrix. Finally, we replaced the target word\nin the original sentence with each of the replacements, yielding 5 sentences with progressively\nmore severe semantic incongruences. The sentence displayed here serves as an example:\nactual sentences are typically much longer.\nFor each sentence where we replaced a word, we thus have the original target and the\ndifferent replacements, for which we can access contextualised token-level representations as\nwell as prototype (type-level) representations, whose similarity relations we want to\ninvestigate. Specifically, we obtained the following representations:\n• contextualised representations of the target (targetc), obtained from the original\nsentences from the CoCA. These representations encode the token-level semantics of\nthe target word in contexts in which it is observed in a representative corpus of English.\n• contextualised representations of the replacement (replacementc), obtained from the\nsentences extracted from the CoCA where the target word was substituted with each\nof the replacements. These representations encode the token-level semantics of each\nreplacement word in a context where it has not been observed in our sample.\n• average prototype representations of the target or the replacement (targetp−avg ,\nreplacementp−avg ), obtained by averaging the token-level representations of the target\nMEANING MODULATIONS 41\nor the replacement when occurring in 50 original sentences from the CoCA. These\nrepresentations encode the type-level lexical semantics of each word – target or\nreplacement –, considering a sample of the contexts in which it occurs in a\nrepresentative sample of English (see Experiments 1–3).\n• non-contextualised prototype representation of the target or the replacement\n(targetp−nc, replacementp−nc), obtained by feeding the target or the replacement word\nto the BERT embedder in isolation. These representations encode the type-level lexical\nsemantics of each target word, operationalized as the representation of a word outside\nof any sentence context (see Experiments 1–3).\nGiven these representations, we derived the following cosine similarities from BERT\nrepresentations, which we compare to “psychological ground truth” SWoW-EN similarity\nrelations:\n1. cos(targetp−[avg|nc],replacementp−[avg|nc]) (the shortcutp−[avg|nc] denotes both\np−avg and p−nc, so this term describes two different measures): the cosine similarity\nbetween the prototype representations of target and replacement. In line with\ncorrelations with MEN and SimLex reported in Experiment 1: Semantic relatedness\nand similarity norms, we expect targets and replacements found to be closer in the\nSWoW similarity space to be also embedded closer by BERT, at various layers.\n2. cos(targetc,replacementc): the cosine similarity between the contextualised\nrepresentations of target and replacement. We expect that replacements that are\nfarther from the target in the SWoW similarity space will be harder to integrate in the\nsentence context, resulting in less similar token-level representations of the replacement\nand the target.\n3. cos(targetp−[avg|nc],replacementc): the cosine similarity between the contextualised\nrepresentation of the replacement word and the prototype of the target word. Again,\nwe expect these similarities to mirror those in the SWoW similarity space, as the\nprototype target vector encodes the target lexical semantics while the replacement\nword is embedded in a sentence where it does not fit. If context sufficed to shift the\nreplacement’s token-level representation close to the target prototype regardless of how\nscarce the semantic similarity between the replacement and the target is, it would be\nevidence that the semantic core learned by BERT would be too easily overridden and\nthat any word in context would be interpreted simply based on the context.\nMEANING MODULATIONS 42\n4. cos(replacementc,replacementp−[avg|nc]): the cosine similarity between the\ncontextualised and prototype representations of the replacement words. The\ncontextualised representation is extracted from a sentence where the token is not\noriginally found (since it replaces another word). The prototype representation, on the\ncontrary, supposedly captures the lexical semantics of the word regardless of context.\nWe can therefore examine how the sentence context affects the token-level\nrepresentation, and we expect that the two representations are similar at superficial\nlayers but diverge at deeper layers, where context will have shifted the contextualised\nrepresentation of the replacement towards the semantics of the original target. This\nshift is expected to be stronger for replacements that are farther away from the target,\nsince we can expect context to exert a stronger influence in these cases.\nWe are interested in the extent to which relative relations of proximity observed in\nSWoW similarity patterns predict pairwise similarities computed from BERT embeddings at\ndifferent layers, and therefore fit a series of regression models where BERT similarities are\nthe dependent variable and SWoW similarities are used as the independent variable in\ninteraction with BERT’s layer.\nWe first averaged the BERT cosine similarity measures for each target-replacement\npair over the 50 sentences in which a replacement substituted the target. Then, we applied a\nBox-Cox transformation andz-standardization to cosine similarity measures separately per\nlayer, so that each cosine similarity measure at each layer was centered around 0: negative\nvalues would indicate lower similarity. Transformation and standardization were applied to\ncounter the known anisomorphism of BERT embeddings, which becomes more and more\npronounced at deeper layers (Ethayarajh, 2019).12\nWe fitted different GAMs, each predicting a different cosine similarity in BERT as a\nfunction of the cosine similarity in the SWoW similarity space (included as a simple smooth),\nthe layer (also included as a simple smooth), and a partial tensor product betweenSWoW\ncosine similarity and layer. The first model predictedcos(targetp,replacementp); the second\ntargeted cos(targetc,replacementc); the third consideredcos(targetp,replacementc), and\nthe fourth addressedcos(replacementp,replacementc). We fitted separate models for\nprototype vectors derived by averaging contextualised representations and by presenting\nnon-contextualised words. We expect similarities from the SWoW space to reliably predict\n12 Essentially, the cosine similarity across BERT embeddings becomes larger between any two pair of words\nat deeper layers, since representations occupy a narrower and narrower hyper-cone of the space. The same\npair of unrelated words, e.g.dog : fog, would have a numerically small similarity at layer 0 which would\nhowever grow at deeper layer, without this necessarily meaning that the two words grew closer relative to\nother word pairs.\nMEANING MODULATIONS 43\ncosine similarities in BERT at all layers. We chose to fit GAMs to explore the possibility\nthat the relation takes a non-linear form, rather than assuming a linear relation. Given that\nwe z-standardized within each layer, the layer variable by itself should have no effect;\nhowever, the partial tensor product can reveal whether the relation between BERT and\nSWoW similarities changes over layers, and thus depends on the (non-)success of the\ncontextualization process.\nResults\nWe first see that BERT prototypes preserve relations of similarity found in the SWoW\nsimilarity space (Figure 6, although differences emerge between averaged and\nnon-contextualised prototypes and we see a general worsening of the fit at deeper layers.\nMuch like averaged prototypes fitted relatedness and similarity ratings (Experiment 1), they\nalso preserve relations derived from free associations, again with a slight advantage at mid\nlayers. However, we see that at layer 12, the deepest one, similar words in SWoW are not\nembedded in a way that preserves relative similarity in BERT. We get back to this pattern\nin the discussion. Non-contextualised prototypes show a good fit at superficial layers, which\nhowever degrades very quickly until no clear relation is observed at layer 12.\nFigure 7 shows the relation between similarity in the SWoW similarity space and the\nBERT similarity between target and replacement contextualised embeddings; we present a\nsingle plot since no prototyping is involved. Here we see that there is a fundamental\ncoherence between BERT and a psychologically grounded model of meaning derived from\nfree association data (De Deyne et al., 2019). The plot exhibits a remarkable fit between\ncosine similarities derived from the two semantic models, one behavioral and the other\ncomputational. This shows that contextualisation does not override differences in lexical\nsemantics entirely: If the token-level BERT representations were fully determined by context,\nthen the contextualized target and replacement representations would have become virtually\nidentical and would thusnot mirror the SWoW semantic similarity structure. Instead, the\nrelation between the two models is remarkably consistent at all BERT layers, from the early\nand still relatively context-independent layers to the late and heavily contextualized layers.\nFigure 8 shows the relation between SWoW similarities and\ncos(targetp,replacementc), separately for the two prototyping methods (left: average\nprototypes; right: non-contextualised prototypes). We see that averaging yields rather\nconsistent relations between BERT and SWoW over all layers, with high similarity resulting\nfrom SWoW’s free associations patterns predicting high relative similarity in BERT (with\nthe curves flattening at very high SWoW similarities). When deriving BERT prototypes by\nfeeding isolated words we see a positive relation at superficial layers which however largely\nMEANING MODULATIONS 44\ndisappears at the deepest layer, a pattern we have seen in all analyses comparing semantic\nrepresentations so far. This strengthens the interpretation that when BERT has no context\nto integrate and is asked to perform a task which is quite far from its training objective, its\nrepresentations no longer capture semantic relations robustly.\nThe difference between averaged and non-contextualised representations is also found\nwhen consideringcos(replacementp,replacementc) (see Figure 9). Here we expect to see an\nindirect effect of the similarity between target and replacement in the SWoW similarity\nspace. Consider the sentenceThe pride of lions chased the gazelleand consider replacing the\ntarget gazelle with a semantically similar word,rhino, and a dissimilar word,fork. At the\nearly layers,replacementp and replacementc are always very similar, because context has\nnot yet affected these embeddings. As this is independent of the relation between target and\nreplacement, we expect no relation between similarities in BERT and similarities in SWoW\nat the early layers. At deeper layers, however, the context (in which the replacement is\nnormally not found) will exert more and more influence onreplacementc: if the replacement\nis very dissimilar than the target (as withfork), the context should shift the replacement’s\ntoken-level representation further away from the replacement prototype and closer to the\ntarget prototype (which in turn leads tocos(replacementc,replacementp) becoming more\naligned withcos(target,replacement) from SWoW). If the replacement already is very\nsimilar to the target, this should happen less.\nIn Figure 9, we see that the expected relation between BERT and SWoW similarities\nemerges over layers at least up to a certain similarity score in the SWoW similarity space,\nwhereas the effect appears flat around 0 at the lower end of the similarity values in the\nSWoW semantic space. This indicates that contextualisation starts exerting a relevant\ninfluence already after only a few layers have been traversed, implying that context starts\nplaying a role soon in BERT’s architecture. Therefore, the contextualized representation of\nthe replacement changes through BERT’s layers and becomes more and more similar to that\nof the target, as reflected in the fact that the SWoW similarity between target and\nreplacement predicts the similarity between the contextualised and prototype representations\nof the replacement. This pattern attests to a systematic and meaningful influence of context\non lexical representations in BERT. Moreover, we see that this pattern emerges only when\nthe target and the replacement are rather similar in SWoW. When this is not the case,\nBERT struggles to morph the replacement’s contextualised representation into one\nresembling the target. This again confirms the role of a word’s semantic core, which\ninfluences the degree to which it can or cannot be contextualised successfully.\nIn the next experiment we go beyond comparing individual representations and\nadditionally consider their semantic neighborhoods. This will allow us to better understand\nMEANING MODULATIONS 45\nwhere, in the underlying semantic space, representations shift through contextualisation, and\nto better characterise the influence that contextualisation exerts on the static representations.\nWe will discuss the results of these two related experiments at the end of the section.\nExperiment 4b: Neighborhood structure\nThe previous analysis remains mostly blind as to how exactly contextualisation unfolds and\nhow meaning representations shift with respect to the larger semantic space when context\nexerts its influence. Even if they preserve relative pairwise similarities, for example, the\ncontextualised representation of the replacement could move away from its corresponding\nprototype because the contextualised representation becomes meaningless. For example,\ngazelle and fork might be dissimilar because the contextA pride of lions chased the [word].\npulls the embedding offork towards totally unrelated words to bothgazelle and fork. If this\nis the case, we contend that contextualisation takes place but still fails, since we expect the\ncontext to influencefork in a systematic way. However, pairwise similarity patterns\ndescribed above might result from contextualisation pulling the embedding of the\nreplacement (fork) closer to the representation that the targetgazelle gets in the same\nsentence. By comparing the semantic neighbors of the replacement and the target in the\nsame sentence, we can check to what extent the former representation gets dragged towards\nthe latter by context, and get a more detailed picture of how representations shift in the\nembedding space as a function of their context-independent similarity and the level of\ncontextualisation (operationalized as the BERT layer).\nData\nWe used the same targets, replacements, and sentences as in Experiment 4a: Pairwise\nsimilarities of targets and replacements.\nMethods\nFollowing a method introduced in the study of language change (Gonen et al., 2021) to\ndetect words whose meaning changed over time, we rely on the Jaccard coefficient (a\nstandard measure for semantic neighborhood comparisons; Cassani et al., 2021; Hamilton\net al., 2016), which is computed as the ratio between the cardinality of the intersection\nbetween two sets and the cardinality of their union. Higher coefficients indicate a higher set\noverlap, and thus a higher consistency of the neighborhoods. Unlike cosine similarities,\nJaccard coefficients are not affected by the anisomorphism of BERT spaces, removing the\nneed for further transformations and making the results more interpretable at face value.\nMEANING MODULATIONS 46\nWe started by retrieving the 250 nearest neighbors at each layer for the four target\nrepresentations (targetp, targetc, replacementp, replacementc). Neighbors were retrieved\namong the prototype representations at the same layer and ranked by cosine similarity.\nPrototype representations exist in the same embedding space as the contextualised\nrepresentations and can thus be directly compared. Neighbors of contextualised\nrepresentations were retrieved separately from averaged and non-contextualised prototypes,\nwhereas neighbors of prototype representations were derived among prototypes obtained in\nthe same way, so the neighbors of an averaged prototype can only be other averaged\nprototypes. The set of possible neighbors was the same we used to compute SND, consisting\nof 24,861 word types.\nWe then computed the following metrics to capture the coherence of semantic\nneighborhoods:\n• jaccard(targetc,replacementc): the Jaccard coefficient between the 250 nearest\nneighbors of the contextualised target and the 250 nearest neighbors of the\ncontextualised replacement. The higher the Jaccard coefficient, the more neighbors are\nshared and thus the more the two neighborhoods are consistent with each other.\n• jaccard(replacementc,targetp−[avg|nc]): the Jaccard coefficient between the 250 nearest\nneighbors of the contextualised replacement and the 250 nearest neighbors of the\nprototype target.\n• jaccard(replacementc,replacementp−[avg|nc]): the Jaccard coefficient between the 250\nnearest neighbors of the contextualised replacement and the 250 nearest neighbors of\nthe prototype replacement.\n• ∆jaccard = jaccard(targetc−1,targetc12) - jaccard(replacementc−1,replacementc12):\nthe difference between (i) the Jaccard coefficient between the 250 nearest neighbors of\nthe contextualised target at layer -1 (the fully static embedding) and layer 12 (after\ncontext has been fully integrated in the token representation), and (ii) the Jaccard\ncoefficient between the nearest neighbors of the contextualised replacement at layers -1\nand 12. We did this for every sentence in which the target appears, with the prototype\nrepresentations of all targets and replacements at layers -1 and 12 respectively serving\nas possible neighbors. A positive number indicates that the target’s neighbors were\nmore coherent than the replacement’s neighbors between layers -1 and 12.\nFor the first two Jaccard coefficients,jaccard(targetc,replacementc) and\njaccard(replacementc,targetp−[avg|nc]), we expect the coefficient (i) to increase at deeper\nlayers, which would indicate that context has indeed shifted the replacement representations\nMEANING MODULATIONS 47\ntowards that of the target (contextualised and prototype), and (ii) to drop when similarity in\nthe SWoW similarity space also drops, showing that it is harder to integrate more dissimilar\nwords in context. On the contrary, we expectjaccard(replacementc,replacementp−[avg|nc])\nto be high at superficial layers, where context has not yet affected the contextualised\nrepresentation of the replacement, which is thus close to the corresponding prototype, but\nlower at deeper layers, where context has shifted the representation towards that of the\ntarget. For the latter reason, we also expect the coefficient to drop when similarity in the\nSWoW similarity space drops, consistently with the other two coefficients.\nFinally, concerning∆jaccard, we expect the target to have a higher coherence\nbetween neighbors at layer -1 and 12. On the contrary, the replacement’s neighbors should\ndiffer more between superficial and deep layers, as the sentence context will drag the\nreplacement’s representation away from its prototype representation (hence, its semantic\ncore) to look more like the target. We expect, therefore, that∆jaccard increases when target\nand replacement are farther apart: While the target-component of the difference should\nremain unaffected, replacements that are farther from the targets they replace are expected\nto be more affected by context and thus move farther away from their prototype (resulting in\nsmaller Jaccard coefficients for the replacement-component and thus a larger overall∆).\nWe once again averaged Jaccard coefficients for the sametarget : replacementpair\nover the 50 sentences in which a same replacement substituted the target word, and fitted a\nGAM to predict each Jaccard coefficient as a function of layer (simple smooth), cosine\nsimilarity between target and replacement in the SWoW similarity space (simple smooth),\nand a partial tensor product between the two.\nResults\nFigure 10 shows the predicted coherence for the first three Jaccard coefficients\n(jaccard(targetc,replacementc): top panels;jaccard(replacementc,targetp−[avg|nc]): mid\npanels; jaccard(replacementc,replacementp−[avg|nc]): bottom panels) as a function of\nsimilarity between target and replacement in the SWoW similarity space, separately for the\ntwo methods to derive prototypes (averaging: left panels, non-contextualised: right panels).\njaccard(targetc,replacementc) increases with the increase in SWoW similarity as well as at\ndeeper layers with a significant interaction, in line with our predictions. This shows how the\ncontextualised embedding of the replacement gets pushed closer to the contextualised\nembedding of the target when the target-replacement similarity increases and when context\nexerts a stronger influence. This confirms that the embedding’s replacement changes in the\ndirection of the semantics presupposed by the target it replaces (Nieuwland & Van Berkum,\n2006). It further demonstrates that the contextualisation process yields increasingly similar\nMEANING MODULATIONS 48\nresults to the original target when the prototype embedding of the replacement is already\nquite similar.\njaccard(replacementc,targetp−[avg|nc]) presents a similar pattern when considering\naveraged prototypes, but unexpectedly flips when looking at non-contextualised embeddings.\nIn this case, the coherence decreases at deeper layers, suggesting that when the prototype is\nderived with no context, the representation moves away from the target and indicating once\nagain that non-contextualised prototypes deteriorate at deeper layers due to the lack of\nsentence context. This analysis complements the previous one by showing that the\ncontextualised replacement does not simply shift closer to the representation of the target in\nthe same context, but also closer to the prototype representation of the target, at least when\nconsidering averaged prototypes, and further attests to the systematic (and expected)\ninteraction between lexical semantics and context.\nIn the bottom panels, we see howjaccard(replacementc,replacementp−[avg|nc])\nchanges over layers. Unsurprisingly, at layer 0 the SWoW similarity does not influence\nneighborhood coherence, since the neighborhoods being compared pertain to the same word\ntype, and the coherence is rather high. When moving to deeper layers, however, we see the\ncoherence drops substantially, for both prototype approaches (more for non-contextualised\nprototypes). SWoW similarity plays a very minor role at all layers, with slightly more\ncoherent neighborhoods between the contextualised and prototype replacement at higher\nSWoW similarity values, in line with expectations. This confirms that when a target is\nreplaced by a word which is semantically more related, the neighbors of the contextualised\nreplacements tend to overlap more with those of the target, although the main influence here\nis that of contextualisation over layers, which pulls the replacement in context away from its\nlexical semantics. On the basis of this analysis, along with those presented in Experiment 4a:\nPairwise similarities of targets and replacements, non-contextualised embeddings at deeper\nlayers confirm to provide unreliable representations for semantic analyses.\nThe last analysis we present in this section quantifies the difference in coherence\nbetween the neighbors of the contextualised target and the contextualised replacement\nbetween layer -1 (encoding static embeddings) and layer 12 (encoding fully contextualised\nembeddings). Figure 11 shows the estimated effect: As indicated by the blue solid line\nreflecting averaged prototypes, increases in similarity between target and replacement in the\nSWoW similarity space come with a decrease in the Jaccard∆ (edf= 7.698, Ref.df= 8.57,\nF = 120.6, p<. 001), eventually reaching 0 at high SWoW similarity values. For averaged\nprototypes we thus see that when the replacement is semantically dissimilar from the target\nin the base psychological model of meaning (i.e., low values on the x-axis in the plot)\ncontextualisation drags the representation further away from its semantic core than it does\nMEANING MODULATIONS 49\nfor the target. When the replacement is similar to the target, and thus is expected to fit\nbetter in the sentence, the two contextualised representations are affected in a similar way\nand are embedded consistently between the static layer and the fully contextualised layer.\nOn the contrary, the difference is 0 regardless of SWoW similarity when considering\nnon-contextualised prototypes (edf= 1.006, Ref.df= 1.012, F = 0.02, p= 0.907), denoting\nno relation between neighborhood coherence and prototype similarity in a psychological\nmodel of meaning. This, once again, confirms how the embedding space derived from\nnon-contextualised words is a poor reflection of semantic relations.\nDiscussion\nThis set of analyses shows that BERT’s embeddings encode lexical semantics in a way which\nis highly consistent with the SWoW similarity space as probed using both pairwise\nsimilarities and neighborhood coherence. The pattern already emerges at the static\nembedding layer (-1 in the plots) but also once contextualisation unfolds. Thus, BERT\npreserves a semantic core of words even through its contextualisation, meeting our first\ndesideratum. Moreover, in line with previous evidence from Experiment 1: Semantic\nrelatedness and similarity norms and Experiment 2: Primed lexical decision, we see that at\ndeeper layers, non-contextualised prototypes pay a price in the fidelity of their\nrepresentations (as compared to averaged prototypes). It is worth thinking of the objective\nBERT is trained with, predicting a masked word from its context: The embedding should\nsupport an accurate prediction of the word type that occurs in place of the mask. When we\nreplace a target word with a very different word (left end of the x-axis in the plots shown for\nthe last two experiments), contextualisation struggles to integrate such a semantically\ndifferent replacement (due to the static embedding encoding a word’s lexical semantics) and\nthe resulting representation is very different from that of the target prototype (see, for\nexample, Figure 9). However, when the similarity between the target and the replacement in\nthe SWoW similarity space increases, the replacement can be more easily contextualised, as\nevidenced by the higher neighborhood coherence between the neighbors of the contextualised\nreplacement and those of the target (see Figure 10, top and mid row). This pattern captures\nthe vital interaction between lexical semantics on the one hand and context on the other\nhand. Critically, BERT’s context-sensitive architecture allows the model to learn this\ninteraction in a data-driven manner, enabling it to successfully shift a word’s semantic\nrepresentation based on context it appears in.\nExperiment 4b: Neighborhood structure further confirms that while context\ninfluences token-level representations in BERT in systematic and meaningful ways, words\ncannot be arbitrarily replaced in context, meeting the desiderata for requirements 1b and 2\nMEANING MODULATIONS 50\nstated in the Introduction. Contextualisation – the influence exerted by context at deeper\nlayers in the network – shifts the representation in meaningful ways, as captured by the\nneighborhoods: the in-context replacement will be pulled away from its respective prototype\nreplacement and closer to the target representation. Finally, contextualisation is predictably\nmore successful when the semantic cores of target and replacement are more similar.\nThis context sensitivity is important: a newspaper should look less like a prototypical\nnewspaper in the sentenceI used a newspaper to cover my head when it started raining(and,\nfor example, provides very different affordances; Glenberg and Robertson, 2000). Nieuwland\nand Van Berkum (2006) show that in sufficiently supportive discourse contexts,\nanimacy-violating predicates (e.g.,the peanut was in lovepresented after a story about a\npeanut with anthropomorphic features) elicited no N400 components whereas predicates\nrelating to the core meaning of the word form (the peanut was salted) did result in perceived\nanomalies. The authors take this as evidence that language comprehension does not involve\nan initially context-free semantic analysis. Anectodal evidence from Cevoli, Watkins, Gao,\net al. (2022) suggests that when presenting BERT with the discourse prompts used in the\noriginal study, the model predicts words such assinging, talking,or dancing after The peanut\nwas. This indicates that the model’s representation of peanut at deeper layers - those that\nsupport the prediction of masked words - has shifted away from the prototypical\nrepresentation ofpeanut and closer to that ofperson. However, this does not necessarily rule\nout the presence of an underlying stable, static, context-independent semantic representation.\nOur analyses generalise the exploration conducted by Cevoli, Watkins, Gao, et al. (2022) to\nmany different targets and replacement with graded relations of semantic similarity to the\ntarget. Even though our contexts consisted of a single sentence, we still see that context\ninteracts with the static embedding in informative ways, which manifest in how\ncontextualised representations shift in relation to prototype representations. Our results thus\nspeak to the co-existence of context-independent and context-dependent representations: in\nparticular, we see that BERT can in principle account for semantic violations that are\ndetermined by context and go against semantic violations entailed by the\ncontext-independent semantics of concepts (Nieuwland & Van Berkum, 2006).\nBased on our results one could also expect that the lower the semantic similarity\nbetween the context-independent representations of target and replacement, the richer the\ncontext should be to over-ride the expected semantic anomalies. This hypothesis could be\nfurther tested by replicating Nieuwland and Van Berkum (2006)’s experiment varying the\ninformativity and length of the discourse preceding the target violation. BERT thus\npresupposes a model of semantic processing which combines context-independent\nrepresentations, whose role emerges clearly from the wealth of semantic effects for isolated\nMEANING MODULATIONS 51\nwords (some of which we addressed in the first section of this work), with context-dependent\nrepresentations that emerge considering the representations themselves as well as their\nneighborhoods.\nThe analysis we performed can also speak to theories of metaphor (Kintsch, 2000;\nLakoff & Johnson, 1980), for which distributional spaces have already been fruitfully used\nand which have long served as a prominent test case for context-sensitive distributional\nmodels (Harati et al., 2021; Kintsch, 2000; Utsumi, 2011; see Nick Reid and Katz, 2018). We\nshow that BERT can offer a model of how new metaphors could come to be, potentially\npredicting which words could function better as vehicles for certain tenors by considering\nhow well they can be integrated in context and how their nearest neighbors shift with\nrespect to the semantic field being targeted.\nAfter having established that BERT accounts for isolated word effects and that the\nrepresentation it learns integrate context effects with a stable semantic core, we turn to\ninvestigate context effects in more detail. Specifically, we consider how meaning changes as a\nfunction of context when different senses are involved in order to analyse the nature of\nmeaning shifts.\nMeaning in Context\nSo far we have shown that prototype representations capture offline (similarity and\nrelatedness ratings) and online (semantic neighborhood and prime-target similarity effects in\nlexical decision tasks) psycholinguistic semantic effects. Moreover, we saw that the BERT\nmodel is sensitive to the interplay between prototype and context-dependent representations\nby replacing certain words in context with other words at varying degrees of semantic\nsimilarity as gauged from a psychological model of meaning (De Deyne et al., 2019). In the\nlast suite of experiments we address the very same question from the opposite perspective,\nkeeping the target words constant while changing the sentence contexts. This will provide us\nfurther evidence with respect to requirement 2 (appropriate context-sensitivity of token\nrepresentations) and requirement 1a (non-arbitrary word use). We also investigate whether\nBERT preserves meaning differences reflected in lexical taxonomies, by investigating how it\ncaptures different senses of homonyms and polysemous words and whether it even captures\nmore fine-grained meaning differences that do not necessarily match different senses (cf. the\nexamples we provide in the introduction aboutchairs and tomatoes, where the same word in\ndifferent sentences refers to the same sense or even the same exact referent, but evokes very\ndifferent associations, affordances, and semantic features).\nTo this end, we leverage three datasets that allow us to investigate how\nrepresentations change as a function of context when considering words whosemeaning\nMEANING MODULATIONS 52\nchanges as a function of context: (i) theWords in Context(WiC) dataset (Pilehvar &\nCamacho-Collados, 2018); (ii) the stimulus materials from the seminal study by Till et al.,\n1988, who conducted a priming experiment with polysemous primes introduced in\ndisambiguating sentences; and (iii) theMeaning Modulations(MeMo) dataset, which we\ndeveloped for the purpose of this work. This dataset goes beyond the issue of polysemy,\ninstead focusing on nuanced differences in meaning for words used in the same sense but that\nnonetheless undergo slight modulations due to context, such that they would elicit different\nassociations.\nIn studying how context modulates meaning across and within sense boundaries, we\ncompare contextualised embeddings from BERT with the Instance Theory of Semantics\n(ITS), a purely exemplar-based model proposed by Jamieson et al. (2018), which was shown\nto produce representations that support semantic effects pertaining to isolated words as well\nas capture different senses of homonyms.\nExperiment 5: Word Sense Disambiguation\nThe WiC dataset (Pilehvar & Camacho-Collados, 2018) is a popular benchmark in the NLP\ncommunity to test algorithms for Word Sense Disambiguation (WSD): we use it to study\nwhether the representation of a same word type changes following sense distinctions reflected\nin sentence context. As an example, consider the wordbark in sentencesThe bark of the tree\nwas slightly burnt.and We heard the bark of a dog in the distance.. For BERT to offer a\nuseful model of meaning for psychological purposes, we expect the representation ofbark in\nthe two sentences to grow different over layers and to become more similar to that of\npossible synonyms in each context, e.g.,cortex or howl respectively. While previous work has\nextensively addressed BERT’s ability to reflect context-dependent meaning shifts (see\nApidianaki, 2023; Haber & Poesio, 2024, and references therein for a comprehensive\nsummary of how these models have been used to detect different word senses in NLP), our\nanalysis here focuses on how the process unfolds over layers and especially on the relation\nbetween context-dependent representations of appropriate and inappropriate synonyms in\ncontext. Given an ambiguous word, likebark, bothcortex and howl could be valid synonyms\nof the word in isolation: given the evidence presented about replacing related words in\ncontext, we zoom in here on what happens in BERT’s representations when two possible\nsense-dependent synonyms replace an ambiguous word in a context where the synonym is\nindeed a synonym of the ambiguous word, and in a context where it is not.\nMEANING MODULATIONS 53\nData\nThe WiC dataset provides polysemous words, each used in two different target sentences.\nThe original task for which the dataset was created is to determine whether a given target\nword is used in the same sense in the two sentences or not. As an example, consider the\nword ball and two target sentencesThe player swung but missed the ball.and The girl\ndressed up to attend the ball.. In this case, the dataset indicates that the target word is used\nin two different senses. On the contrary, for the target wordbeat and the sentencesWe beat\nthe competition.and Agassi beat Becker in the tennis championship., the gold standard\nindicates the same sense. Human performance puts the upper bound for this dataset at 80%,\nwith BERT scoring above 70% in the task of WSD (Breit et al., 2020), indicating promising\nperformance. Rather than testing BERT on the task again, we aim to investigate how the\nrepresentations of the target word shifts across layers with respect to other words which are\nsemantically related to each sense of the target. We thus provide a more fine-grained and\nsemantically more sensitive analysis than studies on pure word sense disambiguation.\nMethods\nWe first filtered only the items where the target was used in a different sense, since we are\ninterested in how the representation of a word shifts when used to index different meanings.\nThen, we manually annotated a sample of items by adding a synonym of each sense from its\nrespective BabelNet synset13. Going back to theball example introduced before, we\nannotated the first sentence with the synonymsphere (the appropriate synonym for the first\nsentence and theinappropriate synonym for the second) and the second with the synonym\nparty (the appropriate synonym for the second sentence and the inappropriate synonym for\nthe first).\nWe had 92 different target words in our final annotated dataset, each used in a\nvariable number of sentences between 1 (the vast majority of target words, 56) and 12 (the\nhighly ambiguous wordhead). 143 different words were used as the synonym of the target\nword in the first sense. 138 different words were used as the synonym of the target word in\nthe second sense. These statistics show a considerable variability in the number of target\nwords and senses, offering an informative benchmark to assess the ability of computational\nmodels to tease apart different senses of ambiguous words and, crucially, relate those senses\nto appropriate synonyms resulting from expert annotations. Since each item is embedded at\nthe usual 14 BERT layers, we worked with 2,548 data points in the statistical analysis.\n13 We release this resource with our annotations for replication purposes in the OSF repository.\nMEANING MODULATIONS 54\nWe derived the following embeddings from BERT, extracting token-level\nrepresentations at the usual 14 layers:\n• targets1|s2: the contextualised embedding of the polysemous target word (ball in the\nexample) in the first (s1) and second (s2) sentence;\n• appropriates1: the contextualised embedding of the appropriate synonym of the target\nin the first sentence,sphere in the sentenceThe player swung but missed the {word}.;\n• appropriates2: the contextualised embedding of the appropriate synonym of the target\nin the second sentence,party in the sentenceThe girl dressed up to attend the\n{word}.;\n• inappropriates2: the contextualised embedding of the inappropriate synonym of the\ntarget in the second sentence,party in the sentenceThe player swung but missed the\n{word}.;\n• inappropriates1: the contextualised embedding of the inappropriate synonym of the\ntarget in the first sentence,ball in the sentenceThe girl dressed up to attend the\n{word}.;\nFrom these representations, we derived the following metrics:\n• cos(targets1 ,targets2 ): the cosine similarity between the contextualised embeddings of\nthe target word in the two sentences. We expect these representations to diverge over\nlayers, reflecting the fact that before context is integrated, they point to the same\nlocation in semantic memory, but once context is factored in, they identify different\nportions of semantic space, in line with the two different senses.\n• cos(targetsn,appropriatesn): the cosine similarity between the target’s contextualised\nembedding in a sentence and the appropriate synonym’s contextualised embedding in\nthe same sentence (sentence index indicated asn), when using the former to replace\nthe latter in the sentence (e.g.,cos(balls1 ,spheres1 )). We expect this similarity to\nincrease over layers: while at superficial layers the synonym and the target may be\nrather different, especially if the polysemous target is used in an infrequent sense, we\nexpect the two representations to grow similar once context exerts its influence, since\nboth words fit in context and are supposed to share their meaning.\n• cos(targetsn,inappropriatesn): the cosine similarity between the target’s\ncontextualised embedding in a sentence and the inappropriate synonym’s\nMEANING MODULATIONS 55\ncontextualised embedding in the same sentence (sentence index indicated asn), when\nusing the latter to replace the former in the sentence (e.g.,cos(balls1 ,partys1 )). We\nexpect this similarity to increase less than the previous one. It might be tempting to\nhypothesize that this similarity should decrease over layers, but we know from previous\nwork that BERT’s space is strongly anisomorphic (Ethayarajh, 2019), especially at\nhigher layers, so words will have higher similarity values just by virtue of this.\nMoreover, in our first set of analysis, we saw that contextualisation shifts the\nrepresentation of a semantically dissimilar replacement towards that of the target.\nHowever, for contextualisation to reflect a psychologically plausible process, we contend\nit is sufficient to observe thatball becomes, over layers, more similar to the appropriate\nsynonym sphere than to the inappropriate synonymparty when these words’\nrepresentations are extracted from the same sentenceThe player swung but missed the\nball.\nWe analyzed the data using Generalized Additive Mixed Models (Baayen et al., 2017,\nGAMMs). In a first model, we predictedcos(targetc1,targetc2) as a function of layer\n(included as a simple smooth), including a random intercept over the combination of target\nand synonyms, which indicates which observations at different layers pertain to the same\ntrial. We expect the simple smooth of the independent variable to have a negative relation\nwith the dependent variable, reflecting the fact that the two representations are identical at\nthe most superficial layer (since the word type is the same) but progressively grow different\nfollowing the different sentence context.\nWe then fitted a model to analyze whether the polysemous target grows closer to the\nappropriate synonym than to the inappropriate synonym. To this end, we fitted a GAMM\nincluding layer as a simple smooth, condition [appropriate | inappropriate] as a parametric\nterm, an interaction between the two, and random intercepts for the combination of target\nand synonyms to group cosine similarities pertaining to the same pair of sentences over\nlayers. To eliminate the possible confound brought by the anismorphic embedding space, we\nBox-Cox transformed andz-standardized cosine similarities separately for each layer. We\nhypothesize that the standardized cosine between target and appropriate synonym grows\nover layers while the standardized cosine involving target and inappropriate synonym\ndecreases. We thus expect to observe a significant interaction between condition and layer.\nComparison to the ITS model.As a point of comparison, we then turned to\nextract representations using the Instance Theory of Semantics (ITS) model (Jamieson et al.,\n2018), a psychologically-motivated contextualized representation model (see the respective\nsection in the Introduction). We provide a detailed explanation of the equations behind the\nMEANING MODULATIONS 56\nITS model in the Appendix Mathematical details of the Instance Theory of Semantics (ITS,\nas well as a Python implementation of the ITS model in the OSF repository14.\nIn detail, we implemented two versions of the ITS, changing the underlying corpus.\nThe first reproduces the model tested by Jamieson et al. (2018) in their first presentation of\nthe model. Memory traces come from the passages in the TASA corpus, which are made\navailable on the personal page of the first author together with an R implementation of the\nmodel to reproduce results from the paper: we took the input passages as they were\npre-processed by Jamieson and colleagues. Each passage is a few hundred words long, with a\nvocabulary of∼65,000 word types and a little more than 35,000 passages, each yielding a\nmemory trace. We chose to test the very model which was tested by the original authors as\nit was shown to provide interesting empirical results for both isolated words and homonyms\nin minimal contexts. However, since BERT embeddings are derived from a different and\nlarger corpus such as the CoCA, we also implemented a version of the ITS which uses the\nCoCA to derive traces. Moreover, some of the sentences from the WiC dataset mostly\nconsist of function words: using a corpus from which these have been removed (as in the\npre-processing chosen by Jamieson et al. (2018), prevents us from modeling all items in our\ndatasets, introducing a possible bias. We thus implemented a second ITS model using the\nCoCA and a slightly different pre-processing pipeline. We kept the same dimensionality for\nthe word vectors. We further pre-processed the corpus to exclude strings in all caps, which\ntypically indicate extra-linguistic tags in the CoCA, and punctuation marks. Finally, we\nlower-cased all strings. We considered each document from the CoCA to be a passage to\nencode in memory, although a document in the CoCA is considerably longer than a passage\nin the TASA.15 We did not try to optimize the model, e.g., changing the pre-processing\npipeline or other aspects of the retrieval procedure, just like we did not fine-tune the\npre-trained BERT base model, aiming to test whether the general architecture - also\ninstantiated in a way that has had empirical success in the past - meets our desiderata about\nthe relation between context and lexical representations. Finally, we discarded word types\n14 The memory footprint of this model increases considerably with the vocabulary size, as a new high\ndimensional word vector has to be generated for each word type, and with the number of passages for which\na trace is derived: we provide two implementations, one which uses vectorized operations and is faster but\nalso more memory intensive, and another which computes operations sequentially, resulting in slower and less\nmemory intensive computations. We provide the opportunity to speed this up through Python\nmultiprocessing, but the code has only been tested on a Linux server.\n15 There are multiple options as to what to consider as the units to derive traces from: we considered using\nsentences, which however were too many to compute with and sampling introduced a further degree of\nfreedom we preferred to avoid. Ultimately, deriving a trace for each document in the corpus preserved a\nthematic unity in each passage that we contend still captures the purpose of the model, although it differs\nfrom the original implementation.\nMEANING MODULATIONS 57\nwith a frequency lower than 50 (but retaining words which feature in the target datasets\nused in this and following experiment) in the CoCA documents, to have a more manageable\nvocabulary and exclude word types with a lower frequency of occurrence. Despite these\ncares, the wordconservatorism which features in the WiC dataset as a target synonym did\nnot appear in the corpus, forcing us to discard a trial from the analyses. The pre-processed\ncorpus contains close to 250,000 passages (each yielding a memory trace) and a vocabulary\nof ∼95,000 word types.\nWe also pre-processed the materials in the WiC dataset following the same approach,\nremoving punctuation, and excluding word types that did not feature in the vocabulary of\nthe model, and lowercasing all tokens. The ITS model derived from the TASA corpus does\nnot cover a few of the words in the items of our benchmark datasets, and as such cannot\ncapture some items, which we will drop in the statistical analysis.\nThe target measure is the cosine similarity between the echoes of the sentence and\nthe synonym, with two conditions: appropriate - when the synonym matches the correct\nsense of the polysemous target word in the sentence -, and inappropriate - when the\nsynonym matches the other sense of the polysemous target word in the sentence. We expect\nthe cosine similarity between the echoes of sentences and synonyms to be lower in the\ninappropriate condition, and tested this hypothesis by fitting a linear mixed model with the\ncosine similarity as dependent variable, condition (as a binary variable) as the independent\nvariable, and random intercepts for target words.\nResults\nIn the first GAMM predicting the cosine similarity between target word’s representations in\ndifferent sentences extracted using BERT using layer as the independent variable, the\npredictor has a significant effect (BERT layer:edf= 8.922, Ref.df= 8.998, F = 753.7,\np<. 001), which is negative and wiggles at deeper layers (see Figure 12, left panel). This\nconfirms that the contextualized representations of the polysemous target tend to diverge\nover layers. This is especially remarkable considering the anisomorphism of the underlying\nembedding space, which naturally pushes representations closer together.\nResults of the second GAMM confirm that the target interaction between layer and\ncondition (a boolean indicating whether the synonym-context combination is appropriate or\ninappropriate) improves model fit over a simpler model only including the linear combination\nof layer and condition16, with a∆AIC of 107.81 points. Visual inspection of the effects\n16 We compared the target model with a simpler model not including the interaction using thecompareML\nfunction from theitsadug R package (van Rij et al., 2022). For the purpose of using this technique, we fitted\nthe GAMMs using Restricted Maximum Likelihood.\nMEANING MODULATIONS 58\n(Figure 12, right panel) further shows that the interaction goes in the expected direction,\nwith standardized similarities significantly increasing over layers for appropriate synonyms\n(edf= 2.422, Ref.df= 3.014, F = 18.82, p<. 001 and decreasing for inappropriate\nsynonyms (the smooth is the exact mirror since for each congruent pair there is always\nanother incongruent one). We see that the difference grows steadily at initial layers and then\nstabilizes towards deeper layers.\nTurning to the ITS models, the similarity between sentences and synonyms is not\nsignificantly different for appropriate and inappropriate synonyms (TASA:b= −0.0133,\nse= 0.007, t= −1.804, p= 0.0719; CoCA:b= −0.0424, se= 0.05554, t= −0.766,\np= 0.444).\nDiscussion\nThis analysis thus shows that contextualisation works in a meaningful way, confirming\nempirical results from WSD models using BERT (Pilehvar & Camacho-Collados, 2018). Our\nresults further characterize how BERT supports accurate WSD: polysemous words and\nhomonyms have similar representations at superficial layers when used in sentences which\ntrigger different senses, reflecting a stable semantic core. Context integration then operates\non this semantic core, pulling the representations away from each other and closer to those of\nappropriate synonyms for each sense (Cevoli, Watkins, Gao, et al., 2022). This is not merely\nan epiphenomenon of the structure of the embedding space: appropriate synonyms in the\nsame context are more similar to the target when compared to the inappropriate synonym.\nDeeper layers are more sensitive to context.\nIt is important to keep in mind that these pairs are rather noisy in that the\npolysemous target may be more semantically related to one of the two synonyms than the\nother. For example, one trial featuresplace as the polysemous target, with synonymspassage\n(synset number bn:00062707n in BabelNet) andspace (synset number bn:00062701n in\nBabelNet) as synonyms in the sentencesHe lost his place on the page.and A political system\nwith no place for the less prominent groups., respectively.Space and place are more related\nthan passage and place when considering static representations to begin with, and yet the\nrepresentation ofplace in the first sentence grows closer topassage than toplace. The fact\nthat we observe a reliable effect of layer even in the presence of such noise is a further\nindication of the substantial influence of the contextualisation process on semantic\nrepresentations.\nThe observation that the deepest layers exhibit a stable difference between\nappropriate and inappropriate synonyms is of further interest as it suggests that large\ndifferences in meaning that run between separate senses of homonyms and polysemous words\nMEANING MODULATIONS 59\nare reflected already at mid layers. These are also the layers where we observed the best fit\nwith explicit ratings of semantic similarity and relatedness in Experiment Experiment 1:\nSemantic relatedness and similarity norms. Together, these two pieces of evidence suggest\nthat BERT recognizes coarse differences in meaning before getting to the deepest layers,\nconfirming previous evidence about the specialization of different attention blocks for\ndifferent linguistic phenomena (Rogers et al., 2020).\nFinally, the analysis of similarities derived using the ITS model contrasts with the\nevidence provided by Jamieson et al. (2018) that the ITS model can account for meaning\nmodulations that cross sense boundaries. The failure of the model trained on both corpora\nsuggests, thus, that the model may not very robust. Moreover, the presence of stop words\nmay be particularly detrimental to this model: since these tend to frequently occur in all\npassages, they may overshadow subtle differences in the distribution of content words. We\nconsidered the distinctiveness of each passage by considering the Jaccard coefficient between\nthe word types in each passage from the TASA corpus and then each passage of the CoCA\n(both after pre-processing): while the set overlap is around 0.05 for the TASA, it is around\n0.3 for the CoCA, confirming that passages tend to overlap a lot more in the CoCA in terms\nof lexical items. This is however a characteristic of the input, and we contend that a\nsuccessful model should be able to discard useless or misleading information: the weighting\nforeseen by the ITS does not seem to handle the noise in memory traces sufficiently well for\nthe model to handle realistic sentences.\nIn this study, we considered the relation between fully contextualised representations.\nIn the next one, instead, we consider how contextualised representations of homonyms and\npolysemous words in disambiguating contexts interact with the corresponding prototype\nrepresentations.\nExperiment 6: Priming in context\nIn the previous study, we have seen that contextualised BERT representations are able to\nsuccessfully disambiguate word senses in sentence contexts, thereby focusing on\ncontext-dependent representationsalone: we only compared contextualised representations,\nconsidering the polysemous word and two sense-related synonyms. Here, we address another\ncrucial issue in contextualisation: What is the relation between context-dependent\nrepresentations of ambiguous word types and prototype-representations, which inevitably\ncollapse different senses? In order to answer this question, we rely on the stimuli from the\nstudy by Till et al. (1988) (see also Kintsch, 1988). We follow the approach outlined in\nM. N. Jones et al. (2006), analysing the relations across stimuli and comparing the patterns\nof similarity produced by the model with the qualitative pattern observed in the experiment.\nMEANING MODULATIONS 60\nData\nThe experimental material consists of 56 texts from the original study by Till et al. (1988,\nAppendix A). These were written around 28 ambiguous words, with two sentences per\nambiguous word (such as“For the third time, the worried player swung but missed the ball.\nHe knew what the coach would say.”versus “The lonely maiden had great hopes as she\ndressed to go to the ball. This was her only opportunity to meet people.”. As for these\nexamples, the texts always consisted of two sentences. In 26 texts, the critical ambiguous\nword was presented as the last word of the first sentence (as in the example provided here).\nIn the other 30 texts, it was presented as the last word of the second sentence instead.\nEach text was paired with a context-appropriate associate word for the critical\nambiguous word (bat for the first text,dance for the second), taken from the dataset by\nCramer (1970). These associate words served as the real-word targets for the lexical decision\ntask: After presentation of the critical, ambiguous word (here,ball), participants were\nrequired to make a lexical decision task for a target word which was either a pseudoword or,\ncritically, a context-appropriate associate word (bat for the first sentence,dance for the\nsecond) or context-inappropriate associate word (dance for the first sentence,bat for the\nsecond)17. The context-appropriate word for one sentence in the text pair was always used as\nthe context-inappropriate word for the other text.\nIn their Experiment 1, Till et al., 1988 found no response time differences between\nthe appropriate and inappropriate associate when it was presented very shortly after the\ncritical ambiguous word (stimulus onset asynchrony (SOA) = 333 ms), but observed\nsignificantly faster responses for the context-appropriate associate after some delay (SOA =\n1000 ms). In a second experiment, Till et al. (1988) found the difference also at short SOAs,\nbut still smaller than at long SOAs, confirming the main pattern of Experiment 1. This\nsuggests that the sentence contexts activate a disambiguated meaning representation of the\ncritical (originally ambiguous) word, which in turn primes context-appropriate words to a\nstronger degree than context-inappropriate ones (this disambiguation process appears to\nrequire some time to unfold, as indicated by the effects of SOA).\nOur final dataset consists of 28 critical words (e.g.,ball) used in two different\nsentences in two different senses (see the description above), each paired with two target\nwords (e.g.,bat and dance) each associated to either of the senses in which the critical word\nis used, for a total of 56 different target words, on which lexical decision RTs were collected\n17 The original study by Till et al. (1988) also included additional experimental conditions with appropriate\nor inappropriate inferences, which we did not consider for the present analysis.\nMEANING MODULATIONS 61\nin the original study. Each item (critical and target words) was embedded at the usual 14\nBert layers, for a total of 1568 data points.\nMethods\nIn this experiment we compare the contextualised representation of the critical polysemous\nword (which appears in a sentence which is supposed to affect its meaning) to the prototype\nrepresentation of the target word (presented in isolation). With respect to the studies\npresented in Replacements, the current experiment provides a complementary test of\nwhether models like BERT can be fruitfully used to compare contextualised and\nnon-contextualised representations: Priming effects in context are one case in which the\npossibility to do just this can help us understand semantic effects better.\nOur hypothesis is straightforward: Extensive previous research has shown that cosine\nsimilarities between distributional semantic vectors are well-suited to capture priming effects\nin lexical decision (Gatti et al., 2022; Günther et al., 2016b; M. N. Jones et al., 2006;\nMandera et al., 2017; Petilli et al., 2021). If contextualised BERT representations are\ncognitively adequate token-level meaning representations for words in sentence contexts, then\nwe have to expect that the cosine similarities between the contextualised BERT\nrepresentations for the critical words and the prototype representations of the\n(context-appropriate and context-inappropriate) target words in a priming LD experiment as\nthe one by Till et al. (1988) exhibits a pattern consistent with behavioral evidence from Till\net al. (1988) (in line with the methodology in M. N. Jones et al., 2006).\nWe fed the stimulus materials (sentence prompt, critical word, and target word) to\nour BERT embedder, to obtain the following three representations:\n• criticalc: the contextualised embedding of the critical polysemous word (ball, in the\nexample above) in the context of the sentence, at the usual 14 layers;\n• targetnon−cont: the non-contextualised prototype embedding of the target word (bat or\ndance depending on the sentence), which participants had to rate as a word or\npseudoword, extracted at the usual 14 layers;\n• targetavg : the averaged prototype embedding of the target word (bat or dance\ndepending on the sentence), which participants had to rate as a word or pseudoword,\nextracted at the usual 14 layers\nNote that, unlike in the WiC experiment, the embeddings of the target words (bat or\ndance) do not change when they are used as appropriate or inappropriate targets, since the\nrepresentation is a prototype. Appropriateness or inappropriateness of the synonym here\nMEANING MODULATIONS 62\ndoes not depend on a difference in the representation of the target, but on a difference in the\nembedding of the critical word, which is the same in both conditions but embedded in\ndifferent sentences. From these representations, we computed the following cosine similarity\nscores in the appropriate and inappropriate condition:\n• cos(criticalc,targetnon−cont)\n• cos(criticalc,targetavg )\nto capture to what extent the polysemous critical word is similar to the prototype\nrepresentation (non-contextualised or averaged) of the target word, which was judged for\nlexicality in isolation. The hypothesis is that the similarity grows in the appropriate\ncondition but decreases in the inappropriate condition (once the anisomorphism of the\nunderlying embedding space has been accounted for by standardising similarities).\nTurning to the ITS models, we derived echoes using the same memory traces derived\nin Experiment 5: Word Sense Disambiguation. The target measure is the cosine similarity\nbetween the echoes of the sentence (including the polysemous critical word) and the target\nword which elicited the lexical decision, with two conditions: appropriate - when the target\nword matches the sense of the polysemous critical word in the sentence -, and inappropriate -\nwhen the target word matches the other sense of the polysemous critical word in the\nsentence. Much like in Experiment 5: Word Sense Disambiguation, we expect the cosine\nsimilarity between the echoes of sentences and target words to be lower in the inappropriate\ncondition, and tested this hypothesis by fitting a linear mixed model with the cosine\nsimilarity as dependent variable, condition (as a binary variable) as the independent variable,\nand random intercepts for critical words.\nWhen analysing BERT representations, we fitted a GAMM for each prototyping\nmethod, including condition [appropriate | inappropriate] as a parametric term, layer as a\nsimple smooth, an interaction between the two, and random intercepts for each combination\nof critical and target word. As usual, to avoid that the underlying anisomorphism of the\nembedding space at deeper layer confounds results, we Box-Cox transformed and\nz-standardized the cosine similarities by layer.\nResults\nFor both prototyping methods we see that the target interaction improves model fit over a\nmodel only including a simple linear combination between layer and condition\n(∆AIC = 112.43 for averaged prototypes;∆AIC = 53.79 for non-contextualised prototypes).\nVisual inspection of the effects (see Figure 13, left panel: averaged prototypes; right panel:\nnon-contextualised prototypes) confirms that the interaction goes in the expected direction,\nMEANING MODULATIONS 63\nwith appropriate target words given the sense of the critical word in context growing more\nsimilar over layers (averaging:edf= 2.399, Ref.df= 2.986, F = 20.23, p<. 001;\nnon-contextualised: edf= 3.197, Ref.df= 3.967, F = 7.736, p<. 001). The cosine similarity\nbetween the critical word and the inappropriate target word, on the contrary, decreases over\nlayers (the smooth terms for layer and the inappropriate condition are exact mirrors of those\nreported above since each critical word occurs in two sentences), showing that context\noperates in the expected direction. Importantly, at layer -1 (i.e., for the static embedding)\nthe cosine similarity is not different between the two conditions. We also see that the\nstatistical models explain a substantial amount of deviance (63.9% for averaged prototypes\nand 52.8% for non-contextualised prototypes), confirming that the interplay of\ncontextualisation and condition accounts for much of the phenomenon at hand. Comparing\nprototyping methods, we again see that non-contextualised prototypes become unreliable at\nvery deep layers, where the presentation of an isolated word to a model trained on rich\nsentence context creates instability in the representations. With averaged prototypes, on the\ncontrary, the effect is stronger moving from superficial to mid layers and again plateaus at\ndeeper layers.\nTurning to the ITS models, the similarity between sentences and synonyms is not\nsignificantly lower in the inappropriate condition when using both corpora to derive traces\n(TASA:b= −0.0134, se= 0.0145, t= −0.920, p= 0.361; CoCA:b= −0.0081, se= 0.1116,\nt= 0.073, p= 0.942). Moreover, the similarity between target words and sentences including\nambiguous critical words is not significantly different from 0 also in the appropriate\ncondition, with a strongly bimodal distribution and the two modes around 0.8 and -0.8 in\ncosine similarities for both instantations of the ITS model we probed. This distribution\nsuggests that, next to missing the distinction between appropriate and inappropriate target\nwords given the ambiguous critical word embedded in a sentence, the resulting echoes do not\ncapture semantic relations in general. This result stands in contrast with evidence provided\nby Jamieson et al. (2018); we address this in the discussion.\nDiscussion\nIn this experiment, we observe a similar pattern to the one reported for word sense\ndisambiguation using the WiC dataset. The two analyses share many traits, but whereas the\nanalysis of theWiC dataset compared fully contextualised representations, the current\nanalysis extends it to comparing contextualised and prototype representations. In line with\nevidence from studies discussed in the Replacements section, this further confirms that\nBERT is a viable tool to investigate the interplay between words in and outside of context.\nMEANING MODULATIONS 64\nContext operates in a systematic way on representations and shifts their position in the\nspace relative to other words, as evidenced by the analysis we just described.\nThe pattern we show mirrors the findings reported by Till et al. (1988): At superficial\nlayers we observe no or only scarce differences between appropriate and inappropriate target\nwords, which grow larger at deeper layers. By drawing a parallel between SOA and degree of\ncontextualisation (with the simplified assumption that a context-sensitive processing\nmechanism unfolding over time, as in Kintsch, 1988’s model, is roughly mirrored by the\npassing of information through BERT’s layers), the lack of differences at superficial layers\nreflects the lack of differences in RTs at short SOAs, whereas the differences we observe at\ndeeper layers can be taken to mirror the difference in RTs at long SOAs. In psychological\nterms (Kintsch, 1988, 2001), at short SOAs, language processing has not yet managed to\nintegrate contextual information and thus the decision operates on representations which still\nheavily depend on the semantic core, which pertains to the type more than to the context.\nAt longer SOAs, however, when context has been integrated and the representation has been\nshifted towards a specific sense which either fits or does not fit with the semantics of the\ntarget, RTs are faster for appropriate targets, just like the representations of the critical\nword and the appropriate target have grown closer. Of course, the underlying assumption of\na parallel between processing time and contextualisation unfolding through BERT’s layers\nhas to be empirically investigated in future studies.\nIn addition to mirroring behavioral results obtained in a seminal study on semantic\nprocessing of ambiguous words, our analysis further shows that contextualisation in BERT\nproduces contextualised representations whose similarity to prototype representations\nchanges in semantically meaningful ways. We discussed in the Introduction how meaning\nvariation could be recovered post-hoc from static embeddings (Beekhuizen et al., 2021),\nsuggesting that usage patterns provide models with sufficient information to learn about\ndifferent senses. BERT appears to implement learning mechanisms which equip it with the\nability to magnify certain aspects of the static representation and suppress others depending\non context. This process results in different similarity profiles for the same word type in\ndifferent contexts.\nUnlike what we observe with BERT embeddings, the echoes derived using both\ninstantiations of the ITS model did not capture the hypothesized relations, and showed other\nconcerning properties. Differently from the setup used by Jamieson et al. (2018) for their\nevaluation of joint probes to capture semantic ambiguity, the probes used in this experiment\nincluded much longer sentences. This poses a problem for the ITS model as currently\nspecified, since when probes contain multiple words the activation used to weigh traces when\ncomputing the echo is multiplied several times. This yields very small echoes, that tend to\nMEANING MODULATIONS 65\nshrink the more elements a probe contains. Smaller activations result in smaller traces,\nwhich result in smaller echoes: this process, we suggest, results in very brittle echoes which\nmay look more or less similar to the echoes of other probes following noise more than signal\nfrom semantic relations found in the input. Crucially, this behavior does not only manifest in\nthe model derived from the CoCA but also in that derived from the very same passages\nwhich yielded a successful model in a previous study by Jamieson et al. (2018). Moreover,\nwhereas when considering BERT the analysis of the WiC dataset and the current dataset\npresented important conceptual differences (a comparison of contextualised embeddings in\nthe WiC dataset against a comparison of contextualised and prototype embeddings in the\ncurrent experiment), the two analyses using the ITS model are isomorphic: the echo of an\nisolated word is compared to the echo of a complex probe (a sentence), overshadowing\nimportant differences. As we have already mentioned, the ITS cannot derive the echo of a\nword in context but only the echo of the context. Given the empirical results and these\nconsiderations, we suggest that the representations produced by the ITS model as currently\nimplemented do not offer a particularly useful tool to study meaning modulations in natural\nlanguage. Conversely, our analyses do suggest that contextualised embeddings derived using\ntransformer models such as BERT do offer a fruitful tool to study meaning representations\nin isolation and in context.\nExperiment 7: Meaning Modulations\nThe previous two studies investigated whether BERT captures differences in meaning for\nhomonyms and polysemous words. However, as argued in the Introduction to this article,\nthis reflects only a (prominent) portion of the much wider phenomenon of contextualized,\ntoken-level meaning representations. In the present experiment, we therefore zoom in on\nsubtler context-dependent meaning modulations that, we contend, exist even when such\nmodulations may not result in discrete meanings (i.e., different senses). We operationalize\nthis intuition as different associations being elicited by a word used in the same sense in\ndifferent sentence contexts. To expand on thetomato example we mentioned in the\nIntroduction, we can use the wordtomato to refer to the exact same referent, but by using it\nin different sentences, the same word indexing the same referent may evoke different\nassociations (Zwaan et al., 2002). Saying thatI plucked a fresh tomato from the garden.may\nevoke associations with plants, a round shape, and solid texture, while sayingParts of the\ntomato are still visible in the sauce.may evoke associations with spaghetti, hot temperatures,\nand mushy texture. We argue that a good model of semantic memory should be sensitive to\nthese modulations, and thus the representation oftomato in context should shift\nsystematically as a function of context, not only when context differs between different\nMEANING MODULATIONS 66\nsenses of a word but also following these subtler meaning modulations. This analysis is the\nfirst empirical study of how lexical representations of words used in different sentence\ncontexts yet still in the same sense vary systematically with respect to prototype\nrepresentations of semantically related lexical items. We argue that such an analysis can\noffer a more detailed characterization of how meaning shifts as a function of abstract\nprototype representations and linguistic context, an interplay that has so far only be\naddressed for words used in different senses.\nData\nIn order to explore this issue, we created theMeaning Modulation(MeMo) dataset,\nconsisting of 49 target words, each used in at least three different sentences in the same sense\nbut with subtle meaning modulations. For each target we manually chose an associated word\nthat reflects the meaning of the target in the sentence. For example, consider the target\nword happy (see Table 2), defined asEmotion experienced when in a state of well-beingin\nWordNet 3.0 or asEmotional state characterized by feelings of enjoyment, pleasure, and\nsatisfaction. in Wikipedia. MeMo contains three sentences featuringhappy as target word,\neach matched with a context-appropriate associated word (see 2). The associated words of\nthe other sentences containing the same target are taken as context-inappropriate associates.\nThe dataset contains nouns, verbs, and adjectives as both target and associated words. We\ngenerated items based on our own intuitions and finalised the associated words by consensus\namong all five authors. The sample size was determined based on our own intuition as we\ndid not have prior evidence on which to base a power analysis, nor we had a precise\nexpectation of the smallest effect of interest nor of the variance-covariance matrix involving\nrandom effects (Lakens, 2022). With 49 target words, a minimum of three associates per\ntarget word, and considering the 14 layers in BERT base, there are in total 7,140\nobservations in the MeMo dataset, with 510 unique combinations of target word, associate,\nand sentence. 2,184 observations pertain to appropriate associates (156 clusters, with 14\nobservations each, one per layer) and 4,956 observations pertain to inappropriate associates\n(354 clusters, with 14 observations each, one per layer). We performed simulations to get a\nsense of how robust the effect is and whether we find evidence the current dataset may lead\nus to report an effect which is not there. These simulations are reported in Appendix MeMo\ndataset reliability analysis.\nOur hypothesis is that the contextualised representation of the target word derived\nfrom a sentence will shift towards the prototype representation of the appropriate associated\nword, and further away from the inappropriate associated words. Importantly,all associated\nwords have some considerable degree of association with and similarity to the target word.\nMEANING MODULATIONS 67\nTarget Sentence Associate\nhappy My grandparents married in 1949 and had a\nhappy marriage.\nlasting\nhappy The owner was so happy to see his dog return\nhome after it had been missing for three days.\nrelieved\nhappy I’m always happy to discover a new little\nrestaurant.\nexcited\nTable 2\nExample items from the MeMo (Meaning Modulations) dataset. There are 49 target words in\nthe dataset, each used in at least three sentences and paired to an associated word.\nIn this, theMeMo dataset is particularly challenging. Unlike the previous two analyses, we\ngo beyond discrete senses in this experiment and continue to explore whether contextualised\nand prototype representations can be fruitfully compared and are shown to encode\nappropriate semantic relations.\nMethods\nWe derived the same representations outlined in Experiment 6: Priming in context when\ndescribing the modeling study of the stimuli from Till et al. (1988) and used them to\ncompute the same cosine similarity scores between the word in the sentence and the\nprototype representations of the associates, except that in this case the inappropriate\nassociates are more than one. We analyze the cosine similarities taking the same approach\noutlined in Experiment 5: Word Sense Disambiguation and Experiment 6: Priming in\ncontext and formulate a consistent hypothesis: We expect the similarity between\ncontextualised target word and appropriate associate to grow over layers, while the similarity\nbetween the contextualised target word and the inappropriate associates is expected to\ndecrease over layers (always once the anisomorphism of the underlying space has been\naccounted for). We thus expect a significant interaction between layer and condition.\nTurning to the ITS models, we again used the same two memories we used in the\nprevious experiments, one derived from the TASA corpus and one derived from the CoCA.\nThe target measure in the current experiment is the cosine similarity between the echoes of\nthe sentence (including the target word whose meaning modulations we aim to capture) and\nthe associate word, with the usual two conditions: appropriate - when the associate aligns\nwith the meaning modulation afforded by the sentence, and inappropriate - when the\nassociate does not align with the meaning modulation afforded by the sentence. As for both\nprevious datasets, we expect the cosine similarity between the echoes of sentences and\nassociates to be lower in the inappropriate condition, and tested this hypothesis by fitting a\nMEANING MODULATIONS 68\nlinear mixed model with the cosine similarity as dependent variable, condition (as a binary\nvariable) as the independent variable, and random intercepts for target words.\nResults\nWe do indeed observe a significant interaction between condition and layer for averaged\nBERT prototypes (∆AIC = −60.40 with respect to a simpler model using the linear\ncombination between layer and condition), but not for non-contextualised prototypes. We\nthus limit ourselves to the analysis of averaged prototypes (which, as already established\nacross previous studies, in general appear to be better suited to capture semantic relations\ninvolving prototypes). Visual inspection of the effect (Figure 14) shows that layer has a\npositive and rather linear effect on standardized cosine similarities between target word and\nappropriate associates (edf= 1.616, Ref.df= 2.004, F = 21.71, p<. 001) whereas it has an\nequally linear but negative effect for inappropriate associates (edf= 1.027, Ref.df= 1.054,\nF = 17.95, p<. 001).\nAs observed in the previous experiment, the difference between appropriate and\ninappropriate associates was not predicted to be significantly different with either of the ITS\nmodels (TASA:b= 0.0054, se= 0.0085, t= 0.638, p= 0.524; CoCA:b= 0.0425,\nse= 0.0661, t= 0.643, p= 0.52). Moreover, the similarity between sentences and\nappropriate associates was not different from 0, with a bimodal distribution that mirrors the\none described for the previous experiment.\nDiscussion\nThe pattern observed here thus reflects the one found when analysing stimulus materials\nfrom Till et al. (1988). However, beyond this previous observation, the present results show\nthat BERT representations shift in context following intuitions about associations in\nmeaning that are not simply confined to different related or unrelated senses, but also extend\nto meaning modulations within the same sense of a word. This opens the possibility to study\nmeaning differences in context to a more fine-grained level and better investigate how\ncontext affects meaning representations with respect to context-independent ones.\nIt is interesting to note that the effect size of layer shrunk forMeMo (and\ndisappeared altogether when considering non-contextualised prototypes): the difference\nbetween appropriate and inappropriate words to the target in context is arguably larger\nwhen the target is used in different senses (Cevoli, Watkins, Gao, et al., 2022), as opposed to\nwhen it is used in the same sense but with subtler meaning modulations (as is the case in\nMeMo). The instability of non-contextualised prototypes, especially at deeper layers, washes\nMEANING MODULATIONS 69\nout such a difference; however, the effect persists when deriving prototype embeddings by\naveraging representations.\nThe shape of the effect is also interesting, albeit only at a speculative level: while we\nsaw curves plateauing for both WiC (Pilehvar & Camacho-Collados, 2018) and the stimuli by\nTill et al. (1988) – suggesting that the very deep layers did not exert a further influence on\nthe target representations – we observe a linear effect forMeMo. This sits well with the\nfocus of the stimuli in this dataset on finer modulations in meaning that are virtually\nentirely dependent on context. Our results thus suggest that BERT can even capture such\nfine-grained modulations, and it does so more effectively the more contextualized its\nrepresentations are.\nFinally, the ITS models did not exhibit the expected behavior. The MeMo dataset\ncontains fairly long and complex probes, rather different than the ones used in the WiC\ndataset and in the study by Jamieson et al. (2018) but more in line with those used in the\nstimuli of Till et al. (1988). The ITS model as currently specified does not seem able to\nhandle the complexity of such probes, regardless of the presence of very frequent items like\nstop words.\nMeaning in Context: Summary\nThe last set of analyses, together with previous ones, satisfies all the desiderata we put\nforward at the beginning of this paper. Representations have a stable semantic core which\nenters meaningful relations with contextualised representations. Moreover, words cannot\nsimply be exchanged with one another while leaving it to the context to disambiguate the\nmeaning: replacing inappropriate synonyms in context results in lower similarity to a\npolysemous target. However, context does influence the representation of a word token,\nshifting it with respect to the static semantic space and other contextualised representations\naffected by the same context.\nImportantly, we showed that BERT’s representations capture meaning shifts both\nacross and within word senses, and do so at different places in the architectures.\nIn Experiment 5: Word Sense Disambiguation and Experiment 6: Priming in context, we see\nthat representations of appropriate and inappropriate associate words change steadily over\nthe first few layers but the difference stabilizes around layer 9, suggesting that differences\nacross sense boundaries are captured early. On the contrary, meaning modulations within a\nsame sense, targeted in Experiment 7: Meaning Modulations, are captured also at deeper\nlayers, where the influence of context is more pronounced (as expected). Recent work by\nTrott and Bergen (2023) highlights how humans are influenced by differences in discrete\nword sense as well as by continuous meaning variations, and also that BERT fails to capture\nMEANING MODULATIONS 70\nthis tension. Our evidence suggests that the model might, provided that one considers how\nrepresentations shift throughout the architecture’s latent space and how context is\nintegrated, althouth a thorough empirical evaluation is necessary to assess this hypothesis.\nNext to showing that BERT embeddings reflect lexical ambiguity along discrete\nsenses, we provide the first piece of evidence that the model can account for subtler\nmodulations in meaning, operationalised as context-dependent associations of unambiguous\nwords used in different contexts. The MeMo dataset offers a first test bed for probing models\non their ability to reflect the intuition that meaning can change within the boundaries of a\nsame conventional word sense, slightly but consistently andpredictably altering the pattern\nof associations the word elicits in a specific context as opposed to a different context or in\nisolation. More than anything, this set of studies further characterizes meaning as\nparticularly fluid and shows that a model such as BERT can capture this fluidity at various\nscales and considering both exemplars and prototypes (Murphy, 2002b).\nFinally, while we cannot exclude that an optimized version of the ITS model would fit\nthe data well, we do contend that the problem lies in the weighting scheme put forward in\nthis model rather than in a lack of optimization - also considering that we evaluated the very\nsame model which was shown to have empirical success on context-dependent meaning\nvariations across sense boundaries. Since echoes result from the weighted sum of all memory\ntraces and since the weighting depends on the similarity between each item in the probe and\nthe memory trace, the lack or presence of similarity between any item in the probe and the\ntrace can make the resulting echo rather brittle: It is unlikely that in a complex enough\nsentence all word types contribute to the meaning of the probe in the same way by virtue of\ncomparable similarities with all traces. On the contrary, a model that implements a learning\nstrategy grounded in context-dependent prediction and updates based on errors, such as\ntransformer models, appears better equipped to downplay frequent yet uninformative\nelements than a weighted analogy function based on similarity over recordings of experience.\nFurthermore, the attention mechanism implemented by BERT and similar models aims\nprecisely to learn which other words in the context matter most to shift the representation of\na certain lexical element. Finally, as already pointed out in the Introduction, the ITS model\ninherently collapses the meaning of a word in context with the meaning of the context, a\nconfusion that obfuscates the contribution of individual words to the context and prevents\nan analysis of how each word interacts with its linguistic context.\nGeneral Discussion\nIn this work we analyzed to what extent the semantic representations learned by recent\ntransformer-based language models, here exemplified by BERT, can offer a useful tool to\nMEANING MODULATIONS 71\ninvestigate psycholinguistic effects pertaining to semantics. Crucially, we used the same\nmodel to derive context-dependent and context-independent representations to investigate\nthe tension between exemplar-based (implemented in our model as the token-level\nrepresentations) and prototype-based (implemented as type-level static representations)\naccounts of semantic representation (Murphy, 2002b). First, we established that the\nprototype embeddings derived from BERT capture behavioral patterns (semantic similarity\nand relatedness, Bruni, Tran, et al., 2014; Hill et al., 2015; semantic richness effects in lexical\ndecision, Yap et al., 2011; and prime-target similarity in primed lexical decision, Hutchison,\nBalota, Neely, Cortese, Cohen-Shikora, Tse, et al., 2013; Koriat, 1981), attesting to their\nviability as implementations of a semantic memory model. We then showed that the\ncontextualized embeddings learned by BERT have a stable semantic core, and that the effect\nof the sentence context on this representation interacts with said semantic core: Replacing\nwords in context yields representations of the replacements that shift closer to the replaced\nword (as evidenced by the similarity of the lexical representations as well as of their\nneighborhoods). Moreover, this shift is modulated by the similarity of the static\nrepresentations in a psychological model of semantics (De Deyne et al., 2019). Finally, we\nshow that contextualisation follows meaning variations at the level of discrete senses (Cevoli,\nWatkins, Gao, et al., 2022; Li & Joanisse, 2021; Rodd, 2020), but also captures subtler yet\nsystematic variations pertaining to words used in the same sense in different contexts. The\nsimulations we report thus allow us to conclude that BERT embeddings meet our desiderata\nfor psychologically valid semantic representations: words do have a semantic core, which\ncaptures behavioral patterns observed for isolated words; they cannot be exchanged randomly\nin context, but carry a distinctive footprint which originates in their co-occurrence patterns;\nand they are sensitive to context, yielding meaning variations which exist on a continuum.\nOur study highlights several psycholinguistically relevant properties of these\nembeddings, which are discussed in more detail in what follows. First, we showed that\nexplicit and implicit semantic effects are best captured at intermediate layers especially when\nderiving a word’s prototype representation by averaging a handful of contextualised\nrepresentations for that word. These averaged prototypes provide a consistently better fit\nthan the de-contextualized prototype representations BERT learns and stores, highlighting\nhow token-level representations can complement type-level ones and in relation with specific\naspects of the model architecture. This extrapolation of prototypes from exemplar\nrepresentations is in line with the fundamentals of psychological prototype models (Rosch,\n1978) as well as recent computational work on exemplar and prototype representations\n(Battleday et al., 2020; Günther et al., 2023). Moreover, we observed that the deepest layers\nstruggle with dealing with isolated words and that the very last layer is biased by context\nMEANING MODULATIONS 72\nover a word’s semantic core, whereas intermediate layers capture both prototype effects and\ninteractions with context-dependent representations more reliably. This highlights structural\nproperties of these models and the representations they learn that should inform as well as\nconstrain their application to study psycholinguistic effects. Finally, our study is the first to\nshow how meaning variation systematically affects individual occurrences of each word\nwhether they encode different senses or not, and that these graded difference is captured by\nthe architecture (which, as we discussed before, builds upon but significantly extends\npsychological semantic representation models such as by Rodd, 2020). Whereas differences\nbetween tokens of different word senses tend to diverge already at superficial layers,\nreflecting a strong interaction between context and semantic core, differences between tokens\nof a same sense keep growing until the very last layers, underlying how crucial context is in\nmodulating meaning variations in the face of meaning stability. Contextualised embeddings\ncan thus be fruitfully used to study these modulations further and how we encode and\nprocess them, contributing to the debate on exemplar vs. prototype accounts of semantics by\noffering a possible balance point: a system that abstracts what can be abstracted from the\nenvironment because of its invariance and learns a model of how context can affect these\nrepresentations so that they can be used flexibly and productively to capture the endless\nvariability of experience.\nImplications of the model architecture\nWhen evaluating the BERT representation system from a cognitive perspective, it is useful\nto go back to its architecture, especially the role of transformer encoders and of the learning\nobjective. BERT (Devlin et al., 2019a) is trained to predict masked words from context as\nwell as to predict the next sentence. Hence, the representations it develops support an\naccurate execution of these tasks. In particular, the learning objective of word prediction is\ntaken to occupy an important role in language processing (Federmeier, 2007; Huettig, 2015;\nWillems et al., 2016) and is conceptually similar to that of countless other DSMs\n(Bojanowski et al., 2017; Li and Joanisse, 2021; Mandera et al., 2017; Mikolov, Chen, et al.,\n2013; see Lenci et al., 2022). While the focus on prediction in language has traditionally\nbeen on comprehension and processing (Frank & Willems, 2017; Monsalve et al., 2012;\nWillems et al., 2016), recent studies have started to consider the role of prediction in learning\n(Fazekas et al., 2020; Vujović et al., 2021), suggesting that it provides a productive\nexplanation of behavior. BERT, and similar models, offer a possible account of how learning\nand processing are inherently linked, with representations being learned and updated as a\nresult of prediction in context.\nMEANING MODULATIONS 73\nAt a fundamental level, the model will develop similar representations when it needs\nto output the same prediction, that is the same token. However, because of ambiguity, many\ntokens in the sentence context will end up supporting very different predictions: observing\nthe wordball may nudge the model to predictbat but alsogown. However, the co-occurrence\npatterns across words are also predictable and the model can learn that, given a certain\nsentence context, it should prioritize certain words over others because their co-presence is\ninformative with respect to a certain prediction instead of another. For example, the\npresence of the wordstriker together withball would call for predictingbat and notgown.\nThis is what attention heads in transformer encoders do: they are trained to attend to\ncontext to improve prediction in context. Therefore, when a word is replaced in context, the\nreplacement will be represented slightly closer to the intended target than it would be in a\nsemantically inappropriate context: The model has learned to attend to context and that a\ncertain context calls for a certain prediction, which however has to be combined with what is\neventually observed. If the context grows, as in the story of the amorous peanut (Nieuwland\n& Van Berkum, 2006), the evidence will push the model to substantially alter the\nrepresentation, following what it learned about the co-occurrence of context and target\nwords. This does not rule out the existence of static, context-independent representations.\nRather it highlights how such context-independent representations constrain and influence\nthe accommodation of semantic violations. In the limit, if no co-occurring word has any\ncontext-independent representations, then context can mean anything and becomes\nunconstrained. The peanut can bein lovebecause the co-occurring words have a certain\nmeaning, which does not fit withpeanuts but is coherent and informative enough to warp the\nrepresentation of the target word and derive a representation that fits in context. Our\nanalyses predict that this process can be easier or harder depending on the similarity of\ncontext-independent representations of target and context.\nThese properties of the BERT representation system provide important insights\nabout how human speakers can acquire and develop useful semantic representations that are\nboth sufficiently stable and sufficiently flexible to be used in actual communication\n(Pickering & Garrod, 2004). In cognitive science, “standard” distributional semantic models\nhave been established as plausible and high-performing learning models for semantic\nrepresentations (Günther et al., 2019; Hollis, 2017; Mandera et al., 2017), building on the\nfundamental assumption that humans utilize statistical regularities – that is, context\ninformation – in their environment to assign meaning to stimuli (Griffiths et al., 2007; Lenci,\n2008). The results of the present study further extend this without changing this underlying\nassumption (Lenci et al., 2022): After all, BERT also derives its representations by\nconsidering context information, it just incorporates a wider notion of context (crossing the\nMEANING MODULATIONS 74\narbitrary sentence boundary incorporated by many previous models) and more elaborate\nlearning techniques. Thus, the model and results of the present study provide a proof of\nprinciple of how humans can acquire context-sensitive semantic representations by learning\nfrom the statistical patterns in their linguistic experience.\nContextualised representations in psycholinguistics research\nOur results on context-dependent meaning patterns highlight that BERT provides an\neffective implementation of a model of semantics which is aware of the linguistic context in\nwhich a word occurs (Cevoli, Watkins, Gao, et al., 2022; Li & Joanisse, 2021; Rodd, 2020).\nThe layered architecture also offers the possibility to study how context is integrated in the\nlexical representation of a word, and whether this speaks to how humans achieve the same\nbroad goal. This is an important difference with respect to the implementation by Li and\nJoanisse (2021) which relied on an LSTM network. There, one could consider the static\nrepresentation of the word on the one hand, and the representation the model develops\nconsidering the preceding context and the word itself on the other hand. In this sense, it is\nunclear whether the context-dependent representation really encodes the context-dependent\nmeaning of the last word (i.e., the word in question) or rather the meaning of the sentence\nup to that point. The model we consider, on the contrary, allows to more effectively tease\nthe two apart. Moreover, unlike the model by Jamieson et al. (2018), which relies on static\nrepresentations (encoded using random vectors matched to each word type) and sees context\neffect arising at the level of the full word combination rather than the individual word,\nBERT embeddings actually shift in semantic space by virtue of the influence that the\nsentence context exerts on a learned semantic representations which captures isolated word\neffects - a feature that random vectors would not have by design.\nBERT’s semantic representations are more reminiscent of Rodd (2020)’s proposal,\nalthough the underlying model does not foresee explicit attractors. Nonetheless, our study\non replacements, especially considering neighborhood similarity, hints to the fact that such\nattractors are an emergent phenomenon in BERT’s semantic space, even if no explicit role\nfor them is hard-coded into the model. We saw that contextual representations are dragged\ntowards or away from coherent semantic neighborhoods which reflect stable configurations of\nthe underlying semantic space. Thus, attractors may indeed signal stable states of the model,\nwhich correspond to representations that come to occupy coherent positions in the resulting\nembedding space. However, these attractors are not discrete nor are they all-or-nothing:\nrepresentations can, and do, exist outside of these, highlighting how meaning variations are\nbest construed as continuous.\nMEANING MODULATIONS 75\nMeaning ambiguity beyond word senses\nFurther challenging the notion of polysemy as an explanatory construct, we showed that\nBERT is also able to encode differences in word meaning that do not pertain to different\nsenses (Haber & Poesio, 2024). BERT embeddings themselves offer evidence that meaning\nmay be systematically altered by context beyond discrete senses. This further calls into\nquestions the traditional categorical distinction between homonyms, polysemous, and\nunambiguous words: we observe systematic meaning modulations also for words supposedly\nused in the same sense. The continuous, distributed, context-dependent representations that\nmodels such as BERT offer thus provide a new tool to evaluate semantic aspects in\npsycholinguistic tasks, which moves away both from the rigid partitioning of thesauri and\nfrom the monolithic representations of static DSMs towards a continuous and graded account\nof the meanings related to a word.\nThis study further qualifies extant models of how people deal with ambiguity in\nnatural language, which have focused primarily on modeling discrete senses. Rather than\naiming to encode or represent different senses, BERT’s ability to develop representations\nthat reflect lexical ambiguity (Apidianaki, 2023; Cevoli, Watkins, Gao, et al., 2022; Haber &\nPoesio, 2024) does not stem from the explicit goal of keeping track of ambiguity, but rather\nof optimising prediction (Kveraga et al., 2007; N. J. Smith & Levy, 2013). That we can\nrecover clusters of representations which align with word senses is a by-product of what the\nmodel learns to do: predicting masked words while exploiting linguistic context to do this\nbetter. It does not rely on explicit attractors (contrary to Rodd, 2020’s account), but rather\nachieves this by implementing the basic principle that no two representations are alike unless\ntheir contexts of occurrence match entirely. All the model learns are token-level\nrepresentations: we argue that what we refer to aspolysemy is a simplified description of the\nfact that these token-level representations cluster more or less depending on context (Cevoli,\nWatkins, Gao, et al., 2022).\nHowever, recent research by Trott and Bergen (2023) highlights how people are\nsensitive to sense boundaries in a way which a model like BERT fails to entirely capture,\ndescribing semantic representations as both discrete (with marked differences along word\nsense boundaries) and continuous (since graded differences in similarity explain more\nvariance than discrete senses). We used the same architecture and focused our attention on\ngraded relations bothacross and within word senses, and our results suggest that BERT\nmight handle meaning variations that cross or do not cross sense boundaries in different\nplaces of its architecture. In detail, we noticed that two different senses of a homonym do\nnot change throughout the last layers, with the model shifting the representation enough\nover the first layers. On the contrary, meaning modulations within the same sense are\nMEANING MODULATIONS 76\nreflected in subtler but continuous changes in the underlying distributed representation until\nthe very last layer. It might therefore be useful to consider how representation unfolds\nthroughout the architecture to better grasp the discrete aspects of word meaning in this\nmodel, next to the graded semantic relations we have focused on in our experiments.\nToken-level, exemplar, and prototype representations\nThe ability to capture fine-grained ambiguities would seem to put BERT closer to exemplar\nthan prototype accounts of semantic representation (Ambridge, 2020; Murphy, 2002a), since\neach token is encoded differently. However, unlike exemplar-based accounts, BERT does not\nstore each representation. To the contrary, it actively learns a static representation – the\nembedding at layer 0 – before positional information is integrated. Crucially, the reliability\nof these representations evidenced throughout our experiments shows that BERT\nrepresentations encodemeaning dominance(Rodd, 2020, and references therein), the\nobservation that when presented in isolation, people interpret words according to their most\nfrequent meaning. The relation between contextualised and static representations in the\nreplacement experiment, as well as the patterns observed when modeling Till et al. (1988)’s\nmaterial and the MeMo dataset, confirm that prototype representations are sensitive to these\nfrequency patterns in natural language and show that BERT prototype do capture\nmeaningful invariances in language experience, since we find stable relations that mirror\nthose found in a psychological model of meaning grounded in free associates (De Deyne\net al., 2019). Next to this, however, BERT also encodes a model of how context interacts\nwith these static representations, relying on the many systematic patterns in language to\nbuild a model of context influence that operates on stored abstract prototype representations\nto yield ephemeral token-level representations. We contend that BERT implements a more\nparsimonious account than storage-based exemplar models and a more flexible one than\nprototype models, which strikes a productive balance between capturing the invariance in\nexperience together with its idiosyncracies. Crucially, it achieves this bylearning an abstract\nmodel of how the latter affects the former to yield infinitely varied yet systematic\nrepresentations.\nAn important comparison in our study involves the way in which prototype\nrepresentations (i.e., static embeddings) should be derived (Apidianaki, 2023). Next to\nassessing the viability of embeddings at layer 0 before including positional encoding, we\ntested two methods which have been widely used in the literature (Bommasani et al., 2020;\nLenci et al., 2022): on the one hand, we derived prototype representations by averaging\ncontextual representations extracted from a sample of sentences featuring the target word\n(averaged); on the other, we fed the target word outside of any sentence context to the model\nMEANING MODULATIONS 77\n(non-contextual). While both approaches show some effectiveness, the former is\nunequivocally better in that it explains more variance when modeling LD tasks, it affords\nhigher and more stable correlations with semantic similarity and relatedness norms, it\ncaptures context-dependent effects in the simulations performed on the MeMo dataset and\non the stimuli from Till et al. (1988), and better preserves semantic relations in the\nreplacement study. non-contextualised prototypes offer a particularly poor performance at\ndeeper layers, where the influence of context on the resulting representation is highest. We\nhypothesize that this failure follows from the substantial difference between isolated words\nand the language on which BERT is trained, consisting of full passages. When the model\ncannot rely on any linguistic context, the relations it learned between lexical representations\nand context become useless, resulting in unreliable representations.\nThis observation poses a challenge to our claim that BERT provides a useful account\nof semantic memory, as the dynamic update of representations based on context fails when\ncontext is absent, unlike the behavior exhibited in psycholinguistic experiments for decades\nby participants presented with isolated words. Moreover, BERT shows much stronger\nempirical fit when considering exemplars and using them to compute new prototype\nrepresentations that are not learned by the system, and which are not an inherent part of\nthe model architecture but rather a post-hoc addition. Does this mean that the only way for\nBERT to provide adequate accounts of relevant psycholinguistic tasks depends on the\nstorage of token-level representations (which are required to compute these type-level\nprototype representations) after all? We believe the answer is no: such token-level,\ncontext-dependent representations need not be stored, but they can be the result of\nsimulations triggered when encountering a word. In line with what has been proposed in the\nembodied cognition literature (Barsalou, 1999; Dove, 2014; Fischer & Zwaan, 2008; Glenberg\n& Kaschak, 2002), conceptual understanding is mediated by simulations of the interactions\nwith the concept (via perception, proprioception, motor interactions and more). We suggest\nthat context-dependent lexical representations arise due to simulations which also involve the\nlinguistic contexts in which a word is encountered18. Much like embodied simulations\ncapitalize on interactions with a given object, we propose that conceptual understanding\nsimilarly leverages encounters with a word in linguistic contexts. The situational context in\nwhich a word has been experienced, even in the lack of explicit linguistic context in the here\nand now, will thus interact with the abstract semantic representation to yield different\nsimulations of the word embedded in context, which will yield the context-dependent,\n18 We don’t make any claim here with respect to the role of embodied simulations in conceptual\nunderstanding: By using BERT representations we focus purely on the linguistic side of conceptual\nunderstanding, which may exist next to embodied simulations.\nMEANING MODULATIONS 78\ntoken-level representations we used to derive average prototypes, in line with prepositional\nentropy effects on lexical decision latencies uncovered in Baayen et al. (2011). The fact that\nthese representations work better than the type-level, abstract, context-independent\nprototype representations BERT learns suggests that this process of simulation which\nconsiders linguistic interactions with a word fosters richer, more informative representations\nand underlines the importance of learning a model of the interaction between invariance in\nexperience and context rather than only the former. Why, then, does presenting isolated\nwords yield worse results? And what does this tell us about BERT as a model of semantic\nmemory, if we accept the analogy between isolated word tasks and showing a single word to\nthe model?\nIt is useful to keep in mind that BERT’s output layer relies on the deepest\ntransformer encoder, hence it depends on the representation which results after a token has\ntraversed all layers. The output the model provides at the deepest layer when presented with\nan isolated word, as we have shown, is often inappropriate and, especially when considering\nsemantic similarity and relatedness, indicates a rather uninformative semantic space.\nHowever, it is unclear how the model could support an output based on intermediate layers,\nwhere sensible semantic representations have been observed across tasks also for\nnon-contextualised embeddings. All these remarks call into question the straightforward and\nliteral adoption of BERT as a fully-fledged model of semantic memory at the algorithmic\nlevel of description; rather, we argue that it should be seen as a model at the computational\nlevel of (Marr, 1982): It provides an account ofwhat the target system is doing, but not\nnecessarily an exact account ofhow it is doing it in a mechanistic, step-by-step way. The\nemphasis, we contend, is on the importance of integrating invariance in experience (resulting\nin useful type-level representations) with an abstract model of how variability in experience\ninteracts with invariance (resulting in token-level representations).\nBERT’s objective function is rather narrow, aiming to optimize prediction of masked\nwords and next sentences. This places a large emphasis on the role of a broad linguistic\ncontext, which differs from many language experiences humans have, where linguistic context\nmay not be as extensive (but situational context is arguably far richer). We hypothesize that\nthis strong focus on broad linguistic context leads the model to develop unreliable and\nunstable semantic representations at deeper layers in cases when context is absent. While\nprediction certainly plays a role in language comprehension (Willems et al., 2016) and\nacquisition (Fazekas et al., 2020; Vujović et al., 2021), the specific implementation BERT\nuses is limited and appears to be a prime difference between the model and the processes\nhumans rely upon when deriving semantic representations. More broadly, prediction has\nbeen called into question as the single mechanism supporting language comprehension\nMEANING MODULATIONS 79\n(Huettig, 2015; Huettig & Mani, 2016). And even within the framework of predictive\nprocessing, prediction of upcoming lexical material may be just a part of the puzzle: recent\nwork which relies on the Sentence Gestalt model suggests that a model trained to minimize\nprediction errors not only for upcoming lexical items but also for semantic roles offers a\nbetter account of the N400 (Lopopolo & Rabovsky, 2021), calling for more studies on the\ncognitive plausibility of training objectives and the introduction of new ones which may\nbetter reflect how humans learn.\nWe take these observations on the specific structure of BERT (different layers,\nunclear status of output based on intermediate representations) and its training objective\n(masked word prediction) as a cautionary tale against drawing strict equivalences between\nBERT layers and language processing steps, and against conceptualising BERT as an\nalgorithmic account of semantic memory (Marr, 1982). On the contrary, we believe BERT -\nand the architecture it relies upon - can be a fruitful computational characterization of\nsemantic memory, with a particular focus on how context-independent abstractions and\ncontext-dependent representations interact with each other through a model of how\nexperience consists of invariant patterns embedded in an ever changing flux, which influences\nsuch invariant patterns and at the same time is constrained by them.\nThe emerging picture is thus that of a highly dynamic system, consisting of a set of\nstatic representations which are learned from exposure and constrain the malleability of the\nsemantic space itself. Representations are flexible and can shift, but cannot be entirely\nsuperseded by the limited context used in our experiments (typically a single sentence). This\nlines up with behavioral evidence from Nieuwland and Van Berkum (2006) suggesting that\nmany salient semantic features can be overwritten provided sufficient context, to the point\nthat after having described a peanut with anthropomorphic traits, a sentence such asThe\npeanut was in loveelicits no semantic violation, whereas a sentence such asThe peanut was\nsalted does, in spite of the second being a prototypical feature of peanuts as we know them.\nCevoli, Watkins, Gao, et al. (2022) provide exploratory evidence that when exposed to the\nsame context participants read in Nieuwland and Van Berkum (2006)’s experiment, BERT\nalso predicts verbs which would normally fit a person rather than a peanut, suggesting that\nthe underlying semantic representation has shifted. In addition, a recent study by Michaelov,\nCoulson, et al. (2023) on the same item material shows that transformer-based language\nmodels such as BERT can well account for the effects observed by Nieuwland and\nVan Berkum (2006). Whereas Nieuwland and Van Berkum (2006) took their evidence as\nruling out a stable semantic core, the success of BERT and similar language models in\naccounting for their data (Cevoli, Watkins, Gao, et al., 2022; Michaelov, Coulson, et al.,\n2023), as well as our own evidence (pointing to a different ease with which representations\nMEANING MODULATIONS 80\ncan be morphed to fit a certain context), suggest that the lack of a semantic violation when\nreading of an amorous peanut can be explained by the tight interplay between context and\nlexical representations (of which context also consists of, evidently). We contend that people\ndo have a stable representation for the wordpeanut and also for the other words in the\nsentences used in the experiment: the interaction between them and the knowledge of how\ncontext operates on lexical representations allow people to form a different representation of\nthe target word, which evokes certain semantic features and downplays others.\nLimitations\nDespite the encouraging evidence we provide about BERT offering a useful model of\nsemantic memory which allows the study of isolated word effects, context-dependent effects,\nand their interaction within a single framework, there are a few caveats to keep in mind.\nFirst of all, BERT is a purely linguistic model, in the sense that it learns from language\npatterns alone (which has received considerable criticism from embodiment research; see the\nseminal article by Glenberg and Robertson, 2000, and C. R. Jones et al., 2022, for a more\nrecent version of the same argument also considering BERT). While this is certainly a\nlimitation, it is again not inherent to the class of models of which it is an instance (see\nGünther et al., 2019), which have already started to be enriched with perceptual information.\nFor example, the CLIP (Contrastive Language-Image Pretraining) model (Radford et al.,\n2021) uses the same architecture as BERT (Vaswani et al., 2017) but jointly learns linguistic\nand visual representations from images and corresponding captions (C. Jones et al., 2024;\nPezzelle et al., 2021). A recent stream of studies have started comparing purely linguistic\nrepresentations with multi-modal representations, observing a general advantage for the\nlatter ones in a variety of settings relating to language processing, especially for concrete\nwords (Pezzelle et al., 2021; Tikhonov et al., 2023).\nMoreover, while we considered a variety of tasks relating to words in isolation and in\ncontext, other paradigms in psycholinguistic research could enrich our understanding of the\nextent to which contextualised embeddings learned by LLMs are a useful account of semantic\nrepresentations. We investigated semantic violations in terms of replacements rather than\ncontrolled semantic violations, known to elicit distinctive ERP components such as the N400\n(Dudschig et al., 2016; Hagoort et al., 2004; Kutas & Federmeier, 2011), which could offer a\nmore stringent test. Our study suggests that BERT succeeds at integrating lexical\nrepresentations which are not too different from the original word found in contexts, whereas\nit struggles with more dissimilar representations: The representation of a replacement shifts\ntowards the neighborhood of the target, but never quite gets there. On the psychological\nlevel, this discrepancy could well result in processing costs or difficulties in integrating\nMEANING MODULATIONS 81\ncertain concepts in the current sentence representation (see Brouwer et al., 2017, and\nreferences therein). In line with this intuition, we observed that replacements’ surprisal is\npositively correlated with the dissimilarity between replacement and original target (see\nAppendix Word probability in context). This also influences the similarity between a\ncontextualised embedding at the deepest layer and the static embedding for the same word\ntype at layer 0: the less related the replacement is to the original word, the more dissimilar\nthe contextualised representation is from the static one. These costs (high surprisal and\nmore difficult contextualisation) for unrelated words tie in with evidence on N400 and P600\neffects (Hagoort et al., 1993; Kutas & Hillyard, 1980). Initial explorations suggest Language\nModels relying on contextualised embeddings hold promise in accounting for human\nprocessing data on semantic violations (Michaelov, Bardolph, et al., 2023), with\nmodel-derived measures being better predictors for N400s than human predictability\njudgments (Michaelov et al., 2022). Michaelov, Bardolph, et al. (2023) compared measures of\npredictability and of semantic relatedness in accounting for processing data elicited by\nsentences which did or did not present a semantic violation known to elicit an N400\ncomponent, finding that predictability explains more variance than contextual similarity.\nHowever, the former was operationalized with fully static embeddings: our results show that\ncontextualised embeddings capture more fine-grained semantic effects, especially in context,\nand should thus be preferred for these analyses.\nConclusion\nTo sum up, we analysed BERT representations in a variety of settings, considering both\ncontextualised, token-level and prototype, type-level representations in tasks which rely on\nisolated words and tasks which require the model to capture the interaction with context.\nWe identified two main desiderata for psychologically valid semantic representations: i) that\nwords have a semantic core which cannot be entirely overwritten by context and reflects how\nthe word type is interpreted in isolation and ii) that context co-determines a word’s\nrepresentation when embedded in a sentence. Our analyses allow us to answer positively\nabout BERT representations’ validity and usefulness to study semantic effects in\npsycholinguistics. They do capture a stable semantic core which accounts for isolated word\neffects. Moreover, the word meaning can shift from this core following predictable and\nsystematic context effects. We highlight that static and context-dependent representations\nco-exist in the same embedding space: in this, BERT offers a viable single implementation of\nboth, and holds promise for analysing the interplay between prototypes and exemplars in a\nlearning model (Buijtelaar & Pezzelle, 2023; Madabushi et al., 2022).\nMEANING MODULATIONS 82\nAcknowledgments\nGC thanks Bruno Nicenboim for useful discussions on the scope of this paper. FG is\nsupported by the German Research Foundation (DFG), Emmy-Noether grant “What’s in a\nname?” (project number 459717703). FB is supported by the Hoffman–Yee Research Grants\nProgram and the Stanford Institute for Human-Centered Artificial Intelligence. GA is\npartially supported by funding from the European Research Council (ERC) under the\nEuropean Union’s Horizon 2020 research and innovation program (No. 949944,\nINTEGRATOR), and by Fondazione Cariplo (grant No. 2020-4288, MONICA).\nMEANING MODULATIONS 83\n(a) SimLex\n0.25\n0.50\n0.75\n1.00\n0 4 8 12\nBERT layer\nSpearman correlation\nVars\nSimLex w2v\navgs w2v\nSimLex avgs\nnon.contextualised w2v\nSimLex non.contextualised\nnon.contextualised avgs\nSimLex dataset\n(b) MEN\n0.2\n0.4\n0.6\n0.8\n1.0\n0 4 8 12\nBERT layer\nSpearman correlation\nVars\nMEN w2v\navgs w2v\nMEN avgs\nnon.contextualised w2v\nMEN non.contextualised\nnon.contextualised avgs\nMEN dataset\nFigure 3\nCorrelations with semantic similarity and relatedness norms: SimLex (a) and MEN (b). We\nalso plot the correlations between model-based cosine similarities, computed over the same\nword pairs. Correlations involving behavioral ratings are in shades of blue, correlations\ninvolving model-based cosine similarities are in shades of red.\nMEANING MODULATIONS 84\n(a) Pearson’s r\nlong short\n0 4 8 12 0 4 8 12\n−0.20\n−0.15\n−0.10\n−0.05\nBERT layer\nr\nd averaged non−contextualised\nPearson's r between cos(prime,target) and SPP logRTs\n(b) ∆AIC\nlong short\n0 4 8 12 0 4 8 12\n−300\n−200\n−100\nBERT layer\ndeltaAIC\nd averaged non−contextualised\nDelta AIC when adding cos(prime,target) to GAM predicting SPP RTs\nFigure 4\nPrime-target cosine similarity computed from BERT embeddings and its effect on log\nReaction Times (RTs) in the Semantic Priming Project (SPP) dataset. (a): Pearson’s\ncorrelation between log RTs and BERTcos(prime,target); (b): ∆AIC of the model\nincorporating BERTcos(prime,target) with respect to the baseline multiple regression only\nincluding covariates. The x axis in each plot shows the BERT layers, and the color legend\nindicates the prototyping approach (red: averaged; cyan: non-contextualised). Gray lines\nindicate the score of the referenceword2vec model on each metric. Models were fitted\nseparately for short and long Stimulus Onset Asynchrony (SOA), plotted as facets.\nMEANING MODULATIONS 85\n(a) t statistic\n−10\n−5\n0\n5\n0 4 8 12\nBERT layer\nt\nPrototypes averaged non−contextualised\n (b) ∆AIC\n−100\n−50\n0\n0 4 8 12\nBERT layer\ndelta AIC\nPrototypes averaged non−contextualised\n \nFigure 5\nSemantic Neighborhood Density computed from BERT embeddings at different layers and its\neffect on Reaction Times (RTs) in the English Lexicon Project (ELP): (a): t statistic of SND\nas a predictor in a multiple regression model predicting RTs while controlling for other\ncovariates; (b)∆AIC of the model incorporating SND computed from BERT embeddings with\nrespect to the baseline multiple regression only including covariates. The color legend\nindicates the prototyping approach (cyan: non-contextualised; red: averaged). Horizontal\nblack lines indicate the score of the referenceword2vec model on each metric.\nMEANING MODULATIONS 86\n0.0 0.2 0.4 0.6 0.8 1.0\n−2 −1 0 1\nAveraged prototypes\nSWOW cos(target, repl)\nBERT cos(target_p, repl_p)\n0.0 0.2 0.4 0.6 0.8 1.0\n−2 −1 0 1\nNon−contextual prototypes\nSWOW cos(target, repl)\n layer\n−1\n3\n6\n9\n12\nFigure 6\nPredicted cosine similarity of prototype representations of target (target_p) and replacement\n(repl_p) in BERT (y-axis, fitted values) as a function of the similarity of the two words in a\nsimilarity space derived from the English Small World of Words (SWoW) associations using\na random walk algorithm (x-axis, SWOWcos(target,repl)). Left: averaged prototypes; right:\nnon-contextualised prototypes. Similarities in BERT are Box-Cox transformed and then\nz-standardised. The color and line legends identify BERT’s layer. Both predictors are\nnumerical and included in the model as simple smooths as well as in a partial tensor product.\nMEANING MODULATIONS 87\n0.0 0.2 0.4 0.6 0.8 1.0\n−2 −1 0 1\n \nSWOW cos(target, repl)\nBERT cos(target_c, repl_c)\nlayer\n−1\n3\n6\n9\n12\nFigure 7\nPredicted cosine similarity of contextualised representations of target (target_c) and\nreplacement (repl_c) in BERT (y-axis, fitted values) as a function of the similarity of the\ntwo words in a similarity space derived from the English Small World of Words (SWoW)\nassociations using a random walk algorithm (x-axis, SWOWcos(target,repl)). Similarities\nin BERT are box-cox transformed and then z-standardised. The color and line legends\nidentify BERT’s layer. Both predictors are numerical and included in the model as simple\nsmooths as well as in a partial tensor product.\nMEANING MODULATIONS 88\n0.0 0.2 0.4 0.6 0.8 1.0\n−2 −1 0 1\nAveraged prototypes\nSWOW cos(target, repl)\nBERT cos(target_p, repl_c)\n0.0 0.2 0.4 0.6 0.8 1.0\n−2 −1 0 1\nNon−contextual prototypes\nSWOW cos(target, repl)\n layer\n−1\n3\n6\n9\n12\nFigure 8\nPredicted cosine similarity between target prototype (target_p) and contextualised\nreplacement (repl_c). Left: averaged prototypes; right: non-contextualised prototypes. BERT\ncosine similarities (BERTcos(·,·) have been box-cox-transformed and z-standardised. Lines\nshow the predicted BERT cosine similarity between target and replacement as estimated by a\nGAM fitted with the similarity between target and replacement in the similarity space derived\nfrom the English Small World of Words (SWoW) free association data and BERT’s layer as\npredictors (included as simple smooths and in a partial tensor product). Line and color\nlegends identify the layers.\nMEANING MODULATIONS 89\n0.0 0.2 0.4 0.6 0.8\n−1.0 −0.5 0.0 0.5 1.0\nAveraged prototypes\nSWOW cos(target, repl)\nBERT cos(repl_c, repl_p)\nlayer\n−1\n3\n6\n9\n12\n0.0 0.2 0.4 0.6 0.8\n−1.0 −0.5 0.0 0.5 1.0\nNon−contextual prototypes\nSWOW cos(target, repl)\n \nFigure 9\nPredicted similarity in BERT (y-axis, BERTcos(·,·), fitted values) between the prototype of\nthe replacement (repl_p) and its contextualised representation (repl_c) as a function of the\ncosine similarity between target and replacement the similarity space derived from the English\nSmall World of Words (SWoW) free association data (SWOWcos(target,repl)). The BERT\ncosine similarity was first Box-Cox transformed and then z-standardised. The color and line\nlegends identify the layer. The underlying GAM models cosine similarity in BERT as a\nfunction of cosine similarity between target and replacement in the SWoW similarity space\n(simple smooth), layer (simple smooth), and a partial tensor product between the two.\nMEANING MODULATIONS 90\n0.0 0.2 0.4 0.6 0.8\n−0.1 0.1 0.3 0.5\nAveraged prototype\n \njaccard(repl_c, target_c)\n0.0 0.2 0.4 0.6 0.8\n−0.1 0.1 0.3 0.5\nNon−contextual prototypes\n \n \n0.0 0.2 0.4 0.6 0.8\n−0.1 0.1 0.3 0.5\n \n \njaccard(repl_c, target_p)\n0.0 0.2 0.4 0.6 0.8\n−0.1 0.1 0.3 0.5\n \n \n \nlayer\n0\n3\n6\n9\n12\n0.0 0.2 0.4 0.6 0.8\n0.0 0.4 0.8\n \nSWOW cos(target, repl)\njaccard(repl_c, repl_p)\n0.0 0.2 0.4 0.6 0.8\n0.0 0.4 0.8\n \nSWOW cos(target, repl)\n \nFigure 10\nJaccard coefficient between the nearest neighbors in BERT space of the contextualised\nreplacement (repl_c) and the nearest neighbors of the the contextualised target (target_c,\ntop), prototype target (target_p, mid), and prototype replacement (repl_p, bottom), shown\nseparately for averaged (left) and non-contextualised (right) prototypes. The underlying\nGAMs model Jaccard coefficient (y-axis, fitted values) as a function of similarity between\ntarget and replacement inword2vec (simple smooth, x-axis), layer (simple smooth, color and\nline legends), and a partial tensor product between the two.\nMEANING MODULATIONS 91\n0.0 0.2 0.4 0.6 0.8\n−0.04 −0.02 0.00 0.02 0.04 0.06\n \nSWOW cos(target, repl)\nΔ Jaccard\nprototypes\naveraged\nnon−contextual\nFigure 11\nPredicted∆ (y-axis, fitted values) between (i) the Jaccard coefficient of the sets of nearest\nneighbors of the contextualised target (target_c) at layers -1 and 12, and (ii) the Jaccard\ncoefficient between the sets of nearest neighbors of the contextualised replacement (repl_c) at\nlayers -1 and 12, as a function of the cosine similarity in the SWoW similarity space (x-axis,\nSWOW cos(target,repl)). Color and line legends indicate the prototype method: blue, solid\nline for averaged prototypes; red, dotted line for non-contextualised prototypes. The\nunderlying GAM models the∆ Jaccard coefficient\n(jaccard(targetc−1,targetc12) −jaccard(replacementc−1,replacementc12) as a function of\nthe cosine similarity between target and replacement in the SWoW similarity space (simple\nsmooth). A predicted value of 0 indicates no difference, positive values indicate the target’s\nneighbors are more coherent between superficial and deep layers.)\nMEANING MODULATIONS 92\n(a) same target, different sense\n0 2 4 6 8 10 12\n0.7 0.8 0.9 1.0\n \nlayer\ncos(target_s1, target_s2) (b) similarity by condition\n0 2 4 6 8 10 12\n−1.0 −0.5 0.0 0.5 1.0\nCosine similarity between target and synonym\nlayer\nstandardised cosine\ncondition\nappropriate\ninappropriate\nFigure 12\nWord in Context (WiC) dataset. Effect of layer (x-axis) on (a) the cosine similarity (y-axis)\nbetween polysemous targets used in different sentences (s1 and s2) in different senses; and (b)\nthe layer-wise standardized cosine similarity between polysemous target and appropriate\nsynonym (blue, solid line) and between polysemous target and inappropriate synonym (red,\ndashed line), where the inappropriate synonym is the synonym of the target in the other\nsentence, and thus synonym of the target when used in a different sense.\n(a) averaging\n0 2 4 6 8 10 12\n−1.0 −0.5 0.0 0.5 1.0\n \nlayer\nstandardized cosine\ncondition\nappropriate\ninappropriate (b) non-contextualised\n0 2 4 6 8 10 12\n−1.0 −0.5 0.0 0.5 1.0\nlayer\ncos(critical,target)\ncondition\nappropriate\ninappropriate\nFigure 13\nPriming in Context (Till et al., 1988). Effect of layer (x-axis) on the layer-wise\nz-standardized cosine similarity (y-axis, fitted values) between ambiguous critical word and\ntarget, plotted separately by target relation with the critical word (condition [appropriate -\nblue solid line | inappropriate - red dashed line]) for (a) averaged prototypes and (b)\nnon-contextualised prototypes.\nMEANING MODULATIONS 93\n0 2 4 6 8 10 12\n−1.0 −0.5 0.0 0.5 1.0\nCosine similarity between target and associate (averaging)\nlayer\nstandardised cosine\ncondition\nappropriate\ninappropriate\nFigure 14\nMeaning Modulation (MeMo) dataset. Layer-wise standardized cosine similarity (y-axis,\nfitted values) between the contextualised representation of the target word and the averaged\nprototype representation of the associate word over layers (x-axis), plotted separately for the\ntwo conditions: related (blue, solid line) indicates the target and associate word match; foil\n(red, dashed line) indicates the target and associate do not match.\nMEANING MODULATIONS 94\nReferences\nAmbridge, B. (2020). Against stored abstractions: A radical exemplar model of language\nacquisition. First Language, 40(5-6), 509–559.\nhttps://doi.org/10.1177/0142723719869731\nAndrews, M., Vigliocco, G., & Vinson, D. (2009). Integrating experiential and distributional\ndata to learn semantic representations.Psychological Review, 116, 463–498.\nApidianaki, M. (2023). From word types to tokens and back: A survey of approaches to word\nmeaning representation and interpretation.Computational Linguistics, 49(2),\n465–523. https://doi.org/10.1162/coli_a_00474\nArmstrong, B. C., & Plaut, D. C. (2008). Settling dynamics in distributed networks explain\ntask differences in semantic ambiguity effects: Computational and behavioral\nevidence. In B. C. Love, K. McRae, & V. M. Sloutsky (Eds.),Proceedings of the 30th\nannual meeting of the cognitive science society(pp. 273–278).\nArmstrong, B. C., & Plaut, D. C. (2016). Disparate semantic ambiguity effects from\nsemantic processing dynamics rather than qualitative task differences.Language,\nCognition and Neuroscience, 31(7), 940–966.\nBaayen, R. H., Milin, P., Filipović Ðurđević, D., Hendrix, P., & Marelli, M. (2011). An\namorphous model for morphological processing in visual comprehension based on\nnaive discriminative learning.Psychological Review, 118, 438–481.\nBaayen, R. H., Vasishth, S., Kliegl, R., & Bates, D. (2017). The cave of shadows: Addressing\nthe human factor with generalized additive mixed models.Journal of Memory and\nLanguage, 94, 206–234.\nBalota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler, B., Loftis, B.,\nNeely, J. H., Nelson, D. L., Simpson, G. B., & Treiman, R. (2007). The English\nLexicon Project.Behavior Research Methods, 39, 445–459.\nBaroni, M., Dinu, G., & Kruszewski, G. (2014a). Don’t count, predict! a systematic\ncomparison of context-counting vs. context-predicting semantic vectors.Proceedings\nof the 52nd Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), 238–247. https://doi.org/10.3115/v1/P14-1023\nBaroni, M., Dinu, G., & Kruszewski, G. (2014b). Don’t count, predict! A systematic\ncomparison of context-counting vs. context-predicting semantic vectors.Proceedings\nof ACL 2014, 238–247.\nBarsalou, L. W. (1999). Perceptual symbol systems.Behavioral and Brain Sciences, 22,\n637–660.\nMEANING MODULATIONS 95\nBattleday, R. M., Peterson, J. C., & Griffiths, T. L. (2020). Capturing human categorization\nof natural images by combining deep networks and cognitive models.Nature\nCommunications, 11, 5418.\nBattleday, R. M., Peterson, J. C., & Griffiths, T. L. (2021). From convolutional neural\nnetworks to models of higher-level cognition (and back again).Annals of the New\nYork Academy of Sciences, 1505, 55–78.\nBeekhuizen, B., Armstrong, B. C., & Stevenson, S. (2021). Probing lexical ambiguity: Word\nvectors encode number and relatedness of senses.Cognitive Science, 45(5), e12943.\nBenedek, M., Kenett, Y. N., Umdasch, K., Anaki, D., Faust, M., & Neubauer, A. C. (2017).\nHow semantic memory structure and intelligence contribute to creative thought: A\nnetwork science approach.Thinking & Reasoning, 23, 158–183.\nBhatia, S., & Richie, R. (2022). Transformer networks of human conceptual knowledge.\nPsychological Review.\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with\nsubword information.Transactions of the Association for Computational Linguistics,\n5, 135–146. https://doi.org/10.1162/tacl_a_00051\nBommasani, R., Davis, K., & Cardie, C. (2020). Interpreting pretrained contextualized\nrepresentations via reductions to static embeddings.Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, 4758–4781.\nBonandrini, R., Amenta, S., Sulpizio, S., Tettamanti, M., Mazzucchelli, A., & Marelli, M.\n(2023). Form to meaning mapping and the impact of explicit morpheme combination\nin novel word processing.Cognitive Psychology, 145, 101594.\nBreit, A., Revenko, A., Rezaee, K., Pilehvar, M. T., & Camacho-Collados, J. (2020). Wic-tsv:\nAn evaluation benchmark for target sense verification of words in context.arXiv\npreprint arXiv:2004.15016.\nBrouwer, H., Crocker, M. W., Venhuizen, N. J., & Hoeks, J. C. (2017). A\nneurocomputational model of the n400 and the p600 in language processing.\nCognitive science, 41, 1318–1352.\nBruni, E., Tran, N. K., & Baroni, M. (2014). Multimodal distributional semantics.Journal\nof artificial intelligence research, 49(1), 1–47. https://doi.org/10.1613/jair.4135\nBruni, E., Tran, N.-K., & Baroni, M. (2014). Multimodal distributional semantics.Journal\nof Artificial Intelligence Research, 49, 1–47.\nBrysbaert, M., Mandera, P., McCormick, S. F., & Keuleers, E. (2019a). Word prevalence\nnorms for 62,000 english lemmas.Behavior research methods, 51(2), 467–479.\nBrysbaert, M., Mandera, P., McCormick, S. F., & Keuleers, E. (2019b). Word prevalence\nnorms for 62,000 english lemmas.Behavior Research Methods, 51, 467–479.\nMEANING MODULATIONS 96\nBrysbaert, M., New, B., & Keuleers, E. (2012). Adding part-of-speech information to the\nsubtlex-us word frequencies.Behavior research methods, 44(4), 991–997.\nBrysbaert, M., Warriner, A. B., & Kuperman, V. (2014). Concreteness ratings for 40\nthousand generally known english word lemmas.Behavior research methods, 46(3),\n904–911.\nBuchanan, E. M., Valentine, K. D., & Maxwell, N. P. (2019). English semantic feature\nproduction norms: An extended database of 4436 concepts.Behavior Research\nMethods, 51, 1849–1863.\nBuchanan, L., Westbury, C., & Burgess, C. (2001). Characterizing semantic space:\nNeighborhood effects in word recognition.Psychonomic Bulletin & Review, 8,\n531–544.\nBuijtelaar, L., & Pezzelle, S. (2023). A psycholinguistic analysis of bert’s representations of\ncompounds. arXiv preprint arXiv:2302.07232.\nBybee, J. (2002). Phonological evidence for exemplar storage of multiword sequences.Studies\nin second language acquisition, 24(2), 215–221.\nBybee, J. L. (2013). Usage-based theory and exemplar representations of constructions.\nCamacho-Collados, J., & Pilehvar, T. (2018). From word to sense embeddings: A survey on\nvector representations of meaning.Journal of Artificial Intelligence Research, 63,\n743–788.\nCard, D., Chang, S., Becker, C., Mendelsohn, J., Voigt, R., Boustan, L., Abramitzky, R., &\nJurafsky, D. (2022). Computational analysis of 140 years of us political speeches\nreveals more positive but increasingly polarized framing of immigration.Proceedings\nof the National Academy of Sciences, 119(31), e2120510119.\nCassani, G., Bianchi, F., & Marelli, M. (2021). Words with consistent diachronic usage\npatterns are learned earlier: A computational analysis using temporally aligned word\nembeddings. Cognitive Science, 45(4), e12963.\nCevoli, B., Watkins, C., Gao, Y., & Rastle, K. (2022). Shades of meaning: Natural language\nmodels offer insights and challenges to psychological understanding of lexical\nambiguity.\nCevoli, B., Watkins, C., & Rastle, K. (2022). Prediction as a basis for skilled reading:\nInsights from modern language models.Royal Society Open Science, 9(6), 211837.\nChalkidis, I., Fergadiotis, M., Malakasiotis, P., Aletras, N., & Androutsopoulos, I. (2020).\nLEGAL-BERT: The muppets straight out of law school.Findings of the Association\nfor Computational Linguistics: EMNLP 2020, 2898–2904.\nhttps://doi.org/10.18653/v1/2020.findings-emnlp.261\nMEANING MODULATIONS 97\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P.,\nChung, H. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language\nmodeling with pathways.arXiv preprint arXiv:2204.02311.\nColtheart, M., Davelaar, E., Jonasson, J. T., & Besner, D. (1977). Access to the internal\nlexicon. Attention and performance vi(pp. 535–555). Routledge.\nConnell, L., & Lynott, D. (2014). Principles of representation: Why you can’t represent the\nsame concept twice.Topics in Cognitive Science, 6(3), 390–406.\nCramer, P. (1970). A study of homographs. In L. Postman & G. Keppel (Eds.),Norms of\nword association(pp. 361–382). Academic Press.\nDaelemans, W., & Van Den Bosch, A. (2010). Memory-based learning.The Handbook of\nComputational Linguistics and Natural Language Processing, 154–179.\nDavies, M. (2009). The 385+ million word corpus of contemporary american english\n(1990–2008+): Design, architecture, and linguistic insights.International journal of\ncorpus linguistics, 14(2), 159–190.\nDe Deyne, S., Kenett, Y. N., Anaki, D., Faust, M., & Navarro, D. (2016). Large-scale\nnetwork representations of semantics in the mental lexicon. In M. N. Jones (Ed.),Big\nData in Cognitive Science: From Methods to Insights(pp. 174–202). Routledge.\nDe Deyne, S., Navarro, D. J., Perfors, A., Brysbaert, M., & Storms, G. (2019). The “small\nworld of words” english word association norms for over 12,000 cue words.Behavior\nresearch methods, 51(3), 987–1006.\nDevereux, B. J., Tyler, L. K., Geertzen, J., & Randall, B. (2014). The centre for speech,\nlanguage and the brain (cslb) concept property norms.Behav Res Methods, 46(4),\n1119–27. https://doi.org/10.3758/s13428-013-0420-4\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019a). Bert: Pre-training of deep\nbidirectional transformers for language understanding.Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\n4171–4186.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019b). BERT: Pre-training of deep\nbidirectional transformers for language understanding.Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\n4171–4186. https://doi.org/10.18653/v1/N19-1423\nDove, G. (2014). Thinking in words: Language as an embodied medium of thought.Topics in\nCognitive Science, 6, 371–389. https://doi.org/10.1111/tops.12102\nMEANING MODULATIONS 98\nDudschig, C., Maienborn, C., & Kaup, B. (2016). Is there a difference between stripy\njourneys and stripy ladybirds? The N400 response to semantic and world-knowledge\nviolations during sentence processing.Brain and Cognition, 103, 38–49.\nEthayarajh, K. (2019). How contextual are contextualized word representations? comparing\nthe geometry of bert, elmo, and gpt-2 embeddings.arXiv preprint, arXiv:1909.00512.\nEttinger, A. (2020). What bert is not: Lessons from a new suite of psycholinguistic\ndiagnostics for language models.Transactions of the Association for Computational\nLinguistics, 8, 34–48. https://doi.org/10.1162/tacl_a_00298\nFazekas, J., Jessop, A., Pine, J., & Rowland, C. (2020). Do children learn from their\nprediction mistakes? a registered report evaluating error-based theories of language\nacquisition. Royal Society Open Science, 7(11), 180877.\nFedermeier, K. D. (2007). Thinking ahead: The role and roots of prediction in language\ncomprehension. Psychophysiology, 44(4), 491–505.\nFischer, M. H., & Zwaan, R. A. (2008). Embodied language: A review of the role of the\nmotor system in language comprehension.The Quarterly Journal of Experimental\nPsychology, 61, 825–850.\nFrank, S. L., & Willems, R. M. (2017). Word predictability and semantic similarity show\ndistinct patterns of brain activity during language comprehension.Language,\nCognition and Neuroscience, 32(9), 1192–1203.\nFrisch, S. A. (2017). Exemplar theories in phonology.The routledge handbook of phonological\ntheory (pp. 553–568). Routledge.\nGatti, D., Marelli, M., & Rinaldi, L. (2022). Out-of-vocabulary but not meaningless:\nEvidence for semantic-priming effects in pseudoword processing.Journal of\nExperimental Psychology: General, Advance online publication.\nGlenberg, A. M., & Kaschak, M. P. (2002). Grounding language in action.Psychonomic\nBulletin & Review, 9, 558–565.\nGlenberg, A. M., & Robertson, D. A. (2000). Symbol Grounding and Meaning: A\nComparison of High-Dimensional and Embodies Theories of Meaning.Journal of\nMemory and Language, 43, 379–401.\nGonen, H., Jawahar, G., Seddah, D., & Goldberg, Y. (2021). Simple, interpretable and stable\nmethod for detecting words with usage change across corpora.arXiv preprint\narXiv:2112.14330.\nGriffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007). Topics in semantic representation.\nPsychological Review, 114(2), 211–44. https://doi.org/10.1037/0033-295X.114.2.211\nMEANING MODULATIONS 99\nGünther, F., Rinaldi, L., & Marelli, M. (2019). Vector-space models of semantic\nrepresentation from a cognitive perspective: A discussion of common misconceptions.\nPerspectives on Psychological Science, 14, 1006–1033.\nGünther, F., Dudschig, C., & Kaup, B. (2016a). Latent semantic analysis cosines as a\ncognitive similarity measure: Evidence from priming studies.The Quarterly Journal\nof Experimental Psychology, 69, 626–653.\nGünther, F., Dudschig, C., & Kaup, B. (2016b). Predicting lexical priming effects from\ndistributional semantic similarities: A replication with extension.Frontiers in\nPsychology, 7, 1646.\nGünther, F., & Marelli, M. (2022). Patterns in caoss: Distributed representations predict\nvariation in relational interpretations for familiar and novel compound words.\nCognitive Psychology, 134, 101471.\nGünther, F., Marelli, M., Tureski, S., & Petilli, M. A. (2023). Vispa (vision spaces): A\ncomputer-vision-based representation system for individual images and concept\nprototypes, with large-scale evaluation.Psychological Review, 130, 896–934.\nHaber, J., & Poesio, M. (2024). Polysemy—evidence from linguistics, behavioral science, and\ncontextualized language models.Computational Linguistics, 50(1), 351–417.\nHagoort, P., Brown, C., & Groothusen, J. (1993). The syntactic positive shift (sps) as an erp\nmeasure of syntactic processing.Language and cognitive processes, 8(4), 439–483.\nHagoort, P., Hald, L., Bastiaansen, M., & Petersson, K. M. (2004). Integration of word\nmeaning and world knowledge in language comprehension.Science, 304, 438–441.\nHamilton, W. L., Leskovec, J., & Jurafsky, D. (2016). Cultural shift or linguistic drift?\ncomparing two computational measures of semantic change.Proceedings of the\nConference on Empirical Methods in Natural Language Processing (EMNLP)., 2016,\n2116–2121.\nHao, Y., Mendelsohn, S., Sterneck, R., Martinez, R., & Frank, R. (2020). Probabilistic\npredictions of people perusing: Evaluating metrics of language model performance for\npsycholinguistic modeling.Proceedings of the Workshop on Cognitive Modeling and\nComputational Linguistics, 75–86. https://doi.org/10.18653/v1/2020.cmcl-1.10\nHarati, P., Westbury, C., & Kiaee, M. (2021). Evaluating the predication model of metaphor\ncomprehension: Using word2vec to model best/worst quality judgments of 622 novel\nmetaphors. Behavior Research Methods, 53(5), 2214–2225.\nHarris, Z. (1954). Distributional Structure.Word, 10, 146–162.\nHe, P., Liu, X., Gao, J., & Chen, W. (2021). Deberta: Decoding-enhanced bert with\ndisentangled attention.International Conference on Learning Representations.\nhttps://openreview.net/forum?id=XPZIaotutsD\nMEANING MODULATIONS 100\nHendrix, P., & Sun, C. C. (2020). A word or two about nonwords: Frequency, semantic\nneighborhood density, and orthography-to-semantics consistency effects for nonwords\nin the lexical decision task.Journal of Experimental Psychology: Learning, Memory,\nand Cognition. https://doi.org/10.1037/xlm0000819\nHewitt, J., & Manning, C. D. (2019). A structural probe for finding syntax in word\nrepresentations. NAACL-HLT.\nHeylen, K., Wielfaert, T., Speelman, D., & Geeraerts, D. (2015). Monitoring polysemy: Word\nspace models as a tool for large-scale lexical semantic analysis.Lingua, 157, 153–172.\nHill, F., Reichart, R., & Korhonen, A. (2015). Simlex-999: Evaluating semantic models with\n(genuine) similarity estimation.Computational Linguistics, 41, 665–695.\nHino, Y., Lupker, S. J., & Pexman, P. M. (2002). Ambiguity and synonymy effects in lexical\ndecision, naming, and semantic categorization tasks: Interactions between\northography, phonology, and semantics.Journal of Experimental Psychology:\nLearning, Memory, and Cognition, 28, 686–713.\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory.Neural computation,\n9(8), 1735–1780.\nHollis, G. (2017). Estimating the average need of semantic knowledge from distributional\nsemantic models.Memory & Cognition, 45, 1350–1370.\nHuettig, F. (2015). Four central questions about prediction in language processing.Brain\nresearch, 1626, 118–135.\nHuettig, F., & Mani, N. (2016). Is prediction necessary to understand language? probably\nnot. Language, Cognition and Neuroscience, 31(1), 19–31.\nHummel, J. E., & Holyoak, K. J. (2003). A symbolic-connectionist theory of relational\ninference and generalization.Psychological review, 110(2), 220.\nHussain, Z., Binz, M., Mata, R., & Wulff, D. U. (2023). A tutorial on open-source large\nlanguage models for behavioral science.PsyArXiv preprint.\nHutchison, K. A., Balota, D. A., Cortese, M., & Watson, J. M. (2008). Predicting semantic\npriming at the item level.The Quarterly Journal of Experimental Psychology, 61,\n1036–1066.\nHutchison, K. A., Balota, D. A., Neely, J. H., Cortese, M. J., Cohen-Shikora, E. R.,\nTse, C. S., Yap, M. J., Bengson, J. J., Niemeyer, D., & Buchanan, E. (2013). The\nsemantic priming project.Behav Res Methods, 45(4), 1099–114.\nhttps://doi.org/10.3758/s13428-012-0304-z\nHutchison, K. A., Balota, D. A., Neely, J. H., Cortese, M. J., Cohen-Shikora, E. R.,\nTse, C.-S., Yap, M. J., Bengson, J. J., Niemeyer, D., & Buchanan, E. (2013). The\nsemantic priming project.Behavior Research Methods, 45, 1099–1114.\nMEANING MODULATIONS 101\nJamieson, R. K., Avery, J. E., Johns, B. T., & Jones, M. N. (2018). An instance theory of\nsemantic memory.Computational Brain & Behavior, 1(2), 119–136.\nhttps://doi.org/10.1007/s42113-018-0008-2\nJamieson, R. K., Johns, B. T., Vokey, J. R., & Jones, M. N. (2022). Instance theory as a\ndomain-general framework for cognitive psychology.Nature Reviews Psychology, 1(3),\n174–183.\nJohns, B. T., Jamieson, R. K., Crump, M. J., Jones, M. N., & Mewhort, D. (2020).\nProduction without rules: Using an instance memory model to exploit structure in\nnatural language.Journal of Memory and Language, 115, 104165.\nJones, C., Bergen, B., & Trott, S. (2024). Do Multimodal Large Language Models and\nHumans Ground Language Similarly?Computational Linguistics, 1–25.\nhttps://doi.org/10.1162/coli_a_00531\nJones, C. R., Chang, T. A., Coulson, S., Michaelov, J. A., Trott, S., & Bergen, B. (2022).\nDistributional semantics still can’t account for affordances.Proceedings of the Annual\nMeeting of the Cognitive Science Society, 44.\nJones, M. N., Kintsch, W., & Mewhort, D. J. K. (2006). High-dimensional semantic space\naccounts of priming.Journal of Memory and Language, 55, 534–552.\nJones, M. N., & Mewhort, D. J. K. (2007). Representing word meaning and order\ninformation in a composite holographic lexicon.Psychological Review, 114, 1–37.\nJones, M. N., Willits, J., & Dennis, S. (2015). Models of semantic memory. In J. Busemeyer,\nZ. Wang, J. Townsend, & A. Eidels (Eds.),Oxford Handbook of Mathematical and\nComputational Psychology(pp. 232–254). Oxford University Press.\nKenett, Y. N., Levi, E., Anaki, D., & Faust, M. (2017). The semantic distance task:\nQuantifying semantic distance with semantic network path length.Journal of\nExperimental Psychology: Learning, Memory, and Cognition, 43, 1470–1489.\nKintsch, W. (1988). The use of knowledge in discourse processing: A construction-integration\nmodel. Psychological Review, 95, 163–182.\nKintsch, W. (2001). Predication.Cognitive Science, 25, 173–202.\nKintsch, W. (2000). Metaphor comprehension: A computational theory.Psychonomic\nBulletin & Review, 7, 257–266.\nKohl, P. K. (1993). Early linguistic experience and phonetic perception: Implications for\ntheories of developmental speech perception.Journal of phonetics, 21(1-2), 125–139.\nKoriat, A. (1981). Semantic facilitation in lexical decision as a function of prime-target\nassociation. Memory & Cognition, 9, 587–598.\nKumar, A. A. (2021). Semantic memory: A review of methods, models, and current\nchallenges. Psychonomic Bulletin & Review, 28(1), 40–80.\nMEANING MODULATIONS 102\nKuperman, V., Estes, Z., Brysbaert, M., & Warriner, A. B. (2014). Emotion and language:\nValence and arousal affect word recognition.Journal of Experimental Psychology:\nGeneral, 143, 1065–1081.\nKuperman, V., Stadthagen-Gonzalez, H., & Brysbaert, M. (2012). Age-of-acquisition ratings\nfor 30,000 english words.Behavior research methods, 44(4), 978–990.\nKutas, M., & Federmeier, K. D. (2011). Thirty years and counting: Finding meaning in the\nn400 component of the event-related brain potential (erp).Annual Review of\nPsychology, 62, 621–647.\nKutas, M., & Hillyard, S. A. (1980). Reading senseless sentences: Brain potentials reflect\nsemantic incongruity.Science, 207(4427), 203–205.\nKveraga, K., Ghuman, A. S., & Bar, M. (2007). Top-down predictions in the cognitive brain.\nBrain and cognition, 65(2), 145–168.\nLachmair, M., Dudschig, C., De Filippis, M., de la Vega, I., & Kaup, B. (2011). Root versus\nroof: Automatic activation of location information during word processing.\nPsychonomic Bulletin & Review, 18, 1180–1188.\nLakens, D. (2022). Sample size justification.Collabra: psychology, 8(1), 33267.\nLakoff, G., & Johnson, M. (1980). The metaphorical structure of the human conceptual\nsystem. Cognitive Science, 4, 195–208.\nLandauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The Latent\nSemantic Analysis theory of acquisition, induction, and representation of knowledge.\nPsychological Review, 104, 211–240.\nLeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning.Nature, 521, 436–444.\nLenci, A. (2008). Distributional semantics in linguistic and cognitive research.Italian\nJournal of Linguistics, 20(1), 1–31.\nLenci, A. (2018). Distributional models of word meaning.Annual Review of Linguistics, 4,\n151–171.\nLenci, A., Sahlgren, M., Jeuniaux, P., Cuba Gyllensten, A., & Miliani, M. (2022). A\ncomparative evaluation and analysis of three generations of distributional semantic\nmodels. Language resources and evaluation, 56(4), 1269–1313.\nLi, J., & Joanisse, M. F. (2021). Word senses as clusters of meaning modulations: A\ncomputational model of polysemy.Cognitive Science, 45(4), e12955.\nLiu, Q., Kusner, M. J., & Blunsom, P. (2020). A survey on contextual embeddings.arXiv\npreprint arXiv:2003.07278.\nLopopolo, A., & Rabovsky, M. (2021). Predicting the n400 erp component using the sentence\ngestalt model trained on a large scale corpus.bioRxiv, 2021–05.\nMEANING MODULATIONS 103\nLouwerse, M. M. (2011). Symbol interdependency in symbolic and embodied cognition.\nTopics in Cognitive Science, 3, 273–302.\nLucas, M. (2000). Semantic priming without association.Psychonomic Bulletin & Review, 7,\n618–630.\nLund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical\nco-occurrence. Behavior Research Methods, Instrumentation, and Computers, 28,\n201–208.\nLund, K., Burgess, C., & Atchley, R. A. (1995). Semantic and associative priming in\nhigh-dimensional semantic space. In J. D. Moore & J. F. Lehman (Eds.),Proceedings\nof the 17th Annual Conference of the Cognitive Science Society(pp. 660–665).\nErlbaum.\nLynott, D., Connell, L., Brysbaert, M., Brand, J., & Carney, J. (2020). The lancaster\nsensorimotor norms: Multidimensional measures of perceptual and action strength for\n40,000 english words.Behavior Research Methods, 52, 1271–1291.\nMadabushi, H. T., Divjak, D., & Milin, P. (2022). Abstraction not memory: Bert and the\nenglish article system.arXiv preprint arXiv:2206.04184.\nMandera, P., Keuleers, E., & Brysbaert, M. (2017). Explaining human performance in\npsycholinguistic tasks with models of semantic similarity based on prediction and\ncounting: A review and empirical validation.Journal of Memory and Language, 92,\n57–78.\nMarelli, M., Amenta, S., & Crepaldi, D. (2015). Semantic transparency in free stems: The\neffect of orthography-semantics consistency on word recognition.The Quarterly\nJournal of Experimental Psychology, 68, 1571–1583.\nMarelli, M., & Baroni, M. (2015). Affixation in semantic space: Modeling morpheme\nmeanings with compositional distributional semantics.Psychological Review, 122,\n485–515.\nMarr, D. (1982).Vision: A computational approach. Freeman & Co.\nMcRae, K., Cree, G. S., Seidenberg, M. S., & McNorgan, C. (2005). Semantic feature\nproduction norms for a large set of living and nonliving things.Behavior Research\nMethods, 37, 547–559.\nMichaelov, J. A., Bardolph, M. D., Van Petten, C. K., Bergen, B. K., & Coulson, S. (2023).\nStrong prediction: Language model surprisal explains multiple n400 effects.\nNeurobiology of Language, 1–71.\nMichaelov, J. A., Coulson, S., & Bergen, B. K. (2022). So cloze yet so far: N400 amplitude is\nbetter predicted by distributional information than human predictability judgements.\nMEANING MODULATIONS 104\nIEEE Transactions on Cognitive and Developmental Systems, Advance online\npublication. https://doi.org/10.1109/TCDS.2022.3176783\nMichaelov, J. A., Coulson, S., & Bergen, B. K. (2023). Can peanuts fall in love with\ndistributional semantics?\nMickus, T., Paperno, D., Constant, M., & Van Deemter, K. (2020). What do you mean,\nBERT? Assessing BERT as a Distributional Semantics Model.Proceedings of the\nSociety for Computation in Linguistics, 3, 235–245.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word\nrepresentations in vector space.arXiv:1301.3781v3.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Distributed\nrepresentations of words and phrases and their compositionality.Proceedings of the\n26th International Conference on Neural Information Processing Systems - Volume 2,\n3111–3119.\nMilin, P., Tucker, B. V., & Divjak, D. (2020). A learning perspective on the emergence of\nabstractions: The curious case of phonemes.arXiv preprint arXiv:2012.07499.\nMonsalve, I. F., Frank, S. L., & Vigliocco, G. (2012). Lexical surprisal as a general predictor\nof reading time.Proceedings of the 13th Conference of the European Chapter of the\nAssociation for Computational Linguistics, 398–408.\nMurphy, G. L. (2002a).The Big Book of Concepts. MIT Press.\nMurphy, G. L. (2002b). Conceptual Combination. In G. L. Murphy (Ed.),The Big Book of\nConcepts (pp. 443–475). MIT Press.\nNeely, J. H. (1991). Semantic priming effects in visual word recognition: A selective review of\ncurrent findings and theories. In D. Besner & G. W. Humphreys (Eds.),Basic\nprocesses in reading: Visual word recognition(pp. 264–336). Erlbaum.\nNematzadeh, A., Meylan, S. C., & Griffiths, T. L. (2017). Evaluating vector-space models of\nword representation, or, the unreasonable effectiveness of counting words near other\nwords. Proceedings of the 39th Annual Meeting of the Cognitive Science Society,\n859–864.\nNick Reid, J., & Katz, A. N. (2018). Vector space applications in metaphor comprehension.\nMetaphor and Symbol, 33(4), 280–294.\nNieuwland, M. S., & Van Berkum, J. J. (2006). When peanuts fall in love: N400 evidence for\nthe power of discourse.J Cogn Neurosci, 18(7), 1098–111.\nhttps://doi.org/10.1162/jocn.2006.18.7.1098\nPereira, F., Gershman, S., Ritter, S., & Botvinick, M. (2016). A comparative evaluation of\noff-the-shelf distributed semantic representations for modelling behavioural data.\nCognitive Neuropsychology, 33, 175–190.\nMEANING MODULATIONS 105\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L.\n(2018). Deep contextualized word representations. In M. A. Walker, H. Ji, & A. Stent\n(Eds.), Proceedings of the 2018 conference of the north american chapter of the\nassociation for computational linguistics: Human language technologies (naacl-hlt\n2018) (pp. 2227–2237). Association for Computational Linguistics.\nPetilli, M. A., Günther, F., Vergallito, A., Ciapparelli, M., & Marelli, M. (2021). Data-driven\ncomputational models reveal perceptual simulation in word comprehension.Journal\nof Memory and Language, 117, 104194.\nPexman, P. M., Hargreaves, I. S., Siakaluk, P. D., Bodner, G. E., & Pope, J. (2008). There\nare many ways to be rich: Effects of three measures of semantic richness on visual\nword recognition.Psychonomic Bulletin & Review, 15, 161–167.\nPezzelle, S., Takmaz, E., & Fernández, R. (2021). Word representation learning in\nmultimodal pre-trained transformers: An intrinsic evaluation.Transactions of the\nAssociation for Computational Linguistics, 9, 1563–1579.\nPickering, M. J., & Garrod, S. (2004). Toward a mechanistic psychology of dialogue.\nBehavioral and Brain Sciences, 27(2), 169–190.\nPilehvar, M. T., & Camacho-Collados, J. (2018). Wic: The word-in-context dataset for\nevaluating context-sensitive meaning representations.arXiv preprint\narXiv:1808.09121.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models\nfrom natural language supervision.International conference on machine learning,\n8748–8763.\nRasmy, L., Xiang, Y., Xie, Z., Tao, C., & Zhi, D. (2021). Med-bert: Pretrained\ncontextualized embeddings on large-scale structured electronic health records for\ndisease prediction.NPJ digital medicine, 4(1), 86.\nRodd, J. M. (2020). Settling into semantic space: An ambiguity-focused account of\nword-meaning access.Perspectives on Psychological Science, 15(2), 411–427.\nhttps://doi.org/10.1177/1745691619885860\nRodd, J. M., Gaskell, G., & Marslen-Wilson, W. (2002). Making sense of semantic ambiguity:\nSemantic competition in lexical access.Journal of Memory and Language, 46(2),\n245–266.\nRodd, J. M., Gaskell, M. G., & Marslen-Wilson, W. D. (2004). Modelling the effects of\nsemantic ambiguity in word recognition.Cognitive science, 28(1), 89–104.\nMEANING MODULATIONS 106\nRogers, A., Kovaleva, O., & Rumshisky, A. (2020). A primer in bertology: What we know\nabout how bert works.Transactions of the Association for Computational Linguistics,\n8, 842–866.\nRosch, E. (1978). Principles of Categorization. In E. Rosch & B. B. Lloyd (Eds.),Cognition\nand Categorization(pp. 27–46). Lawrence Erlbaum Associates.\nRosch, E., & Mervis, C. B. (1975). Family resemblances: Studies in the internal structure of\ncategories. Cognitive Psychology, 7, 573–605.\nRoy, A., & Pan, S. (2021). Incorporating medical knowledge in BERT for clinical relation\nextraction. Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, 5357–5366. https://doi.org/10.18653/v1/2021.emnlp-main.435\nSahlgren, M. (2008). The Distributional Hypothesis.Italian Journal of Linguistics, 20, 33–53.\nShah, R., Chawla, K., Eidnani, D., Shah, A., Du, W., Chava, S., Raman, N., Smiley, C.,\nChen, J., & Yang, D. (2022). When FLUE meets FLANG: Benchmarks and large\npretrained language model for financial domain.Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, 2322–2335.\nhttps://aclanthology.org/2022.emnlp-main.148\nShaoul, C., & Westbury, C. (2010). Exploring lexical co-occurrence space using hidex.\nBehavior Research Methods, 42, 393–413.\nSmith, E. E., & Medin, D. L. (1981).Categories and concepts. Harvard University Press.\nSmith, N. J., & Levy, R. (2013). The effect of word predictability on reading time is\nlogarithmic. Cognition, 128(3), 302–319.\nTaylor, J. E., Beith, A., & Sereno, S. C. (2020). Lexops: An r package and user interface for\nthe controlled generation of word stimuli.Behavior Research Methods, 52(6),\n2372–2382.\nTikhonov, A., Bylinina, L., & Paperno, D. (2023). Leverage points in modality shifts:\nComparing language-only and multimodal word representations.arXiv preprint\narXiv:2306.02348.\nTill, R. E., Mross, E. F., & Kintsch, W. (1988). Time course of priming for associate and\ninference words in a discourse context.Memory & cognition, 16(4), 283–298.\nTrott, S., & Bergen, B. (2023). Word meaning is both categorical and continuous.\nPsychological Review.\nUtsumi, A. (2011). Computational exploration of metaphor comprehension processes using a\nsemantic space model.Cognitive science, 35(2), 251–296.\nVan Heuven, W. J., Mandera, P., Keuleers, E., & Brysbaert, M. (2014). Subtlex-uk: A new\nand improved word frequency database for british english.Quarterly journal of\nexperimental psychology, 67(6), 1176–1190.\nMEANING MODULATIONS 107\nvan Rij, J., Wieling, M., Baayen, R. H., & van Rijn, H. (2022). itsadug: Interpreting time\nseries and autocorrelated data using gamms [R package version 2.4.1].\nvan Heuven, W. J., Mandera, P., Keuleers, E., & Brysbaert, M. (2014). SUBTLEX-UK: A\nnew and improved word frequency database for British English.The Quarterly\nJournal of Experimental Psychology, 67, 1176–1190.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &\nPolosukhin, I. (2017). Attention is all you need.Advances in neural information\nprocessing systems, 30.\nVujović, M., Ramscar, M., & Wonnacott, E. (2021). Language learning as uncertainty\nreduction: The role of prediction error in linguistic generalization and item-learning.\nJournal of Memory and Language, 119, 104231.\nVulić, I., Baker, S., Ponti, E. M., Petti, U., Leviant, I., Wing, K., Majewska, O., Bar, E.,\nMalone, M., Poibeau, T., Reichart, R., & Korhonen, A. (2020). Multi-SimLex: A\nlarge-scale evaluation of multilingual and crosslingual lexical semantic similarity.\nComputational Linguistics, 46(4), 847–897. https://doi.org/10.1162/coli_a_00391\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., &\nBowman, S. R. (2019). SuperGLUE: A stickier benchmark for general-purpose\nlanguage understanding systems.arXiv preprint 1905.00537.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. (2018). GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding.\nProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, 353–355. https://doi.org/10.18653/v1/W18-5446\nWarriner, A. B., Kuperman, V., & Brysbaert, M. (2013). Norms of valence, arousal, and\ndominance for 13,915 english lemmas.Behavior research methods, 45(4), 1191–1207.\nWilcox, E., Vani, P., & Levy, R. (2021). A targeted assessment of incremental processing in\nneural language models and humans.Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), 939–952.\nhttps://doi.org/10.18653/v1/2021.acl-long.76\nWillems, R. M., Frank, S. L., Nijhof, A. D., Hagoort, P., & Van den Bosch, A. (2016).\nPrediction during natural language comprehension.Cerebral Cortex, 26(6),\n2506–2516.\nYang, Y., Uy, M. C. S., & Huang, A. (2020). Finbert: A pretrained language model for\nfinancial communications.arXiv preprint arXiv:2006.08097.\nMEANING MODULATIONS 108\nYap, M. J., Tan, S. E., Pexman, P. M., & Hargreaves, I. S. (2011). Is more always better?\nEffects of semantic richness on lexical decision, speeded pronunciation, and semantic\nclassification. Psychonomic Bulletin & Review, 18, 742–750.\nYarkoni, T., Balota, D., & Yap, M. (2008). Moving beyond coltheart’s n: A new measure of\northographic similarity.Psychonomic bulletin & review, 15(5), 971–979.\nYee, E., & Thompson-Schill, S. L. (2016). Putting concepts into context.Psychonomic\nbulletin & review, 23(4), 1015–1027.\nZwaan, R. A., Stanfield, R. A., & Yaxley, R. H. (2002). Language comprehenders mentally\nrepresent the shapes of objects.Psychological Science, 13, 168–171.\nMEANING MODULATIONS 109\nAppendix A\nWord probability in context\nIn this supplementary analysis, we study the relation between the semantic relation between\ntarget and replacement in the Small World of Words similarity space on the one hand, and\nword probability in context on the other. While our work focuses on an analysis of\nrepresentations, it is useful to double check that these representations influence predictions\nof masked word in a sensible way.\nWe thus target the log probability of the replacement in context (Ettinger, 2020),\nwhich gives a first indication of how sensible the underlying semantic space is. We first\naveraged the probability for each target-replacement pair over the 50 sentences per pair. We\nthen fitted a GAM predicting the log probability as a function of the cosine similarity in the\nSWoW similarity space, the frequency of the target, and the frequency of the replacement,\nall entered as simple smooths). Frequency estimates were taken from SUBTLEX-US\n(Brysbaert et al., 2012, accessed via LexOPS Taylor et al., 2020) and were included as we\nexpect the probability to also depend on how frequent the target and replacement are. A\nvery frequent target should yield lower probabilities for any replacement, while a frequent\nreplacement should yield a higher probability no matter what target it replaces. We went a\nstep further and included partial tensor products (i.e., interactions) between cosine similarity\nand target frequency, and between cosine similarity and replacement frequency to investigate\nwhether the frequency of target and replacement modulates the effect of cosine similarity.\nTable A1 summarizes the coefficients, while Figure A1 shows the effects visually.\nPredictor edf Ref.df F p\ns(cos sim) 7.771 8.614 2453.439 <.001\ns(target f) 6.201 7.295 428.370 <.001\ns(replacement f) 5.671 6.869 514.145 <.001\nti(cos sim, target f) 10.571 12.411 13.470 <.001\nti(cos sim, replacement f) 7.774 9.479 9.736 <.001\nTable A1\nCoefficients of the GAM fitted to predict the probability of the replacements in context as a\nfunction of cosine similarity (cos sim) between target - the word which was replaced - and\nreplacement - the word used to replace it, the frequency of the target (target f), the frequency\nof the replacement (replacement f), and partial tensor products between cosine similarity and\ntarget frequency, and between cosine similarity and replacement frequency. The table provides\nthe estimated degrees of freedom (edf), the reference degrees of freedom (Ref.df), the F\nstatistic and the corresponding p-value.\nOur predictions are borne out: the probability of a replacement increases when the\ncosine similarity in the SWoW similarity space between replacement and target increases; it\nMEANING MODULATIONS 110\nalso increases when the frequency of the replacement increases but drops when the frequency\nof the target increases. The interaction between target frequency and cosine similarity shows\nthat, at low frequencies, the effect is entirely reducible to cosine, with low probabilities being\npredicted when the cosine similarity is particularly low. At higher target frequencies,\nhowever, we see that predicted probabilities are low even at non-negligible similarity values,\nsuggesting that when a target is very frequent, the replacement is predicted at low\nprobabilities even if it is semantically more similar, as expected. Turning to the interaction\nbetween replacement frequency and semantic similarity we see a similar, albeit weaker,\npattern in the opposite direction: when the replacement is frequent, the predicted\nprobability is higher even at more moderate values of similarity, again as per expectations.\nThis analysis confirms that BERT embeddings respond in the expected way to the interplay\nbetween frequency and semantic fit.\nMEANING MODULATIONS 111\n0.0 0.2 0.4 0.6 0.8\n−6 −5 −4 −3\nw2v cosine sim\nlog(prob)\n2 3 4 5 6 7\n−6.0 −5.5 −5.0 −4.5\nlog(freq_repl)\nlog(prob)\n2 3 4 5 6 7\n−7.5 −6.5 −5.5 −4.5\nlog(freq_target)\nlog(prob)\n0.0 0.2 0.4 0.6 0.8\n2 3 4 5 6 7\n \nw2v cosine sim\nlog(freq_target)\n −7.5 \n −7 \n −6.5 \n −6  −5.5 \n −5 \n −4.5 \n −4 \n −3.5 \n0.0 0.2 0.4 0.6 0.8\n2 3 4 5 6 7\n \nw2v cosine sim\nlog(freq_repl) −7 \n −6.5 \n −6 \n −5.5 \n −5  −4.5 \n −4 \n −3.5 \n −3 \n −2.5 \n −2 \nFigure A1\nEffects of word2vec similarity, replacement frequency, target frequency and interactions\nbetween layer and frequency measures on log probability of the replacement in English\nsentences originally containing the target (y-axis) as estimated in a GAM where each\npredictor was included as a simple smooth and interactions were included as non-linear\npartial tensor products. The top-left panel shows the effect of cosine similarity in the\nreference word2vec space (x-axis) on log probability (y-axis, fitted values excluding random\neffects); the top-mid panel shows the effect of replacement frequency (x-axis) on log\nprobability (y-axis, fitted values excluding random effects); the top-right panel shows the effect\nof target frequency (x-axis) on log probability (y-axis, fitted values excluding random effects);\nthe bottom-left panel shows the partial tensor product between cosine similarity (x-axis) and\ntarget frequency (y-axis) on log probability (color); the bottom-mid panel shows the partial\ntensor product between cosine similarity (x-axis) and replacement frequency (y-axis) on log\nprobability (color).\nMEANING MODULATIONS 112\nAppendix B\nMeMo dataset reliability analysis\nIn order to assess how robust the interaction between condition [appropriate | inappropriate]\nand layer is when analysing meaning modulations within a same sense, we adopted a\nbootstrapping approach. We sampled 20, 30, and 40 target words without replacement, and\nsubset the MeMo dataset to retrieve only the observations pertaining to the target words in\nthe sample, including both appropriate and inappropriate associates. We then fit a Linear\nMixed Effect Regression (LMER) model with the cosine between the contextualised\nembedding of the target word embedded in the corresponding sentence and the prototype\nembedding of the associate. We fit two separate models for the two prototyping methods,\naveraged and non-contextualised. From this model, we derived the beta coefficient of the\ninteraction term and the corresponding t statistic. This analysis allows us to directly check\nthe direction of the effect as well as the corresponding statistic. Finally, we fit two GAMMs\npredicting the same dependent variable as the LMER model. The first GAMM includes the\nfollowing independent variables: condition as a parametric term and layer as a simple\nsmooth. The second GAMM further includes the interaction between the two terms. We\nthen used thecompareMLfunction from theitsadug package to test whether the second\nmodel has a better fit than the first, i.e., whether the interaction improves model fit\nquantified as the∆AIC. We derived 100 samples, and performed the same analyses on each.\nFinally, we visualize the distribution of the three statistics of interest (beta coefficient of the\ntarget interaction and corresponding t statistic from the LMER, and the∆AIC from the\nGAMMs). Figure B1, Figure B2, and Figure B3 show the distribution of the beta coefficient,\nt statistic, and∆AIC respectively.\nThese analyses show that with the current sample size of target words and associates\nwe are unlikely to overestimate the effect reported for averaged prototypes. When\nconsidering 20 target words we do observe larger variability, showing that certain target\nwords emphasize the effect while others hinder it, but even the sample yielding the smallest\nbeta coefficient (β = 0.0136) yields a robust effect in the predicted direction (t= 3.666) and\nincluding the interaction in the GAMM improves model fit (∆AIC = 12.42). In the same\nvein, the sample yielding the smallest t statistic (t= 3.646) also yields an interaction\ncoefficient in the predicted direction (β = 0.0158) and shows an improvement in model fit\nafter including the target interaction (∆AIC = 12.26, which is also the smallest increase in\nmodel fit). These are however the worst case scenarios, suggesting that the reported effects\nare robust and that the chosen sample size ensures a decent power. We hope this dataset\nsparks interest in such subtle meaning modulations and we look forward to attempts to\nreplicate the effect with different target words, associates, and sentences.\nMEANING MODULATIONS 113\naveraged non−contextualised\n20 30 40 20 30 40\n0.00\n0.01\n0.02\n0.03\n# target words in sample\nbeta coefficient\nPrototyping method averaged non−contextualised\nTarget interaction's beta coeff (LMER) by sample size and prototype, bootstrapped\nFigure B1\nMeaning Modulation (MeMo) dataset. Distribution of the beta coefficient of the target\ninteraction, condition*layer, estimated fitting 100 Linear Mixed Effects Regression (LMER)\nmodels over 100 random samples of 20, 30, and 40 target words. y axis: value of the beta\ncoefficient; x axis: number of target words in each sample; color and faceting: prototyping\nmethod, averaging or non-contextualised. Horizontal lines indicate the value of the beta\ncoefficient estimated using the full MeMo dataset.\nAt the same time, we observe that when using non-contextualised prototypes the\neffect is unlikely to be there and also that with a smaller sample size and a certain subset of\ntarget words we may have incorrectly concluded that the target interaction influenced the\nsimilarity patterns also with this prototyping method. The highest beta coefficient\n(β = 0.0166) has a corresponding t statistic of 3.054, which would have been significant\nunder a LMER analysis. The full MeMo dataset, however, provides sufficient robustness to\nrule out that the target interaction has a reliable effect on non-contextualised prototypes.\nMEANING MODULATIONS 114\naveraged non−contextualised\n20 30 40 20 30 40\n0.0\n2.5\n5.0\n7.5\n10.0\n# target words in sample\nt statistic\nPrototyping method averaged non−contextualised\nTarget interaction's t statistic (LMER) by sample size and prototype, bootstrapped\nFigure B2\nMeaning Modulation (MeMo) dataset. Distribution of the t statistic of the target interaction,\ncondition*layer, estimated fitting 100 Linear Mixed Effects Regression (LMER) models over\n100 random samples of 20, 30, and 40 target words. y axis: value of the t statistic; x axis:\nnumber of target words in each sample; color and faceting: prototyping method, averaging or\nnon-contextualised. Horizontal lines indicate the value of the t statistic estimated using the\nfull MeMo dataset.\nMEANING MODULATIONS 115\naveraged non−contextualised\n20 30 40 20 30 40\n0\n25\n50\n75\n100\n# target words in sample\ndelta AIC\nPrototyping method averaged non−contextualised\ndelta AIC (interaction v. linear combination, GAMM) by sample size and prototype, bootstrapped\nFigure B3\nMeaning Modulation (MeMo) dataset. Distribution of the∆AIC between a GAMM\nincluding and a GAMM not including the target interaction, layer by condition, over 100\nrandom samples of 20, 30, and 40 target words. y axis: value of the∆AIC; x axis: number\nof target words in each sample; color and faceting: prototyping method, averaging or\nnon-contextualised. Horizontal lines indicate the value of the∆AIC estimated using the full\nMeMo dataset.\nMEANING MODULATIONS 116\nAppendix C\nMathematical details of the Instance Theory of Semantics (ITS\nmodel\n• Each letter string (word or nonword) is ann-dimensional vectorw. Each dimensional\nvalue is sampled fromN(0,1\nn ). This vector is assumed to just represent the form of the\nword, and does not yet carry any specific semantic information (the expected similarity\nof these vectors for any word pair is 0)\n• The memory of a given language example (like a word, sentence, or document), called\nan instance contextci, is the vector sum of allh words in that example:\nci =\nj=h∑\nj=1\nwij (C1)\n• All memories for each individual instances are stored in a matrixM, where each row is\nthe memory traceof one language example (soMi = ci). Note that this matrixM can\nhave a very large number of rows, because it has a memory trace of each language\nexample that the individual has ever encountered. The number of columns is alwaysn\n• All information is stored inM. There are no stored representations of word meanings,\nonly memories of individual language examples. Word meanings are created during\nretrieval, where they are referred to asecho\n• During retrieval, a word (that would be encountered by the speaker) serves as aprobe\np into memory. This probe generates anecho from memory, in a two-step process:\n1. The probe activatesall traces in memoryM. The amount of activation of each\ntrace is a function of the cosine similarity between probe and trace (more\nspecifically, the cube):\nai = cos(p,Mi)3 (C2)\n(as the cube of a cosine,ai is bound between -1 and 1). By computing the cube,\nonly very similar traces will receive relevant amounts of activation. This is\nrelevant since there can be a very large number of traces, most of which will be\ncompletely irrelevant to the probe\nMEANING MODULATIONS 117\n2. Then, all activated tracesare summed up via weighted addition to produce the\necho. The weights are the activation values. So for each dimensional valuej in\necho e we have:\nej =\ni=m∑\ni=1\nj=n∑\nj=1\nai ×Mij (C3)\n, withm being the number of memory traces inM, andMij being dimensional\nvalue j of memory traceMi\n3. The echo isthe semantic representation retrieved for the probe\n• The similarity between two echoes is simply calculated as their cosine similarity\ncos(e1,e2)\n• It is also possible to present ajoint probeconsisting of more than one word. In that\ncase, the general retrieval process still works as presented above. The only difference is\nthe computation of activation: In that case, the formula in (C2) is generalized to\nai =\nk=h∏\nk=1\ncos(pk,Mi)3 (C4)\n, withpk being thekth word in the probe (note that fork= 1, (C4) is equivalent to\n(C2)). This will, for example, produce a different echo for the probe [break, glass] than\nfor [break, car]"
}