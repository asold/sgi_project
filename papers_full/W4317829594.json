{
  "title": "FastDTI: Drug-Target Interaction Prediction using Multimodality and Transformers",
  "url": "https://openalex.org/W4317829594",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5021404180",
      "name": "Mathijs Boezer",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5089317938",
      "name": "Maryam Tavakol",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5070404151",
      "name": "Zahra Sajadi",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3084084616",
    "https://openalex.org/W3088849005",
    "https://openalex.org/W2062941476",
    "https://openalex.org/W3028589594",
    "https://openalex.org/W3215525389",
    "https://openalex.org/W3158236124",
    "https://openalex.org/W2808680821",
    "https://openalex.org/W2605952223",
    "https://openalex.org/W6622161800",
    "https://openalex.org/W3096561213",
    "https://openalex.org/W2785947426",
    "https://openalex.org/W2109991441",
    "https://openalex.org/W3125614524",
    "https://openalex.org/W2023782921",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W3155792797",
    "https://openalex.org/W4286762282",
    "https://openalex.org/W2294516783",
    "https://openalex.org/W2035585923",
    "https://openalex.org/W618024573",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2592742128",
    "https://openalex.org/W1554093359",
    "https://openalex.org/W3159276577",
    "https://openalex.org/W4200547951",
    "https://openalex.org/W2753588101",
    "https://openalex.org/W4206419503",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4205696149",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W2911286998",
    "https://openalex.org/W3040739508"
  ],
  "abstract": "Recent advances in machine learning have proved effective in the application of drug discovery by predicting the drugs that are likely to interact with a protein target of a certain disease, leading to prioritizing drug development and re-purposing efforts. State-of-the-art techniques in Drug-Target Interaction (DTI) prediction are often computationally expensive and can only be trained on small specialized datasets. In this paper, we propose a novel architecture, called FastDTI, utilizing pretrained transformers and graph neural networks in a self-supervised manner on large-scale (unlabeled) data, which additionally allows for embedding of multimodal input representations, for both drug and protein properties. Extensive empirical study demonstrates that our approach outperforms state-of-the-art DTI methods on the KIBA benchmark dataset, while greatly improving the computational complexity of training, about 200 times faster, leading to excellent performance results.",
  "full_text": "FastDTI: Drug-Target Interaction Prediction using\nMultimodality and Transformers\nMathijs Boezer, Maryam Tavakol∗, and Seyedeh Zahra Sajadi\nEindhoven University of Technology, Eindhoven, The Netherlands\nAbstract\nRecent advances in machine learning have proved\neffective in the application of drug discovery by pre-\ndicting the drugs that are likely to interact with\na protein target of a certain disease, leading to\nprioritizing drug development and re-purposing ef-\nforts. State-of-the-art techniques in Drug-Target\nInteraction (DTI) prediction are often computa-\ntionally expensive and can only be trained on small\nspecialized datasets. In this paper, we propose a\nnovel architecture, called FastDTI, utilizing pre-\ntrained transformers and graph neural networks in\na self-supervised manner on large-scale (unlabeled)\ndata, which additionally allows for embedding of\nmultimodal input representations, for both drug\nand protein properties. Extensive empirical study\ndemonstrates that our approach outperforms state-\nof-the-art DTI methods on the KIBA benchmark\ndataset, while greatly improving the computational\ncomplexity of training, about 200 times faster, lead-\ning to excellent performance results.\n1 Introduction\nThe recent COVID-19 pandemic, which is not the\nfirst and will likely not be the last pandemic [18],\nhas caused devastating disruptions on health, so-\nciety, and economy world-wide. It has shown that\nnot being able to rapidly develop effective treat-\nments for a new disease is a significant shortcom-\ning of the world’s ability to respond to a pandemic,\ngiven that the pipeline of developing new drugs are\ntoo slow in most scenarios. One way to efficiently\nspeed up the process of finding treatments for a\nnovel disease is to re-purpose existing drugs [3].\n∗Corresponding Author: m.tavakol@tue.nl\nHowever, this can be highly challenging due to the\nmassive number of chemical compounds that form\nthe candidate drugs for the newly-arrived disease\n[9]. Hence, we need functional techniques to accu-\nrately identify which drugs among the thousands of\ncandidates are the best options for further testing.\nUntil now, lots of data have been collected on\nboth drugs and drug targets (i.e., proteins), as\nwell as on the interactions between them [23]. Us-\ning this data, machine learning models can replace\nthe additional testing in the labs and act as a\ndecision-support system in addressing the problem\nof Drug-Target Interaction (DTI) prediction, which\ncan speed up the procedure of drug development\nand hence world’s response to pandemics.\nThe efficacy of machine learning, particularly\ndeep learning approaches, for DTI prediction has\nbeen already demonstrated in the literature [2, 27,\n32, 1, 30, 21, 22]. Early models utilize techniques\nsimilar to those used for recommender systems,\nwith this idea that similar drugs interact with sim-\nilar targets [9, 10]. However, these techniques fail\nto cope with the “cold-start” problem, e.g., drugs\nor proteins that have no known interactions, which\nis the case in the problem at-hand. Docking [14]\nis an alternative method in which interactions are\ndetermined by a physical simulation using spatial\nmodels of the drug and protein. Nevertheless, the\n3D data on either drug or their targets are often not\navailable, which leads to limited benefits of these\nmethods in practice [28, 7].\nOn the other hand, state-of-the-art deep learn-\ning techniques employ various architectures, such\nas convolutional neural networks [2], recurrent neu-\nral networks [30], graph neural networks [15], and\ntransformers [4], to automatically discover complex\nfeatures in the input, and have been shown to be\neffective for DTI prediction. Although these mod-\nhttps://doi.org/10.7557/18.6788\n© The author(s). Licensee Septentrio Academic Publishing, Tromsø, Norway. This is an open access article distributed\nunder the terms and conditions of the Creative Commons Attribution license\n(http://creativecommons.org/licenses/by/4.0/).\n1\nels perform well on small specialized datasets that\nonly deal with a single type of protein, they are\ngreatly limited by their computational complexity\nand, as a result, are unable to train on general (non-\nspecialized) DTI datasets. Therefore, we aim to\nget the best of both worlds, by developing a model\nthat is both accurate and fast, and can be trained\non large amounts of DTI data, instead of being\nrestricted to a single type of protein (out of the\nnumerous types of proteins [13]). Besides, the ex-\nisting work rarely use more than one modality for\nthe drug or the protein, while additional modalities\nmay improve the performance, as DeepH-DTA [2]\nhas indicated by using two modalities for the drug.\nIn this paper, we address these two shortcomings,\nnamely the computational complexity and the lack\nof multimodality for both drug and protein repre-\nsentations, and propose a novel approach, called\nFastDTI, to improve the time complexity as well\nas the performance of predictions. We employ the\nrecent developments in natural language process-\ning and graph neural networks to create a model\nthat can leverage different modalities of the drug\nand protein input, including their properties, in\none model. Subsequently, pretrained submodels are\nutilized to embed their sequences and/or graph rep-\nresentations during the pre-processing step, lead-\ning to reduced computational complexity during\nthe training. Additionally, the modalities of drug\nproperties as well as protein properties are intro-\nduced, providing valuable information to the model\nfor making more accurate predictions.\nOur extensive empirical study illustrates that\nFastDTI outperforms state-of-the-art DTI methods\non the KIBA benchmark dataset, while greatly im-\nproves the computational complexity of the train-\ning, about 200 times faster, leading to excellent per-\nformance results for DTI prediction.\n2 Related Work\nThe application of deep learning in the DTI predic-\ntion problem has significantly improved upon the\ntraditional machine learning methods [31], and is\na notable candidate to train on the large amount\nof available data [9]. The efficacy of deep learning\nis evident from many existing work [2, 27, 32, 1,\n30, 21, 22], which exert various architectures such\nas convolutional neural networks [2], recurrent neu-\nral networks [30], graph neural networks [15], and\ntransformers [4] for addressing the problem.\nAmong the state-of-the-art approaches based on\ndeep learning, DeepH-DTA [2] is the most accurate\nmodel, primarily due to using both graph neural\nnetworks and three modalities of the input. Almost\nall other models leverage two modalities: one for\nthe drug, and one for the protein. For the protein,\nDeepH-DTA uses the protein sequence, and for the\ndrug, both the SMILES sequence [26] and a graph\nrepresentation of the chemical structure, leading to\nthree total modalities. In addition, it employs a\nsophisticated graph neural network named hetero-\ngeneous graph attention (HGAT) [25], which was\nnot applied to DTI prediction before. However,\nDeepH-DTA is limited by its computational com-\nplexity. The authors report that it would take over\nfour days to train on the small KIBA dataset [24],\neven if powerful hardware were used.\nOther state-of-the-art DTI prediction approaches\ninclude GraphDTA [15] and DeepDTA [16].\nGraphDTA introduces the use of graph neural net-\nworks, achieving decent accuracy at the cost of high\ncomplexity. Inversely, DeepDTA [16] is a simple\nand fast model, which has low complexity at the\ncost of low accuracy. In this paper, we present an\napproach that is both accurate and efficient, while\nincorporating more than one modality for the drugs\nas well as for the proteins.\n3 FastDTI\nIn this section, we propose a novel technique for\nDrug-Target Interaction (DTI) prediction, called\nFastDTI, that aims to achieve the performance of\nthe state-of-the-arts while significantly decreasing\ntheir computational complexity. Hence, it allows\nto go beyond small specialized datasets and utilize\nthe large amounts of data that were previously in-\naccessible for DTI prediction problems.\nTo this end, we take into account two main prin-\nciples to ensure both high accuracy of the predic-\ntions and low computational requirements: (i) to\noffload the computation to the pre-processing step\nwhenever possible, in particular, for sequence and\ngraph embedding representations and (ii) to incor-\nporate additional modalities for both drugs and\nproteins. The former follows from the main bot-\ntleneck in the existing work, where the processing\n2\nConcatenation\nConcatenation\nProtein propertiesChemBERTaGrover\nCN1C=N \nC2=C1C \n(=O)N(C(=O) \nN2C)C\nDrug Protein\nChemical properties ProtBert\nDense layer\nConcatenation\n3x  \nDense layer\nBinding\nprediction\nMPPSISAFQAAY\nIGIEVLIALVSV\nVSLAVADVAVGA\nLVIPLAILI...\nAggregate\nAggregate\n{\"spkingdom\": \"Eukarya\",\n\"length\": 326,\n\"location\": \"membrane\",\n\"function\": \"nucleoside\"\n...}\nEncodeEncode\n{\"length\": 31,\n\"mass\": 36511,\n\"pl_area\": 58}\nDense layerDense layerDense layerDense layer\nDense layerDense layer\nFigure 1: The overall architecture of FastDTI. Grover figure is taken from [20] (Fig. 1).\nof the sequences and graphs is very expensive. We\naddress this problem by employing pretrained mod-\nels to compute embeddings of such complex rep-\nresentations. These embeddings can be computed\nduring the pre-processing step, which dramatically\nreduces the workload during the training, without\nsacrificing the performance of the model.\nThe second principle enhances the accuracy by\nproviding additional information to the model in\nthe form of drug and protein properties. Previous\nmethods typically leverage only one modality for\nthe drug and one for the protein. Instead, we add\ntwo more modalities into FastDTI, which compared\nto DeepH-DTA [2], that incorporate three modali-\nties, leads to a total of five modalities. The prop-\nerties used by FastDTI are each specifically chosen\nfor giving valuable information for the prediction\ntask, where some are computable, meaning that\nthey can be derived from the protein or drug se-\nquences, while other properties are obtained from\nmeasurements. Nevertheless, the properties of both\ndrugs and targets can be expanded by incorporat-\ning additional attributes using expert knowledge\nwhich is out of the scope of this work.\nThe overall network architecture of FastDTI is\npictured in Figure 1. On the left, is the component\nthat learns a representation for the drugs, consist-\ning of three sub-components, one for each of the\ndrug modalities. The first modality (right) is a\npretrained transformer model called ChemBERTa\n[5] for the SMILES sequence of the drug, which is\nbased on the RoBERTa architecture [12], trained\non a large dataset of unlabeled SMILES sequences.\nSecond modality (middle) is a pretrained graph\nneural network called Grover [20] for the graph\nstructure of the drug, based on a GTransformer\narchitecture, trained on a large dataset of un-\nlabeled drug graphs. The third one (left) is a\ndense layer for the chemical properties of the\ndrugs, which consist of their sequence length,\nmolecular weight, and polar surface area.\nSimilarly on the right, is the component to learn\nthe protein representations, which leverages a pre-\ntrained model, ProtBERT [8], based on BERT [6],\ntrained on a large dataset of protein sequences, to\nprocess those sequences (right) and a dense layer\nfor the protein properties (left), which include\ntheir sequence length, molecular weight,\nisoelectric point, subcellular location,\nsuperkingdom, and function.\nOnce the encoding of both the proteins and the\ndrugs from different modalities are computed, the\nobtained representations are concatenated and fed\ninto the DTI prediction component, with several\n3\nTable 1: Datasets used for the verification of the properties of FastDTI.\nName #Drugs #Proteins #Interactions Purpose\nKIBA 2,111 229 118,254 Benchmark against SOTA models\nSTITCHfull 116,224,359 9,643,763 ∼1,600,000,000 Creating STITCHhuman\nSTITCHhuman 79,047 1,934 9,418,794 Create a general DTI prediction model\ndense layers and an output layer to produce the fi-\nnal prediction scores. All the layers incorporate a\nReLU activation function. Subseqeuntly, the train-\ning is conducted by optimizing a Mean Squared Er-\nror loss function. In addition, we train the model\nto make it robust to partially missing data due\nto some unknown properties of either proteins or\ndrugs. To do so, some values are replaced by a\nmissing value with a configurable probability dur-\ning training. This ensures the model is exposed to\nvarious combinations of present and missing values,\nsecuring robustness to missing data.\n4 Empirical Study\n4.1 Experimental Setups\nIn this section, we evaluate the performance of\nFastDTI compared to several state-of-the-art tech-\nniques for the problem of drug-target prediction on\nboth small- and large-scale data. Accordingly, we\nfirst conduct the experiments on the KIBA (Ki-\nnase Inhibitor BioActivity) dataset [24], which is a\nrelatively small benchmark data to compare DTI\npredictions methods, including the models that are\ncomputationally expensive and cannot be easily\nevaluated on larger-scale datasets. We prepare the\ndata according to the procedure described in [10],\nfor a fair comparison of DTI prediction models. In\naddition, to examine the performance of FastDTI\non a larger dataset, we use the STITCH data [23],\nwhich is by far the most extensive DTI data that\nexists, and encompasses a vast number of proteins\nacross numerous organisms, not just humans. Ta-\nble 1 summarizes the statistics of both datasets.\nSubsequently, the data is split into training\n(80%) and testing (20%) sets, and we employ 5-\nfold cross-validation on the train set to tune the hy-\nperparameters. After model selection, most dense\nlayers have 1024 neurons, except for the layers af-\nter the first two concatenations, which have 512.\nDropout is set to 0.3 for the KIBA dataset, but 0.0\nfor the STITCH data. In addition, we utilize four\nmetrics to compare the performance of the mod-\nels: (i) Mean Squared Error (MSE), (ii) Concor-\ndance Index (CI), (iii) r2\nm [19], and (iv) seconds per\nepoch (for the KIBA dataset and only for training\ntime). Furthermore, we compute the standard de-\nviation for the CI and r2\nm measures. The standard\ndeviation of the MSE of most models is unfortu-\nnately not available, so this is omitted for FastDTI\nas well. Moreover, a selection of baseline models\nis made from the best-performing state-of-the-art\ndeep learning techniques, as well as two methods\nthat are not based on deep learning. The references\nto the baseline methods are indicated in Table 2\n4.2 Experimental Results\n4.2.1 Overall Performance\nIn the first experiment, we compare FastDTI to\nthe state-of-the-art DTI prediction approaches on\nthe KIBA dataset. This is especially important\nbecause most existing work cannot train on the\nSTITCH dataset, due to high computational re-\nquirements. However, the results on the time com-\nplexity in this experiment can be generalized to a\nlarger data such as STITCH. The evaluation re-\nsults for the baseline methods are taken from the\nreported values by their authors (which use the\nsame setup as this experiment), except for “sec-\nond per epoch”. This metric is independently veri-\nfied to ensure the comparison is made on the same\nhardware, which is a cloud-based machine using\nan NVIDIA RTX A5000 graphics card and AMD\nRyzen Threadripper 1950X processor. Moreover,\nFastDTI is trained 10 times, each for 1000 epochs.\nThe overall performance of different techniques\nin terms of four above-mentioned measures is out-\nlined in Table 2. The experimental results illustrate\nthat FastDTI performs well on the KIBA dataset,\nand outperforms all the competitors on all th ee-\n4\nTable 2: FastDTI compared to various state-of-the-art models, as well as traditional methods of SimBoost\nand KronRLS. Note that seconds per epoch is not a valid metric for KronRLS and SimBoost, due to\nnot training using epochs. Additionally, the authors of C-A DTI did not report the r2\nm or make their\ncode available for measuring the seconds per epoch.\nModel MSE CI (std) r2\nm (std) Seconds / epoch\n(Non-DL) Machine learning methods\nKronRLS [17] 0.411 0.782 (0.001) 0.342 (0.001) N/A\nSimBoost [10] 0.222 0.836 (0.001) 0.629 (0.007) N/A\nDeep learning methods\nFastDTI 0.107 0.931 (0.002) 0.820 (0.004) 6\nDeepH-DTA [2] 0.111 0.927 (0.003) 0.799 (0.004) 1412\nFusionDTA [30] 0.130 0.906 (0.001) 0.793 (0.002) 258\nGraphDTA [15] 0.148 0.891 (0.001) 0.730 (0.015) 1344\nC-A DTI [11] 0.175 0.874 (0.001) N/A N/A\nDeepDTA [16] 0.181 0.868 (0.004) 0.711 (0.021) 162\nML-DTI [29] 0.196 0.862 (0.006) 0.727 (0.012) 25\nvaluated metrics. Additionally, a t-test is carried\nout to compare the r2\nm of FastDTI with DeepH-\nDTA, which results in a conclusion that FastDTI\nhas a higher r2\nm with p < 0.0001. Furthermore,\nour approach is over 200 times faster than the sec-\nond most accurate model in the state-of-the-arts,\nindicating that our approach manages to avoid the\ntrade-off between accuracy and training time.\n4.2.2 Performance on STITCH data\nThe second experiment explores how FastDTI fares\non the STITCH data, which is significantly bigger\nthan the KIBA dataset. Consequently, we examine\nto what extent our approach is able to predict the\nSTITCH interaction values in a reasonable time.\nTo this end, FastDTI is trained on the STITCH\ndata for 50 epochs and the predicted values are\ncompared to the true values in terms of MSE.\nThe evaluation result demonstrates that FastDTI\nis able to achieve the MSE of 0 .094 on the test\nset, indicating that the error of prediction is small\non average. To better understand this value, the\npredictions from our model are visually compared\nagainst the true values in Figure 2. The plot rep-\nresents a random subset of 10,000 points from the\nSTITCH data, due to the full data being too large,\nand the scores are converted from log-odds back to\nthe probabilities for better interpretability. Sub-\nsequently, the figure shows that the model appears\nto have a harder time making predictions compared\nto the first experiment, which might be due to the\nfact that the STITCH data is more complex than\nthe KIBA dataset. However, note that the MSE\nis not comparable between the KIBA and STITCH\ndatasets, due to expressing the interaction strength\nin different units. The STITCH set is much less\nfiltered and contains several orders of magnitude\nmore drugs and proteins, including numerous types\nof proteins, while the KIBA dataset exclusively\ncontains kinases. Nevertheless, the model performs\npromisingly well on this large-scale data.\nFigure 2: FastDTI Performance on STITCH data.\n5\n/uni00000013/uni00000018/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013\n/uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000004f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\n/uni00000030/uni00000052/uni0000004f/uni00000048/uni00000046/uni00000058/uni0000004f/uni00000044/uni00000055/uni00000003/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni00000033/uni00000052/uni0000004f/uni00000044/uni00000055/uni00000003/uni00000056/uni00000058/uni00000055/uni00000049/uni00000044/uni00000046/uni00000048/uni00000003/uni00000044/uni00000055/uni00000048/uni00000044\n/uni00000027/uni00000055/uni00000058/uni0000004a/uni00000003/uni00000053/uni00000055/uni00000052/uni00000053/uni00000048/uni00000055/uni00000057/uni0000005c/uni00000003/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000056\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013\n/uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni00000036/uni00000058/uni00000045/uni00000046/uni00000048/uni0000004f/uni0000004f/uni00000058/uni0000004f/uni00000044/uni00000055/uni00000003/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000052/uni0000004f/uni00000048/uni00000046/uni00000058/uni0000004f/uni00000044/uni00000055/uni00000003/uni00000049/uni00000058/uni00000051/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000004f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\n/uni00000030/uni00000052/uni0000004f/uni00000048/uni00000046/uni00000058/uni0000004f/uni00000044/uni00000055/uni00000003/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni0000002c/uni00000056/uni00000052/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000055/uni0000004c/uni00000046/uni00000003/uni00000053/uni00000052/uni0000004c/uni00000051/uni00000057\n/uni00000036/uni00000058/uni00000053/uni00000048/uni00000055/uni0000004e/uni0000004c/uni00000051/uni0000004a/uni00000047/uni00000052/uni00000050\n/uni00000033/uni00000055/uni00000052/uni00000057/uni00000048/uni0000004c/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000053/uni00000048/uni00000055/uni00000057/uni0000005c/uni00000003/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000056\nFigure 3: The sum of the absolute values for the\nweights of the drug and protein properties used by\nFastDTI on the STITCH dataset.\n4.2.3 Impact of Multimodality\nIn this experiment, we study the impact of multi-\nmodality to verify whether the additional modal-\nities of drug and protein properties would bene-\nfit the performance. Furthermore, the usefulness\nof each individual property is evaluated using the\nweights of the model. To do so, we compare the\nperformance of the model with and without prop-\nerties, and the effect of each property is examined\nfrom the weights that the model assigns to them\nduring the training. Hence, we use two configura-\ntions of the model for this experiment: FastDTI\nin its full form with five modalities and FastDTI\nwithout drug and protein properties. Each model\nis trained 10 times for 50 epochs on the STITCH\ndataset. Consequently, the weights of the proper-\nties in the trained model are extracted, by comput-\ning the sum of absolute values per each property.\nFor properties using a one-hot encoding, the sum is\ntaken over each dimension of the encoding as well.\nThe performance in terms of average MSE over\nthe 10 runs on the test set is 0 .104 without prop-\nerties and 0 .094 with properties, showing a 0 .010\n(with a standard deviation of 0 .003) advantage\nof incorporating the properties. In addition, Fig-\nure 3 displays the weights of the trained net-\nwork per property for drugs as well as proteins,\nwhich demonstrates that for the drug properties,\nthe sequence length has the highest weight, fol-\nlowed by the molecular weight, and finally the\npolar surface area. Note that the sequence\nlength for the drug is the length of the SMILES\nrepresentation. Meanwhile, for the proteins, the\nsubcellular location has the highest weight,\nand the superkingdom has the lowest weight by\na large margin, which denotes lower importance.\nThe results illustrate that the model decreases\nthe average error by 0.010 when additional modal-\nities of the drug and protein properties are incor-\nporated. This indicates that using properties en-\nhances the model’s predictive performance, which\nis a valuable addition to DTI prediction. Nonethe-\nless, the weights for the drug properties show some\nvariation in their usefulness. Interestingly, the\npolar surface area has the lowest weight, even\nthough it is hypothesized to be highly significant\nfor DTI prediction due to indicating the size of\nthe molecule. On the contrary, the molecular\nweight has a significant effect to the model, and the\nsequence length is even more so. Similarly, the\nresults for the protein properties show considerable\nvariance. The superkingdom has a relatively low\nweight, while subcellular location and molecu-\nlar function have high weights. This potentially\nindicates that the superkingdom does not provide\nany significant information that the model can use\nfor DTI prediction, unlike the other properties.\nTo summarize, it is explicit that the model bene-\nfits from additional properties. However, not every\nproperty is equally useful, therefore, there is the\npotential in future work to use expert knowledge\nto design better properties to further enhance the\nperformance of the model for DTI prediction.\n5 Conclusion\nWe presentedFastDTI, a novel approach to fast but\naccurate drug-target interaction prediction that\nleverages pretrained models to compute embed-\ndings of the input while including additional modal-\nities in the form of drug and protein properties.\nPrevious models either achieve a good performance\nat the cost of time complexity or a good train-\ning time at the cost of the performance. FastDTI\nhas a statistically significant improvement in per-\nformance compared to the state-of-the-arts while\n6\nbeing several orders of magnitude faster to train.\nAdditionally, FastDTI is the first deep learning DTI\nmodel to be trained on the STITCH dataset, which\nencompasses a far wider range of proteins, allowing\nfor a general DTI prediction model. The empirical\nresults indicate that our approach outperforms the\ncompetitors in terms of the predictive performance\non the small benchmark dataset, while greatly im-\nproving the computational complexity.\nIn the future, FastDTI can be improved by fur-\nther fine-tuning the pretrained models for the DTI\nprediction task, at the cost of extra training time,\nor by incorporating additional properties for the\ndrugs and proteins. Moreover, the application of\nFastDTI to other related problems, such as protein-\nprotein prediction, can be explored as future work.\nReferences\n[1] K. Abbasi, P. Razzaghi, A. Poso, S. Ghanbari-\nAra, and A. Masoudi-Nejad. Deep learning\nin drug target interaction prediction: Cur-\nrent and future perspectives. Current Medic-\ninal Chemistry , 28(11):2100–2113, 2021. doi:\n10.2174/0929867327666200907141016.\n[2] M. Abdel-Basset, H. Hawash, M. Elhoseny,\nR. K. Chakrabortty, and M. Ryan. DeepH-\nDTA: deep learning for predicting drug-target\ninteractions: a case study of covid-19 drug\nrepurposing. Ieee Access, 8:170433–170451,\n2020. doi: 10.1109/ACCESS.2020.3024238.\n[3] T. Ashburn and K. B. Thor. Drug reposition-\ning: identifying and developing new uses for\nexisting drugs. Nature reviews Drug discovery,\n3(8):673–683, 2004. doi: 10.1038/nrd1468.\n[4] L. Chen, X. Tan, D. Wang, F. Zhong,\nX. Liu, T. Yang, X. Luo, K. Chen, H. Jiang,\nand M. Zheng. Transformercpi: improv-\ning compound–protein interaction prediction\nby sequence-based deep learning with self-\nattention mechanism and label reversal ex-\nperiments. Bioinformatics, 36(16):4406–4414,\n2020. doi: 10.1093/bioinformatics/btaa524.\n[5] S. Chithrananda, G. Grand, and B. Ramsun-\ndar. Chemberta: Large-scale self-supervised\npretraining for molecular property prediction.\narXiv preprint arXiv:2010.09885 , 2020.\n[6] J. Devlin, M. Chang, K. Lee, and\nK. Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805 ,\n2018.\n[7] A. Dhakal, C. McKay, J. J. Tanner, and\nJ. Cheng. Artificial intelligence in the pre-\ndiction of protein–ligand interactions: recent\nadvances and future directions. Briefings in\nBioinformatics, 23(1):bbab476, 2022. doi: 10.\n1093/bib/bbab476.\n[8] A. Elnaggar, M. Heinzinger, C. Dallago,\nG. Rehawi, Y. Wang, L. Jones, T. Gibbs,\nT. Feher, et al. ProtTrans: towards crack-\ning the language of life’s code through self-\nsupervised learning. bioRxiv, pages 2020–07,\n2021. doi: 10.1101/2020.07.12.199554.\n[9] A. Ezzat. Challenges and solutions in drug-\ntarget interaction prediction. PhD thesis, 2018.\n[10] T. He, M. Heidemeyer, F. Ban, A. Cherkasov,\nand M. Ester. Simboost: a read-across\napproach for predicting drug–target binding\naffinities using gradient boosting machines.\nJournal of cheminformatics , 9(1):1–14, 2017.\ndoi: 10.1186/s13321-017-0209-z.\n[11] K. Koyama, K. Kamiya, and K. Shimada.\nCross attention DTI: drug-target interaction\nprediction with cross attention module in the\nblind evaluation setup. 2020.\n[12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi,\nD. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V. Stoyanov. Roberta: A robustly op-\ntimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[13] H. Lodish, A. Berk, C. A. Kaiser, C. Kaiser,\nM. Krieger, M. P. Scott, A. Bretscher,\nH. Ploegh, P. Matsudaira, et al. Molecular\ncell biology. Macmillan, 2008.\n[14] G. M. Morris and M. Lim-Wilby. Molecular\ndocking. In Molecular modeling of proteins ,\npages 365–382. Springer, 2008. doi: 10.1007/\n978-1-59745-177-2 19.\n[15] T. Nguyen, H. Le, T. Quinn, T. Nguyen, T. D.\nLe, and S. Venkatesh. GraphDTA: Predicting\n7\ndrug–target binding affinity with graph neural\nnetworks. Bioinformatics, 37(8):1140–1147,\n2021. doi: 10.1093/bioinformatics/btaa921.\n[16] H. ¨Ozt¨ urk, A.¨Ozg¨ ur, and E. Ozkirimli. Deep-\nDTA: deep drug–target binding affinity predic-\ntion. Bioinformatics, 34(17):i821–i829, 2018.\ndoi: 10.1093/bioinformatics/bty593.\n[17] T. Pahikkala, A. Airola, S. Pietil¨ a,\nS. Shakyawar, A. Szwajda, J. Tang, and\nT. Aittokallio. Toward more realistic drug–\ntarget interaction predictions. Briefings in\nbioinformatics, 16(2):325–337, 2015. doi:\n10.1093/bib/bbu010.\n[18] J. Piret and G. Boivin. Pandemics throughout\nhistory. Frontiers in microbiology, 11:631736,\n2021. doi: 10.3389/fmicb.2020.631736.\n[19] P. Pratim Roy, S. Paul, I. Mitra, and K. Roy.\nOn two novel parameters for validation of pre-\ndictive qsar models. Molecules, 14(5):1660–\n1701, 2009. doi: 10.3390/molecules14051660.\n[20] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei,\nW. Huang, and J. Huang. Self-supervised\ngraph transformer on large-scale molecular\ndata. Advances in Neural Information Pro-\ncessing Systems, 33:12559–12571, 2020.\n[21] S. Z. Sajadi, M. A. Zare Chahooki,\nS. Gharaghani, and K. Abbasi. Autodti++:\ndeep unsupervised learning for dti prediction\nby autoencoders. BMC bioinformatics, 22(1):\n1–19, 2021. doi: 10.1186/s12859-021-04127-2.\n[22] S. Z. Sajadi, M. A. Zare Chahooki, M. Tavakol,\nand S. Gharaghani. Matrix factorization with\ndenoising autoencoders for prediction of drug–\ntarget interactions. Molecular Diversity, pages\n1–11, 2022. doi: 10.1007/s11030-022-10492-8.\n[23] D. Szklarczyk, A. Santos, C. Von Mering, L. J.\nJensen, P. Bork, and M. Kuhn. Stitch 5:\naugmenting protein–chemical interaction net-\nworks with tissue and affinity data. Nucleic\nacids research, 44(D1):D380–D384, 2016. doi:\n10.1093/nar/gkv1277.\n[24] J. Tang, A. Szwajda, S. Shakyawar, T. Xu,\nP. Hintsanen, K. Wennerberg, and T. Ait-\ntokallio. Making sense of large-scale kinase\ninhibitor bioactivity data sets: a comparative\nand integrative analysis. Journal of Chemi-\ncal Information and Modeling , 54(3):735–743,\n2014. doi: 10.1021/ci400709d.\n[25] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui,\nand P. S. Yu. Heterogeneous graph attention\nnetwork. In The world wide web conference ,\npages 2022–2032, 2019. doi: 10.1145/3308558.\n3313562.\n[26] D. Weininger. Smiles, a chemical language\nand information system. 1. introduction to\nmethodology and encoding rules. Journal of\nchemical information and computer sciences ,\n28(1):31–36, 1988. doi: 10.1021/ci00057a005.\n[27] M. Wen, Z. Zhang, S. Niu, H. Sha, R. Yang,\nY. Yun, and H. Lu. Deep-learning-based drug–\ntarget interaction prediction. Journal of pro-\nteome research, 16(4):1401–1409, 2017. doi:\n10.1021/acs.jproteome.6b00618.\n[28] K. W¨ uthrich. Protein structure determina-\ntion in solution by nmr spectroscopy. Journal\nof Biological Chemistry, 265(36):22059–22062,\n1990. doi: 10.1016/S0021-9258(18)45665-7.\n[29] Z. Yang, W. Zhong, L. Zhao, and C. Y.-\nC. Chen. Ml-dti: Mutual learning mecha-\nnism for interpretable drug–target interaction\nprediction. The Journal of Physical Chem-\nistry Letters , 12(17):4247–4261, 2021. doi:\n10.1021/acs.jpclett.1c00867.\n[30] W. Yuan, G. Chen, and C. Y.-C. Chen.\nFusiondta: attention-based feature polymer-\nizer and knowledge distillation for drug-target\nbinding affinity prediction. Briefings in Bioin-\nformatics, 23(1):bbab506, 2022. doi: 10.1093/\nbib/bbab506.\n[31] L. Zhang, J. Tan, D. Han, and H. Zhu. From\nmachine learning to deep learning: progress in\nmachine intelligence for rational drug discov-\nery. Drug discovery today , 22(11):1680–1685,\n2017. doi: 10.1016/j.drudis.2017.08.010.\n[32] P. Zhang, Z. Wei, C. Che, and B. Jin.\nDeepmgt-dti: Transformer network incorpo-\nrating multilayer graph information for drug–\ntarget interaction prediction. Computers in\nBiology and Medicine, page 105214, 2022. doi:\n10.1016/j.compbiomed.2022.105214.\n8",
  "institutions": [
    {
      "id": "https://openalex.org/I83019370",
      "name": "Eindhoven University of Technology",
      "country": "NL"
    }
  ]
}