{
    "title": "Line Segment Detection Using Transformers without Edges",
    "url": "https://openalex.org/W3118587513",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1550480070",
            "name": "Xu, Yifan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2640668307",
            "name": "Xu Weijian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2614804836",
            "name": "Cheung David",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2572082798",
            "name": "Tu, Zhuowen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2955186028",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2129587342",
        "https://openalex.org/W2040260621",
        "https://openalex.org/W2119823327",
        "https://openalex.org/W2095905764",
        "https://openalex.org/W1964005581",
        "https://openalex.org/W2798943056",
        "https://openalex.org/W2165914352",
        "https://openalex.org/W2804078698",
        "https://openalex.org/W2146310974",
        "https://openalex.org/W2294634632",
        "https://openalex.org/W845365781",
        "https://openalex.org/W2099046646",
        "https://openalex.org/W2953106684",
        "https://openalex.org/W2981415272",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2155912251",
        "https://openalex.org/W2122006243",
        "https://openalex.org/W2145158371",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2160072137",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2081356243",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W1498183729",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W3035228270",
        "https://openalex.org/W2953809838",
        "https://openalex.org/W2145023731",
        "https://openalex.org/W2963351448"
    ],
    "abstract": "In this paper, we present a joint end-to-end line segment detection algorithm using Transformers that is post-processing and heuristics-guided intermediate processing (edge/junction/region detection) free. Our method, named LinE segment TRansformers (LETR), takes advantages of having integrated tokenized queries, a self-attention mechanism, and an encoding-decoding strategy within Transformers by skipping standard heuristic designs for the edge element detection and perceptual grouping processes. We equip Transformers with a multi-scale encoder/decoder strategy to perform fine-grained line segment detection under a direct endpoint distance loss. This loss term is particularly suitable for detecting geometric structures such as line segments that are not conveniently represented by the standard bounding box representations. The Transformers learn to gradually refine line segments through layers of self-attention. In our experiments, we show state-of-the-art results on Wireframe and YorkUrban benchmarks.",
    "full_text": "Line Segment Detection Using Transformers without Edges\nYifan Xu*, Weijian Xu *, David Cheung, Zhuowen Tu\nUniversity of California San Diego\n{yix081,wex041,d6cheung,ztu}@ucsd.edu\nAbstract\nIn this paper, we present a joint end-to-end line seg-\nment detection algorithm using Transformers that is post-\nprocessing and heuristics-guided intermediate processing\n(edge/junction/region detection) free. Our method, named\nLinE segment TRansformers (LETR), takes advantages of\nhaving integrated tokenized queries, a self-attention mech-\nanism, and encoding-decoding strategy within Transform-\ners by skipping standard heuristic designs for the edge ele-\nment detection and perceptual grouping processes. We equip\nTransformers with a multi-scale encoder/decoder strategy to\nperform ﬁne-grained line segment detection under a direct\nendpoint distance loss. This loss term is particularly suitable\nfor detecting geometric structures such as line segments that\nare not conveniently represented by the standard bounding\nbox representations. The Transformers learn to gradually\nreﬁne line segments through layers of self-attention. In our\nexperiments, we show state-of-the-art results on Wireframe\nand YorkUrban benchmarks.\n1. Introduction\nLine segment detection is an important mid-level visual\nprocess [22] useful for solving various downstream computer\nvision tasks, including segmentation, 3D reconstruction, im-\nage matching and registration, depth estimation, scene under-\nstanding, object detection, image editing, and shape analysis.\nDespite its practical and scientiﬁc importance, line segment\ndetection remains an unsolved problem in computer vision.\nAlthough dense pixel-wise edge detection has achieved\nan impressive performance [32], reliably extracting line seg-\nments of semantic and perceptual signiﬁcance remains a\nfurther challenge. In natural scenes, line segments of inter-\nest often have heterogeneous structures within the cluttered\nbackground that are locally ambiguous or partially occluded.\nMorphological operators [ 27] operated on detected edges\n[3] often give sub-optimal results. Mid-level representations\nsuch as Gestalt laws [10] and contextual information [28] can\n* indicates equal contribution.\nCode: https://github.com/mlpc-ucsd/LETR.\n(b) TransformerEncoderTransformerDecoderFFNFFNFFNBackboneImage\nPositionalEncoding\nFeatureExtractorImage(a) Line SegmentProposals\nJunctions\nRefinedProposals LoIPooling\nLoIFeaturesLine Segments\nLine Segments\nFigure 1. Pipeline comparison between: (a) holistically-attracted wire-\nframe parsing (HAWP) [34] and (b) our proposed LinE segment TRans-\nformers (LETR). LETR is based on a general-purpose pipeline without\nheuristics-driven intermediate stages for detecting junctions and generating\nline segment proposals.\nplay an important role in the perceptual grouping, but they\nare often hard to be seamlessly integrated into an end-to-end\nline segment detection pipeline. Deep learning techniques\n[16, 20, 14, 32] have provided greatly enhanced feature repre-\nsentation power, and algorithms such as [37, 33, 34] become\nincreasingly feasible in real-world applications. However,\nsystems like [37, 33, 34] still consist of heuristics-guided\nmodules [27] such as edge/junction/region detection, line\ngrouping, and post-processing, limiting the scope of their\nperformance enhancement and further development.\nIn this paper, we skip the traditional edge/junction/region\ndetection + proposals + perceptual grouping pipeline by de-\nsigning a Transformer-based [ 29, 4] joint end-to-end line\nsegment detection algorithm. We are motivated by the fol-\nlowing observations for the Transformer frameworks [29, 4]:\ntokenized queries with an integrated encoding and decoding\nstrategy, self-attention mechanism, and bipartite (Hungarian)\nmatching step, capable of addressing the challenges in line\nsegment detection for edge element detection, perceptual\ngrouping, and set prediction; general-purpose pipelines for\nTransformers that are heuristics free. Our system, named\nLinE segment TRsformer (LETR), enjoys the modeling\npower of a general-purpose Transformer architecture while\nhaving its own enhanced property for detecting ﬁne-grained\ngeometric structures like line segments. LETR is built on\ntop of a seminal work, DEtection TRansformer (DETR)\n[4]. However, as shown in Section 4.4 for ablation stud-\narXiv:2101.01909v2  [cs.CV]  30 Apr 2021\nies, directly applying the DETR object detector [4] for line\nsegment detection does not yield satisfactory results since\nline segments are elongated geometric structures that are not\nfeasible for the bounding box representations.\nOur contributions are summarized as follows.\n• We cast the line segment detection problem in a joint\nend-to-end fashion without explicit edge/junction/region\ndetection and heuristics-guided perceptual grouping pro-\ncesses, which is in distinction to the existing literature in\nthis domain. We achieve state-of-the-art results on the\nWireframe [15] and YorkUrban benchmarks [5].\n• We perform line segment detection using Transformers,\nbased speciﬁcally on DETR [4], to realize tokenized entity\nmodeling, perceptual grouping, and joint detection via an\nintegrated encoder-decoder, a self-attention mechanism,\nand joint query inference within Transformers.\n• We introduce two new algorithmic aspects to DETR [4]:\nﬁrst, a multi-scale encoder/decoder strategy as shown in\nFigure 2; second, a direct endpoint distance loss term in\ntraining, allowing geometric structures like line segments\nto be directly learned and detected — something not feasi-\nble in the standard DETR bounding box representations.\n2. Related Works\n2.1. Line Segment Detection\nTraditional Approaches. Line detection has a long his-\ntory in computer vision. Early pioneering works rely on\nlow-level cues from pre-deﬁned features (e.g. image gra-\ndients). Typically, line (segment) detection performs edge\ndetection [3, 23, 7, 8, 32], followed by a perceptual grouping\n[13, 27, 10] process. Classic perceptual grouping frame-\nworks [ 2, 1, 25, 21, 30] aggregate the low-level cues to\nform line segments in a bottom-up fashion: an image is\npartitioned into line-support regions by grouping similar\npixel-wise features. Line segments are then approximated\nfrom line-support regions and ﬁltered by a validation step to\nremove false positives. Another popular series of line seg-\nment detection approaches are based on Hough transform\n[9, 13, 24, 12] by gathering votes in the parameter space: the\npixel-wise edge map of an image is converted into a param-\neter space representation, in which each point corresponds\nto a unique parameterized line. The points in the parameter\nspace that accumulate sufﬁcient votes from the candidate\nedge pixels are identiﬁed as line predictions. However, due\nto the limitations in the modeling/inference processes, these\ntraditional approaches often produce sub-optimal results.\nDeep Learning Based Approaches. The recent surge\nof deep learning based approaches has achieved much-\nimproved performance on the line segment detection prob-\nlem [15, 33, 37, 36, 34] with the use of learnable features to\ncapture extensive context information. One typical family\nof methods is junction-based pipelines: Deep Wireframe\nParser (DWP) [15] creates two parallel branches to predict\nthe junction heatmap and the line heatmap, followed by a\nmerging procedure. Motivated by [26], L-CNN [37] simpli-\nﬁes [15] into a uniﬁed network. First, a junction proposal\nmodule produces the junction heatmap and then converts\ndetected junctions into line proposals. Second, a line veri-\nﬁcation module classiﬁes proposals and removes unwanted\nfalse-positive lines. Methods like [ 37] are end-to-end, but\nthey are at the instance-level (for detecting the individual line\nsegments). Our LETR, like DETR [4], has a general-purpose\narchitecture that is trained in a holistically end-to-end fash-\nion. PPGNet [36] proposes to create a point-set graph with\njunctions as vertices and model line segments as edges. How-\never, the aforementioned approaches are heavily dependent\non high-quality junction detection, which is error-prone to\nvarious imaging conditions and complex scenarios.\nAnother line of approaches employs dense prediction to\nobtain a surrogate representation map and applies a post-\nprocess procedure to extract line segments: AFM [33] pro-\nposes an attraction ﬁeld map as an intermediate representa-\ntion that contains 2-D projection vectors pointing to associ-\nated lines. A squeeze module then recovers vectorized line\nsegments from the attraction ﬁeld map. Despite a relatively\nsimpler design, [33] demonstrates its inferior performance\ncompared with junction-based approaches. Recently, HAWP\n[34] builds a hybrid model of AFM [33], and L-CNN [37]\nby computing line segment proposals from the attraction\nﬁeld map and then reﬁning proposals with junctions before\nfurther line veriﬁcation.\nIn contrast, as shown in Figure 1, our approach differs\nfrom previous methods by removing heuristics-driven inter-\nmediate stages for detecting edge/junction/region proposals\nand surrogate prediction maps. Our approach is able to\ndirectly predict vectorized line segments while keeping com-\npetitive performances under a general-purpose framework.\n2.2. Transformer Architecture\nTransformers [29] have achieved great success in the nat-\nural language processing ﬁeld and become de facto standard\nbackbone architecture for many language models [ 29, 6].\nIt introduces self-attention and cross-attention modules as\nbasic building blocks, modeling dense relations among ele-\nments of the input sequence. These attention-based mecha-\nnisms also beneﬁt many vision tasks such as video classiﬁca-\ntion [31], semantic segmentation [11], image generation [35],\netc. Recently, end-to-end object detection with Transform-\ners (DETR) [4] reformulates the object detection pipeline\nwith Transformers by eliminating the need for hand-crafted\nanchor boxes and non-maximum suppression steps. Instead,\n[4] proposes to feed a set of object queries into the encoder-\ndecoder architecture with interactions from the image feature\nsequence and generate a ﬁnal set of predictions. A bipar-\ntite matching objective is then optimized to force unique\nSelf-AttentionCross-AttentionFeed-Forward\nDecoding Layer 1 DecodingLayer 2\n...Decoding Layer 6\nFFN\nDecodingLayer 2\n...DecodingLayer 6DecodingLayer 1Fine DecoderCoarse Decoder\nSelf-AttentionFeed-Forward\nEncoding Layer 1Encoding Layer 2\n...\nCoarse EncoderEncoding Layer 6 Encoding Layer 2\n...\nFine EncoderEncoding Layer 6Encoding Layer 1\nFFNFFNFFN\nBackbone\nInitialLineEntities Detected Line SegmentsImage\nCoarseFeaturesPositional Encoding Positional EncodingFineFeatures\nInterm.LineEntities Interm.LineEntities FinalLineEntities\nFigure 2. Schematic illustration of our LETR pipeline: An image is fed into a backbone network and generates two feature maps, which\nare then used by the coarse and the ﬁne encoder respectively. Initial line entities are then ﬁrst reﬁned by the coarse decoder with the\ninteraction of the coarse encoder output, and then the intermediate line entities from the coarse decoder are further reﬁned by the ﬁne decoder\nattending to the ﬁne encoder. Finally, line segments are detected by feed-forward networks (FFNs) on top of line entities.\nassignments between predictions and targets.\nWe introduce two new aspects to DETR [4] when realiz-\ning our LETR: 1) multi-scale encoder and decoder; 2) direct\ndistance loss for the line segments.\n3. Line Segment Detection with Transformers\n3.1. Motivation\nFigure 3. Bounding box representation. Three difﬁcult cases to represent\nline segments using bounding box diagonals. Red lines, black boxes, and\ngray dotted boxes refer to as line segments, the corresponding bounding\nboxes, and anchors respectively.\nDespite the exceptional performance achieved by the re-\ncent deep learning based approaches [ 37, 33, 34] on line\nsegment detection, their pipelines still involve heuristics-\ndriven intermediate representations such as junctions and\nattraction ﬁeld maps, raising an interesting question: Can\nwe directly model all the vectorized line segments with a\nneural network? A naive solution could be simply regarding\nthe line segments as objects and building a pipeline follow-\ning the standard object detection approaches [ 26]. Since\nthe location of 2-D objects is typically parameterized as a\nbounding box, the vectorized line segment can be directly\nread from a diagonal of the bounding box associated with\nFigure 4. Line entity representation. For each row, we show how\na same line entity predicts line segments with same property in three\ndifferent indoor/outdoor scenes. The top line entity is specialized\nfor horizontal line segments in the middle of the ﬁgure, and the\nbottom one prefers to predict vertical line segments with a various\nrange of lengths.\nthe line segment object. However, the limited choices of an-\nchors make it difﬁcult for standard two-stage object detectors\nto predict very short line segments or line segments nearly\nparallel to the axes (see Figure 3). The recently appeared\nDETR [4] eliminates the anchors and the non-maximum sup-\npression, perfectly meets the need of line segment detection.\nHowever, the vanilla DETR still focuses on bounding box\nrepresentation with a GIoU loss. We further convert the box\npredictor in DETR into a vectorized line segment predictor\nby adapting the losses and enhancing the use of multi-scale\nfeatures in our designed model.\n3.2. Overview\nIn a line segment detection task, a detector aims to pre-\ndict a set of line segments from given images. Performing\nline segment detection with Transformers removes the need\nof explicit edge/junction/region detection [37, 34] (see Fig-\nure 1). Our LETR is built purely based on the Transformer\nencoder-decoder structure. The proposed line segment de-\ntection process consists of four stages:\n(1) Image Feature Extraction: Given an image input, we\nobtain the image feature map x ∈RH×W×C from a CNN\nbackbone with reduced dimension. The image feature is\nconcatenated with positional embeddings to obtain spatial\nrelations. (2) Image Feature Encoding: The ﬂattened feature\nmap x ∈RHW×C is then encoded to x′ ∈RHW×C by a\nmulti-head self-attention module and a feed forward network\nmodule following the standard Transformer encoding archi-\ntecture. (3) Line Segment Detection: In the Transformer\ndecoder networks, N learnable line entities l ∈RN×C inter-\nact with the encoder output via the cross-attention module.\n(4) Line Segment Prediction: Line entities make line seg-\nment predictions with two prediction heads built on top of\nthe Transformer decoder. The line coordinates are predicted\nby a multi-layer perceptron (MLP), and the prediction conﬁ-\ndences are scored by a linear layer.\nSelf-Attention and Cross-Attention. We ﬁrst visit the\nscaled dot-product attention popularized by Transformer\narchitectures [29]. The basic scaled dot-product attention\nconsists of a set of mqueries Q ∈Rm×d, a set of nkey-\nvalue pairs notated as a key matrix K ∈Rn×d and a value\nmatrix V ∈Rn×d. Here we set Q, K, V to have same\nfeature dimension d. The attention operation F is deﬁned as:\nF = Att(Q,K,V ) =softmax(QKT\n√\nd\n)V (1)\nIn our encoder-decoder Transformer architecture, we\nadopt two attention modules based on the multi-head at-\ntention, namely the self-attention module (SA) and cross-\nattention (CA) module (see Figure 2). The SA mod-\nule takes in a set of input embeddings notated as x =\n[x1,...,x i] ∈ Ri×d, and outputs a weighted summation\nx′ = [x′\n1,...,x ′\ni] ∈ Ri×d of input embeddings within x\nfollowing Eq.1 where F = Att(Q = x,K = x,V = x).\nThe CA module takes in two sets of input embeddings no-\ntated as x = [x1,...,x i] ∈Ri×d, z = [x1,...,x j] ∈Rj×d\nfollowing Eq.1 where F = Att(Q= z,K = x,V = x).\nTransformer Encoder in LETR is stacked with multiple\nencoder layers. Each encoder layer takes in image features\nx ∈RHW×c from its predecessor encoder layer and pro-\ncesses it with a SA module to learn the pairwise relation.\nThe output features from SA module are passed into a point-\nwise fully-connected layer (FC) with activation and dropout\nlayer followed by another point-wise fully-connected (FC)\nlayer. Layer norm is applied between SA module and ﬁrst\nFC layer and after second FC layer. Residual connection is\nadded before the ﬁrst FC layer and after the second FC layer\nto facilitate optimization of deep layers.\nTransformer Decoder in LETR is stacked with multiple\ndecoder layers. Each decoder layer takes in a set of image\nfeatures x′ ∈RHW×C from the last encoder layer and a\nset of line entities l ∈RN×C from its predecessor decoder\nlayer. The line entities are ﬁrst processed with a SA module,\neach line entity l ∈RC in l attends to different regions of\nimage feature embeddings x′via the CA module. FC layers\nand other modules are added into the pipeline similar to the\nEncoder setting above.\nLine Entity Interpretation. The line entities are analogous\nwith the object queries in DETR [4]. We found each line\nentity has its own preferred existing region, length, and ori-\nentation of potential line segment after the training process\n(shown in Figure 4). We discuss line entities together make\nbetter predictions through self-attention and cross-attention\nreﬁnement when encountering heterogeneous line segment\nstructures in Section 4.4 and Figure 5.\n3.3. Coarse-to-Fine Strategy\nDifferent from object detection, line segment detection\nrequires the detector to consider the local ﬁne-grained details\nof line segments with the global indoor/outdoor structures\ntogether. In our LETR architecture, we propose a coarse-to-\nﬁne strategy to predict line segments in a reﬁnement process.\nThe process allows line entities to make precise predictions\nwith the interaction of multi-scale encoded features while\nhaving an awareness of the holistic architecture with the\ncommunication to other line entities. During the coarse de-\ncoding stage, our line entities attend to potential line segment\nregions, often unevenly distributed, with a low resolution.\nDuring the ﬁne decoding stage, our line entities produce\ndetailed line segment predictions with a high resolution (see\nFigure 2). After each decoding layer at both coarse and ﬁne\ndecoding stage, we require line entities to make predictions\nthrough two shared prediction heads to make more precise\npredictions gradually.\nCoarse Decoding. During the coarse decoding stage, we\npass image features and line entities into an encoder-decoder\nTransformer architecture. The encoder receives coarse fea-\ntures from the output of Conv5 (C5) from ResNet with 1\n32\noriginal resolution. Then, line entity embeddings attend to\ncoarse features from the output of the encoder in the cross-\nattention module at each layer. The coarse decoding stage\nis necessary for success at ﬁne decoding stage and its high\nefﬁciency with less memory and computation cost.\nFine Decoding. The ﬁne decoder inherits line entities from\nthe coarse decoder and high-resolution features from the ﬁne\nencoder. The features to the ﬁne encoder come from the out-\nput of Conv4 (C4) from ResNet with 1\n16 original resolution.\nThe line entity embeddings decode feature information in\nthe same manner as the coarse decoding stage.\n3.4. Line Segment Prediction\nIn the previous decoding procedure, our multi-scale de-\ncoders progressively reﬁne N initial line entities to produce\nsame amount ﬁnal line entities. In the prediction stage. Each\nﬁnal entity lwill be fed into a feed-forward network (FFN),\nwhich consists of a classiﬁer module to predict the conﬁ-\ndence pof being a line segment, and a regression module\nto predict the coordinates of two end points ˆp1 = (ˆx1,ˆy1),\nˆp2 = (ˆx2,ˆy2) that parameterizes the associated line segment\nˆL = (ˆp1,ˆp2).\nBipartite Matching. Generally, there are many more line\nentities provided than actual line segments in the image.\nThus, during the training stage, we conduct a set-based bipar-\ntite matching between line segment predictions and ground-\ntruth targets to determine whether the prediction is associated\nwith an existing line segment or not: Assume there are N\nline segment predictions {(p(i),ˆL(i)); i= 1,...,N }and M\ntargets {L(j); j = 1,...,M }, we optimize a bipartite match-\ning objective on a permutation function σ(·) : Z+ →Z+\nwhich maps prediction indices {1,...,N }to potential target\nindices {1,...,N }(including {1,...,M }for ground-truth\ntargets and {M + 1,...,N }for unmatched predictions):\nLmatch =\nN∑\ni=1\n1{σ(i)≤M}\n[\nλ1d(ˆL(i),L(σ(i))) −λ2p(i)] (2)\nσ∗= arg min\nσ\nLmatch (3)\nwhere d(·,·) represents L1 distance between coordinates and\n1{·}is an indicator function. Lmatch takes both distance and\nconﬁdence into account with balancing coefﬁcients λ1,λ2.\nThe optimal permutation σ∗is computed using a Hungarian\nalgorithm, mapping M positive prediction indices to target\nindices {1,...,M }. During the inference stage, we ﬁlter the\nNline segment predictions by setting a ﬁxed threshold on the\nconﬁdence p(i) if needed due to no ground-truth provided.\n3.5. Line Segment Losses\nWe compute line segment losses based on the optimal per-\nmutation σ∗from the bipartite matching procedure, in which\n{i; σ∗(i) ≤M}represents indices of positive predictions.\nClassiﬁcation Loss. Based on a binary cross-entropy loss,\nwe observe that hard examples are less optimized after learn-\ning rate decay and decide to apply adaptive coefﬁcients in-\nspired by focal loss [18] to the classiﬁcation loss term Lcls:\nL(i)\ncls = −1{σ∗(i)≤M}α1(1 −p(i))γlog p(i) (4)\n−1{σ∗(i)>M}α2p(i)γ\nlog(1 −p(i)) (5)\nDistance Loss. We compute a simple L1-based distance\nloss for line segment endpoint regression:\nL(i)\ndist = 1{σ∗(i)≤M}d(ˆL(i),L(σ∗(i))) (6)\nwhere d(·,·) represents the sum of L1 distances between\nprediction and target coordinates. The distance loss is only\napplied to the positive predictions. Note that we remove the\nGIoU loss from [4] since GIoU is mainly designed for the\nsimilarity between bounding boxes instead of line segments.\nThus, the ﬁnal loss Lof our model is formulated as:\nL=\nN∑\ni=1\nλclsL(i)\ncls + λdistL(i)\ndist (7)\n4. Experiments\n4.1. Datasets\nWe train and evaluate our model on the ShanghaiTech\nWireframe dataset [15], which consists of 5000 training im-\nages and 462 testing images. We also evaluate our model on\nthe YorkUrban dataset [5] with 102 testing images from both\nindoor scenes and outdoor scenes.\nThrough all experiments, we conduct data augmentations\nfor the training set, including random horizontal/vertical ﬂip,\nrandom resize, random crop, and image color jittering. At\nthe training stage, we resize the image to ensure the shortest\nsize is at least 480 and at most 800 pixels while the longest\nsize is at most 1333. At the evaluation stage, we resize the\nimage with the shortest side at least 1100 pixels.\n4.2. Implementation\nNetworks. We adopt both ResNet-50 and ResNet-101 as\nour feature backbone. For an input image X ∈RH0×W0×3,\nthe coarse encoder takes in the feature map from the\nConv5 (C5) layer of ResNet backbone with resolution\nx ∈ RH×W×C where H = H0\n32 ,W = W0\n32 ,C = 2048.\nThe ﬁne encoder takes in a higher resolution feature map\n(H = H0\n16 ,W = W0\n16 ,C = 1024) from the Conv4 (C4) layer\nof ResNet. Feature maps are reduced to 256 channels by\na 1x1 convolution and are fed into the Transformer along\nwith the sine/cosine positional encoding. Our coarse-to-ﬁne\nstrategy consists of two independent encoder-decoder struc-\ntures processing multi-scale image features. Each encoder-\ndecoder structure is constructed with 6 encoder and 6 de-\ncoder layers with 256 channels and 8 attention heads.\nOptimization. We train our model using 4 Titan RTX\nGPUs through all our experiments. Model weights from\nDETR [4] with ResNet-50 and ResNet-101 backbone are\nloaded as pre-training, and we discuss the effectiveness of\npre-training in Section 5. We ﬁrst train the coarse encoder-\ndecoder for 500 epochs until optimal. Then, we freeze the\nweights in the coarse Transformer and train the ﬁne Trans-\nformer initialized by coarse Transformer weights for 325\nepochs (including a 25-epoch focal-loss ﬁne-tuning). We\nadopt deep supervision [ 17, 32] for all decoder layers fol-\nlowing DETR [4]. FFN prediction head weights are shared\nthrough all decoder layers. We use AdamW as the model\n(a) AFM [33]\n (b) LCNN [37]\n (c) HAWP [34]\n (d) LETR (ours)\n (e) Ground-Truth\nFigure 5. Qualitative evaluation of line detection methods. From left to right: the columns are the results from AFM [33], LCNN [37],\nHAWP [34], LETR (ours) and the ground-truth. From top to bottom: the top two rows are the results from the Wireframe test set, and the\nbottom two rows are the results from the YorkUrban test set.\noptimizer and set weight decay as 10−4. The initial learning\nrate is set to 10−4 and is reduced by a factor of 10 every 200\nepochs for the coarse decoding stage and every 120 epochs\nfor the ﬁne prediction stage. We use 1000 line entities in all\nreported benchmarks unless speciﬁed elsewhere. To mitigate\nthe class imbalance issue, we also reduce the classiﬁcation\nweight for background/no-object instances by a factor of 10.\n4.3. Evaluation Metric\nWe evaluate our results based on two heatmap-based met-\nrics, APH and FH, which are widely used in previous LSD\ntask[37, 15], and Structural Average Precision (sAP) which\nis proposed in L-CNN [37]. On top of that, we evaluate the\nresult with a new metric, Structural F-score (sF), for a more\ncomprehensive comparison.\nHeatmap-based metrics, APH, FH: Prediction and ground\ntruth lines are ﬁrst converted to heatmaps by rasterizing the\nlines, and we generate the precision-recall curve comparing\neach pixel along with their conﬁdence. Then we can use the\ncurve to calculate FH and APH.\nStructural-based metrics, sAP[37], sF: Given a set of ground\ntruth line and a set of predicted lines, for each ground-truth\nline L, we deﬁne a predicted line ˆL to be a match of L if\ntheir L2 distance is smaller than the pre-deﬁned threshold\nϑ∈{10,15}. Over the set of lines matched to L, we select\nthe line with the highest conﬁdence as a true positive and\ntreat the rest as candidates for false positives. If the set of\nmatching lines is empty, we would regard this ground-truth\nline as false negative. Each predicted line would be matched\nto at most one ground truth line, and if a line isn’t matched\nto any ground-truth line, then it is considered as a false\npositive. The matching is recomputed at each conﬁdence\nTable 1. Comparison to prior work on Wireframe and YorkUrban benchmarks. Our proposed LETR reaches state-of-the-art perfor-\nmance except sAP10 and sAP15 slightly worse than HAWP [34] in Wireframe. FPS Results for LETRs are tested on a single Tesla V100.\nResults for other prior works are adopted from HAWP paper.\nMethod Wireframe Dataset YorkUrban Dataset FPS\nsAP10 sAP15 sF10 sF15 APH FH sAP10 sAP15 sF10 sF15 APH FH\nLSD [30] / / / / 55.2 62.5 / / / / 50.9 60.1 49.6\nDWP [15] 5.1 5.9 / / 67.8 72.2 2.1 2.6 / / 51.0 61.6 2.24\nAFM [33] 24.4 27.5 / / 69.2 77.2 9.4 11.1 / / 48.2 63.3 13.5\nL-CNN [37] 62.9 64.9 61.3 62.4 82.8 81.3 26.4 27.5 36.9 37.8 59.6 65.3 15.6\nHAWP [34] 66.5 68.2 64.9 65.9 86.1 83.1 28.5 29.7 39.7 40.5 61.2 66.3 29.5\nLETR(ours) 65.2 67.7 65.8 67.1 86.3 83.3 29.4 31.7 40.1 41.8 62.7 66.9 5.04\nFigure 6. Precision-Recall (PR) curves. PR curves of sAP15 and APH for DWP[15], AFM[33], L-CNN[37], HAWP[34] and LETR (ours)\non Wireframe and YorkUrban benchmarks.\nlevel to produce the precision-recall curve, and we consider\nsAP as the area under this curve. Considering FH as the\ncomplementary F-score measurement for APH, we evaluate\nthe F-score measurement for sAP, denoted as sF, to be the\nbest balanced performance measurement.\n4.4. Results and Comparisons\nWe summarize quantitative comparison results between\nLETR and previous line segment detection methods in Table\n1. We report results for LETR with ResNet-101 backbone\nfor Wireframe dataset and results with ResNet-50 backbone\nfor York dataset. Our LETR achieves new state-of-the-art for\nall evaluation metrics on YorkUrban Dataset [5]. In terms of\nheatmap-based evaluation metrics, our LETR is consistently\nbetter than other models for both benchmarks and outper-\nforms HAWP [34] by 1.5 for APH on YorkUrban Dataset.\nWe show PR curve comparison in Figure 6 on sAP15 and\nAPH for both Wireframe [15] and YorkUrban benchmarks.\nIn Figure 6, we notice the current limitation of LETR comes\nfrom lower precision prediction when we include fewer pre-\ndictions compare to HAWP. When we include all sets of\npredictions, LETR predicts slightly better than HAWP and\nother leading methods, which matches our hypothesis that\nholistic prediction fashion can guide line entities to reﬁne\nlow conﬁdent predictions (usually due to local ambiguity\nand occlusion) with high conﬁdent predictions.\nWe also show both Wireframe and YorkUrban line seg-\nment detection qualitative results from LETR and other com-\npeting methods in Figure 5. The top two rows are indoor\nscene detection results from the Wireframe dataset, while the\nbottom two rows are outdoor scene detection results from\nthe YorkUrban dataset.\n5. Ablation Study\nCompare with Object Detection Baselines. We compare\nLETR results with two object detection baseline where the\nline segments are treated as 2-D objects within this context\nin Table 2. We see clear limitations for using bounding box\ndiagonal for both Faster R-CNN and DETR responding to\nour motivation in Section 3.1.\nTable 2. Comparison with object detection baselines on Wire-\nframe [15].\nMethod sAP10 sAP15 sF10 sF15\nFaster R-CNN38.4 40.7 51.5 53.0\nVanilla DETR53.8 57.2 57.2 59.0\nLETR (ours) 65.2 67.7 65.8 67.1\nEffectiveness of Multi-Stage Training.We compare the ef-\nfectiveness of different modules in LETR in Table 3. During\nthe coarse decoding stage, LETR reaches 62.3 and 65.2 for\nsAP10 and sAP15 with encoding features from the C5 layer\nof ResNet backbone, and 63.8 and 66.5 with the one from C4\nof ResNet backbone. The ﬁne decoder reaches 64.7 and 67.4\nfor sAP10 and sAP15 by improving the coarse prediction\nwith ﬁne-grained details from high-resolution features. We\nthen adjust the data imbalance problem with focal loss to\nreach 65.2 and 67.7 for sAP10 and sAP15.\nAs shown in Figure 7 (a), we found it is necessary to train\nthe ﬁne decoding stage after the coarse decoding stage con-\nverges. Training both stages together as a one-stage model\nresults a signiﬁcant worse performance after 400 epochs.\nEffect of Number of Queries. We found a large number of\nline entities is essential to the line segment detection task by\nexperimenting on a wide range of the number of line entities\n(See Figure 7 (c), and using 1000 line entities is optimal for\nthe Wireframe benchmark which contains 74 line segments\nin average.\nTable 3. Effectiveness of modules. Ablation study of the ar-\nchitecture design and learning aspects in the proposed LETR on\nWireframe dataset. (C) indicates the indexed feature used for coarse\ndecoder; (F) indicates the indexed feature used for ﬁne decoder.\nCoarse Decoding Fine Decoding Focal Loss Feature IndexsAP10 sAP15\n✓ C5(C) 62.3 65.2\n✓ C4(C) 63.8 66.5\n✓ ✓ C5(C), C4(F)64.7 67.4\n✓ ✓ ✓ C5(C), C4(F)65.2 67.7\nStage 1\nStage 2\nStage 1Stage 2\nFigure 7. (a) Multi-stage vs. single-stage training. We com-\npare results training coarse and ﬁne layers in single stages and\nmulti-stages (b) Number of decoding layers. We evaluate the\nperformance of outputs from each decoding layer. The 1-6 layers\nare coarse decoder layers and 7-12 layers ﬁne decoder layers. (c)\nNumber of line entities. We test LETR (coarse decoding stage\nonly) with different numbers of line entities on Wireframe.\nEffect of Image Upsampling. All algorithms see the same\ninput image resolution (640×480 typically). However, some\nalgorithms try more precise predictions by upsampling im-\nages. To understand the impact of upsampling, we train and\ntest HAWP and LETR under multiple upsampling scales.\nIn Table 4 below, higher training upsampling resolution im-\nproves both methods. LETR obtains additional gains with\nhigher test upsampling resolution.\nTable 4. Effectiveness of upsampling with Wireframe dataset.\nLETR uses ResNet-101 backbone. * Our LETR-512 resizes orig-\ninal image with the shortest size in a range between 288 and 512\n† Our LETR-800 resizes original image with the shortest size in a\nrange between 480 and 800.\nTrain SizeTest SizesAP10 sAP15 sF10 sF15\nHAWP 512 512 65.7 67.4 64.7 65.8\nHAWP 832 832 67.7 69.1 65.5 66.4\nHAWP 832 1088 65.7 67.1 64.3 65.1\nLETR 512* 512 61.1 64.1 63.1 64.8\nLETR 800† 800 64.3 67.0 65.5 66.9\nLETR 800† 1100 65.2 67.7 65.8 67.1\nEffectiveness of Pretraining. We found model pretraining\nis essential for LETR to obtain state-of-the-art results. With\nDETR pretrained weights for COCO object detection [19],\nour coarse-stage-only model converges at 500 epochs. With\nCNN backbone pretrained weights for ImageNet classiﬁ-\ncation, our coarse-stage-only model converges to a lower\nscore at 900 epochs. Without pretraining, LETR is difﬁcult\nto train due to the limited amount of data in the Wireframe\nbenchmark.\nTable 5. Effectiveness of pretraining. We train LETR (coarse\ndecoding stage only) with two variants. ImageNet represents LETR\nwith ImageNet pretrained ResNet backbone. COCO represents\nLETR with COCO pretrained DETR weights.\nMethod Epochs sAP10 sAP15 sF10 sF15\nImageNet 900 58.4 62.0 62.4 64.6\nCOCO 500 62.3 65.2 64.3 65.9\n6. Visualization\nWe demonstrate LETR’s coarse-to-ﬁne decoding process\nin Figure 8. The ﬁrst two columns are results from the coarse\ndecoder receiving decoded features from the C5 ResNet\nlayer. While the global structure of the scene is well-captured\nefﬁciently, the low-resolution features prevent it from mak-\ning predictions precisely. The last two columns are results\nfrom the ﬁne decoder receiving decoded features from the C4\nResNet layer and line entities from the coarse decoder. The\noverlay of attention heatmaps depicts more detailed relations\nin the image space, which is the key to the detector perfor-\nmance. This ﬁnding is also shown in Figure 7(b), where the\ndecoded output after each layer has consistent improvement\nwith the multi-scale encoder-decoder strategy.\nCoarse Layer 1Coarse Layer 6Fine Layer 1Fine Layer 6sAP15=0.51sAP15=0.49sAP15=0.49sAP15=0.37\nFigure 8. Visualization of LETR coarse-to-ﬁne decoding pro-\ncess. From top to bottom: The 1st row shows line segment detec-\ntion results based on line entities after different layers and the 2nd\nrow shows its corresponding overlay of attention heatmaps. From\nleft to right: The 1st, 2nd, 3rd, 4th columns are coarse decoder\nlayer 1, coarse decoder layer 6, ﬁne decoder layer 1, ﬁne decoder\nlayer 6, respectively.\n7. Conclusion\nIn this paper, we presented LETR, a line segment de-\ntector based on a multi-scale encoder/decoder Transformer\nstructure. By casting the line segment detection problem\nin a holistically end-to-end fashion, we perform set pre-\ndiction without explicit edge/junction/region detection and\nheuristics-guided perceptual grouping processes. A direct\nendpoint distance loss allows geometric structures beyond\nbounding box representations to be modeled and predicted.\nAcknowledgment. This work is funded by NSF IIS-1618477\nand NSF IIS-1717431. We thank Justin Lazarow, Feng Han, Ido\nDurst, Yuezhou Sun, Haoming Zhang, and Heidi Cheng for valu-\nable feedbacks.\nReferences\n[1] Michael Boldt, Richard Weiss, and Edward Riseman. Token-\nbased extraction of straight lines. IEEE Transactions on\nSystems, Man, and Cybernetics, 19(6):1581–1594, 1989.\n[2] J. Brian Burns, Allen R. Hanson, and Edward M. Riseman.\nExtracting straight lines. IEEE Trans. Pattern Anal. Mach.\nIntell., 8(4):425–455, 1986.\n[3] John Canny. A computational approach to edge detection.\nIEEE Transactions on pattern analysis and machine intelli-\ngence, (6):679–698, 1986.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Eur. Conf. Comput.\nVis., 2020.\n[5] Patrick Denis, James H Elder, and Francisco J Estrada. Efﬁ-\ncient edge-based methods for estimating manhattan frames in\nurban imagery. In European conference on computer vision,\n2008.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional transform-\ners for language understanding. In NAACL-HLT, 2019.\n[7] Piotr Dollár, Zhuowen Tu, and Serge Belongie. Supervised\nlearning of edges and object boundaries. In IEEE Conf. Com-\nput. Vis. Pattern Recog., 2006.\n[8] Piotr Dollár and C Lawrence Zitnick. Structured forests for\nfast edge detection. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2013.\n[9] Richard O Duda and Peter E Hart. Use of the hough transfor-\nmation to detect lines and curves in pictures.Communications\nof the ACM, 15(1):11–15, 1972.\n[10] James H Elder and Richard M Goldberg. Ecological statistics\nof gestalt laws for the perceptual organization of contours.\nJournal of Vision, 2(4):5–5, 2002.\n[11] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\nFang, and Hanqing Lu. Dual attention network for scene\nsegmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 3146–3154,\n2019.\n[12] Yasutaka Furukawa and Yoshihisa Shinagawa. Accurate\nand robust line segment extraction by analyzing distribution\naround peaks in hough space. Computer Vision and Image\nUnderstanding, 92(1):1–25, 2003.\n[13] Nicolas Guil, Julio Villalba, and Emilio L Zapata. A fast\nhough transform for segment detection. IEEE Transactions\non Image Processing, 4(11):1541–1548, 1995.\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. InCVPR, 2016.\n[15] Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding,\nShenghua Gao, and Yi Ma. Learning to parse wireframes in\nimages of man-made environments. In IEEE Conf. Comput.\nVis. Pattern Recog., pages 626–635, 2018.\n[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Ima-\ngeNet classiﬁcation with deep convolutional neural networks.\nAdv. Neural Inform. Process. Syst., 2012.\n[17] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Ar-\ntiﬁcial intelligence and statistics, pages 562–570, 2015.\n[18] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,\nand Piotr Dollár. Focal loss for dense object detection. In Int.\nConf. Comput. Vis., pages 2999–3007, 2017.\n[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In Eur.\nConf. Comput. Vis., pages 740–755. Springer, 2014.\n[20] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. IEEE\nConf. Comput. Vis. Pattern Recog., 2015.\n[21] Xiaohu Lu, Jian Yao, Kai Li, and Li Li. Cannylines: A\nparameter-free line segment detector. In 2015 IEEE Interna-\ntional Conference on Image Processing (ICIP), pages 507–\n511. IEEE, 2015.\n[22] David Marr. Vision: A computational investigation into the\nhuman representation and processing of visual information,\nhenry holt and co. Inc., New York, NY, 2(4.2), 1982.\n[23] David R Martin, Charless C Fowlkes, and Jitendra Malik.\nLearning to detect natural image boundaries using local bright-\nness, color, and texture cues. IEEE transactions on pattern\nanalysis and machine intelligence, 26(5):530–549, 2004.\n[24] Jiri Matas, Charles Galambos, and Josef Kittler. Robust detec-\ntion of lines using the progressive probabilistic hough trans-\nform. Computer vision and image understanding, 78(1):119–\n137, 2000.\n[25] Marcos Nieto, Carlos Cuevas, Luis Salgado, and Narciso\nGarcía. Line segment detection using weighted mean shift\nprocedures on a 2d slice sampling strategy. Pattern Analysis\nand Applications, 14(2):149–163, 2011.\n[26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in neural information pro-\ncessing systems, pages 91–99, 2015.\n[27] Stephen M Smith and J Michael Brady. Susan—a new ap-\nproach to low level image processing. International journal\nof computer vision, 23(1):45–78, 1997.\n[28] Zhuowen Tu. Auto-context and its application to high-level\nvision tasks. In IEEE Conference on Computer Vision and\nPattern Recognition, 2008.\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017.\n[30] R G von Gioi, J Jakubowicz, J M Morel, and G Randall.\nLSD: A Fast Line Segment Detector with a False Detection\nControl. IEEE Trans. Pattern Anal. Mach. Intell., 32(4):722–\n732, 2010.\n[31] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n7794–7803, 2018.\n[32] Saining Xie and Zhuowen Tu. Holistically-nested edge detec-\ntion. In Proceedings of the IEEE international conference on\ncomputer vision, pages 1395–1403, 2015.\n[33] Nan Xue, Song Bai, Fudong Wang, Gui-Song Xia, Tianfu Wu,\nand Liangpei Zhang. Learning attraction ﬁeld representation\nfor robust line segment detection. In IEEE Conf. Comput. Vis.\nPattern Recog., 2019.\n[34] Nan Xue, Tianfu Wu, Song Bai, Fudong Wang, Gui-Song Xia,\nLiangpei Zhang, and Philip HS Torr. Holistically-attracted\nwireframe parsing. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n2788–2797, 2020.\n[35] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus\nOdena. Self-attention generative adversarial networks. In\nInternational Conference on Machine Learning, pages 7354–\n7363. PMLR, 2019.\n[36] Ziheng Zhang, Zhengxin Li, Ning Bi, Jia Zheng, Jinlei Wang,\nKun Huang, Weixin Luo, Yanyu Xu, and Shenghua Gao.\nPpgnet: Learning point-pair graph for line segment detection.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2019.\n[37] Yichao Zhou, Haozhi Qi, and Yi Ma. End-to-end wireframe\nparsing. In Proceedings of the IEEE International Conference\non Computer Vision, pages 962–971, 2019."
}