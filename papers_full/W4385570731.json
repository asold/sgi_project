{
  "title": "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering",
  "url": "https://openalex.org/W4385570731",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2116172514",
      "name": "Yujie Wang",
      "affiliations": [
        "Shanxi University"
      ]
    },
    {
      "id": "https://openalex.org/A2105322448",
      "name": "Hu Zhang",
      "affiliations": [
        "Shanxi University"
      ]
    },
    {
      "id": "https://openalex.org/A2131665765",
      "name": "Jiye Liang",
      "affiliations": [
        "Shanxi University"
      ]
    },
    {
      "id": "https://openalex.org/A2103302216",
      "name": "Ru Li",
      "affiliations": [
        "Shanxi University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963995027",
    "https://openalex.org/W3118741274",
    "https://openalex.org/W3122807568",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4385573687",
    "https://openalex.org/W3101126923",
    "https://openalex.org/W3101850416",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3174531908",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3207796810",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4200629408",
    "https://openalex.org/W3015440086",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3186545525",
    "https://openalex.org/W4285606726",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W4307003748",
    "https://openalex.org/W2963895422",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3097986428",
    "https://openalex.org/W4385574243",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2950576363",
    "https://openalex.org/W3035529900",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2989312920",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2998374885",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4297899248",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4226281578",
    "https://openalex.org/W3007685714",
    "https://openalex.org/W3035083185",
    "https://openalex.org/W2987669390",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W3172335055"
  ],
  "abstract": "Recently, knowledge graphs (KGs) have won noteworthy success in commonsense question answering. Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks. However, they ignore (i) optimizing the knowledge representation and structure of subgraphs and (ii) deeply fusing heterogeneous QA context with subgraphs. In this paper, we propose a dynamic heterogeneous-graph reasoning method with LMs and knowledge representation learning (DHLK), which constructs a heterogeneous knowledge graph (HKG) based on multiple knowledge sources and optimizes the structure and knowledge representation of the HKG using a two-stage pruning strategy and knowledge representation learning (KRL). It then performs joint reasoning by LMs and Relation Mask Self-Attention (RMSA). Specifically, DHLK filters key entities based on the dictionary vocabulary to achieve the first-stage pruning while incorporating the paraphrases in the dictionary into the subgraph to construct the HKG. Then, DHLK encodes and fuses the QA context and HKG using LM, and dynamically removes irrelevant KG entities based on the attention weights of LM for the second-stage pruning. Finally, DHLK introduces KRL to optimize the knowledge representation and perform answer reasoning on the HKG by RMSA.We evaluate DHLK at CommonsenseQA and OpenBookQA, and show its improvement on existing LM and LM+KG methods.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 14048–14063\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDynamic Heterogeneous-Graph Reasoning with Language Models and\nKnowledge Representation Learning for Commonsense Question\nAnswering\nYujie Wang1, Hu Zhang1,2,∗, Jiye Liang1,2,∗, Ru Li1,2\n1.School of Computer and Information Technology, Shanxi University, Taiyuan, China\n2.Key Laboratory of Computational Intelligence and Chinese Information\nProcessing of Ministry of Education, Shanxi University, Taiyuan, China\ninit_wang@foxmail.com,{zhanghu,ljy,liru}@sxu.edu.cn\nAbstract\nRecently, knowledge graphs (KGs) have won\nnoteworthy success in commonsense question\nanswering. Existing methods retrieve relevant\nsubgraphs in the KGs through key entities and\nreason about the answer with language mod-\nels (LMs) and graph neural networks. How-\never, they ignore (i) optimizing the knowledge\nrepresentation and structure of subgraphs and\n(ii) deeply fusing heterogeneous QA context\nwith subgraphs. In this paper, we propose a dy-\nnamic heterogeneous-graph reasoning method\nwith LMs and knowledge representation learn-\ning (DHLK), which constructs a heterogeneous\nknowledge graph (HKG) based on multiple\nknowledge sources and optimizes the structure\nand knowledge representation of the HKG us-\ning a two-stage pruning strategy and knowl-\nedge representation learning (KRL). It then\nperforms joint reasoning by LMs and Rela-\ntion Mask Self-Attention (RMSA). Specifically,\nDHLK filters key entities based on the dictio-\nnary vocabulary to achieve the first-stage prun-\ning while incorporating the paraphrases in the\ndictionary into the subgraph to construct the\nHKG. Then, DHLK encodes and fuses the QA\ncontext and HKG using LM, and dynamically\nremoves irrelevant KG entities based on the\nattention weights of LM for the second-stage\npruning. Finally, DHLK introduces KRL to\noptimize the knowledge representation and per-\nform answer reasoning on the HKG by RMSA.\nWe evaluate DHLK at CommonsenseQA and\nOpenBookQA, and show its improvement on\nexisting LM and LM+KG methods.\n1 Introduction\nQuestion answering (QA) is a challenging task that\nrequires machines to understand questions asked\nby natural language and respond to the questions\nbased on the knowledge acquired. Recently, QA\nhas made remarkable progress with the develop-\nment of Language Models (LMs) (Devlin et al.,\n∗Corresponding author.\n2019; Liu et al., 2019; Lan et al., 2020; Raffel\net al., 2020). Fine-tuning based on LMs has now\nbecome a major paradigm for QA tasks. LMs are\npre-trained on a general large-scale corpus contain-\ning rich world knowledge, which the machine can\nutilize when fine-tuning downstream tasks using\nLMs. In some simple, fact-based QA tasks, such as\nSQuAD (Rajpurkar et al., 2016, 2018) and RACE\n(Lai et al., 2017), machine has surpassed humans in\nterms of answer accuracy. However, the machine\nremains less satisfactory in some structured rea-\nsoning QA tasks that require commonsense knowl-\nedge.\nCommonsense knowledge is the general law\nsummarized by human beings through observation,\nresearch, and reflection of various phenomena in\nthe objective world, which is verified by the long-\nterm experience of countless people and is the com-\nmon daily consensus of people. When humans\nanswer questions, they use this knowledge uncon-\nsciously. For example, if you ask “John had an\nurgent matter to attend to at his company, and he\ndrove fast to the company but stopped at an in-\ntersection, what could have happened?”. We can\nreason that John may be passing through the inter-\nsection when the traffic light turns red. Thus, he\nhas to stop and wait for the light to turn green. This\ncommonsense reasoning is easy for humans. How-\never, considering that commonsense knowledge is\na relatively tacit knowledge, LMs do not capture it\nwell.\nKnowledge graphs (KGs) store a large amount\nof commonsense knowledge that can be used by\nmachines to make sound judgments, and this knowl-\nedge can provide the machines with displayed and\ninterpretable evidence. Therefore, some methods\n(Lin et al., 2019; Feng et al., 2020; Yasunaga et al.,\n2021; Sun et al., 2022; Zheng and Kordjamshidi,\n2022; Zhang et al., 2022) have introduced KGs\ninto LMs-based QA methods to model and reason\nabout structured knowledge in KGs through graph\n14048\nWhat furniture  will you normally find near a side chair ?\nA. bedroom                B. table*               C.wheel barrow \nD. building                                          E. office                                    \nQA context \nKnowledge paths and paraphrases\nrelatedto relatedto   side bank table \nrelatedto usedfor   side  top table \nmadeof relatedto   furniture  wood table \nisa relatedto   furniture desk table \nrelatedto   chair \n¾¾¾¾¾ ® ¾¾¾¾¾ ® \n¾¾¾¾¾ ® ¾¾¾¾ ¾ ® \n¾¾¾¾¾ ® ¾¾¾¾¾ ® \n¾¾¾ ® ¾¾¾¾¾ ® \nisa desk table \natlocation atlocation \n  chair cat table \natlocation atlocation   side chair room table \natlocation atlocation   side chair bedroom table \n  ...... \n   side chair : \n¾¾¾¾¾ ® ¾¾¾ ® \n¾¾¾¾¾® ¾¾¾¾¾® \n¾¾¾¾¾® ¾¾¾¾¾® \n¾¾¾¾¾® ¾¾¾¾¾® \na straight-backed chair without arms. \n   a piece of furniture with tableware for a meal laid out on it. \n  ...... \ntable : \nFigure 1: An example from CommonsenseQA , we\nretrieve knowledge paths from ConceptNet (Speer et al.,\n2017) and key entity paraphrases from WordNet (Miller,\n1995) and Wiktionary.\nneural networks (GNNs) (Scarselli et al., 2009).\nRelated methods generally follow the following\nsteps: (i) Extracting key entities in the QA context\nusing entity recognition methods; (ii) Retrieving\nrelevant knowledge subgraphs in KGs based on\nkey entities; (iii) Initializing subgraph entities us-\ning pre-trained word embedding models; and (iv)\nDesigning a GNNs-based reasoning module to per-\nform joint reasoning with LMs. Therefore, sub-\ngraphs’ quality and the joint method of GNNs and\nLMs are crucial to the reasoning performance.\nCurrently, combining LMs and GNNs to solve\ncommonsense QA (CQA) task has proven to be an\neffective method but still contains some problems:\n(i) In the key entities-based subgraph extraction\nmethod, the goodness of the key entities largely\ndetermines the quality of the subgraph. As shown\nin Figure 1, entities such as “wood”, “bank”, “top”,\nand “cat” are some noisy knowledge for the cur-\nrent question, and they affect the model’s judgment\nduring the inference process. But a part of the\nnoisy knowledge can be solved by optimizing key\nentities. In the example of Figure 1, “side chair”\nis a noun phrase, which should be considered as\na whole when retrieving knowledge based on it,\nand this will reduce the introduction of some noisy\nknowledge; (ii) The knowledge representation of\nentities in subgraph are mostly obtained by Glove\n(Pennington et al., 2014), LMs, and so on, ignoring\nthe semantic associations between entities; addi-\ntionally, the knowledge representations obtained\nare less effective; (iii) Given that the QA context\nand subgraph have different structures, existing\nmethods encode QA context and subgraph sepa-\nrately, with shallow interactions only at the GNN\nlayer through message passing (Yasunaga et al.,\n2021; Zheng and Kordjamshidi, 2022) or at the out-\nput layer through attention mechanism (Sun et al.,\n2022) or MLP (Feng et al., 2020; Zhang et al.,\n2022; Yasunaga et al., 2022), lacking deep fusion\nof QA context and subgraph, which will hinder the\ninference capability of the model.\nBased on the above problems, we propose a\nDynamic Heterogeneous-graph reasoning method\nbased on Language models and Knowledge repre-\nsentation learning (DHLK). Specifically, given a\nquestion and choice, we first use KeyBERT (Groo-\ntendorst, 2020) to extract the candidate entities and\nintroduce WordNet (Miller, 1995) and Wiktionary1\nvocabularies to filter the candidate entities and then\nobtain the key entities, which can remove some\nnoisy entities in the subgraph retrieval process and\nrealize first-stage pruning of the subgraph. We also\nincorporate the paraphrases of key entities in the\ntwo dictionaries as entities into the subgraph to con-\nstruct a heterogeneous knowledge graph (HKG).\nThen, we use LM to encode the QA context and\nHKG and fuse the QA context and HKG in the\nencoding process. In addition, we dynamically re-\nmove irrelevant entities according to the attention\nweights of LM to achieve the second-stage pruning\nof the subgraph. Finally, we combine KRL and\nRelation Mask Self-Attention (RMSA) to optimize\nthe knowledge representation of HKG and incor-\nporate the path information in the HKG into the\nQA context. In summary, our contributions are\nthreefold:\n•We construct the HKG based on multiple\nknowledge sources and introduce a two-stage prun-\ning strategy and KRL to optimize the structure and\nknowledge representation of the HKG.\n•We effectively fuse the QA context and HKG\nin the encoding phase of LM to achieve better rea-\nsoning performance.\n•We evaluate our method on CommonsenseQA\nand OpenBookQA, proving the effectiveness of the\nmethod through a series of ablation experiments\nand case studies.\n1https://www.wiktionary.org/\n14049\n2 Related Work\nRecently, large LMs such as UnifiedQA (Khashabi\net al., 2020), T5 (Raffel et al., 2020) and GPT-3\n(Brown et al., 2020) have been widely applied in\nQA tasks, such as open-domain question answering\n(ODQA) and CQA, driving the development of QA.\nHowever, larger LMs result in disproportionate re-\nsource consumption and training time. Therefore,\nmany works have enhanced the reasoning ability\nof machines by introducing external knowledge,\nhoping to achieve good answering results while\nreducing resource consumption and training time.\nKnowledge-enhanced ODQA.ODQA model\nutilizes external knowledge to answer questions,\ntypically consisting of a retriever and a reader com-\nponent. With the development of LMs, Retrieval\nAugmented Architectures (Lewis et al., 2020; Guu\net al., 2020) have become the mainstream method\nfor ODQA. They apply LMs to retriever-reader\nand conduct joint training of the retriever-reader.\nHowever, previous works (Karpukhin et al., 2020;\nIzacard and Grave, 2021) primarily focused on un-\nstructured knowledge sources, such as Wikipedia.\nRecently, some works (Min et al., 2019; Zhou et al.,\n2020; Hu et al., 2022) have started incorporating\nstructured KGs into the retriever-reader architec-\nture to enhance retrieval effectiveness and ques-\ntion answering capabilities. For example, UniK-\nQA (Oguz et al., 2022) converts KG triplets into\ntext and merges them with unstructured knowledge\nrepositories. KG-FiD (Yu et al., 2022) utilizes KG\nto establish relational dependencies between re-\ntrieved paragraphs and employs GNNs to sort and\nprune the retrieved paragraphs. Grape (Ju et al.,\n2022) constructs a localized bipartite graph for each\npair of question and article, learning knowledge\nrepresentations through GNNs.\nKnowledge-enhanced CQA.CQA also requires\nexternal knowledge to answer questions, but it is\nmore focused on commonsense questions. From\nthe perspective of knowledge and QA context fu-\nsion, there are currently two main methods. Some\nworks (Bian et al., 2021; Xu et al., 2021, 2022) feed\nthe retrieved knowledge together with the QA con-\ntext into the LM, utilizing self-attention to fuse the\nknowledge. However, the self-attention treats the\ninput knowledge and QA context indiscriminately,\nwhich can undermine the semantic information of\nthe QA context. Other works (Lin et al., 2019; Feng\net al., 2020; Lv et al., 2020; Yasunaga et al., 2022;\nZheng and Kordjamshidi, 2022) combine LM and\nGNNs to solve CQA. For example, QAGNN (Ya-\nsunaga et al., 2021) uses LM to estimate the im-\nportance of subgraph entities and considers the QA\ncontext as an additional node connected to the sub-\ngraph. JointLK (Sun et al., 2022) uses the bidirec-\ntional attention module to fuse the two modalities\nwhile designing a pruning module to remove ir-\nrelevant entities from the subgraph. GREASELM\n(Zhang et al., 2022) fuses encoding representations\nfrom LM and GNNs through multi-layered modal-\nity interaction operations. However, these works\nencode the QA context and KG subgraph in iso-\nlation, leading to limited interaction between tex-\ntual and KG representations. Additionally, they\ndon’t consider the influence of key entities and\nknowledge representations on subgraph retrieval\nand model inference.\nIn contrast to previous works, we propose to\nreduce noisy knowledge by optimizing the set of\nkey entities in the subgraph retrieval process. In\naddition, we use LM to encode and fuse the two\nmodalities and prune the subgraph according to the\nattention weights of LM. Meanwhile, during the\ninference process, we introduce the KRL algorithm\nto optimize the knowledge representation of the\nsubgraph. Figure 2 shows the overall architecture\nof our method.\n3 Methods\n3.1 Task Formulation\nWe focus on the multi-choice CQA task in this\npaper. Given a question q and a set of candidate\nchoices {c1,c2,...,c b}, we need to select the one\nthat best fits the question’s meaning. In general,\nCQA does not provide the background knowledge\nrelated to the question. Therefore, we need to re-\ntrieve relevant knowledge from KG and combine\nit to reason about the answer. In this paper, we re-\ntrieve a relevant subgraph from ConceptNet based\non key entities in question and choice, and iden-\ntify the paraphrases of the key entities in WordNet\nand Wiktionary. Meanwhile, we explicitly take the\nparaphrases as some additional entities (paraphrase\nentities) connected to the KG subgraph. Therefore,\nour method starts with the HKG construction.\n3.2 HKG Construction\nIn the KG-based CQA task, the subgraph needs\nto be retrieved from the KG based on key entities.\nTherefore, the key entities determine the quality\nof the subgraph. We use KeyBERT to identify\n14050\nQA CE \nLM Encoder \nPE \nDynamic Pruning \nRMSA \nLayer KG2QA Layer KRL \nLayer \nIntegrator & Classifier \nKRL Loss Classifier  Loss \nMasking \nPruning \nĂ\nLM-to-CE Attention LM-to-CE Attention LM-to-CE Attention \n      question entities                  correlatio n entities                  choice entities               paraphrase entities \nReasoning based \non RMSA \nPartOf PartOf \nMaxPooling \nPartOf PartOf \nRMSA Layer \nConstructing HKG \nFigure 2: The overall architecture of our proposed DHLK model, which takes as input the QA context (question\n+ choice) and the entities in the HKG. The CE and PE denote concept entities extracted from ConceptNet and\nparaphrase entities extracted from the dictionaries, respectively.\ncandidate entities ˆE = {ˆe1,ˆe2,..., ˆen}in ques-\ntion and choice. Meanwhile, we identify phrase\nentities in ˆE based on WordNet and Wiktionary\nvocabularies, and remove the subwords that con-\nstitute phrase entities in ˆE, to obtain key entities\nE = {e1,e2,...,e m}and their corresponding para-\nphrases P = {p1,p2,...,p m}. Here n,m denotes\nthe number of candidate entities and key entities,\nand n≥m.\nFollowing the work of Yasunaga et al. (2021),\nwe retrieve the subgraph in ConceptNet according\nto E. The subgraph consists of multiple knowl-\nedge paths within two-hops, and each path con-\ntains at most two triples. Meanwhile, we separately\nconnect the question key entities and choice key\nentities in the subgraph, and define the relation be-\ntween them as “SameQA”. In addition, we consider\nP as paraphrase entities and connect them with the\ncorresponding key entities to construct HKG, and\ndefine the relation between them as “DefAs”. We\ngive all the relations included in HKG in Appendix\nA. From the knowledge source perspective, HKG\ncontains two types of entities, i.e., concept entities\nand paraphrase entities.\n3.3 LM-Based Encoding\nInspired by K-BERT (Liu et al., 2020), we con-\nstruct two visible matrices and use RoBERTa (Liu\net al., 2019) to encode the QA context, concept\nentities, and paraphrase entities in HKG. The visi-\nble matrix and the encoding process are described\nfurther below.\nWe connect the QA context with the concept en-\ntities and construct the visual matrix M according\nto the following rules:\n(i) The tokens contained in the QA context are\nvisible to each other.\n(ii) The tokens belonging to the same concept\nentity are visible to each other.\n(iii) The key entities exist in the concept entities,\nand they are also extracted from the QA context.\nTherefore, the key entities and the corresponding\ntokens in the QA context are visible to each other.\nThe value of Mi,j is 0 or 1, where Mi,j = 1\nmeans that tokens are visible to each other, and\nMi,j = 0means that tokens are invisible to each\nother. In RoBERTa model,M is further defined as\n˜M =\n{0 Mi,j = 1\n−∞Mi,j = 0 (1)\nBased on ˜M, we introduce Mask Self-Attention\n(MSA) into RoBERTa to encode the QA context\nand concept entities. Formally, the MSA is defined\nas\nQi+1,Ki+1,Vi+1 = hiWq,hiWk,hiWv (2)\nsi+1 = Qi+1Ki+1⊤\n√\nd\n(3)\nαi+1 = softmax(si+1 + ˜M) (4)\nhi+1 = si+1Vi+1 (5)\nwhere hi is the hidden state of RoBERTa at i-th\nlayer. Wq, Wk and Wv are trainable model parame-\nters. αi+1 is the attention weights after integrating\n˜M. ddenotes the hidden layer size of RoBERTa.\n14051\nWe feed the QA context, concept entities, andM\ninto RoBERTa to obtain the tokens embeddings of\nthe QA context and concept entities: {˜qi}A\ni=1 ∈Rd\nand {˜ci}Z\ni=1 ∈Rd. Here Aand Zdenote the num-\nber of tokens of QA context and concept entities,\nrespectively.\nSimilarly, we construct a visible matrix ˆM to\nprevent the change in paraphrases meaning due to\nthe interaction between different paraphrases. In\nˆM, only the tokens located in the same paraphrase\nare visible to each other. We connect all the para-\nphrases and feed them into RoBERTa along with\nˆM to obtain the tokens embeddings {˜pi}F\ni=1 ∈Rd\nof the paraphrase entities. Here F denotes the num-\nber of paraphrase tokens.\n3.4 Dynamic Pruning\nAlthough we prune the HKG by filtering key en-\ntities during its construction, noisy entities persist\nin the HKG. Therefore, we prune the HKG in the\nsecond-stage according to the importance of the\nconcept entities to the QA context.\nWe take the embedding representation ˜q1 of the\n[CLS] position in RoBERTa as semantic represen-\ntation of the QA context. For the concept entities in\nHKG, we obtain the token-level attention weights\nw = {wj,wj+1,...,w k}of each entity for ˜q1 by\nequation 3, and then obtain the node-level attention\nweight ˜wby\nˆw= 1\nk\nk∑\ni=j\nwi (6)\n˜w= ˆw−ˆwmin\nˆwmax −ˆwmin\n(7)\nwhere ˆwmax, ˆwmin are the maximum and mini-\nmum values of node-level attention weights. Next,\nwe remove the entities with ˜w<µ in the HKG and\nremove the edges connected to these entities in the\nHKG.\n3.5 KRL Layer\nHKG can be viewed as the knowledge subgraph\ncomposed of multiple triples connections. To ob-\ntain better entity and relation embeddings, we in-\ntroduce KRL to optimize knowledge representation\nand improve the reasoning effect.\nEntity and relation embeddings.For a triplet\n(h,r,t), h,tare the entities in HKG, and r is the\nconcatenated edge between the entities. Based on\nthe tokens embeddings {˜ti}T\ni=1 ∈Rd of each en-\ntity obtained in Section 3.3, we obtain the entity\nembedding ˜eby\n˜e= Wtfavg({˜t1,˜t2,..., ˜tT }) (8)\nwhere Wt ∈Rd×dt is a linear transformation, favg\nis an average pooling function. Similarly, we feed\nall the relations and corresponding paraphrases into\nRoBERTa to obtain the relation embedding ˜rby\nequation 8.\nFor simplicity, we follow TransE (Bordes et al.,\n2013), combined with a negative sampling strategy\nto optimize entity and relation embeddings. TransE\ntraining objective is\nLKRL =\n∑\n(h,r,t)∈S\n∑\n(h′,r,t′)∈S′\n(h,r,t)\n[\nγ+ dr(h,t) −dr(h\n′\n,t\n′\n)\n] (9)\ndr(h,t) =∥h+ r−t∥p (10)\nwhere γ >0 is a margin hyperparamet, dr is the\nscoring function, we take the norm pas 1, and S′is\nthe samples obtained by negative sampling. For the\nnegative sampling strategy, we randomly sample\nentity in other HKGs in the same batch to replace\nthe head entity or tail entity.\n3.6 RMSA Layer\nInspired by (Wang et al., 2020a; Shao et al., 2020),\nwe introduce the relation into Mask Self-Attention\nto construct RMSA and combine LM and RMSA\nfor reasoning.\nFirst, we separately obtain the initial embed-\nding representation E0 = {˜ei}V\ni=1 ∈ Rdt and\nR0 = {˜ri}B\ni=1 ∈Rdt of all entities and the re-\nlations between entities in HKG by Section 3.5.\nHere V and B denote the number of entities and\nrelations, respectively. Then, we apply L-layer\nRMSA to update the embedding representations\nof entities and relations in HKG. Specifically, the\ncomputation process of the l-th layer RMSA can\nbe formulated as\n˜αl−1 = (El−1We\nq )(El−1We\nk + Rl−1Wr\nk )⊤ (11)\nαl−1 = softmax(˜αl−1/\n√\ndt + Mhkg) (12)\n˜El−1 = αl−1(El−1We\nv + Rl−1Wr\nv ) (13)\nEl = LayerNorm( ˜El−1) (14)\n14052\nwhere We\nq , We\nk , We\nv , Wr\nk and Wr\nv are trainable\nmodel parameters, Mhkg is the adjacency matrix\nof HKG after pruning.\nWe obtain the HKG graph embedding represen-\ntation ˜gby\n˜g= fmax(˜eq) (15)\nwhere fmax is maximum pooling function, ˜eq is all\nquestion entities embeddings.\n3.7 Integrator & Answer Prediction\nAfter L-layer RMSA iteration, we obtain the enti-\nties and relations embeddings in HKG. Then, we\nincorporate the path information of HKG into the\nQA context by a KG2QA layer and then connect it\nwith the ˜gto predict the answer.\nKG2QA. HKG is composed of multiple paths\nX = {x1,x2,...,x y}, each of which is a sequence\nof multiple triples. Same as Lin et al. (2019), we\ndefine the k-th path between thei-th question entity\neq\ni ∈Eq and the j-th choice entity ec\nj ∈Ec as\nXi,j[k] = [(eq\ni ,r0,t0),...(tn−1,rn,ec\nj)] (16)\nWe use GRU to encodeX and use the last hidden\nlayer state as X’s embedding representation ˜X.\nNot all paths are helpful for answering questions,\nso we dynamically select the appropriate paths by\nthe relevance between the paths and the QA con-\ntext. First, we compute the similarity score spq\nbetween the paths and QA context through the co-\nsine similarity algorithm. Then, we retain top β%\nof the knowledge paths ˜Xq according to spq. Fi-\nnally, we obtain the QA context representation ˜Qp\nof the fusion paths information by\nspq = softmax\n(\n(˜qWq\nq )( ˜XqWp\nk )⊤\n)\n(17)\nˆQp = LayerNorm(spq ˜XqWp\nv + ˜q) (18)\n˜Qp = favg( ˆQp) (19)\nHere ˜qis the tokens embeddings of the QA context,\nWq\nq , Wp\nk and Wp\nv are trainable model parameters.\nFinally, we feed ˜g, ˜Qp and ˜Qq into the MLP to\npredict the answer probability.\np= MLP([˜g; ˜Qp; ˜Qq]) (20)\nHere ˜Qq is obtained by averaging the pooling of ˜q.\n4 Experiment\n4.1 Datasets\nWe evaluate our method on CommonsenseQA (Tal-\nmor et al., 2019) and OpenBookQA (Mihaylov\net al., 2018). Given that the test set of Common-\nsenseQA is not public, we conduct experiments\non the in-house dataset (IHdata) splitted by (Lin\net al., 2019) (specific details of the datasets are in\nAppendix B).\n4.2 Implementation Details\nFor the CQA tasks, we use two types of exter-\nnal knowledge: knowledge graph and dictionary.\nGiven a question and choice, we extract at most 100\nknowledge paths within two-hops in ConceptNet\n(Speer et al., 2017) based on the question key enti-\nties and the choice key entities. We also retrieve the\nparaphrases of the key entities in WordNet (Miller,\n1995) and Wiktionary. In the experiment, we use\nRoBERTa-large (Liu et al., 2019) as the encoder\nand Adamw (Loshchilov and Hutter, 2019) as the\nmodel optimizer. For some hyperparameters, we\nset the learning rate to 1e-5, the batch size to{4,5},\nthe epochs to {3,6}, RMSA’s layer number L=4,\ndynamic pruning threshold µ=0.38, and knowledge\npath’s retention rate β=40%. Each model is trained\nusing one GPU (NVIDIA_A100), which takes 1.5\nhours on average.\n4.3 Compared Method\nWe compare with the mainstream RoBERTa-\nlarge+KG methods, including RN (Santoro\net al., 2017), RGCN (Schlichtkrull et al., 2018),\nGconAttn (Wang et al., 2019), KagNet (Lin et al.,\n2019), MHGRN (Feng et al., 2020), QAGNN\n(Yasunaga et al., 2021), JointLK (Sun et al.,\n2022), DRGN (Zheng and Kordjamshidi, 2022),\nGREASELM (Zhang et al., 2022) and DRAGON\n(Yasunaga et al., 2022). Meanwhile, we compare\nour method with DESC-KCR (Xu et al., 2021),\nwhich also uses both KG and dictionary types of\nknowledge. But since DESC-KCR uses ALBEER-\nxxlarge (Lan et al., 2020) as the encoder, we re-\ntrained the DESC-KCR model in IHdata using\nRoBERTa-large (Liu et al., 2019) for a fair compar-\nison.\n4.4 Main results\nTable 1 and Table 2 give the experimental re-\nsults on CommonsenseQA and OpenBookQA. On\nboth datasets, our method achieves consistent\n14053\nMethods IHdev-Acc.(%) IHtest-Acc.(%)\nFine-tuned LMs (w/o KBs) 73.07 (±0.45) 68.69 (±0.56)\n+ RGCN 72.69 (±0.19) 68.41 (±0.66)\n+ GconAttn 72.61 (±0.39) 68.59 (±0.96)\n+ KagNet 73.47 (±0.22) 69.01 (±0.76)\n+ RN 74.57 (±0.91) 69.08 (±0.21)\n+ MHGRN 74.45 (±0.10) 71.11 (±0.81)\n+ QA-GNN 76.54 (±0.21) 73.41 (±0.92)\n+ DESC-KCR 78.21(±0.23) 73.78 (±0.39)\n+ DGRN 78.20 74.00\n+ GREASELM 78.5(±0.5) 74.20(±0.4)\n+ JointLK 77.88 (±0.25) 74.43 (±0.83)\n+ DRAGON∗ - 76.00\n+ DRAGON (w/o MLM)∗ - 73.80\n+ DHLK (ours) 79.39(±0.24) 74.68 (±0.26)\nTable 1: Performance comparison on Commonsense\nQA in-house split. The DRAGON model undergoes\nMLM training on the BookCorpus dataset and requires\ntraining on 8 A100 GPUs for 7 days. Meanwhile, our\nmethod outperforms DRAGON when the MLM task is\nremoved.\nMethods RoBERTa AristoRoBERTa\nFine-tuned LMs (w/o KB) 64.80 (±2.37) 78.40 (±1.64)\n+ RGCN 62.45 (±1.57) 74.60 (±2.53)\n+ GconAttn 64.75 (±1.48) 71.80 (±1.21)\n+ RN 65.20 (±1.18) 75.35 (±1.39)\n+ MHGRN 66.85 (±1.19) 80.60\n+ QAGNN 70.58 (±1.42) 82.77 (±1.56)\n+ DESC-KCR∗ - -\n+ DGRN 69.60 84.10\n+ GREASELM - 84.80\n+ JointLK 70.34 (±0.75) 84.92 (±1.07)\n+ DRAGON 72.00 -\n+ DRAGON (w/o MLM) 66.40 -\n+ DHLK (ours) 72.20(±0.40) 86.00(±0.79)\nTable 2: Accuracy on the test set of OpenBookQA.\nMethods with AristoRoBERTa use the textual evidence\nby Clark et al. (2020) as an additional input to the QA\ncontext. DESC-KCR does not provide pre-processed\ndata from OpenBookQA. Therefore we cannot train the\nDESC-KCR model on OpenBookQA.\nimprovements compared to fine-tuned LM and\nother LM+KG methods. On CommonsenseQA,\nDHLK improves 6.32% and 5.99% on IHdev and\nIHtest compared to fine-tuned RoBERTa, respec-\ntively. Compared to other LM+KG methods,\nDHLK has also achieved highly competitive re-\nsults. (DRAGON further pre-trained on BookCor-\npus, so it outperforms us on IHtest.) Similarly,\nour method achieves better experimental results\non OpenBookQA. Compared to the best JointLk\nmethod, our method improves by 1.08%.\nIn Tables 3 and 4, we also compare with sim-\nilar methods in the leaderboard, and our method\nachieves competitive results.\nMethods Dev-Acc. (%) Test-Acc.(%)RoBERTa (Liu et al., 2019) 78.5 72.1RoBERTa + FreeLB (Zhu et al., 2020) 78.81 72.19RoBERTa + HyKAS (Ma et al., 2019) 80.1 73.2RoBERTa + KE 78.7 73.3Albert (Lan et al., 2020) 80.5 73.5RoBERTa + KEDGN (ensemble) - 74.4XLNet + Graph Reasoning (Lv et al., 2020) 79.3 75.3RoBERTa + MHGRN (Feng et al., 2020) - 75.4ALBERT + Path Generator (Wang et al., 2020b) 78.42 75.6RoBERTa + QA-GNN (Yasunaga et al., 2021) - 76.1Albert (Lan et al., 2020) (ensemble) - 76.5RoBERTa + JointLK (Sun et al., 2022) - 76.6RoBERTa + DHLK (ours) 80.85 77.6\nTable 3: Performance comparison on Commonsense\nQA official leaderboard.\nMethods Test-Acc.(%)\nCareful Selection (Banerjee et al., 2019) 72.0\nAristoRoBERTa 77.8\nKF+SIR (Banerjee and Baral, 2020) 80.0\nAristoRoBERTa + PG (Wang et al., 2020b) 80.2\nAristoRoBERTa + MHGRN (Feng et al., 2020) 80.6\nALBERT + KB 81.0\nAristiRoBERTa + QA-GNN (Yasunaga et al., 2021) 82.8\nT5 (Raffel et al., 2020) 83.2\nAristoRoBERTa + DRGN (Sun et al., 2022) 84.1\nAristoRoBERTa + GREASELM (Zhang et al., 2022) 84.8\nAristoRoBERTa + JointLK (Sun et al., 2022) 85.6\nUnifiedQA(11B)∗(Khashabi et al., 2020) 87.2\nAristoRoBERTa + DHLK (our) 86.8\nTable 4: Accuracy on the OpenBookQA leaderboard\ntest set. All listed methods use the provided science\nfacts as an additional input to the language context. The\nUnifiedQA (11B params) is 30x larger than our model.\n5 Analysis\n5.1 Ablation Studies\nWe conduct ablation studies on the Commonsense\nIHdev set to further analyze the effectiveness of\neach module of DHLK.\nImpact of DHLK module.Table 5(a) shows the\nexperimental results after ablation of each model\nof DHLK. Disabling the KG2QA module results\nin a performance decrease of 1.24%, showing that\nKG2QA can effectively incorporate the paths infor-\nmation from HKG into the QA context. Removing\nthe KRL module results in a 0.83% decrease in the\nDHLK’s performance, demonstrating that optimiz-\ning the knowledge representation of HKG by KRL\ncan improve the reasoning ability of the model.\nRemoving the dynamic pruning module results in\n0.66% decrease of DHLK’s performance, which\nproves that there is some unfavorable knowledge in\nHKG for model reasoning. After removing the vis-\nible matrix M in the RoBERTa encoding process,\nthe performance decreases by 3.53%. The reason\nis that when M is removed, all tokens are visible to\neach other when encoding QA context and concept\n14054\nMethods IHdev-Acc.(%)DHLK 79.61- KG2QA 78.37- KRL 78.78- Dynamic pruning 78.95- Visual matrixM 76.08- Paraphrase entities 79.11\n(a) Impact of DHLK module\nRMSA Layers IHdev-Acc.(%)L= 2 78.62L= 3 79.27L= 4 (final) 79.61L= 5 79.19L= 6 78.86\n(b) Impact of RMSA layers\nPruning thresholdµIHdev-Acc.(%)µ= 0.34 77.72µ= 0.36 78.62µ= 0.38 (final) 79.61µ= 0.40 79.36µ= 0.42 78.95\n(c) Impact of HKG pruning threshold\nRetention rateβ(%) IHdev-Acc.(%)β= 36 78.70β= 38 79.44β= 40 (final) 79.61β= 42 78.38β= 44 78.46\n(d) Impact of paths retention rate\nTable 5: Ablation results on the CommonsenseQA\nIHdev set.\nentities by RoBERTa, too many concept entities\ncan change the original meaning of QA context and\nalso have an impact on dynamic pruning. Finally,\nremoving paraphrase entities from HKG results in\na 0.5% performance degradation, which is due to\nthe fact that paraphrase entities can further enhance\nthe semantic representation of key entities.\nImpact of RMSA layers.We further analyze the\neffect of the number of RMSA layers on DHLK. As\nshown in Table 5(b), the DHLK performance grad-\nually increases as the number of layers increases,\nand the best performance is achieved when L= 4.\nImpact of pruning threshold and retention\nrate. We analyze the thresholds of the dynamic\npruning module and the KG2QA module, respec-\ntively (see in Table 5(c) and Table 5(d)). For the\ndynamic pruning module, DHLK achieves the best\nperformance when we remove entities with node-\nlevel attention weights less than 0.38 in the HKG.\nSimilarly, for the KG2QA module, DHLK achieves\nthe best performance when we retain the top 40%\nof the paths most relevant to the QA context.\n(a) HKG constructed based on candidate entities \n(b) First-stage pruning based on  dictionary 's  vocabularies \n(c) Second-stage pruning based on LM's attention weights \nfurniture \nside chair \ndesk \nbuilding \nroom \n... \ntable \nlibr \nary \nfurniture \nside chair \ndesk \nwood \nbuilding \nroom \n... \ntable \nlibr \nary \ndesk \nwood \nbuilding \nroom \n... \ntable \nlibr \nary \nfurniture \nside chair \ndesk \nwood \nbuilding \nroom \n... \ntable \nlibr \nary \nfurniture \nchair \nside \ndesk \nbank \ntop \nwood \nbuilding \nlibrary \nroom side chair \n... \ntable \ncat \nfurniture \nchair \nside \ndesk \nbank \ntop \nwood \nbuilding \nlibrary \nroom side chair \n... \ntable \ncat \nFigure 3: A case study on two-stage pruning. The ques-\ntion and the corresponding answer are “What furniture\nwill you normally find near a side chair?” and “table”.\nFor simplicity, we give only part of the entities in the\nfigure and remove the paraphrased entities.\n5.2 Case study\nWe analyze the two-stage pruning strategy of\nDHLK by a case study. As shown in Figure 3(a),\nwhen we extract the subgraph in ConceptNet based\non the candidate entities, we introduce some unre-\nlated entities to the question inevitably. For exam-\nple, “side chair” is a noun phrase and should be con-\nsidered as a whole. When it is split into “side” and\n“chair”, the “side” has different meanings from it.\nMeanwhile, “side” and “chair” also introduce some\nirrelevant entities to the current question, such as\n“bank”, “top”, “cat”, etc. Therefore, in Figure 3(b)\nwe introduce the dictionary’s vocabularies to fil-\nter the candidate entities and remove the subwords\nthat make up the phrase entities, so that some irrel-\nevant entities such as “bank”, “top” and “cat” can\nbe removed when retrieving the subgraph.\nWe consider the above process as the first-stage\npruning of HKG. However, the subgraph obtained\nby this process is static and there are still some\nnoisy entities in HKG. We think that the entities\nthat are weakly associated with the QA context\nshould be removed dynamically in the model in-\nference process. Therefore, as shown in Figure 3\n(c), in the second-stage pruning, we dynamically re-\nmove entities with less relevance to the QA context,\ne.g., “wood”, during the model reasoning based on\nthe LM’s attention weights.\n5.3 Error Analysis\nTo further analyze why our model fails on some\nquestions. As shown in Appendix C, we randomly\nselect 50 examples for analysis and classify them\ninto the following classes.\nInappropriate paraphrases. Some entities\nhave multiple paraphrases, even though we extract\nparaphrases based on entity POS tags in the QA\ncontext and the similarity of each paraphrase to\nthe QA context, there are still some entities whose\nparaphrases are inappropriate. For example, the\nparaphrase of “fair” in the first example should be\n“(used of hair or skin) pale or light colored”, but the\nparaphrase we extracted is “a competitive exhibi-\ntion of farm products”, which is inconsistent with\nthe question in the example.\nIndistinguishable knowledge paths.When we\nanalyze the error examples, we find that some\nquestions have similar knowledge paths in mul-\ntiple choices. In such cases, the model predicts\nanswers that are also consistent with human com-\nmonsense. For example, in the second example, the\n14055\n“hedgehog” and “porcupine” have similar knowl-\nedge paths and the same paraphrase.\nLack of relevant knowledge.Although we use\nmultiple knowledge sources, there is still much\nknowledge that is not covered. In the third exam-\nple, the question is about the content of the self-\nreferential book written by Kramer. This requires\nsome knowledge of Kramer’s life to answer, but\nwe did not retrieve this knowledge in ConcetNet or\nthe dictionaries.\nIncomprehensible questions.When the ques-\ntion is too long or rather abstract, the model is diffi-\ncult to make correct judgment. The fourth example\nasks “The pencil sharpener in the classroom is bro-\nken, and the teacher tells the students where they\nshould go to find another.”. Although our model\nretrieves the correct paths and paraphrases, it lacks\nfurther understanding of the question and cannot\nmodel the current question scenario. The lack of\nthis ability led to our method’s unsatisfactory re-\nsults in answering some complex questions.\n6 Conclusions\nIn this paper, we propose DHLK, a CQA method\nbased on LM and KRL. Our main innovations in-\nclude: (i) Constructing the HKG based on KG and\ndictionary, and introducing a two-stage pruning\nstrategy and KRL to optimize the structure and\nknowledge representation of the HKG; (ii) Deeply\nfusing the QA context and HKG in the encoding\nstage of LM, and designing a KG2QA module to\nincorporate the paths information of HKG into the\nQA context. The effectiveness of DHLK is demon-\nstrated via experimental results and analysis on\nCommonsenseQA and OpenBookQA.\nLimitations\nIn this section, we will analyze the limitations of\nour method. First, we introduce multiple knowl-\nedge sources to construct HKG, and encoding this\nknowledge through LM consumes more GPU re-\nsources. Second, some useful knowledge may be\nremoved when retrieving knowledge from key enti-\nties optimized by dictionary vocabulary. Then, we\nexperimentally demonstrate that the paraphrase de-\nscriptions are effective in improving the reasoning\nability of the model, but due to resource constraints,\nwe are unable to incorporate the paraphrases of\nall entities into HKG. Finally, our method uses\nthe simpler TransE algorithm when optimizing the\nknowledge representation using KRL due to GPU\nconstraints, which may not be able to model the\ncomplex relationships in HKG well.\nAcknowledgments\nWe thank the anonymous reviewers for their helpful\ncomments and suggestions. This work is supported\nby the National Key Research and Development\nProgram of China (2020AAA0106100), National\nNatural Science Foundation of China (62176145)\nand National Natural Science Foundation of China\n(62076155).\nReferences\nPratyay Banerjee and Chitta Baral. 2020. Knowledge\nfusion and semantic knowledge ranking for open do-\nmain question answering. CoRR, abs/2004.03101.\nPratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra,\nand Chitta Baral. 2019. Careful selection of knowl-\nedge to solve open book question answering. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 6120–6129. Association for Computa-\ntional Linguistics.\nNing Bian, Xianpei Han, Bo Chen, and Le Sun. 2021.\nBenchmarking knowledge-enhanced commonsense\nquestion answering via knowledge-to-text transfor-\nmation. In Thirty-Fifth AAAI Conference on Artificial\nIntelligence, AAAI 2021, Thirty-Third Conference\non Innovative Applications of Artificial Intelligence,\nIAAI 2021, The Eleventh Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2021, Vir-\ntual Event, February 2-9, 2021, pages 12574–12582.\nAAAI Press.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013, Lake\nTahoe, Nevada, United States, pages 2787–2795.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nPeter Clark, Oren Etzioni, Tushar Khot, Daniel\nKhashabi, Bhavana Dalvi Mishra, Kyle Richard-\nson, Ashish Sabharwal, Carissa Schoenick, Oyvind\nTafjord, Niket Tandon, Sumithra Bhakthavatsalam,\nDirk Groeneveld, Michal Guerquin, and Michael\nSchmitz. 2020. From ’f’ to ’a’ on the N.Y . regents\nscience exams: An overview of the aristo project. AI\nMag., 41(4):39–53.\n14056\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng\nWang, Jun Yan, and Xiang Ren. 2020. Scalable multi-\nhop relational reasoning for knowledge-aware ques-\ntion answering. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1295–1309, Online. As-\nsociation for Computational Linguistics.\nMaarten Grootendorst. 2020. Keybert: Minimal key-\nword extraction with bert.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research,\npages 3929–3938. PMLR.\nZiniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang,\nZiyi Yang, Chenguang Zhu, Kai-Wei Chang, and\nYizhou Sun. 2022. Empowering language models\nwith knowledge graph reasoning for open-domain\nquestion answering. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 9562–9581.\nAssociation for Computational Linguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\nEACL 2021, Online, April 19 - 23, 2021, pages 874–\n880. Association for Computational Linguistics.\nMingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang,\nand Yanfang Ye. 2022. Grape: Knowledge graph\nenhanced passage reader for open-domain question\nanswering. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, Abu Dhabi,\nUnited Arab Emirates, December 7-11, 2022, pages\n169–181. Association for Computational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 6769–6781. Associa-\ntion for Computational Linguistics.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\naries with a single QA system. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, Online Event, 16-20 November 2020, volume\nEMNLP 2020 of Findings of ACL, pages 1896–1907.\nAssociation for Computational Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard H. Hovy. 2017. RACE: large-scale read-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2017, Copenhagen, Denmark, September 9-11, 2017,\npages 785–794. Association for Computational Lin-\nguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. Kagnet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2829–2839. Association for\nComputational Linguistics.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-BERT: en-\nabling language representation with knowledge graph.\nIn The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020 , pages\n2901–2908. AAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\n14057\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nShangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan\nDuan, Ming Gong, Linjun Shou, Daxin Jiang, Gui-\nhong Cao, and Songlin Hu. 2020. Graph-based rea-\nsoning over heterogeneous external knowledge for\ncommonsense question answering. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI 2020,\nThe Tenth AAAI Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2020, New York, NY,\nUSA, February 7-12, 2020, pages 8449–8456. AAAI\nPress.\nKaixin Ma, Jonathan Francis, Quanyang Lu, Eric Ny-\nberg, and Alessandro Oltramari. 2019. Towards gen-\neralizable neuro-symbolic systems for commonsense\nquestion answering. CoRR, abs/1910.14087.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? A new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2381–2391. Association for Computational\nLinguistics.\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39–41.\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Knowledge guided text re-\ntrieval and reading for open domain question answer-\ning. CoRR, abs/1911.03868.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin,\nStan Peshterliev, Dmytro Okhonko, Michael Sejr\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\nScott Yih. 2022. Unik-qa: Unified representations\nof structured and unstructured knowledge for open-\ndomain question answering. In Findings of the Asso-\nciation for Computational Linguistics: NAACL 2022,\nSeattle, WA, United States, July 10-15, 2022, pages\n1535–1546. Association for Computational Linguis-\ntics.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2014, October 25-29, 2014, Doha,\nQatar, A meeting of SIGDAT, a Special Interest Group\nof the ACL, pages 1532–1543. ACL.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\nfor squad. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 2: Short Papers, pages 784–789. Association\nfor Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nThe Association for Computational Linguistics.\nAdam Santoro, David Raposo, David G. T. Barrett,\nMateusz Malinowski, Razvan Pascanu, Peter W.\nBattaglia, and Tim Lillicrap. 2017. A simple neu-\nral network module for relational reasoning. In Ad-\nvances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Process-\ning Systems 2017, December 4-9, 2017, Long Beach,\nCA, USA, pages 4967–4976.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus\nHagenbuchner, and Gabriele Monfardini. 2009. The\ngraph neural network model. IEEE Trans. Neural\nNetworks, 20(1):61–80.\nMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter\nBloem, Rianne van den Berg, Ivan Titov, and Max\nWelling. 2018. Modeling relational data with graph\nconvolutional networks. In The Semantic Web - 15th\nInternational Conference, ESWC 2018, Heraklion,\nCrete, Greece, June 3-7, 2018, Proceedings, volume\n10843 of Lecture Notes in Computer Science, pages\n593–607. Springer.\nNan Shao, Yiming Cui, Ting Liu, Shijin Wang, and\nGuoping Hu. 2020. Is graph structure necessary for\nmulti-hop question answering? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 7187–7192. Association for\nComputational Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence, February\n4-9, 2017, San Francisco, California, USA , pages\n4444–4451. AAAI Press.\nYueqing Sun, Qi Shi, Le Qi, and Yu Zhang. 2022.\nJointlk: Joint reasoning with language models and\nknowledge graphs for commonsense question answer-\ning. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL 2022, Seattle, WA, United States, July\n10-15, 2022, pages 5049–5060. Association for Com-\nputational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\n14058\nanswering challenge targeting commonsense knowl-\nedge. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4149–4158. Association for Computational\nLinguistics.\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan,\nand Rui Wang. 2020a. Relational graph attention\nnetwork for aspect-based sentiment analysis. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 3229–3238. Association\nfor Computational Linguistics.\nPeifeng Wang, Nanyun Peng, Filip Ilievski, Pedro A.\nSzekely, and Xiang Ren. 2020b. Connecting the dots:\nA knowledgeable path generator for commonsense\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020\nof Findings of ACL, pages 4129–4140. Association\nfor Computational Linguistics.\nXiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu,\nKartik Talamadupula, Ibrahim Abdelaziz, Maria\nChang, Achille Fokoue, Bassem Makni, Nicholas\nMattei, and Michael Witbrock. 2019. Improving nat-\nural language inference using external knowledge in\nthe science questions domain. In The Thirty-Third\nAAAI Conference on Artificial Intelligence, AAAI\n2019, The Thirty-First Innovative Applications of\nArtificial Intelligence Conference, IAAI 2019, The\nNinth AAAI Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019 , pages 7208–\n7215. AAAI Press.\nYichong Xu, Chenguang Zhu, Shuohang Wang, Siqi\nSun, Hao Cheng, Xiaodong Liu, Jianfeng Gao,\nPengcheng He, Michael Zeng, and Xuedong Huang.\n2022. Human parity on commonsenseqa: Augment-\ning self-attention with external attention. In Proceed-\nings of the Thirty-First International Joint Confer-\nence on Artificial Intelligence, IJCAI 2022, Vienna,\nAustria, 23-29 July 2022, pages 2762–2768. ijcai.org.\nYichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu,\nMichael Zeng, and Xuedong Huang. 2021. Fusing\ncontext into knowledge graph for commonsense ques-\ntion answering. In Findings of the Association for\nComputational Linguistics: ACL/IJCNLP 2021, On-\nline Event, August 1-6, 2021, volume ACL/IJCNLP\n2021 of Findings of ACL, pages 1201–1207. Associ-\nation for Computational Linguistics.\nMichihiro Yasunaga, Antoine Bosselut, Hongyu Ren,\nXikun Zhang, Christopher D. Manning, Percy\nLiang, and Jure Leskovec. 2022. Deep bidirec-\ntional language-knowledge graph pretraining. CoRR,\nabs/2210.09338.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nreasoning with language models and knowledge\ngraphs for question answering. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 535–546. Association\nfor Computational Linguistics.\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2022. Kg-fid: Infus-\ning knowledge graph in fusion-in-decoder for open-\ndomain question answering. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 4961–\n4974. Association for Computational Linguistics.\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\nHongyu Ren, Percy Liang, Christopher D. Manning,\nand Jure Leskovec. 2022. Greaselm: Graph reason-\ning enhanced language models for question answer-\ning. CoRR, abs/2201.08860.\nChen Zheng and Parisa Kordjamshidi. 2022. Dynamic\nrelevance graph network for knowledge-aware ques-\ntion answering. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2022, Gyeongju, Republic of Korea, Oc-\ntober 12-17, 2022, pages 1357–1366. International\nCommittee on Computational Linguistics.\nMantong Zhou, Zhouxing Shi, Minlie Huang, and\nXiaoyan Zhu. 2020. Knowledge-aided open-\ndomain question answering. arXiv preprint\narXiv:2006.05244.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020. Freelb: Enhanced\nadversarial training for natural language understand-\ning. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\n14059\nA Relation types\nTable 6 gives all the relation types used in our\nmethod, 19 relations in total. We view all the rela-\ntions as undirected in our experiments.\nRelations Merged Relation\nAntonym\nDistinctFrom Antonym\nAtLocation\nLocatedNear AtLocation\nCapableOf CapableOf\nCauses\nCausesDesire\nMotivatedByGoal\nCauses\nCreatedBy CreatedBy\nIsA\nInstanceOf\nDefinedAs\nISA\nDesires Desires\nHasSubevent\nHasFirstSubevent\nHasLastSubevent\nHasPrerequisite\nEntails\nMannerOf\nHasSubevent\nPartOf\nHasA PartOf\nHasContext HasContext\nHasProperty HasProperty\nMadeof Madeof\nNotCapableOf NotCapableOf\nNotDesires NotDesires\nReceivesAction ReceivesAction\nRelatedTo\nSimilarTo\nSynonym\nRelatedTo\nUsedFor UsedFor\nSameQA SameQA\nDefAs DefAs\nTable 6: HKG involves relationship types. We follow\nthe relationship type defined by (Yasunaga et al., 2021)\nand add “SameQA” and “DefAS” to it, which represent\nthe relationship between key entities and the relation-\nship between key entities and paraphrase entities, re-\nspectively.\nB Details of Datasets\nCommonsenseQA is a multiple-choice QA dataset\nthat requires different types of commonsense\nknowledge to answer questions, with each question\nDatasets Train Dev Test\nCSQA(Official) 9,741 1,221 1,140\nCSQA(IHdata) 8,500 1,221 1,241\nOBQA 4,957 500 500\nTable 7: Statistics of CommonsenseQA (CSQA) and\nOpenBookQA (OBQA).\ncontaining one correct choice and four distracting\nchoices. The dataset has a total of 12,102 ques-\ntions.\nOpenBookQA is a QA dataset focusing on sci-\nentific facts that require a combination of scientific\nfacts or commonsense knowledge to answer. It con-\ntains 5,957 questions, each containing one correct\nchoice and three distracting choices. We conduct\nexperiments on the official split dataset.\nThe statistics for the datasets are shown in Table\n7.\nC Error types and Examples\nTable 8 gives some examples of error analysis.\nEach example gives a part knowledge paths and\nparaphrase descriptions retrieved in multiple knowl-\nedge sources.\n14060\nError typeExamples\nQuestion What is another name for the color of the fur of a dog with light colored fur?Choices ✓fair|✕basket |✕dog hair|✕game |✕sun\nInappropriatePaths for correct answer colorrelatedto−−−−−→palerelatedto−−−−−→fair; colorisa−−→whiterelatedto−−−−−→fair; ...\nparaphrasesPaths for predicted answer furrelateddto−−−−−−→hair; furrelatedto−−−−−→hairballrelateddto−−−−−−→hair;furpartof−−−−→dogmadeof−−−−−→hair; ...(8/50) Correct paraphrase description fair: (used of hair or skin) pale or light colored.Inappropriate paraphrase description fair: a competitive exhibition of farm products.\nQuestion What animal has quills all over it?IndistinguishableChoices ✕feather |✕chicken |✕calligraphy |✕porcupine|✓hedgehog\nknowledgePaths for correct answer quillpartof−−−−→hedgehog; quillpartof−−−−→porcupinerelatedto−−−−−→hedgehog; ...\npaths Paths for predicted answer quillpartof−−−−→porcupine; quillpartof−−−−→hedgehogrelatedto−−−−−→porcupine; ...(17/50) Paraphrase description porcupine: relatively large rodents with sharp erectile bristles mingled with the fur.hedgehog: relatively large rodents with sharp erectile bristles mingled with the fur. ...\nQuestion Kramer wrote a self-referential book. What might that book be about?Lack of Choices ✕counter |✓coffee table|✕school room |✕backpack|✕bedside table\nrelevant Paths for correct answer bookatlocation−−−−−−→coffee table; bookisa−−→magazinerelatedto−−−−−→coffee table; ...\nknowledgePaths for predicted answer bookpartof−−−−→backpack; bookatlocation−−−−−−→satchelrelatedto−−−−−→backpack; ...(13/50) Paraphrase description coffee table: low table where magazines can be placed and coffee or cocktails are served.backpack: a bag carried by a strap on your back or shoulder. ...\nQuestion The pencil sharpener was broken in the classroom, where did the teacher recommend the student go?Choices ✕home|✓library|✕stationery store |✕cabinet |✕desk drawer\nIncomprehensiblePaths for correct answer pencil sharpeneratlocation−−−−−−→library; pencil sharpeneratlocation−−−−−−→deskatlocation−−−−−−→library;\nquestion classroomatlocation−−−−−−→studentatlocation−−−−−−→library; ...\n(10/50) Paths for predicted answer classroomatlocation−−−−−−→ferretatlocation−−−−−−→home; classroomatlocation−−−−−−→doorrelatedto−−−−−→home;\nclassroomatlocation−−−−−−→poetatlocation−−−−−−→home; ...Paraphrase description classroom: a room in a school where lessons take place.pencil sharpener: a rotary implement for sharpening the point on pencils. ...\nTable 8: Error analyse, we divide the error data into four categories\n14061\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn Section 7 Limitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nIn Section 7 Limitations\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nIn Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nIn Section 1 and 3\n□\u0013 B1. Did you cite the creators of artifacts you used?\nIn Section 1\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nYes, we report the details of the dataset used in Section 4.1.\nC □\u0013 Did you run computational experiments?\nIn Section 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nIn Section 4.2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14062\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nIn Section 5.1\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nIn Section 4.4\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14063",
  "institutions": [
    {
      "id": "https://openalex.org/I181877577",
      "name": "Shanxi University",
      "country": "CN"
    }
  ],
  "cited_by": 12
}