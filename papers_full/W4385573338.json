{
  "title": "BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature",
  "url": "https://openalex.org/W4385573338",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3042748897",
      "name": "Giacomo Frisoni",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2121856765",
      "name": "Miki Mizutani",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2129985721",
      "name": "Gianluca Moro",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A3202618215",
      "name": "Lorenzo Valgimigli",
      "affiliations": [
        "University of Bologna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2174775663",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2479527281",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4225909425",
    "https://openalex.org/W4365511667",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4200505254",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W4223492536",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2095869693",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W3104578551",
    "https://openalex.org/W2788330850",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3186799149",
    "https://openalex.org/W2353702165",
    "https://openalex.org/W2154142897",
    "https://openalex.org/W2476140796",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2963967365",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2186936986",
    "https://openalex.org/W2042980227",
    "https://openalex.org/W3125701197",
    "https://openalex.org/W2345054971",
    "https://openalex.org/W2893958209",
    "https://openalex.org/W4285247398",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W3042301927",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3177415603",
    "https://openalex.org/W2970139579",
    "https://openalex.org/W4225727438",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4221160246",
    "https://openalex.org/W2950729111",
    "https://openalex.org/W2951562155",
    "https://openalex.org/W4206490919",
    "https://openalex.org/W3187073926",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2149369282",
    "https://openalex.org/W2914171828",
    "https://openalex.org/W2071879021",
    "https://openalex.org/W3216626696",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W3203042166",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W3121525843",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2294893199",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4220967417",
    "https://openalex.org/W4294554825",
    "https://openalex.org/W2963829526",
    "https://openalex.org/W2170189740",
    "https://openalex.org/W2516071633",
    "https://openalex.org/W2050644191",
    "https://openalex.org/W2223216628",
    "https://openalex.org/W1922686488",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W3205810519",
    "https://openalex.org/W2788448041",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4234889058",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W4226069413",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W3199241049",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4281483318",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4225644300",
    "https://openalex.org/W4200322598",
    "https://openalex.org/W4225378656"
  ],
  "abstract": "The latest batch of research has equipped language models with the ability to attend over relevant and factual information from non-parametric external sources, drawing a complementary path to architectural scaling. Besides mastering language, exploiting and contextualizing the latent world knowledge is crucial in complex domains like biomedicine. However, most works in the field rely on general-purpose models supported by databases like Wikipedia and Books. We introduce BioReader, the first retrieval-enhanced text-to-text model for biomedical natural language processing. Our domain-specific T5-based solution augments the input prompt by fetching and assembling relevant scientific literature chunks from a neural database with ≈60 million tokens centered on PubMed. We fine-tune and evaluate BioReader on a broad array of downstream tasks, significantly outperforming several state-of-the-art methods despite using up to 3x fewer parameters. In tandem with extensive ablation studies, we show that domain knowledge can be easily altered or supplemented to make the model generate correct predictions bypassing the retraining step and thus addressing the literature overload issue.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5770–5793\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nBIOREADER : a Retrieval-Enhanced Text-to-Text Transformer\nfor Biomedical Literature\nGiacomo Frisoni, Miki Mizutani, Gianluca Moro, Lorenzo Valgimigli\nDepartment of Computer Science and Engineering (DISI)\nUniversity of Bologna, Via dell’Università 50, I-47522 Cesena, Italy\n{giacomo.frisoni, gianluca.moro, lorenzo.valgimigli}@unibo.it\n{miki.mizutani}@studio.unibo.it\nAbstract\nThe latest batch of research has equipped lan-\nguage models with the ability to attend over\nrelevant and factual information from non-\nparametric external sources, drawing a comple-\nmentary path to architectural scaling. Besides\nmastering language, exploiting and contextual-\nizing the latent world knowledge is crucial in\ncomplex domains like biomedicine. However,\nmost works in the field rely on general-purpose\nmodels supported by databases like Wikipedia\nand Books. We introduce BIOREADER 1, the\nfirst retrieval-enhanced text-to-text model for\nbiomedical natural language processing. Our\ndomain-specific T5-based solution augments\nthe input prompt by fetching and assembling\nrelevant scientific literature chunks from a neu-\nral database with ≈60 million tokens centered\non PubMed. We fine-tune and evaluate BIORE-\nADER on a broad array of downstream tasks,\nsignificantly outperforming several state-of-the-\nart methods despite using up to 3x fewer pa-\nrameters. In tandem with extensive ablation\nstudies, we show that domain knowledge can\nbe easily altered or supplemented to make the\nmodel generate correct predictions bypassing\nthe retraining step and thus addressing the liter-\nature overload issue.\n1 Introduction\nIn the last decade, deep learning advancements\nhave boosted the development of many solutions\nfor effectively extracting knowledge from biolog-\nical data (Domeniconi et al., 2014a, 2016a) and\nbiomedical literature (di Lena et al., 2015)—widely\naccessible through repositories such as PubMed,\nPMC, and ScienceDirect. Large pre-trained lan-\nguage models (PLMs) have become the dominant\nNLP paradigm, achieving unprecedented results in\na panoply of tasks, from named entity recognition\n(Lee et al., 2020) and semantic parsing (Frisoni\net al., 2021, 2022b) to information retrieval (Moro\n1https://github.com/disi-unibo-nlp\nBiomedical \nScientific \nEvidence\nIndexing\nHeterogeneity in DNA damage within\nthe cell population was observed as a\nfunction of radiation dose\nRelated \ncontext\ngenomic instability\nand mutationCholestyramine: Concomitant * DRUG *\nadministration decreased the mean AUC\nof total * DRUG * approximately 55%.\nDDI-mechanism\nIn which location is dimeric IgA\nmost likely to be found in\nhighest concentrations?\nColostrum\nQuery\nDocument Classification\nRelation Extraction\n...\nNon-knowledge intensive Tasks\nOpen-domain Question Answering\n...\nKnowledge intensive Tasks\nBioREADER \nPMID: 12947391\nRadiation-induced\ngenomic instability is...\nFigure 1: Illustration of BIOREADER , a text-to-text\nmodel conditioned on scientific evidence retrieved from\nan explicit PubMed-based datastore. Every biomedical\ntask is cast as translating text spans with the help of\nexternal domain knowledge retrieved on the fly.\nand Valgimigli, 2021) and document summariza-\ntion (Moro et al., 2022).\nTo justify this success, PLMs have been shown\nto implicitly hold a substantial amount of in-\ndepth knowledge in their parameters (Petroni et al.,\n2019; Davison et al., 2019), resulting from self-\nsupervised learning on extreme-scale text corpora.\nEfforts to this point have mainly focused on\npredictably improving NLP performance by in-\ncreasing datasets, training compute, or model sizes.\nNotably, the most recent Transformer-based solu-\ntions reach up to 1011 parameters (Brown et al.,\n2020; Rae et al., 2021), with benefits due to ex-\ntended memorization of training data (Carlini et al.,\n2021; Tirumala et al., 2022). However, encoding\nall factual and domain-specific competencies into\nopaque weight matrices is inefficient, especially for\nspecialized, dynamic, and trust-demanding fields\nlike biomedicine (Lötsch et al., 2021)—where\nthe volume of scientific publications evolves and\ngrows continuously (Landhuis, 2016). Indeed,\ncapturing more world facts requires training ever-\n5770\nlarger networks, which can be prohibitively slow\nor expensive. Similarly, changing what a PLM\nknows entails retraining the entire model with\nnew documents. Moreover, high-dimensional non-\ninterpretable parametric spaces make it difficult\nto determine what knowledge is stored where, to\nupdate the theoretical background, or to provide\nprovenance for decisions.\nRecent developments in the field (Lewis et al.,\n2020b; Nakano et al., 2021; Borgeaud et al., 2021)\nhave reversed the architectural scaling trend by\nshowing that smaller PLMs can perform on par\nwith massive models if we augment them with\na way to search for external information. The\nkey intuition is following a retrieve-then-predict\napproach by asking the PLM to directly fetch\npotentially relevant unlabeled world knowledge—\neven structured (Yasunaga et al., 2021)—from\nhighly-comprehensive datastores, and use it as ad-\nditional context during inference. Remarkably,\nsemi-parametric (or hybrid) contributions com-\nbine \"closed-book\" (parametric-only) and \"open-\nbook\" (retrieval-based) methods to complement\neach source. They allow for revising or even sup-\nplementing knowledge dynamically, treating the\nlatter in a more modular and interpretable way.\nOn the other hand, semi-parametric models have\nso far been only investigated for general-domain\nknowledge bases and NLP tasks, e.g., context-free\nquestion answering conditioned on Wikipedia and\nBooks evidence. Based on previous publications\n(Beltagy et al., 2019; Lee et al., 2020; Gu et al.,\n2022), the word distribution shift from general cor-\npora to health informatics corpora prevents or seri-\nously limits the direct application of such models\nto biomedical NLP.\nIn this paper, we introduce BIOREADER , the\nfirst retrieval-enhanced transformer for biomedi-\ncal literature, empowered by a differentiable access\ntowards a large-scale text memory grounded on\nPubMed (≈60M tokens). We continue and build\non a broad spectrum of retrieval work in the re-\nsearch community (Li et al., 2022), exploring effi-\ncient means of augmenting biomedical PLMs with\na domain-specific memory, avoiding expanding\ncomputations significantly. With BIOREADER , the\nbiomedical knowledge is not necessary to be im-\nplicitly stored in model parameters but is explic-\nitly acquired in a plug-and-play manner, leading to\ngreat scalability. Mechanically, BIOREADER is a\nnovel encoder-decoder model based on T5 (Raffel\net al., 2020) and RETRO (Borgeaud et al., 2021),\nwith a frozen neural retriever. It splits the input se-\nquence into chunks and autoregressively retrieves\nscientific text semantically similar to the previous\nfragment. In this way, it expands the input prompt\ncontext for better-predicting tokens in the current\nchunk thanks to a cross-attention mechanism.\nOur main contributions are the following:\n1. We devise BIOREADER , a novel text-to-text\nbiomedical language model fusing memory\nretrieval and generative components (§3).\n2. We advance biomedical NLP research, push-\ning the state-of-the-art in several knowledge-\nintensive and non-knowledge-intensive tasks\nvia fine-tuning (§4 and §5), outperforming\nprevious methods by a significant margin with\nup to 3x fewer parameters. We extensively\nprove the contribution of each module with\nablation studies.\n3. We show that BIOREADER can be improved\nat evaluation time by updating the knowledge\nbase and the number of retrieved neighbors\nwithout retraining (§5), also offering qualita-\ntive benefits in terms of interpretability.\n2 Related Work\nWe first give a bird’s-eye view of existing work\non biomedical language modeling and retrieval-\nenhanced neural networks (see Table 1).\nBiomedical Language Models Transformer-\nbased PLMs have become the first choice for any\ntask in biomedical NLP, counting 40+ models pro-\nposed in just two years (Kalyan et al., 2022). Do-\nmain adaption milestones include contributions\nlike BIOBERT (Lee et al., 2020), PUBMEDBERT\n(Gu et al., 2022), BIOMEGATRON (Shin et al.,\n2020), and SCIFIVE (Phan et al., 2021), as well as\nknowledge-enhanced encoders (Liu et al., 2021).\nCrucially, existing models still struggle to encap-\nsulate high amounts of biomedical knowledge in\ntheir parameters (Frisoni et al., 2020b; Meng et al.,\n2022). As far as we can tell, we are the first to\ninspect retrieval-enhanced biomedical text genera-\ntion, where open-book language models are osten-\nsibly scarce.\nRetrieval-Augmented Neural Networks The re-\ntrieval and knowledge grounding paradigms have\nlately attracted many computational linguists, aim-\ning to design modular architectures capable of sep-\narating memory storage and computational pro-\n5771\nModel Granularity Retriever training Retrieval\nintegration\nUnsupervised\nRetriever\nRetrieval\nSource Task(s)\nkNN-LM\nKhandelwal et al. (2020)Token Frozen (Transformer)Add to probs ✓ Wikipedia, BooksLM\nSPALM\nYogatama et al. (2021a)Token Frozen (Transformer) Gated logits ✓ Wikipedia OpenQA\nDPR\nKarpukhin et al. (2020)Prompt Contrastive proxyExtractive QA Wikipedia OpenQA\nREALM\nGuu et al. (2020) Prompt End-to-End Prepend to prompt ✓ Wikipedia OpenQA\nRAG\nLewis et al. (2020b)Prompt Fine-tuned DPR Cross-attention\n(concatenation) Wikipedia OpenQA, QG, FV\nFID\nIzacard and Grave (2021)Prompt Fine-tuned DPR Cross-attention Wikipedia OpenQA\nEMDR2\nSachan et al. (2021)Prompt End-to-end Cross-attention✓ Wikipedia OpenQA\nRETRO\nBorgeaud et al. (2021)Chunk Frozen (BERT) Chunked\ncross-attention✓ Web, Books, News,\nWikipedia, GitHubOpenQA\nBIOREADER\n(ours) Chunk Frozen (CONTRIEVER) Chunked\ncross-attention✓ PubMed† NER, RE, DC,\nNLI, QA, OpenQA\nTable 1: Comparison of BIOREADER with existing retrieval approaches. LM = language modeling, QG = question\ngeneration, FV = fact verification, NER = named entity recognition, RE = relation extraction, DC = document\nclassification, NLI = natural language inference, (Open)QA = (open-domain) question answering. † highlights\nretrieval sources that are different from training data.\ncessing. A popular strategy (Chen et al., 2017;\nYang et al., 2019; Nie et al., 2019) relies on collect-\ning passages employing untrained sparse-vector\nretrieval methods with inverted index matching,\nsuch as TF-IDF (Domeniconi et al., 2015) and\nBM25 (Robertson and Zaragoza, 2009), eventually\nimproved by re-ranking (Wang et al., 2018). Other\nworks identify relevant neighbors through latent\ntopic modeling (Wei and Croft, 2006; Domeniconi\net al., 2016c), edit-distance (Zhang et al., 2018;\nGu et al., 2018), or algebraic methods (Domeni-\nconi et al., 2016b; Frisoni et al., 2020a; Frisoni\nand Moro, 2020; Frisoni et al., 2020c). The source\ndatabase may also be structured (Ahn et al., 2016;\nYasunaga et al., 2021), and graphs may serve as a\nfoundation for non-parametric retrievers guided\nby entity links to find chains of evidence doc-\numents (Asai et al., 2020). With the success\nof deep learning, retrieving systems have partly\nswitched to dense learned semantic representations\nand distances in embedding spaces, mostly involv-\ning BERT-based architectures. Encodings can be\npre-computed and indexed offline for greater ef-\nficiency and scalability (Grave et al., 2017; Seo\net al., 2018; Khandelwal et al., 2020; Yogatama\net al., 2021b; Borgeaud et al., 2021). For instance,\nkNN-LM (Khandelwal et al., 2020) extends a PLM\nby linearly interpolating its next token distribution\nwith a k-nearest neighbors mechanism, without in-\ncorporating the retrieval process into the training\npipeline. In this sense, k-nearest-neighbors have\nbeen largely investigated in NLP tasks (Domeni-\nconi et al., 2014b). Retrieval metrics may also be\nlearned from data in a task-dependent way instead\nof relying on pre-existing PLMs. Following this\nvision, DPR (Karpukhin et al., 2020) fine-tunes two\nBERT models utilizing a contrastive loss to align\nlabeled query and key embeddings, thereby work-\ning with passage (or chunk) granularities. RAG\n(Lewis et al., 2020b) and FID (Izacard and Grave,\n2021) build upon DPR to incorporate retrieval into\nseq2seq models. Nevertheless, source-target pairs\npreclude the use of abundant unlabeled data. Still,\nexcept for RAG, the retriever network is trained in\nisolation from the downstream task. To overcome\nthis potential issue, end-to-end approaches have\nbeen recently proposed, including REALM (Guu\net al., 2020) and EMDR 2 (Sachan et al., 2021), also\nexploiting unsupervised pre-training objectives to\nreward informative retrieval, like perplexity maxi-\nmization and inverse cloze task as in SPALM (Lee\net al., 2019). On the flip side, joint retriever-reader\nlearning comes with the extra complexity of back-\npropagating while making queries on an entire cor-\npus and periodically updating the embedding ta-\nble, severely limiting scalability. In open-ended\ntext generation, BLENDER BOT 2.0 (Komeili et al.,\n2022) and WEBGPT (Nakano et al., 2021) learn\nto make contextualized internet search queries to\nleverage up-to-the-minute information, but need\nhuge amounts of annotations. Most pertinent to\nour work is RETRO (Borgeaud et al., 2021), a flexi-\n5772\nble autoregressive PLM conditioned on document\nchunks retrieved from trillions of tokens, signifi-\ncantly outperforming GPT-3 (Brown et al., 2020)\nwith an order of magnitude fewer parameters. Like\nkNN-LM , SPALM and RETRO , BIOREADER uses\nfrozen retrieval representations to easily accom-\nmodate the biomedical literature evolution, not re-\nquiring retraining in the event of a knowledge base\nchange. Inspired by the promises of RETRO , whose\ncode and models have not been released, BIORE-\nADER processes arbitrary text sequences by reason-\ning at a sub-sequence level and retrieving different\nbiomedical passages for the different chunks of a\nsequence, thus allowing for repeated retrieval dur-\ning text generation. As suggested by the latest re-\nsearch thread for future directions (Borgeaud et al.,\n2021), we adopt a brand-new architecture derived\nfrom T5 to depend more on the encoder output at\ninference time. T5 fine-tuning has become a sta-\nple of natural language generation, marking off the\nprominent technique of many tasks (Paolini et al.,\n2021; Geng et al., 2022; Frisoni et al., 2022a), char-\nacterized by better grammatical correctness and\ntransfer learning. In contrast to the majority of\nworks that either interpolate output probabilities\n(Khandelwal et al., 2020) or use input concatena-\ntion (Yogatama et al., 2021a; Lewis et al., 2020b;\nGuu et al., 2020) to combine retrieved documents,\nBIOREADER separately encodes input prompts and\nneighbors, then assembled with a chunked cross-\nattention. Our work contrasts previous efforts on\nbuilding T5 closed-domain models without access\nto any external context for knowledge-intensive\ntasks (Roberts et al., 2020).\nMulti-Task Retrieval Prior work has shown\nthat retrieval improves performance across various\nNLP tasks—especially extractive (Guu et al., 2020;\nLee et al., 2019)—when considered in isolation.\nSuch downstream tasks include open-domain ques-\ntion answering (Chen et al., 2017), fact-checking\n(Thorne et al., 2018), machine translation (Zhang\net al., 2018), multi-document summarization (Moro\net al., 2022), and data-to-text (Su et al., 2021). As\nfirst proposed by RAG, we demonstrate that a sin-\ngle retrieval-based seq2seq architecture can outper-\nform several abstract biomedical benchmarks, also\nin multi-task settings. Further, we evince the ad-\nvantages of merging retrieval and generative com-\nponents also in non-knowledge-intensive tasks.\n3 Method\nMotivated by a recent stream of architectural con-\ntributions and training modalities (Guo et al., 2021;\nSanh et al., 2021), our proposed text-to-text model\nBIOREADER extends T5 with the nimble ability\nto generate a sequence conditioned by a collection\nof passages retrieved from a specialized datastore\nwith several millions of biomedical evidence to-\nkens, other than the input. Coarse-grained retrieval\nof contiguous token chunks allows us to retain stor-\nage and computation requirements. The unique\ncharacteristics of the biomedical text and the re-\nlated design choices are summarized in §A. Fig-\nure 2 sketches the overall framework.\nInput segmentation We split the tokenized in-\nput X (max-length n) into a sequence of lchunks\nof size m=n\nl. We use n=512 and m=16. Our ap-\nproach uses retrieval as a way to augment input\nexamples at the granularity of small chunks.\n3.1 Retrieving scientific evidence\nEvidence Datastore We use a retrieval pool dif-\nferent from the training corpus, which is suit-\nable for domain adaptation and knowledge update\n(Li et al., 2022). Our database D is an external\nkey-value store queried during inference. We de-\nrive D from PubMed-RCT (Dernoncourt and Lee,\n2017), consisting of ≈200K English abstracts of\nrandomized controlled trials (RCTs) from the 2016\nMEDLINE/PubMed Baseline Database. We fo-\ncus on RCTs as they are commonly considered\nthe best source of medical evidence (Dickersin and\nLi, 2015). Let f(⋅) be the function that maps a\ntextual context to a fixed-length vector represen-\ntation given by a frozen non-causal bi-directional\nencoder. Within D, each value consists of two con-\ntiguous unlabeled chunks [N,F ], where N is the\nneighbor chunk and F is its continuation in the\noriginal abstract. The corresponding key is f(N),\npre-computed to enable online database modifi-\ncation and retrieval from huge amounts of data.\nUsing both N and F as retrieved tokens helps in-\ncrease model performance (see §D). We implement\nf(⋅) using CONTRIEVER (Izacard et al., 2021), a\ndual-encoder architecture based on BERT-base un-\ncased (WordPiece tokenizer) and trained with the\nMoCo contrastive loss (He et al., 2020) for unsu-\npervised retrieval, where queries and documents\nare encoded independently using the same model.\nAverage pooling is applied over the outputs of the\nlast layer to obtain one-vector representations. Our\n5773\nContriever \nEncoder\nContriever \nEncoder\nNeighbors &\ncontinuations\ntop-K ⨂ \nFrozen kNN Neural Retriever\nSelf-Attention\nAdd & Normalize\nCCA\nEncoder-Decoder Attention\n...\nRETRO BLOCK #i \n...\n...\nT5-AUGMENTED DECODER\nPositional\nEncoding\nEmbedding\nShifted right X\nT5 \nENCODER\nPositional\nEncoding\nEmbedding\nX\nE(K, V)\n(Q)\nInput Tokens X\nH H\nH\n1 2 H3\nH1\n+\nC C1 2 C3\nCA H2\n+\nCA\nEnc. N1\nEnc. N2\nAttend Retrieved\nTokens [N, F]\nChunked Cross-Attention (CCA)[N1, F1]\n[N2, F2]\n[N1, F1]\n[N2, F2]\nFigure 2: An illustration of the BIOREADER architecture. Left: simplified version where a sequence X of length\nn=15 is divided into l=3 chunks of size m=5. For each chunk, we retrieve k=2 scientific evidence neighbors of\nr=10 tokens each (including continuations). The current input prompt X and the fetched tokens are given as input\nto our encoder-decoder architecture based on T5. The fusion of their learned representations is done in the decoder\nvia chunked-cross attention (CCA). Right: autoregressive CCA interaction details.\ndocument index can be seen as a large external\nhuman-readable/writable memory for PLMs to at-\ntend to, on a par with memory networks (Moro\net al., 2018).\nNearest neighbor retrieval For each input\nchunk Cu with u∈{1,l}, we select its top k\nmost similar documents using the dot product\nd(Cu,N)=f(Cu)⊗f(N)(empirically better than\nL2 distance according to preliminary experiments).\nTime- and memory-efficient retrieval is performed\nusing FAISS (Johnson et al., 2021), an open source\nlibrary for approximate nearest neighbor search\nin high dimensional spaces (sub-linear memory\naccess). We denote retrieved token-values as\nRET(Cu)=([N1,F1],..., [Nk,Fk]). A length\nof 16 is used for both Nj and Fj, thus RET(Cu)\nhas a k×rshape, with r=32.\n3.2 Model architecture\nBIOREADER relies on an extended T5 architecture,\nreceiving X chunks as input. Concretely, we keep\nthe T5 encoder unchanged, while we interleave the\nRETRO -blocks proposed by Borgeaud et al. 2021\nand standard T5-blocks in the decoder—design\nchoices are motivated in §D. Symbolizing inter-\nmediate input activations by H ∈ Rn×d, RETRO -\nblocks incorporate information also from the en-\ncoded neighbors E—for which we take the T5-\nencoded RET(Cu) tokens, already supplied with\npositional information. RETRO -blocks compose\nthree different residual operators with signature\nRn×d→Rn×d: a fully connected layer FFW, a stan-\ndard self-attention layer ATT, and a chunked cross-\nattention layer CCA(⋅,E).\nRETRO (H,E) =FFW(CCA(ATT(H),E)) (1)\nT5(H) =FFW(ATT(H)) (2)\nThe hyperparameter P⊆[1,L]determines at which\nlayers a RETRO -block is placed in the decoder\nstack; referring to T5-base (L=12,d=768), we use\nP={9,12}. In these points, the neighbor encodings\nand the trainable BIOREADER input encodings are\nmerged with CCA, replacing the original encoder\noutputs; the resulting representation is then used\nfor the Encoder-Decoder Attention. Our final con-\nfiguration consists of 229.5M parameters.\nChunked cross-attention Following Borgeaud\net al. 2021, we use chunked-cross attention (CCA),\nan autoregressive operator that incorporates the\nretrieved literature evidence into the model. To\ncompute it, a given activation H is first di-\nvided into l−1 chunks, shifting the tokens com-\nposing each chunk of one position to the left\n(H+\nu=(hum+i−1)i∈[1,m] ∈ Rm×d)u∈[1,l−1]. Note\nthat H+\nu holds the embeddings of the last token\nin chunk Cu and of the first m−1 tokens in Cu+1.\nThe cross-attention between H+\nu and Eu is then cal-\nculated across time and neighbors simultaneously.\n5774\nThis means that each input chunk attends only to\nthe neighbors of the preceding chunk; autoregres-\nsion is ensured by the one-token overlap (i.e., the\ndependencies over previous neighbors are propa-\ngated via self-attention operations). The activations\nof the ith token in the uth chunk therefore poten-\ntially depend upon the set of all previous neighbors\nand continuations RET(Cu′)u′<u. A graphical ex-\nample is reported in §F.\nRetrieval-enhanced autoregressive decoding\nToken likelihoods during decoding (i-th token, u-\nth chunk)—with BIOREADER parameterized by\nθ—depend on both previously seen tokens PXu,i\nand antecedent-chunk neighbors PNu recovered\nby D, nailing down a retrieval-enhanced sequence\nlog-likelihood, following Borgeaud et al. 2021.\nPXu,i =x(u−1) m+i∣(xj)j<(u−1) m+i, (3)\nPNu =(RET(Cu′))u′<u, (4)\nL(X∣θ,D)=\nl\n∑\nu=1\nm\n∑\ni=1\nℓθ(PXu,i,PNu). (5)\nWe can select the next token by directly sam-\npling from the D-conditioned distribution with log-\nprobability ℓ. Specifically, we adopt the standard\nT5 greedy decoding approach and setRET(C1)=∅,\nnamely, the likelihood of tokens from the first\nchunk does not depend on any neighbor.\nDifferences compared to RETRO Differently\nfrom RETRO , BIOREADER is characterized by\nan original T5 skeleton (instead of GPT and a\ndecoder-modulated transformer encoder), differ-\nent architectural and retrieval pool design choices,\na biomedical-specific model (also for encoding re-\ntrieved tokens), a non-training-based retrieval pool,\nand a dual-encoder retrieval module trained specifi-\ncally for fetching short documents given a query.\n4 Experimental Setup\nImplementation details, computing infrastructure,\nand experiment hyperparameters are described in\n§B.1 and §B.2.\n4.1 Training\nPre-training corpora In biomedical PLMs, there\nare two main sources for pre-training corpora:\nPubMed abstracts and PMC articles. As demon-\nstrated by prior work (Phan et al., 2021; Gu et al.,\n2022), training on both corpora surprisingly leads\nto a slight degradation in performance compared to\nsolely training on PubMed abstracts. Consequently,\nwe operate on a cleaned and masked version of the\nPubMed database (>32M abstracts)2.\nPre-training and fine-tuning setup We first ini-\ntialize BIOREADER ’s T5-blocks by loading the pre-\ntrained weights of SCIFIVE (PubMed)-base, a state-\nof-the-art T5 model pre-trained on large biomedi-\ncal corpora. Referring to the findings of Borgeaud\net al. 20213, we freeze all the pre-trained weights\nand train only the new CCA parameters (less than\n5% of total weights) with span-based mask learn-\ning. Thus, the original SCIFIVE performance is\nprecisely maintained when BIOREADER language\nmodeling is evaluated without retrieval. We pre-\ntrain CCA layers by using only ≈3% of pre-training\ncorpora instances (≈982K)—a design choice sup-\nported by Borgeaud et al. 2021 as well. Consis-\ntently with the authors of RETRO , we find that\nallowing the entire model to resume training at this\nstage ends in performance worsening. Since the\nsize of PubMed abstracts rarely exceeds 512 to-\nkens (Yuan et al., 2022), we truncate all the input\ntexts to a 512 maximum length for the sake of pre-\ntraining efficiency. Subsequently, we fine-tune all\nthe layers on the supervised target tasks (see §4.2),\nalso making use of a task-specific prefix to let the\nmodel know the requested transformation for each\ninput. During training, we retrieve from D with 9\nneighbors; we raise and reduce the kvalue in the\nevaluation phase (§D).\nObjective BIOREADER is trained with a max-\nimum likelihood objective using teacher forcing\n(Raffel et al., 2020) for all tasks so as to unlock\nmulti-task learning.\n4.2 Downstream Benchmark Tasks\nWe fine-tune BIOREADER on 18 widespread NLP\nhuman-annotated biomedical datasets for 6 down-\nstream task categories. Evaluation datasets mostly\ncome from BLURB (Gu et al., 2022), a broad-\ncoverage benchmark for PubMed-based biomedical\nNLP applications, tracking progress by the com-\nmunity. Unless otherwise specified, we follow the\nsame preprocessing techniques and train/dev/test\nsets as Phan et al. 2021. Descriptive statistics and\npreprocessing details are in §C.\n2gs://scifive/pretrain/pubmed_cleaned\n3Uniquely tuning the new weights of CCA-augmented\nPLMs attains results close to full training from scratch, quickly\nsurpassing the performance of baseline models.\n5775\n• Named entity recognition (NER) . Locate\nand classify named biomedical entities us-\ning IOB tagging (Ramshaw and Marcus,\n1995). We take into account 7 influential\ndatasets: NCBI-disease (Dogan et al., 2014),\nBC5CDR-disease (Li et al., 2016), BC5CDR-\nchemical (Li et al., 2016), BC4CHEMD\n(Krallinger et al., 2015), BC2GM (Smith et al.,\n2008), JNLPBA (Collier and Kim, 2004), and\nSpecies800 (Pafilis et al., 2013). For all NER\ntasks, we evaluate results with the entity-level\nF1-score, ensuring fairness with the other\nbaselines. We italicize that the entity-level\nevaluation does not count the partial predic-\ntion of an entity as true (if the entity has more\nthan one token), tending to show lower scores\nthan plain F1.\n• Relation extraction (RE) . Detect and clas-\nsify semantic relationships involving biomed-\nical entities. We test on CHEMPROT (Do-\ngan et al., 2019) and DDI (Herrero-Zazo\net al., 2013) for chemical-protein and disease-\ndisease interactions, respectively. We evaluate\nthe F1-score of each class in the two datasets.\n• Natural language inference (NLI) . Deter-\nmine the validity of a hypothesis (i.e., true or\nfalse). We utilize the MedNLI dataset from\nMIMIC-III (Romanov and Shivade, 2018)\nwith an accuracy-based evaluation.\n• Document classification (DC). Assign a text\ndocument to a predetermined category. We\nconsider the HoC dataset (Baker et al., 2016),\njudging the F1-score on the sample average.\n• Question answering (QA). Find an answer\nto a question from a gold context snippet. We\ntake factoid questions from BioASQ 4b, 5b,\nand 6b challenges (Tsatsaronis et al., 2015).\n• Open-domain QA (OpenQA). Answer nat-\nural questions without relying on any spec-\nified context paragraph. OpenQA is a com-\nmon knowledge-intensive testbed for retrieve-\nthen-generate models. We refer to MedQA-\nUSMLE (Jin et al., 2021), a 4-way multiple\nchoice QA benchmark entailing biomedical\nand clinical knowledge, where the questions\noriginate from practice tests for the United\nStates Medical License Exams (USMLE). We\npreprocess the dataset by treating questions\nand correct answers as input-output text pairs.\nIn all cases where a question refers to a spe-\ncific set of answers (e.g., \"which of the fol-\nlowing...\"), we append the possible choices to\nthe input text.\nFor QA and OpenQA, we observe standard con-\nventions and evaluate the predicted free text with\nthe Exact Match metric, as initiated by Rajpurkar\net al. 2016. A generated answer (lenient for QA)\nis considered correct if it matches any reference\nanswer after normalization (i.e., lowercasing and\nremoval of articles, punctuation, and duplicated\nwhitespace).\nComparison We head-to-head compare BIORE-\nADER to representative closed-book PLMs, en-\ncompassing prevalent BERT-based models re-\nquiring task-specific architectural choices (predic-\ntion heads) and more flexible encoder-decoder\ngenerative models. The first category includes\nBIOBERT (Lee et al., 2020), SCIBERT (Beltagy\net al., 2019), BLUE BERT (Peng et al., 2019), CLIN -\nICAL BERT (Alsentzer et al., 2019), PUBMEDBERT\n(Gu et al., 2022), PUBMEDELECTRA (Tinn et al.,\n2021), BIOLINK BERT (Yasunaga et al., 2022),\nand BIOMEGATRON (Shin et al., 2020). The\nsecond, T5 (Raffel et al., 2020) and SCIFIVE\n(Phan et al., 2021). Please note that BIOMEGA -\nTRON authors evaluate NER performance by la-\nbeling sub-tokens separately, except for the NCBI-\ndisease dataset, where they observe better results\nwith whole-entity labeling. We also mention BIO-\nROBERTA (Lewis et al., 2020a) and BIOBART\n(Yuan et al., 2022), which are not included due\nto the impossibility of replicating them on the\nBLURB dataset splits.\n4.3 Qualitative Analysis\nOnline evidence datastore update Facts memo-\nrized within traditional PLMs are opaque and stuck\nin time at the point of training (Lazaridou et al.,\n2021). Such static knowledge fails to cope with the\ndynamic state of the biomedical world, where more\nthan 3 papers are registered per minute (Frisoni\net al., 2021). With BIOREADER , we can control\nwhat the model knows by swapping out or integrat-\ning the documents it uses for knowledge retrieval.\nWe test this behavior by adding to D the abstracts\nof 10 recent RCTs on COVID-19 4, checking for\nfactual answers to target open questions without\nretraining. We consider the OpenQA-tuned model,\nassessed on two relevant questions created by us\n4We select RCTs by employing \"review covid19 symp-\ntoms\" (x3), \"review covid19 prevention\" (x3), and \"review\ncovid19\" (x4) as keywords on the PubMed search engine.\n5776\naccording to new RCT contents.\nQuestion answering human evaluation In QA\nand OpenQA benchmarks, BIOREADER outputs\nfull-sentence answers that often do not correspond\nto the ground truth but continue to be semantically\ncorrect. We hypothesize that exact matching under-\nestimates our model performance. For this reason,\nwe hire three expert human annotators proficient in\nEnglish and with biomedical competencies to man-\nually scrutinize model predictions, randomly sam-\npling 120 test set instances (30 from each BioASQ\ndataset and 30 from MedQA-USMLE). We ask the\ngraders to (i) binary label the scientific accuracy\n(factual correctness) of the generated answers, (ii)\nassess language fluency on a 3-point Likert scale\nfrom 1 (worst) to 3 (best). Human evaluation is\nconducted on SCIFIVE and BIOREADER outputs\n(presented in random order) to inspect the retrieval-\naugmentation contribution.\n5 Results\nTable 2 and Table 3 showcase our main results. Our\nscores come from the checkpoint with the lowest\nloss and the best kdiscovered at evaluation time.\nWe push the state-of-the-art on 2/7 NER, 1/2 RE,\n1/1 DC, and 3/3 QA, staying highly competitive\nin all the other cases. We beat SCIFIVE -large (3x\nour size) on 5 different tasks, while, in the major-\nity of cases, we considerably outperform models\nwhich have a comparable number of parameters\nto ours. Overall performances testify to consistent\nretrieval effectiveness. Predicting tokens with the\naid of relevant human-written references alleviates\nthe difficulty of text generation. As expected, we\nnotice that NER, RE, and NLI are the tasks where\nBIOREADER contributes less. Naturally, all these\ntranslations are strictly related to the provided in-\nputs and hardly take advantage of additional exter-\nnal context, which often acts as noise.\nThe strength of BIOREADER is accentuated\nwhen limited training data is available; we point\nup that our biomedical benchmarks only have few\nthousand annotated instances (§C).\nUnexpectedly, the adoption of in-domain vocab-\nularies appears to be non-correlated with higher\ndownstream task scores.\nFrom Table 3, we verify our conjecture about the\ninsufficiency of Exact Match as an OpenQA evalu-\nation metric. Expert assessment results are signif-\nicantly higher than the automatic ones. The aver-\nage Kendall’s coefficient ([−1,1] bound) among\nall evaluators’ inter-rater agreement is 0.91. We\nrecognize many cases where the predicted answer\nis correct though different from the ground truth\n(e.g., \"pd-1\" vs. \"programmed cell death 1\", \"olfac-\ntory groove meningioma\" vs. \"meningioma\"). Our\nqualitative analysis suggests that neighbors help\nthe model to produce not only more syntactically\nfluent but also more factually correct outputs.\nAlthough not retrained, BIOREADER adapts cor-\nrectly to unseen questions on the COVID-19 lit-\nerature in \"zero-shot datastore\" settings (Table 4).\nThis suggests that it learns to use world informa-\ntion independently of the information itself. Input-\noutput examples, accompanied by their retrieved\nneighbors, are exhibited in §F.\nReplicating our solution (see §B) only asks\nfor CCA calibration and task-specific model fine-\ntuning, ultimately saving a vast amount of compu-\ntation and memory. Our model can be handled on\na single GPU machine, while a fully end-to-end\nretriever generally demands industry-scale compu-\ntational resources for training (Seo et al., 2019).\n6 Conclusions\nIn this paper, we introduce BIOREADER , a new\nstate-of-the-art semi-parametric biomedical lan-\nguage model steered by literature passages re-\ntrieved from explicit memory. Our experimental re-\nsults show that augmenting the generation process\nby accessing scientific repositories during training\nand evaluation induces performance gains greater\nthan raw parameter scaling, both on knowledge-\nintensive and non-knowledge-intensive tasks. Not\nonly do we provide a way of handling the opaque-\nness of large language models, but we also prove\nthat updating the datastore helps the model with\ndomain adaption without retraining, a property\nof paramount importance for rapidly evolving do-\nmains like biomedicine. Future work should aim to\nintegrate a differentiable write-to-memory opera-\ntor (Wu et al., 2022), structured retrieval databases\n(e.g., multi-relational graphs from semantic pars-\ning, symbolic knowledge graphs), long text-to-\ntext tasks (Guo et al., 2021; Moro and Ragazzi,\n2022), knowledge-augmented self-alignment pre-\ntraining to rewire the space before retrieval (Liu\net al., 2021), and the evaluation of distributed and\nparallel learning approaches to scale to big data\nrepositories (Cerroni et al., 2013).\n5777\nModel #params In-DomainVocabulary\nNER(F1) RE(F1) DC(F1*)NLI(Acc.)NCBIdiseaseBC5CDRdiseaseBC5CDRchemicalBC4CHEMD BC2GM JNLPBA Species-800 ChemProt DDI HoC MedNLI\nBIOBERT†,‡ 110M ✓ 89.71 87.15 93.47 92.36 84.72 77.49 74.06 76.46 80.88 81.54 —SCIBERT‡ 110M ✓ 88.25 84.70 92.51 — 83.36 78.51 — 75.00 81.22 81.16 —BLUEBERT-base‡ 110M × 88.04 83.69 91.19 — 81.87 77.71 — 71.46 77.78 80.48 —CLINICALBERT‡ 110M × 86.32 83.04 90.80 — 81.71 78.07 — 72.04 78.20 80.74 —PUBMEDBERT-base‡ 110M ✓ 87.82 85.62 93.33 — 84.52 79.10 — 77.24 — — —PUBMEDBERT-large§ 340M ✓ 88.25 85.77 93.22 — 84.72 79.44 — 78.77 82.39 82.57 —PUBMEDELECTRA-base§ 110M ✓ 87.68 84.99 93.19 — 83.79 78.60 — 76.54 80.58 81.45 —PUBMEDELECTRA-large§ 340M ✓ 87.93 84.82 92.90 — 83.87 78.77 — 76.80 78.92 82.37 —\nBIOLINKBERT-base∥ 110M × 88.18 86.10 93.75 — 84.90 79.03 — 77.57 82.72 84.35 —\nBIOLINKBERT-large∥ 340M × 88.76 86.39 94.04 — 85.18 80.06 — 79.98 83.35 84.87 —BIOMEGATRON¶ 345M ✓ 87.10 88.50 92.90 — — — — 77.00 — — —T5-base† 220M × 88.54 86.83 93.61 89.73 82.29 74.56 74.32 84.82 82.04 85.22 83.90T5-large† 770M × 88.78 86.31 94.22 89.96 82.36 75.83 74.66 85.41 83.35 85.68 83.80SCIFIVE-base† 220M × 87.96 87.44 94.35 92.02 83.92 75.60 76.55 88.8383.15 85.89 85.30SCIFIVE-large† 770M × 89.17 86.98 94.66 91.96 83.60 76.08 75.50 87.88 83.67 86.3686.36BIOREADER(ours) 229.5M × 88.90 87.62 94.43 92.81 84.77 77.82 77.44 88.1684.3487.7885.76\nTable 2: Test results on NER, RE, DC, and NLI after fine-tuning. F1* is F1 on sample average. Bold and\nunderline denote the best and second best scores; the gradient of green indicates our improvement compared to\nthe previous state-of-the-art (the deeper, the more). †, ‡, §, ∥ and ¶ baseline results (correctly replicated except for\nPUBMEDBERT-large, PUBMEDELECTRA , BIOMEGATRON ) are from Phan et al. (2021), Gu et al. (2022), Tinn\net al. (2021), Yasunaga et al. (2022), and Shin et al. (2020), respectively. “—” denotes no results are available.\nModel # params In-DomainVocabulary\nAutomatic Evaluation Human EvaluationQA OpenQA QA OpenQA Fluency (Avg)BioAsq 4b BioAsq 5b BioAsq 6b MedQA-USMLE BioAsq 4b BioAsq 5b BioAsq 6b MedQA-USMLEBIOLINKBERT-base 110M × — — — 40.00 — — — — —BIOLINKBERT-large 340M × — — — 44.60 — — — — —SCIFIVE-base 220M × 60.80 59.53 55.56 34.57 79.98 80.02 70.05 38.03 2.49SCIFIVE-large 770M × 62.98 61.67 61.74 35.12 80.23 80.12 71.54 39.78 2.65BIOREADER(ours) 229.5M × 64.13 62.02 62.18 42.96 82.12 81.88 73.35 48.57 2.86\nTable 3: Exact Match accuracy (left) and human-evaluated scientific accuracy (right) on QA and OpenQA tasks.\nBold and underline denote the best and second best scores; our relative human evaluation improvement compared\nto the baseline is picked out with green gradients (the deeper, the more).\nQuestion B IOREADERw/D BIOREADERw/D′\nmedqa: question*: January 2020. A 69-year-old Chinese man comes to the physician\nwith fever, tiredness, cough, dyspnoea, and severe respiratory issues. The clinical\npicture suggests an infectious disease. What is the most likely diagnosis?\n✗ bronchiolitis ✓ COVID-19\nmedqa: question*: Coronaviruses are viruses that can cause illnesses in humans,\nincluding severe respiratory disease and even death. Corona disease-19 virus\n(COVID-19) spread and caused a pandemic that affected people all over the world.\nAs COVID-19 cases continue to rise globally, which are the most effective options\nto prevent contamination and infection transmission?\n✗ disinfect the\nrespiratory tract✓ vaccinate against\nCOVID-19\nTable 4: Answers generated by BIOREADER to context-free COVID-19 questions before ( D) and after ( D′)\nintegrating SARS-CoV-2 evidence into the datastore.\n7 Ethical Considerations\nThe language model’s ability to make the most of\npre-existing domain knowledge could have poten-\ntial ramifications for society, especially in health-\ncare contexts. From an application perspective, re-\nsearchers need better NLP tools to skim the biomed-\nical literature efficiently. Grounding in real fac-\ntual evidence (in this case PubMed’s RCTs) re-\nduces hallucination phenomena and offers more\ncontrol and interpretability. Users could endow\nBIOREADER with a sizeable medical index and\nask it open-domain questions to avoid reading thou-\nsands of publications. Analogously, they could\nclassify documents or perform structured predic-\ntion with a broader and up-to-date vision, going\nbeyond the information provided (in a common-\nsense fashion) and taking advantage of similarities\nbetween tasks thanks to multi-task learning. By\nincluding a retrieval method, BIOREADER remains\nrelatively small in size: plenty of users can de-\nploy it on affordable GPUs and tweak it as needed.\nFurthermore, the applications of this paper are be-\nyond the biomedical domain only, being suitable\nfor targeting (i) limited resource domains, (ii) out-\n5778\nof-distribution issues in downstream tasks (Parmar\net al., 2022), and (iii) domain-adaption with limited\nfine-tuning datasets.\nWith these benefits also come potential down-\nsides. Indeed, any external knowledge source will\nprobably never be entirely factual, coherent, and\ncompletely devoid of bias, particularly on large\nscales. We urge the users to undertake the nec-\nessary quality-assurance testing to understand the\npresence of such issues and evaluate how much\nthey impact the model. On the other side, one\nadvantage of using an explicit external memory\nis that the latter can be easily cleared, edited, or\nretroactively filtered. The same is not true of siloed\nknowledge in traditional PLMs. Like any large\nlanguage model, BIOREADER could be the sub-\nject of concern about its malicious use, although\narguably to a lesser extent. For example, it might\nbe used to automate the production of faked or mis-\nleading content, which could be critical in sensitive\nhealthcare domains.\nWe honor and support the ACL code of Ethics.\nAll pre-trained models and corpora used in this\nwork are publicly available.\n8 Limitations\nChunks may contain only partial information about\nbiomedical evidence, with the risk of generating in-\ncomplete or nonfactual text. Also, multiple chunks\ncan refer to the same fact; even if such retrieved\npassages have the prospect of complementing each\nother, they can cause repetitions or contradictions.\nManaging contradictions—which are natural in the\nscientific evolution of a field over time—is pre-\ncisely one of the main future research directions\nwe envisage. Context-sensitive chunks may also\nbe considered when building the knowledge base\nto avoid splitting within word or entity boundaries,\nwhich is especially risky in biomedicine.\nWe take only abstracts for constructing our\nknowledge base: future work should explore\nmassive-scale full-texts and their implications with\nrespect to the results presented in this paper. In-\ndeed, BIOREADER performances are capped by the\ncontained topic coverage of the selected datastore.\nWe believe that a quantitative assessment of the\nlink between the datastore modifications and their\neffect on model predictions is imperative, drawing\nattention to the need for new benchmarks.\nAdditionally, given the high memory consump-\ntion and large space on disk potentially required\nby FAISS indexes, we suggest the reader adopt a\nBinary Passage Retriever model (Yamada et al.,\n2021), which reduces the index size without losing\ntoo much in performance.\nFinally, our model backbone (SCIFIVE ) may be\nundertrained, reckoning on significantly fewer com-\nputational resources (i.e., a single TPUv2-8) than\nthe ones employed for the originalT5 and baselines\nlike PUBMEDBERT. We show promising results\nin constrained settings imposed by our GPU limi-\ntations, striving to make our work as reproducible\nas possible and leaving the possibility of adapting\nit to more performing hardware. We encourage\nfuture researchers to replicate our paper and unveil\nits real potential with well-trained seq2seq models\nsuch as BIOBART (Yuan et al., 2022), pre-trained\non biomedical corpora with 16 40GB A100 GPUs\nfor 7 days. Alternatively, we suggest bettering\nthe pre-train of bio-T5 models, possibly using the\nDeepNarrow strategy proposed by Tay et al. 2021,\nwhich reduces costs by training 50% fewer param-\neters and being 40% faster.\nWe hope that our work may trigger the commu-\nnity toward the development of new open-book\nbiomedical models and datasets, lowering the en-\ntry barrier and helping to accelerate progress in\nthis vitally important field for positive societal and\nhuman impact.\nAcknowledgements\nWe would like to thank all the anonymous review-\ners for their constructive, detailed, and valuable\ncomments.\nReferences\nSungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, and\nYoshua Bengio. 2016. A neural knowledge language\nmodel. CoRR, abs/1608.00318.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over wikipedia graph\nfor question answering. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\n5779\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan\nHögberg, Ulla Stenius, and Anna Korhonen. 2016.\nAutomatic semantic classification of scientific litera-\nture according to the hallmarks of cancer. Bioinform.,\n32(3):432–440.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, et al. 2021. Improving lan-\nguage models by retrieving from trillions of tokens.\nCoRR, abs/2112.04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B. Brown, Dawn Song, Úl-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.\nExtracting training data from large language models.\nIn 30th USENIX Security Symposium, USENIX Se-\ncurity 2021, August 11-13, 2021, pages 2633–2650.\nUSENIX Association.\nWalter Cerroni, Gianluca Moro, Tommaso Pirini, and\nMarco Ramilli. 2013. Peer-to-peer data mining clas-\nsifiers for decentralized detection of network attacks.\nIn Twenty-Fourth Australasian Database Conference,\nADC 2013, Adelaide, Australia, February 2013, vol-\nume 137 of CRPIT, pages 101–108. Australian Com-\nputer Society.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nNigel Collier and Jin-Dong Kim. 2004. Introduction to\nthe bio-entity recognition task at JNLPBA. In Pro-\nceedings of the International Joint Workshop on Nat-\nural Language Processing in Biomedicine and its Ap-\nplications (NLPBA/BioNLP), pages 73–78, Geneva,\nSwitzerland. COLING.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1173–1178, Hong Kong, China. Association\nfor Computational Linguistics.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence clas-\nsification in medical abstracts. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 308–313, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nPietro di Lena, Giacomo Domeniconi, Luciano Mar-\ngara, and Gianluca Moro. 2015. GOTA: GO term\nannotation of biomedical literature. BMC Bioinform.,\n16:346:1–346:13.\nKay Dickersin and Tianjing Li. 2015. Introduction to\nsystematic review and meta-analysis. Coursera.\nRezarta Islamaj Dogan, Sun Kim, Andrew Chatr-\naryamontri, Chih-Hsuan Wei, Donald C. Comeau,\nRui Antunes, Sérgio Matos, Qingyu Chen, Aparna\nElangovan, Nagesh C. Panyam, Karin Verspoor,\nHongfang Liu, Yanshan Wang, Zhuang Liu, Berna\nAltinel, Zehra Melce Hüsünbeyi, Arzucan Özgür,\nAris Fergadis, Chen-Kai Wang, Hong-Jie Dai, Tung\nTran, Ramakanth Kavuluru, Ling Luo, Albert Steppi,\nJinfeng Zhang, Jinchan Qu, and Zhiyong Lu. 2019.\nOverview of the biocreative VI precision medicine\ntrack: mining protein interactions and mutations for\nprecision medicine. Database J. Biol. Databases\nCuration, 2019:bay147.\nRezarta Islamaj Dogan, Robert Leaman, and Zhiyong\nLu. 2014. NCBI disease corpus: A resource for\ndisease name recognition and concept normalization.\nJ. Biomed. Informatics, 47:1–10.\nG. Domeniconi, M. Masseroli, G. Moro, and P. Pinoli.\n2014a. Discovering new gene functionalities from\nrandom perturbations of known gene ontological an-\nnotations. pages 107–116. INSTICC Press.\nGiacomo Domeniconi, Marco Masseroli, Gianluca\nMoro, and Pietro Pinoli. 2016a. Cross-organism\nlearning method to discover new gene functionalities.\nComput. Methods Programs Biomed., 126:20–34.\nGiacomo Domeniconi, Gianluca Moro, Andrea\nPagliarani, Karin Pasini, and Roberto Pasolini. 2016b.\nJob Recommendation from Semantic Similarity of\nLinkedIn Users’ Skills. In ICPRAM 2016, pages\n270–277. SciTePress.\nGiacomo Domeniconi, Gianluca Moro, Roberto Pa-\nsolini, and Claudio Sartori. 2014b. Iterative Refin-\ning of Category Profiles for Nearest Centroid Cross-\nDomain Text Classification. In IC3K 2014, Rome,\nItaly, October 21-24, 2014, Revised Selected Papers,\nvolume 553, pages 50–67. Springer.\nGiacomo Domeniconi, Gianluca Moro, Roberto Pa-\nsolini, and Claudio Sartori. 2015. A Comparison\nof Term Weighting Schemes for Text Classification\nand Sentiment Analysis with a Supervised Variant of\n5780\ntf.idf. In DATA (Revised Selected Papers), volume\n584, pages 39–58. Springer.\nGiacomo Domeniconi, Konstantinos Semertzidis,\nVanessa López, Elizabeth M. Daly, Spyros Kotoulas,\nand Gianluca Moro. 2016c. A novel method for un-\nsupervised and supervised conversational message\nthread detection. In DATA 2016 - Proc. 5th Int. Conf.\nData Science, Technol. and Appl., Lisbon, Portugal,\n24-26 July, 2016, pages 43–54. SciTePress.\nGiacomo Frisoni and Gianluca Moro. 2020. Phenom-\nena explanation from text: Unsupervised learning of\ninterpretable and statistically significant knowledge.\nIn Data Management Technologies and Applications\n- 9th International Conference, DATA 2020, Virtual\nEvent, July 7-9, 2020, Revised Selected Papers, vol-\nume 1446 of Communications in Computer and In-\nformation Science, pages 293–318. Springer.\nGiacomo Frisoni, Gianluca Moro, and Lorenzo Balzani.\n2022a. Text-to-text extraction and verbalization of\nbiomedical event graphs. In Proceedings of the 29th\nInternational Conference on Computational Linguis-\ntics, pages 2692–2710, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguis-\ntics.\nGiacomo Frisoni, Gianluca Moro, and Antonella Car-\nbonaro. 2020a. Learning Interpretable and Statisti-\ncally Significant Knowledge from Unlabeled Corpora\nof Social Text Messages: A Novel Methodology of\nDescriptive Text Mining. In DATA 2020 - Proc. 9th\nInt. Conf. Data Science, Technol. and Appl., pages\n121–134. SciTePress.\nGiacomo Frisoni, Gianluca Moro, and Antonella Car-\nbonaro. 2020b. Towards Rare Disease Knowledge\nGraph Learning from Social Posts of Patients. In\nRiiForum, pages 577–589. Springer.\nGiacomo Frisoni, Gianluca Moro, and Antonella Car-\nbonaro. 2020c. Unsupervised Descriptive Text Min-\ning for Knowledge Graph Learning. In IC3K 2020 -\nProc. 12th Int. Joint Conf. Knowl. Discovery, Knowl.\nEng. and Knowl. Manage., volume 1, pages 316–324.\nSciTePress.\nGiacomo Frisoni, Gianluca Moro, and Antonella Car-\nbonaro. 2021. A Survey on Event Extraction for Nat-\nural Language Understanding: Riding the Biomed-\nical Literature Wave. IEEE Access , 9:160721–\n160757.\nGiacomo Frisoni, Gianluca Moro, Giulio Carlassare,\nand Antonella Carbonaro. 2022b. Unsupervised\nevent graph representation and similarity learning\non biomedical literature. Sensors, 22(1):3.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge,\nand Yongfeng Zhang. 2022. Recommendation as\nlanguage processing (RLP): A unified pretrain, per-\nsonalized prompt & predict paradigm (P5). CoRR,\nabs/2203.13366.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a\ncontinuous cache. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings. OpenReview.net.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor\nO. K. Li. 2018. Search engine guided neural machine\ntranslation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 5133–5140. AAAI Press.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jian-\nfeng Gao, and Hoifung Poon. 2022. Domain-specific\nlanguage model pretraining for biomedical natural\nlanguage processing. ACM Trans. Comput. Heal. ,\n3(1):2:1–2:23.\nMandy Guo, Joshua Ainslie, David C. Uthus, Santi-\nago Ontañón, Jianmo Ni, Yun-Hsuan Sung, and Yin-\nfei Yang. 2021. Longt5: Efficient text-to-text trans-\nformer for long sequences. CoRR, abs/2112.07916.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 3929–3938. PMLR.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 9726–9735. Computer Vi-\nsion Foundation / IEEE.\nMaría Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMartínez, and Thierry Declerck. 2013. The DDI\ncorpus: An annotated corpus with pharmacological\nsubstances and drug-drug interactions. J. Biomed.\nInformatics, 46(5):914–920.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Towards unsupervised\ndense information retrieval with contrastive learning.\nCoRR, abs/2112.09118.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\n5781\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.\nBillion-scale similarity search with gpus. IEEE\nTrans. Big Data, 7(3):535–547.\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan,\nand Sivanesan Sangeetha. 2022. AMMU: A survey\nof transformer-based biomedical pretrained language\nmodels. J. Biomed. Informatics, 126:103982.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics.\nMartin Krallinger, Obdulia Rabal, Florian Leitner,\nMiguel Vazquez, David Salgado, Zhiyong Lu, Robert\nLeaman, Yanan Lu, Donghong Ji, Daniel M. Lowe,\nRoger A. Sayle, Riza Theresa Batista-Navarro, Rafal\nRak, Torsten Huber, Tim Rocktäschel, Sérgio Matos,\nDavid Campos, Buzhou Tang, Hua Xu, Tsendsuren\nMunkhdalai, Keun Ho Ryu, S. V . Ramanan, P. Senthil\nNathan, Slavko Zitnik, Marko Bajec, Lutz Weber,\nMatthias Irmer, Saber A. Akhondi, Jan A. Kors, Shuo\nXu, Xin An, Utpal Kumar Sikdar, Asif Ekbal, Masa-\nharu Yoshioka, Thaer M. Dieb, Miji Choi, Karin Ver-\nspoor, Madian Khabsa, C. Lee Giles, Hongfang Liu,\nRavikumar Komandur Elayavilli, Andre Lamurias,\nFrancisco M. Couto, Hong-Jie Dai, Richard Tzong-\nHan Tsai, Caglar Ata, Tolga Can, Anabel Usie,\nRui Alves, Isabel Segura-Bedmar, Paloma Martínez,\nJulen Oyarzabal, and Alfonso Valencia. 2015. The\nCHEMDNER corpus of chemicals and drugs and\nits annotation principles. J. Cheminformatics, 7(S-\n1):S2.\nEsther Landhuis. 2016. Scientific literature: Informa-\ntion overload. Nature, 535(7612):457–458.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nTomás Kociský, Sebastian Ruder, Dani Yogatama,\nKris Cao, Susannah Young, and Phil Blunsom. 2021.\nMind the gap: Assessing temporal generalization\nin neural language models. In Advances in Neural\nInformation Processing Systems 34: Annual Confer-\nence on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages\n29348–29363.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020a. Pretrained language models for biomed-\nical and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146–157, Online. Association for Computational Lin-\nguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\nRocktäschel, et al. 2020b. Retrieval-augmented gen-\neration for knowledge-intensive NLP tasks. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation. CoRR, abs/2202.01110.\nJiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers, and\nZhiyong Lu. 2016. Biocreative V CDR task corpus:\na resource for chemical disease relation extraction.\nDatabase J. Biol. Databases Curation, 2016.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021. Self-alignment\npretraining for biomedical entity representations. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4228–4238, Online. Association for Computa-\ntional Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\n5782\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nJörn Lötsch, Dario Kringel, and Alfred Ultsch.\n2021. Explainable artificial intelligence (XAI) in\nbiomedicine: Making AI decisions trustworthy for\nphysicians and patients. BioMedInformatics, 2(1):1–\n17.\nZaiqiao Meng, Fangyu Liu, Ehsan Shareghi, Yixuan Su,\nCharlotte Collins, and Nigel Collier. 2022. Rewire-\nthen-probe: A contrastive recipe for probing biomed-\nical knowledge of pre-trained language models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4798–4810, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nGianluca Moro, Andrea Pagliarani, Roberto Pasolini,\nand Claudio Sartori. 2018. Cross-domain & In-\ndomain Sentiment Analysis with Memory-based\nDeep Neural Networks. In IC3K 2018, volume 1,\npages 127–138. SciTePress.\nGianluca Moro and Luca Ragazzi. 2022. Semantic\nSelf-Segmentation for Abstractive Summarization of\nLong Legal Documents in Low-Resource Regimes.\nIn Thirty-Sixth AAAI Conference on Artificial Intel-\nligence, AAAI 2022, Virtual Event, February 22 -\nMarch 1, 2022, pages 1–9. AAAI Press.\nGianluca Moro, Luca Ragazzi, Lorenzo Valgimigli,\nand Davide Freddi. 2022. Discriminative marginal-\nized probabilistic neural method for multi-document\nsummarization of medical literature. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 180–189, Dublin, Ireland. Association\nfor Computational Linguistics.\nGianluca Moro and Lorenzo Valgimigli. 2021. Efficient\nself-supervised metric information retrieval: A bibli-\nography based method applied to COVID literature.\nSensors, 21(19).\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nCoRR, abs/2112.09332.\nYixin Nie, Songhe Wang, and Mohit Bansal. 2019.\nRevealing the importance of semantic retrieval for\nmachine reading at scale. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2553–2566, Hong Kong,\nChina. Association for Computational Linguistics.\nEvangelos Pafilis, Sune P Frankild, Lucia Fanini,\nSarah Faulwetter, Christina Pavloudi, Aikaterini\nVasileiadou, Christos Arvanitidis, and Lars Juhl\nJensen. 2013. The species and organisms resources\nfor fast and accurate identification of taxonomic\nnames in text. PloS one, 8(6):e65390.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone,\nJie Ma, Alessandro Achille, Rishita Anubhai,\nCícero Nogueira dos Santos, Bing Xiang, and Ste-\nfano Soatto. 2021. Structured prediction as transla-\ntion between augmented natural languages. In 9th\nInternational Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net.\nMihir Parmar, Swaroop Mishra, Mor Geva, and Chitta\nBaral. 2022. Don’t blame the annotator: Bias al-\nready starts in the annotation instructions. CoRR,\nabs/2205.00415.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward Z.\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024–8035.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of BERT and ELMo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task, pages 58–65, Florence,\nItaly. Association for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language Models as Knowl-\nedge Bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nLong N. Phan, James T. Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature. CoRR,\nabs/2106.03598.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah\nYoung, et al. 2021. Scaling language models: Meth-\nods, analysis & insights from training gopher. CoRR,\nabs/2112.11446.\n5783\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nLance Ramshaw and Mitch Marcus. 1995. Text chunk-\ning using transformation-based learning. In Third\nWorkshop on Very Large Corpora.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nStephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333–389.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from natural language inference in the clini-\ncal domain. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1586–1596, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances\nin Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 25968–25981.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M. Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea\nSantilli, Thibault Févry, Jason Alan Fries, Ryan Tee-\nhan, Stella Biderman, Leo Gao, Tali Bers, Thomas\nWolf, and Alexander M. Rush. 2021. Multitask\nprompted training enables zero-shot task generaliza-\ntion. CoRR, abs/2110.08207.\nMinjoon Seo, Tom Kwiatkowski, Ankur Parikh, Ali\nFarhadi, and Hannaneh Hajishirzi. 2018. Phrase-\nindexed question answering: A new challenge for\nscalable document comprehension. In Proceedings\nof the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 559–564, Brussels,\nBelgium. Association for Computational Linguistics.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.\nReal-time open-domain question answering with\ndense-sparse phrase index. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4430–4441, Florence, Italy.\nAssociation for Computational Linguistics.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020. BioMegatron: Larger\nbiomedical domain language model. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4700–4706, Online. Association for Computational\nLinguistics.\nLarry Smith, Lorraine K Tanabe, Cheng-Ju Kuo,\nI Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger,\nChristoph M Friedrich, Kuzman Ganchev, Manabu\nTorii, et al. 2008. Overview of biocreative ii gene\nmention recognition. Genome biology, 9(2):1–19.\nYixuan Su, Zaiqiao Meng, Simon Baker, and Nigel Col-\nlier. 2021. Few-shot table-to-text generation with\nprototype memory. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages\n910–917, Punta Cana, Dominican Republic. Associa-\ntion for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fe-\ndus, Samira Abnar, Hyung Won Chung, Sharan\nNarang, Dani Yogatama, Ashish Vaswani, and Don-\nald Metzler. 2021. Scale efficiently: Insights from\npre-training and fine-tuning transformers. CoRR,\nabs/2109.10686.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nRobert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xi-\naodong Liu, Tristan Naumann, Jianfeng Gao, and\nHoifung Poon. 2021. Fine-tuning large neural lan-\nguage models for biomedical natural language pro-\ncessing. CoRR, abs/2112.07869.\nKushal Tirumala, Aram H. Markosyan, Luke Zettle-\nmoyer, and Armen Aghajanyan. 2022. Memorization\nwithout overfitting: Analyzing the training dynamics\nof large language models. CoRR, abs/2205.10770.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R. Alvers, Dirk Weissenborn, Anastasia\n5784\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, Yannis Almirantis, John Pavlopoulos, Nico-\nlas Baskiotis, Patrick Gallinari, Thierry Artières,\nAxel-Cyrille Ngonga Ngomo, Norman Heino, Éric\nGaussier, Liliana Barrio-Alvers, Michael Schroeder,\nIon Androutsopoulos, and Georgios Paliouras. 2015.\nAn overview of the BIOASQ large-scale biomedical\nsemantic indexing and question answering competi-\ntion. BMC Bioinform., 16:138:1–138:28.\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo\nWang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R3: Re-\ninforced ranker-reader for open-domain question an-\nswering. In Proceedings of the Thirty-Second AAAI\nConference on Artificial Intelligence, (AAAI-18), the\n30th innovative Applications of Artificial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence (EAAI-18),\nNew Orleans, Louisiana, USA, February 2-7, 2018,\npages 5981–5988. AAAI Press.\nXing Wei and W. Bruce Croft. 2006. Lda-based docu-\nment models for ad-hoc retrieval. In SIGIR, pages\n178–185. ACM.\nYuhuai Wu, Markus N. Rabe, DeLesley Hutchins, and\nChristian Szegedy. 2022. Memorizing transformers.\nCoRR, abs/2203.08913.\nIkuya Yamada, Akari Asai, and Hannaneh Hajishirzi.\n2021. Efficient passage retrieval with hashing for\nopen-domain question answering. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 979–986, Online.\nAssociation for Computational Linguistics.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nBERTserini. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 72–77, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. LinkBERT: Pretraining language models with\ndocument links. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 8003–8016,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nReasoning with language models and knowledge\ngraphs for question answering. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 535–546, Online.\nAssociation for Computational Linguistics.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021a. Adaptive semiparametric\nlanguage models. Transactions of the Association for\nComputational Linguistics, 9:362–373.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021b. Adaptive semiparametric\nlanguage models. Transactions of the Association for\nComputational Linguistics, 9:362–373.\nHongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang,\nYutao Xie, and Sheng Yu. 2022. BioBART: Pretrain-\ning and evaluation of a biomedical generative lan-\nguage model. In Proceedings of the 21st Workshop\non Biomedical Language Processing, pages 97–109,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nJingyi Zhang, Masao Utiyama, Eiichro Sumita, Gra-\nham Neubig, and Satoshi Nakamura. 2018. Guiding\nneural machine translation with retrieved translation\npieces. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 1325–1335,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nA Biomedical Needs\nCompared to the open domain, biomedicine raises\nsubstantial challenges and constraints:\n• specialized jargon and professional language;\n• overarching information truly hard to inter-\npret;\n• synonyms (see UMLS) and special tokens;\n• narrow margin for interpretation, rephrasing,\nand creativity;\n• clauses are often interdependent and express\ncomplex interactions;\n• non-tolerance of factual mistakes;\n• knowledge rapidly evolves over time.\nWe cope with these needs by utilizing a domain-\nspecific model, a semantic dense retrieval of com-\nmonsense or domain-specific related knowledge\n(disjoint from the training dataset), and an in-depth\nevaluation (also with multi-task learning).\nB Reproducibility\nB.1 Implementation and Training Details\nHardware Setup We ran each experiment on a\nworkstation having one Nvidia GeForce RTX3090\nGPU with 24GB of dedicated memory, 64GB of\nRAM, and an Intel® Core™ i9-10900X1080 CPU\n@ 3.70GHz.\n5785\nModel We implement BIOREADER using Py-\nTorch 1.9 (Paszke et al., 2019) as framework, evolv-\ning the T5ForConditionalGeneration\ncode5 from HuggingFace and taking LabML 6\nand open contributions7 as references for RETRO -\nblocks. We only use the T5-base configuration\n(12 layers, 768-dimensional hidden size, and 12\nattention heads) as a baseline due to GPU memory\nconstraints. Nevertheless, we believe that our\nresults would generalize to larger configurations.\nEvidence datastore Each abstract in PubMed-\nRCT gets split into the desired chunk length and\nthen padded if the last chunk is too short. We com-\npute chunk embeddings by taking the mean pooling\nof the hidden states produced by the encoder8. We\nleverage Autofaiss9 for automatically building\nthe document indices and then calculating the k-\nnearest neighbors for all chunks. Creating an entire\nFAISS index on our knowledge base D with ap-\nproximately 200K abstracts and 60M tokens takes\n2 hours (≈1.5GB index file, ≈0.7GB chunk file).\nData leakage is not possible with different sources\nfor queries and neighbors; so we do not filter out\nneighbors originating from the same document as\nthe training sequence.\nExperiment tracking We track all our trainings\nwith Weights & Biases10 and monitor CO2 emis-\nsions with CodeCarbon 11. Moreover, we profile\nthe neighbors’ retrieval speed with custom code.\nPre-training After initializing the model param-\neters (warm-up) with SCIFIVE (PubMed)-base, we\ncontinuously pre-train BIOREADER for 122K steps\nwith a batch size of 8. We take two SCIFIVE pre-\ntraining files as our corpus12. Here, spans of text\n(i.e., consecutive tokens) are randomly replaced\nby a sentinel unique masked token <M>; the target\nsequence consists of the concatenation of the same\n5https://huggingface.co/docs/\ntransformers/model_doc/t5\n6https://nn.labml.ai/transformers/\nretro/model.html\n7https://github.com/lucidrains/\nRETRO-pytorch\n8We also tried[CLS] but found no consistent best strategy\n(the optimal one varies on different encoders).\n9https://github.com/criteo/autofaiss\n10https://wandb.ai\n11https://github.com/mlco2/codecarbon\n12Masked pre-training files: gs://scifive/\npretrain/pubmed_cleaned/abs_1_30.tsv\nand gs://scifive/pretrain/pubmed_cleaned/\nabs_1_16.tsv\nsentinel tokens and the real dropped-out spans (self-\nsupervised learning). We use Adam (Kingma and\nBa, 2015) as optimizer with a constant learning\nrate of 1e-4 and a dropout rate of 10%. With k=9\nand max-length n=512, the training time is ≈10\nhours (1 second per iter), 0.2682 kg CO2 impact.\nWe highlight that decreasing the retrieved chunks\nto k=2 reduces the time required to 0.7 seconds\nper iter while increasing the max-length to n=1024\nleads to >20 hours. We perform the retrieval of\nall chunks in parallel by putting them into a sin-\ngle batch; retrieving one chunk at a time causes a\nstrong deterioration in performance (≈2.5 days for\nk=9).\nFine-tuning After pre-training, we fine-tune\nBIOREADER on the various downstream tasks,\nchoosing a multi-task learning configuration for\nthe NER datasets. Due to its unavailability in\nthe original paper, we re-calculate SCIFIVE Exact\nMatch accuracy for QA. We perform training for\n30 epochs with a batch size of 4 for tasks with 256\ninput length (2 otherwise), AdamW (Loshchilov\nand Hutter, 2019), learning rate2e-4, and dropout\nrate 10%. We find a large batch size to be very ben-\neficial; we simulate a batch size of 128 with 32\nand 64 gradient accumulation steps, thus helping\nto prevent overfitting. Maximum input and output\nlengths for each task are in Table 7. Each fine-\ntuning takes between 13 and 20 hours.\nUsed models Table 5 enumerates all the models\nused in this study, linking to specific versions.\nB.2 Hyperparameters\nWe list the hyperparameters used for training\nBIOREADER in Table 6. An insight into their effect\nis given in §D.\nC Evaluation Datasets Insights\nTable 7 reports a complete overview of our bench-\nmark datasets and their composition.\nD Ablations\nWe study several research questions to understand\nthe effect of important design choices and hyper-\nparameters on downstream biomedical NLP per-\nformance. We test on lightweight settings to save\ncomputation time without affecting comparability.\nWe pre-train on ≈15K instances13 and fine-tune for\n13gs://scifive/pretrain/pubmed_cleaned/\nabs_1_23.tsv\n5786\nModel URL\nBIOBERT https://huggingface.co/dmis-lab/biobert-base-cased-v1.1\nBIOBERT-NLI https://huggingface.co/gsarti/biobert-nli\nSCIBERT https://huggingface.co/allenai/scibert_scivocab_cased\nBLUEBERT https://huggingface.co/bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\nCLINICALBERT https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\nPUBMEDBERT-base https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\nBIOLINKBERT-basehttps://huggingface.co/michiyasunaga/BioLinkBERT-base\nBIOLINKBERT-largehttps://huggingface.co/michiyasunaga/BioLinkBERT-large\nT5-base https://huggingface.co/t5-base\nT5-large https://huggingface.co/t5-large\nSCIFIVE-base https://huggingface.co/razent/SciFive-base-Pubmed\nSCIFIVE-large https://huggingface.co/razent/SciFive-large-Pubmed_PMC\nTable 5: List of the models used in this study.\nHyperparameter Search space\nPre-training learning rate\n{1e-5, 3e-5, 5e-5,\n1e-4∗(Yuan et al., 2022),\n2e-4(Borgeaud et al., 2021)}\nFine-tuning learning rate 2e-4(linear scheduler)\nPre-training and fine-tuning dropout rate 0.10\nPre-training Optimizer Adam\nFine-tuning Optimizer AdamW,\n(0.9β1, 0.99β2, no weight decay)\nPre-training batch size {2, 6, 8 ∗}\nFine-tuning batch size\n4 for RE, DC, and NLI,\n2 for NER, QA, and OpenQA,\ngradient_accumulation_steps=32\nPre-training iterations on PubMed sample 122K (0.3 epochs)\nFine-tuning epochs on downstream tasks 30\nPre-training and fine-tuning CCA positionP {6, 9, 12}, {9, 12}∗, {6, 9}, {12}\nPre-training and fine-tuning number of neighborsk [2, 9] (9∗)\nChunk sizem {8,16∗, 32 (Borgeaud et al., 2021)}\nPre-training and fine-tuning checkpoint frequency 10.000 steps\nTable 6: Hyperparameters along with their search grid. ∗marks the values used to obtain the reported results.\none epoch on a small corpus version of each down-\nstream task (5k train data and 500 test data), batch\nsize of 2. We exclude QA and OpenQA bench-\nmarks due to their need for human judgment as a\nproper quality indication (§5).\nRQ1. What is the best architectural setting (po-\nsition and quantity) for the CCA layers? We\nstudy three architecture variants for CCA layers.\n• Encoder-only (Enc). CCA is done within the\nencoder after the standard self-attention layer.\n• Encoder-Decoder (EncDec). CCA is done\nbetween the encoder and the decoder. After\nthe encoder, the encoded retrieved neighbors\nare incorporated once with the encoder out-\nput through CCA and are saved as an inde-\npendent variable. In the decoder, there is a\nsecond layer of Encoder-Decoder Attention to\nblend the CCA output with the decoder hidden\nstates.\n• Decoder-only (Dec). CCA is done within the\ndecoder, after the standard self-attention layer,\nand before the Encoder-Decoder Attention\nlayer. Encoded neighbors are integrated into\nencoder outputs with CCA and replace the en-\ncoder outputs themselves.\nWe find that the Dec architecture is the best setting\nfor the CCA layers, while the EncDec architecture\nis a close second. The Enc architecture is not ef-\nfective, and we hypothesize that it is important for\nthe raw inputs without neighbor information to be\ninitially seen by the decoder. Furthermore, we find\nthat 2 layers for CCA in a 12-layer model represent\nan optimal setting. The best results are obtained by\ncomparing in contrast to 3 and 4 CCA layers.\nRQ2. How does the chunk size impact the re-\nsults? We examine how the chunk size (min Eq.\n5787\nTask Dataset Biomedical\nDomain\n# Instances Task Type Input Length Target LengthTrain Dev Test\nSelf-Supervised\nLearning PubMed All 512 512\nNER\nNCBI-disease Disease 5,134 787 960\nMulti-Task 512 512\nBC5CDR-disease Disease 4,182 4,244 4,424\nBC5CDR-chemical Chemical 5,203 5,347 5,538\nBC4CHEMD Chemical 30,682 26,364 26,364\nBC2GM Chemical 12,574 5,038 5,038\nJNLPBA Gene 46,750 4,551 8,662\nSpecies-800 Species 10,771 1,630 1,630\nRE Chemprot Protein-chemical 18,035 11,268 15,745 Single-Task 256 16\nDDI Disease-disease 25,296 2,496 5,716 Single-Task 256 16\nDC HoC Cancer 1,295 186 371 Single-Task 256 64\nNLI MedNLI Clinical 11,232 1,395 1,422 Single-Task 256 12\nQA\nBioASQ4-factoid All 3,264 3,590 652 Single-Task 512 128\nBioASQ5-factoid All 3,264 496 495 Single-Task 512 128\nBioASQ6-factoid All 4,772 478 531 Single-Task 512 128\nOpenQA MedQA-USMLE Clinical 10,178 1,272 1,273 Single-Task 512 128\nTable 7: Basic statistics of the biomedical evaluation datasets with input and target sequence length settings,\nincluding self-supervised learning. \"# Instances\" denotes the number of entity or relation mentions (NER, RE) /\nlabeled documents or sentence pairs (DC, NLI) / queries (QA, OpenQA).\n5) affects the model performance. To this end, we\nre-build the datastore D and re-run pre-training and\nfine-tuning by varying m∈{8,16,32}, where 32\nrepresents the same input to chunk ratio as RETRO .\nResults are quite similar (Figure 3). Surprisingly,\nm=16 has average better accuracies/F1-scores. Af-\nter scanning some neighborhood examples, we be-\nlieve this is due to the greater compression of in-\nformation content within RCTs’ abstracts. Reason-\nably, the increase in the chunk size is directly pro-\nportional to the memory occupation and inversely\nproportional to the computation time required (23\nminutes for m=8, 12 minutes for m=16, 7 minutes\nfor m=32). Low NER F1 scores are justified by the\nneed for more training data and time to accomplish\nadaption. Clearly, the text-to-text instances belong-\ning to this task type are more distant from human\nlanguage due to entity labels directly inserted in\nthe text through augmentation.\nRQ3. What is the most effective neural retriever\nfor building the evidence datastore? The prove-\nnance of the continuous representations used for\nthe neural retrieval phase is pivotal. Previous work\nlike RETRO exploits BERT embeddings indepen-\ndently of the architecture, assuming that the non-\nfrozen encoder part of the model will learn to adapt.\nWe explore different frozen bi-directional models\nfor encoding neighbors within the datastore D: (i)\nPUBMEDBERT—one of the most effective biomed-\n8 16 32\n10\n20\n30\n40\n50\n60\n70\n80\n90Test Performance\n0\n1\n2\n3\n4\n5\n6\nGPU memory (GB)\nNER ChemProt DDI\nHoC MedNLI Memory\nFigure 3: Overall test performance and GPU memory\noccupation for different chunk sizes ( Dec CCA, SCI-\nFIVE neighbor encoder).\n.\n5788\nical *BERT (Liu et al., 2021), (ii) BIOBERT-NLI\nsentence transformer—pre-trained on sentence sim-\nilarity, (iii) the SCIFIVE encoder, and (iv) the\nCONTRIEVER query/document encoder. Table 8\nshows the results. We can see that utilizing a CON-\nTRIEVER -based encoder for both chunked input\nprompts (queries) and neighbors give general better\nresults, rewarding space-homogeneity. Moreover,\nfrom qualitative analysis, we find that the tokens\nretrieved by CONTRIEVER are more relevant than\nthe ones obtained through the SCIFIVE -encoder.\nDataset (Metric) PubMedBert BioBert SciFive Contriever\nChemProt (F1) 81.82 83.84 87.60 87.77\nDDI (F1) 51.65 60.32 64.70 49.91\nMedNLI (Acc) 63.36 72.86 76.71 75.59\nNER (F1) 15.17 16.82 18.31 18.33\nHoC (F1*) 60.28 57.84 64.70 77.06\nTable 8: Downstream test results with different query-\nneighbors encoders (chunk size 16, Dec CCA). Best\nscores are in bold.\nRQ4. What is the contribution of continuation\nchunks? A BIOREADER model is trained by at-\ntending, for a given chunk, to both the neighbors of\nthe preceding chunk N and their continuation F in\ntime. We measure how training and evaluating only\non neighbors affects performance (Table 9). We\nobserve that attending to both neighbors and their\ncontinuation is generally the most effective choice.\nOne exception to this claim is NLI, for which we\nregister a decrease in accuracy of more than 20\npoints. We believe it is normal behavior: as the\nexternal context increases (i.e., higher k-values or\ncontinuations), the model tends to divert attention\nfrom the two sentences under evaluation and make\nerroneous predictions.\nDataset (Metric)NNN-only N+FN+FN+F\nNER (F1) 19.37 19.98\nChemProt (F1) 80.79 84.96\nDDI (F1) 58.79 64.66\nHoC (F1*) 70.42 72.32\nMedNLI (Acc) 76.54 52.63\nTable 9: Downstream test results with and without con-\ntinuation chunks (chunk size 16, Dec CCA, SCIFIVE\nneighbor encoder). Best scores are in bold.\nRQ5. What is the impact of the number of\ntraining neighbors? During training, we retrieve\nthe top-kneighboring chunks for each query. We\nweigh the effect of training with multiple numbers\nof neighbors, considering k∈{2,3,..., 9}. Figure\n4 summarizes the resulting performance. We find\nthat results are quite stable within the small tested\nrange, with no particular k-value giving substantial\nperformance improvement. We emphasize that a\nsimple solution for reducing the required compu-\ntational budget consists in training the model with\nfewer retrieval passages. In this paper, we select\nk=9 due to the tiny superior performance and the\ncontained overhead (see §B). We have the flexibil-\nity to adjust the number of retrieved neighbors at\nevaluation time, which can affect performance and\nruntime.\n2 3 4 5 6 7 8 910\n20\n30\n40\n50\n60\n70\n80\n90\nTraining k\nTest performance\nNER CHEM PROT DDI\nHOC MED NLI\nFigure 4: Impact of the number of nearest neighbors\nkduring training (chunk size 16, Dec CCA, SCIFIVE\nneighbor encoder).\nRQ6. How the model scales with the number of\nretrieved passages during the evaluation? We\ninvestigate the performance of BIOREADER as we\nvary kduring evaluation. In a general way, we ob-\nserve that whenkis small (2<k<15), performances\nare relatively the same. However, as kapproaches\n30, the results drop notably (more than 2 points\nless on average). The reason for such degenera-\ntion is that, as kincreases, the top-kneighbors are\nlikely to contain more information that is irrelevant\nto the input prompt or repeated by other chunks.\nk=1 or k=2 lead to minor improvements in non-\nknowledge-intensive tasks like RE or NLI.\nE Loss and Perplexity\nWe outline the loss and perplexity curves at the\nend of the pre-training process (Figure 5). In doing\nthis, we compare BIOREADER with our baseline,\ni.e., training continuation of SCIFIVE -base with all\nthe layers unfrozen (no architectural changes, no\nneighbors, no CCA). Both the loss for tokens and\nthe perplexity (which indicates better generaliza-\n5789\ntion performance) are reduced by BIOREADER in\na pronounced way.\nF Visualization\nF.1 Chunked-Cross Attention\nFigure 6 illustrates the simplified functioning of\nchunked-cross attention, the step where the model\ncan glance at the external information it needs to\ncorrectly predict the next token.\nF.2 Input-Neighbors-Output Examples\nWe check out how the retrieved chunks guide the\ndecoding (Table 10), seeking overlapping between\nsampled and neighbor tokens. Using a contextual-\naware PLM encoder, we capture lexical varia-\ntions and semantic relationships between the input\nprompt and the searched chunks. Retrieval supplies\nmore insights on the output of BIOREADER , as the\nuser can directly visualize or modify the neighbors\nthat are being used. One can also verify the source\ndocuments from which the utilized knowledge orig-\ninates, which means our model also has increased\ninterpretability and debuggability compared to stan-\ndard language models. So, BIOREADER engenders\nappropriate user trust, supporting a great under-\nstanding of the modeled process. We find that\nBIOREADER uses its non-parametric memory to\ncue the encoder-decoder model into generating cor-\nrect tokens. We note that the role of the retrieved\nknowledge changes depending on the task. In non-\nknowledge intensive scenarios, neighbors offer an\nextended view of the meaning of the phrases men-\ntioned in the input prompt, giving to BIOREADER\nrelated examples that can be helpful to predict a\nclass label better. In knowledge-intensive tasks re-\nquiring free-text generation, retrieved neighbors\nsuggest factual evidence fragments that guide the\nconstruction of the output text token-after-token, re-\nducing hallucinations and making the model more\nknowledgeable.\nF.3 Zero-shot Generalization Via Datastore\nUpdate\nFigure 7 displays a graphical representation of the\nBIOREADER output in Table 3. To the best of\nour knowledge, we are the first to test PLM trans-\nfer learning by explicit memory substitution only\ninstead of closed-book generalization. For this rea-\nson, we coin the term \"zero-shot datastore\".\n5790\nBIOREADER SCIFIVE -base\n0 100 200 3004\n6\n8\n10\nStep\nLoss\n0 100 200 3000\n0.2\n0.4\n0.6\n0.8\n1.0\n⋅104\nStep\nPerplexity\nFigure 5: Loss and perplexity curves after pre-training.\nT5 \nENCODER\nT5-AUGMENTED DECODER\nRETRO DECODER BLOCK9\n...\nT5 DECODER BLOCK\n1\n...\nT5 DECODER BLOCK\n8\nBioREADER\nType 1 diabetesmellitus( is a chronicdiseasethat startsearlyin lifeT1DM )N1\nThe diseaseN2 processleadingto clinicaltype1 diabetesoften starts duringthe first yearslife\nWhendoestype1 diabetesstart?X Type1 diabetesstarts\nChunked\nCross-Attention\nFigure 6: BIOREADER decoder block retrieving information from nearest neighbor chunks using CCA. Adapted\nfrom https://jalammar.github.io/illustrated-retrieval-transformer/.\n5791\nTask Input Output N1−31N1−31N1−31 N1−32N1−32N1−32 N1−33N1−33N1−33\nRE ddi: The concomitantintake of * DRUG *and * DRUG *does not affectthepharmacokineticsof either alcohol oracamprosate.\nGround truth:DDI-false\nOurs:DDI-false\nThis was a multicentre, randomised,double-blind,placebo- andactive-controlled\nrequire more medicationthanyounger children to achieve asimilar therapeutic response\ngreater ease ofadministrationwhen compared with oxytocin\nused in these dosesseems to besafe for day care surgerywithonly a tiny increase incirculating plasma.\nthe uniforminjection ofvaccineantigen into muscle tissue ininfants.\nNoadverse eventscould be relatedto theuse PSD.\nHabitual caffeine use appearstominimally reducecaffeineeffects.\nto sodium benzoate containingpharmaceutical formulations\nDC hoc: The present study suggests thatMGN-3may represent animmunologically relevant productforactivating innateimmunityinmultiple myelomapatients andwarrantsfurthertesting todemonstrate clinical efficacy\nGround truth:avoiding immunedestruction\nOurs:avoiding immunedestruction\nMGN-3/BioBranis an arabinoxylanextracted from rise brancell-mediatedimmune responseplays a role in wart resolution.fromtheir disease and therapy.\ninflammatory responsethroughmodulation of the neurohumoralresponse to stress.\nefficacy by way of modulatingcellular immune function. a probe forchallenge studies.\nboosting the immune system. decrease in receptor-mediatedapoptosis. can be a diagnostic parameter.\nQA bioasq4b: question*: what is targetedby monoclonal antibodypembrolizumab?context*: pembrolizumab versusipilimumab in advancedmelanoma. background: theimmune checkpoint inhibitor ipilimumabis thestandard-of-care treatmentforpatientswith advancedmelanoma. pembrolizumab inhibits theprogrammed cell death 1 (pd-1)immunecheckpoint and hasantitumoractivityinpatients with advanced melanoma. . .\nGround truth:programmed cell death 1\nOurs:pd-1\nbevacizumabwhen added to standardchemotherapyin a real-world\nIn randomized sequence,patients receivedoralmontelukast\nRotavirusis a leading causeof morbidity and mortalityin children younger\na novel selectiveestrogen-receptormodulator, in postmenolerability and safetyof TMC278, a non-nu\nZiprasidoneis notcurrently approved bythe United States Foodand Drug Administration\nacy and safety of Fibrocaps,a ready-to-use acy and safety of Fibrocaps,a ready-to-use\nAlthough platin-basedchemotherapyhas becomea standard treatment fornon-small cell\nOpenQAmedqa: question*: A 73-year-old man hastype 2 diabetes mellitus,hypertension,hypercholesterolemia, andcoronary artery disease.The physician prescribes a drug that inhibitsintestinalcholesterolabsorption.The addition of this drug is most likely to increasethe risk of which of the following adverse effects?Hepatotoxicity HyperkalemiaCutaneousflushing Hyperuricemia\nGround truth:Hepatotoxicity\nOurs:Hepatotoxicity\nArterial hypertensionis a primecause of morbidity and mortality in\nyears or younger who weredischarged from the hospital afteracoronary heart disease\nhypothalamic cholinergicneurotransmission plays a major\nObesityis a highly prevalentmedical condition and iscommonly accompanied by\nised on acholesterol-loweringdiet and simvastatin 40 mg dailyhepatotoxicityhave beenobserved in obese patients\nCapillary glucose levelsdecreased by 2.9 and 2.6mmol\nobesity-relatedrenal failureafter lower torso ischemia\nhyperglycemiaand thefrequency of white!75!greenhypoglycemia.we conducted\nTable 10: Cherry picked input-output examples and retrieval influence. We show the first three neighborsN1−3\nu of\nthe chunks u∈{1,2,3}. We highlight the latent semantic overlap between the input and the retrieved neighbors.\n5792\nOpenQA fine-tuning with retrieval from the PubMed-RCT datastore\nZero-shot generalization with retrieval from a Covid-19 specialized datastore\nBiomedical \nScientific \nEvidence\nIndexing\nRelated \ncontextQuery\nBiomedical \nScientific \nEvidence\nAdditional Indexing\nCovid-19 Literature\nBioREADER\nA 37-year-old man with no signiﬁcant past medical history is rear-ended\nin a motor vehicle accident. He reported signiﬁcant neck pain to\nemergency responders, but otherwise denies weakness, numbness or\n\u0000ngling in his extremi\u0000es. His vitals on presenta\u0000on to the ED are HR 90,\nBP 140/80, RR 20, SpO2 98%. What is the most appropriate next step\nupon presenta\u0000on to the emergency room?\nCervical immbolization\nA thymic sample from a fetus is examined. One cell\ntype found was double-positive for the CD4 and\nCD8 receptors. What is the identity of these\ndouble-positive cells?\nImmature T-cells of the thymic cortex\nParents bring an 11-month-old baby to the clinic because the baby has\na fever of 39.0°C (102.2°F). The baby is irritated and crying constantly.\nShe is up to date on immuniza\u0000ons. A complete physical examina\u0000on\nreveals no signiﬁcant ﬁndings, and all laboratory tests are nega\u0000ve.\nFive days a\u0000er resolu\u0000on of her fever, she develops a transient\nmaculopapular rash. What is the most likely diagnosis?\nRoseola\nJanuary 2019. A chinese 69-year-old man comes to the physician with\nfever, tiredness, cough, dyspnoea, and severe respiratory issues. The\nclinical picture suggests an infectious disease. What is the most likely\ndiagnosis?\nCoronaviruses are viruses that can cause illnesses in humans,\nincluding severe respiratory disease and even death. Corona\ndisease-19 virus (COVID-19) spread and caused a pandemic\nthat affected people all over the world. As COVID-19 cases\ncontinue to rise globally, which are the most effective options\nto prevent contamination and infection transmission?\nCOVID-19\nVaccinate against COVID-19\nFigure 7: BIOREADER adapts and provides correct answers to unseen context-free Covid-19 questions only through\na datastore enrichment (no retraining).\n5793",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7950937151908875
    },
    {
      "name": "Biomedicine",
      "score": 0.7165477871894836
    },
    {
      "name": "Transformer",
      "score": 0.6729990243911743
    },
    {
      "name": "Language model",
      "score": 0.510370671749115
    },
    {
      "name": "Latent Dirichlet allocation",
      "score": 0.49259549379348755
    },
    {
      "name": "Question answering",
      "score": 0.47113293409347534
    },
    {
      "name": "Information retrieval",
      "score": 0.47025835514068604
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4580825865268707
    },
    {
      "name": "Natural language processing",
      "score": 0.4446238875389099
    },
    {
      "name": "Topic model",
      "score": 0.35102251172065735
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9360294",
      "name": "University of Bologna",
      "country": "IT"
    }
  ],
  "cited_by": 22
}