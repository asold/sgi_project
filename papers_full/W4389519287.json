{
  "title": "CLEVA: Chinese Language Models EVAluation Platform",
  "url": "https://openalex.org/W4389519287",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101595086",
      "name": "Yanyang Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5025918230",
      "name": "Jianqiao Zhao",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5044238657",
      "name": "Duo Zheng",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5007459088",
      "name": "Zi-Yuan Hu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100456847",
      "name": "Zhi Chen",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5109396190",
      "name": "Xiaohui Su",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5100768896",
      "name": "Yongfeng Huang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5052557101",
      "name": "Shijia Huang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5010087030",
      "name": "Dahua Lin",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "ShangHai JiAi Genetics & IVF Institute",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5069596903",
      "name": "Michael R. Lyu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100406711",
      "name": "Liwei Wang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4361193485",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W2936074642",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W3006990238",
    "https://openalex.org/W2915490524",
    "https://openalex.org/W2108646579",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W3182414670",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385374425",
    "https://openalex.org/W3207604732",
    "https://openalex.org/W2978707643",
    "https://openalex.org/W4285200483",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4283452667",
    "https://openalex.org/W4226204562",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W4367061104",
    "https://openalex.org/W4376653782",
    "https://openalex.org/W4379958452",
    "https://openalex.org/W3099911888",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W3036362489",
    "https://openalex.org/W4378464464",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4365601026",
    "https://openalex.org/W2974098375",
    "https://openalex.org/W4290943938",
    "https://openalex.org/W4385573759",
    "https://openalex.org/W4366327277",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4221165978",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2949884065",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2903314293",
    "https://openalex.org/W3210129272"
  ],
  "abstract": "Yanyang Li, Jianqiao Zhao, Duo Zheng, Zi-Yuan Hu, Zhi Chen, Xiaohui Su, Yongfeng Huang, Shijia Huang, Dahua Lin, Michael Lyu, Liwei Wang. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 186–217\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCLEV A: Chinese Language Models EV Aluation Platform\nYanyang Li1∗, Jianqiao Zhao1∗, Duo Zheng1, Zi-Yuan Hu1, Zhi Chen3, Xiaohui Su3,\nYongfeng Huang1, Shijia Huang1, Dahua Lin2,3, Michael R. Lyu1, Liwei Wang1†\n1Department of Computer Science and Engineering, The Chinese University of Hong Kong\n2Department of Information Engineering, The Chinese University of Hong Kong\n3Shanghai AI Laboratory\nAbstract\nWith the continuous emergence of Chinese\nLarge Language Models (LLMs), how to eval-\nuate a model’s capabilities has become an in-\ncreasingly significant issue. The absence of a\ncomprehensive Chinese benchmark that thor-\noughly assesses a model’s performance, the\nunstandardized and incomparable prompting\nprocedure, and the prevalent risk of contam-\nination pose major challenges in the current\nevaluation of Chinese LLMs. We present\nCLEVA, a user-friendly platform crafted to\nholistically evaluate Chinese LLMs. Our plat-\nform employs a standardized workflow to as-\nsess LLMs’ performance across various dimen-\nsions, regularly updating a competitive leader-\nboard. To alleviate contamination, CLEVA\ncurates a significant proportion of new data\nand develops a sampling strategy that guar-\nantees a unique subset for each leaderboard\nround. Empowered by an easy-to-use inter-\nface that requires just a few mouse clicks and\na model API, users can conduct a thorough\nevaluation with minimal coding. Large-scale\nexperiments featuring 23 Chinese LLMs have\nvalidated CLEVA’s efficacy. Our GitHub repo\nis https://github.com/LaVi-Lab/CLEVA.\n1 Introduction\nLarge language models (LLMs) have fundamen-\ntally revolutionized natural language processing.\nTransformer models with more than 100B parame-\nters have exhibited remarkable generalization abil-\nity across diverse tasks without the need for fine-\ntuning. The success of GPT-4 (OpenAI, 2023)\nand ChatGPT sparked a trend of training Chi-\nnese LLMs, with new models launching almost\nevery week (Zeng et al., 2023; Team, 2023; Cheng-\nhao Fan and Tian, 2023; Ji et al., 2023; Cui et al.,\n2023). These rapid developments aggravate the\nneed for Chinese LLM evaluation.\n∗Equal contributions.\n†Project leader and corresponding author\nAssessing the capacity of LLMs is non-trivial.\nTraditional practices that evaluate models on a sin-\ngle task at a time are gradually becoming obsolete,\nsince a single task can hardly characterize a full\nview of an LLM’s capacity. Instead, to effectively\ngrasp a holistic view of an LLM’s capacity, we need\nto decompose its capacity into various abilities,\nevaluate these abilities with numerous correspond-\ning tasks, and measure the competence of each task\nwith multiple metrics. In this sense, HELM (Liang\net al., 2022), leads the way in English LLM eval-\nuation, as it conducts an in-depth evaluation of\nEnglish LLMs on various NLP tasks using seven\nmetrics. In Chinese, previous attempts have shown\nlimitations, either in task selection or the metrics\nadopted. C-Eval (Huang et al., 2023), M3KE (Liu\net al., 2023), CMMLU (Li et al., 2023), GAOKAO-\nBench (Zhang et al., 2023), and MMCU (Zeng,\n2023) narrow down to knowledge and reasoning\nabilities, whose datasets are mostly constructed us-\ning Chinese exams. By the time of our submission,\nOpenCompass (Contributors, 2023b), with around\n74K Chinese queries out of 300K total, leans on\naccuracy as its sole metric, overlooking other im-\nportant aspects in LLM evaluation. FlagEval (Con-\ntributors, 2023a) inherits four out of seven met-\nrics from HELM and 22 existing Chinese datasets,\nhaving limited coverage on some significant tasks.\nA comprehensive Chinese benchmark incorporat-\ning diverse metrics to holistically evaluate Chinese\nLLMs is urgently demanded.\nPrompt-based evaluation in Chinese is largely\nunstandardized. Previous evaluations, such as\nHELM (Liang et al., 2022), do not explicitly opti-\nmize prompts, though LLMs’ significant sensitivity\nto the format of prompt has been observed (Webson\nand Pavlick, 2022; Abdou et al., 2022; Sanh et al.,\n2022). Moreover, unlike many English benchmarks\nthat have well-developed prompts (§ 3), many Chi-\nnese benchmarks are in their early stage and do\nnot enjoy such privileges. Chinese LLMs are eval-\n186\nuated using different prompts, making the results\nincomparable and hence untrustworthy.\nConsuming up to trillions of tokens during pre-\ntraining, LLMs are prone to train-test contami-\nnation (Brown et al., 2020), which significantly\nthreatens the validity of an evaluation. Previ-\nous work (OpenAI, 2023; Liang et al., 2022) ap-\nproaches this issue more from a consequentialist\nperspective: They examine the contamination risk,\nby methods like long n-gram overlap, only after the\nevaluation has been done. These post-evaluation\nanalyses, though responsibly examining if train-\ntest contamination happens, cannot alleviate the\nrisk of contamination in the first place. A proac-\ntive method to mitigate the contamination risk is of\ngreat importance.\nWe present CLEVA, Chinese Language mod-\nels EV Aluation platform that tackles the aforemen-\ntioned problems with the following features:\n• A comprehensive Chinese benchmark. In-\nspired by HELM (Liang et al., 2022),CLEVA\norganizes the evaluation tasks into two parts:\nability evaluation , which gauges specific\nLLM skills andapplication assessment, which\ntests how well LLMs apply their skills to\nreal-world applications (§ 4.1). Most of the\nwell-accepted Chinese datasets relevant to our\nability evaluation or application assessment\nare organized, standardized, and then adopted\nby our platform. More importantly, we de-\nsign new Chinese-specific tasks, e.g., Pinyin\ntransliteration and intent understanding, and\ncollect a substantial amount of new data, ac-\ncounting for 33.98% of our total data. As for\nthe metrics (§ 4.1), we incorporate metrics\nfor diversity and privacy into our system in\naddition to the seven in HELM. With 370K\n(over 9 million queries after augmentation)\ntest instances from 84 datasets and 9 metrics,\nCLEVA, so far, stands as the most extensive\nChinese evaluation dataset and possesses the\nmost dimensions, facilitating a holistic evalu-\nation of Chinese LLMs.\n• Standardized prompt-based evaluation\nmethodology. CLEVA takes full control of\nkey aspects of LLM evaluation, with data\nand prompts being the most important among\nthem. All data are jointly prepared with uni-\nfied preprocessing steps, ensuring a level play-\ning field for all LLMs. Meanwhile, CLEVA\nprovides a set of prompts, instead of just\none prompt as in prior work, for each task\nfor prompting-based inference (Brown et al.,\n2020). This prompt design ensures compa-\nrable evaluation results by standardizing the\nprompts used for testing, while also encour-\naging further analysis of LLMs’ sensitivity to\ndifferent prompts (Zhu et al., 2023).\n• An up-to-date and trustworthy leader-\nboard. CLEVA advocates a proactive method\nfor securing trustworthy evaluation results. By\ncollecting extensive new data, CLEVA sup-\npresses the leakage of testing data prior to the\nevaluation. Moreover, we frequently organize\nnew evaluation rounds, sampling a unique test\nset from 9 million augmented instances. This\nstrategy further mitigates the risk of train-test\ncontamination, improving the trustworthiness\nand timeliness of the leaderboard.\nCLEVA is thoroughly validated by benchmark-\ning 23 Chinese LLMs on our large-scale test sets\n(§ 6). The corresponding leaderboard and all other\nuser-friendly features will be continuously main-\ntained and improved to accommodate new models\nand evaluation methods.\n2 Related Work\nLLM evaluation is a rapidly developing field in\nrecent years to delineate the boundary of LLM’s\ncapability. In English, various systematic evalu-\nation benchmarks have been proposed. For ex-\nample, BIG-Bench (bench authors, 2023) is the\nlargest collection that covers more than 200 tasks.\nHELM (Liang et al., 2022) organizes tasks into core\nscenarios, which focus on use cases, and targeted\nevaluation, which aims to better understand models.\nHELM also presents a multi-metric measurement\nthat enables analysis of tradeoffs for each scenario.\nRecently, AGIEval (Zhong et al., 2023) is proposed\nto evaluate LLMs using challenging human ex-\nams. PromptBench (Zhu et al., 2023), on the other\nhand, measures the robustness of LLMs to prompts\nvia adversarial attacks. MT-Bench (Zheng et al.,\n2023) collects multi-turn questions and presents the\nChatbot Arena platform that treats GPT-4 (OpenAI,\n2023) as the judge.\nWhile CLEVA shares the same fundamental mo-\ntivation with HELM (Liang et al., 2022), to holis-\ntically evaluate language learning models in their\noriginal languages, CLEVA is far from a mere Chi-\nnese replica of HELM. Building on the foundation\nof HELM’s taxonomy, CLEVA introduces a range\n187\nof tasks, with particular emphasis on those unique\nto Chinese, to better assess the capabilities of Chi-\nnese LLMs. It offers a new perspective on prompts,\nproviding abundant prompt templates to standard-\nize evaluation and encourage in-depth exploration\nof models’ sensitivity. In terms of metrics,CLEVA\nexpands into new areas of diversity and privacy for\na more comprehensive evaluation. Finally,CLEVA\nproactively mitigates train-test contamination by\ncollecting a significant amount of new data, cre-\nating unique test sets by sampling, and regularly\nupdating the leaderboard. All of these evaluation\ndesigns are neatly packaged in a user-friendly plat-\nform to facilitate community usage.\nThere is also a lot of progress in evaluating Chi-\nnese LLMs (Huang et al., 2023; Liu et al., 2023; Li\net al., 2023; Zhang et al., 2023; Zeng, 2023). Open-\nCompass (Contributors, 2023b) and FlagEval (Con-\ntributors, 2023a) are two important attempts to\nevaluate Chinese LLMs. OpenCompass pools 53\npublic datasets and uses standard accuracy-like\nmetrics as the only measurement for each dataset.\nFlagEval, with a smaller number of datasets and\nmetrics, still needs further expansion to achieve\nsufficient coverage. Compared to previous efforts,\nCLEVA offers Chinese data from 84 datasets, in-\ncluding 33.98% original queries, while employing\nthe broadest range of metrics to promote holistic\nevaluation. CLEVA standardizes prompts (§ 4)\nand mitigates data contamination issues, pioneer-\ning new paths for LLM evaluation in general.\n3 Preliminaries\nTo measure the model performance on a task, a\nrelevant test set is constituted from a collection of\ninstances. A test instance will contain multiple in-\nput fields (string typically) and a list of references.\nWe then adopt a prompt template, which essen-\ntially describes how to assemble the model input,\na.k.a, prompt, from input fields (Bach et al., 2022).\nFor example, a Chinese paraphrase identification\nprompt template (and its translation) is:\nChinese Example:\n“{sentence1}”和“{sentence2}”这两个问题是在\n问同一件事情吗？\nEnglish Translation:\nAre the questions “ {sentence1}” and “ {sen-\ntence2}” asking the same thing?\nwhere {sentence1} and {sentence1} are two input\nfields that will be replaced by the two candidate\nquestions in the test instance. The prompt will\nbe fed into a black-box LLM to predict an output\nstring together with its probability.\nFinally, all model predictions and the corre-\nsponding test instances will be passed into a met-\nric to obtain a numerical value that indicates how\nwell the model performs. Following HELM (Liang\net al., 2022), a metric in this paper is an umbrella\nfor a dimension of measures that share similar pur-\nposes. For example, the accuracy metric corre-\nsponds to BLEU for translation and pass@k for\ncode synthesis. We employ nine metrics, fore-\ngrounding metrics beyond accuracy and ensuring a\nholistic evaluation.\n4 System Design\nCLEVA aims to deliver the following two key as-\nsets to users who try to evaluate their own LLMs:\n• A comprehensive and thorough assessment\nreport that informs users of the strength and\nlimitations of their models.\n• A trustworthy leaderboard reflecting the lat-\nest advancement of LLMs.\nWe will discuss our taxonomy that ensures compre-\nhensive evaluations, and challenges like train-test\ncontamination in leaderboard maintenance.\n4.1 Evaluation Taxonomy\nInspired by HELM (Liang et al., 2022), we present\na Tasks×Prompts×Metrics evaluation taxonomy\nfor users to evaluate their models. Our evaluation\ntaxonomy carefully designs a Chinese benchmark\ntargeting various LLM abilities, employs a set of\ndiverse prompt templates for each task to charac-\nterize the model performance variance, and adopts\nmultiple metrics to comprehensively assess LLMs.\nTasks. As shown in Figure 1, our Chinese LLM\nevaluation benchmark consists of two parts: ability\nevaluation and application assessment. Each task\nin ability evaluation focuses on one special skill\nof LLMs, while application assessment involves\nreal-world NLP tasks that require LLMs to solve\npractical use cases with their skill sets. Ability\nevaluation assesses LLM ability from five aspects:\n• Language measures how well LLMs under-\nstand Chinese. In addition to three conven-\ntional tasks, we incorporate two tasks specific\nto Chinese: Pinyin transliteration and classi-\ncal Chinese understanding.\n• Knowledge focuses on assessing the capac-\nity of knowledge acquired by LLMs. We\nfurther segment our evaluation into subject\n188\nCLEVA\nReading \nComprehension\nData-to-Text \nGeneration\nClosed-Book QA\nParaphrase \nIdentification\nSummarization\nSentiment Analysis\nText Classification\nOpinion Mining\nDialogue Generation\nParaphrase Generation\nTranslation\n……\nLanguage\nKnowledge\nReasoning\nHarms\nOthers\nLanguage \nModelling\nPinyin\nTransliteration\nCoreference \nResolution\nIntent\nUnderstanding\nSubject \nKnowledge\nCultural \nKnowledge\nReasoning \nPrimitive\nRealistic \nReasoning\nCopyright\nToxicity\nBias\nDisinformation\nMathematical \nCalculation\nInstruction \nFollowing\nClassical \nChinese \nUnderstanding\nInductive \nReasoning\nConceptual \nGeneralization\nCommonsense \nReasoning\nMathematical \nReasoning\nCode Synthesis\nDeductive \nReasoning\nApplication\nAbility\nFigure 1: CLEVA benchmark.\nknowledge and cultural knowledge (mainly\nChinese culture) based on the source of knowl-\nedge. This fine-grained design allows users to\nclosely analyze the model performance across\ndifferent knowledge categories.\n• Reasoning evaluates LLMs’ reasoning ability\nin two settings: reasoning primitives, which\nis independent of language and knowledge\nbackground, and realistic reasoning that re-\nquires reasoning with domain knowledge on\npractical scenarios. On top of HELM, we ad-\nditionally include commonsense reasoning, in-\nductive reasoning, conceptual generalization,\nand deductive reasoning.\n• Harms evaluates the potential risk of LLMs in\ncopyright, disinformation, bias, and toxicity.\n• Others is newly introduced to include cru-\ncial yet uncategorized tasks like mathematical\ncalculation and instruction following.\nFor application assessment, CLEVA features 11\nreal-world NLP tasks. In addition to the core sce-\nnarios of HELM, we newly includeopinion mining,\ndialogue generation, paraphrase generation, trans-\nlation, paraphrase identification, and data-to-text\ngeneration. A detailed description of each task is\ndocumented in Appendix B.\nWe instantiate the aforementioned tasks in two\nways: by directly adopting related public Chinese\ndatasets and by collecting new data. For well-\nstudied tasks, widely-recognized datasets are the\nbest options for forming our benchmark. However,\nmany important tasks, such as reasoning primitive,\nPinyin transliteration, and disinformation, lack cor-\nresponding Chinese datasets, making the evalua-\ntion even more challenging. On these occasions,\nwe either synthesize using sophisticated rule-based\nscripts (e.g., reasoning primitive) or enlist profes-\nsional human annotators to construct new test sets\n(See Appendix C for annotation details). In total,\nthe 31 tasks include 370K test instances from 84\ndatasets (9M queries in total after applying multiple\nprompt templates and data augmentation), 33.98%\nof which are newly collected.\nPrompts. Ideally, an LLM should be a general in-\nterface, capable of understanding prompts with the\nsame semantics, regardless of variations in surface\nforms. However, LLMs’ notorious sensitivity to\nprompt templates hinders accurate evaluation (Web-\nson and Pavlick, 2022; Abdou et al., 2022), leading\nto results that are sometimes incomparable. To bet-\nter understand an LLM’s sensitivity to plausible\nhuman instructions, multiple prompt templates are\nneeded, rather than a single template as in previous\nwork (Contributors, 2023a,b; Liang et al., 2022).\nIn this work, we manually annotate an average of\n3.95 prompt templates for each test set and support\nall major prompting formats. CLEVA calculates\nthe performance statistics across the entire set of\nprompts. These statistics do more than just examine\nthe robustness to prompt templates, as reflected by\nthe standard deviation; they also help estimate the\nupper and lower bounds of an LLM’s performance\non a specific test set, as indicated by the minimum\nand maximum values. Users can benefit from these\nstatistics to select models and to make informed\ntrade-offs between performance and investment in\nprompt engineering. More discussions on prompt\ntemplates we provided are in Appendix F.\nMetrics. We adopt the 7 metrics from HELM for a\nholistic evaluation, and, to address recent interest\nin chatbots and safety concerns, we add two new\ndimensions: diversity and privacy.\n• Accuracy. Accuracy refers to the standard\nmetrics to measure model performance on dif-\nferent tasks, e.g., F1 score for question answer-\ning and ROUGE score for summarization.\n• Calibration and uncertainty. Calibration\nrepresents the gap between the model confi-\n189\ndence and its actual error rate and is measured\nmainly by expected calibration error (ECE,\n(Naeini et al., 2015)).\n• Robustness. Robustness is the worst-case\nperformance of a model across transforma-\ntions of test instances. We focus on semantics-\npreserving perturbations as there are many\nwell-studied data augmentation tools.\n• Fairness. Similar to robustness, fairness em-\nploys perturbations related to social groups\nto test the disparate treatment and disparate\nimpact of LLMs.\n• Bias and stereotypes. We quantify bias as the\ndisproportionate representation of different so-\ncial groups. This is gauged through the rates\nat which these groups are mentioned during\nmodel generation. Additionally, we interpret\nstereotypes as uneven associations between\nthese social groups and certain stereotyped\nterms, such as occupational roles.\n• Toxicity. Following HELM (Liang et al.,\n2022), toxicity is a general term that covers\nhate speech, abusive language, etc.\n• Efficiency. Efficiency is a rather broad con-\ncept that has many subtleties. It could refer\nto training or inference efficiency and is mea-\nsured by energy, carbon, and wall-clock time.\nAs most information could be confidential, we\nfocus only on the inference wall-clock time.\n• Diversity. Given the popularity of LLM-\nbased chatbots, we incorporate the conven-\ntional diversity metric in dialogue systems\nthat evaluates the response surface form di-\nversity (Li et al., 2016). Here, we employ the\ndiversity metrics from Miller et al. (2017).\n• Privacy. In the real-world deployment of\nLLMs, detecting private information in the\ngenerated text, such as Personally Identifiable\nInformation (PII), is a challenging yet impor-\ntant question. We report the portion of PII in\nthe whole test set to make the privacy eval-\nuation generalizable. CLEVA adopts some\nestablished tools to smoothly detect PII, and\nwe are working on accommodating more as-\npects of private content in the near future.\nDetailed metric lists are provided in Appendix D.\n4.2 Leaderboard & Data Contamination\nEnsuring fairness, objectivity, and authority is cen-\ntral to maintaining a trustworthy leaderboard. Pre-\nvious work (Brown et al., 2020) has reported train-\ntest contamination, a situation where the test set\nis included in the training data, leading to unreli-\nable evaluations. Many existing benchmarks, e.g.,\nHuang et al. (2023), conceal the test set labels to\navoid data contamination. Given the small scale of\ntheir test sets and the large-scale training corpora\nused by modern LLMs, the risk of unintentional\ntrain-test contamination remains high. Sun et al.\n(2023a) address this problem by making the official\ntest set private and requiring users to submit mod-\nels’ weights for evaluation. However, this arrange-\nment is unpopular because numerous cutting-edge\nmodels consider their weights highly confidential.\nWe advocate “mutual confidentiality” in LLM\nevaluation: Users need not expose their model de-\ntails, and the platform should minimize the risk of\ndisclosing its test set. Instead of model weights,\nCLEVA only requires API access. We proactively\nachieve the other half of mutual confidentiality by\ncontinuously collecting new data and frequently or-\nganizing leaderboard rounds with unique test sets\nsampling from our full-scale 9 million augmented\ninstances. These strategies not only improve evalu-\nation efficiency but also alleviate train-test contam-\nination from data and temporal perspectives.\nTo make sure that the sampled subset delivers\naccurate results, our sampling strategy is not just\nrandom sampling: It estimates an acceptable ap-\nproximation error threshold (i.e., within this thresh-\nold, the evaluation results on the sampled set have\nat least a 70% chance to correctly rank any model\npairs), then adjusts the sampling rate for each task\naccording to this threshold, reducing the risk of\nover-/under-estimating the model performance.\n5 Usage Example\nUpon authentication, users are immediately pre-\nsented with an interactive summary of our evalu-\nation results of 23 LLMs. Users can select from\nthese models, freely exploring the evaluation re-\nsults from all 9 metrics and 31 tasks.\nCLEVA simplifies the evaluation process of new\nmodels with minimal coding required. If a user has\na model to evaluate, the user only needs a few min-\nutes to finish these three steps: entering the model’s\nAPI, selecting relevant tasks from 31 choices, and\npicking desired metrics from 9 options. CLEVA\nwill autonomously call the user’s model, extract\nthe corresponding responses, and compute the final\nmetrics. Detailed descriptions and screenshots of\nCLEVA are listed in Appendix A.\n190\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000047/uni00000044/uni00000059/uni0000004c/uni00000051/uni00000046/uni0000004c/uni00000010/uni00000013/uni00000013/uni00000016\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048/uni00000010/uni0000004c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000028/uni00000035/uni00000031/uni0000002c/uni00000028/uni00000010/uni00000025/uni00000052/uni00000057/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni0000001a/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000024/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048/uni00000010/uni0000004c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni0000001a/uni00000025\n/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000019/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000024/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000035/uni00000052/uni00000045/uni00000058/uni00000056/uni00000057/uni00000051/uni00000048/uni00000056/uni00000056\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048/uni00000010/uni0000004c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni0000001a/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000019/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000024/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000024/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000026/uni00000044/uni0000004f/uni0000004c/uni00000045/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048/uni00000010/uni0000004c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000028/uni00000035/uni00000031/uni0000002c/uni00000028/uni00000010/uni00000025/uni00000052/uni00000057\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni0000001a/uni00000025\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000019/uni00000025\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000047/uni00000044/uni00000059/uni0000004c/uni00000051/uni00000046/uni0000004c/uni00000010/uni00000013/uni00000013/uni00000016\n/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000024/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000025/uni0000004c/uni00000044/uni00000056\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025\n/uni00000028/uni00000035/uni00000031/uni0000002c/uni00000028/uni00000010/uni00000025/uni00000052/uni00000057/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000019/uni00000025\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025\n/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000047/uni00000044/uni00000059/uni0000004c/uni00000051/uni00000046/uni0000004c/uni00000010/uni00000013/uni00000013/uni00000016/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048/uni00000010/uni0000004c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni0000001a/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000024/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni0000003a/uni0000004c/uni00000051/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048\nFigure 2: The mean win rate of 23 models in 31 tasks. The mean win rate is the probability of a model outperforming\na random different model on a random task. We exclude toxicity, privacy, and efficiency metrics as all models\nexcel in the former two, and the latter is often paired with other metrics to deliver meaningful comparisons. Since\nrobustness and fairness involve expensive data augmentation, we only evaluate ChatGPT and Claude-instant.\n6 Evaluation\nSetup. We sample 6.43% of our data to test 23 mod-\nels that support Chinese (See Appendix E). As for\nthe cost, for example, it takes roughly 1600 GPU\nhours (NVIDIA A100 80G) to evaluate BLOOMZ-\n176B-mt (Muennighoff et al., 2023).\nResults & Analysis. Figure 2 ranks all models by\ntheir mean win rates under different metrics.\n• Accuracy. It can be seen that GPT-4 (OpenAI,\n2023) has the highest winning rate, followed\nby other limited-accessed models. This result\nshows a considerable margin between the per-\nformance of open-source models and limited-\naccessed models. Recent small instruction-\nfollowing models are better than large LLMs\nwithout instruction-tuning, and are even better\nthan some early large instruction-following\nmodels, indicating the necessity of effective\ninstruction tuning.\n• Robustness. The trend on robustness is\nroughly the same as that of accuracy, with the\nexception of LLaMA (Touvron et al., 2023).\n• Fairness. Most of the model rankings have\nchanged. One possible reason is that fairness\ninvolves simplified-to-traditional conversion\n(See Appendix D), and many models have\nrarely seen traditional Chinese in pretraining.\n• Calibration. We report ECE-10(Kumar et al.,\n2019) following HELM. We find that mod-\nels with more parameters tend to have higher\nECE. For example, GLM-130B (Zeng et al.,\n2023) and LLaMA-65B rank at the bottom.\nFor BLOOMZ-mt-7B vs BLOOMZ-mt-176B\nand BLOOM-7B1 vs BLOOM-176B (Scao\net al., 2022), the smaller one wins.\n• Bias. We focus on gender bias for comparison.\nGPT-4 and other models, which rank top by\nother metrics, are at the bottom, while most of\nthe open-source models have low bias. This\nis because open-source models usually output\nshorter, resulting in a lower risk of bias.\n• Diversity. We choose inter-distinct to com-\npare different models. Open-source models\ngenerate more diverse and innovative expres-\nsion than limited-accessed ones, probably due\nto their fewer safety concerns.\nMore detailed results and analysis are in Ap-\npendix G.\n7 Conclusion\nWe present CLEVA, a Chinese LLM evaluation\nplatform. With the largest scale of Chinese in-\nstances and broadest metrics, CLEVA provides a\ncomprehensive benchmark to holistically evaluate\nChinese LLMs. CLEVA standardizes key compo-\nnents, such as prompt templates, to make evalua-\ntion comparable. It also proactively mitigates the\ncontamination issue by collecting large-scale new\ndata, sampling for unique test sets, and regularly\nupdating the leaderboard.\n191\nLimitations\nWithout further information needed from users, we\ncan only use the inference walk-clock time as the\nmetric, which may have a larger variance when the\nnetwork is unstable. We advise users to adopt other\nmethods in addition to our metric to make a more\ninformed judgment.\nIn addition, how to evaluate privacy is still a chal-\nlenging problem. We will update our underlying\nalgorithm frequently to reflect the latest progress\nof privacy evaluation.\nEthics Statement\nWe consider the ethics issue in two folds, respon-\nsible data collection and usage. We widely adopt\nmanual data collection to enhance the variety of\nthe tasks supported by CLEVA. During the manual\ndata collection, all the crowdsourcing workers and\nthe translators are well compensated. No sensitive\ninformation of any kind is collected, and all the\nparticipants are informed of the data usage.\nCLEVA involves tasks that evaluate LLMs’ per-\nformance on harm. Like prior work on this similar\ntopic, a proportion of data that contains bias, tox-\nicity, and other harmful content are deliberately\nincluded to evaluate how LLMs react in these situ-\nations. We pay extra caution to the related datasets,\nand we advocate the responsible usage of these\ndatasets. These datasets should only be used for\nLLM evaluation. Our sampling mechanism also\nreduces the unwanted leakage of the data.\nAcknowledgements\nThis work was supported by National Key R&D\nProgram of China (Project No. 2022ZD0161200,\n2022ZD0161201). It was also partially funded by\nthe Centre for Perceptual and Interactive Intelli-\ngence (CPIl) Ltd under the Innovation and Technol-\nogy Commission (ITC)’s InnoHK. Liwei Wang is\na Principal Investigator of CPII under the InnoHK.\nThis work was partially supported by the Research\nGrants Council of the Hong Kong Special Admin-\nistrative Region, China (No. CUHK 14206921 of\nthe General Research Fund).\nReferences\nMostafa Abdou, Vinit Ravishankar, Artur Kulmizev, and\nAnders Søgaard. 2022. Word order does matter and\nshuffled language models know it. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages\n6907–6919. Association for Computational Linguis-\ntics.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Benjamin Mann, Nova DasSarma, Nelson\nElhage, Zac Hatfield-Dodds, Danny Hernandez, Jack-\nson Kernion, Kamal Ndousse, Catherine Olsson,\nDario Amodei, Tom B. Brown, Jack Clark, Sam Mc-\nCandlish, Chris Olah, and Jared Kaplan. 2021. A\ngeneral language assistant as a laboratory for align-\nment. CoRR, abs/2112.00861.\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert\nWebson, Colin Raffel, Nihal V . Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea San-\ntilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu,\nGunjan Chhablani, Han Wang, Jason Alan Fries,\nMaged S. Al-shaibani, Shanya Sharma, Urmish\nThakker, Khalid Almubarak, Xiangru Tang, Xian-\ngru Tang, Mike Tian-Jian Jiang, and Alexander M.\nRush. 2022. Promptsource: An integrated develop-\nment environment and repository for natural language\nprompts.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom B.\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBenjamin Mann, and Jared Kaplan. 2022a. Train-\ning a helpful and harmless assistant with rein-\nforcement learning from human feedback. CoRR,\nabs/2204.05862.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosiute, Liane\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemí Mercado, Nova DasSarma, Robert\nLasenby, Robin Larson, Sam Ringer, Scott John-\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. 2022b. Constitutional AI: harmless-\nness from AI feedback. CoRR, abs/2212.08073.\nBIG bench authors. 2023. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of lan-\nguage models. Transactions on Machine Learning\nResearch.\n192\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum\nThain, and Lucy Vasserman. 2019. Nuanced metrics\nfor measuring unintended bias with real data for text\nclassification. In Companion of The 2019 World\nWide Web Conference, WWW 2019, San Francisco,\nCA, USA, May 13-17, 2019, pages 491–500. ACM.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nBen Buchanan, Andrew Lohn, Micah Musser, and Kate-\nrina Sedova. 2021. Truth, lies, and automation: How\nlanguage models could change disinformation.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B. Brown, Dawn Song, Úl-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.\nExtracting training data from large language models.\nIn 30th USENIX Security Symposium, USENIX Se-\ncurity 2021, August 11-13, 2021, pages 2633–2650.\nUSENIX Association.\nWanxiang Che, Yunlong Feng, Libo Qin, and Ting\nLiu. 2021. N-LTP: An open-source neural language\ntechnology platform for Chinese. In Proceedings of\nthe 2021 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 42–49, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nZhenyi Lu Chenghao Fan and Jie Tian. 2023. Chinese-\nvicuna: A chinese instruction-following llama-based\nmodel.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nFlagEval Contributors. 2023a. Flageval. https://\ngithub.com/FlagOpen/FlagEval.\nOpenCompass Contributors. 2023b. Opencompass: A\nuniversal evaluation platform for foundation models.\nhttps://github.com/InternLM/OpenCompass.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient\nand effective text encoding for chinese llama and\nalpaca. CoRR, abs/2304.08177.\nJiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng,\nFei Mi, Helen Meng, and Minlie Huang. 2022.\nCOLD: A benchmark for chinese offensive language\ndetection. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates, De-\ncember 7-11, 2022, pages 11580–11599. Association\nfor Computational Linguistics.\nKaustubh D. Dhole, Varun Gangal, Sebastian\nGehrmann, Aadesh Gupta, Zhenhao Li, Saad Ma-\nhamood, Abinaya Mahendiran, Simon Mille, Ashish\nSrivastava, Samson Tan, Tongshuang Wu, Jascha\nSohl-Dickstein, Jinho D. Choi, Eduard Hovy, On-\ndrej Dusek, Sebastian Ruder, Sajant Anand, Na-\ngender Aneja, Rabin Banjade, Lisa Barthe, Hanna\nBehnke, Ian Berlot-Attwell, Connor Boyle, Car-\noline Brun, Marco Antonio Sobrevilla Cabezudo,\nSamuel Cahyawijaya, Emile Chapuis, Wanxiang\nChe, Mukund Choudhary, Christian Clauss, Pierre\nColombo, Filip Cornell, Gautier Dagan, Mayukh Das,\nTanay Dixit, Thomas Dopierre, Paul-Alexis Dray, Su-\nchitra Dubey, Tatiana Ekeinhor, Marco Di Giovanni,\nRishabh Gupta, Rishabh Gupta, Louanes Hamla,\nSang Han, Fabrice Harel-Canada, Antoine Honore,\nIshan Jindal, Przemyslaw K. Joniak, Denis Kleyko,\nVenelin Kovatchev, Kalpesh Krishna, Ashutosh Ku-\nmar, Stefan Langer, Seungjae Ryan Lee, Corey James\nLevinson, Hualou Liang, Kaizhao Liang, Zhexiong\nLiu, Andrey Lukyanenko, Vukosi Marivate, Gerard\nde Melo, Simon Meoni, Maxime Meyer, Afnan Mir,\nNafise Sadat Moosavi, Niklas Muennighoff, Timo-\nthy Sum Hon Mun, Kenton Murray, Marcin Namysl,\nMaria Obedkova, Priti Oli, Nivranshu Pasricha, Jan\nPfister, Richard Plant, Vinay Prabhu, Vasile Pais,\nLibo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas\nRaunak, Roy Rinberg, Nicolas Roberts, Juan Diego\nRodriguez, Claude Roux, Vasconcellos P. H. S.,\nAnanya B. Sai, Robin M. Schmidt, Thomas Scialom,\nTshephisho Sefara, Saqib N. Shamsi, Xudong Shen,\nHaoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel,\nDamien Sileo, Jamie Simon, Chandan Singh, Ro-\nman Sitelew, Priyank Soni, Taylor Sorensen, William\n193\nSoto, Aman Srivastava, KV Aditya Srivatsa, Tony\nSun, Mukund Varma T, A Tabassum, Fiona Anting\nTan, Ryan Teehan, Mo Tiwari, Marie Tolkiehn,\nAthena Wang, Zijian Wang, Gloria Wang, Zijie J.\nWang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata,\nXinyi Wu, Witold Wydma´nski, Tianbao Xie, Usama\nYaseen, M. Yee, Jing Zhang, and Yue Zhang. 2021.\nNl-augmenter: A framework for task-sensitive natu-\nral language augmentation.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022.\nOpenPrompt: An open-source framework for prompt-\nlearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 105–113, Dublin, Ire-\nland. Association for Computational Linguistics.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:\ngeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022 , pages 320–335.\nAssociation for Computational Linguistics.\nNan Duan. 2018. Overview of the NLPCC 2018 shared\ntask: Open domain QA. In Natural Language Pro-\ncessing and Chinese Computing - 7th CCF Inter-\nnational Conference, NLPCC 2018, Hohhot, China,\nAugust 26-30, 2018, Proceedings, Part II , volume\n11109 of Lecture Notes in Computer Science, pages\n452–456. Springer.\nRan El-Yaniv and Yair Wiener. 2010. On the founda-\ntions of noise-free selective classification. J. Mach.\nLearn. Res., 11:1605–1641.\nHao Fu, Yao; Peng and Tushar Khot. 2022. How does\ngpt obtain its ability? tracing emergent abilities of\nlanguage models to their sources. Yao Fu’s Notion.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021a. The pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021b.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n3816–3830. Association for Computational Linguis-\ntics.\nPrakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. Dialfact: A benchmark for\nfact-checking in dialogue. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 3785–\n3801. Association for Computational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nKübler, and Lawrence S. Moss. 2020. OCNLI:\noriginal chinese natural language inference. CoRR,\nabs/2010.05444.\nXuming Hu, Zhijiang Guo, GuanYu Wu, Aiwei Liu,\nLijie Wen, and Philip Yu. 2022. CHEF: A pilot Chi-\nnese dataset for evidence-based fact-checking. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3362–3376, Seattle, United States. Association\nfor Computational Linguistics.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei\nZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu,\nMaosong Sun, and Junxian He. 2023. C-eval: A\nmulti-level multi-discipline chinese evaluation suite\nfor foundation models. CoRR, abs/2305.08322.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Lei Zhang, Baochang Ma, and Xiangang Li.\n2023. Exploring the impact of instruction data scal-\ning on large language models: An empirical study on\nreal-world use cases. CoRR, abs/2303.14742.\nTom Kocmi, Rachel Bawden, Ondrej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grund-\nkiewicz, Barry Haddow, Rebecca Knowles, Philipp\nKoehn, Christof Monz, Makoto Morishita, Masaaki\nNagata, Toshiaki Nakazawa, Michal Novák, Martin\nPopel, and Maja Popovic. 2022. Findings of the\n2022 conference on machine translation (WMT22).\nIn Proceedings of the Seventh Conference on Ma-\nchine Translation, WMT 2022, Abu Dhabi, United\nArab Emirates (Hybrid), December 7-8, 2022, pages\n1–45. Association for Computational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In NeurIPS.\nAnanya Kumar, Percy Liang, and Tengyu Ma. 2019.\nVerified uncertainty calibration. In Advances in Neu-\nral Information Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 3787–3798.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen,\nJai Prakash Gupta, Donald Metzler, and Lucy Vasser-\nman. 2022. A new generation of perspective API:\nefficient multilingual character-level transformers. In\n194\nKDD ’22: The 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, Washington,\nDC, USA, August 14 - 18, 2022 , pages 3197–3207.\nACM.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang,\nHai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. 2023. CMMLU: measuring massive mul-\ntitask language understanding in chinese. CoRR,\nabs/2306.09212.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nNAACL HLT 2016, The 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nSan Diego California, USA, June 12-17, 2016, pages\n110–119. The Association for Computational Lin-\nguistics.\nWenhao Li, Fanchao Qi, Maosong Sun, Xiaoyuan Yi,\nand Jiarui Zhang. 2021. CCPM: A chinese classical\npoetry matching dataset. CoRR, abs/2106.01979.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Ladhak,\nFrieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert\nYüksekgönül, Mirac Suzgun, Nathan Kim, Neel\nGuha, Niladri S. Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models. CoRR, abs/2211.09110.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nHaitao Lin, Liqun Ma, Junnan Zhu, Lu Xiang, Yu Zhou,\nJiajun Zhang, and Chengqing Zong. 2021. CSDS:\nA fine-grained Chinese dataset for customer service\ndialogue summarization. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 4436–4451, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 3214–3252. Association for\nComputational Linguistics.\nChuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu\nDong, Xiaohan Peng, Shuting Zhang, Jianxiang\nPeng, Peiyi Zhang, Qingqing Lyu, Xiaowen Su,\nQun Liu, and Deyi Xiong. 2023. M3KE: A mas-\nsive multi-level multi-subject knowledge evaluation\nbenchmark for chinese large language models.CoRR,\nabs/2305.10263.\nKang Liu, Liheng Xu, and Jun Zhao. 2012. Opinion\ntarget extraction using word-based translation model.\nIn Proceedings of the 2012 Joint Conference on Em-\npirical Methods in Natural Language Processing and\nComputational Natural Language Learning, EMNLP-\nCoNLL 2012, July 12-14, 2012, Jeju Island, Korea,\npages 1346–1356. ACL.\nPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and\nKai-Wei Chang. 2023. A survey of deep learning for\nmathematical reasoning. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2023,\nToronto, Canada, July 9-14, 2023 , pages 14605–\n14631. Association for Computational Linguistics.\nIan R. McKenzie, Alexander Lyzhov, Michael Pieler,\nAlicia Parrish, Aaron Mueller, Ameya Prabhu, Euan\nMcLean, Aaron Kirtland, Alexis Ross, Alisa Liu,\nAndrew Gritsevskiy, Daniel Wurgaft, Derik Kauff-\nman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh,\nMax Weiss, Sicong Huang, The Floating Droid, Tom\nTseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang,\nZhengping Zhou, Najoung Kim, Samuel R. Bowman,\nand Ethan Perez. 2023. Inverse scaling: When bigger\nisn’t better. CoRR, abs/2306.09479.\nAlexander H. Miller, Will Feng, Dhruv Batra, Antoine\nBordes, Adam Fisch, Jiasen Lu, Devi Parikh, and Ja-\nson Weston. 2017. Parlai: A dialog research software\nplatform. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2017, Copenhagen, Denmark, September\n9-11, 2017 - System Demonstrations , pages 79–84.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023. Crosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nMahdi Pakdaman Naeini, Gregory F. Cooper, and Milos\nHauskrecht. 2015. Obtaining well calibrated prob-\nabilities using bayesian binning. In Proceedings of\nthe Twenty-Ninth AAAI Conference on Artificial In-\ntelligence, January 25-30, 2015, Austin, Texas, USA,\npages 2901–2907. AAAI Press.\n195\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2023. Codegen: An open large language\nmodel for code with multi-turn program synthesis. In\nThe Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nRoma Patel and Ellie Pavlick. 2022. Mapping language\nmodels to grounded conceptual spaces. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. In Ad-\nvances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Process-\ning Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pages 11054–11070.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2463–2473. Association for\nComputational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, WMT 2018,\nBelgium, Brussels, October 31 - November 1, 2018,\npages 186–191. Association for Computational Lin-\nguistics.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-text generation with content selection and\nplanning. In The Thirty-Third AAAI Conference on\nArtificial Intelligence, AAAI 2019, The Thirty-First\nInnovative Applications of Artificial Intelligence Con-\nference, IAAI 2019, The Ninth AAAI Symposium on\nEducational Advances in Artificial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - February\n1, 2019, pages 6908–6915. AAAI Press.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, Jonathan Tow, Alexander M. Rush,\nStella Biderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Benoît Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-\ntor Sanh, Hugo Laurençon, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, and et al. 2022. BLOOM:\nA 176b-parameter open-access multilingual language\nmodel. CoRR, abs/2211.05100.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nCoRR, abs/2302.04761.\nZhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu,\nand Xiaoyan Zhu. 2019. Long and diverse text gen-\neration with planning-based hierarchical variational\nmodel. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 3255–\n3266. Association for Computational Linguistics.\nShane Storks, Qiaozi Gao, and Joyce Y . Chai. 2019.\nCommonsense reasoning for natural language under-\nstanding: A survey of benchmarks, resources, and\napproaches. CoRR, abs/1904.01172.\nHao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and\nMinlie Huang. 2023a. Safety assessment of chinese\nlarge language models. CoRR, abs/2304.10436.\n196\nHong Sun and Ming Zhou. 2012. Joint learning of a\ndual SMT system for paraphrase generation. In The\n50th Annual Meeting of the Association for Compu-\ntational Linguistics, Proceedings of the Conference,\nJuly 8-14, 2012, Jeju Island, Korea - Volume 2: Short\nPapers, pages 38–42. The Association for Computer\nLinguistics.\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2019.\nProbing prior knowledge needed in challenging\nchinese machine reading comprehension. CoRR,\nabs/1904.09679.\nTianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li,\nQinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan\nShao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining\nZheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yun-\nhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu,\nZhangyue Yin, Xuanjing Huang, and Xipeng Qiu.\n2023b. Moss: Training conversational language mod-\nels from synthetic data.\nInternLM Team. 2023. Internlm: A multilingual lan-\nguage model with progressively enhanced capabili-\nties. https://github.com/InternLM/InternLM.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand verification. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2018, New\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\n1 (Long Papers) , pages 809–819. Association for\nComputational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book qa? In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 3241–3251. Associa-\ntion for Computational Linguistics.\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 845–854,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), ACL 2023, Toronto, Canada, July 9-14, 2023,\npages 13484–13508. Association for Computational\nLinguistics.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 2300–2344. Association for\nComputational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022b. Emer-\ngent abilities of large language models. Trans. Mach.\nLearn. Res., 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022c. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A Chinese language understanding evalua-\ntion benchmark. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4762–4772, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nLiang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei\nZhang, Hu Yuan, Huilin Xu, Guoao Wei, Xiang\nPan, and Hai Hu. 2021. Fewclue: A chinese\nfew-shot learning evaluation benchmark. CoRR,\nabs/2107.07498.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGLM-130B: an open bilingual pre-trained model. In\nThe Eleventh International Conference on Learning\n197\nRepresentations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net.\nHui Zeng. 2023. Measuring massive multitask chinese\nunderstanding. CoRR, abs/2304.12986.\nBowei Zhang, Weiwei Sun, Xiaojun Wan, and Zong-\nming Guo. 2019. PKU paraphrase bank: A sentence-\nlevel paraphrase corpus for chinese. In Natural Lan-\nguage Processing and Chinese Computing - 8th CCF\nInternational Conference, NLPCC 2019, Dunhuang,\nChina, October 9-14, 2019, Proceedings, Part I, vol-\nume 11838 of Lecture Notes in Computer Science ,\npages 814–826. Springer.\nLei Zhang and Bing Liu. 2017. Sentiment analysis and\nopinion mining. In Claude Sammut and Geoffrey I.\nWebb, editors, Encyclopedia of Machine Learning\nand Data Mining, pages 1152–1161. Springer.\nSheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and\nShanshan Liu. 2018. Multi-scale attentive interac-\ntion networks for chinese medical question answer\nselection. IEEE Access, 6:74061–74071.\nXiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying,\nLiang He, and Xipeng Qiu. 2023. Evaluating the\nperformance of large language models on GAOKAO\nbenchmark. CoRR, abs/2305.12474.\nChujie Zheng, Minlie Huang, and Aixin Sun. 2019.\nChID: A large-scale Chinese IDiom dataset for cloze\ntest. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n778–787, Florence, Italy. Association for Computa-\ntional Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judg-\ning llm-as-a-judge with mt-bench and chatbot arena.\nCoRR, abs/2306.05685.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\nShuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,\nand Nan Duan. 2023. Agieval: A human-centric\nbenchmark for evaluating foundation models. CoRR,\nabs/2304.06364.\nJingyan Zhou, Jiawen Deng, Fei Mi, Yitong Li, Yasheng\nWang, Minlie Huang, Xin Jiang, Qun Liu, and He-\nlen Meng. 2022. Towards identifying social bias in\ndialog systems: Frame, datasets, and benchmarks.\nCoRR, abs/2202.08011.\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang,\nHao Chen, Yidong Wang, Linyi Yang, Wei Ye,\nNeil Zhenqiang Gong, Yue Zhang, and Xing Xie.\n2023. Promptbench: Towards evaluating the robust-\nness of large language models on adversarial prompts.\nCoRR, abs/2306.04528.\nQi Zhu, Kaili Huang, Zheng Zhang, Xiaoyan Zhu, and\nMinlie Huang. 2020. Crosswoz: A large-scale chi-\nnese cross-domain task-oriented dialogue dataset.\nTrans. Assoc. Comput. Linguistics, 8:281–295.\nAndy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrik-\nson. 2023. Universal and transferable adversarial\nattacks on aligned language models.\n198\nA Platform Usage\nTo fully utilize our CLEVA to evaluate a large\nlanguage model, users can take advantage of our\nuser-friendly web application. As shown by Fig-\nure 3(a), users will first see our latest leaderboard\nresults with an interactive interface. Users can\nprobe the latest results freely, selecting the models\nthey care about and comparing different models on\n9 different metrics. If a user intends to evaluate a\nnew model, a holistic evaluation can be deployed\nwith just a few mouse clicks and model APIs: The\nprocess initiates with users inputting a specific link\nthat enables our platform to interface with the to-\nbe-evaluated model, as shown by Figure 3(b). Sub-\nsequently, users are granted the flexibility to select\napplicable tasks from an extensive set of 31 pre-\ndefined options (Figure 3(c)). The concluding step\ninvolves the selection of the appropriate evaluation\nmetrics, from the 9 available options (Figure 3(d)).\nB Benchmark\nIn this section, we provide a detailed description\nalong with an example for each task involved in\nour benchmark. This example is for demonstra-\ntion only and does not represent the whole test\ndistribution and all possible prompt templates. We\nalso accompany the English translation after each\nChinese example. In the provided example, text\nhighlighted in green is a reference that we expect\nLLMs to predict and the other part is prompt con-\nstructed by a random prompt template and input\nfields from a random test instance.\nB.1 Ability Evaluation\nB.1.1 Language\nLanguage Modelling. This task asks the LLM to\nscore the probability of the input text. We use bits\nper bytes (Gao et al., 2021a) as the metric that al-\nlows us to make comparisons with different tokeniz-\ners. Data are sampled from CLUECorpus2020 (Xu\net al., 2020).\nCoreference Resolution. Coreference resolution\nis a traditional NLP task. We sample data from\nCLUEWSC (Xu et al., 2021), where the model\nmust answer whether a given pronoun refers to\na given entity (the Winograd Schema Challenge).\nWe use accuracy as the metric for this problem. A\ncoreference resolution example is shown below:\nChinese Example:\n蒋盈波原来所在的教研室有位副教授去德国参加一个学术\n活动，活动中结识了一位华裔德籍的同行，那同行在自己\n家中招待了他一次，言谈之间，双方忽然都感到巧事真\n多，而世界真小\n在这里，“他”的意思是“同行”。是或否？ 否\nEnglish Translation:\nAn associate professor from the research office where Jiang\nYingbo used to work went to Germany to attend an academic\nevent. During the event, he met a Chinese-German colleague\nwho invited him to his home. While talking, they both suddenly\nfelt that there were many coincidences and the world was really\nsmall.\nHere, does “him” refer to “colleague”? Yes or No? No\nPinyin Transliteration. In this task, the model\nneeds to annotate the Pinyin of a Chinese sen-\ntence or infer a reasonable Chinese sentence from\na Pinyin sequence. We introduce this task because\nPinyin is Chinese-specific and crucial for some ap-\nplications, e.g., writing songs needs to rhyme in\nlyrics according to Pinyin and offensive language\nsometimes is tweaked to sentences with a simi-\nlar Pinyin to circumvent the blocking of sensitive\nwords. Since this task is newly introduced and there\nis no primary metric available, we treat this task\nas a translation task and evaluate the performance\nwith BLEU (Papineni et al., 2002). A Chinese-to-\nPinyin transliteration example is shown below:\nChinese Example:\n将以下句子在汉字和汉语拼音之间进行转译。\n汉字：因此，依靠科技进步，强化科学管理已成\n为实现油田稳产的当务之急\n拼音： y¯ın cˇı，y¯ı kào k¯e jì jìn bù，qiáng huà k¯e xué guˇan lˇı\nyˇı ch¯eng wéi shí xiàn yóu tián wˇen chˇan dí dàng wù zh¯ı jí\nEnglish Translation:\nTranslate the following sentence between Chinese and Pinyin.\nChinese: Therefore, relying on technological progress\nand strengthening scientific management has become an urgent\ntask to achieve stable oilfield production\nPinyin: y¯ın cˇı, y¯ı kào k¯e jì jìn bù, qiáng huà k¯e xué guˇan lˇı\nyˇı ch¯eng wéi shí xiàn yóu tián wˇen chˇan dí dàng wù zh¯ı jí\nIntent Understanding. We introduce this task\nto test whether Chinese LLMs could capture the\nwriting intent of the authors of a long document.\nThis task helps measure how well LLMs can un-\nderstand implications. We formulate this task as a\nmulti-choice problem and adopt accuracy to assess\nthe performance. An example is shown below:\nChinese Example:\n亚马孙丛林中的雄性蓝蝶带有彩虹般的蓝色光辉，半公里\n外就能看到。其光辉如此强烈，有的竟能反射70%的蓝色\n光线，远远超过蓝色涂料的反射率。蓝蝶耀眼的光辉，原\n是一种警号，使别的雄性蓝蝶在远处就能知所趋避。蓝光\n越强，示警作用越显著。物竞天择，适者生存。亿万年的\n自然选择，使亚马孙蓝蝶翅膀有了如此奇妙的性能。\n. . . . . .\n对有关蓝蝶的仿生研究，理解不恰当的一项是\nA. 在蓝蝶仿生的各类应用研究中，证券防伪的研究最有\n成效。\nB. 翅膀上的羽状物的构造和尺寸，是仿生学家们极感兴\n趣的课题。\nC. 新型的变幻色彩的迷彩服，可能将与蓝蝶翅膀的反光\n结构有关。\n199\n(a) Evaluation Results Overview\n (b) Step 1: Provide APIs for Evaluation\n(c) Step 2: Select Relevant Tasks\n (d) Step 3: Select Desired Metrics\nFigure 3: CLEVA provides a user-friendly interface. With only several clicks and minimum coding, evaluating a\nnew language model can be deployed in a few minutes.\nD. 对蓝蝶翅膀的反光机理的应用研究，目前还没取得突\n破性的结果。\n答： A\nEnglish Translation:\nMale blue butterflies in the Amazon jungle have a rainbow-like\nblue glow that can be seen from half a kilometer away. Their\nglow is so intense that some can reflect 70% of blue light, far\nexceeding the reflectivity of blue paint. The dazzling glow of\nthe blue butterfly is actually a warning signal, allowing other\nmale blue butterflies to know where to avoid from a distance.\nThe stronger the blue light, the more obvious the warning\neffect. Survival of the fittest. Millions of years of natural\nselection have given the wings of Amazon blue butterflies such\na wonderful performance.\n. . . . . .\nRegarding the bionic research on blue butterflies, one item that\nis not properly understood is\nA. Among the various applications of bionic research on blue\nbutterflies, the research on securities anti-counterfeiting is the\nmost effective.\nB. The structure and size of the feather-like structures on the\nwings are topics of great interest to bionics researchers.\nC. The new type of color-changing camouflage suit may be\nrelated to the reflective structure of the blue butterfly wings.\nD. The application research on the reflection mechanism of the\nblue butterfly wings has not achieved any breakthrough results\nso far.\nAnswer: A\nClassical Chinese Understanding. Classical\nChinese plays an important role in Chinese culture.\nQuatrain, poetry and etc. are all rooted in classi-\ncal Chinese and most of them frequently appear in\nmodern Chinese literature. Therefore we include\nthis task to examine the model’s understanding of\nclassical Chinese. We sample multi-choice ques-\ntions from CCPM (Li et al., 2021) that inquire\nabout the semantic equivalence between a modern\nChinese sentence and a list of classical Chinese\ncandidates. We use accuracy as the primary metric.\nBelow is an example:\nChinese Example:\n“山间连绵阴雨刚刚有了一点停止的意思。”这句话可以用\n以下哪句古文来表达：\nA. 寒雨初开霁\nB. 山晓雨初霁\nC. 宿雨天初霁\nD. 山雨初含霁\n答： D\nEnglish Translation:\n“The continuous rain in the mountains has just shown a little\nsign of stopping.” Which of the following ancient Chinese\nsentences can be used to express this sentence:\nA. Cold rain just stops\nB. A morning in mountains, rain just stops\nC. An over-night rain just stops\nD. Rain in the mountain is stopping\nAnswer: D\nB.1.2 Knowledge\nSubject Knowledge. This task is in the format of\nfact completion (Petroni et al., 2019), where LLMs\nfill in the blank of a Chinese sentence with entities.\nHere we construct the dataset as in Petroni et al.\n(2019), which tests the knowledge from 13 subjects\nand 1 general domain. The metric is Accuracy@K\n(K = 1, 5). We provide a math knowledge exam-\nple:\nChinese Example:\n婆罗摩笈多公式描述了__ -> 四边形\nEnglish Translation:\nThe Brahmagupta formula describes__ -> quadrilateral\n200\nCultural Knowledge. Here we query Chinese\nLLMs with multi-choice questions related to Chi-\nnese culture, e.g., idioms. Data are sampled from\nChID (Zheng et al., 2019). We adopt accuracy as\nthe primary metric and show an idiom example\nbelow:\nChinese Example:\n不过，要想变得____，要想自己能够成就一番事业的话，\n不是说来就来的，或者说任何一个出色的人，他们都得经\n历过不少的磨难，以及在经受住了一些挫折之后，才能真\n正成才成人，才能成为一个实力超群的人物，让自己的人\n生过得越来越顺当. . .\n上面这个句子下划线处可以填写哪个成语？\nA. 足智多谋\nB. 语无伦次\nC. 绣花枕头\n答： A\nEnglish Translation:\nHowever, if you want to become ____ to achieve something\non your own, it doesn’t come easily. Any outstanding person\nhas to go through a lot of hardships and setbacks before they\ncan truly succeed and become a person of exceptional ability,\nmaking their life smoother and smoother. . .\nWhich idiom can be filled in the blank in the sentence above?\nA. Wise and resourceful\nB. Speak incoherently\nC. Pretty on the outside but lacking substance underneath\nAnswer: A\nB.1.3 Reasoning\nReasoning Primitive. Following HELM (Liang\net al., 2022), reasoning primitive is a collection\nof reasoning tasks independent of language and\nknowledge background and focuses on abstracted\nreasoning capacity evaluation. It includes tasks like\nnon-ampliative reasoning, ampliative reasoning, re-\ncursive hierarchy and etc. Readers can refer to\nLiang et al. (2022) for more details. Here we syn-\nthesize the dataset similar to HELM (Liang et al.,\n2022) and use exact match to evaluate the final per-\nformance. Below is a recursive hierarchy example\n(in Dyck languages):\n[ [ [ [ [ { [ [ [ [ { { ( ( ) [ ( ( [ { } ] ) { { } } ) [ [ ] ] ( ) ] ) [ [ ( ( ) )\n( ) ] ] } } ] ] ] ] } ] ] ] ] ] [ { } ]\nRealistic Reasoning. Contrary to reasoning prim-\nitive, in-the-wild reasoning combines the abstract\nreasoning skill of LLMs and their knowledge as\nwell as the understanding of context (e.g., math-\nematical reasoning requires LLMs to be able to\nperform simple arithmetics). We choose the fol-\nlowing reasoning tasks that not only help better\nsurface the reasoning skills of LLMs but also have\npractical applications.\n• Inductive Reasoning is to draw conclusions\nby going through a set of examples. Here the\nmodel needs to infer the rule from the few-\nshot demonstrations we provided and apply\nthe rule to new examples. Data are collected\nfrom BIG-Bench (bench authors, 2023). We\nuse exact match as the evaluation metric and\nan example goes like this:\nChinese Example:\n推断符号-> 的含义并计算下列公式。\n512 + 372 -> 885\n528 + 170 -> 699\n859 + 133 -> 993\n199 + 944 -> 1144\n154 + 521 -> 676\n67 + 987 -> 1055\nEnglish Translation:\nInfer the meaning of the symbol -> and calculate the\nfollowing formula.\n512 + 372 -> 885\n528 + 170 -> 699\n859 + 133 -> 993\n199 + 944 -> 1144\n154 + 521 -> 676\n67 + 987 -> 1055\n• Deductive Reasoning is contrasted with in-\nductive reasoning, where the model pro-\ngresses from conclusions to specific exam-\nples. We provide an example of modus tol-\nlens1, a form of deductive argument, in which\nthe model predicts whether a given conclu-\nsion is valid or not according to the previous\nstatements. Data are translated from McKen-\nzie et al. (2023) and we use accuracy as the\nevaluation metric.\nChinese Example:\n考虑以下事实：\n1.如果朱莉娅喜欢甲壳虫乐队，那么朱莉娅就是吉\n他手。\n2.朱莉娅不是吉他手。\n结论：因此，朱莉娅不喜欢甲壳虫乐队。\n问题：根据陈述1.和2.，结论是否有效？\n回答： 是\nEnglish Translation:\nConsider the following facts:\n1. If Julia likes the Beatles, then Julia is a guitarist.\n2. Julia is not a guitarist.\nConclusion: Therefore, Julia does not like the Beatles.\nQuestion: Based on statements 1. and 2., is\nthe conclusion valid?\nAnswer: Yes\n• Commonsense Reasoning is an umbrella of\nall related tasks, e.g., natural language in-\nference and commonsense question answer-\ning (Storks et al., 2019). We mainly evaluate\nthe classical natural language inference (data\nare sampled from OCNLI (Hu et al., 2020))\nand commonsense question answering (data\nare translated from McKenzie et al. (2023)).\nWe organize them into multi-choice tasks and\nadopt accuracy for assessment. Here we pro-\nvide a textual entailment example:\nChinese Example:\n是否可以从“篮子嘛,一块钱,一块钱啊.”中推断\n出“这个篮子是可以卖的。”？\n1https://plato.stanford.edu/entries/\nlogic-ancient/#ForModPonModTol\n201\nA. 总是可以\nB. 有时可以\nC. 不可以\n答： A\nEnglish Translation:\nCan it be inferred from “The basket, one yuan, one\nyuan.” that “This basket is for sale.”?\nA. Always\nB. Sometimes\nC. Never\nAnswer: A\n• Mathematical Reasoning also has a rather\nbig scope that envelopes various tasks, e.g.,\nmath word problem (MWP) solving, theorem\nproving and etc. (Lu et al., 2023). Here we fo-\ncus on MWP and adoptexact match for evalua-\ntion. Data are sampled from Math23K (Wang\net al., 2017). An MWP in our benchmark is:\nChinese Example:\n问题：一个饲养场，养鸭1200只，养的鸡比养的\n鸭多(3/5)，养的鸡比鸭多多少只？\n答案： 720\nEnglish Translation:\nQuestion: A farm has 1200 ducks, and the number of\nchickens raised is (3/5) more than the number of ducks\nraised. How many more chickens are there than ducks?\nAnswer: 720\n• Code Synthesis is a task to synthesize an exe-\ncutable program that matches the requirement\nwritten in natural language. Data are trans-\nlated from HumanEval (Chen et al., 2021) and\nwe use pass@k as the metric (k = 1, 10, 100).\nAn example is shown below:\nChinese Example:\ndef is_sorted(lst):\n'''\n给定一个数字列表，返回它们是否以升序排序。\n如果列表有两个及以上的相同数字，则返回False。\n假设没有负数且只有整数。\n示例：\nis_sorted([5]) -> True\nis_sorted([1, 2, 3, 4, 5]) -> True\nis_sorted([1, 3, 2, 4, 5]) -> False\nis_sorted([1, 2, 3, 4, 5, 6]) -> True\nis_sorted([1, 2, 3, 4, 5, 6, 7]) -> True\nis_sorted([1, 3, 2, 4, 5, 6, 7]) -> False\nis_sorted([1, 2, 2, 3, 3, 4]) -> True\nis_sorted([1, 2, 2, 2, 3, 4]) -> False\n'''\ncount_digit = dict([(i, 0) for i in lst])\nfor i in lst:\ncount_digit[i]+=1\nif any(count_digit[i] > 2 for i in lst):\nreturn False\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst)))\nreturn True\nelse:\nreturn False\nEnglish Translation:\ndef is_sorted(lst):\n'''\nGiven a list of numbers, return whether they are sorted in\nascending order.\nIf the list has two or more identical numbers, return False.\nAssume that there are no negative numbers and only integers.\nExamples：\nis_sorted([5]) -> True\nis_sorted([1, 2, 3, 4, 5]) -> True\nis_sorted([1, 3, 2, 4, 5]) -> False\nis_sorted([1, 2, 3, 4, 5, 6]) -> True\nis_sorted([1, 2, 3, 4, 5, 6, 7]) -> True\nis_sorted([1, 3, 2, 4, 5, 6, 7]) -> False\nis_sorted([1, 2, 2, 3, 3, 4]) -> True\nis_sorted([1, 2, 2, 2, 3, 4]) -> False\n'''\ncount_digit = dict([(i, 0) for i in lst])\nfor i in lst:\ncount_digit[i]+=1\nif any(count_digit[i] > 2 for i in lst):\nreturn False\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst)))\nreturn True\nelse:\nreturn False\n• Conceptual Generalization is a new task that\nis similar to inductive reasoning, where the\nmodel must reason over concrete examples to\nget a general rule and apply it to unseen exam-\nples. The reason we separate this task from\ninductive reasoning is that this task is special-\nized in reasoning over physical concepts or\nrelations like directions. Data are synthesized\nas suggested by Patel and Pavlick (2022) and\nwe employ top-k accuracy (k = 1) to mea-\nsure the performance. An example is shown\nbelow:\nChinese Example:\n世界：\n[0, 1, 0, 0]\n[0, 0, 0, 0]\n答案：顶\n世界：\n[1, 0, 0]\n[0, 0, 0]\n答案：左\n世界：\n[0, 1]\n[0, 0]\n答案：上\n世界：\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[1, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n答案： 左\nEnglish Translation:\nWorld:\n[0, 1, 0, 0]\n[0, 0, 0, 0]\nAnswer: top\nWorld:\n[1, 0, 0]\n[0, 0, 0]\nAnswer: left\nWorld:\n[0, 1]\n[0, 0]\n202\nAnswer: up\nWorld:\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[1, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\nAnswer: left\nB.1.4 Harms\nCopyright. This task was initially introduced by\nHELM (Liang et al., 2022) to examine the model’s\nability to generate verbatim content and measure\nthe underlying legal risk. We similarly extract some\ninitial portion of copyrighted Chinese materials\nlike books to construct prompts and let the model\ncontinue generation from this prompt. We follow\nCarlini et al. (2021) to collect text data and code\ndata are sampled from HELM (Liang et al., 2022).\nWe use longest common sequence, edit distance\nand edit similarity normalized by prefix length as\nevaluation metrics.\nToxicity. Here we choose the toxicity detection\ntask to study the toxicity of Chinese LLMs (Borkan\net al., 2019). In this task, we present a Chinese\nsentence to the model and ask the model whether\nthe given sentence is toxic or not. We sample data\nfrom COLD (Deng et al., 2022) and choose accu-\nracy as the metric.\nBias. Similar to the toxicity part, we ask the\nmodel to determine whether a given text is bi-\nased. We sample data from CDial-Bias (Zhou et al.,\n2022), which covers four demographic categories,\nincluding race, gender, region, and occupation. Mi-\ncro F1 is the primary metric.\nDisinformation. According to HELM (Liang\net al., 2022), disinformation refers to\nfalse information that is disseminated by\nan actor with the intent to deceive, mis-\nlead, or otherwise influence the behavior\nof the target. . .\nHowever, related tasks described by Buchanan\net al. (2021) are not well-developed in the Chinese\nworld. We take a step to advance in this topic\nand focus on detecting “false information” that\nclosely resembles hallucination detection and fact\nchecking (Thorne et al., 2018; Gupta et al., 2022).\nWe present a text that may contain hallucinated\nfacts to the model and ask it whether this statement\nis true. We use accuracy as this is a classification\nproblem. Data are sampled from CHEF (Hu et al.,\n2022).\nChinese Example:\n第33届金鸡奖揭晓：黄晓明、周冬雨再拿最佳男女主角。\n上述说法是否为真？\n答： 真\nEnglish Translation:\nThe 33rd Golden Rooster Awards were announced: Huang\nXiaoming and Zhou Dongyu won the Best Actor and Actress\nagain.\nIs it True or False?\nAnswer: True\nB.1.5 Others\nMathematical Calculation. Calculation is a fun-\ndamental skill for LLMs to execute a lot of tasks,\ne.g., comparing the price of tickets. To examine\nthis skill, we provide two types of test instances\nand both of them involve basic arithmetic:\n• The first type directly queries the model with\nmathematical equations. This format is more\nlikely to test the memorization of LLMs on\narithmetic.\n11 + 32 -> 43\n• The second type expresses the equation in a\nnatural language format. This type checks\nwhether LLMs could generalize what they\nhave memorized in mathematical format to\nnatural language format.\nChinese Example:\n问：假设-48 + 62 = n。n 的值是多少？答： 14\nEnglish Translation:\nQuestion: Suppose -48 + 62 = n. What is the value of\nn? Answer: 14\nFor both types of instances, we utilize exact\nmatch to evaluate the performance. Despite the\nsecond type of instances being similar to MWPs\nin mathematical reasoning, test instances here only\nrequire the model to execute one-step arithmetic,\nwhile MWPs in mathematical reasoning are far\nmore complicated and need multi-hop reasoning.\nData are collected or translated from bench authors\n(2023); McKenzie et al. (2023).\nInstruction Following. The success of recent\nLLMs is larger attributed to instruction tuning (Wei\net al., 2022a; Ouyang et al., 2022), which un-\nlocks the great potential of large models (Fu and\nKhot, 2022). Although the extensive application of\nprompting has demonstrated the strong capability\nof LLMs on understanding human instructions, it\nis natural to ask if this is just an illusion of fre-\nquentists or if LLMs truly master this. It is thus\nimportant to evaluate LLMs on long-tailed instruc-\ntions. These instructions could be underlying bugs\n203\nof LLMs that are vulnerable to attacks and lead to\npotential risk (Zou et al., 2023). Here we translate\nsome of these instructions from McKenzie et al.\n(2023) that do not relate to common NLP tasks but\nmost LLMs perform poorly. Below is an example:\nChinese Example:\n将“+”视为数字1而不是数学运算。问：6+1的第一位数字\n是啥？答： 6\nEnglish Translation:\nConsider “+” as the number 1 instead of a mathematical\noperation. Question: What is the first digit of 6+1? Answer: 6\nWe formulate the data in this task into a multi-\nchoice problem and use accuracy for measurement.\nB.2 Application Assessment\nReading Comprehension. Reading comprehen-\nsion is a type of question-answering task, where we\npresent both the question and context to the model\nbefore it returns the answer. Our data for this task\nare sampled from C3 (Sun et al., 2019) and are of\nthe multi-choice format, therefore we use accuracy\nfor evaluation. An example is given here:\nChinese Example:\n阅读以下内容，选择合适的选项回答：\n女：听说你儿子跟你的关系不是很好?\n男：说实话我不是一个好父亲，因为忙，没时间管他，我\n们之间几乎没有沟通。因为我，他也有很大的压力。\n问题：男的和儿子的关系为什么不好?\n选项：\nA. 儿子态度不好\nB. 双方缺少交流\nC. 儿子工作很忙\nD. 父亲压力太大\n答： B\nEnglish Translation:\nRead the following content and choose the appropriate option\nto answer:\nWoman: I heard that your relationship with your son\nis not very good?\nMan: To be honest, I’m not a good father. I’m busy and\ndon’t have time to take care of him. We hardly communicate.\nBecause of me, he also has a lot of pressure.\nQuestion: Why is the relationship between the man\nand his son not good?\nOptions:\nA. The son has a bad attitude\nB. Lack of communication between the two\nC. The son is very busy with work\nD. The father is under too much pressure\nAnswer: B\nClosed-Book QA. A more challenging setting\nof question-answering is closed-book QA (Wang\net al., 2021), where the model is given no extra\ninformation and attempts to answer the question\nbased on its own knowledge. Data are sampled or\ntranslated from Duan (2018); Zhang et al. (2018);\nLin et al. (2022). An example is shown below and\nwe use exact match as the metric:\nChinese Example:\n问题：谁能描述一下氧化镁的外观？\n答案： 白色疏松粉末\nEnglish Translation:\nQuestion: Who can describe the appearance of magnesium\noxide?\nAnswer: White, loose powder.\nParaphrase Identification. In this task, a pair\nof sentences is passed to the model and the model\ndecides whether they are discussing the same thing\nor not. We formulate the sampled data from\nCLUE (Xu et al., 2020) and FewCLUE (Xu et al.,\n2021) into a binary-choice format and leverage ac-\ncuracy for assessment.\nChinese Example:\n你的火气大吗\n你火气大不大\n这两个句子表达的意思相同吗？是或否？ 是\nEnglish Translation:\nDo you have a bad temper?\nAre you quick to anger?\nDo these two sentences express the same meaning? Yes or\nNo? Yes\nSummarization. In text summarization, the\nmodel needs to abstract a long, unstructured text\nand generate a short summarization. Note that\nsome of the data-to-text generation tasks (dis-\ncussed later) also borrow the name “summariza-\ntion”. The main difference between data-to-text\ngeneration and text summarization in our bench-\nmark is whether the context is written in a pro-\ngramming language (then it is data-to-text gener-\nation) or the natural language because these two\nlanguages are distinct in nature. We sample data\nfrom CSDS (Lin et al., 2021) and useROUGE (Lin,\n2004) to evaluate the results.\nChinese Example:\n莫言获奖，围绕在莫言身边的出版商也笑开颜。北京精典\n博维文化发展有限公司拥有莫言中国内地所有作品及延伸\n品出版权。莫言获得诺贝尔文学奖，不仅会使公司业绩\n有“可观”的提升，还将加速该公司上市的进程。\nTL;DR： 诺奖花落莫言签约书商IPO提速\nEnglish Translation:\nWhen Mo Yan won the award, the publishers around him\nwere also happy. Beijing Jingdian Bowei Culture Media Co.,\nLtd. owns the publishing rights to all of Mo Yan’s work and\nderivatives in mainland China. Mo Yan’s winning of the Nobel\nPrize in Literature will not only bring a “considerable” increase\nto the company’s profit but also accelerate the process of the\ncompany’s listing.\nTL;DR: Nobel Prize goes to Mo Yan, accelerating the IPO of\nhis contracted publisher.\nData-to-Text Generation. Data-to-text genera-\ntion is of growing interest recently as people try to\nuse LLMs to assist their work, e.g., generating a re-\nport from an Excel table. This topic has long been\nexplored prior to LLMs (Puduppully et al., 2019),\nespecially under the name of summarization. We\n204\nsample data from Shao et al. (2019) and use BLEU\nfor measurement. An example of generating an\nadvertising proposal based on a structured table (in\nthe Markdown format2) is shown here:\nChinese Example:\n给定衣服的特点描述，生成相应的广告文案。\n衣服特点：\n|版型|宽松|\n|风格|休闲|\n|图案|印花|\n|图案|手绘|\n|衣样式|衬衫|\n广告文案：\n这款衬衫给人的第一印象就是风格独特，衬衫表面的\n士兵手绘图案印花精致有趣，真叫人忍不住多看几眼，\n浓浓的复古风也富于这款衬衫艺术感，就像巴黎卢浮宫\n内展示的名画一般。在款式上借鉴了睡衣版型，宽松\n舒适，休闲随性。\nEnglish Translation:\nGiven the description of the features of a clothing item, generate\na corresponding advertisement copy.\nClothing features:\n|Fit |Loose |\n|Style |Casual |\n|Pattern |Textile printing |\n|Pattern |Hand-painted |\n|Clothing type |Shirt |\nAdvertisement copy:\nThe first impression this shirt gives is its unique style. The\nsoldier pattern hand-painted on the shirt is exquisite and\ninteresting, making one can’t help but take a few more glances.\nThe strong retro style also gives this shirt an artistic sense, just\nlike the famous paintings in the Louvre in Paris. In terms of\nstyle, it fits like a pajama, which is loose and comfortable,\ncasual and natural.\nSentiment Analysis. Given a text, the model pre-\ndicts the sentiment label (“Positive”) in sentiment\nanalysis. Since it is a classification task, we use ac-\ncuracy for evaluation. Our data are collected from\nFewCLUE (Xu et al., 2021). A sentiment analysis\nexample is shown below:\nChinese Example:\n这个产品评价是正面还是负面的？\n评价：今天刚拿到手机，打电话时发现手机听筒\n有吱吱吱的杂声，不满意，真怀疑是不是正品\n答案： 负面\nEnglish Translation:\nIs this product review Positive or Negative?\nReview: Just got the phone today and found that\nthere is a squeaking noise in the earpiece when making a call.\nNot satisfied, really doubt if it is genuine.\nAnswer: Negative\nText Classification. Similar to sentiment analy-\nsis, text classification predicts the answer from a\nfixed set of labels for a given text. Instead of the bi-\nnary label in sentiment analysis, text classification\nin general has a larger label space. We collect data\nfrom FewCLUE (Xu et al., 2021) and SPR 3. We\nadopt accuracy and an example is shown below:\n2https://en.wikipedia.org/wiki/Markdown\n3https://github.com/DUTIR-Emotion-Group/\nCCL2020-Humor-Computation\nChinese Example:\n“全国青年教师教学艺术大赛举行”这段新闻的类别属\n于 教育\nEnglish Translation:\nThe category of the news “The National Young Teachers’\nTeaching Art Competition is held” is education\nOpinion Mining. Opinion mining is a large topic\nthat consists of vast tasks and has a close connec-\ntion with sentiment analysis (Zhang and Liu, 2017).\nAn exemplary task of opinion mining that we test\nhere is opinion target extraction (Liu et al., 2012).\nWe adopt exact match for evaluation in the context\nof the LLM era and show an example below:\nChinese Example:\n“《恋恋笔记本》是导演尼克·卡萨维茨2004年的一部爱情\n类影片。”中主要围绕着什么进行描述？\n恋恋笔记本\nEnglish Translation:\nWhat is the main focus of the description in “The Notebook\nis a 2004 romance film directed by Nick Cassavetes.”?\nThe Notebook\nDialogue Generation The popularity of Chat-\nGPT has shifted the interaction between humans\nand LLMs from a single-turn prompt continuation\nto a multi-turn conversation (OpenAI, 2023). It is\nthus important to evaluate LLMs in a multi-turn\nconversation setup, i.e., in the dialogue genera-\ntion task. In this task, we use data from Cross-\nWOZ (Zhu et al., 2020) and report BLEU and uni-\ngram F1. A conversation example is shown below:\nChinese Example:\n用户：你这看的什么视频？\n系统：是爱奇艺新出的《飞行少年》。\n用户：好看吗？没事我也回家看看。\n系统：挺好看的，是向祖国70周年的献礼剧。\n用户：都谁主演的啊？\n系统： 严屹宽和一些年轻演员，有闫妮，不过是客串。\nEnglish Translation:\nUser: What video are you watching?\nSystem: The Eyas, on iQIYI\nUser: Is it good? I am going to watch it at home if I have spare\ntime.\nSystem: It’s pretty good. A TV series to celebrate the 70th\nbirthday of our country.\nUser: Who is starring in it?\nSystem: Yikuan Yan and other young actors. Ni Yan also appears\nin a cameo.\nParaphrase Generation. Paraphrasing and\nrewriting is a common task in NLP. We show a\ntext to the model and the model produces new text\nthat is of the same meaning as the original text but\nof a different surface form. Following Sun and\nZhou (2012), we choose iBLEU to evaluate the\nperformance and utilize data from PKU Paraphrase\nBank (Zhang et al., 2019).\nChinese Example:\n一个句子的原句为：\n从梅森苍白的唇间吐出了几乎听不见的回答。\n205\n它可以被复述为：\n梅森先生苍白的嘴唇间溜出一个听不清楚的回答。\nEnglish Translation:\nThe original sentence is:\nA barely audible answer came from Mason’s pale lips.\nIt can be paraphrased as:\nMr. Mason’s pale mouth let out an unclear answer.\nTranslation. Machine translation is not a\nChinese-specific task but is multilingual. How-\never, the success of Chinese LLMs relies heavily on\nbilingual (Chinese and English) data (Team, 2023;\nZeng et al., 2023) and thus most Chinese LLMs\nare born to be capable of translating English text\nto and from Chinese. Our data are collected from\nthe past WMT competitions (Kocmi et al., 2022).\nWe employ SacreBLEU (Post, 2018) as the evalua-\ntion metric and an English-to-Chinese translation\nexample is shown below:\nChinese Example:\n英文：House rebukes Trump on border wall, but he plans veto\n中文： 众议院在边境墙问题上指责特朗普，但他计划使用\n一票否决权\nEnglish Translation:\nEnglish: House rebukes Trump on border wall, but he plans veto\nChinese: 众议院在边境墙问题上指责特朗普，但他计划使用\n一票否决权\nC Manual Data Collection\nWe collect data on an extensive scale, comprising\n33.98% of our entire benchmark. Besides con-\nstructing new test instances using sophisticated\nrules, manual annotation and composition serve\nas vital new data sources in many complicated\ntasks. We conducted rigorous screening, training,\nexamination, and other quality control measures\nto ensure all crowdsourced work meets our high\nstandards. In screening, we require each crowd-\nsourcing worker to have at least a bachelor’s de-\ngree in a related major, and all translators must hold\nprofessional certificates. Before the manual collec-\ntion, we prepare a detailed instruction handbook\nfor each task, equipping qualified workers with the\nnecessary knowledge and using in-domain exam-\nples to further clarify the requirements. During the\ncollection process, we addressed all questions from\ncrowdsourcing workers through an instant message\nplatform. Automatic methods, as well as ample\neye tests, were adopted both during and after the\ncollection to guarantee fine-grained quality.\nD Metrics\nD.1 Accuracy\nFor each task in our benchmark, we list and under-\nline the corresponding evaluation metrics for each\ntask in Appendix B.\nD.2 Calibration and uncertainty\nWe mainly report the values of the following met-\nrics:\n• Expected calibration error (Kumar et al.,\n2019) (ECE) measures the difference between\nthe model’s predicted probability and its exact-\nmatch accuracy.\n• Selective classification accuracy (El-Yaniv\nand Wiener, 2010) computes the accuracy for\nthe C-fraction of examples where the model\nassigns the highest probability.\nD.3 Robustness\nFollowing HELM (Liang et al., 2022), we report\nthe worst-case accuracy, which averages the poor-\nest result among transformations of each test in-\nstance. Inspired by NL-Augmentor (Dhole et al.,\n2021), we implement the transformation recipe as\nthe composition of the following perturbations:\n• Synonym perturbation randomly substi-\ntutes Chinese words with their synonyms with\na probability of 0.3.\n• Butter finger perturbationrandomly re-\nplaces Chinese words with other words that\nhave the same toneless Pinyin with a probabil-\nity of 0.05.\n• Character swapping randomly swaps any\ntwo Chinese characters with a probability of\n0.05.\nWe utilize LTP (Che et al., 2021) to perform word\nsegmentation.\nD.4 Fairness\nWe similarly adopt worst-case accuracy as in ro-\nbustness to report fairness. We support 4 transfor-\nmation recipes inspired by the perturbations from\nNL-Augmentor (Dhole et al., 2021):\n• Simplified to traditional conversion\nconverts both the prompt and references from\nChinese Simplified to Chinese Traditional4.\n• Mandarin to Cantonese conversiontrans-\nlates both the prompt and references from\n4https://github.com/BYVoid/OpenCC\n206\nMandarin to Cantonese. Here we adopt a rule-\nbased approach5 which first maps phrases in\nMandarin to their translations in Cantonese\nand then converts the resulting string from\nChinese Simplified to Chinese Traditional.\nWe are aware that this implementation has\ncertain limitations and there is ample room\nfor improvement.\n• Chinese name perturbationrandomly sub-\nstitutes all occurrences of a Chinese name\nwith another feasible Chinese name with a\nprobability of 0.5.\n• Gender term perturbation randomly flips\nall gender terms of a source gender to their\ncounterparts in a target gender with a proba-\nbility of 0.5.\nD.5 Bias and stereotypes\nWe follow metrics from HELM (Liang et al., 2022)\nto quantify bias and stereotypes:\n• Bias: we adopt the demographic representa-\ntion in HELM, which measures the uneven-\nness of gender or race terms for all social\ngroups.\n• Stereotypes: we adopt the stereotypical as-\nsociations in HELM, which computes the un-\nevenness of gender or race terms for all social\ngroups when co-occurred with an adjective or\nprofession term, then averages over all adjec-\ntive or profession terms.\nD.6 Toxicity\nWe employ the toxic fraction metric from\nHELM (Liang et al., 2022), which is the fraction\nof instances that are classified as toxic according to\nthe Perspective API (Lees et al., 2022)6. We use a\nthreshold of 0.5 to determine whether an instance\nis toxic or not.\nD.7 Efficiency\nAs stated in the main text, we focus only on in-\nference wall-clock time because limited statistics\ncould be reliably collected from users. Concretely,\nwe adopt queries per second (QPS), the amount\nof queries processed by a model API in a sec-\nond, which is a common metric for measuring the\nthroughput of online services.\n5https://justyy.com/tools/chinese-converter/\n6https://perspectiveapi.com/\nD.8 Diversity\nHere we adopt inter-distinct and intra-\ndistinct (Miller et al., 2017) to quantify\nsurface-form diversity.\n• Inter-distinct collects n-gram statistics from\nall instances in the test set and computes the n-\ngram diversity, which is the rate of all distinct\nn-grams against all n-grams.\n• Intra-distinct evaluates the n-gram diversity\nper instance and averages across all instances.\nD.9 Privacy\nWe pay close attention to current research on pri-\nvacy evaluation. For example, Carlini et al. (2021)\nutilize adversarial attacks to yield meaningful out-\ncomes. We so far focus on the detection of person-\nally identifiable information (PII) and are striving\nto involve more aspects in the near future.\nTo evaluate privacy from the PII perspective, we\ndefine PII_match, a metric similar to the toxic frac-\ntion which represents the proportion of instances\nthat contains PII:\nPII_match = 1\nN\nN∑\ni=1\nI [PII_Detect(yi) > 0]\n(1)\nwhere N is the number of test instances, yi is the\ngenerated text for i-th instance and PII_Detectis\nthe tool that returns the number of PII entities in yi.\nWe use Azure PII detection service7 to instantiate\nPII_Detect.\nE Models\nTable 1 is the summary of Chinese LLMs we eval-\nuated in our leaderboard.\nGPT (Ouyang et al., 2022; Brown et al., 2020)\nis a family of autoregressive LLMs from OpenAI.\nThe most recent and powerful GPT models are\nChatGPT8, text-davinci-0039, and GPT-4 (OpenAI,\n2023). We test all these three models in our evalua-\ntion.\nClaude (Askell et al., 2021; Bai et al., 2022b,a)\nis another family of autogressive models from\nAnthropic, which include Claude and Claude-\n7https://learn.microsoft.com/en-us/\nazure/ai-services/language-service/\npersonally-identifiable-information/overview\n8https://openai.com/blog/chatgpt\n9https://platform.openai.com/docs/models/\ngpt-3-5\n207\nModel Version Organization Access#Param. Window SizeInstructionTuningArchitecture\nChatGPT gpt-turbo-3.5(2023/07/11) OpenAI limited - 4096 ✓ GPTtext-davinci-003text-davinci-003(2023/06/17) OpenAI limited 175B 4097 ✓ GPTGPT-4 gpt-4(2023/07/11) OpenAI limited - 8192 ✓ GPT\nClaude claude-1(2023/07/07) Anthropic limited - 100000 ✓ -Claude-instantclaude–1(2023/07/21) Anthropic limited - 100000 ✓ -\nInternLM-104B (2023/07/13) Shanghai AI Lab & SenseTime limited 104B 2000 ✓ GPT\nERNIE-Bot (2023/07/09) Baidu Inc. limited - 2000 ✓ -\nChatGLM-6Bv0.1.0 Tsinghua University open6B 2048 ✓ GLMChatGLM2-6Bv1.0 Tsinghua University open6B 2048 ✓ GLMGLM-130B - Tsinghua University open 130B 2048 ✓ GLM\nBLOOMZ-7B1-mt - BigScience open 7B 2048 ✓ BLOOMBLOOM-7B1 - BigScience open 7B 2048 ✗ BLOOMBLOOMZ-176B-mt - BigScience open 176B 2048 ✓ BLOOMBLOOM-176B - BigScience open 176B 2048 ✗ BLOOM\nLLaMA-7B - Meta open 7B 2048 ✗ LLaMALLaMA-65B - Meta open 65B 2048 ✗ LLaMA\nVicuna-7B v1.1 LMSYS open 7B 2048 ✓ LLaMAVicuna-13B v1.1 LMSYS open 13B 2048 ✓ LLaMA\nBELLE BELLE-7B-2M Beike Inc. open 7B 2048 ✓ BLOOM\nChinese-Vicuna-7BChinese-Vicuna-lora-13b-belle-and-guanacoCui et al. open 7B 2048 ✓ LLaMA\nChinese-Alpaca-7BChinese-Alpaca-7B Fan et al. open 7B 2048 ✓ LLaMA\nMOSS-16B moss-moon-003-sft Fudan University open 16B 2048 ✓ CodeGen\nBaichuan-7B - Baichuan Inc. open 7B 4096 ✗ LLaMA\nTable 1: 23 Chinese LLMs evaluated in this work. For limited-accessed models, we mark the timestamp where\nwe finalized the evaluation in the format of (YYYY/MM/DD). For models with the same public name but have\ndifferent versions, we also provide the version we used to conduct the experiment. Note that the unit of window size\nof ERNIE-Bot is characters instead of tokens.\ninstant10. Both models are evaluated in our ex-\nperiments.\nInternLM (Team, 2023) is a GPT-like Chinese\nLLM trained by Shanghai AI Laboratory and Sense-\nTime. It has a limited-accessed 104B and an open-\nsource 7B version. We evaluate the 104B version\nin our experiments.\nERNIE-Bot11 is a Chinese LLM launched by\nBaidu Inc. We observe that some datasets trig-\nger the safety measure of ERNIE-Bot and obtain\ninvalid responses. This fact leads to a poor result\nin our evaluation.\nGLM (Du et al., 2022) is a Chinese LLM family\nfrom Tsinghua University trained with autoregres-\nsive blank infilling. We only assess their open-\nsource GLM-130B (Zeng et al., 2023), ChatGLM-\n6B12 and ChatGLM2-6B13.\nBLOOM (Scao et al., 2022) is a family of open-\nsource multilingual LLMs from BigScience. It\nis not fine-tuned and has an instruction-following\nversion BLOOMZ (Muennighoff et al., 2023).\nIn our experiment, we test the pretraining-only\nBLOOM-7B1 and BLOOM-176B from BLOOM,\n10https://www.anthropic.com/index/\nintroducing-claude\n11https://yiyan.baidu.com/welcome\n12https://github.com/THUDM/ChatGLM-6B\n13https://github.com/thudm/chatglm2-6b\nand the instruction-following BLOOMZ-7B1-mt\nand BLOOMZ-176B-mt from BLOOMZ.\nLLaMA (Touvron et al., 2023) is a more recently\nreleased open-source autoregressive English LLM\nfamily from Meta and is pretrained only. We ex-\nperiment with LLaMA-7B (the smallest one) and\nLLaMA-65B (the largest one).\nVicuna (Chiang et al., 2023) is a series of\ninstruction-following models built on top of\nLLaMA (Touvron et al., 2023). It comes from\nLMSYS. We evaluate both Vicuna-7B and Vicuna-\n13B.\nBELLE (Ji et al., 2023) refers to a series of\ninstruction-following models from Beike Inc., fine-\ntuned on various pretrained models like BLOOM\nand LLaMA. We assess their BLOOMZ-based 7B\nvariant.\nChinese-Vicuna (Chenghao Fan and Tian, 2023) is\na Chinese instruction-following model fine-tuned\nfrom LLaMA and has 7B and 13B two variants.\nWe experiment with the 7B version.\nChinese-Alpaca (Cui et al., 2023) is a family\nof LLaMA-based Chinese LLMs. They extend\nthe original LLaMA’s vocabulary for better Chi-\nnese modeling and open-source fine-tuned Chinese\nLLMs with various model scales. We test their\nearly 7B instruction-following model.\nMOSS (Sun et al., 2023b) is pretrained and fine-\n208\ntuned from CodeGen (Nijkamp et al., 2023) by Fu-\ndan University. It includes the pretrained model, an\ninstruction-following model, and a tool-augmented\ninstruction-following model (Schick et al., 2023).\nWe evaluate the instruction-following version14 in\nour experiment.\nBaichuan15 is a pretrained Chinese LLM from\nBaichuan Inc., with the same architecture as\nLLaMA. We test the early 7B version and a new\n13B version16 is released by the time of writing.\nF Prompting\nF.1 Settings\nThe prompt setting remains the same as the com-\nmon practice (Brown et al., 2020; Liang et al.,\n2022), where we randomly choose 5 in-context\ntraining examples (a.k.a., demonstrations) for few-\nshot prompting. To mimic true few-shot set-\nting (Perez et al., 2021), these 5 in-context training\nexamples will be fixed for all test instances. For\nclassification, we sample one example for each of\nthe 5 most frequent labels if the number of possi-\nble labels is larger than 5. If the length of 5-shot\ndemonstrations exceeds the context window size of\na model (e.g., reading comprehension), we reduce\nthe number of in-context examples.\nF.2 Format\nCompletion-style few-shot prompting. Given\nthe description of the task, sampled demonstrations,\nand a test instance, we use the below template to\nconstruct the few-shot prompt for prompting con-\nventional LLMs (a string):\n{instruction}\\n\\n{demonstration1}\\n\\n. . .\n{demonstration5}\\n\\n. . .{prompt}{prediction}\nwhere {instruction} is the task description,\n{demonstration1} is the concatenation of the\nprompt and reference of the first in-context exam-\nple, \\n is the line break and{prompt} is the prompt\nof the test instance. The model will continue the\nprompt and complete the generation in {predic-\ntion}. We denote this type of prompt template as\nCompletion. A mathematical calculation example\nis shown below (we use an English prompt template\nfor demonstration only and all prompt templates in\nour benchmark are Chinese):\n14https://huggingface.co/fnlp/\nmoss-moon-003-sft\n15https://github.com/baichuan-inc/baichuan-7B\n16https://github.com/baichuan-inc/Baichuan-13B\nCalculate the following formula.\n758 + 445 -> 1203\n758 + 445 -> 1203\n140 + 361 -> 501\nChatbot-style few-shot prompting. The popu-\nlarity of ChatGPT has led to an outbreak of LLM-\nbased chatbot (Team, 2023; Chenghao Fan and\nTian, 2023). Existing work (Huang et al., 2023)\nshows that the best few-shot prompting strategy for\nchatbots is different from the one for conventional\nLLMs. Specifically, the instruction, demonstra-\ntions, and test prompt should not be concatenated\ntogether but organized as a dialogue history, where\nthe instruction serves as the system prompt and the\nprompt and reference of a demonstration form a\ndialogue turn. The previous example will be reor-\nganized as below before feeding into the chatbot:\nSystem:\nCalculate the following formula.\nUser:\n758 + 445 ->\nAssistant:\n1203\nUser:\n163 + 140 ->\nAssistant:\n303\nUser:\n140 + 361 ->\nAssistant:\n501\nwhere System: is the field to set up the chatbot\nand we will put the instruction here. User: and\nAssistant: stand for the prompt and reference re-\nspectively. We denote this type of prompt template\nas Chatbot.\nMulti-choice problem format. As discussed in\nLiang et al. (2022), there are two strategies when\nconstructing prompts for multi-choice problems:\n• Separate (Brown et al., 2020) scores each\nchoice by concatenating it with the prompt\nand takes the one with the highest probability\nas the prediction.\n• Joint (Hendrycks et al., 2021) puts all\nchoices into the prompt and lets LLMs gen-\nerate the choice index (e.g., “{question} A.\n{choice1} B. {choice2} Answer:”).\nIn general, Separateapproach better estimates the\nmodel performance as the output space is restricted,\nwhile Joint approach is more economic since the\nmodel only needs to infer once to get the final an-\n209\nswer. We consider both types when crafting prompt\ntemplates for multi-choice problems.\nChain-of-Thought. Chain-of-Thought (Wei\net al., 2022c; Kojima et al., 2022) (CoT) is a\ncrucial technique to elicit the reasoning ability\nof LLMs. We also support CoT in CLEVA and\nprovide the corresponding prompt templates for\nthe mathematical reasoning task. An example\nof CoT is shown below, where highlighted text\nis predicted by the model and text in red is the\nintermediate reasoning process and text in green is\nthe final answer.\nQuestion: A community has 8 buildings, each\nwith 102 residents. On average, each household\npays 9 yuan per month for water. How much does\nthis community pay for water in total per month?\nAnswer: Let’s think step by step. First, each\nhousehold pays 9 yuan per month for water, and\neach building has 102 residents. Therefore, the\ntotal monthly water bill for each building is: 9 *\n102 = 918 yuan. The community has a total of 8\nbuildings, so the total monthly water bill for the\ncommunity is: 918 * 8 = 7344 yuan. Therefore,\nthe answer is 7344.\nF.3 System Design\nPrevious work (Bach et al., 2022; Ding et al.,\n2022) has presented various approaches to design\na prompting language that enables flexible prompt\nconstruction. However, their methods have shown\nlimitations in handling the vast variety of tasks for\nlarge-scale evaluation: Their prompting languages\ncan only manipulate strings, while many tasks are\norganized in a structured data format, e.g., dialogue\ngeneration and data-to-text generation.\nWe therefore devise a slightly complicated\nprompting language that accommodates cus-\ntomized prompt construction of structured data\nwhile preserving usability. We start the description\nwith an instance in JSON format from the dialogue\ngeneration task:\n{\n\"history\": [\n{\n\"utterance\": \"Who is the US president?\",\n\"role\": \"usr\"\n},\n{\n\"utterance\": \"Joe Biden.\",\n\"role\": \"sys\"\n},\n{\n\"utterance\": \"Then who is his wife?\",\n\"role\": \"usr\"\n}\n],\n\"role\": \"sys\",\n\"label\": [\n\"Jill Biden.\"\n]\n}\nand a prompt template example written as a JSON\ndictionary (\\n is the line breaking):\n{\n\"verbalizer\": {\n\"role\": {\n\"sys\": \"Assistant\",\n\"usr\": \"User\"\n}\n},\n\"history\": {\n\"item_separator\": \"\\n\",\n\"item_template\": \"{role}: {utterance}\",\n\"item_index\": null\n},\n\"input\": \"{history}\\n{role}:\",\n\"label\": \" {label}\"\n}\nThe general pipeline of our prompt construction\nis as follows (we mark the field from the instance\nin green and the one from the prompt template in\nblue):\n1. We first map values of all fields in a test in-\nstance according to user-defined mappings in\nverbalizer(Gao et al., 2021b). In our exam-\nple, all “usr” and “sys” will be replaced with\n“User” and “Assistant” respectively.\n2. We then stringify each field in the test instance.\nWe organize all structured data fields in the\nformat of a list of dictionaries ( history in\nour example) and apply the following rules to\nprocess them:\n(a) For each entry (a dictionary), we in-\ndependently stringify it by composing\nall its fields via a template defined in\nthe Python f-String syntax 17. For in-\nstance, an utterance in the dialogue his-\ntory “Who is the US president?” from the\nspeaker “User” will be formatted into\n“User: Who is the US president? ” ac-\ncording to item_template in a prompt\ntemplate field that shares the same name\nas history.\n(b) We then index all stringified entries\n(by prepending an index like “A. ”\nto each entry) if needed and concate-\nnate them with a user-defined separator\nitem_separator to stringify the whole\ndata structure. In our case, we do not\napply any indexing (an empty option in\nitem_index) and directly assemble the\nfinal string of history with \\n:.\nUser: Who is the US president?\nAssistant: Joe Biden.\nUser: Then who is his wife?\n3. We finally construct the prompt and references\n17https://peps.python.org/pep-0498/\n210\nfrom all stringified fields. According toinput,\nthe resulting prompt in our example will be:\nUser: Who is the US president?\nAssistant: Joe Biden.\nUser: Then who is his wife?\nAssistant:\nFor the references, we directly apply labelin\nthe prompt template to every entry in label,\nresulting in “ Jill Biden” here.\nThough not shown in the example above, another\ncrucial part is to attach specific post-processing\nsteps tailored to a prompt template. For example, if\nwe index the choices in an instance from a multiple-\nchoice task by capital letters like “A. ”, we should\ncapitalize the initial output letter for a more accu-\nrate evaluation. In our system, we achieve this by\npassing a list of predefined options to the subfield\npostprocess in the prompt template field meta,\nwhich executes the script of each post-processing\noption on the output consecutively.\nG Results\nIn this section, we provide the complete evaluation\nresults and breakdown analysis of our benchmark.\nG.1 Meta Analysis\nTo validate the uniqueness and reasonability of di-\nversity and privacy, we examine the correlation\nbetween accuracy and these two newly introduced\nmetrics. Figure 4 shows the scatter plot. We can see\nthat there is a weak positive correlation between\naccuracy and diversity, justified by a value of 0.30\nin Pearson’s r (P-value is 9.9 ×10−9). This phe-\nnomenon suggests that a strong Chinese LLM is\nlikely to be able to produce diverse text. On the\nother hand, privacy seems to have no strong correla-\ntion to accuracy, with a value of -0.10 in Pearson’s\nr (P-value is 0.05). These weak correlations indi-\ncate the uniqueness of privacy and diversity as they\ncan not be easily encompassed by a single accuracy\nmetric.\nG.2 Ability Evaluation\nIn this section, we focus on the analysis of ability\nevaluation. Given that there are too many models\nfor comparison, we select several interested groups\nof models in the visualization. Figure 5 compares\n4 groups of models, each group consisting of two\ncategories with three top-performing models. We\nhave the following observations:\n• Although outstanding Chinese model like\nInternLM-104B is comparable and even out-\nperforms the best English models in some\ntasks, most high-ranking models in our Chi-\nnese benchmark are English models.\n• The gap between limited-accessed and open-\nsource models (Liang et al., 2022) is also\nwitnessed in Chinese LLMs. We believe\nthis gap could be narrowed down by fine-\ntuning a large-scale (with 100B and more\nparameters) Chinese LLM with the most re-\ncent instruction tuning strategies. Figure 2\nshows that the well-performing open-source\nmodels are small models fine-tuned by the\nmost recent and advanced techniques like Self-\nInstruct (Wang et al., 2023). These models\nmainly lag behind the limited-accessed model\nin many reasoning and knowledge-intensive\ntasks as shown in Figure 5, which could be\naddressed by scaling up the model size (Liang\net al., 2022; Fu and Khot, 2022).\n• Aligned with Liang et al. (2022); Fu and Khot\n(2022), large LLMs show clear advantages\nover the small ones in many reasoning and\nknowledge-intensive tasks.\n• Instructing tuning is indeed a crucial tech-\nnique to unleash the full potential of\nLLMs (Fu and Khot, 2022). Some small\ninstruction-following models are even more\npowerful than those without instruction-\ntuning. For example, InternLM-104B is\nmuch better than BLOOM-176B. In addition,\ninstruction-following models are generally\nless sensitive to the choice of prompt tem-\nplates (with a smaller area around each point),\nsuggesting that instruction tuning improves\nthe model’s robustness to prompt templates.\nMoreover, we also observe some interesting phe-\nnomena in Figure 5: Inverse scaling (McKenzie\net al., 2023) seems to appear in our instruction\nfollowing task, where the larger GPT-4, InternLM-\n104B, and LLaMA-65B is worse than MOSS-16B.\nAccording to our marking of tasks with a great stan-\ndard deviation in Figure 5, they all are theemergent\nability (Wei et al., 2022b) candidate in the Chinese\nworld, e.g., mathematical reasoning, code synthe-\nsis, Pinyin transliteration and etc. We are aware\nthat the analysis here is not a rigorous study that\nverified the existence of inverse scaling and emer-\ngent ability in certain Chinese tasks and we leave\nit for future work. In the end, we find some tasks\n211\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001c\n/uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000013/uni00000011/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000016\n/uni00000013/uni00000011/uni00000013/uni00000019\n/uni00000013/uni00000011/uni00000013/uni0000001b\n/uni00000013/uni00000011/uni00000014/uni00000014\n/uni00000013/uni00000011/uni00000014/uni00000017\n/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000033/uni0000004c/uni00000051/uni0000005c/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000045/uni0000004d/uni00000048/uni00000046/uni00000057/uni00000003/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048\n/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000050/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048\n/uni0000002c/uni00000051/uni00000047/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000051/uni00000046/uni00000048/uni00000053/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni00000036/uni0000005c/uni00000051/uni00000057/uni0000004b/uni00000048/uni00000056/uni0000004c/uni00000056\n/uni00000026/uni00000052/uni00000053/uni0000005c/uni00000055/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000026/uni00000044/uni0000004f/uni00000046/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\nFigure 4: Correlation between diversity or privacy and accuracy on all tasks in a scatter plot format. Each point is a\nmodel’s performance of diversity/privacy and accuracy on a specific task.\n(e.g., inductive reasoning) that are difficult even for\nthe most powerful GPT-4, indicating an unresolved\nproblem that we could work on in the future.\nWe analyze the knowledge of different Chinese\nLLMs in Figure 6 by utilizing questions from 14\nsubjects. We see that large models outperform\nsmall models in this knowledge-intensive task on\nmany subjects, e.g., GPT-4, Claude, and InternLM-\n104B are much better than MOSS-16B and Vicuna-\n13B. Notably, Baichuan-7B possesses a high quan-\ntity of knowledge and is comparable to large mod-\nels. This fact explains why it performs so well in\nknowledge-intensive tasks like classical Chinese\nunderstanding, commonsense reasoning and etc.,\nas shown in Figure 5.\nWe also empirically examine the rationality of\nthe design and structure of our ability evaluation by\ncomputing the correlation between any pair of tasks\nand manually checking with the human prior. As\nshown in Figure 7, most pairs of tasks that both not\nbelonging to the same aspect (e.g., knowledge) do\nnot share a statistically significant correlation, e.g.,\nconceptual generalization and cultural knowledge.\nSome statistically significant correlations are well-\nmatch with our expectations (not exhausted):\n• A good performance on coreference resolution\nand cultural knowledge helps to identify toxic\nand biased content (Pearson’s r >0.6);\n• Commonsense reasoning ability is also re-\nquired for toxicity and bias as this harmful\ncontent could be implicit (Pearson’s r >0.5);\n• There is a strong positive correlation among al-\nmost all reasoning tasks (Pearson’s r >0.5);\n• More subject knowledge improves conceptual\ngeneralization and commonsense reasoning\n(Pearson’s r ≥0.6);\n• More cultural knowledge yields a better result\nin classical Chinese understanding (Pearson’s\nr = 0.85);\n• Mathematical calculation is almost mandatory\nfor mathematical reasoning (Pearson’s r ≈\n0.8);\nThese observations in general justify the rationality\nof our taxonomy.\nIn addition, we observe some interesting phe-\nnomena. Reasoning primitive has a strong positive\ncorrelation with Pinyin transliteration (Pearson’s\nr ≈0.9). This indicates that some sort of reasoning\nis required for Pinyin transliteration. For example,\na valid Pinyin sequence matches the appearance of\neach character and its Pinyin precisely. The model\nneeds to follow this rule to predict correctly. How-\never, there are also some counter-intuition observa-\ntions that could not be explained easily: A strong\npositive correlation (Pearson’s r = 0.76) between\nreasoning primitive and classical Chinese under-\nstanding reveals the distinct mechanism beneath\nLLMs and the human brain.\nG.3 Application Assessment\nFigure 8 compares the performance of models in ap-\nplication assessment tasks. The conclusions are in\nline with those in Figure 2: Most high-ranked mod-\nels are English models and are limited-accessed.\nInterestingly, we see that English models tend to\nhave fewer “weak spots”, a task that the model per-\nforms poorly compared to other models. It could\nbe the fact that we choose more Chinese models\nthat span a wide quality range, while English mod-\nels are mainly the famous ones with the guarantee\nin quality. We observe that English open-source\n212\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000028/uni00000051/uni0000004a/uni0000004f/uni0000004c/uni00000056/uni0000004b/uni00000003/uni00000059/uni00000056/uni00000003/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni0000002f/uni0000004c/uni00000050/uni0000004c/uni00000057/uni00000048/uni00000047/uni00000003/uni00000059/uni00000056/uni00000003/uni00000032/uni00000053/uni00000048/uni00000051\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni0000002f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni00000059/uni00000056/uni00000003/uni00000036/uni00000050/uni00000044/uni0000004f/uni0000004f\n/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000047/uni00000044/uni00000059/uni0000004c/uni00000051/uni00000046/uni0000004c/uni00000010/uni00000013/uni00000013/uni00000016\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000027/uni0000004c/uni00000056/uni0000004c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000045/uni0000004d/uni00000048/uni00000046/uni00000057/uni00000003/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048\n/uni00000025/uni0000004c/uni00000044/uni00000056\n/uni0000002c/uni00000051/uni00000047/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000037/uni00000052/uni0000005b/uni0000004c/uni00000046/uni0000004c/uni00000057/uni0000005c\n/uni00000026/uni00000052/uni00000055/uni00000048/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000035/uni00000048/uni00000056/uni00000052/uni0000004f/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000056/uni00000048/uni00000051/uni00000056/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000003/uni00000038/uni00000051/uni00000047/uni00000048/uni00000055/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000057/uni00000003/uni00000038/uni00000051/uni00000047/uni00000048/uni00000055/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000050/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000029/uni00000052/uni0000004f/uni0000004f/uni00000052/uni0000005a/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000051/uni00000046/uni00000048/uni00000053/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni00000058/uni0000004f/uni00000057/uni00000058/uni00000055/uni00000044/uni0000004f/uni00000003/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000026/uni00000044/uni0000004f/uni00000046/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000033/uni0000004c/uni00000051/uni0000005c/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni00000052/uni00000053/uni0000005c/uni00000055/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni00000027/uni00000048/uni00000047/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni00000036/uni0000005c/uni00000051/uni00000057/uni0000004b/uni00000048/uni00000056/uni0000004c/uni00000056\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000037/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000059/uni00000056/uni00000003/uni00000033/uni00000055/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000048/uni00000047\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025\nFigure 5: Comparison between three best-performing models from two categories on all ability evaluation tasks.\nModels in the left legend column belong to the first category and those in the right belong to the second category. For\nexample, GPT-4, Claude and LLaMA-65B are English models. There are 8 categories: Chinese are Chinese-focused\nmodels (with tailored strategies to improve Chinese modeling), English are English-focused models, Open are\nopen-source models, Limited are limited-accessed models, Large are models with more than 50B parameters (We\nchoose text-davinci-003 rather than GPT-4 and ChatGPT as its size has been reported), Small are models with\nfewer than 50B parameters, Tuned are instruction-following models and Pretrained are pretrained models (without\ninstruction tuning). Each point represents the mean performance of the model on a specific task and the area around\neach point is of the size of ±standard deviation. We rank tasks in the x-axis by the standard deviation and the task\nwith a larger standard deviation is closer to the right. We mark tasks with a standard deviation larger than 0.1 by\ngray shadow. These tasks imply the plausible emergent abilities of Chinese LLMs. Note that we normalize the\nscore in the copyright task across models and then subtract it from 1 to convert it to a metric whose value is larger\nimplying a better result.\nmodels do not work well on translation and text\nclassification.\nWe show the distribution of different metrics at\ndifferent tasks in Figure 9.\n• Accuracy. Multi-choice tasks like reading\ncomprehension, text classification, and senti-\nment analysis have a high accuracy mean but\nmodels are clearly differentiated. On the other\nhand, generation tasks have a low median and\nmost models are close to each other in general.\n• Efficiency. There is a large difference in effi-\nciency among models. This is because there\nexist many unfair comparisons. For example,\nlimited-accessed models do not provide de-\ntails on how many resources they invest when\nserving each query.\n• Robustness & Fairness. For robustness and\nfairness, they have a similar trend as accuracy\nbut with a relatively lower value, probably\nbecause they share the same base metric on\n213\n/uni00000024/uni00000055/uni00000057\n/uni00000025/uni0000004c/uni00000052/uni00000050/uni00000048/uni00000047/uni0000004c/uni00000046/uni0000004c/uni00000051/uni00000048/uni00000026/uni0000004b/uni00000048/uni00000050/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000005c\n/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000048/uni00000055/uni00000003/uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000028/uni00000046/uni00000052/uni00000051/uni00000052/uni00000050/uni0000004c/uni00000046/uni00000056/uni0000002a/uni00000048/uni00000052/uni0000004a/uni00000055/uni00000044/uni00000053/uni0000004b/uni0000005c\n/uni0000002b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni0000002f/uni00000044/uni0000005a\n/uni0000002f/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000056/uni00000033/uni0000004b/uni0000004c/uni0000004f/uni00000052/uni00000056/uni00000052/uni00000053/uni0000004b/uni0000005c/uni00000033/uni0000004b/uni0000005c/uni00000056/uni0000004c/uni00000046/uni00000056/uni00000033/uni00000052/uni0000004f/uni0000004c/uni00000057/uni0000004c/uni00000046/uni00000056/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000030/uni00000044/uni00000057/uni00000046/uni0000004b\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000028/uni00000035/uni00000031/uni0000002c/uni00000028/uni00000025/uni00000052/uni00000057\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025\nFigure 6: The performance of models on 14 subjects in the subject knowledge task. We select the best-performing\nmodels from top-10 institutions according to accuracy.\n/uni0000002f/uni00000044/uni00000051/uni0000004a/uni00000058/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000055/uni00000048/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000035/uni00000048/uni00000056/uni00000052/uni0000004f/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni0000004c/uni00000051/uni0000005c/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000003/uni00000038/uni00000051/uni00000047/uni00000048/uni00000055/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000057/uni00000003/uni00000038/uni00000051/uni00000047/uni00000048/uni00000055/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000036/uni00000058/uni00000045/uni0000004d/uni00000048/uni00000046/uni00000057/uni00000003/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048/uni00000026/uni00000058/uni0000004f/uni00000057/uni00000058/uni00000055/uni00000044/uni0000004f/uni00000003/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000050/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048/uni0000002c/uni00000051/uni00000047/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000051/uni00000046/uni00000048/uni00000053/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000056/uni00000048/uni00000051/uni00000056/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni00000036/uni0000005c/uni00000051/uni00000057/uni0000004b/uni00000048/uni00000056/uni0000004c/uni00000056\n/uni00000027/uni00000048/uni00000047/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000053/uni0000005c/uni00000055/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000037/uni00000052/uni0000005b/uni0000004c/uni00000046/uni0000004c/uni00000057/uni0000005c/uni00000025/uni0000004c/uni00000044/uni00000056\n/uni00000027/uni0000004c/uni00000056/uni0000004c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000026/uni00000044/uni0000004f/uni00000046/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni00000052/uni00000055/uni00000048/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000035/uni00000048/uni00000056/uni00000052/uni0000004f/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000033/uni0000004c/uni00000051/uni0000005c/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000003/uni00000038/uni00000051/uni00000047/uni00000048/uni00000055/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000057/uni00000003/uni00000038/uni00000051/uni00000047/uni00000048/uni00000055/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a\n/uni00000036/uni00000058/uni00000045/uni0000004d/uni00000048/uni00000046/uni00000057/uni00000003/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048\n/uni00000026/uni00000058/uni0000004f/uni00000057/uni00000058/uni00000055/uni00000044/uni0000004f/uni00000003/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048\n/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000050/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048\n/uni0000002c/uni00000051/uni00000047/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000051/uni00000046/uni00000048/uni00000053/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000056/uni00000048/uni00000051/uni00000056/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni00000036/uni0000005c/uni00000051/uni00000057/uni0000004b/uni00000048/uni00000056/uni0000004c/uni00000056\n/uni00000027/uni00000048/uni00000047/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000053/uni0000005c/uni00000055/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni00000037/uni00000052/uni0000005b/uni0000004c/uni00000046/uni0000004c/uni00000057/uni0000005c\n/uni00000025/uni0000004c/uni00000044/uni00000056\n/uni00000027/uni0000004c/uni00000056/uni0000004c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000048/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000026/uni00000044/uni0000004f/uni00000046/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000029/uni00000052/uni0000004f/uni0000004f/uni00000052/uni0000005a/uni0000004c/uni00000051/uni0000004a\n/uni00000015/uni00000011/uni0000001c/uni0000001b\n/uni00000010/uni00000019/uni00000011/uni00000018/uni00000015/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000015*\n/uni00000010/uni00000014/uni0000001c/uni00000011/uni00000018/uni0000001c/uni0000001b/uni00000013/uni00000011/uni00000019/uni0000001a* /uni00000019/uni00000016/uni00000011/uni0000001c/uni00000019*\n/uni00000010/uni0000001c/uni00000011/uni00000018/uni00000013/uni00000018/uni00000019/uni00000011/uni00000018/uni00000015* /uni00000019/uni0000001b/uni00000011/uni00000018/uni00000015* /uni00000018/uni00000014/uni00000011/uni0000001b/uni0000001b*\n/uni00000010/uni00000016/uni0000001b/uni00000011/uni00000013/uni00000013/uni00000017/uni0000001c/uni00000011/uni00000018/uni0000001b* /uni00000019/uni0000001a/uni00000011/uni00000018/uni00000018* /uni00000018/uni0000001c/uni00000011/uni00000018/uni00000014* /uni00000018/uni00000015/uni00000011/uni0000001a/uni0000001a*\n/uni00000010/uni00000014/uni00000015/uni00000011/uni0000001b/uni00000014/uni0000001b/uni00000018/uni00000011/uni00000018/uni00000016* /uni00000019/uni00000014/uni00000011/uni00000015/uni00000017* /uni0000001b/uni00000017/uni00000011/uni00000018/uni00000016* /uni00000019/uni00000018/uni00000011/uni00000018/uni0000001b* /uni00000019/uni00000014/uni00000011/uni00000019/uni00000016*\n/uni00000010/uni0000001c/uni00000011/uni0000001b/uni00000013/uni0000001b/uni00000017/uni00000011/uni0000001b/uni00000018* /uni0000001b/uni0000001b/uni00000011/uni0000001a/uni0000001c* /uni0000001a/uni00000019/uni00000011/uni00000016/uni00000018* /uni0000001a/uni00000018/uni00000011/uni0000001c/uni0000001a* /uni00000018/uni00000019/uni00000011/uni0000001b/uni0000001c* /uni0000001a/uni00000015/uni00000011/uni0000001b/uni00000018*\n/uni00000017/uni00000011/uni0000001c/uni0000001c/uni0000001a/uni00000017/uni00000011/uni0000001a/uni00000018* /uni00000019/uni00000014/uni00000011/uni00000017/uni00000015* /uni00000019/uni0000001a/uni00000011/uni00000017/uni0000001c* /uni00000016/uni0000001a/uni00000011/uni0000001c/uni00000019/uni00000018/uni00000019/uni00000011/uni0000001c/uni0000001c* /uni00000019/uni00000015/uni00000011/uni0000001c/uni0000001a* /uni00000019/uni00000014/uni00000011/uni00000019/uni00000018*\n/uni00000010/uni00000015/uni00000014/uni00000011/uni0000001a/uni00000013/uni00000015/uni00000018/uni00000011/uni00000017/uni00000016/uni0000001a/uni00000014/uni00000011/uni00000017/uni0000001b* /uni00000016/uni00000013/uni00000011/uni00000013/uni00000018/uni00000017/uni00000013/uni00000011/uni00000017/uni0000001c/uni00000019/uni0000001a/uni00000011/uni0000001c/uni00000015* /uni00000015/uni00000017/uni00000011/uni0000001b/uni0000001c/uni00000017/uni0000001a/uni00000011/uni0000001a/uni00000018* /uni00000015/uni00000019/uni00000011/uni0000001b/uni00000013\n/uni00000010/uni00000014/uni0000001c/uni00000011/uni0000001a/uni00000018/uni00000018/uni0000001a/uni00000011/uni0000001b/uni00000013* /uni00000019/uni00000016/uni00000011/uni0000001a/uni0000001b* /uni00000019/uni00000013/uni00000011/uni00000015/uni00000016* /uni00000019/uni00000019/uni00000011/uni00000019/uni0000001b* /uni00000018/uni0000001c/uni00000011/uni00000016/uni0000001a* /uni00000018/uni00000018/uni00000011/uni0000001a/uni00000016* /uni00000019/uni00000016/uni00000011/uni0000001c/uni0000001b* /uni00000018/uni0000001c/uni00000011/uni00000015/uni00000019* /uni00000018/uni00000014/uni00000011/uni00000014/uni0000001b*\n/uni00000010/uni00000013/uni00000011/uni00000013/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001c/uni0000001a* /uni00000017/uni0000001c/uni00000011/uni00000017/uni00000018* /uni0000001a/uni00000018/uni00000011/uni0000001a/uni00000016* /uni00000019/uni00000016/uni00000011/uni00000018/uni00000018* /uni00000017/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000001b/uni00000016/uni00000011/uni0000001c/uni0000001c* /uni00000019/uni0000001c/uni00000011/uni0000001c/uni00000015* /uni00000018/uni00000018/uni00000011/uni00000016/uni00000015* /uni00000014/uni00000018/uni00000011/uni00000018/uni00000014/uni00000017/uni00000014/uni00000011/uni0000001a/uni00000016*\n/uni00000016/uni00000011/uni00000017/uni00000019/uni0000001b/uni00000017/uni00000011/uni0000001b/uni00000016* /uni00000019/uni00000013/uni00000011/uni00000016/uni00000017* /uni0000001a/uni00000016/uni00000011/uni00000014/uni00000013* /uni00000019/uni0000001b/uni00000011/uni00000017/uni0000001b* /uni00000016/uni00000015/uni00000011/uni00000015/uni00000015/uni0000001b/uni00000013/uni00000011/uni00000014/uni0000001b* /uni0000001b/uni00000014/uni00000011/uni00000019/uni0000001b* /uni00000017/uni0000001b/uni00000011/uni00000018/uni00000016* /uni00000014/uni0000001b/uni00000011/uni00000019/uni0000001a/uni00000017/uni0000001c/uni00000011/uni00000017/uni00000016* /uni0000001b/uni0000001a/uni00000011/uni00000017/uni0000001a*\n/uni00000014/uni0000001a/uni00000011/uni00000017/uni00000016/uni00000010/uni00000014/uni00000018/uni00000011/uni00000016/uni00000013/uni00000010/uni00000014/uni00000014/uni00000011/uni0000001c/uni00000016/uni00000010/uni00000014/uni00000018/uni00000011/uni00000016/uni0000001c/uni00000010/uni00000016/uni00000013/uni00000011/uni00000016/uni00000015/uni00000010/uni00000019/uni00000011/uni00000016/uni00000016/uni00000010/uni00000015/uni0000001b/uni00000011/uni00000013/uni00000015/uni00000010/uni00000015/uni00000014/uni00000011/uni00000013/uni00000016/uni00000016/uni00000011/uni0000001c/uni00000014/uni00000014/uni00000019/uni00000011/uni00000013/uni00000018/uni00000015/uni00000014/uni00000011/uni0000001a/uni00000014/uni00000010/uni00000016/uni00000014/uni00000011/uni00000014/uni0000001b/uni00000010/uni00000015/uni00000016/uni00000011/uni00000019/uni00000018\n/uni0000001c/uni00000011/uni00000015/uni0000001a/uni00000010/uni00000014/uni00000013/uni00000011/uni00000018/uni00000017/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000010/uni00000014/uni00000013/uni00000011/uni00000018/uni00000019/uni00000010/uni00000014/uni00000013/uni00000011/uni00000015/uni00000018/uni00000016/uni00000015/uni00000011/uni00000013/uni00000019/uni00000010/uni00000015/uni00000011/uni00000018/uni00000018/uni00000010/uni0000001c/uni00000011/uni00000017/uni00000018/uni00000015/uni00000011/uni0000001b/uni00000016/uni00000014/uni00000016/uni00000011/uni00000015/uni00000018/uni00000010/uni00000015/uni00000011/uni00000019/uni0000001c/uni00000010/uni00000016/uni00000015/uni00000011/uni00000019/uni00000016/uni00000010/uni00000016/uni00000017/uni00000011/uni00000015/uni00000015/uni00000016/uni00000011/uni00000018/uni00000017\n/uni00000010/uni00000018/uni00000011/uni0000001c/uni00000017/uni0000001b/uni00000019/uni00000011/uni0000001c/uni0000001a* /uni00000019/uni0000001b/uni00000011/uni00000016/uni00000015* /uni0000001a/uni00000017/uni00000011/uni0000001a/uni00000013* /uni00000019/uni00000018/uni00000011/uni00000016/uni00000014* /uni00000018/uni0000001c/uni00000011/uni00000015/uni00000016* /uni0000001b/uni0000001c/uni00000011/uni0000001a/uni0000001b* /uni0000001b/uni00000013/uni00000011/uni00000017/uni00000016* /uni00000019/uni00000016/uni00000011/uni00000014/uni00000018* /uni00000015/uni00000019/uni00000011/uni00000016/uni0000001b/uni00000019/uni00000017/uni00000011/uni00000018/uni0000001b* /uni0000001a/uni00000017/uni00000011/uni00000015/uni00000018* /uni0000001a/uni00000019/uni00000011/uni0000001a/uni00000019* /uni00000010/uni00000015/uni00000015/uni00000011/uni0000001c/uni00000013/uni00000017/uni00000011/uni00000018/uni00000013\n/uni00000010/uni0000001a/uni00000011/uni0000001c/uni00000014/uni00000019/uni00000018/uni00000011/uni0000001a/uni0000001b* /uni00000019/uni00000019/uni00000011/uni00000014/uni0000001a* /uni00000019/uni0000001a/uni00000011/uni00000018/uni00000018* /uni00000017/uni0000001b/uni00000011/uni00000019/uni00000013* /uni00000019/uni00000018/uni00000011/uni00000015/uni0000001b* /uni00000019/uni00000017/uni00000011/uni00000013/uni00000013* /uni00000019/uni00000019/uni00000011/uni0000001c/uni00000017* /uni00000018/uni00000016/uni00000011/uni0000001c/uni00000015* /uni00000017/uni0000001b/uni00000011/uni00000013/uni00000013* /uni00000018/uni00000013/uni00000011/uni00000016/uni0000001b* /uni00000019/uni00000018/uni00000011/uni0000001c/uni0000001b* /uni00000019/uni0000001b/uni00000011/uni00000013/uni0000001b* /uni00000010/uni00000019/uni00000011/uni0000001b/uni00000017/uni00000010/uni00000015/uni00000016/uni00000011/uni00000019/uni00000015/uni00000019/uni00000014/uni00000011/uni00000019/uni0000001a*\n/uni00000010/uni00000017/uni00000011/uni00000017/uni00000014/uni00000018/uni0000001b/uni00000011/uni00000018/uni0000001b* /uni00000017/uni0000001b/uni00000011/uni00000015/uni0000001a* /uni00000019/uni0000001b/uni00000011/uni0000001c/uni0000001a* /uni00000015/uni00000018/uni00000011/uni00000019/uni00000015/uni00000017/uni0000001a/uni00000011/uni00000016/uni0000001c* /uni00000019/uni00000013/uni00000011/uni00000017/uni00000013* /uni00000018/uni00000013/uni00000011/uni00000013/uni00000014* /uni00000016/uni00000019/uni00000011/uni0000001a/uni00000014/uni00000017/uni00000014/uni00000011/uni00000017/uni00000018* /uni00000016/uni0000001c/uni00000011/uni00000013/uni00000017/uni00000017/uni0000001c/uni00000011/uni00000013/uni0000001a* /uni00000018/uni00000015/uni00000011/uni00000017/uni00000013* /uni00000015/uni00000013/uni00000011/uni00000019/uni0000001a/uni00000013/uni00000011/uni0000001a/uni00000019/uni00000017/uni0000001c/uni00000011/uni0000001a/uni00000013* /uni00000018/uni00000017/uni00000011/uni00000017/uni00000019*\n/uni00000014/uni0000001c/uni00000011/uni00000015/uni0000001b/uni0000001a/uni0000001c/uni00000011/uni00000019/uni0000001a* /uni0000001a/uni00000015/uni00000011/uni00000014/uni00000019* /uni0000001a/uni00000016/uni00000011/uni00000016/uni00000015* /uni00000018/uni00000014/uni00000011/uni0000001a/uni00000019* /uni00000016/uni0000001a/uni00000011/uni00000016/uni00000015/uni00000019/uni00000018/uni00000011/uni00000016/uni00000014* /uni0000001a/uni0000001b/uni00000011/uni0000001a/uni0000001c* /uni00000018/uni0000001c/uni00000011/uni00000016/uni00000015* /uni00000016/uni00000016/uni00000011/uni0000001c/uni00000018/uni00000016/uni00000018/uni00000011/uni00000014/uni00000015/uni0000001a/uni0000001b/uni00000011/uni00000019/uni00000016* /uni0000001a/uni0000001a/uni00000011/uni00000013/uni00000017* /uni00000010/uni00000015/uni00000014/uni00000011/uni0000001a/uni0000001a/uni00000010/uni00000015/uni00000015/uni00000011/uni00000016/uni00000015/uni00000018/uni0000001c/uni00000011/uni00000016/uni00000019* /uni00000019/uni00000019/uni00000011/uni00000017/uni00000016* /uni00000018/uni00000014/uni00000011/uni00000019/uni00000019*\n/uni00000014/uni00000015/uni00000011/uni0000001a/uni00000014/uni00000015/uni00000014/uni00000011/uni00000014/uni00000018/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000019/uni00000014/uni00000013/uni00000011/uni00000017/uni00000018/uni00000015/uni00000014/uni00000011/uni00000018/uni00000016/uni00000015/uni00000011/uni00000013/uni00000013/uni00000014/uni00000018/uni00000011/uni00000017/uni00000014/uni0000001a/uni00000011/uni0000001c/uni0000001b/uni00000016/uni0000001b/uni00000011/uni0000001b/uni00000016/uni00000014/uni00000015/uni00000011/uni00000017/uni00000013/uni00000018/uni00000017/uni00000011/uni00000014/uni00000018* /uni00000018/uni00000011/uni0000001c/uni00000018/uni00000015/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000016/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000010/uni00000015/uni00000017/uni00000011/uni00000018/uni00000018/uni00000014/uni00000018/uni00000011/uni00000017/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013/uni00000018/uni00000014/uni0000001c/uni00000011/uni00000016/uni0000001b/uni00000014/uni00000011/uni00000018/uni00000019\n/uni00000010/uni00000015/uni00000013\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\nFigure 7: Correlation between different tasks in ability evaluation. Each entry is Pearson’s r between two tasks\nfrom the corresponding row and column. ∗denotes that the correlation coefficient is statistically significant with a\nP-value lower than 0.05.\naugmented data. We observe that some tasks\nare more sensitive to noise, e.g., sentiment\nanalysis and opinion mining.\n• Calibration. We compare the values on\nECE-10(Kumar et al., 2019). In general, mod-\nels have a high ECE, making them less valu-\nable in assisting human decisions.\n• Diversity. We focus on the inter-distinct met-\nric. We see that most models have a simi-\nlar level of diversity in most tasks. Their\ndifferences become obvious only in some\nknowledge-intensive tasks like closed-book\nQA and tasks that have multiple feasible cor-\nrect answers, e.g., summarization, dialogue\ngeneration, and data-to-text generation.\n• Bias. We choose to compare gender bias. We\nobserve that models in data-to-text generation,\nsummarization, and dialogue generation ex-\nhibit a strong tendency to produce biased con-\ntent. These results could be partially attributed\n214\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000034/uni00000058/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000024/uni00000051/uni00000056/uni0000005a/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048\n/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057\n/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056\n/uni00000037/uni00000048/uni0000005b/uni00000057\n/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048\n/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048\n/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057\n/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni0000001c/uni00000019\n/uni00000013/uni00000011/uni00000018/uni00000013\n/uni00000013/uni00000011/uni0000001a/uni0000001b\n/uni00000013/uni00000011/uni00000018/uni00000016\n/uni00000013/uni00000011/uni0000001c/uni00000014\n/uni00000013/uni00000011/uni00000019/uni00000017\n/uni00000013/uni00000011/uni0000001c/uni00000014\n/uni00000013/uni00000011/uni00000016/uni00000013\n/uni00000013/uni00000011/uni00000015/uni0000001a\n/uni00000013/uni00000011/uni00000017/uni00000018\n/uni00000013/uni00000011/uni00000016/uni0000001a\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000028/uni00000035/uni00000031/uni0000002c/uni00000028/uni00000010/uni00000025/uni00000052/uni00000057\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025\n/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000034/uni00000058/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000024/uni00000051/uni00000056/uni0000005a/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048\n/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057\n/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056\n/uni00000037/uni00000048/uni0000005b/uni00000057\n/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048\n/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048\n/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057\n/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni0000001c/uni00000019\n/uni00000013/uni00000011/uni00000018/uni00000013\n/uni00000013/uni00000011/uni0000001a/uni0000001b\n/uni00000013/uni00000011/uni00000018/uni00000016\n/uni00000013/uni00000011/uni0000001c/uni00000014\n/uni00000013/uni00000011/uni00000019/uni00000017\n/uni00000013/uni00000011/uni0000001c/uni00000014\n/uni00000013/uni00000011/uni00000016/uni00000013\n/uni00000013/uni00000011/uni00000015/uni0000001a\n/uni00000013/uni00000011/uni00000017/uni00000018\n/uni00000013/uni00000011/uni00000016/uni0000001a\n/uni00000028/uni00000051/uni0000004a/uni0000004f/uni0000004c/uni00000056/uni0000004b/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\nFigure 8: Comparison among models from different groups in tasks of application assessment. We choose the best\nmodels for each institution and divide them into 2 groups based on the language they focus on: Chinese or English.\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000014/uni00000014\n/uni00000013/uni00000011/uni00000016/uni00000015\n/uni00000013/uni00000011/uni00000018/uni00000017\n/uni00000013/uni00000011/uni0000001a/uni00000018\n/uni00000013/uni00000011/uni0000001c/uni00000019\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013\n/uni00000016\n/uni0000001a\n/uni00000014/uni00000013\n/uni00000014/uni00000016\n/uni00000028/uni00000049/uni00000049/uni0000004c/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000015\n/uni00000013/uni00000011/uni00000017/uni00000017\n/uni00000013/uni00000011/uni00000019/uni00000019\n/uni00000013/uni00000011/uni0000001b/uni0000001b\n/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000017\n/uni00000013/uni00000011/uni00000015/uni00000017\n/uni00000013/uni00000011/uni00000017/uni00000018\n/uni00000013/uni00000011/uni00000019/uni00000019\n/uni00000013/uni00000011/uni0000001b/uni00000019\n/uni00000035/uni00000052/uni00000045/uni00000058/uni00000056/uni00000057/uni00000051/uni00000048/uni00000056/uni00000056\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000018\n/uni00000013/uni00000011/uni00000015/uni00000019\n/uni00000013/uni00000011/uni00000017/uni0000001a\n/uni00000013/uni00000011/uni00000019/uni0000001b\n/uni00000013/uni00000011/uni0000001b/uni0000001c\n/uni00000026/uni00000044/uni0000004f/uni0000004c/uni00000045/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000019\n/uni00000013/uni00000011/uni00000015/uni0000001a\n/uni00000013/uni00000011/uni00000017/uni0000001b\n/uni00000013/uni00000011/uni00000019/uni0000001b\n/uni00000013/uni00000011/uni0000001b/uni0000001c\n/uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni0000001a\n/uni00000013/uni00000011/uni00000014/uni0000001b\n/uni00000013/uni00000011/uni00000015/uni0000001c\n/uni00000013/uni00000011/uni00000016/uni0000001c\n/uni00000013/uni00000011/uni00000018/uni00000013\n/uni00000025/uni0000004c/uni00000044/uni00000056\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000019\n/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015\n/uni00000013/uni00000011/uni00000013/uni00000014/uni0000001b\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000016\n/uni00000037/uni00000052/uni0000005b/uni0000004c/uni00000046/uni0000004c/uni00000057/uni0000005c\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000016\n/uni00000013/uni00000011/uni00000013/uni0000001a\n/uni00000013/uni00000011/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000014/uni00000017\n/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c\nFigure 9: The performance distributions of application assessment tasks under different metrics. Some tasks are\nmissing in some metrics because they are unavailable, e.g., models merely generate an index in the text classification\ntask, thus metrics that evaluate the generated text like diversity, bias, toxicity, and privacy are not applicable.\nto the bias in the dataset domain.\n• Privacy & Toxicity. For toxicity and diver-\nsity, it is meaningless to compare as almost\nall values are low. The only exception is dia-\nlogue generation in privacy. This is because\nour data contains inquiries for detailed con-\ntact information. The implication of a high\nvalue of privacy metric in dialogue generation\nis mixed: It means that the model understands\nusers’ requests and attempts to address them\n215\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000053/uni0000004c/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000010/uni00000057/uni00000052/uni00000010/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000018\n/uni00000013/uni00000011/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000014/uni00000018\n/uni00000013/uni00000011/uni00000015/uni00000013\n/uni00000036/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni00000055/uni00000047/uni00000003/uni00000027/uni00000048/uni00000059/uni0000004c/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000013/uni00000017/uni00000025\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000028/uni00000035/uni00000031/uni0000002c/uni00000028/uni00000010/uni00000025/uni00000052/uni00000057\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025\nFigure 10: The accuracy standard deviation of different models in different prompt templates from different\napplication assessment tasks. We select the best-performing models from top-10 institutions according to accuracy.\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018/uni00000037/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000056\n/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000025/uni00000028/uni0000002f/uni0000002f/uni00000028/uni00000010/uni0000001a/uni00000025\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000010/uni00000050/uni00000057/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018/uni00000033/uni00000055/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000048/uni00000047/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000056\n/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000035/uni00000048/uni00000044/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000055/uni00000048/uni0000004b/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000003/uni0000002c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni0000001a/uni00000025/uni00000014/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056\n/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000026/uni0000004b/uni00000052/uni0000004c/uni00000046/uni00000048/uni00000003/uni00000036/uni00000048/uni00000053/uni00000044/uni00000055/uni00000044/uni00000057/uni00000048/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000026/uni0000004b/uni00000052/uni0000004c/uni00000046/uni00000048/uni00000003/uni0000002d/uni00000052/uni0000004c/uni00000051/uni00000057\nFigure 11: The accuracy of different models in multi-choice tasks withSeparateand Jointstyle prompt templates.\nwith concrete information. It also implies that\nthe model has a higher risk of hallucination\nthat leads to potential harm.\nAt the end of this section, we study the prompt\ntemplate sensitivity, one of the key features in\nCLEVA. Figure 10 presents the accuracy standard\ndeviation of different prompt templates of different\nmodels. We find that instruction-following models\nhave a lower level of standard deviations and thus\nare more robust to variations in prompt templates,\nconsistent with the conclusion in ability evaluation.\nWe also see that small models like ChatGLM2-6B\nand Baichuan-7B have relatively higher standard\ndeviations compared with large models. Interest-\ningly, strong models like GPT-4 have a relatively\nlarge variance in some tasks like summarization.\nA possible reason is that models are sensitive to\nsome keywords in the instruction, e.g., almost all\nmodels perform better in prompt templates that con-\ntain “zh¯ai yào” (means “summarize” in English) in\nthe summarization task. We also find that limited-\naccessed models sometimes refuse to answer. For\nexample, ERNIE-Bot refuses to answer about 4\ntasks, resulting in a lower ranking in Figure 2.\nG.4 Prompting Analysis\nAs discussed in Appendix F, there are two feasi-\nble prompt template types for multi-choice tasks:\nSeperate that feeds each choice with the prompt\nseparately and Jointthat concatenates all choices\nand feeds once. We compare the model perfor-\nmance on these two types of prompt templates\nin multi-choice tasks from application assessment.\nFigure 11 shows that despite the cost of Separate,\n216\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048/uni00000010/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni00000052/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000026/uni0000004b/uni00000044/uni00000057/uni00000045/uni00000052/uni00000057\nFigure 12: The accuracy of different chatbots with Completion and Chatbot style few-shot prompt templates.\n/uni00000013/uni00000014/uni00000015/uni00000017/uni0000001b /uni00000014/uni00000019\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056\n/uni00000013/uni00000011/uni00000014/uni00000018\n/uni00000013/uni00000011/uni00000015/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000018\n/uni00000013/uni00000011/uni00000016/uni00000013\n/uni00000013/uni00000011/uni00000016/uni00000018\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni0000004f/uni00000052/uni00000056/uni00000048/uni00000047/uni00000010/uni00000025/uni00000052/uni00000052/uni0000004e/uni00000003/uni00000034/uni00000024\n/uni00000013/uni00000014/uni00000015/uni00000017/uni0000001b /uni00000014/uni00000019\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000014/uni00000015/uni00000017/uni0000001b /uni00000014/uni00000019\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000013/uni00000011/uni0000001c\n/uni00000036/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000024/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056\n/uni00000013/uni00000014/uni00000015/uni00000017/uni0000001b /uni00000014/uni00000019\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni00000010/uni00000014/uni0000001a/uni00000019/uni00000025/uni0000002a/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000013/uni00000025/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000019/uni00000018/uni00000025/uni00000030/uni00000032/uni00000036/uni00000036/uni00000010/uni00000014/uni00000019/uni00000025/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000025/uni00000044/uni0000004c/uni00000046/uni0000004b/uni00000058/uni00000044/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000015/uni00000010/uni00000019/uni00000025\nFigure 13: The accuracy of different models with various numbers of few-shot demonstrations.\nit is more friendly to models without instruction\ntuning as they perform much better than Joint.\nThis is because Separate restricts the model to\noutput choices only, reducing the errors caused by\nunconstrained generation. However, for instruction-\nfollowing models, Joint yield more advantages\n(e.g., ChatGLM2-6B in text classification, reading\ncomprehension, and sentiment analysis) as some\nSeparate prompt templates may not include all\npossible choices in the prompt. In this case, mod-\nels are likely to produce other viable answers that\ncould not be parsed by automatic metrics (e.g.,\nparaphrasing the correct answer).\nSimilarly, we discuss the impact of Completion\nand Chatbot style few-shot prompting strategies,\nwhere the former concatenates everything into a\nstring and the latter orgainzes demonstrations into\na structured dialogue history. Figure 12 illustrates\nthe impact of these two styles of few-shot prompt-\ning strategies in various chatbots. We see that al-\nmost all chatbots perform better withChatbotthan\nwith Completion, demonstrating the effectiveness\nof this tailored strategy. We also notice that GPT-4\nand ChatGPT from OpenAI are not sensitive to the\nfew-shot prompting styles. After taking a closer\nlook at the generation results, we find that most\nchatbots do not follow the format described in the\ninstruction and illustrated in the in-context exam-\nples to customize their answers, resulting in invalid\npostprocessing of automatic metrics. For instance,\nmost prompts ask the model to output the answer\nonly, but Claude and ChatGLM6-2B tend to pro-\nvide an explanation first.\nWe also investigate how the performance varies\nas the number of in-context examples increases for\nChinese LLMs. Figure 13 visualizes the overall\ntrends of different models in different tasks. In gen-\neral, most models perform better with more demon-\nstrations and are saturated with around 4-8 training\nsamples. In line with existing work (Liang et al.,\n2022), models without instruction tuning benefit\nmore from few-shot demonstrations. We observe\nthat many models suffer from performance degra-\ndation in the text classification task. We believe this\nis because our test set has a relatively large label\nspace and including more demonstrations distracts\nthe models.\n217",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6185657978057861
    },
    {
      "name": "Chen",
      "score": 0.4835907816886902
    },
    {
      "name": "Natural language",
      "score": 0.4346771240234375
    },
    {
      "name": "Natural language processing",
      "score": 0.43380600214004517
    },
    {
      "name": "Programming language",
      "score": 0.41268038749694824
    },
    {
      "name": "Artificial intelligence",
      "score": 0.338469922542572
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}