{
  "title": "A Systematic Investigation of Commonsense Knowledge in Large Language Models",
  "url": "https://openalex.org/W4385567227",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3022390796",
      "name": "Xiang Lorraine Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2509308542",
      "name": "Adhiguna Kuncoro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2929722776",
      "name": "Jordan Hoffmann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4311887876",
      "name": "Cyprien de Masson d’Autume",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A297118547",
      "name": "Phil Blunsom",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117710609",
      "name": "Aida Nematzadeh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4300427683",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2911681509",
    "https://openalex.org/W3154295049",
    "https://openalex.org/W2964235839",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W3034912286",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W3178522238",
    "https://openalex.org/W4287204036",
    "https://openalex.org/W3154463028",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W2970692082",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2970946372",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3166444100",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W4288243162",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W3174660442",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3104499181",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2031614786",
    "https://openalex.org/W2897037347",
    "https://openalex.org/W2754517384",
    "https://openalex.org/W2050482109",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3116216579",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2963672599",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W3175910413",
    "https://openalex.org/W3100307207",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W181868102",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W3101056292",
    "https://openalex.org/W2896739098",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W4288262459"
  ],
  "abstract": "Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d'Autume, Phil Blunsom, Aida Nematzadeh. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11838–11855\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nA Systematic Investigation of Commonsense Knowledge\nin Large Language Models\nXiang Lorraine Li†∗ Adhiguna Kuncoro‡ Jordan Hoffmann⋆♢\nCyprien de Masson d’Autume♦♢ Phil Blunsom▲♣♢ Aida Nematzadeh‡\n†Allen Institute for Artificial Intelligence ‡DeepMind\n⋆Inflection AI ♦Reka ▲Cohere ♠University of Oxford\nlorrainel@allenai.org nematzadeh@google.com\nAbstract\nLanguage models (LMs) trained on large\namounts of data (e.g., Brown et al., 2020; Pat-\nwary et al., 2021) have shown impressive per-\nformance on many NLP tasks under the zero-\nshot and few-shot setup. Here we aim to better\nunderstand the extent to which such models\nlearn commonsense knowledge — a critical\ncomponent of many NLP applications. We con-\nduct a systematic and rigorous zero-shot and\nfew-shot commonsense evaluation of large pre-\ntrained LMs, where we: (i) carefully control\nfor the LMs’ ability to exploit potential surface\ncues and annotation artefacts, and (ii) account\nfor variations in performance that arise from\nfactors that are not related to commonsense\nknowledge. Our findings highlight the limi-\ntations of pre-trained LMs in acquiring com-\nmonsense knowledge without task-specific su-\npervision; furthermore, using larger models or\nfew-shot evaluation are insufficient to achieve\nhuman-level commonsense performance.\n1 Introduction\nCommon sense — the implicit knowledge about\neveryday situation that is shared by humans — is\nan important prerequisite for developing general-\npurpose intelligent systems (McCarthy et al., 1960;\nLiu and Singh, 2004; Gunning, 2018). Intrigu-\ningly, recent large language models (LMs, Brown\net al., 2020; Patwary et al., 2021; Rae et al., 2021)\nhave achieved remarkable performance at various\ncommon sense benchmarks (e.g., Sakaguchi et al.,\n2020; Zellers et al., 2019a; Bisk et al., 2020b; Sap\net al., 2019b), even when they are evaluated in\na zero-shot or few-shot fashion, without explicit\ncommonsense supervision. We revisit this apparent\nsuccess, and conduct a rigorous study to better un-\nderstand the extent to which such pre-trained LMs\nare able to capture commonsense knowledge.\n∗ Work done during DeepMind internship when Lorraine\nwas a PhD student at UMass Amherst. ♢ Work done at Deep-\nMind\nFigure 1: The experiment settings with their correspond-\ning input to the LM. The example is taken from Social\nIQa (Sap et al., 2019b) where we convert questions to\nnatural text using the rules of Shwartz et al. (2020); this\nconversion yields to better performance (§5).\nIn this work, we focus on zero- and few-\nshot evaluations of pre-trained LMs without\ncommonsense-specific fine-tuning for two reasons:\nFirst, we aim to examine if a pre-trained LM is\nable to acquire general commonsense knowledge.\nAs pre-trained LMs constitute a foundational build-\ning block of NLP today, any deficiencies in their\ncommonsense understanding can thus adversely\nmanifest in downstream applications (Bommasani\net al., 2021). Fine-tuning the LM would make it\nhard to disentangle how much of the commonsense\nknowledge is acquired by the underlying LM, as op-\nposed to the task-specific supervision from a bench-\nmark (Yogatama et al., 2019). Second, human-\nannotated commonsense datasets are expensive to\ncollect due to the vast, diverse, and growing nature\nof commonsense knowledge (Elazar et al., 2021).\nConcretely, our work differs from prior work\non commonsense evaluation of LMs (Brown et al.,\n2020; Patwary et al., 2021) by way of a more rigor-\nous evaluation, in which we: (i) carefully control\nfor the LM’s ability to exploit potential surface\ncues and annotation artefacts to predict the answer,\nwithout reasoning over the context. We further (ii)\naccount for the variations in factors influencing the\nLM’s performance, which arise from certain evalu-\nation design choices — independently of common-\n11838\nsense knowledge in the models. We systematically\nconduct this study on four commonsense bench-\nmarks, six model sizes (up to a very large LM with\n280B parameters), and multiple evaluation settings\n(e.g., different score functions and prompt format).\nWe begin with our first question: When evaluat-\ning a large LM in a zero-shot setting, how does its\nzero-shot performance compare to a strong base-\nline (§3)? Controlling for the LM’s ability to guess\nthe correct answer, without even looking at the\nquestion (Poliak et al., 2018; Trichelair et al., 2019,\nAnswer-only baseline, top of Fig. 1), we find that,\ndespite the LM’s strong zero-shot performance, the\nAnswer-only baseline can nevertheless perform sur-\nprisingly well on some benchmarks. Despite the\nclear importance of comparing with answer-only\nbaselines as shown in Figure 2, these comparisons\nare absent from recent work on large LMs (Zhou\net al., 2020; Brown et al., 2020; Rae et al., 2021).\nFurthermore, increasing model size alone is un-\nlikely to bridge the gap with human performance\nin the near future: Our analysis of scaling behavior\nsuggests that much larger dense LMs (with 100T\nto 1018 parameters — which are infeasibly large at\npresent) are needed to achieve human performance\nfor 3 out of 4 benchmarks.\nDoes familiarizing the LM with the task format\nusing a few-shot evaluation setting substantially\nimprove performance (§4)? We find that the few-\nshot evaluation (using up to 64 examples) does not\nsubstantially improve the LMs’ performance for\nmost tasks except Social IQa. Moreover, using the\nfew-shot/in-context demonstration setting fails to\nbridge the gap between the LM and current SOTA.\nFinally, we ask: to what extent does the model’s\nzero-shot performance vary depending on certain\nevaluation design choices, such as the format of\nthe prompt or the score function (§5) ? We find\nthat these design choices — though they have little\nto do with common sense — can result in large\nfluctuations in performance (up to 19%). This find-\ning challenges the notion that large LMs are largely\nable to work well out-of-the-box with minimal task-\nspecific tuning. Based on these findings, we em-\nphasize the need to carefully select such design\nchoices, explicitly state them to enable fair compar-\nison with prior work, and quantify the robustness of\nthe observed results across different design choices.\nAll in all, our findings suggest that acquiring\nhuman-level commonsense knowledge, without re-\nlying on surface cues or task-specific supervision,\nChoices Knowledge Types Questions\nHellaSwag (Zellers et al., 2019a) 4 Temporal, Physical 10042\nWinoGrande (Sakaguchi et al., 2020) 2 Social, Physical 1267\nSocial IQa(Sap et al., 2019b) 3 Social 1954\nPIQA (Bisk et al., 2020b) 2 Physical 1838\nTable 1: Benchmark Statistics. Choices: the number\nof candidate answers for each question; Questions: the\nnumber of candidate answers for each question.\nremains beyond the reach of current large LMs.\nGiven the marginal improvements from increasing\nmodel size, we conjecture that other techniques,\nsuch as explicit commonsense supervision, multi-\nmodal grounding, or physical embodiment (Bisk\net al., 2020a), are promising ways forward.\n2 Experimental Setting\nWe begin by outlining our experimental setup, and\ndescribe the benchmarks, model, baselines, and\nother relevant experimental settings.\n2.1 Commonsense Benchmarks\nCommonsense knowledge spans many categories,\nsuch as physical common sense (e.g., a car is heav-\nier than an apple), social common sense ( e.g., a\nperson will feel happy after receiving gifts), and\ntemporal common sense (e.g., cooking an egg takes\nless time than baking a cake). Given this diverse\nnature of commonsense knowledge, various bench-\nmarks have been proposed to test these different\ntypes of knowledge (e.g., Zellers et al., 2019a; Sak-\naguchi et al., 2020; Sap et al., 2019b; Bisk et al.,\n2020b; Lin et al., 2020; Boratko et al., 2020).\nCommonsense benchmarks broadly consist of\ntwo tasks: (a) multiple-choice evaluation (Zellers\net al., 2018, 2019a; Sap et al., 2019b; Bisk et al.,\n2020b), where a model needs to choose the correct\nanswer from a list of plausible answers; (b) gen-\nerative evaluation (Boratko et al., 2020; Lin et al.,\n2020, 2021), which requires a model to generate\nan answer given a question and some additional\ncontext. Here we focus on multiple-choice bench-\nmarks, since they provide a more reliable automatic\nmetric (i.e., accuracy), whereas automated metrics\nused to evaluate language generation (e.g., BLEU,\nPapineni et al., 2002) do not correlate perfectly\nwith human judgment (Liu et al., 2016; Novikova\net al., 2017).1 We use a diverse set of four represen-\ntative multiple-choice commonsense benchmarks\n1Human judgment of LM output is not only costly to ob-\ntain, but also imperfect (Clark et al., 2021), compounding the\ndifficulty of commonsense evaluation in a generation setup.\n11839\nto better understand the extent to which pre-trained\nLMs are able to acquire different types of common-\nsense knowledge. We use the validation split of\neach benchmark, as their test splits are not public.\nHellaSwag (Zellers et al., 2019a) is designed to\nevaluate a model’s ability to understand physical,\ngrounded, and temporal common sense. Given a\nfour-sentence story, the model must choose the cor-\nrect ending from four candidates. The stories are\neither video captions from AcitivityNet (Heilbron\net al., 2015), or WikiHow passages (Koupaee and\nWang, 2018). When evaluating LMs on a similar\ndataset (Zellers et al., 2018), incorrect answers can\nbe easy to distinguish from correct ones; hence in\nconstructing HellaSwag, Zellers et al. (2019a) re-\nmoved easy negatives through adversarial filtering.\nWinoGrande (Sakaguchi et al., 2020) is a co-\nreference resolution benchmark that mainly exam-\nines physical and social common sense. Each exam-\nple consists of a sentence (e.g., “The trophy did not\nfit the suitcase because it is too big.”) and two can-\ndidate entities (e.g., “trophy” or “suitcase”). The\ntask is to choose the correct entity for the pronoun,\ne.g., “it” refers to “trophy” in the example.\nSocial IQa(Sap et al., 2019b) focuses on evalu-\nating social commonsense, in particular theory of\nmind — the capacity to reason about others’ mental\nstates (Flavell, 2004). Given context sentences and\na corresponding question, the task is to choose the\ncorrect response from three candidates. Annota-\ntors use the ATOMIC knowledge base (Sap et al.,\n2019a) to create context sentence and questions;\nthe answers are provided by additional annotators.\nPIQA (Bisk et al., 2020b), short for physical in-\nteraction question answering, mainly covers the\nphysical aspect of common sense. Each data point\nconsists of a task and two alternative solutions to\nfinish the task; one of which is correct. The tasks\nare curated from a website2 with instructions for ev-\neryday tasks (e.g., separating egg yolks from eggs);\nthe solutions are provided by human annotators.\n2.2 Pre-trained Language Model\nWe use the pre-trained language model of Rae et al.\n(2021), Gopher, which is an autoregressive Trans-\nformer (Vaswani et al., 2017) language model with\n280 billion parameters. We choose Gopher be-\ncause of its excellent zero-shot and few-shot per-\nformance at various benchmarks, in addition to its\nlarge model size, which has been shown to improve\n2https://www.instructables.com/\nlanguage modeling and downstream performance\n(Kaplan et al., 2020). Notably, Gopher is more than\n50% larger than GPT3 and as of March 2022, is\none of the largest dense LMs developed to date.\nGopher hyper-parameters. The pre-trained Go-\npher language model has 80 layers, 128 attention\nheads, 128-dimensional key/value vectors, and a\nfeedforward layer dimension of 16,384. To bet-\nter understand the effect of different model sizes\n(§3.2), we experiment with five other model sizes:\n44M, 117M, 417M, 1.4B, and 7.1B. Similar to Go-\npher, each of these models was pre-trained by Rae\net al. (2021); a full list of model hyper-parameters\nis summarized in Table 1 of Rae et al. (2021). Each\nmodel is trained by subsampling from the Mas-\nsiveText dataset, which consists of more than 2\ntrillion tokens from various domains including web\npages, news, books, and codes (Rae et al., 2021).\nThe authors have removed documents that overlap\nsignificantly with the evaluation sets from training\nset including benchmarks used in our work. We\nuse TPUv3 to conduct all evaluations, with an esti-\nmated total compute budget of 2 ×1020 FLOPs.\nScore function. On the multiple-choice bench-\nmarks, we evaluate the pre-trained LM by calcu-\nlating the score for each answer choice under the\nmodel, and select the highest-scoring answer ˆ y:\nˆ y= arg max\ny∈Y (x)\nsθ(y|x);\nhere x denotes the question or prompt,Y (x) the set\nof answer choices for a given question, and sθ(·)\nthe score of an answer choice y given x, under\nthe pre-trained LM with parameters θ. We provide\nsome examples in Table 2. 3 For Social IQa, we\nconvert questions to natural text using the rules\nof Shwartz et al. (2020); we find this natural text\nformat to yield better results, as discussed in §5.\nUnless otherwise stated, we use cross-entropy\n(or token-level log prob) to score each answer:\nsθ(y|x) =\n∑∥y∥\ni=0 log(pθ(yi|x, y0...yi−1))\n∥y∥ . (1)\nThis score function reduces the impact of length;\nwithout dividing by ∥y∥, longer answers might\nhave lower probabilities (Stahlberg and Byrne,\n2019). GPT3 (Brown et al., 2020) also employs\nthis score function for zero-shot evaluation.\n3For Social IQa, we concatenate the context sentence and\nquestion together to form the prompt x.\n11840\nDataset Prompt: x Answer: y\nHellaSwag A woman is outside with a bucket and a dog. The dog is running\naround trying to avoid a bath. She gets the dog wet, then it runs away again.\nWinoGrande The GPS and map helped me navigate home. I got lost when the GPS got turned off.\nSocial IQa Jordan was in charge of taking the food on the camping trip and\nleft all the food at home. Jordan felt\nhorrible that he let his friends down on\nthe camping trip.\nPIQA Make Halloween lanterns. Draw ghost faces on empty milk bottles,\nput a candle in each one.\nTable 2: Examples of the prompt x and the correct answer y in different benchmarks.\n2.3 Baselines\nWe compare the performance of Gopher with two\nbaselines. The first, simple baseline is to randomly\nselect an answer candidate, where the chance of se-\nlecting the correct one is 1\nnumber of choices . We hence-\nforth refer to this as the Random Baseline. We ex-\nperiment with two other baselines: Either choosing\nthe majority label from the training data, or choos-\ning the longest answer. We omit these baselines as\nthey perform similarly to the Random Baseline.\nMore importantly, we consider an Answer-only\nBaseline, where we select the highest-scoring an-\nswer choice under the LM, without conditioning\non the question. More formally, this baseline con-\nsiders sθ(y), as opposed to sθ(y|x) in Eq. 1. This\nbaseline reveals the extent to which the pre-trained\nLM conducts the appropriate reasoning over the\ncontext to select the answer, as opposed to relying\non potential surface cues or annotation artefacts\nthat make the correct answer a priori more prob-\nable than the rest. We illustrate this baseline at\nthe top of Fig. 1. For WinoGrande, we calculate\nthe cross-entropy of the text starting by the pro-\nnoun replacement, as shown in Table 2. Ideally,\neach answer choice should be equally likely if we\ndo not consider the question, and the Answer-only\nperformance should be close to the Random base-\nline. Similar hypothesis-only baselines are well-\nstudied for natural language inference datasets (Po-\nliak et al., 2018); Trichelair et al. (2019) further\nexplored such an Answer-only baseline, albeit only\non the SW AG benchmark (Zellers et al., 2018).\n3 Zero-shot Performance\nIn Fig. 2, we report the zero-shot performance\nof our pre-trained LM (with 280B parameters,\n§2.2) on the four commonsense benchmarks, along-\nside: (i) the Random and Answer-only baselines,\nand (ii) the current state-of-the-art (SOTA) re-\nsult. The SOTA results are achieved by the UNI-\nCORN (Lourie et al., 2021) model with 11B pa-\nrameters, which is pre-trained on 6 existing com-\nmonsense datasets (Zellers et al., 2019a; Bisk et al.,\n2020b; Sap et al., 2019b; Sakaguchi et al., 2020;\nBhagavatula et al., 2020; Huang et al., 2019).\nZero-shot performance. At first glance, we ob-\nserve strong zero-shot results, outperforming the\nRandom Baseline in all benchmarks (compare\n“Rand” and “ZS” in Fig. 2). However, the gap\nbetween the stronger Answer-only baseline and the\nzero-shot result is smaller for all benchmarks (com-\npare “Answer” and “ZS”): Whereas this gap is still\nsizable for HellaSwag and WinoGrande (>20), it is\nmuch smaller for Social IQa and PIQA. Finally, in\nall cases, there is still a large gap between the SOTA\nand zero-shot performance (>10); this gap is largest\nfor WinoGrande and Social IQa, suggesting that\nsocial and physical commonsense is challenging\nfor pre-trained LMs — even a large one with 280B\nparameters — without task-specific supervision.4\n3.1 Answer-only bias\nAs shown in Fig. 3, the performance gap between\nthe Random and Answer-only baselines is notably\nlarge for HellaSwag and PIQA, where the Answer-\nonly baseline outperforms the Random baseline\nby more than 32% and 23%, respectively. This\nlarge gap highlights an existing answer-only bias\nin these benchmarks: the correct answer can, in\nfact, be selected by the LM without conducting the\nappropriate commonsense reasoning over the pro-\nvided context. On the other hand, the Answer-only\nbaseline performs similarly to the random base-\nline on WinoGrande and Social IQa; hence, the\nzero-shot performance on these benchmarks is a\nmore reliable estimate of the model’s acquisition of\n4We remark that the 530B-parameter LM of Patwary et al.\n(2021) achieves slightly better performance than Gopher on\nHellaSwag (80.2), PIQA (82), and WinoGrande (73), although\nthere remains a large gap with the SOTA performance.\n11841\nRand Answer ZS SOTA\n0\n20\n40\n60\n80Accuracy (%)25.0\n57.03\n79.14\n93.85\nBenchmark = HellaSwag\nRand Answer ZS SOTA\n50.0\n73.18\n80.47\n90.13\nBenchmark = PIQA\nRand Answer ZS SOTA\n33.33 36.34\n50.15\n83.15\nBenchmark = Social IQa\nRand Answer ZS SOTA\n50.0 50.83\n71.11\n91.28\nBenchmark = WinoGrande\nFigure 2: Random Baseline (Rand), Answer-only Baseline (Answer), zero-shot (ZS), and the current state-of-the-art\n(SOTA) for each benchmark, which is achieved by UNICORN (Lourie et al., 2021).\nHellaSwag PIQA Social IQa WinoGrande\n0\n5\n10\n15\n20\n25\n30Accuracy Difference (%)\n32.03\n23.18\n3.01\n0.83\nFigure 3: The performance gap between Answer-only\nand Random baselines for each benchmark.\ncommonsense knowledge. Given the existing (and\nsometimes inevitable) answer-only biases in some\nbenchmarks, it is important to contextualize the\nzero-shot results by comparing with strong base-\nlines, although such comparisons are missing from\nrecent work (e.g., Zhou et al., 2020; Brown et al.,\n2020; Rae et al., 2021).\n3.2 Does Increasing Model Size Help?\nGopher (the largest LM we have access to) achieves\na decent zero-shot performance for most common-\nsense benchmarks, but maintains a notable gap with\nfine-tuned SOTA results. Can we eventually reach\nhuman-level performance on these commonsense\nbenchmarks by increasing model size alone?\nSince we do not have access to larger language\nmodels than Gopher, we examine the extent to\nwhich zero-shot performance improves when us-\ning Gopher compared to a range of smaller models\n(i.e., scaling plots). Such scaling plot can help\nus predict the performance for larger models than\nGopher. To that end, we use 6 pre-trained model\nsizes from 44M to 280B parameters (see §2.2). 5\nWe present the findings in Table 3. On all four\n5Each model size is trained on the same dataset; hence any\nperformance differences can be attributed to model size.\nAnswer ZS FS(1) FS(10) FS(64)\nHellaSwag\n44M 25.8 28.0 28.0 28.1 27.9\n117M 29.2 33.5 33.3 34.0 33.5\n417M 35.6 44.1 43.4 43.3 43.3\n1.4B 43.2 56.7 56.4 56.2 56.5\n7.1B 50.4 69.5 67.6 67.9 67.9\nGopher 57.0 79.1 77.8 79.2 79.3\nWinoGrande\n44M 48.5 51.3 51.1 50.8 50.6\n117M 50.8 52.0 51.9 50.9 50.8\n400M 49.9 52.2 51.8 50.8 52.5\n1.3B 49.7 58.1 56.4 56.0 57.3\n7B 52.4 64.6 62.1 63.1 62.0\nGopher 50.8 71.1 69.2 71.4 74.6\nSocial IQa\n44M 35.5 42.0 41.2 40.9 40.9\n117M 36.1 43.7 42.7 42.1 42.2\n400M 36.0 45.6 44.5 45.2 45.3\n1.3B 35.8 46.9 46.4 48.6 50.5\n7B 36.9 48.1 48.1 52.9 54.2\nGopher 36.3 50.2 50.2 55.3 57.5\nPIQA\n44M 60.2 62.6 62.1 62.3 61.3\n117M 62.1 65.5 64.6 65.1 65.3\n400M 65.9 70.9 68.8 70.5 70.1\n1.3B 68.4 74.4 73.3 74.4 74.6\n7B 70.0 77.4 75.5 77.6 78.1\nGopher 73.2 80.5 79.3 81.4 81.5\nTable 3: Performance of all models across benchmarks\nunder different experimental settings. Ans: Answer-\nonly Baseline; ZS: zero-shot performance; FS(n): few-\nshot performance where n is the number of examples.\nbenchmarks, the LM’s zero-shot performance (Ta-\nble 3, ZS column) consistently gets better as we\nuse increasingly larger models. This finding is also\nconsistent with that of Brown et al. (2020), who\nshowed that larger models have better performance\nat HellaSwag, WinoGrande, and PIQA. But, cru-\ncially, we argue that this does not necessarily mean\nthat larger models are better at commonsense rea-\nsoning: For HellaSwag and PIQA, the Answer-only\nbaseline also substantially improves with model\nsize (Table 3, Answer column). Hence, for these\nbenchmarks, larger models are also better at ex-\nploiting potential surface cues and annotation arte-\nfacts to guess the correct answer, without reasoning\nover the context. To properly assess commonsense\nreasoning, we should focus on the performance dif-\nference between the zero-shot and the Answer-only\nbaseline.\n11842\n44M 117M 417M 1.4B 7.1BGopher\n0\n5\n10\n15\n20Accuracy Difference (%)2.2\n4.34\n8.48\n13.42\n19.02\n22.11\nDataset = HellaSwag\n44M 117M 417M 1.4B 7.1BGopher\n2.39 3.37\n5.01 6.04\n7.4 7.29\nDataset = PIQA\n44M 117M 417M 1.4B 7.1BGopher\n6.5 7.52\n9.62\n11.05 11.16\n13.82\nDataset = Social IQa\n44M 117M 417M 1.4B 7.1BGopher\n2.76\n1.19 2.37\n8.36\n12.15\n20.28\nDataset = WinoGrande\nFigure 4: The difference between zero-shot performance and Answer-only baseline for different model sizes.\nWe plot this performance difference with respect\nto different model sizes in Fig. 4. We observe\nthat larger models have better performance across\nbenchmarks — when increasing model size, the\nzero-shot performance gains are more than the per-\nformance gains of the Answer-only baseline. Nev-\nertheless, the magnitude of this improvement varies\ndepending on the benchmark: We see a substantial\nimprovement on WinoGrande, but smaller improve-\nments on HellaSwag, Social IQa and PIQA.\nScaling behavior. Based on these trends, what\nmodel size would be required to achieve human-\nlevel performance on these benchmarks? Through\na linear regression analysis (see Appendix B for\nmore details), given the current rate of improve-\nment in performance when gradually increasing\nthe model size from 44M up to 280B, we need a\nmodel of at least 1.4T parameters to achieve human\nperformance on HellaSwag, and a model of >100T\nparameters (∼400x larger than Gopher) for other\nbenchmarks. This result suggests that training ever-\nlarger models may not help us reach human perfor-\nmance, at least in the near future. Indeed, given the\nenormous compute costs for training even larger\nLMs than the Gopher model with 280B parameters,\nwe conjecture that there are more efficient ways\nof acquiring commonsense knowledge in an unsu-\npervised fashion, for instance through multi-modal\nlearning and grounding (Bisk et al., 2020a).\n4 Few-shot Performance\nRecent work has shown that large LMs can per-\nform surprisingly well at various tasks in a few-\nshot fashion (Brown et al., 2020; Patwary et al.,\n2021). Under this setup, the model is provided\nwith n examples of the downstream task, which\nare then appended to the prefix. Concretely, for\nthe four commonsense benchmarks, we append n\nexamples that include the question and the correct\nanswer; these examples — which are randomly\nsampled from the training split of each benchmark\n— appear before the evaluated question, as shown\nin Fig. 1. This few-shot formulation is appealing\nas it relies only on a small number of task-specific\nexamples to get the LM accustomed to the task,\nwithout any fine-tuning. To what extent can we\nimprove the model performance on commonsense\nbenchmarks, by shifting from the zero-shot to the\nfew-shot evaluation protocol?6\nIn Fig. 5, we compare the performance of Go-\npher under different evaluation protocols: (i) zero-\nshot and (ii) few-shot ( n) where we use n ∈\n{1, 10, 64}examples. We run the few-shot ex-\nperiments between 5 and 10 times — sampling\ndifferent examples each time — and report the av-\nerage performance. The variance across runs is\nvery small and is shown as the error bar in Fig. 5.7\nInterestingly, model performance with few-shot (1)\nis sometimes worse than the zero-shot model, but\nthe few-shot (10) and (64) models outperform their\nzero-shot counterpart (albeit sometimes by small\nmargins). On HellaSwag and PIQA, we do not\nobserve substantial improvement from few-shot\nevaluation compared to the zero-shot baseline (less\nthan 2%).8 While few-shot evaluation does not\nhelp much for most datasets, the only exception is\nSocial IQa, where the few-shot (64) model outper-\nforms the zero-shot model by a > 7% margin. We\nattribute this to the less natural text of Social IQa;9\nhence adding task-specific examples provides in-\nformation about what is expected of the task.\n6The ability of large LMs to perform few-shot/in-context\nlearning was first demonstrated by GPT3. Here we use an\neven-larger model than GPT3, which we expect to be able to\nleverage in-context learning to a similar extent as GPT3.\n7Our findings on the small variance with different few-shot\nexamples is consistent with Min et al. (2022), who found that\nreplacing real examples with random labels can work as well.\n8In few-shot experiments (n = 50), Brown et al. (2020)\nalso found small improvements for PIQA and HellaSwag\n(<1.5%), with a larger improvement (7.5%) for WinoGrande.\n9We found that Gopher has the highest perplexity when\npredicting Social IQa answers compared to the other datasets.\n11843\nZS FS(1) FS(10) FS(64)\nExp\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\n0.79 0.78 0.79 0.79\nDataset = HellaSwag\nZS FS(1) FS(10) FS(64)\nExp\n0.8 0.79\n0.81 0.82\nDataset = PIQA\nZS FS(1) FS(10) FS(64)\nExp\n0.5 0.5\n0.55\n0.57\nDataset = Social IQa\nZS FS(1) FS(10) FS(64)\nExp\n0.71\n0.69\n0.71\n0.75\nDataset = WinoGrande\nFigure 5: Accuracy on the benchmarks for zero-shot (ZS) and few-shot (FS) settings (with 1, 10, and 64 examples).\nWe additionally report the error bars, although the error bars are not always visible due to the very small variance.\nOverall, we observe that the usefulness of the\nfew-shot setting is benchmark dependent. More-\nover, using task-specific examples in a few-shot\nsetting does not bridge the gap to SOTA or human\nperformance for any of the benchmarks.\nKnowledge base retrieval. We further examine\nif adding pre-extracted commonsense knowledge\nbase triplets to the context — as a different form\nof few-shot/in-context learning — helps improve\nmodel performance. (See Appendix D for details.)\nIn contrast to work of Shwartz and Choi (2020),\nwe observe no improvements when appending the\ntriplets; we attribute this discrepancy to the strong\nperformance of our base models (see §5).\n5 Robustness of Reported Results\nDifferent evaluation design choices — such as\nthe format of the prompt or the choice of score\nfunctions — can impact the LM’s zero-shot per-\nformance, and crucially result in different conclu-\nsions about a model’s commonsense understanding\nability. Moreover, the lack of a standardized zero-\nshot LM evaluation protocol makes direct com-\nparisons between papers difficult (Shwartz et al.,\n2020; Bosselut et al., 2021). To what extent can we\nattribute variance in the reported results to these\nevaluation design choices — even though they have\nlittle to do with commonsense knowledge?\nModel. Quantifying the robustness of the re-\nported results necessitates scoring a large num-\nber of examples under different evaluation design\nchoices, which is infeasible to do with the largest\n(280B-parameter) model that has a slow inference\nspeed. Hence, we conduct the following experi-\nments using the 7B-parameter model, which is still\n∼5 times larger than GPT2 (Radford et al., 2019).\nScore functions. Prior work employs different\nscore functions to assess the plausibility of each\nanswer choice given a question (Brown et al., 2020;\nShwartz et al., 2020; Bosselut et al., 2021; Holtz-\nman et al., 2021), which makes a direct comparison\nbetween different results challenging. Here we in-\nvestigate the impact of different score functions\non the reported performance. In addition to cross-\nentropy (defined in §2.2), we experiment with two\nother score functions. The first is sequence log\nprobability, defined as the log probability of the\nanswer choice y conditional on the question x. Let-\nting yi be the i-th token in the answer y:\ns(y|x) =\n∥y∥∑\ni=0\nlog(p(yi|x, y0...yi−1)) (2)\nAnother widely used score function (Bosselut\net al., 2021; Holtzman et al., 2021) is point-wise\nmutual information. This score function takes into\naccount the probability of the answer choices alone,\nand the probability of the answer choices condi-\ntional on the question. This metric assesses whether\nthe question adds additional information, as com-\nmonsense reasoning should be established within\nthe context of the question. As this score function\naccounts for the prior probability of answer options,\nit can yield lower accuracy than score functions like\ncross-entropy that do not account for such factor\n(Answer-only baseline, §2.3).\ns(y|x) =PMI (y, x) =log p(y|x)\np(y) (3)\nPrompt format. Another important factor is the\nformat of the prompt; here we consider a few\nsuch choices. In addition to the concatenation of\nthe question and the answer, we experiment with\nadding special symbols \"[Question]\" and \"[An-\nswer]\" to specify the question and the answer\n11844\n(Brown et al., 2020). Moreover, for Social IQa\nand PIQA, we experiment with a set of predefined\nrules (taken from Shwartz et al., 2020) to convert\nthe questions into sentences, which are closer to\nthe LM’s pre-training data format. Finally, we find\nthat having the correct lower/upper case and punc-\ntuation is important; thus we manually checked all\nbenchmarks to correct for case and punctuation.10\nScored text. The next option is whether to score\nthe entire question–answer pair (Shwartz et al.,\n2020), or only the answer choice (conditional on\nthe given question as prefix) as done by Brown et al.\n(2020) i.e., whether to calculate s(x; y) or s(y|x),\nwhere ; implies text concatenation.\n5.1 Do These Design Choices Matter?\nTable 4 shows the performance difference of using\nthe worst versus the best design choices, which are\nindependently optimized for each task. To sweep\nover the above design choices, instead of consider-\ning all combinations of parameters, we iterate the\noptions in one category (e.g., score function), while\nfixing the parameters in the other categories.11\nOverall, we observe a difference between the\nbest and worst settings on all benchmarks; this\ngap is especially large for HellaSwag and PIQA.\nThis result shows that large language models do\nnot simply work out of the box for some common-\nsense benchmarks, because for some tasks, these\nevaluation design choices can account for a large\nvariation in model performance. We find that the\nscore function plays the most important role —\ncross-entropy yields the highest accuracy values\nacross most benchmarks, but sequence log prob-\nability achieves a slightly better performance for\nWinoGrande. However, when using these scores,\nwe should account for the Answer-only baseline\n(§3). Moreover, converting questions to sentences\nmakes the largest difference for Social IQa. We\nalso find that scoring the answer conditional on the\nquestion — as opposed to scoring the concatena-\ntion of questions and answers — works best, except\nfor WinoGrande, which has no questions.\n10Recent work learns the prefix that would maximize perfor-\nmance (e.g., Li and Liang, 2021). Here we focus on evaluation\nsetups with no parameter updates, and leave this extension to\nfuture work. Our findings also indicate that the score function\nchoice — which is not covered by lightweight fine-tuning ap-\nproaches — is more important than the prompt format (§5.1).\n11This decision saves compute resources, while offering a\nlower boundon the performance variations. Our goal here is\nnot to seek the highest achievable performance, but to under-\nstand how much performance varies across different settings.\nWorst Best Difference\nHellaSwag 50.8 70.5 19.7\nPIQA 62.5 78.7 16.2\nSocial IQa 43.9 48.5 4.6\nWinoGrande 59.7 62.0 2.3\nTable 4: The performance difference between the worst\nand best design choices for each benchmark.\nAnswer-length bias. Although cross-entropy\ngenerally achieves the best reported performance,\nthis score function is sensitive to answer lengths.\nAs shown in Appendix C, cross-entropy tends to\nassign higher scores to longer answers; to vary-\ning extent, this pattern holds for PIQA, Social IQa,\nand WinoGrande. We attribute this to the higher\nprobability assigned to subsequent tokens in the\nsequence, as such tokens have the most context\nand thus can be more easily predicted than tokens\nin the beginning of the answer. As longer answers\nhave more such easier-to-predict tokens, their cross-\nentropy tends to be lower. This pattern is reversed\nin metrics such as sequence log probability, where\nshorter sequences often have higher scores (Koehn\nand Knowles, 2017; Stahlberg and Byrne, 2019).\nNote that this bias does not change the results re-\nported in this work since there is no correlation be-\ntween answer length and correctness (Appendix C).\nTakeaways. We conclude this section with three\nconcrete recommendations for future work.\n• Although cross-entropy often achieves the best\nperformance, it does not take into account the\nprobability of selecting the correct answer with-\nout reasoning over the context (§3). We recom-\nmend future work to either: (i) use cross-entropy\nand report the gap with the answer-only baseline,\nor (ii) use the PMI score function, which already\ntakes the probability of the answer into account.\n• In the same way that we search for the best model\nhyper-parameters, future work should search\nover certain important evaluation design choices,\nsuch as the format of the prompt, and whether to\nconvert the questions into declarative sentences.\n• Lastly, we strongly encourage future work to re-\nport the variance of the observed results across\ndifferent design choices. This can provide an\nindication of the robustness of the language mod-\nels’ performance on commonsense benchmarks.\n11845\n6 Related Work\nWhile recent work evaluates LMs against common-\nsense benchmarks in a zero- and few-shot fashion,\nthey do not examine the extent to which model per-\nformance can be attributed to superficial cues or\nannotation artefacts in a given dataset (e.g., through\nstrong baselines), nor do they quantify how robust\nthe model performance is under different evalua-\ntion design choices. Trichelair et al. (2019); Elazar\net al. (2021) investigate the existence of dataset bias\nin commonsense co-reference resolution bench-\nmarks (Levesque et al., 2012; Sakaguchi et al.,\n2020) and SWAG (Zellers et al., 2018); here we\nconduct a more comprehensive investigation on\nfour diverse commonsense benchmarks.\nAnother line of work probe for commonsense\nknowledge in LMs through knowledge base com-\npletion (Petroni et al., 2019; Davison et al., 2019)\nor manually-designed probing tasks (Weir et al.,\n2020; Shwartz and Choi, 2020). Zhou et al. (2020)\nevaluate pre-trained LMs against commonsense\nbenchmarks and propose a new dataset requiring\nmulti-hop reasoning. In contrast, we focus on zero-\nand few-shot evaluation of commonsense under-\nstanding using the existing benchmarks.\n7 Conclusion\nWe conduct a systematic and rigorous study of large\nLM performance on a diverse set of commonsense\nbenchmarks, in a zero-shot and few-shot fashion.\nWhile pre-trained LMs can seemingly achieve a\ngood zero-shot performance on these benchmarks,\nthese results can be partially attributed to the LM’s\nability to exploit potential surface cues and annota-\ntion artefacts to guess the correct answer, without\nreasoning over the provided context. We further\nobserved that substantially increasing model size\nyields rather small improvements on most com-\nmonsense benchmarks: Based on the scaling plots,\nachieving human-level performance requires much\nlarger model sizes than what is currently feasible.\nIn addition, model performance can be highly sen-\nsitive to certain evaluation design choices. Overall,\nour findings offer valuable insights and best prac-\ntices for rigorously evaluating large LMs.\nEthical Considerations\nThe primary aim of this paper is to conduct a sys-\ntematic and rigorous commonsense evaluation of a\nlarge language model, which — in the case of this\nwork — is achieved by using the pre-trained Go-\npher language model (Rae et al., 2021) with 280B\nparameters. Hence, the same risks stemming from\nlarge language model research are also broadly ap-\nplicable to this work (Bender et al., 2021). We\nbriefly discuss these ethical considerations below.\nTraining compute. In practice, pre-training large\nlanguage models like Gopher requires an enormous\namount of compute, which may contribute to in-\ncreased carbon emissions (Strubell et al., 2019;\nPatterson et al., 2021). In this work, we do not pre-\ntrain the language model from scratch, although we\nacknowledge that conducting inference and evalua-\ntion with large language models like Gopher still\nhas substantial computational costs. Given the need\nto construct even-larger language models ( >100\ntrillion parameters) to achieve human-level perfor-\nmance on most of these benchmarks in an unsuper-\nvised fashion (§3.2), we encourage future work to\nfocus on potentially more efficient ways of acquir-\ning commonsense knowledge directly from data,\ne.g., through multi-modal learning, grounding, and\nhuman interaction (Bisk et al., 2020a).\nFairness and bias. Given the enormous size of\nthe pre-training data — about 2 trillion tokens in\nthe case of Gopher pre-training — it is conceivable\nthat the training dataset may inadvertently contain\ntoxic and biased material. Such toxic material —\nwhich is not always easily identifiable in the large\ntraining dataset — can in turn encourage the model\nto produce biased, harmful, or toxic output, es-\npecially when they are prompted with toxic text\n(Gehman et al., 2020). In fact, Rae et al. (2021)\ndemonstrated that — up to a certain model size\n— larger language models may respond to toxic\nprompts with greater toxicity compared to smaller\nones. Furthermore, the enormous size of the train-\ning data does not necessarily guarantee diversity:\nWe expect the training data to contain a smaller\nproportion of vernacular or regional English that is\nused by underrepresented communities (Blodgett\net al., 2016; Bender et al., 2021). Furthermore, the\nlanguage model may also acquire harmful biases\nand stereotypes, e.g., assign lower probabilities\nto women becoming doctors as opposed to men\n(Rudinger et al., 2018; Cao and Daumé III, 2021).\nLanguage model misuse. Our work highlights\nboth the success and limitations of large language\nmodels at multiple commonsense benchmarks.\nNevertheless, the success and expressive power\n11846\nof large language models come at the expense of\npotential misuse. Given their ability to generate\nrealistic-looking — albeit not necessarily factual\n— content, large language models can also be used\nfor malicious purposes. For instance, large lan-\nguage models can be used to generate convincing\nfake news (Zellers et al., 2019b), and more power-\nful generator can in turn generate even more con-\nvincing and influential fake news. Given the diffi-\nculty of manually distinguishing between human-\ngenerated text and machine-generated ones (Clark\net al., 2021), how we can better detect and defend\nagainst malicious use of large language models is\nan important and exciting avenue for future work.\nLimitations\nThere are limitations to this work: first, we only\nassessed models’ performance on multiple-choice\nquestions (and not in a generative setting). Mul-\ntiple choice problems have a more reliable auto-\nmatic metric; in contrast, metrics used for genera-\ntive tasks do not always accurately reflect human\njudgment (Clark et al., 2021) Second, we only eval-\nuate the benchmarks on one family of models, the\nGopher models and their variants; given the com-\nputational cost and also the lack of availability of\ndifferent large language models (LLM), we cannot\nrun our experiments on different model families\nthan Gopher. However, we include zero-shot re-\nsults on common-sense benchmarks from existing\nwork on other LLMs in the paper (such as the GPT2\nresult in Table 7). Moreover, LLMs behave very\nsimilarly on various benchmarks, and we expect\nour results to generalize to other LLMs as well.\nLast but not least, we only evaluate models that\nare solely trained on language. Recent multimodal\nmodels have shown impressive performance on a\nrange of tasks (Saharia et al., 2022). Will models\ntrained on multiple modalities have more common-\nsense? We aim to answer this question in future\nwork.\nAcknowledgments\nWe would like to thank Ivana Kaji´c, Laura Rimell\nfor their detailed comments on our paper. Also,\nthanks to Stella Biderman and the anonymous re-\nviewers for their helpful feedback. We also thank\nJack W. Rae and the other authors from the Gopher\npaper for providing efficient evaluation pipelines\nfor models from the Gopher family.\nReferences\nLisa Bauer and Mohit Bansal. 2021. Identify, align, and\nintegrate: Matching knowledge graphs to common-\nsense reasoning tasks. EACL.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? . In Proc. of FAccT.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Wen tau Yih, and Yejin\nChoi. 2020. Abductive commonsense reasoning. In\nInternational Conference on Learning Representa-\ntions.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020a.\nExperience grounds language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020b. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 7432–7439.\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor.\n2016. Demographic dialectal variation in social me-\ndia: A case study of African-American English. In\nProc. of EMNLP.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, S. Buch, D. Card, Ro-\ndrigo Castellon, Niladri S. Chatterji, Annie Chen,\nKathleen Creel, Jared Davis, Dora Demszky, Chris\nDonahue, Moussa Doumbouya, Esin Durmus, Ste-\nfano Ermon, John Etchemendy, Kawin Ethayarajh,\nLi Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E.\nGillespie, Karan Goel, Noah D. Goodman, Shelby\nGrossman, Neel Guha, Tatsunori Hashimoto, Peter\nHenderson, John Hewitt, Daniel E. Ho, Jenny Hong,\nKyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain,\nDan Jurafsky, Pratyusha Kalluri, Siddharth Karam-\ncheti, Geoff Keeling, Fereshte Khani, O. Khattab,\nPang Wei Koh, Mark S. Krass, Ranjay Krishna, Ro-\nhith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina\nLee, Tony Lee, Jure Leskovec, Isabelle Levent, Xi-\nang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik,\nChristopher D. Manning, Suvir P. Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika\nNarayan, Deepak Narayanan, Ben Newman, Allen\nNie, Juan Carlos Niebles, Hamed Nilforoshan, J. F.\nNyarko, Giray Ogut, Laurel Orr, Isabel Papadim-\nitriou, Joon Sung Park, Chris Piech, Eva Portelance,\nChristopher Potts, Aditi Raghunathan, Robert Re-\nich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani,\nCamilo Ruiz, Jackson K. Ryan, Christopher R’e,\n11847\nDorsa Sadigh, Shiori Sagawa, Keshav Santhanam,\nAndy Shih, Krishna Parasuram Srinivasan, Alex\nTamkin, Rohan Taori, Armin W. Thomas, Florian\nTramèr, Rose E. Wang, William Wang, Bohan Wu,\nJiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro\nYasunaga, Jiaxuan You, Matei A. Zaharia, Michael\nZhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang,\nLucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021.\nOn the opportunities and risks of foundation models.\nArXiv, abs/2108.07258.\nMichael Boratko, Xiang Lorraine Li, Rajarshi Das, Tim\nO’Gorman, Dan Le, and Andrew McCallum. 2020.\nProtoqa: A question answering dataset for prototypi-\ncal common-sense reasoning. EMNLP 2020.\nAntoine Bosselut, Ronan Le Bras, and Yejin Choi. 2021.\nDynamic neuro-symbolic knowledge graph construc-\ntion for zero-shot commonsense question answering.\nIn Proceedings of the 35th AAAI Conference on Arti-\nficial Intelligence (AAAI).\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Çelikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. In ACL.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nYang Trista Cao and Hal Daumé III. 2021. Toward\ngender-inclusive coreference resolution: An analysis\nof gender and bias throughout the machine learning\nlifecycle*. Computational Linguistics.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282–7296, Online.\nAssociation for Computational Linguistics.\nJoe Davison, Joshua Feldman, and Alexander M Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1173–1178.\nYanai Elazar, Hongming Zhang, Yoav Goldberg, and\nDan Roth. 2021. Back to square one: Bias detection,\ntraining and commonsense disentanglement in the\nwinograd schema. arXiv preprint arXiv:2104.08161.\nJohn H Flavell. 2004. Theory-of-mind development:\nRetrospect and prospect. Merrill-Palmer Quarterly\n(1982-), pages 274–290.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings of EMNLP.\nJonathan Gordon and Benjamin Van Durme. 2013. Re-\nporting bias and knowledge acquisition. In Proceed-\nings of the 2013 workshop on Automated knowledge\nbase construction, pages 25–30.\nDavid Gunning. 2018. Machine common sense concept\npaper. arXiv preprint arXiv:1810.07528.\nFabian Caba Heilbron, Victor Escorcia, Bernard\nGhanem, and Juan Carlos Niebles. 2015. Activitynet:\nA large-scale video benchmark for human activity\nunderstanding. In 2015 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n961–970.\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle\nPineau, Doina Precup, and David Meger. 2018. Deep\nreinforcement learning that matters. In Proc. of\nAAAI.\nAri Holtzman, Peter West, Vered Schwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form competi-\ntion: Why the highest probability answer isn’t always\nright. arXiv preprint arXiv:2104.08315.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos qa: Machine reading com-\nprehension with contextual commonsense reasoning.\nEMNLP, abs/1909.00277.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nPhilipp Koehn and Rebecca Knowles. 2017. Six\nchallenges for neural machine translation. In\nNMT@ACL.\nMahnaz Koupaee and William Yang Wang. 2018. Wiki-\nhow: A large scale text summarization dataset. arXiv\npreprint arXiv:1810.09305.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth International Conference on the Principles of\nKnowledge Representation and Reasoning.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProc. of ACL-IJCNLP.\n11848\nBill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil\nZaheer, Xiang Ren, and William W Cohen. 2021.\nDifferentiable open-ended commonsense reasoning.\nNAACL.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow NOT to evaluate your dialogue system: An em-\npirical study of unsupervised evaluation metrics for\ndialogue response generation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing.\nHugo Liu and Push Singh. 2004. Commonsense rea-\nsoning in and over natural language. In International\nConference on Knowledge-Based and Intelligent In-\nformation and Engineering Systems, pages 293–306.\nSpringer.\nNicholas Lourie, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Unicorn on rainbow: A\nuniversal commonsense reasoning model on a new\nmultitask benchmark. In AAAI.\nJohn McCarthy et al. 1960. Programs with common\nsense. RLE and MIT computation center.\nGábor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In Proc. of ICLR.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint.\nJekaterina Novikova, Ond ˇrej Dušek, Amanda Cer-\ncas Curry, and Verena Rieser. 2017. Why we need\nnew evaluation metrics for NLG. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In ACL.\nDavid A. Patterson, Joseph Gonzalez, Quoc V . Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid R. So, Maud Texier, and Jeff Dean. 2021.\nCarbon emissions and large neural network training.\nCoRR, abs/2104.10350.\nMostofa Patwary, Mohammad Shoeybi, Patrick LeGres-\nley, Shrimai Prabhumoye, Jared Casper, Vijay Ko-\nrthikanti, Vartika Singh, Julie Bernauer, Michael\nHouston, Bryan Catanzaro, Shaden Smith, Brandon\nNorick, Samyam Rajbhandari, Zhun Liu, George\nZerveas, Elton Zhang, Reza Yazdani Aminabadi, Xia\nSong, Yuxiong He, Jeffrey Zhu, Jennifer Cruzan,\nUmesh Madan, Luis Vargas, and Saurabh Tiwary.\n2021. Using deepspeed and megatron to train\nmegatron-turing nlg 530b, the world’s largest and\nmost powerful generative language model.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines for natural language infer-\nence. In The Seventh Joint Conference on Lexical\nand Computational Semantics (*SEM).\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-\ndhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake A. Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam S. Isaac, Edward Lockhart, Simon Osindero,\nLaura Rimell, Chris Dyer, Oriol Vinyals, Kareem\nAyoub, Jeff Stanway, Lorrayne Bennett, Demis Hass-\nabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021.\nScaling language models: Methods, analysis & in-\nsights from training gopher. CoRR, abs/2112.11446.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proc. of NAACL-HLT.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\nRapha Gontijo Lopes, et al. 2022. Photorealistic\ntext-to-image diffusion models with deep language\nunderstanding. arXiv preprint arXiv:2205.11487.\n11849\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-\nvatula, and Yejin Choi. 2020. Winogrande: An ad-\nversarial winograd schema challenge at scale. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 8732–8740.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019a.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 33, pages\n3027–3035.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019b. Socialiqa: Common-\nsense reasoning about social interactions. Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing.\nVered Shwartz and Yejin Choi. 2020. Do neural lan-\nguage models overcome reporting bias? In COLING.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, , and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nEMNLP.\nFelix Stahlberg and Bill Byrne. 2019. On NMT search\nerrors and model errors: Cat got your tongue? In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3354–\n3360, Hong Kong, China. Association for Computa-\ntional Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proc. of ACL.\nPaul Trichelair, Ali Emami, Adam Trischler, Kaheer\nSuleman, and Jackie Chi Kit Cheung. 2019. How\nreasonable are common-sense reasoning tasks: A\ncase-study on the Winograd schema challenge and\nSWAG. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3382–3387, Hong Kong, China. Association for Com-\nputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nNathaniel Weir, Adam Poliak, and Benjamin Van\nDurme. 2020. Probing neural language models for\nhuman tacit assumptions. arXiv: Computation and\nLanguage.\nDani Yogatama, Cyprien de Masson d’Autume, Jerome\nConnor, Tomas Kocisky, Mike Chrzanowski, Ling-\npeng Kong, Angeliki Lazaridou, Wang Ling, Lei\nYu, Chris Dyer, et al. 2019. Learning and evaluat-\ning general linguistic intelligence. arXiv preprint\narXiv:1901.11373.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. EMNLP.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019a. Hellaswag: Can\na machine really finish your sentence? In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019b. Defending against Neural Fake\nNews.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 34,\npages 9733–9740.\n11850\nA Appendix Structure\nWe begin by quantifying the scaling behavior of the\nmodel to predict how performance changes with\nlarger model sizes (Appendix B). We then plot\nthe relationship between cross-entropy and answer\nlength for each of the four datasets (Appendix C).\nAfter that, we describe experiments that use knowl-\nedge base triplets as a form of in-context learning\n(Appendix D). Lastly, in Appendix E, we provide\nqualitative examples that show which examples: (i)\nall model sizes get right, (ii) all model sizes get\nwrong, and (iii) only the larger models get right.\nB Scaling Behavior\nWhen we estimate the performance needed to reach\nhuman-level performance, we fit a linear model to\nestimate accuracy from log(params). We derive\nthe human performance from each respective pa-\nper and/or leaderboard. For HellaSwag and PIQA,\nhuman-level performance is at 95%. For Wino-\nGrande, it is at 94% and for Social IQa it is at 84%.\nOn HellaSwag, we predict that 1.4T parameters\nare needed to achieve human-level performance;\non PIQA we predict 102T parameters; on Wino-\nGrande we predict over 2000 Trillion parameters.\nSocial IQa scales particularly poorly, and we esti-\nmate over 1018 parameters being needed.\n11851\nC Cross-entropy vs answer length for all\ndatasets\n(a) Answer length vs cross-entropy (average log probabil-\nity across tokens) for PIQA.\n(b) Answer length vs cross-entropy (average log probabil-\nity across tokens) for SocialIQA.\n(a) Answer length vs cross-entropy (average log probabil-\nity across tokens) for HellaSW AG.\n(b) Answer length vs cross-entropy (average log probabil-\nity across tokens) for Winogrande.\n11852\nD Commonsense Knowledge Bases\nGiven the implicit nature of commonsense knowl-\nedge, a language model’s pretraining corpora might\nnot contain all of the supporting evidence that is\nrequired to answer commonsense understanding\nquestions — a phenomenon widely known as the\nreporting bias problem (Gordon and Van Durme,\n2013). Thus, prior work has proposed to use exter-\nnal knowledge bases for improving the zero-shot\nperformance of LMs on commonsense benchmarks\n(Bosselut et al., 2021; Bauer and Bansal, 2021).\nThese approaches are particularly interesting, as\nthe knowledge base augmentation only happens at\ntest time, rendering this approach compatible with\nany pretrained generative LM. While prior work\nhas shown the effectiveness of this approach over\nthe zero-shot baseline that lacks access to common-\nsense knowledge bases (CSKBs), we find that the\nperformance of the baseline model is highly sen-\nsitive to certain evaluation design choices (§5). A\nnatural question, therefore, is the following: If we\ncarefully optimize the evaluation design choices of\nthe baseline model, would we still observe similar\nimprovements through CSKB augmentation?\nSetup. To answer this, we replicate prior work\nby adding commonsense knowledge base entries at\ntest time; such knowledge base triplets can poten-\ntially provide the relevant implicit commonsense\nknowledge that makes the correct answer more\nlikely than the rest. To ensure the generality of our\nfindings, we apply this approach to multiple model\nsizes that we explored in §3.2. Here we consider\nthe pre-extracted knowledge base triplets that are\nmade publicly available by Shwartz et al. (2020).\nWe use a similar score function as Shwartz et al.\n(2020), where, for each answer choice y ∈Y (x),\nwe choose the knowledge base triplet that yields\nthe highest score:12\nskg(y|x) ≜\n∑\nt∈T\ns(y; t|x) ≈maxt∈T s(y; t|x),\nwhere s(y; t|x) denotes the cross-entropy of the\nconcatenated answer choice y and the extracted\nknowledge base triplet t, conditional on the ques-\ntion/context x. Here T denotes the set of all ex-\ntracted commonsense knowledge triplets, which\nare generated from Comet (Bosselut et al., 2019).\n12We experimented with other score functions, such as ap-\npending the extracted knowledge base triplets to the question\ninstead of the answer, although this approach does not yield\nbetter results than the one proposed by Shwartz et al. (2020).\nZS w/t Comet w/t Atomic w/t CN\n44M 42.3 42.9 42.3 40.6\n117M 43.6 44.0 43.6 42.2\n400M 46.3 46.8 44.7 44.1\n1.3B 47.0 46.8 46.4 44.7\n7B 48.5 48.6 47.5 46.1\nZS w/t Comet Self-Talk\nGPT2 41.113 47.5 46.2\nTable 7: Zero-shot performance on Social IQa when us-\ning different knowledge bases. GPT2 results are taken\nfrom Shwartz et al. (2020). ZS: zero-shot performance;\nCN: ConceptNet. We do not include the Gopher results\n— with 280B parameters — due to computational con-\nsiderations and much slower inference.\nOne key difference is that we score the answer and\nknowledge base triplet conditional on the question,\nwhereas Shwartz et al. (2020) scored the concate-\nnation of question, answer, and triplet instead.\nIn Table 7, we summarize our results on Social\nIQa, which has the highest gap between the zero-\nshot and SOTA performance (Fig. 2). We compare\nour results with those of Shwartz et al. (2020), who\nused GPT2 as the base model. Our results in Ta-\nble 7 provide an interesting contrast to the find-\nings of Shwartz et al. (2020): Our baseline zero-\nshot model with 1.3B parameters achieves an accu-\nracy of 47.0% on Social IQa, substantially outper-\nforming the reported GPT2 result of Shwartz et al.\n(2020) — which achieves 41.1% — despite the fact\nthat GPT2 has more parameters (1.5B vs our 1.3B).\nIn fact, the same 1.3B zero-shot model — which\ndoes not benefit from any commonsense knowl-\nedge base triplets — nearly matches the perfor-\nmance of GPT2 augmented with Comet (Bosselut\net al., 2019) (47.0% for our zero-shot 1.3B model\nvs 47.5% for GPT2 augmented with COMET; Ta-\nble 7), and also outperforms the GPT2 model that\nis augmented with self-talk. Nevertheless, we find\nthat adding knowledge base triplets fails to yield\nsubstantial improvements for our models; this find-\ning is consistent across three different knowledge\nbases and five model sizes. On the contrary, adding\nsuch knowledge base triplets can occasionally de-\ncrease performance compared to the zero-shot base-\nline.\nWe remark on two significant aspects of our find-\nings. First, it is important to compare proposed\nimprovements against strong, well-tuned baselines\n13By similarly tuning the evaluation design choices, we\nachieved 46.7 when evaluating GPT2 in the zero-shot setting.\n11853\n(Henderson et al., 2018; Melis et al., 2018), which\ncan achieve surprisingly competitive performance.\nWe identify the choice of the scored span as a partic-\nularly important design choice: Whereas Shwartz\net al. (2020) scored the GPT2 model on the con-\ncatenation of both question and answer, we instead\ncalculate the cross-entropy of the answer given the\nquestion. Second, certain improvements that are\nobserved under a particular set of evaluation design\nchoices may not necessarily be replicated under a\ndifferent set. This finding reiterates the importance\nof explicitly stating the evaluation design choices\nused in each experiment, and identifying whether\nor not the observed improvements are robust across\ndifferent evaluation design choices (§5).\nE Examples\nE.1 Social IQa\nAll Models Incorrect\n{ ’ c o n t e x t ’ : \" Tracy didn ’ t go home\nt h a t e v e n i n g and r e s i s t e d\nR i l e y ’ s a t t a c k s . \" ,\n’ q u e s t i o n ’ : ’ What does Tracy need\nt o do b e f o r e t h i s ? ’ ,\n’ answerA ’ : ’ make a new plan ’ ,\n’ answerB ’ : ’Go home and s e e R i l e y\n’ ,\n’ answerC ’ : ’ Find somewhere t o go\n’ ,\n’ c o r r e c t ’ : ’C’ }\n{ ’ c o n t e x t ’ : ’ Aubrey k e p t t h e baby\nup a t n i g h t t o watch f o r a\nc o n c u s s i o n . ’ ,\n’ q u e s t i o n ’ : ’ What w i l l happen t o\nAubrey ? ’ ,\n’ answerA ’ : \" The baby f e l l a s l e e p\nd e s p i t e Aubrey ’ s b e s t e f f o r t \" ,\n’ answerB ’ : ’ g e t s so s l e e p y b u t\ns t a y s awake anyway ’ ,\n’ answerC ’ : ’ and t h e baby b o t h\nf e l l a s l e e p l a t e i n t h e n i g h t\n’ ,\n’ c o r r e c t ’ : ’B’ }\nAll Models Correct\n{ ’ c o n t e x t ’ : ’ K e n d a l l opened t h e i r\nmouth t o s p e a k and what came\no u t shocked e v e r y o n e . ’ ,\n’ q u e s t i o n ’ : ’How would you\nd e s c r i b e K e n d a l l ? ’ ,\n’ answerA ’ : ’ a v e r y q u i e t person ’ ,\n’ answerB ’ : ’ a v e r y p a s s i v e person\n’ ,\n’ answerC ’ : ’ a v e r y a g g r e s s i v e and\nt a l k a t i v e person ’ ,\n’ c o r r e c t ’ : ’C’ }\n{ ’ c o n t e x t ’ : ’ Sydney went t o our\nf a m i l y farm , t a k i n g t h e t r a s h\nw i t h her , and s e t i t on f i r e\non t h e ground . ’ ,\n’ q u e s t i o n ’ : ’How would Sydney\nf e e l a f t e r w a r d s ? ’ ,\n’ answerA ’ : ’ f e e l i n g s t r o n g ’ ,\n’ answerB ’ : ’ b u r n i n g down ’ ,\n’ answerC ’ : ’ u p s e t b e c a u s e t h e\nf i r e has g o t t e n o u t o f c o n t r o l\n’ ,\n’ c o r r e c t ’ : ’C’ }\n{ ’ c o n t e x t ’ : ’ Robin always g e t s\np i z z a on t h e way home from\nwork f o r h e r f a m i l y on F r i d a y s\n. ’ ,\n’ q u e s t i o n ’ : ’ What w i l l Robin want\nt o do n e x t ? ’ ,\n’ answerA ’ : ’ p i c k up t h e p i z z a ’ ,\n’ answerB ’ : ’ c o m p l a i n t o t h e\no t h e r s ’ ,\n’ answerC ’ : ’ f i n i s h work ’ ,\n’ c o r r e c t ’ : ’A’ }\nLarger Models CorrectThe 1.4B, 7.1B, and\n280B model all got the following correct:\n{ ’ c o n t e x t ’ : ’ Alex p a i d e x t r a\nmoney t o g e t more s e c r e t\nd e t a i l s a b o u t t h e game\ns t r a t e g y . ’ ,\n’ q u e s t i o n ’ : ’ What w i l l Alex want\nt o do n e x t ? ’ ,\n’ answerA ’ : ’ p l a y t h e game more ’ ,\n’ answerB ’ : ’ i g n o r e t h e a d v i c e ’ ,\n’ answerC ’ : ’ s t o p p l a y i n g t h e\nv i d e o game ’ ,\n’ c o r r e c t ’ : ’A’ }\nThe 417M, 7.1B, and 280B model all got the fol-\nlowing correct:\n{ ’ c o n t e x t ’ : ’ Kai and S k y l a r were\ngood f r i e n d s . Kai had f i n a l l y\nworked up t h e c o u r a g e t o ask\nS k y l a r on a d a t e . They gave\n11854\nS k y l a r a m e a n i n g f u l g i f t t o\nt e s t t h e w a t e r s . ’ ,\n’ q u e s t i o n ’ : ’ What w i l l Kai want\nt o do n e x t ? ’ ,\n’ answerA ’ : ’ say t h a n k you f o r t h e\ng i f t ’ ,\n’ answerB ’ : ’ Find o u t w h e t h e r\nS k y l a r r e c i p r o c a t e s t h e\nf e e l i n g s ’ ,\n’ answerC ’ : \" T e l l S k y l a r they ’ d\nl i k e t o j u s t be f r i e n d s \" ,\n’ c o r r e c t ’ : ’B’ }\nE.2 WinoGrande\nAll Models Incorrect\n{ ’ l a b e l ’ : 1 ,\n’ o p t i o n 1 ’ : ’ Tanya ’ ,\n’ o p t i o n 2 ’ : ’ Sarah ’ ,\n’ s e n t e n c e ’ : ’ Tanya was\nu n r e c o g n i z a b l e a f t e r S a r a h\nwas done b e a t i n g them , so _\nended up g o i n g t o j a i l . ’ }\n{ ’ l a b e l ’ : 1 ,\n’ o p t i o n 1 ’ : ’ Logan ’ ,\n’ o p t i o n 2 ’ : ’ J u s t i n ’ ,\n’ s e n t e n c e ’ : ’ A f t e r Logan p i t c h e d\na b a l l t h a t g o t c l o b b e r e d\nf o r a home run by J u s t i n i n a\nb a s e b a l l game , _ f e l t\ne x u l t a n t . ’ }\nAll Models Correct\n{ ’ l a b e l ’ : 1 ,\n’ o p t i o n 1 ’ : ’ s a u s a g e ’ ,\n’ o p t i o n 2 ’ : ’ b a l l ’ ,\n’ s e n t e n c e ’ : b ’ When t h e dog\nb e h a v e s I l i k e t o g i v e him a\ns a u s a g e o t h e r w i s e I g i v e him\na b a l l . I gave him t h e _\ns i n c e he was bad . ’ }\n{ ’ l a b e l ’ : 1 ,\n’ o p t i o n 1 ’ : ’ Kayla ’ ,\n’ o p t i o n 2 ’ : ’ N a t a l i e ’ ,\n’ s e n t e n c e ’ : ’ Kayla always wears\ns u n s c r e e n o u t d o o r s b u t\nN a t a l i e doesn ’ t b e c a u s e _ i s n\n’ t c o n c e r n e d a b o u t g e t t i n g\nneck w r i n k l e s . ’ }\nOnly Large Models CorrectModels 400M and\nlarger got the following correct:\n{ ’ l a b e l ’ : 0 ,\n’ o p t i o n 1 ’ : ’ Nick ’ ,\n’ o p t i o n 2 ’ : ’ Ryan ’ ,\n’ s e n t e n c e ’ : ’ Nick d i d n o t l i k e\ns a u c e s made from tomato , o n l y\ncreamy s a u c e s . Ryan knew\nt h i s so he o n l y made w h i t e\ns a u c e when _ came o v e r . ’ }\nModels 1.4B and larger got the following correct:\n{ ’ l a b e l ’ : 0 ,\n’ o p t i o n 1 ’ : ’Adam ’ ,\n’ o p t i o n 2 ’ : ’ Jason ’ ,\n’ s e n t e n c e ’ : ’Adam l o v e d dogs b u t\nJ a s o n was a f r a i d o f them , so\no n l y _ p e t t e d t h e p o o d l e . ’ }\n11855",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6666498184204102
    },
    {
      "name": "Natural language processing",
      "score": 0.5740638971328735
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.521104633808136
    },
    {
      "name": "Natural language",
      "score": 0.488934189081192
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.4813533425331116
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4772194027900696
    },
    {
      "name": "Cognitive science",
      "score": 0.36607903242111206
    },
    {
      "name": "Knowledge extraction",
      "score": 0.23931720852851868
    },
    {
      "name": "Psychology",
      "score": 0.14497143030166626
    }
  ]
}