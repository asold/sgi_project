{
  "title": "Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model",
  "url": "https://openalex.org/W2741045570",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2741739164",
      "name": "Paria Jamshid Lou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097308019",
      "name": "Mark Johnson",
      "affiliations": [
        "Macquarie University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2169058332",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2250330117",
    "https://openalex.org/W2250355711",
    "https://openalex.org/W2293765256",
    "https://openalex.org/W2203858612",
    "https://openalex.org/W3207342693",
    "https://openalex.org/W2989792753",
    "https://openalex.org/W2130300813",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2154118774",
    "https://openalex.org/W2214962597",
    "https://openalex.org/W2963729456",
    "https://openalex.org/W2406344726",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3157159878",
    "https://openalex.org/W2021208044",
    "https://openalex.org/W1993617199",
    "https://openalex.org/W2566299766",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2400986993",
    "https://openalex.org/W2161274063"
  ],
  "abstract": "This paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model. The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis. The LSTM language model scores, along with other features, are used in a MaxEnt reranker to identify the most plausible analysis. We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the state-of-the-art in disfluency detection.",
  "full_text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 547–553\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2087\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 547–553\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2087\nDisﬂuency Detection using a Noisy Channel Model and a Deep Neural\nLanguage Model\nParia Jamshid Lou\nDepartment of Computing\nMacquarie University\nSydney, Australia\nparia.jamshid-lou@hdr.mq.edu.au\nMark Johnson\nDepartment of Computing\nMacquarie University\nSydney, Australia\nmark.johnson@mq.edu.au\nAbstract\nThis paper presents a model for disﬂu-\nency detection in spontaneous speech tran-\nscripts called LSTM Noisy Channel Model.\nThe model uses a Noisy Channel Model\n(NCM) to generate n-best candidate dis-\nﬂuency analyses and a Long Short-Term\nMemory (LSTM) language model to score\nthe underlying ﬂuent sentences of each\nanalysis. The LSTM language model\nscores, along with other features, are used\nin a MaxEnt reranker to identify the most\nplausible analysis. We show that using\nan LSTM language model in the reranking\nprocess of noisy channel disﬂuency model\nimproves the state-of-the-art in disﬂuency\ndetection.\n1 Introduction\nDisﬂuency is a characteristic of spontaneous\nspeech which is not present in written text.\nDisﬂuencies are informally deﬁned as interrup-\ntions in the normal ﬂow of speech that occur\nin different forms, including false starts, correc-\ntions, repetitions and ﬁlled pauses. According to\nShriberg’s (1994) deﬁnition, the basic pattern of\nspeech disﬂuencies contains three parts: reparan-\ndum1, interregnum and repair. Example 1 illus-\ntrates a disﬂuent structure, where the reparandum\nto Boston is the part of the utterance that is re-\nplaced, the interregnum uh, I mean is an optional\npart of a disﬂuent structure that consists of a ﬁlled\npause uh and a discourse markerI mean and the re-\npair to Denver replaces the reparandum. The ﬂu-\nent version of Example 1 is obtained by deleting\n1Reparandum is sometimes called edit.\nreparandum and interregnum words.\nI want a ﬂight\nreparandum\n  \nto Boston,\nuh, I mean  \ninterregnum\nto Denver  \nrepair\non Friday (1)\nWhile disﬂuency rate varies with the context,\nage and gender of speaker, Bortfeld et al. (2001)\nreported disﬂuencies once in every 17 words.\nSuch frequency is high enough to reduce the read-\nability of speech transcripts. Moreover, disﬂuen-\ncies pose a major challenge to natural language\nprocessing tasks, such as dialogue systems, that\nrely on speech transcripts (Ostendorf et al., 2008).\nSince such systems are usually trained on ﬂuent,\nclean corpora, it is important to apply a speech\ndisﬂuency detection system as a pre-processor to\nﬁnd and remove disﬂuencies from input data. By\ndisﬂuency detection, we usually mean identifying\nand deleting reparandum words. Filled pauses and\ndiscourse markers belong to a closed set of words,\nso they are trivial to detect (Johnson and Charniak,\n2004).\nIn this paper, we introduce a new model for de-\ntecting restart and repair disﬂuencies in sponta-\nneous speech transcripts calledLSTM Noisy Chan-\nnel Model (LSTM-NCM). The model uses a Noisy\nChannel Model (NCM) to generate n-best candi-\ndate disﬂuency analyses, and a Long Short-Term\nMemory (LSTM) language model to rescore the\nNCM analyses. The language model scores are\nused as features in a MaxEnt reranker to select the\nmost plausible analysis. We show that this novel\napproach improves the current state-of-the-art.\n2 Related Work\nApproaches to disﬂuency detection task fall into\nthree main categories: sequence tagging, parsing-\nbased and noisy channel model. The sequence\n547\ntagging models label words as ﬂuent or disﬂuent\nusing a variety of different techniques, including\nconditional random ﬁelds (Ostendorf and Hahn,\n2013; Zayats et al., 2014; Ferguson et al., 2015),\nhidden Markov models (Liu et al., 2006; Schuler\net al., 2010) or recurrent neural networks (Hough\nand Schlangen, 2015; Zayats et al., 2016). Al-\nthough sequence tagging models can be easily\ngeneralized to a wide range of domains, they\nrequire a speciﬁc state space for disﬂuency de-\ntection, such as begin-inside-outside (BIO) style\nstates that label words as being inside or outside of\na reparandum word sequence. The parsing-based\napproaches refer to parsers that detect disﬂuen-\ncies, as well as identifying the syntactic structure\nof the sentence (Rasooli and Tetreault, 2013; Hon-\nnibal and Johnson, 2014; Yoshikawa et al., 2016).\nTraining a parsing-based model requires large an-\nnotated tree-banks that contain both disﬂuencies\nand syntactic structures. Noisy channel models\n(NCMs) use the similarity between reparandum\nand repair as an indicator of disﬂuency. However,\napplying an effective language model (LM) inside\nan NCM is computationally complex. To allevi-\nate this problem, some researchers use more effec-\ntive LMs to rescore the NCM disﬂuency analyses.\nJohnson and Charniak (2004) applied a syntactic\nparsing-based LM trained on the ﬂuent version of\nthe Switchboard corpus to rescore the disﬂuency\nanalyses. Zwarts and Johnson (2011) trained ex-\nternal n-gram LMs on a variety of large speech\nand non-speech corpora to rank the analyses. Us-\ning the external LM probabilities as features into\nthe reranker improved the baseline NCM (Johnson\nand Charniak, 2004). The idea of applying exter-\nnal language models in the reranking process of\nthe NCM motivates our model in this work.\n3 LSTM Noisy Channel Model\nWe follow Johnson and Charniak (2004) in us-\ning an NCM to ﬁnd the n-best candidate disﬂu-\nency analyses for each sentence. The NCM, how-\never, lacks an effective language model to capture\nmore complicated language structures. To over-\ncome this problem, our idea is to use different\nLSTM language models to score the underlying\nﬂuent sentences of the analyses proposed by the\nNCM and use the language model scores as fea-\ntures to a MaxEnt reranker to select the best anal-\nysis. In the following, we describe our model and\nits components in detail.\nIn the NCM of speech disﬂuency, we assume\nthat there is a well-formed source utterance X to\nwhich some noise is added and generates a disﬂu-\nent utterance Y as follows.\nX = a ﬂight to Denver\nY = a ﬂight to Boston uh I mean to Denver\nGivenY , the goal of the NCM is to ﬁnd the most\nlikely source sentence ˆX such that:\nˆX = argmax\nX\nP(Y |X)P(X) (2)\nAs shown in Equation 2, the NCM contains two\ncomponents: the channel model P(Y |X) and the\nlanguage model P(X). Calculating the channel\nmodel and language model probabilities, the NCM\ngenerates 25-best candidate disﬂuency analyses as\nfollows.\n(3)\nExample 3 shows sample outputs of the NCM,\nwhere potential reparandum words are speciﬁed\nwith strikethrough text. The MaxEnt reranker is\napplied on the candidate analyses of the NCM to\nselect the most plausible one.\n3.1 Channel Model\nWe assume thatX is a substring ofY , so the source\nsentence X is obtained by deleting words from\nY . For each sentence Y , there are only a ﬁnite\nnumber of potential source sentences. However,\nwith the increase in the length of Y , the number\nof possible source sentences X grows exponen-\ntially, so it is not feasible to do exhaustive search.\nMoreover, since disﬂuent utterances may contain\nan unbounded number of crossed dependencies,\na context-free grammar is not suitable for ﬁnding\nthe alignments. The crossed dependencies refer to\nthe relation between repair and reparandum words\nwhich are usually the same or very similar words\nin roughly the same order as in Example 4.\n(4)\nWe apply a Tree Adjoining Grammar (TAG)\nbased transducer (Johnson and Charniak, 2004)\n548\nwhich is a more expressive formalism and pro-\nvides a systematic way of formalising the chan-\nnel model. The TAG channel model encodes the\ncrossed dependencies of speech disﬂuency, rather\nthan reﬂecting the syntactic structure of the sen-\ntence. The TAG transducer is effectively a simple\nﬁrst-order Markov model which generates each\nword in the reparandum conditioned on the pre-\nceding word in the reparandum and the corre-\nsponding word in the repair. Further detail about\nthe TAG channel model can be found in (Johnson\nand Charniak, 2004).\n3.2 Language Model\nThe language model of the NCM evaluates the\nﬂuency of the sentence with disﬂuency removed.\nThe language model is expected to assign a very\nhigh probability to a ﬂuent sentence X (e.g. a\nﬂight to Denver) and a lower probability to a sen-\ntence Y which still contains disﬂuency (e.g. a\nﬂight to Boston uh I mean to Denver ). However,\nit is computationally complex to use an effective\nlanguage model within the NCM. The reason is\nthe polynomial-time dynamic programming pars-\ning algorithms of TAG can be used to search for\nlikely repairs if they are used with simple language\nmodels such as a bigram LM (Johnson and Char-\nniak, 2004). The bigram LM within the NCM\nis too simple to capture more complicated lan-\nguage structure. In order to alleviate this problem,\nwe follow Zwarts and Johnson (2011) by training\nLMs on different corpora, but we apply state-of-\nthe-art recurrent neural network (RNN) language\nmodels.\nLSTM\nWe use a long short-term memory (LSTM) neu-\nral network for training language models. LSTM\nis a particular type of recurrent neural net-\nworks which has achieved state-of-the-art perfor-\nmance in many tasks including language mod-\nelling (Mikolov et al., 2010; Jozefowicz et al.,\n2016). LSTM is able to learn long dependencies\nbetween words, which can be highly beneﬁcial for\nthe speech disﬂuency detection task. Moreover, it\nallows for adopting a distributed representation of\nwords by constructing word embedding (Mikolov\net al., 2013).\nWe train forward and backward (i.e. input sen-\ntences are given in reverse order) LSTM language\nmodels using truncated backpropagation through\ntime algorithm (Rumelhart et al., 1986) with mini-\nbatch size 20 and total number of epochs 13.\nThe LSTM model has two layers and 200 hidden\nunits. The initial learning rate for stochastic gra-\ndient optimizer is chosen to 1 which is decayed by\n0.5 for each epoch after maximum epoch 4. We\nlimit the maximum sentence length for training\nour model due to the high computational complex-\nity of longer histories in the LSTM. In our exper-\niments, considering maximum 50 words for each\nsentence leads to good results. The size of word\nembedding is 200 and it is randomly initialized for\nall LSTM LMs2.\nUsing each forward and backward LSTM lan-\nguage model, we assign a probability to the under-\nlying ﬂuent parts of each candidate analysis.\n3.3 Reranker\nIn order to rank the the 25-best candidate disﬂu-\nency analyses of the NCM and select the most\nsuitable one, we apply the MaxEnt reranker pro-\nposed by Johnson et al. (2004). We use the fea-\nture set introduced by Zwarts and Johnson (2011),\nbut instead of n-gram scores, we apply the LSTM\nlanguage model probabilities. The features are so\ngood that the reranker without any external lan-\nguage model is already a state-of-the-art system,\nproviding a very strong baseline for our work.\nThe reranker uses both model-based scores (in-\ncluding NCM scores and LM probabilities) and\nsurface pattern features (which are boolean in-\ndicators) as described in Table 1. Our reranker\noptimizes the expected f-score approximation de-\nscribed in Zwarts and Johnson (2011) with L2 reg-\nularisation.\n4 Corpora for Language Modelling\nIn this work, we train forward and backward\nLSTM language models on Switchboard (Godfrey\nand Holliman, 1993) and Fisher (Cieri et al., 2004)\ncorpora. Fisher consists of 2 .2 ×107 tokens of\ntranscribed text, but disﬂuencies are not annotated\nin it. Switchboard is the largest available cor-\npus (1 .2 ×106 tokens) in which disﬂuencies are\nannotated according to Shriberg’s (1994) scheme.\nSince the bigram language model of the NCM\nis trained on this corpus, we cannot directly use\nSwitchboard to build LSTM LMs. The reason is\nthat if the training data of Switchboard is used both\nfor predicting language ﬂuency and optimizing the\nloss function, the reranker will overestimate the\n2All code is written in TensorFlow (Abadi et al., 2015)\n549\nmodel-based features\n1-2. forward & backward LSTM LM scores\n3-7. log probability of the entire NCM\n8. sum of the log LM probability & the log\nchannel model probability plus number of ed-\nits in the sentence\n9. channel model probability\nsurface pattern features\n10. CopyFlags X Y: if there is an exact copy in\nthe input text of length X (1 ≤X ≤3) and the\ngap between the copies is Y (0 ≤Y ≤3)\n11. WordsFlags L n R: number of ﬂags to the\nleft (L) and to the right (R) of a 3-gram area\n(0 ≤L, R ≤1)\n12. SentenceEdgeFlags B L: it captures the lo-\ncation and length of disﬂuency. The Boolean B\nsentence initial or sentence ﬁnal disﬂuency, L\n(1 ≤L ≤3) records the length of the ﬂags.\nTable 1: The features used in the reranker. They,\nexcept for the ﬁrst and second one, were applied\nby Zwarts and Johnson (2011).\nweight related to the LM features extracted from\nSwitchboard. This is because the ﬂuent sentence\nitself is part of the language model (Zwarts and\nJohnson, 2011). As a solution, we apply a k-fold\ncross-validation (k = 20) to train the LSTM lan-\nguage models when using Switchboard corpus.\nWe follow Charniak and Johnson (2001) in\nsplitting Switchboard corpus into training, devel-\nopment and test set. The training data consists of\nall sw[23] ∗.dps ﬁles, development training con-\nsists of all sw4[5-9] ∗.dps ﬁles and test data con-\nsists of all sw4[0-1]∗.dps ﬁles. Following Johnson\nand Charniak (2004), we remove all partial words\nand punctuation from the training data. Although\npartial words are very strong indicators of disﬂu-\nency, standard speech recognizers never produce\nthem in their outputs, so this makes our evaluation\nboth harder and more realistic.\n5 Results and Discussion\nWe assess the proposed model for disﬂuency de-\ntection with all MaxEnt features described in Ta-\nble 1 against the baseline model. The noisy chan-\nnel model with exactly the same reranker features\nexcept the LSTM LMs forms the baseline model.\nTo evaluate our system, we use two metrics\nf-score and error rate . Charniak and John-\nson (2001) used the f-score of labelling reparanda\nor “edited” words, while Fiscus et al (2004) de-\nﬁned an “error rate” measure, which is the number\nof words falsely labelled divided by the number\nof reparanda words. Since only 6% of words are\ndisﬂuent in Switchboard corpus, accuracy is not a\ngood measure of system performance. F-score, on\nthe other hand, focuses more on detecting “edited”\nwords, so it is a decent metric for highly skewed\ndata.\nAccording to Tables 2 and 3, the LSTM noisy\nchannel model outperforms the baseline. The\nexperiment on Switchboard and Fisher corpora\ndemonstrates that the LSTM LMs provide infor-\nmation about the global ﬂuency of an analysis that\nthe local features of the reranker do not capture.\nThe LSTM language model trained on Switch-\nboard corpus results in the greatest improvement.\nSwitchboard is in the same domain as the test\ndata and it is also disﬂuency annotated. Either\nor both of these might be the reason why Switch-\nboard seems to be better in comparison with Fisher\nwhich is a larger corpus and might be expected\nto make a better language model. Moreover,\nthe backward LSTMs have better performance in\ncomparison with the forward ones. It seems when\nsentences are fed in reverse order, the model can\nmore easily detect the unexpected word order as-\nsociated with the reparandum to detect disﬂuen-\ncies. In other words, that the disﬂuency is ob-\nserved “after” the ﬂuent repair in a backward lan-\nguage model is helpful for recognizing disﬂuen-\ncies.\nbaseline 85.3\ncorpus forward backward both\nSwitchboard 86.1 86.6 86.8\nFisher 86.2 86.5 86.3\nTable 2: F-scores on the dev set for a variety of\nLSTM language models.\nbaseline 27.0\ncorpus forward backward both\nSwitchboard 25.5 24.8 24.3\nFisher 25.6 25.0 25.3\nTable 3: Expected error rates on the dev set for a\nvariey of LSTM language models.\nWe compare the performance of Kneser-Ney\n550\nsmoothed 4-gram language models with the\nLSTM corresponding on the reranking process of\nthe noisy channel model. We estimate the 4-\ngram models and assign probabilities to the ﬂu-\nent parts of disﬂueny analyses using the SRILM\ntoolkit (Stolcke, 2002). As Tables 4 and 5 show\nincluding scores from a conventional 4-gram lan-\nguage model does not improve the model’s abil-\nity to ﬁnd disﬂuencies, suggesting that the LSTM\nmodel contains all the useful information that the\n4-gram model does. In order to give a more gen-\neral idea on the performance of LSTM over stan-\ndard LM, we evaluate our model when the lan-\nguage model scores are used as the only features\nof the reranker. The f-score for the NCM alone\nwithout applying the reranker is 78.7, while using\n4-gram language model scores in the reranker in-\ncreases the f-score to 81 .0. Replacing the 4-gram\nscores with LSTM language model probabilities\nleads to further improvement, resulting an f-score\n82.3.\nbaseline 85.3\ncorpus 4-gram LSTM both\nSwitchboard 85.1 86.8 86.1\nFisher 85.6 86.3 86\nTable 4: F-score for 4-gram, LSTM and combina-\ntion of both language models.\nbaseline 27.0\ncorpus 4-gram LSTM both\nSwitchboard 27.5 24.3 26\nFisher 26.6 25.3 26\nTable 5: Expected error rates for 4-gram, LSTM\nand combination of both language models.\nWe also compare our best model on the develop-\nment set to the state-of-the-art methods in the liter-\nature. As shown in Table 6, the LSTM noisy chan-\nnel model outperforms the results of prior work,\nachieving a state-of-the-art performance of 86 .8.\nIt also has better performance in comparison with\nFerguson et al. (2015) and Zayat et al.’s (2016)\nmodels, even though they use richer input that in-\ncludes prosodic features or partial words.\n6 Conclusion and Future Work\nIn this paper, we present a new model for dis-\nﬂuency detection from spontaneous speech tran-\nModel f-score\nYoshikawa et al. (2016) 62.5\nJohnson and Charniak (2004) 79.7\nJohnson et al. (2004) 81.0\nRasooli and Tetreault (2013) 81.4\nQian and Liu (2013) 82.1\nHonnibal and Johnson (2014) 84.1\nFerguson et al. (2015) * 85.4\nZwarts and Johnson (2011) 85.7\nZayats et al. (2016) * 85.9\nLSTM-NCM 86.8\nTable 6: Comparison of the LSTM-NCM to state-\nof-the-art methods on the dev set. *Models have\nused richer input.\nscripts. It uses a long short-term memory neural\nnetwork language model to rescore the candidate\ndisﬂuency analyses produced by a noisy channel\nmodel. The LSTM language model scores as fea-\ntures in a MaxEnt reranker improves the model’s\nability to detect restart and repair disﬂuencies. The\nmodel outperforms other models reported in the\nliterature, including models that exploit richer in-\nformation from the input. As future work, we ap-\nply more complex LSTM language models such\nas sequence-to-sequence on the reranking process\nof the noisy channel model. We also intend to in-\nvestigate the effect of integrating LSTM language\nmodels into other kinds of disﬂuency detection\nmodels, such as sequence labelling and parsing-\nbased models.\nAcknowledgements\nWe would like to thank the anonymous review-\ners for their insightful comments and sugges-\ntions. This research was supported by a Google\naward through the Natural Language Understand-\ning Focused Program, and under the Australian\nResearch Councils Discovery Projects funding\nscheme (project number DP160102156).\nReferences\nMartin Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Andrew Harp,\nGeoffrey Irving, Michael Isard, Yangqing Jia, Rafal\nJozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\nLevenberg, Dan Mane, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\n551\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-\nwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-\nvan, Fernanda Viegas, Oriol Vinyals, Pete Warden,\nMartin Wattenberg, Martin Wicke, Yuan Yu, and Xi-\naoqiang Zheng. 2015. TensorFlow: Large-scale ma-\nchine learning on heterogeneous systems. Software\navailable from tensorﬂow.org.\nHeather Bortfeld, Silvia Leon, Jonathan Bloom,\nMichael Schober, and Susan Brennan. 2001. Disﬂu-\nency rates in conversation: Effects of age, relation-\nship, topic, role, and gender. Language and Speech\n44(2):123–147.\nEugene Charniak and Mark Johnson. 2001. Edit\ndetection and parsing for transcribed speech. In\nProceedings of the 2nd Meeting of the North\nAmerican Chapter of the Association for Com-\nputational Linguistics on Language Technologies .\nStroudsburg, USA, NAACL’01, pages 118–126.\nhttp://aclweb.org/anthology/N01-1016.\nChristopher Cieri, David Miller, and Kevin Walker.\n2004. Fisher English training speech part 1 tran-\nscripts LDC2004T19. Published by: Linguistic\nData Consortium, Philadelphia, USA.\nJames Ferguson, Greg Durrett, and Dan Klein. 2015.\nDisﬂuency detection with a semi-Markov model and\nprosodic features. In Proceedings of the Confer-\nence of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies. Denver, USA, NAACL’15,\npages 257–262.\nJonathan Fiscus, John Garofolo, Audrey Le, Alvin\nMartin, David Pallet, Mark Przybocki, and Greg\nSanders. 2004. Results of the fall 2004 STT and\nMDE evaluation. In Proceedings of Rich Transcrip-\ntion Fall Workshop.\nJohn Godfrey and Edward Holliman. 1993.\nSwitchboard-1 release 2 LDC97S62. Published by:\nLinguistic Data Consortium, Philadelphia, USA.\nMatthew Honnibal and Mark Johnson. 2014. Joint\nincremental disﬂuency detection and depen-\ndency parsing. Transactions of the Association\nfor Computational Linguistics 2(1):131–142.\nhttp://www.aclweb.org/anthology/Q14-1011.\nJulian Hough and David Schlangen. 2015. Recurrent\nneural networks for incremental disﬂuency detec-\ntion. In Proceedings of the 16th Annual Conference\nof the International Speech Communication Associ-\nation (INTERSPEECH) . Dresden, Germany, pages\n845–853.\nMark Johnson and Eugene Charniak. 2004. A\nTAG-based noisy channel model of speech re-\npairs. In Proceedings of the 42nd Annual Meet-\ning on Association for Computational Linguis-\ntics. Barcelona, Spain, ACL’04, pages 33–39.\nhttp://aclweb.org/anthology/P04-1005.\nMark Johnson, Eugene Charniak, and Matthew Lease.\n2004. An improved model for recognizing disﬂu-\nencies in conversational speech. In Proceedings of\nRich Transcription Workshop.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. CoRR abs/1602.02410.\nYang Liu, Elizabeth Shriberg, Andreas Stolckeand,\nDustin Hillard, Mari Ostendorf, and Mary Harper.\n2006. Enriching speech recognition with auto-\nmatic detection of sentence boundaries and disﬂu-\nencies. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing 14(5):1526–1540.\nTomas Mikolov, Martin Karaﬁat, Lukas Burget, Jan\nCernocky, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proceed-\nings of the 11th Annual Conference of the Interna-\ntional Speech Communication Association (INTER-\nSPEECH). Makuhari, Japan, pages 1045–1048.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proceedings of the 27th Annual Conference\non Neural Information Processing Systems (NIPS) .\nCurran Associates Inc., pages 3111–3119.\nMari Ostendorf, Benoit Favre, Ralph Grishman, Dilek\nHakkani-Tur, Mary Harper, Dustin Hillard, Julia\nHirschberg, Heng Ji, Jeremy G. Kahn, Yang Liu,\nSameer Maskey, Evgeny Matusov, Hermann Ney,\nAndrew Rosenberg, Elizabeth Shriberg, Wen Wang,\nand Chuck Wooters. 2008. Speech segmentation\nand its impact on spoken document processing.\nIEEE Signal Processing Magazine 25(3):59–69.\nMari Ostendorf and Sangyun Hahn. 2013. A sequen-\ntial repetition model for improved disﬂuency detec-\ntion. In Proceedings of the 14th Annual Conference\nof the International Speech Communication Associ-\nation (INTERSPEECH). Lyon, France, pages 2624–\n2628.\nXian Qian and Yang Liu. 2013. Disﬂuency detec-\ntion using multi-step stacked learning. In Pro-\nceedings of the Conference of the North Amer-\nican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technolo-\ngies. Atlanta, USA, NAACL’13, pages 820–825.\nhttp://aclweb.org/anthology/N13-1102.\nMohammad Sadegh Rasooli and Joel Tetreault. 2013.\nJoint parsing and disﬂuency detection in linear time.\nIn Proceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing . Associ-\nation for Computational Linguistics, Seattle, USA,\npages 124–129. http://aclweb.org/anthology/D13-\n1013.\nDavid Rumelhart, James McClelland, and PDP Re-\nsearch Group. 1986. Parallel Distributed Process-\ning: Explorations in the Microstructure of Cogni-\ntion, volume 1. MIT Press.\n552\nWilliam Schuler, Samir AbdelRahman, Tim Miller, and\nLane Schwartz. 2010. Broad-coverage parsing us-\ning human-like memory constraints. Computational\nLinguistics 36(1):1–30.\nElizabeth Shriberg. 1994. Preliminaries to a theory of\nspeech disﬂuencies. Ph.D. thesis, University of Cal-\nifornia, Berkeley, USA.\nAndreas Stolcke. 2002. SRILM: An extensible lan-\nguage modeling toolkit. In Proceedings of Interna-\ntional Conference on Spoken Language Processing.\nAssociation for Computational Linguistics, Denver,\nColorado, USA, volume 2, pages 901–904.\nMasashi Yoshikawa, Hiroyuki Shindo, and Yuji Mat-\nsumoto. 2016. Joint transition-based dependency\nparsing and disﬂuency detection for automatic\nspeech recognition texts. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) . pages 1036–1041.\nhttp://aclweb.org/anthology/D16-1109.\nVictoria Zayats, Mari Ostendorf, and Hannaneh Ha-\njishirzi. 2014. Multi-domain disﬂuency and re-\npair detection. In Proceedings of the 15th Annual\nConference of the International Speech Commu-\nnication Association (INTERSPEECH) . Singapore,\npages 2907–2911.\nVictoria Zayats, Mari Ostendorf, and Hannaneh Ha-\njishirzi. 2016. Disﬂuency detection using a bidirec-\ntional LSTM. In Proceedings of the 16th Annual\nConference of the International Speech Communica-\ntion Association (INTERSPEECH) . San Francisco,\nUSA, pages 2523–2527.\nSimon Zwarts and Mark Johnson. 2011. The impact\nof language models and loss functions on repair\ndisﬂuency detection. In Proceedings of the 49th\nAnnual Meeting of the Association for Computa-\ntional Linguistics: Human Language Technologies .\nAssociation for Computational Linguistics, Port-\nland, USA, volume 1 of HLT’11, pages 703–711.\nhttp://aclweb.org/anthology/P11-1071.\n553",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8259425163269043
    },
    {
      "name": "Language model",
      "score": 0.6728143095970154
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6336708664894104
    },
    {
      "name": "Speech recognition",
      "score": 0.5937058925628662
    },
    {
      "name": "Channel (broadcasting)",
      "score": 0.5813239812850952
    },
    {
      "name": "Natural language processing",
      "score": 0.4590408504009247
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ]
}