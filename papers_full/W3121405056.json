{
  "title": "A transformer based approach for fighting COVID-19 fake news",
  "url": "https://openalex.org/W3121405056",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4282078281",
      "name": "Shifath, S. M. Sadiq-Ur-Rahman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282078280",
      "name": "Khan, Mohammad Faiyaz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2497377559",
      "name": "Islam Md. Saiful",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2794520057",
    "https://openalex.org/W3153760598",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3157497262",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2735017898",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3037681951",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W3036607812",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2055522016",
    "https://openalex.org/W4205767499",
    "https://openalex.org/W2964068236",
    "https://openalex.org/W2977526300"
  ],
  "abstract": "The rapid outbreak of COVID-19 has caused humanity to come to a stand-still and brought with it a plethora of other problems. COVID-19 is the first pandemic in history when humanity is the most technologically advanced and relies heavily on social media platforms for connectivity and other benefits. Unfortunately, fake news and misinformation regarding this virus is also available to people and causing some massive problems. So, fighting this infodemic has become a significant challenge. We present our solution for the \"Constraint@AAAI2021 - COVID19 Fake News Detection in English\" challenge in this work. After extensive experimentation with numerous architectures and techniques, we use eight different transformer-based pre-trained models with additional layers to construct a stacking ensemble classifier and fine-tuned them for our purpose. We achieved 0.979906542 accuracy, 0.979913119 precision, 0.979906542 recall, and 0.979907901 f1-score on the test dataset of the competition.",
  "full_text": "A transformer based approach for ﬁghting\nCOVID-19 fake news\nS.M. Sadiq-Ur-Rahman Shifath1[0000−0003−2428−6595], Mohammad Faiyaz\nKhan2[0000−0002−2155−5991], and Md. Saiful Islam 3[0000−0001−9236−380X]\n1 Department of Computer Science and Engineering,\nShahjalal University of Science and Technology, Sylhet, Bangladesh\nshifathrahman472533@gmail.com\n2 mfaiyazkhan@student.sust.edu\n3 saiful-cse@sust.edu\nAbstract. The rapid outbreak of COVID-19 has caused humanity to\ncome to a stand-still and brought with it a plethora of other problems.\nCOVID-19 is the ﬁrst pandemic in history when humanity is the most\ntechnologically advanced and relies heavily on social media platforms\nfor connectivity and other beneﬁts. Unfortunately, fake news and mis-\ninformation regarding this virus is also available to people and causing\nsome massive problems. So, ﬁghting this infodemic has become a signiﬁ-\ncant challenge. We present our solution for the ”Constraint@AAAI2021 -\nCOVID19 Fake News Detection in English” challenge in this work. After\nextensive experimentation with numerous architectures and techniques,\nwe use eight diﬀerent transformer-based pre-trained models with addi-\ntional layers to construct a stacking ensemble classiﬁer and ﬁne-tuned\nthem for our purpose. We achieved 0.979906542 accuracy, 0.979913119\nprecision, 0.979906542 recall, and 0.979907901 f1-score on the test dataset\nof the competition.\nKeywords: fake news detection · COVID-19 · infodemic · Coronavirus\n· text classiﬁcation.\n1 Introduction\nThe Coronavirus disease 2019 (COVID-19) is an infectious disease caused by\nSARS coronavirus 2. It has impacted almost every country and changed people\nworldwide’s social, economic, and psychological states. Especially in the past\nfew months, people have become more information-hungry on this topic. Hence,\nthey are exposed to a signiﬁcant amount of interaction to the information circled\naround coronavirus through various platforms. In parallel, the infodemic of false\nand misinformation regarding the virus has been rising.\nAs the idea of ”stay-at-home” is proved to be the most eﬀective precaution\nagainst the virus, people’s most preferred solution for communication or enter-\ntainment has been various online and social media platforms. Hence, the number\nof people exposed to rumors or misinformations is more signiﬁcant than ever be-\nfore. In [13], Facebook advertisements across 64 countries were examined, and it\narXiv:2101.12027v1  [cs.CL]  28 Jan 2021\n2 S.M. S.R. Shifath et al.\nwas found that 5% of the advertisements contain possible errors or misinforma-\ntions. Besides, many online news portals purposefully present misguiding news\nto increase their popularity. In recent times, a cluster of fake news about lock-\ndowns, possible remedies, vaccinations have caused panic among people. People\nstarted to pile up stocks of sanitizers, masks, and the supply chain disrupted out\nof fear. Fighting against this ever-increasing amount of fake or misleading news\nhas proven to be as crucial as ﬁnding remedies against the virus to overcome the\npandemic.\nFake news detection is a critical task as it requires the identiﬁcation and de-\ntection of various news types like clickbait, propaganda, satire, misinformation,\nfalsiﬁcation, sloppy journalism, and many more. Traditionally, machine learning-\nbased classiﬁers have been a go-to solution in this domain. In recent years,\nsequence models like RNN, LSTM, and CNN have shown good competency.\nHowever, the introduction of transformers has caused a considerable perfor-\nmance gain. In this work, we propose a solution to the ”Constraint@AAAI2021\n- COVID19 Fake News Detection in English” task on the provided dataset[16].\nOur contributions to this task are the following:\n– We perform extensive experimentation like training classic machine learning\nmodels and traditional text classiﬁcation models like Bidirectional LSTM,\none dimensional CNN on the competition dataset, and incorporating publicly\navailable external datasets for training our models.\n– We ﬁne-tune state-of-the-art transformer based pre-trained models as per the\nrequirement of our task. We also experiment with additional R-CNN [8], mul-\ntichannel CNN with attention [12], multilayer perceptron (MLP) modules to\nincrease the model’s performance. Finally, we construct a stacking ensem-\nble classiﬁer of eight transformer models BERT[3], GPT-2[20], XLNet[24],\nRoBERTa[11], DistillRoBERTa, ALBERT[9], BART[10], and DeBERTa[6],\neach with additional MLP layers.\n– We also present our overall workﬂow, comparison, and performance analysis\nof our experiments on the validation set.\n2 Related Work\nThe issue of fake news detection has been well studied in various ﬁelds. Existing\nmachine learning-based methods like support vector machine (SVM)[5], decision\ntree [2] have worked as baselines for this task. In [21], a simple classiﬁer was used\nto leverage the Term Frequency(TF), Term Frequency and Inverse Document\nFrequency(TF-IDF), and cosine similarity between vectors as features to provide\na baseline for fake news detection task on the Fake News Challenge (FNC-1)\ndataset4.\nDeep learning linguistic models like Convolutional Neural Networks (CNN),\nRecurrent Neural Network (RNN), and Long Short-Term Memory (LSTM) can\nanalyse variable length sequential data and discover hidden complex patterns\n4 http://www.fakenewschallenge.org/\nA transformer based approach for ﬁghting COVID-19 fake news 3\nin textual data. In [1] , a fake news detection model based on Bidirectional\nLSTM[22] is presented. In [14], Bidirectional LSTM along with GloVe[17] and\nELMO[18] embeddings were used to encode the claim and sentences.\nDespite the eﬃcacy of the above-stated language models, they still fell short\nof producing signiﬁcant accuracy. In recent years, transformers and their var-\nious modiﬁcations have brought about considerable performance improvement\nin various natural language processing tasks. In [7], the contextual relationship\nbetween the headline and main text of news was analysed using BERT[3]. They\nshowed that ﬁne-tuning the pre-trained BERT for speciﬁc tasks like fake news\ndetection can outperform existing sequence models.\nResearch eﬀorts focusing on COVID-19 fake news detection have also been\nmade. In [4], a detection model based on the information provided by the World\nHealth Organization, UNICEF, and the United Nations was proposed. Ten ma-\nchine learning algorithms were used along with a voting ensemble classiﬁer. In\n[23], a Twitter dataset containing 7623 tweets and corresponding labels was\nintroduced.\n3 Models\nWe use an ensemble of multiple varieties of pre-trained transformers for the task\nat hand. Transformers are state-of-the-art tools for natural language processing.\nThey have stacked blocks of identical encoders and decoders with self-attention.\nFollowing are the short descriptions of the varieties of transformers that are used:\n1. BERT[3]: BERT(Bidirectional Encoder Representations from Transform-\ners) is a bidirectional language transformer. It is trained based on masked\nlanguage modeling and next sentence prediction on a sizeable unlabelled text\ncorpus.\n2. GPT-2[20]: It is a modiﬁed transformer with a larger context and vocab-\nulary size. In contrast to the ordinary transformers, it has an additional\nnormalization layer after the self-attention block.\n3. XLNet[24]: It is a modiﬁed version of the transformer-XL. It is trained to\nlearn the bidirectional context with an autoregressive method.\n4. RoBERTa[11]:It is an optimized BERT. It is trained or more robust data\nand includes ﬁne-tuning the original BERT without the next-sentence pre-\ndiction objective.\n5. DistilRoBERTa: It is a distilled and faster version of the RoBERTa base\nversion.\n6. ALBERT[9]: It splits the embedding matrix into two smaller metrics and\nuses two-parameter reduction techniques to increase training speed and de-\ncrease memory consumption.\n7. Bart[10]: It is introduced by facebook as a sequence-to-sequence machine\ntranslation model. It is a fusion between BERT[3] and GPT[19].\n8. DeBERTa[6]: It is built on RoBERTa with some modiﬁcations like disen-\ntangled attention and enhanced mask decoder.\n4 S.M. S.R. Shifath et al.\nMultilayer Perceptron (MLP):Each of the previous models is followed\nby a multilayer perceptron module. It consists of a fully connected layer with 64\nhidden units, followed by a normalization layer, followed by a linear layer with\ntanh activation, a dropout layer and a softmax layer for generating two class\nclassiﬁcation probabilities. All the weights are initialized by Xavier initialization.\nEnsemble Module: For ensembling, we train a meta-learner consisting of\nthe following units: a fully connected layer with 64 hidden units, a linear layer\nwith tanh activation function, followed by a fully connected layer with 128 hidden\nunits, a linear layer with ReLU activation function, and a softmax classiﬁer.\nFigure 1 contains an overview of our proposed architecture.\nFig. 1.Overview of our proposed method. tweets are given as inputs to individual\nmodels to produce outputs. then all the outputs are combined to build a 1x8 feature\nvector and are fed to the meta learner to generate the ensembled ﬁnal output.\n4 Dataset\nThe dataset[16] provided for the competition has social media posts related to\nCOVID-19 and corresponding labels indicating if the social media posts are fake\nor real. The dataset is divided into three sets, which are train, validation, and\ntest. The train set contains 6420 samples. Test and validation set each contains\n2140 social media posts with their corresponding labels. The train set consists\nof 3360 real and 3060 fake social media posts, whereas the test and validation\nset consists of 1120 real and 1020 fake social media posts. On average, the train,\nvalidation, and test set contain 27.0, 26.79, and 27.46 words, respectively, in\nA transformer based approach for ﬁghting COVID-19 fake news 5\na social media post. We also used additional data from [23] for training our\nmodel as experimentation. It has 7623 labeled data. We used each data title\nas a replacement of ’tweet’ in the competition data because both have similar\nlengths. We converted the multi-label classiﬁcation data to binary as per the\nlabel type. As a result, 7501 were classiﬁed as fake, and 122 were classiﬁed as\nreal.\n5 Experimentation and Result Analysis\nWe perform experiments primarily on traditional language models such as Bidi-\nrectional LSTM(Bi-LSTM) [22] with attention, 1 dimensional CNN(1D-CNN),\nHierarchical Attention Networks(HAN)[25], Recurrent Convolutional Neural Networks(RCNN)[8],\nand Multichannel CNN with Attention(AMCNN)[12] on the competition dataset.\nWe also experiment with transformer-based pre-trained models like BERT and\nRoBERTa. The result of these experiments is shown in the table 1.\nTable 1.Comparison of the performance of transformer based models with traditional\nlanguage models on the validation dataset\nModel Accuracy f1-score Precision Recall\nFake Real Fake Real Fake Real\nBi-LSTM + attention 0.928 0.931 0.924 0.931 0.925 0.932 0.924\n1D-CNN 0.926 0.931 0.920 0.908 0.948 0.948 0.948\nHAN 0.930 0.933 0.928 0.943 0.918 0.923 0.938\nAMCNN 0.926 0.931 0.920 0.908 0.949 0.956 0.893\nRCNN 0.933 0.937 0.928 0.921 0.947 0.954 0.910\nBERT 0.971 0.969 0.972 0.977 0.965 0.961 0.980\nRoBERTa 0.979 0.977 0.980 0.981 0.976 0.974 0.983\nFrom table 1, it is clearly evident that transformer-based pre-trained models\nshowed radical improvements on scores than the traditional models. But in these\nexperiments, we use just a dense layer with a softmax classiﬁer on top of the\ntransformers for training and prediction. Still, there is a lot of scopes to improve\nit further. According to table 1 it is also clear that, among the traditional models\nscores for RCNN is better than the others. So, we choose RCNN and add it on\ntop of BERT and RoBERTa and train these combined models. In these cases, we\nﬁnd that none of these additions improves the performance of the transformer-\nbased models with a simple classiﬁcation layer. The output of these experiments\nis shown in table 2 with the corresponding model’s name.\nWe experiment with a few classic machine learning classiﬁers such as Decision\nTree(DT)[2], support vector machine (SVM)[5] with diﬀerent kernels on top of\ntransformer-based models. Among these, SVM with Radial Basis Function(RBF)\nkernel achieves highest score which is almost similar to the simple classiﬁer with\n6 S.M. S.R. Shifath et al.\nTable 2.Comparison of the performance among diﬀerent models combined with trans-\nformer based models on the validation dataset\nModel Accuracy f1-score Precision Recall\nFake Real Fake Real Fake Real\nBERT + RCNN 0.967 0.965 0.969 0.980 0.956 0.950 0.982\nRoBERTa + RCNN 0.968 0.966 0.970 0.988 0.956 0.950 0.989\nRoBERTa + SVM 0.978 0.977 0.979 0.987 0.970 0.967 0.988\nRoBERTa + MLP 0.979 0.977 0.980 0.981 0.976 0.974 0.983\na slight improvement. We also experiment with diﬀerent combinations of MLP\non top of RoBERTa and ﬁnd that the best performing MLP performed better\nthan SVM with RBF kernel. The results are shown in table 2.\nSince none of those models except MLP on top of RoBERTa perform better\nthan the transformer-based models with a simple classiﬁer, we add a Multi-\nLayer Perceptron (MLP) on top of transformer-based models. We test diﬀerent\ncombinations of MLPs and choose the best performing combination among these.\nInitially, we experiment with only BERT and RoBERTa among the transformer-\nbased pre-trained models. To add diversity and capture diﬀerent hidden infor-\nmation of the data, we select the most suitable eight models among diﬀerent\ntransformer-based models based on their performance and low resource necessity.\nThese are BERT, RoBERTa, XLNet, GPT-2, ALBERT, DistilRoBERTa,\nBART and DeBERTa. We experiment on each of these models with base ar-\nchitecture since we do not have enough resources. We add the best performing\nMLP on top of each. These models are tuned individually on the train and val-\nidation dataset. The results of the individual performance are shown in table\n3.\nTable 3.Comparison of the performance of various modiﬁed transformer models on\nthe validation dataset\nModel Accuracy f1-score Precision Recall\nFake Real Fake Real Fake Real\nRoBERTa + MLP 0.979 0.977 0.980 0.981 0.976 0.974 0.983\nBERT + MLP 0.971 0.969 0.972 0.977 0.965 0.961 0.980\nXLNet + MLP 0.977 0.975 0.978 0.982 0.972 0.969 0.984\nGPT-2 + MLP 0.974 0.973 0.976 0.974 0.974 0.972 0.977\nDistilRoBERTa + MLP 0.978 0.977 0.979 0.986 0.971 0.968 0.988\nALBERT + MLP 0.962 0.960 0.964 0.976 0.951 0.944 0.979\nBART + MLP 0.978 0.976 0.979 0.988 0.969 0.965 0.989\nDeBERTa + MLP 0.973 0.971 0.975 0.988 0.960 0.955 0.989\nA transformer based approach for ﬁghting COVID-19 fake news 7\nFrom table 3, it can be carefully observed that diﬀerent models among the\ntransformer-based models with MLP can capture features diﬀerently. As a result,\ntheir combination in prediction can help to improve the overall performance. So,\nwe ﬁnally experiment with diﬀerent ensemble techniques.\nFor ensembling, we build a dataset based on the predictions from the individ-\nual models of table 3. We form a feature vector where the features are predictions\nfrom each previous model for a particular training sample. We use it to train a\nmeta-learner to model the individual predictions into a more generalized ﬁnal\noutput. We experiment with random forest and SVM classiﬁers and achieve ac-\ncuracy of .9785 and .9789, respectively on the validation set. Since these methods\ndo not yield signiﬁcant improvement, we introduce a more robust and complex\nmeta-learner consisting of fully connected layers and achieve considerable per-\nformance gain. We try diﬀerent combinations of ensemble of transformer based\nmodels also. The results of best 3 ensembled models are summarised in table 4.\nTable 4.Quantitative comparison of ensemble models on the validation dataset.\nModel Accuracy f1-score Precision Recall\nFake Real Fake Real Fake Real\nEnsemble-v1 0.981 0.980 0.982 0.980 0.983 0.981 0.981\nEnsemble-v2 0.983 0.982 0.984 0.986 0.980 0.978 0.988\nEnsemble-v3 0.984 0.983 0.984 0.983 0.984 0.982 0.985\nHere, Ensemble-v1 consists of RoBERTa, BERT, XLNet, GPT-2. Ensemble-\nv2 has ALBERT, BART, DeBERTa, and the previous models from Ensemble-v1.\nDistilRoBERTa is added to the previous seven models in the Ensemble-v3. From\ntable 4, it can be seen that having more individual models in the ﬁnal ensemble\nclassiﬁer usually yields better generalisation and achieves higher accuracy.\nOne crucial factor is that despite adding additional data for training, all of\nour models’ performance in the validation set decreases. One probable reason for\nthis decline might be the sizeable imbalance in the fake and real news counts.\nSo, we choose to discard additional data for our ﬁnal model training.\n5.1 Hyper-parameters\nWe test diﬀerent hyper-parameters like the number of layers, number of units in a\nlayer, learning rate, weight decay, dropouts, normalization, etc. within a feasible\nrange. In all the models we use diﬀerent learning rates between 1e-3 and 2e-6.\nLearning rate 2e-6 is used in most cases since the dataset is not so large. Using\nthis learning rate helps to converge to the global minimum easily. For traditional\nmodels, we test a diﬀerent combination of several layers with many units. We\nﬁnd Bidirectional LSTM models with 256 layers with 128 hidden units performed\nbest. For 1D-CNN models, we use a layer with 256 ﬁlters and ﬁlter sizes 1-6. For\n8 S.M. S.R. Shifath et al.\nall traditional models, we use a similar structure with a little variation. In case\nof the transformer-based pre-trained models, we use the base architectures. It is\nbecause using large models on small datasets can cause the models to over-ﬁt.\nAlso, we face a resource limitation for experimenting with larger models.\n6 Conclusion\nIn this work, we have presented our overall workﬂow for the fake news detec-\ntion task. We have conducted a number of experiments and provided a com-\nprehensive solution based on modiﬁed transformers with additional layers and\nan ensemble classiﬁer. Our method achieves comparative accuracy on the test\ndataset and therefore helps us to place 20 in the leaderboard of ”CONSTRAINT\n2021 Shared Tasks: Detecting English COVID-19 Fake News and Hindi Hostile\nPosts”[15] competition. Based on our experience from this task, we feel that a\nmore balanced additional training data may help our model perform better. Be-\nsides, extracting more information like parts of speech, named entities, number of\nwords, punctuation, hashtags, website links etc. in social media posts and using\nthese as meta-data during training can increase the methodology’s performance.\nReferences\n1. Bahad, P., Saxena, P., Kamal, R.: Fake news detection using bi-directional lstm-\nrecurrent neural network. Procedia Computer Science 165, 74–82 (2019)\n2. Breiman, L., Friedman, J., Olshen, R., Stone, C.: Classiﬁcation and regression trees\n(wadsworth, belmont, ca). ISBN-13 pp. 978–0412048418 (1984)\n3. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n4. Elhadad, M.K., Li, K.F., Gebali, F.: Detecting misleading information on covid-19.\nIEEE Access 8, 165201–165215 (2020)\n5. Girosi, F., Niyogi, P., Poggio, T., Vapnik, V.: Comparing support vector machines\nwith gaussian kernels to radial basis function classiﬁers. Tech. rep., Technical Re-\nport 1599, Massachusetts Institute of Techology, MA, USA (1996)\n6. He, P., Liu, X., Gao, J., Chen, W.: Deberta: Decoding-enhanced bert with disen-\ntangled attention. arXiv preprint arXiv:2006.03654 (2020)\n7. Jwa, H., Oh, D., Park, K., Kang, J.M., Lim, H.: exbake: Automatic fake news\ndetection model based on bidirectional encoder representations from transformers\n(bert). Applied Sciences 9(19), 4062 (2019)\n8. Lai, S., Xu, L., Liu, K., Zhao, J.: Recurrent convolutional neural networks for text\nclassiﬁcation. In: Twenty-ninth AAAI conference on artiﬁcial intelligence (2015)\n9. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A\nlite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942 (2019)\n10. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,\nStoyanov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461 (2019)\nA transformer based approach for ﬁghting COVID-19 fake news 9\n11. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 (2019)\n12. Liu, Z., Huang, H., Lu, C., Lyu, S.: Multichannel cnn with attention for text\nclassiﬁcation. arXiv preprint arXiv:2006.16174 (2020)\n13. Mejova, Y., Weber, I., Fernandez-Luque, L.: Online health monitoring using face-\nbook advertisement audience estimates in the united states: evaluation study. JMIR\npublic health and surveillance 4(1), e30 (2018)\n14. Nie, Y., Chen, H., Bansal, M.: Combining fact extraction and veriﬁcation with\nneural semantic matching networks. In: Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence. vol. 33, pp. 6859–6866 (2019)\n15. Patwa, P., Bhardwaj, M., Guptha, V., Kumari, G., Sharma, S., PYKL, S., Das, A.,\nEkbal, A., Akhtar, S., Chakraborty, T.: Overview of constraint 2021 shared tasks:\nDetecting english covid-19 fake news and hindi hostile posts. In: Proceedings of the\nFirst Workshop on Combating Online Hostile Posts in Regional Languages during\nEmergency Situation (CONSTRAINT). Springer (2021)\n16. Patwa, P., Sharma, S., PYKL, S., Guptha, V., Kumari, G., Akhtar, M.S., Ekbal,\nA., Das, A., Chakraborty, T.: Fighting an infodemic: Covid-19 fake news dataset.\narXiv preprint arXiv:2011.03327 (2020)\n17. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre-\nsentation. In: Empirical Methods in Natural Language Processing (EMNLP). pp.\n1532–1543 (2014), http://www.aclweb.org/anthology/D14-1162\n18. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,\nZettlemoyer, L.: Deep contextualized word representations. arXiv preprint\narXiv:1802.05365 (2018)\n19. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language un-\nderstanding by generative pre-training (2018)\n20. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nmodels are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\n21. Riedel, B., Augenstein, I., Spithourakis, G.P., Riedel, S.: A simple but tough-to-\nbeat baseline for the fake news challenge stance detection task. arXiv preprint\narXiv:1707.03264 (2017)\n22. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE trans-\nactions on Signal Processing 45(11), 2673–2681 (1997)\n23. Shahi, G.K., Nandini, D.: FakeCovid – a multilingual cross-domain fact check news\ndataset for covid-19. In: Workshop Proceedings of the 14th International AAAI\nConference on Web and Social Media (2020), http://workshop-proceedings.\nicwsm.org/pdf/2020_14.pdf\n24. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:\nGeneralized autoregressive pretraining for language understanding. In: Advances\nin neural information processing systems. pp. 5753–5763 (2019)\n25. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E.: Hierarchical attention\nnetworks for document classiﬁcation. In: Proceedings of the 2016 conference of the\nNorth American chapter of the association for computational linguistics: human\nlanguage technologies. pp. 1480–1489 (2016)",
  "topic": "Coronavirus disease 2019 (COVID-19)",
  "concepts": [
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.6572340726852417
    },
    {
      "name": "Transformer",
      "score": 0.5449863076210022
    },
    {
      "name": "Fake news",
      "score": 0.5079136490821838
    },
    {
      "name": "Computer science",
      "score": 0.4366096258163452
    },
    {
      "name": "Business",
      "score": 0.3788023889064789
    },
    {
      "name": "Computer security",
      "score": 0.37027233839035034
    },
    {
      "name": "Internet privacy",
      "score": 0.3382747769355774
    },
    {
      "name": "Engineering",
      "score": 0.1702168583869934
    },
    {
      "name": "Electrical engineering",
      "score": 0.14587906002998352
    },
    {
      "name": "Medicine",
      "score": 0.13519224524497986
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0705992579460144
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 31
}