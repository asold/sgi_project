{
  "title": "Lane Transformer: A High-Efficiency Trajectory Prediction Model",
  "url": "https://openalex.org/W4313590738",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2112831929",
      "name": "Zhibo Wang",
      "affiliations": [
        "Fudan University",
        "Shanghai Institute for Science of Science",
        "Shanghai Center for Brain Science and Brain-Inspired Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2111740225",
      "name": "Jiayu Guo",
      "affiliations": [
        "Fudan University",
        "Shanghai Institute for Science of Science",
        "Shanghai Center for Brain Science and Brain-Inspired Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2305321667",
      "name": "Zheng-Ming Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150586200",
      "name": "Haiqiang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125818342",
      "name": "Junping Zhang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2091440585",
      "name": "Jian Pu",
      "affiliations": [
        "Shanghai Institute for Science of Science",
        "Fudan University",
        "Shanghai Center for Brain Science and Brain-Inspired Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285214895",
    "https://openalex.org/W4292263008",
    "https://openalex.org/W4226377266",
    "https://openalex.org/W4226440390",
    "https://openalex.org/W4226381010",
    "https://openalex.org/W4285212922",
    "https://openalex.org/W4285106399",
    "https://openalex.org/W4285244183",
    "https://openalex.org/W4225754812",
    "https://openalex.org/W6769043036",
    "https://openalex.org/W2967177252",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3034722190",
    "https://openalex.org/W3180491419",
    "https://openalex.org/W3108486966",
    "https://openalex.org/W2424778531",
    "https://openalex.org/W2963914175",
    "https://openalex.org/W2567297805",
    "https://openalex.org/W2963759562",
    "https://openalex.org/W3116651890",
    "https://openalex.org/W3196864007",
    "https://openalex.org/W3010072020",
    "https://openalex.org/W3204875639",
    "https://openalex.org/W6782468546",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3097237405",
    "https://openalex.org/W3139491754",
    "https://openalex.org/W3169575318",
    "https://openalex.org/W3016826426",
    "https://openalex.org/W2963001155",
    "https://openalex.org/W2990973279",
    "https://openalex.org/W2955189650",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W4312731878",
    "https://openalex.org/W6803278392",
    "https://openalex.org/W3209837334",
    "https://openalex.org/W6838387208",
    "https://openalex.org/W4281263026"
  ],
  "abstract": "Trajectory prediction is a crucial step in the pipeline for autonomous driving because it not only improves the planning of future routes, but also ensures vehicle safety. On the basis of deep neural networks, numerous trajectory prediction models have been proposed and have already achieved high performance on public datasets due to the well-designed model structure and complex optimization procedure. However, the majority of these methods overlook the fact that vehicles&#x2019; limited computing resources can be utilized for online real-time inference. We proposed a Lane Transformer to achieve high accuracy and efficiency in trajectory prediction to tackle this problem. On the one hand, inspired by the well-known transformer, we use attention blocks to replace the commonly used Graph Convolution Network (GCN) in trajectory prediction models, thereby drastically reducing the time cost while maintaining the accuracy. In contrast, we construct our prediction model to be compatible with TensorRT, allowing it to be further optimized and easily transformed into a deployment-friendly form of TensorRT. Experiments demonstrate that our model outperforms the baseline LaneGCN model in quantitative prediction accuracy on the Argoverse dataset by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$10\\times $ </tex-math></inline-formula> to <inline-formula> <tex-math notation=\"LaTeX\">$25\\times $ </tex-math></inline-formula>. Our <inline-formula> <tex-math notation=\"LaTeX\">$7ms$ </tex-math></inline-formula> inference time is the fastest among all open source methods currently available. Our code is publicly available at: <uri>https://github.com/mmdzb/Lane-Transformer</uri>.",
  "full_text": "Received 26 September 2022; revised 20 November 2022; accepted 30 December 2022. Date of publication 3 January 2023;\ndate of current version 17 January 2023.\nDigital Object Identifier 10.1109/OJITS.2023.3233952\nLane Transformer: A High-Efﬁciency\nTrajectory Prediction Model\nZHIBO WANG1, JIAYU GUO1, ZHENGMING HU2, HAIQIANG ZHANG2,\nJUNPING ZHANG 3 (Senior Member, IEEE), AND JIAN PU 1 (Member, IEEE)\n1Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai 200433, China\n2Autonomous Driving General Algorithm Department, Mogo Auto Intelligence and Telematics Information Technology Company Ltd., Beijing 100013, China\n3School of Computer Science, Fudan University, Shanghai 200433, China\nCORRESPONDING AUTHOR: J. PU (e-mail: jianpu@fudan.edu.cn)\nThis work was supported in part by the Shanghai Municipal Science and Technology Major Project under Grant 2018SHZDZX01;\nin part by ZJ Lab; and in part by the Shanghai Center for Brain Science and Brain-Inspired Technology.\nABSTRACT Trajectory prediction is a crucial step in the pipeline for autonomous driving because\nit not only improves the planning of future routes, but also ensures vehicle safety. On the basis of\ndeep neural networks, numerous trajectory prediction models have been proposed and have already\nachieved high performance on public datasets due to the well-designed model structure and complex\noptimization procedure. However, the majority of these methods overlook the fact that vehicles’ limited\ncomputing resources can be utilized for online real-time inference. We proposed a Lane Transformer to\nachieve high accuracy and efficiency in trajectory prediction to tackle this problem. On the one hand,\ninspired by the well-known transformer, we use attention blocks to replace the commonly used Graph\nConvolution Network (GCN) in trajectory prediction models, thereby drastically reducing the time cost\nwhile maintaining the accuracy. In contrast, we construct our prediction model to be compatible with\nTensorRT, allowing it to be further optimized and easily transformed into a deployment-friendly form\nof TensorRT. Experiments demonstrate that our model outperforms the baseline LaneGCN model in\nquantitative prediction accuracy on the Argoverse dataset by a factor of 10 × to 25 ×.O u r 7ms inference\ntime is the fastest among all open source methods currently available. Our code is publicly available at:\nhttps://github.com/mmdzb/Lane-Transformer.\nINDEX TERMS Trajectory prediction, transformer, multi-head attention, TensorRT.\nI. INTRODUCTION\nW\nITH the rapid development of computer science\nand electrical engineering, autonomous systems have\nbegun to play an increasingly important role in our\ndaily life [ 1], [ 2], [ 3]. Among them, autonomous driv-\ning is undoubtedly one of the most important research\nfields [ 4], [ 5]. Due to autonomous driving’s close rela-\ntionship with human life, safety is the primary concern.\nPredicting the future trajectories of nearby vehicles is neces-\nsary to assist the autonomous driving system in determining\nThe review of this article was arranged by Associate Editor\nDr. Vesna Šešum-Cavi ´c.\nthe optimal path planning solution and preventing potential\ncollisions [ 6], [ 7], [ 8], [ 9].\nFor human drivers, future trajectories of nearby vehi-\ncles are predicted based on two types of information: past\ntrajectories of nearby vehicles and the structure of roads.\nFor autonomous vehicles, things are the same. Basically,\nthe inputs of a trajectory prediction model are twofold: past\ntrajectories and High Definition (HD) maps. Autonomous\ndriving systems have to understand the meaning of this\ninformation and make good use of it. An example of a\ntrajectory prediction task is shown in Fig. 1.\nCurrently, learning-based methods have shown their effec-\ntiveness in trajectory prediction tasks. On the one hand,\nresearchers considered trajectories and HD maps as raster\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 VOLUME 4, 2023\nFIGURE 1. An example of trajectory prediction scenario. Normally, a scenario\nincludes trajectory information of different agents (e.g., cars, trucks, bikes and\npedestrians) and HD map information. The target is to predict the future of interested\nagents.\nimages and applied a CNN-based method to extract fea-\ntures and make predictions [ 10], [ 11]. Thanks to substantial\noff-the-shelf CNN-based models, such as VGG-16 [ 12]\nand ResNet [ 13], the implementation of these models is\neasy. However, these methods actually neglect the geomet-\nric features of trajectories, which is the key feature for\nunderstanding their hidden information. Recently, researchers\nstarted to develop the vectorized approach, i.e., they use a\nset of points and vectors to represent the geometric features\nof both trajectories and maps and then use graph convolution\nnetworks (GCNs) as the backbone of the model. The idea\nis fairly successful and achieves state-of-the-art performance\nmetrics on open datasets [ 14], [ 15]. Although the prediction\nperformance is significantly improved by those methods, the\nsuccessful deployment on real vehicles is always overlooked.\nThe models achieving SOTA performance are usually too\ncomplicated to deploy, as they require both large computing\npower and memory.\nTo address this challenge, in this work, we adopt the\nattention mechanism from transformers to replace the time-\nconsuming graph convolution. Compared with convolution,\nthe attention mechanism can achieve the same performance\naccording to existing experiments and at the same time\nis more deployment-friendly because many related works\nhave been performed. Therefore, it is a great choice for\nboth feature extraction and combining vehicle-vehicle and\nvehicle-road information.\nMoreover, since autonomous driving is a highly\napplication-driven application, the model’s most important\nperformance characteristic should be whether it can be\ndeployed on real cars, in other words, whether the model can\nbe transformed into the form of TensorRT, which is the most\ncommonly used deployment SDK. To accomplish this, we\nstrictly adhere to the coding style of TensorRT when imple-\nmenting the model. As a result, our model can be easily\nconverted into TensorRT and accelerated further.\nOur Lane Transformer is inspired by the well-known\nLaneGCN model [ 16], which was one of the first deep learn-\ning models to apply GCN to trajectory prediction tasks. The\nresults of quantitative and qualitative experiments indicate\nthat our model possesses the following benefits.\n• By replacing graph convolution with an attention mech-\nanism, our model achieves state-of-the-art efficiency\nwhile maintaining the performance of the LaneGCN\nmodel’s baseline metrics.\n• We minimize the model’s parameters so that the model’s\nsize is smaller than other existing methods, making our\nmodel more appropriate for applications with limited\nmemory.\n• We strictly adhere to the TensorRT coding style so\nthat our model can be easily converted into TensorRT\nformat, which is a prerequisite for deployment.\nII. RELATED WORK\nA. TRAJECTORY PREDICTION\nTo date, many trajectory prediction methods have been\nproposed, and they perform well on open datasets. First,\ndue to the sequential feature of trajectories, it is natural\nfor researchers to develop an idea based on RNNs [ 17],\n[18], [ 19]. However, although this idea works well for the\nprediction of pedestrians’ trajectories, researchers soon found\nthat it is not suitable for the predictions of vehicles because\nunlike pedestrians, the past trajectory of a vehicle is insuffi-\ncient for predicting its future trajectory. The trajectories of\nother nearby vehicles and the geometric structures of roads\nare the real keys that determine a vehicle’s future trajec-\ntory. To this end, today’s trajectory prediction models can\nbe roughly divided into two categories: raster-based methods\nand vector-based methods.\n1) RASTER-BASED METHODS\nThe basic idea of raster-based methods is simple: we encode\nthe trajectories and HD maps into a stack of images and use\nCNN-based methods to extract the hidden features and model\nthe interactions between vehicles and roads [ 10], [ 11], [ 20],\n[21], [ 22], [ 23]. Since CNN-based computer vision has been\na hot research field in recent years, we have a large num-\nber of off-the-shelf models to use, which is convenient. For\nexample, [ 20] model the scene into an image with multiple\nchannels, and each channel represents a type of information\nin the scene (such as past trajectories, roads or the position of\nother vehicles). However, rasterization of the scene neglects\nthe geometric features of vehicles and roads, such as the\ndirection of cars and roads. As a result, raster-based methods\nshow poorer performance than vector-based methods.\n2) VECTOR-BASED METHODS\nSimilar to raster-based methods, vector-based methods also\ntry to unify the inputs of the trajectories and HD maps [ 14],\n[15], [ 16], [ 24]. However, the difference is that vector-based\nmethods treat both trajectories and roads as sets of points and\nencode them into a directed graph. Thus, the nodes and edges\nof this graph keep both the position and geometric structures\nof the scene. Then, we can use graph convolution as the tool\nto extract the features and combine them. Vector Net [ 14]\nis one of the first models based on this idea, and it reaches\nstate-of-the-art performance back then. LaneGCN [ 16]i s\nVOLUME 4, 2023 3\nWANG et al.: LANE TRANSFORMER: A HIGH-EFFICIENCY TRAJECTORY PREDICTION MODEL\nsimilar to Vector Net in the basic idea but different in the\ntotal structure of the model. Both VectorNet and LaneGCN\ncan be considered the origination of vector-based methods.\nLargely part of following works these two years are based\non their ideas and achieve a better performance on open\ndatasets [ 25].\nTo ensure the multi-modality of outputs to gain a higher\nperformance, recent researchers have paid more attention to\nthe trajectory generation block of the model. GOHOME [ 22]\noutputs a heatmap representation of the possible final points\nof the future trajectory and uses NMS (Non-Maximum\nSuppression) to avoid the similar outputs. DenseTNT [ 24]\nfirst samples a group of points on the road as the candidates\nfor final points and outputs a confidence score for each point.\nMultipath [ 10] outputs the distribution of future behavior\nparameterized as a GMM(Gaussian Mixture Model).\nB. TRANSFORMER\nAfter being proposed in 2017, Transformer soon became\none of the most popular model structures in the deep learn-\ning field [ 26]. On the one hand, its unbelievable high\nperformance in NLP(Natural Language Processing) shows its\nstrong ability as a novel model. On the other hand, the unex-\nplainable feature of Transformer causes heated discussion on\nwhether we can depend on this ‘black box’.\nThe most important part of the transformer is the atten-\ntion mechanism, which can be partly seen as a weighting\nprocedure. Many experiments have already proven that the\nattention mechanism has the ability to learn, encode and\ndecode complex features. Recent works show that trans-\nformer and attention mechanisms can also be used in\ncomputer vision tasks [ 27], [ 28] or to be more specific,\ntrajection prediction tasks [ 29], [ 30]. For example, mmTrans-\nformer [ 31] uses a pure transformer as the backbone and\nachieves state-of-the-art performance. Reference [ 32]u s e d\nan attention mechanism as a part of a trajectory prediction\nmodel and provided a series of experiments to show the\ninfluence of different types of attention blocks. Scene\nTransformer [ 33] adopts a scene-centric approach based on\nthe attention mechanism to achieve high performance on\nboth single- and multiple-agent prediction tasks. All of these\nworks show that the transformer has the potential to replace\nthe convolution blocks in a trajectory prediction model.\nIII. METHOD\nA. PROBLEM DEFINITION\nIn the section above, we only provide a vague definition of\nthe trajectory prediction task: input trajectories, HD maps\nand output future trajectories. Here, we provide a more\ndetailed description of the trajectory prediction task.\nHere is the basic mathematical modeling of trajectory\nprediction. First, for a given scene, there are n agents in\nit, including vehicles, pedestrians, and bikes, denoted as A.\nWe have the past trajectory information of the target vehicle\natarget and its nearby neighbors Anbrs ={ a1,a2,..., an−1},\nin an observation time period tobs. Each past trajectory is\ndefined as follows (taking ai as an example):\nPi =\n[\np−tobs+1\ni ,p−tobs+2\ni ,..., p0\ni\n]\n, (1)\nwhere pt\ni = [xt\ni,yt\ni] denotes the position of agent i at time\nstep t. Usually we use the bird’s-eye-view coordinate system\nto represent the scene.\nOn the other hand, we also have the HD map information\nin a given area, including lanes, traffic lights, sidewalks, etc.\nThe mathematical definition of roads is similar to trajectories\nsince they are all modeled as a series of points.\nFor the output, we aim to predict the future trajectory of\nthe target vehicle atarget in the time period tpred, defined as\nfollows:\nPi =\n[\np1\ni ,p2\ni ,..., p\ntpred\ni\n]\n. (2)\nSimilar to the input, pt\ni = [xt\ni,yt\ni] denotes the position of\nagent i at time step t. Finally, we evaluate the performance\nof the model based on Pi.\nB. OVERALL MODEL\nThe overall framework of the Lane Transformer is illustrated\nin Fig. 2. Basically, inspired by LaneGCN [ 16], our model\nis made up of four main components. First, we preprocess\nthe raw input data, including trajectories and HD maps, into\nvector form. Second, we use an attention-based encoder to\nextract the hidden feature of each agent and road. Third,\nwe use a stack of transformer blocks to fuse the features\nbetween agents and roads and an attention-based block to\naggregate the high-order interaction. Finally, an MLP-based\ndecoder receives the combined information of trajectories\nand maps and then generates the future trajectories of the\ntarget agent.\nC. DATA PREPROCESSING\nNormally, trajectories and HD maps in datasets are annotated\nas a series of points with additional semantic information,\n(e.g., speed limit of the road, whether the traffic line is dashed\nand solid). However, points themselves can only represent\nthe static position of vehicles and roads, instead of their\ngeometric motion features such as directions. Therefore, we\nadopt a vectorization preprocess for both trajectories and HD\nmaps.\nFor agents, we sample the trajectory points with the\nsame time interval and then connect the neighboring points\ninto vectors. Thus, each trajectory contains several vec-\ntors. For roads, we first cut them into segments with the\nsame length(some researchers call these segments poly-\nlines or splines), then sample them with the same distance\nand connect the neighboring points into vectors. Similar\nto a trajectory, each road segment contains several vectors.\nTherefore, we successfully unify the form of trajectories and\nHD maps as vectors, as shown below:\nd\ni =\n[\nxi,yi,xpre\ni ,ypre\ni ,/Delta1x,/Delta1y,vid,sid\n]\n, (3)\n4 VOLUME 4, 2023\nFIGURE 2. Overall framework of the Lane Transformer. Trajectory and HD map data are ﬁrst preprocessed into the form of vectors and passed through the agent and map\nencoder separately. Then, the encoded features are sent into the fusion block to aggregate both local and global interactions. Finally, the feature vectors are sent into different\ndecoders, which ensures the multi-modality of the results, to generate the ﬁnal prediction of the future trajectory.\nwhere [ xi,yi] is the position of point i in the BEV coordinate\nsystem, and [ xpre\ni ,ypre\ni ] is the position of its pre-neighbor.\n/Delta1x and /Delta1y are the displacement between point i and its\npre-neighbor, which represent the direction of trajectories or\nroads. vid and sid are the identification of the vector and\nthe segment (trajectory or road) to which it belongs. These\ntwo IDs help us to specify the position of this vector in the\nscene.\nIt is worth mentioning that to improve the robustness of\nthe model, we avoid the use of absolute coordinates. To be\nmore specific, we first normalize all of the trajectories and\nHD maps into the new relative coordinate system centered\nat the target agent’s position at the last time stamp. This pro-\ncess helps us avoid the potential quantitative issue ( e.g., the\nlarge difference in the absolute coordinates between scenes A\nand B).\nD. SCENE CONTEXT ENCODING\nAfter preprocessing, we now have the trajectories and HD\nmaps in the unified form of vectors. Then, the model needs\nto learn the spatial information of inputs by encoding them\ninto feature vectors. Unlike most of the previous trajectory\nprediction studies, we take efficiency as an important issue.\nThus, we try to design an effective encoder while keeping its\nstructure as simple as possible. Referencing existing works\nand our own experiments, we adopt the combination of an\nattention block, CNN and MLP as the encoder. More details\nare as follows.\nFor roads, we first use an MLP-based encoder to transform\nthe input vector at each time stamp ( e.g., vector of road\nsegment i at position t) into features:\nf\nt\ni = δmap\n(\ndt\ni;Wmap\n)\n, (4)\nwhere δmap is a 3-layer MLP with a ReLU non-linear layer\nand Wmap is the weight matrix that is learnable. Thus, we\nnow have the feature matrix with a size of (M, L, H), where\nM is the number of road segments in the scene, L is the fixed\nlength of one segment ( e.g., 10m) and H is the length of the\nhidden feature. However, to predict the future trajectory, the\nseparate feature of each vector is insufficient. For example,\neven if two road segments in the first half have the same\nstructure, the difference in the last half can result in a total\ndifference in geometric meaning. Therefore, we use a self-\nattention block to encode the overall feature of one road\nsegment and form a single feature vector for each agent.\nTo be more specific, we first calculate the query, key and\nvalue matrix:\nq\nt\ni = Wqf t\ni ,kt\ni = Wkf t\ni ,vt\ni = Wqf t\ni , (5)\nwhere Wq,Wk,Wv are the learnable weight matrices. Then,\nwe take these three matrices as the inputs of the weighting\nblock based on softmax:\nht\ni = softmax\n(\nqt\ni · kt\ni\nT\n√dk\n)\nvt\ni, (6)\nwhere dk is the length of matrix k. Finally, we adopt a\n2-layer MLP to aggregate the features of vectors within a\nroad segment:\nh\ni = δagg\n(\nht\ni;Wagg\n)\n, (7)\nwhere δagg is a 2-layer MLP with a ReLU non-linear layer\nand Wagg is the weight matrix that is learnable. Now, we\nhave the feature vector for each road segment, stored as a\n2D matrix (M, H), where M is the number of road segments\nand H is the length of hidden features.\nFor agents, we use similar techniques to encode and\naggregate the information. In particular, we use a trajec-\ntory encoder block to encode each vector into the form of\na feature vector. Then, similar to roads, even two vehicles\nVOLUME 4, 2023 5\nWANG et al.: LANE TRANSFORMER: A HIGH-EFFICIENCY TRAJECTORY PREDICTION MODEL\nFIGURE 3. We adopt a multi-head attention block to aggregate the information of the agent and road.\nhave the same movement in the first half of their trajectory,\nand the differences in the last half of trajectories can lead to\na totally different future trajectory. Therefore, we use a self-\nattention block to encode the overall feature of one trajectory\nin the observed time period and form a single feature vector\nfor each agent. Finally, a 2-layer MLP-based aggregator is\nused to construct a single feature vector for each trajectory.\nOne aspect worth mentioning is the agent encoder. In\nour experiments, we found that, unlike roads, trajectory data\nare usually non-smooth. Reasons for this phenomenon are\nunclear and might be due to the uncertainty of the loca-\ntion system, such as GPS. However, this phenomenon does\ninfluence trajectory prediction because non-smooth trajec-\ntories cannot represent the real motion pattern of vehicles.\nThus, we use a 1D CNN-based trajectory encoder in the first\nstage of the agent encoder, instead of the MLP-based block\nin the road encoder. Because of the wider receptive field\ncompared with MLP, the CNN-based encoder smooths the\ntrajectories and reduces the influence of bad data quality.\nE. HIERARCHICAL INFORMATION FUSION\nFor now, we have the feature vector of each agent and road\nin the form of a 2D matrix. The shape of agents’ feature\nmatrix is (N,L) and the shape of roads’ feature matrix is\n(M,L), where N is the number of agents, M is the num-\nber of roads and L is the length of feature vectors. Since\nthe motion of a vehicle is influenced by its past trajectory,\nits neighbors’ past trajectories and the road structure, we\nneed to fuse the feature vectors of both agents and roads.\nMoreover, local interactions and global interactions should\nboth be considered. For example, imagine a scene with three\nvehicles in it, A, B and C, where A is our target agent. On\nthe one hand, the geometric relation between vehicles A and\nB may influence A’s future trajectory, the same for A and C.\nOn the other hand, when we take the scene as a whole and\nconsider both A, B and C, their interactions might result in a\ntotally different future motion. Thus, we propose a hierarchi-\ncal information fusion block based on a transformer, which\nfuses the information at both the local and global levels.\nThanks to the ability to capture long-range spatio-temporal\ndependencies, attention-based transformer blocks are well-\nused in deep learning models, especially those focused on\nthe combination of information. Therefore, we adopt the\nattention mechanism as the foundation of our hierarchical\ninformation fusion block.\n1) LOCAL INTERACTION\nFor local information fusion, we adopt a structure similar to\nthe transformer decoder. The network structure is shown in\nFig. 3. The first multi-head attention block can be consid-\nered a self-attention procedure, and the second multi-head\nattention block, which is responsible for information aggre-\ngation, works as a cross-attention. We adopt four layers of\nthis transformer block as a stack to increase the model’s\nability to represent complex situations. It is worth mention-\ning that the positions of the agent feature and road feature\nin Fig. 3 are exchangeable, which means that we can fuse\nthe information from roads to agents or vice versa. Actually,\nin our implementation, only the second and fourth layers of\nthe transformer block are the same as in Fig. 3.T h ef i r s t\nand third layers of the transformer block exchange the input\npositions of the agent and road. Through this exchange tech-\nnique, we can better realize the information aggregation and\navoid the bias caused by agents or roads alone.\n2) GLOBAL INTERACTION\nThe attention block does not change the shape of the fea-\nture matrix, so the feature matrix of agents and roads are\nstill (N,L) and (M,L). However, the L length feature now\ncontains not only its own geometric feature but also its\ninteraction information with each other agent and road.\nTherefore, what we need is now to aggregate all of the\nlocal interactions in the scene, or so-called global interaction\nfusion. Similar to local fusion, we still adopt an attention-\nbased block to achieve this goal, or to be more specific,\na self-attention process between each agent’s feature based\non the target agent, resulting in a single feature vector that\ncontains the whole geometric information in the scene from\nthe view of the target agent.\nF. TRAJECTORY PREDICTION\nMulti-modality is a significant feature of trajectory prediction\ntasks. In other words, for a given scene including past\ntrajectories and road structures, multiple possible future tra-\njectories exist. For example, at a crossroad, turning right or\nleft is possible. Therefore, it requires the decoder of our\n6 VOLUME 4, 2023\nmodel, the trajectory construction block, to have the abil-\nity to generate K different future trajectories, where K is a\nparameter to be defined.\nReferencing the trajectory prediction works by\nfar [ 14], [ 32], we adopt the MLP-based decoder in\nour model. To ensure the multi-modality of our model,\nour trajectory prediction block consists of six MLP-based\ntrajectory decoders with the same structure. During training,\nthe parameters of these six decoders have different forms\nand thus achieve the multi-modality of trajectories:\np\nk\ni = φk\ndec(hi;Wk), (8)\nwhere pk\ni is the kth predicted trajectory of agent i, φk\ndec is\nthe kth trajectory decoder, Wk is the weight matrix of the kth\ntrajectory decoder and hi is the feature vector.\nFurthermore, we adopt a single MLP to generate the con-\nfidence score of each prediction, which helps us choose the\nmost possible trajectory:\nci = φscoring(hi;Wscr), (9)\nwhere ci is the vector consisting of the confidence score of\neach trajectory, φscoring is the scoring decoder, Wscoring is\nthe weight matrix and hi is the feature vector.\nG. TRAINING LOSS\nInspired by the variety loss [ 34], [ 35] and LaneGCN’s loss\nfunction [ 16], we adopt the following loss function:\nL = Lreg + Lcls, (10)\nwhere Lreg is the regression loss and Lcls is the classification\nloss.\nIn the calculation of loss, we first define the trajectory\nthat has the closest distance to the ground truth at the final\npoint as the target trajectory, annotated as k\n∗. The following\ncalculation will be based on this trajectory.\nFor classification loss, we define the loss function as\nfollows:\nLcls = 1\nN(K − 1)\nN∑\ni=1\n∑\nk̸=k∗\nmax\n(\n0,ck\ni + ϵ − ck∗\ni\n)\n, (11)\nwhere N is the number of agents, ck\ni is the confidence score\nof the kth predicted trajectory of agent i and ϵ is the threshold,\nwhich is 0.2 here.\nFor regression loss, unlike previous work, we adopt a\ntwo-part structure loss function:\nLreg = αLFDE + (1 − α)LADE, (12)\nwhere LFDE is the MSE loss between trajectory k∗ and\nground truth, LADE is the Euclidean distance between tra-\njectory k∗ and ground truth at the final time stamp and α is\na weight factor that balances the two loss factors (we adopt\nα=0.5 in our experiments).\nH. CODE STYLE\nTo make it possible for our model to be deployed on\nreal cars, we must ensure that our model can be easily\ntransformed into the form of TensorRT. TensorRT [ 36]i s\nan SDK for high-performance deep learning inference in\nC++ form that includes a deep learning inference optimizer\nand runtime that delivers low latency and high through-\nput for inference applications, developed by Nvidia. It is\nthe foundation of the deployment of deep learning models\nand has been used in many real scenarios, such as online\nshopping and recommender systems. However, due to the\nasynchronous development between TensorRT and Python,\nsome codes cannot be transformed into TensorRT. Here are\nsome examples: we always use ‘ −1’ in Python to represent\nthe default length of a vector, but this can not be recognized\nin TensorRT; the data type of the mask in the model input\nhas to be ‘bool’ in some versions of TensorRT but could\nbe ‘float’ in some other versions; some operation functions\nare not implemented in TensorRT, so we need to avoid these\nfunctions, such as the ‘flatten()’ function in PyTorch. Overall,\nwe carefully avoid these issues and strictly follow the code\nstyle of TensorRT when building the model. Therefore, we\nensured that our model could be easily transformed into\nTensorRT with minor adjustments.\nIV. EXPERIMENTS\nIn this section, we first describe the experimental settings\nand details, including the datasets, implementation details\nand metrics. Then, we show the overall performance of our\nmodel from the view of quantity and quality compared with\nother state-of-the-art models. Finally, comprehensive ablation\nstudies are performed to prove the efficiency and accuracy\nof our model structure.\nA. EXPERIMENTAL SETUP\n1) DATASETS\nWe evaluate our model on the Argoverse dataset [ 37],\nthe widely used large-scale motion forecasting dataset.\nThe Argoverse dataset contains a total of 323557 trajec-\ntory prediction scenes sampled from two cities, Pittsburgh\nand Miami, in 2019. These scenes are divided into three\nparts, training, validation and test, with 205,942, 39,472,\nand 78,143 samples, respectively. Each sample contains an\ninteresting target called an “agent”, and our task is to predict\nits future trajectory. For the training and validation sets, each\nscene contains sequence data of 5 seconds, sampled at 10 Hz.\nThe first two seconds of data are used as the input of the\nmodel and the last three seconds of data are only used as\nthe ground truth to evaluate the performance of the model.\nFor the test set, it only provides the data of the first two\nseconds of trajectory and requires our model to propose the\nprediction of three seconds of trajectory in the future. For\nthe data structure, both trajectory data and map data are in\nthe form of a series of points. Each point contains its X and\nY coordinates. For trajectories, these points are the position\nof the agent at each timestamp. Maps are the set of lane\nVOLUME 4, 2023 7\nWANG et al.: LANE TRANSFORMER: A HIGH-EFFICIENCY TRAJECTORY PREDICTION MODEL\nTABLE 1. Quantitative results on the Argoverse Dataset. The results of our model are compared with Argoverse baseline and LaneGCN in the table.\ncenterlines and their semantic information, including traffic\nlights or turning rules.\n2) IMPLEMENTATION DETAILS\nAs we mentioned in the previous section, to improve the\nrobustness of our model, we first centered the BEV coordi-\nnate system of a given scene at the position of the agent at\nt = 0, which is the last observed position of the agent. We\ntrain the model on three GeForce GTX 1080Ti for 64 epochs\nwith a batch size of 128 and the Adam optimizer. For the\nlearning rate, we adpot the warm-up trick [ 38]: the learn-\ning rate gradually increases from 5 × 10\n−4 to 5 × 10−3\nin the first 10 epochs and then decays to 80% after every\n10 epochs [ 39]. The training process takes approximately\n17 hours to complete.\n3) METRICS\nWe evaluate our model on the standard metrics for tra-\njectory prediction, including minADE (minimum Average\nDisplacement Error), minFDE (minimum Final Displacement\nError) and MR (Miss Rate). For each sample, our model\noutputs K different predictions (K = 6 in our experiments)\nand defines the trajectory that has the closest distance to\nthe ground truth at the last timestamp as the best-predicted\ntrajectory, annotated as ˆp.\nThe minADE score measures the l\n2 distance in\nmeters between the best-predicted trajectory and the\nground-truth trajectory averaged over all future time\nstamps:\nminADE = 1\nT\nT∑\nt=1\n ˆpt\ni − pt\ni\n\n2, (13)\nand the minFDE score measures the displacement at the final\ntime stamp T:\nminFDE =\n\n ˆp\nT\ni − pT\ni\n\n\n2\n, (14)\nwhere ˆpt\ni here denotes agent i’s predicted position at\ntime stamp t and pt\ni is agent i’s true position at time\nstamp t.\nAs ˆp and p are vectors containing x and y coordinates of\nagents, minADE and minFDE represent both the longitudinal\nand lateral errors of predictions.\nThe MR score refers to the ratio of samples where\nthe distance between the best-predicted trajectory end-\npoint and the ground truth endpoint is larger than two\nmeters.\nTABLE 2. Inference time on the Argoverse Dataset. The inference time is reported by\nthe authors or calculated using the ofﬁcial implementations.\nB. QUANTITATIVE RESULTS\n1) METRICS PERFORMANCE\nAs we mention before, the main target of our model is\nto achieve a higher efficiency while maintaining the met-\nrics performance of the baseline LaneGCN. Thus, here, we\nmainly compare the metrics of minADE, minFDE and MR\nof our model with LaneGCN. Furthermore, we also provide\nthe baseline result of the Argoverse official to show that\nour model predicts an ideal future trajectory. We provide\nthe result in two different settings: K = 1 and K = 6.T h e\nK = 6 result shows the multi-modality ability of the model,\nand the K = 1 result shows the model’s ability to judge the\nmost possible trajectory.\nAs shown in Table 1, our model achieves a similar\nperformance as our baseline model LaneGCN, even a few\nbetter when K =6, which is the main ranking metric on open\ndatasets. On the other hand, comparing with the Argoverse\nBaseline, which is a non-deep-learning Weighted Nearest\nNeighbor regression method, shows that our model predicts\nan ideal and convincible future trajectory.\n2) INFERENCE TIME AND MODEL SIZE\nHigh inferencing efficiency and small model size are the\nmain advantages of the Lane Transformer. Here, we compare\nour model with several state-of-the-art models on the leader\nboard of the Argoverse Motion Forecasting Challenge. We\ndefine the inference time as the time cost of predicting the\nthree-second future trajectory of the target vehicle in one\nscene. The results are shown in Table 2.\nAmong the models we compare, LaneGCN [ 16] and\nHiVT [ 40] are based on the GCN block and predict the\nfuture motion based on the regression of the whole tra-\njectory. On the other hand, HOME +GOHOME [ 22], [ 42],\nTHOMAS [ 41] and DenseTNT [ 24] focused on the possi-\nbility distribution of the final point and used optimization\n8 VOLUME 4, 2023\nTABLE 3. Parameter number of different models. The model size is reported by the\nauthors or calculated using the ofﬁcial implementations.\nto increase the accuracy of the model. All of these models\nare or used to be the state-of-the-art model on the leader\nboard, considering their metrics performance. However, as\nfor efficiency, our model performs better. This result proves\nthat our methods, including replacing the GCN block with\ntransformer blocks and reducing the model parameters, are\nbeneficial for model efficiency.\nTo further prove this, we show the model size of these\nmodels in Table 3. There is an interesting phenomenon here:\nunlike the intuitive sense that a larger model runs slower,\nthe model size and model efficiency are not exactly the\nsame. Better model construction can also influence model\nefficiency. However, for models that are based on similar\nideas, such as LaneGCN and HiVT, a smaller model size\nwill reduce the inference time. For our model, which has a\nsimilar idea as LaneGCN, this rule works clearly. With the\nsmallest model size, our model achieves the best efficiency\ncompared with others.\nOn the other hand, as we mentioned before, we follow\nthe code style of TensorRT when constructing the model,\nwhich means our model can be easily transformed into the\nform of TensorRT. If the code of the model does not fol-\nlow the code style of TensorRT, which is common for many\nstate-of-the-art models, it will face many obstacles during the\ntransformation from Python to TensorRT. Actually, among\nthe models we compared before, after removing those that\nare closed source so we cannot read the code, none of them\ncan be easily transformed into TensorRT, which means these\nmodels can never be put into real vehicles. Here, the advan-\ntage of our model is clear, i.e., it can satisfy the requirement\nof deployment in real vehicles, which is the main goal of\nautonomous driving. Besides, TensorRT can further accel-\nerate the model. As shown in the last line of Table 2,t h e\nTensorRT version of our model is even faster than the Python\nversion, which is sufficient for real-time inference.\nC. QUALITATIVE RESULTS\nA ss h o w ni nF i g . 4, we present the qualitative results of our\nmodel on the Argoverse validation set. To clearly show the\nscene, we only visualize the centerlines of roads and the\ntrajectories of the target agent. In the image, the past trajec-\ntories are shown in orange and the ground truth trajectories\nare shown in red. To show the multi-modality of our model,\nwe visualize the total of 6 predicted trajectories with blue\nlines.\nTABLE 4. Ablation experiments of the fusion block.\nTABLE 5. Ablation experiments of the attention mechanism in the scene context\nencoder.\nThe visualization results prove that our model can make\nthe accurate prediction of future trajectories since the blue\narrows, which show the final position of the predicted trajec-\ntories, are close to the red arrows, which is the ground-truth.\nIn addition, the multi-modality of our model is shown. For\nexample, in the crossroad, our model will give the prediction\nof both going straightforward or turning.\nD. ABLATION STUDY\n1) IMPORTANCE OF HIERARCHICAL INFORMATION\nFUSION BLOCK\nAs we mentioned before, we adopt the transformer-based\nhierarchical information fusion block to aggregate the local\nand global information between roads and trajectories. To\nillustrate the importance of this block, we conduct the follow-\ning experiment: since attention-based blocks do not change\nthe size of the vector, we can simply remove those blocks.\nBy comparing the results before and after, the importance\nof these blocks is shown. The results of these experiments\nare shown in Table 4.\nAccording to the results, the local fusion block plays\na more important role in the model than global fusion,\nwhile both of these blocks improve the performance of the\nmodel. This phenomenon inspires us that although global\ninformation will influence the prediction of future trajecto-\nries, a vehicle’s future is more likely to be decided based\non its nearby vehicles.\n2) IMPORTANCE OF ATTENTION MECHANISM IN SCENE\nCONTEXT ENCODER\nTo further illustrate the function of the attention mechanism,\nwe also need to prove its importance in the scene context\nencoder. Unlike the fusion block, we cannot simply remove\nthe whole block since the structure of the data has been\nchanged in this block. Thus, we remove the attention block in\nthe actor encoder or road encoder to compare their influence\non the performance of the model. The results are shown in\nTable 5.\nFrom the experiments, we find that, basically, the attention\nmechanism in both the agent and map encoder helps the\nmodel to better encode the feature and improve the model\nperformance, which proves that the attention block is useful\nVOLUME 4, 2023 9\nWANG et al.: LANE TRANSFORMER: A HIGH-EFFICIENCY TRAJECTORY PREDICTION MODEL\nFIGURE 4. Visualization of the road and the trajectories of the target agent on the Argoverse validation set. The red, blue and orange lines represent the ground truth,\npredicted trajectories and observed trajectories, respectively.\nTABLE 6. Ablation experiments of the bottle neck and smooth encoder. Both the quantitative and efﬁciency results are shown in the table.\nfor the information encoding procedure. Although MR is\nalmost the same, lower minADE and minFDE ensure a better\ntrajectory quality.\n3) EXPERIMENTS OF BOTTLE NECK BLOCK AND\nSMOOTH BLOCK\nIn some recent research works [ 43], we noticed that some\nmodel structures can further improve the performance of\n10 VOLUME 4, 2023\nTABLE 7. Comparison of efﬁciency between transformer block and GCN block.\nthe model, especially those that can be transformed into\nTensorRT. Despite those that are not suitable for our model\ndue to the difference in backbone, we found that the bot-\ntle neck block might be an interesting attempt. The bottle\nneck block is proposed in ResNet [ 13]. Basically, in this\nblock, the length of feature vectors are reduced at first\nand then enlarged to the original length. Through this\nprocedure, the information is distilled and thus reduces\nthe influence of unrelated information. We try to put the\nbottle neck block at the end of each transformer block,\naccording to the instruction of [ 43] and the results are shown\nin Table 6.\nOn the other hand, as we mentioned before, we adopt\na CNN-based encoder to address the non-smooth trajectory\ndata. To illustrate the performance of this block, we compare\nthe performance of our model with and without this smooth\nencoder block, and the results are also shown in Table 6.\nFrom the table, it is clear that both the bottle neck\nand smooth encoder have some benefits for the model’s\nperformance. However, the smooth encoder causes a much\ngreater improvement than the bottle neck block. This phe-\nnomenon shows that compared with the model structure,\nbetter data quality might be the key for higher performance.\nFurthermore, since both the bottle neck block and smooth\nencoder are CNN based, which is quite time consuming,\nboth of these blocks will delay the inference of the model.\nSince we are looking forward to obtaining a model with\nhigh efficiency, we give up the bottle neck block and only\nkeep the smooth encoder in our model’s final version.\n4) COMPARISON OF EFFICIENCY BETWEEN\nTRANSFORMER BLOCK AND GCN BLOCK\nTo further prove that the transformer block is more efficient\nthan the GCN block, we compare the efficiency of the fusion\nblock between our model and LaneGCN. The A2L block\nin LaneGCN acts the same function as the first transformer\nfusion block in our model. The inputs of these two blocks are\nsimilar: both include the agent features and road features, the\nsizes of which are (N,L) and (M,L). They both generate the\nsame output: the road features, which keep the size (M,L),\nafter being fused with the agent features. To ensure equality,\nwe run both our model and LaneGCN on the Argoverse\nvalidation set and compute the average time consumed by\nour transformer block or GCN block. The efficiencies of\nthese two blocks are shown in Table 7.\nFrom the table, we note that the transformer block in our\nmodel is more efficient than the GCN block in the LaneGCN,\nwhile the i/o data structure and function are similar.\nV. CONCLUSION\nIn this paper, we have proposed the Lane Transformer model,\nwhich is a highly efficient model for trajectory prediction.\nDue to the switch from the GCN to the transformer block,\nour model achieves state-of-the-art efficiency performance in\ncomparison to other existing open source models. In contrast\nto the baseline LaneGCN, our speedup is 10 × to 25 × while\nmaintaining comparable prediction accuracies on the public\ndataset. Importantly, since we strictly adhere to the code\nstyle of TensorRT, our model can be easily transformed into\nTensorRT format, which facilitates future deployment on real\nvehicles.\nREFERENCES\n[1] E. Zhang, N. Masoud, M. Bandegi, and R. K. Malhan, “Predicting\nrisky driving in a connected vehicle environment,” IEEE Trans. Intell.\nTransp. Syst., vol. 23, no. 10, pp. 17177–17188, Oct. 2022.\n[2] Z. Li, X. Huang, T. Mu, and J. Wang, “Attention-based lane change\nand crash risk prediction model in highways,” IEEE Trans. Intell.\nTransp. Syst., vol. 23, no. 12, pp. 22909–22922, Dec. 2022.\n[3] O. Scheel, N. S. Nagaraja, L. Schwarz, N. Navab, and F. Tombari,\n“Recurrent models for lane change prediction and situation\nassessment,” IEEE Trans. Intell. Transp. Syst. , vol. 23, no. 10,\npp. 17284–17300, Oct. 2022.\n[4] Q. Zhang et al., “TrajGEN: Generating realistic and diverse trajectories\nwith reactive and feasible agent behaviors for autonomous driving,”\nIEEE Trans. Intell. Transp. Syst. , vol. 23, no. 12, pp. 24474–24487,\nDec. 2022.\n[5] Y . Guo, D. Yao, B. Li, Z. He, H. Gao, and L. Li, “Trajectory planning\nfor an autonomous vehicle in spatially constrained environments,”\nIEEE Trans. Intell. Transp. Syst. , vol. 23, no. 10, pp. 18326–18336,\nOct. 2022.\n[6] Y . Lu, W. Wang, X. Hu, P. Xu, S. Zhou, and M. Cai, “Vehicle trajectory\nprediction in connected environments via heterogeneous context-aware\ngraph convolutional networks,” IEEE Trans. Intell. Transp. Syst. , early\naccess, May 24, 2022, doi: 10.1109/TITS.2022.3173944.\n[7] K. Chen, X. Song, H. Yuan, and X. Ren, “Fully convolutional\nencoder-decoder with an attention mechanism for practical pedes-\ntrian trajectory prediction,” IEEE Trans. Intell. Transp. Syst. , vol. 23,\nno. 11, pp. 20046–20060, Nov. 2022.\n[8] K. Zhang, X. Feng, L. Wu, and Z. He, “Trajectory prediction\nfor autonomous driving using spatial-temporal graph attention\ntransformer,” IEEE Trans. Intell. Transp. Syst. , vol. 23, no. 11,\npp. 22343–22353, Nov. 2022.\n[9] X. Liu, Y . Wang, K. Jiang, Z. Zhou, K. Nam, and C. Yin, “Interactive\ntrajectory prediction using a driving risk map-integrated deep learning\nmethod for surrounding vehicles on highways,” IEEE Trans. Intell.\nTransp. Syst., vol. 23, no. 10, pp. 19076–19087, Oct. 2022.\n[10] Y . Chai, B. Sapp, M. Bansal, and D. Anguelov, “Multipath: Multiple\nprobabilistic anchor trajectory hypotheses for behavior prediction,” in\nProc. Conf. Robot Learn. (CoRL) , Osaka, Japan, 2019, pp. 86–99.\n[11] H. Cui et al., “Multimodal trajectory predictions for autonomous driv-\ning using deep convolutional networks,” in Proc. IEEE Int. Conf.\nRobot. Autom. (ICRA) , 2019, pp. 2090–2096.\n[12] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Represent.\n(ICLR), San Diego, CA, USA, 2015, pp. 1–14.\n[13] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Las Vegas, NV , USA, 2016, pp. 770–778.\n[14] J. Gao et al., “VectorNet: Encoding hd maps and agent dynamics from\nvectorized representation,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR) , 2020, pp. 11522–11530.\n[15] M. Ye, T. Cao, and Q. Chen, “TPCN: Temporal point cloud networks\nfor motion forecasting,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR) , 2021, pp. 11313–11322.\n[16] M. Liang et al., “Learning lane graph representations for motion fore-\ncasting,” in Proc. Eur. Conf. Comput. Vis. (ECCV) , Glasgow, U.K.,\n2020, pp. 541–556.\nVOLUME 4, 2023 11\nWANG et al.: LANE TRANSFORMER: A HIGH-EFFICIENCY TRAJECTORY PREDICTION MODEL\n[17] A. Alahi, K. Goel, V . Ramanathan, A. Robicquet, L. Fei-Fei, and\nS. Savarese, “Social LSTM: Human trajectory prediction in crowded\nspaces,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,\nLas Vegas, NV , USA, 2016, pp. 961–971.\n[18] F. Altché and A. de La Fortelle, “An LSTM network for highway\ntrajectory prediction,” in Proc. IEEE Int. Conf. Intell. Transp. Syst.\n(ITSC), Yokohama, Japan, 2017, pp. 353–359.\n[19] A. Khosroshahi, E. Ohn-Bar, and M. M. Trivedi, “Surround vehicles\ntrajectory analysis with recurrent neural networks,” in Proc. IEEE\nInt. Conf. Intell. Transp. Syst. (ITSC) , Rio de Janeiro, Brazil, 2016,\npp. 2267–2272.\n[20] J. Hong, B. Sapp, and J. Philbin, “Rules of the road: Predicting driving\nbehavior with a convolutional model of semantic interactions,” in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Long Beach, CA,\nUSA, 2019, pp. 8446–8454.\n[21] T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone,\n“Trajectron++: Dynamically-feasible trajectory forecasting with het-\nerogeneous data,” in Proc. Eur. Conf. Comput. Vis. (ECCV) , Glasgow,\nU.K., 2020, pp. 683–700.\n[22] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde,\n“GOHOME: Graph-oriented heatmap output for future motion esti-\nmation,” in Proc. IEEE Int. Conf. Robot. Autom. (ICRA) , Philadelphia,\nPA, USA, 2022, pp. 9107–9114.\n[23] N. Djuric et al., “Uncertainty-aware short-term motion prediction\nof traffic actors for autonomous driving,” in Proc. IEEE Conf.\nAppl. Comput. Vis. (WACV) , Snowmass Village, CO, USA, 2020,\npp. 2084–2093.\n[24] J. Gu, C. Sun, and H. Zhao, “DenseTNT: End-to-end trajectory\nprediction from dense goal sets,” in Proc. IEEE Int. Conf. Comput.\nVis. (ICCV), 2021, pp. 15283–15292.\n[25] H. Zhao et al., “TNT: Target-driven trajectory prediction,” in Proc.\nConf. Robot Learn. (CoRL) , 2020, pp. 895–904.\n[26] A. Vaswani et al., “Attention is all you need,” in Proc. Adv.\nNeural Inf. Process. Syst. (NeurIPS) , Long Beach, CA, USA, 2017,\npp. 5998–6008.\n[27] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers\nfor image recognition at scale,” in Proc. Int. Conf. Learn. Represent.\n(ICLR), 2020, pp. 1–22.\n[28] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , 2021,\npp. 10012–10022.\n[29] C. Yu, X. Ma, J. Ren, H. Zhao, and S. Yi, “Spatio-temporal graph\ntransformer networks for pedestrian trajectory prediction,” in Proc.\nEur. Conf. Comput. Vis. (ECCV) , Glasgow, U.K., 2020, pp. 507–523.\n[30] Y . Yuan, X. Weng, Y . Ou, and K. Kitani, “Agentformer: Agent-\naware transformers for socio-temporal multi-agent forecasting,” in\nProc. IEEE Int. Conf. Comput. Vis. (ICCV) , 2021, pp. 9793–9803.\n[31] Y . Liu, J. Zhang, L. Fang, Q. Jiang, and B. Zhou, “Multimodal motion\nprediction with stacked transformers,” in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. (CVPR) , 2021, pp. 7573–7582.\n[32] K. Messaoud, I. Yahiaoui, A. Verroust-Blondet, and F. Nashashibi,\n“Attention based vehicle trajectory prediction,” IEEE Trans. Intell.\nVeh., vol. 6, no. 1, pp. 175–185, Mar. 2021.\n[33] J. Ngiam et al., “Scene transformer: A unified architecture for\npredicting future trajectories of multiple agents,” in Proc. Int. Conf.\nLearn. Represent. (ICLR) , 2022, pp. 1–25.\n[34] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi,\n“Social GAN: Socially acceptable trajectories with generative adver-\nsarial networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Salt Lake City, UT, USA, 2018, pp. 2255–2264.\n[35] L. Thiede and P. Brahma, “Analyzing the variety loss in the context of\nprobabilistic trajectory prediction,” in Proc. IEEE Int. Conf. Comput.\nVis. (ICCV), Seoul, South Korea, 2019, pp. 9953–9962.\n[36] (Nvidia, Santa Clara, CA, USA).\nTensorRT, Devoloper Guide .\n(2021). Accessed: Jul. 20, 2022. Available: https://docs.nvidia.com/\ndeeplearning/tensorrt/developer-guide/index.html\n[37] M. Chang et al., “Argoverse: 3D tracking and forecasting with rich\nmaps,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,\nLong Beach, CA, USA, 2019, pp. 8740–8749.\n[38] I. Loshchilov and F. Hutter, “SGDR: stochastic gradient descent with\nwarm restarts,” in Proc. Int. Conf. Learn. Represent. (ICLR) , 2017,\npp. 1–16.\n[39] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\nin Proc. Int. Conf. Learn. Represent. (ICLR) , New Orleans, LA, USA,\n2019, pp. 1–19.\n[40] Z. Zhou, L. Ye, J. Wang, K. Wu, and K. Lu, “HiVT: Hierarchical\nvector transformer for multi-agent motion prediction,” in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. (CVPR) , Long Beach, CA, USA,\n2019, pp. 8823–8833.\n[41] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde,\n“THOMAS: Trajectory heatmap output with learned multi-agent sam-\npling,” in Proc. Int. Conf. Learn. Represent. (ICLR) , 2022, pp. 1–18.\n[42] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde,\n“HOME: Heatmap output for future motion estimation,” in Proc. IEEE\nInt. Conf. Intell. Transp. Syst. (ITSC) , Indianapolis, IN, USA, 2021,\npp. 500–507.\n[43] X. Xia et al., “TRT-ViT: TensorRT-oriented vision transformer,” 2022,\narXiv:2205.09579.\nZHIBO WANG received the B.S. degree in\ncomputer science from the Beijing Institute of\nTechnology, Beijing, China, in 2021. He is cur-\nrently pursuing the M.S. degree with the Institute\nof Science and Technology for Brain-Inspired\nIntelligence, Fudan University, Shanghai, China.\nHis research focuses on trajectory prediction\nbased on machine learning and its application in\nautonomous driving systems.\nJIAYU GUO received the bachelor of science\ndegree from Wuhan University in 2020. He is\ncurrently pursuing the Ph.D. degree with ISTBI,\nFudan University, China. He is interested in\ntransformer-based models for trajectory prediction\nand robust prediction across different scenarios.\nZHENGMING HU received the master’s degree in\nmechanical design from the Wuhan University\nof Science and Technology in 2011. He is a\nSenior Engineer with Mogo Auto Intelligence and\nTelematics Information Technology Company Ltd.\nHis research and expertise are multi-sensor fusion\nand trajectory prediction.\nHAIQIANG ZHANG received the Ph.D. degree from\nthe Beijing Institute of Technology, Beijing, China,\nin 2010. He is currently a Technique Director\nwith Mogo Auto Intelligence and Telematics\nInformation Technology Company Ltd., Beijing,\nChina. His current research interest is to\ndevelop perception and localization methods for\nautonomous driving.\n12 VOLUME 4, 2023\nJUNPING ZHANG (Senior Member, IEEE)\nreceived the B.S. degree in automation from\nXiangtan University, China, in 1992, the M.S.\ndegree in control theory and control engineer-\ning from Hunan University, Changsha, China, in\n2000, and the Ph.D. degree in intelligent systems\nand pattern recognition from the Institution of\nAutomation, Chinese Academy of Sciences in\n2003. He has been a Professor with the School of\nComputer Science, Fudan University since 2006.\nHis research interests include machine learning,\nimage processing, biometric authentication, and intelligent transportation\nsystems. He has been an Associate Editor of IEEE I\nNTELLIGENT SYSTEMS\nsince 2009 and was an Associate Editor of IEEE T RANSACTIONS ON\nINTELLIGENT TRANSPORTATION SYSTEMS from 2010 to 2018. He has\nbeen an Associate Editor of IEEE T RANSACTIONS ON INTELLIGENT\nVEHICLES since 2022.\nJIAN PU (Member, IEEE) received the Ph.D.\ndegree from Fudan University, Shanghai, China,\nin 2014, where he is currently a Young Principal\nInvestigator with the Institute of Science and\nTechnology for Brain-Inspired Intelligence. He\nwas a Postdoctoral Researcher with the Institute\nof Neuroscience, Chinese Academy of Sciences,\nChina, from 2014 to 2016, and was an Associate\nProfessor with the School of Computer Science\nand Software Engineering, East China Normal\nUniversity from 2016 to 2019. His current research\ninterest is to develop machine learning and computer vision methods for\nautonomous driving.\nVOLUME 4, 2023 13",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6241158843040466
    },
    {
      "name": "Trajectory",
      "score": 0.5791396498680115
    },
    {
      "name": "Computer science",
      "score": 0.4027976393699646
    },
    {
      "name": "Environmental science",
      "score": 0.3304446339607239
    },
    {
      "name": "Engineering",
      "score": 0.2089959979057312
    },
    {
      "name": "Electrical engineering",
      "score": 0.19746273756027222
    },
    {
      "name": "Physics",
      "score": 0.14370831847190857
    },
    {
      "name": "Voltage",
      "score": 0.07326284050941467
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ]
}