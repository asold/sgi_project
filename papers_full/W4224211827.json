{
  "title": "A multi-head attention-based transformer model for traffic flow forecasting with a comparative analysis to recurrent neural networks",
  "url": "https://openalex.org/W4224211827",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2113042744",
      "name": "Selim Reza",
      "affiliations": [
        "Universidade do Porto"
      ]
    },
    {
      "id": "https://openalex.org/A2788547895",
      "name": "Marta Campos Ferreira",
      "affiliations": [
        "Universidade do Porto"
      ]
    },
    {
      "id": "https://openalex.org/A2560826926",
      "name": "J. J. M. Machado",
      "affiliations": [
        "Universidade do Porto"
      ]
    },
    {
      "id": "https://openalex.org/A2152596491",
      "name": "João Manuel R. S. Tavares",
      "affiliations": [
        "Universidade do Porto"
      ]
    },
    {
      "id": "https://openalex.org/A2113042744",
      "name": "Selim Reza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2788547895",
      "name": "Marta Campos Ferreira",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2560826926",
      "name": "J. J. M. Machado",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152596491",
      "name": "João Manuel R. S. Tavares",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6693953293",
    "https://openalex.org/W6638824847",
    "https://openalex.org/W6732160842",
    "https://openalex.org/W3210899060",
    "https://openalex.org/W2731150448",
    "https://openalex.org/W6731671061",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2724431948",
    "https://openalex.org/W3194625338",
    "https://openalex.org/W2889516516",
    "https://openalex.org/W3096622037",
    "https://openalex.org/W2996919856",
    "https://openalex.org/W6764679822",
    "https://openalex.org/W3154414470",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W3107328345",
    "https://openalex.org/W2004353783",
    "https://openalex.org/W6780476144",
    "https://openalex.org/W2593182953",
    "https://openalex.org/W2986595349",
    "https://openalex.org/W4221027504",
    "https://openalex.org/W3215339282",
    "https://openalex.org/W2885195348",
    "https://openalex.org/W3127229177",
    "https://openalex.org/W3134947046",
    "https://openalex.org/W1529404569",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6794133808",
    "https://openalex.org/W2573587735",
    "https://openalex.org/W4255306897",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3193812480",
    "https://openalex.org/W3203619751",
    "https://openalex.org/W4253857705",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W1536078675",
    "https://openalex.org/W3103305799",
    "https://openalex.org/W3103720336",
    "https://openalex.org/W3000386982",
    "https://openalex.org/W4213113494",
    "https://openalex.org/W2954731415",
    "https://openalex.org/W4297954654",
    "https://openalex.org/W3195162744",
    "https://openalex.org/W3016030466",
    "https://openalex.org/W3101978253",
    "https://openalex.org/W3125634432"
  ],
  "abstract": "Traffic flow forecasting is an essential component of an intelligent transportation system to mitigate congestion. Recurrent neural networks, particularly gated recurrent units and long short-term memory, have been the state-of-the-art traffic flow forecasting models for the last few years. However, a more sophisticated and resilient model is necessary to effectively acquire long-range correlations in the time-series data sequence under analysis. The dominant performance of transformers by overcoming the drawbacks of recurrent neural networks in natural language processing might tackle this need and lead to successful time-series forecasting. This article presents a multi-head attention based transformer model for traffic flow forecasting with a comparative analysis between a gated recurrent unit and a long-short term memory-based model on PeMS dataset in this context. The model uses 5 heads with 5 identical layers of encoder and decoder and relies on Square Subsequent Masking techniques. The results demonstrate the promising performance of the transform-based model in predicting long-term traffic flow patterns effectively after feeding it with substantial amount of data. It also demonstrates its worthiness by increasing the mean squared errors and mean absolute percentage errors by (1.25−47.8)% and (32.4−83.8)%, respectively, concerning the current baselines.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8162131905555725
    },
    {
      "name": "Transformer",
      "score": 0.6894692182540894
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5600921511650085
    },
    {
      "name": "Artificial neural network",
      "score": 0.5293745994567871
    },
    {
      "name": "Encoder",
      "score": 0.5039953589439392
    },
    {
      "name": "Time series",
      "score": 0.45972415804862976
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4244995713233948
    },
    {
      "name": "Long short term memory",
      "score": 0.41253894567489624
    },
    {
      "name": "Machine learning",
      "score": 0.39733195304870605
    },
    {
      "name": "Data mining",
      "score": 0.32485318183898926
    },
    {
      "name": "Voltage",
      "score": 0.10937035083770752
    },
    {
      "name": "Engineering",
      "score": 0.09161120653152466
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}