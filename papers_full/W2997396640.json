{
  "title": "Combining Neural Language Models for Word Sense Induction",
  "url": "https://openalex.org/W2997396640",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5047490968",
      "name": "Nikolay Arefyev",
      "affiliations": [
        "Samsung (Russia)"
      ]
    },
    {
      "id": "https://openalex.org/A5043031382",
      "name": "Boris Sheludko",
      "affiliations": [
        "Samsung (Russia)"
      ]
    },
    {
      "id": "https://openalex.org/A5007659254",
      "name": "Tatiana V. Aleksashina",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2789044180",
    "https://openalex.org/W2965105341",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2058223376",
    "https://openalex.org/W2507974895",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2888539709",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2035242626",
    "https://openalex.org/W6600655081",
    "https://openalex.org/W1660828347",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3089007792",
    "https://openalex.org/W2803796482",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2141719394",
    "https://openalex.org/W2792924782",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2940642449",
    "https://openalex.org/W3042821973",
    "https://openalex.org/W2250962194",
    "https://openalex.org/W1513718494",
    "https://openalex.org/W70010159",
    "https://openalex.org/W2888282873",
    "https://openalex.org/W2802069384",
    "https://openalex.org/W1495028359",
    "https://openalex.org/W2248425509",
    "https://openalex.org/W2117805747",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2787560479"
  ],
  "abstract": null,
  "full_text": "Combining Neural Language Models for Word\nSense Induction\nNikolay Arefyev1,2, Boris Sheludko1,2, and Tatiana Aleksashina3\n1 Samsung R&D Institute Russia, Moscow, Russia\n2 Lomonosov Moscow State University, Moscow, Russia\n3 SlickJump, Moscow, Russia\nAbstract. Word sense induction (WSI) is the problem of grouping oc-\ncurrences of an ambiguous word according to the expressed sense of this\nword. Recently a new approach to this task was proposed, which gener-\nates possible substitutes for the ambiguous word in a particular context\nusing neural language models, and then clusters sparse bag-of-words vec-\ntors built from these substitutes. In this work, we apply this approach to\ntheRussianlanguageandimproveitintwoways.First,weproposemeth-\nods of combining left and right contexts, resulting in better substitutes\ngenerated. Second, instead of ﬁxed number of clusters for all ambiguous\nwords we propose a technique for selecting individual number of clus-\nters for each word. Our approach established new state-of-the-art level,\nimproving current best results of WSI for the Russian language on two\nRUSSE 2018 datasets by a large margin.\nKeywords: word sense induction, contextual substitutes, neural lan-\nguage models\n1 Introduction\nAmbiguity, including lexical ambiguity, when single word has several meanings,\nis one of the fundamental properties of natural languages, and is among the most\nchallenging problems for NLP. For instance, modern neural machine translation\nsystems are still surprisingly bad at translating ambiguous words [5], although\nthere is some progress in the latest Transformer-based systems compared to pre-\nviously popular RNN-based ones [19]. Word Sense Induction (WSI) task can\nbe seen as clustering of occurrences of an ambiguous word according to their\nmeaning. A dataset for WSI consists of text fragments containing ambiguous\nwords. Each occurrence of these words is hand-labeled with the expressed sense\naccording to some sense inventory (a dictionary or a lexical ontology). A WSI\nsystem gets a list of ambiguous words and text fragments as an input. For each\nambiguous word the system should cluster its occurrences into unknown number\nof clusters corresponding to this word’s senses. This is in contrast to Word Sense\nDisambiguation (WSD) task where systems are also given the sense inventory\nused by annotators, so both the number of senses and some contextual infor-\nmation about these senses (in the form of their deﬁnitions or related words) are\navailable.\narXiv:2006.13200v1  [cs.CL]  23 Jun 2020\n2 Combining Neural Language Models for Word Sense Induction\nRecently [3] has proposed a new approach to WSI that generates contextual\nsubstitutes (i.e. words, which can appear instead of the ambiguous word in a\nparticular context) using bidirectional neural language models (LMs), and clus-\ntersTFIDF-scaledbag-of-wordsvectorsofthesubstitutes.Thisapproachshowed\nSOTA results in SemEval 2013 WSI shared task for English. For instance, for the\nword build substitutes likemanufacture, make, assemble, ship, export are gener-\nated when it is used inManufacturing some goodssense anderect, rebuild, open\nare generated for theConstructing a buildingsense which allows distinguishing\nthese senses. We improved this approach in two ways. First, the base approach\nsimply unites substitutes retrieved from probability distributions estimated by\nforward and backward LMs each given only one-sided context. This results in\nnoisy substitutes when either left or right context is short or non-informative.\nWe explored several methods of combining forward and backward distributions\nand show that substitutes retrieved from a combined distribution perform much\nbetter. Second, the base method used the same number of clusters for each word\n(average number of senses per word was found to be optimal). We show that\nusing a ﬁxed number of clusters for all words has a huge negative eﬀect on the\nWSI results and propose a method for selecting individual number of clusters\nfor each ambiguous word. Our approach has achieved new SOTA results on the\nRUSSE 2018 WSI shared task for the Russian language [16] with a large im-\nprovement over previous best results according to the oﬃcial leaderboard4. Also\nwe compare performance of several pretrained Russian neural LMs in WSI.\n2 Related work\nExisting WSI methods can be roughly categorized by their relatedness to one of\nthe following lines of research.Latent Variable Methodsdeﬁne a probabilistic\nmodel of a text generation process that includes latent variables corresponding\nto word senses. Posterior probability given unlabeled corpus is estimated to solve\nWSI task. For instance, [13] relies on the Hierarchical Dirichlet Process and [6]\nemploys the Stick-breaking Process. In [2] a rather complicated custom graph-\nical model is proposed which aims at solving the sense granularity problem.\nGraph Clustering methods like [21,9] ﬁrst build a graph with nodes corre-\nsponding to words, and weighted edges representing semantic similarity strength\nor co-occurrence frequency. Then graph clustering algorithms are applied to split\nneighbours of an ambiguous word into clusters interpreted as this word’s senses.\nContext clusteringmethods represent each occurrence of an ambiguous word\nas a vector that encodes its context. For example, in [5,12] a weighted average of\nthe context word embeddings is calculated, then a general clustering algorithm\nis applied.\nOur work is mostly related to the line of research, which exploitscontextual\nsubstitutes for the ambiguous word to diﬀerentiate between its senses. One of\n4 https://competitions.codalab.org/competitions/public_submissions/17806,\nhttps://competitions.codalab.org/competitions/public_submissions/17809,\nsee post-competition tabs\nCombining Neural Language Models for Word Sense Induction 3\nthebestperformingsystemsatSemEval2013generatedsubstitutesusingn-gram\nlanguage models [7]. Later [3] proposed using neural language models and a few\nother tweaks, establishing a new SOTA on this dataset. In section 3 we describe\ntheir method with slight modiﬁcations (adapting it to the RUSSE 2018 task)\nas our base approach and then propose several improvements. As an alternative\nto language models, [1] propose employing context2vec model [15] to generate\nsubstitutes and building from them the average word2vec representation instead\nof the bag-of-words representation. Context2vec model merges information from\nleft and right context internally, which may result in better substitutes. How-\never, context2vec requires lots of resources to train and is not readily available\nfor many languages, including Russian. In contrast, neural LMs have become\na standard resource available for many languages. Thus, in the current work\nwe focus on using pretrained LMs and improving results for WSI by externally\ncombining information from left and right context. In the preliminary exper-\niments we tried using multilingual BERT model [8], which also combines left\nand right context internally and is pretrained on texts in diﬀerent languages, in-\ncluding Russian. However, this model’s vocabulary consists mainly of subwords\n(frequently occurring pieces of words similar to morphemes). Using BERT in a\nnaive way (similarly to LMs) to generate substitutes results in small subwords\ngenerated that perform poorly for WSI. More sophisticated algorithms like [22]\nare required to generate whole words with BERT and we leave it for the future\nwork.\nSeveralcompetitionswereorganizedtocompareapproachestoWSI.SemEval\n2010 task 14 [14] and SemEval 2013 task 13 [11] are the most popular ones\nfor English. For the Russian language RUSSE 2018 competition [16] has been\nheld recently. Three datasets varying in context length and sense granularity\nhave been built for this competition. In this paper we evaluate our methods on\nthese datasets and compare our results with the best results of this competition.\nRUSSE 2018 requires hard clustering of text fragments containing an ambiguous\nword, i.e. each example shall appear in one and only one cluster. In this aspect\nit is similar to SemEval 2010 and, unlike SemEval 2013, which requires soft clus-\ntering (i.e. a probability distribution over clusters for each example). Adjusted\nRand Index (ARI) was used as a quality measure of a clustering for each word.\nThe weighted average of ARIs across all ambiguous words with weights propor-\ntional to the number of examples per word was used as the ﬁnal metric. The\nwinner of the competition didn’t submit a paper, so little is known regarding the\nbest approach, except that it used algebraic operations on word2vec embeddings\nto identify word senses [16]. However, the 2nd and the 3rd best results on all\ndatasets were achieved by calculating weighted average of word2vec embeddings\nfor context words and clustering them with either agglomerative clustering or\naﬃnity propagation [5,12].\nIn the post-competition period [18] proposed pretraining the Transformer\nsequence transduction model [20] to recover ambiguous words hidden from it’s\ninput (an approach similar to the BERT model pretraining proposed later in\n[8]) and using outputs as well as hidden states from this model to represent the\n4 Combining Neural Language Models for Word Sense Induction\nambiguous word in a context. They achieved new SOTA on one of the datasets.\nAccording to the oﬃcial post-evaluation leaderboard no other improvements has\nbeen achieved on RUSSE 2018 datasets yet (by the time of May 05 2019 and\nexcluding this paper’s results).\n3 Approach\n3.1 Baselines\nWe use the method from [3] as our baseline, with slight modiﬁcations to account\nfor the diﬀerences in the datasets. Suppose our examples look likelcr , wherec\nis the target ambiguous word andl,r are its left and right contexts. The original\nmethod does the following:\n1. Use pretrained forward and backward LMs to estimate probabilities for each\nword w to be a substitute for c given only the left contextPfwd (w|l) or\nonly the right contextPbwd(w|r). To provide more information to the LMs\nand bias it towards generating co-hyponyms of the target word, the target\nword can be included in the context using dynamic symmetric pattern “T\nand _” / “_ and T”. For instance, for the sentenceThese apples are sold\neverywhere insteadof “These _”wepass “These apples and _”totheforward\nLM, and instead of“_ are sold everywhere”we pass“_ and apples are sold\neverywhere”to the backward LM. The underscore denotes the position for\nwhich the model predicts possible words.\n2. Take top K predictions from the forward and the backward LM indepen-\ndently, renormalize their probabilities so that they sum to one, and sample\nL substitutes from each distribution, resulting in 2L substitutes. Do it S\ntimes. Each of S sets of substitutes is called a representative of the original\nexample. Build TFIDF BOW vectors for all representatives of all examples\nfor a particular ambiguous word. Additionally, all substitutes are lemma-\ntized to get rid of the grammatical bias (LMs can generate only plural or\nonly singular substitutes depending on the grammatical form of the ambigu-\nous word, so these substitutes will never intersect even for the same sense of\nthe word unless they are lemmatized).\n3. Finally, TFIDF BOW vectors are clustered using agglomerative clustering\nwith cosine distance, average linkage and the same number of clusters for all\nwords (we select it on the train set of each RUSSE 2018 dataset separately\nwhile [3] used the average number of senses in the test set of SemEval 2013).\nThe required probability distribution over clusters for each example is es-\ntimated from the number of representatives of this example found in each\ncluster.\nTo obtain hard clustering required by RUSSE 2018 we can simply select\nthe most probable cluster for each example (this method is denoted assam-\npling in our experiments). However, we have found that skipping sampling and\nusing S=1 representative consisting of top K predictions from each LM, while\nCombining Neural Language Models for Word Sense Induction 5\nbeing conceptually simpler and deterministic, also delivers better results (base\nmethod). Also we have found that dynamic symmetric patterns (we have simply\ntranslated “T and _” to Russian) sometimes help a little but generally hurt a\nlot for our best models, so we don’t use them by default. This is in line with\nthe ablation study from [3] showing that the patterns are useful for verbs and\nadjectives but almost useless for nouns, which RUSSE 2018 datasets consist of.\nWe leave integration of the patterns into our best models and experimenting on\nother datasets containing other parts of speech for the future work.\n3.2 Combining LMs\nDuring preliminary experiments we have found that using substitutes retrieved\nfrom forward and backward LMs independently results in lots of substitutes not\nrelated to the target word. For instance, consider the case when the ambiguous\nword is the ﬁrst word of a sentence. The forward LM will simply predict all\nwordswhichcanappearastheﬁrstword,andthesewordswillbeunrelatedtothe\nmeaning of the target word. Using patterns like “T and _” / “_ and T” improves\nthis situation to some extent, at least the context will always contain the target\nword. However, we noticed other problems related to these patterns. Often after\nand the model generates not nouns which are meant to be co-hyponyms of the\ntarget word but verbs instead. Probably this is related to the agreement in\nnumber between the noun and its syntactically related words. For instance, the\nLM cannot generate coordinated subjects for a singular predicate, so it tries to\ngenerate a coordinate clause instead.\nTo solve these problems we propose taking top K words from a combination\nof distributions predicted by forward and backward LMs. We experiment with\nthe following combinations.\nAverage (avg): (Pfwd + Pbwd)/2\nPositionally weighted average (pos-weight-avg): αPfwd + (1−α)Pbwd,\nwhere αis a function of normalized (divided by the example length) position of\nthe ambiguous word in the example:\nα(pos) =max(min(0.5,0.5β−1pos),0.5β−1(pos−1 + 2β))\nThis allows to weigh forward and backward LMs equally when both left and\nright context is larger thanβ times example length words and underweigh one\nof them when corresponding context becomes short.β is a hyperparameter to\nbe selected.\nBayesian combination (bayes-comb): using Bayes’ rule and supposing left\nand right context are independent given the target word we estimate the prob-\nability we are interested in as\nP(w|l,r) =P(l,r|w)P(w)\nP(l,r) = P(l|w)P(r|w)P(w)\nP(l,r) ∝P(w|l)P(w|r)\nP(w)\nThe numerator is estimated usingPfwd and Pbwd, but pretrained LMs that we\nuse don’t contain word frequencies in their vocabularies, so we cannot directly\n6 Combining Neural Language Models for Word Sense Induction\nestimate the denominator. However, their vocabularies are sorted by frequency,\nso we can estimate word frequency ranks and approximate the denominator\nwith Zipf distribution :P(w) ∝1/(rank(w))z. So ﬁnally we approximate the\nconditional probability of a substitute given context as:\nˆP(w|l,r) ∝Pfwd Pbwd(rank(w))z\nInterestingly,pointwisemutualinformation(PMI)whichisanotherpopularmea-\nsure of relatedness between a word and a context can be approximated by exactly\nthe same formula, but with diﬀerent value ofz:\nPMI (w,(l,r)) =P(w|l,r)\nP(w) ∝P(w|l)P(w|r)\n(P(w))2\nˆPMI (w,(l,r)) ∝Pfwd Pbwd(rank(w))2z\nIn contrast to conditional probability, PMI discounts frequent words and pro-\nmotes rare ones. When we selectz as a hyperparameter on the train set, eﬀec-\ntivelyweselectfromthefamilyofrelatednessmetricsoftheform P(w|l,r)/(P(w))k\none, which is optimal regarding the ﬁnal evaluation metric (ARI).\n3.3 Clustering\nFollowing the original method, we exploit agglomerative clustering, but for each\nword select individual number of clusters. This approach is not only linguistically\nmore plausible than using the same number of clusters for all words, but also\nresulted in signiﬁcant improvement of the ﬁnal results.\nAgglomerative clustering has three hyperparameters: the function deﬁning\nthe distance between examples (aﬃnity), the function deﬁning the distance be-\ntween clusters (linkage), and the number of clusters. Initially each example is\nput into a separate cluster. At each iteration, two nearest clusters are merged,\nuntil the speciﬁed number of clusters is reached. We use cosine aﬃnity and aver-\nage linkage (the distance between clusters is the average cosine distance between\ntheir members). For each word we select the number of clusters, which maximizes\nthe silhouette score:\n1\nn\nn∑\ni=1\nbi −ai\nmax(ai,bi),\nwhere ai is the mean distance from the i-th example to all examples from the\nsame cluster, andbi is the mean distance from the i-th example to all examples\nfrom the nearest diﬀerent cluster. We compare this approach to another one,\nwhich sets for all words a single number of clusters selected on the train set.\n4 Experiments and results\n4.1 Datasets and evaluation\nWe evaluate our methods on the three datasets from RUSSE 2018 WSI shared\ntask for the Russian language [16] with diﬀerent sense granularity, context length\nCombining Neural Language Models for Word Sense Induction 7\nand number of examples. Both sense inventory and examples for the active-dict\ndataset were extracted from the Active Dictionary of Russian [4], which is an\nexplanatory dictionary of the contemporary Russian language. It contains 253\nambiguous words with 3.5 senses per word on average. The bts-rnc dataset con-\ntains examples for 81 ambiguous words extracted from the Russian National\nCorpus5 and labeled with their senses according to the Large Explanatory Dic-\ntionary6, has 3.1 senses per word on average. There are more examples per word\ncompared to the active-dict (124 vs. 23 on average) and the examples are twice\nas long. The wiki-wiki dataset is the smallest one, it contains only 9 ambiguous\nwords with 2.2 senses and 109 examples per word on average. This dataset con-\ntains only homonyms (words having several unrelated senses), so the senses are\ncoarse-grained. Each dataset of RUSSE 2018 is split into train, public test and\nprivate test parts. We held out the private test parts to compare our best meth-\nods with previous SOTA results, and used the train and the public test parts for\nmodels and hyperparameters selection. We didn’t use wiki-wiki for development\nand comparison of LM combination methods due to its small size. However, we\ndo compare our best models to previously best results on all three datasets.\nThe metric used in RUSSE 2018 competition for the ﬁnal ranking of partic-\nipants is Adjusted Rand Index (ARI). However, diﬀerent clustering evaluation\nmetrics may exhibit diﬀerent biases. For better comparison of our results to the\nprevious best results we also adopt two complementary metrics used in the Se-\nmEval 2010 competition which also requires hard clustering, namely V-measure\nand paired F-score [14]. The former metric is biased towards a large number of\nclusters, while the latter metric prefers small number of clusters, so the geomet-\nric mean of these two metrics (AVG) proposed by [23] is usually reported. How-\never, all aforementioned metrics are aﬀected both by vectorization and clustering\nmethods along with their hyperparameters which makes it diﬃcult to compare\nvectorization methods alone. To estimate vectorization quality while abstracting\nfrom clustering hyperparameters selection, we propose and exploit maxARI met-\nric, which is maximum possible ARI achieved by agglomerative clustering with\nall possible hyperparameter values (varying distance metric, linkage and, most\nimportantly, the number of clusters). Clearly, this is an overestimation of pos-\nsible results for a particular vectorization when using agglomerative clustering\nbecause it selects hyperparameters using gold labels. However, we have found\nthis metric very useful for intermediate comparison and selecting hyperparame-\nters of vectorization methods.\n4.2 Comparison of LMs for the Russian Language\nWe compare ELMo LMs [17] trained on three diﬀerent corpora (Wikipedia,\nWMT News and Twitter) for the Russian language.7 Also we try ULMFiT [10]\nLM trained on a subset of the Russian Taiga corpus8 which is available for the\n5 http://ruscorpora.ru\n6 http://gramota.ru/slovari/info/bts\n7 http://docs.deeppavlov.ai/en/master/intro/pretrained_vectors.html\n8 https://github.com/mamamot/Russian-ULMFit\n8 Combining Neural Language Models for Word Sense Induction\nRussian language only as a forward LM. To make the comparison fair we used\nforward and backward LMs separately. Table 1 contains main characteristics of\nthese models taken from corresponding web pages.\nmodel corpora size, tokensvocabulary size, wordsperplexity\nelmo-news 946M 1M 49.9\nelmo-twitter 810M 1M 94.1\nelmo-wiki 386M 1M 43.7\nulmﬁt 208M 60K 21.98\nTable 1: LMs for the Russian language\nFig.1: Comparison of unidirectional LMs available for Russian. Number of clus-\nters (nc) is selected for each word individually by maximizing either ARI using\ngold labels (for maxARI estimation) or silhouette score.\nFigure1showsthatELMoLMstrainedontheRussianWikipediaoutperform\nall other LMs by a large margin on the wiki-wiki dataset, which was also built\nfrom the Russian Wikipedia. All backward LMs are signiﬁcantly better than\ntheir forward counterparts on this dataset due to longer right contexts. This\nis simply because contexts in wiki-wiki are rather large and frequently contain\nseveral occurrences of the ambiguous word, while we generate substitutes for\nthe ﬁrst occurrence only. For the other two datasets ELMo LMs trained on\nWMT News and Wikipedia give comparable results, the former having slightly\nbetter maxARI on bts-rnc. ELMo LMs trained on Twitter perform slightly but\nconsistently worse, and ULMFiT gives much worse results than other models. We\nsuppose that bad performance of the ULMFiT model for WSI may be related\nto relatively small vocabulary (60K words compared to 1M in ELMo LMs),\nwhich may prevent generating substitutes that can discriminate diﬀerent senses.\nBased on these results we have selected ELMo LMs trained on WMT News\nfor all further experiments on bts-rnc and active-dict, and the ones trained on\nWikipedia for wiki-wiki. It is also worth exploring combinations of diﬀerent\nmodels trained on diﬀerent corpora in the future work.\nCombining Neural Language Models for Word Sense Induction 9\n4.3 Comparison of LM combination methods\nFigure 2 shows WSI results on the train sets depending on forward and back-\nward LM combination method compared to the baselines and unidirectional LMs\nalone. Bayesian combination is the best combination method on all datasets ac-\ncording to both maxARI and ARI (excluding maxARI on bts-rnc for the ﬁxed\nnumber of clusters which is worse than individual number of clusters anyway).\nSurprisingly, the original method (sampling) often performs worse than unidi-\nrectional LMs. However, our deterministic modiﬁcation (base) performs better\nor comparable to them and all proposed combination methods improve its re-\nsults.\nFig.2: Comparison of LM combination methods. Fixed nc denotes using the\nsame number of clusters for all words.\nIt is trivial that performance upper bound when selecting individual number\nof clusters per word (nc by gold labels) is always better than using the same\nnumber of clusters for all words (ﬁxed nc by gold labels). However, we would\nlike to emphasize the large diﬀerence. Surprisingly, selecting individual number\nof clusters based on silhouette score (nc by silhouette), which doesn’t exploit\ngold labels, often gives better or similar results to the upper bound for the ﬁxed\nnumber of clusters. Unfortunately, the margin between ARI and maxARI is\nstill large so it worth experimenting with other methods of selecting individual\nnumber of clusters.\n4.4 Comparison to the previous best results\nTable 2 compares our baselines and best methods to the previous best methods\naccording to the oﬃcial leaderboard. The best previous results are improved on\nbts-rnc and active-dict by a large margin (which is especially large on bts-rnc\npresumably due to longer contexts which make neural LMs generating better\nsubstitutes). The results of our baseline method is also better than previous\nresults on bts-rnc, but little worse on active-dict, which underlines the beneﬁts\nof combining forward and backward LMs properly, especially when contexts\nare short. For both the baseline and the best vectorization method selecting the\nnumber of clusters based on silhouette score works signiﬁcantly better than using\nﬁxed number of clusters selected on the train sets.\n10 Combining Neural Language Models for Word Sense Induction\nModel\nbts-rnc wiki-wiki active-dict\nTest ARI Test ARI Test ARI\nPublic Private Public Private Public Private\nbayes-comb-silnc 0.502 0.451 0.651 0.616 0.331 0.298\nbayes-comb-ﬁxnc 0.464 0.438 0.651 0.682 0.304 0.260\nbase-silnc 0.365 0.362 0.651 0.646 0.202 0.162\nbase-ﬁxnc 0.328 0.298 0.651 0.394 0.143 0.141\npost-competition improvement - - - - 0.307 0.234\ncompetition best result 0.351 0.338 1.0 0.962 0.264 0.248\ncompetition 2nd best result 0.281 0.281 1.0 0.659 0.236 0.227\nTable 2: Comparison with previous best results. Selecting the number of clusters\nindividually using silhouette score (silnc) or as a hyperparameter on train (ﬁxnc).\nIn the public wiki-wiki test set there are only 2 words, one of them was\nclustered perfectly by all methods and another has only one sense while our\nmethods split it into two clusters. The public test set contains 4 words for which\nour results are comparable to the competition 2nd best results but are much\nworse than the best results. The analysis of these results has revealed that such\na large gap is mostly due to suboptimal number of clusters selected by our\nmethods on wiki-wiki. Using our vectorization and clustering but with the same\nnumber of clusters as in the best submission improves our ARI on the private\ntest set to 0.89 while maxARI is 0.95. Also it is possible that substitutes-based\nmethods are suboptimal for homonyms because unrelated senses are likely to\nbe accompanied by unrelated context words, so context clustering approaches\nmay perform better in this scenario. However, experiments on larger datasets\nconsisting of homonyms are needed to check this hypothesis.\nModel Public Test Private Test\nF-Sc V-M AVG #Cl F-Sc V-M AVG #Cl\nbayes-comb-silnc 0.795 0.454 0.601 4.0 0.776 0.432 0.579 3.8\nbayes-comb-ﬁxnc 0.798 0.404 0.568 4.0 0.772 0.421 0.570 4.0\nbase-silnc 0.721 0.351 0.503 2.9 0.702 0.363 0.505 2.9\nbase-ﬁxnc 0.774 0.271 0.458 4.0 0.756 0.294 0.471 4.0\npost-competition improvement - - - - - - - -\ncompetition best result 0.731 0.271 0.445 2.4 0.710 0.3 0.462 2.3\ncompetition 2nd best result 0.692 0.288 0.446 10.0 0.683 0.298 0.451 10.0\n1 cluster per word 0.764 0.0 0.0 1.0 0.726 0.0 0.0 1.0\n1 cluster per instance 0.0 0.221 0.004 130.6 0.0 0.244 0.005 127.5\nTable 3: Semeval 2010 metrics on bts-rnc.\nTo provide better comparison, in tables 3,4 we report the metrics from Se-\nmEval 2010 Task 14 which is a similar competition for English. We have down-\nloaded the previous best submissions from the RUSSE 2018 leaderboard and\nused the oﬃcial SemEval 2010 evaluation script to calculate paired F-score (F-\nSc) and V-measure (V-M). The results of two primitive baselines, one placing all\nexamples into a single cluster and another allocating a separate cluster for each\nexample, are calculated to show that both F-score and V-measure are highly\nCombining Neural Language Models for Word Sense Induction 11\nbiased towards small or large number of clusters and rather useless in isolation.\nThus, we additionally report their geometric mean (AVG) and the average num-\nberofclustersperword(#Cl).Onbts-rncourmethodsoutperformallpreviously\nbest methods according to all metrics. On active-dict F-score of our methods is\nlittle worse, but V-measure and, most importantly, AVG are better. Worse F-\nScore can be explained by the larger number of clusters our methods produce.\nAppendix 4.5 provides more detailed analysis of pros and cons of our approach\ncompared to the previous best submissions.\nModel Public Test Private Test\nF-Sc V-M AVG #Cl F-Sc V-M AVG #Cl\nbayes-comb-silnc 0.484 0.538 0.511 5.2 0.459 0.505 0.482 5.4\nbayes-comb-ﬁxnc 0.453 0.527 0.489 6.0 0.421 0.479 0.449 6.0\nbase-silnc 0.401 0.395 0.398 4.6 0.362 0.365 0.363 5.1\nbase-ﬁxnc 0.349 0.367 0.358 5.0 0.351 0.352 0.351 5.0\npost-competition improvement0.513 0.451 0.481 3.0 0.464 0.389 0.425 3.0\ncompetition best result 0.489 0.411 0.445 3.2 0.467 0.392 0.428 3.4\ncompetition 2nd best result 0.468 0.377 0.420 3.0 0.468 0.371 0.417 3.0\n1 cluster per word 0.433 0.0 0.0 1.0 0.437 0.0 0.0 1.0\n1 cluster per instance 0.0 0.55 0.014 21.9 0.0 0.543 0.010 22.4\nTable 4: Semeval 2010 metrics on active-dict.\n4.5 Analysis of the results\nInthissectionweanswertwoquestions:doweselectthenumberofclustersbetter\nand can we cluster a target word occurrences into given number of clusters better\nthanthepreviousbestmethod?Wecompareourbestsubmissionstotheprevious\nbest submissions according to the private test ARI on bts-rnc and active-dict.\nTo answer the ﬁrst question, we calculated the mean squared error (MSE)\nbetween the number of clusters in each submission and the true number of senses.\nOur method estimates the number of senses much worse (MSE is 3.41 versus\n1.65 on bts-rnc and 8.48 versus 1.20 on active-dict), usually returning larger\nnumber of clusters. But is the optimal number of clusters equal to the true\nnumber of senses? In our case the data contains outliers (i.e. points which are\nfar from all other points due to non-informative context or other vectorization\nproblems). Thus, the number of clusters should be larger than the true number\nof senses. Otherwise, some senses will be merged with other senses while outliers\nwill occupy their clusters. The results below and Appendix B provide some\nevidences that our method better estimates the optimal number of clusters.\nIt is hard to make comparison of vectorization and clustering algorithms\nseparately, since we don’t have vectors and cannot vary the number of clusters for\nthe previous methods, only clustering results are available. One thing we can do\nis clustering examples of i-th word into the number of clustersPi taken from the\nprevious best submission, but using our vectors and clustering algorithm. Then\nthe diﬀerence in performance cannot be due to the diﬀerence in the number of\n12 Combining Neural Language Models for Word Sense Induction\nclusters selected. This is achieved by two techniques. The ﬁrst one (bayes-comb-\nprevnc) simply sets the number of clusters equal toPi in our agglomerative\nclustering, which is suboptimal due to outliers. Figure 3 shows that this works\nmuch worse than the number of clusters selected using silhouette score according\nto all metrics (except F-score on active-dict). Compared to the previous best\nsubmissions the results are mixed. Another technique (bayes-comb-prevnc2) ﬁrst\nclusters the occurrences of each word into the number of clustersSi selected by\nsilhouette score. IfSi >Pi, then we leavePi largest clusters intact and distribute\nall other examples among them by moving each example to the nearest cluster.\nOtherwise, we simply cluster examples intoPi clusters like before. This results\nin much better clustering compared to the previous best submissions given the\nsame number of clusters according to all metrics. To conclude, compared to the\nprevious best submissions our approach has selected the number of clusters that\nis better according to several evaluation metrics for our vectorization, but further\nfrom the true number of senses. It can also cluster the datasets better when the\nnumber of clusters is taken from another submissions and is not optimal given\nour vectorization.\nFig.3: Comparison of our and previous best submissions with diﬀerent and equal\nnumber of clusters.\n5 Conclusion\nBidirectional neural LMs are a powerful instrument for diﬀerent tasks including\nsubstitutes generation for WSI, but some tricks should be used to apply them\nto this task properly. We have proposed and compared several methods of com-\nbining forward and backward LMs for better substitutes generation. Also we\nhave proposed a technique for selecting individual number of clusters per word\nand this improved results even further. We improved previous best results on\ntwo datasets from RUSSE 2018 WSI shared task for the Russian language by\na large margin. Finally, we have compared several Russian LMs regarding their\nperformance for WSI.\nCombining Neural Language Models for Word Sense Induction 13\n6 Acknowledgements\nWe are grateful to Dima Lipin, Artem Grachev and Alex Nevidomsky for their\nvaluable help.\nReferences\n1. Alagi´ c, D.,ˇSnajder, J., Pad´ o, S.: Leveraging lexical substitutes for unsupervised\nword sense induction. In: Thirty-Second AAAI Conference on Artiﬁcial Intelligence\n(2018)\n2. Amplayo, R.K., won Hwang, S., Song, M.: Autosense model for word sense induc-\ntion. In: AAAI (2019)\n3. Amrami, A., Goldberg, Y.: Word sense induction with neural biLM and symmetric\npatterns. In: Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing. pp. 4860–4867. Association for Computational Linguistics,\nBrussels, Belgium (2018),https://www.aclweb.org/anthology/D18-1523\n4. Apresjan, V.: Active dictionary of the russian language: theory and practice.\nMeaning-Text Theory 2011, 13–24 (2011)\n5. Arefyev, N., Ermolaev, P., Panchenko, A.: How much does a word weigh? weight-\ning word embeddings for word sense induction. In: Computational Linguistics and\nIntellectual Technologies: Papers from the Annual International Conference “Dia-\nlogue”. pp. 68–84. RSUH, Moscow, Russia (2018)\n6. Bartunov, S., Kondrashkin, D., Osokin, A., Vetrov, D.: Breaking sticks and ambi-\nguities with adaptive skip-gram. In: Proceedings of the International Conference\non Artiﬁcial Intelligence and Statistics (AISTATS) (2016)\n7. Baskaya, O., Sert, E., Cirik, V., Yuret, D.: Ai-ku: Using substitute vectors and\nco-occurrence modeling for word sense induction and disambiguation. In: Second\nJoint Conference on Lexical and Computational Semantics (* SEM), Volume 2:\nProceedings of the Seventh International Workshop on Semantic Evaluation (Se-\nmEval 2013). vol. 2, pp. 300–306 (2013)\n8. Devlin,J.,Chang,M.W.,Lee,K.,Toutanova,K.:Bert:Pre-trainingofdeepbidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n9. Hope, D., Keller, B.: UoS: A Graph-Based System for Graded Word Sense In-\nduction. In: Second Joint Conference on Lexical and Computational Semantics\n(*SEM), Volume 2: Proceedings of the Seventh International Workshop on Seman-\ntic Evaluation (SemEval 2013). pp. 689–694. No. 1, Atlanta, Georgia, USA (2013),\nhttp://www.aclweb.org/anthology/S13-2113\n10. Howard, J., Ruder, S.: Universal language model ﬁne-tuning for text classiﬁcation.\nIn: Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). pp. 328–339. Association for Computational\nLinguistics, Melbourne, Australia (2018), https://www.aclweb.org/anthology/\nP18-1031\n11. Jurgens, D., Klapaftis, I.: Semeval-2013 task 13: Word sense induction for graded\nand non-graded senses. In: Second Joint Conference on Lexical and Computational\nSemantics (* SEM), Volume 2: Proceedings of the Seventh International Workshop\non Semantic Evaluation (SemEval 2013). vol. 2, pp. 290–299 (2013)\n12. Kutuzov, A.: Russian word sense induction by clustering averaged word embed-\ndings. CoRR abs/1805.02258 (2018),http://arxiv.org/abs/1805.02258\n14 Combining Neural Language Models for Word Sense Induction\n13. Lau, J.H., Cook, P., Baldwin, T.: unimelb: Topic Modelling-based Word Sense\nInduction. In: Second Joint Conference on Lexical and Computational Semantics\n(*SEM): SemEval 2013). vol. 2, pp. 307–311. Atlanta, Georgia, USA (2013),http:\n//www.aclweb.org/anthology/S13-2051\n14. Manandhar, S., Klapaftis, I.P., Dligach, D., Pradhan, S.S.: Semeval-2010 task 14:\nWord sense induction & disambiguation. In: Proceedings of the 5th international\nworkshop on semantic evaluation. pp. 63–68. Association for Computational Lin-\nguistics (2010)\n15. Melamud, O., Goldberger, J., Dagan, I.: context2vec: Learning generic context em-\nbedding with bidirectional lstm. In: Proceedings of The 20th SIGNLL Conference\non Computational Natural Language Learning. pp. 51–61 (2016)\n16. Panchenko, A., Lopukhina, A., Ustalov, D., Lopukhin, K., Arefyev, N., Leon-\ntyev, A., Loukachevitch, N.: RUSSE’2018: A Shared Task on Word Sense In-\nduction for the Russian Language. In: Computational Linguistics and Intellec-\ntual Technologies: Papers from the Annual International Conference “Dialogue”.\npp. 547–564. RSUH, Moscow, Russia (2018),http://www.dialog-21.ru/media/\n4324/panchenkoa.pdf\n17. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle-\nmoyer, L.: Deep contextualized word representations. In: Proc. of NAACL (2018)\n18. Struyanskiy, O., Arefyev, N.: Neural networks with attention for word sense induc-\ntion. In: Supplementary Proceedings of the Seventh International Conference on\nAnalysis of Images, Social Networks and Texts (AIST 2018), Moscow, Russia, July\n5 - 7, 2018. pp. 208–213 (2018),http://ceur-ws.org/Vol-2268/paper23.pdf\n19. Tang, G., M¨ uller, M., Rios, A., Sennrich, R.: Why self-attention? a targeted evalu-\nation of neural machine translation architectures. arXiv preprint arXiv:1808.08946\n(2018)\n20. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. pp. 5998–6008 (2017)\n21. V´ eronis, J.: Hyperlex: lexical cartography for information retrieval. Computer\nSpeech & Language 18(3), 223–252 (2004)\n22. Wang, A., Cho, K.: BERT has a mouth, and it must speak: BERT as a markov\nrandom ﬁeld language model. CoRR abs/1902.04094 (2019),http://arxiv.org/\nabs/1902.04094\n23. Wang, J., Bansal, M., Gimpel, K., Ziebart, B.D., Yu, C.T.: A sense-topic model for\nword sense induction with unsupervised data enrichment. TACL 3, 59–71 (2015)\nCombining Neural Language Models for Word Sense Induction 15\nA Examples of substitutes generated\nTable 5 provides examples of discriminative substitutes with their relative fre-\nquencies for each of two most frequent senses of several words. A substitute is\ncalled discriminative if it is frequently generated for one sense of an ambigu-\nsous word, but rarely for another. Formally, we take substitutes with the largest\nP(w|sense1)\nP(w|sense2) , whereP(w|sensei) is estimated using add-one smoothing:\nP(w|sensei) = cnt(w|sensei) + 1\ncnt(sensei) +|vocab|\nAdditionally, we leave only substitutes which were generated at lest 10 times for\none of the senses.\nБалка Штамп Лавка\nNumber of examples: 81\nSense1: Горизонтальный\nопорный брус\nNumber of examples: 45\nSense1: Рельефное устройство\nдля получения одинаковых\nграфических оттисков\nNumber of examples: 67\nSense1: Скамья для\nсидения или лежания\nперегородка 0.56/0.00\nлюстра 0.52/0.00\nкарниз 0.48/0.00\nкрышка 0.43/0.00\nпанель 0.41/0.00\nкозырёк 0.35/0.00\nкаркас 0.33/0.00\nпотолок 0.33/0.00\nперекрытие 0.33/0.00\nпластина 0.32/0.00\nсправка 0.58/0.00\nпечать 0.56/0.00\nподпись 0.53/0.00\nпометка 0.51/0.00\nбирка 0.44/0.00\nталон 0.42/0.00\nксерокопия 0.42/0.00\nвыписка 0.42/0.00\nпрописка 0.40/0.00\nбланк 0.40/0.00\nкорточки 0.49/0.00\nподушка 0.46/0.00\nковрик 0.46/0.00\nтрибуна 0.39/0.00\nпростыня 0.37/0.00\nодеяло 0.36/0.00\nчетвереньки 0.36/0.00\nпалуба 0.33/0.00\nкаталка 0.28/0.00\nносилки 0.25/0.00\nNumber of examples: 38\nSense2: Длинный и\nширокий овраг\nNumber of examples: 47\nSense2: Принятый образец,\nкоторому следуют\nбез размышлений\nNumber of examples: 82\nSense2: Небольшой\nмагазин\nдеревня 0.01/0.47\nстепь 0.01/0.53\nречушка 0.00/0.29\nтайга 0.00/0.29\nперевал 0.00/0.29\nбухта 0.00/0.29\nозеро 0.00/0.29\nдолина 0.00/0.34\nречка 0.01/0.74\nроща 0.00/0.42\nгений 0.02/0.26\nстереотип 0.04/0.40\nсюжет 0.02/0.28\nмиф 0.02/0.28\nпафос 0.02/0.28\nритм 0.00/0.23\nскучный 0.00/0.23\nканон 0.00/0.26\nстиль 0.00/0.28\nжанр 0.00/0.30\nгостиница 0.00/0.28\nзакусочный 0.00/0.30\nтипография 0.00/0.33\nкасса 0.00/0.34\nфабрика 0.00/0.34\nсупермаркет 0.00/0.35\nбар 0.00/0.37\nкофейня 0.00/0.37\nмагазин 0.00/0.44\nаптека 0.00/0.48\nTable 5: Discriminative substitutes for several words from bts-rnc train\nTable 6 lists ten most probable substitutes according to the combined dis-\ntribution and according to the forward and the backward LM distributions sep-\narately for several examples. Substitutes from unidirectional distributions are\nvery sensitive to the position of the target word. When either left or right con-\ntext doesn’t contain enough information at least halve of the substitutes will be\nnot related to the target word. Combined distribution provides more relevant\nsubstitutes.\n16 Combining Neural Language Models for Word Sense Induction\nBayes-comb Base forward Base backward\nНет , я по-прежнему проживаю в своей квартире , и в паспорте есть нужный\nштамп. Просто сотни жителей в моем и соседних домах уже несколько\nмесяцев живут\nштамп, этаж, номер, дом, ключ,\nабзац, прочерк, подпункт, пункт,\nпробел\nштамп, номер, пункт,\nдокумент, знак\nул, м., обл,\nсм, пр\nОн был очень высок , наклонил голову , словно подпирая плечом\nпотолочнуюбалку, посмотрел на Сьянову серьезными черными глазами .\nзанавеску, перегородку, ручку,\nподушку, стенку, табуретку,\nпроволоку, стену, раму, плиту\nстенку, подушку,\nперегородку, дверь,\nстену\nувидел, помню,\nоглянулся,\nпосмотрел, встал\nИногда туманным , осенним вечером он проходил вдольопушкилеса ,\nшурша омертвевшими листьями , подобрав длинную , черную рясу ;\nопушки, кромки, заснеженного,\nживописного, посреди,\nсоснового, цветущего, чащи,\nтропического, стога\nзабора, берега,\nстен, реки,\nкромки\nпосреди, глубь,\nопушке, вдоль,\nвглубь\nОднажды , когда страховой агент заполнял мойполис, он допустил\nошибку и написал меньшее количество лошадиных сил .\nбланк, талон, формуляр, полис,\nанкету, талончик, бак, протокол,\nводительский, профиль\nбланк, номер,\nполис, паспорт,\nанкету\nвидимо, вероятно,\nвозможно, значит,\nкажется\nзаранее старается выговорить для себя немало льгот . Так , например ,\nу него остаетсяпостпочетного президента ФХР , солидная\nпенсия ( около 100 тысяч рублей в месяц ) , оплачиваемая\nстипендия, привилегия, оклад,\nвиза, зарплата, диплом, значок,\nвакансия, лицензия, выслуга\nправо, возможность,\nшанс, квартира,\nмасса\nзвание, приз,\nдолжность,\nстипендия, пост\nTable 6: Substitutes generated for randomly selected examples.\nB The number of clusters selected\nFigure 4 plots distributions of the diﬀerences between the true number of senses,\nthe number of clusters in submissions and the optimal number of clusters. Silhou-\nette score gives the number of clusters, which is usually larger than the number\nof senses, but is near the optimum with respect to ARI and given our vectors.\nThe previous best submissions better estimate the true number of senses.\nC Hyperparameters\nTable 7 shows the selected hyperparameters for the methods described in sec-\ntion 3. For bts-rnc and active-dict datasets hyperparameters were selected using\ngrid search on corresponding train sets. For wiki-wiki we used the hyperparam-\neters from bts-rnc due to very small size of wiki-wiki train set. We selected the\nfollowing hyperparameters.\n1. Add bias(True/False). Ignoring bias in the softmax layer of the LM was\nproposed by [3] to improve substitutes, because adding bias results in pre-\ndiction of frequency words instead of rare but relevant substitutes.\n2. Normalize output embeddings(True/False). Similarly to ignoring bias,\nthis may result in prediction of more relevant substitutes.\nCombining Neural Language Models for Word Sense Induction 17\ntrue_nc-silnc\ntrue_nc-prev_best_nc\nmax_ari_nc-silnc\nbts-rnc\n6\n 4\n 2\n 0 2 4 6\ndifference in the number of clusters\ntrue_nc-silnc\ntrue_nc-prev_best_nc\nmax_ari_nc-silnc\nactive-dict\nFig.4: Comparison of the number of clusters in our (silnc) and previous best\nsubmissions (prev_best_nc) with the true number of senses (true_nc) and the\noptimal number of clusters (max_ari_nc).\n3. K (10-400). The number of substitutes from each distribution.\n4. Exclude Target(True/False). We want the substitutes for diﬀerent senses\nof the target word to be non-overlapping. Thus, it may be beneﬁcial to\nexclude the target word from the substitutes.\n5. TFIDF (True/False). Applying TFIDF transformation to bag-of-words vec-\ntors of substitutes sometimes improve performance.\n6. S (=20). The number of representatives for each example. It didn’t aﬀect\nthe performance so we use the value from [3].\n7. L (4-30). The number of substitutes to sample from top K predictions.\n8. z (1.0-3.0). The parameter of Zipf distribution.\n9. β (0.1-0.5). Relative length of the left or the right context after which the\ndiscounting of the corresponding LM begins.\nMethod Add bias\nNormalize\noutput\nembeddings\nK Exclude\nTarget TF-IDF S L z β\nbts-rnc\nbayes comb False False 200 True False - - 2.0 -\npos weight avg False False 200 True False - - - 0.1\navg False False 150 True False - - - -\nbase False False 200 True False - - - -\nsampling False False 200 True True 20 15 - -\nforward False True 150 True False - - - -\nbackward False False 300 True False - - - -\nactive-dict\nbayes comb False False 200 True True - - 2.0 -\npos weight avg False False 200 True True - - - 0.1\navg False False 150 True True - - - -\nbase False False 200 True True - - - -\nsampling False False 200 True True 20 10 - -\nforward False True 100 True True - - - -\nbackward False True 300 True True - - - -\nTable 7: Selected hyperparameters",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8697085380554199
    },
    {
      "name": "Word (group theory)",
      "score": 0.8376482725143433
    },
    {
      "name": "Natural language processing",
      "score": 0.6503104567527771
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6474425792694092
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6445496082305908
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5931410193443298
    },
    {
      "name": "Language model",
      "score": 0.503419816493988
    },
    {
      "name": "Task (project management)",
      "score": 0.5026960372924805
    },
    {
      "name": "Word-sense disambiguation",
      "score": 0.43537747859954834
    },
    {
      "name": "Artificial neural network",
      "score": 0.4282747507095337
    },
    {
      "name": "Linguistics",
      "score": 0.15600553154945374
    },
    {
      "name": "Machine learning",
      "score": 0.14434540271759033
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "WordNet",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210141363",
      "name": "Samsung (Russia)",
      "country": "RU"
    }
  ]
}