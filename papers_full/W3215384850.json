{
    "title": "Visual-Semantic Transformer for Scene Text Recognition",
    "url": "https://openalex.org/W3215384850",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5101650475",
            "name": "Xin Tang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5063437341",
            "name": "Yongquan Lai",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5054514318",
            "name": "Ying Liu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100715195",
            "name": "Yuanyuan Fu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5101550831",
            "name": "Rui Fang",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1998042868",
        "https://openalex.org/W3034447740",
        "https://openalex.org/W3169016642",
        "https://openalex.org/W2144554289",
        "https://openalex.org/W3003938330",
        "https://openalex.org/W3035106683",
        "https://openalex.org/W603908379",
        "https://openalex.org/W2146835493",
        "https://openalex.org/W1971822075",
        "https://openalex.org/W3004846386",
        "https://openalex.org/W2194187530",
        "https://openalex.org/W3175618949",
        "https://openalex.org/W3035449864",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2520774189",
        "https://openalex.org/W2795619303",
        "https://openalex.org/W3174658120",
        "https://openalex.org/W3003642782",
        "https://openalex.org/W3099782249",
        "https://openalex.org/W3202415716",
        "https://openalex.org/W2343052201",
        "https://openalex.org/W2998382406",
        "https://openalex.org/W1922126009",
        "https://openalex.org/W2997864923",
        "https://openalex.org/W3013224334",
        "https://openalex.org/W2950178297",
        "https://openalex.org/W2140132917",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W3202912918",
        "https://openalex.org/W1491389626",
        "https://openalex.org/W2768779776",
        "https://openalex.org/W2982148195",
        "https://openalex.org/W2965066169",
        "https://openalex.org/W2593572697",
        "https://openalex.org/W3175855397",
        "https://openalex.org/W3043311956",
        "https://openalex.org/W3186819370",
        "https://openalex.org/W2948798935",
        "https://openalex.org/W3082397598",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W3036601975",
        "https://openalex.org/W2972424968",
        "https://openalex.org/W2952285877",
        "https://openalex.org/W2963585992",
        "https://openalex.org/W2964300754",
        "https://openalex.org/W2810983211",
        "https://openalex.org/W3014074635",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W3110267192",
        "https://openalex.org/W1981283549",
        "https://openalex.org/W3134064484",
        "https://openalex.org/W3135651738",
        "https://openalex.org/W2965097167",
        "https://openalex.org/W70975097"
    ],
    "abstract": "Modeling semantic information is helpful for scene text recognition. In this work, we propose to model semantic and visual information jointly with a Visual-Semantic Transformer (VST). The VST first explicitly extracts primary semantic information from visual feature maps with a transformer module and a primary visual-semantic alignment module. The semantic information is then joined with the visual feature maps (viewed as a sequence) to form a pseudo multi-domain sequence combining visual and semantic information, which is subsequently fed into an transformer-based interaction module to enable learning of interactions between visual and semantic features. In this way, the visual features can be enhanced by the semantic information and vice versus. The enhanced version of visual features are further decoded by a secondary visual-semantic alignment module which shares weights with the primary one. Finally, the decoded visual features and the enhanced semantic features are jointly processed by the third transformer module obtaining the final text prediction. Experiments on seven public benchmarks including regular/ irregular text recognition datasets verifies the effectiveness our proposed model, reaching state of the art on four of the seven benchmarks.",
    "full_text": "Visual-Semantic Transformer for Scene Text Recognition\nXin Tang*, Yongquan Lai*, Ying Liu, Yuanyuan Fu, Rui Fang\nVisual Computing Group, Ping An Property & Casualty Insurance Company, Shenzhen, China\ntangxin051@pingan.com.cn,tangxint@gmail.com\nAbstract\nModeling semantic information is helpful for scene text\nrecognition. In this work, we propose to model semantic\nand visual information jointly with a Visual-Semantic Trans-\nformer (VST). The VST ﬁrst explicitly extracts primary se-\nmantic information from visual feature maps with a trans-\nformer module and a primary visual-semantic alignment\nmodule. The semantic information is then joined with the vi-\nsual feature maps (viewed as a sequence) to form a pseudo\nmulti-domain sequence combining visual and semantic in-\nformation, which is subsequently fed into an transformer-\nbased interaction module to enable learning of interactions\nbetween visual and semantic features. In this way, the visual\nfeatures can be enhanced by the semantic information and\nvice versus. The enhanced version of visual features are fur-\nther decoded by a secondary visual-semantic alignment mod-\nule which shares weights with the primary one. Finally, the\ndecoded visual features and the enhanced semantic features\nare jointly processed by the third transformer module ob-\ntaining the ﬁnal text prediction. Experiments on seven pub-\nlic benchmarks including regular/ irregular text recognition\ndatasets veriﬁes the effectiveness our proposed model, reach-\ning state of the art on four of the seven benchmarks.\nIntroduction\nScene text recognition (STR) is the task of recognizing text\nfrom images taken in complex scenes such as street view. It\nis an inherently difﬁcult task due to the variance of shape,\ncolor, scale and appearance of the embedded text. Clutter\nbackground, large perspective distortion, lighting condition,\ndegraded image quality due to motion/out-of-focus blur also\nimpose severe challenges to the successfully solving the\ntask, resulting severe miss-predictions.\nDespite its difﬁculty, STR has many real-world applica-\ntions ranging from self-driving cars (Yu et al. 2019), street\nimage understanding to applications such as instant transla-\ntion and intelligent text reading in smart-phones (Wu et al.\n2019). For decades, STR has been an active research direc-\ntion (Baek et al. 2019; Li et al. 2019; Shi et al. 2018; Wan\net al. 2020; Long, He, and Yao 2021), attracting many efforts\nin designing new models and creating new datasets in order\nto solve the problem.\n*These authors contributed equally.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nA complete approach to recognizing text from scene im-\nages usually involves text detection and text recognition. In\nthis paper, we assume that text detection is done and only\nfocus on the recognition part. That is, we assume the input\nto our model is a cropped image with regular or irregular\ncharacters lying in it. A large portion of the previous work\nand many open-source datasets (Karatzas et al. 2013, 2015)\nfollow from this assumption. In this work, we will refer to\nSTR as just recognizing text from cropped images.\nThe approaches to solving STR problem can be roughly\ndivided into two categories: linguistic-based and linguistic-\nfree. Linguistic-based methods refer to those which incor-\nporate vocabulary (dictionary) or lexicon (parts of words),\nwhile linguistic-free methods use only the images them-\nselves without relying on explicit language modeling,\nwhether from internal or external, pretrained or from-\nscratch.\nThe motivation of this work is multi-fold. Firstly, to deal\nwith the cases when visual information alone is inadequate,\nsemantic or linguistic features has been introduced in vari-\nous effects, among which (Qiao et al. 2020) propose a se-\nmantic enhanced encoder-decoder framework to recognize\nlow-quality scene texts. A semantic module is designed to\ndirectly produce semantic features that are consistent with\nthe word embedding learned from a pretrained language\nmodel. Inspired by their efforts in exploiting semantic fea-\ntures, we also explicitly model semantic information . But\nunlike their approach, we achieve semantic modeling with-\nout relying on external language models but instead using an\nalignment module.\nOn the other hand, our work is also partially inspired\nfrom wav2vec (Baevski et al. 2020) in audio community.\nThe wav2vec model ﬁrst extract primary audio features from\nwaveform using 1D convolution. The features are processed\nby a succeeding transformer module, resulting in secondary\ncontextualized audio features. The secondary audio features\ninteract with the primary features by predicting them back\nat the next few time steps. Similarly, we extract semantic\nand visual features at different stages, making them interact\nwith each other so that the overall recognition performance\nis improved.\nApart from the aforementioned work, we have also noted\nthat many efforts have been devoted to modeling visual-\nsemantic relation. We argue that extracting semantic fea-\narXiv:2112.00948v1  [cs.CV]  2 Dec 2021\nConvNet\nconcat\nFFN\nAdd&Norm\nMHSA\nAdd&Norm\nN x\nFFN\nAdd&Norm\nMHSA\nAdd&Norm\nNx\nFC+softmax\nVS-align\n“TIMES”\nVS-align\n  Domain  embedding\nn x d\nW x H x 3\nw x h x d\nt x d\nn x d\n2t x d\nt x d\nt x d\n+\nreshape\nn x d t x d\n t x 2d        t x c\n2t x d        t x 2d\nPositional encoding\nVisual Module\nSemantic Module, optional\nC\no\nn\nv\nN\net\nConvNet Module\nCross-entrogy loss\nInteraction Module\nCross-entrogy loss with softmax\nCross-entrogy loss with softmax\nFFN\nAdd&Norm\nMHSA\nAdd&Norm\nNx\n+\nprimary semantic featuresprimary visualfeatures secondary visual features secondary semantic features Tertiary semantic features\n+\nFigure 1: The architecture of Visual-Semantic Transformer (VST). The VST consists of several key modules, namely ConvNet\n(C), Visual module (V), Interaction module (I), Semantic module (S) and two weight-sharing Visual-Semantic Alignment (A,\nor vs align) modules. Best view in color.\ntures from appearance and then enforce interaction with vi-\nsual features is one key to successfully solving STR. To this\nend, we propose a novel transformer model for scene text\nrecognition. Our model is a uniﬁed end-to-end text recog-\nnizer which converts image patches into text in parallel (i.e.,\nnon-autoregressively), without using complex decoder such\nas CTC (Graves et al. 2006). The model, which we coin\nVisual-Semantic Transformer (VST), is able to learn seman-\ntic features from appearance and combine them back with\nvisual features to enable visual-semantic interaction using\nmulti-head self-attentions (MHSA).\nThe overall architecture of VST is shown in Fig. 1. The\nVST ﬁrst explicitly extracts primary semantic information\nfrom visual feature maps with a transformer module and a\nprimary visual-semantic alignment module (vs-align). The\nsemantic information is then joined with the visual feature\nmaps (viewed as a sequence) to form a pseudo multi-domain\nsequence combining visual and semantic information, which\nis subsequently fed into the second transformer module to\nenable learning of interactions between visual and seman-\ntic features. In this way, the visual features can be enhanced\nby the semantic information and vice versus. The visual fea-\ntures are further decoded with a secondary vs-align mod-\nule which shares weights with the primary one. Finally, the\ndecoded visual features and the semantic features can be\njointly processed by the third transform module and the ﬁnal\nsoftmax layer to obtain the resulting text. Overall, our ap-\nproach is end-to-end and conceptually simple. Experiments\non public benchmarks on regular/ irregular text recognition\ntasks demonstrate the effectiveness of our proposed model.\nThe main contributions of this paper are as follows,\n• We propose a novel visual-semantic transformer to ef-\nfectively solve STR problem, surpassing or on par with\nstate-of-the-art in most STR datasets.\n• We design weight-sharing visual-semantic alignment\nmodules to explicitly enforce the learning of semantic in-\nformation without external language models.\n• We introduce an interaction module that allows visual\nand semantic information to globally interact with and\nenhance from each other.\nIn this work, semantic information refers to the infor-\nmation that connects visual appearance and the underlying\nlinguistic information. In other words, it is the information\nextracted from visual features which are very closely related\nto the text represented by the scene text image. Semantic\ninformation is distinguished from the term language or lin-\nguistic information, because the later usually refers to the in-\nformation extracted directly from text solely. Semantic fea-\nture extraction is not language modeling or word embedding\nbecause features are not computed directed from real charac-\nter sequence, but rather learned from image appearance and\nforced to be consistent with language. The semantic feature\nis ready to convert into text characters using a simple linear\nprobe. Under this deﬁnition, our model can be categorized\nas linguistic-free, as we do not require an external language\nmodel. Instead, we explicitly model semantic information\nwhich can be seen as pseudo-linguistic information.\nIn the following sections, we will brieﬂy review the re-\nlated work and then introduce our VST model in details, fol-\nlowed by extensive experiments and conclusion.\nRelated Work\nText recognition has been an active research area for\ndecades. See (Long, He, and Yao 2021; Chen et al. 2021)\nfor comprehensive reviews. Due to page limitation, we can\nonly list a portion of recent work here.\nRecently, (Nguyen et al. 2021) incorporate a dictionary\nin training and inference stage to help selecting the most\ncompatible outcome for STR. (Feng et al. 2021) introduce\ncharacter center segmentation branch to extract semantic\nfeatures which encode the category and position of charac-\nters for improving video text detection performance. (Patel\net al. 2016) propose to generate contextualized lexicons for\nscene images with only visual information. (Sabir, Moreno-\nNoguer, and Padr ´o 2018) use language model to build the\nsemantic correlation between scene and text in order to re-\nrank the recognition results. (Zheng, Wang, and Betke 2019)\nalso propose to use pretrained language models for correct-\ning predictions. Very recently, (Wang et al. 2021b) propose\nto learn the linguistic rules in the visual space by randomly\nmask out some characters from the image and predict them\nback.\nSimilar to speech recognition, scene text recognition can\nbe treated as a sequence-to-sequence (seq2seq) mapping\nproblem (Jaderberg, Vedaldi, and Zisserman 2014; Jader-\nberg et al. 2015; Qiao et al. 2020; Yue et al. 2020). (Li\net al. 2019) combine convolution and LSTM as an encoder,\nthen use another LSTM as decoder to predict text attentively,\nquite similar to the show, attend and tellwork on image cap-\ntioning (Xu et al. 2015). ASTER (Shi et al. 2018) takes a\ntwo-stage approach to ﬁrst rectify curved text images and\nthen perform recognition using seq2seq model with atten-\ntion. CRNN (Shi, Bai, and Yao 2016) adapts CNN to obtain\nvisual features, which are then fed into LSTM module with\nCTC loss (Graves et al. 2006) for text prediction. STAR-Net\n(Liu et al. 2016) uses spatial transformer (Jaderberg et al.\n2015) to tackle challenges brought by image distortion. At-\ntention can be added to the seq2seq models (Li et al. 2019;\nBhunia et al. 2021) in a straightfoward way to alleviate bot-\ntleneck effects brought by seq2seq models. (Litman et al.\n2020) propose stacked block architecture with intermediate\nsupervision to train a deep BiLSTM encoder, while atten-\ntion is used in decoding stage to exploit contextualized vi-\nsual features. (Aberdam et al. 2021) propose seq2seq con-\ntrastive learning of visual representations which can be ap-\nplied to text recognition. DAN (Wang et al. 2020) decou-\nples alignment from the decoding stage into the early con-\nventional encoder network. (Wang et al. 2021a) propose an\nalignment module enabling text recognizer to recognize doc-\nument level image. (Wang et al. 2019b) use adversarial loss\nto handle low-resolution image.\nTransformers have also been successfully applied to\nSTR. ABINet (Fang et al. 2021) enforces the bidirec-\ntional language-model (LM) to only learn linguistic rules\nby gradient-stopping in training. The decoding is in an itera-\ntive way allowing the predictions to be reﬁned progressively.\nTheir conv+transformer visual module and transformer-\nLM can be separately pretrained to improve performance.\nHRGAT (Yang et al. 2020) connects CNN feature maps to a\ntransformer-based autoregressive decoder, where the cross-\nattention is guided by holistic representation obtained by\naverage-pooling of 2d feature maps.\nPerhaps more related, SRN (Yu et al. 2020) incorperates\nvisual-to-semantic embedding block and cross-entropy loss\nto align with ground-truth text, but they use argmax em-\nbedding, which is different from our direct use of prob-\nability vector that enables smooth gradient ﬂow in train-\ning. Our work is also innovative in many ways such as\nvisual-semantic alignment, multiple-stage semantic process-\ning and transformer-based visual-semantic interaction. In-\nstead of using argmax, (Bhunia et al. 2021) use Gumbel-\nsoftmax (Jang, Gu, and Poole 2016) for extracting semantic\ninformation, which is then fed into succeeding transformer-\nbased visual-semantic reasoning module. The decoding in-\nvolves complex multiple-stage attentional LSTM that cou-\nples with feature pyramid networks. Our approach is not\nonly conceptually much simpler and computationally more\nefﬁcient, but also more effective in solving STR.\nVisual-Semantic transformer\nLinear Softmax\n  Attention maps \n  Visual features \n  Semantic features\nFigure 2: The architecture of vs-align module. Visual fea-\ntures (viewed as feature maps) are projected using a linear\nlayer and further normalized using sofmtax operator, obtain-\ning tattention maps, each of which has the same spatial di-\nmension as the origin feature map. The tattention maps and\nthe original d visual feature maps together will reduce (by\nmultiplication) to tsemantic features in Rd.\nWe will introduce the visual-semantic transformer (VST)\nin this section . The VST consists of several key modules,\nnamely ConvNet (C), Visual module (V), Interaction mod-\nule (I), Semantic module (S) and two weight-sharing Visual-\nSemantic Alignment (A, or vs-align) module. Module C can\nbe any convnet, Module V , I, S are basically transfomer\nblocks, while vs-align is an attention-based alignment block.\nSince a large portion of our model consists of transformer\nblocks, we name it visual-semantic transformer, implying\nthat it is a transformer that explicitly models visual and se-\nmantic information.\nThe overview of the architecture is shown Fig. 1. The con-\nvnet module extracts local visual features from cropped im-\nages. The visual module transforms the local visual features\ninto contextualized global visual features, which we call pri-\nmary visual features. The ﬁrst vs-align module converts the\nprimary visual features into primary semantic features. The\ninteraction module then transforms primary visual and pri-\nmary semantic features by allowing them to interact with\neach other through MHSA, producing secondary visual and\nsecondary semantic features respectively. The secondary vi-\nsual features are fed into the second vs-align module (which\nshares weights with the ﬁrst one), obtaining tertiary (third)\nsemantic features. Secondary and tertiary semantic features\ntogether will be processed by the ﬁnal semantic module to\nobtain the resulting text prediction.\nTo enforce the learning of semantic information, the two\nvs-align modules share weights. In other words, the im-\nprovement of the second vs-align module is immediately\nhelpful for improving the accuracy in aligning the primary\nsemantic features, which in return will help improve the\noverall recognition accuracy. We found that this design is\nvery useful for extracting primary semantic information, as\nseen in Fig. 3.\nThe VST has two variants, namely VST-F (full) or VST-B\n(basic), depending on whether the module S is used or not.\nIn VST-F, there are three losses in training and the output of\nﬁnal softmax is used for text decoding. For VST-B, there are\ntwo losses (blue and green in Fig. 1) and the semantic stream\nof module I’s output are passed to softmax layer, while the\nvisual stream will go through additional vs-align module be-\nfore softmax and CE loss. The decoding for VST-B can be\nevaluated separately or jointly (by probability voting).\nThe ConvNet (C) Module\nWe use resnet-like architecture in extracting local features.\nThe input images are ﬁrst resized to have the same height,\nwith aspect ratio kept unchanged. During training and test-\ning, replication-padding (padding using values from the im-\nage border) is used for batch processing.\nAssuming that input image is of size W ×H ×3, the\nconvnet module will produce feature maps of sizew×h×d.\nThe features will be fed into the visual module.\nThe Visual (V) Module\nConvnets are good at learning local features but hard to learn\nglobal correlation between features that locate far apart. In-\nspired from the success of transformers when applied to vi-\nsion task, we insert a transformer module here in order to\nenhance the feature maps by allowing them to interact with\neach other. The visual modules takes the convolution fea-\nture maps as input, producing primary visual features. The\nprimary visual features together encode completely the ap-\npearance information of the input image. They are then con-\nverted to primary semantic features by the vs-align module.\nThe visual module consists of MHSA, layer-norm, and\nfeed-forward network, as described in (Vaswani et al. 2017),\nbut with minor modiﬁcation that puts layer-norm before\nMHSA (Dosovitskiy et al. 2020; Wang et al. 2019a). Note\nthat Fig. 1 does NOT reﬂect this modiﬁcation. The feature\nmaps are viewed/reshaped as a sequence of dimensiondand\nlength n= w×h. the module V , takes the feature sequence\nas input and generate a sequence of the same dimension and\nlength. The interaction between visual features themselves\nmainly takes place in the MHSA layers.\nThe Visual-Semantic Alignment (A) Module\nParallel attentions has been used to map visual features into\nsemantics(Wang et al. 2021b; Lyu et al. 2019). Compared\nwith previous work, the vs-align module in this work is\nmuch simpler but still effective. Note that we do not use\nthe name semantic decoder here because the output of the\nmodule is not actually the character index, but rather the\nprobability distribution of the characters. In other words, the\nsemantic feature encodes the probability distribution at each\ncandidate location, which can be readily converted into char-\nacter index withargmaxoperation in decoding stage (not in\ntraining). This allows the gradients to ﬂow smoothly inside\nthe VST.\nThe architecture of vs-align is depicted in Fig. 2. As\nshown in the ﬁgure, the visual features (viewed as feature\nmaps) are projected using a linear layer and normalize using\nsoftmax operator, obtaining tattention maps each of which\nhas the same spatial dimension as the origin feature map.\nThe alignment module can also be formulated mathemati-\ncally as follows,\nS = softmax(QVT)V (1)\nwhere S ∈Rt×d is the semantic sequence, V ∈Rn×d is the\nvisual sequence and Q ∈Rt×d is the trainable projection\nmatrix.\nOur VS-align module is able to learn the relation be-\ntween the semantic and visual features. This is different from\nCTC (Graves et al. 2006), which is good at aligning two se-\nquences with beam search, but not capable of handling in-\nformation along the height direction.\nAs shown in Fig. 1, there are two vs-align modules, which\nshare weights during training and inference. This weight-\nsharing scheme is the key to successfully learning 1st se-\nmantic information. It enables the weights learned in align-\ning 2nd visual features to transfer to aligning 1st visual fea-\ntures, making the early learning of 1st semantics possible.\nThe 1st semantic in return enhances the 1st visual features\nthrough interaction module, further improving the training\nof the 2nd vs-align module. The early learning of seman-\ntics provides more chances to correct the semantic features\nthrough interaction in later stages. If semantic had been\nlearned in the last stage, we would have not change to cor-\nrect it, unless incorporating additional lexicon or dictionary\ninformation (Nguyen et al. 2021)\nThe Interaction (I) Module\nThe interaction module plays the key role of mixing primary\nsemantic features and visual features. The modules takes\nMethod Regular test datasets Irregular test datasets\nIIIT SVT IC03 IC13 IC15 SVTP CUTE\nAON (Cheng et al. 2018) 87.0 82.8 91.5 68.2 73.0 76.8\nASTER (Shi et al. 2018) 93.4 89.5 94.5 91.8 76.1 78.5 79.5\nNRTR (Sheng, Chen, and Xu 2019) 86.5 88.3 95.4 94.7\nSAR (Li et al. 2019) 91.5 84.5 91.0 69.2 76.4 83.3\nDAN (Wang et al. 2020) 94.3 89.2 95.0 93.9 74.5 80.0 84.4\nHRGAT (Yang et al. 2020) 94.7 88.9 93.2 79.5 80.9 85.4\nSRN (Yu et al. 2020) 94.8 91.5 95.5 82.7 85.1 87.8\nSCATTER (Litman et al. 2020) 93.7 92.7 96.3 93.9 82.2 86.9 87.5\nGTC (Hu et al. 2020) 95.5 92.9 95.2 94.3 82.5 86.2 92.3\nRobustScanner (Yue et al. 2020) 95.3 88.1 94.8 77.1 79.5 90.3\n(Bhunia et al. 2021) 95.2 92.2 95.5 84.0 85.7 89.7\nPREN (Yan et al. 2021) 95.6 94.0 95.8 96.4 83.0 87.6 91.7\nABINet-SV (Fang et al. 2021) 95.4 93.2 96.8 84.0 87.0 88.9\nABINet-LV (Fang et al. 2021) 96.2 93.5 97.4 86.0 89.3 89.2\nVST-B 96.3 93.8 96.4 96.4 85.4 88.7 95.1\nVST-F 96.1 93.1 97.1 96.4 85.4 89.1 94.8\nTable 1: When compared with previous work, our approach achieves very competitive results. VST-B denotes the C+V+A+I\nbasic model and VST-F the C+V+A+I+S full model. ’-’ denotes data not available or conﬁg not the same.\ntwo streams , namely the primary semantic features and pri-\nmary visual features as input, producing the secondary vi-\nsual and semantic features. Fixed positional encoding are\nadded into the visual stream. For the semantic stream, learn-\nable position embedding is used. Other positional encoding\nschemes probably help as long as it breaks permutation in-\nvariant property of transformers.\nThe design of the interaction module follows from mod-\nule V , comprising layers of transformer blocks. The module\nenables the feature interaction between a) the visual stream\nand semantic will interact with each other, and b) the fea-\ntures themselves inside the same stream.\nThe interpretation of module can be seen from eq. 2. Fol-\nlowing from the notations in (Vaswani et al. 2017), the in-\nteraction in the MHSA layer is obvious by judging form the\nfollowing equation,\nS = softmax([Qs; Qv][Ks; Kv]T\n√dk\n)[Vs; Vv], (2)\nwhere the subscripts s and v denote semantic and visual part\nrespectively, ’;’ is column representation.\nIn this way, the primary semantic features will look for\nsupport from the spatial visual features. The interaction is\nuseful for enhancing semantic features because they now\nhave attentive access to all spatial location of the visual fea-\ntures. On the other hand, visual features are also enhanced\nnot only because then can interact with all other spatial lo-\ncations regardless of distance, but they can also learn from\nsemantic features, which we hypothesize is useful for deal-\ning with appearance degradation.\nThe semantic features can be seen as pseudo-linguistic\nfeatures, distinguished from visual domain. Hence we add\ndomain embedding before feeding two combined streams\ninto the interaction module.\nOne of the outputs of interaction module is secondary vi-\nsual sequence in Rn×d, which will be converted to tertiary\nModule Regular test datasets Irregular test datasets\nIIIT SVT IC03 IC13 IC15 SVTP CUTE\nCV 95.10 91.94 95.72 95.52 81.83 86.23 91.33\nCV A 95.63 91.94 96.41 95.55 82.31 86.95 91.76\nCV AI /s3 96.40 93.51 96.54 96.35 85.09 88.84 94.79\nCV AI /s2 96.26 93.81 96.30 96.35 85.14 88.68 95.13\nCV AI /p 96.36 93.81 96.42 96.45 85.36 88.68 95.13\nCV AIS 96.06 93.50 97.11 96.40 85.37 89.15 94.79\nTable 2: Recognition accuacy increases when extra module\nis added. / s2, /s3 and /p denote decoding from 2nd/3rd se-\nmantics ( s3) and by probability voting from both seman-\ntics respectively, CV AIS means modules C+V+A+I+S and\nso on.\nsemantic features using vs-align module following by cross-\nentropy loss with softmax, against the ground-truth text. The\nother is semantic features in Rn×d, which will be directly\ncompared against ground-truth text with the same activa-\ntion ans loss type. Note that at inference time, the two loss\nbranches are unneeded.\nThe Semantic (S) Module\nThe semantic module is inserted optionally to play the role\nof further fusing the two semantic streams. When inserted,\nour model is named VST-F (full); otherwise, it is named\nVST-B (basic). The two streams are the secondary seman-\ntic features generated from the intersection module and the\ntertiary (third) semantic features generated from the second\nvs-align module respectively.\nThe layer conﬁgureation of this module also follows from\nthe design of module V and I. Fixed positional encoding is\nadded, but there is no domain or segment embedding be-\ncause the two streams are both semantic. Since the trans-\nformer blocks keep the input shape, we concatenate the two\nstreams along channel direction tot×2dand feed it to linear\nand softmax layer, obtaining the ﬁnal text predictions.\n请输入标\n题\nP R I V A T\nS A TU EL S\nS T ID OU S\nE\nFigure 3: Visualization of attention maps for decoding each character.\n.\nMathematical interpretation\nAs shown in Fig. 1, there are three losses indicated by the\nladders in different colors. The green one measures the dis-\ncrepancy between the the true text labels and semantic fea-\ntures output from second vs-align module using softmax ac-\ntivation and cross-entropy loss. The blue one measures how\ndifferent are the secondary semantic features from the true\ntext. The yellow one is the main loss for decoding ﬁnal text\nfor VST-F, while the other two are auxiliary for improv-\ning convergence performance. For VST-B, since there is no\nmodule S, only blue and green loss are used. For all loss\nbranches, the discrepancy are measured by cross-entropy\nloss with softmax activation.\nFrom the optimization perspective, the VST-F solves the\nfollowing optimization problem,\nmin\nθv,θa,θi,θs\nL(s2,y) +L(s3,y) +L(fθs (s2,s3),y)\nsubject to v1 = fθv (x),s1 = fθa (v1)\ns2,v2 = fθi (s1,v1),s3 = fθa (v2)\n(3)\nwhere θv,θa,θi,θs denote the parameters of module\nC+V , A, I, and S respectively, L is cross-entropy loss,\ns1,s2,s3,v1,v2 are the primary/secondary/tertiary seman-\ntic/visual features resp., x,y are the input mage and text la-\nbel resp. For VST-B, the third term and the corresponding\nconstraint in the above objective function is discarded.\nExperiments\nDatasets\nOur model is trained on three datasets: SynthText (ST)\n(Gupta, Vedaldi, and Zisserman 2016), MJSynth (MJ)\n(Jaderberg et al. 2014, 2016) and SynthAdd (SA) (Li et al.\n2019). The training dataset contains totally 15.7 million\nsynthetic images. For evaluation, we use four regular text\ndatasets: IIIT 5K-words (IIIT) (Mishra, Alahari, and Jawa-\nhar 2012) , Street View Text (SVT) (Wang, Babenko, and\nBelongie 2011), ICDAR 2003 (IC03) (Lucas et al. 2003),\nICDAR 2013 (IC13) (Karatzas et al. 2013) , and three irregu-\nlar text datasets: ICDAR 2015 (IC15) (Karatzas et al. 2015),\nStreet View Text Perspective (SVTP) (Phan et al. 2013) and\nCUTE (Risnumawan et al. 2014) .\nIIIT contains 3000 cropped images collected from Google\nimage search. SVT is collected from Google Street View\ncontaining 647 testing images. Following from (Wang,\nBabenko, and Belongie 2011), we select 867 cropped im-\nages from IC03 for testing. IC13 contains 1095 testing im-\nages and we discard images containing non-alphanumeric\ncharacters or contains fewer than three characters, resulting\nin 1015 images for testing, following from previous work\n(Yu et al. 2020).\nFor irregular datasets, IC15 contains2077 cropped images\nby Google Glasses. We use1811 images after discarding ex-\ntremely distorted images. SVTP and CUTE contains639 and\n288 images respectively.\nImplementation Details\nModule C is implemented as a four-layers resnet, with each\nlayer having 1,2,5 and 3 blocks respectively. Module V , I\nand S each consists of 3 transformer layers. The number of\nparameters are about 63.99M for VST-F, and 63.65M for\nVST-B.\nThe image patches are resized with aspect-ratio un-\nchanged so that H = 48, and the width is trimed or padded\nto W = 160pixels. The feature map size is 6 ×40 ×512,\nw = 40,h = 6,n = 240 The number of class is set to\n38, including 0-9,a-z,[unk],[eos]. We assume that the max-\nimum character length is 25, i.e., t = 25. We use online\ndata augmentations including distortion, stretch, perspective\ntransform, blurring, colour jitter etc. In training, we set the\nbatch size to 256 and sample from ST, MJ, SA datasets with\nweight 0.4,0.4 and 0.2 to balance the datasets (Baek et al.\n2019). We use Adam optimizer with the initial learning rate\n1e-4 and decreased lr to 1-e5 when the loss plateaus. We use\nfor and the training takes roughly 3 days to converge on four\nTesla V100 GPUs.\nComparison with State-of-the-Art\nWe compare VST-B and VST-F with previous work over\nseven public benchmarks in table 1. The proposed VST-\nB and VST-F both achieve or on par with state-of-the-art\napproaches. The large version of the very recent work by\n(Fang et al. 2021) performs slightly better than ours on three\nof the seven datasets, but they used extra curbersome pre-\ntrained language models. Moreover, our VST-B and VST-F\nare about three times faster than theirs in inference. On the\nrest four datasets, we improve SOTA by large margins (0.2%\nIIIT, 0.3% SVT, 0.8% IC03, 2.8% CUTE).\nD O B I E S\nG A B B A N A\nFigure 4: Visualization of attention heatmaps of the primary and secondary vs-align modules. For each of the two examples,\nthe top (bottom) row shows the heatmaps of the primary (secondary) vs-align module.\nplatinum australia candybuilder\ndining cottages music libraries\nFigure 5: Successful cases for fairly hard examples.\nAblation Study\nTo verify whether each module is critical for the ﬁnal perfor-\nmance, ablation study is conducted by subsequently adding\none module starting from the most basic C+V conﬁgura-\ntion. The results are shown in table 2, from which we ob-\nserve a consistent performance gain when a new module\nis added. Also note that C+V+A also performs better than\nC+V , which veriﬁes the effectiveness of vs-align module.\nFor the C+V+A+I conﬁguration, there are three ways of de-\ncoding: decoding from secondary semantics (s2), tertiary se-\nmantics (s3) or by probability voting from both semantics.\nVisualization\nInside the module I, each semantic vector will attend to\nvisual features corresponding to some spatial locations.\nWe can visualize which locations are attended to using\nheapmaps. Fig. 3 shows three examples. The heatmaps are\ncomputed by averaging all 8 attention heads and overlapping\nonto images after upsampling. Note that inside the module\neach head focuses on different aspects of the images, but on\naverage the attention is mainly at the right spatial location\nfor each decoded character. For some characters, the focus\nis slightly shifted, probably due to the translation-invariant\nproperty of convnet, which means that the visual features\ncannot be mapped back uniformly to the corresponding spa-\ntial locations. Overall, the visualization proves that the inter-\naction module works as expected.\ni(1) iake(dali)all(all)   g20c(g20)\n5(s) sole(sale)tai2a(tara)tie(the)\nFigure 6: Failure cases for some confusing images.\nLikewise, we also visualize the attention heatmaps of the\nprimary and secondary vs-align modules. Even though the\ntwo modules share the same weights, their attention maps\nshould be different since the visual and semantic features\nare different. We expect that the attention maps of 2nd vs-\nalign be more precise than the ﬁrst one. This is veriﬁed in\nFig. 4.\nBeside visualization, we select some successful and fail-\nure cases for illustration purpose, shown in Fig. 5 and Fig. 6\nrespectively.\nConclusion\nIn this paper, we propose visual-semantic transformer to\nsolve scene text recognition problem. The VST consists\nof three model explicitly extract semantic features in the\nearly stage by using weight-sharing visual-semantic align-\nment module, making the visual and semantic fusion and\ninteraction possible int the following transforms modules.\nOur model is uniﬁed and end-to-end trainable, and it does\nnot require autoregressive decoding. Extensive experiments\non regular and irregular scene text recognition datasets have\nveriﬁed the effectiveness of our model.\nReferences\nAberdam, A.; Litman, R.; Tsiper, S.; Anschel, O.; Sloss-\nberg, R.; Mazor, S.; Manmatha, R.; and Perona, P. 2021.\nSequence-to-sequence contrastive learning for text recogni-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 15302–15312.\nBaek, J.; Kim, G.; Lee, J.; Park, S.; Han, D.; Yun, S.; Oh,\nS. J.; and Lee, H. 2019. What Is Wrong With Scene Text\nRecognition Model Comparisons? Dataset and Model Anal-\nysis. In 2019 IEEE/CVF International Conference on Com-\nputer Vision (ICCV), 4715–4723.\nBaevski, A.; Zhou, H.; Mohamed, A.; and Auli, M. 2020.\nwav2vec 2.0: A framework for self-supervised learning of\nspeech representations. arXiv preprint arXiv:2006.11477.\nBhunia, A.; Sain, A.; Kumar, A.; Ghose, S.; Chowdhury,\nP. N.; and Song, Y .-Z. 2021. Joint Visual Semantic Rea-\nsoning: Multi-Stage Decoder for Text Recognition. ArXiv,\nabs/2107.12090.\nChen, X.; Jin, L.; Zhu, Y .; Luo, C.; and Wang, T. 2021. Text\nrecognition in the wild: A survey. ACM Computing Surveys\n(CSUR), 54(2): 1–35.\nCheng, Z.; Xu, Y .; Bai, F.; Niu, Y .; Pu, S.; and Zhou, S. 2018.\nAON: Towards Arbitrarily-Oriented Text Recognition. In\n2018 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 5571–5579.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFang, S.; Xie, H.; Wang, Y .; Mao, Z.; and Zhang, Y . 2021.\nRead Like Humans: Autonomous, Bidirectional and Itera-\ntive Language Modeling for Scene Text Recognition.\nFeng, W.; Yin, F.; Zhang, X.-Y .; and Liu, C.-L. 2021.\nSemantic-Aware Video Text Detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1695–1705.\nGraves, A.; Fern´andez, S.; Gomez, F.; and Schmidhuber, J.\n2006. Connectionist temporal classiﬁcation: labelling un-\nsegmented sequence data with recurrent neural networks.\nIn Proceedings of the 23rd international conference on Ma-\nchine learning, 369–376.\nGupta, A.; Vedaldi, A.; and Zisserman, A. 2016. Synthetic\nData for Text Localisation in Natural Images. In IEEE Con-\nference on Computer Vision and Pattern Recognition.\nHu, W.; Cai, X.; Hou, J.; Yi, S.; and Lin, Z. 2020. GTC:\nGuided Training of CTC Towards Efﬁcient and Accurate\nScene Text Recognition. In AAAI.\nJaderberg, M.; Simonyan, K.; Vedaldi, A.; and Zisser-\nman, A. 2014. Synthetic Data and Artiﬁcial Neural Net-\nworks for Natural Scene Text Recognition. arXiv preprint\narXiv:1406.2227.\nJaderberg, M.; Simonyan, K.; Vedaldi, A.; and Zisserman,\nA. 2015. Deep Structured Output Learning for Uncon-\nstrained Text Recognition. In ICLR 2015 : International\nConference on Learning Representations 2015.\nJaderberg, M.; Simonyan, K.; Vedaldi, A.; and Zisserman,\nA. 2016. Reading Text in the Wild with Convolutional Neu-\nral Networks. International Journal of Computer Vision ,\n116(1): 1–20.\nJaderberg, M.; Simonyan, K.; Zisserman, A.; et al. 2015.\nSpatial transformer networks. In Advances in neural infor-\nmation processing systems, 2017–2025.\nJaderberg, M.; Vedaldi, A.; and Zisserman, A. 2014. Deep\nFeatures for Text Spotting. In European Conference on\nComputer Vision, 512–528.\nJang, E.; Gu, S.; and Poole, B. 2016. Categorical\nreparameterization with gumbel-softmax. arXiv preprint\narXiv:1611.01144.\nKaratzas, D.; Gomez-Bigorda, L.; Nicolaou, A.; Ghosh, S.;\nBagdanov, A.; Iwamura, M.; Matas, J.; Neumann, L.; Chan-\ndrasekhar, V . R.; Lu, S.; Shafait, F.; Uchida, S.; and Valveny,\nE. 2015. ICDAR 2015 competition on Robust Reading. In\n2015 13th International Conference on Document Analysis\nand Recognition (ICDAR), 1156–1160.\nKaratzas, D.; Shafait, F.; Uchida, S.; Iwamura, M.; i Big-\norda, L. G.; Mestre, S. R.; Mas, J.; Mota, D. F.; Almazan,\nJ. A.; and de las Heras, L.-P. 2013. ICDAR 2013 Robust\nReading Competition. In 2013 12th International Confer-\nence on Document Analysis and Recognition, 1484–1493.\nLi, H.; Wang, P.; Shen, C.; and Zhang, G. 2019. Show, at-\ntend and read: A simple and strong baseline for irregular text\nrecognition. In Proceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence, volume 33, 8610–8617.\nLitman, R.; Anschel, O.; Tsiper, S.; Litman, R.; Mazor,\nS.; and Manmatha, R. 2020. Scatter: selective context\nattentional scene text recognizer. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 11962–11972.\nLitman, R.; Anschel, O.; Tsiper, S.; Litman, R.; Mazor,\nS.; and Manmatha, R. 2020. SCATTER: Selective Con-\ntext Attentional Scene Text Recognizer. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 11962–11972.\nLiu, W.; Chen, C.; Wong, K.-Y . K.; Su, Z.; and Han, J. 2016.\nSTAR-Net: A SpaTial Attention Residue Network for Scene\nText Recognition. In BMVC, volume 2, 7.\nLong, S.; He, X.; and Yao, C. 2021. Scene Text Detec-\ntion and Recognition: The Deep Learning Era.International\nJournal of Computer Vision, 129(1): 161–184.\nLucas, S.; Panaretos, A.; Sosa, L.; Tang, A.; Wong, S.; and\nYoung, R. 2003. ICDAR 2003 robust reading competitions.\nIn Seventh International Conference on Document Analysis\nand Recognition, 2003. Proceedings., volume 3, 682–687.\nLyu, P.; Yang, Z.; Leng, X.; Wu, X.; Li, R.; and Shen, X.\n2019. 2d attentional irregular scene text recognizer. arXiv\npreprint arXiv:1906.05708.\nMishra, A.; Alahari, K.; and Jawahar, C. 2012. Scene Text\nRecognition using Higher Order Language Priors. In Pro-\nceedings of the British Machine Vision Conference , 127.1–\n127.11. BMV A Press. ISBN 1-901725-46-4.\nNguyen, N.; Nguyen, T.; Tran, V .; Tran, M.-T.; Ngo, T. D.;\nNguyen, T. H.; and Hoai, M. 2021. Dictionary-Guided\nScene Text Recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n7383–7392.\nPatel, Y .; Gomez, L.; Rusinol, M.; and Karatzas, D. 2016.\nDynamic lexicon generation for natural scene images. InEu-\nropean Conference on Computer Vision, 395–410. Springer.\nPhan, T. Q.; Shivakumara, P.; Tian, S.; and Tan, C. L. 2013.\nRecognizing Text with Perspective Distortion in Natural\nScenes. In 2013 IEEE International Conference on Com-\nputer Vision, 569–576.\nQiao, Z.; Zhou, Y .; Yang, D.; Zhou, Y .; and Wang, W. 2020.\nSeed: Semantics enhanced encoder-decoder framework for\nscene text recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n13528–13537.\nQiao, Z.; Zhou, Y .; Yang, D.; Zhou, Y .; and Wang, W. 2020.\nSEED: Semantics Enhanced Encoder-Decoder Framework\nfor Scene Text Recognition. In 2020 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n13528–13537.\nRisnumawan, A.; Shivakumara, P.; Chan, C. S.; and Tan,\nC. L. 2014. A robust arbitrary text detection system for natu-\nral scene images. Expert Systems With Applications, 41(18):\n8027–8048.\nSabir, A.; Moreno-Noguer, F.; and Padr ´o, L. 2018. Vi-\nsual re-ranking with natural language understanding for text\nspotting. In Asian Conference on Computer Vision, 68–82.\nSpringer.\nSheng, F.; Chen, Z.; and Xu, B. 2019. NRTR: A No-\nRecurrence Sequence-to-Sequence Model for Scene Text\nRecognition. In 2019 International Conference on Docu-\nment Analysis and Recognition (ICDAR), 781–786.\nShi, B.; Bai, X.; and Yao, C. 2016. An end-to-end trainable\nneural network for image-based sequence recognition and\nits application to scene text recognition. IEEE transactions\non pattern analysis and machine intelligence, 39(11): 2298–\n2304.\nShi, B.; Yang, M.; Wang, X.; Lyu, P.; Yao, C.; and Bai, X.\n2018. Aster: An attentional scene text recognizer with ﬂexi-\nble rectiﬁcation. IEEE transactions on pattern analysis and\nmachine intelligence, 41(9): 2035–2048.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWan, Z.; Zhang, J.; Zhang, L.; Luo, J.; and Yao, C. 2020.\nOn V ocabulary Reliance in Scene Text Recognition. In\n2020 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 11422–11431.\nWang, K.; Babenko, B.; and Belongie, S. 2011. End-to-end\nscene text recognition. In 2011 International Conference on\nComputer Vision, 1457–1464.\nWang, K.; Babenko, B.; and Belongie, S. 2011. End-to-end\nscene text recognition. In 2011 International Conference on\nComputer Vision, 1457–1464. IEEE.\nWang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; and\nChao, L. S. 2019a. Learning deep transformer models for\nmachine translation. arXiv preprint arXiv:1906.01787.\nWang, T.; Zhu, Y .; Jin, L.; Luo, C.; Chen, X.; Wu, Y .; Wang,\nQ.; and Cai, M. 2020. Decoupled Attention Network for\nText Recognition. In AAAI, 12216–12224.\nWang, T.; Zhu, Y .; Jin, L.; Peng, D.; Li, Z.; He, M.; Wang,\nY .; and Luo, C. 2021a. Implicit Feature Alignment: Learn to\nConvert Text Recognizer to Text Spotter. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 5973–5982.\nWang, W.; Xie, E.; Sun, P.; Wang, W.; Tian, L.; Shen, C.; and\nLuo, P. 2019b. TextSR: Content-aware text super-resolution\nguided by recognition. arXiv preprint arXiv:1909.07113.\nWang, Y .; Xie, H.; Fang, S.; Wang, J.; Zhu, S.; and Zhang,\nY . 2021b. From Two to One: A New Scene Text Recognizer\nwith Visual Language Modeling Network. arXiv preprint\narXiv:2108.09661.\nWu, L.; Zhang, C.; Liu, J.; Han, J.; Liu, J.; Ding, E.; and Bai,\nX. 2019. Editing Text in the Wild. InProceedings of the 27th\nACM International Conference on Multimedia, 1500–1508.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-\nnov, R.; Zemel, R.; and Bengio, Y . 2015. Show, attend\nand tell: Neural image caption generation with visual at-\ntention. In International conference on machine learning ,\n2048–2057. PMLR.\nYan, R.; Peng, L.; Xiao, S.; and Yao, G. 2021. Primitive Rep-\nresentation Learning for Scene Text Recognition. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 284–293.\nYang, L.; Wang, P.; Li, H.; Li, Z.; and Zhang, Y . 2020.\nA Holistic Representation Guided Attention Network for\nScene Text Recognition. Neurocomputing.\nYu, D.; Li, X.; Zhang, C.; Liu, T.; Han, J.; Liu, J.; and\nDing, E. 2020. Towards accurate scene text recognition\nwith semantic reasoning networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 12113–12122.\nYu, H.; Zhang, C.; Li, X.; Han, J.; Ding, E.; and Wang,\nL. 2019. An End-to-End Video Text Detector with Online\nTracking. In 2019 International Conference on Document\nAnalysis and Recognition (ICDAR), 601–606.\nYue, X.; Kuang, Z.; Lin, C.; Sun, H.; and Zhang, W. 2020.\nRobustScanner: Dynamically Enhancing Positional Clues\nfor Robust Text Recognition. In European Conference on\nComputer Vision, 135–151. Springer.\nZheng, Y .; Wang, Q.; and Betke, M. 2019. Deep neural net-\nwork for semantic-based text recognition in images. arXiv\npreprint arXiv:1908.01403."
}