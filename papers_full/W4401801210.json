{
    "title": "Artificial intelligence-based data extraction for next generation risk assessment: Is fine-tuning of a large language model worth the effort?",
    "url": "https://openalex.org/W4401801210",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2045428308",
            "name": "Anna Sonnenburg",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2785274253",
            "name": "Benthe van der Lugt",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4320100217",
            "name": "Johannes Rehn",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2974205552",
            "name": "Paul Wittkowski",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2064881961",
            "name": "Karsten Bech",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3046461886",
            "name": "Florian Padberg",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2562221449",
            "name": "Dimitra Eleftheriadou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4320100228",
            "name": "Todor Dobrikov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2064068282",
            "name": "Hans Bouwmeester",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1783454331",
            "name": "Carla Mereu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2128074773",
            "name": "Ferdinand Graf",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1964217338",
            "name": "Carsten Kneuer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2153221695",
            "name": "Nynke I. Kramer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2775646238",
            "name": "Tilmann Bl√ºmmel",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2065540911",
        "https://openalex.org/W4390948711",
        "https://openalex.org/W4387301543",
        "https://openalex.org/W4211245166",
        "https://openalex.org/W4366819469",
        "https://openalex.org/W6801889697",
        "https://openalex.org/W4393094733",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4200556867",
        "https://openalex.org/W4200540763",
        "https://openalex.org/W2193346814",
        "https://openalex.org/W2989678067",
        "https://openalex.org/W4386002638",
        "https://openalex.org/W3203269986"
    ],
    "abstract": "To underpin scientific evaluations of chemical risks, agencies such as the European Food Safety Authority (EFSA) heavily rely on the outcome of systematic reviews, which currently require extensive manual effort. One specific challenge constitutes the meaningful use of vast amounts of valuable data from new approach methodologies (NAMs) which are mostly reported in an unstructured way in the scientific literature. In the EFSA-initiated project 'AI4NAMS', the potential of large language models (LLMs) was explored. Models from the GPT family, where GPT refers to Generative Pre-trained Transformer, were used for searching, extracting, and integrating data from scientific publications for NAM-based risk assessment. A case study on bisphenol A (BPA), a substance of very high concern due to its adverse effects on human health, focused on the structured extraction of information on test systems measuring biologic activities of BPA. Fine-tuning of a GPT-3 model (Curie base model) for extraction tasks was tested and the performance of the fine-tuned model was compared to the performance of a ready-to-use model (text-davinci-002). To update findings from the AI4NAMS project and to check for technical progress, the fine-tuning exercise was repeated and a newer ready-to-use model (text-davinci-003) served as comparison. In both cases, the fine-tuned Curie model was found to be superior to the ready-to-use model. Performance improvement was also obvious between text-davinci-002 and the newer text-davinci-003. Our findings demonstrate how fine-tuning and the swift general technical development improve model performance and contribute to the growing number of investigations on the use of AI in scientific and regulatory tasks.",
    "full_text": null
}