{
  "title": "Annotating Columns with Pre-trained Language Models",
  "url": "https://openalex.org/W3145728363",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2105175910",
      "name": "Yoshihiko Suhara",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2098958445",
      "name": "Jinfeng Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2113621046",
      "name": "Yuliang Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2100424385",
      "name": "Dan Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4313973536",
      "name": "Ã‡aÄŸatay Demiralp",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2095964268",
      "name": "Chen Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2703614960",
      "name": "Wang-Chiew Tan",
      "affiliations": [
        "Menlo School",
        "Alpha Omega Alpha Medical Honor Society"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2788142400",
    "https://openalex.org/W2971681342",
    "https://openalex.org/W1614862348",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2969723769",
    "https://openalex.org/W2898796029",
    "https://openalex.org/W4288269198",
    "https://openalex.org/W4205922070",
    "https://openalex.org/W2157060173",
    "https://openalex.org/W2941366772",
    "https://openalex.org/W2951621897",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3208057978",
    "https://openalex.org/W3014705052",
    "https://openalex.org/W2111869785",
    "https://openalex.org/W3093907107",
    "https://openalex.org/W2889003264",
    "https://openalex.org/W2070491211",
    "https://openalex.org/W3013008430",
    "https://openalex.org/W2008896880",
    "https://openalex.org/W2889249015",
    "https://openalex.org/W3145728363",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2904530076",
    "https://openalex.org/W3165814564",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2092364718",
    "https://openalex.org/W3129639992",
    "https://openalex.org/W3165753548",
    "https://openalex.org/W2009591769",
    "https://openalex.org/W2168859760",
    "https://openalex.org/W3082424964",
    "https://openalex.org/W2123878016",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3113112245",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W2965875055",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3123375411",
    "https://openalex.org/W2914746235",
    "https://openalex.org/W2753709519",
    "https://openalex.org/W3082197983",
    "https://openalex.org/W2556468274",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3174181645",
    "https://openalex.org/W3112302456",
    "https://openalex.org/W2963722008",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3155299751",
    "https://openalex.org/W3025624935",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3103177583",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3168052339",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3119752913",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2964185501",
    "https://openalex.org/W3002709689",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W1483236033",
    "https://openalex.org/W3010144884",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3094024803",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W3037082750",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3209042722",
    "https://openalex.org/W2948145720",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3095464614"
  ],
  "abstract": "Inferring meta information about tables, such as column headers or\\nrelationships between columns, is an active research topic in data management\\nas we find many tables are missing some of this information. In this paper, we\\nstudy the problem of annotating table columns (i.e., predicting column types\\nand the relationships between columns) using only information from the table\\nitself. We develop a multi-task learning framework (called Doduo) based on\\npre-trained language models, which takes the entire table as input and predicts\\ncolumn types/relations using a single model. Experimental results show that\\nDoduo establishes new state-of-the-art performance on two benchmarks for the\\ncolumn type prediction and column relation prediction tasks with up to 4.0% and\\n11.9% improvements, respectively. We report that Doduo can already outperform\\nthe previous state-of-the-art performance with a minimal number of tokens, only\\n8 tokens per column. We release a toolbox\\n(https://github.com/megagonlabs/doduo) and confirm the effectiveness of Doduo\\non a real-world data science problem through a case study.\\n",
  "full_text": "Annotating Columns with Pre-trained Language Models\nYoshihiko Suhara, Jinfeng Li,\nYuliang Li, Dan Zhang\nMegagon Labs\n{yoshi,jinfeng,yuliang,dan_z}@megagon.ai\nÃ‡aÄŸatay Demiralpâˆ—\nSigma Computing\ncagatay@sigmacomputing.com\nChen Chenâ€ \nMegagon Labs\nchen@megagon.ai\nWang-Chiew Tanâˆ—\nMeta AI\nwangchiew@fb.com\nABSTRACT\nInferring meta information about tables, such as column headers\nor relationships between columns, is an active research topic in\ndata management as we find many tables are missing some of this\ninformation. In this paper, we study the problem of annotating\ntable columns (i.e., predicting column types and the relationships\nbetween columns) using only information from the table itself. We\ndevelop a multi-task learning framework (called Doduo) based on\npre-trained language models, which takes the entire table as input\nand predicts column types/relations using a single model. Experi-\nmental results show that Doduo establishes new state-of-the-art\nperformance on two benchmarks for the column type prediction\nand column relation prediction tasks with up to 4.0% and 11.9%\nimprovements, respectively. We report that Doduo can already\noutperform the previous state-of-the-art performance with a min-\nimal number of tokens, only 8 tokens per column. We release a\ntoolbox1 and confirm the effectiveness of Doduo on a real-world\ndata science problem through a case study.\nACM Reference Format:\nYoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp,\nChen Chen, and Wang-Chiew Tan. 2022. Annotating Columns with Pre-\ntrained Language Models. In Proceedings of the 2022 International Conference\non Management of Data (SIGMOD â€™22), June 12â€“17, 2022, Philadelphia, PA,\nUSA. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3514221.\n3517906\n1 INTRODUCTION\nMeta information about tables, such as column types and relation-\nships between columns (or column relations), is crucial to a vari-\nety of data management tasks, including data quality control [45],\nschema matching [41], and data discovery [8]. Recently, there is\nâˆ—Work done while the author was at Megagon Labs.\nâ€ Deceased.\n1https://github.com/megagonlabs/doduo\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9249-5/22/06. . . $15.00\nhttps://doi.org/10.1145/3514221.3517906\n[CLS] Val 1 Val 2 [CLS] Val 3 [CLS] [SEP]Val 5\n??? ??? ??? \nVal 1 Val 3 Val 5\nVal 2 Val 4 Val 6\n... ...\nE1,[CLS] E1,Val 1 E1,Val 2 E2,[CLS] E2,Val 3 E3,[CLS] E3,Val 5 E3,[SEP]\nTransformer layer\nTransformer layer\nTransformer layer\nE'1,[CLS] E'1,Val 1 E'1,Val 2 E'2,[CLS] E'2,Val 3 E'3,[CLS] E'3,Val 5 E'3,[SEP]\n...\n... ... ...\n... ... ...\n... ... ...\nOutput layer Output layer Output layer\nOutput layer Output layer\n???\n???\nRelation\nRelation\nFigure 1: Overview of Doduoâ€™s model architecture. Doduo se-\nrializes the entire table into a sequence of tokens to make\nit compatible with the Transformer-based architecture. To\nhandle the column type prediction and column relation ex-\ntraction tasks, Doduo implements two different output lay-\ners on top of column representations and a pair of column\nrepresentations, respectively.\nan increasing interest in identifying semantic column types and\nrelations [21, 22, 66]. Semantic column types such as â€œpopulationâ€,\nâ€œcityâ€, and â€œbirth_dateâ€ provide contain finer-grained, richer infor-\nmation than standard DB types such as integer or string. Similarly,\nsemantic column relations such as a binary relation â€œis_birthplace_ofâ€\nconnecting a â€œnameâ€ and a â€œcityâ€ column can provide valuable in-\nformation for understanding semantics of the table. For example,\ncommercial systems (e.g., Google Data Studio [18] , Tableau [46])\nleverage such meta information for better table understanding.\nHowever, semantic column types and relations are typically miss-\ning in tables while annotating such meta information manually can\nbe quite expensive. Thus, it is essential to build models that can\nautomatically assign meta information to tables.\nFigure 2 shows two tables with missing column types and col-\numn relations. The table in Figure 2(a) is about animation films\nand the corresponding directors/producers/release countries of the\nfilms. In the second and third columns, person names will require\ncontext, both in the same column and the other columns, to de-\ntermine the correct column types. For example, George Miller2\n2In this context, George Miller refers to an Australian filmmaker, but there exist\nmore than 30 different Wikipedia articles that refer to different George Miller.\narXiv:2104.01785v2  [cs.DB]  1 Mar 2022\nSIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp, Chen Chen, and Wang-Chiew Tan\n??? ??? ??? ??? \nHappy Feet George Miller, Warren\nColeman, Judy Morris\nBill Miller, George Miller,\nDoug Mitchell USA\nCars John Lasseter, Joe Ranft Darla K. Anderson UK\nFlushed Away David Bowers, Sam Fell Dick Clement, Ian La\nFrenais, Simon Nye France\nfilm \n director \n producer \n country \n??? ??? ??? \nMax BrowneSammamish, Washington Southern California\nThomas Tyner Aloha, Oregon Oregon\nDerrick Henry Yulee, Florida Alabama\nperson \n location \n sports_team \nplace_of_birth \nteam_roster \n(a) (b) \nFigure 2: Two example tables from the WikiTable dataset. (a) The task is to predict the column type of each column based\non the table values. (b) The task is to predict both column types and relationships between columns. The column types (the\ncolumn relations) are depicted at the top (at the bottom) of the table. This example also shows that column types and column\nrelations are inter-dependent and hence, our motivation to develop a unified model for predicting both tasks.\nappears in both columns as a director and a producer, and it is also\na common name. Observing other names in the column helps better\nunderstand the semantics of the column. Furthermore, a column\ntype is sometimes dependent on other columns of the table. Hence,\nby taking contextual information into account, the model can learn\nthat the topic of the table is about (animation) films and understand\nthat the second and third columns are less likely to be politician\nor athlete. To sum up, this example shows that the table context\nand both intra-column and inter-column context can be very useful\nfor column type prediction.\nFigure 2(b) depicts a table with predicted column types and col-\numn relations. The column types personand locationare helpful\nfor predicting the relation place_of_birth. However, it will still\nneed further information to distinguish whether the location is\nplace_of_birthor place_of_death.\nThe example above shows that column type and column relation\nprediction tasks are intrinsically related. Thus it will be synergistic\nto solve the two tasks simultaneously using a single framework.\nTo combine the synergies of column type prediction and column\nrelation prediction tasks, we developDoduo that: (1) learns column\nrepresentations, (2) incorporates table context, and (3) uniformly\nhandles both column annotation tasks. Most importantly, our solu-\ntion (4) shares knowledge between the two tasks.\nDoduo leverages a pre-trained Transformer-based language\nmodels (LMs) and adopts multi-task learning into the model to\nappropriately â€œtransferâ€ shared knowledge from/to the column\ntype/relation prediction task. The use of the pre-trained Transformer-\nbased LM makes Doduo a data-driven representation learning sys-\ntem3 (i.e., feature engineering and/or external knowledge bases\nare not needed) (Challenge 1.) Pre-trained LMâ€™s contextualized\nrepresentations and our table-wise serialization enable Doduo to\nnaturally incorporate table context into the prediction (Challenge\n2) and to handle different tasks using a single model (Challenge 3.)\nLastly, training such atable-wise model via multi-task learning helps\nâ€œtransferâ€ shared knowledge from/to different tasks (Challenge 4.)\nFigure 1 depicts the model architecture of Doduo. Doduo takes\nas input values from multiple columns of a table after serialization\nand predicts column types and column relations as output. Doduo\nconsiders the table context by taking the serialized column values\nof all columns in the same table. This way, both intra-column (i.e.,\nco-occurrence of tokens within the same column) and inter-column\n3In other words, Doduo relies on the general knowledge obtained from text corpora\n(e.g., Wikipedia) and a training set of tables annotated with column types and relations.\n(i.e., co-occurrence of tokens in different columns) information\nis accounted for. Doduo appends a dummy symbol [CLS] at the\nbeginning of each column and uses the corresponding embeddings\nas learned column representations for the column. The output layer\non top of a column embedding (i.e., [CLS]) is used for column\ntype prediction, whereas the output layer for the column relation\nprediction takes the column embeddings of each column pair.\nContributions Our contributions are:\nâ€¢We develop Doduo, a unified framework for both column type\nprediction and column relation prediction. Doduo incorporates\ntable context through the Transformer architecture and is trained\nvia multi-task learning.\nâ€¢Our experimental results show thatDoduo establishes new state-\nof-the-art performance on two benchmarks, namely the Wik-\niTable and VizNet datasets, with up to 4.0% and 11.9% improve-\nments compared to TURL and Sato.\nâ€¢We show that Doduo is data-efficient as it requires less training\ndata or less input data.Doduo achieves competitive performance\nagainst previous state-of-the-art methods using less than half of\nthe training data or only using 8 tokens per column as input.\nâ€¢We release the codebase and models as a toolbox, which can be\nused with just a few lines of Python code. We test the performance\nof the toolbox on a real-world data science problem and verify\nthe effectiveness of Doduo even on out-domain data.\n2 RELATED WORK\nExisting column type prediction models enjoyed the recent ad-\nvances in machine learning by formulating column type prediction\nas a multi-class classification task. Hulsebos et al. [22] developed\na deep learning model called Sherlock, which applies neural net-\nworks on multiple feature sets such as word embeddings, character\nembeddings, and global statistics extracted from individual column\nvalues. Zhang et al. [66] developed Sato, which extends Sherlock\nby incorporating table context and structured output prediction\nto better model the nature of the correlation between columns\nin the same table. Other models such as ColNet [ 9], HNN [ 10],\nMeimei [48], ğ¶2 [24] use external Knowledge Bases (KBs) on top\nof machine learning models to improve column type prediction.\nThose techniques have shown success on column type prediction\ntasks, outperforming classical machine learning models.\nWhile those techniques identify the semantic types of individual\ncolumns, another line of work focuses on column relations between\nAnnotating Columns with Pre-trained Language Models SIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA\npairs of columns in the same table for better understanding ta-\nbles [4, 13, 28, 29, 34, 54]. A column relation is a semantic label be-\ntween a pair of columns in a table, which offers more fine-grained in-\nformation about the table. For example, a relationplace_of_birth\ncan be assigned to a pair of columns person and location to de-\nscribe the relationship between them. Venetis et al. [ 54] use an\nOpen IE tool [62] to extract triples to find relations between entities\nin the target columns. MuÃ±oz et al. [34] use machine learning mod-\nels to filter triple candidates created from DBPedia. Cannaviccio et\nal. [4] use a language model-based ranking method [65], which is\ntrained on a large-scale web corpus, to re-rank relations extracted\nby an open relation extraction tool [35]. Cappuzo et al. [5] represent\ntable structure as a graph and then learn the embeddings from the\ndescriptive summaries generated from the graph.\nRecently, pre-trained Transformer-based Language Models (LMs)\nsuch as BERT, which were originally designed for NLP tasks, have\nshown success in data management tasks. Li et al. [26] show that\npre-trained LMs is a powerful base model for entity matching. Mac-\ndonald et al. [29] proposed applications for entity relation detection.\nTang et al. [49] propose RPTs as a general framework for automating\nhuman-easy data preparation tasks like data cleaning, entity resolu-\ntion and information extraction using pre-trained masked language\nmodels. The power of Transformer-based pre-trained LMs can be\nsummarized into two folds. First, using a stack of Transformer\nblocks (i.e., self-attention layers), the model is able to generate con-\ntextualized embeddings for structured data components like table\ncells, columns, or rows. Second, models pre-trained on large-scale\ntextual corpora can store â€œsemantic knowledgeâ€ from the training\ntext in the form of model parameters. For example, BERT might\nknow that George Miller is a director/producer since the name\nfrequently appears together with â€œdirected/produced byâ€ in the text\ncorpus used for pre-training. In fact, recent studies have shown that\npre-trained LMs store a significant amount of factual knowledge,\nwhich can be retrieved by template-based queries [23, 40, 42].\nThose pre-trained models have also shown success in data man-\nagement tasks on tables. TURL [ 13] is a Transformer-based pre-\ntraining framework for table understanding tasks. Contextualized\nrepresentations for tables are learned in an unsupervised way dur-\ning pre-training and later applied to 6 different tasks in the fine-\ntuning phase. SeLaB [ 52] leverages pre-trained LMs for column\nannotation while incorporating table context. Their approach uses\nfine-tuned BERT models in a two-stage manner. TaPaS[ 20] con-\nducts weakly supervised parsing via pre-training, and TaBERT[63]\npre-trains for a joint understanding of textual and tabular data for\nthe text-to-SQL task. TUTA [58] makes use of different pre-training\nobjectives to obtain representations at token, cell, and table levels\nand propose a tree-based structure to describe spatial and hierarchi-\ncal information in tables. TCN [57] makes use of both information\nwithin the table and across external tables from similar domains to\npredict column type and pairwise column relations.\nIn this paper, we empirically compareDoduo with Sherlock [22],\nSato [66], and TURL [13] as baseline methods. Sherlock is a single-\ncolumn model while Doduo is multi-column by leveraging table\ncontext to predict column types and relations more accurately. Sato\nleverages topic model (LDA) features as table context whileDoduo\ncan additionally take into account fine-grained, token-level inter-\nactions among columns via its built-in self-attention mechanism.\nTable 1: Notations.\nSymbol Description\nğ‘‡ = (ğ‘1,ğ‘2,...,ğ‘ ğ‘›) Columns in a table.\nğ‘ğ‘– = (ğ‘£ğ‘–\n1,ğ‘£ğ‘–\n2,...,ğ‘£ ğ‘–\nğ‘š) Column values.\nğ‘£ğ‘–\nğ‘— = (ğ‘¤ğ‘–\nğ‘—,1,ğ‘¤ğ‘–\nğ‘—,2,...,ğ‘¤ ğ‘–\nğ‘—,ğ¾) A single column value.\nğ·train =\nn\nğ‘‡(ğ‘›),ğ¿(ğ‘›)\ntype,ğ¿(ğ‘›)\nrel\noğ‘\nğ‘›=1\nTraining data\nğ¿type = (ğ‘™1,ğ‘™2,...,ğ‘™ ğ‘›), ğ‘™âˆ— âˆˆCtype Column type labels.\nğ¿rel = (ğ‘™1,2,ğ‘™1,3,...,ğ‘™ 1,ğ‘›), ğ‘™âˆ—,âˆ— âˆˆCrel Column relation labels.\nTURL is also a Transformer-based model likeDoduo but it requires\nadditional meta table information such as table headers for pre-\ntraining. Doduo is more generic as it predicts column types and\nrelations only relying on cell values in the table. See Section 5 for a\nmore detailed comparison.\n3 BACKGROUND\nIn this section, we formally define the two column annotation tasks:\ncolumn type prediction and column relation annotation. We also\nprovide a brief background on pre-trained language models (LMs)\nand how to fine-tune them for performing column annotations.\n3.1 Problem Formulation\nThe goal of the column type prediction task is to classify each\ncolumn to its semantic type , such as â€œcountry nameâ€, â€œpopulationâ€,\nand â€œbirthdayâ€ instead of the standard column types such asstring,\nint, or Datetime. See also Figure 2 for more examples. For column\nrelation annotation, our goal is to classify the relation of each pair\nof columns. In Figure 2, the relation between the â€œpersonâ€ column\nand the â€œlocationâ€ column can be â€œplace_of_birthâ€.\nAs summarized in Table 1, more formally, we consider a standard\nrelational data model where a relationğ‘‡ (i.e., table) consists of a set\nof attributes ğ‘‡ = (ğ‘1,...ğ‘ ğ‘›)(i.e., columns.) We denote by val(ğ‘‡.ğ‘ğ‘–)\nthe sequence of data values stored at the columnğ‘ğ‘–. For each value\nğ‘£ âˆˆval(ğ‘‡.ğ‘ğ‘–), we assumeğ‘£to be of thestringtype and can be split\ninto a sequence of input tokensğ‘£ = [ğ‘¤1,...,ğ‘¤ ğ‘˜]to pre-trained LMs.\nThis approach of casting cell values into text might seem restricted\nsince tables columns can be of numeric types such asfloator date.\nThere has been extensions of the Transformer models to support\nnumeric data [60] and providing such direct support of numeric\ndata is important future work. We also provide a brief analysis on\nDoduoâ€™s performance on numeric column types in Section 5.4.\nProblem 1 (Column type prediction).Given a table ğ‘‡ and a\ncolumn ğ‘ğ‘– in ğ‘‡, a column type prediction model Mwith type vo-\ncabulary Ctype predicts a column type M(ğ‘‡,ğ‘ğ‘–)âˆˆC type that best\ndescribes the semantics of ğ‘ğ‘–.\nProblem 2 (Column relation prediction).Given a table ğ‘‡ and\na pair of columns (ğ‘ğ‘–,ğ‘ğ‘—)in ğ‘‡, a column relation prediction model M\nwith relation vocabulary Crel predicts a relation M(ğ‘‡,ğ‘ğ‘–,ğ‘ğ‘—)âˆˆC rel\nthat best describes the semantics of the relation between ğ‘ğ‘– and ğ‘ğ‘—.\nIn Doduo, we consider the supervised setting of multi-class clas-\nsification. This means that we assume a training data set ğ·train of\ntables annotated with columns types and relations from two fixed\nvocabularies (Ctype,Crel). Note that Doduo does not restrict itself\nto specific choices of vocabularies (Ctype,Crel)which are customiz-\nable by switching the training set ğ·train. In practice, the choice of\n(Ctype,Crel)is ideally application-dependent. For example, if the\nSIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp, Chen Chen, and Wang-Chiew Tan\nHappy Feet ...George .. USA\nCars Darla ... UK\n[CLS] Val 1 Val 2 [CLS] Val 3 [CLS] [SEP]Val 5\nE1,[CLS] E1,Val 1 E1,Val 2 E2,[CLS] E2,Val 3 E3,[CLS] E3,Val 5 E3,[SEP]\nTransformer layer\nTransformer layer\nTransformer layer\nE'1,[CLS] E'1,Val 1 E'1,Val 2 E'2,[CLS] E'2,Val 3 E'3,[CLS] E'3,Val 5 E'3,[SEP]\n...\n... ... ...\n... ... ...\n... ... ...\nFigure 3: How Doduo computes contextualized column em-\nbeddings using the Transformer layers. Each Transformer\nblock calculates an embedding vector for every token based\non surrounding tokens.\ndownstream task requires integration with a Knowledge Base (KB),\nit is ideal to have (Ctype,Crel)aligned with the KBâ€™s type/relation\nvocabulary. In our experiment, we evaluated Doduo on datasets\nannotated with (1) KB types [2] and (2) DBPedia types [36].\nThe size and quality of the training set are also important for\ntraining high-quality column annotation models. While manually\ncreating such datasets can be quite expensive, the datasets used in\nour experiments rely on heuristics that map table meta-data (e.g.,\nheader names, entity links) to type names to create training sets of\nlarge scale. See Section 5.1 for more details.\nWhile KB can work as a training example provider,Doduo does\nnot require the training examples to be from a single source but\ncan combine labels from any resources such as human annotations,\nlabeling rules, and meta-data that can be transformed into the\ncolumn type/relation label format.\nWe also note that the learning goal of Doduo is to train column\nannotation models with high accuracy while being generalizable to\nunannotated tables (e.g., as measured by an unseen test set ğ·test).\nThe column type/relation prediction models of Doduo only con-\nsiders the table content (i.e., cell values) as input. This setting al-\nlows Doduo to be more flexible to practical applications without\nreplying on auxiliary information such as column names, table ti-\ntles/captions, or adjacent tables typically required by existing works\n(See Section 2 for a comprehensive overview).\n3.2 Pre-trained Language Models\nPre-trained Language Models (LMs) emerge as general-purpose\nsolutions to tackle various natural language processing (NLP) tasks.\nRepresentative LMs such as BERT [14] and ERNIE [47] have shown\nleading performance among all solutions in NLP benchmarks such\nas GLUE [17, 56]. These models are pre-trained on large text corpora\nsuch as Wikipedia pages and typically employ multi-layer Trans-\nformer blocks [53] to assign more weights to informative words\nand less weight to stop words for processing raw texts. During pre-\ntraining, a model is trained on self-supervised language prediction\ntasks such as missing token prediction and next-sentence prediction.\nThe purpose is to learn the semantic correlation of word tokens (e.g.,\nsynonyms), such that correlated tokens can be projected to similar\nvector representations. After pre-training, the model is able to learn\nthe lexical meaning of the input sequence in the shallow layers and\nthe syntactic and semantic meanings in the deeper layers [11, 50].\nA special component of pre-trained LMs is the attention mech-\nanism, which embeds a word into a vector based on its context\n(i.e., surrounding words). The same word has different vectors if it\nappears in different sentences, and this is very different from other\nembedding mechanisms such as word2vec [ 32], GloVe [39], and\nfastText [1], which always generate the same vector for the same\nword in any context. Pre-trained LMsâ€™ embeddings are context-\ndependent and thus offer two strengths. First, it can discern poly-\nsemy. For example, the person name George Millerreferring to\na producer is different from the same name that refers to a direc-\ntor. Pre-trained LMs discern the difference and generate different\nvectors. Second, the embedding deals with synonyms well. For ex-\nample, the words Derrick Henry and Derrick Lamar Henry Jr\n(respectively, (USA, US), (Oregon, OR)) are likely the same given their\nrespective contexts. Pre-trained LMs will generate similar word\nvectors accordingly. Due to the two favorable strengths, pre-trained\nmodels should enable the best performance to column annotation\ntasks, where each cell value is succinct, and its meaning highly\ndepends on its surrounding cells.\nThe pre-trained model does not know what to predict for specific\ntasks unless the task is exactly the same the pre-training task. Thus,\na pre-trained LM needs to be fine-tuned with task-specific training\ndata, so the model can be tailored for the task. A task-specific output\nlayer is attached to the final layer of the pre-trained LM, and the\nloss value (e.g., cross-entropy loss) is back-propagated from the\noutput layer to the pre-trained LM for a minor adjustment.\nIn this paper, we fine-tune the popular 12-layer BERT Base\nmodel [14]. However, Doduo is independent of the choice of pre-\ntrained LMs, and Doduo can potentially perform even better with\nlarger pre-trained LMs.\n3.3 Multi-task learning\nMulti-task learning [7] is a type of supervised machine learning\nframework, where the objective function is calculated based on\nmore than one task. Generally, different types of labels are used,\nand the labels may be or may not be annotated on the same example.\nThe intuition and an assumption behind multi-task learning is that\nthe tasks intrinsically share knowledge, and thus, training with the\nsame base model benefits each other.\nThe major benefit of multi-task learning is that it can help im-\nprove the generalization performance of the model, especially when\nthe training data is not sufficient. Multi-task learning can be easily\napplied to Deep Learning models [44] by attaching different output\nlayers to the main model, which is considered a â€œlearnedâ€ represen-\ntation encoder that converts input data to dense representations.\nThere are a variety of approaches for multi-task learning [44],\ndepending on how to model and optimize shared parameters. Multi-\ntask learning models can be split into two categories based on how\nparameters are shared. With hard parameter sharing [6], models for\nmultiple tasks share the same parameters, whereas soft parameter\nsharing [61] adds constraints on distinct models for different tasks.\nIn this paper, we consider hard parameter sharing as it is a more\ncost-effective approach. Among hard parameter sharing models,\nwe choose a joint multi-task learning framework [19] that uses the\nsame base model with different output layers for different tasks.\n4 MODEL\nIn this section, we first introduce a baseline single-column model\nthat fine-tunes a pre-trained LM on individual columns. Then, we\ndescribe Doduoâ€™s model architecture and training procedure.\nAnnotating Columns with Pre-trained Language Models SIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA\n4.1 Single-column Model\nSince LMs take token sequences (i.e., text) as input, one first has to\nconvert a table into token sequences so that they can be meaning-\nfully processed by LMs. A straightforward serialization strategy is\nto simply concatenate column values to make a sequence of tokens\nand feed that sequence as input to the model. That is, suppose a\ncolumn ğ¶ has column values ğ‘£1,...ğ‘£ ğ‘š, the serialized sequence is\nserializesingle (ğ¶)::= [CLS]ğ‘£1 ... ğ‘£ğ‘š[SEP],\nwhere [CLS]and [SEP]are special tokens used to mark the be-\nginning and end of a sequence4. For example, the first column of\nthe first table in Figure 2 is serialized as: [CLS] Happy Feet Cars\nFlushed Away [SEP]. This serialization converts the problem into a\nsequence classification task. Thus, it is straightforward to fine-tune\na BERT model using training data.\nThe column relation prediction task can be also formulated as a\nsequence classification task by converting a pair of columns (instead\nof a single column) into a token sequence in a similar manner. For\nthis case, we also insert additional [SEP]between values of two\ncolumns to help the pre-trained LM distinguish the two columns.\nNamely, given two columnsğ¶ = ğ‘£1,...,ğ‘£ ğ‘š and ğ¶â€²= ğ‘£â€²\n1,...,ğ‘£ â€²ğ‘š, the\nsingle-column model serializes the pair as:\nserializesingle (ğ¶,ğ¶â€²)::= [CLS]ğ‘£1 ... ğ‘£ğ‘š[SEP]ğ‘£â€²\n1 ... ğ‘£â€²\nğ‘š[SEP].\nUsing the above serialization scheme, we can cast the column\ntype and relation prediction tasks as sequence classification and\nsequence-pair classification tasks, which can be solved by LM fine-\ntuning. However, such sequence classifications predict column\ntypes independently, even if they are in the same table. We refer\nto this method as the single-column model. Although the single-\ncolumn model can leverage the language understanding capability\nand knowledge learned by the LM via pre-training, it has an obvi-\nous drawback of treating columns in the same table as independent\nsequences. As a result, the single-column model fails to capture\nthe table context, which is known to be important for the column\nannotation tasks [10, 24, 66].\n4.2 Table Serialization\nIn contrast to the single-column model described above, Doduo\nis a multi-column (or table-wise) model that takes an entire table\nas input. Doduo serializes data entries as follows: for each table\nthat has ğ‘›columns ğ‘‡ = (ğ‘ğ‘–)ğ‘›\nğ‘–=1, where each column hasğ‘ğ‘š column\nvalues ğ‘ğ‘– = (ğ‘£ğ‘–\nğ‘—)ğ‘š\nğ‘—=1. We let\nserialize(ğ‘‡)::= [CLS]ğ‘£1\n1 ... [CLS]ğ‘£ğ‘›\n1 ...ğ‘£ ğ‘›\nğ‘š[SEP].\nFor example, the first table in Figure 2 is serialized as:\n[CLS] Happy Feet, . . . [CLS] George Miller, . . . [CLS] USA, . . . , France [SEP].\nDifferent from the single-column model, which always has a sin-\ngle [CLS]token in the input, Doduoâ€™s serialization method inserts\nas many [CLS]tokens as the number of columns in the input table.\nThis difference makes a change in the classification formulation.\nWhile the single-column model classifies a single sequence (i.e., a\nsingle column) by predicting a single label,Doduo predicts as many\nlabels as the number of [CLS]tokens in the input sequence. The\n4Note that [CLS]and [SEP]are the special tokens for BERT and other LMs may have\nother special tokens, which are usually implemented as part of their tokenizers.\nlearning procedure of Doduo starts by serializing and tokenizing\nall tables in the datasets (Line 3-4 of Algorithm 1.)\nAlgorithm 1: Training procedure of Doduo\nInput: Number of training epochs ğ‘Epoch; For each task ğ‘– âˆˆ[1,ğ‘‡], training\nset ğ·ğ‘–, loss function Lğ‘–, and optimizer ğ‘‚ğ‘–\nOutput: Annotation model M= (LM,{ğ‘”1,...,ğ‘” ğ‘‡})with a language\nmodel LM and ğ‘‡ heads ğ‘”1 to ğ‘”ğ‘‡\n// Initialize model weights\n1 Initialize LM using its pre-trained weights;\n2 Initialize {ğ‘”1,...,ğ‘” ğ‘‡}randomly;\n// Serialize all tables in each training set\n3 for ğ‘– = 1 to ğ‘‡ do\n4 ğ·ğ‘– â†{serialize(ğ‘‡)for ğ‘‡ âˆˆğ·ğ‘–};\n5 for ep = 1 to ğ‘Epoch do\n6 for ğ‘– = 1 to ğ‘‡ do\n7 Randomly split ğ·ğ‘– into mini-batches {ğµ1,...ğµ ğ‘˜};\n8 for ğµin {ğµ1,...ğµ ğ‘˜}do\n// Evaluate the Lğ‘– loss func on the ğ‘”ğ‘– head\n9 ğ¿â†Lğ‘–(ğµ,LM âŠ•ğ‘”ğ‘–);\n// Back-propagate to update model weights\n10 Mâ† back-propagate(M,ğ‘‚ğ‘–,ğœ•ğ¿/ğœ•(LM âŠ•ğ‘”ğ‘–));\n11 return M;\n4.3 Contextualized Column Representations\nWe describe how Doduo obtains table context through contex-\ntualized column embeddings using the Transformer-architecture.\nFigure 3 depicts how each Transformer block of the Doduo aggre-\ngates contextual information from all columns values (including\ndummy [CLS] symbols and themselves) in the same table. Specifi-\ncally, this example illustrates the first Transformer layer calculates\nthe attention vector by aggregating embeddings of other tokens\nbased on the similarity against the second columnâ€™s [CLS]token.\nThus, an attention vector for the same symbol (e.g., George) can be\ndifferent when it appears in a different context. This resolves the\nambiguity issue of conventional word embedding techniques such\nas word2vec or GloVe.\nAfter encoding tokens into token embeddings, a Transformer\nlayer converts a token embedding into key (K), query (Q), and value\n(V) embeddings. A contextualized embedding for a token is calcu-\nlated by the weighted average of value embeddings of all token\nembeddings, where the weights are calculated by the similarity\nbetween the query embedding and key embeddings. By having key\nembeddings and query embeddings separately, the model is able\nto calculate contextualized embeddings in an asymmetric manner.\nThat is, the importance of Happy Feet for George Miller, which\nshould be a key signal to disambiguate the person name, may not\nbe necessarily equal to that of George Miller for Happy Feet.\nFurthermore, a Transformer-based model usually has multiple at-\ntention heads (e.g., 12 attention heads for the BERT base model.)\nDifferent attention heads have different parameters for K, Q, V cal-\nculation so that they can capture different characteristics of input\ndata holistically. Finally, the output of a Transformer block is con-\nverted into the same dimension size as that of the input (e.g, 768 for\nBERT) so that the output of the previous Transformer block can be\ndirectly used as the input to the next Transformer block. The same\nprocedure is carried out as many as the number of Transformer\nblocks (i.e., 12 blocks for the BERT Base model.)\nSIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp, Chen Chen, and Wang-Chiew Tan\nColumn representations. Since Doduo inserts dummy [CLS]\nsymbols for each column, we can consider the output embeddings\nof the pre-trained LM for those symbols as contextualized column\nrepresentations. Note that Doduo is a table-wise model, which takes\nthe entire table as input and thus contextualized column represen-\ntations take into account table context in a holistic manner. For\ncolumn type prediction, Doduo attaches an additional dense layer\nfollowed by output layer with the size of |Ctype|.\nColumn-pair representations. For column relation prediction,\nDoduo concatenates a corresponding pair of contextualized col-\numn representations as a contextualized column-pair represen-\ntation. The additional dense layer should capture combinatorial\ninformation between two column-level representations. Same as\nthe column representations, column-pair representations are also\ntable-wise representations. In the experiment, we also tested a vari-\nant of Doduo that only takes a single column (a single column pair)\nas input for the column type (column relation) prediction task.\nMore formally, given a table ğ‘‡, the language model LM takes\nthe serialized sequence serialize(ğ‘‡)= {ğ‘¡1,...,ğ‘¡ ğ‘}of ğ‘ tokens as\ninput and outputs a sequence LM(ğ‘‡)where each element LM(ğ‘‡)ğ‘–\nis a ğ‘‘-dimensional context-aware embedding of the token ğ‘¡ğ‘–. Let\n{ğ‘–1,...,ğ‘– ğ‘›}be the indices of the inserted special [CLS]tokens. Let\nğ‘”type be the column type prediction dense layer of dimension ğ‘‘Ã—\n|Ctype|. The column type model computes:\nsoftmax(ğ‘”type (LM(ğ‘‡)ğ‘–ğ‘—)) (1)\nas the predicted column type of the ğ‘—-th column. Similarly, for\ncolumn relation prediction with dense layer ğ‘”rel of dimension 2ğ‘‘Ã—\n|Crel|, the column relation model computes:\nsoftmax(ğ‘”rel(LM(ğ‘‡)ğ‘–ğ‘— âŠ•LM(ğ‘‡)ğ‘–ğ‘˜)) (2)\nas the predicted relation between the ğ‘—-th and the ğ‘˜-th column\nof table ğ‘‡. The âŠ•symbol denotes concatenation of two vectors.\nDoduo then feeds the predictions and the groundtruth labels into\na cross entropy loss function to update the model parameters (Line\n9-10, Algorithm 1).\n4.4 Learning from Multiple Tasks\nIn the training phase, Doduo fine-tunes a pre-trained LM using\ntwo different training data and two different objectives. As shown\nin Algorithm 1, Doduo switches the task every epoch and updates\nthe parameters for different objectives using different optimization\nschedulers. This design choice enables Doduo to naturally handle\nimbalanced training data for different tasks. Furthermore, with a\nsingle objective function and a single optimizer, we need to carefully\nchoose hyper-parameter(s) that balance different objective terms\nto create a single objective function (e.g., â„“ = ğœ†â„“1 +(1 âˆ’ğœ†)â„“2 like\nTCN [57].) With our strategy, we can avoid adjusting the hyper-\nparameter. Also, in Section 6, we will show that Doduo can be\nrobustly trained with imbalanced training data.\nNote that Doduo is not limited to training with just two tasks.\nBy adding more output layers and corresponding loss functions,\nDoduo can be used for more than two tasks. We also note that\nfinding relevant tasks is challenging as adding new tasks might not\nnecessarily improve the modelâ€™s performance. This can be due to\nthe tasks not sharing enough common knowledge to improve each\nother, or noisy labels in training sets which can propagate among\nTable 2: Dataset description.\nName # tables # col # col types # col rels\nWikiTable 580,171 3,230,757 255 121\nVizNet 78,733 119,360 78 â€“\ntasks. Finding more relevant tasks and testing Doduo on them are\npart of our future work.\n5 EVALUATION\n5.1 Dataset\nWe used two benchmark datasets for evaluation. The WikiTable\ndataset [13] is a collection of tables collected from Wikipedia, which\nconsists of 580,171 tables in total. The dataset provides both an-\nnotated column types and relations for training and evaluation.\nFor column type prediction, the dataset provides 628,254 columns\nfrom 397,098 tables annotated by 255 column types. For column\nrelations, the dataset provides 62,954 column pairs annotated with\n121 relation types from 52,943 tables for training. According to [13],\nthe type and relation labels are from FreeBase [2] and are obtained\nby aggregating entity links attached to the original tables. For both\ntasks, we used the same train/valid/test splits as TURL [13]. Each\ncolumn/column-pair allows to have more than one annotation, and\nthus, the task is a multi-label classification task.\nThe VizNet dataset [66] is a collection of WebTables, which is\na subset of the original VizNet corpus [21]. The dataset is for the\ncolumn type prediction task. The dataset has 78,733 tables, and\n119,360 columns are annotated with 78 column types. The dataset\nconstructed the columns types by mapping column headers to\nDBpedia types [36] by a set of mapping rules. We used the same\nsplits for the cross-validation to make the evaluation results directly\ncomparable to [66]. Each column has only one label, and thus, the\ntask is a multi-class classification task.\n5.2 Baselines\nTURL [13] is a recently developed pre-trained Transformer-based\nLM for tables. TURL further pre-trains a pre-trained LM using table\ndata, so the model becomes more suitable for tabular data. Since\nTURL relies on entity-linking and meta information such as table\nheaders and table captions, which are not available in our scenario,\nwe used a variant of TURL pre-trained on table values for a fair\ncomparison. Note that to perform column type/relation annotation,\nwe fine-tuned the pre-trained TURL model on the same training\nsets as for Doduo and other baselines.\nSherlock [22] is a single-column prediction model that uses multi-\nple feature sets, including character embeddings, word embeddings,\nparagraph embeddings, and column statistics (e.g., mean, std of\nnumerical values.) A multi-layer â€œsubâ€ neural network is applied\nto each column-wise feature set to calculate compact dense vec-\ntors except for the column statistics feature set, which are already\ncontinuous values. The output of the subnetworks and the column\nstatistics features are fed into the â€œprimaryâ€ neural network that\nconsists of two fully connected layers.\nSato [66] is a multi-column prediction model, which extends Sher-\nlock by adding LDA features to capture table context and a CRF\nlayer to incorporate column type dependency into prediction. Sato\nis the state-of-the-art column type prediction on the VizNet dataset.\nAnnotating Columns with Pre-trained Language Models SIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA\nTable 3: Performance on the WikiTable dataset.\nMethod Col type Col rel\nP R F1 P R F1\nSherlock 88.40 70.55 78.47 â€“ â€“ â€“\nTURL 90.54 87.23 88.86 91.18 90.69 90.94\nDoduo 92.69 92.21 92.45 91.97 91.47 91.72\nTURL+metadata 92.75 92.63 92.69 92.90 93.80 93.35\nDoduo+metadata 93.25 92.34 92.79 91.20 94.50 92.82\n5.3 Experimental Settings\nWe used Adam optimizer with anğœ–of 10âˆ’8. The initial learning rate\nwas set to be 5 Ã—10âˆ’5 with a linear decay scheduler with no warm-\nup. We trainedDoduo for 30 epochs and chose the checkpoint with\nthe highest F1 score on the validation set.\nSince the WikiTable dataset can have multiple labels on each\ncolumn/column pair, we used Binary Cross Entropy loss to formu-\nlate as a multi-label prediction task. For the VizNet dataset, which\nonly has a single annotation on each column, we used Cross En-\ntropy loss to formulate as a multi-class prediction task. Models and\nexperiments were implemented with PyTorch [38] and the Trans-\nformers library [59]. All experiments were conducted on an AWS\np3.8xlarge instance (V100 (16GB)).\nFollowing the previous studies [ 13, 66], we use micro F1 for\nthe WikiTable dataset, and micro F1 and macro F1 for the VizNet\ndataset, as evaluation metrics. The micro F1 score is the weighted\naverage of F1 values based on the sample size of each class, while\nthe macro F1 score is the simple average of F1 values for all classes.\nAdditional results and analysis can be found in Appendix.\n5.4 Main Results\nWikiTable Table 3 shows the micro F1 performance for the col-\numn type prediction and column relation prediction tasks on the\nWikiTable dataset. Doduo significantly outperforms the state-of-\nthe-art method TURL on both of the tasks with improvements of\n4.0% and 0.9%, respectively.\nA significant difference in the model architecture betweenDoduo\nand TURL is whether the model uses full self-attention. In TURL, the\nmodel uses the self-attention mechanism with the â€œcross-columnâ€\nedges removed, which they referred to as visibility matrix [ 13].\nLet us use the example in Figure 3, which depicts how the contex-\ntualized embedding for the second column is calculated. TURLâ€™s\nvisibility matrix removes the connections to [CLS]2 from the cells\nâ€œHappy Feetâ€, â€œCarsâ€, â€œUSAâ€, and â€œUKâ€, whereas our Doduo\nuses the full set of connections.\nSince TURL is designed for tables with meta information (e.g.,\ntable captions or column headers), we consider the major benefit\nof this design (i.e., the visibility matrix) to effectively incorporate\ndescriptions in the meta information into table values. From the\nresults, Doduo with the full self-attention performs better than\nTURL, which indicates that some direct intersections between to-\nkens in different columns and different rows are useful for the\ncolumn annotation problem.\nWe also testedDoduo with metadata, which appends the column\nname to column values for each column before serialization. As\nshown in Table 3, by using column names,Doduo slightly improves\nthe performance and performs competitively against TURL with\nmetadata. This indicates that TURL relies on metadata and Doduo\nTable 4: Performance on the VizNet dataset.\nFull Multi-column only\nMethod Macro F1 Micro F1 Macro F1 Micro F1\nSherlock 69.2 86.7 64.2 87.9\nSato 75.6 88.4 73.5 92.5\nDoduo 84.6 94.3 83.8 96.4\nperforms better and more robustly than TURL when metadata is\nnot available.\nVizNet Table 4 shows the results on the VizNet dataset. Note that\nDoduo is trained only using the column prediction task for the\nVizNet dataset, as column relation labels are not available for the\ndataset. The results show that Doduo outperforms Sherlock and\nSato, the SoTA method for the dataset, by a large margin and estab-\nlishes new state-of-the-art performance with micro F1 (macro F1)\nimprovements of 11.9% (6.7%.)\nAs described in Section 2, Sato is a multi-column model that\nincorporates table context by using LDA features. Different from\nthe LDA features that provide multi-dimensional vector represen-\ntations for the entire table, the Transformer-based architecture\nenables Doduo to capture more fine-grained inter-token relation-\nships through the self-attention mechanism. Furthermore,Doduoâ€™s\ntable-wise design naturally helps incorporate inter-column infor-\nmation into the model.\nPerformance on numeric columns. As mentioned in Section\n3, Doduo casts all cell values as strings thus it may be weak in\nhandling numeric columns. Table 5 shows Doduo performance on\nthe top-15 most numeric column types from the VizNet dataset.\nDoduo did have low performance on some numeric types such as\nranking (33.21%) and capacity (62.55%). However, on these 15 types,\nDoduo achieves an average F1 score of 86.9% which is comparable\nto the overall macro F1 (84.6%) and even slightly better. This can be\ndue to the Transformer model being able to recognize digit patterns\nin the numeric values to predict the correct types. This results aligns\nwith the findings from the NLP literature [16, 55] that Transformer\nmodels can partially handle numeric data.\nTable 5: Doduoâ€™s column type prediction performance on the\n15 most numeric types of the VizNet dataset. We use %num\nto measure how many cell values of a type can be cast as a\nnumeric type (e.g., int, float, date).\ntype %num F1 type %num F1 type %num F1\nplays 100.00 88.55 fileSize 87.84 88.23 grades 67.18 97.68\nrank 93.01 94.52 elevation 87.39 92.14 weight 60.41 97.59\ndepth 92.86 88.45 ranking 86.88 33.21 isbn 43.77 96.51\nsales 92.05 75.13 age 81.04 98.53 capacity 42.06 62.55\nyear 91.47 98.94 birthDate 67.85 95.64 code 35.93 95.43\n6 ANALYSIS\n6.1 Ablation Analysis\nTo verify the effectiveness of multi-task learning and multi-column\narchitecture, we tested variants of Doduo. Dosolo is a Doduo\nmodel without multi-task learning. Thus, we trained Doduo mod-\nels only using training data for the target task (i.e., column type\nprediction or column relation prediction.) DosoloSCol is a single\ncolumn model that only uses column values of the target column\nSIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp, Chen Chen, and Wang-Chiew Tan\nTable 6: Ablation study on the WikiTable dataset.\nMethod Type prediction Relation prediction\nDoduo 92.50 91.90\nw/ shuffled rows 91.94 91.61\nw/ shuffled cols 92.68 91.98\nDosolo 91.37 (1.23% â†“) 91.24 (0.7% â†“)\nDosoloSCol 82.45 (21.9% â†“) 83.08 (9.6% â†“)\nTable 7: Ablation study on the VizNet dataset (Full.)\nMacro F1 Micro F1\nDoduo 84.6 94.3\nDosoloSCol 77.4 (8.5% â†“) 90.2 (4.3% â†“)\n(or target column pair for column relation prediction.) DosoloSCol\nis also trained without multi-task learning.\nTable 6 shows the results of the ablation study. For both of the\ntasks, Dosolo degraded the performance compared to the multi-\ntask learning of Doduo. As DosoloSCol shows significantly lower\nperformance than the others, the results also confirm that the multi-\ncolumn architecture of Doduo successfully captures table context\nto improve the performance on the column type prediction and\ncolumn relation prediction tasks.\nThe same analysis with DosoloSCol on the VizNet dataset is\nshown in Table 7. As expected, the multi-column model (Doduo)\nperforms significantly better than the single-column model (DosoloSCol.)\nThe results further confirm the strengths of the multi-column model.\nWe would like to emphasize that DosoloSCol outperforms Sato,\nwhich incorporates table context as LDA features.\nThe pre-trained LM (e.g., BERT) is sensitive to the input se-\nquence order, which may not reflect the property of tables that\nrows/columns are order-invariant. To verify if Doduo has this lim-\nitation, we trained and evaluated Doduo on two versions of the\nWikiTable dataset, where the input tableâ€™s rows (columns) were\nrandomly shuffled. As shown in Table 6, somewhat surprisingly,\nDoduo shows only subtle degradation for shuffled rows and no sub-\nstantial difference for shuffled columns. We conjecture thatDoduo\nsuccessfully tailors the original position embeddings to be aligned\nwith the table structure during the fine-tuning step.\n6.2 Data Efficiency\nLearning Efficiency Pre-trained LMs are known for its capability\nof training high-quality models using a relatively small number of\nlabeled examples. Furthermore, multi-task learning (i.e., training\nwith multiple tasks simultaneously) should further stabilize the\nperformance with fewer training data for each task. To verify the\neffectiveness of Doduo and Dosolo with respect to label efficiency,\nwe compared the Doduo models trained with different training\ndata sizes (10%, 25%, 50%, and 100%) and evaluated the performance\non the column type prediction and column relation prediction tasks.\nAs shown in Figure 4, Doduo consistently outperforms Dosolo\nand achieves higher than 0.9 F1 scores on both tasks even when\ntrained with half of the training data. In particular, with only 50%\nor fewer labeled examples, Doduo outperforms the SoTA method\nTURL on column-type prediction and achieves comparable perfor-\nmance on column relation prediction.\nInput Data Efficiency A major challenge of applying pre-trained\nLMs to data management tasks is their limits of the maximum input\nsequence length. For example, LMs like BERT can only take at most\n10% 25% 50% 100%\nTraining data ratio (%)\n0.84\n0.86\n0.88\n0.90\n0.92F1\nDoduo\nDosolo\n(a) Column type prediction\n10% 25% 50% 100%\nTraining data ratio (%)\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92F1\nDoduo\nDosolo (b) Column relation prediction\nFigure 4: Performance improvements over increasing the\ntraining data size on the WikiTable dataset. The dashed lines\nin the plots denote the state-of-the-art methods (TURL.)\nTable 8: Comparisons of Doduo with different input token\nsize on the WikiTable dataset.\nMaxToken/col Col type (F1) Col rel (F1) Max. # of cols\n8 89.8 88.9 56\n16 91.4 90.7 30\n32 92.4 91.7 15\n512 tokens so ingesting a full wide table may not be feasible for\nthe LMs. Doduo (or the multi-column model in general) has the\nadvantage that it is input data efficient , meaning that it can make\ntable-wise predictions accurately by only taking a small number\nof samples of each column. This makes Doduo more attractive in\npractice as it can handle large tables with many columns.\nThus, we evaluated different variants of Doduo with shorter\ninput token length to discuss the input data efficiency of Doduo.\nWe would like to emphasize that any of the recent studies applying\npre-trained Transformer-based LMs to data management tasks (e.g.,\n[13, 26, 52, 57]) did not conduct this kind of analysis. Thus, it is still\nnot clear how many tokens should we feed to the model to obtain\nreasonable task performance.\nTable 8 shows the results of Doduo with different max token\nsizes on the WikiTable dataset. We simply truncated column values\nif the number of tokens exceeded the threshold. As shown in the\ntable, the more tokens used, the better performance Doduocan\nachieve, as expected. However, somewhat surprisingly,Doduo al-\nready outperforms TURL using just 8 tokens per column for column\ntype prediction (TURL has micro F1 of 88.86 on WikiTable). For\nthe column relation prediction task, Doduo needs to use more\ntokens to outperform TURL (i.e., 32 tokens to beat TURLâ€™s score\nof 90.94.) This is understandable, as column relation prediction is\nmore contextual than column type prediction, and thus it requires\nmore signals to further narrow down to the correct prediction. For\nthe VizNet dataset, we confirm that Doduo with 8 max tokens\nper column with Doduo (92.5 F1) outperforms the state-of-the-\nart method (i.e., Sato) (88.4 F1) on the task. Table 8 reports how\nmany columns each variant can support under the maximum token\nconfiguration. The average numbers of columns in Web Tables,\nEnterprise Data, and Open Data are reported to be 4, 12, and 16,\nrespectively [12, 33]. Thus, we confirm thatDoduo has a nice input\ndata efficiency property, so it can be also used for â€œwideâ€ tables.\nWe note that 16 columns may not be sufficient for tables outside\nof WebTables. For such cases, a reasonable option is to first split\nthe wide table into clusters of relevant columns (maybe by some\nuser-defined rules), then apply Doduo on each cluster. In this case,\nAnnotating Columns with Pre-trained Language Models SIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA\nDoduo still has the advantage of leveraging partial context of the\ninput table to improve prediction quality.\n7 CASE STUDY: CLUSTERING COLUMNS\nWe apply Doduo to a real data science application scenario of\nclustering relevant columns. On a daily basis, data scientists col-\nlect information from multiple sources and incorporate data from\nvarious tables. Therefore, as a first step of data exploration and\ndata integration [37], it is essential to know which columns are\nsemantically similar. However, this is not always straightforward\nas different column names can be assigned to semantically similar\ncolumns. In this section, we demonstrate the usefulness ofDoduoâ€™s\ncontextualized column embeddings with a case study of clustering\nsemantically similar columns on an enterprise database.\nFor this case study, we use an in-production enterprise database\nfrom the HR domain. Different teams create and update tables\nabout job seekers and companies that are hiring, etc. Despite some\ncompany-wide naming conventions, we observe that semantically\nsimilar columns are sometimes given different column names across\ntables. Some tables have additional meta information describing the\nmeaning of each column, which helps to understand the similarity\nof columns. However, the meta information is missing for many\ncolumns and is not always reliable as they are worded differently\nby different teams. Next, we simulate a workflow of a data scientist\nperforming analysis related to job search and review of companies.\nScenario: Our data scientist Sofia starts by filtering tables with\nkeywords â€œjobsearchâ€ and â€œreviewâ€ and gets 10 tables with 50\ncolumns in total (29 columns of type â€œstringâ€ and 21 columns of\ntype â€œintegerâ€.) To group similar columns together, she can simply\ncreate contextualized column embedding using the Doduo toolbox\nfor all the columns and then apply her favorite clustering algorithm\nto form the final groups.\nTo evaluate Sofiaâ€™s column groups, we generate an initial cluster\nusing both the column names and descriptions and manually refine\nthe results to form the ground-truth5 as shown below:\ndate, IP address, job title, timestamp (unixtime), timestamp (hhmm), counts, status,\nfile path, browser, location, search term, rating, company ID, review ID, user ID\nSofia use the Doduo model trained on the WikiTable dataset to\nobtain contextualized column embeddings for each column (Doduo+\ncolumn value emb.)\nWe compared the method with three baselines and two tradi-\ntional schema matching approaches. We directly used Doduoâ€™s\ncolumn type predictions as the clustering criteria where columns\nwith the same predicted types got assigned into the same clus-\nter (Doduo+predicted type). We tested fastText [1] to verify how\nnon-contextualized column embeddings perform for the task. We\nuse column value embeddings (fastText+column value emb) and\ncolumn name embeddings (fastText+column name emb) with fast-\nText. We choose fastText as a baseline as it offers a widely used\noff-the-shelf toolbox, which is a â€œgo-toâ€ option for data scientists. To\nachieve a fair evaluation of the embedding quality, we use the same\n5We used column name and description for ground-truth creation purpose only as it\nis not available for all tables in practice. The initial clustering uses a combination of\nTF-IDF vectors and fastText embeddings.\nTable 9: Case study results.\nMethod Prec. Recall F1\nDoduo+column value emb 68.19 70.40 69.28\nDoduo+predicted type 44.87 61.32 51.82\nfastText+column value emb 35.90 76.61 48.89\nfastText+column name emb 56.62 74.68 64.40\nCOMA (with column name) 58.47 66.06 62.03\nDistributionBased (with column name) 23.87 69.51 35.53\nk-means clustering algorithm for all models. In addition, we com-\npared with more traditional schema matching approaches tested\nin the experiment suite Valentine [ 25]. We picked the two most\neffective approaches from Valentineâ€™s empirical study: COMA [15]\nand DistributionBased [ 67]. To generate the clustering label for\ncomparison, we went over all possible pairs of tables and connected\nmatched columns to assign the same cluster labels. Since those\nschema matching methods take two tables as input and return pairs\nof matched columns, we regard returned pairs as connected nodes\nin a graph and merge them into connected components to obtain\ncolumn clusters. We use Homogeneity (Precision), Completeness\n(Recall), V-Measure (F1) to evaluate the quality of clusters using\nthe ground-truth cluster assignment.\nAs shown in Table 9,Doduoâ€™s column embeddings show the best\nclustering performance with respect to Precision and F1. The results\nconfirm that contextualized column embeddings are more useful\nthan predicted column types and can be more accurate represen-\ntations than column name/value embeddings created by fastText.\nCompared to Doduo, fastText tends to generate similar embeddings\neven for semantically different columns, which leads to creating\nunnecessarily large clusters that contain many irrelevant columns.\nThis significantly increases (decreases) fastText methodsâ€™ Recall\n(Precision) values in Table 9. COMA shows reasonably good perfor-\nmance and DistributionBased falls short on precision, given both\ncolumn name and content. Doduo outperforms both matching-\nbased approaches using the contextualized embedding. Note that\nboth the column names and column descriptions are not given\nto the Doduo model as input, and the model was trained on the\nWikiTable dataset (i.e., different domain.) Thus, this case study\nalso indicates the transferability of a Doduo model trained on one\ndomain (i.e., Wikipedia tables) to another domain (i.e., enterprise\ndatabase.) so that data scientists can apply the Doduo model in the\ntoolbox for their own needs.\n8 CONCLUSION\nIn this paper, we present Doduo, a unified column annotation\nframework based on pre-trained Transformer language models\nand Multi-task learning. Experiments on two benchmark datasets\nshow that Doduo achieves new state-of-the-performance. With a\nseries of analyses, we confirm that the improvements are benefited\nfrom the multi-task learning framework. Through the analysis,\nwe also confirm that Doduo is data-efficient, as it can achieve\ncompetitive performance as the previous state-of-the-art methods\nonly using 8 tokens per column or about 50% of training data. We\nconduct a case study and verify the effectiveness of Doduo on\na real-world data science problem. We believe our toolbox will\nfurther help researcher/data scientists easily apply the state-of-the-\nart column annotation model to a wide variety of data science and\ndata management problems.\nSIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp, Chen Chen, and Wang-Chiew Tan\nACKNOWLEDGMENTS\nWe thank Jin Wang and Estevam Hruschka for their valuable feed-\nback on the draft.\nREFERENCES\n[1] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. En-\nriching Word Vectors with Subword Information. Transactions of the Association\nfor Computational Linguistics 5 (2017), 135â€“146. https://doi.org/10.1162/tacl_a_\n00051\n[2] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.\n2008. Freebase: a collaboratively created graph database for structuring human\nknowledge. In SIGMOD. 1247â€“1250.\n[3] Ursin Brunner and Kurt Stockinger. 2020. Entity Matching with Transformer\nArchitectures - A Step Forward in Data Integration. In Proceedings of the\n23rd International Conference on Extending Database Technology, EDBT 2020,\nCopenhagen, Denmark, March 30 - April 02, 2020 , Angela Bonifati, Yongluan\nZhou, Marcos Antonio Vaz Salles, Alexander BÃ¶hm, Dan Olteanu, George\nH. L. Fletcher, Arijit Khan, and Bin Yang (Eds.). OpenProceedings.org, 463â€“473.\nhttps://doi.org/10.5441/002/edbt.2020.58\n[4] Matteo Cannaviccio, Denilson Barbosa, and Paolo Merialdo. 2018. Towards\nAnnotating Relational Data on the Web with Language Models. In Proceedings of\nthe 2018 World Wide Web Conference (Lyon, France) (WWW â€™18) . International\nWorld Wide Web Conferences Steering Committee, Republic and Canton of\nGeneva, CHE, 1307â€“1316. https://doi.org/10.1145/3178876.3186029\n[5] Riccardo Cappuzzo, Paolo Papotti, and Saravanan Thirumuruganathan. 2020.\nCreating Embeddings of Heterogeneous Relational Datasets for Data Integration\nTasks. In Proceedings of the 2020 ACM SIGMOD International Conference on Man-\nagement of Data (Portland, OR, USA) (SIGMOD â€™20) . Association for Computing\nMachinery, New York, NY, USA, 1335â€“1349. https://doi.org/10.1145/3318464.\n3389742\n[6] Rich Caruana. 1993. Multitask Learning: A Knowledge-Based Source of Inductive\nBias. In Proceedings of the Tenth International Conference on International Confer-\nence on Machine Learning (Amherst, MA, USA) (ICMLâ€™93). Morgan Kaufmann\nPublishers Inc., San Francisco, CA, USA, 41â€“48.\n[7] R. Caruana. 2004. Multitask Learning. Machine Learning 28 (2004), 41â€“75.\n[8] Adriane Chapman, Elena Simperl, Laura Koesten, George Konstantinidis, Luis-\nDaniel IbÃ¡Ã±ez, Emilia Kacprzak, and Paul Groth. 2020. Dataset search: a survey.\nVLDB J. 29, 1 (2020), 251â€“272.\n[9] Jiaoyan Chen, Ernesto JimÃ©nez-Ruiz, Ian Horrocks, and Charles Sutton. 2019.\nColNet: Embedding the Semantics of Web Tables for Column Type Prediction.\nIn The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The\nThirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019,\nThe Ninth AAAI Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 . AAAI Press,\n29â€“36. https://doi.org/10.1609/aaai.v33i01.330129\n[10] Jiaoyan Chen, Ernesto JimÃ©nez-Ruiz, Ian Horrocks, and Charles Sutton. 2019.\nLearning Semantic Annotations for Tabular Data. In Proceedings of the Twenty-\nEighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,\nChina, August 10-16, 2019 , Sarit Kraus (Ed.). ijcai.org, 2088â€“2094. https://doi.org/\n10.24963/ijcai.2019/289\n[11] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.\n2019. What Does BERT Look at? An Analysis of BERTâ€™s Attention. In Proc.\nBlackBoxNLP â€™19 . 276â€“286.\n[12] Dong Deng, Raul Castro Fernandez, Ziawasch Abedjan, Sibo Wang, Michael\nStonebraker, Ahmed K. Elmagarmid, Ihab F. Ilyas, Samuel Madden, Mourad Ouz-\nzani, and Nan Tang. 2017. The Data Civilizer System. In8th Biennial Conference on\nInnovative Data Systems Research, CIDR 2017, Chaminade, CA, USA, January 8-11,\n2017, Online Proceedings . www.cidrdb.org. http://cidrdb.org/cidr2017/papers/p44-\ndeng-cidr17.pdf\n[13] Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2020. TURL: Table\nUnderstanding through Representation Learning. Proc. VLDB Endow. 14, 3 (nov\n2020), 307â€“319. https://doi.org/10.14778/3430915.3430921\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n4171â€“4186. https://doi.org/10.18653/v1/N19-1423\n[15] Hong-Hai Do and Erhard Rahm. 2002. COMAâ€”a system for flexible combination\nof schema matching approaches. In VLDBâ€™02: Proceedings of the 28th International\nConference on Very Large Databases . Elsevier, 610â€“621.\n[16] Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting Numerical Rea-\nsoning Skills into Language Models. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics . Association for Computational\nLinguistics, Online, 946â€“958. https://doi.org/10.18653/v1/2020.acl-main.89\n[17] GLUE. 2021. GLUE Leaderboard. https://gluebenchmark.com/leaderboard (2021).\n[18] Google. [n.d.]. Google Data Studio. https://datastudio.google.com/\n[19] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher.\n2017. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP\nTasks. In Proceedings of the 2017 Conference on Empirical Methods in Natural\nLanguage Processing . Association for Computational Linguistics, Copenhagen,\nDenmark, 1923â€“1933. https://doi.org/10.18653/v1/D17-1206\n[20] Jonathan Herzig, Pawel Krzysztof Nowak, Thomas MÃ¼ller, Francesco Piccinno,\nand Julian Eisenschlos. 2020. TaPas: Weakly Supervised Table Parsing via Pre-\ntraining. In Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics . Association for Computational Linguistics, Online, 4320â€“4333.\nhttps://doi.org/10.18653/v1/2020.acl-main.398\n[21] Kevin Hu, Snehalkumar â€™Neilâ€™ S. Gaikwad, Madelon Hulsebos, Michiel A. Bakker,\nEmanuel Zgraggen, CÃ©sar Hidalgo, Tim Kraska, Guoliang Li, Arvind Satya-\nnarayan, and Ã‡aÄŸatay Demiralp. 2019. VizNet: Towards A Large-Scale Visu-\nalization Learning and Benchmarking Repository. In Proceedings of the 2019\nCHI Conference on Human Factors in Computing Systems (Glasgow, Scotland\nUk) (CHI â€™19) . Association for Computing Machinery, New York, NY, USA, 1â€“12.\nhttps://doi.org/10.1145/3290605.3300892\n[22] Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind Satya-\nnarayan, Tim Kraska, Ã‡agatay Demiralp, and CÃ©sar Hidalgo. 2019. Sherlock: A\nDeep Learning Approach to Semantic Data Type Detection. In Proceedings of\nthe 25th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining (Anchorage, AK, USA) (KDD â€™19) . Association for Computing Machinery,\nNew York, NY, USA, 1500â€“1508. https://doi.org/10.1145/3292500.3330993\n[23] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How Can\nWe Know What Language Models Know? Transactions of the Association for\nComputational Linguistics 8 (2020), 423â€“438. https://doi.org/10.1162/tacl_a_00324\n[24] Udayan Khurana and Sainyam Galhotra. 2021. Semantic Concept Annotation\nfor Tabular Data. In Proceedings of the 30th ACM International Conference on\nInformation & Knowledge Management . Association for Computing Machinery,\nNew York, NY, USA, 844â€“853. https://doi.org/10.1145/3459637.3482295\n[25] Christos Koutras, George Siachamis, Andra Ionescu, Kyriakos Psarakis, Jerry\nBrons, Marios Fragkoulis, Christoph Lofi, Angela Bonifati, and Asterios Katsi-\nfodimos. 2021. Valentine: Evaluating Matching Techniques for Dataset Discovery.\nIn 2021 IEEE 37th International Conference on Data Engineering (ICDE) . IEEE,\n468â€“479.\n[26] Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan. 2020.\nDeep entity matching with pre-trained language models. Proceedings of the VLDB\nEndowment 14, 1 (Sep 2020), 50â€“60. https://doi.org/10.14778/3421424.3421431\n[27] Yuliang Li, Jinfeng Li, Yoshihiko Suhara, Jin Wang, Wataru Hirota, and Wang-\nChiew Tan. 2021. Deep Entity Matching: Challenges and Opportunities. J. Data\nand Information Quality 13, 1, Article 1 (Jan. 2021), 17 pages. https://doi.org/10.\n1145/3431816\n[28] Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. 2010. Annotating and\nSearching Web Tables Using Entities, Types and Relationships.Proc. VLDB Endow.\n3, 1â€“2 (Sept. 2010), 1338â€“1347. https://doi.org/10.14778/1920841.1921005\n[29] Erin Macdonald and Denilson Barbosa. 2020. Neural Relation Extraction on\nWikipedia Tables for Augmenting Knowledge Graphs . Association for Computing\nMachinery, New York, NY, USA, 2133â€“2136. https://doi.org/10.1145/3340531.\n3412164\n[30] Mohammad Mahdavi and Ziawasch Abedjan. 2020. Baran: Effective error correc-\ntion via a unified context representation and transfer learning. Proceedings of the\nVLDB Endowment (PVLDB) 13, 11 (2020), 1948â€“1961.\n[31] Mohammad Mahdavi, Ziawasch Abedjan, Raul Castro Fernandez, Samuel Mad-\nden, Mourad Ouzzani, Michael Stonebraker, and Nan Tang. 2019. Raha: A\nconfiguration-free error detection system. In Proceedings of the International\nConference on Management of Data (SIGMOD) . ACM, 865â€“882.\n[32] TomÃ¡s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nEstimation of Word Representations in Vector Space. In 1st International Con-\nference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May\n2-4, 2013, Workshop Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.).\nhttp://arxiv.org/abs/1301.3781\n[33] RenÃ©e J. Miller. 2018. Open Data Integration. Proc. VLDB Endow. 11, 12 (Aug.\n2018), 2130â€“2139. https://doi.org/10.14778/3229863.3240491\n[34] Emir MuÃ±oz, Aidan Hogan, and Alessandra Mileo. 2014. Using Linked Data to\nMine RDF from Wikipediaâ€™s Tables. In Proceedings of the 7th ACM International\nConference on Web Search and Data Mining (New York, New York, USA)(WSDM\nâ€™14). Association for Computing Machinery, New York, NY, USA, 533â€“542. https:\n//doi.org/10.1145/2556195.2556266\n[35] Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. PATTY: A\nTaxonomy of Relational Patterns with Semantic Types. In Proceedings of the 2012\nJoint Conference on Empirical Methods in Natural Language Processing and Com-\nputational Natural Language Learning . Association for Computational Linguistics,\nJeju Island, Korea, 1135â€“1145. https://www.aclweb.org/anthology/D12-1104\n[36] Phuc Nguyen, Natthawut Kertkeidkachorn, Ryutaro Ichise, and Hideaki Takeda.\n[n.d.]. MTab4DBpedia: Semantic Annotation for Tabular Data with DBpedia.\n([n. d.]).\nAnnotating Columns with Pre-trained Language Models SIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA\n[37] Masayo Ota, Heiko MÃ¼ller, Juliana Freire, and Divesh Srivastava. 2020. Data-\nDriven Domain Discovery for Structured Datasets. Proc. VLDB Endow. 13, 7\n(March 2020), 953â€“967. https://doi.org/10.14778/3384345.3384346\n[38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-\nmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\nLibrary. In Advances in Neural Information Processing Systems 32 , H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.). Cur-\nran Associates, Inc., 8024â€“8035. http://papers.neurips.cc/paper/9015-pytorch-\nan-imperative-style-high-performance-deep-learning-library.pdf\n[39] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:\nGlobal Vectors for Word Representation. InProceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP) . Association for\nComputational Linguistics, Doha, Qatar, 1532â€“1543. https://doi.org/10.3115/v1/\nD14-1162\n[40] Fabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin,\nYuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases?.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong,\nChina, 2463â€“2473. https://doi.org/10.18653/v1/D19-1250\n[41] Erhard Rahm and Philip A. Bernstein. 2001. A survey of approaches to automatic\nschema matching. VLDB J. 10, 4 (2001), 334â€“350.\n[42] Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge\nCan You Pack Into the Parameters of a Language Model?. InProceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 5418â€“5426. https://doi.org/\n10.18653/v1/2020.emnlp-main.437\n[43] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A Primer in BERTology:\nWhat We Know About How BERT Works. Transactions of the Association for\nComputational Linguistics 8 (2020), 842â€“866. https://doi.org/10.1162/tacl_a_00349\n[44] Sebastian Ruder. 2017. An Overview of Multi-Task Learning in Deep Neural\nNetworks. arXiv:1706.05098 [cs.LG]\n[45] Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix BieÃŸ-\nmann, and Andreas Grafberger. 2018. Automating Large-Scale Data Quality\nVerification. Proc. VLDB Endow. 11, 12 (2018), 1781â€“1794.\n[46] Tableau Software. [n.d.]. Tableau. https://www.tableau.com/\n[47] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and\nHaifeng Wang. 2020. ERNIE 2.0: A Continual Pre-Training Framework for Lan-\nguage Understanding. In AAAI. 8968â€“8975.\n[48] Kunihiro Takeoka, Masafumi Oyamada, Shinji Nakadai, and Takeshi Okadome.\n2019. Meimei: An Efficient Probabilistic Approach for Semantically Annotating\nTables. Proceedings of the AAAI Conference on Artificial Intelligence 33, 01 (Jul.\n2019), 281â€“288. https://doi.org/10.1609/aaai.v33i01.3301281\n[49] Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden,\nand Mourad Ouzzani. 2021. RPT: Relational Pre-Trained Transformer is Almost\nAll You Need towards Democratizing Data Preparation. Proc. VLDB Endow. 14, 8\n(apr 2021), 1254â€“1261. https://doi.org/10.14778/3457390.3457391\n[50] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical\nNLP Pipeline. In ACL. 4593â€“4601.\n[51] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical\nNLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence,\nItaly, 4593â€“4601. https://doi.org/10.18653/v1/P19-1452\n[52] Mohamed Trabelsi, Jin Cao, and Jeff Heflin. 2020. Semantic Labeling Using a\nDeep Contextualized Language Model. arXiv:2010.16037 [cs.LG]\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. NIPS â€™17 . 5998â€“6008.\n[54] Petros Venetis, Alon Halevy, Jayant Madhavan, Marius PaÅŸca, Warren Shen, Fei\nWu, Gengxin Miao, and Chung Wu. 2011. Recovering Semantics of Tables on\nthe Web. Proc. VLDB Endow. 4, 9 (June 2011), 528â€“538. https://doi.org/10.14778/\n2002938.2002939\n[55] Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do\nNLP Models Know Numbers? Probing Numeracy in Embeddings. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP). Association for Computational Linguistics, Hong Kong, China, 5307â€“\n5315. https://doi.org/10.18653/v1/D19-1534\n[56] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for\nNatural Language Understanding. In ICLR.\n[57] Daheng Wang, Prashant Shiralkar, Colin Lockard, Binxuan Huang, Xin Luna\nDong, and Meng Jiang. 2021. TCN: Table Convolutional Network for Web Ta-\nble Interpretation. In Proceedings of the Web Conference 2021 (Ljubljana, Slove-\nnia) (WWW â€™21) . Association for Computing Machinery, New York, NY, USA,\n4020â€“4032. https://doi.org/10.1145/3442381.3450090\n[58] Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei\nZhang. 2021. TUTA: Tree-Based Transformers for Generally Structured Table\nPre-Training. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining (Virtual Event, Singapore) (KDD â€™21) . Association for\nComputing Machinery, New York, NY, USA, 1780â€“1790. https://doi.org/10.1145/\n3447548.3467434\n[59] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations . Association for Computational\nLinguistics, Online, 38â€“45. https://www.aclweb.org/anthology/2020.emnlp-\ndemos.6\n[60] Neo Wu, Bradley Green, Xue Ben, and Shawn Oâ€™Banion. 2020. Deep transformer\nmodels for time series forecasting: The influenza prevalence case. arXiv preprint\narXiv:2001.08317 (2020).\n[61] Yongxin Yang and Timothy M. Hospedales. 2017. Trace Norm Regularised Deep\nMulti-Task Learning. In ICLR â€™17 Workshop Track .\n[62] Alexander Yates, Michele Banko, Matthew Broadhead, Michael Cafarella, Oren\nEtzioni, and Stephen Soderland. 2007. TextRunner: Open Information Extrac-\ntion on the Web. In Proceedings of Human Language Technologies: The Annual\nConference of the North American Chapter of the Association for Computational\nLinguistics (NAACL-HLT). Association for Computational Linguistics, Rochester,\nNew York, USA, 25â€“26. https://www.aclweb.org/anthology/N07-4013\n[63] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020.\nTaBERT: Pretraining for Joint Understanding of Textual and Tabular Data.\nIn Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics . Association for Computational Linguistics, Online, 8413â€“8426.\nhttps://doi.org/10.18653/v1/2020.acl-main.745\n[64] Amir R. Zamir, Alexander Sax, William B. Shen, Leonidas J. Guibas, Jitendra Malik,\nand Silvio Savarese. 2018. Taskonomy: Disentangling Task Transfer Learning. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE.\n[65] ChengXiang Zhai. 2008. Statistical Language Models for Information Retrieval:\nA Critical Review. Found. Trends Inf. Retr. 2, 3 (2008), 137â€“213. https://doi.org/10.\n1561/1500000008\n[66] Dan Zhang, Yoshihiko Suhara, Jinfeng Li, Madelon Hulsebos, Ã‡aÄŸatay Demi-\nralp, and Wang-Chiew Tan. 2020. Sato: Contextual Semantic Type Detec-\ntion in Tables. Proc. VLDB Endow. 13, 12 (July 2020), 1835â€“1848. https:\n//doi.org/10.14778/3407790.3407793\n[67] Meihui Zhang, Marios Hadjieleftheriou, Beng Chin Ooi, Cecilia M Procopiuc,\nand Divesh Srivastava. 2011. Automatic discovery of attributes in relational\ndatabases. In Proceedings of the 2011 ACM SIGMOD International Conference on\nManagement of data . 109â€“120.\nSIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp, Chen Chen, and Wang-Chiew Tan\nA ADDITIONAL ANALYSIS\nA.1 Detailed Results\nFigure 5 shows the F1 score of Doduo and Sato for each class on\nthe VizNet (Full) and VizNet (Multi-column only) datasets. The im-\nprovements against Sato on the two variants of the VizNet datasets\nindicate that Doduo robustly and consistently performs better than\nSato, especially for single-column tables, where Sato cannot benefit\nfrom the table features and CRF. We found that Sato shows zero or\nvery poor F1 values for religion, education, organisation. The\nlabeled columns in the training data in the 1st fold of the VizNet\n(Full) are only 24, 22, and 14, respectively. Sato should suffer from\nthe lack of training examples for those column types, and probably\nthe skewed column type distribution as well. We show thatDoduo\nrobustly performs well on such column types.\nTable 10 shows the performance of Doduo and Dosolo on the\nWikiTable dataset for 6 column types/column relations. We confirm\nthat Doduo tends to perform better for the column types/relations\nthat are less clearly distinguishable (e.g., artist vs. writer, place-of-\nbirth vs. place-lived.)\nA.2 Input Token Length\nFor the VizNet dataset, we tested Doduo and DosoloSCol with dif-\nferent maximum token numbers per column. Table 11 shows the\nsimilar trends as the results on the WikiTable dataset. Doduo with\n8 max tokens per column with Doduo outperforms the state-of-\nthe-art method (i.e., Sato) on the task. As we observe significant dif-\nferences between the multi-column model (Doduo) and the single-\ncolumn model (DosoloSCol), we consider it is mainly because the\nTransformer blocks (i.e., self-attention mechanisms) capture the\ninter-column table context successfully.\nA.3 Learning Efficiency\nA.4 Inter-column Dependency\nA strength of the Transformer architecture is stacked Transformer\nblocks that calculate highly contextual information through the self-\nattention mechanism. As described in Section 4, Doduo uses the\n[CLS] dummy symbols to explicitly obtain contextualized column\nrepresentations. The representations not only take into account\ntable context but also explicitly incorporate the inter-column de-\npendency. That is, as we showed in Figure 2, predictions for some\ncolumns should be relevant and useful for other columns in the\ntable. To the best of our knowledge, none of the existing work that\napplies pre-trained LMs to tables has conducted this type of anal-\nysis for better understanding how the Transformer-based model\ncaptures the semantics of tables.\nThus, we conduct attention analysis to further understand how\nthe attention mechanisms of Doduo (i.e., the pre-trained LM) cap-\ntures the inter-column dependency and the semantic similarity\nbetween them. Following the literature of attention analysis in\nNLP [11, 43, 51], we look into attention weights for the analysis. It\nis known that in pre-trained Transformer-based LMs, the deep layer\nfocuses on semantic similarity between tokens [11, 51]. Therefore,\nto investigate the high-level (semantic) similarity between columns,\nwe looked into the attention weights of the last Transformer block.\nWe used the VizNet dataset (Multi-column only) for the analysis.\nSpecifically, we focus on attention weights between [CLS] tokens\n(i.e., column representations.) Since Transformer-based LMs usually\nhave multiple attention heads (e.g., 12 heads in the BERT Base\nmodel,) we aggregate attention weights of all attention heads. As a\nresult, we obtain anğ‘†Ã—ğ‘†matrix, whereğ‘†denotes the input sequence\nlength. We disregard aggregated attention weights other than those\nfor [CLS] tokens. After masking out any attention weights other\nthan [CLS] tokens, we averaged the matrices obtained from all\ntables in the dataset so that we can create a single |Ctype|Ã—|C type|\nmatrix that represents the dependency between column types. This\ngives us aggregated information about the dependency between\ncolumn types.\nEach element (ğ‘–, ğ‘—) in the final matrix represents how much the\ncolumn type ğ‘– relies on the other column type ğ‘— for its contextual-\nized representation. Note that the dependency of column type ğ‘–(or\nğ‘—) for column type ğ‘— (or ğ‘–) can be different, and thus the matrix is\nnot symmetric. For example, age highly relies on the type origin,\nwhereas the opposite direction has negative attention weight show-\ning a low degree of dependency. To eliminate the influence of the\nco-occurrence of column types, we counted the co-occurrence of\ncolumn types in the same table and normalized the matrix to make\nthe reference point to be zero for more straightforward interpre-\ntation. As a result, the final matrix consists of relative importance\nscores, and higher/lower values mean more/less influence from the\ncolumn type.\nFigure 6 depicts the final matrix in heatmap visualization. Higher\nvalues (colored in red) indicate stronger dependency of column\ntypes (in ğ‘¦-axis) against other column types (in ğ‘¥-axis). For exam-\nple, gender(age) has a higher value against country(origin.) We\ncan interpret that majority of information, which composes the\ncontextualized column representations for gendercolumns, is de-\nrived from origin. On the other hand, the gender column seems\nnot to be important for the origin column. The results confirm\nthat Doduo learns the inter-column dependency through the self-\nattention mechanism and the learned semantic similarity values\nbetween different pairs of column types have different weights,\nwhich the co-occurrence cannot simply explain.\nA.5 Language Model Probing\nRecent applications of pre-trained LMs to data management tasks in-\ncluding entity matching [3, 26] and column annotation tasks [13, 57]\nhave shown great success by improving previous SoTA performance\nby a large margin. However, little is known about how well pre-\ntrained LMs inherently know about the problem in the first place.\nPre-trained LMs are trained on large-scale textual corpora, and the\npre-training process helps the pre-trained LM to memorize and\ngeneralize the knowledge stored in the pre-training corpus.\nThere is a line of recent work that aims to investigate how well\npre-trained LMs know about factual knowledge [23, 40, 42]. The\nstudies have shown that pre-trained LMs store a significant amount\nof factual knowledge through pre-training on large-scale corpora.\nTherefore, we hypothesized that Doduoâ€™s performance was partly\nboosted by the knowledge obtained from the pre-training corpus\nthat might store knowledge relevant to the task. To verify the\nhypothesis, we evaluated if the BERT model, which we used as the\nbase model for the experiments, stored relevant knowledge for the\ncolumn annotation problem.\nAnnotating Columns with Pre-trained Language Models SIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA\nisbn\nyear\nage\nstate\ngrades\nweight\nstatus\nindustry\nclub\ngender\nresult\nreligion\nlanguage\nbirthDate\nfamily\nteam\ncode\ncity\ncategory\ndescription\nduration\ntype\nrank\nsex\nname\naddress\naffiliation\nsymbol\nteamName\nformat\nservice\neducation\nlocation\nelevation\ncounty\nposition\ncompany\ncollection\nalbum\nday\ncountry\nclass\npublisher\ncurrency\norigin\nplays\ndepth\njockey\nfileSize\norder\norganisation\nartist\nbirthPlace\ncontinent\ngenre\nnationality\ncredit\nclassification\nowner\nnotes\narea\ncreator\nregion\nsales\noperator\nproduct\ncomponent\nrequirement\nspecies\nmanufacturer\ncapacity\nrange\nbrand\naffiliate\ncommand\ndirector\nranking\nperson\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Doduo\nSato\ngrades\nage\nstate\nisbn\nyear\nweight\nstatus\nclub\ncity\ncode\nresult\nsymbol\ndescription\ngender\nrank\nbirthDate\nteamName\nname\ncollection\naddress\nsex\ncompany\nposition\nteam\ncategory\nlocation\nplays\nformat\naffiliation\ncounty\njockey\nelevation\nday\ntype\npublisher\nalbum\nlanguage\nduration\nclass\nartist\ncountry\nfamily\nrequirement\ncredit\nfileSize\ncontinent\naffiliate\nsales\nbirthPlace\norder\ngenre\nnotes\norigin\nrange\nowner\nregion\nnationality\ncreator\nmanufacturer\noperator\ncomponent\nproduct\ndepth\nranking\nspecies\ndirector\ncurrency\nbrand\ncapacity\narea\ncommand\nindustry\nperson\nclassification\nservice\nreligion\norganisation\neducation\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Doduo\nSato\nFigure 5: Class F1 values by Doduo and Sato on the VizNet dataset (Above: Full set; Below: Multi-column only.)\nTable 10: Further analysis on column type prediction (left) and on column relation prediction (right.)\nColumn type Doduo (F1) Dosolo (F1)\nmusic.artist 84.03 81.87\nmusic.genre 93.33 87.50\nmusic.writer 75.00 40.00\namerican_football.football_coach 70.59 66.67\namerican_football.football_conference 44.44 36.36\namerican_football.football_team 86.67 86.36\nColumn relation Doduo (F1) Dosolo (F1)\nfilm.film.production_companies 80.95 74.29\nfilm.film.produced_by 43.90 38.89\nfilm.film.story_by 100.00 90.91\npeople.person.place_of_birth 92.00 90.79\npeople.person.place_lived 85.98 77.67\npeople.person.nationality 100.00 98.80\nelevation\ncapacity\nproduct\nage\nduration\ngender\nname\nfamily\nlanguage\nstate\ncompany\ndescription\nteam\naddress\nclass\narea\ntype\nbirthPlace\nregion\ncounty\nowner\nnationality\ncity\ncountry\nmanufacturer\nlocation\nbirthDate\norigin\nelevation\ncapacity\nproduct\nage\nduration\ngender\nname\nfamily\nlanguage\nstate\ncompany\ndescription\nteam\naddress\nclass\narea\ntype\nbirthPlace\nregion\ncounty\nowner\nnationality\ncity\ncountry\nmanufacturer\nlocation\nbirthDate\norigin\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\nFigure 6: Inter-column dependency based on attention anal-\nysis on the VizNet dataset. A higher value (red) indicates that\nthe column type ( ğ‘¦-axis) â€œrelies onâ€ the other column type\n(ğ‘¥-axis) for prediction. Each row denotes the degree of â€œde-\npendenceâ€ against each column. For example, age in ğ‘¦-axis\nhas a high attention weight against origin in ğ‘¥-axis, indi-\ncating that predicting age relies on signals from the origin\ncolumn.\nTable 11: Comparisons with different input token size on the\nVizNet (Full) dataset.\nMethod MaxToken/col Macro F1 Micro F1\nDoduo 8 81.0 92.5\n16 83.6 93.6\n32 83.4 94.2\nDosoloSCol 8 72.7 87.2\n16 76.1 89.1\n32 77.4 90.2\nIn the analysis, following the line of work [23, 40, 42], we use\nthe template-based approach to test if a pre-trained LM knows the\nfactual knowledge. Specifically, we use a template that has a blank\nfield for the column type like below:\nJudy Morris is _____.\nSIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp, Chen Chen, and Wang-Chiew Tan\nIn this example, â€œdirectorâ€ should be a better fit than other column\ntypes (e.g., â€œactorâ€, â€œplayerâ€, etc.)\nIn this way, we conclude that the model knows the fact if the\nmodel judges the template with the true column type (e.g., â€œdirec-\ntorâ€) more likely than other sentences that use different column\ntypes (e.g., â€œactorâ€, â€œplayerâ€, etc.). To evaluate the likelihood of a se-\nquence after filling the blank in the template, we use the perplexity\nscore of a sequence using the pre-trained LM. Perplexity is used to\nmeasure how well the LM can predict each token from the context\n(i.e., the other tokens in the sequence 6) and is calculated by the\naverage likelihood of a sequence. It is a common metric to evaluate\nhow â€œnaturalâ€ the text is from the LM perspective. The perplexity\nof a sequence of tokens ğ‘‹ = (ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘¡)is defined by:\nğ‘ƒğ‘ƒğ¿(ğ‘‹)= exp\n(\nâˆ’1\nğ‘¡\nğ‘¡âˆ‘ï¸\nğ‘–\nlog ğ‘ğœƒ(ğ‘¥ğ‘–|ğ‘¥\\ğ‘–)\n)\n, (3)\nwhere ğ‘ğœƒ(ğ‘¥ğ‘–|ğ‘¥\\ğ‘–)denotes the probability of an LM ğœƒ (e.g., BERT)\npredicting a target token ğ‘¥ğ‘– given the context in ğ‘‹ with ğ‘¥ğ‘– masked\nout. With the same LM, the lower perplexity score for a sentence\nindicates it is easier for the LM to generate the sentence.\nWe use the perplexity to score column types for each column\nvalue (e.g., Judy Morris) by filling each column type name in the tem-\nplate. Then, we can evaluate if the ground truth label (i.e., â€œdirectorâ€\nin this case) has the best (i.e., lowest) perplexity among all candi-\ndates. For the analysis, we use the vanilla BERT (bert-base-uncased)\nmodel, which is the same base model used for Doduo in the exper-\niments. We use the average rank and the normalized PPL (= PPL\n/ Avg. PPL, where Avg. PPL denotes the average perplexity of all\ncolumn types for evaluation.) Since perplexity values for sequences\nwith different lengths are not directly comparable, we selected\ncolumn types that are tokenized into a single token by the BERT\ntokenizer7. As a result, 80 (out of 255) and 75 (out of 78) column\ntypes were selected for the WikiTable and VizNet datasets for the\nanalysis, respectively.\nWe can use the same framework for the column relation predic-\ntion task as well. In this case, we consider a different template that\nhas a blank field for the column relation. For example,\nDerrick Henry _____ Yulee, Florida.\nIn this example, the likelihood of a sentence with â€œwas born inâ€\n(place_of_birth) should be judged higher than that with â€œwas\ndied inâ€ (place_of_death), which requires factual knowledge about\nthe person. Since the column relation types are not written in plain\nlanguage, we manually converted column relation type names so\nthat they better fit in the template (entity 1, relation, entity 2).\nExamples of converted column relation names are (place_of_birth,\nâ€œwas born inâ€), (directed_by, â€œis directed byâ€). We filtered 34 (out\nof 121) column relation types to make sure the converted relation\ntype names have the exact same number of tokens.\nAs a more direct way to test the hypothesis, we also evaluated\na variant of Doduo that randomly initialized model parameters\n6It would be the next token from the previous tokens if the model were an autoregres-\nsive model that only considers backward dependency in each step (e.g., GPT-2.) Since\nBERT has bi-directional connections between any tokens in the input sequence, the\nperplexity should take into account any other tokens in the input sequence to evaluate\nthe likelihood of the target token.\n7Technically, column type names in the WikiTable contain hierarchical information,\nwhich is represented by URI. We used the leaf node as the column type name.\ninstead of using the pre-trained parameters of BERT. In this way,\nwe can test the performance when the model with the identical\narchitecture is trained from scratch only using training data of the\ntarget task. The model did not show meaningful performance (i.e.,\napproximately zero F1 value.) We consider this is mainly because\nthe model is too large to be trained on only the training data (i.e.,\nwithout pre-training.) Thus, we decided to use the â€œlanguage model\nprobingâ€ method to test the hypothesis.\nResults. Table 12 shows the results on the WikiTable dataset. We\nobserve that some column types (e.g., goverment.election, geog-\nraphy.river) show lower average rank and PPL / Avg. PPL (i.e.,\nthe BERT model knows about the facts), whereas some column\ntypes (e.g., biology.organism, royalty.kingdom) show poor perfor-\nmance on the language probing analysis. For example, the â€œgov-\nernment.electionâ€ column type is ranked at 6.74 on average and\nshows a smaller PPL than the average PPL. That means values in the\ncolumns that have the â€œgovernment.electionâ€ ground-truth labels\nare considered â€œmore naturalâ€ to appear with the term â€œelectionâ€8\nthan other column type names by the pre-trained LM (i.e., BERT.)\nAs we used 80 column types for the analysis, the â€œroyalty.kingdomâ€\ncolumn type is almost always ranked at the bottom by the LM. The\npoor performance could be attributed to the lower frequency of the\nterm â€œkingdomâ€ than other terms in the pre-training corpus.\nFor the column relations on the WikiTable dataset, the results in\nTable 12 (Right) indicate that the LM knows about factual knowl-\nedge of persons as the probing performance for relations such as\nplace_of_birth and position is higher. Compared to the prob-\ning results for the column types, the results show less significant\ndifferences between top-5 and bottom column relations. This is\nmainly because the template has three blank fields for two entities\nand one relation, which has a higher chance to create an unnatural\nsentence for the LM than that for column types.\nThe probing analysis on the VizNet dataset shows the same\ntrend as in the WikiTable dataset. In Figure 5, we confirm that\nDoduo has better performance than Sato for all the top-5 column\ntypes. Meanwhile, birthPlaceand nationality, which are in the\nbottom-5 column types for the language model probing analysis, are\namong the few column types where Doduo underperforms Sato.\nThe results support that Doduo may not benefit from relevant\nfactual knowledge stored in the pre-trained LM for the column\ntype.\nNote that the BERT model used for the analysis is not fine-tuned\non the WikiTable/VizNet dataset, but the vanilla BERT model. Thus,\nthe language model probing analysis shows the inherent ability\nof the BERT model, and we confirm that the pre-trained LM does\nstore factual knowledge that is useful for the column annotation\nproblem. This especially explains the significant improvements over\nthe previous SoTA method that does not use pre-trained LM (i.e.,\nSato), as shown in Figure 5.\nB LIMITATIONS AND FUTURE WORK\nWe have discussed why Doduo performs well for the column an-\nnotation problem through the series of analysis. In this section, we\nsummarize the limitations of Doduo and our findings in the paper\nto discuss open questions for future work.\n8Again, we used the leaf node of each column type as the term for the template.\nAnnotating Columns with Pre-trained Language Models SIGMOD â€™22, June 12â€“17, 2022, Philadelphia, PA, USA\nTable 12: Language model probing results on the WikiTable dataset (Left: column type prediction. Right: Column relation\nprediction.) The average rank becomes 1 if the language model always judges the column type (column relation) as the most\nâ€œnaturalâ€ choice among 80 (34) candidates for the target column value (the target column value pair.) We consider the language\nmodel has more prior knowledgeabout the column types (column relations) in Top-5 than those in Bottom-5.\nColumn type Avg. rank (â†“) PPL / Avg.PPL ( â†“)\nTop-5\ngovernment.election 6.74 0.787\ngeography.river 9.25 0.788\nreligion.religion 10.10 0.799\nbook.author 12.72 0.810\neducation.university 15.62 0.829\nBottom-5\nroyalty.monarch 58.24 1.147\nastronomy.constellation 67.47 1.170\nlaw.invention 61.60 1.181\nbiology.organism 71.56 1.205\nroyalty.kingdom 73.37 1.368\nColumn relation Avg. rank (â†“) PPL / Avg.PPL ( â†“)\nTop-5\nperson.place_of_birth 3.69 0.946\nbaseball_player.position_s 5.04 0.961\nlocation.nearby_airports 8.66 0.979\nmailing_address.citytown 7.24 0.980\nfilm.directed_by 8.08 0.984\nBottom-5\naward.award_nominee 16.53 1.019\ntv_program.country_of_origin 16.83 1.030\ncountry.languages_spoken 14.79 1.042\naward_honor.award_winner 21.40 1.047\nevent.entity_involved 19.82 1.072\nTable 13: Language model probing results on the VizNet\ndataset. We observe that the language model stores a certain\namount of factual knowledge about column types listed in\nTop-5, compared to Bottom-5. The general trend is consis-\ntent with Table 12.\nColumn type Avg. rank (â†“) PPL / Avg.PPL ( â†“)\nTop-5\nyear 6.60 0.799\nmanufacturer 20.19 0.810\nday 14.21 0.819\nstate 16.88 0.825\nlanguage 17.23 0.840\nBottom-5\norganisation 61.83 1.146\nnationality 65.81 1.218\ncreator 57.39 1.232\naffiliation 63.85 1.239\nbirthPlace 72.30 1.334\nTable values only vs. with meta information. First, Doduo\ntakes table value only. In most cases, we believe this assumption\nmakes the framework more flexible to be practical. For example,\nspreadsheets and data frames, which are common data format\nchoices for data analysis, do not have table captions and often\nlack meaningful table headers. Nevertheless, we acknowledge that,\nin some cases, meta information plays an important role to com-\nplementing table values to compose the table semantics. As recent\nwork [13, 57] has shown the effectiveness of meta information for\nthe table tasks, understanding when meta information becomes\nessential for the task is still an open question.\nSingle-table model vs. multi-table model. Second, Doduo as-\nsumes the input table to be self-contained. That means columns\nthat are necessary to compose table context should be stored in\nthe same table. Web Tables generally follow the assumption, and\nDoduo shows the strong performance on the WikiTable and VizNet\ndatasets. However, Doduo was not tested on relational tables,\nwhere chunks of information can be split into multiple tables after\ndatabase normalization. In such a scenario, we need to consider\ninter-table relations and information to incorporate key signals\noutside the target table. In fact, contemporaneous work [57] has\ndeveloped a framework that incorporates signals from external ta-\nbles for better column annotation performance. Thus, we consider\njoint modeling of multiple tables should be a future direction.\nClean data vs. dirty data. Third, our framework assumes that\ntable values are â€œcorrect and cleanâ€, which may not always be\ntrue in real-world settings. The input table value should be of the\nhigh-quality, especially when we limit the max input token size\nfor better efficiency. Recent studies that applied pre-trained LMs\nto tables [26, 27] have shown that the pre-trained LM-based ap-\nproach achieves robust improvements even on â€œdirtyâ€ datasets,\nwhere some table values are missing or misplaced. Following the\nerror detection/correction research [30, 31], which has been stud-\nied independently, implementing functionality that alleviates the\ninfluence from the incorrect table values is part of the future work.\nMulti-task learning with more tasks. Lastly and most impor-\ntantly, there are many open questions in applying multi-task learn-\ning to data management tasks. Although we have shown that multi-\ntask learning is useful for the column annotation task, it is not yet\nvery clear what types of relevant tasks are useful for the target\ntask. A line of work in Machine Learning has studies on the task\nsimilarity and the transferability of the model [64]. Therefore, it is\nalso important for us to understand the relationship between task\nsimilarity and benefits of the multi-task learning framework. We\nacknowledge that our study is still preliminary with respect to this\npoint. However, we believe that our study established the first step\nin this research topic toward a wider scope of multi-task learning\napplicability to other data management tasks.",
  "concepts": [
    {
      "name": "Column (typography)",
      "score": 0.8681614398956299
    },
    {
      "name": "Computer science",
      "score": 0.814535915851593
    },
    {
      "name": "Table (database)",
      "score": 0.7434487342834473
    },
    {
      "name": "Task (project management)",
      "score": 0.6012346148490906
    },
    {
      "name": "Relation (database)",
      "score": 0.5001485347747803
    },
    {
      "name": "Toolbox",
      "score": 0.4546816647052765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3949691653251648
    },
    {
      "name": "Natural language processing",
      "score": 0.3833080530166626
    },
    {
      "name": "Data mining",
      "score": 0.3461254835128784
    },
    {
      "name": "Machine learning",
      "score": 0.34456750750541687
    },
    {
      "name": "Programming language",
      "score": 0.3037908971309662
    },
    {
      "name": "Frame (networking)",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "topic": "Column (typography)",
  "institutions": [
    {
      "id": "https://openalex.org/I3197470489",
      "name": "Alpha Omega Alpha Medical Honor Society",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210099336",
      "name": "Menlo School",
      "country": "US"
    }
  ],
  "cited_by": 63
}