{
  "title": "Language-Model-Based Paired Variational Autoencoders for Robotic Language Learning",
  "url": "https://openalex.org/W4294811326",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227274582",
      "name": "Özdemir, Ozan",
      "affiliations": [
        "Universität Hamburg",
        "Hamburg University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3187784403",
      "name": "Kerzel, Matthias",
      "affiliations": [
        "Universität Hamburg",
        "Hamburg University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2322862804",
      "name": "Weber, Cornelius",
      "affiliations": [
        "Hamburg University of Technology",
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A2909040281",
      "name": "Lee Jae Hee",
      "affiliations": [
        "Hamburg University of Technology",
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A3033460025",
      "name": "Wermter, Stefan",
      "affiliations": [
        "Hamburg University of Technology",
        "Universität Hamburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891503716",
    "https://openalex.org/W6789006708",
    "https://openalex.org/W2963086650",
    "https://openalex.org/W3000086239",
    "https://openalex.org/W3100307207",
    "https://openalex.org/W2885804027",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3202006151",
    "https://openalex.org/W6782889966",
    "https://openalex.org/W2962716343",
    "https://openalex.org/W2604838875",
    "https://openalex.org/W3037680014",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3035695796",
    "https://openalex.org/W2771461682",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6640963894",
    "https://openalex.org/W3176484337",
    "https://openalex.org/W3211640812",
    "https://openalex.org/W2774441505",
    "https://openalex.org/W2142344747",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W3038245394",
    "https://openalex.org/W2998012869",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2888471892",
    "https://openalex.org/W6727690538",
    "https://openalex.org/W2862846329",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W3195535318",
    "https://openalex.org/W3100675524",
    "https://openalex.org/W3102561192"
  ],
  "abstract": "Human infants learn language while interacting with their environment in\\nwhich their caregivers may describe the objects and actions they perform.\\nSimilar to human infants, artificial agents can learn language while\\ninteracting with their environment. In this work, first, we present a neural\\nmodel that bidirectionally binds robot actions and their language descriptions\\nin a simple object manipulation scenario. Building on our previous Paired\\nVariational Autoencoders (PVAE) model, we demonstrate the superiority of the\\nvariational autoencoder over standard autoencoders by experimenting with cubes\\nof different colours, and by enabling the production of alternative\\nvocabularies. Additional experiments show that the model's channel-separated\\nvisual feature extraction module can cope with objects of different shapes.\\nNext, we introduce PVAE-BERT, which equips the model with a pretrained\\nlarge-scale language model, i.e., Bidirectional Encoder Representations from\\nTransformers (BERT), enabling the model to go beyond comprehending only the\\npredefined descriptions that the network has been trained on; the recognition\\nof action descriptions generalises to unconstrained natural language as the\\nmodel becomes capable of understanding unlimited variations of the same\\ndescriptions. Our experiments suggest that using a pretrained language model as\\nthe language encoder allows our approach to scale up for real-world scenarios\\nwith instructions from human users.\\n",
  "full_text": "Language Model-Based Paired Variational\nAutoencoders for Robotic Language Learning\nOzan ¨Ozdemir, Matthias Kerzel, Cornelius Weber, Jae Hee Lee, Stefan Wermter\nAbstract—Human infants learn language while interacting with\ntheir environment in which their caregivers may describe the\nobjects and actions they perform. Similar to human infants,\nartificial agents can learn language while interacting with their\nenvironment. In this work, first, we present a neural model\nthat bidirectionally binds robot actions and their language\ndescriptions in a simple object manipulation scenario. Building on\nour previous Paired Variational Autoencoders (PV AE) model, we\ndemonstrate the superiority of the variational autoencoder over\nstandard autoencoders by experimenting with cubes of different\ncolours, and by enabling the production of alternative vocabu-\nlaries. Additional experiments show that the model’s channel-\nseparated visual feature extraction module can cope with objects\nof different shapes. Next, we introduce PV AE-BERT, which\nequips the model with a pretrained large-scale language model,\ni.e., Bidirectional Encoder Representations from Transformers\n(BERT), enabling the model to go beyond comprehending only\nthe predefined descriptions that the network has been trained on;\nthe recognition of action descriptions generalises to unconstrained\nnatural language as the model becomes capable of understanding\nunlimited variations of the same descriptions. Our experiments\nsuggest that using a pretrained language model as the language\nencoder allows our approach to scale up for real-world scenarios\nwith instructions from human users.\nIndex Terms—language grounding, variational autoencoders,\nchannel separation, pretrained language model, object manipu-\nlation\nI. I NTRODUCTION\nHumans use language as a means to understand and to be\nunderstood by their interlocutors. Although we can commu-\nnicate effortlessly in our native language, language is a so-\nphisticated form of interaction which requires comprehension\nand production skills. Understanding language depends also\non the context, because words can have multiple meanings\nand a situation can be explained in many ways. As it is not\nalways possible to describe a situation only in language or\nunderstand it only with the medium of language, we benefit\nfrom other modalities such as vision and proprioception. Sim-\nilarly, artificial agents can utilise the concept of embodiment\n(i.e. acting in the environment) in addition to perception\n(i.e. using multimodal input like audio and vision) for better\ncomprehension and production of language [5]. Human infants\nlearn language in their environment while their caregivers\ndescribe the properties of objects, which they interact with,\nand actions, which are performed on those objects. In a\nsimilar vein, artificial agents can be taught language; different\nO. ¨Ozdemir, M. Kerzel, C. Weber, J. H. Lee and S. Wermter are with\nKnowledge Technology Group, Department of Informatics, University of\nHamburg, Hamburg, Germany (emails: ozan.oezdemir@*, matthias.kerzel@*,\ncornelius.weber@*, jae.hee.lee@*, stefan.wermter@*; * = uni-hamburg.de)\nFig. 1. Our tabletop object manipulation scenario in the simulation environ-\nment: the NICO robot is interacting with toy objects. In the left panel, NICO\nviews all the toy objects; on the right, NICO pulls the red house. In both\npanels, NICO’s field of view is given in the top right inset.\nmodalities such as audio, touch, proprioception and vision can\nbe employed towards learning language in the environment.\nThe field of artificial intelligence has recently seen many\nstudies attempting to learn language in an embodied fashion\n[2], [6], [12], [19], [21]. In this paper, we bidirectionally\nmap language with robot actions by employing three distinct\nmodalities, namely text, proprioception and vision. In our\nrobotic scenario, two objects 1 are placed on a table as the\nNICO (the Neuro-Inspired COmpanion) robot [16] physically\ninteracts with them - see Figure 1. NICO moves objects along\nthe table surface according to given textual descriptions and\nrecognises the actions by translating them to corresponding de-\nscriptions. The possibility of bidirectional translation between\nlanguage and control was realised with a paired recurrent\nautoencoder (PRAE) architecture by Yamada et al. [30], which\naligns the two modalities that are each processed by an au-\ntoencoder. We extended this approach (PRAE) with the Paired\nVariational Autoencoders (PV AE) [32] model, which enriches\nthe language used to describe the actions taken by the robot:\ninstead of mapping a distinct description to each action [30],\nthe PV AE maps multiple descriptions, which are equivalent\nin meaning, to each action. Hence, we have transcended the\nstrict one-to-one mapping between control and language since\nour variational autoencoder-based model can associate each\nrobot action with multiple description alternatives. The PV AE\nis composed of two variational autoencoders (V AEs), one\n1Note that, in the left panel of Fig. 1, we show all the toy objects for\nvisualisation purposes. In all our experiments, there are always only two\nobjects on the table.\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nfor language, the other for action, and both of them consist\nof an LSTM (long short-term memory) [14] encoder and\ndecoder which are suitable for sequential data. The dataset 2,\nwhich our model is trained with, consists of paired textual\ndescriptions and corresponding joint angle values with ego-\ncentric images. The language V AE reconstructs descriptions,\nwhereas the action V AE reconstructs joint angle values that\nare conditioned on the visual features extracted in advance by\nthe channel-separated convolutional autoencoder (CAE) [32]\nfrom egocentric images. The two autoencoders are implicitly\nbound together with an extra loss term which aligns actions\nwith their corresponding descriptions and separates unrelated\nactions and descriptions in the hidden vector space.\nHowever, even with multiple descriptions mapped to a robot\naction as implemented in our previous work [32], replacing\neach word by its alternative does not lift the grammar re-\nstrictions on the language input. In order to process uncon-\nstrained language input, we equip the PV AE architecture with\nthe Bidirectional Encoder Representations from Transformers\n(BERT) language model [8] that has been pretrained on large-\nscale text corpora to enable the recognition of unconstrained\nnatural language commands by human users. To this end, we\nreplace the LSTM language encoder with a pretrained BERT\nmodel so that the PV AE can recognise different commands that\ncorrespond to the same actions as the predefined descriptions\ngiven the same object combinations on the table. This new\nmodel variant, which we call PV AE-BERT, can handle not\nonly the descriptions it is trained with, but also various\ndescriptions equivalent in meaning with different word order\nand/or filler words (e.g., ‘please’, ‘could’, ‘the’, etc.) as our\nanalysis shows. We make use of transfer learning by using\na pretrained language model, hence, benefitting from large\nunlabelled textual data.\nOur contributions can be summarised as follows:\n1) In our previous work [32], we showed that varia-\ntional autoencoders facilitate better one-to-many action-\nto-language translation and that channel separation in\nvisual feature extraction, i.e., training RGB channels\nseparately, results in more accurate recognition of object\ncolours in our object manipulation scenario. In this\nfollow-up work, we extend our dataset with different\nshapes and show that our PV AE with the channel separa-\ntion approach is able to translate from action to language\nwhile manipulating different objects.\n2) Here, we introduce PV AE-BERT, which, by using pre-\ntrained BERT, indicates the potential of our approach to\nbe scaled up for unconstrained instructions from human\nusers.\n3) Additional principle component (PCA) analysis shows\nthat language as well as action representation vectors\narrange according to the semantics of the language\ndescriptions.\nThe remainder of this paper is organised as follows: the\nnext section describes the relevant work, Section 3 presents\n2https://www.inf.uni-hamburg.de/en/inst/ab/wtm/research/corpora.html\nthe architecture of the PV AE and PV AE-BERT models, various\nexperiments and their results are given in Section 4, Section 5\ndiscusses the results and their implications and the last section\nconcludes the paper with final remarks 3.\nII. R ELATED WORK\nThe state-of-the-art approaches in embodied language learn-\ning mostly rely on tabletop environments [11], [13], [24],\n[25], [30] or interactive play environments [19] where a robot\ninteracts with various objects according to given instructions.\nWe categorise these approaches into three groups: those that\ntranslate from language to action, those that translate from\naction to language and those that can translate in both direc-\ntions, i.e., bidirectional approaches. Bidirectional approaches\nallow greater exploitation of available training data as training\nin both directions can be interpreted as multitask learning,\nwhich ultimately leads to more robust and powerful models\nindependent of the translation direction. By using the maxi-\nmum amount of shared weights for multiple tasks, such mod-\nels would be more efficient than independent unidirectional\nnetworks in terms of data utilisation and the model size.\nA. Language-to-Action Translation\nTranslating from language to action is the most common\nform in embodied language learning. Hatori et al. [11] in-\ntroduce a neural network architecture for moving objects\ngiven the visual input and language instructions, as their\nwork focuses on the interaction of a human operator with the\ncomputational neural system that picks and places miscella-\nneous items as per verbal commands. In their scenario, many\nitems with different shape and size (e.g. toys, bottles etc.) are\ndistributed across four bins with many of them being occluded\n- hence, the scene is very complex and cluttered. Given a\npick-and-place instruction from the human operator, the robot\nfirst confirms and then executes it if the instruction is clear.\nOtherwise, the robot asks the human operator to clarify the\ndesired object. The network receives a verbal command from\nthe operator and an RGB image from the environment, and it\nhas separate object recognition and language understanding\nmodules, which are trained jointly to learn the names and\nattributes of the objects.\nShridhar and Hsu [25] propose a comprehensive system for\na robotic arm to pick up objects based on visual and linguistic\ninput. The system consists of multiple modules such as ma-\nnipulation, perception and a neural network architecture, and\nis called INGRESS (Interactive Visual Grounding of Referring\nExpressions). INGRESS is composed of two network streams\n(self-referential and relational) which are trained on large\ndatasets to generate a definitive expression for each object in\nthe scene based on the input image. The generated expression\nis compared with the input expression to detect the desired ob-\nject. INGRESS is therefore responsible for grounding language\nby learning object names and attributes via manipulation. The\napproach can resolve ambiguities when it comes to which\nobject to lift by asking confirmation questions to the user.\n3Our code is available at https://github.com/oo222bs/PV AE-BERT.\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nShao et al. [24] put forward a robot learning framework,\nConcept2Robot, for learning manipulation concepts from hu-\nman video demonstrations in two stages. In the first stage,\nthey use reinforcement learning and, in the second, they utilise\nimitation learning. The architecture consists of three main\nparts: semantic context network, policy network and action\nclassification. The model receives as input a natural language\ndescription for each task alongside an RGB image of the initial\nscene. In return, it is expected to produce the parameters\nof a motion trajectory to accomplish the task in the given\nenvironment.\nLynch and Sermanet [19] introduce the LangLfP (language\nlearning from play) approach, in which they utilise multi-\ncontext imitation to train a single policy based on multiple\nmodalities. Specifically, the policy is trained on both image\nand language goals and this enables the approach to follow\nnatural language instructions during evaluation. During train-\ning, fewer than 1% of the tasks are labelled with natural\nlanguage instructions, because it suffices to train the policy\nfor more than 99% of the cases with goal images only. There-\nfore, only few of the tasks must be labelled with language\ninstructions. Furthermore, they utilise a Transformer-based\n[27] multilingual language encoder, Multilingual Universal\nSentence Encoder [31], to encode linguistic input so that the\nsystem can handle unseen language input like synonyms and\ninstructions in 16 different languages.\nThe language-to-action translation methods are designed\nto act upon a given language input as in textual or verbal\ncommands. They can recognise commands and execute the\ndesired actions. However, they cannot describe the actions that\nthey perform.\nB. Action-to-Language Translation\nAnother class of approaches in embodied language learning\ntranslates action into language. Heinrich et al. [13] intro-\nduce an embodied crossmodal neurocognitive architecture, the\nadaptive multiple timescale recurrent neural network (adaptive\nMTRNN), which enables the robot to acquire language by\nlistening to commands while interacting with objects in a\nplayground environment. The approach has auditory, senso-\nrimotor and visual perception capabilities. Since neurons at\nmultiple timescales facilitate the emergence of hierarchical\nrepresentations, the results indicate good generalisation and\nhierarchical concept decomposition within the network.\nEisermann et al. [9] study the problem of compositional\ngeneralisation, in which they conduct numerous experiments\non a tabletop scenario where a robotic arm manipulates various\nobjects. They utilise a simple LSTM-based network to describe\nthe actions performed on the objects in hindsight - the model\naccepts visual and proprioceptive input and produces textual\ndescriptions. Their results show that with the inclusion of\nproprioception as input and using more data in training,\nthe network’s performance on compositional generalisation\nimproves significantly.\nSimilar to the language-to-action translation methods, the\naction-to-language translation methods work only in one direc-\ntion: they describe the actions they perform in the environment.\nHowever, they are unable to execute a desired action given by\nthe human user. Nevertheless, from the robotics perspective, it\nis desirable to have models that can also translate from action\nto language and not just execute verbal commands; such robots\ncan explain their actions by verbalising an ongoing action,\nwhich also paves the way for more interpretable systems.\nC. Bidirectional Translation\nVery few embodied language learning approaches are ca-\npable of flexibly translating in both directions, hence, bidirec-\ntional. While unidirectional approaches are feasible for smaller\ndatasets, we aim to research architectures that can serve as\nlarge-scale multimodal foundation models and solve multiple\ntasks in different modalities. By generating a discrete set of\nwords, bidirectional models can also provide feedback to a\nuser about the information contained within its continuous\nvariables. By providing rich language descriptions, rather\nthan only performing actions, such models can contribute to\nexplainable AI (XAI) for non-experts. For a comprehensive\noverview of the field of XAI, readers can refer to the survey\npaper by Adadi and Berrada [1].\nIn one of the early examples of bidirectional translation,\nOgata et al. [22] present a model that is aimed at articulation\nand allocation of arm movements by using a parametric\nbias to bind motion and language. The method enables the\nrobot to move its arms according to given sentences and to\ngenerate sentences according to given arm motions. The model\nshows generalisation towards motions and sentences that it has\nnot been trained with. However, it fails to handle complex\nsentences.\nAntunes et al. [3] introduce the multiple timescale long\nshort-term memory (MT-LSTM) model in which the slowest\nlayer establishes a bidirectional connection between action and\nlanguage. The MT-LSTM consists of two components, namely\nlanguage and action streams, each of which is divided into\nthree layers with varying timescales. The two components are\nbound by a slower meaning layer that allows translation from\naction to language and vice versa. The approach shows limited\ngeneralisation capabilities.\nYamada et al. [30] propose the paired recurrent autoencoder\n(PRAE) architecture, which consists of two autoencoders,\nnamely action and description. The action autoencoder takes\nas input joint angle trajectories with visual features and is\nexpected to reconstruct the original joint angle trajectories.\nThe description autoencoder, on the other hand, reads and then\nreconstructs the action descriptions. The dataset that the model\nis trained on consists of pairs of simple robot actions and\ntheir textual descriptions, e.g., ‘pushing away the blue cube’.\nThe model is trained end-to-end, with both autoencoders,\nreconstructing language and action, whilst there is no explicit\nneural connection between the two. The crossmodal pairing\nbetween action and description autoencoders is supplied with\na loss term that aligns the hidden representations of paired\nactions and descriptions. The binding loss allows the PRAE\nto execute actions given instructions as well as translate\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nactions to descriptions. As a bidirectional approach, the PRAE\nis biologically plausible to some extent, since humans can\neasily execute given commands and also describe these actions\nlinguistically. To imitate human-like language recognition and\nproduction, bidirectionality is essential. However, due to its\nuse of standard autoencoders, the PRAE can only bind a\nrobot action with a particular description in a one-to-one way,\nalthough actions can be expressed in different ways. In order\nto map each robot action to multiple description alternatives,\nwe have proposed the PV AE (paired variational autoencoders)\napproach [32] which utilises variational autoencoders (V AEs)\nto randomise the latent representation space and thereby allows\none-to-many translation between action and language. A recent\nreview by Marino [20] highlights similarities between V AEs\nand predictive coding from neuroscience in terms of model\nformulations and inference approaches.\nThis work is an extension of the ICDL article “Embodied\nLanguage Learning with Paired Variational Autoencoders”\n[32]. Inspired by the TransferLangLfP paradigm by Lynch and\nSermanet [19], we propose to use the PV AE with a pretrained\nBERT language model [8] in order to enable the model to\ncomprehend unconstrained language instructions from human\nusers. Furthermore, we conduct experiments using PV AE-\nBERT on our dataset for various use cases and analyse the\ninternal representations for the first time.\nIII. P ROPOSED METHODS : PVAE & PVAE-BERT\nAs can be seen in Figure 2, the PV AE model consists of two\nvariational autoencoders: a language V AE and an action V AE.\nThe former learns to generate descriptions matching original\ndescriptions, whilst the latter learns to reconstruct joint angle\nvalues with conditioning on the visual input. The two autoen-\ncoders do not have any explicit neural connection between\nthem, but instead they are implicitly aligned by the binding\nloss, which brings the two autoencoders closer to each other\nin the latent space over the course of learning by reducing\nthe distance between the two latent variables. First, action\nand language encoder map the input to the latent code, i.e.,\nthe language encoder accepts one-hot encoded descriptions\nword by word as input and produces the encoded descriptions,\nwhereas the action encoder accepts corresponding arm trajec-\ntories and visual features as input and produces the encoded\nactions. Next, the encoded representations are used to extract\nlatent representations by randomly sampling from a Gaussian\ndistribution separately for language and action modalities.\nFinally, from the latent representations, language and action\ndecoders reconstruct the descriptions and joint angle values,\nrespectively.\nOur model is a bidirectional approach, i.e., after training\ntranslation is possible in both directions, action-to-language\nand language-to-action. The PV AE model transforms robot\nactions to descriptions in a one-to-many fashion by appropri-\nately randomising the latent space. PV AE-BERT additionally\nhandles variety in language input by using pretrained BERT as\nthe language encoder module. As part of the action encoder,\nthe visual input features are extracted in advance using a\nchannel-separated CAE (short for convolutional autoencoder),\nwhich improves the ability of the approach to distinguish the\ncolours of cubes. The details of each model component are\ngiven in the following subsections.\nA. Language Variational Autoencoder\nThe language V AE accepts as input one-hot encoded matrix\nof a description word by word in the case of the PV AE or the\ncomplete description altogether for PV AE-BERT, and for both\nthe PV AE and PV AE-BERT, is responsible for reproducing the\noriginal description. It consists of an encoder, a decoder and\nlatent layers (in the bottleneck) where latent representations\nare extracted via sampling. For the PV AE, the language\nencoder embeds a description of length N, (x1, x2, ..., xN ),\ninto two fixed-dimensional vectors zmean and zsigma as follows:\nhenc\nt , cenc\nt = LSTM(xt, henc\nt−1, cenc\nt−1) (1 ≤ t ≤ N),\nzmean = Wenc\nmean · hN + benc\nmean,\nzvar = Wenc\nvar · hN + benc\nvar ,\nzlang = zmean + zvar · N(µ, σ2),\nwhere ht and ct are the hidden and cell state of the LSTM at\ntime step t, respectively, and N is a Gaussian distribution.\nh0 and c0 are set as zero vectors, while µ and σ are 0\nand 0.1, respectively. zlang is the latent representation of a\ndescription. LSTM here, and in the following, is a peephole\nLSTM [23] following the implementation of Yamada et al.\n[30]. The language input is represented in one-hot encoded\nmatrices, whose rows represent the sequence of input words\nand columns represent every word that is in the vocabulary.\nIn each row, only one cell is 1 and the rest are 0, which\ndetermines the word that is given to the model at that time\nstep.\nFor PV AE-BERT, we replace the LSTM language encoder\nwith the pretrained BERT-base model and, following the\nimplementation by Devlin et al. [8], tokenise the descriptions\naccordingly with the subword-based tokeniser WordPiece [29].\nThe language decoder generates a sequence by recursively\nexpanding zlang:\nhdec\n0 , cdec\n0 = Wdec · zlang + bdec,\nhdec\nt , cdec\nt = LSTM(yt−1, hdec\nt−1, cdec\nt−1) (1 ≤ t ≤ N − 1),\nyt = soft(Wout · hdec\nt + bout) (1 ≤ t ≤ N − 1),\nwhere soft denotes the softmax activation function. y0 is the\nfirst symbol indicating the beginning of the sentence, hence\nthe <BOS> tag.\nB. Action Variational Autoencoder\nThe action V AE accepts a sequence of joint angle values and\nvisual features as input and it is responsible for reconstructing\nthe joint angle values. Similar to the language V AE, it is\ncomposed of an encoder, a decoder and latent layers (in\nthe bottleneck) where latent representations are extracted via\nsampling. The action encoder encodes a sequence of length M,\n((j1, v1), (j2, v2), ...,(jM , vM )), which includes concatenation\nof joint angles j and visual features v. Note that the visual\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLz_var\nLz_mean\nLz\n<BOS>\npull red fast <EOS>\npull red fast\nAz_mean\nAz_var\nAz\nj1\nv1\nj2 j3 j4 jM\nvM\n j1\n ĵ2 ĵ3 ĵ4\nBinding\nLoss\nLanguage \nVAE\nAction \nVAE\nLanguage \nEncoder\nAction \nEncoder\nLanguage \nDecoder\nAction \nDecoder\ny1 yN-1\nv1 vM-1\nBottleneck (Latent Representations)\nv1 magnified\nLSTM Sampling\nSampling\nv2 ĵ2 ĵ3 ĵM-1\nv2 v3 v4\ny2 y3\nĵM\nx1 y1 y2 y3\nExample\nImage\n'pull red\nfast' LSTM or BERT\nLSTM LSTM LSTM LSTM\nLSTM LSTM LSTM LSTM\nv3\nLSTM LSTM LSTM LSTM\nFig. 2. The architecture of the proposed PV AE and PV AE-BERT models: the language V AE (blue rectangles) processes descriptions, whilst the action V AE\n(orange rectangles) processes joint angles and images at each time step. The input to the language V AE is the given description x, whereas the action V AE\ntakes as input joint angle values j and visual features v. The two V AEs are implicitly bound via a binding loss in the latent representation space. The image\nfrom which the v 1 is extracted is magnified for visualisation purposes. <BOS> and <EOS> stand for beginning of sentence and end of sentence tags,\nrespectively. The two models differ only by the language encoder employed: the PV AE uses LSTM, whereas PV AE-BERT uses a pretrained BERT model.\nfeatures are extracted by the channel-separated convolutional\nautoencoder beforehand. The equations that define the action\nencoder are as follows 4:\nhenc\nt , cenc\nt = LSTM(vt, jt, henc\nt−1, cenc\nt−1) (1 ≤ t ≤ M),\nzmean = Wenc\nmean · hM + benc\nmean,\nzvar = Wenc\nvar · hM + benc\nvar ,\nzact = zmean + zvar · N(µ, σ2),\nwhere ht and ct are the hidden and cell state of the LSTM at\ntime step t, respectively, and N is a Gaussian distribution. h0,\nc0 are set as zero vectors, while µ and σ are set as 0 and 0.1,\nrespectively. zact is the latent representation of a robot action.\nThe action decoder reconstructs the joint angles:\nhdec\n0 , cdec\n0 = Wdec · zact + bdec,\nhdec\nt , cdec\nt = LSTM(vt, ˆȷt, hdec\nt−1, cdec\nt−1) (1 ≤ t ≤ M − 1),\nˆȷt+1 = tanh(Wout · hdec\nt + bout) (1 ≤ t ≤ M − 1),\nwhere tanh denotes the hyperbolic tangent activation function\nand ˆȷ1 is equal to j1, i.e. joint angle values at the initial time\nstep.\nC. Visual Feature Extraction\nWe utilise a convolutional autoencoder architecture, fol-\nlowing Yamada et al. [30], to extract the visual features of\nthe images. Different from the approach used in [30], we\nchange the number of input channels the model accepts from\nthree to one and train an instance of CAE for each colour\nchannel (red, green and blue) to recognise different colours\nmore accurately: channel separation. Therefore, we call our\nvisual feature extractor the channel-separated CAE. The idea\nbehind the channel-separated CAE is similar to depthwise\nseparable convolutions [7], where completely separating cross-\nchannel convolutions from spatial convolutions leads to better\n4For the sake of clarity, we use mostly the same symbols in the equations\nas in the equations of the language V AE.\nresults in image classification as the network parameters are\nused more efficiently. The channel-separated CAE accepts a\ncolour channel of 120 × 160 RGB images captured by the\ncameras in the eyes of NICO - referred also as the egocentric\nview of the robot - at a time. As can be seen in detail\nin Table I, it consists of a convolutional encoder, a fully-\nconnected bottleneck (incorporates hidden representations) and\na deconvolutional decoder. After training for each colour\nchannel, we extract the visual features of each image for\nevery channel from the middle layer in the bottleneck (FC\n3). The visual features extracted from each channel are then\nconcatenated to make up the ultimate visual features v.\nTABLE I\nDETAILED ARCHITECTURE OF CHANNEL -SEPARATED CAE\nBlock Layer Out Chan. Kernel Size Stride Padding Activation\nEncoder Conv 1 8 4x4 2 1 ReLU\nConv 2 16 4x4 2 1 ReLU\nConv 3 32 4x4 2 1 ReLU\nConv 4 64 8x8 5 2 ReLU\nBottleneck FC 1 384 - - - -\nFC 2 192 - - - -\nFC 3 10 - - - -\nFC 4 192 - - - -\nFC 5 384 - - - -\nDecoder Deconv 1 32 8x8 5 2 ReLU\nDeconv 2 16 4x4 2 1 ReLU\nDeconv 3 8 4x4 2 1 ReLU\nDeconv 4 1 4x4 2 1 Sigmoid\nChannel separation increases the use of computational re-\nsources compared to standard convolution approach, because\nit essentially uses three separate models: even though they\nare identical, they do not share weights. The number of\nmodel parameters is about three times that of the standard\napproach. Therefore, it requires roughly three times more\ncomputational power than the standard approach. Nonetheless,\nchannel separation excels at distinguishing the object colours.\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nD. Sampling and Binding\nStochastic Gradient Variational Bayes-based sampling\n(SGVB) [18] enables one-to-many mapping between action\nand language. The two V AEs have identical random sampling\nprocedures. After producing the latent variables zmean and zvar\nvia the fully connected layers, we utilise a normal distribution\nN(µ, σ2) to derive random values, ϵ, which are, in turn, used\nwith zmean and zvar to arrive at the latent representation z,\nwhich is also known as the reparameterisation trick [18]:\nz = zmean + zvar · ϵ\nwhere ϵ is the approximation of N(0, 0.01).\nAs in the case of [30], to align the latent representations of\nrobot actions and their descriptions, we use an extra loss term\nthat brings the mean hidden features, zmean, of the two V AEs\ncloser to each other. This enables bidirectional translation\nbetween action and language, i.e., the network can transform\nactions to descriptions as well as descriptions to actions, after\ntraining without an explicit fusion of the two modalities. This\nloss term (binding loss) can be calculated as follows:\nLbinding =\nBX\ni\nψ(zlang\nmeani , zact\nmeani ) +\nBX\ni\nX\nj̸=i\nmax\nn\n0, ∆ + ψ(zlang\nmeani , zact\nmeani ) − ψ(zlang\nmeanj , zact\nmeani )\no\n,\nwhere B stands for the batch size and ψ is the Euclidean\ndistance. The first term in the equation binds the paired\ninstructions and actions, whereas the second term separates\nunpaired actions and descriptions. Hyperparameter ∆ is used\nto adjust the separation margin for the second term - the higher\nit is, the further apart the unpaired actions and descriptions are\npushed in the latent space.\nDifferent multi-modal fusion techniques like Gated Multi-\nmodal Unit (GMU) [4], which uses gating and multiplicative\nmechanisms to fuse different modalities, and CentralNet [28],\nwhich fuses information by having a separate network for each\nmodality as well as central joint representations at each layer,\nwere also considered during our work. However, since our\nmodel is bidirectional (must work on both action-to-language\nand language-to-action directions) and must work with either\nlanguage or action input during inference (both GMU and\nCentralNet require all of the modalities to be available), we\nopted for the binding loss for multi-modal integration.\nE. Loss Function\nThe overall loss is calculated as the sum of the reconstruc-\ntion, regularisation and binding losses. The binding loss is\ncalculated for both V AEs jointly. In contrast, the reconstruction\nand regularisation losses are calculated independently for\neach V AE. Following [30], the reconstruction losses for the\nlanguage V AE (cross entropy between input and output words)\nand action V AE (Euclidean distance between original and\ngenerated joint values) are Llang and Lact, respectively:\nLlang = 1\nN − 1\nN−1X\nt=1\n \n−\nV −1X\ni=0\nx[i]\nt+1 log y[i]\nt\n!\n,\nLact = 1\nM − 1\nM−1X\nt=1\n∥jt+1 − ˆȷt+1∥2\n2 ,\nwhere V is the vocabulary size, N is the number of words per\ndescription, M is the sequence length for an action trajectory.\nThe regularisation loss is specific to variational autoencoders;\nit is defined as the Kullback–Leibler divergence for language\nDKLlang and action DKLact . Therefore, the overall loss function\nis as follows:\nLall = αLlang + βLact + γLbinding + αDKLlang + βDKLact\nwhere α, β and γ are weighting factors for different terms\nin the loss function. In our experiments, α and β are set to\n1, whilst γ is set to 2 in order to sufficiently bind the two\nmodalities.\nF . Transformer-Based Language Encoder\nIn order for the model to understand unconstrained language\ninput from non-expert human users, we replace the LSTM for\nthe language encoder with a pretrained BERT-base language\nmodel [8] - see Figure 2. According to [8], BERT is pretrained\nwith the BooksCorpus, which involves 800 million words, and\nEnglish Wikipedia, which involves 2.5 billion words. With the\nintroduction of BERT as the language encoder, we assume\nthat BERT can interpret action descriptions correctly in our\nscenario. However, since language models like BERT are\npretrained exclusively on textual data from the internet, they\nare not specialised for object manipulation environments like\nours. Therefore, the embedding of an instruction like ‘ push\nthe blue object’ may not differ from the embedding\nof another such as ‘ push the red object’ significantly.\nFor this reason, we finetune the pretrained BERT-base, i.e.\nall of BERT’s parameters are updated, during the end-to-\nend training of PV AE-BERT so that it can separate similar\ninstructions from each other, which is critical for our scenario.\nG. Training Details\nTo train the PV AE and PV AE-BERT, we first extract visual\nfeatures using our channel-separated CAE. The visual features\nare used to condition the actions depending on the cube\narrangement, i.e., the execution of a description depends also\non the position of the target cube. For both the PV AE and\nPV AE-BERT, the action encoder and action decoder are each\na two-layer LSTM with a hidden size of 100, whilst the\nlanguage decoder is a single-layer LSTM with the same hidden\nsize. In contrast, the language encoder of PV AE-BERT is the\npretrained BERT-base model with 12 layers, each with 12\nself-attention heads and a hidden size of 768, whereas the\nlanguage encoder of the PV AE is a one-layer LSTM with\na hidden size of 100. Both the PV AE and PV AE-BERT are\ntrained end-to-end with both the language and action V AEs\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\ntogether. The PV AE and PV AE-BERT are trained for 20,000\nand 40,000 iterations, respectively, with the gradient descent\nalgorithm and Adam optimiser [17]. We take the learning rate\nas 10−4 with a batch size of 100 pairs of language and action\nsequences after a few trials with different learning rates and\nbatch sizes. Due to having approximately 110M parameters,\ncompared with the PV AE’s approximately 465K parameters,\nan iteration of PV AE-BERT training takes about 1.4 times\nlonger than an iteration of PV AE training. Therefore, it takes\nabout 2.8 times longer to train PV AE-BERT in total.\nIV. E VALUATION AND RESULTS\nWe evaluate the performance of our PV AE and its variant\nusing BERT, namely PV AE-BERT, with multiple experiments.\nFirst, we compare the original PV AE with PRAE [30] in terms\nof action-to-language translation by conducting experiments\nof varying object colour options to display the superiority\nof variational autoencoders over regular autoencoders and\nthe advantage of using the channel separation technique in\nvisual feature extraction. Different object colour possibilities\ncorrespond to a different corpus and overall dataset size;\nthe more object colour options there are, the larger both\nthe vocabulary and the overall dataset become. Therefore,\nwith these experiments, we also test the scalability of both\napproaches. In order to show the impact of channel separation\non the action-to-language translation performance, we train our\narchitecture with visual features provided by a regular CAE\n(no channel separation) as implemented in [30]. These are\nExperiment 1a (with 3 cube colour alternatives: red, green,\nblue) and Experiment 1b (with 6 cube colour alternatives:\nred, green, blue, yellow, cyan, violet) - see Table III.\nMoreover, in Experiment 2 , we train PV AE-BERT on\nthe dataset with 6 colour alternatives (red, green, blue, yel-\nlow, cyan, violet) to compare it with the standard PV AE\nby conducting action-to-language, language-to-language and\nlanguage-to-action evaluation experiments. This experiment\nuses the pretrained BERT as the language encoder which is\nthen finetuned with the rest of the model during training.\nIn Experiments 1a, 1b and 2, two cubes of different colours\nare placed on a table at which the robot is seated to interact\nwith them. The words (vocabulary) that constitute the de-\nscriptions are given in Table II. We introduce a more diverse\nTABLE II\nVOCABULARY\nOriginal Alternative\nVerb push move-up\npull move-down\nslide move-sideways\nColour red scarlet\ngreen harlequin\nblue azure\nyellow blonde\ncyan greenish-blue\nviolet purple\nSpeed slowly unhurriedly\nfast quickly\nvocabulary by adding an alternative word for each word in the\noriginal vocabulary. As descriptions are composed of 3 words\nwith two alternatives per word, we arrive at 8 variations for\neach description of a given meaning. Table II does not include\nnouns, because we use a predefined grammar, which doesn’t\ninvolve a noun, and the same size cubes for these experiments.\nFor each cube arrangement, the colours of the two cubes\nalways differ to avoid ambiguities in the language description.\nActions, which are transcribed in capitals, are composed of any\nof the three action types PUSH, PULL, SLIDE, two positions\nLEFT, RIGHT and two speed settings SLOWLY , FAST, re-\nsulting in 12 possible actions ( 3 action types × 2 positions ×\n2 speeds), e.g., PUSH-LEFT-SLOWLY means pushing the left\nobject slowly. Every sentence is composed of three words\n(excluding the <BOS/EOS> tags which denote beginning of\nsentence or end of sentence) with the first word indicating\nthe action, the second the cube colour and the last the\nspeed at which the action is performed (e.g., ‘ push green\nslowly’). Therefore, without the alternative words, there are\n18 possible sentences ( 3 action verbs ×3 colours×2 adverbs)\nfor Experiment 1a, whereas, for Experiment 1b and 2, the\nnumber of sentences is 36 as 6 cube colours are used in\nboth experiments. As a result, our dataset consists of 6 cube\narrangements (3 colour alternatives and the colours of the two\ncubes on the table never match) for Experiment 1a, 12 cube\narrangements for Experiments 1b and 2 (3 secondary colours\nare used in addition to 3 primary colours and secondary and\nprimary colours are mutually exclusive), 18×8 = 144 possible\nsentences for Experiment 1a, 36×8 = 288 possible sentences\nfor Experiments 1b and 2 with alternative vocabulary (consult\nTable II) - the factor of 8 because of eight alternatives per\nsentence. We have 72 patterns (action-description-arrangement\ncombinations) for Experiment 1a (12 actions with six cube\narrangements each) and 144 patterns for Experiments 1b and\n2. Following Yamada et al. [30], we choose the patterns\nrigorously to ensure that combinations of action, description\nand cube arrangements used in the test set are excluded from\nthe training set, although the training set includes all possible\ncombinations of action, description and cube arrangements\nthat are not in the test set. For Experiment 1a, 54 patterns\nare used for training while the remaining 18 for testing (for\nExperiments 1b and 2: 108 for training, 36 for testing). Each\npattern is collected six times in the simulation with random\nvariations on the action execution resulting in different joint\ntrajectories. We also use 4-fold cross-validation to provide\nmore reliable results (consult Table III) for Experiment 1.\nExperiment 1c tests for different shapes, other than cubes:\nwe perform the same actions on toy objects, which are a car,\nduck, cup, glass, house and lego brick. For testing the shape\nprocessing capability of the model, all objects are of the same\ncolour, namely yellow. Analogous to the other experiments,\ntwo objects of different shapes are placed on the table. We\nkeep the actions as they are but replace the colours with object\nnames in the descriptions. Before we extract the visual features\nfrom the new images, we train both the regular CAE and the\nchannel-separated CAE with them. Similar to Experiments\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTABLE III\nACTION -TO-LANGUAGE TRANSLATION ACCURACIES AT SENTENCE LEVEL\nMethod Experiment 1a (3 colours) Experiment 1b (6 colours) Experiment 1c (6 shapes)\nTraining Test Training Test Training Test\nPRAE + regular CAE 33.33 ± 1.31% 33.56 ± 3.03% 33.64 ± 1.13% 33.3 ± 0.98% 68.36 ± 2.12% 65.28 ± 2.45%\nPV AE + regular CAE 66.6 ± 1.31% 65.28 ± 6.05% 69.60 ± 0.46% 61.57 ± 2.01% 80.71 ± 1.41% 73.15 ± 1.87%\nPV AE + channel-separated CAE 100.00 ± 0.00% 90.28 ± 4.61% 100.00 ± 0.00% 100.00 ± 0.00% 95.99 ± 3.74% 92.13 ± 2.83%\n1a and 1b, we experiment with three methods: PRAE with\nstandard CAE, PV AE with standard CAE and PV AE with\nchannel-separated CAE.\nWe use NICO (Neuro-Inspired COmpanion) [15], [16] in a\nvirtual environment created with Blender5 for our experiments\n- see Figure 1. NICO is a humanoid robot, has a height of\napproximately one metre and a weight of approximately 20\nkg. The left arm of NICO is used to interact with the objects\nwhile utilising 5 joints. Actions are realised using the inverse\nkinematics solver provided by the simulation environment: for\neach action, first, the starting point and endpoint are adjusted\nmanually, then, the Gaussian deviation is applied around the\nstarting point and endpoint to generate the variations of the\naction, ensuring that there is a slight difference in the overall\ntrajectory. NICO has a camera in each of its eyes, which is\nused to extract egocentric visual images.\nA. Experiment 1\nWe use the same actions as in [30], such as PUSH-RIGHT-\nSLOWLY . We use three colour options for the cubes as in\n[30] for Experiment 1a, but six colours for Experiment 1b.\nHowever, we extend the descriptions in [30] by adding an\nalternative for each word in the original vocabulary. Hence,\nthe vocabulary size of 9 is extended to 17 for Experiment 1a\nand the vocabulary size of 11 is extended to 23 for Experiment\n1b - note that we do not add an alternative for <BOS/EOS>\ntags. Since every sentence consists of three words, we extend\nthe number of sentences by a factor of eight ( 23 = 8).\nAfter training the PV AE and PRAE on the same training set,\nwe test them for action-to-language translation. We consider\nonly those produced descriptions in which all three words\nand the <EOS> tag are correctly predicted as correct. The\nproduced descriptions that have one or more incorrect words\nare considered as false translations. As each description has\nseven more alternatives, predicting any of the eight description\nalternatives is considered correct.\nFor Experiment 1a, our model is able to translate approx-\nimately 90% of the patterns in the test set, whilst PRAE\ncould translate only one third of the patterns, as can be seen\nin Table III. We can, thus, say that our model outperforms\nPRAE in one-to-many mapping. We also test the impact of\nchannel separation on the translation accuracy by training our\nmodel with visual features extracted with the regular CAE\nas described in Yamada et al.’s approach [30]. It is clearly\nindicated in Table III that using variational autoencoders\n5https://www.blender.org/\ninstead of standard ones increases the accuracy significantly.\nUsing PV AE with channel-separated CAE improves the results\nfurther, indicating the superiority of channel separation in our\ntabletop scenario. Therefore, our approach with variational\nautoencoders and a channel-separated CAE is superior to both\nPRAE and PV AE with regular visual feature extraction.\nIn Experiment 1b, in order to test the limits of our PV AE and\nthe impact of more data with a larger corpus, we add three\nmore colour options for the cubes: yellow, cyan and violet.\nThese secondary colours are combined amongst themselves for\nthe arrangements in addition to the colour combinations used\nin the first experiment, i.e., a cube of a primary colour and a\ncube of a secondary colour do not co-occur. Therefore, this ex-\nperiment has 12 arrangements. Moreover, the vocabulary size\nis extended to 23 from 17 in Experiment 1b (two alternative\nwords for each colour - see Table II). As in Experiment 1a,\neach sentence has eight alternative ways to be described.\nWe train both PV AE and PRAE [30] on the extended dataset\nfrom scratch and test both architectures. As shown in Table\nIII (Experiment 1b), PV AE succeeds in performing 100% by\ntranslating every pattern from action to description correctly,\neven for the test set. In contrast, PRAE performs poorly\nin this setting and manages to translate only one third of\nthe descriptions correctly in the test set. Compared with the\naccuracy values reached in the first experiment with less data\nand a smaller corpus, extension of the dataset helps PV AE\nto perform better in translation, whereas PRAE is not able to\ntake advantage of more data. Similar to Experiment 1a, we\nalso test the influence of channel separation on the translation\naccuracy by training PV AE with visual features provided by\na regular CAE. In this setting, PV AE only achieves around\n61% of accuracy in the test set. This highlights once again the\nimportance of channel separation in visual feature extraction\nfor our setup. Whilst the improvement by using our PV AE over\nPRAE is significant, further improvement is made by utilising\nthe channel-separated CAE.\nIn addition, as the results show in the last column of Table\nIII (Experiment 1c), our PV AE with channel separation in\nvisual feature extraction outperforms the other methods even\nwhen manipulated objects have different shapes. Although\nthere is a slight drop in action-language translation perfor-\nmance, it is clear that the PV AE with the channel-separated\nCAE is able to handle different-shaped objects. The PRAE\nmodel performs slightly better than it does in the experiments\nwith cubes of different colours. However, our variational\nautoencoders approach without channel separation improves\nthe translation accuracy by approximately 8%. The channel\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nseparation in visual feature extraction improves the results\neven more similar to Experiment 1a and Experiment 1b, which\nshows the robustness of the channel-separated CAE when\nprocessing different objects.\nB. Experiment 2\nIn this experiment, we test the performance of PV AE-BERT\non action-to-language, language-to-action and language-to-\nlanguage translation. We use the same dataset as in Experiment\n1b for a fair comparison with the original PV AE (LSTM\nlanguage encoder). We thus use the same descriptions, which\nare constructed by using a verb, colour and speed from the\nvocabulary given in Table II as well as the <BOS/EOS>\ntags in the same order. Both PV AE and PV AE-BERT utilise\nchannel-separated CAE-extracted visual features.\nTABLE IV\nSENTENCE TRANSLATION ACCURACIES FOR PVAE-BERT AND PVAE\nPV AE PV AE-BERT\nTranslation Direction Test Accuracy (T - F) Test Accuracy (T - F)\nAction→Language 100.00% (216 - 0) 97.22% (210 - 6)\nLanguage→Language 100.00% (216 - 0) 80.56% (174 - 42)\nAs shown in Table IV, when translating from action to\nlanguage, PV AE-BERT achieves approximately97% accuracy\nfailing to translate only six of the descriptions, which is\ncomparable with the original architecture - the original PV AE\ncorrectly translate all 216 descriptions. The false translations\nare all due to incorrect translation of cube colours, e.g., the\npredicted description is ‘ slide blue slowly’ instead of\nthe ground truth ‘ slide red slowly’. We hypothesise\nthat the slight drop in the performance is due to the relatively\nsmall size of the dataset compared to almost 110 million\nparameters trained in the case of BERT. Nevertheless, these\nresults show that finetuning the BERT during training leads to\nalmost perfect action-to-language translation in our scenario.\nAs can be seen in Figure 3, both the PV AE and PV AE-\nBERT perform decently in language-to-action translation, and\nproduce joint angle values that are in line with and very similar\nto the original descriptions. In the bottom left plot, we can see\nthat the joint trajectories output by the PV AE-BERT are more\naccurate than those produced by the PV AE. We hypothesise\nthat the error margins are negligible and both, PV AE-BERT\nand the PV AE, succeed in language-to-action translation. Since\nwe did not realise the actions with the generated joint values\nin the simulation, we do not report the language-to-action\ntranslation accuracies in Table IV. However, we calculated\nthe mean squared errors (MSE) for both the PV AE and PV AE-\nBERT, which were both very close to zero. Therefore, it is fair\nto say that both architectures recognise language and translate\nit to action successfully.\nLanguage-to-language translation, however, suffers a bigger\nperformance drop when BERT is used as language encoder;\nPV AE-BERT reconstructs around 80% of the descriptions\ncorrectly (see Table IV). We hypothesise that this is partly due\nto having an asymmetric language autoencoder with a BERT\nTABLE V\nVARIATIONS OF DESCRIPTIONS FOR ONE EXAMPLE AND PVAE-BERT\nLANGUAGE -TO-LANGUAGE SENTENCE TRANSLATION ACCURACIES\nVar. Type Example Accuracy\n1 Standard ‘push blue slowly’ 80.56%\n2 Changed Word Order ‘slowly push blue’ 80.56%\n3 Full Command ‘push the blue cube slowly’ 81.02%\n4 ‘please’+Full Command ‘please push the blue cube\nslowly’\n81.94%\n5 Full Command+‘please’ ‘push the blue cube slowly\nplease’\n81.48%\n6 Ch.W.Order+F. Com.+‘pls.’ ‘slowly push the blue cube\nplease’\n81.48%\n7 Polite Request ‘could you please push the\nblue cube slowly?’\n79.63%\nencoder and an LSTM decoder. The BERT-base language\nencoder constitutes the overwhelming majority of parameters\nin the PV AE-BERT model, which renders the language V AE\nheavily skewed to the encoder half. This may affect the\nperformance of the language decoder when translating back\nto the description from the hidden code produced mainly by\nBERT as the decoder’s parameters constitute less than 1% of\nthe parameters of the language V AE. This hypothesis is further\nsupported by the original architecture, which has a symmetric\nlanguage V AE, achieving100% of accuracy in the same task.\nNevertheless, our findings shows that the PV AE-BERT\nmodel achieves stable language-to-language translation perfor-\nmance even when the given descriptions do not comply with\nthe fixed grammar and are full commands such as ‘push the\nblue cube slowly’ or have a different word order such\nas ‘quickly push blue’. To turn predefined descriptions\ninto full commands, we add the words ‘ the’ and ‘ cube’ to\nthe descriptions and we also experiment with adding the word\n‘please’ and changing the word order as can be seen from\nthe examples given in Table V. Although it is not explicitly\nstated in the table for space reasons, we alternate between the\nmain elements of the descriptions as in the other experiments\nfollowing the vocabulary; for example, ‘push’ can be replaced\nby ‘move-up’ and ‘ quickly’ can be replaced by ‘ fast’.\nMoreover, we achieve consistent language-to-action translation\nperformance with PV AE-BERT when we test it with different\ndescription types shown in the table - consult Figure 3 bottom\nright plot. As the PV AE-BERT performs consistently even with\ndescriptions not following the predefined grammar, we can see\nthat the adoption of a language model to the architecture is\npromising towards acquiring natural language understanding\nskills.\nC. Principal Component Analysis on Hidden Representations\nWe have also conducted principal component analysis\n(PCA) on the hidden features extracted from PV AE-BERT.\nFigure 4 shows the latent representations of language in Plot\n(a) and of action in Plot (b). The PCA on the representations of\nlanguage shows that the model learns the compositionality of\nlanguage: the X-axis (principal component PC 1) distinguishes\nthe descriptions in the speed component (adverb), the Y-\naxis (PC 3) distinguishes colour, and the Z-axis (PC 6)\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFig. 3. Examples of language-to-action translation by PV AE-BERT and its comparison with PV AE: in the top row, the two plots represent the ground truth\nand predicted joint trajectories by PV AE-BERT for PUSH-LEFT-SLOWLY and PULL-LEFT-SLOWLY actions. Solid lines show the ground truth, while the\ndashed lines, which are often covered by the solid lines, show the predicted joint angle values. In the bottom row, the left plot shows the total error margin\nof the five joint values produced by PV AE and PV AE-BERT per time step for the PUSH-LEFT-SLOWLY action, while the right plot shows the joint values\nproduced by PV AE-BERT given three variations (see Table V) of the same command for PULL-LEFT-SLOWLY - notice how the joint trajectories overlap\nmost of the time. In all of the plots, the X axis represents the time steps.\ndistinguishes the action type (verb) 6. Plot (b) shows that the\nPCA representations of actions are semantically similar, since\ntheir arrangement coincides with those in Plot (a).\nOur method learns actions according to their paired de-\nscriptions: it learns the colour of the object (an element\nof descriptions) interacted with. However, it does not learn\nthe position of it (an element of actions). We inspected the\nrepresentations along all major principle components, but we\ncould not find any direction along which the position was\nmeaningfully distinguished. For example, in (b), some of the\nfilled red circles (corresponding to description ‘ push red\nslowly’) are paired with the action PUSH-LEFT-SLOWLY\nwhile the others with PUSH-RIGHT-SLOWLY . As actions\nlearned according to their paired descriptions, hence semanti-\ncally, the filled red circles are grouped together even though\nthe red cube may be on the right or left. In contrast, an action\ncan be represented far from another identical action: e.g., the\nrepresentations of ‘ pull red slowly’ (filled red circles\nin Figure 4) are separated from those of ‘ pull yellow\nslowly’ (filled yellow circles) along PC 3, even if they both\ndenote the action PULL-LEFT-SLOWLY . These results indi-\n6The percentages of variance explained were very similar between PC 2\nuntil PC 6; therefore, we selected PC 3 and PC 6 for display as they resolved\ncolour and action type optimally.\ncate that the binding loss has transferred semantically driven\nordering from the language to the action representations.\nWhen our agent receives a language instruction, which\ncontains the colour but not position, the agent is still able\nto perform the action according to the position (cf. Figure 3)\nof the object. The retrieval of the position information must\ntherefore be done by the action decoder: it reads the images\nto obtain the position of the object that has the colour given in\nthe instruction. It is therefore not surprising that the PCA does\nnot reveal any object position encodings in the bottleneck.\nV. D ISCUSSION\nExperiments 1a and 1b show that our variational autoen-\ncoder approach with a channel-separated CAE visual feature\nextraction (‘PV AE + channel-separated CAE’) performs better\nthan the standard autoencoder approach, i.e., PRAE [30], in\nthe one-to-many translation of robot actions into language\ndescriptions. Our approach is superior both in the case of\nthree colour alternatives per cube and in the case of six\ncolour alternatives per cube by a large margin. The additional\nexperiment with six different objects highlights the robust-\nness of our approach against the variation in object types.\nWe demonstrate that a Bayesian inference-based method like\nvariational autoencoders can scale up with more data for gen-\neralisation, whereas standard autoencoders cannot capitalise\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFig. 4. Hidden features of language (a) and hidden features of action (b): PCA was performed jointly on the hidden features of 36 descriptions and the\nhidden features of 144 actions. For (b), each unique action (12 in total) occurs 12 times as there are 12 possible cube arrangements; therefore, 144 points are\nshown. For both (a) and (b), we label the points according to descriptions, i.e., for (b), actions are also labelled according to their paired descriptions. As can\nbe seen from the legend, different shapes, colours and fillings indicate the verb (action type), object colour and adverb (speed), respectively.\non a larger dataset, since the proposed PV AE model achieves\nbetter accuracy when the dataset and the corpus are extended\nwith three extra colours or six different objects. Additionally,\nstandard autoencoders are fairly limited in coping with the\ndiversification of language as they do not have the capacity\nto learn the mapping between an action and many descrip-\ntions. In contrast, variational autoencoders yield remarkably\nbetter results in one-to-many translation between actions and\ndescriptions, because stochastic generation (random normal\ndistribution) within the latent feature extraction allows latent\nrepresentations to slightly vary, which leads to V AEs learning\nmultiple descriptions rather than a particular description for\neach action.\nA closer look into action-to-language translation accuracies\nachieved by the PRAE for Experiments 1a and 1b shows\nthat having more variety in the data (i.e. more colour options\nfor cubes) does not help the standard autoencoder approach\nto learn one-to-many binding between action and language.\nBoth in the first case with three colour alternatives and in the\nsecond case with six colour alternatives, the PRAE manages to\ntranslate only around one third of the samples from actions to\ndescriptions correctly. In contrast, the accuracies achieved by\nour proposed PV AE for both datasets prove that the variational\nautoencoder approach can benefit from more data as the test\naccuracy for the ‘PV AE + channel-separated CAE’ goes up by\napproximately 10% to 100% when three more colour options\nare added to the dataset.\nFurthermore, training the PV AE with the visual features\nextracted by the standard CAE demonstrates that training\nand extracting features from each RGB channel separately\nmitigates the colour distinction issue for cubes when the\nvisual input, like in our setup, includes objects covering a\nrelatively small portion of the visual field. The ‘PV AE +\nregular CAE’ variant performs significantly worse than our\n‘PV AE + channel-separated CAE’ approach. This also demon-\nstrates the importance of the visual modality for the overall\nperformance of the approach. Our analysis on the incorrectly\ntranslated descriptions shows that a large amount of all errors\ncommitted by the ‘PV AE + regular CAE’ were caused due\nto cube colour distinction failures such as translating ‘ slide\nred fast’ as ‘ slide green fast’, which proves the\nchannel-separated CAE’s superiority over the standard CAE\nin visual feature extraction in our scenario. Moreover, using\nthe channel-separated CAE for visual feature extraction rather\nthan the standard CAE results in better action-to-language\ntranslation accuracy even when the objects are of various\nshapes. This indicates that the channel-separated CAE not\nonly works well with cubes of different colours but also\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nobjects of different shapes. We emphasise the superiority of\nchannel separation in our scenario, which is tested and proven\nin a simulation environment. For real-world scenarios with\ndifferent lighting conditions, it is advisable to take into account\nalso the channel interaction [26] to have more robust visual\nfeature extraction.\nExperiment 2 indicates the potential of utilising a pretrained\nlanguage model like BERT for the interpretation of language\ndescriptions. This extension produces comparable results to the\noriginal PV AE with the LSTM language encoder in language-\nto-action and action-to-language translations. The drop in\nlanguage-to-language performance to 80% is most probably\ncaused by the asymmetric language V AE of the PV AE-BERT\nmodel that consists of a feedforward BERT encoder with\nattention mechanisms, which reads the entire input sequence in\nparallel, and of a recurrent LSTM decoder, which produces the\noutput sequentially. A previous study on a text classification\ntask also shows that LSTM models outperform BERT on\na relatively small corpus because, with its large number of\nparameters, BERT tends to overfit when the dataset size is\nsmall [10]. Furthermore, we have also tested the PV AE-\nBERT, which was trained on predefined descriptions, with\nfull sentence descriptions - e.g. ‘ push the blue cube\nslowly’ for ‘ push blue slowly’ - and with variations\nof the descriptions that have a different word order. We have\nconfirmed that PV AE-BERT achieves the same performance\nin language-to-action and language-to-language translations.\nThis is promising for the future because the pretrained BERT\nallows the model to understand unconstrained natural language\ncommands that do not conform to the defined grammar.\nThe PCA conducted on the hidden features of PV AE-BERT\nshows that our method can learn language and robot actions\ncompositionally and semantically. Although it is not explicitly\ngiven, we have also confirmed that both the PV AE and PV AE-\nBERT are able to reconstruct joint values almost perfectly\naccurately when we analysed the action-to-action translation\nresults. Together with the language-to-language performance,\naction-to-action capability of both variants of our architecture\ndemonstrates that the two variational autoencoders (language\nand action) in our approach retain their reconstructive nature.\nVI. C ONCLUSION\nIn this study, we have reported the findings of previous\nwork and its extension with several experiments. We have\nshown that variational autoencoders outperform standard au-\ntoencoders in terms of one-to-many translation of robot actions\nto descriptions. Furthermore, the superiority of our channel-\nseparated visual feature extraction has been proven with an\nextra experiment that involves different types of objects. In\naddition, using the PV AE with a BERT model pretrained on\nlarge text corpora, instead of the LSTM encoder trained on\nour small predefined grammar, unveils promising scaling-up\nopportunities for the proposed approach, and it offers the\npossibility to map unconstrained natural language descriptions\nwith actions.\nIn the future, we will collect descriptions via crowdsourcing\nin order to investigate the viability of using a pretrained\nlanguage model as an encoder to relate from language to motor\ncontrol. We will also seek ways to bind the two modalities in\na more biologically plausible way. Moreover, increasing the\ncomplexity of the scenario with more objects in general and\non the table simultaneously may shed light to the scalability of\nour approach. Lastly, we will transfer our simulation scenario\nto the real world and conduct experiments on the real robot.\nACKNOWLEDGMENT\nThe authors gratefully acknowledge support from the Ger-\nman Research Foundation DFG, project CML (TRR 169).\nREFERENCES\n[1] Amina Adadi and Mohammed Berrada. Peeking inside the black-box: A\nsurvey on explainable artificial intelligence (xai). IEEE Access, 6:52138–\n52160, 2018.\n[2] Ahmed Akakzia, C ´edric Colas, Pierre-Yves Oudeyer, Mohamed\nChetouani, and Olivier Sigaud. Grounding Language to Autonomously-\nAcquired Skills via Goal Generation. In International Conference on\nLearning Representations, Virtual (formerly Vienna, Austria), 2021.\n[3] Alexandre Antunes, Alban Laflaquiere, Tetsuya Ogata, and Angelo Can-\ngelosi. A bi-directional multiple timescales LSTM model for grounding\nof actions and verbs. In 2019 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 2614–2621, 2019.\n[4] John Arevalo, Thamar Solorio, Manuel Montes-y Gomez, and Fabio A\nGonz´alez. Gated multimodal networks. Neural Computing and Appli-\ncations, 32(14):10209–10228, 2020.\n[5] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua\nBengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May,\nAleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. Experience\ngrounds language. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, pages 8718–8735. Associa-\ntion for Computational Linguistics, November 2020.\n[6] Joyce Y Chai, Qiaozi Gao, Lanbo She, Shaohua Yang, Sari Saba-Sadiya,\nand Guangyue Xu. Language to action: Towards interactive task learning\nwith physical agents. In IJCAI, pages 2–9, 2018.\n[7] Franc ¸ois Chollet. Xception: Deep learning with depthwise separable\nconvolutions. In 2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1800–1807, 2017.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep bidirectional transformers for language\nunderstanding. In NAACL-HLT (1), 2019.\n[9] Aaron Eisermann, Jae Hee Lee, Cornelius Weber, and Stefan Wermter.\nGeneralization in multimodal language learning from simulation. In\nProceedings of the International Joint Conference on Neural Networks\n(IJCNN 2021), Jul 2021.\n[10] Aysu Ezen-Can. A comparison of LSTM and BERT for small corpus.\narXiv preprint arXiv:2009.05451, 2020.\n[11] Jun Hatori, Yuta Kikuchi, Sosuke Kobayashi, Kuniyuki Takahashi, Yuta\nTsuboi, Yuya Unno, Wilson Ko, and Jethro Tan. Interactively picking\nreal-world objects with unconstrained spoken language instructions.\nIn 2018 IEEE International Conference on Robotics and Automation\n(ICRA), pages 3774–3781. IEEE, 2018.\n[12] Stefan Heinrich and Stefan Wermter. Interactive natural language\nacquisition in a multi-modal recurrent neural architecture. Connection\nScience, 30(1):99–133, 2018.\n[13] Stefan Heinrich, Yuan Yao, Tobias Hinz, Zhiyuan Liu, Thomas Hummel,\nMatthias Kerzel, Cornelius Weber, and Stefan Wermter. Crossmodal\nlanguage grounding in an embodied neurocognitive model. Frontiers in\nNeurorobotics, 14:52, 2020.\n[14] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735–1780, 1997.\n[15] Matthias Kerzel, Theresa Pekarek-Rosin, Erik Strahl, Stefan Heinrich,\nand Stefan Wermter. Teaching NICO how to grasp: an empirical study\non crossmodal social interaction as a key factor for robots learning from\nhumans. Frontiers in Neurorobotics, 14:28, 2020.\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n[16] Matthias Kerzel, Erik Strahl, Sven Magg, Nicol ´as Navarro-Guerrero,\nStefan Heinrich, and Stefan Wermter. NICO—Neuro-Inspired COm-\npanion: A developmental humanoid robot platform for multimodal\ninteraction. In 2017 26th IEEE International Symposium on Robot and\nHuman Interactive Communication (RO-MAN), pages 113–120, 2017.\n[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic opti-\nmization. In 3rd International Conference on Learning Representations,\nICLR, San Diego, CA, USA, May 7-9, 2015.\n[18] Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes.\nIn Proceedings of International Conference on Learning Representations\n(ICLR), Banff, AB, Canada, April 14-16, 2014, 2014.\n[19] Corey Lynch and Pierre Sermanet. Language conditioned imitation\nlearning over unstructured data. Robotics: Science and Systems, 2021.\n[20] Joseph Marino. Predictive coding, variational autoencoders, and biolog-\nical connections. Neural Computation, 34(1):1–44, 2021.\n[21] Hwei Geok Ng, Paul Anton, Marc Br ¨ugger, Nikhil Churamani, Erik\nFließwasser, Thomas Hummel, Julius Mayer, Waleed Mustafa, Thi\nLinh Chi Nguyen, Quan Nguyen, et al. Hey robot, why don’t you\ntalk to me? In 2017 26th IEEE International Symposium on Robot and\nHuman Interactive Communication (RO-MAN), pages 728–731, 2017.\n[22] Tetsuya Ogata, Masamitsu Murase, Jun Tani, Kazunori Komatani, and\nHiroshi G. Okuno. Two-way translation of compound sentences and arm\nmotions by recurrent neural networks. In 2007 IEEE/RSJ International\nConference on Intelligent Robots and Systems, pages 1858–1863, 2007.\n[23] Has ¸im Sak, Andrew Senior, and Franc ¸oise Beaufays. Long short-term\nmemory recurrent neural network architectures for large scale acoustic\nmodeling. In Proceedings of Interspeech 2014, pages 338–342, 2014.\n[24] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette\nBohg. Concept2robot: Learning manipulation concepts from instructions\nand human demonstrations. In Proceedings of Robotics: Science and\nSystems (RSS), 2020.\n[25] Mohit Shridhar, Dixant Mittal, and David Hsu. INGRESS: Interactive\nvisual grounding of referring expressions. The International Journal of\nRobotics Research, 39(2-3):217–232, 2020.\n[26] Du Tran, Heng Wang, Matt Feiszli, and Lorenzo Torresani. Video\nclassification with channel-separated convolutional networks. In 2019\nIEEE/CVF International Conference on Computer Vision (ICCV), pages\n5551–5560, 2019.\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In Advances in Neural Information Processing Systems,\npages 5998–6008, 2017.\n[28] Valentin Vielzeuf, Alexis Lechervy, St ´ephane Pateux, and Fr ´ed´eric\nJurie. Centralnet: A multilayer approach for multimodal fusion. In\nLaura Leal-Taix ´e and Stefan Roth, editors, Computer Vision – ECCV\n2018 Workshops, pages 575–589, Cham, 2019. Springer International\nPublishing.\n[29] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad\nNorouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,\nKlaus Macherey, et al. Google’s neural machine translation system:\nBridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[30] Tatsuro Yamada, Hiroyuki Matsunaga, and Tetsuya Ogata. Paired re-\ncurrent autoencoders for bidirectional translation between robot actions\nand linguistic descriptions. IEEE Robotics and Automation Letters,\n3(4):3441–3448, 2018.\n[31] Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah\nConstant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-\nhsuan Sung, Brian Strope, and Ray Kurzweil. Multilingual universal\nsentence encoder for semantic retrieval. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics:\nSystem Demonstrations, pages 87–94, Online, July 2020. Association\nfor Computational Linguistics.\n[32] Ozan ¨Ozdemir, Matthias Kerzel, and Stefan Wermter. Embodied\nlanguage learning with paired variational autoencoders. In 2021 IEEE\nInternational Conference on Development and Learning (ICDL), pages\n1–6. IEEE, Aug 2021.\nOzan ¨Ozdemir is a doctoral candidate and working\nas a research associate in the Knowledge Technology\ngroup, University of Hamburg, Germany. He has a\nBSc degree in computer engineering from Yildiz\nTechnical University. He has received his MSc de-\ngree in Intelligent Adaptive Systems at the Univer-\nsity of Hamburg. His research interests are embodied\nand crossmodal language learning, autoencoders, re-\ncurrent neural networks and large language models.\nMatthias Kerzel received his MSc and PhD in\ncomputer science from the Universit ¨at Hamburg,\nGermany. He is currently a postdoctoral research\nand teaching associate at the Knowledge Technology\nGroup of Prof. Stefan Wermter at the University\nof Hamburg. He has given lectures on Knowledge\nProcessing in Intelligent Systems, Neural Networks\nand Bio-inspired Artificial Intelligence. He is cur-\nrently the Secretary of the European Neural Network\nSociety and worked in the organising committee\nof the International Conference on Artificial Neural\nNetworks conferences. His research interests are in developmental neuro-\nrobotics, hybrid neurosymbolic architectures, explainable AI and human-robot\ninteraction. He is currently involved in the international SFB/TRR-169 large-\nscale project on crossmodal learning.\nCornelius Weber graduated in physics at Universit¨at\nBielefeld, Germany and received his PhD in com-\nputer science at Technische Universit ¨at Berlin. Fol-\nlowing positions were a Postdoctoral Fellow in Brain\nand Cognitive Sciences, University of Rochester,\nUSA; Research Scientist in Hybrid Intelligent Sys-\ntems, University of Sunderland, UK; Junior Fellow\nat the Frankfurt Institute for Advanced Studies,\nGermany. Currently he is Lab Manager at Knowl-\nedge Technology, Universit¨at Hamburg. His interests\nare in computational neuroscience, development of\nvisual feature detectors, neural models of representations and transformations,\nreinforcement learning and robot control, grounded language learning, human-\nrobot interaction and related applications in social assistive robotics.\nJae Hee Lee is a postdoctoral research associate\nin the Knowledge Technology Group, University of\nHamburg, Germany. He has worked on topics in\nmultimodal learning, grounded language understand-\ning, and spatial and temporal reasoning. Jae Hee\nLee received his Diplom degree in mathematics and\ndoctoral degree in computer science from the Uni-\nversity of Bremen, Germany. He was a postdoctoral\nresearcher at the Australian National University,\nUniversity of Technology Sydney (Australia) and\nCardiff University (UK).\nStefan Wermter (Member, IEEE) is currently a Full\nProfessor with the University of Hamburg, Hamburg,\nGermany, where he is also the Director of the\nDepartment of Informatics, Knowledge Technology\nInstitute. Currently, he is a co-coordinator of the In-\nternational Collaborative Research Centre on Cross-\nmodal Learning (TRR-169) and a coordinator of the\nEuropean Training Network TRAIL on transparent\ninterpretable robots. His main research interests are\nin the fields of neural networks, hybrid knowledge\ntechnology, cognitive robotics and human–robot in-\nteraction. He is an Associate Editor of Connection Science and International\nJournal for Hybrid Intelligent Systems. He is on the Editorial Board of the\njournals Cognitive Systems Research, Cognitive Computation and Journal of\nComputational Intelligence. He is serving as the President for the European\nNeural Network Society.\nThis article has been accepted for publication in IEEE Transactions on Cognitive and Developmental Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCDS.2022.3204452\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8544769287109375
    },
    {
      "name": "Autoencoder",
      "score": 0.7666946053504944
    },
    {
      "name": "Language model",
      "score": 0.6709972620010376
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6619288325309753
    },
    {
      "name": "Transformer",
      "score": 0.6157922744750977
    },
    {
      "name": "Natural language",
      "score": 0.5236663222312927
    },
    {
      "name": "Robot",
      "score": 0.5207988023757935
    },
    {
      "name": "Encoder",
      "score": 0.5117887854576111
    },
    {
      "name": "Object (grammar)",
      "score": 0.4686816334724426
    },
    {
      "name": "Artificial neural network",
      "score": 0.44159257411956787
    },
    {
      "name": "Natural language processing",
      "score": 0.4071333408355713
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}