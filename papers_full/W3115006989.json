{
  "title": "Summarization of COVID-19 news documents deep learning-based using transformer architecture",
  "url": "https://openalex.org/W3115006989",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2786271804",
      "name": "Nur Hayatin",
      "affiliations": [
        "Universitas Muhammadiyah Malang",
        "Universitas Muhammadiyah Jember"
      ]
    },
    {
      "id": "https://openalex.org/A3096636497",
      "name": "Kharisma Muzaki Ghufron",
      "affiliations": [
        "Universitas Muhammadiyah Jember",
        "Universitas Muhammadiyah Malang"
      ]
    },
    {
      "id": "https://openalex.org/A2735961444",
      "name": "Galih Wasis Wicaksono",
      "affiliations": [
        "Universitas Muhammadiyah Jember"
      ]
    },
    {
      "id": "https://openalex.org/A2786271804",
      "name": "Nur Hayatin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3096636497",
      "name": "Kharisma Muzaki Ghufron",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2735961444",
      "name": "Galih Wasis Wicaksono",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3038383319",
    "https://openalex.org/W3042114636",
    "https://openalex.org/W2187999008",
    "https://openalex.org/W2613310014",
    "https://openalex.org/W2997520071",
    "https://openalex.org/W6756778025",
    "https://openalex.org/W3011218678",
    "https://openalex.org/W2964910540",
    "https://openalex.org/W2998706140",
    "https://openalex.org/W3011891578",
    "https://openalex.org/W2982784916",
    "https://openalex.org/W3033893309",
    "https://openalex.org/W2943396484",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3016441316",
    "https://openalex.org/W2924210235",
    "https://openalex.org/W3000003809",
    "https://openalex.org/W2783180099",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2909602489",
    "https://openalex.org/W3005080142",
    "https://openalex.org/W2755930428",
    "https://openalex.org/W2990853187",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3008902715",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2904790185",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4247707017",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2311138967",
    "https://openalex.org/W2154652894"
  ],
  "abstract": "Facing the news on the internet about the spreading of Corona virus disease 2019 (COVID-19) is challenging because it is required a long time to get valuable information from the news. Deep learning has a significant impact on NLP research. However, the deep learning models used in several studies, especially in document summary, still have a deficiency. For example, the maximum output of long text provides incorrectly. The other results are redundant, or the characters repeatedly appeared so that the resulting sentences were less organized, and the recall value obtained was low. This study aims to summarize using a deep learning model implemented to COVID-19 news documents. We proposed transformer as base language models with architectural modification as the basis for designing the model to improve results significantly in document summarization. We make a transformer-based architecture model with encoder and decoder that can be done several times repeatedly and make a comparison of layer modifications based on scoring. From the resulting experiment used, ROUGE-1 and ROUGE-2 show the good performance for the proposed model with scores 0.58 and 0.42, respectively, with a training time of 11438 seconds. The model proposed was evidently effective in improving result performance in abstractive document summarization.",
  "full_text": "TELKOMNIKA Telecommunication, Computing, Electronics and Control  \nVol. 19, No. 3, June 2021, pp. 754~761 \nISSN: 1693-6930, accredited First Grade by Kemenristekdikti, Decree No: 21/E/KPT/2018  \nDOI: 10.12928/TELKOMNIKA.v19i3.18356 ï² 754 \n  \nJournal homepage: http://journal.uad.ac.id/index.php/TELKOMNIKA \nSummarization of COVID-19 news documents \ndeep learning-based using transformer architecture \n \n \nNur Hayatin, Kharisma Muzaki Ghufron, Galih Wasis Wicaksono \nDepartment of Informatics, Faculty of Engineering, University of Muhammadiyah Malang, Indonesia \n \n \nArticle Info  ABSTRACT  \nArticle history: \nReceived Jul 25, 2020 \nRevised Oct 12, 2020 \nAccepted Oct 23, 2020 \n \n Facing the news on the internet about the spreading of Corona virus disease \n2019 (COVID -19) is challenging because it is required a long time to get \nvaluable information from the news. Deep learning has a significant impact on \nNLP research. However, the deep learning models used in several studies, \nespecially in document summary, still have a defici ency. For example, the \nmaximum output of long text provides incorrectly. The other results are \nredundant, or the characters repeatedly appeared so that the resulting sentences \nwere less organized, and the recall value obtained was low. This study aims to \nsummarize using a deep learning mo del implemented to COVID -19 news \ndocuments. We proposed transformer as base language models  with \narchitectural modification as the basis for designing the model to improve \nresults significantly in document summarization. We  make a  \ntransformer-based architecture model with encoder and decoder that can be \ndone several times repeatedly and make a comparison of layer modifications \nbased on scoring. From the resulting experiment used, ROUGE -1 and \nROUGE-2 show the good performanc e for the proposed model with scores \n0.58 and 0.42, respectively, with a training time of 11438 seconds. The model \nproposed was evidently effective in improving result performance in \nabstractive document summarization. \nKeywords: \nCOVID-19 \nDeep learning  \nNews summarization \nTransformer architecture \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nKharisma Muzaki Ghufron \nDepartment of Informatics \nUniversity of Muhammadiyah Malang \nCampus III UMM, 246 Raya Tlogomas St., Malang, Jawa Timur 65144, Indonesia \nEmail: kharisma.muzaki@webmail.umm.ac.id \n \n \n1. INTRODUCTION  \nIn early 2020 the world was hit by Corona virus disease 2019 (COVID-19) pandemic, which affected \nglobal life. All people with various backgrounds and fields of science discuss COVID -19 pandemic, both \nthrough social media and web news. Likewise, with data scientists, several studies primarily related to natural \nlanguage processing (NLP) on COVID -19 also began to be carried out . Such as predictive models that can \nestimate returns on stocks from countries most affected by the COVID-19 pandemic [1], also modeled causality \nusing neural networks to explore misinformation on social media during the COVID -19 pandemic [2]. As far \nas our observation, none of these studies related to the COVID -19 news document has been done previously. \nMeanwhile, one of the news media trends recently is the publication of news documents about COVID -19, \nwhich was released quickly and updated every day, so that the release resulted in a lot of data and news about \nthe COVID-19. This was proofed by the search results from the Google search engine that gave a lot of output \nabove 6 billion documents for COVID-19 keywords that are accessed in July 2020. This is a challenge in how \n\nTELKOMNIKA Telecommun Comput El Control  ï² \n \nSummarization of COVID-19 news documents deep learning-based usingâ€¦ (Nur Hayatin) \n755 \nto be able to find the relevant information from the document collection, especially from the news documents. \nNews documents are one of the many unstructured d ata that are found and easily accessed on the internet. \nMultiple types of data that are scattered on the internet, including news documents, increase rapid  \ngrowth [3, 4] and increase exponentially [4]. With the rapid growth of data, the summarization of information \nbecomes important to meet the needs of internet users.  In summarizing documents, two types can be used, \nnamely abstractive and extractive. An extractive summary is a concise text method which consists of three \nstages: text representation, sentence evaluation, and sentence selection using a statistical model [5]. Abstractive \nsummarization works by producing new sentences in summary based on an existing text by repeating new \nwords as an extraction act [6]. \nSeveral studies on abstractive news summar ies using deep learning have been done previously. \nAbstractive summarizations were conducted on the Chinese news dataset using public opinion [7]. Meanwhile, \nother research focused on the keywords that exist in the text sentences that can work effectively to produce \ninterpreted texts [8]. The other model that was used for abstractive news summary is sequence modeling, such \nas long short-term memory (LSTM) and recurrent neural network (RNN). The sequence to sequence the RNNs \nmodel has successfully reduced th e training loss for abstractive summary used amazon fine food reviews \ndataset [9]. Unfortunately, the maximum output of long text provides incorrectly . The research provide s a \ncorrect summary only for short text. Another study also conducted experiments by  doing a combination of \nlocal attention and LSTM in which the results of the summarization of the characters repeatedly appeared so \nthat the resulting sentences were less organized and the recall value obtained was low [10]. However, the \nrepetitive workings found in the recurrent model like RNN and LSTM, prevent the model from conducting \nparallel training and limit the ability to know context with longer input sequences  [11].  \nTransformer, as base language models, has significantly impacted the NLP research  field to replace \nthe deficiency of both LSTM , CNN and RNN based as a deep learning architecture  [12, 13], so that many \nreasons why the transformer was chosen as base model architecture. Various studies applied to transformer \narchitecture have been carried out and have improved results significantly in document summarization [14]. In \nprevious studies, transformers was used as a detection irony grouping in  Spanish using pre -training Twitter \nword research results compared to LSTM attention, and the deep averaging network showed an increase \nsignificantly on performances  [15]. The transformer also succeeded in making Chinese story -generation by \ncreating two layers of self-attention and reducing the number of encoder and decoder layers to identic one. The \nresults showed a low loss and an increase significantly from the base layer of the transformer model [16]. \nMeanwhile, the use of the transformer was carried ou t successfully using a combined modification of the  \nbidirectional encoder representations from transformers (BERT) as a transformer-based encoder and decoder in \nJapanese abstractive summarization, which has resulted in good average accuracy and the lowest loss value [17]. \nIn this study, we propose a transformer-based model to summarize COVID-19 news with several methods and \nstages. Another discussion from the result is present by make sublayer modifications to determine the effect of \nparameters on the encoder and decoder layers. \n \n \n2. DATASET \nThe dataset we used came from news documents about COVID-19 that was published on the Kaggle \nplatform [18] from the Canadian broadcasting corporation (CBC) news site, with a total number of documents \nthat were used to build the model is 2755 documents. The relevance of the news in the dataset containing \nvariations combined topics related to COVID -19 are processed using the crawler with the keyword  \nCOVID-19 published from January 08, 2020, until March 03, 2020.  In the dataset, there are text description \ncontains the news content, and the description feature is a summary of the news content. Specific keywords in \nnews content are listed in the word coronavirus. There is a variety of mixed news content, but the news content \nis more important in the amount of COVID-19 growth in each region. \n \n \n3. PREPROCESSING \nSeveral previous studies have shown the results of their research by doing preprocessing can increase \naccuracy results by a percentage of 2%  [19], preprocessing is also used in some words that have the form of \nmisspellings [20]. At the preprocessing stage, several processes occur.  i.e., contractions, lowercasing & \nprintable checks, splitting data, tokenization, and word embedding. We divide the dataset into three forms, i.e., \ntraining, validation,  and testing, with percentages of 70%, 10%,  and 20%, respectively. Contractions and \nprintable checks, mapped out contraction words from the list of word contractions. These words were defined \nby ourselves to get the original form terms such as \"don't\" become \"do not,\" then printable check used to delete \ncharacters other than punctuation marks, and ASCII letters. After that,  due to memory limitations we did \ndistribute control to limit text which was needed as a  training model. Furthermore, tokenization is the process \n     ï²          ISSN: 1693-6930 \nTELKOMNIKA Telecommun Comput El Control, Vol. 19, No. 3, June 2021:  754 - 761 \n756 \nof breaking text into separate word s and adding unique tokens. In the modification of the model, we use a  \npre-trained word embedding global vector (GloVe) with a vocabulary of 2.2  M to present each word in a  \n300-dimensional vector size [21]. Previous studies have shown that unsupervised comparison results based on \ntext summarization using word embedding are more effective than using a bag of words  [22]. \n \n \n4. TRANSFORMER MODEL \nWe analy zed in Figure 1, and there are encoder and decoder layer s that have the dropout and \nNormalization in each sublayer. We use of gaussian error linear unit (GELU) in the feed -forward network is \nused only once on each encoder or decoder layer, due to GELU has high complexity in the NLP field but the \nperformance produced is superior compared to other activation functions such as ELU and ReLU  [23]. The \nmulti-head attention formulation can be seen following in (1) [14]. h is the total attention carried out in parallel \nso that every â„ğ‘’ğ‘ğ‘‘ğ‘–  is carried out the attention function contained in (2) [14]. \n \n \n \n \nFigure 1. Transformer model proposed \n \n \nğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘(ğ‘„, ğ¾, ğ‘‰) = ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„ğ‘’ğ‘ğ‘‘1, â€¦ , â„ğ‘’ğ‘ğ‘‘â„ )ğ‘Šğ‘‚    (1) \n \nâ„ğ‘’ğ‘ğ‘‘ğ‘– =  ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„ğ‘Šğ‘–\nğ‘„ , ğ¾ğ‘Šğ‘–\nğ¾, ğ‘‰ğ‘Šğ‘–\nğ‘‰)       (2) \n \nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„, ğ¾, ğ‘‰) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(\nğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n)      (3) \n \nThe attention function can be defined as a function that performs the mapping of the query Q is the \ntarget sequence; the key pair K and the value V are derived from the sequence. Each Q, K, V , and output \nmapping are defined in vector form. The weight of ea ch calculated value is a representation of adjusting the \nquery to the key. The query and key dimensions are defined as ğ‘‘ğ‘˜, and the values dimension ğ‘‘ğ‘£ is used as the \nAttention parameter found in (3) [14]. Multi-head attention combines several attenti on models to each of the \nğ‘„, ğ¾, ğ‘‰ models. The weighting dimension of a sequence is defined as ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™  so that its representation is in \nmulti-head ğ‘Šğ‘‚ âˆˆ  â„ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—ğ‘‘ğ‘˜ . The primary difference between a masked multi -head attention and a  \nmulti-head attention is that some tokens contained in a sequence are randomly removed to train the model to \nunderstand the context contained in the sequence. Transformer also performs positional encoding (ğ‘ƒğ¸), which \nis the injection of some information on each wo rd position contained in a sequence. PE has the same \ndimensions as ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ . In this paper, we use sine -cosine positional encoding, where the formula equation can \nbe seen in (4) and (5), the pos is a position, and i is dimension.  \n \nğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–) = sin (\nğ‘ğ‘œğ‘ \n10000\n2ğ‘–\nğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™\n)      (4) \n\nTELKOMNIKA Telecommun Comput El Control  ï² \n \nSummarization of COVID-19 news documents deep learning-based usingâ€¦ (Nur Hayatin) \n757 \nğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–+1) = cos (\nğ‘ğ‘œğ‘ \n10000\n2ğ‘–\nğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™\n)       (5) \n \nDropout reduces the loss value during the training process,  also helps prevent overfitting  [24]. The \nnormalization layer normalized values come from the hidden layer. Perform on small batch sizes dependent to \nreduce memory cost, normalization can rely for increase training accuracy  [25]. The normalization layer \nminimize parameter change during propagated through the deep networks  [26]. \n \n \n5. SCORING \nThe summarization result measurements are performed using recall-oriented understanding for gisting \nevaluation (ROUGE) [27]. We chose the ROUGE -N method, which was represented in (6), in which the \ncalculation is based on n-gram recall [27]. Where n is the length of n-gram, Ref is a set of reference summaries. \nğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘šğ‘ğ‘¡ğ‘â„(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›) is the calculation of the maximum number of n -grams co-occurring on the generated \nsummaries model and the set of reference summaries. ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›) is the number of n -grams in reference \nsummaries. \n \nğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ‘ =\n Î£\nğ‘†âˆˆğ‘…ğ‘’ğ‘“\nÎ£\n ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›âˆˆğ‘† ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘šğ‘ğ‘¡ğ‘â„(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›)\n Î£\nğ‘†âˆˆğ‘…ğ‘’ğ‘“\nÎ£\n ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›âˆˆğ‘† ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›)\n     (6) \n \n \n6. EXPERIMENT AND DISCUSSION \nIn accordance with Figure 2, the experiment from all transformer summarization models that we build \nfirst performed preprocessing on the text. The use of preprocessing is to reduce less relevant features, and the \namount of memory needed to carry out the training process  [28]. Then after preprocessing, we perform all \nmodels transformer-based architecture with encoder and decoder that can be done several times repeatedly and \nmake a comparison of layer modifications based on scoring. \n \n \n \n \nFigure 2. The news summarization experiment of COVID-19 \n \n \n6.1. Software and splitting \nThe specifications in the experiment used Google colab cloud computing with data as follows: Intel \nXeon CPU 2.20GHz, 14GB RAM, Tesla P100 16GB GPU. The Python TensorFlow library is used as a deep \nbackend learning where calculations are performed on the GPU. The results of splitting on the COVID-19 news \ndocument dataset were 1928 documents to conduct training, 275 documents as validation during the training \nprocess, and 552 documents to test the results of the training model, for each training model. Validating perform \nunder a batch of 32 documents to calculate the average ROUGE-1 score and loss value. The maximum length \nof the news text content of the entire training document was equal to 600 and 25 in the text description. \n\n     ï²          ISSN: 1693-6930 \nTELKOMNIKA Telecommun Comput El Control, Vol. 19, No. 3, June 2021:  754 - 761 \n758 \n6.2.  Experiment scenario \nIn this research experiment, we use Adam which is the stochastic based optimization method to update \nthe weight value of the loss value measurement results  [29], where the calculation of the loss value of the \nweight value used sparse s oftmax cross-entropy. Table 1 shown the experiment scenario of several different \nparameters used to build the transformer deep learning model. Adam optimization parameters used are  \nbeta1=0.9, beta2=0.98, epsilon=1e -9. All models had carried out fairly ite rations of 40 epochs on training \nexperiments. From the existing transformer design model in previous studies, we chose the transformer C \nmodel (TCM) [14] as a comparison test with some of the models that we proposed. The selection is because \nTCM has the most straightforward design and the results of trials with other architectures that are more complex \nby 1%. The model that we proposed includes tokenization and word embedding, and in the form of parameter \nchanges modification of the encoder -decoder layer, or can be called a modified base model transformer with \ndistribute c ontrol tokenization and GloVe word embedding . The postfix number is a representation of the \nnumber of identical layer encoder decoders (MTDTG Nx). The activation function in the MTDTG model which \nwas used in this research is  the GELU function to calculate the weight of the sequence in the feed -forward \nlayer, whereas in previous studies using ReLU as an activation function. \n \n \nTable 1. Scenario experiment \nParameter TCM [14] Our proposed models \nMTDTG 2 MTDTG 5 MTDTG 6 \nheads 8 10 8 10 \nlearning rate 1e-3 1e-3 1e-5 1e-2 \nnode feed-forward 256 512 256 512 \ndropout rate 0.1 0.2 0.1 0.2 \nattention dropout rate 0.1 0.2 0.1 0.2 \nencoder layer 2 2 5 6 \ndecoder layer 2 2 5 6 \nactivation function ReLU GELU GELU GELU \n \n \n6.3.  Experiment result \nDuring the training process, a loss value and ROUGE-1 is obtained, as shown in Figure 3. TCM has \ndecreased loss in epoch 40, so that a loss value of 6.3 is obtained.  In other models ranging from epoch 18 -27 \nloss in MTDTG 2 gradually decreased at 5.5, then climbed back up because of the variant batch in the document \nvary, so that the model must recognize new data again. Other models as shown in Figure 3 (a), the graph depicts \nthat the MTDTG 5 decreased gradually in loss at epoch 10 and 25 with obtained loss value of 4.8. The latest \nresults of our experiment on MTDTG 6 have decreased a loss value to 4.7 which is not much different compared \nto MTDTG 5 with a difference of 0.1%.  During the training phase, each epoch validation based on maximum \nROUGE-1 score is considered to save the weight model.  This experiment used ROUGE-1 to summarization \nresult measure, the result of ROUGE-1 was shown by Figure 3 (b). The graph explained the validation process \nwhere each epoch TCM has a maximum score 0.20. Our proposed model shown by MTDTG 2, MDTG 5, and \nMTDTG 6, those models have an outperformed result measured by ROUGE -1 with score 0.54, 0.59, and  \n0.60 respectively. \n \n \n \n(a)  (b) \n \nFigure 3. Comparison; (a) model training loss and (b) ROUGE-1 score validation  \n \n \n6.4.  Summarization model \nWe try to explore the results of MTDTG 6, on the word cloud, as seen in Figure 4. Word cloud \ndescribes the  word frequency representation of the whole document from the generated summarization \nconducted by MTDTG 6. Total all unique words while performing summarization appeared in Figure 4 is 200 \n\nTELKOMNIKA Telecommun Comput El Control  ï² \n \nSummarization of COVID-19 news documents deep learning-based usingâ€¦ (Nur Hayatin) \n759 \nwords that used the MTDTG 6 model. Most word generated on summarization is â€˜sayâ€™, this result is caused by \nthe dataset used related to news content to get information from an interview on experts or government officials. \nThe word \"Canadian\" also appeared, because this word related to the news that has been reported mostly on \nCanadian during the coronavirus situation. \nWe classified keyword based on frequent of appearance. On Table 2, there are two types of keyword: \n\"most\" (keywords have a high frequent), and \"least\"  (keywords appeared rarely). The percentage of overall \nword cloud summarization results from Figure 4 is seen the table. The most 5 words that have high an appear \nfrequency are \"say, will, coronavirus, people, pandemic\".  These keywords prove that the main topic of news \nreported widely during this period is about the corona virus pandemic. It caused the go vernment of Canada to \ntake policies focused on public health which concerned with the safety of citizens. The other hand, the fewest \n5 words found: \"families, disease, grocery store, first nation, stock market\". The main thing that had been less \nconcern from report news is that the coronavirus pandemic affected the weak of many sectors, including trading \nand economy, and also less reported about the policy issue to close public places i.e. park and store.   \n \n \n \n \nFigure 4. Word cloud summarization MTDTG 6 \n \n \nTable 2. Most and least word representation \nType Rank Keywords Percentage \nmost 1-5 say, will, coronavirus, people, pandemic 0.83 \n6-10 province, new, case, canadian, coronavirus outbreak 0.56 \n11-15 public health, government, city, canada, said 0.45 \n16-20 day, monday, spread, novel coronavirus, two 0.38 \n21-25 outbreak, china, announced, home, now 0.34 \nleast 196-200 disease, families, masks, major, caused 0.08 \n191-195 death, threat, operations, warning, place 0.08 \n186-190 price, potential, store, nurse, park 0.08 \n181-185 provincial health, care workers, self isolation, slow spread, stock market 0.08 \n 176-180 first nation, grocery store, expert, public, including 0.09 \n \n \n6.5.  Testing model \nThe distribution of ROUGE-1 and ROUGE-2 scores on MTDTG 6 is shown in Figure 5, the number \nof summarization results is at most 80 -100% with a total of 209 documents, while at least 10% is scored with \na total of 33 documents. Inversely pr oportional to ROUGE-2, where the most distribution in the result of the \nscore is in the 0-20% because ROUGE-2 uses the bigram mechanism. So, there are several overlapping words \nbetween summarization and references. The comparison results  test also  shown in  Table 3. The results of \nMTDTG 5 and MTDTG 6 on the ROUGE-1 score have a difference of 1% when used on the ROUGE-2 score. \nThere is quite a difference of 3%. The evaluation results show that MTDTG 6 with a ROUGE -1 score of 0.58 \nand a ROUGE-2 score of 0.42 is the best model that can be used in the test data. \n \n \n \n \nFigure 5. Distribution ROUGE score on test documents \n\n     ï²          ISSN: 1693-6930 \nTELKOMNIKA Telecommun Comput El Control, Vol. 19, No. 3, June 2021:  754 - 761 \n760 \nFrom the results of the entire trial of the test document, there is only MTDTG 6 architecture that can be \nsupplied with the maximum due to the memory limitations handled by the GPU. In the experiment, the researcher \noften got constraints on out of memory ( OOM) resources. This obstacle can be overcome by reducing the \narchitectural design model, especially the most important things, i.e., the number of feed-forward networks, batch \nsize, and the number of encoder-decoders. The disadvantage of MTDTG 6 is that the memory needed to conduct \ntraining is more significant because we use 300-dimensional GloVe as word embedding. However, the results of the \ntest model can increase by a percentage of 13% in ROUGE-1 and 16% ROUGE-2 compared to TCM. \n \n \nTable 3. Overall comparison score model \nModel Validation Test Training time (second) Maximum ROUGE-1 ROUGE-1 ROUGE-2 \nTCM [14] 0.20 0.45 0.26 1723 \nMTDTG 2 0.54 0.51 0.34 5046 \nMTDTG 5 0.59 0.56 0.38 7606 \nMTDTG 6 0.60 0.58 0.42 11438 \n \n \n7. CONCLUSION \nSummarization of news documents COVID-19 based on deep learning using transformer architecture \ncan be done by compiling various models and methods of activation functions. We proposed the transformer \nwith architectural modification as the basis for designi ng the model in abstractive document summarization, \nwhich was evidently effective in improving result performance. The best model MTDTG 6 performs that was \nmeasured using the ROUGE-1, and ROUGE-2 has obtained a good score of 0.58 and 0.42, respectively, wi th \na training time of 11438 seconds. Based on word clouds from all documents with the most discussion, we found \nthat the most reported in news related to the policy and regulation from the government on public health \nservices prioritization during COVID -19 pandemic. Since the research of COVID -19 news document \nabstractive summaries is minimal, many research opportunities can be done further by modifying the encoder \nand decoder layer to get better model quality results, and they also work with faster trainin g time. The \nintegration of several transformer architecture models can also be done, such as the use of the T5 or BART \nmodels to summarize until the quality of existing research can be evaluated or compared.  \n \n \nREFERENCES \n[1] A. A. Salisu and X. V. Vo, â€œPredicting stock returns in the presence of COVID -19 pandemic: The role of health \nnews,â€ Int. Rev. Financ. Anal., vol. 71, 2020. \n[2] A. K. M. N. Islam, S. Laato, S. Talukder, and E. Sutinen, â€œMisinformation sharing  and social media fatigue during \nCOVID-19: An affordance and cognitive load perspective,â€ Technol. Forecast. Soc. Change, vol. 159, 2020. \n[3] A. Khan and N. Salim, â€œA review on abstractive summarization methods,â€ J. Theor. Appl. Inf. Technol. , vol. 59,  \nno. 1, pp. 64-72, 2014. \n[4] M. Marjani, et al., â€œBig IoT Data Analytics: Architecture, Opportunities, and Open Research Challenges,â€ IEEE \nAccess, vol. 5, pp. 5247-5261, 2017. \n[5] T. UÃ§kan and A. KarcÄ±, â€œExtractive multi -document text summarization based on graph independent sets,â€ Egypt. \nInformatics J., vol. 21, no. 3, 2020. \n[6] S. Gupta, et al., â€œAbstractive summarization: An overview of the state of the art,â€ Expert Syst. Appl., vol. 121, pp. 49-65, 2019. \n[7] Y. Huang, Z. Yu, J. Guo, Z. Yu, and Y. Xian, â€œLegal public opinion news abstractive summarization by incorporating \ntopic information,â€ Int. J. Mach. Learn. Cybern., vol. 11, pp. 2039-2050, 2020. \n[8] C. Yuan, Z. Bao, M. Sanderson, and Y. Tang, â€œIncorporating word attention with convolutional neural networks for \nabstractive summarization,â€ World Wide Web, vol. 23, no. 1, pp. 267-287, 2020. \n[9] A. K. Mohammad Masum, et al., â€œAbstractive method of text summarization with sequence to sequence RNNs,â€ \n2019 10th Int. Conf. Comput. Commun. Netw. Technol. ICCCNT 2019, 2019, pp. 1-5. \n[10] P. M. Hanunggul and S. Suyanto, â€œThe Impact of Local Attention in LSTM for Abstractive Text Su mmarization,â€ \n2019 2nd Int. Semin. Res. Inf. Technol. Intell. Syst. ISRITI 2019, pp. 54-57, 2019 \n[11] B. Myagmar, J. Li, and S. Kimura, â€œCross -Domain Sentiment Classification with Bidirectional Contextualized \nTransformer Language Models,â€ IEEE Access, vol. 7, pp. 163219-163230, 2019. \n[12] Y. Chen and H. Li, â€œDAM: Transformer -based relation detection for Question Answering over Knowledge Base,â€ \nKnowledge-Based Syst., vol. 201-202, 2020. \n[13] T. A. Fuad, M. T. Nayeem, A. Mahmud, and Y. Chali, â€œNeural sentence  fusion for diversity driven abstractive  \nmulti-document summarization,â€ Comput. Speech Lang., vol. 58, pp. 216-230, 2019. \n[14] A. Vaswani, et al., â€œAttention is all you need,â€ Adv. Neural Inf. Process. Syst., vol. 2017, pp. 5999-6009, 2017. \n[15] J. Ã. GonzÃ¡lez, L. F. Hurtado, and F. Pla, â€œTransformer based contextualization of pre-trained word embeddings for \nirony detection in Twitter,â€ Inf. Process. Manag., vol. 57, no. 4, 2020. \n[16] J. W. Lin, Y. C. Gao, and R. G. Chang, â€œChinese Story Generation with Fa stText Transformer Network,â€ 1st Int. \nConf. Artif. Intell. Inf. Commun. ICAIIC 2019, pp. 395-398, 2019. \nTELKOMNIKA Telecommun Comput El Control  ï² \n \nSummarization of COVID-19 news documents deep learning-based usingâ€¦ (Nur Hayatin) \n761 \n[17] Y. Iwasaki, A. Yamashita, Y. Konno, and K. Matsubayashi, â€œJapanese abstractive text summarization using BERT,â€ \nProc. - 2019 Int. Conf. Technol. Appl. Artif. Intell. TAAI 2019, 2019. \n[18] Ryan Han, â€œCOVID-19 News Articles Open Research Dataset | Kaggle,â€ 2020. [Online]. Available:  \nhttps://www.kaggle.com/ryanxjhan/cbc-news-coronavirus-articles-march-26 (accessed Jul. 22, 2020). \n[19] L. Ruhwinaningsih and T. Djatna, â€œA Sentiment Knowledge Discovery Model in Twitterâ€™s TV Content Using \nStochastic Gradient Descent Algorithm,â€ TELKOMNIKA Telecommunication Computing Electrical Electronics and \nControl, vol. 14, no. 3, pp. 1067-1076, 2016. \n[20] M. A. Fauzi, R. F. N. Firmansyah, and T. Afirianto, â€œImproving sentiment analysis of short informal Indonesian \nproduct reviews using synonym based feature expansion,â€ TELKOMNIKA Telecommunication Computing Electrical \nElectronics and Control, vol. 16, no. 3, pp. 1345-1350, 2018. \n[21] J. Pennington, R. Socher, and C. D. Manning, â€œGloVe: Global vectors for word represent ation,â€ 2014. [Online]. \nAvailable: https://nlp.stanford.edu/projects/glove/. \n[22] N. Alami, M. Meknassi, and N. En -nahnahi, â€œEnhancing unsupervised neural networks based text summarization \nwith word embedding and ensemble learning,â€ Expert Syst. Appl., vol. 123, pp. 195-211, 2019. \n[23] D. Hendrycks and K. Gimpel, â€œGaussian Error Linear Units (GELUs),â€ Cornell University, pp. 1-9, 2016. [Online]. \nAvailable: http://arxiv.org/abs/1606.08415. \n[24] B. D. Satoto, I. Utoyo, R. Rulaningtyas, and E. B. Khoendori, â€œAn improvement of Gram -negative bacteria \nidentification using convolutional neural network with fine tuning,â€ TELKOMNIKA Telecommunication Computing \nElectrical Electronics and Control, vol. 18, no. 3, pp. 1397-1405, 2020. \n[25] E. Gibson et al., â€œNiftyNet: a deep -learning platform for medical imaging,â€ Comput. Methods Programs Biomed. , \nvol. 158, pp. 113-122, 2018. \n[26] I. Nurhaida, V. Ayumi, D. Fitrianah, R. A. M. Zen, H. Noprisson, and H. Wei, â€œ Implementation of deep neural \nnetworks (DNN) with batch normalization for batik pattern recognition,â€ Int. J. Electr. Comput. Eng., vol. 10, no. 2, \npp. 2045-2053, 2020, doi: 10.11591/ijece.v10i2.pp2045-2053. \n[27] C. Y. Lin, â€œRouge: A package for automatic evaluation of summaries,â€ Proc. Work. text Summ. branches out (WAS \n2004), no. 1, 2004, pp. 25-26. \n[28] S. A. Alsaidi, A. T. Sadiq, and H. S. Abdullah, â€œEnglish poems categorization using text mining and rough set theory,â€ \nBull. Electr. Eng. Informatics, vol. 9, no. 4, pp. 1701-1710, 2020. \n[29] D. P. Kingma and J. L. Ba, â€œAdam: A method for stochastic optimization,â€ 3rd Int. Conf. Learn. Represent. ICLR \n2015 - Conf. Track Proc., 2015, pp. 1-15. \n \n \nBIOGRAPHIES OF AUTHORS \n \n \nNur Hayatin is a lecturer at the University of Muhammadiyah Malang. She received her Master \nin Informatics Engineering from the In stitute of Technology Sepuluh Nop ember Surabaya, \nIndonesia, with an area of interest in data science, where she teaches courses related to Natural \nLanguage Processing. Her main research interest is text analytics, social media analytics, data \nmining, and information retrieval. Email: noorhayatin@umm.ac.id \n  \n \nKharisma Muzaki Ghufron is currently completing a Bachelor's degree from the Informatics \nDepartment, Faculty of Engineering , at the University of Muhammadiyah Malang, Indonesia. \nHis interests include natural language processing and deep learning architecture.  Email: \nkharisma.muzaki@webmail.umm.ac.id  \n  \n \nGalih Wasis Wicaksono  is a lecturer in the Informatics department at the University of \nMuhammadiyah Malang, Indonesia. He teaches logic & computing and computer reasoning \ncourses with an area of interest in data science. The focus of his research is case-based reasoning \nand online learning. Email: galih.w.w@umm.ac.id  \n \n \n",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9158933162689209
    },
    {
      "name": "Computer science",
      "score": 0.8379684686660767
    },
    {
      "name": "Transformer",
      "score": 0.7948999404907227
    },
    {
      "name": "Deep learning",
      "score": 0.747469425201416
    },
    {
      "name": "Encoder",
      "score": 0.6303787231445312
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5993163585662842
    },
    {
      "name": "Architecture",
      "score": 0.515086829662323
    },
    {
      "name": "Natural language processing",
      "score": 0.5110493898391724
    },
    {
      "name": "Language model",
      "score": 0.4632839560508728
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}