{
  "title": "Integrating Transformer and Paraphrase Rules for Sentence Simplification",
  "url": "https://openalex.org/W2963658612",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2158585179",
      "name": "Sanqiang Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151369885",
      "name": "Rui Meng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2629467299",
      "name": "Daqing He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159266267",
      "name": "Andi Saptono",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A182589101",
      "name": "Bambang Parmanto",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250531756",
    "https://openalex.org/W2605243085",
    "https://openalex.org/W4297779254",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2251044566",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2112282043",
    "https://openalex.org/W2508661403",
    "https://openalex.org/W22627370",
    "https://openalex.org/W2115769265",
    "https://openalex.org/W2963792777",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2952136670",
    "https://openalex.org/W1746111881",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2156422881",
    "https://openalex.org/W2009114231",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2109802560",
    "https://openalex.org/W2108373063",
    "https://openalex.org/W2963448850",
    "https://openalex.org/W2534253848"
  ],
  "abstract": "Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from machine translation studies and implicitly learned simplification mapping rules from normal-simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we propose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state-of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/Sanqiang/text_simplification.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3164–3173\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n3164\nIntegrating Transformer and Paraphrase Rules\nfor Sentence Simpliﬁcation\nSanqiang Zhao†, Rui Meng†, Daqing He†, Saptono Andi∗, Parmanto Bambang∗\n†Department of Informatics and Networked Systems , School of Computing and Information\n∗Department of Health Information Management, School of Health and Rehabilitation Sciences\nUniversity of Pittsburgh\nPittsburgh, PA, 15213\n{sanqiang.zhao, rui.meng, daqing, ans38, parmanto}@pitt.edu\nAbstract\nSentence simpliﬁcation aims to reduce the\ncomplexity of a sentence while retaining its\noriginal meaning. Current models for sen-\ntence simpliﬁcation adopted ideas from ma-\nchine translation studies and implicitly learned\nsimpliﬁcation mapping rules from normal-\nsimple sentence pairs. In this paper, we ex-\nplore a novel model based on a multi-layer and\nmulti-head attention architecture and we pro-\npose two innovative approaches to integrate\nthe Simple PPDB (A Paraphrase Database\nfor Simpliﬁcation), an external paraphrase\nknowledge base for simpliﬁcation that cov-\ners a wide range of real-world simpliﬁcation\nrules. The experiments show that the inte-\ngration provides two major beneﬁts: (1) the\nintegrated model outperforms multiple state-\nof-the-art baseline models for sentence sim-\npliﬁcation in the literature (2) through anal-\nysis of the rule utilization, the model seeks\nto select more accurate simpliﬁcation rules.\nThe code and models used in the paper\nare available at https://github.com/\nSanqiang/text_simplification.\n1 Introduction\nSentence simpliﬁcation aims to reduce the com-\nplexity of a sentence while retaining its original\nmeaning. It can beneﬁt individuals with low-\nliteracy skills (Watanabe et al., 2009) including\nchildren, non-native speakers and individuals with\nlanguage impairments such as dyslexia (Rello\net al., 2013), aphasic (Carroll et al., 1999).\nMost of the previous studies tackled this task\nin a way similar to machine translation (Xu et al.,\n2015a; Zhang and Lapata, 2017), in which models\nare trained on a large number of pairs of sentences,\neach consisting of a normal sentence and a simpli-\nﬁed sentence. Statistical and neural network mod-\neling are two major methods used for this task.\nThe statistical models have the beneﬁt of easily in-\ntegrating with human-curated rules and features,\nthus they generally perform well even they are\ntrained with a limited number of data. In con-\ntrast, neural network models could learn the sim-\nplifying rules automatically without the need for\nfeature engineering, but at the cost of requiring a\nhuge amount of training data. Even though models\nbased on neural networks have outperformed the\nstatistical methods in multiple Natural Language\nProcessing (NLP) tasks, their performance in sen-\ntence simpliﬁcation is still inferior to that of statis-\ntical models (Xu et al., 2015a; Zhang and Lapata,\n2017). We speculate that current training datasets\nmay not be large and broad enough to cover com-\nmon simpliﬁcation situations. However, human-\ncreated resources do exist which can provide abun-\ndant knowledge for simpliﬁcation. This motivates\nus to investigate if it is possible to train neural net-\nwork models with these types of resources.\nAnother limitation to using existing neural net-\nwork models for sentence simpliﬁcation is that\nthey are only able to capture frequent transforma-\ntions; they have difﬁculty in learning rules that\nare not frequently observed despite their signiﬁ-\ncance. This may be due to nature of neural net-\nworks (Feng et al., 2017): during training, a neu-\nral network tunes its parameters to learn how to\nsimplify different aspects of the sentence, which\nmeans that all the simpliﬁcation rules are actu-\nally contained in the shared parameters. There-\nfore, if one simpliﬁcation rule appears more fre-\nquently than others, the model will be trained to\nbe more focused on it than the infrequent ones.\nMeanwhile, models tend to treat infrequent rules\nas noise if they are merely trained using sentence\npairs. If we can leverage an additional memory\ncomponent to maintain simpliﬁcation rules indi-\nvidually, it would prevent the model from forget-\nting low-frequency rules as well as help it to dis-\ntinguish real rules from noise. Therefore, we pro-\npose the Deep Memory Augmented Sentence Sim-\npliﬁcation (DMASS) model. For comparison pur-\n3165\npose, we also introduce another approach, Deep\nCritic Sentence Simpliﬁcation (DCSS) model, to\nencourage applying the less frequently occurring\nrules by revising the loss function. It this way,\nsimpliﬁcation rules are encouraged to maintained\ninternally in the shared parameters while avoiding\nthe consumption of an unwieldy amount of addi-\ntional memory.\nIn this study, we propose two improvements\nto the neural network models for sentence sim-\npliﬁcation. For the ﬁrst improvement, we pro-\npose to use a multi-layer, multi-head attention ar-\nchitecture (Vaswani et al., 2017). Compared to\nRNN/LSTM (Recurrent Neural Network / Long\nShort-term Memory), the multi-layer, multi-head\nattention model would be able to selectively\nchoose the correct words in the normal sentence\nand simplify them more accurately.\nSecondly, we propose two new approaches\nto integrate neural networks with human-curated\nsimpliﬁcation rules. Note that previous studies\nrarely tried to incorporate explicit human lan-\nguage knowledge into the encoder-decoder model.\nOur ﬁrst approach, DMASS, maintains additional\nmemory to recognize the context and output of\neach simpliﬁcation rules. Our second approach,\nDCSS, follows a more traditional approach to en-\ncode the context and output of each simpliﬁcation\nrules into the shared parameters.\nOur empirical study demonstrates that our\nmodel outperforms all the previous sentence sim-\npliﬁcation models. They achieve both a good cov-\nerage of rules to be applied (recall) and a high ac-\ncuracy gained by applying the correct rules (preci-\nsion).\n2 Related Work\nSentence Simpliﬁcation For statistical model-\ning, Zhu et al. (2010) proposed a tree-based sen-\ntence simpliﬁcation model drawing inspiration\nfrom statistical machine translation. Woodsend\nand Lapata (2011) employed quasi-synchronous\ngrammar and integer programming to score the\nsimpliﬁcation rules. Wubben et al. (2012) pro-\nposed a two-stage model PBMT-R, where a stan-\ndard phrase-based machine translation (PBMT)\nmodel was trained on normal-simple aligned sen-\ntence pairs, and several best generations from\nPBMT were re-ranked based how dissimilar they\nwere to a normal sentence. Hybrid, a model pro-\nposed by Narayan and Gardent (2014) was also a\ntwo-stage model combining a deep semantic anal-\nysis and machine translation framework. SBMT-\nSARI (Xu et al., 2016) achieved state-of-the-art\nperformance by employing an external knowledge\nbase to promote simpliﬁcation. In terms of neu-\nral network models, Zhang and Lapata (2017) ar-\ngued that the RNN/LSTM model generated sen-\ntences but it does not have the capability to sim-\nplify them. They proposed DRESS and DRESS-\nLS that employ reinforcement learning to reward\nsimpler outputs. As they indicated, the perfor-\nmance is still inferior due to the lack of external\nknowledge. Our proposed model is designed to\naddress the deﬁciency of current neural network\nmodels which are not able to integrate an external\nknowledge base.\nAugmented Dynamic Memory Despite posi-\ntive results obtained so far, a particular problem\nwith the neural network approach is that it has\na tendency towards favoring to frequent observa-\ntions but overlooking special cases that are not fre-\nquently observed. This weakness with regard to\ninfrequent cases has been noticed by a number of\nresearchers who propose an augmented dynamic\nmemory for multiple applications, such as lan-\nguage models (Daniluk et al., 2017; Grave et al.,\n2016), question answering (Miller et al., 2016),\nand machine translation (Feng et al., 2017; Tu\net al., 2017). We ﬁnd that current sentence sim-\npliﬁcation models suffer from a similar neglect of\ninfrequent simpliﬁcation rules, which inspires us\nto explore augmented dynamic memory.\n3 Our Sentence Simpliﬁcation Models\n3.1 Multi-Layer, Multi-Head Attention\nOur basic neural network-based sentence simpliﬁ-\ncation model utilizes a multi-layer and multi-head\nattention architecture (Vaswani et al., 2017). As\nshown in Figure 1, our model based on the Trans-\nformer architecture works as follows: given a pair\nconsisting a normal sentence I and a simple sen-\ntence O, the model learns the mapping from I to\nO.\nThe encoder part of the model (see the left part\nof Figure 1) encodes the normal sentence with a\nstack of Lidentical layers. Each layer has two sub-\nlayers: one layer is for multi-head self-attention\nand the other one is a fully connected feed-forward\nneural network for transformation. The multi-head\nself-attention layer encodes the output from the\n3166\nFigure 1: Diagram of the Transformer architecture\nprevious layer into hidden state e(s,l) (step s and\nlayer l) as shown in Equation 1, whereαenc\n(s′,l) indi-\ncates the attention distribution over the step s′and\nlayer l. Each hidden state summarizes the hidden\nstates in the previous layer through the multi-head\nattention function a() (Vaswani et al., 2017) where\nH refers to the number of heads.\nThe right part of Figure 1 denotes the decoder\nfor generating the simpliﬁed sentence. The de-\ncoder also consists of a stack ofLidentical layers.\nIn addition to the same two sub-layers as those\nin the encoder part, the decoder also inserts an-\nother multi-head attention layer aiming to attend\non the encoder outputs. The bottom multi-head\nself-attention plays the same role as the one in the\nencoder, where the hidden state d(s,l) is computed\nin the Equation 2. The upper multi-head attention\nlayer is used to seek relevant information from en-\ncoder outputs. Through the same mechanism, con-\ntext vector c(s,l) (step sand layer l) is computed in\nthe Equation 3.\ne(s,l) =\n∑\ns′\nαenc\n(s′,l)e(s′,l−1), α enc\n(s′,l) =a(e(s,l),e(s′,l−1),H)1\n(1)\nd(s,l) =\n∑\ns′\nαdec\n(s′,l)d(s′,l−1), α dec\n(s′,l) =a(d(s,l),c(s′,l−1),H)2\n(2)\nc(s,l) =\n∑\ns′\nαdec2\n(s′,l)e(s′,L), α dec2\n(s′,l) =a(d(s,l),e(s′,L),H)\n(3)\nThe model is trained to minimize the negative\nlog-likelihood of the simple sentence, Lseq =\n−logP(O|I,θ) where θrepresents all the param-\neters in the current model.\n3.2 Integrating with Simple PPDB\nA previous study (Xu et al., 2016) has demon-\nstrated the beneﬁts of using an external knowledge\nbase in conjunction with a statistical simpliﬁcation\nmodel. However, as far as we know, no efforts\nhave been made to integrate neural network mod-\nels with the knowledge base, and our study is the\nﬁrst to meet this goal.\nWeight Type Rule\n0.99623 [VP] recipient →have receive\n0.75530 [NN] recipient →winner\n0.58694 [NN] recipient →receiver\n0.46935 [NN] recipient →host\nTable 1: Examples from the Simple PPDB\nSimple PPDB (Pavlick and Callison-Burch,\n2016) refers to a paraphrase knowledge base for\nsimpliﬁcation. It is a reﬁned version of another\nknowledge, PPDB (Ganitkevitch et al., 2013),\nwhich was originally designed to support para-\nphrase. Simple PPDB contains 4.5 million para-\nphrase rules, each of which provides the mapping\nfrom a normal phrase to a simpliﬁed phrase, the\nsyntactic type of the normal phrase, and the sim-\npliﬁcation weight. Table 1 shows four examples,\nwhere “recipient” can be simpliﬁed to “winner”\nwith a weight 0.75530 if “recipient” is a singular\nnoun (NN).\n3.2.1 Deep Critic Sentence Simpliﬁcation\nModel (DCSS)\nThe Simple PPDB offers guidance about whether\na word needs to be simpliﬁed and how it should\n1The lowest hidden state e(:,0) is the word embedding.\n2The lowest context vector c(:,0) is the word embedding.\n3167\nbe simpliﬁed. The Deep Critic Sentence Simpli-\nﬁcation (DCSS) model is designed to apply rules\nidentiﬁed by the Simple PPDB by introducing a\nnew loss function. Different from the standard loss\nfunction that minimizes the distance away from\nthe ground truth, the new loss function aims to\nmaximize the likelihood of applying simpliﬁcation\nrules. It also reweights the probability of generat-\ning each word by its simpliﬁcation weight in order\nto relieve the problem of overlooking infrequent\nsimpliﬁcation rules.\nFor example, given a normal sentence in the\ntraining set, “the recipient of the kate greenaway\nmedal”, the simpliﬁed sentence is “the winner of\nthe kate greenaway medal.”, where “recipient” is\nsimpliﬁed to “winner”, which is identiﬁed by Sim-\nple PPDB. The major goal of the loss functions is\nto support the model in generating the simpliﬁed\nword “winner” while deterring the model from\ngenerating the word “recipient”. Speciﬁcally, for\nan applicable simpliﬁcation rule, our new loss\nfunction maximizes the probability of generating\nthe simpliﬁed form (word “winner”) and mean-\nwhile minimizes the probability of generating the\noriginal form (word “recipient”). As in Equation\n4, where wrule indicates the weight of the simpli-\nﬁcation rule provided by the Simple PPDB, once\nthe model generates “recipient”, the model is criti-\ncized to generate word “winner”; when model pre-\ndicts correctly with “winner”, the model is trained\nto minimize the probability of “recipient”. In this\nway, the model avoids selecting normal words and\ninstead becomes inclined to choose the simpliﬁed\nwords.\nLcritic =\n\n\n\n−wrulelogP(winner|I,θ)\nif model generates recipient\nwrulelogP(recipient|I,θ)\nif model generates winner\n(4)\nThe Lcritic merely focuses on the words identi-\nﬁed by the Simple PPDB and Lseq focuses on the\nentire vocabulary. So, the model is trained in an\nend-to-end fashion by minimizingLseq and Lcritic\nalternately.\n3.2.2 Deep Memory Augmented Sentence\nSimpliﬁcation Model (DMASS)\nDCSS, similar to the majority of neural network\nmodels, uses a piece of shared memory, i.e. the\nparameters, as the media to store the learned rules\nfrom the data. As a result, it still focuses much\nmore on rules that are frequently observed and ig-\nnores the rules observed infrequently. However,\ninfrequent rules are still important, particularly\nwhen the training data is limited.\nIn order to make full use of the rules in the\nknowledge base, we introduce the Deep Memory\nAugmented Sentence Simpliﬁcation (DMASS)\nmodel. DMASS has an augmented dynamic mem-\nory to record multiple key-value pairs for each rule\nin the Simple PPDB. The key vector stores a con-\ntext vector that is computed based on the weighted\naverage of encoder hidden states and the current\ndecoder hidden states. The value vector stores the\noutput vector.\nOur DMASS model is illustrated in Figure 2.\nGiven the same example normal sentence “ the re-\ncipient of kate greenaway medal”, Simple PPDB\ndetermines that the word “recipient” should be\nsimpliﬁed to “winner”. The encoder represents the\nnormal sentence as a list of hidden states, [ e(1,L),\ne(2,L), ...] where L indicates the ﬁnal layer of\nencoder hidden states. When predicting the next\nword in the simpliﬁed sentence, the decoder of\nlayer j represents the previous words as hidden\nstates [d(1,j), ... ]. c(1,j) refers to the current con-\ntext vector following attention layer, which is the\nweighted average of [ e(1,L), e(2,L), ...] based on\nd(1,j). A feed-forward fully connected neural net-\nwork (FFN) combines the output of the decoder\nand the output from memory read module into the\nﬁnal output rwinner. In addition to the word pre-\ndiction, c(1,j) and rwinner will be sent to memory\nupdate module.\nIn the remainder of this section, we will in-\ntroduce the two modules of DMASS mentioned\nabove: Memory Read Module and Memory Up-\ndate Module.\nMemory Read Module The memory read mod-\nule incorporates rules into prediction. As shown\nin Figure 2, current augmented memory contains\nthree candidate rules for the word “recipient”,\nwhich indicates that it can be simpliﬁed into “win-\nner”, “receiver” or “host”, respectively. The cur-\nrent context vector c(1,j) is treated as a query\nto search for suitable rules by using Equation 5,\nwhere αr\ni denotes the weight for ith rule, which is\ncomputed through the dot product between current\ncontext vector c(1,j) and ci. Then using Equation\n6, αr\ni weights each output vector to generate mem-\n3168\nFigure 2: Diagram of DMASS Model\nory read output.\nαr\ni = ei∑\nj ej\nei = exp(c(1,j) ·ci) (5)\nro =\n∑\nαr\ni rr rr ∈[rwinner,rreceiver,rhost] (6)\nMemory Update Module The task of the mem-\nory update module is to update the key and value\nvectors in the augmented memory. Once the\nmodel predicts the output vector rwinner, both\nrwinner and the current context vectorc1,j are sent\nto the memory update module. If the augmented\nmemory does not contain the key-value pair for the\nrule, c1,j and rwinner are appended to the memory.\nIf the augmented memory contains the key-value\npair, the key vector is updated as the mean of cur-\nrent key vector andc1,j. Similarly, the value vector\nis also updated as the mean of current value vector\nand rwinner.\n4 Experiments\nDataset We utilize the datasetWikiLarge(Zhang\nand Lapata, 2017) for training. It is the largest\nWikipedia corpus, constructed by merging previ-\nously created simpliﬁcation corpora. Speciﬁcally,\nthe training dataset contains 296,402 normal-\nsimple sentence pairs gathered from (Zhu et al.,\n2010; Woodsend and Lapata, 2011; Kauchak,\n2013). For validation and testing, we use the\ndataset Turk created by (Xu et al., 2016). In this\ndataset, eight simpliﬁed reference sentences for\neach normal sentence are used as the ground-truth,\nall of which are generated by Amazon Mechani-\ncal Turk workers. The Turk dataset contains 2,000\ndata samples for validation and 356 samples for\ntesting. We consider the Turk to be the most reli-\nable data set because (1) it is human-generated and\n(2) it contains multiple simpliﬁcation references\nfor each normal sentence due to the existence of\nmultiple equally good simpliﬁcations of each sen-\ntence. We also include the second test setNewsela,\na corpus introduced by (Xu et al., 2015b) who ar-\ngue that only using normal-simple sentence pairs\nfrom Wikipedia is suboptimal due to the automatic\nsentence alignment which unavoidably introduces\nerrors, and the uniform writing style which leads\nto systems that generalize poorly. The test set\ncontains 1,419 normal-simple sentence pairs3. To\ndemonstrate that our models are able to perform\nwell on a different style of corpus, we report the\nresults of Newsela test set by using the models\ntrained/tuned on Turk dataset. Following Zhang\nand Lapata (2017)’s way, we tag and anonymize\nname entities with a special token in the format of\nNE@N, where NE includes{PER,LOC,ORG }\nand N indicates the Nth distinct NE type of entity.\nWe also replace those tokens occurring three times\nor less in the training set with a mark “UNK” as\nmentioned in (Zhang and Lapata, 2017).\nEvaluation Metrics We report the results of\nthe experiment with two metrics that are widely\nused in the literature: SARI (Xu et al., 2016)\nand FKGL (Kincaid et al., 1975). FKGL com-\nputes the sentence length and word length as a\nway to measure the simplicity of a sentence. The\nlower value of FKGL indicates simpler sentence.\nFKGL measures the simplicity of a sentence with-\nout considering the ground truth simpliﬁcation ref-\nerences and it correlates little with human judg-\nment (Xu et al., 2016), so we also use another\nmetric, SARI. SARI, which stands for “System\noutput Against References and against the normal\nsentence”, computes the arithmetic mean of N-\ngrams (N includes 1,2,3 and 4) F1-score of three\nrewrite operations: addition, deletion, and keep-\ning. Speciﬁcally, it rewards addition operations\nwhere a word in the generated simpliﬁed sentence\ndoes not appear in the normal sentence but is men-\ntioned in the reference sentences. It also rewards\nwords kept or deleted in both the simpliﬁed sen-\ntence and the reference sentences. In our experi-\nment, we also present the F1-score of three rewrite\n3Because the earlier publications don’t provide pre-\nprocess details, we use our own script to pre-process the arti-\ncles into sentence pairs.\n3169\noperations: addition, deletion, and keeping. Xu\net al. (2016) demonstrated that SARI correlates\nmost closely to human judgments in sentence sim-\npliﬁcation tasks. Thus, we treated SARI as the\nmost important measurement in our study.\nBecause SARI rewards deleting and adding sep-\narately, we also include another metric to measure\nthe correctness of lexical transformation, namely\nword simpliﬁcation, veriﬁed by Simple PPDB. By\ncomparing the normal sentence and ground truth\nsimpliﬁed references, we collect rules that are cor-\nrect to be used for simplifying each normal sen-\ntence. Then we calculate the precision, recall,\nand F1 score for using the correct rules. As a re-\nsult, the recall expresses the coverage of rules to\nbe applied, and the precision implies the accuracy\ngained by applying the correct rules.\nTraining Details We initialized the encoder and\ndecoder word embedding lookup matrices with\n300-dimensional Glove vectors(Pennington et al.,\n2014). The word embedding dimensionality and\nthe number of hidden units are set to 300. During\nthe training, we regularize all layers with a dropout\nrate of 0.2 (Srivastava et al., 2014). For multi-\nlayer and multi-head architecture, 4 encoder and\ndecoder layers (set L as 4) and 5 multi-attention\nheads (set H as 5) are used. We will discuss\nthe trade-off between different layers and differ-\nent heads in Sections 4.1. For DMASS, we use\nthe context vector based on the ﬁrst layer of the\ndecoder (set j as 1). For optimization, we use Ada-\ngrad (Duchi et al., 2011) with the learning rate set\nto 0.1. The gradient is truncated by 4 (Pascanu\net al., 2013).\n4.1 Impacts of Multi-Layer, Multi-Head\nAttention Architecture\nThe reason to employ the Transformer architec-\nture in the sentence simpliﬁcation task is that we\nbelieve that its multi-layer, multi-head attention\nprovides a better capability of modeling both the\noverall context and the important cues for sentence\nsimpliﬁcation. In this section, we examine the\napplicability of multi-layer, multi-head attention\narchitecture to the sentence simpliﬁcation task.\nWe compare our results against the RNN/LSTM-\nbased sentence simpliﬁcation models. Note that\nthe results of our models presented here have not\nbeen integrated with the Simple PPDB.\nTable 2 shows the experiment results where\nLxHy indicates a run with Transformer using x\nlayers and y heads. When compared with results\nof RNN/LSTM, our Transformer-based model\nperformed better in terms of SARI and FKGL val-\nues. In addition, with the increased number of lay-\ners or heads, the values of SARI and FKGL im-\nprove accordingly. In the remainder of this sec-\ntion, we analyze the insights of these results in de-\ntail.\nIn our tasks, FKGL measures the sentence\nlength and the word length as two factors for\nevaluating a simpliﬁed sentence. Therefore, we\ninclude Wlen(Word Length) and Slen(Sentence\nLength) into our analysis. As shown in Ta-\nble 2, models with higher numbers of layers and/or\nheads do generally reduce the average word length\nand the average sentence length, which indicates\nthat the higher number of layers and/or heads in\nthe model leads to simpler outcomes.\nIt has been found that SARI correlates most\nclosely to human judgment (Xu et al., 2016). To\nfurther analyze the effects of SARI, we study the\nimpacts of three rewrite operations in SARI: add,\ndelete, and keep. As shown in Table 2, we ﬁnd\nthat the improvement mostly results from cor-\nrectly adding simpliﬁed words and deleting nor-\nmal words, but not from keeping words. By an-\nalyzing the outputs, the increased number of lay-\ners or heads results in better capability to simplify\nthe words. Speciﬁcally, models with the greater\nnumber of layers or heads tend to remove the nor-\nmal words and add simpliﬁed words. However,\nthey may introduce inaccurate simpliﬁed words,\nthereby driving down the F1 score for keeping\nwords. We believe the Simple PPDB, which offers\nguidance about whether words need to be simpli-\nﬁed and how they should be simpliﬁed, provides\nan ideal method to alleviate this issue.\n4.2 Impacts of Integrating the Simple PPDB\nIn order to make comprehensive comparisons with\nthe state-of-the-art models, we include multiple\nbaselines from the literature, including PBMT-\nR (Wubben et al., 2012), Hybrid (Narayan and\nGardent, 2014), and SBMT-SARI (Xu et al.,\n2016). We also include several strong baselines\nbased on neural networks such as RNN/LSTM,\nDRESS, DRESS-LS (Zhang and Lapata, 2017) as\nshown in Tables 3 and 4 We developed three mod-\nels for this experiment. They are DMASS, DCSS,\nand DMASS+DCSS, where DMASS+DCSS indi-\ncates the combination of DMASS and DCSS. The\n3170\nModel FKGL Factors in FKGL SARI F1 for operations in FKGL\nWLen SLen Add Delete Keep\nRNN/LSTM 8.67 1.34 21.68 35.66 3.00 28.95 75.03\nTransformer (L1H5) 8.59 1.34 21.39 35.88 2.69 30.46 74.50\nTransformer (L2H5) 8.11 1.33 20.52 36.88 3.48 33.26 73.91\nTransformer (L3H5) 7.71 1.32 19.77 38.02 4.14 37.41 72.51\nTransformer (L4H1) 7.49 1.31 19.41 37.88 4.05 37.35 72.23\nTransformer (L4H2) 7.40 1.31 19.19 38.35 4.58 39.77 70.70\nTransformer (L4H5) 7.22 1.30 19.00 38.84 4.78 41.19 70.53\nTable 2: Comparison of transformers with different layers and heads of attention on Turk dataset\nModel FKGL Factors in FKGL SARI F1 for operations of SARI Rule Utilization\nWLen SLen Add Delete Keep Prec Recall F1\nPBMT-R 8.35 1.30 22.08 38.56 5.73 36.93 73.02 14.60 22.29 15.01\nHybrid 4.71 1.28 13.38 31.40 5.49 45.48 46.86 10.62 7.61 7.62\nSBMT-SARI 7.49 1.18 23.50 39.96 5.97 41.43 72.51 13.30 28.96 15.77\nRNN/LSTM 8.67 1.34 21.68 35.66 3.00 28.95 75.03 13.67 14.83 11.65\nDRESS 6.80 1.34 16.55 37.08 2.94 43.14 65.16 13.06 12.50 10.77\nDRESS-LS 6.92 1.35 16.76 37.27 2.82 42.21 66.78 12.40 11.36 9.83\nDMASS 7.41 1.29 20.00 39.81 5.04 41.94 72.46 17.97 25.54 18.12\nDCSS 7.34 1.31 19.30 39.26 5.29 41.24 71.26 13.14 21.30 13.87\nDMASS+DCSS 7.18 1.27 20.10 40.42 5.48 45.55 70.22 16.25 30.42 18.98\nDMASSbeam=4 8.20 1.30 21.66 39.16 4.90 38.41 74.18 18.53 25.46 18.40\nDCSSbeam=4 7.97 1.32 20.56 39.11 5.10 38.87 73.36 14.36 20.96 14.48\nDMASS+DCSSbeam=4 7.93 1.28 21.49 40.34 5.73 42.55 72.74 18.55 31.56 20.81\nDMASSbeam=8 8.23 1.30 21.68 39.15 4.95 37.80 74.69 18.44 25.34 18.32\nDCSSbeam=8 7.97 1.32 20.56 39.11 5.10 38.87 73.36 14.37 20.96 14.80\nDMASS+DCSSbeam=8 8.04 1.29 21.64 40.45 5.72 42.23 73.41 19.46 31.99 21.51\nTable 3: Performance of baselines and proposed models on the Turk dataset.\nModel FKGL Factors in FKGL SARI F1 for operations of SARI Rule Utilization\nWLen SLen Add Delete Keep Prec Recall F1\nRNN/LSTM 6.09 1.22 18.67 21.09 11.10 38.78 13.39 12.62 22.63 14.68\nDRESS 4.96 1.23 15.27 25.70 10.65 52.59 13.86 12.56 17.88 13.28\nDRESS-LS 5.07 1.24 15.47 24.91 11.21 49.74 13.76 12.61 17.50 13.42\nDMASS 5.38 1.20 17.47 25.41 11.88 50.39 13.97 16.32 34.79 20.00\nDCSS 5.64 1.22 17.58 24.31 13.52 45.60 13.81 15.20 30.38 18.39\nDMASS+DCSS 5.17 1.18 17.60 27.28 11.56 56.10 14.19 15.98 40.64 20.98\nDMASSbeam=4 5.64 1.21 17.79 24.09 13.96 44.47 13.85 17.40 35.97 21.37\nDCSSbeam=4 5.80 1.22 17.85 23.28 15.28 40.76 13.81 16.77 31.81 20.06\nDMASS+DCSSbeam=4 5.42 1.19 17.81 26.39 13.92 51.13 14.13 18.71 43.36 24.23\nDMASSbeam=8 5.68 1.21 17.83 23.95 14.25 43.74 13.86 17.69 36.37 21.74\nDCSSbeam=8 5.77 1.22 17.76 23.18 15.65 40.08 13.82 17.18 32.18 20.50\nDMASS+DCSSbeam=8 5.43 1.19 17.83 26.29 14.08 50.62 14.17 18.89 43.54 24.47\nTable 4: Performance of baselines and proposed models on the Newsela dataset.\nsubscript beamindicates the size of beam search.\nResults with FKGL Metric As shown in Tables\n3 and 4, Hybrid achieves the lowest (thus the best)\nFKGL score, and DRESS and DRESS-LS have the\nsecond best FKGL scores. All the other models in-\ncluding ours do not perform as well as these two.\nBut FKGL measures the simplicity of a sentence\nwithout considering the ground truth simpliﬁca-\ntion references, so high FKGL may be at the cost\nof losing information and readability.\nTo further analyze the FKGL results, we exam-\nine the average sentence length and word length of\nthe outcomes of the models and they are listed as\nWLen (Word Length) and SLen (Sentence Length)\nin Tables 3 and 4. Hybrid, DRESS, and DRESS-\nLS are good at generating shorter sentences, but\nthey are not as good at choosing shorter words.\nIn contrast, SBMT-SARI, DCSS, and DMASS all\ngenerate shorter words. Therefore, we believe\nthat, by optimizing language model as a goal for\nthe reinforcement learning, DRESS and DRESS-\nLS are tuned to simplify sentences by shortening\n3171\nthe sentence lengths. In contrast, with the help\nof an integrated external knowledge base, SBMT-\nSARI and our models have more capability to gen-\nerate shorter words in order to simplify sentences.\nTherefore, these two sets of models complete sen-\ntence simpliﬁcation tasks via different routes, and\nperhaps there should be an exploration of combin-\ning these two routes for even more successful sen-\ntence simpliﬁcation.\nAnother interesting ﬁnding is that the larger\nbeam search size increases average word length\nslightly. This is because the larger beam search\nsize mitigates the issue of the inaccurate simpliﬁ-\ncation so that fewer words are simpliﬁed. To mea-\nsure the correctness of simpliﬁcation, we analyze\nthe SARI metric and Rule Utilization.\nResults with SARI Metric SARI is the most\nreliable metric for the sentence simpliﬁcation\ntask (Xu et al., 2016), therefore we would like\nto present more detailed discussion regarding the\nSARI results. As shown in Tables 3 and 4,\nDMASS+DCSS achieves the best SARI score,\nwhich demonstrates the effectiveness of integrat-\ning the knowledge base Simple PPDB for sentence\nsimpliﬁcation.\nTo further examine the impacts of the F1\nscores for three operations in calculating the\nSARI scores, as shown in Tables 3 and 4,\nDMASS+DCSS, as well as other models with\nhigh SARI performance beneﬁt greatly by cor-\nrectly adding and deleting words. We believe\nthese beneﬁts mostly result from the integration\nwith the knowledge base, which provides reliable\nguidance about which words to modify. SBMT-\nSARI, which represents a previous state-of-the-art\nmodel that also integrates with knowledge bases,\nperforms best in correctly adding new words but\nperforms inferiorly in deleting/keeping words. By\nanalyzing the outputs, SBMT-SARI acts aggres-\nsively to simplify as many words as possible. But\nit also results in incorrect simpliﬁcation. DRESS\nand DRESS-LS are inclined to generate the shorter\nsentence, which leads to high F1 scores for delet-\ning words, but it lags behind other models in\nadding/keeping words.\nDMASS leverages an additional memory com-\nponent to maintain the simpliﬁcation rules; DCSS\nuses internal memory to store those rules. A\nlarge number of simpliﬁcation rules might con-\nfuse the model with limited internal memory. This\nmight be the reason why DMASS works better\nthan DCSS. By taking a two-way advantage of\nboth models, DMASS+DCSS takes a two-ﬁsted\napproach to store the simpliﬁcation rules in both\nadditional and internal memory. As a result,\nDMASS+DCSS achieves the best performance in\nSARI.\nResults with Rule Utilization In this section,\nwe evaluate the models’ capabilities for word\ntransformation. The majority of previous ap-\nproaches, except for the SBMT-SARI, perform\npoorly in recall. We believe the knowledge base\nSimple PPDB will reduce uncertainty in the word\nselection.\nAs before, SBMT-SARI acts aggressively to\nsimplify every word in the sentence. Such an\naggressive action leads to relatively high perfor-\nmance in recall. However, it does not achieve\na strong performance in precision. DMASS per-\nforms better in terms of rule utilization as com-\npared to DCSS by leveraging an additional mem-\nory. DMASS+DCSS takes advantage of both ap-\nproaches that store the simpliﬁcation rules in addi-\ntional and internal memory. This combined model\nis guaranteed to apply more accurate rules.\nAs compared to the loose relationship between\nSARI and beam search size, we ﬁnd that that beam\nsearch size correlates strongly with the perfor-\nmance in rule utilization. Thus, we believe larger\nbeam search size contributes to good coverage of\nrules to be applied as well as accuracy in applying\nrules.\n5 Conclusion\nIn this paper, we propose two innovative ap-\nproaches for sentence simpliﬁcation based on neu-\nral networks. Both approaches are based on multi-\nlayer and multi-head attention architecture and in-\ntegrated with the Simple PPDB, an external sen-\ntence simpliﬁcation knowledge base, in differ-\nent ways. By conducting a set of experiments,\nwe demonstrate that the proposed models per-\nform better than existing methods and achieve new\nstate-of-the-art in sentence simpliﬁcation. Our ex-\nperiments ﬁrstly prove that the multi-layer and\nmulti-head attention architecture has an excellent\ncapability to understand the text by accurately se-\nlecting speciﬁc words in a normal sentence and\nthen choosing right simpliﬁed words. Secondly,\nby integrating with the knowledge base, our mod-\nels outperform multiple state-of-the-art baselines\nfor sentence simpliﬁcation. Compared to previous\n3172\nmodels which integrated with the knowledge base,\nour models, especially, DMASS+DCSS, provide\nboth good coverage of rules to be applied and ac-\ncuracy in applying the correct rules. In future, we\nwould like to investigate deeper into the different\neffects of additional memory and internal memory.\n6 Acknowledge\nThis research was supported in part by the Uni-\nversity of Pittsburgh Center for Research Com-\nputing through the resources provided. The re-\nsearch is funded in part by grants the National In-\nstitute on Disability, Independent Living, and Re-\nhabilitation Research (NIDILRR) #90RE5018 and\n#90DP0064, and by Pittsburgh Health Data Al-\nliance’s “CARE” Project.\nReferences\nJohn A Carroll, Guido Minnen, Darren Pearce, Yvonne\nCanning, Siobhan Devlin, and John Tait. 1999. Sim-\nplifying text for language-impaired readers. In\nEACL, pages 269–270.\nMichał Daniluk, Tim Rockt ¨aschel, Johannes Welbl,\nand Sebastian Riedel. 2017. Frustratingly short at-\ntention spans in neural language modeling. arXiv\npreprint arXiv:1702.04521.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine\nLearning Research, 12(Jul):2121–2159.\nYang Feng, Shiyue Zhang, Andi Zhang, Dong\nWang, and Andrew Abel. 2017. Memory-\naugmented neural machine translation. arXiv\npreprint arXiv:1708.02005.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. Ppdb: The paraphrase\ndatabase. In HLT-NAACL, pages 758–764.\nEdouard Grave, Armand Joulin, and Nicolas\nUsunier. 2016. Improving neural language\nmodels with a continuous cache. arXiv preprint\narXiv:1612.04426.\nDavid Kauchak. 2013. Improving text simpliﬁcation\nlanguage modeling using unsimpliﬁed text data. In\nACL (1), pages 1537–1546.\nJ Peter Kincaid, Robert P Fishburne Jr, Richard L\nRogers, and Brad S Chissom. 1975. Derivation of\nnew readability formulas (automated readability in-\ndex, fog count and ﬂesch reading ease formula) for\nnavy enlisted personnel. Technical report, Naval\nTechnical Training Command Millington TN Re-\nsearch Branch.\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason We-\nston. 2016. Key-value memory networks for\ndirectly reading documents. arXiv preprint\narXiv:1606.03126.\nShashi Narayan and Claire Gardent. 2014. Hybrid sim-\npliﬁcation using deep semantics and machine trans-\nlation. In the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 435–445.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural\nnetworks. In International Conference on Machine\nLearning, pages 1310–1318.\nEllie Pavlick and Chris Callison-Burch. 2016. Sim-\nple ppdb: A paraphrase database for simpliﬁcation.\nIn The 54th Annual Meeting of the Association for\nComputational Linguistics, page 143.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nLuz Rello, Clara Bayarri, Azuki G `orriz, Ricardo\nBaeza-Yates, Saurabh Gupta, Gaurang Kanvinde,\nHoracio Saggion, Stefan Bott, Roberto Carlini, and\nVasile Topac. 2013. Dyswebxia 2.0!: more acces-\nsible text for people with dyslexia. In Proceedings\nof the 10th International Cross-Disciplinary Confer-\nence on Web Accessibility, page 25. ACM.\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. Journal of machine learning re-\nsearch, 15(1):1929–1958.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong\nZhang. 2017. Learning to remember translation\nhistory with a continuous cache. arXiv preprint\narXiv:1711.09367.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nWillian Massami Watanabe, Arnaldo Candido Junior,\nVin´ıcius Rodriguez Uzˆeda, Renata Pontin de Mat-\ntos Fortes, Thiago Alexandre Salgueiro Pardo, and\nSandra Maria Alu ´ısio. 2009. Facilita: reading as-\nsistance for low-literacy readers. In Proceedings of\nthe 27th ACM international conference on Design of\ncommunication, pages 29–36. ACM.\nKristian Woodsend and Mirella Lapata. 2011. Learn-\ning to simplify sentences with quasi-synchronous\ngrammar and integer programming. In Proceedings\nof the conference on empirical methods in natural\nlanguage processing, pages 409–420. Association\nfor Computational Linguistics.\n3173\nSander Wubben, Antal Van Den Bosch, and Emiel\nKrahmer. 2012. Sentence simpliﬁcation by mono-\nlingual machine translation. In Proceedings of the\n50th Annual Meeting of the Association for Compu-\ntational Linguistics: Long Papers-Volume 1, pages\n1015–1024. Association for Computational Linguis-\ntics.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015a. Show, attend and tell:\nNeural image caption generation with visual at-\ntention. In International Conference on Machine\nLearning, pages 2048–2057.\nWei Xu, Chris Callison-Burch, and Courtney Napoles.\n2015b. Problems in current text simpliﬁcation re-\nsearch: New data can help. Transactions of the\nAssociation of Computational Linguistics, 3(1):283–\n297.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze\nChen, and Chris Callison-Burch. 2016. Optimizing\nstatistical machine translation for text simpliﬁcation.\nTransactions of the Association for Computational\nLinguistics, 4:401–415.\nXingxing Zhang and Mirella Lapata. 2017. Sen-\ntence simpliﬁcation with deep reinforcement learn-\ning. arXiv preprint arXiv:1703.10931.\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych.\n2010. A monolingual tree-based translation model\nfor sentence simpliﬁcation. In Proceedings of the\n23rd international conference on computational lin-\nguistics, pages 1353–1361. Association for Compu-\ntational Linguistics.",
  "topic": "Paraphrase",
  "concepts": [
    {
      "name": "Paraphrase",
      "score": 0.9435049295425415
    },
    {
      "name": "Computer science",
      "score": 0.8287453651428223
    },
    {
      "name": "Sentence",
      "score": 0.8110910654067993
    },
    {
      "name": "Natural language processing",
      "score": 0.6379698514938354
    },
    {
      "name": "Machine translation",
      "score": 0.5774455666542053
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5556527972221375
    },
    {
      "name": "Transformer",
      "score": 0.5451452732086182
    },
    {
      "name": "Text simplification",
      "score": 0.4637935757637024
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4178745150566101
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}