{
  "title": "Automation of Systematic Reviews with Large Language Models",
  "url": "https://openalex.org/W4411259709",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2407547513",
      "name": "Christian Cao",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2001240559",
      "name": "Rohit Arora",
      "affiliations": [
        "University of Calgary",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5118137444",
      "name": "Paul Cento",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A3032688004",
      "name": "Katherine Manta",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A4365250414",
      "name": "Elina Farahani",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Matthew Cecere",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A3215316777",
      "name": "Anabel Selemon",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A3081690906",
      "name": "Jason Sang",
      "affiliations": [
        null
      ]
    },
    {
      "id": null,
      "name": "Ling Xi Gong",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2082387095",
      "name": "Robert Kloosterman",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Scott Jiang",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A5113237493",
      "name": "Richard Saleh",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Denis Margalik",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2107706604",
      "name": "James Lin",
      "affiliations": [
        "Moscow Institute of Thermal Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3024151441",
      "name": "Jane Jomy",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2514537360",
      "name": "Jerry Xie",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2102071759",
      "name": "David Chen",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2942246607",
      "name": "Jaswanth Gorla",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2097332976",
      "name": "Sylvia Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2313381360",
      "name": "Kelvin Zhang",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2125513122",
      "name": "Harriet Ware",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2409836335",
      "name": "Mairead Whelan",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2970267332",
      "name": "Bijan Teja",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2720175348",
      "name": "Alexander A. Leung",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2920912142",
      "name": "Lina Ghosn",
      "affiliations": [
        "Sorbonne Paris Cité",
        "Université Sorbonne Paris Nord"
      ]
    },
    {
      "id": "https://openalex.org/A2682774646",
      "name": "Rahul K. Arora",
      "affiliations": [
        "University of Calgary",
        "Harvard University"
      ]
    },
    {
      "id": null,
      "name": "Allen S. Detsky",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2528797979",
      "name": "Michael Noetel",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2130257262",
      "name": "David B Emerson",
      "affiliations": [
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A108308545",
      "name": "Isabelle Boutron",
      "affiliations": [
        "Sorbonne Paris Cité"
      ]
    },
    {
      "id": "https://openalex.org/A204361474",
      "name": "David Moher",
      "affiliations": [
        "Ottawa Hospital Research Institute",
        "Ottawa Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2394567673",
      "name": "George-Church",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A1975261093",
      "name": "Niklas Bobrovitz",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2407547513",
      "name": "Christian Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001240559",
      "name": "Rohit Arora",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5118137444",
      "name": "Paul Cento",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3032688004",
      "name": "Katherine Manta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4365250414",
      "name": "Elina Farahani",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Matthew Cecere",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3215316777",
      "name": "Anabel Selemon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3081690906",
      "name": "Jason Sang",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ling Xi Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2082387095",
      "name": "Robert Kloosterman",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Scott Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5113237493",
      "name": "Richard Saleh",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Denis Margalik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107706604",
      "name": "James Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3024151441",
      "name": "Jane Jomy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2514537360",
      "name": "Jerry Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102071759",
      "name": "David Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2942246607",
      "name": "Jaswanth Gorla",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097332976",
      "name": "Sylvia Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2313381360",
      "name": "Kelvin Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125513122",
      "name": "Harriet Ware",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2409836335",
      "name": "Mairead Whelan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2970267332",
      "name": "Bijan Teja",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2720175348",
      "name": "Alexander A. Leung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2920912142",
      "name": "Lina Ghosn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2682774646",
      "name": "Rahul K. Arora",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2528797979",
      "name": "Michael Noetel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130257262",
      "name": "David B Emerson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A108308545",
      "name": "Isabelle Boutron",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A204361474",
      "name": "David Moher",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2394567673",
      "name": "George-Church",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975261093",
      "name": "Niklas Bobrovitz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2593758073",
    "https://openalex.org/W2969341057",
    "https://openalex.org/W141798963",
    "https://openalex.org/W4395050309",
    "https://openalex.org/W4403778417",
    "https://openalex.org/W4407873728",
    "https://openalex.org/W4407009455",
    "https://openalex.org/W4392343921",
    "https://openalex.org/W2978795816",
    "https://openalex.org/W4378190436",
    "https://openalex.org/W2012944180",
    "https://openalex.org/W2048775918",
    "https://openalex.org/W1980640998",
    "https://openalex.org/W4387666266",
    "https://openalex.org/W4280532161",
    "https://openalex.org/W2961552777",
    "https://openalex.org/W4389285887",
    "https://openalex.org/W2520714999",
    "https://openalex.org/W2770117783",
    "https://openalex.org/W2928947675",
    "https://openalex.org/W3011914336",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W4401715425",
    "https://openalex.org/W3114128166",
    "https://openalex.org/W2949074827",
    "https://openalex.org/W2966150926",
    "https://openalex.org/W3173470851",
    "https://openalex.org/W4317356311",
    "https://openalex.org/W4388578725",
    "https://openalex.org/W4282922351",
    "https://openalex.org/W4308706077",
    "https://openalex.org/W4243639737",
    "https://openalex.org/W4288692554",
    "https://openalex.org/W3115280303",
    "https://openalex.org/W4406924296"
  ],
  "abstract": "Abstract Systematic reviews (SRs) inform evidence-based decision making. Yet, they take over a year to complete, are prone to human error, and face challenges with reproducibility; limiting access to timely and reliable information. We developed otto-SR , an end-to-end agentic workflow using large language models (LLMs) to support and automate the SR workflow from initial search to analysis. We found that otto-SR outperformed traditional dual human workflows in SR screening ( otto-SR : 96.7% sensitivity, 97.9% specificity; human: 81.7% sensitivity, 98.1% specificity) and data extraction ( otto-SR : 93.1% accuracy; human: 79.7% accuracy). Using otto-SR , we reproduced and updated an entire issue of Cochrane reviews (n=12) in two days, representing approximately 12 work-years of traditional systematic review work. Across Cochrane reviews, otto-SR incorrectly excluded a median of 0 studies (IQR 0 to 0.25), and found a median of 2.0 (IQR 1 to 6.5) eligible studies likely missed by the original authors. Meta-analyses revealed that otto-SR generated newly statistically significant findings in 2 reviews and negated significance in 1 review. These findings demonstrate that LLMs can rapidly conduct and update systematic reviews with superhuman performance, laying the foundation for automated, scalable, and reliable evidence synthesis.",
  "full_text": "Automation of Systematic Reviews\nwith Large Language Models\nChristian Cao1†, Rohit Arora2†, Paul Cento3†, Katherine Manta1, Elina Farahani1, Matthew Cecere1, Anabel\nSelemon4, Jason Sang3, Ling Xi Gong2, Robert Kloosterman1, Scott Jiang5, Richard Saleh1,\nDenis Margalik1, James Lin6, Jane Jomy1, Jerry Xie7, David Chen1, Jaswanth Gorla1, Sylvia Lee8,\nKelvin Zhang7, Harriet Ware9, Mairead Whelan9, Bijan Teja1,10, Alexander A. Leung 9, Lina Ghosn11,12,13,\nRahul K. Arora9, Michael Noetel14, David B. Emerson15, Isabelle Boutron11,12,13,\nDavid Moher1,16,17, George Church2, Niklas Bobrovitz9\n1University of Toronto; 2Harvard Medical School; 3Independent Researcher; 4McGill University; 5University of British Columbia;\n6Massachusetts Institute of Technology; 7University of Waterloo; 8Mount Sinai Hospital; 9University of Calgary; 10St. Michael’s Hospital;\n11Université Paris Cité; 12Université Sorbonne Paris Nord; 13Cochrane France; 14The University of Queensland; 15Vector Institute;\n16Ottawa Hospital Research Institute; 17University of Ottawa.\nAbstract\nSystematic reviews (SRs) inform evidence-based decision making. Yet, they take over a year to complete, are\nprone to human error, and face challenges with reproducibility; limiting access to timely and reliable information.\nWe developed otto-SR, an end-to-end agentic workflow using large language models (LLMs) to support and\nautomate the SR workflow from initial search to analysis. We found that otto-SR outperformed traditional dual\nhuman workflows in SR screening (otto-SR: 96.7% sensitivity, 97.9% specificity; human: 81.7% sensitivity, 98.1%\nspecificity) and data extraction (otto-SR: 93.1% accuracy; human: 79.7% accuracy). Using otto-SR, we reproduced\nand updated an entire issue of Cochrane reviews (n=12) in two days, representing approximately 12 work-years\nof traditional systematic review work. Across Cochrane reviews, otto-SR incorrectly excluded a median of 0\nstudies (IQR 0 to 0.25), and found a median of 2.0 (IQR 1 to 6.5) eligible studies likely missed by the original\nauthors. Meta-analyses revealed that otto-SR generated newly statistically significant conclusions in 2 reviews\nand negated significance in 1 review. These findings demonstrate that LLMs can autonomously conduct and\nupdate systematic reviews with superhuman performance, laying the foundation for automated, scalable, and\nreliable evidence synthesis.\n1 Introduction\nSystematic reviews (SRs) are the foundation of evidence-based decision-making. However, SRs are incredibly\nresource-intensive, typically taking over 16 months and costing upwards of $100,000 to complete 1,2. Delays\nin completing SRs can have major consequences for evidence-based practice, including prolonged use of\nineffective or harmful treatments initially supported by less rigorous evidence 3.\nWhile several tools have been developed to accelerate SRs 4,5, none are capable of full automation with\nhuman-level accuracy. However, large language models (LLMs) offer new avenues to achieve automation with\ntheir ability to process and reason about natural language. We previously demonstrated that LLMs can achieve\nhigh screening performance6. Other recent work has demonstrated promise for LLMs in data extraction 7,8,\nthough these studies rely on self-defined reference standards and evaluate on small datasets.\nWe introduce an LLM-based workflow (otto-SR) to support automated and human-in-the-loop SR workflows,\nfrom initial search to data analysis. Our framework uses GPT-4.1 (OpenAI) for screening articles and o3-\nmini-high (OpenAI) for data extraction, targeting tasks that typically consume the majority of researcher time\nand effort. We evaluate our workflow on these core SR components, article screening and data extraction,\nwith performance comparisons to traditional human workflows and other SR automation tools. To assess\nreal-world utility, we reproduced and updated an entire issue of Cochrane reviews (n=12) using otto-SR, in\nunder two days. otto-SR is designed to work alongside researchers, requiring only a protocol (objectives,\neligibility criteria), search results, and defined extraction variables.\n† Equal contribution and correspondence to: christian.cao@mail.utoronto.ca; rohitarora@g.harvard.edu; paul.cento@gmail.com\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nFigure 1: An automated systematic review workflow using LLMs. Infographic displaying the end-to-end SR\nprocess for humans (grey) and otto-SR (green).\n2 An agentic workflow for systematic review automation\nThe gold-standard systematic review workflow begins with a comprehensive search to capture all potentially\nrelevant citations9. These citations undergo abstract and full-text screening by two human reviewers indepen-\ndently, with disagreements resolved by a third reviewer. The final set of relevant articles then undergo data\nextraction by two human reviewers independently, again adjudicated by a third reviewer when discrepancies\narise. The complete human workflow is illustrated in Figure 1 (top).\notto-SR is an end-to-end LLM-based workflow supporting both fully automated and human-in-the-loop\nsystematic reviews. Citations identified from the original search are directly uploaded, in RIS format, to\nthe otto-SR screening agent, which uses GPT-4.1 to screen abstract and full-text articles as a standalone\nreviewer. The resulting set of included articles is then fed into the otto-SR extraction agent, which performs\ndata extraction with the o3-mini-high model. For full-text screening and data extraction, retrieved PDFs are\nprocessed by Gemini 2.0 flash and converted into structured Markdown (MD) files for downstream tasks. An\noverview of the otto-SR workflow is provided in Figure 1 (bottom).\n3 LLMs achieve state-of-the-art SR screening performance\nWe previously found that GPT4-preview could achieve high screening performance with effective prompting\nstrategies6. Aiming to improve on these findings, we developed a screening agent leveraging GPT-4.1, a model\nwhich excels at instruction following10,11, paired with optimized prompting strategies 6, to screen articles at\nabstract and full-text stages. The agent was prompted using the original, unaltered objectives and eligibility\ncriteria from each respective review (Supplementary Notes). Full-text article PDFs were converted into\nmarkdown format with the Gemini 2.0 Flash model for full-text screening.\nWe evaluated the performance of the otto-SR screening agent on the complete original search across five\nreviews (n=32,357 citations) covering four Oxford Centre for Evidence-Based Medicine (CEBM) question types:\nprevalence, diagnostic test accuracy, prognosis, intervention benefits (Extended Data Table 1). Dual human\nreviewers and Elicit (a commercial LLM-based SR automation software) were evaluated against a random\nrepresentative sample of records for each review (n=1,767 citations) (Methods). The reference standard for\ninclusion/exclusion decisions was based on the original authors’ final decisions after full-text screening.\nTo validate the proficiency of our human reviewers in screening, we conducted a calibration exercise (n=400\ncitations) where we compared the SR screening performance of our reviewers to the original study authors 12,\nwho had independently re-screened the same set of articles. We found that the performance of our human\n2\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nFigure 2: otto-SR screening agent (GPT-4.1) achieves superhuman screening sensitivity, specificity, and\naccuracy. A. Diagram of otto-SR abstract screening agent (left), sensitivity, specificity of otto-SR screening agent,\ndual human reviewers, and Elicit, for abstract screening evaluated across five reviews (middle). Weighted\naverages for sensitivity and specificity across comparator groups (right). Error bars indicate 95% confidence\nintervals. B. diagram of otto-SR full-text screening agent (left), sensitivity, specificity, and accuracy of otto-SR\nscreening agent evaluated across five reviews, and dual human reviewers for full-text screening evaluated\nacross five reviews (middle). Weighted average for sensitivity and specificity across otto-SR (five reviews) and\ndual human (four reviews) (right). Error bars indicate 95% confidence intervals.\nreviewers closely aligned with the original study authors (Our team: 80.2% sensitivity 97.7% specificity\nvs. original author team: 81.3% sensitivity, 98.1% specificity) providing confidence that our reviewers were\nreflective of expert-level screening (Extended Data Table 2).\nAt the abstract screening stage, the otto-SR screening agent achieved the highest sensitivity (weighted\nsensitivity 96.6% [total range, 94.1-100.0%]) (Fig. 2, Extended Data Table 3). In comparison, Elicit (88.5%\n[76.9-100%] sensitivity) and dual human reviewers (87.3% [84.1-100%] sensitivity) had lower sensitivity. Dual\nhuman reviewers achieved the highest specificity in abstract screening (95.7% [92.5-98.7%] specificity), followed\nby the otto-SR screening agent (93.9% [83.6-97.7%] specificity) and Elicit (84.2% [65.7-95.9%] specificity).\nAfter full-text screening, the otto-SR screening agent maintained the highest sensitivity (96.2% [92.3-100%]\nsensitivity), while human reviewers had a marked drop in sensitivity (63.3% [44.1-93.8%] sensitivity) (Fig. 2,\nExtended Data Table 4). This decline was largely driven by poor performance on screening the “Reinfection”\nreview (44.1% sensitivity, 95.3% specificity), likely due to complex inclusion criteria involving test-negative\nstudy designs, multiple interventions, and multiple time-specific outcomes. After removing this outlier review,\nhuman reviewers achieved a weighted sensitivity of 81.7% [76.4%-93.8%]. Specificity remained high for both\nthe otto-SR screening agent (96.9% [90.7-98.7%] specificity) and dual human reviewers (98.1% [96.7-100.0%]\nspecificity). Elicit was not included in this comparison as it did not support full-text screening.\n3\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nTogether, these findings suggest that the otto-SR screening agent can capture more relevant studies (true\npositives) than traditional dual human screening, while maintaining comparable specificity (minimizing false\ninclusions).\n4 LLMs achieve state-of-the-art SR data extraction performance\nGiven the time-intensive nature of manual data extraction in SRs, we explored if advances in LLM reasoning\ncould provide a path towards automation. To this end, we developed an extraction agent using the OpenAI o3-\nmini-high model13, selected for its strong scientific reasoning, robust long-context retrieval, and cost-efficiency.\nIn all cases, the otto-SR extraction agent was prompted with original author-defined variable descriptions.\nFull-text article PDFs were also converted into markdown format with the Gemini 2.0 Flash model for data\nextraction.\nWe evaluated the performance of the otto-SR extraction agent and Elicit in data extraction across seven\nreviews (n=4,559 data points, 495 studies) (Fig. 3A, Extended Data Table 5). Dual human reviewers were\nassessed on a randomly sampled subset of articles from each review based on a McNemar test sample size\napproximation (n=1,453 data points, 156 studies) (Methods). Extracted variables included key descriptive and\noutcome data used by the original authors for downstream analysis (see Supplementary Notes).\nData extraction accuracy was determined through an LLM-as-a-judge framework to compare AI- or human\nextracted values against the original author extractions (Methods). However, given the known variability\nin dual human data extraction accuracy (reported rates: 65.8-85.5%) 14–19, original author-extracted values\nwere not treated as a definitive gold standard (Fig. 3B). Instead, we applied a blinded adjudication process\nto resolve discrepancies between otto-SR extraction and the original Cochrane authors. A panel of blinded\nFigure 3: otto-SR extraction agent (o3-mini-high) performance on systematic review data extraction. A. Bar\ngraph displaying data extraction accuracy of the otto-SR extraction agent (green) (4,459 data points), Elicit (teal)\n(4,459 data points), and dual human reviewers (grey) (1,453 data points) across 7 different systematic reviews.\nError bars represent 95% confidence intervals. Shading represents pre- (lighter) and post- (darker) human\nadjudicated correction. B. Dot plot depicting literature-derived human reviewer performance comparison\nagainst human reviewers in this study. Dots represent mean value and upper and lower bars represent range.\nC. Bar graph depicting dual human adjudicator decisions for values marked as incongruent between original\nreview and otto-SR extraction agent, Elicit, and dual human. Blue represents the newly conducted review being\ncorrect, while tan represents the original study authors being correct. Error bars represent 95% confidence\nintervals.\n4\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nhuman reviewers compared randomized pairs of responses (otto-SR vs. original author) and selected the\nmost accurate value (see Methods). We used these judgements to construct a corrected gold standard for\nperformance evaluations.\nAcross all seven reviews, the otto-SR extraction agent achieved an average weighted accuracy of 93.1%\n(91.1-97.0%), outperforming both dual human reviewers at 79.7% (69.1-91.0%), and Elicit at 74.8% (58.8-83.1%)\n(Fig. 3A, Extended Data Figure 6). When otto-SR extracted different values to the original authors, the blinded\nhuman reviewer panel sided with the otto-SR data extraction agent in 69.3% of cases (Fig. 3C). In contrast, for\ndiscrepancies between original authors and our two human extractors or Elicit, the blinded reviewer panel\nsided with the dual human extractors in 28.1% of cases, and Elicit in 22.4% of cases (Fig. 3C).\nIn the 6.9% of cases where the otto-SR extraction agent was incorrect, post-hoc analysis revealed that 0.83%\n(39/4459) of data points were inaccessible to the model (supplementary files or data obtained through data\nrequest), 0.67% (30/4459) resulted from parsing errors, and 0.49% (22/4459) were cases where neither the\notto-SR data extraction agent nor original author extraction was correct (Extended Data Figure 1).\n5 An agentic workflow of LLMs can rapidly reproduce and update\nreviews\nGiven the high performance of our screening and extraction agents, we combined them into an agentic\nworkflow, dubbed otto-SR (Fig. 4A). To evaluate the real-world applicability of otto-SR, we conducted a\nreproducibility assessment of a complete issue of SRs published in the Cochrane Database of Systematic\nReviews.\nWe randomly selected the April 2024 issue of the Cochrane Database (Extended Data Table 7). Of the 14\nreviews in this issue, one review was excluded due to a lack of publicly available data, and a second review was\nexcluded due to the absence of a reproducible search strategy (Extended Data Table 7). For the 12 remaining\nreviews, we reproduced their reported search strategies, updating searches to May 8, 2025, and identified\n146,276 citations. These citations were deduplicated and then screened at both the abstract and full-text stages\nusing the otto-SR screening agent with original Cochrane review eligibility criteria (Supplementary Notes).\nTo ensure a focused and interpretable comparison, we diverged from Cochrane methodology in one key\nrespect. Cochrane reviews typically include all studies, regardless of whether they report the review’s\nprimary outcome, to allow for all comparisons based on the available data (e.g., all intervention and outcome\ncombinations). In contrast, we focused our analysis to reproduce each review’s predefined primary outcome.\nThis constraint provided a clearer distinction for study eligibility.\nThe otto-SR screening agent correctly identified all included studies (n=64) across the 12 Cochrane reviews.\nCitations passing screening then had primary outcome data extracted using the otto-SR extraction agent and\noriginal Cochrane study variable definitions (Supplementary Notes). otto-SR extraction results with missing\nprimary outcome values, duplicate studies, or missing intervention-comparator groups were programmatically\nexcluded (Methods). After this process, otto-SR incorrectly excluded a median of 0 articles (IQR 0 to 0.25)\n(Extended Data Table 8). Incorrect exclusions were due to LLM-inaccessible supplementary data (n=2), or a\nfailure to extract reported outcome values when present (n=2).\nAfter filtering our results to align with the original search cutoffs, we identified 54 additional eligible studies\nthrough otto-SR (median 2, IQR: 1 to 6.25 per review) that were likely missed in the original Cochrane reviews\n(Methods). otto-SR also incorrectly included 10 false positive articles after human review; however 9/10 may\nhave contained relevant data available through additional author correspondence. Updating the search to May\n8, 2025 identified another 14 new eligible studies (total n = 64, median 2.5, IQR 1 to 7.25 per review) (Extended\nData Table 8). The updated search identified two additional false positive studies, one of which may have\ncontained relevant data.\nExtracted data was subsequently meta-analyzed using the same analytical methods as the original reviews,\nacross three comparisons: (1) ‘Matched’ where otto-SR was restricted to the same set of articles as included in\nthe original Cochrane analysis. (2) ‘Expanded’ which included all eligible studies identified by otto-SR, filtered\nto the original search cutoff date. (3) ‘Update’ which evaluated all articles with an updated May 8, 2025 search\ncutoff.\nGiven potential data extraction errors by original Cochrane authors and otto-SR, we derived corrected values\nfor each comparison through dual human review. This also included removal of false positive articles and\n5\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nFigure 4: Evaluating otto-SR for automation of systematic reviews. A. Infographic depicting use of otto-SR\nfor systematic review automation in a complete edition of the Cochrane Database of Systematic Reviews (n =\n12). B. Forest plots depicting differences between otto-SR (green), original Cochrane study authors (purple),\nand corrected standard (gold). Each row is representative of meta-analyzed estimates derived in a systematic\nreview. Error bars represent 95% confidence interval, MD = Mean Difference, OR = Odds Ratio, RR = Risk\nRatio, SMD = Standardized Mean Difference. The matched comparison (left) shows estimates derived from\narticles only included in the original Cochrane reviews. The expanded comparison (middle) displays estimates\nderived from additional articles identified by otto-SR falling within the original search dates. The update plot\n(right) displays estimates derived from all articles found by otto-SR in a May 8 2025 search. *otto-SR discovered\na new treatment group, mixed oral / enteral nutrition, which was not found in the original Cochrane review,\nconsequently no matched analysis was conducted. **workplace citations were provided by original study\nauthors due to challenges with the electronic search, consequently no updated search was performed.\n6\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \naddition of false negative articles. For each review, we also generated corresponding Cochrane meta-analyses\nusing the original author-extracted data. All original Cochrane data, otto-SR extracted data, and corrected data\n(including notes) are provided in Supplementary Data 1.\nIn the ‘Matched’ comparison group, otto-SR produced meta-analyzed effect estimates which had overlapping\n95% CIs with both the original Cochrane data and corrected datasets across all reviews (Fig. 4B, left; Extended\nData Table 9). In the ‘Expanded’ analysis, two reviews (nutrition, depression) yielded new statistically\nsignificant effect estimates (Fig. 4B, middle), while the estimate from one review (alcohol) lost statistical\nsignificance compared to the original Cochrane estimates (Fig. 4B, middle). These trends were consistent in\nthe corrected ‘Expanded’, otto-SR ‘updated’, and the corrected ‘Update’ analyses (Fig. 4B, right).\nOne illustrative example comes from the nutrition review, where otto-SR identified 5 additional studies.\nThis led to the new finding that preoperative immune-enhancing supplementation before gastric surgery is\nassociated with a one-day reduction in mean hospital stay compared to usual care (otto-SR: MD -1.20 [95% CI\n-2.28 to -0.11], 9 studies; Cochrane: MD -0.19 [-1.44 to 1.07], 4 studies). Detailed effect estimates and 95% CIs\nare provided for all groups and comparisons are provided in Extended Data Table 9.\n6 Discussion\nSystematic review workflows are often hindered by the time- and labor-intensive demands of screening and\ndata extraction. In this study, we demonstrate that otto-SR, an end-to-end SR automation pipeline, powered by\nGPT 4.1 and o3-mini-high, can accelerate these steps without compromising performance.\nOur findings highlight several opportunities for LLMs to be implemented in systematic reviews. First,\nworkflows like otto-SR can be used to update existing systematic reviews by leveraging their original published\nprotocols. This provides a unique advantage: it enables direct comparison of screening and extraction results\nagainst the original review, facilitating validation and assessments of reproducibility. Second, the ability to\nrapidly process articles opens the door to truly living systematic reviews–where updates could be performed\nmonthly, weekly, or even daily–ensuring constant access to the most current evidence. Third, otto-SR may\nbe used to generate de novo reviews, provided that researchers develop clear, detailed protocols akin to\nthose pre-registered in PROSPERO. In all cases, structured and clear methodology is essential for ensuring\ninterpretability, reproducibility, and high-quality automation.\nOur Cochrane reproducibility assessment highlighted common reproducibility challenges. All 12 reviews\nhad issues with search reproducibility and 2 reviews lacked methodological clarity. These findings align\nwith prior work by Rethlefsen et al20, who found that only 1% of reviews have a fully reproducible search\nstrategy. Previous studies have also shown that reproducibility failures can occur at every stage of the SR\nprocess20–24. To address these challenges, we suggest that published SRs include the following : (i) the complete\nsearch strategy; (ii) raw search files (e.g., RIS file); (iii) raw data extraction outputs with data dictionaries;\n(iv) list of data procured via author correspondence; and (v) the complete code used for analyses. Current\nreporting guidelines for systematic reviews (PRISMA) endorse most, but not all of these points 25. While\nCochrane reviews routinely provide such materials, most other SRs do not, limiting reproducibility and\nexternal validation.\nThe performance of LLMs in conducting evidence synthesis, as demonstrated in this study, also highlights\nan opportunity to reconsider how scientific content is published. While most research is written for human\nreaders, the rise of LLM-supported evidence curation highlights the value of making studies machine-readable.\nUsing structured formats like markdown or html (as now offered by Arxiv), and providing raw numerical\ndata from figures could support this new paradigm.\nOur work has several limitations. First, although our analysis was validated across a wide range of SRs,\nfurther research is needed to assess the generalizability to other clinical questions and qualitative reviews.\nSecond, our LLM parser may have incorrectly extracted information from PDF articles. In future studies,\nenhanced vision capabilities of new models may better support screening/extraction efforts. Thirdly, our\nworkflow was limited to the main text of articles and did not extract data from supplementary tables or figures.\nWhile we could have manually incorporated these materials, we intentionally avoided human intervention to\ntest the end-to-end capabilities of our automated workflow. Fourth, due to the vast number of data points (and\ntheir unstandardized nature), we employed an LLM-as-a-judge framework to assess data extraction accuracy.\nAlthough this approach has been previously validated 26,27, LLM judgements may still introduce occasional\nerrors. Fifth, we encountered instances where the original author’s decisions for data extraction appeared\n7\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \ninaccurate. However, we addressed this by performing randomized and blinded adjudication, adopting best\npractices seen in radiology, ophthalmology, and clinical trials28–31. This approach represented a methodological\nimprovement over prior work that relied on self-created ground truths 7. Finally, with respect to the Cochrane\nreproducibility assessment, it’s possible that content specific experts would make different article inclusion\nand data extraction decisions. However, we closely adhered to each review’s protocol, contacted study authors\nto clarify methodological uncertainties, and documented discrepancies.\n7 Conclusion\nIn conclusion, our study marks a major advancement in the development of SR automation tools using LLMs.\nThe immediate applications of this include: rapid updates and truly ‘living’ reviews, mass assessments of\nreproducibility across the SR literature, and faster de novoreviews. Future research should focus on developing\ncomprehensive and complete benchmarks of SRs to better support and refine automation efforts. We also\nencourage research into the capabilities of LLMs for other SR workflow tasks, such as search term generation\nand risk of bias assessment. The implementation of fully autonomous SRs could accelerate the synthesis of\nup-to-date evidence, save thousands of hours of manual work, and provide significant benefits in medicine\nand other fields.\n8 Methods\n8.1 Article Screening Datasets\nTo identify putative screening datasets, we leveraged the previously published BenchSR database of published\nSRs6. In brief, this consisted of 10 distinct SRs spanning nine unique clinical domains and contained study\ninformation (review objectives, inclusion/exclusion criteria) and the complete set of labeled ‘included’ and\n‘excluded’ citations from the original search of each SR.\nFrom the BenchSR database, we performed stratified random sampling across the four Oxford CEBM review\nquestions, selecting SRs for each type. Our sample included various datasets: the SeroTracker dataset 32\nfor reviews of prevalence (calibration set adapted from Perlman-Arrow et al.12), the Reinfection 33 and PA-\nOutcomes34 datasets for reviews of intervention benefits, the PA-Testing35 dataset for reviews of diagnostic\ntest accuracy, and SVCF36 dataset for reviews of prognosis (Extended Data Table 1).\n8.2 LLM Screening Methodology\nWe developed a novel LLM-based screening system adapted from our previously validated ScreenPrompt\napproach6. Our LLM based screening agent uses the GPT-4.1 model with a 32,768-token output limit and\ndefault parameters (temperature=1, frequency_penalty=0, presence_penalty=0, top_p=1).\nFor full-text screening, we implemented a PDF parsing pipeline using the Gemini 2.0 Flash model to convert\nfull-text documents into structured Markdown inputs given a simple prompt (Supplementary Methods),\nwhich was then processed by GPT 4.1 for full-text screening. Full-text PDF articles were programmatically\nretrieved through OpenAlex37, a comprehensive corpus of over 240 million scholarly works sourced from open\nplatforms such as Crossref, MAG, DataCite, HAL, PubMed, Institutional Repositories. Articles that were not\navailable through OpenAlex but were included after abstract screening were retrieved via institutional access.\nIn all instances, GPT-4.1 was prompted using the original, unaltered objectives and eligibility criteria from\neach respective review (Supplementary Notes).\n8.3 Article Screening Benchmarking\nWe evaluated screening performance using a diagnostic test accuracy (DTA) study design. Our reference\nstandard was the final article inclusion or exclusion decisions of the original review authors after full-text\nscreening. “Included” articles represented the final set of articles included in each review, and “excluded”\narticles represented articles excluded from title, abstract, and full-text screening in each review.\nWe tested the otto-SR screening agent as a standalone reviewer across the full set of citations retrieved in\neach SR (n = 32,357) (Extended Data Table 3, 4). Screening was conducted in two stages: first, an abstract-level\n8\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nscreen was applied to all citations; those marked as “included” then underwent full-text screening to determine\nthe final set of included articles.\nFor comparison, we assessed the performance of Elicit (evaluated on April 12, 2025), a commercially available\nsystematic review automation software, and a panel of human reviewers against a representative sample\nof citations from each SR. To assess sensitivity, we included all articles deemed ‘included’ in each SR (i.e.,\nentire inclusion set) where possible. For specificity, we determined a minimum specificity sample size of\n139 ‘excluded’ articles with Cochran’s sample size 38, based on an expected specificity of 90%, 5% margin of\nerror, and 95% confidence level. These excluded articles were randomly sampled from each review’s pool of\nnon-included citations (Extended Data Table 3, 4).\nTo evaluate the performance of Elicit in screening, we uploaded all PDF articles for each sample into the\nplatform. Elicit was tested on a sample of citations, rather than the full dataset, due to its 500-record screening\nlimit per review. The inclusion criteria provided to Elicit was identical to the inclusion criteria provided to the\notto-SR screening agent. As Elicit does not natively support exclusion criteria, we tested both inclusion criteria\nalone and inclusion criteria combined with inverse exclusion terms. Performance was higher using inclusion\ncriteria alone (Extended Data Table 1), so this approach was adopted. Elicit automatically retrieved titles\nand abstracts, followed by screening using a default inclusion score threshold of 2.5. No full-text screening\nworkflow was available. All evaluations were conducted using Elicit’s paid “Pro” plan.\nTo evaluate the performance of humans in data extraction, we assembled a panel of four graduate-level\nresearchers with past SR experience (1 BSc, 3 MSc; all current MD students) to perform dual screening 9. All\nscreening was performed independently and in duplicate. Conflicts during screening were resolved by a third\nindependent reviewer. Screening followed a standard end-to-end workflow: all citations were screened at the\ntitle/abstract stage, and citations deemed eligible by reviewers were advanced to full-text screening.\n8.4 Human Calibration\nTo verify screening proficiency, human reviewers first completed a calibration exercise using a set of citations\nfrom SeroTracker, a comprehensive systematic review on SARS-CoV-2 32,39. In SeroTracker, the original\nstudy authors conducted a study to assess internal consistency through re-screening a previously screened\ndataset using the same eligibility criteria 12. Our reviewers screened this same dataset, and we compared\ntheir performance against the original SeroTracker authors’ results.The performance of our reviewers was\ncomparable to the original SeroTracker authors. Details are found in Extended Data Table 2.\n8.5 Screening Data Analysis\nWe assessed the performance of the otto-SR screening agent, Elicit, and dual human reviewers by analyzing\naccuracy, sensitivity, specificity; and reported true positives, true negatives, false positives, and false negatives.\nWe calculated 95% CIs for weighted (pooled-denominator) sensitivity and specificity using the Wilson method40\nwith the binom package in R.\n8.6 Data Extraction Datasets\nWe utilized four datasets (SeroTracker, PA-Outcomes, PA-Testing, Sepsis) from the BenchSR database that\ncontained raw data extraction results provided by the original authors. In addition, we identified three external\nSRs (CKD41, Process42, Psyc-meds43) that also provided publicly accessible raw data extraction information\n(Extended Data Table 5). Variables assessed for extraction included key descriptive and outcome data used by\nthe original authors for downstream analysis (see Supplementary Notes).\n8.7 LLM Data Extraction Methodology\nWe developed a novel LLM-based data extraction agent using prompting best practices. Our data extraction\nagent uses the o3-mini-high model from OpenAI, high reasoning effort and a 100,000-token output limit.\nThe same markdown extracted from full-article PDF as used in the full-text screens were passed as inputs to\no3-mini-high for extraction. In all cases, the extraction agent was prompted with author-defined variables and\ncorresponding descriptions (Supplementary Notes).\n9\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n8.8 Data Extraction Benchmarking\nWe evaluated the performance of the otto-SR data extraction agent, Elicit, and dual human reviewers for data\nextraction across all datasets. The variable definitions used for extraction are provided in the Supplementary\nNotes.\nFor the otto-SR data extraction agent and Elicit, we used the complete set of articles with available data\nextraction results. Due to its extensive size (n=2,736 included articles), the SeroTracker dataset was randomly\ndownsampled to 100 articles for evaluation. For the Psyc-meds dataset, only studies with published data were\nincluded (Extended Data Table 5).\nTo evaluate the performance of Elicit in data extraction (evaluated on March 22, 2025), we uploaded all\narticles into the Elicit data extraction platform and used the same variable descriptions provided to the otto-SR\nextraction agent (Supplementary notes). All extractions were performed with the ‘high accuracy’ feature,\naccessed through the Elicit ‘Pro’ paid plan. In cases where Elicit encountered an error or failed to extract data,\nwe retried up to a maximum of 5 times.\nTo evaluate the performance of humans in screening, we assembled a panel of seven graduate-level\nresearchers with past SR experience (3 BSc, 4 MSc; all current MD students). Human data extraction was\nperformed independently and in duplicate. Discrepancies were resolved by a third human reviewer. For\nhuman data extraction, we determined sample size using a McNemar test for sample size approximation.\nUsing an estimated human accuracy of 80% (reported rates: 65.8-85.5%) 14–19, LLM accuracy of 90%, and 95%\nconfidence, we determined a minimum number of 204 variables per study to be extracted. Article counts for\neach testing dataset are provided in Extended Data Table 5.\nDue to the unstandardized nature of data extraction results (e.g., SeroTracker review - name of immunoassay\nused), we used an LLM-as-a-judge framework to programmatically determine data extraction accuracy. In this\nsetup, the o3-mini-high LLM was used to compare each AI- or human-extracted value to the original author\nvalue and determine if the two were equivalent. This evaluation method has been validated in prior LLM\nbenchmarking efforts, including the LLM Chatbot arena 26 and OpenAI’s HealthBench27.\n8.9 Data Extraction Correction\nPrior research has shown wide variability in the accuracy of dual human extraction, with reported rates\nranging from 65.8-85.5%14–19. As such, original author-provided values did not represent a reliable ground\ntruth. To address this, we conducted a blinded correction process for cases where the LLM-as-a-judge flagged\ndiscrepancies between otto-SR extracted values and original review author extracted values. A panel of three\nindependent, experienced graduate-level human reviewers validated outputs. Reviewers were presented with\ntwo anonymized and randomized responses (LLM and original author) and asked to select one of four options:\nOption A correct, Option B correct, Both correct, or Neither correct. Each disagreement was evaluated in\nparallel and resolved by a third independent arbitrator. The final adjudicated results were used to construct\ncorrected ground truth datasets. To evaluate the accuracy of otto-SR, Elicit, and dual human reviewers, we\nthen applied the LLM-as-a-judge framework to compare each system’s outputs against this corrected dataset.\nThis adjudication framework was adapted from established protocols in radiology, ophthalmology, and clinical\ntrials28–31.\nWe note a potential limitation in our validation process: when otto-SR and the original authors produced\nidentical values, we assumed these were correct without further adjudication. Consequently, if both sources\nmade the same systematic error, it would go undetected. This approach could potentially bias our evaluation\nagainst alternative models (e.g., Elicit or dual human reviewers) that disagreed with both reference sources. To\nevaluate this limitation, we performed spot checks on a random 10% sample of extractions where otto-SR and\nthe original authors agreed, finding no errors or inconsistencies.\nWe additionally performed a post-hoc review of incorrect AI outputs to classify errors as either ‘parsing\nerrors’ (i.e., errors with the PDF parsing pipeline), ‘inaccessible’ (i.e., data accessible only through author\ncorrespondence or supplementary material), or ‘true errors’ (i.e., cases where the original author values were\ncorrectly extracted).\n8.10 Extraction Data Analysis\nWe assessed the performance of the otto-SR data extraction agent, Elicit, and human reviewers by analyzing\nthe total accuracy at a variable level per study. If the human adjudicator classification was “inaccessible”, the\n10\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \ndata point was removed from analysis for otto-SR, Elicit, and human reviewers. We calculated 95% CIs for\nweighted (pooled-denominator) accuracy using the Wilson method 40 with the binom package in R.\n8.11 Cochrane Reproducibility\nTo evaluate the reliability and generalizability of our automated systematic review workflow, we conducted a\nfocused reproducibility assessment using an entire issue of the Cochrane Database of Systematic Reviews. Our\naim was to approximate each review’s workflow end-to-end, from literature search through to data extraction\nand meta-analysis, using the otto-SR pipeline.\nWe selected the April 2024 issue through random sampling. Of the 14 reviews published, two were\nexcluded: one due to the absence of downloadable data, and another due to an irreproducible search strategy\n(authors provided a search strategy to populate the Cochrane specialized register, but not for the review itself).\nThis left 12 eligible reviews spanning a range of clinical domains (Extended Data Table 7). The Cochrane\ndatabase was chosen for its rigorous and standardized reporting practices, public data availability, and detailed\nmethodological documentation.\n8.12 Cochrane Database Searches\nThe original search strategy of each Cochrane review was reproduced using the exact terms and filters\ndescribed in the review methods. Searches were limited to institutionally accessible databases. In cases where\ndatabases lacked precise date filtering (e.g., supporting month but not day-level granularity), we applied\npost-hoc filtering to approximate the original search window (Supplementary Data 2).\nAfter each search, we cross-referenced our list of articles with those included in the original Cochrane\nreviews. Articles that were not retrievable from the original search were excluded from downstream screening,\ndata extraction and analysis.\n8.13 Cochrane Screening\nAll retrieved citations underwent abstract and full-text screening with the otto-SR screening agent, prompted\nwith the inclusion and exclusion criteria, objectives, and review protocols from each Cochrane review\n(Supplementary Notes).\nTo ensure a focused and interpretable comparison, we deviated from Cochrane’s inclusion practice in one\nkey respect. Cochrane reviews typically include all studies reporting the eligible population and intervention\nof interest. This approach allows authors to explore all comparisons based on the available data (e.g., all\nintervention and outcome combinations, including those not pre-specified). While valuable for comprehensive\nsynthesis, the generation of comparisons after screening can make study eligibility unclear.\nInstead of focusing on all possible comparisons, we focused our analysis to reproduce each Cochrane\nreview’s primary analytical comparison. This constraint allowed for unambiguous inclusion criteria, where\nstudies had to meet specific predefined interventions, comparators, and outcome criteria. Citations without\na retrievable abstract or DOI/trial identifier were excluded. Screening decisions were compared against\nCochrane author decisions to calculate true positives, false negatives, false positives, and true negatives.\n8.14 Cochrane Data Extraction\nFor all studies passing full-text screening, outcome data was extracted using the otto-SR extraction agent,\nfocusing exclusively on the primary outcome defined in each Cochrane review. To ensure consistency, we used\nthe original author-defined variable names and extraction logic (Supplementary Notes).\nData extraction also served as a secondary filter. While otto-SR achieved high specificity (~97%), for a review\nof 10,000 citations, this would equate to nearly 300 false positive articles. To counteract this, studies were\nprogrammatically excluded if they returned unavailable or unreportable values for the primary outcome (e.g.,\n“na” values), were identified as duplicates, or involved ineligible intervention-comparator pairs (e.g., Drug A\nvs. Drug B when only Drug A vs. placebo was eligible). This secondary filtering step helped remove residual\nfalse positives from the screening phase, though it may have introduced occasional misclassifications (then\nlabeled as false negatives).\n11\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n8.15 Analysis\nAll meta-analyses were conducted using the metafor package in R (Code and Dataset Availability). To ensure\nfair comparison, we matched the original authors’ specified meta-analytic model (random-effects, fixed-effect),\neffect size metric (risk ratio, odds ratio, rate ratio, mean difference, standardized mean difference), and\ncontinuity correction approach, where reported. We conducted four meta-analytical comparisons: (1) Cochrane\n– meta-analysis using the original author-extracted data. (2) Matched – otto-SR results filtered to match the\nCochrane primary analysis study set. (3) Expanded – all eligible studies included by otto-SR under the original\nsearch cut-off. (4) Updated – all eligible studies included by otto-SR from an updated search extending to\nMay 8 2025. We also derived ‘corrected’ values (see below), for the ‘matched,’ ‘expanded,’ and ‘updated’\ncomparisons that served as the reference ground truth for analytical comparison.\n8.16 Cochrane Data Correction and Comparison\nTo address known concerns about the reliability of original author-extracted data, we conducted an adjudication\nprocess to derive corrected data extraction and screening information for the ‘Matched,’ ‘Expanded,’ and\n‘Updated’ analyses. For our ‘Matched’ analysis, a panel of two human reviewers compared data extraction and\nscreening decisions from the original Cochrane authors and the otto-SR extraction agent, selecting the correct\nvalue through re-assessment of the source article. In our ‘Expanded’ and ‘Updated’ analysis, where Cochrane\ndata was not available, a panel of two human reviewers compared otto-SR data extraction and screening\ndecisions against original study articles, selecting the correct value through re-assessment of the source article.\nFinal articles included in the corrected analysis consisted of all otto-SR true positive articles, and any Cochrane\ntrue positive articles missed by otto-SR. The extracted values in this final dataset reflected the most accurate,\nreviewer-verified information and served as the reference standard for Cochrane and otto-SR performance\ncomparisons (Supplementary Data 1 for raw and corrected values, reviewer notes, and error classifications).\nTo ensure consistency and transparency, we applied standardized rules for study eligibility across analysis\nsets. First, articles had to be retrievable through our reproduced search; unretrievable citations were excluded\nfrom all otto-SR-based analyses. Second, for author data requests, we included studies in the Cochrane analysis\nonly if authors explicitly stated that they contacted study authors and specified which studies and outcomes\nwere supplemented. If a data request was suspected for a study, but the review only reported vague references\nto data requests without further specification, the study was considered unverifiable and excluded from both\nthe Cochrane and otto-SR-corrected analyses. For instance, in the ACEi review, the authors appeared to assign\nzero mortality events in studies that did not report mortality or adverse events; but did not clearly state\nwhich studies had data requests. For such cases, we excluded those studies from the Cochrane and otto-SR\ncorrected analyses to avoid introducing speculative data. Suspected data requests occurred in three reviews (4\nstudies, Nutrition; 15 studies, ACEi; 4 studies, Depression). A high-level summary of methodological issues is\nprovided in Extended Data Table 7. Detailed notes for the exclusion of studies are provided in Supplementary\nData 1.\nStudies with supplementary data (not extractable by otto-SR) were included in the Cochrane and corrected\nanalyses, thereby penalizing the model. If the data was inaccessible to otto-SR due to format limitations (e.g.,\nembedded figures), the study was excluded from otto-SR analyses but retained in the Cochrane and corrected\nsets. These criteria aimed to balance reproducibility, verifiability, and the practical constraints of automation.\n12\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n9 Acknowledgements\nWe thank Guravneet Gill for assistance with database access; Zion Chan and Li Li for discussions during\nproject development; and Luke Son for technical insights. We acknowledge all systematic review authors\nwhose published data and methodologies enabled this reproducibility assessment.\n10 Author Information\nThese authors contributed equally: C.C, R.A., P .C.\nAuthor contribution statements: C.C, R.A, N.B contributed to the conception and design of the work. C.C,\nR.A, K.M, M.C, E.F, A.S, R.K, R.S, D.M, J.L, J.J, D.C, J.G, S.L contributed to data acquisition, cleaning, human\ncomparisons, and human arbitration. C.C, R.A, P .C, J.S, S.J, J.X, K.Z generated code for evaluations and\nbenchmarking. C.C, R.A, P .C, L.X.G analyzed study data. N.B, G.M.C, D.M, I.B, D.B.E, R.K.A, L.G, M.N,\nA.A.L, B.T, M.W, H.W contributed to project supervision and provided feedback on the study. C.C, R.A, P .C,\nprepared the original draft of the manuscript with input from all co-authors. All authors were responsible for\nreview and editing of the manuscript. All authors debated, discussed, edited, and approved the final version\nof the manuscript.\n11 Ethics Declaration\nThere was no direct funding support for this manuscript.\nN.B reports grants from the Public Health Agency of Canada through Canada’s COVID-19 Immunity Task\nForce, the World Health Organization Health Emergencies Programme, the Robert Koch Institute, and the\nCanadian Medical Association Joule Innovation Fund, the Canadian Association of Emergency Physicians and\nAlberta Health Services Emergency Strategic Clinical Network.\nDisclosures for G.M.C. can be found at http://arep.med.harvard.edu/gmc/tech.html.\nR.K.A. is employed at OpenAI and owns stock as part of the standard compensation package.\nR.A. reports grants from the CIHR Institute of Genetics.\nC.C. P .C. J.S. are founders of and hold equity in otto review, LLC. R.A is a non equity holding founder/advisor\nin otto review, LLC.\nNo funding source had any role in the design of this study, its execution, analyses, interpretation of the data,\nor decision to submit results.\n12 Code and Dataset Availability\nAll datasets and code used for data analysis will be made available on publication.\n13\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n13 References\n1. Borah R, Brown AW, Capers PL, Kaiser KA. Analysis of the time and workers needed to conduct systematic\nreviews of medical interventions using data from the PROSPERO registry. BMJ Open. 2017 Feb;7(2):e012545.\n2. Michelson M, Reuter K. The significant cost of systematic reviews and meta-analyses: A call for greater\ninvolvement of machine learning to assess the promise of clinical trials. Contemp Clin Trials Commun. 2019\nDec;16:100443.\n3. Prasad V , Cifu A. Medical reversal: why we must raise the bar before adopting new technologies. Yale J Biol\nMed. 2011 Dec;84(4):471–8.\n4. Fabiano N, Gupta A, Bhambra N, Luu B, Wong S, Maaz M, et al. How to optimize the systematic review\nprocess using AI tools. JCPP Adv. 2024 Jun;4(2):e12234.\n5. Ge L, Agrawal R, Singer M, Kannapiran P , De Castro Molina JA, Teow KL, et al. Leveraging artificial\nintelligence to enhance systematic reviews in health research: advanced tools and challenges. Syst Rev. 2024\nOct 25;13(1):269.\n6. Cao C, Sang J, Arora R, Chen D, Kloosterman R, Cecere M, et al. Development of Prompt Templates for Large\nLanguage Model–Driven Screening in Systematic Reviews. Ann Intern Med. 2025 Feb 25;ANNALS-24-02189.\n7. Lai H, Liu J, Bai C, Liu H, Pan B, Luo X, et al. Language models for data extraction and risk of bias\nassessment in complementary medicine. Npj Digit Med. 2025 Jan 31;8(1):74.\n8. Gartlehner G, Kahwati L, Hilscher R, Thomas I, Kugley S, Crotty K, et al. Data extraction for evidence\nsynthesis using a large language model: A proof-of-concept study. Res Synth Methods. 2024 Jul;15(4):576–89.\n9. Cumpston M, Li T, Page MJ, Chandler J, Welch VA, Higgins JP , et al. Updated guidance for trusted system-\natic reviews: a new edition of the Cochrane Handbook for Systematic Reviews of Interventions. Cochrane\nEditorial Unit, editor. Cochrane Database Syst Rev[Internet]. 2019 Oct 3 [cited 2024 Jun 1]; Available from:\nhttps://doi.wiley.com/10.1002/14651858.ED000142\n10. OpenAI. Introducing GPT-4.1 in the API [Internet]. OpenAI. [cited 2025 Jun 8]. Available from:\nhttps://openai.com/index/gpt-4-1/\n11. Sirdeshmukh V , Deshpande K, Mols J, Jin L, Cardona EY, Lee D, et al. MultiChallenge: A Realistic\nMulti-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs [Internet]. arXiv; 2025 [cited\n2025 Jun 8]. Available from: https://arxiv.org/abs/2501.17399\n12. Perlman-Arrow S, Loo N, Bobrovitz N, Yan T, Arora RK. A real-world evaluation of the implementation of\nNLP technology in abstract screening of a systematic review. Res Synth Methods. 2023 Jul;14(4):608–21.\n13. OpenAI. OpenAI o3-mini System Card [Internet]. OpenAI; 2025. Available from:\nhttps://cdn.openai.com/o3-mini-system-card-feb10.pdf\n14. Horton J, Vandermeer B, Hartling L, Tjosvold L, Klassen TP , Buscemi N. Systematic review data extraction:\ncross-sectional study showed that experience did not increase accuracy. J Clin Epidemiol. 2010 Mar;63(3):289–98.\n15. Buscemi N, Hartling L, Vandermeer B, Tjosvold L, Klassen TP . Single data extraction generated more errors\nthan double data extraction in systematic reviews. J Clin Epidemiol. 2006 Jul;59(7):697–703.\n16. Carroll C, Scope A, Kaltenthaler E. A case study of binary outcome data extraction across three systematic\nreviews of hip arthroplasty: errors and differences of selection. BMC Res Notes. 2013 Dec;6(1):539.\n17. Tang L, Wang R, Doi SAR, Furuya-Kanamori L, Lin L, Qin Z, et al. Double data extraction was insufficient\nfor minimizing errors in evidence synthesis: a randomized controlled trial [Internet]. 2023 [cited 2025 Jun 8].\nAvailable from: http://medrxiv.org/lookup/doi/10.1101/2023.10.16.23297056\n18. Xu C, Yu T, Furuya-Kanamori L, Lin L, Zorzela L, Zhou X, et al. Validity of data extraction in evidence\nsynthesis practice of adverse events: reproducibility study. BMJ. 2022 May 10;e069155.\n19. Li T, Saldanha IJ, Jap J, Smith BT, Canner J, Hutfless SM, et al. A randomized trial provided new evidence\non the accuracy and efficiency of traditional vs. electronically annotated abstraction approaches in systematic\nreviews. J Clin Epidemiol. 2019 Nov;115:77–89.\n20. Rethlefsen ML, Brigham TJ, Price C, Moher D, Bouter LM, Kirkham JJ, et al. Systematic review search\nstrategies are poorly reported and not reproducible: a cross-sectional metaresearch study. J Clin Epidemiol.\n2024 Feb;166:111229.\n21. Ioannidis JPA. The Mass Production of Redundant, Misleading, and Conflicted Systematic Reviews and\nMeta-analyses. Milbank Q. 2016 Sep;94(3):485–514.\n22. Mathes T, Klaßen P , Pieper D. Frequency of data extraction errors and methods to increase data extraction\nquality: a methodological review. BMC Med Res Methodol. 2017 Nov 28;17(1):152.\n23. Bertizzolo L, Bossuyt P , Atal I, Ravaud P , Dechartres A. Disagreements in risk of bias assessment for\n14\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nrandomised controlled trials included in more than one Cochrane systematic reviews: a research on research\nstudy using cross-sectional design. BMJ Open. 2019 Apr;9(4):e028382.\n24. Shah K, Egan G, Huan L (Nichoe), Kirkham J, Reid E, Tejani AM. Outcome reporting bias in Cochrane\nsystematic reviews: a cross-sectional analysis. BMJ Open. 2020 Mar;10(3):e032497.\n25. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, et al. The PRISMA 2020\nstatement: an updated guideline for reporting systematic reviews. BMJ. 2021 Mar 29;n71.\n26. Zheng L, Chiang WL, Sheng Y, Zhuang S, Wu Z, Zhuang Y, et al. Judging LLM-as-a-Judge with MT-Bench\nand Chatbot Arena [Internet]. arXiv; 2023 [cited 2025 Jun 8]. Available from: https://arxiv.org/abs/2306.05685\n27. Arora R, Wei J, Hicks R, Bowman P , Quinonero-Candela J, Tsimpourlas T, et al. HealthBench: Evaluating\nLarge Language Models Towards Improved Human Health [Internet]. OpenAI; 2025 [cited 2025 Jun 8].\nAvailable from: https://openai.com/index/healthbench/\n28. Plesner LL, Müller FC, Brejnebøl MW, Krag CH, Laustrup LC, Rasmussen F, et al. Using AI to Identify\nUnremarkable Chest Radiographs for Automatic Reporting. Radiology. 2024 Aug 1;312(2):e240272.\n29. Nguyen HQ, Lam K, Le LT, Pham HH, Tran DQ, Nguyen DB, et al. VinDr-CXR: An open dataset of chest\nX-rays with radiologist’s annotations. Sci Data. 2022 Jul 20;9(1):429.\n30. Gulshan V , Rajan RP , Widner K, Wu D, Wubbels P , Rhodes T, et al. Performance of a Deep-Learning\nAlgorithm vs Manual Grading for Detecting Diabetic Retinopathy in India. JAMA Ophthalmol. 2019 Sep\n1;137(9):987.\n31. Godolphin PJ, Bath PM, Algra A, Berge E, Brown MM, Chalmers J, et al. Outcome Assessment by Central\nAdjudicators Versus Site Investigators in Stroke Trials: A Systematic Review and Meta-Analysis. Stroke. 2019\nAug;50(8):2187–96.\n32. Bobrovitz N, Arora RK, Cao C, Boucher E, Liu M, Donnici C, et al. Global seroprevalence of SARS-\nCoV-2 antibodies: A systematic review and meta-analysis. Khudyakov YE, editor. PLOS ONE. 2021 Jun\n23;16(6):e0252617.\n33. Bobrovitz N, Ware H, Ma X, Li Z, Hosseini R, Cao C, et al. Protective effectiveness of previous SARS-CoV-2\ninfection and hybrid immunity against the omicron variant and severe disease: a systematic review and\nmeta-regression. Lancet Infect Dis. 2023 May;23(5):556–67.\n34. Samnani S, Cenzer I, Kline GA, Lee SJ, Hundemer GL, McClurg C, et al. Time to Benefit of Surgery vs\nTargeted Medical Therapy for Patients With Primary Aldosteronism: A Meta-analysis. J Clin Endocrinol Metab.\n2024 Feb 20;109(3):e1280–9.\n35. Leung AA, Symonds CJ, Hundemer GL, Ronksley PE, Lorenzetti DL, Pasieka JL, et al. Performance of\nConfirmatory Tests for Diagnosing Primary Aldosteronism: a Systematic Review and Meta-Analysis. Hyperten-\nsion. 2022 Aug;79(8):1835–44.\n36. Mascarenhas D, Weisz D, Jasani B, Persad N, Main E. Premedication for rapid sequence intubation in\nneonates - a network meta-analysis. PROSPERO 2022 CRD42022384259 [Internet]. PROSPERO. Available from:\nhttps://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42022384259\n37. Priem J, Piwowar H, Orr R. OpenAlex: A fully-open index of scholarly works, authors, venues, institutions,\nand concepts [Internet]. arXiv; 2022 [cited 2025 Jun 8]. Available from: https://arxiv.org/abs/2205.01833\n38. Cochran WG. Sampling techniques, 3rd edition. John Wiley; 2002.\n39. Bergeri I, Whelan MG, Ware H, Subissi L, Nardone A, Lewis HC, et al. Global SARS-CoV-2 seroprevalence\nfrom January 2020 to April 2022: A systematic review and meta-analysis of standardized population-based\nstudies. Suthar AB, editor. PLOS Med. 2022 Nov 10;19(11):e1004107.\n40. Wilson EB. Probable Inference, the Law of Succession, and Statistical Inference. J Am Stat Assoc. 1927\nJun;22(158):209–12.\n41. Cleary F, Prieto-Merino D, Nitsch D. A systematic review of statistical methodology used to evaluate\nprogression of chronic kidney disease using electronic healthcare records. Aoun M, editor. PLOS ONE. 2022\nJul 29;17(7):e0264167.\n42. Antonacci G, Lennox L, Barlow J, Evans L, Reed J. Process mapping in healthcare: a systematic review.\nBMC Health Serv Res. 2021 Apr 14;21(1):342.\n43. Kopcalic K, Arcaro J, Pinto A, Ali S, Barbui C, Curatoli C, et al. Antidepressants versus placebo for gener-\nalised anxiety disorder (GAD). Cochrane Central Editorial Service, editor. Cochrane Database Syst Rev[Internet].\n2025 Jan 30 [cited 2025 Jun 8];2025(2). Available from: http://doi.wiley.com/10.1002/14651858.CD012942.pub2\n15\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nAutomation of Systematic Reviews\nwith Large Language Models\nExtended Data Tables and Figures\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n1\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n2\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n3\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n4\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n5\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n6\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n7\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n8\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n9\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n10\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n11\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n12\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n13\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n14\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n15\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n16\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \n17\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint \nExtended Data Figure 1: Data extraction answer classification. Bar graph displaying answer classification\nresulting from dual human adjudication across o3-mini-high, dual human extraction, and Elicit across all 7\nevaluated systematic reviews.\n18\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 13, 2025. ; https://doi.org/10.1101/2025.06.13.25329541doi: medRxiv preprint ",
  "topic": "Automation",
  "concepts": [
    {
      "name": "Automation",
      "score": 0.6079993844032288
    },
    {
      "name": "Computer science",
      "score": 0.5366498827934265
    },
    {
      "name": "Systems engineering",
      "score": 0.32378441095352173
    },
    {
      "name": "Engineering",
      "score": 0.2213008999824524
    },
    {
      "name": "Mechanical engineering",
      "score": 0.08316141366958618
    }
  ]
}