{
  "title": "STGATE: Spatial-temporal graph attention network with a transformer encoder for EEG-based emotion recognition",
  "url": "https://openalex.org/W4365448982",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2112110935",
      "name": "Jingcong Li",
      "affiliations": [
        "South China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2120436549",
      "name": "Wei-Jian Pan",
      "affiliations": [
        "South China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2104609916",
      "name": "Haiyun Huang",
      "affiliations": [
        "South China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2138770452",
      "name": "Jiahui Pan",
      "affiliations": [
        "South China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "South China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2112110935",
      "name": "Jingcong Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120436549",
      "name": "Wei-Jian Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104609916",
      "name": "Haiyun Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2138770452",
      "name": "Jiahui Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4317727959",
    "https://openalex.org/W2035083235",
    "https://openalex.org/W1575429381",
    "https://openalex.org/W2625929003",
    "https://openalex.org/W2765856398",
    "https://openalex.org/W2162418306",
    "https://openalex.org/W2792193826",
    "https://openalex.org/W6629091712",
    "https://openalex.org/W2122241189",
    "https://openalex.org/W3008194092",
    "https://openalex.org/W3014215018",
    "https://openalex.org/W2570179224",
    "https://openalex.org/W2915893085",
    "https://openalex.org/W2133085034",
    "https://openalex.org/W6776303749",
    "https://openalex.org/W3140416091",
    "https://openalex.org/W2946526173",
    "https://openalex.org/W4291652904",
    "https://openalex.org/W2172947082",
    "https://openalex.org/W2962949934",
    "https://openalex.org/W2045468728",
    "https://openalex.org/W3089475900",
    "https://openalex.org/W4224930828",
    "https://openalex.org/W2134050473",
    "https://openalex.org/W3157658976",
    "https://openalex.org/W2081420711",
    "https://openalex.org/W2604255570",
    "https://openalex.org/W2146010402",
    "https://openalex.org/W6779685995",
    "https://openalex.org/W2599124244",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2171782446",
    "https://openalex.org/W6782016555",
    "https://openalex.org/W2900672720",
    "https://openalex.org/W6679967756",
    "https://openalex.org/W3205895886",
    "https://openalex.org/W6801611706",
    "https://openalex.org/W3033046106",
    "https://openalex.org/W3041561163",
    "https://openalex.org/W6842972433",
    "https://openalex.org/W76639474",
    "https://openalex.org/W2041684398",
    "https://openalex.org/W4313825935",
    "https://openalex.org/W2908954394",
    "https://openalex.org/W3080459878",
    "https://openalex.org/W2115403315",
    "https://openalex.org/W1989976960",
    "https://openalex.org/W2584561145",
    "https://openalex.org/W6758532884",
    "https://openalex.org/W2988016651",
    "https://openalex.org/W6755310813",
    "https://openalex.org/W6805037698",
    "https://openalex.org/W4239447739",
    "https://openalex.org/W6660089423",
    "https://openalex.org/W2810418809",
    "https://openalex.org/W2790404832",
    "https://openalex.org/W3164743527",
    "https://openalex.org/W1999771120",
    "https://openalex.org/W3087220290",
    "https://openalex.org/W2323851044",
    "https://openalex.org/W3083218890",
    "https://openalex.org/W2748454491",
    "https://openalex.org/W2139624420",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2037057652",
    "https://openalex.org/W2907492528",
    "https://openalex.org/W2801464374",
    "https://openalex.org/W3013371719",
    "https://openalex.org/W2970007912",
    "https://openalex.org/W3115745466",
    "https://openalex.org/W2786768213",
    "https://openalex.org/W1947251450",
    "https://openalex.org/W2962905870",
    "https://openalex.org/W3024961463",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W4238327235",
    "https://openalex.org/W3082112788",
    "https://openalex.org/W1555767263",
    "https://openalex.org/W4200175924",
    "https://openalex.org/W2912296668",
    "https://openalex.org/W4297779039",
    "https://openalex.org/W4321497876",
    "https://openalex.org/W2623902889",
    "https://openalex.org/W3198543070",
    "https://openalex.org/W2134602648",
    "https://openalex.org/W4254724182",
    "https://openalex.org/W4200322992",
    "https://openalex.org/W4316171088",
    "https://openalex.org/W2038610431",
    "https://openalex.org/W3034369844",
    "https://openalex.org/W2541711215",
    "https://openalex.org/W4210257598"
  ],
  "abstract": "Electroencephalogram (EEG) is a crucial and widely utilized technique in neuroscience research. In this paper, we introduce a novel graph neural network called the spatial-temporal graph attention network with a transformer encoder (STGATE) to learn graph representations of emotion EEG signals and improve emotion recognition performance. In STGATE, a transformer-encoder is applied for capturing time-frequency features which are fed into a spatial-temporal graph attention for emotion classification. Using a dynamic adjacency matrix, the proposed STGATE adaptively learns intrinsic connections between different EEG channels. To evaluate the cross-subject emotion recognition performance, leave-one-subject-out experiments are carried out on three public emotion recognition datasets, i.e., SEED, SEED-IV, and DREAMER. The proposed STGATE model achieved a state-of-the-art EEG-based emotion recognition performance accuracy of 90.37% in SEED, 76.43% in SEED-IV, and 76.35% in DREAMER dataset, respectively. The experiments demonstrated the effectiveness of the proposed STGATE model for cross-subject EEG emotion recognition and its potential for graph-based neuroscience research.",
  "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/three.tnum April /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nOPEN ACCESS\nEDITED BY\nRedha Taiar,\nUniversité de Reims Champagne-Ardenne,\nFrance\nREVIEWED BY\nMohammad Ashraful Amin,\nNorth South University, Bangladesh\nJinyi Long,\nJinan University, China\n*CORRESPONDENCE\nFei Wang\nscutauwf@foxmail.com\nSPECIALTY SECTION\nThis article was submitted to\nBrain-Computer Interfaces,\na section of the journal\nFrontiers in Human Neuroscience\nRECEIVED /two.tnum/zero.tnum February /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /two.tnum/seven.tnum March /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/three.tnum April /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nLi J, Pan W, Huang H, Pan J and Wang F (/two.tnum/zero.tnum/two.tnum/three.tnum)\nSTGATE: Spatial-temporal graph attention\nnetwork with a transformer encoder for\nEEG-based emotion recognition.\nFront. Hum. Neurosci./one.tnum/seven.tnum:/one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Li, Pan, Huang, Pan and Wang. This is\nan open-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nSTGATE: Spatial-temporal graph\nattention network with a\ntransformer encoder for\nEEG-based emotion recognition\nJingcong Li, Weijian Pan, Haiyun Huang, Jiahui Pan and\nFei Wang*\nSchool of Software, South China Normal University, Guangzhou, China\nElectroencephalogram (EEG) is a crucial and widely utilized techn ique in\nneuroscience research. In this paper, we introduce a novel graph neu ral network\ncalled the spatial-temporal graph attention network with a tran sformer encoder\n(STGATE) to learn graph representations of emotion EEG signa ls and improve\nemotion recognition performance. In STGATE, a transformer-encod er is applied\nfor capturing time-frequency features which are fed into a spatial -temporal\ngraph attention for emotion classiﬁcation. Using a dynamic adja cency matrix,\nthe proposed STGATE adaptively learns intrinsic connections b etween diﬀerent\nEEG channels. To evaluate the cross-subject emotion recognitio n performance,\nleave-one-subject-out experiments are carried out on three p ublic emotion\nrecognition datasets, i.e., SEED, SEED-IV, and DREAMER. The pro posed STGATE\nmodel achieved a state-of-the-art EEG-based emotion recogni tion performance\naccuracy of /nine.tnum/zero.tnum./three.tnum/seven.tnum% in SEED, /seven.tnum/six.tnum./four.tnum/three.tnum% in SEED-IV, and /seven.tnum/six.tnum./three.tnum/five.tnum% in DREAMER dataset,\nrespectively. The experiments demonstrated the eﬀectivenes s of the proposed\nSTGATE model for cross-subject EEG emotion recognition and its po tential for\ngraph-based neuroscience research.\nKEYWORDS\nEEG-based emotion classiﬁcation, EEG, deep learning, graph neu ral network, transformer\nencoder\n/one.tnum. Introduction\nEmotion is a generalization of subjective human experience and behavior. Emotions\naﬀect our perceptions and attitudes dramatically and play an essential role in human-\ncomputer interaction (HCI) (\nJerritta et al., 2011 ). Emotions have a signiﬁcant impact on our\nevaluation, attitudes, behavior, decisions, cognition, learning, perception, and understanding\n(\nBrosch et al., 2013 ). Moreover, emotions can act as a motivational mechanism that enhances\nusers’ attention, interest, and motivation, thereby promoting their learning and cognition\n(\nTyng et al., 2017 ). In the context of human-computer interaction (HCI), emotions can serve\nas a valuable feedback mechanism that increases user satisfaction, engagement, and loyalty\n(\nBrave and Nass, 2007 ; Jeon, 2017).\nAs a crucial and fundamental research area of aﬀective computing and neuroscience,\nemotion recognition has attracted great attention from the academy and enterprise ﬁelds\nin recent years (\nCambria et al., 2017 ; Torres et al., 2020 ). Emotion recognition technology\ncan generally be categorized into two major categories ( Shu et al., 2018 ). The ﬁrst\ncategory involves non-physiological signals such as facial expressions, speech, gestures,\nand posture (\nSchuller et al., 2003 ; Anderson and McOwan, 2006 ; Castellano et al., 2008 ).\nFrontiers in Human Neuroscience /zero.tnum/one.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nThe second category is based on physiological signals such\nas electroencephalogram (EEG), electrocardiogram (ECG),\nelectromyography (EMG), skin temperature (SKT), and others\n(\nEgger et al., 2019 ). Physiological-based emotion recognition\nis considered more reliable as it is diﬃcult for individuals to\ndeliberately control their physiological signals. Among the\nphysiological signals, EEG signals are widely used in neural\nengineering and brain-computer interfaces (BCIs) research due\nto their high temporal resolution, non-invasiveness, and low cost\n(\nCraik et al., 2019 ). Emotional states are closely related to neural\nactivity produced by the central nervous system ( Torres et al.,\n2020). This neural activity can be directly measured using EEG\ndevices, making EEG-based emotion recognition increasingly\npopular in various ﬁelds, such as education, health, entertainment\n(\nXu et al., 2018 ; Suhaimi et al., 2020 ; Abdel-Hamid, 2023 ;\nMoontaha et al., 2023 ).\nA major problem with recognizing emotions is that emotions\nshould be deﬁned and accessed quantitatively. There are two\ndiﬀerent models used to deﬁne emotions: the discrete model and\nthe dimensional model (\nShu et al., 2018 ). According to the discrete\nmodel, emotions are divided into several basic categories, such\nas sadness, fear, disgust, surprise, happiness, and anger. These\nemotions can form more complex emotion categories through a\ncertain combination of patterns (\nPeter and Herbon, 2006 ; Van den\nBroek, 2013 ). The dimensional emotion model maps emotional\nstates into the points on a certain coordinate system. Diﬀerent\nemotional states are distributed in diﬀerent positions in the\ncoordinate system, and the distance between positions reﬂects the\ndiﬀerence between diﬀerent emotional states (\nWioleta, 2013; Poria\net al., 2017 ; He et al., 2020 ). Diﬀerent from discrete emotion\nmodels, the dimensional emotion model is continuous and has the\nadvantages of a wide range of emotions and the ability to describe\nthe evolution of emotions.\nIn recent decades, EEG-based emotion recognition has\nattracted much attention from researchers (\nJerritta et al., 2011 ).\nA typical recognition process of emotional EEG usually consists\nof two parts: EEG feature extraction and emotion classiﬁcation\n(\nAlarcao and Fonseca, 2017 ). EEG is a highly dynamic and\nnonlinear signal with a large amount and redundancy of data.\nThus, feature extraction is an important step in emotion evaluation\nbecause high-resolution features are essential for eﬀective pattern\nrecognition (\nHe et al., 2020 ). EEG features can be mainly\ndivided into time-domain features, frequency-domain features,\nand time-frequency features (\nJenke et al., 2014 ; Stancin et al.,\n2021; Huang et al., 2022 ). One of the widely used methods\nof frequency domain feature analysis of EEG signals is to\ndecompose EEG signals into several frequency bands, including\ndelta (1–3 Hz), theta (4–7 Hz), alpha (8–13 Hz), beta (14–\n30 Hz), and gamma ( >31 Hz) (\nAftanas et al., 2004 ; Davidson,\n2004; Li and Lu, 2009 ). EEG features can be extracted from\neach band. The common time domain features include statistical\nfeatures (\nLiu and Sourina, 2014 ) and Hjorth features ( Hjorth,\n1970). Commonly used frequency domain features include power\nspectral density (PSD) ( Thammasan et al., 2016 ), diﬀerential\nentropy (DE) ( Shi et al., 2013 ), and rational asymmetry (RASM)\n(Zheng et al., 2017 ). The common time-frequency domain\nfeatures include wavelet features ( Akin, 2002 ), short-time Fourier\ntransform ( Kıymık et al., 2005 ), and Hilbert-Huang transform\n(Hadjidimitriou and Hadjileontiadis, 2012 ).\nOne of the most successful methods for recognizing emotions\nbased on EEG signals is Deep Neural Networks (DNN) ( Zhang\net al., 2020 ; Ozdemir et al., 2021 ). The Convolutional Neural\nNetworks (CNN) method was proven to be a powerful tool to\nmodel structured data in many applications, ranging from image\nclassiﬁcation and video processing to speech recognition and\nnatural language understanding (\nGu et al., 2018 ). However, EEG\nsignals can be considered non-Euclidean data in order to extract the\nrelationship of diﬀerent brain regions, and Convolutional Neural\nNetworks (CNN) may not be eﬀective in capturing the hidden\npatterns of non-Euclidean data (\nMicheloyannis et al., 2006 ). In\nrecent years, Graph Neural Networks (GNNs) have been developed\nrapidly and oﬀer a potential solution to extract correlation features\namong EEG channels in emotion recognition tasks (\nWu et al.,\n2020). In the graph representation of emotional EEG signals, each\nEEG channel corresponds to a vertex node, and the connections\nbetween vertex nodes correspond to edges in the graph, making it\nsuitable for encoding the correlation among the brain regions in the\nmultichannel EEG signal (\nJia et al., 2020 ). However, constructing a\nbetter graph representation of EEG signals for emotion recognition\nproblems remains challenging as the spatial position, which must\nbe predetermined before building the EEG emotion recognition\nmodel, is diﬀerent from the functional connections among EEG\nchannels (\nSong et al., 2018 ).\nTo leverage both spatial relationships and time-frequency\ninformation, many researchers have extended graph neural\nnetworks by spatial-temporal attention. Spatial temporal attention\nis a mechanism that captures the dynamic relationship between\nspatial and temporal dimensions in data. It consists of two kinds\nof attention: spatial attention, which focuses on the relevant\nregions or nodes in space; and temporal attention, which focuses\non the time steps in time dimension. Sartipi et al. proposed\nthe novel spatial-temporal attention neural network (STANN)\nto extract discriminative spatial and temporal features of EEG\nsignals by a parallel structure of the multi-column convolutional\nneural network and attention-based bidirectional long-short term\nmemory (\nSartipi et al., 2021 ). Li X. et al. (2021) proposed a\nmodel called attention-based spatial =temporal graphic long short-\nterm memory (ASTG-LSTM), in which a speciﬁc spatial-temporal\nattention embedded into the model to improve the invariance\nability against the emotional intensity ﬂuctuation.\nLiu et al. (2022)\nproposed a spatial-temporal attention to explore the relationship\nbetween emotion and spatial-temporal EEG features. Therefore, it\nis reasonable to consider incorporating spatial-temporal attention\nto improve classiﬁcation accuracy.\nIn this paper, we propose a novel model, STGATE, which\ncombines a transformer learning block (TLB) and a Spatial-\ntemporal Graph Attention (STGAT) mechanism. TLB utilizes 2D\nconvolutional layers and a transformer encoder to extract time-\nfrequency information, while the STGAT utilizes both spatial and\ntemporal attention mechanisms to learn connections between brain\nregions and temporal information, respectively. Our approach\ntreats EEG signals as graph data and incorporates them into\ngraph neural networks to capture correlations between EEG\nchannels. Unlike the GNN methods, the adjacency matrix learned\nFrontiers in Human Neuroscience /zero.tnum/two.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nby STGATE can provide a better graph representation because\nit is adaptively updated by spatial attention during the training\nprocess. The main contributions of this paper can be summarized\nas follows:\n• This paper proposes a novel spatial-temporal graph attention\nnetwork with a transformer encoder (termed STGATE) for\nEEG-based emotion recognition.\n• STGATE utilizes a transformer learning block and spatial-\ntemporal graph attention. This allows it to capture electrode-\nlevel time-frequency representations. It also helps STGATE\nlearn the emotional brain activities within and among diﬀerent\nbrain functional areas.\n• STGATE achieved state-of-the-art performance with a cross-\nsubject accuracy of 90.37% in SEED, 76.43% in SEED-IV , and\n77.44% and 75.26% in the valence and arousal dimensions\nof the DREAMER dataset, respectively. Extensive ablation\nstudies and analysis experiments were conducted to validate\nthe eﬃciency of the proposed STGATE.\nRemainder of this paper is organized as follows. The proposed\nSTGATE method is presented in Section 3. The datasets and\nexperiment settings are presented in Section 4. In Section 5,\nnumerical emotion recognition experiments on the SEED, SEED-\nIV , and DREAMER datasets are carried out. In addition, the\nperformance of the current methods and the proposed methods\nare presented and compared. Some discussions and analyzes of the\nproposed model are presented in Section 5. The conclusions of this\npaper are given in Section 6.\n/two.tnum. Related work\n/two.tnum./one.tnum. Emotion recognition\nEmotion recognition is crucial for research in aﬀective\ncomputing and neuroscience. Many studies on EEG-based emotion\nrecognition focus on feature engineering or deep learning. Long\nshort-term memory (LSTM) has been utilized to learn features from\nraw EEG signals and has achieved higher average accuracy than\ntraditional techniques (\nAlhagry et al., 2017 ). A deep adaptation\nnetwork has also been used to eliminate individual diﬀerences in\nEEG signals for eﬀective model implementation (\nLi et al., 2018 ).\nHowever, the inter-channel correlation of EEG signals for\nemotion recognition is critical. Song et al. (2018) proposed a novel\ndynamic graph convolutional neural network to dynamically learn\nthe intrinsic relationship between diﬀerent channels. To capture\nboth local and global relations among diﬀerent EEG channels,\nZhong et al. proposed a regularized graph neural network (RGNN)\nfor EEG-based emotion recognition, which models the inter-\nchannel relations in EEG signals via an adjacency matrix (\nZhong\net al., 2020 ). A graph convolutional broad network was designed\nto explore the deeper-level information of graph-structured data\nand achieved high performance in EEG-based emotion recognition\n(\nZhang et al., 2019 ). Li et al. proposed a Multi-Domain Adaptive\nGraph Convolutional Network (MD-AGCN), fusing the knowledge\nof both the frequency domain and the temporal domain to\nfully utilize the complementary information of EEG signals (\nLi\nR. et al., 2021 ) designed a model called ST-GCLSTM, which\nutilizes spatial attention to modify adjacency matrices to adaptively\nlearn the intrinsic connection among diﬀerent EEG channels\n(\nFeng et al., 2022 ).\nVarious methods and classiﬁers have been proposed and\napplied to the problem of EEG-based emotion recognition. To\nimprove the accuracy of emotion recognition, this paper proposes\nSTGATE, a model that extracts time-frequency and spatial features\nfrom EEG signals.\n/two.tnum./two.tnum. Graph attention network\nAccording to previous studies, graph convolutional neural\nnetworks are divided into spectral and spatial methods (\nChen et al.,\n2020). The spectral method uses the convolution theorem to map\nthe signal to the spectral space, which overcomes the non-Euclidean\ndata missing translation invariance feature. The spatial method\noperates directly on the graph data and achieves the convolution\neﬀect by aggregating the information of neighboring nodes.\nGraph attention networks (GATs) are a kind of network based\non an attention mechanism to classify graph-structured data,\nwhich belongs to the spatial method of graph convolutional neural\nnetwork (\nVeliˇckovi´c et al., 2017 ). The basic idea is to calculate the\nhidden representation of each graph node in the graph data by\naggregating the information of neighboring points using the self-\nattention strategy and to deﬁne the information fusion using the\nattention mechanism function. Unlike other graph networks, GAT\ncalculates the association weights by the feature representations\nof the nodes instead of calculating the weights based on the\ninformation of the edges. The input to a graph attention network\nis a series of feature vectors of nodes, which can be expressed\nas H =\n{ − →h 1, − →h 2, . . ., − →h N\n}\n, − →h i ∈ RN×F, where N is the\nnumber of vertices, and F represents feature dimensions. The graph\nattention network uses a self-attentive mechanism to compute the\nattention coeﬃcients of the input feature vectors and normalize\nthem as follows:\neij = a\n(\nW− →h i, W− →h j\n)\n(1)\nα ij = Softmax j\n(\neij\n)\n= exp\n(\neij\n)\n∑\nk∈Ni exp ( eik) (2)\nwhere eij represents attention weights between node i and node\nj, and aij is the normalized attention weight, indicating the\nimportance of node i to node j, − →h is the eigenvector; W is the\nweight matrix in (1, 2). The attention weights and expressions can\nbe represented as follows:\nα ij =\nexp\n(\nLeakyReLU\n( − →a T\n[\nW⃗hi∥W⃗hj\n]))\n∑\nk∈Ni exp\n(\nLeakyReLU\n( − →a T\n[\nW⃗hi∥W⃗hk\n])) (3)\nwhere || denotes the concatenation operation, Ni denotes the\nset of neighboring nodes of the i th node, − →a T represents the\nFrontiers in Human Neuroscience /zero.tnum/three.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nFIGURE /one.tnum\nThe overall structure of STGATE. The model contains two main modu les, Transformer Learning Block and Spatial-Temporal Graph At tention. The ﬁrst\nmodule aims to learn time-frequency representations. The secon d module focuses on building dynamic graph representations by usi ng an attention\nmechanism.\ntranspose of the attention weight vector, and LeakyReLU denotes\nthe nonlinear activation function in Equation (3). To make the\nnetwork more informative, the graph attention network uses a\nmulti-head mechanism that makes each head capture diﬀerent\ninformation. The information from multiple heads is fused through\na linear layer, and the attention coeﬃcients are combined with the\ncorresponding feature vectors to compute the ﬁnal output features\nof each node.\n⃗h′\ni = σ\n\n 1\nK\nK∑\nk=1\n∑\nj∈Ni\nα k\nijWk⃗hj\n\n (4)\nwhere W is the weight matrix of the linear layer, σ is the nonlinear\nactivation function, and ⃗h′\ni is the ﬁnal output vector of the graph\nattention network in Equation (4).\nThe graph attention network assigns diﬀerent weights to the\nnodes (EEG channels) through the attention mechanism, which\neﬀectively improves the representational capability of the network.\nAt the same time, the graph attention network operates very\neﬃciently with a computational complexity of O\n(\n|V|FF′ + |E|F′)\n,\nwhere F is the dimension of the input vector, |V| is the number of\nnodes, and |E| is the number of edges.\n/three.tnum. Methods\nAs shown in Figure 1, the architecture for our proposed model\nconsists of two modules. The upper module is an electrode-level\nlearning block for extracting time-frequency information. The\nbottom module is a dynamic graph convolution for the correlation\nof EEG channels by constructing the adjacency matrix during the\ntraining and testing processes.\n/three.tnum./one.tnum. Transformer learning block\nFigure 1 illustrates the transformer learning block (TLB),\nwhich aims to learn electrode-level time-frequency representations\nfrom EEG signals. TLB comprises two main components. The\nﬁrst component is a stack of multi-kernel convolutions that\ndownsample ﬁve frequency bands and extract multi-scale features.\nPrevious studies have reported that network connections in the\nhigh gamma band are denser among diﬀerent emotional states,\nsuch as happiness, neutrality, and sadness, compared to other\nfrequency bands (\nYang et al., 2020 ). Similarly, Newson and\nThiagarajan (2019) found that emotional disorders are more related\nto higher frequencies, including the alpha, beta, and gamma bands.\nTherefore, emotional states are more relevant to the alpha (8–\n13 Hz), beta (14–30 Hz), and gamma ( >31 Hz) frequency bands\n(\nDing et al., 2020 ). To better make use of all informative frequency\nbands, the kernels of the convolution layers are set to 1, 3, and\n3. The 3*3 convolution layers are adopted followed by the 1*1\nconvolution layer, which aims to add network nonlinear features.\nTransformer-based methods have achieved great success in many\nareas (\nRaganato and Tiedemann, 2018 ; Liu et al., 2021 ). The multi-\nhead self-attention and parallel inputting have superior abilities to\ncapture long-range dependencies. The positional embedding learns\nthe positional information of the sequence. To enhance the long-\nrange dependencies captured in EEG, the second component uses\na transformer encoder to map the EEG sequence to a new encoded\nsequence that contains more temporal information to enhance the\nlong-range dependency capturing ability in EEG.\n/three.tnum./two.tnum. Graph representation\nUsually, EEG signals are measured by placing the electrodes\non the corresponding locations in the human brain scalp, and\nFrontiers in Human Neuroscience /zero.tnum/four.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nthe brain electrodes measure voltage changes generated by neural\nactivity in the cerebral cortex (\nSubha et al., 2010 ). The distribution\nposition of the brain electrodes is deﬁned by some standards,\nsuch as the international 10/20 system. The distribution position\nof the brain electrodes is ﬁxed and regular, so the EEG signal\nchannels can be considered classical non-Euclidean structured data\n(\nMicheloyannis et al., 2006 ), which are well suited to be represented\nby graphical data.\nG = (V, E) (5)\nV = {vi | i = 1, . . ., N} (6)\nE =\n{\neij | vi, vj ∈ V\n}\n(7)\nA =\n{\naij\n}\n(8)\nA segment of EEG signals collected by a brain electrode can\nbe considered as a node of the graph. Therefore, we regard multi-\nchannel EEG signals as a graph. G denotes a graph, V denotes the\nset of vertices in graph G, and E represents the set of edges in\nEquations (5–7). N is the number of brain electrodes in Equation\n(6). In the graph representation of EEG signals, a node vi is usually\nused to represent an EEG electrode, while an edge eij represents the\ncorrelation between nodes vi and vj. A is the adjacency matrix of\ngraph G. aij represents the strength of the correlation of nodes vi\nand vj in Equation (8). The adjacency matrix A is a learnable matrix\nand can be dynamically modiﬁed during the training process.\nGenerally, we model the EEG signal as an undirected graph and use\nthis undirected graph as the input to the adaptive graph module.\nThe initial set of edges of the undirected graph obtained from\nthe above modeling is determined by the kNN algorithm, which\ncomputes graph edges to the nearest k points.\n/three.tnum./three.tnum. Spatial-temporal graph attention\nNeural activity in diﬀerent brain regions has an intrinsic\ncorrelation during the emotional experience. EEG signals recorded\nby brain electrodes can also reﬂect some intrinsic correlation in\ndiﬀerent brain regions. Therefore, we proposed spatial-temporal\ngraph attention (STGAT) to capture correlations between EEG\nelectrodes in the spatial domain and temporal EEG information in\nthe temporal domain. Speciﬁcally, STGAT dynamically learns the\nadjacency matrix A through a spatial attention mechanism during\nthe training process and uses temporal attention to further learn the\ntemporal information in EEG.\nSpatial attention can be implemented with the\nfollowing formula:\nS = V ·σ\n(\nW1XhW2 + bs\n)\n(9)\nA = S − E[S]\n√Var[S] (10)\nwhere S ∈ RB×N×N is a weight matrix, which represents the\nimportance of edges. A represents the dynamic adjacency matrix.\nXh ∈ RB×N×C×Tr is the input of the block. B is the batch size.\nN is the number of vertices of the input data. C represents the\n2D convolution channels. Tr denotes the length of the temporal\ndimension. V, bs ∈ RN×N , W1 ∈ RTr , and W2 ∈ RC×N are\nlearnable parameters, and σ denotes the Tanh activation function.\nWe adopt batch normalization to reduce internal covariate shifts\nand accelerate training (\nSanturkar et al., 2018 ). E[·] and Var[·]\ndenote the mini-batch mean and mini-batch variance of S. The\nvalue of an element Sij indicates the strength of the connection\nbetween node i and node j. We use the spatial attention matrix\nA as the adjacency matrix so that the adjacency matrix can be\ndynamically constructed by the corresponding input features. To\nobtain better representations of EEG signals, we adopt a Top-K\nalgorithm to maintain the 10 edges with the highest weight and\ndiscard the others. The Top-K operation is applied as follows:\n\n\n\n\n\nfor i = 1, 2, . . ., N\nindex = argtopk(A[i, :])\nA[i,\nindex ] = 0\n(11)\nwhere the argtopk(·) is a function to obtain the index of the top-k\nlargest values of each vector in Equation (11). The use of a dynamic\nadjacency matrix in EEG emotion recognition has contributed\nto the ability to dynamically learn the intrinsic relationship\nbetween diﬀerent EEG channels, which can reﬂect the brain\nconnectivity patterns associated with diﬀerent emotional states.\nMoreover, the dynamic adjacency matrix can adapt to diﬀerent\nsubjects, thereby improving the cross-subject generalization\nability of EEG emotion recognition models. By applying graph\nconvolution on multichannel EEG features using the dynamic\nadjacency matrix, more discriminative features can be extracted for\nemotion classiﬁcation.\nTemporal attention is designed to dynamically capture the\ncorrelation between emotional EEG signals in the time domain. The\ntemporal attention mechanism is deﬁned as follows:\nT = V ·σ\n(\nW3XhW4 + bt\n)\n(12)\nˆT = T − E[T]\n√Var[T] (13)\nW3 ∈ RTr and W4 ∈ RC×N are learnable parameters, and σ\ndenotes the tanh activation function. Having the temporal attention\nweight matrix, we tuned the input Xh by the temporal attention:\nˆXh = ˆTXh (14)\nWe utilize temporal attention to focus on valuable temporal\ninformation in EEG-based emotion recognition. The purpose\nof time-domain attention is to uncover the temporal patterns\nin EEG signals and assign importance weights based on their\nintrinsic similarities. By combining spatial attention with temporal\nattention, the model can extract more discriminative features from\nEEG signals and enhance the accuracy of emotion recognition. We\nuse ˆXh as the input to the graph attention network and A as the\nadjacency matrix of the graph data.\nFrontiers in Human Neuroscience /zero.tnum/five.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\n/four.tnum. Experiment\n/four.tnum./one.tnum. Datasets\nThe SEED dataset is an EEG-based dataset collected in the\nBCMI lab of Shanghai Jiao Tong University, known as the\nSJTU Emotion EEG Dataset (\nZheng and Lu, 2015 ). The dataset\ncontains a total of 62 channels of EEG signals from 15 subjects\nfor 15 experiments. The researchers prepared 15 movie clips\nof approximately 4 min, which were divided into 3 categories:\nnegative, neutral, and positive. Positive movies are comedies that\nstimulate positive emotions such as happiness; negative movies are\ntragic movies that stimulate negative emotions such as sadness,\nand neutral movies are world heritage documentaries that do not\nstimulate positive or negative emotions. The subjects were asked to\nwatch these movie clips and were given 45 s to self-evaluate and\ncalm down after each clip was shown.\nThe SEED-IV dataset is also from the BCMI lab (\nZheng\net al., 2018 ). This dataset features 168 movie clips that serve as\na repository for four emotions (happy, sad, fearful, and neutral).\nForty-four participants (22 females, all college students) were\nrecruited to assess their emotions while watching the movie clips\nusing keywords from four discrete emotions (happy, sad, neutral,\nand fearful) and rating 10 points (from –5 to 5) on two dimensions:\nvalence and arousal.\nThe DREAMER dataset is a commonly used emotion\nrecognition dataset (\nKatsigiannis and Ramzan, 2017 ). Researchers\nhad subjects watch edited movie clips to elicit emotions from\nsubjects and recorded EEG data using a 14-channel EEG\nacquisition device. These ﬁlm clips consist of selected scenes\nfrom various movies that have been demonstrated to elicit a\ndiverse array of emotions (\nGabert-Quillen et al., 2015 ). After each\nmovie clip was played, the researchers classiﬁed the emotions\nbased on the subjects’ ratings using 3 dimensions: potency,\narousal, and dominance. The dataset contained 2 clips each of 9\nemotion-evoking movies of happiness, excitement, bliss, calmness,\nanger, disgust, fear, sadness, and surprise, for a total of 18\nmovie clips.\n/four.tnum./two.tnum. Experiment settings\nThe STGATE model is implemented by PyTorch 1.10. The\nhyperparameters were tuned to obtain the best performance on the\nvalidation datasets.\nThe cross-subject experiments are conducted. Since there\nwere multiple subjects, the leave-one-subject-out (LOSO) cross-\nvalidation strategy was applied in the experiments. The EEG data\nof one subject were used as the validation dataset, while the data of\nthe other subjects were used as the training dataset. We repeatedly\nperformed ten rounds of cross-validation experiments and the\naverage accuracy and standard deviation of the test set are adopted\nas the performance criteria. The experiments in this paper use the\nAdam optimizer to accelerate the training process of the model with\na batch size of 16 and a learning rate of 0.00001 (\nKingma and Ba,\n2014). Additionally, we use the Dropout algorithm to suppress the\noverﬁtting phenomenon of the model. The drop rate is set to 0.3.\nDuring the training process, the training set is stopped when the\ntraining loss is lower than 0.15.\n/five.tnum. Results\n/five.tnum./one.tnum. Results of experiments\nTables 1, 2 summarize the experimental results in terms of\nthe average EEG emotion recognition accuracies and standard\ndeviations of the STGATE method. To validate the eﬀectiveness\nof our proposed method for EEG emotion recognition, we\ncompared it with various machine learning and deep learning\nmethods. Conventional classiﬁers such as supported vector\nmachine (SVM) (\nZhong et al., 2020 ) and transductive SVM (T-\nSVM) ( Collobert et al., 2006 ) can be applied for cross-subject\nemotion recognition problems. Domain adaptation methods such\nas Transfer Component Analysis (TCA) (\nPan et al., 2010 )\ncan also handle cross-subject emotion recognition problems.\nPretrained Convolutional Neural Network (CNN) architectures\nhave also been used in emotion recognition tasks (\nCimtay\nand Ekmekcioglu, 2020 ). Rahman et al. proposed a method\nTABLE /one.tnum Leave-one-subject-out emotion recognition (accuracy/standard\ndeviation) on the SEED and SEED-IV datasets.\nModel SEED SEED-IV\nSVM (Zhong et al., 2020 ) 56.73/16.29 37.99/12.52\nT-SVM (Collobert et al., 2006 ) 72.53/14.00 (-)/(-)\nTCA (Pan et al., 2010 ) 63.64/14.88 56.56/13.77\nCNN (Cimtay and Ekmekcioglu, 2020 ) 78.34/6.11 (-)/(-)\nPCA+ANN (Rahman et al., 2020 ) 84.3/(-) (-)/(-)\nRODAN (Lew et al., 2020 ) (-)/(-) 60.75/(-)\nDAN (Li et al., 2018 ) 83.81/8.56 58.87/8.13\nBiHDM (Li et al., 2020 ) 85.40/7.53 69.03/8.66\nDGCNN (Song et al., 2018 ) 79.95/9.02 (-)/(-)\nGCN (Kipf and Welling, 2016 ) 84.95/7.51 72.23/4.01\nGAT (Veliˇckovi´c et al., 2017 ) 85.56/6.75 70.33/4.57\nSTGATE (Ours) 90.37/6.18 76.43/5.01\nThe bold values indicate the maximum value of the current column in table s.\nTABLE /two.tnum Leave-one-subject-out emotion recognition (accuracy/standard\ndeviation) on the DREAMER dataset.\nModel Valence Arousal Average\nSVM (Zheng and Lu, 2015 ) 56.57/(-) 58.91/(-) 57.74/(-)\nDBN (Zheng and Lu, 2015 ) 56.43/(-) 58.94/(-) 57.685/(-)\nDGCNN (Song et al., 2018 ) 59.64/(-) 62.91/(-) 61.28/(-)\nADDA-TCN (He et al., 2022 ) 66.56/10.04 63.69/6.57 65.13/8.31\nGCN (Kipf and Welling, 2016 ) 76.43/10.13 74.01/10.76 75.22/10.45\nGAT (Veliˇckovi´c et al., 2017 ) 75.70/9.64 75.31/10.71 75.51/10.18\nSTGATE (Ours) 77.44/8.40 75.26/9.71 76.35/8.40\nThe bold values indicate the maximum value of the current column in table s.\nFrontiers in Human Neuroscience /zero.tnum/six.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nthat hybridizes Principal Component Analysis (PCA) and t-\nstatistics for feature extraction, and Artiﬁcial Neural Network\n(ANN) is applied for classiﬁcation (\nRahman et al., 2020 ). To\ndeal with the domain shift problem between diﬀerent subjects,\na deep domain adaptation network (DAN) was proposed for\ncross-subject EEG signal recognition (\nLi et al., 2018 ). Likewise,\nto model asymmetric diﬀerences between two hemispheres of\nthe EEG signal, a novel bi-hemispheric discrepancy model\n(BiHDM) was proposed for EEG emotion recognition (\nLi et al.,\n2020). He et al. explored the feasibility of combining Temporal\nConvolutional Networks (TCNs) and Adversarial Discriminative\nDomain Adaptation (ADDA) algorithms to solve the domain\nshift problem in EEG-based cross-subject emotion recognition\n(\nHe et al., 2022 ). The Dynamical Graph Convolutional Neural\nNetwork (DGCNN) is a novel EEG-based emotion recognition\nmodel in which graph spectral convolution operation with\ndynamical adjacent matrix is applied (\nSong et al., 2018 ).\nLew et al. propose a Regionally-Operated Domain Adversarial\nNetwork (RODAN) incorporate the attention mechanism to\nenable cross-domain learning to capture both spatial-temporal\nrelationships among the EEG electrodes and an adversarial\nmechanism to reduce the domain shift in EEG signals (\nLew et al.,\n2020).\nMachine learning methods such as supported vector machine\n(SVM) and transfer component analysis (TCA) can be applied\nto address cross-subject emotion recognition problems. According\nto the experimental results in the SEED dataset and SEED-\nIV dataset, both machine learning methods, SVM, T-SVM and\nTCA, give lower accuracy than deep learning models. The\nperformance of many deep learning methods, such as CNN,\nPCA+ANN, RODAN, DGCNN, DAN, and BiHDM, are better\nthan that of the traditional machine learning methods (SVM and\nTCA), indicating that machine learning has diﬃculty obtaining\nvalid features.\nThe proposed method achieves the highest accuracy in\nSEED, SEED-IV and DREAMER dataset. STGATE achieve 90.37%\nin SEED dataset and 76.43% in SEED-IV dataset. STGATE\nachieves 77.44% in the valence dimension, 75.26% in the arousal\ndimension, and 76.35% in the average value in both dimensions\nin DREAMER dataset, because the proposed STGATE can\nextract more useful information in the temporal and spatial\ndimensions. The proposed method treats EEG signals as non-\nEuclidean data and uses graph representations and attention\nmechanisms to extract the spatial and temporal characteristics of\nEEGs. STGATE compensates for the limitations of convolutional\nneural networks and can handle the feature extraction problem\nof non-Euclidean data with topological graph structure. The\ncombination of the transformer encoder and STGAT enhances the\nperformance of the network. The modeled graph representations\nrestore the spatial and temporal connectivity of the data and\nmake STGATE extract more discriminative emotional features\nthat can be used to accurately classify and identify the emotional\nstates of subjects. The proposed STGATE extracts electrode-level\ninformation through TLB and spatial features based on an adaptive\ngraph structure. Therefore, we can see that our proposed method\nachieves the best accuracy results on the SEED, SEED-IV , and\nDREAMER datasets.\n/five.tnum./two.tnum. Ablation study\nTo verify the eﬀectiveness of each module in STGATE, we\nremoved them one at a time or replaced some of the layers and\nevaluated the performance of the ablated model. As shown in\nTable 3, we trained several models to verify the impact of the\nmodules in STGATE. For the baseline model, we only deployed a\nseries of multi-kernel 2D convolutions during training and testing.\n\"TGAT\" refers to using spatial-temporal graph attention without\nspatial attention and is similar to “SGAT.” The TLB was utilized to\nattentively fuse the node-level features, and the convolutional layers\ndownsampled the multi-channel EEG input and learned the time-\nfrequency representations. The transformer encoder enhanced the\nlong-range dependency capturing ability. According to the results\nshown in\nTable 3, removing the TLB module caused the accuracy\nto drop from 90.27% to 87.88%, a decrease of 2.39%. When we\nremoved the transformer encoder in the TLB, the accuracy dropped\nfrom 86.2% to 83.86%, a decrease of 2.36%. The results show the\neﬀectiveness of the TLB module.\nThe STGAT module aimed to learn the dynamic spatial-\ntemporal representations of the graph. Spatial attention built an\nadaptive adjacency matrix through several learnable parameters,\nmaking the graph structure dynamically change during the\ntraining process. The dynamic adjacency matrix had the potential\nto extract informative correlations among electrodes. Temporal\nattention was similar to spatial attention and captured temporal\ninformation through several learnable parameters. As shown in\nTable 3, removing the STGAT module caused the accuracy to drop\nfrom 90.27% to 86.22%, a decrease of 4.05%. When we removed\nthe spatial and temporal attention in STGAT, the accuracy dropped\nfrom 87.88% to 86.84%, decreasing by 1.04%, and 87.88% to\n86.67%, decreasing by 1.21%, respectively.\nThe performance of the STGATE model outperforms that of\nother models by a signiﬁcant margin. This is attributed to the\nability of the spatial and temporal attention modules to capture\npotential EEG signal features, while the transformer encoder helps\nto enhance long-range dependency capturing ability. The models\nwith STGAT and TLB modules signiﬁcantly outperform those\nwithout these modules. The transformer learning block aggregates\ntime-frequency features using convolution and a transformer\nTABLE /three.tnum Performance of our proposed modules on the SEED and SEED-IV\ndatasets.\nModel SEED SEED-IV\nBaseline 83.86/9.88 71.17/4.48\nTLB 86.22/8.57 73.98/4.12\nSGAT 86.67/6.98 75.39/5.49\nSGAT+TLB 88.56/5.91 75.85/5.06\nTGAT 86.84/7.40 76.04/4.76\nTGAT+TLB 89.30/5.69 76.28/4.06\nSGAT+TGAT (STGAT) 87.88/7.38 75.52/4.30\nSGAT+ TGAT+ TLB (STGATE) 90.37/6.18 76.43/5.01\nThe bold values indicate the maximum value of the current column in table s.\nFrontiers in Human Neuroscience /zero.tnum/seven.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nencoder, while the spatial-temporal graph attention captures\ninter-channel connections via an adaptive adjacency matrix and\ntemporal information using temporal attention. Therefore, both the\ntransformer learning block and the dynamic graph convolution are\nessential components of the STGATE model.\n/five.tnum./three.tnum. The impact of feature selection\nRaw emotional EEG is a non-linear random signal with\na large amount of data redundancy and a low signal-to-noise\nFIGURE /two.tnum\nThe impact of diﬀerent features on SEED accuracy.\nratio ( Balasubramanian et al., 2018 ). The EEG signal features,\nsuch as power spectral density (PSD) and diﬀerential entropy,\nare more representative of the prominent features of the EEG\nsignal in certain aspects. Therefore, the emotion classiﬁcation task\ngenerally uses the feature of the EEG signal for classiﬁcation.\nTherefore, we study the impact of the feature selection of\nSTGATE on classiﬁcation performance.\nFigure 2 shows the\nemotion recognition accuracies and standard deviations of the\nproposed STGATE model with diﬀerent features, including\nDE, PSD, ASM, DASM, and RASM features, in the SEED\ndataset. The DE feature is the most discriminated feature, while\nthe performance of other features is much lower. The DE\nfeature obtained the highest classiﬁcation accuracy (90.37%) and\nlowest standard deviation, followed by PSD (83.17%). RASM\n(rational asymmetry), DASM (diﬀerential asymmetry), and ASM\n(asymmetry) are calculated from DE features designed to express\nasymmetry (\nShi et al., 2013 ). The average accuracies of the\nRASM, DASM, and ASM features are close to each other, 78.07%,\n75.76%, and 77.75%, respectively. The result implies that the DE\nfeature is more suitable for EEG emotion recognition than the\ntraditional feature.\n/five.tnum./four.tnum. Visualization\nAs shown in\nFigure 3, the topographic map is utilized to\nanalyze the inter-channel connections of the learned graphs\nfor emotion recognition in the SEED dataset. The adjacent\nmatrices are extracted at the end of training and transformed\ninto a topographic map. To better show which part of the\nconnections is more informative, we extracted the adjacency\nmatrices of the EEG samples of all the subjects, averaged all\nFIGURE /three.tnum\nTopographic map of adjacency matrices on SEED datasets. The weigh ted electrodes are mainly distributed in the frontal and pariet al lobes.Frontiers in Human Neuroscience /zero.tnum/eight.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nthe matrices, took the largest ten values, and set the others\nto zero. The topographic map of the adjacency matrices is\nshown in\nFigure 3. According to the topographic map, the\nfrontal lobe plays an important role in the classiﬁcation of\nemotions. The F5, FC5, FC1, FZ, F8, AF8, CP2, FC6, FT8,\nand C2 channels have more weight than other channels, which\nmeans that these channels provide more information during\nthe training process. According to previous studies, the pre-\nfrontal, parietal and occipital channels may be the most associated\nwith emotions (\nZheng and Lu, 2015 ; Zhong et al., 2020 ; Ding\net al., 2021 ). The visualization results basically coincide with the\nobservations in neuroscience. Therefore, the topographic map\nindicates that the dynamic adjacency matrix gives more weight to\nthe emotionally relevant EEG channels to enhance the potential\nability of STGATE.\n/six.tnum. Conclusion\nIn this paper, we proposed STGATE, a novel method for\nEEG-based emotion recognition that can dynamically learn the\ninter-channel relationships of EEG emotion signals. The STGATE\nis composed of two modules, TLB and STGAT. The TLB\nmodule employs 2D convolutions and a transformer encoder to\ndownsample EEG signals and capture long-range information. The\nSTGAT module dynamically captures correlations between EEG\nelectrodes in the spatial domain and temporal EEG information\nusing a time-spatial attention mechanism. The experimental\nresults demonstrate that STGATE achieves higher classiﬁcation\naccuracies compared to existing methods for cross-subject EEG-\nbased emotion recognition. However, a limitation of this study\nis the small sample size of the publicly available dataset used\nin the article and the lack of suﬃcient reliable data within the\ndataset. Nevertheless, our proposed method has the potential\nto inspire new methodologies for emotion recognition and\naﬀective computing.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nJL proposed the idea and wrote the manuscript. WP conducted\nthe experiments. HH and JP provided advice on the research\napproaches, signal processing, and checked and revised the\nmanuscript. FW oﬀered important help on guided the experiments\nand analysis methods. All authors contributed to the article and\napproved the submitted version.\nFunding\nThis work was supported by the STI 2030-Major Projects\n2022ZD0208900, the National Natural Science Foundation of\nChina (Grant Nos. 62006082 and 61906019), the Key Realm R\nand D Program of Guangzhou (Grant No. 202007030005), the\nGuangdong Basic and Applied Basic Research Foundation\n(Grant Nos. 2021A1515011600, 2020A1515110294, and\n2021A1515011853), and Guangzhou Science and Technology\nPlan Project (Grant No. 202102020877).\nAcknowledgments\nWe thank the editors, reviewers, and editorial staﬀs who take\npart in the publication process of this paper.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbdel-Hamid, L. (2023). An eﬃcient machine learning-based em otional valence\nrecognition approach towards wearable eeg. Sensors 23, 1255. doi: 10.3390/s230\n31255\nAftanas, L. I., Reva, N. V., Varlamov, A. A., Pavlov, S. V., and M akhnev, V. P.\n(2004). Analysis of evoked eeg synchronization and desynchr onization in conditions\nof emotional activation in humans: temporal and topographic ch aracteristics. Neurosci.\nBehav. Physiol. 34, 859–867. doi: 10.1023/B:NEAB.0000038139.39812.eb\nAkin, M. (2002). Comparison of wavelet transform and ﬀt method s in the analysis\nof eeg signals. J. Med. Syst . 26, 241–247. doi: 10.1023/A:1015075101937\nAlarcao, S. M., and Fonseca, M. J. (2017). Emotions recogniti on using eeg signals: a\nsurvey. IEEE Trans. Aﬀect. Comput . 10, 374–393. doi: 10.1109/TAFFC.2017.2714671\nAlhagry, S., Fahmy, A. A., and El-Khoribi, R. A. (2017). Emotio n recognition\nbased on eeg using lstm recurrent neural network. Emotion 8, 355–358.\ndoi: 10.14569/IJACSA.2017.081046\nAnderson, K., and McOwan, P. W. (2006). A real-time automated system for the\nrecognition of human facial expressions. IEEE Trans. Syst. Man Cybern. B 36, 96–105.\ndoi: 10.1109/TSMCB.2005.854502\nBalasubramanian, G., Kanagasabai, A., Mohan, J., and Seshadr i, N. G. (2018). Music\ninduced emotion using wavelet packet decomposition–an EEG stu dy. Biomed. Signal\nProcess. Control. 42:115–128. doi: 10.1016/j.bspc.2018.01.015\nBrave, S., and Nass, C. (2007). “Emotion in human-computer in teraction, ” in The\nHuman-Computer Interaction Handbook (CRC Press), 103–118.\nFrontiers in Human Neuroscience /zero.tnum/nine.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nBrosch, T., Scherer, K., Grandjean, D., and Sander, D. (2013) . The impact of\nemotion on perception, attention, memory, and decision-maki ng. Swiss Med. Wkly .\n143, w13786-w13786. doi: 10.4414/smw.2013.13786\nCambria, E., Das, D., Bandyopadhyay, S., and Feraco, A. (2017 ). Aﬀective\nComputing and Sentiment Analysis . Springer.\nCastellano, G., Kessous, L., and Caridakis, G. (2008). Emotion Recognition Through\nMultiple Modalities: Face, Body Gesture, Speech . Springer.\nChen, Z., Chen, F., Zhang, L., Ji, T., Fu, K., Zhao, L., et al. (2 020). Bridging the\ngap between spatial and spectral domains: A survey on graph neur al networks. arXiv\npreprint arXiv:2002.11867. doi: 10.48550/arXiv.2002.11867\nCimtay, Y., and Ekmekcioglu, E. (2020). Investigating the us e of pretrained\nconvolutional neural network on cross-subject and cross-da taset eeg emotion\nrecognition. Sensors 20, 2034. doi: 10.3390/s20072034\nCollobert, R., Sinz, F., Weston, J., Bottou, L., and Joachims, T.\n(2006). Large scale transductive svms. J. Mach. Learn. Res . 7, 1687–1712.\ndoi: 10.1016/j.neucom.2017.01.012\nCraik, A., He, Y., and Contreras-Vidal, J. L. (2019). Deep lear ning for\nelectroencephalogram (EEG) classiﬁcation tasks: a review. J. Neural Eng . 16, 031001.\ndoi: 10.1088/1741-2552/ab0ab5\nDavidson, R. J. (2004). What does the prefrontal cortex “do” i n aﬀect:\nperspectives on frontal eeg asymmetry research. Biol. Psychol . 67, 219–234.\ndoi: 10.1016/j.biopsycho.2004.03.008\nDing, Y., Robinson, N., Zeng, Q., Chen, D., Wai, A. A. P., Lee, T.-S., et al. (2020).\n“Tsception: a deep learning framework for emotion detection u sing EEG, ” in 2020\nInternational Joint Conference on Neural Networks (IJCNN) (Glasgow, UK: IEEE), 1–7.\nDing, Y., Robinson, N., Zhang, S., Zeng, Q., and Guan, C. (2021 ). Tsception:\ncapturing temporal dynamics and spatial asymmetry from eeg for emotion recognition.\narXiv preprint arXiv:2104.02935. doi: 10.1109/TAFFC.2022.3169001\nEgger, M., Ley, M., and Hanke, S. (2019). Emotion recognitio n from physiological\nsignal analysis: a review. Electron. Notes Theor. Comput. Sci . 343, 35–55.\ndoi: 10.1016/j.entcs.2019.04.009\nFeng, L., Cheng, C., Zhao, M., Deng, H., and Zhang, Y. (2022). Eeg-\nbased emotion recognition using spatial-temporal graph convolu tional lstm\nwith attention mechanism. IEEE J. Biomed. Health Inform . 26, 5406–5417.\ndoi: 10.1109/JBHI.2022.3198688\nGabert-Quillen, C. A., Bartolini, E. E., Abravanel, B. T., and Sa nislow, C.\nA. (2015). Ratings for emotion ﬁlm clips. Behav. Res. Methods 47, 773–787.\ndoi: 10.3758/s13428-014-0500-0\nGu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., et al. (2018).\nRecent advances in convolutional neural networks. Pattern Recognit . 77, 354–377.\ndoi: 10.1016/j.patcog.2017.10.013\nHadjidimitriou, S. K., and Hadjileontiadis, L. J. (2012). To ward an eeg-based\nrecognition of music liking using time-frequency analysis. IEEE Trans. Biomed. Eng .\n59, 3498–3510. doi: 10.1109/TBME.2012.2217495\nHe, Z., Li, Z., Yang, F., Wang, L., Li, J., Zhou, C., et al. (2020 ). Advances in\nmultimodal emotion recognition based on brain-computer inte rfaces. Brain Sci . 10,\n687. doi: 10.3390/brainsci10100687\nHe, Z., Zhong, Y., and Pan, J. (2022). “Joint temporal convolut ional networks and\nadversarial discriminative domain adaptation for EEG-based cross-subject emotion\nrecognition, ” inICASSP 2022-2022 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) (Singapore: IEEE), 3214–3218.\nHjorth, B. (1970). Eeg analysis based on time domain properties .\nElectroencephalogr. Clin. Neurophysiol . 29, 306–310. doi: 10.1016/0013-4694(70)9\n0143-4\nHuang, E., Zheng, X., Fang, Y., and Zhang, Z. (2022). Classiﬁc ation of motor\nimagery eeg based on time-domain and frequency-domain dual-s tream convolutional\nneural network. IRBM 43, 107–113. doi: 10.1016/j.irbm.2021.04.004\nJenke, R., Peer, A., and Buss, M. (2014). Feature extraction and selection\nfor emotion recognition from EEG. IEEE Trans. Aﬀect. Comput . 5, 327–339.\ndoi: 10.1109/TAFFC.2014.2339834\nJeon, M. (2017). “Emotions and aﬀect in human factors and hum an-computer\ninteraction: taxonomy, theories, approaches, and methods, ” in Emotions and Aﬀect in\nHuman Factors and Human-Computer Interaction , 3–26.\nJerritta, S., Murugappan, M., Nagarajan, R., and Wan, K. (2011 ). “Physiological\nsignals based human emotion recognition: a review, ” in 2011 IEEE 7th International\nColloquium on Signal Processing and its Applications (Penang: IEEE), 410–415.\nJia, Z., Lin, Y., Wang, J., Zhou, R., Ning, X., He, Y., et al. (20 20). “Graphsleepnet:\nadaptive spatial-temporal graph convolutional networks for sleep stage classiﬁcation, ”\nin IJCAI, 1324–1330.\nKatsigiannis, S., and Ramzan, N. (2017). Dreamer: a databas e for emotion\nrecognition through eeg and ecg signals from wireless low-cost oﬀ-the-shelf devices.\nIEEE J. Biomed. Health Inform . 22, 98–107. doi: 10.1109/JBHI.2017.2688239\nKingma, D. P., and Ba, J. (2014). Adam: a method for stochasti c optimization. arXiv\npreprint arXiv:1412.6980. doi: 10.48550/arXiv.1412.6980\nKipf, T. N., and Welling, M. (2016). Semi-supervised classiﬁcati on\nwith graph convolutional networks. arXiv preprint arXiv:1609.02907.\ndoi: 10.48550/arXiv.1609.02907\nKıymık, M. K., Güler, N., Dizibüyük, A., and Akın, M. (2005). Co mparison\nof stft and wavelet transform methods in determining epileptic s eizure activity\nin eeg signals for real-time application. Comput. Biol. Med . 35, 603–616.\ndoi: 10.1016/j.compbiomed.2004.05.001\nLew, W.-C. L., Wang, D., Shylouskaya, K., Zhang, Z., Lim, J.-H ., Ang, K. K., et al.\n(2020). “EEG-based emotion recognition using spatial-tempora l representation via bi-\ngru, ” in2020 42nd Annual International Conference of the IEEE Engineering in Med icine\nBiology Society (EMBC) (Montreal, QC: IEEE), 116–119.\nLi, H., Jin, Y.-M., Zheng, W.-L., and Lu, B.-L. (2018). “Cros s-subject emotion\nrecognition using deep adaptation networks, ” in Neural Information Processing: 25th\nInternational Conference, ICONIP 2018, Siem Reap (Cambodia: Springer), 403–413.\nLi, M., and Lu, B.-L. (2009). “Emotion classiﬁcation based on gamma-band EEG, ” in\n2009 Annual International Conference of the IEEE Engineering in Medicin e and Biology\nSociety (Minneapolis, MN: IEEE), 1223–1226.\nLi, R., Wang, Y., and Lu, B.-L. (2021). “A multi-domain adaptiv e graph\nconvolutional network for eeg-based emotion recognition, ” in Proceedings of the 29th\nACM International Conference on Multimedia , 5565–5573.\nLi, X., Zheng, W., Zong, Y., Chang, H., and Lu, C. (2021). “Att ention-based\nspatio-temporal graphic lstm for eeg emotion recognition, ” in 2021 International Joint\nConference on Neural Networks (IJCNN) (Shenzhen: IEEE), 1–8.\nLi, Y., Wang, L., Zheng, W., Zong, Y., Qi, L., Cui, Z., et al. (20 20). A novel bi-\nhemispheric discrepancy model for EEG emotion recognition. IEEE Trans. Cogn. Dev.\nSyst. 13, 354–367. doi: 10.1109/TCDS.2020.2999337\nLiu, A. T., Li, S.-W., and Lee, H.-y. (2021). Tera: Self-superv ised learning of\ntransformer encoder representation for speech. IEEE/ACM Trans. Audio Speech Lang.\nProcess. 29:2351–2366. doi: 10.1109/TASLP.2021.3095662\nLiu, J., Wu, H., Zhang, L., and Zhao, Y. (2022). “Spatial-tempor al transformers for\neeg emotion recognition, ” in 2022 The 6th International Conference on Advances in\nArtiﬁcial Intelligence , 116–120.\nLiu, Y., and Sourina, O. (2014). Real-Time Subject-Dependent EEG-Based Emotion\nRecognition Algorithm. Springer.\nMicheloyannis, S., Pachou, E., Stam, C. J., Vourkas, M., Erim aki, S., and Tsirka,\nV. (2006). Using graph theoretical analysis of multi channel EE G to evaluate the\nneural eﬃciency hypothesis. Neurosci. Lett . 402, 273–277. doi: 10.1016/j.neulet.2006.\n04.006\nMoontaha, S., Schumann, F. E. F., and Arnrich, B. (2023). Onli ne\nlearning for wearable eeg-based emotion classiﬁcation. Sensors 23, 2387.\ndoi: 10.20944/preprints202301.0156.v1\nNewson, J. J., and Thiagarajan, T. C. (2019). Eeg frequency ba nds in psychiatric\ndisorders: a review of resting state studies. Front. Hum. Neurosci . 12, 521.\ndoi: 10.3389/fnhum.2018.00521\nOzdemir, M. A., Degirmenci, M., Izci, E., and Akan, A. (2021) . Eeg-based\nemotion recognition with deep convolutional neural network s. Biomed. Eng. 66, 43–57.\ndoi: 10.1515/bmt-2019-0306\nPan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. (2010). Doma in adaptation\nvia transfer component analysis. IEEE Trans. Neural Networks 22, 199–210.\ndoi: 10.1109/TNN.2010.2091281\nPeter, C., and Herbon, A. (2006). Emotion representation and\nphysiology assignments in digital systems. Interact Comput . 18, 139–170.\ndoi: 10.1016/j.intcom.2005.10.006\nPoria, S., Cambria, E., Bajpai, R., and Hussain, A. (2017). A r eview of aﬀective\ncomputing: from unimodal analysis to multimodal fusion. Inf. Fusion 37:98–125.\ndoi: 10.1016/j.inﬀus.2017.02.003\nRaganato, A., and Tiedemann, J. (2018). “An analysis of encod er representations in\ntransformer-based machine translation, ” in Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP . The Association\nfor Computational Linguistics.\nRahman, M. A., Hossain, M. F., Hossain, M., and Ahmmed, R. (20 20). Employing\npca and t-statistical approach for feature extraction and class iﬁcation of emotion\nfrom multichannel eeg signal. Egyptian Inform. J . 21, 23–35. doi: 10.1016/j.eij.2019.\n10.002\nSanturkar, S., Tsipras, D., Ilyas, A., and Madry, A. (2018). “H ow does batch\nnormalization help optimization?” in Advances in Neural Information Processing\nSystems, Vol. 31 .\nSartipi, S., Torkamani-Azar, M., and Cetin, M. (2021). “EEG e motion recognition\nvia graph-based spatio-temporal attention neural networks, ” in 2021 43rd Annual\nInternational Conference of the IEEE Engineering in Medicine and Biolo gy Society\n(EMBC) (Mexico: IEEE), 571–574.\nSchuller, B., Rigoll, G., and Lang, M. (2003). “Hidden markov mode l-based speech\nemotion recognition, ” in2003 IEEE International Conference on Acoustics, Speech, and\nSignal Processing, 2003. Proceedings. (ICASSP’03) , volume 2 (Baltimore, MD|: IEEE),\nII-1.\nFrontiers in Human Neuroscience /one.tnum/zero.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnhum./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/six.tnum/nine.tnum/nine.tnum/four.tnum/nine.tnum\nShi, L.-C., Jiao, Y.-Y., and Lu, B.-L. (2013). “Diﬀerential entropy feature for EEG-\nbased vigilance estimation, ” in 2013 35th Annual International Conference of the IEEE\nEngineering in Medicine and Biology Society (EMBC) (Osaka: IEEE), 6627–6630.\nShu, L., Xie, J., Yang, M., Li, Z., Li, Z., Liao, D., et al. (2018 ). A review of emotion\nrecognition using physiological signals. Sensors 18, 2074. doi: 10.3390/s18072074\nSong, T., Zheng, W., Song, P., and Cui, Z. (2018). Eeg emotion recognition using\ndynamical graph convolutional neural networks. IEEE Trans. Aﬀect. Comput . 11,\n532–541. doi: 10.1109/TAFFC.2018.2817622\nStancin, I., Cifrek, M., and Jovic, A. (2021). A review of eeg signal features\nand their application in driver drowsiness detection systems. Sensors 21, 3786.\ndoi: 10.3390/s21113786\nSubha, D. P., Joseph, P. K., Acharya, R., and Lim, C. M. (2010). EEG signal analysis:\na survey. J. Med. Syst . 34, 195–212. doi: 10.1007/s10916-008-9231-z\nSuhaimi, N. S., Mountstephens, J., Teo, J., et al. (2020). EEG-b ased emotion\nrecognition: a state-of-the-art review of current trends a nd opportunities. Comput.\nIntell. Neurosci. 2020, 8875426. doi: 10.1155/2020/8875426\nThammasan, N., Moriyama, K., Fukui, K.-i., and Numao, M. (20 16). Continuous\nmusic-emotion recognition based on electroencephalogram. IEICE Trans. Inf. Syst . 99,\n1234–1241. doi: 10.1587/transinf.2015EDP7251\nTorres, E. P., Torres, E. A., Hernández-Álvarez, M., and Yoo, S. G. (2020). EEG-\nbased bci emotion recognition: a survey. Sensors 20, 5083. doi: 10.3390/s20185083\nTyng, C. M., Amin, H. U., Saad, M. N., and Malik, A. S. (2017). Th e\ninﬂuences of emotion on learning and memory. Front. Psychol . 8, 1454.\ndoi: 10.3389/fpsyg.2017.01454\nVan den Broek, E. L. (2013). Ubiquitous emotion-aware computi ng. Pers. Ubiquit.\nComput. 17, 53–67. doi: 10.1007/s00779-011-0479-9\nVeliˇckovi,´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and\nBengio, Y. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903.\ndoi: 10.48550/arXiv.1710.10903\nWioleta, S. (2013). “Using physiological signals for emotion re cognition, ” in2013 6th\nInternational Conference on Human System Interactions (HSI) (Sopot: IEEE), 556–561.\nWu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S. Y. (2 020). A\ncomprehensive survey on graph neural networks. IEEE Trans. Neural Netw. Learn. Syst .\n32, 4–24. doi: 10.1109/TNNLS.2020.2978386\nXu, T., Zhou, Y., Wang, Z., and Peng, Y. (2018). Learning emot ions eeg-\nbased recognition and brain activity: a survey study on bci f or intelligent\ntutoring system. Procedia Comput. Sci . 130, 376–382. doi: 10.1016/j.procs.2018.\n04.056\nYang, K., Tong, L., Shu, J., Zhuang, N., Yan, B., and Zeng, Y. ( 2020). High gamma\nband eeg closely related to emotion: evidence from functional n etwork. Front. Hum.\nNeurosci. 14, 89. doi: 10.3389/fnhum.2020.00089\nZhang, T., Wang, X., Xu, X., and Chen, C. P. (2019). Gcb-net: Gr aph convolutional\nbroad network and its application in emotion recognition. IEEE Trans. Aﬀect. Comput .\n13, 379–388. doi: 10.1109/TAFFC.2019.2937768\nZhang, Y., Chen, J., Tan, J. H., Chen, Y., Chen, Y., Li, D., et a l. (2020). An\ninvestigation of deep learning models for eeg-based emotion r ecognition. Front.\nNeurosci. 14, 622759. doi: 10.3389/fnins.2020.622759\nZheng, W.-L., Liu, W., Lu, Y., Lu, B.-L., and Cichocki, A. (20 18). Emotionmeter:\na multimodal framework for recognizing human emotions. IEEE Trans. Cybern . 49,\n1110–1122. doi: 10.1109/TCYB.2018.2797176\nZheng, W.-L., and Lu, B.-L. (2015). Investigating critical frequency bands and\nchannels for eeg-based emotion recognition with deep neural networks. IEEE Trans.\nAuton. Ment. Dev . 7, 162–175. doi: 10.1109/TAMD.2015.2431497\nZheng, W.-L., Zhu, J.-Y., and Lu, B.-L. (2017). Identifying stable patterns over\ntime for emotion recognition from eeg. IEEE Trans. Aﬀect. Comput . 10, 417–429.\ndoi: 10.1109/TAFFC.2017.2712143\nZhong, P., Wang, D., and Miao, C. (2020). Eeg-based emotion r ecognition using\nregularized graph neural networks. IEEE Trans. Aﬀect. Comput . 13, 1290–1301.\ndoi: 10.1109/TAFFC.2020.2994159\nFrontiers in Human Neuroscience /one.tnum/one.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6903678774833679
    },
    {
      "name": "Electroencephalography",
      "score": 0.675331711769104
    },
    {
      "name": "Encoder",
      "score": 0.6251310706138611
    },
    {
      "name": "Emotion recognition",
      "score": 0.5633795857429504
    },
    {
      "name": "Graph",
      "score": 0.5259715914726257
    },
    {
      "name": "Transformer",
      "score": 0.5230320692062378
    },
    {
      "name": "Adjacency matrix",
      "score": 0.49436283111572266
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48114144802093506
    },
    {
      "name": "Attention network",
      "score": 0.47571972012519836
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4352790117263794
    },
    {
      "name": "Speech recognition",
      "score": 0.41681379079818726
    },
    {
      "name": "Psychology",
      "score": 0.17789965867996216
    },
    {
      "name": "Neuroscience",
      "score": 0.1278558075428009
    },
    {
      "name": "Theoretical computer science",
      "score": 0.09123948216438293
    },
    {
      "name": "Engineering",
      "score": 0.07144555449485779
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I187400657",
      "name": "South China Normal University",
      "country": "CN"
    }
  ]
}