{
  "title": "Why large language models are poor theories of human linguistic cognition: A reply to Piantadosi",
  "url": "https://openalex.org/W4389784425",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1744836963",
      "name": "Roni Katzir",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A1744836963",
      "name": "Roni Katzir",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4309419389",
    "https://openalex.org/W4236828747",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2793996000",
    "https://openalex.org/W3188901289",
    "https://openalex.org/W4229781645",
    "https://openalex.org/W4206415992",
    "https://openalex.org/W1573042291",
    "https://openalex.org/W1607278777",
    "https://openalex.org/W2000196122",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2127771367",
    "https://openalex.org/W2101820488",
    "https://openalex.org/W1977124973",
    "https://openalex.org/W2092961256",
    "https://openalex.org/W4385681673",
    "https://openalex.org/W3159684727",
    "https://openalex.org/W3133115838",
    "https://openalex.org/W6635165999",
    "https://openalex.org/W4384562887",
    "https://openalex.org/W2005592929",
    "https://openalex.org/W2007601421",
    "https://openalex.org/W2278572546",
    "https://openalex.org/W6948936239",
    "https://openalex.org/W3022354305",
    "https://openalex.org/W2112200207",
    "https://openalex.org/W2805227907",
    "https://openalex.org/W3206978203",
    "https://openalex.org/W2983068647",
    "https://openalex.org/W3167116354",
    "https://openalex.org/W4312063606",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4303579584",
    "https://openalex.org/W4385570848",
    "https://openalex.org/W1599880985"
  ],
  "abstract": "In a recent manuscript entitled “Modern language models refute Chomsky’s approach to language”, Steven Piantadosi proposes that large language models such as GPT-3 can serve as serious theories of human linguistic cognition. In fact, he maintains that these models are significantly better linguistic theories than proposals emerging from within generative linguistics. The present note explains why this claim is wrong.",
  "full_text": "Forum\nWhy Large Language Models Are Poor Theories of \nHuman Linguistic Cognition: A Reply to Piantadosi\nRoni Katzir1,2\n[1] Department of Linguistics, Tel Aviv University, Tel Aviv, Israel. [2] Sagol School of Neuroscience, Tel Aviv University, \nTel Aviv, Israel. \nBiolinguistics, 2023, Vol. 17, Article e13153, https://doi.org/10.5964/bioling.13153\nReceived: 2023-11-01 • Accepted: 2023-11-05 • Published (VoR): 2023-12-15\nHandling Editor: Patrick C. Trettenbrein, Max Planck Institute for Human Cognitive and Brain Sciences & \nUniversity of Göttingen, Germany\nCorresponding Author: Roni Katzir, Department of Linguistics, The Lester and Sally Entin’s Faculty of Humanities, \nTel Aviv University, P.O. box: 39040, 69978 Tel Aviv, Israel. E-mail: rkatzir@tauex.tau.ac.il\nAbstract\nIn a recent manuscript entitled “Modern language models refute Chomsky’s approach to language”, \nSteven Piantadosi proposes that large language models such as GPT-3 can serve as serious theories \nof human linguistic cognition. In fact, he maintains that these models are significantly better \nlinguistic theories than proposals emerging from within generative linguistics. The present note \nexplains why this claim is wrong.\nKeywords\nlarge language models, generative linguistics, learning, generalization, typology, competence, performance\n1. Introduction\nIn a recent manuscript entitled “Modern language models refute Chomsky’s approach \nto language”, Steven Piantadosi (in press)  proposes that large language models (LLMs) \nsuch as GPT-3 can serve as serious theories of human linguistic cognition. In fact, he \nmaintains that these models are significantly better linguistic theories than proposals \nemerging from within generative linguistics. He takes this to amount to a refutation of \nthe generative approach.\nPiantadosi’s proposal is remarkable. Not because he proposes to examine LLMs using \ntools from cognitive science: others have done so fruitfully before (see, e.g., Baroni, 2022; \nGulordava et al. 2018 ; Lakretz et al., 2021 ; Warstadt et al., 2019 ; Wilcox et al., 2022 ). \nRather, what makes Piantadosi’s paper so surprising is his suggestion that LLMs are good \nThis is an open access article distributed under the terms of the Creative Commons \nAttribution 4.0 International License, CC BY 4.0 , which permits unrestricted use, \ndistribution, and reproduction, provided the original work is properly cited.\ntheories of (actual) human cognition. Since LLMs were designed to be useful engineering \ntools, discovering that they teach us about how humans work would be startling indeed, \nakin to discovering that a newly designed drone accidentally solves an open problem in \navian flight. Still, this is Piantadosi’s claim, and the present note shows why it is wrong.1\n2. Linguistic Biases and Representations 1: \nLearning\nIf LLMs are even remotely like humans, their linguistic behavior should be similar to that \nof humans after training on the same amount of linguistic data. This is just a necessary \ncondition of adequacy, and it is a weak one: a model could be very different from us \nand still make the same distinctions that we make after similar training. (In the sections \nbelow I mention multiple ways in which a model can differ fundamentally from us \neven if its linguistic preferences after training are like ours.) But current LLMs fail to \nclear even this low bar, showing that using them to model human linguistic cognition is \nmisguided.\nHere is a simple illustration. In the following exchange, sentence (1) is an acceptable \nEnglish while sentence (2) is not, but ChatGPT thinks that the opposite is true:2\nUser:\nI will give you a pair of sentences. Your task is to tell me which of \nthe two is a better sentence in English.\nGPT:\nSure, please provide the pair of sentences, and I'll help you \ndetermine which one is better in English.\nUser:\n1. The person that Mary met yesterday and that John will talk to Ed \nabout tomorrow arrived.\n2. The person that Mary met yesterday and that John will talk to Ed \nabout his new idea tomorrow arrived.\n1) Obviously there is nothing wrong with considering unusual theories. While scientific work typically focuses on \njust a handful of theories at a time—those that seem the most promising at the moment—it can be a productive \nexercise to occasionally examine alternative hypotheses that don’t seem as promising. But as engineering products \ndeveloped without cognitive plausibility as a goal, LLMs are very unpromising a priori as models of human cognition, \nas mentioned above, and with on the order of 1011 parameters in current models, they are not very likely to be \nparticularly insightful, either. (At this level of complexity one might simply use a living child as a model.) For the \npurposes of the present discussion, though, I will set aside such considerations and focus on standard empirical tests \nof adequacy, showing that LLMs fail on all of them.\n2) Here and below the exchanges with ChatGPT are meant only as convenient illustrations of the general points \nmade in the text, points that concern the systematic properties of LLMs and do not depend on the anecdotal behavior \nof any particular model on a particular example. Unless otherwise specified, all illustrations use exchanges with \nGPT-4 from 2023-04-19.\nLarge Language Models and Human Linguistic Cognition 2\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nGPT:\nSentence 2 is better in English: \"The person that Mary met \nyesterday and that John will talk to Ed about his new idea tomorrow \narrived. \"\nThis sentence is clearer and more informative, as it provides \nadditional context (mentioning the new idea) compared to \nSentence 1.\nThe above is not a random anecdote. It illustrates a robust, systematic generalization, \noriginally noted by Ross (1967): If you leave a gap in one conjunct (“... that Mary met \n__ yesterday”) you need to leave a gap in the other conjunct (“that John will talk to Ed \nabout __ tomorrow” and not “John will talk to Ed about his new idea  tomorrow”) as \nwell. Every English-speaking child has a solid grasp of this generalization, but as we just \nsaw, ChatGPT does not. Other current LLMs seem to do no better: In Lan, Chemla, & \nKatzir (2022) we checked how four different LLMs handle Ross’s generalization, and all \nfour failed. Presumably there are too few examples of this kind in the data to support the \nlearning of Ross’s generalization from a human-sized corpus (or even one several orders \nof magnitude bigger) by a learner that is not suitably biased (see Pearl & Sprouse, 2013 \nfor discussion of the frequency of relevant data in corpora). 3 Children do arrive at the \nrelevant constraint, which shows that they are suitably biased and that LLMs are poor \ntheories of human linguistic cognition.4\nA similar point could be made with any number of textbook linguistic phenomena. \nSee, for example, Yedetore et al. (2023) for an illustration of the same point using the phe­\nnomenon of the reversal of order between the subject and the auxiliary verb in languages \nlike English. In fact, much of the research in theoretical linguistics concerns systematic \naspects of human linguistic cognition that could provide alternative illustrations to the \nsame point: when exposed to corpora that are similar in size to what children receive (or \neven much bigger corpora), LLMs fail to exhibit knowledge of fundamental, systematic \naspects of language. They are too different from us to pass even this elementary test of \nadequacy. And obviously nothing will change in this conclusion if the next GPT, with \neven more parameters and trained on even more data, will be able to acquire a given gen­\neralization. The point is not that the relevant knowledge cannot be learned in principle \nfrom data. (Given suitable biases and representations, the grammatical knowledge that \n3) To be sure, human learners are exposed to much extra-linguistic information that current LLMs are not trained on \nand that could in principle give humans an unfair advantage in certain tasks (in particular, in tasks involving world \nknowledge). However, for Ross’s generalization I know of no reason to think that such extra-linguistic information \nwould be relevant in any way.\n4) This is an instance of the argument from the poverty of the stimulus, a central form of argument in linguistics ever \nsince Chomsky (1971, 1975). Piantadosi claims that LLMs eliminate the argument from the poverty of the stimulus \n(p. 19), but as shown by the failure of LLMs to acquire constraints such as the one just mentioned, this argument is \nstill here.\nKatzir 3\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nlinguists propose can be acquired from a sufficiently large corpus. See Hsu et al., 2013 for \ndiscussion and references. Though whether LLMs can do the same is unclear.) Rather, the \npoint is that what LLMs succeed or fail to learn from a given corpus is entirely different \nfrom what humans succeed or fail to learn from a comparable amount of information. \nThis, in turn, shows that their representations and inductive biases are so different from \nours as to make their use as a model of human linguistic cognition nonsensical.5\n3. Linguistic Biases and Representations 2: \nTypology\nInnate biases express themselves not just in the knowledge attained by children but \nalso in typological tendencies (that is, in cross-linguistic patterns). For example, many \nlanguages exhibit the same constraint as English does with respect to gaps in conjunc­\ntion, and there are no known languages that exhibit the opposite constraint (allowing for \na gap in a single conjunct but not in both). Many other cross-linguistic generalizations \nhave been studied by linguists. For example, no known language allows for an adjunct \nquestion (“how” or “why”) to ask about something that has taken place inside a relative \nclause. So, for example, “How did you know the man who broke the window?” cannot be \nanswered with “with a hammer, ” and “Why did you know the man who broke the win­\ndow?” cannot be answered with “because he was angry”. The same holds for all known \nlanguages in which this has been tested. A generalization of a different kind is that \nphonological processes are always regular: they can be captured with a finite-state device \nthat has no access to working memory. So, for example, while there are many unbounded \nphonological patterns, none are palindrome-like (see Heinz & Idsardi, 2013 ). In syntax, \nmeanwhile, roughly the opposite holds: dependencies are overwhelmingly hierarchical, \nwhile linear processes are rare or nonexistent. Another typological generalization, in yet \nanother domain, is that nominal quantifiers (“every”, “some”, “most”, etc.) are generally \nconservative: when they take two arguments, they only care about individuals that \nsatisfy their first argument (Barwise & Cooper, 1981; Keenan & Stavi, 1986). For example, \nno language has a quantifier “gleeb” such that “Gleeb boys smoke” is true exactly when \nthere are more boys than smokers (a meaning that would care about smokers that are \nnot boys). Note that there is nothing inherently strange about such a meaning. The verb \n“outnumbers” expresses exactly this notion. It’s just that quantifiers seem to never have \nsuch meanings.\n5) Of course, if one is so inclined one can try to make LLMs closer to humans by designing them in advance with \nrepresentations and biases that arise from research in theoretical linguistics. But doing so would amount to reversing \nPiantadosi’s idea: Rather than using LLMs as a linguistic theory one would be using linguistic theory to design LLMs.\nLarge Language Models and Human Linguistic Cognition 4\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nCross-linguistic generalizations of this kind are central reasons for linguists to argue \nfor the innate components that they propose and that look nothing like LLMs. Piantadosi \ndismisses such generalizations, referring to Evans and Levinson (2009) . But Evans and \nLevinson do not address the generalizations just mentioned, and none of the many other \npapers that Piantadosi lists in his discussion of why languages are the way they are (pp. \n25–26) questions the accuracy of these generalizations or offers a way to derive them \nwithout the kinds of biases and representations that linguists argue for. LLMs are not \nbiased in a way that would lead to these generalizations (nor is there any reason to think \nthat just making future LLMs bigger and more powerful than current ones should change \nthis), and in the absence of other explanations for why these generalizations should arise \nfrom an unbiased learner, LLMs remain a deeply implausible model for human linguistic \ncognition.6\n4. Competence vs. Performance\nGenerative linguistics has long maintained that there is a meaningful distinction between \nlinguistic competence and performance ( Chomsky, 1965; Miller & Chomsky 1963; Yngve, \n1960). For example, limitations of working memory are taken to be responsible for the \nrapidly increasing difficulty of processing sentences with center embedding (“The mouse \nthe cat the dog chased bit died”). LLMs make no such distinction, which is generally \nunproblematic given their use as an engineering tool. Piantadosi, however, seems to find \nthe lack of a distinction between competence and performance unproblematic even in \ntheir use as scientific models of human cognition. This is surprising. When humans \nencounter a sentence with center embedding they often make mistakes, but this is \nnot because they don’t know how center embedding works. Rather, it is because of \ntheir struggle to harness real-time processing resources to handle the sentence under \n6) In his appendix Piantadosi suggests that the typological patterns in the present note do not, in fact, favor the \ninnate components that linguists argue for over LLMs. For the generalizations concerning gaps and questions he \nrefers to papers that attempt to reduce constraints on gaps and questions to non-grammatical factors such as \nfrequency, memory, or pragmatics. Linguists have repeatedly explained over the years why such reductions do not \nwork (see Pérez-Leroux & Kahnemuyipour, 2014; Phillips, 2013; Sprouse et al., 2012, among others) and I will not \nrehearse the arguments here. As to the generalization that says that phonology is regular (contrasting with syntax), \nPiantadosi cites evidence for hierarchical structures in non-human species, but I fail to see how this bears on the \npoint made in the main text (namely, that phonology is regular while syntax is overwhelmingly hierarchical and \nthat this motivates representations and biases of the kind linguists argue for but would be entirely unexpected if we \nwere LLMs). As to the generalization that says that nominal quantifiers are conservative, Piantadosi suggests that \nthis can be derived from general principles and does not require the kinds of biases and representations that linguists \nargue for. He supports his claim by referring to several recent papers that study how considerations such as ease \nof learning, simplicity, and efficient communication might affect the typology of quantifiers (Carcassi et al., 2021; \nSteinert-Threlkeld, 2021; Steinert-Threlkeld & Szymanik, 2019; van de Pol et al., 2021). As these papers (and the more \nrecent, van de Pol et al., 2023) make very clear, however, no derivation of the conservativity universal in terms of \nsuch considerations is currently available.\nKatzir 5\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nconsideration. This struggle, which varies with available resources and conditions (noise, \nsleep, etc.) is itself evidence for the distinction between competence and performance. \nAnd given more time, human speakers often succeed where at first they failed. This \nis part of what a theory of linguistic cognition needs to account for, and it is what \nthe distinction between competence and performance provides. But LLMs have no such \ndistinction: their behavior directly reflects their competence, and when they fail it is \ntheir competence that is at fault. For example, the competence of an LLM might support \nthe correct parse of a center-embedded construction, as seems to be the case quite \ngenerally for GPT-4. Alternatively, its competence might support an incorrect parse, as \nseems to be the case for GPT-3.5 in many examples. But either way, it is never the \ncase that an LLM’s competence supports the correct parse and it is its working-memory \nlimitations that make it fail (and possibly recover given more time or memory resources). \nTo illustrate this we need to look at failures and ask whether they stem from competence \nor from performance. GPT-4 is therefore unhelpful for such an illustration in the case of \ncenter embedding, and I will illustrate the point with GPT-3.5 instead:\nUser:\nIn the sentence \"The mouse that the cat that the dog painted taught \nsang\", what did the mouse do? And what did the cat do?\nGPT:\nThe sentence \"The mouse that the cat that the dog painted taught \nsang\" is a grammatically correct sentence, but it is semantically \nnonsensical because it contains a syntactical ambiguity that makes \nit difficult to determine its meaning.\nThe sentence can be interpreted in different ways depending on \nhow one parses it. One possible interpretation is that the mouse \ntaught someone or something to sing, and the cat painted a dog. \nAnother possible interpretation is that the mouse sang a song that \nwas taught to it by a cat that was painted by a dog.\nTherefore, it is impossible to determine with certainty what the \nmouse or the cat did in this sentence without additonal context or \nclarification.\nUser:\nTry to think a bit longer and answer the question again\nGPT:\nI apologize, but even after careful consideration, I must reiterate that \nthe sentence \"The mouse that the cat that the dog painted taught \nsang\" is still semantically nonsensical due to the syntactical \nambiguity it presents. It is not possible to determine with certainty \nwhat the mouse or the cat did in this sentence without further \nclarification or context.\nChatGPT is clearly having difficulties with the center-embedding sentence above. As \nmentioned above, humans do, too. But differently from humans, and as just discussed, \nLarge Language Models and Human Linguistic Cognition 6\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nfurther time and resources are of no use to ChatGPT. When it misparses a sentence, this \nis because its knowledge is flawed, not because processing problems got in the way.\nThe competence-performance distinction helps put into context various apparent \nsimilarities between humans and LLMs that have been reported in the literature. For \nexample, Gulordava et al. (2018) show that LLMs can learn non-local agreement and that \nthey make agreement errors that are in some ways similar to those of human speakers. \nThe competence-performance distinction helps illustrate that these errors are not, in fact, \nhuman-like: when humans make an agreement error and are given a chance to reread \ntheir sentence, they will often correct it; for the LLM, the error is a matter of competence \nand will not be corrected. This is not a problem for Gulordava et al. (2018) , who avoid \nmaking claims about LLMs as linguistic theories, but it is a problem for Piantadosi’s view.\n5. Likely vs. Grammatical\nLLMs also lack a distinction between likelihood and grammaticality. The two notions \noften overlap, but they are conceptually distinct: some things are unlikely but correct, \nand others are likely but incorrect. Human speakers can tease these notions apart (as \nChomsky, 1957 pointed out and as discussed in further detail by Berwick, 2018; Sprouse \net al., 2018 , and others). LLMs cannot: any attempt they might make to judge goodness \nis based on likelihood. 7 This means that they will generally prefer an ungrammatical but \nsomewhat likely continuation over a grammatical but unlikely one:\nUser:\nHere is the beginning of an English sentence: \"The little duck that \nmet the horses with the blue spots yesterday ... \". Which of the \nfollowing two words is a better next word, and why? (a) are, (b) \ndestroys.\nGPT:\nThe better next word in this case would be (a) \"are\". The sentence is \ndescribing a sitation in which the little duck met the horses, so it \nmakes sense to continue with a verb that maintains the context. \n\"Are\" is a plural form of \"to be\" and can be used to describe the state \n7) Piantadosi suggests that probabilities are a good thing in a linguistic model because of compression (pp. 17–18). \nBut he confuses two possible roles for probabilities: as part of linguistic knowledge and as part of the learning model. \nThe generative approach, starting with the remarks that Piantadosi brings from Chomsky (1957), has rejected a role \nfor probabilities within the grammar. (This rejection is based on empirical considerations rather than on any hostility \nto probabilities: if probabilities can be part of the grammar, one would expect to occasionally find grammatical \nprocesses that are sensitive to probabilities; such processes have not yet been found, at least in syntax and semantics, \nso it makes methodological sense to prevent them from being stated within the grammar in the first place.) But \nnon-probabilistic grammars can still be learned in a probabilistic, or compression-based framework. In fact, some \nrecent generative work argues that this is exactly the correct approach to learning (Katzir, 2014; Rasin & Katzir, 2020).\nKatzir 7\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nof the horses or a characteristic they possess. In this case, it could be \nused to further describe the horses with the blue spots.\nOption (b) \"destroys\" changes the context abruptly and introduces a \nviolent action that is not in line with the rest of the sentence. It \nwould not fit well within the context provided by the beginning of \nthe sentence.\nIn this case, ChatGPT chose the ungrammatical but frequent word “are” as the continua­\ntion instead of the grammatical (but perhaps unlikely) continuation “destroys”. Future \nmodels may of course perform better on examples such as the above, but the point \nremains: the distinction between likely and grammatical, which all humans have, is \nentirely foreign to ChatGPT and its fellow LLMs.\n6. Generalization\nTaking a step back from the linguistic behavior of LLMs, let us look at how such models \ngeneralize more broadly. Piantadosi (in press)  likens LLMs to automated scientists, or \nlinguists (p. 20). But consider the following exchange with ChatGPT:\nUser:\nHere are a few strings that follow a certain pattern: aabbbccddd, \nabbcdd, abcd, aaaabccccd, aabbccdd, aaabbbbbcccddddd, aaabcccd. \nCan you list five other strings that follow the same pattern?\nGPT:\nSure! The pattern seems to be a string with increasing consecutive \noccurrences of each letter in the alphabet starting from 'a'. Here are \nfive other strings that follow the same pattern:\n1. aabbcccddddeeeee\n2. aaabbbbccccdd\n3. aabccddeee\n4. aabbccdde\n5. aaabbbbcccccdddddd\nUser:\nAnother string following the same pattern starts like this: aaaaabbc. \nCan you complete the string?\nGPT:\nCertainly! Following the pattern of increasing consecutive \noccurrences of each letter in the alphabet starting from 'a', the \ncompleted string would be: aaaaabbcddddeeeeeee.\nEvery string in the input sequence involves a’s followed by b’s followed by c’s followed \nby d’s, as any human reader can immediately see. A slightly closer glance shows that the \nnumber of a’s and c’s is the same in each string, and similarly for the number of b’s and \nd’s. That is, the sequence conforms to the pattern a mbncmdn. ChatGPT, however, suggests \nLarge Language Models and Human Linguistic Cognition 8\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nthat e’s can also be added (#1, #3, and #4) and seems entirely unaware that a’s and c’s \nshould match in number or that b’s and d’s should match in number. This suggestion \nwould be disappointing if it came from any scientist, automated or otherwise, and it is \nclearly very different from what humans conclude about the same data. And ChatGPT is \nnot alone in this. In Lan, Geyer, et al. (2022) we looked at several current neural networks \nand found that they performed poorly on a range of patterns similar to the one in the \nexchange above. One central factor behind this failure is the learning method: LLMs \nare trained using backpropagation, which pushes the network in very un-scientist-like \ndirections and prevents it from generalizing and reasoning about inputs in anything \neven remotely similar to how humans generalize. When instead of backpropagation \nwe trained networks using Minimum Description Length (MDL)—a learning criterion \nthat does correspond to rational scientific reasoning—the networks were able to find \nperfect solutions to patterns that remained outside the reach of networks trained with \nbackpropagation. Could there eventually be future LLMs (MDL-based or otherwise) that \nwould strike us as scientist-like? Perhaps. But if such models do arrive they will be very \ndifferent from current LLMs. In the meantime, any suggestion that LLMs are automated \nscientists should be treated with suspicion.\n7. Summary\nPiantadosi’s excitement is premature. While LLMs are successful as engineering tools, \nwe saw that they are very poor theories of human linguistic cognition. This is hardly \na critique of the efforts behind these models: to my knowledge, all current LLMs were \ndeveloped with engineering goals rather than the goals of cognitive science in mind. \nNor do I see any reason to doubt that more human-like AI could in principle be built \nin the future. But current LLMs remain the stochastic parrots that Bender et al. (2021) \ntell us they are. 8 And, as highlighted multiple times above, simply making LLMs bigger \nand training them on more data will ensure that nothing changes in this regard. Using \nsuch models to write entertaining poems and short stories is one thing. Using them to \nunderstand the human faculty of language instead of doing actual linguistics is quite \nanother.\n8) Note, however, that differently from LLMs, actual parrots are, in fact, intelligent.\nKatzir 9\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nFunding: The author has no funding to report.\nAcknowledgments: Special thanks to Moysh Bar-Lev, Danny Fox, and Ezer Rasin for detailed comments on several \nversions of this note. I am also grateful to Asaf Bachrach, Gašper Beguš, Bob Berwick, Noam Chomsky, Noam Glazer, \nYosef Grodzinsky, Andreas Haida, Nimrod Katzir, Nur Lan, Roger Levy, Eyal Marco, Matan Mazor, Fereshteh \nModarresi, Jon Rawski, Raj Singh, Dominique Sportiche, Uri Valevski, and the audiences at Tel Aviv University, \nHumboldt University, and École Normale Supérieure, as well as the editorial team at Biolinguistics.\nCompeting Interests: The author has declared that no competing interests exist.\nGeneral Note: This is a minimally-revised version of a note (Katzir, 2023) written in response to a version of \nPiantadosi’s paper that was posted on LingBuzz in March 2023 (Piantadosi, in press, v1). Later versions of his paper \ninclude an appendix in which he replies to several responses to his paper, including Kodner et al. (2023), Milway \n(2023), Moro et al. (2023), Rawski and Baumont (2023), and the present paper. His reply ignores most of the \narguments in the present paper but does attempt to address one of them, and I have now added a brief counter-reply \n(Fn. 6). Page numbers throughout the text refer to the version of Piantadosi’s paper posted on LingBuzz in November \n2023 (Piantadosi, in press, v7).\nReferences\nBaroni, M. (2022). On the proper role of linguistically oriented deep net analysis in linguistic \ntheorising. In J.-P. Bernardy & S. Lappin (Eds.), Algebraic structures in natural language (pp. 1–\n16). CRC Press.\nBarwise, J., & Cooper, R. (1981). Generalized quantifiers and natural language. Linguistics and \nPhilosophy, 4, 159–219. https://doi.org/10.1007/BF00350139\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic \nparrots: Can language models be too big? In Association for Computing Machinery (Ed.), \nProceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610–\n623). Association for Computing Machinery.\nBerwick, R. C. (2018). Revolutionary new ideas appear infrequently. In N. Hornstein, H. Lasnik, P. \nPatel-Grosz, & C. Yang (Eds.), Syntactic structures after 60 years: The impact of the Chomskyan \nrevolution in linguistics (Vol. 60, pp. 177–194). Walter de Gruyter.\nCarcassi, F., Steinert-Threlkeld, S., & Szymanik, J. (2021). Monotone quantifiers emerge via iterated \nlearning. Cognitive Science, 45(8), Article e13027. https://doi.org/10.1111/cogs.13027\nChomsky, N. (1957). Syntactic structures. Mouton.\nChomsky, N. (1965). Aspects of the theory of syntax. MIT Press.\nChomsky, N. (1971). Problems of knowledge and freedom: The Russell lectures. Pantheon Books.\nChomsky, N. (1975). Current issues in linguistic theory. Mouton.\nLarge Language Models and Human Linguistic Cognition 10\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nEvans, N., & Levinson, S. (2009). The myth of language universals: Language diversity and its \nimportance for cognitive science. Behavioral and Brain Sciences, 32, 429–492. \nhttps://doi.org/10.1017/S0140525X0999094X\nGulordava, K., Bojanowski, P., Grave, E., Linzen, T., & Baroni, M. (2018). Colorless green recurrent \nnetworks dream hierarchically. In Association for Computational Linguistics (Ed.), Proceedings \nof NAACL 2018 (pp. 1195–1205). Association for Computational Linguistics\nHeinz, J., & Idsardi, W. (2013). What complexity differences reveal about domains in language. \nTopics in Cognitive Science, 5(1), 111–131. https://doi.org/10.1111/tops.12000\nHsu, A. S., Chater, N., & Vitányi, P. (2013). Language learning from positive evidence, reconsidered: \nA simplicity-based approach. Topics in Cognitive Science, 5(1), 35–55. \nhttps://doi.org/10.1111/tops.12005\nKatzir, R. (2014). A cognitively plausible model for grammar induction. Journal of Language \nModelling, 2(2), 213–248. \nKatzir, R. (2023). Why large language models are poor theories of human linguistic cognition. A reply \nto Piantadosi (2023). https://ling.auf.net/lingbuzz/007190.\nKeenan, E. L., & Stavi, J. (1986). A semantic characterization of natural language determiners. \nLinguistics and Philosophy, 9, 253–326. https://doi.org/10.1007/BF00630273\nKodner, J., Payne, S., & Heinz, J. (2023). Why linguistics will thrive in the 21st century: A reply to \nPiantadosi (2023). arXiv. https://doi.org/10.48550/arXiv.2308.03228\nLakretz, Y., Hupkes, D., Vergallito, A., Marelli, M., Baroni, M., & Dehaene, S. (2021). Mechanisms for \nhandling nested dependencies in neural-network language models and humans. Cognition, 213, \nArticle 104699. https://doi.org/10.1016/j.cognition.2021.104699\nLan, N., Chemla, E., & Katzir, R. (2022). Large language models and the argument from the poverty of \nthe stimulus. https://ling.auf.net/lingbuzz/006829.\nLan, N., Geyer, M., Chemla, E., & Katzir, R. (2022). Minimum description length recurrent neural \nnetworks. Transactions of the Association for Computational Linguistics, 10, 785–799. \nhttps://doi.org/10.1162/tacl_a_00489\nMiller, G., & Chomsky, N. (1963). Finitary models of language users. In R. D. Luce, R. R. Bush, & E. \nGalanter (Eds.), Handbook of mathematical psychology (Vol. 2, pp. 419–491). Wiley.\nMilway, D. (2023). A response to Piantadosi (2023). https://lingbuzz.net/lingbuzz/007264\nMoro, A., Greco, M., & Cappa, S. F. (2023). Large languages, impossible languages and human \nbrains. Cortex, 167, 82–85. https://doi.org/10.1016/j.cortex.2023.07.003\nPearl, L., & Sprouse, J. (2013). Syntactic islands and learning biases: Combining experimental \nsyntax and computational modeling to investigate the language acquisition problem. Language \nAcquisition, 20(1), 23–68. https://doi.org/10.1080/10489223.2012.738742\nPérez-Leroux, A. T., & Kahnemuyipour, A. (2014). News, somewhat exaggerated: Commentary on \nAmbridge, Pine, and Lieven. Language, 90(3), e115–e125. https://doi.org/10.1353/lan.2014.0049\nPhillips, C. (2013). On the nature of island constraints I: Language processing and reductionist \naccounts. In J. Sprouse & N. Hornstein (Eds.), Experimental syntax and island effects (pp. 64–\n108). Cambridge University Press.\nKatzir 11\nBiolinguistics\n2023, Vol. 17, Article e13153\nhttps://doi.org/10.5964/bioling.13153\n\nPiantadosi, S. T. (in press). Modern language models refute Chomsky's approach to language. In E. \nGibson & M. Poliak, (Eds.), From fieldwork to linguistic theory: A tribute to Dan Everett. \nLanguage Science Press. Preprint available at https://ling.auf.net/lingbuzz/007180\nRasin, E., & Katzir, R. (2020). A conditional learnability argument for constraints on underlying \nrepresentations. Journal of Linguistics, 56(4), 745–773. \nhttps://doi.org/10.1017/S0022226720000146\nRawski, J., & Baumont, J. (2023). Modern language models refute nothing. \nhttps://lingbuzz.net/lingbuzz/007203\nRoss, J. R. (1967). Constraints on variables in syntax [Doctoral thesis]. MIT.\nSprouse, J., Wagers, M., & Phillips, C. (2012). A test of the relation between working-memory \ncapacity and syntactic island effects. Language, 88(1), 82–123. \nhttps://doi.org/10.1353/lan.2012.0004\nSprouse, J., Yankama, B., Indurkhya, S., Fong, S., & Berwick, R. C. (2018). Colorless green ideas do \nsleep furiously: Gradient acceptability and the nature of the grammar. Linguistic Review, 35(3), \n575–599. https://doi.org/10.1515/tlr-2018-0005\nSteinert-Threlkeld, S. (2021). Quantifiers in natural language: Efficient communication and degrees \nof semantic universals. Entropy, 23(10), Article 1335. https://doi.org/10.3390/e23101335\nSteinert-Threlkeld, S., & Szymanik, J. (2019). Learnability and semantic universals. Semantics and \nPragmatics, 12(4), Article 4. https://doi.org/10.3765/sp.12.4\nvan de Pol, I., Lodder, P., van Maanen, L., Steinert-Threlkeld, S., & Szymanik, J. (2021). Quantifiers \nsatisfying semantic universals are simpler. Proceedings of the Annual Meeting of the Cognitive \nScience Society, 43, 756–762.\nvan de Pol, I., Lodder, P., van Maanen, L., Steinert-Threlkeld, S., & Szymanik, J. (2023). Quantifiers \nsatisfying semantic universals have shorter minimal description length. Cognition, 232, Article \n105150. https://doi.org/10.1016/j.cognition.2022.105150\nWarstadt, A., Singh, A., & Bowman, S. R. (2019). Neural network acceptability judgments. \nTransactions of the Association for Computational Linguistics, 7, 625–641. \nhttps://doi.org/10.1162/tacl_a_00290\nWilcox, E., Futrell, R., & Levy, R. (2022). Using computational models to test syntactic learnability. \nLinguistic Inquiry. Advance online publication. https://doi.org/10.1162/ling_a_00491\nYedetore, A., Linzen, T., Frank, R., & McCoy, R. T. (2023). How poor is the stimulus? Evaluating \nhierarchical generalization in neural networks trained on child-directed speech. arXiv. \nhttps://doi.org/10.18653/v1/2023.acl-long.521\nYngve, V. H. (1960). A model and a hypothesis for language structure. Proceedings of the American \nPhilosophical Society, 104(5), 444–466. \nLarge Language Models and Human Linguistic Cognition 12\nPsychOpen GOLD is a publishing service by\nLeibniz Institute for Psychology (ZPID), Germany.\nwww.leibniz-psychology.org\n",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7808544635772705
    },
    {
      "name": "Linguistics",
      "score": 0.6696228981018066
    },
    {
      "name": "Theoretical linguistics",
      "score": 0.6344510912895203
    },
    {
      "name": "Cognition",
      "score": 0.586255669593811
    },
    {
      "name": "Cognitive science",
      "score": 0.567789614200592
    },
    {
      "name": "Cognitive linguistics",
      "score": 0.46674424409866333
    },
    {
      "name": "Human language",
      "score": 0.45825305581092834
    },
    {
      "name": "Linguistic description",
      "score": 0.44045963883399963
    },
    {
      "name": "Computer science",
      "score": 0.4040529131889343
    },
    {
      "name": "Psychology",
      "score": 0.36302369832992554
    },
    {
      "name": "Philosophy",
      "score": 0.26686373353004456
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    }
  ]
}