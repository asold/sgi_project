{
  "title": "Unsupervised Neural Machine Translation with Generative Language Models Only",
  "url": "https://openalex.org/W3207771108",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5025285284",
      "name": "Jesse Michael Han",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5085244021",
      "name": "I. Babuschkin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019471288",
      "name": "Harrison Edwards",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5043840749",
      "name": "Arvind Neelakantan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5044212194",
      "name": "Tao Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5078756156",
      "name": "Stanislas Polu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5031013142",
      "name": "Alex Ray",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5083109184",
      "name": "Pranav Shyam",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101696375",
      "name": "Aditya Ramesh",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5051250767",
      "name": "Alec Radford",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5006446297",
      "name": "Ilya Sutskever",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2591807639",
    "https://openalex.org/W2251033195",
    "https://openalex.org/W3034942609",
    "https://openalex.org/W2952444318",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3170253133",
    "https://openalex.org/W3119378114",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W3048713172",
    "https://openalex.org/W3131945380",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W2121745180",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3154863804",
    "https://openalex.org/W3010293452",
    "https://openalex.org/W3201357245"
  ],
  "abstract": "We show how to derive state-of-the-art unsupervised neural machine translation systems from generatively pre-trained language models. Our method consists of three steps: few-shot amplification, distillation, and backtranslation. We first use the zero-shot translation ability of large pre-trained language models to generate translations for a small set of unlabeled sentences. We then amplify these zero-shot translations by using them as few-shot demonstrations for sampling a larger synthetic dataset. This dataset is distilled by discarding the few-shot demonstrations and then fine-tuning. During backtranslation, we repeatedly generate translations for a set of inputs and then fine-tune a single language model on both directions of the translation task at once, ensuring cycle-consistency by swapping the roles of gold monotext and generated translations when fine-tuning. By using our method to leverage GPT-3's zero-shot translation capability, we achieve a new state-of-the-art in unsupervised translation on the WMT14 English-French benchmark, attaining a BLEU score of 42.1.",
  "full_text": "UNSUPERVISED NEURAL MACHINE TRANSLATION\nWITH GENERATIVE LANGUAGE MODELS ONLY\nJesse Michael Han\nIgor Babuschkin Harrison Edwards Arvind Neelakantan Tao Xu Stanislas Polu\nAlex Ray Pranav Shyam Aditya Ramesh Alec Radford Ilya Sutskever\nOpenAI\nABSTRACT\nWe show how to derive state-of-the-art unsupervised neural machine translation\nsystems from generatively pre-trained language models. Our method consists of\nthree steps: few-shot ampliﬁcation, distillation, and backtranslation. We ﬁrst use\nthe zero-shot translation ability of large pre-trained language models to generate\ntranslations for a small set of unlabeled sentences. We then amplify these zero-\nshot translations by using them as few-shot demonstrations for sampling a larger\nsynthetic dataset. This dataset is distilled by discarding the few-shot demonstra-\ntions and then ﬁne-tuning. During backtranslation, we repeatedly generate trans-\nlations for a set of inputs and then ﬁne-tune a single language model on both\ndirections of the translation task at once, ensuring cycle-consistency by swapping\nthe roles of gold monotext and generated translations when ﬁne-tuning. By us-\ning our method to leverage GPT-3’s zero-shot translation capability, we achieve\na new state-of-the-art in unsupervised translation on the WMT14 English-French\nbenchmark, attaining a BLEU score of 42.1.\n1 I NTRODUCTION\nRecent work on generative pre-training has shown that with sufﬁcient data and scale (Kaplan et al.,\n2020; Henighan et al., 2020), large language models (LMs) can learn a diverse suite of tasks without\nexplicit supervision (Radford et al., 2019), and that even stronger performance on these tasks can be\nelicited using few-shot demonstrations (Brown et al., 2020). While few-shot prompting is ﬂexible\nand enables strong performance on a diverse suite of NLP tasks to be coaxed out of generatively\npre-trained LMs without further ﬁne-tuning, its beneﬁts are most pronounced with larger models,\nwith commensurate training, inference, compute, and data costs. Furthermore, the very generality\nof the pre-training objective which enables multi-task learning can produce LMs with more knowl-\nedge than is immediately apparent, requiring carefully designed prompts to bring out fully. The\ndesire to unlock and amplify these latent abilities while also reducing the cost of few-shot prompt-\ning motivates our present work, which allows us to continue ﬁne-tuning our models, obtaining more\nperformance from smaller models and pushing our larger models even further, without resorting to\nfew-shot prompting at test time or any additional supervision at train time.\nWe target the domain of unsupervised neural machine translation (NMT), which typically involves\nbootstrapping a weak translation model before amplifying its translation ability via backtransla-\ntion. Recent work in unsupervised NMT has been dominated by large encoder-decoder architectures\nwhere the bootstrap is implemented by denoising/autoencoding tasks (e.g., multilingual Cloze (De-\nvlin et al., 2019; Conneau & Lample, 2019), masked-span prediction (Raffel et al., 2020; Xue et al.,\n2021), reconstruction from corrupted inputs (Wang et al., 2019; Liu et al., 2020)) intended to pro-\nduce strong encoders and aligned multilingual representations for decoding. In our present work, we\nshow that generative language modeling alone can implement the entire unsupervised NMT pipeline,\n1\narXiv:2110.05448v1  [cs.CL]  11 Oct 2021\nand derive state-of-the-art unsupervised NMT systems using only generatively pre-trained language\nmodels. We implement the bootstrap by ﬁrst sampling a small number of zero-shot translations from\nGPT-3. These are then used as few-shot prompts to sample a larger dataset of synthetic translations.\nThe few-shot prompts are then discarded and the generated samples are distilled by ﬁne-tuning the\nmodel on these synthetic data in the zero-shot format. This produces a language model aligned to\nour translation format and amenable to large-scale backtranslation. By using our method to leverage\nGPT-3’s zero-shot translation capability, we achieve a new state-of-the-art in unsupervised transla-\ntion on the WMT14 English-French benchmark, attaining a BLEU score of 42.1.\n2 B ACKGROUND AND RELATED WORK\nThe modern approach to unsupervised neural machine translation typically involves encoder-\ndecoder architectures jointly trained via denoising autoencoding / reconstruction tasks (Vincent\net al., 2008; Conneau & Lample, 2019; Liu et al., 2020; Ma et al., 2020; Raffel et al., 2020; Xue\net al., 2021; Wang et al., 2019; Liu et al., 2020; Song et al., 2019) and backtranslation (Sennrich\net al., 2016; Edunov et al., 2018; Cotterell & Kreutzer, 2018). This approach to unsupervised NMT\nis codiﬁed by Artetxe et al. (2018) and Lample et al. (2018), although various ideas can be traced\nback further: unsupervised machine translation was framed as a deciphering task by Ravi & Knight\n(2011) and backtranslation was ﬁrst introduced for machine translation as a method for data aug-\nmentation using target-side monolingual data by Sennrich et al. (2016). Denoising autoencoding\nwith a bilingual encoder can be viewed as a kind of latent bilingual lexicon induction, necessary for\nproducing sufﬁciently aligned embeddings to kick-start backtranslation; such techniques have been\nextensively studied in the context of machine translation (Artetxe et al., 2017; Klementiev et al.,\n2012; Vulic & Moens, 2015; Hu et al., 2017; Goyal et al., 2016; Shen et al., 2017).\nAt the same time, recent work on large-scale generative pre-training has demonstrated that with\nsufﬁcient data and model scale (Kaplan et al., 2020; Henighan et al., 2020), transformer language\nmodels begin learning a variety of tasks without explicit supervision (Radford et al., 2019) and\nthat even stronger performance can be coaxed from them using few-shot prompts (Brown et al.,\n2020). Our present work uniﬁes these lines of research by using generative language modeling\nto simplify unsupervised NMT even further: we show how with sufﬁcient scale, pre-training, and\nclever prompting, a single generative language model can implement the entire unsupervised neural\nmachine translation pipeline, avoiding optimizations such as denoising autoencoding, auxiliary /\nadversarial losses in latent space, or ad-hoc bilingual dictionaries.\nOur reliance on large-scale generative pre-training is similar to prior work in unsupervised NMT\nwhich uses large-scale language modeling tasks on internet data as part of the bootstrap (Conneau &\nLample, 2019; Conneau et al., 2020; Liu et al., 2020). The role of few-shot prompting and distillation\nin our method is related to recent work on unsupervised data augmentation using language models\n(Anaby-Tavor et al., 2020; Schick et al., 2021; Kumar et al., 2020; Papanikolaou & Pierleoni, 2020;\nSchick & Sch ¨utze, 2021; Yang et al., 2020) and is also in the same spirit as recent work on self-\ntraining and noisy-student training (Mi et al., 2021; Vu et al., 2021; Xie et al., 2020). Recent work\non scaling laws for neural machine translation has shown that transformer decoders exhibit more\nfavorable scaling than encoders (Ghorbani et al., 2021). The few-shot distillation component of our\nmethod bears some resemblance to contemporaneous work by Wang et al. (2021b) which uses few-\nshot prompting for unsupervised data augmentation, though they focus only on inference for text\nclassiﬁcation rather than generation for sequence-to-sequence tasks like machine translation and do\nnot study the phenomena of self-ampliﬁcation nor few-shot data efﬁciency (Section 6) as we do.\n3 B ACKTRANSLATION VIA LANGUAGE MODELING\nBacktranslation was ﬁrst introduced in the context of machine translation as a method for data aug-\nmentation using target-side monolingual data by sampling synthetic source-to-target data from an-\nother target-to-source translation model (Bojar & Tamchyna, 2011; Sennrich et al., 2016; Poncelas\net al., 2018). In our present work, we cast machine translation as a language modeling task and\njointly train and sample generations from a single language model for both source-to-target and\ntarget-to-source translation.\n2\nAlgorithm 1Iterated backtranslation using a single generative language model\nInput: Source monotext MS; target monotext MT ; number of iterations I; number of samples per\niteration J; monotext formatter f(·); bitext formatter g(·,·); parameters θ of language model\npθ(·) trained to complete outputs of f to outputs of g.\nOutput: Final model parameters θ.\n1: for i= 1to I do\n2: Bback ←∅\n3: for j = 1to J do\n4: y ∼MS ∪MT\n5: ˜x ∼pθ(·| f(y))\n6: Bback ←Bback ∪{⟨˜x,y⟩}\n7: estimate θ by maximizing log pθ of g(˜x,y) for ⟨˜x,y⟩∈B back\nGiven bitext 〈seq1, seq2〉 in languages L1 and L2, we format the translation task as follows:\n[L1] <seq1> [[TRANSLATE]] [L2] <seq2>\nAt test-time, the LM is prompted with [L1] <seq> [[TRANSLATE]] [L2] and we parse\na candidate translation <sampledSeq> from the sampled completion. Backtranslation is im-\nplemented by reversing the roles of seq and sampledSeq and ﬁne-tuning on the bitext\n〈sampledSeq, seq〉.\nWe remark that in contrast to the interpretation of backtranslation as a wake-sleep algorithm (Cot-\nterell & Kreutzer, 2018), where the forwards and backwards translators are trained alternately, we\nuse a single language model for both forwards and backwards translation and train on both direc-\ntions jointly at every iteration. There are various ways to train a model using backtranslation, e.g.,\ncompletely online (interleaving minibatch gradient updates and sampling) versus ofﬂine (backtrans-\nlating the entire training dataset at each epoch; potentially re-training the model from scratch after\nsampling new backtranslations). In practice, we ﬁnd that data scaling of a model’s optimal test loss\nand BLEU score quickly saturates on backtranslations from previous versions of the model, and opt\nfor a semi-online setup where we synchronously sample a relatively small number of L1-L2 and\nL2-L1 pairs before resuming training for a single epoch on the newly sampled data. We refer to this\nas a single iteration of backtranslation.\nFormally, Algorithm 1 describes our implemention of backtranslation using a single generative\nlanguage model pθ(·). We assume that pθ(·) has already been trained to complete format-\nted monotext ( [L1] <seq1> [[TRANSLATE]] [L2]) to formatted bitext ( [L1] <seq1>\n[[TRANSLATE]] [L2] <seq2>).\n4 T HE BOOTSTRAP : GENERATIVE PRE -TRAINING , FEW-SHOT\nAMPLIFICATION , AND DISTILLATION\nThe modern approach to unsupervised NMT is parametrized by a choice of initialization or boot-\nstrap. The bootstrap has typically relied on some form of unsupervised cross-lingual representation\nlearning, e.g., bilingual dictionaries initialized from unsupervised cross-lingual word embeddings\n(Lample et al., 2018; Artetxe et al., 2018) or multilingual masked language modeling followed by\ndenoising autoencoding with a shared encoder and decoder (Conneau & Lample, 2019).\nIn Section 3, we formulated iterative backtranslation in terms of language modeling, assuming a lan-\nguage model which has already been trained to follow a particular instruction format for translation.\nTo complete our procedure, we must supply such a language model. Unlike previous work on un-\nsupervised NMT, we use language models from the GPT-3 family (Brown et al., 2020) which have\nbeen generatively pre-trained on a large corpus of Internet data. A key observation from the body\nof work around GPT-3 is that generative pre-training at scale induces strong in-context metalearning\nabilities, two special cases of which are (1) instruction following and (2) few-shot prompting: a\nsufﬁciently trained large language model beneﬁts from both detailed natural language descriptions\nof tasks and, when given in-context examples, can achieve strong performance on a diverse suite of\n3\n[en]\n[fr]\n[en]\n[fr]\n[en]\n[fr]\n[en]\n[fr]\nprompt\nsample }\nfew-shot prompting finetuning\n[en] I'll pursue solitary pathways through twilit meadows with\nonly this one dream: you come too. [[TRANSLATE]] [fr] Je\nm'éloigne seul dans des chemins obscurs, dans les champs\nde l'ombre, avec seulement ce seul rêve : tu viens aussi.\n[en] [fr][[TRANSLATE]]\nFigure 1: Illustration of our bootstrap procedure, which we call few-shot distillation. We use few-\nshot prompts sampled from GPT-3 to generate an initial dataset of synthetic translations from a gen-\neratively pre-trained language model (left). The few-shot examples are then discarded and the syn-\nthetic bitext reformatted for ﬁne-tuning on the autoregressive language modeling objective (right).\ntasks (e.g., question-answering, natural language inference, translation.) We implement the boot-\nstrap by exploiting both of these abilities, by using natural language instruction to produce zero-shot\ntranslations and few-shot prompting during ampliﬁcation.\n4.1 F EW-SHOT AMPLIFICATION AND DISTILLATION\nIt thus remains to adapt our generatively pre-trained models’ few-shot translation ability to the\nzero-shot format speciﬁed in Section 3. We do this in a two-stage process. We ﬁrst sample a small\nnumber of zero-shot translations from GPT-3. Given bitext 〈srcSeq, tgtSeq〉 in srcLang\nand tgtLang, and a stop-sequence <sep>, we use the following format for zero-shot prompting:\n<sep> Given the following passage in <srcLang>: <sep> <srcSeq> <sep>\na good <tgtLang> translation is: <sep> <tgtSeq> <sep>.\nAt test-time, we sample a completion until the stop-sequence <sep> is detected; throughout we set\n<sep> to be \\n---\\n.\nWe amplify these zero-shot translations by using them as few-shot prompts to sample a much larger\nsynthetic dataset from a smaller model. We then distill this dataset by discarding the few-shot\nprompts and ﬁne-tuning on formatted bitext, producing a language model aligned with our task\nformat and amenable to backtranslation. In detail, we implement the bootstrap as follows:\n1. Generatively pre-train a language model pθ(·) on a large corpus of Internet data.\n2. Sample a pool of NS synthetic target-side translations andNS target-side translations zero-\nshot from another language modelq(·) for few-shot prompting. Using kfew-shot examples\nrandomly drawn from NS (resp. NT ), sample CS synthetic target-side translations (resp.\nCT synthetic source-side translations) from pθ(·), using the monolingual source-side cor-\npus MS (resp. target-side corpus MT ).\n3. Discard the few-shot prompts, reformat the (gold prompt, sampled translation) data as spec-\niﬁed in Section 3, and ﬁne-tune the language model pθ(·) on these data.\n4. Reverse all data and continue ﬁne-tuning the language model pθ(·) on the backtranslations\n(sampled translation, gold prompt).\nWhy amplify and distill? While few-shot prompting is ﬂexible and enables strong performance\non a diverse suite of NLP tasks to be coaxed out of generatively pre-trained LMs, its beneﬁts are\nmost pronounced with larger models, with commensurate training, inference, compute, and data\n4\ncosts. It is also unclear how to iteratively ﬁne-tune a language model in a way that preserves its\nfew-shot ability while remaining aligned with a zero-shot format like in Section 3. Few-shot ampli-\nﬁcation allows us to generate data for the bootstrap in an unsupervised fashion, possibly avoiding\nthe overhead of few-shot sampling from GPT-3 itself by few-shot prompting a smaller modelpθ(·),\nwhile distillation enables iterative backtranslation.\n5 R ESULTS\nExperimental setup For our experiments, we focus on the well-studied WMT14 English-French\nbenchmark. In the notation of Algorithm 1, we obtain source and target monotext MS and MT\nby splitting the WMT14 English-French training set in half, each with approximately twenty mil-\nlion examples, and use only the English text from one half and only French text from the other to\navoid implicit sentence-level alignment between source and target monotext. At each iteration of\nbacktranslation, we sample one million translations in either direction, i.e,. J = 2e6, and train for\none epoch on the newly sampled data. For all of our results, unless otherwise speciﬁed, we run 40\niterations of backtranslation after the bootstrap and report BLEU using the ﬁnal model checkpoint.\nTo implement the bootstrap, we additionally set aside 2048 training examples, and sample NS =\n1024 English-French (resp. NT = 1024 French-English) translations zero-shot from GPT-3 to\nuse as few-shot prompts. During few-shot ampliﬁcation, we sample four million initial target- and\nsource-side translations respectively using few-shot prompts, i.e., CS = CT = 4e6 in the notation\nof Section 4.1, drawing monolingual prompts from as MS and MT deﬁned above. We ﬁne-tune\nfor two epochs in the forwards direction (distillation) and for another two epochs in the backwards\ndirection (initial backtranslation). For few-shot prompting, we use k = 3in-context examples. In\nSection 6.3.1 we will see that we can minimize the number of few-shot examples to NS = NT = 3\nwith little effect on evaluation BLEU score after iterative backtranslation.\nWe use the same training setup and BPE tokenizer as GPT-3. During ﬁne-tuning, we use a constant\nlearning rate of 0.05 ·ℓ, where ℓis the pre-training learning rate, a weight decay of 0.1, and residual\ndropout 0.1. When sampling during the bootstrap or during backtranslation, we default to using\ntemperature τ = 0.3. We ablate other values of τ in Section 6.1. We also ﬁlter all ﬁne-tuning bitext\nby length, discarding pairs with a source/target length ratio exceeding 2.0.\nWe report BLEU score on the ofﬁcial WMT14 English-French test set with greedy (argmax) sam-\npling and sacreBLEU1 (Post, 2018). In Table 3 we give a comparison to previous work on unsuper-\nvised NMT using multi-bleu.perl and the XLM (Conneau & Lample, 2019) tokenizer.\n5.1 F EW-SHOT SELF -DISTILLATION AND BACKTRANSLATION\nWe ﬁrst report results using self-distillation, i.e., where during the bootstrap (Section 4) we sam-\nple from a single model which is then trained to imitate and then backtranslate its own few-shot\nprompted generations; for these experiments, the few-shot demonstrations themselves are generated\nzero-shot by GPT-3. This is then followed by the iterative backtranslation procedure described in\nSection 3. We apply this methodology to the small, medium, large, and xl models from the\nGPT-3 family (Brown et al., 2020), with 125M, 350M, 760M, and 1.3B parameters respectively.\nTable 1 displays test BLEU throughout our procedure for all model sizes. We see that translation\nout of English beneﬁts signiﬁcantly from the backtranslation part of the bootstrap alone. We also\nsee that our models are much stronger at the translation task compared to few-shot prompting after\nonly self-distillation. Finally, all models beneﬁt signiﬁcantly from iterative backtranslation, with\nEnglish-French BLEU always converging to a slightly higher value than the reverse direction.\n5.2 D ISTILLING SELF -AMPLIFIED GPT-3 INTO SMALLER MODELS\nAlthough we do not apply our full methodology to the 175B parameter GPT-3 model due to compute\nconstraints, we observe that for few-shot distillation, instead of training a model on few-shot samples\nfrom itself, we can just as well distill on few-shot samples from a much larger model instead—in\nthis case, the full-size 175B parameter GPT-3 model (henceforth just “GPT-3”). That is, we use\nGPT-3 to self-amplify its own zero-shot translations to produce an initial dataset for distillation.\n1Signature: BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20.\n5\nsmall medium large xl\nfew-shot (τ = 0.0) en-fr 1.15 7.71 13.07 14.28\nfr-en 5.04 16.87 20.25 23.0\nfew-shot (τ = 0.3) en-fr 1.02 7.36 11.89 13.58\nfr-en 4.46 16.13 20.7 22.07\nfew-shot (τ = 1.0) en-fr 0.25 2.12 2.68 3.38\nfr-en 1.22 5.45 6.14 9.32\ndistillation en-fr 0.61 9.51 17.68 22.19\nfr-en 4.31 23.67 29.38 31.12\ninitial backtranslation en-fr 7.94 29.84 33.59 34.71\nfr-en 1.5 23.12 28.58 30.52\nafter backtranslation en-fr 30.48 36.53 37.59 39.12\nfr-en 27.24 32.15 34.79 35.43\nTable 1: English-French (top) and French-English (bottom) test BLEU throughout the few-shot\nself-distillation bootstrap across multiple model scales.\nsmall medium large xl\ndistillation en-fr 34.13 36.03 37.21 37.08\nfr-en 32.34 34.96 36.12 36.34\ninitial backtranslation en-fr 34.71 36.31 38.89 39.05\nfr-en 30.95 33.73 35.16 36.51\nafter backtranslation en-fr 35.62 37.79 38.91 39.79\nfr-en 31.28 34.08 35.57 35.97\nafter backtranslation (+CC100) en-fr 39.02 41.31 41.97 42.08\nfr-en 33.43 35.69 36.85 37.09\nTable 2: English-French (top) and French-English (bottom) test BLEU throughout the bootstrap\nand after iterative backtranslation, this time using generations from self-ampliﬁed GPT-3 for the\nbootstrap. We observe the best performance by mixing in monotext from the English and French\ncomponents of the CC100 dataset (Wenzek et al., 2020; Conneau et al., 2020) during backtranslation.\n6\nWe now proceed to apply the same method as in Section 5.1 to all model sizes, but this time using\nfew-shot samples from GPT-3 for the bootstrap. We display the evaluation BLEU scores throughout\nthe bootstrap and after iterative backtranslation in Table 2. Interestingly, the higher-quality samples\nfrom GPT-3 appear to saturate the smaller models and they improve very little. Motivated by the\npossibility that our models are beginning to overﬁt to the WMT14 English-French training data,\nwe attempt another experiment where 50% of the monotext for backtranslation is sampled from the\nEnglish and French components of the CC100 dataset (Conneau et al., 2020). The extra monolingual\ndata signiﬁcantly beneﬁts all model scales, improving English-French BLEU by approximately 3\npoints compared to iterative backtranslation on WMT data alone. With this setup, the xl attains a\nnew unsupervised state-of-art of 42.1 BLEU on the WMT14 English-French benchmark.\n6 D ISCUSSION AND FURTHER ABLATIONS\nBias towards English generationPrevious work (Brown et al., 2020) has shown that after genera-\ntive pre-training on a corpus of English-dominated Internet text, GPT-3 models are far more capable\nof translating into English than translating out of English. This is reﬂected by the disparity between\nEnglish-French and French-English BLEU scores immediately after few-shot distillation and before\nbacktranslation on the few-shot prompted data. Interestingly, after only two epochs of backtransla-\ntion on the relatively scarce few-shot prompted data, this gap is reversed, with all models achieving\nsigniﬁcantly higher English-French BLEU than French-English BLEU. The data efﬁciency of the\nbootstrap suggests that coming out of pre-training, the models are merely misaligned rather than\ndeﬁcient in knowledge about French, and that their latent knowledge about translation out of En-\nglish can be surfaced using backtranslation. Relatedly, high-quality samples in one language in the\nprevious round of backtranslation lead to higher-quality synthetic bitext for training the reverse di-\nrection in the next. This turns the asymmetry towards English generation into an advantage during\nbacktranslation. However, if the initial disparity between the quality of the translation directions is\nextreme (as with the self-distilled small, which after distillation achieves <1 BLEU for English-\nFrench versus ≈5 BLEU for French-English), then we see that the evaluation BLEU scores for either\ndirection are unstable and oscillates between iterations, though they eventually converge upwards as\nbacktranslation continues.\nComparison to previous work In Table 3, we compare the BLEU scores attained by our best\nmodel (an xl distilled on self-ampliﬁed GPT-3 followed by 40 rounds of backtranslation) to prior\nwork in unsupervised neural machine translation on the WMT14 English-French benchmark. To\nensure comparability to prior work, we report tokenized BLEU usingmulti-bleu.perl and the\nXLM tokenizer. This was used to report the few- and zero-shot performance of GPT-3 in Brown\net al. (2020), which we also include in Table 3 for completeness. We emphasize the improvement\nof our model compared to zero-shot GPT-3, which was used to initialize the bootstrap.\nXLM MASS CUNMT XLM+ CBD xl GPT-3 (fs) GPT-3 (zs)\nen-fr 33.4 37.5 37.6 40.2 38.2 41.7 32.6 25.2\nfr-en 33.3 34.9 35.2 36.9 35.5 38.0 39.2 21.2\nTable 3: Comparison of our best model—an xl distilled on self-ampliﬁed GPT-3 followed by 40\nrounds of iterative backtranslation—to prior work (Conneau & Lample, 2019; Song et al., 2019;\nWang et al., 2021a; Keung et al., 2020; Nguyen et al., 2021) in unsupervised NMT on the WMT14\nEnglish-French benchmark. Bold indicates unsupervised state-of-the-art and underline indicates\nfew-shot state-of-the-art.\nPotential data contamination from pre-training For high-resource language pairs such as\nEnglish-French, naturally occuring demonstrations of translation are virtually guaranteed to appear\nin Common Crawl-like datasets; indeed, Radford et al. (2019) provide examples of English-French\nparallel text embedded in the WebText corpus. Train/test contamination is also a growing area of\nconcern when training large language models on internet-derived data. The data contamination\nstudy conducted by Brown et al. (2020) found virtually no test set contamination for the WMT\ntranslation datasets they considered, including the WMT14 English-French dataset. We emphasize\n7\nthat throughout our entire procedure, no explicit supervision is given for the translation task during\npre-training, distillation, or backtranslation.\n6.1 A BLATING TEMPERATURE FOR FEW -SHOT DISTILLATION\nself-distill backtrans. τ = 0.0 backtrans. τ = 0.3 backtrans. τ = 1.0\nτ = 0.0 en-fr 20.3 34.4 34.7 27.8\nfr-en 29.9 29.3 29.6 24.7\nτ = 0.3 en-fr 20.6 33.9 35.1 27.6\nfr-en 29.2 28.9 29.9 24.4\nτ = 1.0 en-fr 20.2 34.9 34.6 27.6\nfr-en 29.0 29.2 29.2 24.9\nTable 4: English-French (top) and French-English (bottom) test BLEU using few-shot prompted\nsamples generated with temperatures τ = 0.0,0.3,1.0 throughout the bootstrap. We see that the\ntemperature used for sampling has little effect on evaluation BLEU after few-shot distillation, while\nhigh-temperature samples are harmful during the backtranslation part of the bootstrap.\nIt was shown by Edunov et al. (2018) that backtranslation is more effective when the translations are\nslightly noisy, i.e., sampled with nonzero temperature or via a noised beam search. This motivated\nour use of the temperature τ = 0.3 throughout. We ablate this choice of temperature when sampling\ndata for few-shot distillation, and study the effect of usingτ = 0.0 and τ = 1.0 during the bootstrap\nusing a large model. We display the results in Table 4. We see that lower temperatures lead\nto marginally higher test BLEU scores during distillation while τ = 1.0 results in lower test loss\nand no overﬁtting after two epochs of training. However, regardless of the temperature of samples\nused for self-distillation, the differences in both test BLEU and test loss almost vanish after the\nbacktranslation part of the bootstrap when training to backtranslate low temperature samples ( τ =\n0.0 or τ = 0.3).\n6.2 F EW-SHOT SELF -AMPLIFICATION\nWe observed that few-shot prompting GPT-3 with its own zero-shot translations produced better\ntranslations than zero-shot prompting alone. We investigate this further by comparing the BLEU\nscores of zero-shot translations (sampled using the same prompt described in Section 4) to the BLEU\nscores of self-ampliﬁed few-shot prompted translations (i.e., where the few-shot demonstrations are\nthe zero-shot translations sampled from the same model) for all the model sizes studied in this paper.\nOur results are displayed in Table 5. We see that self-ampliﬁcation improves translation quality at\nall model scales.\nsmall medium large xl GPT-3\nzero-shot en-fr 0.57 1.23 1.90 2.84 26.19\nfr-en 2.00 13.92 8.14 19.60 25.49\nself-ampliﬁed en-fr 1.39 8.98 12.46 14.32 29.96\nfr-en 5.76 16.75 21.75 23.98 31.75\nTable 5: Zero-shot versus few-shot self-ampliﬁed test BLEU for all model sizes studied in this paper.\nFor zero-shot generation we use the same prompt format described in Section 4. For self-ampliﬁed\ngeneration, we use the model’s own zero-shot generations as in-context few-shot examples.\n6.3 U SING REAL FEW -SHOT EXAMPLES\nSo far our results have been completely unsupervised, but few-shot learning is typically studied\nin the context of semi-supervised learning (Wang et al., 2020), where the few-shot demonstrations\nare real training data. In this section, we ablate the usage of synthetic few-shot translations in our\n8\nsmall medium large xl\nfew-shot (τ = 0.0) en-fr 1.09 7.19 11.8 13.35\nfr-en 3.86 14.58 20.34 23.01\nfew-shot (τ = 0.3) en-fr 1.09 6.83 11.38 13.08\nfr-en 4.13 14.86 19.92 22.04\nfew-shot (τ = 1.0) en-fr 0.33 1.74 2.34 2.94\nfr-en 0.94 4.18 4.64 7.25\ndistillation en-fr 0.39 7.63 17.27 19.81\nfr-en 3.9 20.29 27.65 30.89\ninitial backtranslation en-fr 7.77 24.71 29.64 33.78\nfr-en 1.7 18.9 26.61 30.93\nafter backtranslation en-fr 31.23 34.42 37.86 39.39\nfr-en 27.45 29.96 34.23 34.97\nTable 6: English-French (top) and French-English (bottom) test BLEU throughout the few-shot self-\ndistillation bootstrap across multiple model scales, this time using real few-shot examples. We see\nthat performance after backtranslation is equivalent to that reported in Table 1.\nsmall large\ndistillation en-fr 32.95 36.0\nfr-en 32.45 36.29\ninitial backtranslation en-fr 36.32 38.72\nfr-en 32.43 36.61\nafter backtranslation en-fr 36.38 39.36\nfr-en 32.66 35.67\nafter backtranslation (+CC100) en-fr 39.01 42.03\nfr-en 34.17 36.94\nTable 7: English-French (top) and French-English (bottom) test BLEU of the small and large\nmodels throughout the bootstrap and after iterative backtranslation, where for the bootstrap we use\ngenerations from 175B GPT-3 prompted using real few-shot examples. Similarly to Table 2, we\nobserve a boost in ﬁnal BLEU score when, after the bootstrap, we additionally sample monolingual\ntext from the English and French portions of the CC100 dataset.\nmethodology and reproduce our experiments from Section 5 using real few-shot demonstrations.\nWe observe virtually no difference in BLEU score after iterative backtranslation.\nWe modify the few-shot prompting described in Section 5 as follows. Rather than sampling zero-\nshot translations for each half of our held-out pool of N=2048 training examples, we sample from\nthese examples directly during few-shot prompting.\nTable 6 displays test BLEU throughout the bootstrap and after iterative backtranslation for the same\nmodel sizes studied in Section 5.1. We see that our models converge to the same test BLEU (c.f. Sec-\ntion 5.1). Table 7 displays analogous results when distilling samples from GPT-3 with the small\nand large models, this time few-shot prompted using real examples. We again see that using real\nrather than synthetic few-shot demonstrations to sample the initial bootstrap data from GPT-3 has\nno effect on ﬁnal BLEU score after iterative backtranslation.\n6.3.1 A LMOST -UNSUPERVISED MACHINE TRANSLATION WITH THREE EXAMPLES ONLY\nFinally, we show that even in the semi-supervised setting, we can minimize the supervision available\nfrom few-shot demonstrations with no difference in test BLEU after backtranslation coverges. Ta-\nble 8 displays the BLEU scores of few-shot sampled translations across various orders of magnitude\n9\nN=3 N=8 N=16 N=32 N=64 N=128 N=256 N=512 N=1024 N=2048\nen-fr 12.6 12.4 12.7 13.1 13.2 13.0 12.7 12.9 12.7 12.8\nfr-en 21.5 21.3 22.1 22.4 21.9 22.3 22.1 22.1 22.2 22.1\nTable 8: BLEU scores (calculated over 4096 random training examples) for the few-shot prompted\ntranslations from a large model, as the total number of available few-shot examples varies from\nN = 3 to N = 2048. We see that N has minimal impact on the BLEU score of the sampled\ntranslations. Moreover, the difference in BLEU between the models bootstrapped using N = 3\nversus N = 2048disappears after iterative backtranslation.\nof N, the number of available few-shot examples. Remarkably, even when N is decreased to 3, there\nis only a slight negative impact on the BLEU score of the few-shot sampled translations. We do not\nablate lower values of N in order to maintain the assumption of k=3 distinct in-context examples\nfor few-shot prompting. We then run our entire procedure with a large model, using N=3 real\nfew-shot demonstrations for the bootstrap followed by iterative backtranslation. We observe a ﬁnal\nEnglish-French BLEU of38.0 and French-English BLEU of34.2, on par with the ﬁnal BLEU scores\nreported in Table 6.\n7 C ONCLUSION AND FUTURE DIRECTIONS\nWe remark that backtranslation, like reinforcement learning, is simply a way of exchanging com-\npute for data. Instead of grounding the model with a reward signal from an environment, however,\nbacktranslation exploits the symmetry of the translation task to ground the model by training it to\ncross-lingually denoise its own samples. Our present work can be viewed as part of a recent trend\ntowards data-driven architecture engineering (Rabe et al., 2021; Wu et al., 2021; Rozi `ere et al.,\n2021), where task-speciﬁc inductive biases, if any, are engineered into and learned from the training\ndata instead of being hardcoded into the model architecture. In formulating the translation task in\nterms of language modeling, we see that the input-output inductive bias imposed by an encoder-\ndecoder architecture can be simulated with prompt formatting. Similarly, we see that generative\nlanguage modeling at sufﬁcient scale combined with clever prompting for automated data gener-\nation can attain state-of-the-art results in unsupervised translation, rendering methods intended to\nproduce strong encoders and aligned multilingual representations unnecessary.\nAlthough we have focused solely on the domain of machine translation in this work, our method-\nology is applicable to any sequence-to-sequence task whose forwards and inverse directions are\n(1) jointly learnable by an autoregressive decoder-only transformer and (2) amenable to few-shot\nprompting after large-scale generative pre-training. Backtranslation is simply reverse self-training\n(Bojar & Tamchyna, 2011) and is fundamentally untied to the translation domain; we invite the re-\nsearch community at large to further explore this technique, moving beyond translation and towards\napplications reﬂecting the full generality of the transformer architecture.\nREFERENCES\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlo-\nmov, Naama Tepper, and Naama Zwerdling. Do not have enough data? deep learning to the\nrescue! In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA,\nFebruary 7-12, 2020, pp. 7383–7390. AAAI Press, 2020. URL https://aaai.org/ojs/\nindex.php/AAAI/article/view/6233.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning bilingual word embeddings with (almost)\nno bilingual data. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July\n30 - August 4, Volume 1: Long Papers, pp. 451–462. Association for Computational Linguistics,\n2017. doi: 10.18653/v1/P17-1042. URL https://doi.org/10.18653/v1/P17-1042.\n10\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine\ntranslation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedings . OpenReview.net, 2018.\nURL https://openreview.net/forum?id=Sy2ogebAW.\nOndrej Bojar and Ales Tamchyna. Improving translation model by monolingual data. In Chris\nCallison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan (eds.),Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, WMT@EMNLP 2011, Edinburgh, Scotland, UK,\nJuly 30-31, 2011, pp. 330–336. Association for Computational Linguistics, 2011. URL https:\n//aclanthology.org/W11-2138/.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot\nlearners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nAlexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Hanna M.\nWallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman\nGarnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pp. 7057–7067, 2019. URL https://proceedings.neurips.cc/paper/\n2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm ´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-\nsupervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Na-\ntalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pp. 8440–8451.\nAssociation for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.747. URL\nhttps://doi.org/10.18653/v1/2020.acl-main.747.\nRyan Cotterell and Julia Kreutzer. Explaining and generalizing back-translation through wake-sleep.\nCoRR, abs/1806.04402, 2018. URL http://arxiv.org/abs/1806.04402.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and\nThamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pp. 4171–\n4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL\nhttps://doi.org/10.18653/v1/n19-1423.\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at\nscale. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.),Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pp. 489–500. Association for Computational Linguistics, 2018.\ndoi: 10.18653/v1/d18-1045. URL https://doi.org/10.18653/v1/d18-1045.\nBehrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Gar-\ncia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. CoRR,\nabs/2109.07740, 2021. URL https://arxiv.org/abs/2109.07740.\nAnirudh Goyal, Alex Lamb, Ying Zhang, Saizheng Zhang, Aaron C. Courville, and Yoshua\nBengio. Professor forcing: A new algorithm for training recurrent networks. In Daniel D.\nLee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.),\n11\nAdvances in Neural Information Processing Systems 29: Annual Conference on Neu-\nral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pp.\n4601–4609, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/\n16026d60ff9b54410b3435b403afd226-Abstract.html.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo\nJun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Rad-\nford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam\nMcCandlish. Scaling laws for autoregressive generative modeling. CoRR, abs/2010.14701, 2020.\nURL https://arxiv.org/abs/2010.14701.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. Controllable text\ngeneration. CoRR, abs/1703.00955, 2017. URL http://arxiv.org/abs/1703.00955.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.\nPhillip Keung, Julian Salazar, Yichao Lu, and Noah A. Smith. Unsupervised bitext mining and\ntranslation via self-trained contextual embeddings. Trans. Assoc. Comput. Linguistics , 8:828–\n841, 2020. URL https://transacl.org/ojs/index.php/tacl/article/view/\n2233.\nAlexandre Klementiev, Ivan Titov, and Binod Bhattarai. Inducing crosslingual distributed represen-\ntations of words. In Martin Kay and Christian Boitet (eds.), COLING 2012, 24th International\nConference on Computational Linguistics, Proceedings of the Conference: Technical Papers,\n8-15 December 2012, Mumbai, India , pp. 1459–1474. Indian Institute of Technology Bombay,\n2012. URL https://aclanthology.org/C12-1089/.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho. Data augmentation using pre-trained trans-\nformer models. CoRR, abs/2003.02245, 2020. URL https://arxiv.org/abs/2003.\n02245.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised\nmachine translation using monolingual corpora only. In 6th International Conference on Learn-\ning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net, 2018. URLhttps://openreview.net/forum?id=\nrkYTTf-AZ.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike\nLewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine transla-\ntion. Trans. Assoc. Comput. Linguistics, 8:726–742, 2020. URL https://transacl.org/\nojs/index.php/tacl/article/view/2107.\nShuming Ma, Jian Yang, Haoyang Huang, Zewen Chi, Li Dong, Dongdong Zhang, Hany Hassan\nAwadalla, Alexandre Muzio, Akiko Eriguchi, Saksham Singhal, Xia Song, Arul Menezes, and\nFuru Wei. XLM-T: scaling up multilingual machine translation with pretrained cross-lingual\ntransformer encoders. CoRR, abs/2012.15547, 2020. URL https://arxiv.org/abs/\n2012.15547.\nFei Mi, Wanhao Zhou, Fengyu Cai, Lingjing Kong, Minlie Huang, and Boi Faltings. Self-\ntraining improves pre-training for few-shot learning in task-oriented dialog systems. CoRR,\nabs/2108.12589, 2021. URL https://arxiv.org/abs/2108.12589.\nXuan-Phi Nguyen, Shaﬁq R. Joty, Thanh-Tung Nguyen, Kui Wu, and Ai Ti Aw. Cross-model\nback-translated distillation for unsupervised machine translation. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Re-\nsearch, pp. 8073–8083. PMLR, 2021. URL http://proceedings.mlr.press/v139/\nnguyen21c.html.\nYannis Papanikolaou and Andrea Pierleoni. DARE: data augmented relation extraction with GPT-2.\nCoRR, abs/2004.13845, 2020. URL https://arxiv.org/abs/2004.13845.\n12\nAlberto Poncelas, Dimitar Sht. Shterionov, Andy Way, Gideon Maillette de Buy Wenniger, and Pey-\nman Passban. Investigating backtranslation in neural machine translation.CoRR, abs/1804.06189,\n2018. URL http://arxiv.org/abs/1804.06189.\nMatt Post. A call for clarity in reporting BLEU scores. In Ondrej Bojar, Rajen Chatterjee, Chris-\ntian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno-\nYepes, Philipp Koehn, Christof Monz, Matteo Negri, Aur ´elie N ´ev´eol, Mariana L. Neves, Matt\nPost, Lucia Specia, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Third Con-\nference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October\n31 - November 1, 2018 , pp. 186–191. Association for Computational Linguistics, 2018. doi:\n10.18653/v1/w18-6319. URL https://doi.org/10.18653/v1/w18-6319.\nMarkus Norman Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. Mathematical reason-\ning via self-supervised skip-tree training. In 9th International Conference on Learning Repre-\nsentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL\nhttps://openreview.net/forum?id=YmqAnY0CMEy.\nAlec Radford, Jeffrey Wu, Rewon Chield, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-\ntext transformer. J. Mach. Learn. Res. , 21:140:1–140:67, 2020. URL http://jmlr.org/\npapers/v21/20-074.html.\nSujith Ravi and Kevin Knight. Deciphering foreign language. In Dekang Lin, Yuji Matsumoto,\nand Rada Mihalcea (eds.), The 49th Annual Meeting of the Association for Computational Lin-\nguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011,\nPortland, Oregon, USA , pp. 12–21. The Association for Computer Linguistics, 2011. URL\nhttps://aclanthology.org/P11-1002/.\nBaptiste Rozi`ere, Marie-Anne Lachaux, Marc Szafraniec, and Guillaume Lample. DOBF: A deob-\nfuscation pre-training objective for programming languages. CoRR, abs/2102.07492, 2021. URL\nhttps://arxiv.org/abs/2102.07492.\nTimo Schick and Hinrich Sch ¨utze. Generating datasets with pretrained language models. CoRR,\nabs/2104.07540, 2021. URL https://arxiv.org/abs/2104.07540.\nTimo Schick, Sahana Udupa, and Hinrich Sch ¨utze. Self-diagnosis and self-debiasing: A proposal\nfor reducing corpus-based bias in NLP. CoRR, abs/2103.00453, 2021. URL https://arxiv.\norg/abs/2103.00453.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long\nPapers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-1009. URL\nhttps://doi.org/10.18653/v1/p16-1009.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. Style transfer from non-\nparallel text by cross-alignment. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett (eds.), Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural In-\nformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pp.\n6830–6841, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n2d2c8394e31101a261abf1784302bf75-Abstract.html.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: masked sequence to se-\nquence pre-training for language generation. In Kamalika Chaudhuri and Ruslan Salakhutdinov\n(eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-\n15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning\nResearch, pp. 5926–5936. PMLR, 2019. URL http://proceedings.mlr.press/v97/\nsong19d.html.\n13\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and\ncomposing robust features with denoising autoencoders. In William W. Cohen, Andrew McCal-\nlum, and Sam T. Roweis (eds.),Machine Learning, Proceedings of the Twenty-Fifth International\nConference (ICML 2008), Helsinki, Finland, June 5-9, 2008 , volume 307 of ACM International\nConference Proceeding Series , pp. 1096–1103. ACM, 2008. doi: 10.1145/1390156.1390294.\nURL https://doi.org/10.1145/1390156.1390294.\nTu Vu, Minh-Thang Luong, Quoc V . Le, Grady Simon, and Mohit Iyyer. Strata: Self-training with\ntask augmentation for better few-shot learning. CoRR, abs/2109.06270, 2021. URL https:\n//arxiv.org/abs/2109.06270.\nIvan Vulic and Marie-Francine Moens. Bilingual word embeddings from non-parallel document-\naligned data applied to bilingual lexicon induction. In Proceedings of the 53rd Annual Meeting\nof the Association for Computational Linguistics and the 7th International Joint Conference on\nNatural Language Processing of the Asian Federation of Natural Language Processing, ACL\n2015, July 26-31, 2015, Beijing, China, Volume 2: Short Papers , pp. 719–725. The Association\nfor Computer Linguistics, 2015. doi: 10.3115/v1/p15-2118. URL https://doi.org/10.\n3115/v1/p15-2118.\nLiang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, and Jingming Liu. Denoising based sequence-to-\nsequence pre-training for text generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xi-\naojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language Process-\ning, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pp. 4001–4013. Asso-\nciation for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1412. URL https:\n//doi.org/10.18653/v1/D19-1412.\nMingxuan Wang, Hongxiao Bai, Lei Li, and Hai Zhao. Cross-lingual supervision improves un-\nsupervised neural machine translation. In Young-bum Kim, Yunyao Li, and Owen Rambow\n(eds.), Proceedings of the 2021 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies: Industry Papers, NAACL-\nHLT 2021, Online, June 6-11, 2021 , pp. 89–96. Association for Computational Linguistics,\n2021a. doi: 10.18653/v1/2021.naacl-industry.12. URL https://doi.org/10.18653/v1/\n2021.naacl-industry.12.\nYaqing Wang, Quanming Yao, James T. Kwok, and Lionel M. Ni. Generalizing from a few ex-\namples: A survey on few-shot learning. ACM Comput. Surv. , 53(3):63:1–63:34, 2020. doi:\n10.1145/3386252. URL https://doi.org/10.1145/3386252.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao. Towards zero-label language learning.\nCoRR, abs/2109.09193, 2021b. URL https://arxiv.org/abs/2109.09193.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm´an,\nArmand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from\nweb crawl data. In Nicoletta Calzolari, Fr ´ed´eric B ´echet, Philippe Blache, Khalid Choukri,\nChristopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mar-\niani, H ´el`ene Mazo, Asunci ´on Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings\nof The 12th Language Resources and Evaluation Conference, LREC 2020, Marseille, France,\nMay 11-16, 2020 , pp. 4003–4012. European Language Resources Association, 2020. URL\nhttps://aclanthology.org/2020.lrec-1.494/.\nYuhuai Wu, Markus N. Rabe, Wenda Li, Jimmy Ba, Roger B. Grosse, and Christian Szegedy. LIME:\nlearning inductive bias for primitives of mathematical reasoning. In Marina Meila and Tong Zhang\n(eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-\n24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pp.\n11251–11262. PMLR, 2021. URL http://proceedings.mlr.press/v139/wu21c.\nhtml.\nQizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and Quoc V . Le. Self-training with\nnoisy student improves imagenet classiﬁcation. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June\n13-19, 2020 , pp. 10684–10695. Computer Vision Foundation / IEEE, 2020. doi:\n14\n10.1109/CVPR42600.2020.01070. URL https://openaccess.thecvf.com/\ncontent_CVPR_2020/html/Xie_Self-Training_With_Noisy_Student_\nImproves_ImageNet_Classification_CVPR_2020_paper.html.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. In\nKristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T¨ur, Iz Beltagy, Steven\nBethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pp. 483–498. As-\nsociation for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.41. URL\nhttps://doi.org/10.18653/v1/2021.naacl-main.41.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-\nPing Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. G-daug: Generative data\naugmentation for commonsense reasoning. In Trevor Cohn, Yulan He, and Yang Liu (eds.),\nFindings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-\n20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 1008–1025. Association for\nComputational Linguistics, 2020. doi: 10.18653/v1/2020.ﬁndings-emnlp.90. URL https:\n//doi.org/10.18653/v1/2020.findings-emnlp.90.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.797301709651947
    },
    {
      "name": "Machine translation",
      "score": 0.7512593269348145
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7071391344070435
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6617377996444702
    },
    {
      "name": "Translation (biology)",
      "score": 0.5846866965293884
    },
    {
      "name": "BLEU",
      "score": 0.5630745887756348
    },
    {
      "name": "Natural language processing",
      "score": 0.5581451654434204
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5387552380561829
    },
    {
      "name": "Language model",
      "score": 0.5071022510528564
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4827541708946228
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4827350974082947
    },
    {
      "name": "Generative grammar",
      "score": 0.47222471237182617
    },
    {
      "name": "Task (project management)",
      "score": 0.4102243483066559
    },
    {
      "name": "Programming language",
      "score": 0.06538504362106323
    },
    {
      "name": "Engineering",
      "score": 0.06234562397003174
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}