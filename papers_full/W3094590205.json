{
  "title": "Real-Time Execution of Large-scale Language Models on Mobile",
  "url": "https://openalex.org/W3094590205",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2123642615",
      "name": "Niu Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227506973",
      "name": "Kong, Zhenglun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115059058",
      "name": "Yuan Geng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354500975",
      "name": "Jiang, Weiwen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282758149",
      "name": "Guan, Jiexiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744413473",
      "name": "Ding, Caiwen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102662307",
      "name": "Zhao Pu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124833733",
      "name": "Liu, Sijia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125634674",
      "name": "Ren, Bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A127007376",
      "name": "Wang, Yanzhi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2043701535",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W2949251082",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2119144962",
    "https://openalex.org/W2904699287",
    "https://openalex.org/W2976833415",
    "https://openalex.org/W2902251695",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2886953980",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2513419314"
  ],
  "abstract": "Pre-trained large-scale language models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. However, the limited weight storage and computational speed on hardware platforms have impeded the popularity of pre-trained models, especially in the era of edge computing. In this paper, we seek to find the best model structure of BERT for a given computation size to match specific devices. We propose the first compiler-aware neural architecture optimization framework. Our framework can guarantee the identified model to meet both resource and real-time specifications of mobile devices, thus achieving real-time execution of large transformer-based models like BERT variants. We evaluate our model on several NLP tasks, achieving competitive results on well-known benchmarks with lower latency on mobile devices. Specifically, our model is 5.2x faster on CPU and 4.1x faster on GPU with 0.5-2% accuracy loss compared with BERT-base. Our overall framework achieves up to 7.8x speedup compared with TensorFlow-Lite with only minor accuracy loss.",
  "full_text": "Real-Time Execution of Large-scale Language Models on Mobile\nWei Niu† 1, Zhenglun Kong† 2, Geng Yuan2, Weiwen Jiang4, Jiexiong Guan1, Caiwen Ding5, Pu\nZhao 2, Sijia Liu3, Bin Ren1, Yanzhi Wang2\n1 College of William and Mary, 2 Northeastern University, 3 MIT-IBM Watson AI Lab, IBM Research,4 University of Notre\nDame, 5 University of Connecticut\n1 wniu@email.wm.edu, 2 kong.zhe@northeastern.edu, 2 yuan.geng@northeastern.edu, 4 wjiang2@nd.edu, 1\njguan@email.wm.edu, 5 caiwen.ding@uconn.edu, 2 zhao.pu@northeastern.edu, 3 sijia.liu@ibm.com, 1 bren@cs.wm.edu, 2\nyanz.wang@northeastern.edu\nAbstract\nPre-trained large-scale language models have increasingly\ndemonstrated high accuracy on many natural language pro-\ncessing (NLP) tasks. However, the limited weight storage and\ncomputational speed on hardware platforms have impeded\nthe popularity of pre-trained models, especially in the era of\nedge computing. In this paper, we seek to ﬁnd the best model\nstructure of BERT for a given computation size to match spe-\nciﬁc devices. We propose the ﬁrst compiler-aware neural ar-\nchitecture optimization framework. Our framework can guar-\nantee the identiﬁed model to meet both resource and real-time\nspeciﬁcations of mobile devices, thus achieving real-time ex-\necution of large transformer-based models like BERT vari-\nants. We evaluate our model on several NLP tasks, achieving\ncompetitive results on well-known benchmarks with lower\nlatency on mobile devices. Speciﬁcally, our model is 5.2 ×\nfaster on CPU and 4.1 × faster on GPU with 0.5-2% accu-\nracy loss compared with BERTBASE. Our overall framework\nachieves up to 7.8× speedup compared with TensorFlow-Lite\nwith only minor accuracy loss.\n1 Introduction\nPre-trained large-scale language models such as BERT (De-\nvlin et al. 2018), XLNet (Yang et al. 2019), RoBERTa (Liu\net al. 2019), and GPT-2 (Radford et al. 2019) have substan-\ntially advanced the state-of-the-art across a wide spectrum\nof NLP tasks. With the increasing popularity of mobile AI\napplications and the concerns of information security and\nprivacy, it is desirable to deploy these well trained models\non edge devices, and furthermore, to meet real-time require-\nments. However, these models often consist of hundreds (or\neven thousands) of layers and hundreds of millions of pa-\nrameters. Therefore, how to accommodate the large and ex-\ntremely deep models, such as BERT to edge device becomes\nan imminent problem.\nThere have been some efforts to compress BERT model\nwhile maintaining the accuracy for downstream NLP tasks.\nMobileBERT (Sun et al. 2020) is able to reduce the memory\nrequirement, but there is still a considerable execution over-\nhead due to the large number of model layers, thus leading to\nhigh inference latency. Moreover, the large number of model\nlayers also brings challenges in compiling models to mobile\n†These authors contributed equally\ndevices. To the best of our knowledge, only TensorFlow-\nLite (TFLite) (TensorFlow 2017) supports deploying BERT\nmodels on mobile CPU (not on mobile GPU), while no other\nframeworks can even support BERT models on mobile CPU.\nIn this paper, we propose the Compiler-Aware Neural Ar-\nchitecture Optimization framework to search for the best\nBERT structure for mobile devices. This is the ﬁrst frame-\nwork that involves compiler optimizations in the NAS loop,\naiming to co-optimize the model accuracy and computation\nresource usage. The architectures generated from the frame-\nwork can be compiled to target mobile devices for real-time\nexecution. The contributions are summarized as follows:\n• We advance a compiler-aware neural architecture opti-\nmization framework to search for a desirable architecture\nfor BERT models to achieve a good balance between ac-\ncuracy and latency.\n• We propose a highly effective layer fusion method to re-\nduce intermediate results to achieve better performance\non both mobile CPU and GPU.\n• We evaluate our framework on multiple BERT variants,\nand compare with a state-of-the-art framework, TFLite,\nproving that our framework outperforms TFLite by up to\n7.8× speedup. Particularly, This is the ﬁrst framework\nsupporting BERT execution on both mobile CPU and\nGPU.\nEvaluation results show that our models can achieve sig-\nniﬁcantly lower latency with minor accuracy loss. Speciﬁ-\ncally, our model (executed on our framework) is 5.2× faster\non CPU and 4.1 × faster on GPU with 0.5-2% accuracy\nloss compared with BERTBASE. These results demonstrate\nthat our framework can achieve real-time execution of large\ntransformer-based models on an off-the-shelf mobile phone.\n2 Motivation\nCurrent transformer-based language models have hundreds\nof millions of parameters. Many of them are extremely deep,\nleading to high inference latency on edge devices. Take\nBERT (BERTBASE) as an example. Its high memory and\ncomputation cost makes it hard to be deployed on edge\ndevices with limited resource. MobileBERT addresses this\nissue by designing a new model based on BERT LARGE\nand distilling it to a small one with the size of 23% of\narXiv:2009.06823v2  [cs.CL]  22 Oct 2020\nFramework overview\nController\nTrainer\nAccuracy\nGenerated \nmodel\nLatency, \nutilization, etc.\nComputational graph\nGraph optimization\nOther code opt\nTesting on: CPU/GPU \nLP fusion\nPolyhedral \ncode generation\nTraining Inference compiler code generation\nLayer fusion\n1)\n1\n3\n2\nFigure 1: Overview of compiler-aware neural architecture\noptimization.\nBERTBASE. However, MobileBERT’s layer count remains\nthe same as BERT LARGE (over 1,000 computation lay-\ners). As a result, although MobileBERT has much fewer\nFLOPs (19%) compared to VGG-16 (Simonyan and Zisser-\nman 2014), it still runs 6.2 × slower than VGG-16 on mo-\nbile, far from real-time execution. Other compact models\n(e.g., DistilBERT and TinyBERT) remove over half num-\nber of layers compared to BERT BASE by using knowledge\ndistillation. These efforts raise a few open questions:\n• Does BERT really need more layers? Under similar\nFLOPs, which type of models show higher accuracy on\ndownstream tasks, wider ones or deeper ones?\n• If deeper models are preferred, how can we accelerate the\ninference to achieve real-time execution?\n3 Compiler-aware Neural Architecture\nOptimization Framework\n3.1 Overview\nAlthough the hardware-aware NAS has been proposed to op-\ntimize network architectures with the awareness of latency;\nhowever, there is still a missing link between neural net-\nwork search and compiler optimization. For instance, all the\nexisting hardware-aware NAS: MnasNet (Tan et al. 2018),\nFBNet (Wu et al. 2018), ProxylessNAS (Cai, Zhu, and Han\n2018) assumes a general, non-optimized compiler. It may\nbe ﬁne for computer vision applications with shallow layers,\nbut for the network with hundreds of layers, the inference\nlatency can easily exceed the target without the optimization\nof the compiler, rendering the hardware-aware NAS useless.\nIn this work, we involve the compiler optimizations in the\nNAS search loop, and propose the ﬁrst compiler-aware neu-\nral architecture optimization framework. Our framework can\nguarantee the identiﬁed model to meet both resource and\nreal-time speciﬁcations of mobile devices, thus achieving\nreal-time execution of large transformer-based models like\nBERT variants while maintaining accuracy. Our framework\nconsists of two processes:training and compiler code gener-\nation (as shown in Fig. 1). The training process consists of a\ncontroller for predicting/generating the model hyperparam-\neters (i.e., network architecture), and a trainer to train the\npredicted model and (quickly) evaluate its accuracy by ﬁne-\ntuning the model to downstream tasks. The compiler code\ngeneration process takes the predicted model and returns ex-\necution information (e.g. latency, number of fused layers,\nCPU/GPU utilization). The execution information together\nwith the model accuracy from the training process will be\nModel MRPC STS-B RTE CoLA Latency\nCPU/GPU\nBERTBASE 88.9 85.8 66.4 52.1 257/186\nDistilBERT 85.0 - 65.5 51.3 145/133\nMobileBERT 88.8 84.4 66.2 50.5 73/69\nBERT(ours) w/o distill. 84.9 81.6 63.8 45.7 60/54\nBERT(ours) 88.5 83.8 65.8 49.7 60/54\nBERT(ours) with NAS 88.4 83.5 65.6 49.2 49/45\nTable 1: Evaluation results on GLUE benchmark. MRPC,\nSTS-B, RTE, and CoLA columns show accuracy, and the\nlast column shows inference latency on mobile CPU and\nGPU (with a unit of ms). All models are optimized with\nlayer fusion and code generation (i.e., they already run faster\nthan their TFLite implementation) with a ﬁxed sequence\nlength of 128. MobileBERT and BERT(ours) are trained\nwith knowledge distillation, while BERT(ours) w/o distill.\nis trained directly from a deep-narrow structure.\nfeedback to the controller to improve the prediction of neu-\nral architectures. After the compiler-aware NAS, the gener-\nated codes by our optimized compiler will be deployed for\nmobile CPU/GPU executions.\n4 Experiments\n4.1 Methodology\nModels and datasets.We test our framework on three main-\nstream BERT models: BERTBASE (Devlin et al. 2018), Dis-\ntilBERT (Sanh et al. 2019), and MobileBERT (Sun et al.\n2020). For pre-training, we use the same corpus as the orig-\ninal BERT model: BooksCorpus (Zhu et al. 2015) and En-\nglish Wikipedia datasets (Devlin et al. 2018). We ﬁne-tune\nthe pre-trained models on GLUE benchmark (Wang et al.\n2018).\nEvaluation setup. Our training is executed on GPU-AI\n(Bridges GPU Artiﬁcial Intelligence) nodes on the Ex-\ntreme Science and Engineering Discovery Environment\n(XSEDE) (Towns et al. 2014). We use two node types: V olta\n32 and V olta 16.We conduct the experiments using Hugging-\nFace Transformer toolkit (Wolf et al. 2019).\nWe evaluate our framework on a Samsung Galaxy S20\ncell phone with Qualcomm Snapdragon 865 which consists\nof a Qualcomm Kryo 585 Octa-core CPU and a Qualcomm\nAdreno 650 GPU. We use a Samsung Galaxy S10 with a\nQualcomm Snapdragon 855 that consists of a Kryo 485\nOcta-core CPU and an Adreno 640 GPU, and an Honor\nMagic 2 with a Kirin 980 that consists of an ARM Octa-\ncore CPU and a Mali-G76 GPU for portability evaluation.\nFor each model, we run our framework and TFLite 100 times\nwith 8 threads on CPU and all pipelines on GPU.\n4.2 Accuracy and Latency Results\nWe compare the accuracy and latency of six models:\nBERTBASE, MobileBERT, DistilBERT, BERT(ours) w/o\ndistillation, BERT(ours), and BERT(ours) with NAS. We ap-\nply layer fusion to all BERT variants to show the effective-\nness of compiler-aware model optimization.\nFramework #FLOPs TFLite Our framework (without layer fusion) Our framework (with layer fusion)\nDevice CPU CPU Speedup GPU Speedup CPU Speedup GPU Speedup\nDistilBERT with NAS 10.9G 188ms 157ms 1.2 × 237ms 0.8 × 105ms 1.8 × 86ms 2.2 ×\nBERTBASE with NAS 21.8G 352ms 276ms 1.3 × 412ms 0.9 × 196ms 1.8 × 147ms 2.4 ×\nBERT(ours) with NAS 4.6G 98ms 89ms 1.1 × 152ms 0.6 × 49ms 2.0 × 45ms 2.2 ×\nTable 2: Inference latency comparison of our framework and TFLite on mobile CPU and GPU, demonstrating effectiveness of\nlayer fusion. All models are generated with English Wikipedia dataset. TFLite does not support BERT on mobile GPU.\nBERT(ours) w/o distillation is directly trained with a 28-\ntransformer block deep-and-narrow structure; BERT(ours)\nis derived from further distilling from a teacher model. Note\nthat BERT(ours) uses the same distillation method as Mo-\nbileBERT. For BERT(ours) with NAS, 200 training epochs\nare used for the overall NAS. The models are evaluated on\nfour downstream tasks: MRPC, STS-B, RTE, and CoLA.\nAccuracy and latency results are shown in Table 1, with\nthe optimizations of our proposed layer fusion. We can see\nthat BERT(ours) improves accuracy by 3-4% compared to\nBERT(ours) w/o distillation under the same latency. All of\nour three models can achieve notably lower latency com-\npared to BERTBASE, DistilBERT, and MobileBERT.\nBy further applying our compiler-aware NAS, which is\nour BERT(ours) with NAS model, we manage to signiﬁ-\ncantly reduce latency compared to BERTBASE, DistilBERT,\nand MobileBERT on both CPU and GPU. Compared with\nBERTBASE, our model is 5.2 × faster on CPU and 4.1 ×\nfaster on GPU with 0.5-2% accuracy loss. Compared with\nMobileBERT, our model is 1.49× faster on CPU and 1.53×\nfaster on GPU with only 0.4-1% accuracy decrease.\n4.3 Effectiveness of Compiler Optimizations\nTable 2 shows inference latency comparison results. The\nfully optimized framework can achieve up to 2.0× speedup\non CPU, and 2.4 × on GPU, over TFLite’s CPU execution.\nNotably, comparing to BERT BASE on TFLite, our frame-\nwork (BERT(ours) with NAS on GPU) can achieve up to\n7.8× speedup. Without compiler optimizations, our baseline\nimplementation runs slightly better than TFLite on CPU, be-\ncause TFLite is already optimized for BERT models. With-\nout compiler optimizations, GPU performance is unusually\nworse than CPU (only 0.6 × speedup for BERT(ours) over\nTFLite on CPU). This is because extremely deep layers gen-\nerate many intermediate data, while mobile GPU memory\nperforms worse than CPU due to its smaller and simpler\ncache hierarchy.\n5 Conclusion\nThis paper presents a compiler-aware neural architecture op-\ntimization framework to search for the best BERT structure\nfor mobile devices. The proposed framework guarantees the\nidentiﬁed model to meet both resource and real-time spec-\niﬁcations of mobile devices, achieving real-time execution\nof large transformer-based models (like BERT and its vari-\nants). The proposed framework achieves up to7.8× speedup\nover TFLite. Our BERT model generated from the frame-\nwork outperforms both BERT BASE and MobileBERT with\nsmall accuracy loss on popular NLP downstream tasks.\nReferences\nCai, H.; Zhu, L.; and Han, S. 2018. ProxylessNAS: Direct\nNeural Architecture Search on Target Task and Hardware.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners.\nSanh, V .; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter.\nSimonyan, K.; and Zisserman, A. 2014. Very Deep Convo-\nlutional Networks for Large-Scale Image Recognition.\nSun, Z.; Yu, H.; Song, X.; Liu, R.; Yang, Y .; and Zhou, D.\n2020. MobileBERT: a Compact Task-Agnostic BERT for\nResource-Limited Devices. Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics\ndoi:10.18653/v1/2020.acl-main.195. URL http://dx.doi.org/\n10.18653/v1/2020.acl-main.195.\nTan, M.; Chen, B.; Pang, R.; Vasudevan, V .; Sandler, M.;\nHoward, A.; and Le, Q. V . 2018. MnasNet: Platform-Aware\nNeural Architecture Search for Mobile.\nTensorFlow. 2017. https://www.tensorﬂow.org/mobile/\ntﬂite/.\nTowns, J.; Cockerill, T.; Dahan, M.; Foster, I.; Gaither,\nK.; Grimshaw, A.; Hazlewood, V .; Lathrop, S.; Lifka, D.;\nPeterson, G. D.; Roskies, R.; Scott, J. R.; and Wilkins-\nDiehr, N. 2014. XSEDE: Accelerating Scientiﬁc Discov-\nery. Computing in Science & Engineering16(5): 62–74.\nISSN 1521-9615. doi:10.1109/MCSE.2014.80. URL doi.\nieeecomputersociety.org/10.1109/MCSE.2014.80.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2018. Glue: A multi-task benchmark and\nanalysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; et al.\n2019. Huggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nWu, B.; Dai, X.; Zhang, P.; Wang, Y .; Sun, F.; Wu, Y .;\nTian, Y .; Vajda, P.; Jia, Y .; and Keutzer, K. 2018. FB-\nNet: Hardware-Aware Efﬁcient ConvNet Design via Differ-\nentiable Neural Architecture Search.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR.; and Le, Q. V . 2019. XLNet: Generalized Autoregressive\nPretraining for Language Understanding.\nZhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning Books and\nMovies: Towards Story-like Visual Explanations by Watch-\ning Movies and Reading Books.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6676431894302368
    },
    {
      "name": "Scale (ratio)",
      "score": 0.6378125548362732
    },
    {
      "name": "Programming language",
      "score": 0.40237170457839966
    },
    {
      "name": "Geography",
      "score": 0.1286841332912445
    },
    {
      "name": "Cartography",
      "score": 0.0967758297920227
    }
  ]
}