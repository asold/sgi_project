{
    "title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
    "url": "https://openalex.org/W4285163271",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3011258465",
            "name": "Oliver Eberle",
            "affiliations": [
                "Technische Universität Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A2093000834",
            "name": "Stephanie Brandl",
            "affiliations": [
                "Technische Universität Berlin",
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A4265770410",
            "name": "Jonas Pilot",
            "affiliations": [
                "Technische Universität Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A2100615786",
            "name": "Anders Søgaard",
            "affiliations": [
                "University of Copenhagen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2252143990",
        "https://openalex.org/W3173936365",
        "https://openalex.org/W2962703144",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W2094539281",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W1497599070",
        "https://openalex.org/W2607262101",
        "https://openalex.org/W2251007032",
        "https://openalex.org/W2973136764",
        "https://openalex.org/W3092785544",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3116662254",
        "https://openalex.org/W2985540049",
        "https://openalex.org/W3037725825",
        "https://openalex.org/W2067359214",
        "https://openalex.org/W2905110202",
        "https://openalex.org/W3152698349",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W1974841121",
        "https://openalex.org/W3034779619",
        "https://openalex.org/W2156548034",
        "https://openalex.org/W2518578398",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2154796476",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W2962816513",
        "https://openalex.org/W3171355829",
        "https://openalex.org/W4211165038",
        "https://openalex.org/W2496369668",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2618905749",
        "https://openalex.org/W4221167913",
        "https://openalex.org/W1986398135",
        "https://openalex.org/W2914854991",
        "https://openalex.org/W1538131130",
        "https://openalex.org/W2946296745",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2529173830",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2949547800",
        "https://openalex.org/W2510850444",
        "https://openalex.org/W2963095307",
        "https://openalex.org/W2117686553",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W2517394272",
        "https://openalex.org/W2947010243",
        "https://openalex.org/W2964178496",
        "https://openalex.org/W2116245261",
        "https://openalex.org/W2250892821",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2033362164",
        "https://openalex.org/W2056667867",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W4230405732",
        "https://openalex.org/W3134751001",
        "https://openalex.org/W2972680241",
        "https://openalex.org/W2986138048",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2251049744",
        "https://openalex.org/W2561412020",
        "https://openalex.org/W1964424952",
        "https://openalex.org/W1979818667",
        "https://openalex.org/W1989110682",
        "https://openalex.org/W2964194677",
        "https://openalex.org/W3153307767",
        "https://openalex.org/W3103099263",
        "https://openalex.org/W4236120890",
        "https://openalex.org/W2111465651",
        "https://openalex.org/W2415973339",
        "https://openalex.org/W2738450183",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W2898936689",
        "https://openalex.org/W3106388706",
        "https://openalex.org/W1787224781",
        "https://openalex.org/W2963973027",
        "https://openalex.org/W2739890004",
        "https://openalex.org/W2984673553",
        "https://openalex.org/W2138292450"
    ],
    "abstract": "Learned self-attention functions in state-of-the-art NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pre-trained self-attention for human attention depends on 'what is in the tail', e.g., the syntactic nature of rare contexts.Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lower-entropy attention vectors are more faithful.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4295 - 4309\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nDo Transformer Models Show Similar Attention Patterns to\nTask-Specific Human Gaze?\nOliver Eberle∗,1, Stephanie Brandl∗,1,2, Jonas Pilot1, Anders Søgaard2\n1Machine Learning Group, TU Berlin, Germany 2University of Copenhagen, Denmark\noliver.eberle@tu-berlin.de, {brandl, soegaard}@di.ku.dk\n∗Authors contributed equally.\nAbstract\nLearned self-attention functions in state-of-the-\nart NLP models often correlate with human\nattention. We investigate whether self-attention\nin large-scale pre-trained language models is as\npredictive of human eye fixation patterns dur-\ning task-reading as classical cognitive models\nof human attention. We compare attention func-\ntions across two task-specific reading datasets\nfor sentiment analysis and relation extraction.\nWe find the predictiveness of large-scale pre-\ntrained self-attention for human attention de-\npends on ‘what is in the tail’, e.g., the syntactic\nnature of rare contexts. Further, we observe\nthat task-specific fine-tuning does not increase\nthe correlation with human task-specific read-\ning. Through an input reduction experiment\nwe give complementary insights on the spar-\nsity and fidelity trade-off, showing that lower-\nentropy attention vectors are more faithful.\n1 Introduction\nThe usefulness of learned self-attention functions\noften correlates with how well it aligns with human\nattention (Das et al., 2016; Klerke et al., 2016; Bar-\nrett et al., 2018; Zhang and Zhang, 2019; Klerke\nand Plank, 2019). In this paper, we evaluate how\nwell attention flow (Abnar and Zuidema, 2020)\nin large language models, namely BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019) and T5\n(Raffel et al., 2020), aligns with human eye fix-\nations during task-specific reading, compared to\nother shallow sequence labeling models (Lecun\nand Bengio, 1995; Vaswani et al., 2017) and a clas-\nsic, heuristic model of human reading (Reichle\net al., 2003). We compare the learned attention\nfunctions and the heuristic model across two task-\nspecific English reading tasks, namely sentiment\nanalysis (SST movie reviews) and relation extrac-\ntion (Wikipedia), as well as natural reading, us-\ning a publicly available dataset with eye-tracking\nrecordings of native speakers of English (Hollen-\nstein et al., 2018).\nContributions We compare human and model\nattention patterns on both sentiment reading and\nrelation extraction tasks. In our analysis, we com-\npare human attention to pre-trained Transformers\n(BERT, RoBERTa and T5), from-scratch training\nof two shallow sequence labeling architectures (Le-\ncun and Bengio, 1995; Vaswani et al., 2017), as\nwell as to a frequency baseline and a heuristic, cog-\nnitively inspired model of human reading called the\nE-Z Reader (Reichle et al., 2003). We find that the\nheuristic model correlates well with human read-\ning, as has been reported in Sood et al. (2020b).\nHowever when we apply attention flow (Abnar\nand Zuidema, 2020), the pre-trained Transformer\nmodels also reach comparable levels of correlation\nstrength. Further fine-tuning experiments on BERT\ndid not result in increased correlation to human fix-\nations. To understand what drives the differences\nbetween models, we perform an in-depth analysis\nof the effect of word predictability and POS tags\non correlation strength. It reveals that Transformer\nmodels do not accurately capture tail phenomena\nfor hard-to-predict words (in contrast to the E-Z\nReader) and that Transformer attention flow shows\ncomparably weak correlation on (proper) nouns\nwhile the E-Z Reader predicts importance of these\nmore accurately, especially on the sentiment read-\ning task. In addition, we investigate a subset of\nthe ZuCo corpus for which aligned task-specific\nand natural reading data is available and find that\nTransformers correlate stronger to natural reading\npatterns. We test faithfulness of these different at-\ntention patterns to produce the correct classification\nvia an input reduction experiment on task-tuned\nBERT models. Our results highlight the trade-off\nbetween model faithfulness and sparsity when com-\nparing importance scores to human attention, i.e.,\nless sparse (higher entropy) attention vectors tend\nto be less faithful with respect to model predic-\ntions. Our code is available at github.com/\noeberle/task_gaze_transformers.\n4295\n2 Pre-trained Language Models vs\nCognitive Models\nChurch and Liberman (2021) discuss how NLP\nhas historically benefited from rationalist and em-\npiricist methodologies, something that holds for\ncognitive modeling in general. The vast major-\nity of application-oriented work in NLP today re-\nlies on pre-trained language models or other large-\nscale data-driven models, but in cognitive model-\ning, most approaches remain heuristic and rule-\nbased, or hybrid, e.g., relying on probabilistic lan-\nguage models to quantify surprisal (Rayner and\nReichle, 2010; Milledge and Blythe, 2019). This\nis for good reasons: Cognitive modeling values in-\nterpretability (even) more, often suffers from data\nscarcity, and is less concerned with model reusabil-\nity across different contexts.\nThis paper presents a head-to-head comparison\nof the E-Z Reader and pre-trained Transformer-\nbased language models. We are not the first to\nevaluate pre-trained language models and large-\nscale data-driven models as if they were cognitive\nmodels. Chrupała and Alishahi (2019), for exam-\nple, use representational similarity analysis to cor-\nrelate sentence encodings in pre-trained language\nmodels with fMRI signals; Abdou et al. (2019) cor-\nrelate sentence encodings with gaze-derived repre-\nsentations. More generally, it has been argued that\ncognitive evaluations are in some cases practically\nsuperior to standard evaluation methodologies in\nNLP (Søgaard, 2016; Hollenstein et al., 2019). We\nreturn to this in the Discussion and Conclusion §6.\nCommonly, pre-trained language models are dis-\nregarded as cognitive models, since they are most\noften implemented as computationally demand-\ning batch learning algorithms, processing data “at\nonce”. Günther et al. (2019) points out that this\nis an artefact of their implementation, and online\nlearning of pre-trained language models is possible,\nyet impractical. Generally, several researchers have\nargued for taking pre-trained language models se-\nriously as cognitive models (Rogers and Wolmetz,\n2016; Mandera et al., 2017; Günther et al., 2019).\nIn the last section, §6, we discuss some of the im-\nplications of comparisons of pre-trained language\nmodels and cognitive models – for cognitive mod-\neling, as well as for NLP. In our experiments, we\nfocus on Transformer architectures that are cur-\nrently the dominating pre-trained language models\nand a de facto baseline for modern NLP research.\n3 Experiments\n3.1 Data\nThe ZuCo dataset (Hollenstein et al., 2018) con-\ntains eye-tracking data for 12 participants (all En-\nglish native speakers) performing natural reading\nand relation extraction on 300 and 407 English\nsentences from the Wikipedia relation extraction\ncorpus (Culotta et al., 2006) respectively and senti-\nment reading on 400 samples of the Stanford Sen-\ntiment Treebank (SST) (Socher et al., 2013). For\nour analysis, we extract and average word-based\ntotal fixation times across participants and focus on\nthe task-specific relation extraction and sentiment\nreading samples.\n3.2 Models\nBelow we briefly describe our used models and\nrefer to Appendix A for more details.\nTransformers The superior performance of\nTransformer architectures across broad sets of NLP\ntasks raises the question of how task-related atten-\ntion patterns really are. In our experiments, we\nfocus on comparing task-modulated human fixa-\ntions to attention patterns extracted from the fol-\nlowing commonly used models: (a) We use both\npre-trained uncased BERT-base and large models\n(Devlin et al., 2019) as well as fine-tuned BERT\nmodels on the respective tasks. BERT was orig-\ninally pre-trained on the English Wikipedia and\nthe BookCorpus. (b) The RoBERTa model has\nthe same architecture as BERT and demonstrates\nbetter performance on downstream tasks using an\nimproved pre-training scheme and the use of addi-\ntional news article data (Liu et al., 2019). (c) The\nText-to-Text Transfer Transformer (T5) uses an\nencoder-decoder structure to enable parallel task-\ntraining and has demonstrated state-of-the-art per-\nformance over several transfer tasks including senti-\nment analysis and natural language inference (Raf-\nfel et al., 2020).\nWe evaluate different ways of extracting token-\nlevel importance scores: We collect attention repre-\nsentations and compute the mean attention vector\nover the final layer heads to capture the mixing of\ninformation in Transformer self-attention modules\nas in Hollenstein and Beinborn (2021) and present\nthis as mean for all aforementioned Transformers.\nTo capture the layer-wise structure of deep Trans-\nformer models we compute attention flow (Abnar\nand Zuidema, 2020). This approach considers the\n4296\nSentiment Reading (SST)                                                    Relation Extraction (Wikipedia)\nFigure 1: Spearman correlation analysis between human attention and different models for two task settings. Solid\nbar edges indicate sentence-level correlations in contrast to a token-level analysis. Left: Sentiment Reading on the\nSST dataset. Right: Relation Extraction on Wikipedia. Standard deviations over five seeds are shown for fine-tuned\nmodels and correlations are statistically significant with p <0.01 unless stated otherwise (ns: not significant).\nattention matrices as a graph, where tokens are\nrepresented as nodes and attention scores as edges\nbetween consecutive layers. The edge values de-\nfine the maximal flow possible between a pair of\nnodes. Flow between edges is thus (i) limited to\nthe maximal attention between any two consecu-\ntive layers for this token and (ii) conserved such\nthat the sum of incoming flow must be equal to the\nsum of outgoing flow. We denote the attention flow\npropagated back from layer L as flow L.\nShallow Models We ground our analysis on\nTransformers by comparing them to relatively shal-\nlow models that were trained from-scratch and eval-\nuate how well they coincide with human fixation.\nWe train a standard CNN (Kim, 2014) network\nwith multiple filter sizes on pre-trained GloVe em-\nbeddings (Pennington et al., 2014). Importance\nscores over tokens are extracted using Layerwise\nRelevance Propagation (LRP) (Arras et al., 2016,\n2017) which has been demonstrated to produce\nrobust explanations by iterating over layers and re-\ndistributing relevance from outer layers towards the\ninput (Bach et al., 2015; Samek et al., 2021). In par-\nallel, we use a shallow multi-head self-attention\nnetwork (Lin et al., 2017) on GloVe vectors with a\nlinear read-out layer for which we compute token\nrelevance scores using LRP.\nE-Z Reader As a cognitive model for human\nreading, we compute task-neutral fixation times\nusing the E-Z Reader (Reichle et al., 1998) model.\nThe E-Z Reader is a multi-stage, hybrid model,\nwhich relies on ann-gram model and several heuris-\ntics, based, for example, on theoretical assumptions\nabout the role of predictability and average saccade\nlength. Additionally, we compare to a frequency\nbaseline using word statistics of the BNC (British\nNational Corpus, Kilgarriff (1995))1 as proposed\nby Barrett et al. (2018).\n3.3 Optimization\nFor training models on the different tasks we re-\nmove all sentences that overlap between ZuCo and\nthe original SST and Wikipedia datasets. Models\nare then trained on the remaining train-split data\nuntil early stopping is reached and we report re-\nsults over five runs. We provide further details on\nthe optimization and model task performance in\nAppendix A.\n3.4 Metric\nTo compare models with human attention, we com-\npute Spearman correlation between human and\nmodel-based importance vectors after concatena-\ntion of individual sentences as well as on a token-\nlevel, see Hollenstein and Beinborn (2021). This\nenables us to distinguish unrelated effects caused\nby varying sentence length from token-level im-\nportance. As described before, we extract human\nattention from gaze (ZuCo), simulated gaze (E-Z\nReader), raw attentions (BERT, RoBERTa, T5), rel-\nevance scores (CNN, self-attention) and inverse\ntoken probability scores (BNC).2 We use ZuCo to-\n1We compute the negative log-transformed probability of\neach lower-cased token corresponding to an inverse relation\nbetween word-frequency and human gaze duration (Rayner\nand Duffy, 1986)\n2First and last token bins from each sentence are ignored to\navoid the influence of sentence border effects in Transformers\n(Clark et al., 2019) and for which the E-Z Reader does not\ncompute fixations.\n4297\nkens to align sentences across tokenizers and apply\nmax-pooling of scores when bins are merged.\n3.5 Main result\nTo evaluate how well model and human attention\npatterns for sentiment reading and relation extrac-\ntion align, we compute pair-wise correlation scores\nas displayed in Figure 1. Reported correlations are\nstatistically significant with p <0.01 if not indi-\ncated otherwise (ns: not significant). After ranking\nbased on the correlations on sentence-level, we ob-\nserve clear differences between sentiment reading\non SST and relation extraction on Wikipedia for\nthe different models. For sentiment reading, the\nE-Z Reader and BNC show the highest correlations\nfollowed by the Transformer attention flow values\n(the ranking between E-Z/BNC and Transformer\nflows is significant at p <0.05 ). For relation ex-\ntraction, we see the highest correlation for BERT-\nbase attention flows (with and without fine-tuning)\nand BERT-large followed by the E-Z Reader (rank-\ning is significant at p <0.05). On the lower end,\ncomputing means over BERT attentions across the\nlast layer shows weak to no correlations for both\ntasks.3 The shallow architectures result in low to\nmoderate correlations with a distinctive gap to at-\ntention flow. Focusing on flow values for Trans-\nformers, BNC and E-Z Reader, correlations are sta-\nble across word and sentence length. Correlations\ngrouped by sentence length shows stable values\naround 0.6 (SST) and 0.4−0.6 (Wikipedia) except\nfor shorter sentences where correlations fluctuate.\nTo check the linear relationship between human\nand model attention patterns we additionally com-\npute token- and sentence-level Pearson correlations\nwhich can be found in Appendix B. Results confirm\nthat Spearman and Pearson correlation coefficients\nas well as rankings hardly differ - which suggests a\nlinear relationship - and that correlation strength is\nin line with Hollenstein and Beinborn (2021).\n4 Analyses\nIn addition to our main result – that pre-trained\nlanguage models are competitive to heuristic cog-\nnitive models in predicting human eye fixations\nduring reading – we present a detailed analysis, in-\nvestigating what our main results depend on, where\n3We have experimented with oracle analyses selecting the\nmaximally correlating attention head in the last layer for each\nsentence and find that correlations are generally weaker than\nwith attention flow.\npre-trained language models improve on cognitive\nmodels, and where they are still challenged.\nFine-tuning BERT does not change correlations\nto human attention We find that fine-tuning base\nand large BERT models on either task does not sig-\nnificantly change correlations and are of similar\nstrength to untuned models. This observation can\nbe embedded into findings that Transformers are\nequipped with overcomplete sets of attention func-\ntions that hardly change until the later layers, if at\nall, during fine-tuning and that this change is also\ndependent on the tuning task itself (Kovaleva et al.,\n2019; Zhao and Bethard, 2020). In addition, we ob-\nserve that Transformer flows propagated back from\nearly, medium and final layers do not considerably\nchange correlations to human attention. This can\nbe explained by attention flow filtering the path of\nminimal value at each layer as discussed in Abnar\nand Zuidema (2020).\nAttention flow is important The correlation\nanalysis emphasizes that we need to capture the\nlayered propagation structure in Transformer mod-\nels, e.g., by using attention flow, in order to extract\nimportance scores that are competitive with cog-\nnitive models. Interestingly, selecting the highest\ncorrelating head for the last attention layer pro-\nduces generally weaker correlation than attention\nflows.3 This offers additional evidence that raw\nattention weights do not reliably correspond to to-\nken relevance (Serrano and Smith, 2019; Abnar\nand Zuidema, 2020) and, thus, are of limited use to\ncompare task attention to human gaze.\nDifferences between language models BERT,\nRoBERTa and T5 are large-scale pretrained lan-\nguage models based on Transformers, but they\nalso differ in various ways. One key difference\nis that BERT and RoBERTa use absolute posi-\ntion encodings, while T5 uses relative encodings.\nBERT and RoBERTa differ in that (i) BERT has\na next-sentence-prediction auxiliary objective; (ii)\nRoBERTa and T5 were trained on more data; (iii)\nRoBERTa uses dynamic masking and trains with\nlarger mini-batches and learning rates, while T5\nuses multi-word masking; (iv) RoBERTa uses byte\npair encoding for subword segmentation. We leave\nit as an open question whether the superior at-\ntention flows of BERT, compared to RoBERTa\nand T5, has to do with training data, next sen-\ntence prediction, or fortunate hyper-parameter set-\ntings, but note that BERT is also known to have\n4298\nFigure 2: Upper: Correlations between human fixation and different models for SST (left) and Relation Extraction\n(right) for the six most common POS tags. Lower: Average attention value after standardization (mean=0, std=1)\nfor respective POS tag and model.\nFigure 3: Correlation between human fixations and different models for SST(left) and Wikipedia (right) with respect\nto word predictability in equally sized bins. Word predictability scores, were calculated with a 5-gram Kneser-Ney\nlanguage model. Respective bin limits are given on the x-axis. Samples for every other bin are displayed on the\nupper x-axis.\nhigher alignment with human-generated explana-\ntions than other large-scale pre-trained language\nmodels (Prasad et al., 2021).\nE-Z Reader is less sensitive to hard-to-predict\nwords and POS We compare correlations to hu-\nman fixations with attention flow values for Trans-\nformer models in the last layer, the E-Z Reader and\nthe BNC baseline for different word predictabil-\nity scores computed with a 5-gram Kneser-Ney\nlanguage model (Kneser and Ney, 1995; Chelba\net al., 2013). Figure 3 shows the results on SST\nand Wikipedia for equally sized bins of word pre-\ndictability scores. We can see that the Transformer\nmodels correlate better for more predictable words\non both datasets whereas the E-Z Reader is less in-\nfluenced by word predictability and already shows\nmedium correlation on the most hard-to-predict\nwords (0.3 − 0.4 for both, SST and Wikipedia).\nIn fact, on SST, Transformers only pass the E-Z\nReader on the most predictable tokens (word pre-\ndictability > 0.03).\nWe also compare correlations to human fixations\nbased on the top-6 (most tokens) Part-of-speech\n(POS) tags. On SST, correlations with E-Z Reader\nare very consistent across POS tags whereas atten-\ntion flow shows weak correlations on proper nouns\n(0.12), nouns (0.16) and verbs (0.16) as presented\nin Figure 2. The BNC frequency baseline correlates\nwell with human fixations on adpositions (ADP)\nwhich both assign comparably low values. Proper\nnouns (PROPN) are overestimated in BNC as a\nresult of their infrequent occurrence.\nInput reduction When comparing machines to\nhumans we typically regard the psychophysical\ndata as the gold standard. We will now take the\nmodel perspective and test fidelity of both human\nand model attention patterns in task-tuned models.\nBy this we aim to test how effective the exact token\nranking based on attention scores is at producing\nthe correct output probability. We perform such\nan input reduction analysis (Feng et al., 2018) us-\ning fine-tuned BERT models for both sentiment\nclassification and relation extraction as the refer-\nence model and present results in Figure 4. In\n4299\n0 0.5 1\n.\n0.3\n0.5\n0.7class probability\n0 0.5 1\n.\n0.1\n0.3\n0.5\nNOUN\nADJ\nPROPN\n0.1\n0.3POS fraction\nPROPNPUNCT NOUN\n0.1\n0.5\nTSR (ZuCo)\nE-Z Reader\nBERT flow 11\nRoBERT a flow 11\nT5 flow 11\nCNN (LRP)\nself-attention (LRP)\nBNC prob\nRandom\nfraction of tokens flipped\nFigure 4: Results of our reduction analysis where most\nimportant tokens are selected and fed into fine-tuned\nBERT models for sentiment classification (left) and re-\nlation extraction (right). Upper: we gradually measure\noutput probability for the true label. Higher area under\nthe curve reflects a stronger model sensitivity to adding\nimportant tokens. Lower: Fractions of Most-selected\nPOS tags at the first flip are displayed for human atten-\ntion (TSR), flow 11, E-Z and BNC token probability.\nour analysis, we observe - as to be expected - that\nadding tokens according to token probability (BNC\nprob) performs even worse than randomly adding\ntokens. From-scratch trained models (CNN and\nself-attention) are most effective in selecting task-\nrelevant tokens, and even more so than using any\nTransformer attention flow. Adding tokens based\non human attention is as effective for the senti-\nment task as the E-Z Reader. Interestingly, for the\nrelation extraction task, human attention vectors\nprovide the most effective flipping order after the\nrelevance-based shallow methods. All Transformer-\nbased flows perform comparably in both tasks. To\nbetter understand what drives these effects we ex-\ntract the fraction of POS tags for the first added\ntoken (see Figure 4 and full results in the Appendix\nFigure 5). For sentiment reading, the flipping ac-\ncording to CNN relevances puts more emphasis on\nadjectives (ADJ) whereas the other methods tend\nto flip nouns (NOUN) first. Across the Transformer\nmodels RoBERTa relies much less on adjectives\nthan any other model. In the relation extraction\ntask, we observe that proper nouns (PROPN) are\ndominant (and adjectives play almost no role) in\nall model systems which highlights the role of task\nnature on the importance assignment. In addition,\nTSR (ZuCo)\nE-Z Reader\nBNC inv prob\nCNN (LRP)\nself-attention\n(LRP)\nBERT flow 11\nRoBERTa flow 11\nT5 flow 11\nBERT mean\nRoBERTa mean\nT5 mean\nSR 3.44 3.44 3.40 2.93 2.16 3.57 3.61 3.61 2.37 2.65 2.45\nTSR 3.38 3.46 3.39 2.98 1.81 3.54 3.60 3.63 2.48 2.56 2.29\nTable 1: Mean entropy over all sentences for each task\nsetting. Lower entropy means sparser token importance.\nThe maximal entropy of a uniform model is 4.09 bits.\nwe see that the E-Z Reader overestimates the im-\nportance of punctuation, whereas proper nouns are\nleast dominant in comparison to the other models.\nEntropy levels of Transformer flow is similar to\nthose in human attention Averaged sentence-\nlevel entropy values on both datasets reveal that\nBERT, RoBERTa and T5 attention flow, the E-Z\nReader and BNC obtain similar levels of sparsity\nas human attention around 3.4-3.6 bits as sum-\nmarized in Table 1. Entropies are lower for the\nshallow networks with self-attention (LRP) at 1.8-\n2.2 bits and CNN (LRP) at around 2.9 bits. This\ndifference in sparsity levels might explain the ad-\nvantage of CNN and shallow self-attention in the\ninput reduction analysis: Early addition of few\nbut very relevant words has a strong effect on the\nmodel’s decision compared to less sparse scoring\nas, e.g. in Transformers. The shallow models were\nalso trained from-scratch for the respective tasks\nwhereas all other models (including human atten-\ntion) are heavily influenced by a more general mod-\neling of language which could explain attention to\nbe distributed more broadly over all tokens.\nBERT mean\nRoBERTa mean\nT5 mean\nfine-BERT mean\nT5 flow 11\nRoBERTa flow 11\nBNC inv prob\nE-Z Reader\nfine-BERT flow 11\nBERT flow 11\nZuCo NR\nNR .12 .09 .16 .15 .48 .52 .58 .57 .67 .69 -\nTSR .12 .14 .20 .23 .45 .48 .49 .53 .61 .62 .72\nTable 2: Correlations between human fixations and mod-\nels on 48 duplicates appearing in the ZuCo dataset for\nboth natural reading (NR) and relation extraction (task-\nspecific reading - TSR).\nNatural reading versus task-specific reading A\nunique feature of the ZuCo dataset is that it con-\ntains a subset of sentences that were presented to\nparticipants both in a task-specific (relation extrac-\ntion) and a natural reading setting. This allows for\n4300\na direct comparison of how correlation strength is\ninfluenced by the task. In Table 2 correlations of\nhuman gaze-based attention with model attentions\nare shown. The highest correlation can be observed\nwhen comparing human attention for task-specific\nand natural reading (0.72). The remaining model\ncorrelations correspond to the ranking and corre-\nlation strength observed in the main result (see\nFigure 1). We observe lower correlation scores for\nthe task-specific reading as compared to normal\nreading among attention flow, the E-Z Reader and\nBNC. This suggests that these models capture the\nstatistics of natural reading - as is expected for a\ncognitive model designed to the natural reading\nparadigm - and that task-related changes in human\nfixation patterns are not reflected in Transformer\nattention flows. Interestingly, averaged last layer\nattention heads show a reverse effect (but at much\nweaker correlation strength). This might suggest\nthat pre-training in Transformer models induces\nspecificity of later layer attention heads to task-\nsolving instead of general natural reading patterns.\n5 Related Work\nSaliency modeling Early computational mod-\nels of visual attention have used bottom-up ap-\nproaches to model the neural circuitry represent-\ning pre-attentive selection processes from visual\ninput (Koch and Ullman, 1985) and later the cen-\ntral idea of a saliency map was introduced (Niebur\nand Koch, 1996). A central hypothesis studying\neye movements under task conditions is known as\nYarbus theorem stating that a task can be directly\ndecoded from fixation patterns (Yarbus, 1967)\nwhich has found varying support (Greene et al.,\n2012; Henderson et al., 2013; Borji and Itti, 2014).\nMore recently, extracting features from deep\npre-trained filters in combination with readout net-\nworks has boosted performance on the saliency\ntask (Kümmerer et al., 2016). This progress has\nenabled modeling of more complex gaze patterns,\ne.g. vision-language tasks such as image caption-\ning (Sugano and Bulling, 2016), visual question\nanswering (Das et al., 2016) or text-guided object\ndetection (Vasudevan et al., 2018).\nPredicting text gaze patterns has been studied\nextensively, often in the context of probabilistic\n(Feng, 2006; Hara et al., 2012; Matthies and Sø-\ngaard, 2013; Hahn and Keller, 2016) or token\ntransition models (Nilsson and Nivre, 2009; Haji-\nAbolhassani and Clark, 2014; Coutrot et al., 2017).\nMore recently deep language features have been\nused as feature extractors in modeling text saliency\n(Sood et al., 2020a; Hollenstein et al., 2021) open-\ning the question of their cognitive plausibility.\nEye-tracking signals for NLP Augmenting ma-\nchine learning models using human gaze informa-\ntion has been shown to improve performance for\na number of different settings: Human attention\npatterns as regularization during model training\nhave resulted in comparable or improved task per-\nformance in tagging part-of-speech (Barrett and\nSøgaard, 2015a,b; Barrett et al., 2018), sentence\ncompression (Klerke et al., 2016), detecting senti-\nment (Mishra et al., 2016, 2017) or reading com-\nprehension (Malmaud et al., 2020). In these works,\ngeneral free-viewing gaze data is used without con-\nsideration of the specific training task which opens\nthe question of task-modulation in human reading.\nFrom natural to task-specific reading Recent\nwork on reading often analyses eye-tracking data\nin combination with neuroimaging techniques such\nas EEG (Wenzel et al., 2017) and f-MRI (Hillen\net al., 2013; Choi et al., 2014). Research questions\nthereby focus either on detecting relevant parts in\ntext (Loboda et al., 2011; Wenzel et al., 2017) or the\ndifference between natural and pseudo-reading, i.e.,\ntext without syntax/semantics (Hillen et al., 2013)\nor pseudo-words (Choi et al., 2014). To the best\nof our knowledge there has not been any work on\ncomparing fixations between natural reading and\ntask-specific reading on classical NLP tasks such\nas relation extraction or sentiment classification.\n6 Discussion and Conclusion\nIn this paper, we have compared attention and rel-\nevance mechanisms of a wide range of models to\nhuman gaze patterns when solving sentiment clas-\nsification on SST movie reviews and relation ex-\ntraction on Wikipedia articles. We generally found\nthat Transformer architectures are competitive with\nthe E-Z Reader, but only when computing atten-\ntion flow scores. We generally saw weaker cor-\nrelations for relation extraction on Wikpedia, pre-\nsumably due to simpler sentence structures and the\noccurrence of polarity words. In the following, we\ndiscuss implications of our findings on NLP and\nCognitive Science in more detail.\nLessons for NLP One implication of the above\nfor NLP follows from the importance of attention\n4301\nflow in our experiments: Using human gaze to regu-\nlarize or supervise attention weights has proven ef-\nfective in previous work (§5), but we observed that\ncorrelations with task-specific human attention in-\ncrease significantly by using layer-dependent atten-\ntion flow compared to using raw attention weights.\nThis insight motivates going beyond regularizing\nraw attention weights or directly injecting human\nattention vectors during training, to instead opti-\nmize for correlation between attention flow and\nhuman attention. Jointly modeling language and\nhuman gaze has recently shown to yield compet-\nitive performance on paraphrase generation and\nsentence compression while resulting in more task-\nspecific attention heads (Sood et al., 2020b). For\nthis study natural gaze patterns were also simulated\nusing the E-Z Reader.\nAnother potential implication concerns inter-\npretability. It remains an open problem how best to\ninterpret self-attention modules (Jain and Wallace,\n2019; Wiegreffe and Pinter, 2019), and whether\nthey provide meaningful explanations for model\npredictions. Including gradient information to\nexplain Transformers has recently been consid-\nered to improve their interpretability (Chefer et al.,\n2021b,a; Ali et al., 2022). A successful expla-\nnation of a machine learning model should be\nfaithful, human-interpretable and practical to ap-\nply (Samek et al., 2021). Faithfulness and prac-\nticality is often evaluated using automated proce-\ndures such as input reduction experiments or mea-\nsuring time and model complexity. By contrast,\njudging human-interpretability typically requires\ncostly experiments in well-controlled settings and\nobtaining human gold-standards for interpretability\nremain difficult (Miller, 2019; Schmidt and Bieß-\nmann, 2019). Using gaze data to evaluate the faith-\nfulness and trustworthiness of machine learning\nmodels is a promising approach to increase model\ntransparency.\nLessons for Cognitive Science Attention flow\nin Transformers, especially for BERT models, cor-\nrelates surprisingly well with human task-specific\nreading, but what does this tell us about the short-\ncomings of our cognitive models? We know that\nword frequency and semantic relationships between\nwords influence word fixation times (Rayner, 1998).\nIn our experiments, we see relatively high correla-\ntion between human fixations and the inverse word\nprobability baseline which raises the question to\nwhat extent reading gaze is driven by low-level pat-\nterns such as word frequency or syntactic structure\nin contrast to more high-level semantic context or\nwrap-up effects.\nIn computer vision, cognitively inspired bottom-\nup models, e.g., using intensity and contrast fea-\ntures, are able to explain at most half of the gaze fix-\nation information in comparison to the human gold\nstandard (Kümmerer et al., 2017). The robustness\nof the E-Z Reader on movie reviews is likely due to\nits explicit modeling of low-level properties such\nas word frequency or sentence length. BERT was\nrecently shown to be primarily modeling higher-\norder word co-occurrence statistics (Sinha et al.,\n2021). We argue that while Transformers are lim-\nited, e.g., in not capturing the dependency of human\ngaze on word length (Kliegl et al., 2004), cogni-\ntive models seem to underestimate the role of word\nco-occurrence statistics.\nDuring reading, humans are faced with a trade-\noff between the precision of reading comprehen-\nsion and reading speed, by avoiding unnecessary\nfixations (Hahn and Keller, 2016). This trade-off\nis related to the input reduction experiments per-\nformed in Section 4. Here, we observe that shallow\nmethods score well at being sparse and effective in\nchanging model output towards the correct class,\nbut produce only weak correlation to human read-\ning patterns when compared to layered language\nmodels. In comparison, extracted attention flow\nfrom pre-trained Transformer models correlates\nmuch better with human attention, but offers less\nsparse token attention. In other words, our results\nshow that task-specific reading is sub-optimal rel-\native to solving tasks and heavily regularized by\nnatural reading patterns (see also our comparison\nof task-specific and natural reading in Section 4).\nConclusion In our experiments, we first and\nforemost found that Transformers, and especially\nBERT models, are competitive to the E-Z Reader\nin terms of explaining human attention in task-\nspecific reading. For this to be the case, comput-\ning attention flow scores (rather than raw attention\nweights) is important. Even so, the E-Z Reader\nremains better at hard-to-predict words and is less\nsensitive to part of speech. While Transformers\nthus have some limitations compared to the E-\nZ Reader, our results indicate that cognitive models\nhave placed too little weight on high-level word co-\noccurrence statistics. Generally, Transformers and\nthe E-Z Reader correlate much better with human\nattention than other, shallow from-scratch trained\n4302\nsequence labeling architectures. Our input reduc-\ntion experiments suggest that in a sense, both pre-\ntrained language models and humans have subop-\ntimal, i.e., less sparse, task-solving strategies, and\nare heavily regularized by what is optimal in natu-\nral reading contexts.\nAcknowledgements\nThis work was partially funded by the German\nMinistry for Education and Research as BIFOLD\n– Berlin Institute for the Foundations of Learning\nand Data (ref. 01IS18025A and ref. 01IS18037A),\nas well as by the Platform Intelligence in News\nproject, which is supported by Innovation Fund\nDenmark via the Grand Solutions program. We\nthank Mostafa Abdou for fruitful discussions and\nHeather Lent, Miryam de Lhoneux and Vinit Rav-\nishankar for proof-reading and valuable inputs on\nthe manuscript.\nReferences\nMostafa Abdou, Artur Kulmizev, Felix Hill, Daniel M.\nLow, and Anders Søgaard. 2019. Higher-order com-\nparisons of sentence encoder representations. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5838–\n5845, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention flow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4190–4197, On-\nline. Association for Computational Linguistics.\nAmeen Ali, Thomas Schnake, Oliver Eberle, Gré-\ngoire Montavon, Klaus-Robert Müller, and Lior\nWolf. 2022. XAI for transformers: Better expla-\nnations through conservative propagation. CoRR,\nabs/2202.07304.\nLeila Arras, Franziska Horn, Grégoire Montavon, Klaus-\nRobert Müller, and Wojciech Samek. 2016. Explain-\ning predictions of non-linear classifiers in NLP. In\nProceedings of the 1st Workshop on Representation\nLearning for NLP, pages 1–7, Berlin, Germany. As-\nsociation for Computational Linguistics.\nLeila Arras, Franziska Horn, Grégoire Montavon, Klaus-\nRobert Müller, and Wojciech Samek. 2017. \"What\nis relevant in a text document?\": An interpretable\nmachine learning approach. PLOS ONE, 12(8):1–23.\nSebastian Bach, Alexander Binder, Grégoire Montavon,\nFrederick Klauschen, Klaus-Robert Müller, and Wo-\njciech Samek. 2015. On pixel-wise explanations\nfor non-linear classifier decisions by layer-wise rele-\nvance propagation. PLoS ONE, 10(7):e0130140.\nMaria Barrett, Joachim Bingel, Nora Hollenstein, Marek\nRei, and Anders Søgaard. 2018. Sequence classi-\nfication with human attention. In Proceedings of\nthe 22nd Conference on Computational Natural Lan-\nguage Learning, pages 302–312, Brussels, Belgium.\nAssociation for Computational Linguistics.\nMaria Barrett and Anders Søgaard. 2015a. Reading\nbehavior predicts syntactic categories. In Proceed-\nings of the Nineteenth Conference on Computational\nNatural Language Learning, pages 345–349, Beijing,\nChina. Association for Computational Linguistics.\nMaria Barrett and Anders Søgaard. 2015b. Using read-\ning behavior to predict grammatical functions. In\nProceedings of the Sixth Workshop on Cognitive As-\npects of Computational Language Learning, pages\n1–5, Lisbon, Portugal. Association for Computational\nLinguistics.\nAli Borji and Laurent Itti. 2014. Defending yarbus: eye\nmovements reveal observers’ task. Journal of vision,\n14(3):29.\nHila Chefer, Shir Gur, and Lior Wolf. 2021a. Generic\nattention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 397–406.\nHila Chefer, Shir Gur, and Lior Wolf. 2021b. Trans-\nformer interpretability beyond attention visualization.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n782–791.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robinson.\n2013. One billion word benchmark for measuring\nprogress in statistical language modeling. Technical\nreport, Google.\nWonil Choi, Rutvik H Desai, and John M Henderson.\n2014. The neural substrates of natural reading: a\ncomparison of normal and nonword text using eye-\ntracking and fmri. Frontiers in human neuroscience,\n8:1024.\nGrzegorz Chrupała and Afra Alishahi. 2019. Correlat-\ning neural and symbolic representations of language.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 2952–\n2962, Florence, Italy. Association for Computational\nLinguistics.\nKenneth Church and Mark Liberman. 2021. The future\nof computational linguistics: On beyond alchemy.\nFrontiers in Artificial Intelligence, 4:10.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\n4303\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nAntoine Coutrot, Janet H. Hsiao, and Antoni B. Chan.\n2017. Scanpath modeling and classification with\nhidden markov models.\nAron Culotta, Andrew McCallum, and Jonathan Betz.\n2006. Integrating probabilistic extraction models and\ndata mining to discover relations and patterns in text.\nIn Proceedings of the Human Language Technology\nConference of the NAACL, Main Conference, pages\n296–303.\nAbhishek Das, Harsh Agrawal, Larry Zitnick, Devi\nParikh, and Dhruv Batra. 2016. Human attention\nin visual question answering: Do humans and deep\nnetworks look at the same regions? In Proceedings\nof the 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 932–937, Austin,\nTexas. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nGary Feng. 2006. Eye movements as time-series ran-\ndom variables: A stochastic model of eye move-\nment control in reading. Cognitive Systems Research,\n7(1):70–95. Models of Eye-Movement Control in\nReading.\nShi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,\nPedro Rodriguez, and Jordan Boyd-Graber. 2018.\nPathologies of neural models make interpretations\ndifficult. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3719–3728, Brussels, Belgium. Association\nfor Computational Linguistics.\nMichelle Greene, Tommy Liu, and Jeremy Wolfe. 2012.\nReconsidering yarbus: A failure to predict observers’\ntask from eye movement patterns. Vision research,\n62:1–8.\nFritz Günther, Luca Rinaldi, and Marco Marelli. 2019.\nVector-space models of semantic representation from\na cognitive perspective: A discussion of common mis-\nconceptions. Perspectives on Psychological Science,\n14(6):1006–1033. PMID: 31505121.\nMichael Hahn and Frank Keller. 2016. Modeling human\nreading with neural attention. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 85–95, Austin, Texas.\nAssociation for Computational Linguistics.\nAmin Haji-Abolhassani and James J. Clark. 2014. An\ninverse yarbus process: Predicting observers’ task\nfrom eye movement patterns. Vision Research ,\n103:127–142.\nTadayoshi Hara, Daichi Mochihashi, Yoshinobu Kano,\nand Akiko Aizawa. 2012. Predicting word fixations\nin text with a CRF model for capturing general read-\ning strategies among readers. In Proceedings of the\nFirst Workshop on Eye-tracking and Natural Lan-\nguage Processing, pages 55–70, Mumbai, India. The\nCOLING 2012 Organizing Committee.\nJohn Henderson, Svetlana Shinkareva, Jing Wang,\nSteven Luke, and Jenn Olejarczyk. 2013. Predict-\ning cognitive state from eye movements. PloS one,\n8:e64937.\nRebekka Hillen, Thomas Günther, Claudia Kohlen, Cor-\nnelia Eckers, Muna van Ermingen-Marbach, Katha-\nrina Sass, Wolfgang Scharke, Josefine V ollmar, Ralph\nRadach, and Stefan Heim. 2013. Identifying brain\nsystems for gaze orienting during reading: fmri inves-\ntigation of the landolt paradigm. Frontiers in human\nneuroscience, 7:384.\nNora Hollenstein and Lisa Beinborn. 2021. Relative\nimportance in sentence processing. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 141–150, Online.\nAssociation for Computational Linguistics.\nNora Hollenstein, Antonio de la Torre, Nicolas Langer,\nand Ce Zhang. 2019. CogniVal: A framework for\ncognitive word embedding evaluation. In Proceed-\nings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL), pages 538–549,\nHong Kong, China. Association for Computational\nLinguistics.\nNora Hollenstein, Federico Pirovano, Ce Zhang, Lena\nJäger, and Lisa Beinborn. 2021. Multilingual lan-\nguage models predict human reading behavior. arXiv\npreprint arXiv:2104.05433.\nNora Hollenstein, Jonathan Rotsztejn, Marius Troen-\ndle, Andreas Pedroni, Ce Zhang, and Nicolas Langer.\n2018. Zuco, a simultaneous eeg and eye-tracking\nresource for natural sentence reading. Scientific data,\n5(1):1–13.\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength Natural Language Processing in Python.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\n4304\nAdam Kilgarriff. 1995. BNC database and word fre-\nquency lists. Accessed: 07/2020.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classification. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Linguis-\ntics.\nSigrid Klerke, Yoav Goldberg, and Anders Søgaard.\n2016. Improving sentence compression by learning\nto predict gaze. In Proceedings of the 2016 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1528–1533, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nSigrid Klerke and Barbara Plank. 2019. At a glance:\nThe impact of gaze aggregation views on syntac-\ntic tagging. In Proceedings of the Beyond Vision\nand LANguage: inTEgrating Real-world kNowledge\n(LANTERN), pages 51–61, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nReinhold Kliegl, Ellen Grabner, Martin Rolfs, and Ralf\nEngbert. 2004. Length, frequency, and predictabil-\nity effects of words on eye movements in reading.\nEuropean Journal of Cognitive Psychology , 16(1-\n2):262–284.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In 1995\ninternational conference on acoustics, speech, and\nsignal processing, volume 1, pages 181–184. IEEE.\nChristof Koch and Shimon Ullman. 1985. Shifts in\nSelective Visual Attention: Towards the Underlying\nNeural Circuitry. Human Neurobiology, 4:219–227.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. arXiv preprint arXiv:1908.08593.\nMatthias Kümmerer, Thomas S. A. Wallis, and\nMatthias Bethge. 2016. DeepGaze II: Reading fix-\nations from deep features trained on object recog-\nnition. arXiv:1610.01563 [cs, q-bio, stat] . ArXiv:\n1610.01563.\nMatthias Kümmerer, Thomas S.A. Wallis, Leon A.\nGatys, and Matthias Bethge. 2017. Understanding\nlow- and high-level contributions to fixation predic-\ntion. In 2017 IEEE International Conference on\nComputer Vision (ICCV), pages 4799–4808.\nYann Lecun and Yoshua Bengio. 1995.Convolutional\nNetworks for Images, Speech and Time Series, pages\n255–258. The MIT Press.\nZhouhan Lin, Minwei Feng, Cícero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. CoRR, abs/1703.03130.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTomasz D Loboda, Peter Brusilovsky, and Jöerg Brun-\nstein. 2011. Inferring word relevance from eye-\nmovements of readers. In Proceedings of the 16th in-\nternational conference on Intelligent user interfaces,\npages 175–184.\nJonathan Malmaud, Roger Levy, and Yevgeni Berzak.\n2020. Bridging information-seeking human gaze and\nmachine reading comprehension. In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, pages 142–152, Online. Association\nfor Computational Linguistics.\nPawel Mandera, Emmanuel Keuleers, and Marc Brys-\nbaert. 2017. Explaining human performance in psy-\ncholinguistic tasks with models of semantic similarity\nbased on prediction and counting : a review and em-\npirical validation. Journal of Memory and Language,\n92:57–78.\nFranz Matthies and Anders Søgaard. 2013. With blink-\ners on: Robust prediction of eye movements across\nreaders. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 803–807, Seattle, Washington, USA. Associa-\ntion for Computational Linguistics.\nSara V . Milledge and Hazel I. Blythe. 2019. The chang-\ning role of phonology in reading development. Vi-\nsion, 3(2).\nTim Miller. 2019. Explanation in artificial intelligence:\nInsights from the social sciences. Artificial Intelli-\ngence, 267:1–38.\nAbhijit Mishra, Kuntal Dey, and Pushpak Bhattacharyya.\n2017. Learning cognitive features from gaze data for\nsentiment and sarcasm classification using convo-\nlutional neural network. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 377–387,\nVancouver, Canada. Association for Computational\nLinguistics.\nAbhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal\nDey, and Pushpak Bhattacharyya. 2016. Leveraging\ncognitive features for sentiment analysis. In Proceed-\nings of The 20th SIGNLL Conference on Computa-\ntional Natural Language Learning, pages 156–166,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nGrégoire Montavon, Alexander Binder, Sebastian La-\npuschkin, Wojciech Samek, and Klaus-Robert Müller.\n2019. Layer-Wise Relevance Propagation: An\nOverview, pages 193–209. Springer International\nPublishing, Cham.\n4305\nE. Niebur and C. Koch. 1996. Control of selective visual\nattention: Modeling the “where” pathway. In D. S\nTouretzky, M. C. Mozer, and M. E. Hasselmo, editors,\nAdvances in Neural Information Processing Systems,\nvolume 8, pages 802–808. MIT Press, Cambridge,\nMA.\nMattias Nilsson and Joakim Nivre. 2009. Learning\nwhere to look: Modeling eye movements in read-\ning. In Proceedings of the Thirteenth Conference on\nComputational Natural Language Learning, CoNLL\n2009, Boulder, Colorado, USA, June 4-5, 2009, pages\n93–101. ACL.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532–1543.\nGrusha Prasad, Yixin Nie, Mohit Bansal, Robin Jia,\nDouwe Kiela, and Adina Williams. 2021. To what ex-\ntent do human explanations of model behavior align\nwith actual model behavior? In Proceedings of the\nFourth BlackboxNLP Workshop on Analyzing and\nInterpreting Neural Networks for NLP, pages 1–14,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nKeith Rayner. 1998. Eye movements in reading and\ninformation processing: 20 years of research. Psy-\nchological bulletin, 124(3):372.\nKeith Rayner and Susan A Duffy. 1986. Lexical com-\nplexity and fixation times in reading: effects of word\nfrequency, verb complexity, and lexical ambiguity.\nMemory amp; cognition, 14(3):191—201.\nKeith Rayner and Erik D. Reichle. 2010. Models of the\nreading process. WIREs Cognitive Science, 1(6):787–\n799.\nErik D Reichle, Alexander Pollatsek, Donald L Fisher,\nand Keith Rayner. 1998. Toward a model of eye\nmovement control in reading. Psychological review,\n105(1):125.\nErik D. Reichle, Keith Rayner, and Alexander Pollatsek.\n2003. The e-z reader model of eye-movement con-\ntrol in reading: comparisons to other models. The\nBehavioral and brain sciences, 26(4):445–476.\nTimothy Rogers and Michael Wolmetz. 2016. Concep-\ntual knowledge representation: A cross-section of\ncurrent research. Cognitive Neuropsychology, 33:1–\n9.\nWojciech Samek, Grégoire Montavon, Sebastian La-\npuschkin, Christopher J. Anders, and Klaus-Robert\nMüller. 2021. Explaining deep neural networks and\nbeyond: A review of methods and applications. Pro-\nceedings of the IEEE, 109(3):247–278.\nPhilipp Schmidt and Felix Bießmann. 2019. Quanti-\nfying interpretability and trust in machine learning\nsystems. CoRR, abs/1901.08558.\nSofia Serrano and Noah A. Smith. 2019. Is attention in-\nterpretable? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2931–2951, Florence, Italy. Association for\nComputational Linguistics.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nAnders Søgaard. 2016. Evaluating word embeddings\nwith fMRI and eye-tracking. In Proceedings of the\n1st Workshop on Evaluating Vector-Space Represen-\ntations for NLP , pages 116–121, Berlin, Germany.\nAssociation for Computational Linguistics.\nEkta Sood, Simon Tannert, Diego Frassinelli, Andreas\nBulling, and Ngoc Thang Vu. 2020a. Interpreting\nattention models with human visual attention in ma-\nchine reading comprehension. In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, pages 12–25, Online. Association\nfor Computational Linguistics.\nEkta Sood, Simon Tannert, Philipp Mueller, and An-\ndreas Bulling. 2020b. Improving natural language\nprocessing tasks with human gaze-guided neural at-\ntention. In Advances in Neural Information Process-\ning Systems, volume 33, pages 6327–6341. Curran\nAssociates, Inc.\nAndreas Stolcke. 2002. Srilm – an extensible language\nmodeling toolkit. In In Proceedings of the 7th Inter-\nnational Conference on Spoken Language Processing\n(ICSLP 2002, pages 901–904.\nYusuke Sugano and Andreas Bulling. 2016. Seeing\nwith humans: Gaze-assisted neural image captioning.\nCoRR, abs/1608.05203.\nArun Balajee Vasudevan, Dengxin Dai, and Luc Van\nGool. 2018. Object referring in videos with language\nand human gaze. In 2018 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2018,\nSalt Lake City, UT, USA, June 18-22, 2018 , pages\n4129–4138. IEEE Computer Society.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\n4306\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nMarkus Andreas Wenzel, Mihail Bogojeski, and Ben-\njamin Blankertz. 2017. Real-time inference of word\nrelevance from electroencephalogram and eye gaze.\nJournal of neural engineering, 14(5):056007.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nAlfred L. Yarbus. 1967. Eye Movements and Vision .\nPlenum. New York.\nYingyi Zhang and Chengzhi Zhang. 2019. Using hu-\nman attention to extract keyphrase from microblog\npost. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n5867–5872, Florence, Italy. Association for Compu-\ntational Linguistics.\nYiyun Zhao and Steven Bethard. 2020. How does\nBERT’s attention change when you fine-tune? an\nanalysis methodology and a case study in negation\nscope. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4729–4747, Online. Association for Computational\nLinguistics.\nA Model and Optimization Details\nIn the following we present details for all modes\nand describe the training details used for task-\ntunning. Model performance over five runs is re-\nported in Table 3.\nA.1 CNN\nThe CNN models use 300-dimensional pre-trained\nGloVe_840B (Pennington et al., 2014) embeddings.\nInput sentences are tokenized using the SpaCy to-\nkenizer (Honnibal et al., 2020). We use 150 con-\nvolutional filters of filter sizes s = [3, 4, 5] with\nReLu activation, followed by a max-pooling-layer\nand apply dropout of p = 0.5 of the linear clas-\nsification layer during training. For training we\nuse a batchsize of bs = 50 and train all model\nparameters using the Adam optimizer with a learn-\ning rate of lr = 1e − 4 for a maximum num-\nber of T = 20 epochs. For all model trainings,\nwe apply early stopping to avoid overfitting dur-\ning training and stop optimization as soon as the\nvalidation loss begins to increase. To compute\nLRP relevances we use the general formulation\nof LRP propagation rules with γ = 0. for the lin-\near readout layers (Montavon et al., 2019). We\ntake absolute values over resulting relevance scores\nsince we find they correlate best with human at-\ntention in comparison to raw and rectified pro-\ncessing. For propagation through the max-pooling\nlayer we apply the winner-take-all principle and for\nconvolutional layers we use the LRP-γ redistribu-\ntion rule and select γ = 0.5 after a search over\nγ = [0., 0.25, 0.5, 0.75, 1.0] resulting in largest\ncorrelations to human attention.\nA.2 Self-Attention model\nFor the multi-head self-attention model again use\n300-dimensional pre-trained GloVe_840B embed-\ndings and tokenized via SpaCy. The architecture\nconsists of a set of k = 3self-attention heads for\nthe SR task and k = 8 for REL. The resulting\nsentence representation is then fed into a linear\nclassification readout layer with γ = 0. and which\nwe also use for the propagation to input embed-\ndings. During optimization we use lr = 1e − 4,\nbs = 50and T = 50.\nA.3 Transformer Models\nWe use standard BERT-base/large-uncased archi-\ntectures and tokenizers as provided by the hug-\ngingface library (Wolf et al., 2020). For BERT-\nbase fine-tuning we use lr = 1e − 5 for REL and\nlr = 1e − 6 for SR, bs = 32and T = 50for both\ntasks. For BERT-large we uselr = 1e − 5 for REL\nand lr = 5e − 7 for SR, bs = 16and T = 50. For\nRoBERTa and T5 we use the RoBERTa-base and\nT5-base checkpoints and respective tokenizers.\nA.4 E-Z Reader\nWe use version 10.2 of the E-Z Reader with de-\nfault parameters and 1000 repetitions. Cloze scores,\ni.e. word predictability scores, were therefore com-\nputed using a 5-gram Kneser-Ney language model\n(Kneser and Ney, 1995) as provided by the SRI\nLanguage Modeling Toolkit (Stolcke, 2002) and\n4307\nAcc (SR) F1 (SR) Acc (REL) F1 (REL)\nself-attention 69.0 ± 0.2 64 .5 ± 2.2 67.5 ± 1.3 55 .5 ± 2.0\nCNN 71.3 ± 0.2 69 .8 ± 1.7 74.0 ± 1.9 68 .7 ± 4.8\nBERT-base 76.0 ± 0.1 67 .0 ± 3.0 78.3 ± 1.5 72 .7 ± 3.3\nBERT-large 76.4 ± 0.1 63 .8 ± 1.3 78.9 ± 2.3 71 .0 ± 2.7\nTable 3: Accuracy and F1 scores after fine-tuning on the respective task dataset over five runs: sentiment reading on\nSST (SR) and relation extraction on Wikipedia (REL). Samples that overlap with the ZuCo dataset were filtered out.\ntrained on the 1 billion token dataset (Chelba et al.,\n2013). Resulting perplexity on the held-out test set\nwas ppl = 81.9. Then, word-based total fixation\ntimes are computed from the E-Z Readers trace\nfiles and averaged over all subjects.\nB Spearman versus Pearson correlation\non sentence and token level\nIn addition to Spearman correlation over all tokens,\nwe also report Pearson correlation coefficients on\na sentence and token-level. Results are displayed\nin Table 4. Compared to Spearman correlation\non all tokens, the ranking does hardly change for\nPearson or sentence-level correlations. Absolute\ncorrelation coefficients are higher for Spearman\ncompared to Pearson and also are slightly higher\non the sentence-level as compared to the token-\nlevel analysis. Biggest changes occur in a drop for\nBNC when Spearman correlation is calculated on\nall tokens for relation extraction and an increase\nfor self-attention (LRP) in sentiment reading. We\nhypothesize that both effects can be traced back to\nthe level of sparsity and the corresponding ranking\nfor Spearman correlations. In our entropy analysis\nwe found that, i.e. self-attention shows a sparser\nrepresentation which was likely caused by the over-\nconfidence of the model, and which could explain\nthe higher rank-based correlation.\nC Input reduction - POS tag analysis\nFigure 5 shows the full distribution of POS tags\nof the first tokens flipped. This extends Figure 4\nwhere we only show the first 3 POS tags.\nD Entropy analysis\nWe compute entropy values for different attention\nand relevance scores in both task settings. To com-\npensate for different sentence lengths we perform\na stratified analysis such that every sentence length\noccurs equally often in both tasks. Sentence lengths\nNOUN\nADJVERBPROPNPUNCT\nADVPRONPARTAUXCCONJSCONJADPNUMDETSYMINTJ\n0.0\n0.15\n0.3\nPOS fraction\nSentiment Reading TSR (ZuCo)\nBERT flow 11\nT5 flow 11\nRoBERT a flow 11\nE-Z Reader\nCNN (LRP)\nPROPNNOUNPUNCTNUMVERBADJ ADVPART\nX\nSYM AUXPRONADPSCONJ\n0.0\n0.25\n0.5\nPOS fraction\nRelation Extraction\nFigure 5: Full distribution of POS tags of most impor-\ntant first flip tokens for the task of sentiment reading\n(top) and relation extraction (bottom).\nwhich merely occur in one of the two tasks, are ex-\ncluded from the sampling. Maximum entropy is\nreached for uniformly distributed token scores.\n4308\nSR TSR\ntok sent tok sent\npearson spearman pearson spearman pearson spearman pearson spearman\nBNC inv prob 0.57 0.66 0.62 0.64 0.34 0.41 0.45 0.46\nCNN (LRP) 0.17 0.27 0.27 0.26 0.13 0.14 0.21 0.18\nself-attention 0.36 0.48 0.43 0.54 0.27 0.49 0.44 0.61\nself-attention (LRP) 0.07 0.39 0.34 0.43 0.09 0.31 0.28 0.36\nBERT flow 0 0.52 0.62 0.61 0.63 0.47 0.55 0.55 0.60\nBERT flow 5 0.53 0.61 0.60 0.61 0.49 0.53 0.57 0.57\nBERT flow 11 0.54 0.62 0.62 0.63 0.51 0.56 0.60 0.61\nfine-BERT flow 0 0.52 0.62 0.61 0.63 0.47 0.55 0.55 0.60\nfine-BERT flow 5 0.53 0.61 0.59 0.61 0.50 0.54 0.59 0.59\nfine-BERT flow 11 0.54 0.62 0.62 0.63 0.51 0.56 0.60 0.60\nBERT-large flow 0 0.51 0.61 0.62 0.63 0.47 0.54 0.57 0.60\nBERT-large flow 11 0.55 0.63 0.62 0.62 0.50 0.55 0.57 0.57\nBERT-large flow 23 0.55 0.63 0.62 0.62 0.50 0.55 0.57 0.57\nfine-BERT-large flow 0 0.51 0.61 0.62 0.63 0.47 0.54 0.57 0.60\nfine-BERT-large flow 11 0.55 0.63 0.62 0.62 0.50 0.55 0.57 0.57\nfine-BERT-large flow 23 0.55 0.63 0.62 0.62 0.50 0.55 0.57 0.57\nRoBERTa flow 0 0.44 0.54 0.52 0.55 0.35 0.43 0.42 0.47\nRoBERTa flow 5 0.32 0.42 0.45 0.46 0.26 0.33 0.36 0.40\nRoBERTa flow 11 0.44 0.51 0.51 0.52 0.37 0.41 0.45 0.46\nT5 flow 0 0.44 0.53 0.51 0.54 0.37 0.44 0.47 0.50\nT5 flow 5 0.43 0.50 0.49 0.49 0.35 0.40 0.44 0.43\nT5 flow 11 0.44 0.51 0.51 0.53 0.37 0.42 0.46 0.46\nBERT mean 0.04 0.14 0.10 0.11 -0.03 0.11 0.02 0.09\nfine-BERT mean 0.03 0.09 0.05 0.03 -0.03 0.10 0.02 0.08\nBERT-large mean -0.01 0.20 0.10 0.28 -0.03 0.14 -0.01 0.14\nfine-BERT-large mean -0.02 0.11 0.04 0.17 -0.09 -0.05 -0.12 -0.06\nRoBERTa mean 0.22 0.22 0.26 0.21 0.08 0.10 0.14 0.10\nT5 mean -0.00 0.06 -0.00 0.07 -0.02 0.10 0.02 0.19\nE-Z Reader 0.64 0.65 0.69 0.67 0.46 0.51 0.56 0.56\nTable 4: Full correlation analysis for sentiment reading (left) and relation extraction (right). We show Spearman and\nPearson correlation coefficients between human fixations and models. Correlation coefficients were calculated per\nsentence and averaged (sen) or after concatenation of all sentences (tok)\n4309"
}