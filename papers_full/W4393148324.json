{
    "title": "Transformer-Based No-Reference Image Quality Assessment via Supervised Contrastive Learning",
    "url": "https://openalex.org/W4393148324",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5016680439",
            "name": "Jinsong Shi",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A5101746588",
            "name": "Pan Gao",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A5061215231",
            "name": "Jie Qin",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3100498948",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W2148349024",
        "https://openalex.org/W2068403421",
        "https://openalex.org/W2092377492",
        "https://openalex.org/W3100404621",
        "https://openalex.org/W3194280054",
        "https://openalex.org/W2987283559",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W3091249416",
        "https://openalex.org/W6800329793",
        "https://openalex.org/W6776700526",
        "https://openalex.org/W6730909395",
        "https://openalex.org/W2171349048",
        "https://openalex.org/W1978598038",
        "https://openalex.org/W2059968011",
        "https://openalex.org/W2953590133",
        "https://openalex.org/W2795855127",
        "https://openalex.org/W2130172650",
        "https://openalex.org/W2914913933",
        "https://openalex.org/W2768340063",
        "https://openalex.org/W3210514413",
        "https://openalex.org/W1987931295",
        "https://openalex.org/W1982471090",
        "https://openalex.org/W2163370434",
        "https://openalex.org/W6704369950",
        "https://openalex.org/W1971155098",
        "https://openalex.org/W2162692770",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W3035719652",
        "https://openalex.org/W2979303786",
        "https://openalex.org/W6671646756",
        "https://openalex.org/W2133665775",
        "https://openalex.org/W6753412334",
        "https://openalex.org/W2473697052",
        "https://openalex.org/W2162915697",
        "https://openalex.org/W2159814221",
        "https://openalex.org/W4224267514",
        "https://openalex.org/W2061513831",
        "https://openalex.org/W1987489060",
        "https://openalex.org/W2995489114",
        "https://openalex.org/W6788648500",
        "https://openalex.org/W1977725648",
        "https://openalex.org/W1922292441",
        "https://openalex.org/W2905544033",
        "https://openalex.org/W3016039927",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W1974013408",
        "https://openalex.org/W3034882073",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3194293177",
        "https://openalex.org/W4287812705",
        "https://openalex.org/W2963420272",
        "https://openalex.org/W2964065910",
        "https://openalex.org/W4214745154",
        "https://openalex.org/W2963074118",
        "https://openalex.org/W2785325870",
        "https://openalex.org/W3035712445",
        "https://openalex.org/W2752223497",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W3000809580",
        "https://openalex.org/W2566149141",
        "https://openalex.org/W2085518012"
    ],
    "abstract": "Image Quality Assessment (IQA) has long been a research hotspot in the field of image processing, especially No-Reference Image Quality Assessment (NR-IQA). Due to the powerful feature extraction ability, existing Convolution Neural Network (CNN) and Transformers based NR-IQA methods have achieved considerable progress. However, they still exhibit limited capability when facing unknown authentic distortion datasets. To further improve NR-IQA performance, in this paper, a novel supervised contrastive learning (SCL) and Transformer-based NR-IQA model SaTQA is proposed. We first train a model on a large-scale synthetic dataset by SCL (no image subjective score is required) to extract degradation features of images with various distortion types and levels. To further extract distortion information from images, we propose a backbone network incorporating the Multi-Stream Block (MSB) by combining the CNN inductive bias and Transformer long-term dependence modeling capability. Finally, we propose the Patch Attention Block (PAB) to obtain the final distorted image quality score by fusing the degradation features learned from contrastive learning with the perceptual distortion information extracted by the backbone network. Experimental results on six standard IQA datasets show that SaTQA outperforms the state-of-the-art methods for both synthetic and authentic datasets. Code is available at https://github.com/I2-Multimedia-Lab/SaTQA.",
    "full_text": "Transformer-Based No-Reference Image Quality Assessment via Supervised\nContrastive Learning\nJinsong Shi, Pan Gao*, Jie Qin*\nCollege of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics\n{srache, pan.gao}@nuaa.edu.cn, qinjiebuaa@gmail.com\nAbstract\nImage Quality Assessment (IQA) has long been a research\nhotspot in the field of image processing, especially No-\nReference Image Quality Assessment (NR-IQA). Due to the\npowerful feature extraction ability, existing Convolution Neu-\nral Network (CNN) and Transformers based NR-IQA meth-\nods have achieved considerable progress. However, they still\nexhibit limited capability when facing unknown authentic\ndistortion datasets. To further improve NR-IQA performance,\nin this paper, a novel supervised contrastive learning (SCL)\nand Transformer-based NR-IQA model SaTQA is proposed.\nWe first train a model on a large-scale synthetic dataset by\nSCL (no image subjective score is required) to extract degra-\ndation features of images with various distortion types and\nlevels. To further extract distortion information from images,\nwe propose a backbone network incorporating the Multi-\nStream Block (MSB) by combining the CNN inductive bias\nand Transformer long-term dependence modeling capabil-\nity. Finally, we propose the Patch Attention Block (PAB)\nto obtain the final distorted image quality score by fusing\nthe degradation features learned from contrastive learning\nwith the perceptual distortion information extracted by the\nbackbone network. Experimental results on six standard IQA\ndatasets show that SaTQA outperforms the state-of-the-art\nmethods for both synthetic and authentic datasets. Code is\navailable at https://github.com/I2-Multimedia-Lab/SaTQA.\nIntroduction\nImage Quality Assessment (IQA) refers to the quantitative\nanalysis of the content of an image, thus quantifying the de-\ngree of visual distortion of a distorted image. The relevant\nevaluation methods are generally divided into two types:\nsubjective quality assessment (Mantiuk, Tomaszewska, and\nMantiuk 2012) and objective quality assessment (Yang et al.\n2009). Among them, the objective quality assessment is di-\nvided into three categories according to the extent of infor-\nmation the algorithm needs from reference image (Wang\nand Bovik 2006): full reference quality assessment (FR-\nIQA) (Wang et al. 2004), reduced-reference quality assess-\nment (RR-IQA) (Rehman and Wang 2012) and no reference\nquality assessment (NR-IQA) (Mittal, Moorthy, and Bovik\n*Corresponding authors\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nLIVE-FB KONIQ LIVE…Challenge\n0.6\n0.7\n0.8\n0.9PLCC\nHyperIQA\nTReS\nMANIQA\nSaTQA\n0.82 0.84 0.86 0.88\nSROCC\n0.82\n0.84\n0.86\n0.88\n0.90PLCC\nMetaIQA\nDBCNN\nHyperIQA\nTReS\nMANIQA\nSaTQA\nFigure 1: Quantitative comparison of NR-IQA methods. The\nleft figure presents our PLCC results on three authentic\ndatasets compared to the latest NR-IQA method. The right\nfigure presents the average performance of our method com-\npared to other methods on all datasets. Higher coefficient\nmatches perceptual score better.\n2012). The FR-IQA and RR-IQA methods calculate the im-\nage quality by comparing the difference between the ref-\nerence image and the distorted image, while the NR-IQA\nmethod evaluates the image quality based on the distorted\nimage itself without any reference information, making it\nthe most challenging task and more promising in practical\napplications.\nWith recent advance of deep learning techniques in com-\nputer vision, more and more data-driven approaches have\nbeen developed for NR-IQA (Mittal, Moorthy, and Bovik\n2012; Zhang, Zhang, and Bovik 2015; Xue, Zhang, and Mou\n2013; Golestaneh, Dadsetan, and Kitani 2022; Yang et al.\n2022). However, most existing deep learning based NR-IQA\nmodels are trained using the whole training dataset without\ndistinguishing different distortion types and levels, which\nmay render them exhibiting limited capability when facing\na large-scale authentic dataset, such as LIVE-FB (Ying et al.\n2020). As shown in Fig 1, even the recent most powerful\nTransformer-based NR-IQA model MANIQA (Yang et al.\n2022), the PLCC result on LIVE-FB is down to less than\n0.6. This is because, for human eyes, even a same level\nof distortion but induced by two different types of noises\nmay result in very different visual perception. Thus, without\ndistinguishing different noise types and levels during train-\ning, the model may not perform well in the face of a large-\nscale dataset containing excessive distortions. Some CNN-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4829\nbased methods, such as CONTRIQUE (Madhusudana et al.\n2022), attempted to solve this problem by specifying the\ndistortion type and level or using other type of content as\nthe label for distortion classification during training. How-\never, this strategy is not applicable to large-scale authen-\ntic dataset, where the distortion type and level are impos-\nsible to explicitly specify. To tackle this problem, we pro-\npose a supervised contrastive learning based quality assess-\nment method. By conducting self-supervised pre-training on\nthe large scale synthetic dataset KADIS (Lin, Hosu, and\nSaupe 2020), degradation features corresponding to differ-\nent types and levels of distorted images are obtained as\nprior knowledge for final quality score regression. In the\narchitecture design of IQA model, traditional methods usu-\nally employ CNN for perceptual distortion feature extrac-\ntion (Saad, Bovik, and Charrier 2012; Mittal, Moorthy, and\nBovik 2012; Zhang, Zhang, and Bovik 2015; Xue, Zhang,\nand Mou 2013). However, the perception of image quality\nby humans is usually performed by viewing the entire im-\nage, i.e., by integrating the content distortion globally over\nthe whole image. Due to limited receptive field of CNN, the\nperformance of CNN based IQA models may be restricted.\nOn the contrary to CNN, Transformer can integrate global\ndistortion information of images due to its long range depen-\ndency modeling ability, which is thus naturally suitable for\nNR-IQA. Inspired by this, a plethora of Transformer based\nNR-IQA approaches have been proposed (You and Korho-\nnen 2021; Ke et al. 2021; Golestaneh, Dadsetan, and Ki-\ntani 2022; Yang et al. 2022). Nevertheless, pure Transformer\nhas weak ability to extract local details such as image edges\nand textures, which may degrade the final prediction perfor-\nmance. In addition, most previous NR-IQA methods, such\nas MANIQA (Yang et al. 2022), used the pretrained Trans-\nformer as the backbone that is originally designed for image\nclassification, which may not be well suitable for IQA that\nis closely related to human eye perception. As can be seen\nfrom the Grad-CAM (Selvaraju et al. 2017) in Fig. 2, pure\nViT (Dosovitskiy et al. 2020) model focuses more on the se-\nmantic region of the image, while missing the distorted areas\nof the image. To this end, we propose a Multi-Stream Block\n(MSB), which leverages the CNN and the Transformer fea-\ntures, aiming to combine the strengths of CNN’s edge ex-\ntraction while maintaining the global modeling capability\nof the Transformer. We further propose the Patch Attention\nBlock (PAB) to fuse the learned perceptual feature with the\ndegradation features from contrastive learning, and the fused\nfeatures will be used for the prediction of objective image\nquality scores.\nThe main contributions of this paper are as follows:\n• We use supervised contrastive learning to perform\nself-supervised pre-training on the large-scale dataset\nKADIS, and the learned degradation features corre-\nsponding to various distortion types and levels of images\nare used to guide the training of the quality score gener-\nation network.\n• We propose the MSB module that combines the features\nof CNN and Transformer, which introduces the induc-\ntive bias in CNN while ensuring the long-term depen-\nFigure 2: Grad-CAM (Selvaraju et al. 2017) of ViT and\nSaTQA on distorted image.\ndency modeling capability of Transformer for enhancing\nthe ability of the backbone network to extract perceptual\ndistortion information.\n• Extensive experiments on six IQA datasets containing\nsynthetic and authentic show that our proposed model\noutperforms current mainstream NR-IQA methods by a\nlarge margin.\nRelated Work\nNo-Reference Image Quality Assessment\nThe previous NR-IQA methods were mainly oriented to\nquality assessment task for specific distortion types (Li et al.\n2013, 2015; Liu, Klomp, and Heynderickx 2009; Wang et al.\n2019). These methods use statistical information on images\nto perform quality assessment of images with known dis-\ntortion types. However, they are less effective when quality\nassessment is performed on images with unknown distortion\ntypes and therefore restrictive. Current NR-IQA methods fo-\ncus on general-purpose, and can be further divided into two\ntypes according to the feature extraction method: natural\nscene statistics (NSS)-based metrics (Gao et al. 2013; Mittal,\nMoorthy, and Bovik 2012; Moorthy and Bovik 2010; Saad,\nBovik, and Charrier 2012; Fang et al. 2014) and learning-\nbased metrics (Ye and Doermann 2012; Ye et al. 2012;\nZhang et al. 2015, 2018; Lin and Wang 2018; Su et al. 2020;\nZhu et al. 2020). The NSS-based methods consider that nat-\nural images have regular statistical features (e.g., brightness,\ngradient, etc.), and images with different distortion types and\nlevels will have different effects on this regularity. Based\non this regulation, (Mittal, Moorthy, and Bovik 2012) used\nthe NSS features in the spatial domain to construct the NR-\nIQA model. (Fang et al. 2014) used the distribution func-\ntion obtained by fitting the distorted image features to con-\nstruct the evaluation model based on the moment charac-\nteristics of the image and the information entropy. (Moorthy\nand Bovik 2010) used the discrete wavelet transform (DWT)\nto extract NSS features, while (Saad, Bovik, and Charrier\n2012) used the discrete cosine transform (DCT) to evaluate\nthe image quality. However, the NR-IQA methods based on\nNSS require manual extraction of local or global features\nof the image, and the feature representation extracted by\nthis hand-crafted approach is often not effective. Although\nthese NR-IQA methods show some performance gain on\nsome synthetic distorted datasets, the results are not as good\nas on authentic distorted datasets. With the success of deep\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4830\nFeature Extraction\nScore Prediction\nMulti-Stream\nBlock\nStage3\n×3 Patch Attention Block\nQuality\nScore\n··· \nDistortion\nDist.\nPooling\nand\nConcat\nEncoder\nProjector\nContrastive\nLoss\nSelf-Supervised Pre-training\nDegradation\nRef.\nInference\nProjector\nMulti-Stream\nBlock\nPatch Sampling\nStage1\n×1\nMulti-Stream\nBlock\nPatch Sampling\nStage2\n×5\nFigure 3: The overall framework of our approach for no-reference image quality assessment. Pi, Pj, and Pk refer to the\ndegradation features, zi, zj, and zk correspond to the vectors after projected. ϕ denotes stopping gradient propagation.\nlearning on various vision tasks, CNN-based NR-IQA meth-\nods have also achieved some success on distorted datasets.\nDBCNN (Zhang et al. 2018) uses a CNN structure based on\nbilinear pooling for NR-IQA model building and achieves\ngood performance gain on synthetic datasets. Hallucinated-\nIQA (Lin and Wang 2018) proposes an NR-IQA model\nbased on generative adversarial networks, which obtains the\nfinal quality score by pairing the generated hallucinated ref-\nerence image with the distorted image. HyperIQA (Su et al.\n2020) predicts image quality by fusing distortion informa-\ntion from images of different scales. MetaIQA (Zhu et al.\n2020) trains the model to acquire prior knowledge of dis-\ntorted images via meta-learning and solves the generaliza-\ntion performance problem of NR-IQA task to some extent.\nTransformer for IQA\nIn the past few years, CNNs had become the popular back-\nbone of computer vision tasks and achieved success in the\nfield of IQA. Unfortunately, CNNs are highly flawed in cap-\nturing non-local information and have a strong localization\nbias. In addition, the shared convolutional kernel weights\nand translational invariance properties of CNNs also lead\nto the inability of CNNs to handle complex feature com-\nbinations. Inspired by the use of Transformer for long-range\ndependency modeling in the NLP domain (Vaswani et al.\n2017), ViT (Dosovitskiy et al. 2020), a Transformer model\nfor computer vision tasks, has achieved initial success. In\nthe field of IQA, IQT (You and Korhonen 2021) applies\na hybrid architecture to extract feature information of ref-\nerence and distorted images by CNN and use it as input\nto Transformer. MUSIQ (Ke et al. 2021) proposes a multi-\nscale image quality Transformer, which solves the problem\nof different image resolutions by encoding three scales of\nan image. TReS (Golestaneh, Dadsetan, and Kitani 2022)\nproposes to use relative ranking and self-consistency loss\nto train the model and thus enhance the evaluation per-\nformance. MANIQA (Yang et al. 2022) proposes a multi-\ndimension attention mechanism for multi-dimensional inter-\naction on channel and spatial domains, and achieves promis-\ning evaluation performance on synthetic datasets. However,\nall these Transformer based models are trained without dis-\ntinguishing the latent degradation from other ones, which\nmay not generalize well to a large-scale authentic dataset.\nContrastive Learning\nUnlike supervised learning, self-supervised learning aims to\nacquire features using unlabeled data. Current contrastive\nlearning methods mainly focus on instance recognition\ntasks, i.e., the image itself and its corresponding enhanced\nsamples are considered as the same class (Dosovitskiy et al.\n2014; Chen et al. 2020; He et al. 2020). In addition, some\nself-supervised methods learn data features by constructing\nauxiliary tasks, which are often large and do not require\nmanual annotation. Usually, these auxiliary tasks include ro-\ntation prediction (Gidaris, Singh, and Komodakis 2018), im-\nage restoration (Pathak et al. 2016), etc. (Liu, Van De Wei-\njer, and Bagdanov 2019) used image ranking as an auxiliary\ntask and the proposed NR-IQA model achieved good perfor-\nmance on synthetic datasets. CONTRIQUE (Madhusudana\net al. 2022) used distortion type and level as an auxiliary task\nby comparing user-generated content (UGC) with the im-\nages in synthetic datasets, finally, the learned features were\nused for image quality prediction. In this paper, we proposed\na supervised contrastive learning (SCL) approach inspired\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4831\nby this auxiliary task construction method. But, taking one\nstep further, we remove the use of UGC images and use the\nreference images from the synthetic dataset alone for com-\nparasion.\nProposed Method\nIn this section, we detail our proposed model, which is an\nNR-IQA method based on supervised contrastive learning\nand Transformer named SaTQA, and Fig. 3 shows the ar-\nchitecture of our proposed model.\nOverall Pipeline\nGiven a distorted image I ∈ RH×W×3, where H and W\ndenote height and width, respectively, and the goal of NR-\nIQA is to predict its objective quality score. First, we use\nViT (Dosovitskiy et al. 2020) to extract the original features\nof the image, where each layer of extracted features defines\nFi ∈ R(h×w)×Ci , where i ∈ {1,2, ...,12}. In this paper, we\nextract four of these layers and concatenate them together by\nchannel dimension, denoted as bF ∈ R\nP\ni (h×w)×Ci , where\ni ∈ {1,2, 5, 6}, Ci denotes the dimension of the ViT layer,\nh and w denotes the height and width of the correspond-\ning output, respectively. Then we downscale bF along chan-\nnel dimension to get bF\n′\n∈ Rh×w×D. Next we use a three-\nstage MSB module for image distortion extraction, where\nthe first two stages are down-sampled after MSB process-\ning. The distortion information is extracted by the backbone\nnetwork and the output is R ∈ R(h4×w4)×D, where D de-\nnotes the channel dimension of the features, h4 and w4 de-\nnotes the height and width of the last stage output. At the\nsame time, the distorted image is passed through ResNet-\n50 (He et al. 2016) trained with SCL and the output is P,\nP ∈ R(h4×w4)×P\ni ci . We make P and R have the same\nshape, and thus D = P\ni ci. Next, the PAB module fuses the\nfeatures of P and R to obtain the feature S for the image\nquality, and the final S is regressed to the objective quality\nscore corresponding to the image.\nSupervised Contrastive Learning\nSelf-supervised learning improves model performance by\naugmenting the model’s ability to extract data features\nthrough auxiliary tasks. In the NR-IQA domain, we need the\nmodel to distinguish the distortion types and levels of differ-\nent images, and the target of this auxiliary task is thus similar\nto the classification of image. However, different from tra-\nditional supervised classification, we train the model using\nSCL on a large-scale synthetic dataset to obtain a more ro-\nbust and generalized degradation features than classification,\nso that the degradation from the same distortion type (level)\nare pulled closer together than degradation from different\ndistortion type (level). We then generalize the pre-trained\nmodel to other dataset with latent degradation, e.g., authen-\ntic distortion dataset. As shown in Fig. 4, at the beginning,\nthe distribution of various distorted images on the feature\nspace is random and unordered. Our goal is that images with\nthe same distortion and level can be clustered together after\nusing supervised contrastive learning.\nSCL \nColor diffusion\nHigh sharpen\n Brighten\nFigure 4: Left: Distribution of images with the same level of\ndifferent distortion types in the feature space. Right: Distri-\nbution of images after training with SCL, where the images\nwith color diffusion, high sharpen, brighten are clustered to-\ngether, respectively.\nIn our proposed supervised contrastive learning for dis-\ntorted image clustering, firstly, Iv\nu is defined to denote the\ndistorted image, u ∈ {1, ..., U} to denote the different dis-\ntortion types corresponding to I, and v ∈ {1, ..., V} to de-\nnote the distortion level under each u, where U and V de-\nnote the total number of distortion types and levels. Thus,\ntogether with the class of the reference image, there are a\ntotal of U × V + 1 classes. That is, this auxiliary task is\ntransformed into distinguish different types and levels of\ndistorted images problem. Inspired by (Chen et al. 2020),\nwe use encoder network f(.) together with projector g(.)\nstructure to learn the degradation features of the images.\nIn this paper, the encoder uses ResNet-50, and the projec-\ntor uses a multilayer perceptron (MLP). For a given image\nx, r = f(x), z = g(r)(z ∈ RD), where z denotes the\nD-dimensional feature of the output, r denotes the feature\nrepresentation of the encoder output. As with (Chen et al.\n2020), the encoder output is L2 normalized before being\npassed to the projector. The contrast loss function uses a su-\npervised normalized temperature-scaled cross entropy (NT-\nXent) (Khosla et al. 2020), which can be formulated as:\nLfea = 1\n|P(i)|\nX\nj∈P(i)\n−log exp(ϕ(zi, zj)/τ)PN\nk=1 Ik̸=i exp(ϕ(zi, zk)/τ)\n(1)\nwhere N denotes the number of all distortions and refer-\nences images in a mini-batch, I denotes the indicator func-\ntion, τ denotes the temperature parameter, P(i) denotes the\nset belonging to the same classi, and |P(i)| is its cardinality.\nϕ(m, n)denotes ϕ(m, n) =mT n/∥m∥2∥n∥2.\nMulti-Stream Block\nAs shown in Fig. 5 (a), we propose a Multi-Stream Block\n(MSB) that combines CNN to extract low-level edge fea-\ntures and Transformer to extract high-level content features\nof images. First, we reshape the feature bF′ after ViT ex-\ntraction into feature map eF ∈ RD×h×w; then we divide\nit into three parts, χ1 ∈ RD1×h×w, χ2 ∈ RD2×h×w, and\nχ3 ∈ RD3×h×w, along the channel dimension, where χ1\nand χ2 are feed to CNN and χ3 is feed to Transformer. χ1\nwill go through deformable convolution (DeformConv) and\nlinear layers to get χ′\n1. χ2 will first go through depthwise\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4832\n+\nLinear\nMHSA\nPool & Up\nDwConvDeformConv\nLinear\nLow-Level\nHigh-Level\n…\n…\nC\nCBAM\nC\n+\nConcat\nElement-wise\nAddition\nCross-Attention\n… …\nDegradation Distortion\nFFN\nAdd & Norm\n…\nFusion\n(a) Multi-Stream Block (b) Patch Attention Block\nFigure 5: Overview of MSB and PAB.\nconvolution (DwConv) and max-pooling layers, and then\nup-sample to getχ ′\n2. χ3 will go through Multi-Head Self-\nAttention (MHSA) and linear layers to get χ′\n3. These can be\nformulated as:\nχ′\n1 = FC(Deform(Conv(χ1)) (2)\nχ′\n2 = UpSample(MaxPool(DwConv(χ2))) (3)\nχ′\n3 = FC(MHSA(χ3)) (4)\nFinally, χ′\n1, χ′\n2 and χ′\n3 are concatenated together and re-\nshaped, which is denoted as eF′ ∈ RD×h×w. In addition,\nwe connect eF to eF′ with residual learning through the\nCBAM (Woo et al. 2018) (i.e., channel attention and spatial\nattention) module. In this paper, the percentage of χ1, χ2\nand χ3 in the channel dimension will change with different\nstages. At the beginning, all three share the same percentage\nin the channel, and then χ3 will gradually increase its per-\ncentage in the total channels. Note that, in CNN branches,\ndeformable convolution can augment the spatial sampling\nlocations, and depthwise convolution can reduce the num-\nber of computations and supplement the channel attention\nthat is not modeled in Transformer branch.\nPatch Attention Block\nAfter self-supervised pre-training on the large-scale IQA\ndataset KADIS, the degradationP can be used to distinguish\nthe distortion level and type of the image to be evaluated. As\nshown in Fig. 5 (b), We map P to Query (Q) and R to Key\n(K) and Value (V ), and then calculate them by Eq. (5) to\nobtain the final image quality features S. With this kind of\ncomputation, the model can achieve good performance in\nthe evaluation and converge faster even on small-scale IQA\ndatasets due to P-assisted training.\nS = Softmax\n\u0012Q × KT\n√\nD\n\u0013\n× V (5)\nQuality Prediction\nFor score prediction, we first reshape the tokens from patch\nattention block into feature maps, and then the quality score\nDatasets Dist. Images Dist.\ntypes Dataset types\nLIVE 799 5 Synthetic\nCSIQ\n866 6 Synthetic\nTID2013 3000 24 Synthetic\nKADID-10K 10125 25 Synthetic\nKADIS 700k 25 Synthetic\nLIVEC 1162 - Authentic\nKONIQ 10073 - Authentic\nLIVE-FB 39810 - Authentic\nTable 1: Summary of IQA datasets.\nwas regressed with the quality vector obtained via global av-\nerage pooling (GAP). In each batch of images, we train our\nquality prediction model by minimizing the regression loss\nL as follows:\nL = 1\nN\nNX\ni=1\n∥pi − yi∥1 (6)\nwhere N denotes the batch size, pi denotes the image qual-\nity score predicted by the model for the ith image, and yi\ndenotes the corresponding objective quality score.\nExperiments\nDatasets\nFor self-supervised pre-training, we use the KADIS (Lin,\nHosu, and Saupe 2020) dataset. The KADIS dataset con-\ntains 140,000 original reference images, 700,000 distorted\nimages, and has no subjective quality score. It has 25 distor-\ntion types with five distortion levels for each distortion type.\nIn addition, we evaluated the performance of our proposed\nmodel on six standard synthetic and authentic IQA datasets.\nThe synthetic datasets include CSIQ (Larson and Chandler\n2010), TID2013 (Ponomarenko et al. 2015), and KADID-\n10K (Lin, Hosu, and Saupe 2019), and the authentic datasets\ninclude LIVE Challenge (Ghadiyaram and Bovik 2015),\nKONIQ (Hosu et al. 2020), and LIVE-FB (Ying et al. 2020).\nTab. 1 shows a summary of the datasets used in the experi-\nments. The commonly used observer ratings for the images\nare expressed by Mean opinion score (MOS) and Differen-\ntial Mean opinion score (DMOS), where larger MOS values\nindicate better image quality and larger DMOS values indi-\ncate poorer image quality.\nEvaluation Metric\nWe use Spearman’s rank order correlation coefficient\n(SROCC) and Pearson’s linear correlation coefficient\n(PLCC) to measure the performance of the NR-IQA method.\nThe values of SROCC and PLCC are in the range [-1, 1],\nand the higher absolute values indicate higher correlation\nand vice versa.\nImplementation Details\nIn this paper, all of our experiments are performed using Py-\nTorch on a single NVIDIA GeForce RTX3090. During im-\nage preprocessing, we cropped 8 random 224×224 patches\nfrom each image and randomly flipped the cropped patches\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4833\nCSIQ TID2013 KADID-10K LIVE Challenge KONIQ LIVE-FB\nPLCC SROCC PLCC SROCC PLCC SROCC PLCC SROCC PLCC SROCC PLCC SROCC\nDIIVINE 0.776 0.804 0.567 0.643 0.435 0.413 0.591 0.588 0.558 0.546 0.187 0.092\nBRISQUE 0.748 0.812 0.571 0.626 0.567 0.528 0.629 0.629 0.685 0.681 0.341 0.303\nILNIQE 0.865 0.822 0.648 0.521 0.558 0.534 0.508 0.508 0.537 0.523 0.332 0.294\nBIECON 0.823 0.815 0.762 0.717 0.648 0.623 0.613 0.613 0.654 0.651 0.428 0.407\nMEON 0.864 0.852 0.824 0.808 0.691 0.604 0.710 0.697 0.628 0.611 0.394 0.365\nWaDIQaM 0.844 0.852 0.855 0.835 0.752 0.739 0.671 0.682 0.807 0.804 0.467 0.455\nDBCNN 0.959 0.946 0.865 0.816 0.856 0.851 0.869 0.869 0.884 0.875 0.551 0.545\nTIQA 0.838 0.825 0.858 0.846 0.855 0.850 0.861 0.845 0.903 0.892 0.581 0.541\nMetaIQA 0.908 0.899 0.868 0.856 0.775 0.762 0.802 0.835 0.856 0.887 0.507 0.540\nP2P-BM 0.902 0.899 0.856 0.862 0.849 0.840 0.842 0.844 0.885 0.872 0.598 0.526\nHyperIQA 0.942 0.923 0.858 0.840 0.842 0.844 0.885 0.872 0.920 0.907 0.602 0.544\nTReS 0.942 0.922 0.883 0.863 0.858 0.859 0.877 0.846 0.928 0.915 0.625 0.554\nMANIQA 0.968 0.961 0.943 0.937 0.946 0.944 0.887 0.871 0.915 0.880 0.597 0.543\nSaTQA (Ours) 0.972 0.965 0.948 0.938 0.949 0.946 0.903 0.877 0.941 0.930 0.676 0.582\nTable 2: Comparison of SaTQA v.s. state-of-the-art NR-IQA algorithms on synthetically and authentically distorted datasets.\nThe best performances are in bold, and the second best are underlined. Some are borrowed from (Golestaneh, Dadsetan, and\nKitani 2022).\nDataset LIVE CSIQ\nType JP2K JPEG WN GB\nFF WN JPEG JP2K FN\nGB CC\nBRISQUE 0.929 0.965 0.982 0.964\n0.828 0.723 0.806 0.840 0.378\n0.820 0.804\nILNIQE 0.894 0.941 0.981 0.915\n0.833 0.850 0.899 0.906 0.874\n0.858 0.501\nHOSA 0.935 0.954 0.975 0.954\n0.954 0.604 0.733 0.818 0.500\n0.841 0.716\nBIECON 0.952 0.974 0.980 0.956 0.923 0.902 0.942 0.954 0.884\n0.946 0.523\nWaDIQaM 0.942 0.953 0.982 0.938\n0.923 0.974 0.853 0.947 0.882\n0.976 0.923\nPQR 0.953 0.965 0.981 0.944 0.921 0.915 0.934 0.955 0.926\n0.921 0.837\nHyperIQA 0.949 0.961 0.982 0.926\n0.934 0.927 0.934 0.960 0.931\n0.915 0.874\nMANIQA 0.870 0.895 0.984 0.959\n0.896 0.966 0.971 0.973 0.977\n0.956 0.946\nOurs 0.947 0.965 0.989 0.988 0.955 0.985 0.984 0.991 0.986\n0.980 0.970\nTable 3: SROCC comparisons of individual distortion types on the LIVE and CSIQ datasets.\nhorizontally. The trained patches inherit the quality score\nof the original images. The Encoder used for contrastive\nlearning is a modified ResNet-50 (He et al. 2016), and the\nfeature extraction part of the backbone network uses ViT-\nB/8 (Dosovitskiy et al. 2020), where the patch size is 8.\nSaTQA contains three stages, the first two stages consist of\nMulti-Steam Block (MSB) and Patch Sampling, and the last\nstage contains only MSB, where D is set to 768 and h4 and\nw4 are 7. In the three stages, the number of channels per\nbranch within MSB D1, D2 and D3 are [256, 256, 256],\n[192,192, 384], and [48,48,672]. The same training strategy\nas used in the existing NR-IQA algorithm. For the synthetic\ndataset, we divide the dataset based on the reference im-\nage. In the training process, we use AdamW optimizer with\nlearning rate of 2 × 10−5, weight decay of 1 × 10−2, and\nlearning strategy of cosine annealing, where Tmax is set to\n50 and etamin is 0. Experiments are trained for 150 epochs.\nThe loss function used is L1Loss, and the batch size is 4. For\ntesting, we generate the final objective quality score by aver-\naging the 8 cropped patch scores of the distorted images. We\nreport the average of the results by running the experiment\n10 times with different seeds.\nPerformance Comparison\nIndividual dataset evaluations. As shown in Tab. 2, the\nSROCC and PLCC of SaTQA on all datasets are outper-\nform of the state-of-the-art methods (Saad, Bovik, and Char-\nrier 2012; Mittal, Moorthy, and Bovik 2012; Zhang, Zhang,\nand Bovik 2015; Kim and Lee 2016; Ma et al. 2017; Bosse\net al. 2017; Zhang et al. 2018; You and Korhonen 2021; Zhu\net al. 2020; Ying et al. 2020; Su et al. 2020; Golestaneh,\nDadsetan, and Kitani 2022; Yang et al. 2022; Xu et al.\n2016; Zeng, Zhang, and Bovik 2017). As the current corre-\nlation coefficients of the NR-IQA method on most syhthetic\ndatasets already converge to 1 (especially the performance\non CSIQ approaches saturation), and it is thus extremely\ndifficult to further make significant improvement on those\ndatasets. Nevertheless, the performance of SaTQA on these\nsynthetic datasets is still improved, which demonstrates the\neffectiveness of our proposed model. Moreover, SaTQA per-\nforms well on the authentic dataset, not only on the small-\nscale dataset LIVE Challenge, but also on the current large-\nscale LIVE-FB dataset.\nIndividual distortion evaluations.Since there are various\ntypes of distortion in images, especially in the authentic\ndataset, an image may contain multiple types of distortion\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4834\nDMOS ↓\nMANIQA\nSaTQA(Ours)\n0.0081 (1)\n0.0531 (1)\n0.0198 (1)\n0.1805 (2)\n0.2074 (2)\n0.1825 (2)\n0.5178 (3)\n0.5179 (3)\n0.5420 (3)\n0.7246 (4)\n0.8139 (5)\n0.7820 (4)\n0.7907 (5)\n0.7549 (4)\n0.8123 (5)\nFigure 6: Example images of JPEG distortion types from the CSIQ test dataset. DMOS denotes the human rating, and smaller\nDMOS values indicate better image quality. The number in the parenthesis denotes the rank among considered distorted images.\nTrained on KonIQ LIVEC LIVE\nTest on LIVEC KONIQ CSIQ TID2013\nWaDIQaM 0.682 0.711 0.704 0.462\nP2P-BM 0.770 0.740 0.712 0.488\nHyperIQA 0.785 0.772 0.744 0.551\nTReS 0.786 0.733 0.761 0.562\nMANIQA 0.773 0.759 0.813 0.581\nProposed 0.791 0.788 0.831 0.617\nTable 4: SROCC evaluations on cross datasets, where bold\nentries indicate the best performers.\n(e.g., blur, noise, etc.). To validate the general performance\nof SaTQA on the distortion types, we conducted experi-\nments on synthetic datasets LIVE and CSIQ.As can be seen\nfrom Tab. 3, our model has a strong generalization perfor-\nmance on CSIQ and LIVE dataset.\nCross-dataset evaluations.To verify the generalization per-\nformance of the model on different datasets, we conducted\ncross-dataset experiments. In this validation, the training\nprocess is performed on a specific dataset and the test is ex-\necuted on another different dataset without any parameter\ntuning during the test. As shown in Tab. 4, our model has\nstrong generalization ability.\nAblation Study\nIn Tab. 5, we provide ablation studies to validate the effect\nof each component of our proposed model. We use the ViT\npre-trained on ImageNet as our baseline (#1), with the patch\nsize of 8, and the original CLS token for quality regression.\nSupervised Contrastive Learning.To efficiently utilize a\nlarge amount of unlabeled data and enhance the prediction\nperformance of the model on the authentic IQA dataset, we\npropose the SCL module. The results in #2 indicate that the\nuse of SCL can significantly improve the performance of\nthe model on the LIVE Challenge (+0.044 PLCC, +0.037\nSROCC). For CSIQ dataset (+0.041 PLCC, +0.04 SROCC).\nMulti-Stream Block.To leverage the feature extraction ca-\npabilities of CNN and Transformer for IQA tasks, we pro-\npose the MSB. In #3, the model has 0.01 improvement in\nSROCC and PLCC on both datasets.\nPatch Attention Block. PAB is propesed to utilize the\nSCL MSB PAB LIVEC CSIQ\nPLCC SROCC PLCC SROCC\n0.827 0.803 0.922 0.916\n✓ 0.871 0.840 0.963 0.956\n✓ ✓ 0.880 0.851 0.970 0.963\n✓ ✓ ✓ 0.903 0.877 0.972 0.965\nTable 5: Ablation study on modules. #1 represents the base-\nline performance without using any proposed components.\ndegradation learned by SCL as a query to compute cross-\nattention with the perceptual distortion extracted by MSB.\nFrom #4, by adding PAB, the model has a 0.02 improve-\nment in SROCC and PLCC on the LIVE Challenge dataset\nand a 0.002 increase on the CSIQ dataset.\nVisual Result Analysis\nWe provide the DMOS prediction results of MANIQA and\nSaTQA for JPEG distortion images on CSIQ test set as an\nexample. As shown in Fig. 6, it can be seen that SaTQA\nperforms better than MANIQA both in image distortion\nlevel ranking and in overall DMOS value prediction, where\nMANIQA produces errors in the prediction of the fourth and\nfifth distortion levels.\nConclusion\nIn this paper, we propose SaTQA, a specialized model for\nNR-IQA by combining supervised contrastive learning and\nTransformer. Initially, SCL is utilized to address incapability\nof existing models when facing authentic datasets for qual-\nity prediction. To enhance the feature extraction capability\nfor distorted images and overcome the limitations of global\nmodeling in CNN, we introduce the MSB module. Addi-\ntionally, we design the PAB module to effectively fuse im-\nage degradation features and distortion information. Experi-\nmental results on six standard IQA datasets demonstrate the\nremarkable performance achieved by our model.\nAcknowledgements\nThis work was supported in part by the Natural Science\nFoundation of China (No. 62272227 and No. 62276129),\nand the Natural Science Foundation of Jiangsu Province\n(No. BK20220890).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4835\nReferences\nBosse, S.; Maniry, D.; M ¨uller, K.-R.; Wiegand, T.; and\nSamek, W. 2017. Deep neural networks for no-reference and\nfull-reference image quality assessment. IEEE Transactions\non image processing, 27(1): 206–219.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.\nA simple framework for contrastive learning of visual repre-\nsentations. In International conference on machine learning,\n1597–1607. PMLR.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nDosovitskiy, A.; Springenberg, J. T.; Riedmiller, M.; and\nBrox, T. 2014. Discriminative unsupervised feature learn-\ning with convolutional neural networks. Advances in neural\ninformation processing systems, 27.\nFang, Y .; Ma, K.; Wang, Z.; Lin, W.; Fang, Z.; and Zhai, G.\n2014. No-reference quality assessment of contrast-distorted\nimages based on natural scene statistics. IEEE Signal Pro-\ncessing Letters, 22(7): 838–842.\nGao, X.; Gao, F.; Tao, D.; and Li, X. 2013. Universal blind\nimage quality assessment metrics via natural scene statistics\nand multiple kernel learning. IEEE Transactions on neural\nnetworks and learning systems, 24(12).\nGhadiyaram, D.; and Bovik, A. C. 2015. Massive online\ncrowdsourced study of subjective and objective picture qual-\nity. IEEE Transactions on Image Processing, 25(1): 372–\n387.\nGidaris, S.; Singh, P.; and Komodakis, N. 2018. Unsuper-\nvised representation learning by predicting image rotations.\narXiv preprint arXiv:1803.07728.\nGolestaneh, S. A.; Dadsetan, S.; and Kitani, K. M. 2022. No-\nreference image quality assessment via transformers, rela-\ntive ranking, and self-consistency. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, 1220–1230.\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020.\nMomentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 9729–9738.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHosu, V .; Lin, H.; Sziranyi, T.; and Saupe, D. 2020. KonIQ-\n10k: An ecologically valid database for deep learning of\nblind image quality assessment. IEEE Transactions on Im-\nage Processing, 29: 4041–4056.\nKe, J.; Wang, Q.; Wang, Y .; Milanfar, P.; and Yang, F. 2021.\nMusiq: Multi-scale image quality transformer. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, 5148–5157.\nKhosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y .; Isola,\nP.; Maschinot, A.; Liu, C.; and Krishnan, D. 2020. Super-\nvised contrastive learning. Advances in Neural Information\nProcessing Systems, 33: 18661–18673.\nKim, J.; and Lee, S. 2016. Fully deep blind image quality\npredictor. IEEE Journal of selected topics in signal process-\ning, 11(1): 206–220.\nLarson, E. C.; and Chandler, D. M. 2010. Most apparent dis-\ntortion: full-reference image quality assessment and the role\nof strategy. Journal of electronic imaging, 19(1): 011006.\nLi, L.; Lin, W.; Wang, X.; Yang, G.; Bahrami, K.; and Kot,\nA. C. 2015. No-reference image blur assessment based on\ndiscrete orthogonal moments. IEEE transactions on cyber-\nnetics, 46(1): 39–50.\nLi, L.; Zhu, H.; Yang, G.; and Qian, J. 2013. Referenceless\nmeasure of blocking artifacts by Tchebichef kernel analysis.\nIEEE Signal Processing Letters, 21(1): 122–125.\nLin, H.; Hosu, V .; and Saupe, D. 2019. KADID-10k: A\nlarge-scale artificially distorted IQA database. In 2019\nEleventh International Conference on Quality of Multime-\ndia Experience (QoMEX), 1–3. IEEE.\nLin, H.; Hosu, V .; and Saupe, D. 2020. Weak super-\nvision for deep IQA feature learning. arXiv preprint\narXiv:2001.08113.\nLin, K.-Y .; and Wang, G. 2018. Hallucinated-IQA: No-\nreference image quality assessment via adversarial learning.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 732–741.\nLiu, H.; Klomp, N.; and Heynderickx, I. 2009. A no-\nreference metric for perceived ringing artifacts in images.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 20(4): 529–539.\nLiu, X.; Van De Weijer, J.; and Bagdanov, A. D. 2019. Ex-\nploiting unlabeled data in cnns by self-supervised learning\nto rank. IEEE transactions on pattern analysis and machine\nintelligence, 41(8): 1862–1878.\nMa, K.; Liu, W.; Zhang, K.; Duanmu, Z.; Wang, Z.; and Zuo,\nW. 2017. End-to-end blind image quality assessment using\ndeep neural networks. IEEE Transactions on Image Pro-\ncessing, 27(3): 1202–1213.\nMadhusudana, P. C.; Birkbeck, N.; Wang, Y .; Adsumilli, B.;\nand Bovik, A. C. 2022. Image quality assessment using con-\ntrastive learning. IEEE Transactions on Image Processing,\n31: 4149–4161.\nMantiuk, R. K.; Tomaszewska, A.; and Mantiuk, R. 2012.\nComparison of four subjective methods for image quality\nassessment. In Computer graphics forum, volume 31, 2478–\n2491. Wiley Online Library.\nMittal, A.; Moorthy, A. K.; and Bovik, A. C. 2012. No-\nreference image quality assessment in the spatial domain.\nIEEE Transactions on image processing, 21(12): 4695–\n4708.\nMoorthy, A. K.; and Bovik, A. C. 2010. A two-step frame-\nwork for constructing blind image quality indices.IEEE Sig-\nnal processing letters, 17(5): 513–516.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4836\nPathak, D.; Krahenbuhl, P.; Donahue, J.; Darrell, T.; and\nEfros, A. A. 2016. Context encoders: Feature learning by\ninpainting. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 2536–2544.\nPonomarenko, N.; Jin, L.; Ieremeiev, O.; Lukin, V .; Egiazar-\nian, K.; Astola, J.; V ozel, B.; Chehdi, K.; Carli, M.; Battisti,\nF.; et al. 2015. Image database TID2013: Peculiarities, re-\nsults and perspectives. Signal processing: Image communi-\ncation, 30: 57–77.\nRehman, A.; and Wang, Z. 2012. Reduced-reference image\nquality assessment by structural similarity estimation. IEEE\ntransactions on image processing, 21(8): 3378–3389.\nSaad, M. A.; Bovik, A. C.; and Charrier, C. 2012. Blind im-\nage quality assessment: A natural scene statistics approach\nin the DCT domain. IEEE transactions on Image Process-\ning, 21(8): 3339–3352.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual explana-\ntions from deep networks via gradient-based localization. In\nProceedings of the IEEE international conference on com-\nputer vision, 618–626.\nSu, S.; Yan, Q.; Zhu, Y .; Zhang, C.; Ge, X.; Sun, J.; and\nZhang, Y . 2020. Blindly assess image quality in the wild\nguided by a self-adaptive hyper network. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3667–3676.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, G.; Wang, Z.; Gu, K.; Li, L.; Xia, Z.; and Wu, L.\n2019. Blind quality metric of DIBR-synthesized images in\nthe discrete wavelet transform domain. IEEE Transactions\non Image Processing, 29: 1802–1814.\nWang, Z.; and Bovik, A. C. 2006. Modern image quality\nassessment. Synthesis Lectures on Image, Video, and Multi-\nmedia Processing, 2(1): 1–156.\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image process-\ning, 13(4): 600–612.\nWoo, S.; Park, J.; Lee, J.-Y .; and Kweon, I. S. 2018. Cbam:\nConvolutional block attention module. InProceedings of the\nEuropean conference on computer vision (ECCV), 3–19.\nXu, J.; Ye, P.; Li, Q.; Du, H.; Liu, Y .; and Doermann, D.\n2016. Blind image quality assessment based on high order\nstatistics aggregation. IEEE Transactions on Image Process-\ning, 25(9): 4444–4457.\nXue, W.; Zhang, L.; and Mou, X. 2013. Learning without\nhuman scores for blind image quality assessment. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 995–1002.\nYang, J.; Hou, C.; Zhou, Y .; Zhang, Z.; and Guo, J. 2009.\nObjective quality assessment method of stereo images. In\n2009 3DTV Conference: The True Vision-Capture, Trans-\nmission and Display of 3D Video, 1–4. IEEE.\nYang, S.; Wu, T.; Shi, S.; Lao, S.; Gong, Y .; Cao, M.; Wang,\nJ.; and Yang, Y . 2022. MANIQA: Multi-dimension Atten-\ntion Network for No-Reference Image Quality Assessment.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 1191–1200.\nYe, P.; and Doermann, D. 2012. No-reference image quality\nassessment using visual codebooks. IEEE Transactions on\nImage Processing, 21(7): 3129–3138.\nYe, P.; Kumar, J.; Kang, L.; and Doermann, D. 2012. Unsu-\npervised feature learning framework for no-reference image\nquality assessment. In 2012 IEEE conference on computer\nvision and pattern recognition, 1098–1105. IEEE.\nYing, Z.; Niu, H.; Gupta, P.; Mahajan, D.; Ghadiyaram, D.;\nand Bovik, A. 2020. From patches to pictures (PaQ-2-PiQ):\nMapping the perceptual space of picture quality. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 3575–3585.\nYou, J.; and Korhonen, J. 2021. Transformer for image qual-\nity assessment. In 2021 IEEE International Conference on\nImage Processing (ICIP), 1389–1393. IEEE.\nZeng, H.; Zhang, L.; and Bovik, A. C. 2017. A probabilistic\nquality representation approach to deep blind image quality\nprediction. arXiv preprint arXiv:1708.08190.\nZhang, L.; Zhang, L.; and Bovik, A. C. 2015. A feature-\nenriched completely blind image quality evaluator. IEEE\nTransactions on Image Processing, 24(8): 2579–2591.\nZhang, P.; Zhou, W.; Wu, L.; and Li, H. 2015. SOM: Se-\nmantic obviousness metric for image quality assessment. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2394–2402.\nZhang, W.; Ma, K.; Yan, J.; Deng, D.; and Wang, Z. 2018.\nBlind image quality assessment using a deep bilinear convo-\nlutional neural network. IEEE Transactions on Circuits and\nSystems for Video Technology, 30(1): 36–47.\nZhu, H.; Li, L.; Wu, J.; Dong, W.; and Shi, G. 2020.\nMetaIQA: Deep meta-learning for no-reference image qual-\nity assessment. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 14143–14152.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4837"
}