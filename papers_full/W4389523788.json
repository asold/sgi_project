{
    "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
    "url": "https://openalex.org/W4389523788",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2013855027",
            "name": "Cheng Jiayang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2026110207",
            "name": "Qiu Lin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282702999",
            "name": "Chan, Tsz Ho",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287069947",
            "name": "Fang, Tianqing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1980576081",
            "name": "Wang Wei-qi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5095649140",
            "name": "Chan Chun-Kit",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4228088037",
            "name": "Ru, Dongyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2203536932",
            "name": "Guo, Qipeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2318538990",
            "name": "Zhang, Hongming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2651021975",
            "name": "Song, Yangqiu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1920224989",
            "name": "ZHANG Yue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2003519119",
            "name": "Zhang Zheng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3156636935",
        "https://openalex.org/W4385572719",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2251861449",
        "https://openalex.org/W4367628242",
        "https://openalex.org/W2143283816",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2850240473",
        "https://openalex.org/W4385573692",
        "https://openalex.org/W4365512576",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W3117575631",
        "https://openalex.org/W4386841414",
        "https://openalex.org/W4387725212",
        "https://openalex.org/W4377864835",
        "https://openalex.org/W4389009551",
        "https://openalex.org/W3176909894",
        "https://openalex.org/W4367047420",
        "https://openalex.org/W2962797076",
        "https://openalex.org/W3174091926",
        "https://openalex.org/W3205668414",
        "https://openalex.org/W4283016630",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W2145454741",
        "https://openalex.org/W4365211517",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4385573282",
        "https://openalex.org/W4386510385",
        "https://openalex.org/W4387596421",
        "https://openalex.org/W4285255684",
        "https://openalex.org/W3174082608",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4241881032",
        "https://openalex.org/W1660519191",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2963983586",
        "https://openalex.org/W4229954498",
        "https://openalex.org/W3185618248",
        "https://openalex.org/W4385573468",
        "https://openalex.org/W1975879668",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4285107543",
        "https://openalex.org/W4382469053",
        "https://openalex.org/W2950824039",
        "https://openalex.org/W3160844635",
        "https://openalex.org/W4385430086",
        "https://openalex.org/W4378084339",
        "https://openalex.org/W4385570578",
        "https://openalex.org/W4376122574",
        "https://openalex.org/W3140590322",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W2460442863",
        "https://openalex.org/W4381104096",
        "https://openalex.org/W4298149550",
        "https://openalex.org/W4381567839"
    ],
    "abstract": "Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans). Furthermore, we observe that the data in StoryAnalogy can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11518‚Äì11537\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nSTORYANALOGY : Deriving Story-level Analogies from Large Language\nModels to Unlock Analogical Understanding\nCheng Jiayang‚ô†‚ô¢ Lin Qiu‚ô¢ Tsz Ho Chan‚ô† Tianqing Fang‚ô† Weiqi Wang‚ô†\nChunkit Chan‚ô† Dongyu Ru‚ô¢ Qipeng Guo‚ô¢ Hongming Zhang‚ô†\nYangqiu Song‚ô† Yue Zhang‚Ä† Zheng Zhang‚ô¢\n‚ô†The Hong Kong University of Science and Technology\n‚Ä†Westlake University ‚ô¢Amazon AWS AI\n{jchengaj, yqsong}@cse.ust.hk zhangyue@westlake.edu.cn zhaz@amazon.com\nAbstract\nAnalogy-making between narratives is crucial\nfor human reasoning. In this paper, we evaluate\nthe ability to identify and generate analogies\nby constructing a first-of-its-kind large-scale\nstory-level analogy corpus, STORYANALOGY ,\nwhich contains 24K story pairs from diverse\ndomains with human annotations on two simi-\nlarities from the extended Structure-Mapping\nTheory. We design a set of tests on STO-\nRYANALOGY , presenting the first evaluation\nof story-level analogy identification and gener-\nation. Interestingly, we find that the analogy\nidentification tasks are incredibly difficult not\nonly for sentence embedding models but also\nfor the recent large language models (LLMs)\nsuch as ChatGPT and LLaMa. ChatGPT, for\nexample, only achieved around 30% accuracy\nin multiple-choice questions (compared to over\n85% accuracy for humans). Furthermore, we\nobserve that the data in STORYANALOGY can\nimprove the quality of analogy generation in\nLLMs, where a fine-tuned FlanT5-xxl model\nachieves comparable performance to zero-shot\nChatGPT.1\n1 Introduction\nAnalogy-making plays a central role in human rea-\nsoning abilities. By drawing similarities between\nseemingly unrelated concepts (e.g., in Figure 1,\n‚Äúvirus‚Äù v.s. ‚Äúburglar‚Äù) and processes (‚Äúthe virus\ninvades cells‚Äù v.s. ‚Äúthe burglar breaks into the\nhouse‚Äù), we can infer that the virus infiltrates and\ndamages cells in a similar way to how a burglar\nbreaks into a house to steal or cause harm. These\nstory-level analogies, which involve comparing\nentire narratives or coherent sequences of events,\nenable intelligent agents to gain insights (Boden,\n2009; Ding et al., 2023; Bhavya et al., 2023) and un-\nderstand complex phenomena (Webb et al., 2022).\n1This work was done when Jiayang was an intern at\nAmazon AWS AI Lab. Code and data are released at:\nhttps://github.com/loginaway/StoryAnalogy.\nS1: The virus \nü¶†invades cells\nüß´. As a result, their DNA\nüß¨is damaged.S2: The burglar\nü¶π breaks into the house \nüè†. As a result, the valuables\nüè∫inside are smashed\nFigure 1: An example of analogy between story S1: the\ninvasion of cells by a virus, and S2: a burglar breaking\ninto a house.\nDespite its significance, there has been limited\nresearch on story analogies. One of the reasons\nis the lack of available data and evaluation bench-\nmarks. In contrast, the community has predom-\ninantly focused on word-level analogies, which\ninvolve identifying relational similarities between\npairs of concepts (e.g., king to man is like queen\nto woman) (Mikolov et al., 2013; Gladkova et al.,\n2016; Czinczoll et al., 2022).\nIn this work, we introduce STORYANALOGY , a\nlarge-scale story-level analogy corpus derived from\nvarious domains: scientific scripts, social narra-\ntives, word analogies, and knowledge graph triples,\nto facilitate the study of complex analogies. The\nstory-level analogies we examine contain richer\nrelational details, such as relations between enti-\nties (e.g., virus, invades, cells) and between events\n(e.g., the virus invades cells, as a result, the virus\ndamages DNAs).\nOne of the challenges in building\nSTORYANALOGY is establishing a clear and\nspecific way to evaluate story analogies. To\naddress this problem, we extend the Structure-\nMapping Theory (SMT; Gentner, 1983) to evaluate\non longer texts. According to SMT, analogies hold\n(e.g., the hydrogen atom vs. the Solar System )\nbecause of the similarity in relational information\n(e.g., the relative motion between objects), rather\nthan attributive information (e.g., size), between\nthe source and target. Conversely, if both types\nof information are similar, the source and target\n11518\nEn#ty/topicsimilarity\nRela#on similarity\nMere-appearance\nLiteral similarityAnalogyFood goes up from the stomach. The food enters the esophagus.Magma goes up from the inside of the planet. The magma enters volcanos.\nThe flashlight is turned on. Two contact strips touch one another.These rocks become volcanos. The volcanos erupt many times.\nMagma rises from deep in the earth. The magma goes into volcanos.\nAnomaly (dissimilarity)Source:\nFigure 2: The similarity space, showing different kinds\nof matches in terms of the degree of relation similarity\nversus entity similarity\n . According to SMT,\nwe can classify the type of matches (Analogy, Literal\nsimilarity, Anomaly, and Mere-appearance) between the\nsource and target story by the two similarities. The\nfigure is an extension with story examples based on the\nvisualization in Gentner and Markman (1997).\nexhibit a literal similarity (e.g., the X12 star system\nv.s. the Solar System). Inspired by this notion,\nwe extend SMT to the story level (¬ß 2.1). We use\nentity and relation similarity to assess the level of\nsimilarity in attributes and relations between the\nsource and target stories. Additionally, we propose\nan analogy score based on these two similarities\nto quantify the degree of analogy between stories.\nFigure 2 provides a visual representation of the\nsimilarity space spanned by the two similarities.\nWe then collect candidate story analogies for\nsimilarity annotations. Since story analogies are\nscarce in free texts2, we use large language models\n(LLMs) to generate story pairs that are likely to\nbe analogies. The stories are sourced from various\ndomains, including scientific scripts (Dalvi et al.,\n2018), social commonsense stories (Mostafazadeh\net al., 2016), word-level analogies (Turney et al.,\n2003; Czinczoll et al., 2022), and knowledge\ngraphs (Speer et al., 2017). Next, we conduct\ncrowd-sourcing to obtain similarity annotations for\neach candidate story pair. As a result, we create\nSTORYANALOGY , which consists of 24K diverse\nstory pairs, each with human annotation guided by\nthe extended SMT.\nBased on STORYANALOGY , we curate a set of\ntests to evaluate the analogy identification ability\nof models. Our findings indicate that both com-\npetitive encoder models (such as SimCSE (Gao\net al., 2021) and OpenAI‚Äôs text-embedding-002)\nand LLMs (such as ChatGPT (OpenAI, 2022) and\nLLaMa (Touvron et al., 2023)) have a significant\n2Analogies are only present in approximately 3% of a\nscientific corpus (Sultan and Shahaf, 2023), and the prevalence\nis expected to be even lower in general texts.\ngap compared to human performance in terms of\npredicting the level of analogy between stories. We\nfurther evaluate LLMs using multiple choice ques-\ntions derived from the story candidates. Even the\nbest-performing LLM still falls short of human\nperformance by 37.7%. Furthermore, we discover\nthat using stories in STORYANALOGY can enhance\nmodels‚Äô ability to identify and generate analogies.\nBy employing few-shot in-context learning and\nfinetuning on STORYANALOGY , baseline models\nachieve a considerable performance boost. For\ninstance, a fine-tuned FlanT5-xxl model exhibits\ngeneration quality on par with zero-shot ChatGPT.\nWe hope that the data and evaluation settings we\nproposed in this study will benefit the research\ncommunity in the area of story analogies.\n2 S TORYANALOGY\nConventional benchmarks in computational anal-\nogy primarily focus on word-level analogies (e.g.\nword to language is like note to music). How-\never, less attention has been given to more sophisti-\ncated analogies. We introduce STORYANALOGY ,\na dataset of 24,388 pairs of stories (e.g., ‚ÄúThe virus\ninvades cells and DNAs are damaged. ‚Äùversus ‚ÄúA\nburglar breaks into the house and smashes the valu-\nables inside. ‚Äù), each annotated with two dimen-\nsions of similarity based to SMT.\n2.1 Evaluating story analogies\nTo assess the degree of analogy between a pair\nof instances, recent studies classify story pairs us-\ning a set of labels. For instance, Sultan and Sha-\nhaf (2023) use 5 labels including not-analogy,\nself-analogy, close-analogy, far-analogy,\nand sub-analogy. Nagarajah et al. (2022) use\n6 labels: shallow attribute analogy , deep\nattribute analogy , relational analogy ,\nevent analogy , structural analogy , and\nmoral/purpose. However, they observed very\npoor agreement among annotators for most labels,\nwhich indicates a vague understanding of the task.\nMaking comparisons across these studies are chal-\nlenging due to the vastly different settings.\nIn cognitive psychology, the Structure Mapping\nTheory (SMT; Gentner, 1983) is well-known for\nits explanation of the cognitive process of mak-\ning analogies between objects. SMT evaluates ob-\nject comparisons from two perspectives: (a) the\nattributes of objects and (b) the relational struc-\ntures between objects. Analogies between objects\n11519\nSource story Target story Scores\n Domain\nThe stream becomes a river. The river continues to\nflow along the same path for a long time.\nA person grows from a child into an adult. As time\npasses, the person experiences ongoing growth and\nmaturation.\n: 0.6\n : 2.8 PP\nThey left him the key to the entrance. When Tom\nwent over he realized it was the wrong key.\nThey gave her the password to the website. When\nJane logged in, she realized it was the wrong pass-\nword.\n: 1.0\n : 2.7 ROC\nFoundations are poured to support the walls and\nroofs of buildings. The structure of the building is\nonly as strong as it‚Äôs foundation.\nReasons are formulated to make theories. The con-\nclusions of theories are only as dependable as their\ninitial premises.\n: 0.6\n : 1.8 W A\nHis memory has broken into fragmented pieces. He\ncan recall flashes and images of the past, but nothing\nconcrete or clear.\nHis memories remain a confused mess. Nothing\nholds together and what he remembers don‚Äôt make\nsense.\n: 2.7\n : 3.0 W A\nThe student opens the book and begins to read. The\nknowledge gained from the book is absorbed by the\nstudent.\nThe cat sees a mouse and begins to chase it. The\ncat honing its hunting skills through practice and\nrepetition.\n: 0.8\n : 1.4 CN\nTable 1: Examples in STORYANALOGY with annotations from each domain. We report the EntSim\n and RelSim\nfrom crowd workers\n . The Domain column indicates the source of the story pairs. ‚ÄúPP‚Äù, ‚ÄúROC‚Äù, ‚ÄúW A‚Äù, and\n‚ÄúCN‚Äù are short for ‚ÄúProPara‚Äù, ‚ÄúROCStories‚Äù, ‚ÄúWord Analogy‚Äù, and ‚ÄúConceptNet‚Äù, respectively.\noccur when they have similar relational structures\nbut dissimilar attributes (e.g., the hydrogen atom\nv.s. the Solar System). In contrast, literal similarity\noccurs when objects have both similar relational\nstructures and attributes (e.g., the X12 star system\nv.s. the Solar System).\nBased on SMT, we propose to compare stories\nby their entity and relation similarity. These mea-\nsures assess the degree of similarity in terms of at-\ntributive and relational structures, respectively. We\nprovide necessary extensions to their definitions:\nEntity similarity (EntSim\n ). The similarity\nof entity and topics discussed between a pair of\nstories, ranging from 0 (unrelated) to 3 (almost\nequivalent). This score should be high if the two\nstories are both discussing apples and pears, even\nif they differ greatly in the details.\nRelation similarity (RelSim\n ). The similarity\nof relational structures between a pair of stories,\nranging from 0 (very poor alignment) to 3 (align-\nment). In this context, the relational structures\nrefer to the connections between elements at dif-\nferent levels. For instance, first-order relations can\nbe regarded as the relationship between entities,\nsuch as predicates. Second-order relations, on the\nother hand, represent connections between higher\ngranularity elements, such as the logical connec-\ntion between events or sentences. We encourage\nannotators to also consider higher-order relational\nsimilarity, such as the moral or purpose behind the\nstories.\nWe present the established similarity space with\nexample source and target stories in Figure 2.\nModeling the analogy score(Œ±). We discuss pos-\nsible definitions of theanalogy score(Œ±). The score\nŒ±should be proportional to the level of analogy be-\ntween a pair of stories. Defining Œ±to be equivalent\nwith RelSim has been adopted in word analogy\n(Ushio et al., 2021a). However, this definition can-\nnot distinguish analogy from literal similarity, as\nboth of them have high RelSim (Figure 2). We\ncan alleviate this problem by introducing EntSim\nto the definition of Œ±: according to SMT, anal-\nogy happens when the RelSim between the source\nand target story is high and the EntSim is low3.\nTherefore, in the rest of this paper, we define Œ±as\nRelSim/EntSim4.\n2.2 Distilling story analogies from LLMs\nObtaining a large number of story analogy by re-\ntrieval is difficult. Evidence from Sultan and Sha-\nhaf (2023) shows that the prevalence of analogies\nwithin a categorized dataset is around 3%. It is\nexpected that the ratio is much lower in general cor-\npora. Identifying analogies by retrieving from gen-\neral corpora would thus require huge human efforts,\nmaking it unrealistic to build a large-scale story\n3‚ÄúAn analogy is a comparison in which relational predi-\ncates, but few or no object attributes, can be mapped from\nbase to target.‚Äù (Gentner, 1983)\n4In practice, we compute it by RelSim/(1+EntSim) to\nensure numerical stability.\n11520\nanalogy collection in this way. Recently observa-\ntions suggest that LLMs are capable of understand-\ning and predicting analogies for problem-solving\n(Webb et al., 2022), cross-domain creativities (Ding\net al., 2023), and generating explanations for word\nanalogies (Bhavya et al., 2022). In addition to these\nfindings, we discover that LLMs can generate high-\nquality story analogies (i.e., with more than a half\ngenerations being analogies). Here, we introduce\nthe pipeline for generating story analogies. The\ngenerated analogies are further annotated by crowd\nannotators for verification. (Details are in ¬ß A.1.)\nCurating seed examples. The first step is to cu-\nrate a seed set of story analogies. We ask experts\nfrom our team to write story analogies. To ensure\ndiversity, the experts are required to consider mul-\ntiple domains and are allowed to search in corpora\nor on the Internet. They then determine whether\nthese story pairs are indeed analogies Examples\nthat are not considered analogies are removed from\nthe gold set. As a result, we obtained a total of 28\nstory analogy examples, each containing a pair of\nstories and the corresponding entities.\nSource data. To guarantee the coverage of topics,\nwe sample from corpora of four domains to gener-\nate stories, including (1) scientific scripts ProPara\n(Dalvi et al., 2018), (2) social commonsense stories\nROCStories (Mostafazadeh et al., 2016), (3) word\nanalogy evaluation sets5 SAT (Turney et al., 2003),\nU2 and U46, and SCAN (Czinczoll et al., 2022). ,\nand (4) the commonsense KG ConceptNet7 (Speer\net al., 2017). Note that, source data (1) and (2)\nconsist of stories, while (3) and (4) consist of word\npairs.\nGenerating story candidates. Using the seed ex-\namples and source data, we prompt LLMs8 to gen-\nerate analogies. Due to the different formats of the\nsource data, the story pairs are generated using two\ndifferent paradigms (Details are in ¬ß A.1.):\nGenerating from story pairs. Given a source story\nand several source-target story pairs sampled from\nthe seed examples, we prompt an LLM to generate\n5After manual inspection of all word analogy datasets, we\ndo not include classic datasets such as Google (Mikolov et al.,\n2013), where the relations between words are relatively easier\nsyntactic or shallow semantic relations, such as (‚Äúsimilar:\nsimilarly‚Äù, ‚Äúrare: rarely‚Äù).\n6https://englishforeveryone.org/Topics/\nAnalogies.html\n7We consider entity pairs in triples that share the same\nrelations from ConceptNet. https://huggingface.co/\ndatasets/relbert/analogy_questions\n8OpenAI‚Äôs text-davinci-003 is used in the generation.\nthe target story.\nGenerating from word pairs. Given a word analogy\npair (e.g. ‚Äúword‚Äù, ‚Äúlanguage‚Äù and ‚Äúnote‚Äù, ‚Äúmu-\nsic‚Äù), together with source-target analogies with\nthe corresponding entities from seed examples, an\nLLM is prompted to generate both the source and\ntarget stories.\n2.3 Annotation\nTo evaluate each candidate story pair under the ex-\ntended SMT, we conduct crowd annotations on\nAmazon Mechanical Turk 9. We recruit crowd\nworkers to annotate the entity and relation similari-\nties for the collected pairs. In addition, workers are\nrequired to label an instance as ‚Äúpoor quality‚Äù if\nthey find the generated content broken or toxic. The\nannotation consists of the following two rounds:\n(i) Qualification round. We first annotate 80 can-\ndidate story pairs (20 from each domain) to curate\na qualification set. Three domain experts from our\nteam are asked to read through the annotation in-\nstruction and independently annotate EntSim and\nRelSimfor these pairs. The Spearman‚Äôs œÅbetween\neach annotator‚Äôs prediction with the average scores\nof the others ranges from 93% to 96% on EntSim,\nand from 89% to 95% on RelSim.\nWe invite crowd workers who have ‚â•90% his-\ntory approval rates and have ‚â•1K HITs approved\nto attend the qualification. Workers whose predic-\ntions achieve ‚â•70% Spearman‚Äôs œÅwith the average\nscores from three experts pass the qualification. As\na result, 158 and 80 workers passed the qualifica-\ntion for EntSimand RelSim, respectively.\n(ii) Main round. Qualified crowd workers are in-\nvited to attend the main round annotations. We\nassign 5 different annotators to give predictions\nfor each similarity of a story pair. To guarantee\nthe annotation quality, we follow the annotation\nsetting in (Agirre et al., 2012). We split the main\nround into multiple mini-rounds, each with 1K-2K\ncandidate pairs. After each mini-round, we filter\nout and disqualify workers who do not show sig-\nnificant correlations with the average scores of the\nothers. They are paid more than what is required\nby the local wage law. In addition, experts from\nour team manually check the quality of annotations\nand write feedback to workers correspondingly.\nThe generated contents sometimes contain hallu-\ncinations or toxic contents. We filter out story pairs\nlabeled as ‚Äúpoor quality‚Äù by more than 10% an-\n9https://www.mturk.com/\n11521\nEntSimRelSim\n0 1 2 3\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0Density\nSource:\nConceptNet\nProPara\nROCStories\nWordAnalogy\nFigure 3: Distributions of EntSim and RelSim on four\ndata domains in STORYANALOGY . Notably, the distri-\nbutions of EntSim and RelSim on ROCStories tend to\nskew towards higher values. This could be attributed to\nthe fact that stories from this source primarily revolve\nhuman-focused social narratives.\nnotators, which accounts for 142 instances. For\neach story pair, we adopt the average scores from\nworkers as the predicted EntSim and RelSim.\n2.4 Analysis of S TORYANALOGY\nTo assess inter-annotator agreement, we randomly\nsampled 1K instances with 3 independent annota-\ntions from our dataset. The Fleiss‚Äôs kappa (Fleiss,\n1971) on the binarized annotations of EntSim are\n47%, and 42% on RelSim, indicating moderate\nagreement among annotators. In addition, we ad-\nditionally obtained expert annotations on 200 ran-\ndomly sampled instances. The averaged Spear-\nman‚Äôs correlation between crowd and expert anno-\ntations on EntSimand RelSimis 64.7% and 69.9%,\nrespectively.\nThe final dataset consists of 24,388 story pairs on\nfour domains: ProPara (6.9K), ROCStories (4.9K),\nWord-Analogy (7.5K), and ConceptNet (5.0K). Sto-\nries in STORYANALOGY have 19.94 tokens on av-\nerage. The distributions of EntSimand RelSimare\npresented in Figure 3. We randomly select 500\ninstances from each domain as the test set, and\nanother 500 instances from each domain as the\nvalidation set. Examples of STORYANALOGY are\nshown in Table 1.\n3 Story Analogy Identification\nWe begin by assessing the ability of models toiden-\ntify story analogies using two different setups. The\nfirst evaluation setup is similar to Semantic Tex-\ntual Similarity (STS) tasks (Agirre et al., 2012),\nwhere we calculate the Spearman‚Äôs correlation be-\ntween models‚Äô predicted similarity and theanalogy\nscores (Œ±) derived from annotations (¬ß 3.1). For\nthe second evaluation, we reframe our dataset as\nmultiple-choice questions and evaluate LLMs on\nthis set (¬ß 3.2).\n3.1 Correlation with the analogy scoreŒ±\nSimilar to the STS-style evaluation (Agirre et al.,\n2012), we assess whether models can predict\nanalogy scores based on embeddings (for encoder\nmodels) or by generation (for LLMs). We use\na model to predict the similarity f(‚ãÖ,‚ãÖ) for\ntwo stories. For encoder models, f(s1,s2) =\nCosine(Encoder(s1), Encoder(s2)). For\nLLMs, we prompt them to predict the EntSimand\nRelSim for the two stories. Finally, Spearman‚Äôs\ncorrelations between the predicted similarity and\nthe respective scores are reported.\nSetups. We consider both encoder models and\nLLMs as baselines. Details are in ¬ß A.2.\nThe encoder models we evaluate include\nRoBERTa (Liu et al., 2019), SimCSE (Gao et al.,\n2021), OpenAI-ada (text-embedding-ada-002),\nDiscourse Marker Representation (DMR) (Ru et al.,\n2023), RelBERT (Ushio et al., 2021b), and GloVe\nembeddings (Pennington et al., 2014) on nouns,\nverbs, or all words 10. In addition to the unsu-\npervised encoder models, we also fine-tune two\nmodels on the training set: a regression model,\nRoBERTa-Reg, which has a multilayer percep-\ntron on top of the RoBERTa model that predicts\nEntSim and RelSim, and a contrastive learning-\nbased model, RoBERTa-CL, which uses a con-\ntrastive learning objective to optimize its represen-\ntations.\nFor LLMs, we test FlanT5 (Chung et al., 2022),\nLLaMa (Touvron et al., 2023), ChatGPT (OpenAI,\n2022), and GPT-3.5 (text-davinci-003). Each\nmodel input is composed of three parts: the in-\nstructions, which give explanations to the similar-\nity scores; N examples, and the query story pair.\nWe evaluate models with two instructions (short\nand long, where short instructions only contain the\nlabels, and long instructions additionally have label\ndefinitions), and N is set to 0, 1, or 3.\nResults. The overall evaluation results are pre-\nsented in Table 2. Generally, the models perform\nrelatively poorly on the analogy score Œ±, indicat-\ning that there is still room for improvement on\nSTORYANALOGY .\n10We use Stanza (Qi et al., 2020) to conduct part-of-speech\ntagging for words.\n11522\nProPara ROCStories Word-Analogy ConceptNet Mean\nE R Œ± E R Œ± E R Œ± E R Œ± E R Œ±\nRandom 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nHuman 54.9 64.6 67.9 82.9 70.1 55.2 67.1 69.4 58.3 53.7 75.5 68.5 64.7 69.9 62.5\nEncoder models\nRoBERTa 45.2 41.9 6.2 20.6 22.2 7.6 34.8 24.5 0.9 34.9 28.8 9.8 33.9 29.4 6.1\nSimCSE 48.0 38.4 1.8 14.4 12.7 2.6 43.2 26.8 -2.0 30.7 21.2 3.7 34.1 24.8 1.5\nOpenAI-ada 52.8 43.9 3.4 22.3 21.7 4.5 41.3 24.0 -3.8 32.3 17.8 -1.2 37.2 26.9 0.7\nDMR 34.8 42.0 12.6 20.1 35.0 20.1 17.3 18.7 7.3 21.9 19.1 5.5 23.5 28.7 11.4\nRelBERT 37.9 38.8 7.5 15.6 20.6 9.1 28.6 15.5 -3.6 26.6 24.7 8.2 27.2 24.9 5.3\nGloVe-Noun 35.2 18.5 -7.8 9.4 6.9 2.5 29.8 14.2 -5.6 27.7 13.0 -2.2 25.5 13.2 -3.3\nGloVe-Verb 27.3 44.8 21.3 22.7 34.2 17.3 9.6 7.0 1.3 13.0 1.0 -7.2 18.1 21.7 8.2\nGloVe-All 36.3 29.0 -1.0 28.7 27.2 4.8 26.1 12.3 -5.1 18.6 3.3 -7.9 27.4 18.0 -2.3\nLLMs\nFlanT5-xxl 41.9 21.8 4.8 9.4 -8.7 -2.4 40.0 27.5 8.3 37.7 26.7 8.1 32.3 16.8 4.7\nLLaMa-65B 16.8 3.8 -1.3 0.4 -10.2 -7.7 31.6 25.3 9.5 13.8 22.1 4.2 15.6 10.3 1.2\nGPT-3.5 24.1 11.4 -6.4 -3.2 -2.8 -6.4 34.2 26.9 8.6 28.8 30.0 7.9 21.0 16.4 0.9\nChatGPT 26.9 11.9 -2.3 1.7 -4.4 -5.0 46.6 31.4 3.4 32.1 36.4 13.1 26.8 18.8 2.3\nFinetuned models\nRoBERTa-Reg 38.5 34.8 16.6 14.5 26.2 12.0 20.1 28.8 19.4 23.8 32.0 20.1 24.2 30.5 17.0\nRoBERTa-CL 25.7 53.9 35.2 29.1 47.2 28.3 33.8 40.9 21.1 26.0 30.8 15.3 28.7 43.2 25.0\nTable 2: STS-style evaluation on different domains of STORYANALOGY . The values represent the Spearman‚Äôs\ncorrelation (%) between the model prediction and scores from dataset (E, R, and Œ±). Here, E, R, and Œ±correspond\nto EntSim, RelSim, and the analogy score RelSim/EntSim, respectively. The LLM performance is evaluated under\nthe ‚Äúlong instruction+3-shot‚Äù setting.\n30.131.732.3\n‚àí0.7\n16.815.6\n27.725.821.0\n35.731.526.8\n18.019.016.8\n 3.7\n17.1\n10.3 10.512.716.4\n21.520.718.8\n 2.6 4.4 4.7 0.5\n 6.3\n 1.2 ‚àí0.8 0.6 0.9 3.6 5.3 2.3\n31.931.530.5\n‚àí1.7\n16.213.1\n26.525.0\n17.7\n33.031.428.0\n16.917.014.6\n‚àí4.8\n13.1\n 8.3 12.510.915.3\n21.320.318.2\n 4.7 3.5 1.6 0.4 4.0‚àí0.6 ‚àí0.9‚àí2.6‚àí1.9\n 6.9 6.8\n‚àí0.4\nLong Short\nERR/E\nFlanT5‚àíxxlLLaMa‚àí65BChatGPT GPT‚àí3.5 FlanT5‚àíxxlLLaMa‚àí65BChatGPT GPT‚àí3.5\n‚àí10\n0\n10\n20\n30\n40\n‚àí10\n0\n10\n20\n30\n40\n‚àí10\n0\n10\n20\n30\n40Spearman's correlation (%)\nN: 0 1 3\nFigure 4: Spearman‚Äôs œÅ(%) of LLMs, averaged across\ndata domains. Here E, R, and R/E indicate EntSim,\nRelSim, and RelSim/EntSim (Œ±).\nWe have the following observations: (1) Simi-\nlarities from state-of-the-art sentence embedding\nmodels are not good indicators for story analogy.\nEncoders such as RoBERTa, SimCSE, and OpenAI-\nada show relatively good correlation with EntSim\nand RelSim, but they perform poorly on the anal-\nogy score Œ±. This suggests that their embeddings\nare suitable for literal similarity retrieval but not\nanalogy retrieval. (2) Relational feature-aware\nmodels are better at analogy identification. Ad-\nditionally, we find that encoder models aware of\nrelation information, such as DMR (discourse rela-\ntion), RelBERT (inter-word relation), and GloVe-\nVerb (predicates), correlate better with the anal-\nogy score Œ±. (3) Finetuning improves models‚Äô\nanalogy identification ability. The finetuned mod-\nels, RoBERTa-Reg and RoBERTa-CL, are the top-\nperforming models that significantly outperform all\nthe other baselines on Œ±. (4) Generally, LLMs do\nnot perform well on the analogy scoreŒ±. As shown\nin Figure 4, most LLMs can benefit from longer\ninstructions as the extra definitions help in under-\nstanding the scores. Moreover, we find that despite\nits size, FlanT5-xxl is one of the best-performing\nLLMs in terms of predicting EntSim and RelSim.\n3.2 Multiple choice evaluation\nWe construct a multiple-choice evaluation set using\nthe annotated story pairs. First, we gather story\npairs with EntSim < 1.0 and RelSim > 2.0. For\neach target story, we choose 3 negative choices\nto form the candidates. Out of these, two (easy)\nnegative choices are randomly selected, while one\n(hard) negative example is chosen by retrieving sto-\nries with high nounal similarity (measured by the\ncosine similarity of the nounal GloVe embeddings)\nand < 50% token overlap. An example question is\nprovided in Table 3. To assess human performance,\nwe conduct human annotations.\nWe assess LLMs on multiple-choice questions.\nEach model input consists of an instruction, N ex-\namples of multiple-choice questions, and the query\n11523\nQuestion:Which candidate story is the best creative analogy for the\nsource story?\nSource:Carbonic acid in rainwater breaks down rock. Plants grow\nin rock.\n(0) Plants and animals grow and reproduce. The population\nsize gets larger and larger.\n(1) Recyclables are placed in a centralized container for the\nhouse. Recyclables are picked up by a recycling company.\n(2) Salty ocean water erodes metal. Corals thrive on metal.\n(3) The roots of the growing plants start to break up the rock.\nThe plant acids dissolve the rock.\nAnswer: (2)\nTable 3: An example of the multiple choice question.\nThe goal is to select a candidate story that is the best\nanalogy for the source story.\nModel N-shot0 1 3\nFlanT5-xxl 45.0 46.3 45.0LLaMa-65B 27.7 28.6 29.5ChatGPT 35.8 29.2 32.3GPT-3.5 44.2 34.1 33.1\nModel Question templateA B C\nFlanT5-xxl 41.2 47.1 48.0LLaMa-65B 29.6 26.0 30.2ChatGPT 30.1 33.3 33.9GPT-3.5 33.7 33.6 44.0\nTable 4: Multiple choice evaluation results. Each value\nrepresents the average accuracy (%) across different\nnumber of demonstrations (left) or across three question\ntemplates (right). The random and human performance\nare 25% and 85.7%, respectively.\nmultiple-choice question. We evaluate the models\nusing three different instructions, such as ‚ÄúWhich\ncandidate story is the best creative\nanalogy for the source story? ‚Äù, where N\ncan be 0, 1, or 3. As a baseline, we obtain the per-\nformance of the analogy retrieval model in (Sultan\nand Shahaf, 2023) on our multiple choice questions,\nwhich achieves an accuracy of 44.9%.\nResults. The results are presented in Table 4. Inter-\nestingly, while annotators can answer the questions\ncorrectly at an accuracy of 85.7%, LLMs struggle\non selecting the most analogous story (the averaged\naccuracy for text-davinci-003is merely 37.1%).\nIncreasing the number of demonstrations does not\nshow consistent benefits to model prediction. Also,\nwe find that explicitly instructing models to choose\nthe ‚Äúcreative analogy‚Äù (¬ß A.3, question template B)\nor to provide a definition of SMT when explaining\nanalogies (template C) yields better performance\ncompared to simply asking models to select the\nbest analogy (template A).\nWe present the breakdown ratio of the percent-\nage of types of choices selected in Table 5. We\nhave the following observations: (1) LLMs can can\ndifferentiate between randomly sampled easy nega-\ntives and other choices. The proportion of easy neg-\nTarget Hard Easy\nRandom 25.0 25.0 50.0\n(Sultan and Shahaf, 2023) 44.9 17.8 37.2\nFlanT5-xxl 45.4 37.2 17.4\nLLaMa-65B 28.6 59.7 11.7\nChatGPT 32.4 59.5 8.1\nGPT-3.5 37.1 55.8 7.1\nTable 5: Breakdown ratio (%) of model predictions:\nThe table presents the percentage of different types of\nchoices selected. ‚ÄúTarget‚Äù refers to the ground-truth\ntarget, which is the analogous story. ‚ÄúHard‚Äù and ‚ÄúEasy‚Äù\nrefer to the negative examples sampled using nounal\nsimilarity and random sampling, respectively.\natives they select is less than 20%, whereas random\nchance would be 50%. Furthermore, more power-\nful LLMs like GPT-3.5 are better at this judgement\ncompared to LLaMa and FlanT5-xxl. (2) LLMs\ncan be easily distracted by hard negatives, as they\noften have a similar or higher chance of selecting\nhard negative choices instead of the targets. This\nsuggests that the models prioritize surface similar-\nity over structural similarity, despite the latter being\nmore important in identifying analogies.11 (3) In\ncomparison, the baseline model from (Sultan and\nShahaf, 2023) is more resilient against hard nega-\ntive distractions. This is likely due to its framework\ndesign, which captures the structural similarity be-\ntween stories by clustering entities and finding the\nmappings between clusters.\n4 Story Analogy Generation\nWe examine whether the dataset STORYANALOGY\ncan enhance the ability of analogy generation. We\nevaluate FlanT5 (Chung et al., 2022), LLaMa (Tou-\nvron et al., 2023), ChatGPT (OpenAI, 2022), and\nGPT-3.5 in zero-shot and few-shot settings using 40\nsource stories from the test set. To explore the po-\ntential of smaller models in generating high-quality\nanalogies, we fine-tuned FlanT5-xl (3B parame-\nters) and FlanT5-xxl (11B parameters) using the\nsame template.\nA crowd annotation is conducted to evaluate the\nquality of the generated stories from the models\nmentioned above. Workers are provided with a\nsource story and its corresponding generated target\nstory. They are then asked to assess the following:\n(1) Whether the target story is an analogy for the\n11This phenomenon was also observed in visual analogies\n(Bitton et al., 2023), where they found that that models can\nsolve visual analogies well when the distractors are random,\nbut struggle with difficult distractors.\n11524\nSetting Model Generation quality\nAnalogy Novelty Plausibility\nZero\nFlanT5-xl 52.5 48.3 92.5\nFlanT5-xxl 46.7 49.2 92.5\nLLaMa-65B 38.3 39.2 93.3\nChatGPT 70.0 72.5 90.8\nGPT-3.5 75.8 81.7 87.5\nFew\nFlanT5-xl 48.3 50.0 91.7\nFlanT5-xxl 40.0 43.3 85.0\nLLaMa-65B 66.7 66.7 92.5\nChatGPT 78.3 83.3 86.7\nGPT-3.5 77.5 79.2 88.3\nTuned FlanT5-xl 65.8 79.2 88.3\nFlanT5-xxl 72.5 81.7 86.7\nTable 6: The crowd-annotated generation quality (%) in\nterms of (1) Whether the target story is considered an\nanalogy to the source; (2) Novelty of the target story;\n(3) Plausibility of the generations.\nsource (as opposed to being a literal similarity or\nsomething else); (2) whether the target story is\nnovel compared to the source; and (3) whether the\ntarget is plausible (More details can be found in\n¬ß A.4). The average scores from three annotators\nare reported in Table 6. Example generations are\nshown in Table 7.\nUnder the zero-shot setting, we observe that\nFlanT5 and LLaMa struggle to generate meaning-\nful analogies. They often tend to repeat patterns\nfrom the source stories (e.g., only replacing one\nword). In contrast, ChatGPT and GPT-3.5 produce\nmore flexible stories that are frequently considered\nas analogies and novel.\nStories in STORYANALOGY can help models\ngenerate better analogies. With a few demon-\nstrations, we observe a significant improvement\nin the generation quality of LLaMa (+28.4% and\n+27.5%). Moderate improvement on ChatGPT\nand GPT-3.5 is also observed. Notably, finetun-\ning smaller LMs enhanced their generation quality.\nThe finetuned FlanT5-xxl model performs better\nthan the zero-shot ChatGPT and is comparable to\nthe few-shot ChatGPT and GPT-3.5, despite hav-\ning fewer parameters. Furthermore, while mod-\nels become more creative through finetuning and\nin-context learning, their generation plausibility\ndecreases, indicating an increase in hallucination.\n5 Related Work\nWord-level analogy. One of the famous works on\nword-level computational analogy was (Mikolov\net al., 2013), where they found that word analogies\ncan be predicted by word vector offsets. For in-\nstance,\n‚Üí\nKing ‚àí\n‚Üí\nMan +\n‚Üí\nWoman‚âà\n‚Üí\nQueen. With\nSource A projectile is affected by gravity. It falls and picks up\nspeed.\nZero-shot\nChatGPT\nJust as a sled sliding down a steep hill gains momentum as\nit accelerates, so does a projectile as it falls under the force\nof gravity.\nGPT-3.5 An Olympic runner is running a middle distance race.\nLLaMa-65BA projectile is affected by gravity. It falls and picks up\nspeed.\nFlanT5-xlA rocket is affected by gravity. It falls and picks up speed.\nFlanT5-xxlA car is affected by gravity. It picks up speed and goes\nfaster.\nFew-shot\nChatGPT A seed is planted in fertile soil. It takes root and grows,\nflourishing into a healthy plant.\nGPT-3.5 An alpha particle is buffeted by the atmosphere. It spirals\nand eventually finds the ground.\nLLaMa-65BA projectile is affected by gravity. It falls and picks up\nspeed.\nFlanT5-xlA rocket is affected by gravity. It falls and picks up speed.\nFlanT5-xxlA car is affected by gravity. It accelerates and picks up\nspeed.\nTuned\nFlanT5-xlA meteor is propelled by a tug. It moves and finds its way.\nFlanT5-xxlAn idea is planted by brainstorm. It takes shape and comes\nto fruition.\nTable 7: Examples showing the source story and model\ngenerations under zero-shot, few-shot, and finetuning\nsettings.\nthe development of pretrained language models\n(PLMs) such as BERT (Devlin et al., 2018), there\nhave been works utilizing PLMs to solve word\nanalogies by LM perplexity (Ushio et al., 2021a),\npretrain relational embedding on certain prompt\ntemplates (Ushio et al., 2021b), or use word analo-\ngies as latent restriction to implicitly probe rela-\ntional knowledge (Rezaee and Camacho-Collados,\n2022).\nIn this line of work, a typical evaluation setting\nis ranking word pairs based on their relational sim-\nilarity with the source pair (Mikolov et al., 2013;\nCzinczoll et al., 2022). For instance, given a word\npair A:B, the aim is to select a target pair C:D such\nthat the relation between C and D is the most simi-\nlar to A:B among all candidates. This is similar to\nour multiple-choice evaluation setting.\nIn comparison, only a handful of research has\nbeen done in sentence or paragraph-level analogy:\nAnalogous text retrieval. Built on the famous\nstructure mapping theory, SME (Falkenhainer et al.,\n1989) and LRME (Turney, 2008) model the anal-\nogy retrieval problem as an entity-mapping prob-\nlem. They then solve this problem through web\nmining. Sultan and Shahaf (2023) develops a QA-\nSRL based analogy retrieval method to conduct\n11525\nentity mapping. However, these works evaluate\ntheir methods by annotating the precision of the\ntop-ranked results, leaving no large-scale analogy\nevaluation benchmarks to date.\nAnalogy generation. Recently, there have been\nattempts at pretraining or prompting LMs for anal-\nogy generation. Bhavya et al. (2022); Webb et al.\n(2022) evaluated LLMs‚Äô ability on solving word\nanalogy tasks, where they found that large language\nmodels such as GPT-3.5 can surpass human perfor-\nmance on certain word analogy tasks. Ding et al.\n(2023) evaluated LLMs‚Äô creativity in terms of cross-\ndomain analogies. Bhavya et al. (2022); Chen et al.\n(2022a) evaluated LMs‚Äô ability on generating ex-\nplanations for word analogies. Bhavya et al. (2023)\nproposed a novel analogy mining framework based\non generation.\nAnalogy benchmarks. There are many word-level\nanalogy datasets. Google (Mikolov et al., 2013),\nBATS (Gladkova et al., 2016) contain relatively\neasier syntactic or shallow semantic relations. In\ncontrast, U2 and U4, and Czinczoll et al. (2022)\ninclude examples with relatively more abstract re-\nlations. To the best of our knowledge, there is no\nlarge-scale story-level analogy data or resources as\nof the time of writing. The only related works here\nare (Li and Zhao, 2021; Zhu and de Melo, 2020),\nwhich transform word analogy pairs into sentence\npairs with a few templates. Nagarajah et al. (2022)\ntried to annotate a tiny scale story analogy bench-\nmark based on fables, but they failed to achieve.\nWijesiriwardene et al. (2023) re-organized sentence\nrelation datasets, where they viewed such relations\n(e.g., entailment, negation) are analogy, which is\nfundamentally different from our settings.\nAnalogy in other domains. In addition to analo-\ngies on word pairs and stories, there have been\nrelated studies on other topics. Hope et al. (2017)\ncontribute a method for analogy mining over prod-\nucts. Chan et al. (2018) mine analogies from re-\nsearch papers with respect to their background, pur-\npose, mechanism, and findings. Gilon et al. (2018)\ndevelop a search engine for expressing and abstract-\ning specific design needs. Recently, Bitton et al.\n(2023) propose a visual analogies dataset, V ASR,\nwhere they found that models struggle to find out\nanalogies when given carefully chosen distractors.\n6 Conclusion\nWe introduce STORYANALOGY , a multi-domain\nstory-level analogy corpus with 24K story analo-\ngies pairs annotated on two similarities under the\nextended SMT. To assess the analogy identifica-\ntion and generation capabilities of various mod-\nels, we have devised a series of tests based on\nSTORYANALOGY . The experimental findings indi-\ncate that current encoder models and LLMs still fall\nshort of human performance in analogy identifica-\ntion. Additionally, we demonstrate that generative\nmodels can greatly benefit from our dataset.\nLimitations\nWe attempted to ensure dataset coverage by uti-\nlizing seed data from various sources. However,\nthere are still specific domains that we were un-\nable to include, such as biomedical stories or aca-\ndemic articles. We can extend the annotation to\nthese domains using the annotation framework and\nevaluation metrics mentioned in this paper. Ad-\nditionally, we have explored applications such as\nanalogy identification (Section 3) and generation\n(Section 4). The potential of STORYANALOGY to\nbe applied for creativity generation tasks (such as\npoetry, lyrics and humor generation) has not been\nfully investigated. Further development on other\nsources and applications is left as future work.\nEthics Statement\nThe generated knowledge in STORYANALOGY\nhave been carefully evaluated by crowd annota-\ntors to remove any possible toxic or counterfactual\ncontent. We set the threshold to be as low as 10%,\nsuch that any annotator‚Äôs labeling an instance as\ntoxic will lead to its removal. 142 instances are\nremoved during this process. We conformed to rec-\nognized privacy practices and rigorously followed\nthe data usage policy. We declare that all authors\nof this paper acknowledge the ACM Code of Ethics\nand honor the code of conduct.\nAcknowledgements\nThe authors of this paper were supported by the\nNSFC Fund (U20B2053) from the NSFC of China,\nthe RIF (R6020-19 and R6021-20) and the GRF\n(16211520 and 16205322) from RGC of Hong\nKong. We also thank the support from the UGC\nResearch Matching Grants (RMGS20EG01-D,\nRMGS20CR11, RMGS20CR12, RMGS20EG19,\nRMGS20EG21, RMGS23CR05, RMGS23EG08).\n11526\nReferences\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012. Semeval-2012 task 6: A\npilot on semantic textual similarity. In * SEM 2012:\nThe First Joint Conference on Lexical and Compu-\ntational Semantics‚ÄìVolume 1: Proceedings of the\nmain conference and the shared task, and Volume\n2: Proceedings of the Sixth International Workshop\non Semantic Evaluation (SemEval 2012), pages 385‚Äì\n393.\nBhavya Bhavya, Jinjun Xiong, and Chengxiang Zhai.\n2022. Analogy generation by prompting large lan-\nguage models: A case study of instructgpt. arXiv\npreprint arXiv:2210.04186.\nBhavya Bhavya, Jinjun Xiong, and Chengxiang Zhai.\n2023. Cam: A large language model-based creative\nanalogy mining framework. In Proceedings of the\nACM Web Conference 2023, pages 3903‚Äì3914.\nYonatan Bitton, Ron Yosef, Eliyahu Strugo, Dafna Sha-\nhaf, Roy Schwartz, and Gabriel Stanovsky. 2023.\nVasr: Visual analogies of situation recognition. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 37, pages 241‚Äì249.\nMargaret A Boden. 2009. Computer models of creativ-\nity. AI Magazine, 30(3):23‚Äì23.\nChunkit Chan and Tsz Ho Chan. 2023. Discourse-aware\nprompt for argument impact classification. In Pro-\nceedings of the 15th International Conference on\nMachine Learning and Computing, ICMLC 2023,\nZhuhai, China, February 17-20, 2023 , pages 165‚Äì\n171. ACM.\nChunkit Chan, Jiayang Cheng, Weiqi Wang, Yuxin\nJiang, Tianqing Fang, Xin Liu, and Yangqiu Song.\n2023a. Chatgpt evaluation on sentence level rela-\ntions: A focus on temporal, causal, and discourse\nrelations. CoRR, abs/2304.14827.\nChunkit Chan, Xin Liu, Tsz Ho Chan, Jiayang\nCheng, Yangqiu Song, Ginny Y . Wong, and Si-\nmon See. 2023b. Self-consistent narrative prompts\non abductive natural language inference. CoRR,\nabs/2309.08303.\nChunkit Chan, Xin Liu, Jiayang Cheng, Zihan Li,\nYangqiu Song, Ginny Y . Wong, and Simon See.\n2023c. Discoprompt: Path prediction prompt tun-\ning for implicit discourse relation recognition. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 35‚Äì57. Association for Computational\nLinguistics.\nJoel Chan, Joseph Chee Chang, Tom Hope, Dafna Sha-\nhaf, and Aniket Kittur. 2018. Solvent: A mixed initia-\ntive system for finding analogies between research pa-\npers. Proceedings of the ACM on Human-Computer\nInteraction, 2(CSCW):1‚Äì21.\nJiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao\nLi, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua\nXiao, and Hao Zhou. 2022a. E-kar: A benchmark for\nrationalizing natural language analogical reasoning.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 3941‚Äì3955.\nYi Chen, Jiayang Cheng, Haiyun Jiang, Lemao Liu,\nHaisong Zhang, Shuming Shi, and Ruifeng Xu.\n2022b. Learning from sibling mentions with scal-\nable graph inference in fine-grained entity typing. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2076‚Äì2087.\nJiayang Cheng, Haiyun Jiang, Deqing Yang, and\nYanghua Xiao. 2021. A question-answering based\nframework for relation extraction validation. CoRR,\nabs/2104.02934.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nLi Cui, Deqing Yang, Jiayang Cheng, and Yanghua Xiao.\n2021a. Incorporating syntactic information into rela-\ntion representations for enhanced relation extraction.\nIn Pacific-Asia Conference on Knowledge Discovery\nand Data Mining, pages 416‚Äì428. Springer.\nLi Cui, Deqing Yang, Jiaxin Yu, Chengwei Hu, Jiayang\nCheng, Jingjie Yi, and Yanghua Xiao. 2021b. Refin-\ning sample embeddings with relation prototypes to\nenhance continual relation extraction. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 232‚Äì243.\nTamara Czinczoll, Helen Yannakoudakis, Pushkar\nMishra, and Ekaterina Shutova. 2022. Scientific\nand creative analogies in pretrained language models.\narXiv preprint arXiv:2211.15268.\nBhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau\nYih, and Peter Clark. 2018. Tracking state changes\nin procedural text: a challenge dataset and models\nfor process paragraph comprehension. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1595‚Äì1604.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\n11527\nZijian Ding, Arvind Srinivasan, Stephen MacNeil, and\nJoel Chan. 2023. Fluid transformers and creative\nanalogies: Exploring large language models‚Äô capacity\nfor augmenting cross-domain analogical creativity.\narXiv preprint arXiv:2302.12832.\nBrian Falkenhainer, Kenneth D Forbus, and Dedre Gen-\ntner. 1989. The structure-mapping engine: Algo-\nrithm and examples. Artificial intelligence, 41(1):1‚Äì\n63.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6894‚Äì6910.\nDedre Gentner. 1983. Structure-mapping: A theoretical\nframework for analogy. Cognitive science, 7(2):155‚Äì\n170.\nDedre Gentner and Arthur B Markman. 1997. Struc-\nture mapping in analogy and similarity. American\npsychologist, 52(1):45.\nKarni Gilon, Joel Chan, Felicia Y Ng, Hila Liifshitz-\nAssaf, Aniket Kittur, and Dafna Shahaf. 2018. Anal-\nogy mining for specific design needs. In Proceedings\nof the 2018 CHI Conference on Human Factors in\nComputing Systems, pages 1‚Äì11.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Mat-\nsuoka. 2016. Analogy-based detection of morpholog-\nical and semantic relations with word embeddings:\nwhat works and what doesn‚Äôt. In Proceedings of the\nNAACL Student Research Workshop, pages 8‚Äì15.\nTom Hope, Joel Chan, Aniket Kittur, and Dafna Sha-\nhaf. 2017. Accelerating innovation through analogy\nmining. In Proceedings of the 23rd ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining, pages 235‚Äì243.\nYuxin Jiang, Chunkit Chan, Mingyang Chen, and\nWei Wang. 2023. Lion: Adversarial distillation\nof closed-source large language model. CoRR,\nabs/2305.12870.\nYuxin Jiang, Linhan Zhang, and Wei Wang. 2022. Im-\nproved universal sentence embeddings with prompt-\nbased contrastive learning and energy-based learning.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 3021‚Äì3035.\nAssociation for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nHaoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiao-\njin Zhang, Qi Hu, Chunkit Chan, and Yangqiu Song.\n2023a. Privacy in large language models: Attacks,\ndefenses and future directions.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and\nYangqiu Song. 2023b. Multi-step jailbreaking pri-\nvacy attacks on chatgpt. CoRR, abs/2304.05197.\nYian Li and Hai Zhao. 2021. Pre-training universal\nlanguage representation. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5122‚Äì5133.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in neural information processing systems,\n26.\nHaoran Mo, Edgar Simo-Serra, Chengying Gao,\nChangqing Zou, and Ruomei Wang. 2021. General\nvirtual sketching framework for vector line art. ACM\nTrans. Graph., 40(4).\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and evaluation framework for deeper under-\nstanding of commonsense stories. arXiv preprint\narXiv:1604.01696.\nThiloshon Nagarajah, Filip Ilievski, and Jay Pujara.\n2022. Understanding narratives through dimensions\nof analogy. arXiv preprint arXiv:2206.07167.\nOpenAI. 2022. Introducing chatgpt.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532‚Äì1543.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D Manning. 2020. Stanza: A python\nnatural language processing toolkit for many human\nlanguages. arXiv preprint arXiv:2003.07082.\nPartha Pratim Ray. 2023. Chatgpt: A comprehensive\nreview on background, applications, key challenges,\nbias, ethics, limitations and future scope. Internet of\nThings and Cyber-Physical Systems.\n11528\nKiamehr Rezaee and Jose Camacho-Collados. 2022.\nProbing relational knowledge in language models\nvia word analogies. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n3930‚Äì3936.\nDongyu Ru, Lin Qiu, Xipeng Qiu, Yue Zhang, and\nZheng Zhang. 2023. Distributed marker representa-\ntion for ambiguous discourse markers and entangled\nrelations. CoRR, abs/2306.10658.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI confer-\nence on artificial intelligence, volume 31.\nOren Sultan and Dafna Shahaf. 2023. Life is a circus\nand we are the clowns: Automatically finding analo-\ngies between situations and processes.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nPeter D Turney. 2008. The latent relation mapping\nengine: Algorithm and experiments. Journal of Arti-\nficial Intelligence Research, 33:615‚Äì655.\nPeter D Turney, Michael L Littman, Jeffrey Bigham, and\nVictor Shnayder. 2003. Combining independent mod-\nules in lexical multiple-choice problems. In RANLP,\npages 101‚Äì110.\nAsahi Ushio, Luis Espinosa Anke, Steven Schockaert,\nand Jose Camacho-Collados. 2021a. Bert is to nlp\nwhat alexnet is to cv: Can pre-trained language mod-\nels identify analogies? In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3609‚Äì3624.\nAsahi Ushio, Jose Camacho-Collados, and Steven\nSchockaert. 2021b. Distilling relation embeddings\nfrom pretrained language models. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 9044‚Äì9062.\nCunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xian-\ngru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi\nYao, Wenyang Gao, Xuming Hu, Zehan Qi, et al.\n2023. Survey on factuality in large language models:\nKnowledge, retrieval and domain-specificity. arXiv\npreprint arXiv:2310.07521.\nTaylor Webb, Keith J Holyoak, and Hongjing Lu. 2022.\nEmergent analogical reasoning in large language\nmodels. arXiv preprint arXiv:2212.09196.\nThilini Wijesiriwardene, Ruwan Wickramarachchi, Bi-\nmal G. Gajera, Shreeyash Mukul Gowaikar, Chan-\ndan Gupta, Aman Chadha, Aishwarya Naresh Re-\nganti, Amit P. Sheth, and Amitava Das. 2023. ANA-\nLOGICAL - A novel benchmark for long text anal-\nogy evaluation in large language models. CoRR,\nabs/2305.05050.\nLiu Zhenyuan, Michal Piovar Àáci, Christian Hafner,\nRapha√´l Charrondi√®re, and Bernd Bickel. 2023.\nDirectionality-aware design of embroidery patterns.\nIn Computer Graphics Forum, volume 42, pages 397‚Äì\n409. Wiley Online Library.\nXunjie Zhu and Gerard de Melo. 2020. Sentence analo-\ngies: Linguistic regularities in sentence embeddings.\nIn Proceedings of the 28th International Conference\non Computational Linguistics, pages 3389‚Äì3400.\nA Appendix\nA.1 Details in creating S TORYANALOGY .\nA.1.1 Details in generating candidates.\nWe prompt text-davinci-003 model to collect\nthe story analogy candidates. The prompt template\nis\nDemonstrations. The in-context learning seed\nexamples are presented in Table 8. In addition to\nthe golden story analogies, we also curated the cor-\nresponding keyword pairs with regard to each story\npair. This keyword pairs are useful for prompting\nto generate candidate stories from the Word Anal-\nogy and ConceptNet inputs, where their input data\nformat are word-pairs (e.g. word: language :: note:\nmusic).\nTo construct a list of demonstrations for each\ndata source, we ask experts to construct a set of\nanalogous story pairs by web-searching and revis-\ning the results. Then, to ensure the diversity of\nthe analogy, we make a list of orthogonal topics\nin each dataset, and randomly sampled demonstra-\ntions from these subtopics every time we construct\na prompt.\nPrompt templates for ‚Äúgenerating from story\npairs‚Äù. The template for the demonstration is:\n‚ÄúExample:\\n(1){source story(i)}\\nAn\nanalogy for story (1) can\nbe:\\n(2){target story(i)}‚Äù\nIt is concatenated with a prompt at the end:\n‚ÄúExample:\\n(1){source story}\\nAn analogy\nfor story (1) can be:‚Äù\nPrompt templates for ‚Äúgenerating from word\npairs‚Äù. The prompt template for generating from\nword pairs is: ‚Äú Write a group of 2-sentence\n11529\nSource story Target story\nThe stream becomes a river. The river continues to\nflow along the same path for a long time.\nENTITY: stream, river\nA person grows from a child into an adult. As time\npasses, the person experiences ongoing growth and\nmaturation.\nENTITY: child, adult\nMagma rises from deep in the earth. The magma\ngoes into volcanos.\nENTITY: magma, volcanos\nFood goes up from the stomach. The food enters\nthe esophagus.\nENTITY: food, esophagus\nThe plasma membrane encloses the animal cell. It\ncontrols the movement of materials into and out of\nthe cell.\nENTITY: plasma membrane, cell\nSecurity guards monitor the doors of the factory.\nThey manage the entry and exit of personnel to and\nfrom the factory.\nENTITY: security guard, factory\nThe tadpole begins storing food in the tail. The\ntadpole develops hind legs and lives off food stored\nin the it‚Äôs tail.\nENTITY: tadpole, food\nA person saves money in a savings account. The\nperson relies on the saved funds to meet future fi-\nnancial obligations and sustain their lifestyle.\nENTITY: human, money\nThe sediment near the bottom is compressed by the\nweight of newer sediment. The sediment becomes\nsedimentary rock as it is pushed together by the\nheavy weight.\nENTITY: sediment, sedimentary rock\nA person‚Äôs ideas and beliefs are shaped by their\nexperiences and influences. The person‚Äôs thoughts\nand opinions become more solidified and defined as\nthey are influenced by outside forces.\nENTITY: belief, solidified belief\nMorgan enjoyed long walks on the beach. She and\nher boyfriend decided to go for a long walk.\nENTITY: beach, walking\nLenny liked to climb trees. He embarked on a tree-\nclimbing expedition in the woods.\nENTITY: woods, climbing trees\nHe got a call from his girlfriend, asking where he\nwas. Frank suddenly realized he had a date that\nnight.\nENTITY: call, date\nShe received a notification on her phone, reminding\nher of an upcoming meeting. Jane suddenly remem-\nbered there was an important presentation to give.\nENTITY: notification, presentation\nShe was petrified and prayed to get out of the test.\nOn the last day of lessons, the bus broke down and\nshe was spared.\nENTITY: test, fear\nHe was terrified of the upcoming job interview.\nDue to oversleeping on the day of the interview,\nhe missed the appointment and thus avoided the\nstress.\nENTITY: job interview, stress\nHe is only two weeks into his job and he is nervous.\nEvery time he responds to calls he gets very worried.\nENTITY: job, nervous\nHaving recently started a relationship, she is grap-\npling with anxiety. She becomes highly anxious\nwhenever they have a disagreement.\nENTITY: relationship, anxious\nShe made sure she was quiet and respected others‚Äô\nspace. It was strange that on Wednesday, she came\nto the office hung over.\nENTITY: introverted, getting drunk\nJames took care to comply with the rules and\ndemonstrate deference towards authority figures.\nSurprisingly, he was caught shoplifting on a Friday.\nENTITY: disciplined, shoplifting\nTable 8: Seed analogy examples for generation candidates STORYANALOGY . We sample 5 pairs from propara and\n5 paris from rocstories.\n11530\nstories in around 30 words given the\nkeyword(s).\\Hint: Story 2 should be\nanalogous to story 1.\\n Example 0:\\n\nKeywords for story 0: {keywords}‚Äù\nIn addition, we notice that the story pairs gen-\nerated this way tend to have low entity similarity\n(since their entities are pre-given). Therefore, we\nadditionally prompt LLMs to write a set of similar\nkeywords for the source first, and then write a cor-\nresponding story, which does not need to be similar\nto the source.\n‚ÄúWrite a group of 2-sentence stories in\naround 30 words given the keyword(s).\nKeywords 1: {}\nStory 1: {}\nGive a set of keywords similar to keywords\n1, and then write a corresponding story\n(the stories do not have to be similar):‚Äù\nA.1.2 Annotation templates.\nIn this section, we showcase our templates used for\nannotation on the Amazon Mechanical Turk plat-\nform. The instructions used for evaluating entity\nand relation similarities are presented in Figure 6\nand Figure 8, respectively. Followed by these in-\nstructions, questions are presented using templates\nshown in Figure 7 and Figure 9.\nA.2 Details in ¬ß 3.1.\nA.2.1 Baselines\nDMR Note that DMR requires two sentences as\ninput. We use the NLTK toolkit to tokenize the\nstories into two sentences. In cases where there are\nnot enough two sentences, we try to split the story\nby the first comma. If there is no comma in the\nstory, the DMV vector is computed between the\nstory and an empty string ‚Äú ‚Äù. This only accounts\nfor a small number of stories (10-100).\nGloVe We use the glove-840B-300D version12.\nWe first use the Stanza part-of-speech (PoS) anno-\ntation tool to parse the PoS of words. Then, we\nreturn the summation of embeddings for all the\nwords with the corresponding PoS. Specifically, we\ndetermine nouns if a word‚Äôs uposis in {‚ÄúPROPN‚Äù,\n‚ÄúNOUN‚Äù}. We detect verbs if its uposis ‚ÄúVERB‚Äù\nor its xposstarts with ‚ÄúVB‚Äù.\nRoBERTa-Reg We apply an MLP on top of the\nRoBERTa model and output two digits, which cor-\nrespond to the EntSimand RelSim, respectively.\n12https://nlp.stanford.edu/data/glove.840B.\n300d.zip\nRoBERTa-CL Details for training the con-\ntrastive learning model. We adopt the SimCSE\ntraining script13 for training our contrastive learn-\ning model. The positive pairs are filtered according\nto the EntSim <= 1.0 and RelSim>= 2.0.\nA.2.2 Prompt templates for similarity\nprediction.\nThe templates used for prompting LLMs to gener-\nate similarity predictions are presented below.\nThe ‚Äúlong instruction‚Äù template for EntSim pre-\ndiction.\n‚ÄúEvaluate the entity similarity between\na pair of stories. Assign the pair a score\nbetween 0 and 3 as follows:\n0 : Unrelated. The two stories\nare talking about different topics and\nentities of different types.\n1 : Somewhat related. The two stories\ntalk about different entities, and some\nof them have similar or related types.\n2 : Somewhat equivalent. The two stories\nhave different entities, but they have\nthe same types.\n3 : Almost equivalent. The entities\nin the two stories are overlapped or\nsynonymous.\nFollowing the above instruction, evaluate\nthe entity / topic similarity for S1 and\nS2 (only answer by a score from 0, 1, 2,\n3):\n{N-DEMONSTRATIONS HERE} Q:\nS1 - {INPUT-S1}\nS2 - {INPUT-S2}\nScore :‚Äù\nThe ‚Äúlong instruction‚Äù template for RelSim pre-\ndiction.\n‚ÄúEvaluate the relation similarity between\na pair of stories. Assign the pair a\nscore between 0 and 3 as follows:\n0 : Very poor alignment. Most if not all\nrelationships do not align.\n1 : Alignment with significant mismatches.\nSome of the relationships align, but\nthere are some significant mismatches.\n2 : Alignment with insignificant\nmismatches. Most of the relationships\nalign except for some insignificant\nmismatches.\n3 : Alignment. The relationships can\n13https://github.com/princeton-nlp/SimCSE\n11531\nalign very well between the two stories.\nFollowing the above instruction, evaluate\nthe relational similarity for S1 and S2\n(only answer by a score from 0, 1, 2, 3):\n{N-DEMONSTRATIONS HERE} Q:\nS1 - {INPUT-S1}\nS2 - {INPUT-S2}\nScore :‚Äù\nHere, we insert N‚àà {0,1,3} demonstrations\nat the ‚ÄúN-DEMONSTRATION HERE‚Äù and fill in the\nstory pairs at ‚ÄúINPUT-S1‚Äù and ‚ÄúINPUT-S2‚Äù. The\n‚Äúshort instruction‚Äù templates are similar, with the\nonly difference that the detailed definition of scores\nis removed. For instance, ‚Äú0 : Unrelated. The\ntwo stories are talking about different\ntopics and entities of different types.‚Äù\nis replaced with ‚Äú0 : Unrelated.‚Äù\nA.3 Details in ¬ß 3.2.\nTo construct the multiple-choice evaluation set, we\ngather story analogy pairs with EntSim < 1.0 and\nRelSim > 2.0. Next, we sample negatives for each\nstory analogy pairs to form multiple choice ques-\ntions. Similar to the GloVe baseline in ¬ß A.2, we\nobtain the nounal embedding for each story, and\nretrieve stories with high cosine similarities while\nhave <50% overlapped tokens as the hard negative\nchoices. We manually inspect the overall qual-\nity of the multiple choice questions constructed\nin this manner. We excluded the questions gener-\nated from the ROCStories split due to their lower\nquality, likely because the unusual distribution of\nEntSimin this split made it difficult to use the same\nmethod for creating the dataset as in the other splits\n(Figure 3). The resulting multiple choice dataset\nconsists of 360 questions.\nBaselines in (Sultan and Shahaf, 2023) We ap-\nply both the FMQ and FMV models, as suggested\nin (Sultan and Shahaf, 2023), to our story analogy\nidentification task. To be precise, we gather the in-\ntermediate story pair similarities generated by their\nmodels. Afterwards, we choose the option that\nexhibits the highest similarity to the source story.\nNotably, the lengths of the stories in our dataset\nare considerably shorter than datasets used in its\npaper. Therefore, when running the baseline on our\ndataset, we adjusted the threshold of the similar-\nity filter to better suit our settings. We selected a\nthreshold of 0.3 for FMQ and 0.2 for FMV . For the\nother implementation details, we follow the origi-\nnal settings in their code repo at https://github.\ncom/orensul/analogies_mining. As for the re-\nsult, we discovered that FMQ and FMV exhibited\ncomparable performance (44.9% versus 44.7%) on\nthe multiple choice dataset. The result from FMQ\nare reported in the main paper.\nA.3.1 Prompt templates for multiple-choice\nevaluation.\nThe prompt template used in multiple-choice eval-\nuation is:\n‚Äú {QUESTION}\nSource story: {}\nCandidate stories:\n(0): {}\n(1): {}\n(2): {}\n(3): {}\nAnswer: ‚Äù\nwhere ‚ÄúQUESTION‚Äù is replaced with one of the fol-\nlowing questions:\nA: ‚ÄúSelect the candidate that best matches\nthe source story as an analogy.‚Äù\nB: ‚Äú Which candidate story is the best\ncreative analogy for the source story?‚Äù\nC: ‚ÄúA creative analogy should have fewer\nsimilar entities but similar relational\nstructures to the source story. Which\ncandidate story is the best creative\nanalogy for the source story?‚Äù\nA.4 Details in the evaluation of story analogy\ngeneration.\nA.4.1 Generation setups.\nThe models are evaluated under zero-shot, few-\nshot, and instruction-tuning settings. For zero-\nshot and few-shot prompting, the templates are:\n‚ÄúWrite an analogy for story 1.\\n \\nStory\n1: {}\\nStory 2:‚Äù This template is also used in\nthe finetuning setting. For finetuning, we employ\nDeepSpeed14 to accelerate the training on a single\n8*V100 (32GB) instance.\nA.4.2 Annotation.\nWe conduct crowd annotation on AMT to evaluate\nthe generation quality. The annotation instruction\nis presented in Figure 5. During the annotation,\nthe meta information of the target generation is\nhidden from the annotators and the requesters. In\naddition, the target stories are shuffled such that\n14https://www.microsoft.com/en-us/research/\nproject/deepspeed/\n11532\nannotators cannot find out which models are used\nto generate the stories based on the order.\nA.5 Miscellaneous\nIn this section, we present some discussions that\ntook place during the reviewing process.\nA.5.1 Potential applications of this work.\nAnalogy Mining for Art and Design. There have\nbeen various studies focusing on building analog-\nical search engines. Hope et al. (2017) contribute\na method for analogy mining over products. Chan\net al. (2018) mine analogies from research papers\nwith respect to their background, purpose, mecha-\nnism, and findings. Gilon et al. (2018) develop a\nsearch engine for expressing and abstracting spe-\ncific design needs. Recently, Bitton et al. (2023)\npropose a visual analogies dataset, V ASR, where\nthey found that models struggle to find out analo-\ngies when given carefully chosen distractors. In\ncomputer graphics, some graphics design algo-\nrithms take as input an image from the user, and\ntransform it to some other types of visual designs\nthat are similar to the given image, such as em-\nbroidery patterns (Zhenyuan et al., 2023) and vec-\ntor line arts (Mo et al., 2021). This category of\nwork establishes connections between images and\napplication-specific graphics patterns. With im-\nages as a guidance, the complicated visual design\nprocesses are made easy and intuitive for nonpro-\nfessional users.\nAnalogical Reasoning. Large language models\n(LLMs) have demonstrated impressive abilities in\nfew-shot and zero-shot learning (Kaplan et al.,\n2020; OpenAI, 2022, 2023). Recently, Chat-\nGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023),\nAlpaca (Taori et al., 2023) and their following\nworks (Chiang et al., 2023; Jiang et al., 2023) have\nachieved remarkable performance on a wide range\nof benchmarks. It is believed that they have ac-\nquired certain kind of analogical reasoning ability\nthat are not only task-specific (Webb et al., 2022;\nDing et al., 2023) , but also omnipresent through-\nout the prompting process of LLMs, and there are\na lot of prompt engineering work to leverage this\ncharacteristic for downstream tasks (Jiang et al.,\n2022; Chan et al., 2023b,a,c; Chan and Chan, 2023).\nMeanwhile, it is important to note that LLMs also\nexhibit potential issues related to hallucination, bi-\nases, and privacy (Ray, 2023; Li et al., 2023a,b;\nWang et al., 2023). Mitigating such issues often\nrequires building up knowledge bases (Cheng et al.,\n2021; Cui et al., 2021b,a), where analogy could be\na useful angle to improve automatic building per-\nformance (Chen et al., 2022b). The data and evalua-\ntion metrics in this work may serve as a benchmark\nin evaluating one of the analogical reasoning abili-\nties.\nA.5.2 Why the predictions of individual scores\nare good, but the prediction of Œ±is bad.\nOriginal question: How is it that models are\nso good at individually predicting EntSim and\nRelSim(in Section 3.1), but they are not that good\nat predicting the analogy score Œ±?\nSince the analogy score is computed from both\nEntSim and RelSim, the prediction of the analogy\nscore relies on predicting the gap between EntSim\nand RelSim, which is harder than predicting each\nsimilarity alone. A case is presented below to illus-\ntrate this.\nSuppose we have four story pairs, and their\nground-truth scores are: EntSim = [2, 1, 3,\n0], RelSim = [0, 1, 2, 3].\nThe corresponding predictions are: EntSim‚Äô=\n[0, 2, 3, 0], RelSim‚Äô= [1, 0, 2, 1].\nThen, the analogy scores and the predicted anal-\nogy scores can be computed from the above values\n(using Œ± = RelSim\nEntSim+1 )): Œ± = [0, 0.5, 0.5, 3],\nŒ±‚Äô = [1, 0, 0.5, 1].\nFinally, we can compute the Spearman‚Äôs correla-\ntion coefficients as:\nCorr(RelSim, RelSim‚Äô)=0.316\nCorr(EntSim, EntSim‚Äô)=0.632\nCorr(Œ±, Œ±‚Äô) = 0\nHere, though the predictions of the respective\nscores have medium correlation with the ground-\ntruths, the prediction of Œ±has zero correlation.\n11533\nFigure 5: The annotation instruction for generation quality evaluation.\nSurvey Instructions (Click to Collapse)\nEntity/Topic Similarity Evaluation\nHi! Welcome to our main round HITs. In this survey, you will be given a few pairs of stories. For each pair of stories, your task is to evaluate the similarity\nbetween these two stories. Specifically, you need to consider the topic similarity between them, which is helpful to think from the entities in stories. Take the\nfollowing two stories as an example:\nThe virus invades cells. As a result, the DNAs are damaged.\nThe burglar breaks into the house. As a result, the valuables inside are smashed.\nThe entities within the first story are: virus, cells, DNA, while in the second story: burglar, house, valuables. We then know that the similarity is relatively\nlow here, as these two groups of entities are distant.\nEntity/Topic Similarity Rating\nTo determine the similarity, you are required to score it from 0 to 3. The higher the score, the more close two stories are.\nFor first time readers, you are recommended to read through the Full Instruction here.\nIn short, the definitions and examples of the four scores are provided below:\n3: Almost Equivalent\nDefinition: The entities in the two stories are overlapped or synonymous.\nToxins harm the body. The body tries to react to the toxins. Poison enters the body. The body tries to fight off the poison.\nBogart lived on a farm. He loved bacon. Mary lived on a farm. She loved bacon.\n2: Somewhat equivalent\nDefinition: The two stories have different entities, but they have the same types.\nJohn walked over to talk to her. Five minutes later he returned to a grill full of\nburned burgers.\nMary went inside to grab a drink. When she came back, the vegetables on\nthe grill were charred.\nBogart lived on a farm. He loved bacon. Jane lived in the city. She loved pizza.\nWater vapor condenses. Clouds form. Ice cubes form when water freezes. The water molecules become more\norganized and compact.\n1: Somewhat related\nDefinition: The two stories talk about different entities, and some of them have similar or related types.\nMy wife went to the store to buy moving boxes. She bought boxes that were\ntoo large.\nI went to the grocery store to buy fruit. I bought apples that were too ripe.\nPut the aluminum can into a recycle bin. The cans are transported to a\nfacility.\nPut the dirty dishes into the dishwasher. The dishes are transported to the\nsink for cleaning.\n0: Unrelated\nDefinition: The two stories are talking about different topics and entities of different types.\nThis liquid is known as magma. The magma rises to the earth's surface in\nvolcanoes where it cools and hardens.\nSecurity guards monitor the doors of the factory. They control the movement\nof people into and out of the factory.\nThe volcanos erupt many times. The size of the rocky area grows. A plant grows larger over time. The plant's roots spread and deepen.\nAdditional Hints\nNote that, some stories presented in this task are generated by machine, so they could be ungrammatical or counterfactual. When you find\nsuch stories, please tick the box within the respective question box.\nHuman names and pronouns are seen as synonymous between stories. For example, Bogart lived on a farm and Mary lived on a farm can be treated as two\nidentical stories.\nPair 1\nBogart lived on a farm. He loved bacon.\nJane lived in the city. She loved pizza.\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 2: ${q2_id}\n${q2_text1}\n${q2_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 3: ${q3_id}\n${q3_text1}\n${q3_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 4: ${q4_id}\n${q4_text1}\n${q4_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 5: ${q5_id}\n${q5_text1}\n${q5_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 6: ${q6_id}\n${q6_text1}\n${q6_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 7: ${q7_id}\n${q7_text1}\n${q7_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 8: ${q8_id}\n${q8_text1}\n${q8_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 9: ${q9_id}\n${q9_text1}\n${q9_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 10: ${q10_id}\n${q10_text1}\n${q10_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nPreviewing Answers Submitted by Workers\nThis message is only visible to you and will not be shown to Workers.\nYou can test completing the task below and click \"Submit\" in order to preview the data and format of the submitted results.\nSubmit\nFigure 6: The instructions used for evaluating entity similarity in human annotations.\n11534\nSource story Target story Scores\n Œ± Domain\nThe stream becomes a river. The river continues to\nflow along the same path for a long time.\nA person grows from a child into an adult. As time\npasses, the person experiences ongoing growth and\nmaturation.\n: 0.6\n : 2.8 1.6 PP\nFertilize the soil. Mix seeds into the fertilized soil.Apply lotion to the skin. Massage the lotion into\nthe skin.\n : 0.0\n : 2.6 2.6 PP\nFill the tray with cool water. Place the tray in the\nfreezer.\nFill the bucket with warm water. Place the bucket\nin the refrigerator.\n : 3.0\n : 3.0 0.8 PP\nThe resulting material disappears. The plant be-\ncomes one with the soil.\nThe water evaporates. The liquid turns into vapor\nand disperses into the air.\n : 2.0\n : 0.4 0.1 PP\nThe gas condenses in the condenser and becomes\na liquid again. Heat is radiated away from the con-\ndenser.\nAn emotion is expressed and released. A calming\neffect follows after the expression.\n : 0.3\n : 0.0 0.0 PP\nThey left him the key to the entrance. When Tom\nwent over he realized it was the wrong key.\nThey gave her the password to the website. When\nJane logged in, she realized it was the wrong pass-\nword.\n: 1.0\n : 2.7 1.3 ROC\nI was building a dresser. I had several tools to help.I was baking a cake. I had several ingredients to\nhelp.\n : 1.0\n : 3.0 1.5 ROC\nIt‚Äôs broken. I have to buy a new one. It‚Äôs expired. I have to get a new one.\n : 3.0\n : 3.0 0.8L ROC\nHis cellmate tried to bully the man. The man fought\nhis cellmate.\nHis classmate tried to intimidate him. The man\nstood his ground and refused to be bullied.\n: 2.7\n : 1.5 0.4 ROC\nThe fight lasted until 10 am. We finally just went to\nbed out of exhaustion.\nThe argument went on until midnight. We eventu-\nally just gave up and went home in defeat.\n: 1.3\n : 0.0 0.0 ROC\nFoundations are poured to support the walls and\nroofs of buildings. The structure of the building is\nonly as strong as it‚Äôs foundation.\nReasons are formulated to make theories. The con-\nclusions of theories are only as dependable as their\ninitial premises.\n: 0.6\n : 1.8 1.1 W A\nThe ground for the building is solid and secure. This\ngives the building its foundation and stability.\nThe reasons for the theory provide a rational expla-\nnation. This informs the decision-making process\nthat supports the theories accuracy.\n: 0.4\n : 2.8 2.0 W A\nHis memory has broken into fragmented pieces. He\ncan recall flashes and images of the past, but nothing\nconcrete or clear.\nHis memories remain a confused mess. Nothing\nholds together and what he remembers don‚Äôt make\nsense.\n: 2.7\n : 3.0 0.8 W A\nHeat energy is transferred from one point to another.\nTransfers between different substances cause tem-\nperature changes.\nSolid materials undergo phase transitions when en-\nergy is added. Changes in pressure can also result\nin phase transitions.\n: 2.8\n : 1.0 0.3 W A\nShe laughed and let go of all of her worries. Her\ncarefree attitude was liberating.\nHe delved into the unknown without a second\nthought, ignorant of the knowledge to come.\n: 0.8\n : 0.0 0.0 W A\nThe student opens the book and begins to read. The\nknowledge gained from the book is absorbed by the\nstudent.\nThe cat sees a mouse and begins to chase it. The\ncat honing its hunting skills through practice and\nrepetition.\n: 0.8\n : 1.4 0.8 CN\nThe trigger is pulled and the pistol shoots. The gun\nfires.\nA meteorite impacts Saturn‚Äôs surface. The planet is\nbuffeted by these larger objects.\n : 0.4\n : 2.8 2.0 CN\nHe knew his only way out was to commit suicide.\nHe was determined to die, no matter what.\nShe figured an overdose was the only way out.\nWithin minutes, she had taken her last breath and\ndied.\n: 3.0\n : 2.6 0.7 CN\nShe purchased a round-trip ticket for her travels.\nShe left with the assurance that she would return.\nShe chose her destination for her vacation with ex-\ncitement. She anticipated what her journey would\nbring.\n: 2.8\n : 1.4 0.4 CN\nThe rain began to pour and gradually, the river\nstarted to overflow. It was the start of a devastating\nflood.\nThe scissors snipped away, trimming her locks until\nher hair was just right. She became the proud owner\nof a new, short hairstyle.\n: 0.0\n : 0.0 0.0 CN\nTable 9: Examples in STORYANALOGY with annotations from each domain. We report the EntSim\n and RelSim\nfrom crowd workers\n . The Domain column indicates the source of the story pairs. ‚ÄúPP‚Äù, ‚ÄúROC‚Äù, ‚ÄúW A‚Äù, and\n‚ÄúCN‚Äù are short for ‚ÄúProPara‚Äù, ‚ÄúROCStories‚Äù, ‚ÄúWord Analogy‚Äù, and ‚ÄúConceptNet‚Äù, respectively.\n11535\nSurvey Instructions (Click to Collapse)\nEntity/Topic Similarity Evaluation\nHi! Welcome to our main round HITs. In this survey, you will be given a few pairs of stories. For each pair of stories, your task is to evaluate the similarity\nbetween these two stories. Specifically, you need to consider the topic similarity between them, which is helpful to think from the entities in stories. Take the\nfollowing two stories as an example:\nThe virus invades cells. As a result, the DNAs are damaged.\nThe burglar breaks into the house. As a result, the valuables inside are smashed.\nThe entities within the first story are: virus, cells, DNA, while in the second story: burglar, house, valuables. We then know that the similarity is relatively\nlow here, as these two groups of entities are distant.\nEntity/Topic Similarity Rating\nTo determine the similarity, you are required to score it from 0 to 3. The higher the score, the more close two stories are.\nFor first time readers, you are recommended to read through the Full Instruction here.\nIn short, the definitions and examples of the four scores are provided below:\n3: Almost Equivalent\nDefinition: The entities in the two stories are overlapped or synonymous.\nToxins harm the body. The body tries to react to the toxins. Poison enters the body. The body tries to fight off the poison.\nBogart lived on a farm. He loved bacon. Mary lived on a farm. She loved bacon.\n2: Somewhat equivalent\nDefinition: The two stories have different entities, but they have the same types.\nJohn walked over to talk to her. Five minutes later he returned to a grill full of\nburned burgers.\nMary went inside to grab a drink. When she came back, the vegetables on\nthe grill were charred.\nBogart lived on a farm. He loved bacon. Jane lived in the city. She loved pizza.\nWater vapor condenses. Clouds form. Ice cubes form when water freezes. The water molecules become more\norganized and compact.\n1: Somewhat related\nDefinition: The two stories talk about different entities, and some of them have similar or related types.\nMy wife went to the store to buy moving boxes. She bought boxes that were\ntoo large.\nI went to the grocery store to buy fruit. I bought apples that were too ripe.\nPut the aluminum can into a recycle bin. The cans are transported to a\nfacility.\nPut the dirty dishes into the dishwasher. The dishes are transported to the\nsink for cleaning.\n0: Unrelated\nDefinition: The two stories are talking about different topics and entities of different types.\nThis liquid is known as magma. The magma rises to the earth's surface in\nvolcanoes where it cools and hardens.\nSecurity guards monitor the doors of the factory. They control the movement\nof people into and out of the factory.\nThe volcanos erupt many times. The size of the rocky area grows. A plant grows larger over time. The plant's roots spread and deepen.\nAdditional Hints\nNote that, some stories presented in this task are generated by machine, so they could be ungrammatical or counterfactual. When you find\nsuch stories, please tick the box within the respective question box.\nHuman names and pronouns are seen as synonymous between stories. For example, Bogart lived on a farm and Mary lived on a farm can be treated as two\nidentical stories.\nPair 1\nBogart lived on a farm. He loved bacon.\nJane lived in the city. She loved pizza.\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 2: ${q2_id}\n${q2_text1}\n${q2_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 3: ${q3_id}\n${q3_text1}\n${q3_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 4: ${q4_id}\n${q4_text1}\n${q4_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 5: ${q5_id}\n${q5_text1}\n${q5_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 6: ${q6_id}\n${q6_text1}\n${q6_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 7: ${q7_id}\n${q7_text1}\n${q7_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 8: ${q8_id}\n${q8_text1}\n${q8_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 9: ${q9_id}\n${q9_text1}\n${q9_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nPair 10: ${q10_id}\n${q10_text1}\n${q10_text2}\nHow will you rate the topic similarity between these two stories?\nAlmost equivalent! The entities in the two stories are overlapped or synonymous.\nSomewhat equivalent! The two stories have different entities, but they have the same types.\nSomewhat related. The two stories talk about different entities, and some of them have similar or related types.\nUnrelated. The two stories are talking about different topics and entities of different types.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nPreviewing Answers Submitted by Workers\nThis message is only visible to you and will not be shown to Workers.\nYou can test completing the task below and click \"Submit\" in order to preview the data and format of the submitted results.\nSubmit\nFigure 7: The template for presenting a question regarding the evaluation of entity similarity in human annotations.\nSurvey Instructions (Click to Collapse)\nRelational Similarity Evaluation\nHi! Welcome to our main round HITs. In this survey, you will be given a few pairs of stories. For each pair of stories, your task is to evaluate the similarity\nbetween these two stories. Specifically, you need to consider the relation similarity between them, which is helpful to think about how the entities are\nconnected and how does the event change within both stories.\nYou are REQUIRED to read through the Full Instruction here.\nRelational Similarity Rating\nTo determine the relational similarity, you are required to score it from 0 to 3. The higher the score, the more close two stories are.\nIn short, the definitions and examples of the four scores are provided below, first column is denoted as Story A and the second column contains Story B:\n3: Perfect Alignment\nDefinition: The relationships can align very well between the two stories.\nThe animal's heart rate and breathing rate slow. The\nanimal loses weight more slowly than usual.\nThe car's engine and exhaust system slow down. The car uses less fuel than usual.\nThe stream becomes a river. The river continues to\nflow along the same path for a long time.\nA plant grows from a seed into a mature plant. The plant continues to grow and thrive in its\nenvironment over time.\nMany more dead plants sink in the same area. The\ndead plants join together forming peat.\nMany more leaves fall to the ground in the same area. The leaves pile up forming a layer of mulch.\n2: Alignment with insignificant mismatches\nDefinition: Most of the relationships align except for some insignificant mismatches. (\"insignificant\" means that the differences do not invalidate\nthe relational similarity of the story pair.)\nMagma rises from deep in the earth. The magma goes\ninto volcano.\nOxygen goes from the lungs to the blood. Blood goes to the heart.\n(Explanation: In story B, the oxygen goes from the lungs to the heart through blood, which is\nslightly different from the relationship of story A.)\nSarah was on a bus to her work. She had to pee very\nbadly.\nTom was driving to a client meeting. He suddenly realized he had a pressing need to use the\nbathroom.\n(Explanation: The logical connection (inter-event/state relationship) between events can be slightly\ndifferent. In story B, Tom \"suddenly realized\" the urgent need to go to the bathroom, while this is\nmissing in story A.)\n1: Alignment with significant mismatches\nDefinition: Some relationships align, but there are some significant mismatches.\nThe sound wave returns to the bat. The bat hears the\nechoed sound.\nA person speaks to another person. The other person hears the words and responds.\n(Explanation: Many event/state relationships in A cannot align with B.)\nThe cans are transported to a facility. The cans are\nshredded by a machine.\nA package is delivered to a warehouse. The package is opened and its contents are sorted by\nworkers.\n(Explanation: The relationships in the underlined part cannot align with story A.)\nJo wanted to impress his friends. He went to a gator\nwrestling show.\nTom loved dancing. He went to a salsa club.\n(Explanation: The relationships in the underlined part do not align.)\n0: Very poor alignment\nDefinition: Most if not all relationships do not align.\nMorgan enjoyed long walks on the beach. She and\nher boyfriend decided to go for a long walk.\nI decided to take my girlfriend to the beach. As we were walking she paused.\nThis liquid is known as magma. The magma rises to\nthe earth's surface in volcanoes where it cools and\nhardens.\nSecurity guards monitor the doors of the factory. They control the movement of people into and out\nof the factory.\nAdditional Hints\nNote that, some stories presented in this task are generated by machine, so they could be ungrammatical or counterfactual. When you find\nsuch stories, please tick the box below the respective question.\nHuman names and pronouns are seen as synonymous between stories. For example, Bogart lived on a farm and Mary lived on a farm can be treated as two\nidentical stories.\nPair 1\nThe animal's heart rate and breathing rate slow. The animal loses weight more\nslowly than usual.\nThe car's engine and exhaust system slow down. The car uses less fuel than usual.\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 2: ${q2_id}\n${q2_text1}\n${q2_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 3: ${q3_id}\n${q3_text1}\n${q3_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 4: ${q4_id}\n${q4_text1}\n${q4_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 5: ${q5_id}\n${q5_text1}\n${q5_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 6: ${q6_id}\n${q6_text1}\n${q6_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 7: ${q7_id}\n${q7_text1}\n${q7_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 8: ${q8_id}\n${q8_text1}\n${q8_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 9: ${q9_id}\n${q9_text1}\n${q9_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 10: ${q10_id}\n${q10_text1}\n${q10_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nPreviewing Answers Submitted by Workers\nThis message is only visible to you and will not be shown to Workers.\nYou can test completing the task below and click \"Submit\" in order to preview the data and format of the submitted results.\nSubmit\nFigure 8: The instructions used for evaluating relation similarity in human annotations.\n11536\nSurvey Instructions (Click to Collapse)\nRelational Similarity Evaluation\nHi! Welcome to our main round HITs. In this survey, you will be given a few pairs of stories. For each pair of stories, your task is to evaluate the similarity\nbetween these two stories. Specifically, you need to consider the relation similarity between them, which is helpful to think about how the entities are\nconnected and how does the event change within both stories.\nYou are REQUIRED to read through the Full Instruction here.\nRelational Similarity Rating\nTo determine the relational similarity, you are required to score it from 0 to 3. The higher the score, the more close two stories are.\nIn short, the definitions and examples of the four scores are provided below, first column is denoted as Story A and the second column contains Story B:\n3: Perfect Alignment\nDefinition: The relationships can align very well between the two stories.\nThe animal's heart rate and breathing rate slow. The\nanimal loses weight more slowly than usual.\nThe car's engine and exhaust system slow down. The car uses less fuel than usual.\nThe stream becomes a river. The river continues to\nflow along the same path for a long time.\nA plant grows from a seed into a mature plant. The plant continues to grow and thrive in its\nenvironment over time.\nMany more dead plants sink in the same area. The\ndead plants join together forming peat.\nMany more leaves fall to the ground in the same area. The leaves pile up forming a layer of mulch.\n2: Alignment with insignificant mismatches\nDefinition: Most of the relationships align except for some insignificant mismatches. (\"insignificant\" means that the differences do not invalidate\nthe relational similarity of the story pair.)\nMagma rises from deep in the earth. The magma goes\ninto volcano.\nOxygen goes from the lungs to the blood. Blood goes to the heart.\n(Explanation: In story B, the oxygen goes from the lungs to the heart through blood, which is\nslightly different from the relationship of story A.)\nSarah was on a bus to her work. She had to pee very\nbadly.\nTom was driving to a client meeting. He suddenly realized he had a pressing need to use the\nbathroom.\n(Explanation: The logical connection (inter-event/state relationship) between events can be slightly\ndifferent. In story B, Tom \"suddenly realized\" the urgent need to go to the bathroom, while this is\nmissing in story A.)\n1: Alignment with significant mismatches\nDefinition: Some relationships align, but there are some significant mismatches.\nThe sound wave returns to the bat. The bat hears the\nechoed sound.\nA person speaks to another person. The other person hears the words and responds.\n(Explanation: Many event/state relationships in A cannot align with B.)\nThe cans are transported to a facility. The cans are\nshredded by a machine.\nA package is delivered to a warehouse. The package is opened and its contents are sorted by\nworkers.\n(Explanation: The relationships in the underlined part cannot align with story A.)\nJo wanted to impress his friends. He went to a gator\nwrestling show.\nTom loved dancing. He went to a salsa club.\n(Explanation: The relationships in the underlined part do not align.)\n0: Very poor alignment\nDefinition: Most if not all relationships do not align.\nMorgan enjoyed long walks on the beach. She and\nher boyfriend decided to go for a long walk.\nI decided to take my girlfriend to the beach. As we were walking she paused.\nThis liquid is known as magma. The magma rises to\nthe earth's surface in volcanoes where it cools and\nhardens.\nSecurity guards monitor the doors of the factory. They control the movement of people into and out\nof the factory.\nAdditional Hints\nNote that, some stories presented in this task are generated by machine, so they could be ungrammatical or counterfactual. When you find\nsuch stories, please tick the box below the respective question.\nHuman names and pronouns are seen as synonymous between stories. For example, Bogart lived on a farm and Mary lived on a farm can be treated as two\nidentical stories.\nPair 1\nThe animal's heart rate and breathing rate slow. The animal loses weight more\nslowly than usual.\nThe car's engine and exhaust system slow down. The car uses less fuel than usual.\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 2: ${q2_id}\n${q2_text1}\n${q2_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 3: ${q3_id}\n${q3_text1}\n${q3_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 4: ${q4_id}\n${q4_text1}\n${q4_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 5: ${q5_id}\n${q5_text1}\n${q5_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 6: ${q6_id}\n${q6_text1}\n${q6_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 7: ${q7_id}\n${q7_text1}\n${q7_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 8: ${q8_id}\n${q8_text1}\n${q8_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 9: ${q9_id}\n${q9_text1}\n${q9_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nPair 10: ${q10_id}\n${q10_text1}\n${q10_text2}\nHow will you rate the relational similarity between these two stories?\nPerfect Alignment! The relationships can align very well between these two stories.\nAlignment with insignificant mismatches. Most of the relationships align except for some insignificant\nmismatches.\nAlignment with significant mismatches. Some relationships align, but there are some significant mismatches.\nVery poor alignment. Most or even all relationships do not align.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nAt least one of these stories are ungrammatical or counterfactual.\nPreviewing Answers Submitted by Workers\nThis message is only visible to you and will not be shown to Workers.\nYou can test completing the task below and click \"Submit\" in order to preview the data and format of the submitted results.\nSubmit\nFigure 9: The template for presenting a question regarding the evaluation of relation similarity in human annotations.\n11537"
}