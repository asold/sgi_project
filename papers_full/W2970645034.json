{
  "title": "PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation",
  "url": "https://openalex.org/W2970645034",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A1972032321",
      "name": "Wei Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097071571",
      "name": "Xiaofeng Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112375820",
      "name": "Keqiang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120599630",
      "name": "Xun Luo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2970782612",
      "name": "Xiepeng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2048190986",
      "name": "Yuan Ni",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095693315",
      "name": "Xie Guotong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2922551710",
    "https://openalex.org/W2937297214",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2952650870",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4303684868",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2970986790",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2895604144",
    "https://openalex.org/W2963769536",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2136189984",
    "https://openalex.org/W2905933322",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W4235216760",
    "https://openalex.org/W2769041395",
    "https://openalex.org/W2265846598"
  ],
  "abstract": "This paper describes the models designated for the MEDIQA 2019 shared tasks by the team PANLP. We take advantages of the recent advances in pre-trained bidirectional transformer language models such as BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019b). We find that pre-trained language models can significantly outperform traditional deep learning models. Transfer learning from the NLI task to the RQE task is also experimented, which proves to be useful in improving the results of fine-tuning MT-DNN large. A knowledge distillation process is implemented, to distill the knowledge contained in a set of models and transfer it into an single model, whose performance turns out to be comparable with that obtained by the ensemble of that set of models. Finally, for test submissions, model ensemble and a re-ranking process are implemented to boost the performances. Our models participated in all three tasks and ranked the 1st place for the RQE task, and the 2nd place for the NLI task, and also the 2nd place for the QA task.",
  "full_text": "Proceedings of the BioNLP 2019 workshop, pages 380–388\nFlorence, Italy, August 1, 2019.c⃝2019 Association for Computational Linguistics\n380\nPANLP at MEDIQA 2019: Pre-trained Language Models, Transfer\nLearning and Knowledge Distillation\nWei Zhu, Xiaofeng Zhou, Keqiang Wang, Xun Luo, Xiepeng Li, Yuan Ni, Guotong Xie\nPingan Health Tech, Shanghai, China\n{zhuwei972, zhouxiaofeng824, wangkeqiang265, luoxun492, lixiepeng538,\nniyuan442, xieguotong}@pingan.com.cn\nAbstract\nThis paper describes the models designated for\nthe MEDIQA 2019 shared tasks by the team\nPANLP. We take advantages of the recent ad-\nvances in pre-trained bidirectional transformer\nlanguage models such as BERT (Devlin et al.,\n2018) and MT-DNN (Liu et al., 2019b). We\nﬁnd that pre-trained language models can sig-\nniﬁcantly outperform traditional deep learning\nmodels. Transfer learning from the NLI task\nto the RQE task is also experimented, which\nproves to be useful in improving the results of\nﬁne-tuning MT-DNN large. A knowledge dis-\ntillation process is implemented, to distill the\nknowledge contained in a set of models and\ntransfer it into an single model, whose perfor-\nmance turns out to be comparable with that ob-\ntained by the ensemble of that set of models.\nFinally, for test submissions, model ensemble\nand a re-ranking process are implemented to\nboost the performances. Our models partici-\npated in all three tasks and ranked the 1st place\nfor the RQE task, and the 2nd place for the NLI\ntask, and also the 2nd place for the QA task.\n1 Introduction\nThere are three tasks in the MEDIQA 2019 shared\ntasks (see Ben Abacha et al. (2019) for details of\nthe tasks). The ﬁrst one, NLI, consists in identi-\nfying three inference relations between two sen-\ntences: Entailment, Neutral and Contradiction.\nThe second one, RQE, requires one to identify\nwhether one question entails the other, where the\ndeﬁnition of entailment is that a question A entails\na question B if every answer to B is also a com-\nplete or partial answer to A. The third task, QA,\nconsiders not only the identiﬁcation of entailment\nfor the asked question among a set of retrieved\nquestions, but also the ranks of retrieved answers.\nIn this work, we demonstrate that we can\nachieve signiﬁcant performance gains over tra-\nditional deep learning models like ESIM (Chen\net al., 2016), by adapting pre-trained language\nmodels into the medical domain. Language model\npre-training has shown to be effective for learn-\ning universal language representations by lever-\naging large amounts of unlabeled data. Some of\nthe most famous examples are GPT-V2 (see Rad-\nford et al., 2019) and BERT ( by Devlin et al.,\n2018). These are neural network language models\ntrained on text data using unsupervised objectives.\nFor example, BERT is based on a multi-layer bidi-\nrectional Transformer, and is trained on plain text\nfor masked word prediction and next sentence pre-\ndiction tasks. To apply a pre-trained model to\nspeciﬁc NLU tasks such as tasks for MEDIQA\n2019 shared tasks, we often need to ﬁne-tune the\nmodel with additional task-speciﬁc layers using\ntask-speciﬁc training data. For example, Devlin\net al. (2018) show that BERT can be ﬁne-tuned\nthis way to create state-of-the-art models for a\nrange of NLU tasks, such as question answering\nand natural language inference.\nWe also tryout a transfer learning procedure,\nwhere an intermediate model obtained on the NLI\ntask is used to be ﬁne-tuned on the RQE task. Al-\nthough this procedure cannot consistently improve\nthe dev set performance for all the models, it is\nproven to be beneﬁcial on the test set by adding\nvariety to the model pool.\nTo further improve the performance of sin-\ngle models, we implement a knowledge distil-\nlation procedure on the RQE task and the NLI\ntask. Knowledge distillation distills or transfers\nthe knowledge from a (set of) large, cumber-\nsome model(s) to a lighter, easier-to-deploy sin-\ngle model, without signiﬁcant loss in performance\n(Liu et al., 2019a; Tan et al., 2019). Knowledge\ndistillation recently has attracted a lot of atten-\ntions. We believe it is interesting and of great\nimportance to explore this method on the appli-\ncations of the medical domain.\n381\nFor test submissions, model ensemble is used to\nobtain more stable and unbiased predictions. We\nonly adopt a simple ensemble model, that is, av-\neraging the class probabilities of different models.\nAfter obtaining test predictions, for the NLI and\nRQE task, simple re-ranking operations among\npairs with the same premise are used to boost the\nperformance metrics.\nThe rest of the paper is organized as follows. In\nSection 2 , we demonstrate our experiments on the\nthree tasks. In Section 3, transfer learning from\nNLI to RQE is presented. Section 4 elaborates\non the knowledge distillation and the correspond-\ning experimental results. Section 5 and Section 6\npresent the model ensemble technique and the re-\nranking strategies. Section 7 explains our submis-\nsion records in detail. Section 8 concludes and dis-\ncusses future work.\n2 Pairwise Text Modeling\nThis section elaborates on the fundamental meth-\nods we used for the three tasks.\n2.1 RQE\nThe RQE task, as a pairwise text classiﬁca-\ntion task, deﬁned here involves a premise P =\n(p1, p2, ..., pm) of m words, which is a medical\nquestion posted online, and a hypothesis H =\n(h1, h2, ..., hn) of n words, which is a standard\nfrequently asked question that is collected to build\na QA system, and aims to ﬁnd a logical relation-\nship R between P and H. For the RQE task,\nrelationship R is either true or false, indicating\nwhether the premise entails the hypothesis or not.\nWe mainly experiment on two groups of models,\none using ﬁxed pre-trained embedding 1, the other\nemploying pre-trained language models.\nTraditional deep learning models typically use\na ﬁxed pre-trained word embedding to map words\ninto low-dimensional vector space, and then use\nsome kind of encoders to encode and pool the\ncontexts of the premise to vector r1 and hy-\npothesis H to r2. And the features provided\nto the classiﬁcation layer is concat(r1, r2, ||r1 −\nr2||, r1 ∗r2). (see Bowman et al., 2015) Then\nthe classiﬁcation output layer is usually a dense\nlayer with soft-max output. We experiment with\nthe following 4 traditional deep learning models.\nThe ﬁrst model, which will be called Weighted-\n1We will refer to this type of models as traditional deep\nlearning models\nTransformer-NLI model, encodes the sentences\nvia a shared Weighted Transformer module (see\nAhmed et al., 2017 for details). The second model,\ncalled RCNN-NLI, encodes the premise and hy-\npothesis via the RCNN model (see Lai et al.,\n2015). The third model we consider, is the decom-\nposable attention model by Parikh et al. (2016).\nThe fourth model is the ESIM model by Chen et al.\n(2016), which is one of the most popular models\nin the natural language inference task. We will\nnot elaborate on the speciﬁc architecture of the last\ntwo models since readers can refer to the original\npapers for details.\nFor the RQE task, the pre-trained language\nmodels we considered are as follows: (a) the orig-\ninal BERT models (both base and large models);\n(b) the Bio-BERT model by Lee et al. (2019)\nwhich is pre-trained on scientiﬁc literature in bio-\nmedical domain; (c) the Sci-BERT model by Belt-\nagy et al. (2019) which is trained on academic pa-\npers from the corpus of semanticscholar.org; (d)\nMT-DNN models (see Liu et al., 2019b), which\nare based on BERT and go through a multi-task\nlearning procedure on the GLUE benchmark. On\ntop of the transformer encoders from the pre-\ntrained language model, we implement two kinds\nof output modules: (a) linear projection, which\nwill be referred to as LP0, which is to take the hid-\nden state corresponding to the ﬁrst token [CLS]\nof the sentence pair; (b) a more sophisticated\nclassiﬁcation module called stochastic answer net-\nwork (henceforth SAN) proposed by Liu et al.\n(2017). Rather than directly predicting the entail-\nment given the input, SAN maintains a state and\niteratively reﬁnes its predictions.\nWhen implementing the traditional deep learn-\ning models, the Glove embedding (Pennington\net al., 2014) is used. Before training, we use\nthe Uniﬁed Medical System (UMLS) provided by\nprovided by the National Library of Medicine 2 to\nreplace all the abbreviations (e.g., IBS) of a med-\nical concept or entity to its full name, or to the\nsame name that appears in the same pair. We tune\nthe hyper-parameters on the dev set, and report the\nbest performance obtained by each model in Ta-\nble 1.\nAmong the four traditional models, RCNN-\nNLI performs the worst. Although a power-\nful model as shown in Ahmed et al. (2017),\n2https://www.nlm.nih.gov/research/\numls/\n382\nModel valid acc\nWeighted-Transformer-NLI 0.6821\nRCNN-NLI 0.5530\nDecomposable attention 0.6854\nESIM 0.7218\nBERT base + linear projection 0.7815\nBERT base + SAN 0.7119\nBERT large + linear projection 0.7782\nBERT large + SAN 0.7682\nBio-BERT + linear projection 0.4338\nBio-BERT + SAN 0.4305\nSci-BERT + linear projection 0.7547\nSci-BERT + SAN 0.5993\nMT-DNN base + linear projection 0.8378\nMT-DNN base + SAN 0.7715\nMT-DNN large + linear projection 0.7881\nMT-DNN large + SAN 0.7815\nTable 1: performances of different models on the valid set of the RQE task.\nWeighted-Transformer-NLI cannot perform very\nwell on this dataset. The ESIM model performs\nthe best among the four. However the traditional\ndeep learning models cannot perform well enough\nwhen compared with the results on the Round 1\nleader board. We believe the reasons are as fol-\nlows. First, the dataset is relatively small, thus\nmodels like Weighted-Transformer-NLI will im-\nmediately over-ﬁt. 3 Second, the distribution of\ntraining data for RQE task is different from the\ndistributions of the dev and test data. We see\nmost of the pairs in train set have approximately\nequal length, and there are 1, 445 pairs in which\nthe premise and hypothesis are exactly the same.\nMeanwhile, in dev and test sets, the premise is\nusually much longer than the hypothesis.\nWhen compared with traditional deep learning\nmodels, the pre-trained language models perform\nsigniﬁcantly better on the dev set. In addition,\none can see that adding a sophisticated output\nmodule like SAN on top of the pre-trained lan-\nguage model tends to worsen the dev performance.\nAmong all the BERT model family, the MT-DNN\nmodel (base model) performs best, and the orig-\ninal BERT base model performs slightly worse.\nSince the MT-DNN family are BERT models ﬁne-\ntuned on GLUE benchmark via a multi-task learn-\ning mechanism, and in GLUE eight out of nine\n3Readers can refer to Guo et al. (2019) for more detailed\ndiscussions on how transformer models performs unsatisfy-\ningly on medium or small datasets, when directly trained\nfrom scratch.\nlayers to freeze valid acc\n0 0.7782\n1 0.8013\n3 0.7914\n6 0.7881\n9 0.8179\n10 0.8344\n11 0.8378\nTable 2: performances of the MT-DNN base model\nwith linear projection, when different number of lay-\ners are freezed during ﬁne-tuning on the RQE dataset\ntasks are pairwise text modeling tasks, MT-DNN\nare more equipped to model pairwise text classiﬁ-\ncation tasks on different domains than the original\nBERT model. And we can see that MT-DNN base\nperforms better than MT-DNN large, which is in\ncontradiction to the results on the GLUE bench-\nmark reported in Liu et al. (2019b). Sci-BERT\nand Bio-BERT model does not perform well. We\nbelieve the reasons are that the Sci-BERT and Bio-\nBERT models share the same feature that they are\ntrained on scientiﬁc literature, in which the lan-\nguage is more formal and rigid. However, texts in\nRQE is drawn from online questions from medi-\ncal forums, thus Sci-BERT and Bio-BERT are not\nsuitable for this task.\nWe also notice that freezing the lower bi-\ndirectional transformer layers of MT-DNN signif-\nicantly improves the dev set accuracy. In Table 2,\n383\nModel valid acc\nESIM (by Romanov and Shivade, 2018) 0.7440\nInferSent (by Romanov and Shivade, 2018) 0.7600\nBERT base + linear projection 0.8186\nBERT base + SAN 0.8143\nBERT large + linear projection 0.8229\nBERT large + SAN 0.8280\nBio-BERT + linear projection 0.6824\nBio-BERT + SAN 0.6882\nSci-BERT + linear projection 0.8466\nSci-BERT + SAN 0.8251\nMT-DNN base + linear projection 0.8265\nMT-DNN base + SAN 0.8287\nMT-DNN large + linear projection 0.8420\nMT-DNN large + SAN 0.8327\nTable 3: performances of different models on the valid set of the NLI task.\nwe can see that freezing 11 lower layers of the MT-\nDNN base performs best. During training of dif-\nferent models, even traditional deep learning mod-\nels, we notice that a model can easily over-ﬁt on\nthe training set of RQE, ﬁne-tuning the whole lan-\nguage model will introduce much bias into the\nmodel. Meanwhile freezing the lower layers can\nalleviate over-ﬁtting and maintain the generaliza-\ntion ability of the pre-trained models.\n2.2 NLI\nFor the NLI task, we are tasked to identify the\nrelationship R between the premise and the hy-\npothesis, which is among the following three: en-\ntailment, neutral or contradiction. Romanov and\nShivade (2018) has done a thorough investiga-\ntion on how traditional deep learning models like\nESIM and InferSent perform on the original NLI\ndatasets. Thus to save time, we only implement\nwith pre-trained language models for this task.\nThe BERT based models we tried are the same\nas we investigate on the RQE datasets, whose re-\nsults are reported in Table 3. It turns out, the\nBERT-based model signiﬁcantly outperforms the\ntraditional models. MT-DNN models still perform\nquite well, but the Sci-BERT with linear projec-\ntion achieves the highest accuracy on the dev set.\nThe Bio-BERT model still cannot achieve satisfy-\ning results. We ﬁnd that models behave quite dif-\nferently on NLI compared with the RQE datasets.\nFirst, on the NLI dataset, BERT large and the MT-\nDNN large, which is derived from BERT large,\nperform better than their base counterparts, BERT\nbase and MT-DNN base. Second, during tuning\nthe hyper-parameters, we ﬁnd that freezing layers\nleads to performance loss. Third, the SAN output\nmodule does not lead to signiﬁcant performance\nchange except for Sci-BERT, whereas on the RQE\ndataset adding SAN module usually leads to sig-\nniﬁcant performance loss.\n2.3 QA\nOn the basis of the results obtained on RQE and\nNLI task, we found that the MT-DNN models out-\nperform other pre-trained language models. Thus,\nwith limited time, in the QA task we chose to di-\nrectly look into the MT-DNN models on the QA\ndatasets.\nThe QA task requires us not only give a binary\nlabel to an answer, but also rank the answers of\nthe same questions. There are two perspectives\nof treating such a task: classiﬁcation and regres-\nsion. The classiﬁcation model just distinguishes\nwhether the question and the answer match, and\nthe output of Softmax layer can be used to rank the\nanswers. However, the regression model is able\nto predict the matching degree between questions\nand answers, and rank the answers according to\nthe matching degree. The ﬁnal result achieved is a\ncombination of two models.\nFrom the perspective of the classiﬁcation\nmodel, answers with ReferenceScore less than 3\nare given a not entailment label, and the rest are\nlabeled entailment. The dataset obtained with this\ntreatment is called the QA-C dataset. Table 4 re-\nports the performance on the dev set. To align\n384\nModel acc Spearman’s Rank Corr\nMT-DNN base on QA-R 0.8248 0.1478\nMT-DNN large on QA-R 0.8333 0.2054\nMT-DNN base + linear projection on QA-C 0.7479 0.0557\nMT-DNN base + SAN on QA-C 0.7607 -0.0108\nMT-DNN large + linear projection on QA-C 0.8333 0.0803\nMT-DNN large + SAN on QA-C 0.8120 0.2146\nTable 4: performances of different models on the valid set of the QA task. Here accuracy is calculated on the whole\ndev set.\nModel dev acc\nMT-DNN base 0.8378\nMT-DNN base + transfer learning on NLI 0.8220\nMT-DNN large 0.7881\nMT-DNN large + transfer learning on NLI 0.7957\nTable 5: The performance on the RQE dev set, when we apply transfer learning, compared with the performances\nobtained by directly ﬁne-tuning the MT-DNN models on the RQE dataset.\nwith the leader board, we calculated accuracy and\nSpearman’s Rank Correlation Coefﬁcient (hence-\nforth SRCC). As is shown in Table 4, BERT base\ncan achieve accuracy of 0.7478 after ﬁne-tuning.\nHowever, SRCC is 0.057, which is quite poor.\nThe results demonstrate that a binary classiﬁca-\ntion model helps us to get a fair accuracy score,\nbut it omits all the ranking information like Ref-\nerenceRank and ReferenceScore from the original\ndata. Thus the resulting model can not tell whether\nan answer is better than another. Bearing that in\nmind, we decided to introduce a related but dif-\nferent model to specialize in providing ranking in-\nformation, while leave the accuracy metric to the\nclassiﬁcation model.\nThe new model we are introducing treats the\ntask at hand as a regression task. For a sample\ndata, the input is a pair composed of a query and\nan answer. The target value is the relevance score\nbetween the query and the answer, which is de-\nﬁned as follows:\nscore = ReferenceScore +1/ReferenceRank.\n(1)\nThe reciprocal of the ReferenceRank is used to\nenlarge the gaps of relevance scores among differ-\nent answers. The dataset obtained with the above\nmodiﬁcation is called the QA-R dataset. The re-\ngression model is also built on the pre-trained lan-\nguage models by replacing the classiﬁcation out-\nput module with a regression task header (see\nequation (2) of Liu et al., 2019b). Table 4 shows\nthat we can obtain a huge bump on SRCC with\nthe regression model. The best dev SRCC we can\nobtain is 0.148, which is the result of ﬁne-tuning\nthe MT-DNN large model. With a threshold for\nthe relevance score, we can also get the classiﬁ-\ncation label from the regression label. After ad-\njusting the threshold, we can also get accuracy of\n0.8247. Thus, we can conclude that the regres-\nsion model works better in capturing the ranking\ninformation without reducing the accuracy of the\nmodel.\nBy observing the SRCC obtained at each epoch\nduring training, we can see the following phe-\nnomenon: SRCC can improve from 0.125 to 0.273\nafter a single epoch, and suddenly drop to 0.023\non the next one. SRCC seems to be quite unstable,\nwhich will be problematical when making predic-\ntions for the unknown test set. This is a problem\nthat we fail to solve at the end of competition and\nrequires further investigations.\n3 Transfer learning\nWe also experimented with transfer learning for\nthe RQE task. The procedure is to ﬁrst ﬁne-tune\na MT-DNN model on the NLI dataset for a cer-\ntain number of epochs, then the obtained model\nwill further be ﬁne-tuned on the RQE dataset. Our\nmotivation is that ﬁrst ﬁne-tuning on the NLI task\ncan help the pre-trained language model to adapt\nto the medical domain, thus making the training\non RQE more stable. Table 5 reports that after the\ntransfer learning procedure, MT-DNN base model\nperforms worse, but it makes the MT-DNN large\n385\nmodel perform slightly better.\n4 knowledge distillation\nIn this section, we experiment on the idea of\nknowledge distillation (Hinton et al., 2015), to\nfurther boost the performance of single models.\nWe implement knowledge distillation on each task\nseparately.4 The procedure is as follows:\n•train a set of models on each tasks. Follow-\ning Liu et al. (2019a), the set of models are:\nMT-DNN base and MT-DNN large, with dif-\nferent dropout rates ranged in0.1, 0.3, 0.5 for\nthe task speciﬁc output layers, while keep-\ning the hyper-parameters of lower BERT en-\ncoders the same with those in the previous\nsection.\n•ensemble the above models to get a label\nmodel (Ratner et al., 2018) 5. This so-called\nlabel model is constructed by modeling a\ngenerative model over all the label func-\ntions, i.e., the single models, to maximize the\nlog likelihood, give the label matrix (Ratner\net al., 2017). The label model is a general-\nization of the so-called teacher model in (Liu\net al., 2019a), where the teacher model is sim-\nply the average of class probabilities.\n•The end model (or called the student model\nby Liu et al., 2019a) is trained on the soft\ntargets given out by the label model. Here,\ntraining on the soft targets means the cross-\nentropy loss is averaged with the class prob-\nabilities as weights.\n•Inference is the same for end model with\nother normal models.\nIn Table 6, we can see that knowledge distilla-\ntion can signiﬁcantly improve the performance on\nthe NLI task, and can even achieve better results\nthan model ensemble. However, on the RQE task,\nknowledge distillation cannot perform better than\nmodel ensemble, but still outperforms the best sin-\ngle model.\n4Liu et al. (2019a) extends the knowledge distillation to\nmulti-task learning setting, which is a direction we need to\nexplore in future work.\n5There are alternative terminologies for knowledge distil-\nlation. We mainly follow Ratner et al. (2018).\n5 Ensemble\nSince the test set is small, one single model is too\nbiased to achieve great results on the test dataset.\nEnsemble learning is an effective approach to im-\nprove model generalization, and has been used to\nachieve new state-of-the-art results in a wide range\nof natural language understanding (NLU) tasks\n(Devlin et al., 2018, Liu et al., 2017).\nFor the MEDIQA 2019 shared task, we only\nadopt a simple ensemble approach, that is, aver-\naging the softmax outputs from different models,\nor different runs or epochs of the same model, and\nmakes prediction based on these averaged class\nprobabilities. All our submissions follow this en-\nsemble strategy. 6\n6 Re-ranking strategies for the NLI and\nRQE tasks\nThe previous sections demonstrate how deep\nlearning models perform on the task datasets.\nHowever, in order to obtain more competitive re-\nsults, one could adopt some simple heuristics.\nFor the NLI task, after observing the task\ndatasets, we can see that one premise is grouped\nwith three different hypothesis, and the latter are\nlabeled with entailment, neutral and contradiction\nrespectively. We call the three pairs with the same\npremise a group. Our sentence pair model does\nnot know the idea of groups, thus the labels corre-\nsponding to the maximum class probabilities ob-\ntained by soft-max layer can conﬂict with one an-\nother. For example, two pairs in the same group\nmay both be labeled as entailment. To eliminate\nthe above conﬂicts, we adopt the following heuris-\ntic post-processing procedure:\n•obtain the label predictions directly from the\nsoftmax output. If there is no conﬂict in a\ngroup, accept the predictions. Otherwise, in\nthis group:\n•Give the contradiction label to the pair with\nthe highest score for this label\n•Between the remaining two pairs, decide\nwhich one should get theneutral label via the\nscores for this label\n6We deﬁnitely can try some more sophisticated ensem-\nble methods, but we believe experimenting different learn-\ning strategies like MTL and knowledge distillation is more\nmeaningful for research purpose, and is in alignment with the\nobjective of the MEDIQA 2019 share tasks.\n386\nModel NLI RQE\nbest single model 0.8466 0.8378\nmodel ensemble 0.8638 0.8477\nknowledge distillation 0.8667 0.8411\nTable 6: Comparison of performances on the dev sets, among the best single model, ensemble model and the\nmodel obtained by knowledge distillation.\n•the remaining pair get the entailment label\nFor the RQE task, since the label is binary, and\nthe number of pairs in a group in this task varies,\nthe re-ranking heuristic is a little different, which\nis elaborated as follows.\n•obtain the score of the entailment label from\nthe model\n•for each group, rank the pairs by their scores.\n•denote the number of pairs in a group as n,\nthen we directly label the lastmax(1, [n/2]−\n1) as negative pairs. and the top pair as posi-\ntive pair\n•For the rest of pairs, we choose a threshold\nt, if the score of a pair is higher than t, it is\nlabeled entailment, otherwise it is labeled as\nnot entailment. We choose the threshold to\nobtain the highest accuracy on the dev set\n7 Submission results\nThis section discusses the submission results on\nthe leader boards.\nFirst, let us look at the submission history on\nthe RQE task (presented here in Table 7). The\nﬁrst submission is a single MT-DNN base model\ntrained only on the training data, with re-ranking.\nOn the second submission, we add the available\ndev set in, and re-train all the models. The en-\nsemble of a MT-DNN base and a MT-DNN large\nafter re-ranking push the test accuracy to 0.736.\nThen we tryout transfer learning on the third run,\ntwo runs of MT-DNN large, which go through the\ntransfer learning process described in Section 3,\nachieves 0.745 after re-ranking. Adding the end\nmodel after knowledge distillation to the combina-\ntion in the third run makes the performance drops\nslightly to 0.740. For the ﬁnal submission, we\njust ensemble all the models available, and achieve\n0.749 on the test set, which ranks the ﬁrst on the\nRQE task.\nTable 8 presents the submission records on the\nNLI task. On the ﬁrst submission, we experi-\nment the model obtained by knowledge distilla-\ntion, which obtains 0.865 on accuracy. The sec-\nond submission, we use a single MT-DNN large\nﬁne-tuned on the train set and post-processed for\nre-ranking. The accuracy is 0.916 for this sub-\nmission. Then the ensemble of four models, the\n8-th epoch of 2 different runs of MT-DNN large,\nthe 10-th epoch of 2 different runs of Sci-BERT,\nachieves an accuracy of 0.946 after re-ranking.\nThe ﬁnal submission combines MT-DNN large,\nSci-BERT, MT-DNN large after knowledge distil-\nlation, obtains 0.966 after re-ranking, which ranks\nthe third on the leader board.\nFor the QA task, the ﬁrst two submissions are\nbased on a single MT-DNN large model ﬁne-tuned\non QA-R data set, chosen from two different train-\ning epochs. The ﬁrst submission with accuracy\nof 0.73 is chosen because in this epoch of train-\ning, we achieved the best Spearman’s rho result\non the dev dataset; Similarly,the second submis-\nsion with accuracy of 0.733 is chosen at the epoch\nwhere we achieved best ACC result on the dev\ndataset. From the third round, we started apply-\ning ensemble strategy by considering some well\nperforming epochs at different runs together. The\ntwo submissions with accuracy of 0.774 and 0.777\nare the results of different processing strategies:\nmax score and mean score. According to the re-\nsults obtained, we ﬁnd that ”max score” strategy\nperforms slightly better on SRCC, while ”mean\nscore” works better on ACC.\n8 Conclusion and discussions\nTo conclude, we have shown that domain adapta-\ntion with the pre-trained language models achieves\nsigniﬁcant improvement over traditional deep\nlearning models on the MEDIQA 2019 shared\ntasks. We also experimented transfer learning\nfrom the NLI task to the RQE task. Knowl-\nedge distillation obtains a single model which sig-\nniﬁcantly outperforms the single models trained\n387\nSubmission No. test acc details\n1 0.675 1 * MT-DNN base (trained on train set) + re-rank\n2 0.736 1 * MT-DNN base + 1 * MT-DNN large + re-rank\n3 0.745 2 * MT-DNN large (TL) + re-rank\n4 0.740 1 * MT-DNN large (KD) + 2 * MT-DNN large (TL) + re-rank\n5 0.749 2 * MT-DNN base + 2 * MT-DNN large (TL)\n+ 1 * MT-DNN large (KD) + 1 * MT-DNN large\n+ re-rank\nTable 7: The submission results on the RQE task. Multiplication symbol ”*” here means multiple runs or epochs\nof the same model (with different random seed). ”TL” means the model go through transfer learning on the NLI\ntask. ”KD” means the model is obtained via knowledge distillation. Without declaration, all the models here are\ntrained on the train and dev set.\nSubmission No. test acc details\n1 0.865 1 * MT-DNN large (KD)\n2 0.916 1 * MT-DNN large (on train set) + re-rank\n3 0.946 2 * MT-DNN large + 2 * Sci-BERT + re-rank\n4 0.966 4 * MT-DNN large + 4 * Sci-BERT\n+ 2 * MT-DNN large (KD) + re-rank\nTable 8: The submission records on the NLI task. Multiplication symbol ”*” here means multiple runs or epochs\nof the same model (with different random seed). ”KD” means the model is obtained via knowledge distillation.\nWithout declaration, all the models here are trained on the train and dev set.\nSubmission No. test acc test Spearman’s rho details\n1 0.730 0.236 MT-DNN large (epoch with best training SRCC)\n2 0.736 0.204 MT-DNN large (epoch with best training ACC)\n3 0.774 0.22 MT-DNN large ensemble(rank by max socre)\n4 0.777 0.18 MT-DNN large ensemble(rank by mean socre)\n5 0.772 0.204 MT-DNN large ensemble(rank by mean socre)\nTable 9: The submission results on the QA task.\nin the usual way. Our submission results, al-\nthough including model ensemble and re-ranking,\nare strong demonstration of the power of language\nmodel pre-training, transfer learning and knowl-\nedge distillation.\nHowever, due to the limited time and the fact\nthat we participate all three tasks at once, we\nhaven’t exhaustively explore all the possible ways\nto boost the performance on the leader board,\ne.g., utilizing external sources such as medical\nknowledge bases to rule out false positive answers.\nMulti-task learning is also a direction that we need\nto pay more attention to.\nIn addition, the heuristics adopted in the re-\nranking strategies resemble the relevance ranking\ntask (Huang et al., 2013), where one compares\ndifferent pairs in a group to obtain the ﬁnal deci-\nsions. Due to time constraint, we didn’t implement\na pairwise relevance ranking model on top of the\nMT-DNN model, but this research direction will\nbe investigated by us in future work.\nReferences\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted Transformer Network\nfor Machine Translation. arXiv e-prints, page\narXiv:1711.02132.\nIz Beltagy, Arman Cohan, and Kyle Lo. 2019.\nSciBERT: Pretrained Contextualized Embed-\ndings for Scientiﬁc Text. arXiv e-prints, page\narXiv:1903.10676.\nAsma Ben Abacha, Chaitanya Shivade, and Dina\nDemner-Fushman. 2019. Overview of the mediqa\n2019 shared task on textual inference, question en-\ntailment and question answering. In Proceedings of\nthe BioNLP 2019 workshop, Florence, Italy, August\n1, 2019. Association for Computational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\n388\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2016. Enhanced LSTM for\nNatural Language Inference. arXiv e-prints, page\narXiv:1609.06038.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training\nof Deep Bidirectional Transformers for Lan-\nguage Understanding. arXiv e-prints , page\narXiv:1810.04805.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yun-\nfan Shao, Xiangyang Xue, and Zheng Zhang.\n2019. Star-Transformer. arXiv e-prints, page\narXiv:1902.09113.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the Knowledge in a Neural Network.\narXiv e-prints, page arXiv:1503.02531.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In Proceedings of the 22nd ACM\ninternational conference on Information & Knowl-\nedge Management, pages 2333–2338. ACM.\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.\nRecurrent convolutional neural networks for text\nclassiﬁcation.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. arXiv e-prints , page\narXiv:1901.08746.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Improving Multi-Task Deep Neu-\nral Networks via Knowledge Distillation for Natu-\nral Language Understanding. arXiv e-prints, page\narXiv:1904.09482.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-Task Deep Neural Net-\nworks for Natural Language Understanding. arXiv\ne-prints, page arXiv:1901.11504.\nXiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng\nGao. 2017. Stochastic Answer Networks for Ma-\nchine Reading Comprehension. arXiv e-prints, page\narXiv:1712.03556.\nAnkur P. Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A Decomposable Attention\nModel for Natural Language Inference. arXiv e-\nprints, page arXiv:1606.01933.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532–\n1543.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1:8.\nAlexander Ratner, Stephen H. Bach, Henry Ehren-\nberg, Jason Fries, Sen Wu, and Christopher R ´e.\n2017. Snorkel: Rapid Training Data Creation\nwith Weak Supervision. arXiv e-prints , page\narXiv:1711.10160.\nAlexander Ratner, Braden Hancock, Jared Dunnmon,\nFrederic Sala, Shreyash Pandey, and Christopher\nR´e. 2018. Training Complex Models with Multi-\nTask Weak Supervision. arXiv e-prints, page\narXiv:1810.02840.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from Natural Language Inference in\nthe Clinical Domain. arXiv e-prints , page\narXiv:1808.06752.\nXu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-\nYan Liu. 2019. Multilingual Neural Machine Trans-\nlation with Knowledge Distillation. arXiv e-prints,\npage arXiv:1902.10461.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8360425233840942
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6803074479103088
    },
    {
      "name": "Transformer",
      "score": 0.6800103783607483
    },
    {
      "name": "Task (project management)",
      "score": 0.6118882894515991
    },
    {
      "name": "Transfer of learning",
      "score": 0.6076894998550415
    },
    {
      "name": "Machine learning",
      "score": 0.5719547867774963
    },
    {
      "name": "Natural language processing",
      "score": 0.543993353843689
    },
    {
      "name": "Language model",
      "score": 0.5364587306976318
    },
    {
      "name": "Test set",
      "score": 0.506827175617218
    },
    {
      "name": "Process (computing)",
      "score": 0.5033900141716003
    },
    {
      "name": "Distillation",
      "score": 0.48311710357666016
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.469229519367218
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4672679007053375
    },
    {
      "name": "Ensemble learning",
      "score": 0.45720013976097107
    },
    {
      "name": "Knowledge transfer",
      "score": 0.4116651117801666
    },
    {
      "name": "Engineering",
      "score": 0.061429738998413086
    },
    {
      "name": "Voltage",
      "score": 0.05795750021934509
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Knowledge management",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}