{
    "title": "PlaTe: Visually-Grounded Planning With Transformers in Procedural Tasks",
    "url": "https://openalex.org/W3199332348",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2750616529",
            "name": "Sun Jian-Kai",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4222266840",
            "name": "Huang, De-An",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2104247429",
            "name": "Lu Bo",
            "affiliations": [
                "Soochow University"
            ]
        },
        {
            "id": "https://openalex.org/A4221787128",
            "name": "Liu, Yun-Hui",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2962448985",
            "name": "Zhou, Bolei",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        },
        {
            "id": "https://openalex.org/A4221766847",
            "name": "Garg, Animesh",
            "affiliations": [
                "Nvidia (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2575705757",
        "https://openalex.org/W2526050071",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W6752217255",
        "https://openalex.org/W2963523627",
        "https://openalex.org/W6803039479",
        "https://openalex.org/W2963368541",
        "https://openalex.org/W3203256294",
        "https://openalex.org/W6747866816",
        "https://openalex.org/W2957775769",
        "https://openalex.org/W2477205648",
        "https://openalex.org/W6798219303",
        "https://openalex.org/W2964055695",
        "https://openalex.org/W3115021520",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6765493377",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6756256016",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6640212811",
        "https://openalex.org/W6769673253",
        "https://openalex.org/W3106768499",
        "https://openalex.org/W6753611882",
        "https://openalex.org/W1510247770",
        "https://openalex.org/W2979490629",
        "https://openalex.org/W6803843005",
        "https://openalex.org/W2594270457",
        "https://openalex.org/W2960406243",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W2969818684",
        "https://openalex.org/W2981635073",
        "https://openalex.org/W2804010078",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2952132648",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3090536218",
        "https://openalex.org/W2969776042",
        "https://openalex.org/W2963642716",
        "https://openalex.org/W4287079536",
        "https://openalex.org/W2995301359",
        "https://openalex.org/W2954460650",
        "https://openalex.org/W2903388201",
        "https://openalex.org/W4289744728",
        "https://openalex.org/W2784025607",
        "https://openalex.org/W3209548326",
        "https://openalex.org/W3158726102",
        "https://openalex.org/W2751973545",
        "https://openalex.org/W3211174185",
        "https://openalex.org/W3208673210",
        "https://openalex.org/W2900152462",
        "https://openalex.org/W2982437637",
        "https://openalex.org/W4289294484",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3208836296",
        "https://openalex.org/W3130800560",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2953326790",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3090901006",
        "https://openalex.org/W2776202271",
        "https://openalex.org/W3123650687"
    ],
    "abstract": "In this work, we study the problem of how to leverage instructional videos to facilitate the understanding of human decision-making processes, focusing on training a model with the ability to plan a goal-directed procedure from real-world videos. Learning structured and plannable state and action spaces directly from unstructured videos is the key technical challenge of our task. There are two problems: first, the appearance gap between the training and validation datasets could be large for unstructured videos; second, these gaps lead to decision errors that compound over the steps. We address these limitations with Planning Transformer (PlaTe), which has the advantage of circumventing the compounding prediction errors that occur with single-step models during long model-based rollouts. Our method simultaneously learns the latent state and action information of assigned tasks and the representations of the decision-making process from human demonstrations. Experiments conducted on real-world instructional videos and an interactive environment show that our method can achieve a better performance in reaching the indicated goal than previous algorithms. We also validated the possibility of applying procedural tasks on a UR-5 platform. We make our code publicly available and support academic research purposes.",
    "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022 1\nPlaTe: Visually-Grounded Planning with\nTransformers in Procedural Tasks\nJiankai Sun1, De-An Huang 2, Bo Lu 3, Yun-Hui Liu4, Bolei Zhou 5, Animesh Garg 2,6\nAbstract—In this work, we study the problem of how to\nleverage instructional videos to facilitate the understanding of\nhuman decision-making processes, focusing on training a model\nwith the ability to plan a goal-directed procedure from real-world\nvideos. Learning structured and plannable state and action spaces\ndirectly from unstructured videos is the key technical challenge\nof our task. There are two problems: ﬁrst, the appearance gap\nbetween the training and validation datasets could be large for\nunstructured videos; second, these gaps lead to decision errors\nthat compound over the steps. We address these limitations\nwith Planning Transformer (PlaTe), which has the advantage of\ncircumventing the compounding prediction errors that occur with\nsingle-step models during long model-based rollouts. Our method\nsimultaneously learns the latent state and action information of\nassigned tasks and the representations of the decision-making\nprocess from human demonstrations. Experiments conducted\non real-world instructional videos show that our method can\nachieve a better performance in reaching the indicated goal\nthan previous algorithms. We also validated the possibility\nof applying procedural tasks on a UR-5 platform. Please see\npair.toronto.edu/plate-planner for additional details.\nIndex Terms—Deep Learning for Visual Perception, Task\nPlanning, Embodied Cognitive Science\nI. I NTRODUCTION\nI\nNTELLIGENT reasoning in embodied environments re-\nquires that an agent has explicit representations of parts or\naspects of its environment to reason about [1]. As a generic\nreasoning application, action planning and learning are crucial\nskills for cognitive robotics. Planning, in the traditional AI\nsense, means deliberating about a course of actions for an\nagent to take for achieving a given set of goals. The desired\nplan is a set of actions whose execution transforms the initial\nsituation into the goal situation (goal situations need not be\nunique). Normally, actions in a plan cannot be executed in\narbitrary sequence, but have to obey an ordering, ensuring\nthat all preconditions of each action are valid at the time of its\nexecution. In practice, there are two challenges for planning.\nFirst, typically not all information that would be needed is\navailable. Planning is meant for real environments in which\nmany parameters are unknown or unknowable. Second, even if\nManuscript received: September 9, 2021; Revised December 7, 2021;\nAccepted January 10, 2022.\nThis paper was recommended for publication by Editor Cesar Cadena upon\nevaluation of the Associate Editor and Reviewers’ comments.\n1Stanford University, Stanford, California, United States of America, 94305.\n2NVIDIA, Cupertino, CA, United States of America, 95014.\n3Robotics and Microsystems Center, School of Mechanical and Electric\nEngineering, Soochow University, Suzhou, Jiangsu, China.\n4The Chinese University of Hong Kong, Hong Kong.\n5University of California, Los Angeles, USA.\n6University of Toronto & Vector Institute, Canada.\nFig. 1: PlaTe Overview. Given a visual observation as start and goal,\nthe encoder f (·) extracts the feature about the planning trajectory.\nThis transformer-based procedure planning model is responsible for\nlearning plannable latent representations ˆs and actions ˆa.\neverything for a complete planning were known, then it would\nvery likely be so computationally intensive that it would run\ntoo slowly in the real world. Thus, many planning systems\nand their underlying planning algorithms accept the restric-\ntive assumptions of information completeness, determinism,\ninstantaneousness, and idleness [1], [2].\nProcedure planning in instructional videos [3] (as shown in\nFigure 1) aims to make goal-conditioned decisions by planning\na sequence of high-level actions that can bring the agent\nfrom current observation to the goal. Planning in instructional\nvideos is a meaningful task since the ability to perform\neffective planning is crucial for an instruction-following agent.\nAlthough learning from instructional videos is natural to\nhumans, it is challenging for the AI system because it requires\nunderstanding human behaviors in the videos, focusing on\nactions and intentions. How to learn structured and plannable\nstate and action spaces directly from unstructured videos is\nthe key technical challenge of our task.\nIt is crucial for autonomous agents to plan for complex\ntasks in everyday settings from visual observations [3]. Al-\nthough reinforcement learning provides a powerful and general\nframework for decision making and control, its application in\npractice is often hindered by the need for extensive feature and\nreward engineering [4], [5]. Moreover, deep RL algorithms\nare often sensitive to factors such as reward sparsity and\nmagnitude, making well-performing reward functions partic-\nularly difﬁcult to engineer. In many real-world applications,\nspecifying a proper reward function is difﬁcult.\nIn this paper, we proposed a new framework for procedure\nplanning from visual observations. We address these limi-\ntations with a new formulation of procedure planning and\narXiv:2109.04869v2  [cs.RO]  2 Mar 2022\n2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022\nnovel algorithms for modeling human behavior through a\nTransformer-based planning network PlaTe. Our method si-\nmultaneously learns the high-level action planning of assigned\ntasks and the representations of the decision-making process\nfrom human demonstrations.\nWe summarize our contributions as follows:\n• We proposed a novel method, Pla nning T ransformer\nnetwork (PlaTe), for procedure planning in instructional\nvideos task, which enjoys the advantage of long-term\nplanning. We introduce some critical design choices that\nassist in learning cross-modal correspondence and, more\nimportantly, improve the accuracy of generated planning\nsequences.\n• We integrate Beam Search to PlaTe to prevent it from\nlarge search discrepancies, and reduce the performance\ndegradation.\n• Experimental results show that our framework outper-\nforms the baselines in the procedure planning task on\nCrossTask, a real-world dataset. We also validated the\npossibility of applying procedural tasks on a real UR-5\nplatform.\nII. R ELATED WORK\nSelf-Attention and Transformer. Transformer-based archi-\ntectures, eschew the use of recurrence in neural networks\nand instead trust entirely on self-attention mechanisms to\ndraw global dependencies between inputs and outputs. Self-\nattention [6] is particularly suitable for procedure planning,\nwhich can be seen as a sequence modeling task. Compared\nwith Recurrent Neural Networks (RNNs), long short-term\nmemory (LSTM) [7] and gated recurrent neural networks [8],\nthe advantages of self-attention includes avoiding compress-\ning the whole past into a ﬁxed-size hidden state, less total\ncomputational complexity per layer, and more parallelizable\ncomputations. In this paper, thanks to Transformers’ compu-\ntational efﬁciency and scalability, we explore the possibility of\nmarrying Transformer-based architecture for procedure plan-\nning.\nLearning to Plan from Pixels. Another related work is\nlearning dynamics models for model-based RL [9], [10], [11].\nRecent works have shown that deep networks can learn to plan\ndirectly from pixel observations in domains such as table-top\nmanipulation [12], [13], [14], navigation [15], [16], [17], and\nlocomotion in joint space [18]. Universal Planning Networks\n(UPN) [13] assumes the action space to be differentiable and\nuses a gradient descent planner to learn representations from\nexpert demonstrations. Prior work [3] proposes the conjugate\ndynamics model to expedite the latent space learning, but\nsuffers from compounding error. While they attempt to solve\nthe same problem, however there exist some key methodical\ndifferences. Mainly, PlaTe uses a transformer model to plan\nactions, which results in improved success rate, accuracy, and\nmIoU. Further, even with a good planner, due to the stochastic\nnature of predictions, planning is often low quality with a\nsingle roll-out. We address this issue with a beam search\nmethod. The adversarial training scheme proposed by Bi et\nal. [19] is a complex, multi-stage pipeline. In contrast, we\nimplement an end-to-end transformer architecture, simple but\nefﬁcient.\nLearning from Instructional Videos. The interest has dra-\nmatically increased in recent years in understanding human\nbehaviors by analyzing instructional videos [20], [21], [22].\nEvent discovery tasks such as action recognition and temporal\naction segmentation [23], [24], [25], [26], state understand-\ning [27], video prediction [28] and video summarization / cap-\ntioning [29], [20] study recognition of human actions in video\nsequences. The other works [30] perform egocentric action an-\nticipation model the relationships between past, future events,\nand incomplete observations. Action label prediction [31], [32]\naddresses the problem of anticipating all activities within a\ntime horizon. However, the correct answer is often not unique\ndue to the large uncertainty in human actions.\nIII. O UR METHOD : PLATE\nA. Problem Setup\nWe consider a similar setup to [3]: given the start visual\nobservation ot and a visual goal og that indicates the desired\noutcome for a particular task. During training, we have access\nto the observation-action pairs {(ot:g,at:g)}∼ πE that were\ncollected by an expert attempting to reach the goals. When\ntesting, only the start visual observation ot and a visual goal\nog are given. Our objective is to plan a sequence of actions\n[ ˆat ,..., ˆat+T−1] that can bring the underlying state of ot to\nthat of og. T is the horizon of planning, which means the\nnumber of task-level action steps the model is allowed to take.\nFigure 1 shows a goal-oriented plannable example where the\nintermediate steps of performing a complex planning task are\nplanned.\nOur key insight is that the compounding error can be\nreduced by jointly learning the state and action representation\nwith Transformer-based architecture. As shown in the overall\narchitecture in Figure 2, the procedure planning problem\np( ˆa1:T |o1,og) is formulated as maximizing the probability\np( ˆa1:T |o1,og)\n=Pr(ˆs1|o1)Pr(ˆsg|og)\nT\n∏\nt=1\nPr( ˆat |ˆst , ˆat−1, ˆsg)Pr(ˆst+1|ˆst , ˆat , ˆsg). (1)\nIn the following sections, we ﬁrst discuss how to encode\nthe latent semantic representation. Then, we will introduce\nhow to solve the long-term procedure planning task with\ntransformer-based architecture. Lastly, we will discuss how to\napply learned representation to solve the procedure planning\nby integrating Beam Search.\nB. Latent Semantic Representation\nFirst, we use the state encoder ˆ s = f (o) that encodes the\nvisual observation to a latent semantic representation, then\nadded with learnable positional encoding, before they were\ninput into the transformer layers.\nThe remaining question is: how to learn a planning model\np( ˆa1:T , ˆs1:T |ˆs1, ˆsg) to plan the action sequence and correspond-\ning latent state representation? We assume the underlying pro-\ncess in Figure 1 is a fully observable goal-conditioned Markov\nSUN et al.: PLATE: VISUALLY-GROUNDED PLANNING WITH TRANSFORMERS IN PROCEDURAL TASKS 3\nFig. 2: PlaTe Framework. Given the start and goal visual observations, encoder f outputs the latent representations ˆs. We set ˆa0 = 0. The\nlatent representation and predicted action are inferred using transformer-based action prediction model h(·) and state prediction model g(·).\nThe goal state sg is an input to both models. During training, the ground-truth action a and state s are given. During inference, we use Beam\nSearch to enhance the trained h(·),g(·). The right part shows the Transformer architecture we used for the planning model. Query(), Key(),\nand Value() are linear layers.\nDecision Process ( S,A ,T ), where S is the state space,\nA is the action space, T : S ×A →S is the unknown\ntransition probability distribution. We denote h( ˆat |ˆst , ˆat−1, ˆsg)\nas the action prediction model conditioned on the current\nstate, previous action, and goal, g(ˆst+1|ˆst , ˆat , ˆsg) as the state\nprediction model conditioned on the previous state, goal, and\nprevious action. They plan the sequence of actions and hidden\nstates from the initial state to the goal state. In this way, we\nare able to factorize the planning model p( ˆa1:T , ˆs1:T |ˆs1, ˆsg) as:\np( ˆa1:T , ˆs1:T |ˆs1, ˆsg)\n=\nT\n∏\nt=1\nh( ˆat |ˆst , ˆat−1, ˆsg)g(ˆst+1|ˆst , ˆat , ˆsg), (2)\nwhere we use the convention that ˆs0 = 0, ˆa0 = 0. The data is in\nthe form of tuples ( st , at , st+1). We assume to start at s1, and\nreach a goal sg, in T timesteps. We need to generate an action\nsequence a1:T of length T, which yields a state trajectory of\nlength T +1.\nC. Transition Transformer\nWe propose a transformer-based network architecture that\ncan learn the action-state correlation and generate planning\nsequences. The overview of this architecture is shown in\nFigure 2. Our design choices are explained in detail below.\nWe introduce two cross-modal transformers: the action\ntransformer h( ˆat |ˆst , ˆat−1, ˆsg), which learns the correspondence\nbetween previous action feature ˆat−1 and state feature ˆst , ˆsg\nand generates the action prediction ˆ at ; the state transformer\ng(ˆst+1|ˆst , ˆat , ˆsg), which learns the correspondence between the\nstate feature ˆst , ˆsg and action feature ˆ at and generates the\nfuture state prediction ˆst+1. The model takes as input the whole\nsequence during training and all past pairs of state-action\nduring inference, similarly to how transformer-like models\nusually work.\nSpeciﬁcally, the output of the attention layer, the context\nvector CCC is computed using the query vector QQQ and the key\nKKK value VVV pair from the input with an upper triangular look-\nahead mask MMM via\nCCC = Attn(QQQ,,,KKK,,,VVV,,,MMM)\n= softmax\n(QQQKKKT +MMM√\nD\n)\nVVV, (3)\nwhere D is the number of channels in the attention layer. The\nlook-ahead-mask MMM is a triangular matrix to ensure that the\npredictions can depend only on the known outputs before.\nD. Beam Search in Procedure Planning\nGiven a transformer planning model Pθ parameterized by θ\nand an input x, which contains the information of current state,\nprevious action step, and goal state, the problem of procedure\nplanning task consists of ﬁnding a action sequence ˆa such that\nˆa = argmaxa∈A Pθ (a|x), where A is the set of all sequences.\na = {ˆa1,···, ˆaT }can be regarded as a sequence of tokens from\nvocabulary V , where T is the length of the sequence ˆa. Then\nPθ (ˆa|x) can be factored as\nlogPθ (ˆa|x) =log\nT\n∏\nt=1\nPθ ( ˆat |{ˆs1, ˆa0};···;{ˆst , ˆat−1}; ˆsg)\n=\nT\n∑\nt=1\nlogPθ ( ˆat |{ˆs1, ˆa0};···;{ˆst , ˆat−1}; ˆsg).\n(4)\nThe discrepancy gap is the difference in log-probability\nbetween the most likely token and the chosen token [33], [34].\nAt time step t, the discrepancy gap is\nmax\nˆa∈V\n[logPθ (ˆa|{ˆs1, ˆa0};···;{ˆst , ˆat−1}; ˆsg)\n−logPθ ( ˆat |{ˆs1, ˆa0};···,{ˆst , ˆat−1}; ˆsg)].\n(5)\n4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022\nAlgorithm 1 PlaTe: Planning Inference Phase\nInput: sequence x, Beginning Of Sequence BOS, End\nOf Sequence EOS, buffer B, buffer size N , score function\nscore(·,·), maximum sequence length nmax, maximum beam\nsize k, planning model p(ˆsi\nt , ˆai\nt |ˆsi\nt−1, ˆai\nt−1, ˆsi\nT )\nOutput: searched sequence B.max()\nB0 ←{< 0,BOS >}\nfor t ∈{1,···,nmax −1}do\nB ←/ 0\nfor < w,(ˆsi\nt−1, ˆai\nt−1, ˆsi\nT ) >∈Bt−1 do\nif ˆsi\nt−1.last() =EOS then\nB.add(< w,(ˆsi\nt−1, ˆai\nt−1, ˆsi\nT ) >)\ncontinue\nend if\n(ˆsi\nt , ˆai\nt ) ←p(ˆsi\nt , ˆai\nt |ˆsi\nt−1, ˆai\nt−1, ˆsi\nT )\nw ←score(ˆsi\nt , ˆai\nt , ˆsi\nT )\nB.add(< w,(ˆsi\nt , ˆai\nt , ˆsi\nT ) >)\nend for\nBt ←B.top(k)\nend for\nreturn B.max()\nTo avoid long-term procedure planning from signiﬁcant search\ndiscrepancies, we introduce the discrepancy-constrained Beam\nSearch during the inference phase of procedure planning. The\naction log-probability output by action prediction model h(·)\nis used as the score function. The inference algorithm is shown\nas Algorithm 1. In this way, we can reduce the performance\ndegradation.\nE. Learning\nAs shown in Figure 2, we have three main components\nto optimize: state encoder f , action prediction model h, and\nstate prediction model g. We refer to the expert trajectory as\nτE = {(sE\nt ,aE\nt )}and predicted trajectory τ = {(ˆst , ˆat )}as state-\naction pairs visited by the current planning model.\nWe optimize by descending the gradient in Equation 6.\nmin\nθ\nT\n∑\nt=1\n||ˆst −sE\nt ||2 +CE( ˆat ,aE\nt ) (6)\nwhere CE is the cross-entropy loss. In training, T-step se-\nquence is output once. In testing, single-step inference is made\nwith Beam Search.\nIV. E XPERIMENTS\nIn our experiments, we aim to answer the following ques-\ntions: (1) Is PlaTe efﬁcient and scalable to procedure plan-\nning tasks? (2) Can PlaTe learn to plan on the interactive\nenvironment? To answer Question 1, we evaluate PlaTe on\nCrossTask, a real-world ofﬂine instructional video dataset. We\nshow procedure planning with our algorithm performs better\non the CrossTask dataset than previous methods. To answer\nQuestion 2, we evaluate our method on a real-world UR-5\nrobot arm platform.\nA. Experimental Setup\nDatasets. We ﬁrst evaluate PlaTe on an instructional video\ndataset CrossTask [21]. For real-world UR-5 experiments, we\ncollect a UR-5 Reaching Datasetwhich consists of 100 trajec-\ntories (2150 ﬁrst-person-view RGB image and corresponding\naction pairs) as a training set and evaluate on a real UR-5\nplatform.\nImplementation Details. We compare with the baselines\nin [3] with the metrics such as success rate, accuracy, and\nmIoU. We use the Transformer architecture [6] as the transition\nmodel with 8 self-attention layers and 8 heads. The transition\nmodel is two-headed: one for action prediction the other for\nstate prediction. Let Ls be the hidden dimension for state and\nLa be the embedding dimension for action. The state encoder\nin our model is two fully-connected layers with [64, Ls] units\nin each layer and Leaky-ReLU as non-linearity function. We\nencode (ot ,og) to be Ls-dim st . at is encoded (in transformer\nterminology: ”tokenized”) to be a La-dim embedding and\nconcatenated together with st to be provided as input to the\ntransformer. The size of the input of the transformer is Ls +La\nand the size of the output of the transformer is also Ls + La.\nst+1 is the ﬁrst Ls-dim features split from transformer output.\nat+1 is decoded from the rest of transformer output. In our\nexperiments, Ls = 32 and La is the total number of possible\nactions. For CrossTask experiments,La = 105. One-hot vectors\nare used for this action classiﬁcation purpose. During training,\nall models are optimized by Adam [35] with the starting\nlearning rate of 10 −4. We train our model for 200 epochs\non a single GTX 1080 Ti GPU.\nB. Evaluating Procedure Planning on CrossTask\nFirst, we choose a real-world instructional video dataset\nCrossTask [21] to conduct our experiments. CrossTask com-\nprises 2 ,750 videos (212 hours). Each video depicts one of\nthe 18 primary long-horizon tasks. To test the trained agent’s\ngeneralization capability, for the videos in each task, we\nrandomly divide the videos in each task into 70% /30% splits\nfor training and testing. Each video can be regarded as a\nsequence of images Vi (where i is the index of frames) that\nhave annotated with a sequence of action labels aj and each\naction starts at frame index sj and ends at frame index ej. Same\nas the setup of [3]: we choose frames around the beginning\nof the captions Vst −δ/2:st +δ/2 as ot , caption description at as\nthe semantic meaning of action, and images nearby the end\nVet −δ/2:et +δ/2 as the next observation ot+1. Here, δ controls\nthe duration of each observation, and we set δ = 2 for all\ndata we have used in our paper. Our state-space S is the\npre-computed features provided in CrossTask: each second\nof the video is encoded into a 3 ,200-dimensional feature\nvector , which is a concatenation of the I3D [36], Resnet-\n152 [37], and audio VGG features [38]. The action space A\nis constructed by enumerating all combinations of predicates\nand objects, which provides 105 action labels and is shared\nacross all 18 tasks. Our method is suitable for modeling longer\ntrajectories, but we restrict the experiments to horizontal\nlengths T = 3 ∼4 to maintain a consistent comparison with\nstate-of-the-art methods.\nSUN et al.: PLATE: VISUALLY-GROUNDED PLANNING WITH TRANSFORMERS IN PROCEDURAL TASKS 5\nFig. 3: Qualitative results of procedure planning on CrossTask . Qualitative results of procedure planning for Make Pancakes. The top\nrow describes the correct action sequence required to ”make pancakes”. To examine our method’s robustness, We vary the start and goal\nobservations to evaluate our method. The results show that our approach is robust to perform planning within different stages in the video.\nTABLE I: CrossTask Results. Our model outperforms baselines in terms of success rate, accuracy, and mIoU.\nMethod Prediction Length T = 3 Prediction Length T = 4\nSuccess Rate (%) Accuracy (%) mIoU (%) Success Rate (%) Accuracy (%) mIoU (%)\nRandom <0.01 0.94 1.66 <0.01 0.83 1.66\nRB [29] 8.05 23.30 32.06 3.95 22.22 36.97\nWLTDO [18] 1.87 21.64 31.70 0.77 17.92 26.43\nUAAA [32] 2.15 20.21 30.87 0.98 19.86 27.09\nUPN [13] 2.89 24.39 31.56 1.19 21.59 27.85\nDDN [3] 12.18 31.29 47.48 5.97 27.10 48.46\nPlaTe (Ours) 111666...000000 333666...111777 666555...999111 111444...000000 333555...222999 555555...333666\nRecall that in procedure planning, given the start and\ngoal observations o1 and og, the agent needs to output a\nvalid procedure {a1,···,aT }to reach the speciﬁed goal. As\nillustrated in Table I, as instructional videos’ action space\nis not continuous, the gradient-based planner of UPN cannot\nwork well. By introducing Beam Search, our PlaTe has a better\nperformance in terms of success rate, accuracy, and mIoU.\nBy designing a model with transformer-based components, we\nshow that our model outperforms all the baseline approaches\non real-world videos.\nIn Figure 3, we visualize some examples of the predicted\nprocedure planning results on CrossTask, where the task is to\nMake Pancake. Our model is able to predict a sequence of ac-\ntions with correct ordering. Speciﬁcally, the most challenging\nstep in Make Pancakeis the ”add ﬂour” and ”add sugar” step,\nwhere visual differences are not signiﬁcant, and it can only be\ninferred from context and sequence relationships.\nC. Evaluating Procedure Planning on Real Robot\nPrevious Procedure Planning research has rarely reported\nexperimental results in real robots. There remains a gap\nbetween ofﬂine training and real-world applications. To val-\nidate the possibility of applying procedural tasks in the real\nenvironment, we conduct experiments on ”Reaching a block”\n(cf. Figure 4) using a Universal Robot UR5 system. While\nthis task is easy in simulation, it can be difﬁcult for real\nrobot [39]. Our agent learns to achieve tasks by imitating\nTABLE II: Success Rate (%) of UR5.\nMethod UR5\nPrediction Length T = 3 T = 4\nRandom <0.01 <0.01\nRB [29] 32 26\nRL [3] 50 44\nWLTDO [18] 42 38\nUAAA [32] 44 40\nUPN [13] 44 38\nDDN [3] 52 46\nPlaTe (Ours) 666000 555222\nexpert demonstrations. After the RGB-D camera is installed\nin front of the manipulator, the observation space includes\nthe raw RGB images at the current position and goal posi-\ntion. The action space includes None, Up, Down, Left,\nRight, Forward, Backward. UR5 Reacher consists of\nepisodes of interactions, where each episode is T time steps\nlong. The ﬁngertip of UR5 Reacher is conﬁned within a 3-\ndimensional 0.7m × 0.5m × 0.4m boundary. The robot is\nalso constrained within a joint-angular boundary to avoid self-\ncollision.\nIn the task of Reaching, the robot is required to start at\nthe current observation and then move to a goal observation.\nSince the controller of the robot is imperfect, we consider a\nreach to be successful if the robot reaches within 5cm of the\nblock. The UR5 robotic arm is controlled by human volunteers\nto reach the target, and thus 100 ofﬂine expert demonstration\n6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022\nFig. 4: Qualitative results of procedure planning on UR5 .\ntrajectories are generated. All methods are evaluated on a real\nUR5 robotic arm for 50 episodes. As shown in Table II, our\napproach outperforms the other methods. Even though other\nbaselines have succeeded in the ofﬂine dataset, real robotic\nreaching lags far behind human performance and remains\nunsolved in the ﬁeld of robot learning.\nV. C ONCLUSION\nTo conclude, we propose a cross-modal transformer-based\narchitecture to address the procedure planning problem, which\ncan capture long-term time dependencies. Moreover, We pro-\npose to enhance the transformer-based planner with Beam\nSearch. Finally, we evaluate our method on a real-world in-\nstructional video dataset. The results indicate that our method\ncan learn a meaningful action sequence for planning and\nrecover the human decision-making process. We also validated\nthe possibility of applying procedural tasks on a real UR-5\nplatform.\nACKNOWLEDGMENT\nAnimesh Garg is supported in part by CIFAR AI Chair at\nthe Vector Institute for AI and NSERC Discovery Award.\nREFERENCES\n[1] M. Beetz, R. Chatila, J. Hertzberg, and F. Pecora, “Ai reasoning methods\nfor robotics,” in Springer Handbook of Robotics, 2016.\n[2] J. Sun, H. Sun, T. Han, and B. Zhou, “Neuro-symbolic program\nsearch for autonomous driving decision module design,” in Proceedings\nof the 2020 Conference on Robot Learning , ser. Proceedings of\nMachine Learning Research, J. Kober, F. Ramos, and C. Tomlin, Eds.,\nvol. 155. PMLR, 16–18 Nov 2021, pp. 21–30. [Online]. Available:\nhttps://proceedings.mlr.press/v155/sun21a.html\n[3] C.-Y . Chang, D.-A. Huang, D. Xu, E. Adeli, L. Fei-Fei, and J. C.\nNiebles, “Procedure planning in instructional videos,” in ECCV.\nSpringer, 2020, pp. 334–350.\n[4] J. Sun, L. Yu, P. Dong, B. Lu, and B. Zhou, “Adversarial inverse rein-\nforcement learning with self-attention dynamics model,” IEEE Robotics\nand Automation Letters, vol. 6, no. 2, pp. 1880–1886, 2021.\n[5] J. Huang, S. Xie, J. Sun, Q. Ma, C. Liu, D. Lin, and B. Zhou, “Learning a\ndecision module by imitating driver’s control behaviors,” inProceedings\nof the Conference on Robot Learning (CoRL) 2020.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS,\nvol. 30. Curran Associates, Inc., 2017.\n[7] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\nComput., 1997.\n[8] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Empirical eval-\nuation of gated recurrent neural networks on sequence modeling,”\narXiv:1412.3555, 2014.\n[9] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and\nJ. Davidson, “Learning latent dynamics for planning from pixels,” in\nICML, 2019.\n[10] K. Fang, Y . Zhu, A. Garg, S. Savarese, and L. Fei-Fei, “Dynamics learn-\ning with cascaded variational inference for multi-step manipulation,”\narXiv:1910.13395, 2019.\n[11] B. Amos, S. Stanton, D. Yarats, and A. G. Wilson, “On the model-\nbased stochastic value gradient for continuous reinforcement learning\nlearning,” in L4DC, 2021, pp. 6–20.\n[12] T. Kurutach, A. Tamar, G. Yang, S. J. Russell, and P. Abbeel, “Learning\nplannable representations with causal infogan,” in NeurIPS, vol. 31.\nCurran Associates, Inc., 2018, pp. 8733–8744.\n[13] A. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn, “Universal\nplanning networks: Learning generalizable representations for visuomo-\ntor control,” in ICML, 2018.\n[14] X. Chen, Z. Ye, J. Sun, Y . Fan, F. Hu, C. Wang, and C. Lu, “Transferable\nactive grasping and real embodied dataset,” in 2020 IEEE International\nConference on Robotics and Automation (ICRA), 2020, pp. 3611–3618.\n[15] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven\nexploration by self-supervised prediction,” in ICML, 2017.\n[16] J. Qiu, L. Chen, X. Gu, F. P.-W. Lo, Y .-Y . Tsai, J. Sun, J. Liu, and B. Lo,\n“Egocentric human trajectory forecasting with a wearable camera and\nmulti-modal fusion,” arXiv:2111.00993, 2021.\n[17] B. Pan, J. Sun, H. Y . T. Leung, A. Andonian, and B. Zhou, “Cross-view\nsemantic segmentation for sensing surroundings,” IEEE Robotics and\nAutomation Letters, vol. 5, no. 3, pp. 4867–4873, 2020.\n[18] K. Ehsani, H. Bagherinezhad, J. Redmon, R. Mottaghi, and A. Farhadi,\n“Who let the dogs out? modeling dog behavior from visual data,” in\nCVPR, 2018, pp. 4051–4060.\n[19] J. Bi, J. Luo, and C. Xu, “Procedure planning in instructional videos via\ncontextual modeling and model-based policy learning,” in ICCV, 2021,\npp. 15 611–15 620.\n[20] L. Zhou, C. Xu, and J. Corso, “Towards automatic learning of procedures\nfrom web instructional videos,” in AAAI, 2018.\n[21] D. Zhukov, J.-B. Alayrac, R. G. Cinbis, D. Fouhey, I. Laptev, and\nJ. Sivic, “Cross-task weakly supervised learning from instructional\nvideos,” in CVPR, 2019, pp. 3537–3545.\n[22] B. Pan, J. Sun, W. Lin, L. Wang, and W. Lin, “Cross-stream selective\nnetworks for action recognition,” in 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops (CVPRW), 2019,\npp. 454–460.\n[23] D.-A. Huang, L. Fei-Fei, and J. C. Niebles, “Connectionist temporal\nmodeling for weakly supervised action labeling,” in ECCV, 2016.\n[24] V . Blukis, C. Paxton, D. Fox, A. Garg, and Y . Artzi, “A persistent spa-\ntial semantic representation for high-level natural language instruction\nexecution,” in CoRL, 2021.\n[25] D. Xu, S. Nair, Y . Zhu, J. Gao, A. Garg, L. Fei-Fei, and S. Savarese,\n“Neural task programming: Learning to generalize across hierarchical\ntasks,” in ICRA. IEEE, 2018, pp. 3795–3802.\n[26] D.-A. Huang, S. Nair, D. Xu, Y . Zhu, A. Garg, L. Fei-Fei, S. Savarese,\nand J. C. Niebles, “Neural task graphs: Generalizing to unseen tasks\nfrom a single video demonstration,” in CVPR, 2019, pp. 8565–8574.\n[27] J.-B. Alayrac, I. Laptev, J. Sivic, and S. Lacoste-Julien, “Joint discovery\nof object states and manipulation actions,” in ICCV, 2017.\n[28] W. Yu, W. Chen, S. Yin, S. Easterbrook, and A. Garg, “Modular action\nconcept grounding in semantic video prediction,” 2020.\n[29] C. Sun, A. Myers, C. V ondrick, K. Murphy, and C. Schmid, “Videobert:\nA joint model for video and language representation learning,” in ICCV,\n2019, pp. 7464–7473.\n[30] X. Wang, J.-F. Hu, J.-H. Lai, J. Zhang, and W.-S. Zheng, “Progressive\nteacher-student learning for early action prediction,” in CVPR, 2019.\n[31] F. Sener and A. Yao, “Zero-shot anticipation for instructional activities,”\nin ICCV, 2019.\n[32] Y . Abu Farha and J. Gall, “Uncertainty-aware anticipation of activities,”\nin ICCV Workshops, 2019.\n[33] E. Cohen and C. Beck, “Empirical analysis of beam search performance\ndegradation in neural sequence models,” in ICML, 2019, pp. 1290–1299.\n[34] C. Meister, T. Vieira, and R. Cotterell, “Best-ﬁrst beam search,” ACL,\npp. 795–809, 2020.\nSUN et al.: PLATE: VISUALLY-GROUNDED PLANNING WITH TRANSFORMERS IN PROCEDURAL TASKS 7\n[35] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nICLR, 2015.\n[36] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new\nmodel and the kinetics dataset,” in CVPR, 2017.\n[37] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in CVPR, 2016.\n[38] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen,\nR. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, M. Slaney,\nR. J. Weiss, and K. Wilson, “Cnn architectures for large-scale audio\nclassiﬁcation,” in ICASSP, 2017.\n[39] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement\nlearning for robotic manipulation with asynchronous off-policy updates,”\nin IEEE ICRA, 2017."
}