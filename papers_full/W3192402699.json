{
    "title": "Mounting Video Metadata on Transformer-based Language Model for Open-ended Video Question Answering",
    "url": "https://openalex.org/W3192402699",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5022465625",
            "name": "Donggeon Lee",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5054074338",
            "name": "Seong-Ho Choi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5084585820",
            "name": "Youwon Jang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5050928023",
            "name": "Byoung‐Tak Zhang",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2962910007",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2808181286",
        "https://openalex.org/W3004165599",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W2526050071",
        "https://openalex.org/W3021096110",
        "https://openalex.org/W2911400095",
        "https://openalex.org/W3010593057",
        "https://openalex.org/W2606982687",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2963541336",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2751525844",
        "https://openalex.org/W3016254928",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2963403868"
    ],
    "abstract": "Video question answering has recently received a lot of attention from multimodal video researchers. Most video question answering datasets are usually in the form of multiple-choice. But, the model for the multiple-choice task does not infer the answer. Rather it compares the answer candidates for picking the correct answer. Furthermore, it makes it difficult to extend to other tasks. In this paper, we challenge the existing multiple-choice video question answering by changing it to open-ended video question answering. To tackle open-ended question answering, we use the pretrained GPT2 model. The model is fine-tuned with video inputs and subtitles. An ablation study is performed by changing the existing DramaQA dataset to an open-ended question answering, and it shows that performance can be improved using video metadata.",
    "full_text": "Mounting Video Metadata on Transformer-based Language Model for\nOpen-ended Video Question Answering\nDonggeon Lee\nSeoul National University\ndglee@bi.snu.ac.kr\nSeongho Choi\nSeoul National University\nshchoi@bi.snu.ac.kr\nYouwon Jang\nSeoul National University\nywjang@bi.snu.ac.kr\nByoung-Tak Zhang\nSeoul National University\nbtzhang@bi.snu.ac.kr\nAbstract\nVideo question answering has recently received a lot of\nattention from multimodal video researchers. Most video\nquestion answering datasets are usually in the form of\nmultiple-choice. But, the model for the multiple-choice task\ndoes not infer the answer. Rather it compares the answer\ncandidates for picking the correct answer. Furthermore, it\nmakes it difﬁcult to extend to other tasks. In this paper, we\nchallenge the existing multiple-choice video question an-\nswering by changing it to open-ended video question an-\nswering. To tackle open-ended question answering, we use\nthe pretrained GPT2 model. The model is ﬁne-tuned with\nvideo inputs and subtitles. An ablation study is performed\nby changing the existing DramaQA dataset to an open-\nended question answering, and it shows that performance\ncan be improved using video metadata.\n1. Introduction\nTransformers are now the de facto standard for language\nmodeling and recently extending their applications in vi-\nsion and multimodal domain [19, 4]. Transformers in the vi-\nsion and language domain are usually pretrained with large-\nscale datasets and applied to various downstream tasks.\nAmong downstream tasks, video question answering eval-\nuates whether the model understands various dimensions\nof video contents and is usually done in multiple-choice.\nHowever, when learning a model for multiple-choice video\nquestion answering, the model selects the correct answer\nby comparing the similarity between the question and the\nanswer candidates rather than inferring the correct answer\nto the question. But, selecting the correct answer through\ncomparison with the answer candidates does not perform\nthe reasoning required in the question and answering, mak-\ning it difﬁcult to generalize for other tasks.\nIn this paper, we tackle the current multiple-choice video\nquestion answering dataset by changing it into an open-\nended format. The answer candidates are not given in open-\nended multimodal video question answering, so the model\ninfers the correct answer through reasoning. In addition, it\nis possible to develop a model that can be applied to other\ntasks except for the decoder part that generates the correct\nanswer.\nChallenging open-ended multimodal video question an-\nswering, we propose an extended model that learns various\nmodalities together based on the recently proposed Trans-\nformer language model. The proposed model receives vari-\nous metadata and language input of video. The results show\nthat performance can be improved by combining multiple\nmetadata rather than features from raw videos.\nThis paper is organized as follows. Chapter 2 exam-\nines related works to video question answering and open-\nended question answering. Chapter 3 describes the pro-\nposed model and learning strategy. Chapter 4 examines the\ndataset and experimental settings, as well as the quantita-\ntive results. Finally, in Chapter 5, the conclusion and future\nresearch directions are described.\n2. Related Work\n2.1. Video Question Answering\nA variety of video question-answering datasets have\nbeen proposed, including MovieQA[17], PororoQA[10],\nTGIF-QA[9], TVQA[11], DramaQA[5], and are mostly in\nthe multiple-choice format. A VSD Dataset[1] is character-\nized by the fact that question-answering for video is in the\nform of dialogue, which is out of the existing multiple-\nchoice form.\nRecently, various approaches have been proposed for\nvideo story question answering, which can be divided\n1\narXiv:2108.05158v1  [cs.CV]  11 Aug 2021\ninto three categories. There are techniques using Memory\nNetwork[17, 10], Attention[10, 11], and Transformer[21].\nMemory networks stores and utilizes key information about\na question-answering in a memory network to ﬁnd it among\nmany information in a long video. Attention effectively rep-\nresents only the representation of visual/verbal core infor-\nmation by progressing attention across layers. Techniques\nutilizing context matching by applying attention achieved\nhigh performance in question-and-answer by comparing the\ncontext of a question-and-answer with the context of a given\nvideo in detail. Recently, researchers propose transformer-\nbased models for video question answering. [18] proposed\ntransformer and the proposed architecture brought a huge\nperformance improvement in language modeling, and there\nis a move to expand it to a video domain. Recent state-of-\nart models show that these techniques can perform well in\nmodeling the video as well as the language.\n2.2. Opend-Ended Question Answering\nIn the H. Xue et al.[20], Z. Zhao et al.[23], pointed out\nthat the existing video question answering task used only\none static image and text and also dealt with it as a short-\nword-oriented multiple-choice problem. It is emphasized\nthat this approach cannot utilize the sequential and temporal\ninformation of the video. Therefore, its usability is limited\nin that the answer is chosen within given answers. In the\nabove papers, the sequential/time information of the video\nwas utilized to ﬁnally generate answers through decoders,\nresulting in better results than traditional methods (Mean-\nVQA, SS-VQA, etc.). However, the issues addressed by\nthe above papers are limited in that they are short-lived,\nalthough open-ended, and the format of questions and an-\nswers is also simple.\nIn the [12], the author conducted a study on A VSD\ntask[1](Given video and ten turns of question answer-\ning a text, task generates natural language answers to the\nlast question) based on Transformer(GPT2[15]). This pa-\nper extracts features from video and text with I3D[3] and\nVGGish[7], applies positional encoding, Beam Search, re-\nceives good results from several metrics (BLEU, METEOR,\nCIDEr, etc.). However, the model is not much different from\nB, and the position and video feature information was not\nused properly.\n3. Method\n3.1. Formulation\nThe purpose of our model is to integrate multimodal in-\nformation (e.g., subtitle, video, audio, question, etc.) to gen-\nerate the open-ended answer.\nOur model consists of inputs of video, question and\noutputs of answer. The video is represented as V =\n({v1, . . . ,vN }, {m1, . . . ,mN }, {s1, . . . ,sM }). vnis rep-\nresenting the n-th frame in V, mn means a image features,\nand a visual meta data, the information such as person, per-\nson’s emotion and behavior, in bounding box corresponding\nto n-th frame, sm is m-th subtitle in the entire video V. The\nquestion is represented as Q = {w1\na, . . . , wL\nq }, and the an-\nswer is represented as A = {w1\na, . . . , wK\na }.\nEach frame can be expressed as vvn by extracting 3\nframes per second from video and then feeding in the pre-\ntrained I3D[3] model to extract feature vectors.\nThere is information about the character in the form of\n{c1\nvn, . . . ,cImn\nmn } in each mn. and information about each\ncharacter is represented as ci\nmn = (fi\nmn, pi\nmn, bi\nmn, ei\nmn).\nfi\nmn is a feature representation of the character’s im-\nage of bounding box using a pre-trained ResNet152[6]\nmodel. pi\nmn is a word embedding representation using a\npre-trainned GPT2 model. bi\nmn is the character’s behavior.\nei\nmn is a word embedding representation of the character’s\nemotion.\nEach sm an be expressed as (psm, {w1\nsm, . . . , wJsm\nsm })\nwhich which can be divided into sentence,\n{w1\nsm, . . . , wJsm\nsm }, which can be divided into a word\nwj\nsm and a speaker psm Both speakers and words can be\nexpressed in a previous way. Sentences can also be broken\ndown into words using the GPT2 tokenizer.\n3.2. GPT2\nWe reference and use GPT2, a transformer model, which\nuses attention in place of the previous recurrence- and\nconvolution-based architectures. Attention mechanisms al-\nlow the model to selectively focus on segments of input text\nit predicts to be the most relevant.\nGPT2 models receive the feature, segment, and posi-\ntion as inputs. Feature refers to data that embeds text input\nthrough GPT2 tokenizer, segment refers to data that means\na token type of each word, such as ¡eos¿ and ¡sos¿, and po-\nsition refers to the location of each word in the sentence.\n3.2.1 Feature Embedding\nFeature embedding input is all of the preceding\n(vvn, {c1\nmn, . . . ,cImn\nmn }) to a two-dimensional sequence\nover time. Subsequent (psm, {w1\nsm, . . . , wJsm\nsm }) similarly\nleads to a two-dimensional sequence over time. Finally,\nwe attach {w1\nq, . . . , wL\nq }. Therefore, the sequence length\nis N + ∑N\nmn=1 Imn + M + ∑M\nsm=1 Jsm + L. On the\nother hand, if features are extracted using I3D or ResNet,\nthe features are different from those extracted with GPT2\nmodels, so the dimensions are adjusted through a layer of\nlearnable linear layers.\n2\n1 2 3 4 9 10 11 125 6 7 8 13 14 … n-3 n-2 n-1\n[V] [V] [Bbf] … [Per][Bbf] [Per] [Beh] [Emo] [Scr] [Que]\nV_1 V_n Bbf_1 … Per_kBbf_k Per_1 Beh_1 Emo_1 Scr_m que\nn\n[Beh] [Spk][Emo] [Spk] [Scr] …\nBeh_k Spk_mEmo_k Spk_1 Scr_1 …\nText EmbedderVideo Embedder\n[Bbf]\n…\nHaeyoung : “Hey, I don’t \ncare if I die.”\nQue : “Who is drinking \nin the shot?”\nDecoder\nFeature\nEncode\n[V]\n…\nFeature\nEncode\nHaeyoung\nPosition\nSegment\nFeature\nis in .drinking shotthe\nPretrained Transformer\nVideo features Text features\nSubtitle\nWord EmbedWord Embed\nWord Embed\nFigure 1. Multimodal transformer model architecture. The video embedder is a linear layer which embeds feature of video size to feature\nof embedding size, and the text embedder is a linear layer which embeds feature of vocab size to feature of embedding size. denote We\nused the following segment tokens [V] : Video, [Bbf] : feature of bounding box, [Per] : person’s name, [Beh] : person’s behavior, [Emo] :\nperson’s emotion, [Spk] : speaker, [Scr] : script, [Que] : question.\nVfeature = [{(vvn, {c1\nmn, . . . ,cImn\nmn })},\n{(psm, {w1\nsm, . . . , wJsm\nsm })},\n{qw1, . . . , qwL}]\n3.2.2 Segment Embedding\nNotation Description\n[V] I3D feature for each frame\n[BBF] 2D ResNet feature for each bounding box\n[PER] Name of each character\n[BEH] Behavior of each character\n[EMO] Emotion of each character\n[SPK] Speaker of each subtitle\n[SCR] Each subtitle\n[QUE] Question\nTable 1. Notation and description for segments.\nSegment embedding distinguishes the various inputs that\nenter the video. The distinguishing features can be divided\ninto eight as Table 1.\nFor each of these eight Feature categories, Segment em-\nbedding was performed using special token in GPT2.\nMethod Bleu Meteor Bertscore Bleurt Time\nBeam 0.69 0.2 0.34 0.62 8 min\nNucleus 0.68 0.18 0.32 0.6 130 min\nTable 2. It is a description of the performance and time required\nfor each Decoding Method for 4385 data in a subtitle-only envi-\nronment.\n3.3. Decoding Method\nTo ﬁnd an effective decoding method for multimodal an-\nswer generation, we try the decoding methods, including\nbeam search and Nucleus Sampling[8] which samples text\nfrom the dynamic nucleus of the probability distribution.\nAlthough beam search showed slightly high performance,\nit took about 16 times more time to use it in real-time, so\nNeclues Sampling was used.\n3.4. Implementation Details\nAll experiments are run on NVIDIA [TITAN Xp]. Be-\ncause of the lack of memory, we use a batch size of 1 input\nunit. We use AdamW optimizer[13] with a learning rate of\n1e-4 and weight decay of 1e-5. Cross-entropy loss is used\nto train the model.\n3\n4. Results\n4.1. Evaluation\nThe evaluation is carried out using BLEU[14] based\non n-gram, METEOR[2] considering recall as a traditional\nmetric to evaluate the generated text. In addition, we eval-\nuate the answers generated with a total of four metrics, in-\ncluding BERTScore[22] which is measured based on a sim-\nilarity between each token embedding and BLEURT[16]\nwhich uses the pre-learned model as metric.\n4.2. Quantitative Results\nModel Bleu Meteor Bertscore Bleurt\nS 0.68 0.18 0.32 0.6\nS + V 0.65 0.1 0.3 0.59\nS + B 0.697 0.202 0.35 0.6\nS + M 0.733 0.281 0.378 0.62\nS + M, V 0.726 0.263 0.38 0.61\nS + M, B 0.733 0.276 0.38 0.62\nS + M, V , B 0.724 0.258 0.37 0.61\nTable 3. Quantitative experimental results for the DramaQA val-\nidation set. S stands for subtitle, V stands for video features ex-\ntracted from I3D, B stands for bounding box features extracted\nfrom ResNet, and M stands for visual metadata composed of per-\nson, emotion, and behavior.\nTable 3 shows metadata plays a major role in improv-\ning performance. Our model is based on GPT2, so there is\nlanguage bias. It helps improve performance with language\nmetadata.\nThe information in bounding box features also helps an-\nswer questions by looking at S / B + S. However, comparing\nM + S / B, M + S did not improve performance.\nVideo information lowers performance. For reasons, a\ntransformer-based model is a model with large language\nbias, and the entire video that is irrelevant to the question\nworks even worse than bounding box features.\n5. Conclusion\nIn this paper, we challenge the existing multiple-choice\nvideo question answer by converting it into an open-ended\nform. We construct the model in the form of a multimodal\ntransformer by adding video and metadata from video to the\nexisting pre-trained language model. Ablation studies using\nthe DramaQA dataset showed that video metadata helped\nperformance.\nFor future work, we plan to use the dense caption fea-\ntures in the video space transferred into the language space\nto circumvent the language bias problem.\nReferences\n[1] Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang,\nAnoop Cherian, Irfan Essa, Dhruv Batra, Tim K. Marks,\nChiori Hori, Peter Anderson, Stefan Lee, and Devi Parikh.\nAudio-visual scene-aware dialog, 2019. 1, 2\n[2] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic\nmetric for mt evaluation with improved correlation with hu-\nman judgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine trans-\nlation and/or summarization, pages 65–72, 2005. 4\n[3] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299–6308, 2017. 2\n[4] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning. In ECCV,\n2020. 1\n[5] Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong\nSeo, Youwon Jang, Seungchan Lee, Minsu Lee, and Byoung-\nTak Zhang. Dramaqa: Character-centered video story under-\nstanding with hierarchical qa, 2020. 1\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 2\n[7] Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis,\nJort F. Gemmeke, Aren Jansen, Channing Moore, Manoj\nPlakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm\nSlaney, Ron Weiss, and Kevin Wilson. Cnn architectures for\nlarge-scale audio classiﬁcation. In International Conference\non Acoustics, Speech and Signal Processing (ICASSP). 2017.\n2\n[8] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\nChoi. The curious case of neural text degeneration. arXiv\npreprint arXiv:1904.09751, 2019. 3\n[9] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and\nGunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in\nvisual question answering. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n2758–2766, 2017. 1\n[10] Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and\nByoung-Tak Zhang. Deepstory: Video story qa by deep em-\nbedded memory networks, 2017. 1, 2\n[11] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg.\nTvqa: Localized, compositional video question answering,\n2019. 1, 2\n[12] Zekang Li, Zongjia Li, Jinchao Zhang, Yang Feng, Cheng\nNiu, and Jie Zhou. Bridging text and video: A universal\nmultimodal transformer for video-audio scene-aware dialog,\n2020. 2\n[13] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 3\n[14] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics , pages 311–318,\n2002. 4\n[15] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 2\n4\n[16] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt:\nLearning robust metrics for text generation. arXiv preprint\narXiv:2004.04696, 2020. 4\n[17] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,\nAntonio Torralba, Raquel Urtasun, and Sanja Fidler.\nMovieqa: Understanding stories in movies through question-\nanswering, 2016. 1, 2\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 2\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, undeﬁnedukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Proceed-\nings of the 31st International Conference on Neural Infor-\nmation Processing Systems, NIPS’17, page 6000–6010, Red\nHook, NY , USA, 2017. Curran Associates Inc. 1\n[20] Hongyang Xue, Zhou Zhao, and Deng Cai. Unifying the\nvideo and question attentions for open-ended video ques-\ntion answering. IEEE Transactions on Image Processing ,\n26(12):5656–5666, 2017. 2\n[21] Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta\nNakashima, and Haruo Takemura. Bert representations for\nvideo question answering. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision ,\npages 1556–1565, 2020. 2\n[22] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. Bertscore: Evaluating text generation\nwith bert. arXiv preprint arXiv:1904.09675, 2019. 4\n[23] Zhou Zhao, Zhu Zhang, Shuwen Xiao, Zhou Yu, Jun Yu,\nDeng Cai, Fei Wu, and Yueting Zhuang. Open-ended long-\nform video question answering via adaptive hierarchical re-\ninforced networks. In IJCAI, volume 3, page 4, 2018. 2\n5"
}