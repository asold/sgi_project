{
  "title": "T5QL: Taming language models for SQL generation",
  "url": "https://openalex.org/W4385573324",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Samuel David Arcadinho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098415982",
      "name": "David Aparicio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2255142345",
      "name": "Hugo Veiga",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2576885138",
      "name": "Antonio Alegria",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3108032709",
    "https://openalex.org/W4385572722",
    "https://openalex.org/W2953081964",
    "https://openalex.org/W2963868406",
    "https://openalex.org/W1773774036",
    "https://openalex.org/W2970694516",
    "https://openalex.org/W4385572953",
    "https://openalex.org/W3203259592",
    "https://openalex.org/W3214600982",
    "https://openalex.org/W4288025992",
    "https://openalex.org/W3103801878",
    "https://openalex.org/W3200079259",
    "https://openalex.org/W2952781527",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W3035172316",
    "https://openalex.org/W4236265809",
    "https://openalex.org/W2594782638",
    "https://openalex.org/W2016206563",
    "https://openalex.org/W3173274550",
    "https://openalex.org/W2889467844",
    "https://openalex.org/W3122604893",
    "https://openalex.org/W4288109580",
    "https://openalex.org/W4297578259",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287659415",
    "https://openalex.org/W3006127095",
    "https://openalex.org/W4287550997",
    "https://openalex.org/W4226242393"
  ],
  "abstract": "Automatic SQL generation has been an active research area, aiming at streamlining the access to databases by writing natural language with the given intent instead of writing SQL. Current SOTA methods for semantic parsing depend on LLMs to achieve high predictive accuracy on benchmark datasets. This reduces their applicability, since LLMs requires expensive GPUs. Furthermore, SOTA methods are ungrounded and thus not guaranteed to always generate valid SQL. Here we propose T5QL, a new SQL generation method that improves the performance in benchmark datasets when using smaller LMs, namely T5-Base, by 13pp when compared against SOTA methods. Additionally, T5QL is guaranteed to always output valid SQL using a context-free grammar to constrain SQL generation. Finally, we show that dividing semantic parsing in two tasks, candidate SQLs generation and candidate re-ranking, is a promising research avenue that can reduce the need for large LMs.",
  "full_text": "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 276 - 286\nDecember 7, 2022 ©2022 Association for Computational Linguistics\nT5QL: Taming language models for SQL generation\nSamuel Arcadinho, David Aparício, Hugo Veiga, António Alegria\nOutsystems\n{samuel.arcadinho, david.aparicio, hugo.veiga, antonio.alegria}@outsystems.com\nAbstract\nAutomatic SQL generation has been an active\nresearch area, aiming at streamlining the ac-\ncess to databases by writing natural language\nwith the given intent instead of writing SQL.\nCurrent state-of-the-art (SOTA) methods for\nsemantic parsing depend on large language\nmodels (LLMs) to achieve high predictive ac-\ncuracy on benchmark datasets. This reduces\ntheir applicability, since LLMs require expen-\nsive GPUs. Furthermore, SOTA methods are\nungrounded and thus not guaranteed to always\ngenerate valid SQL. Here we propose T5QL, a\nnew SQL generation method that improves the\nperformance in benchmark datasets when us-\ning smaller LMs, namely T5-Base, by ≈13pp\nwhen compared against SOTA methods. Addi-\ntionally, T5QL is guaranteed to always output\nvalid SQL using a context-free grammar to con-\nstrain SQL generation. Finally, we show that\ndividing semantic parsing in two tasks, candi-\ndate SQLs generation and candidate re-ranking,\nis a promising research avenue that can reduce\nthe need for large LMs.\n1 Introduction\nAutomated code generation has long been consid-\nered one of the fundamental tasks in computer sci-\nence (Pnueli and Rosner, 1989). Recently, deep\nlearning (DL) methods for code generation have\nbeen proposed which overcome the lack of flexibil-\nity of more traditional approaches (Le et al., 2020).\nSome DL approaches can act as code completion\ntools (Svyatkovskiy et al., 2020; Chen et al., 2021)\nwhile others can use natural language (NL) as input\nto generate code (Yin and Neubig, 2017), i.e., se-\nmantic parsing (Kamath and Das, 2018). The latter\nmethods are particularly helpful for developers that\nare not proficient in all programming languages\nthat are part of their development pipeline. For\nexample, a developer might be familiar with the\ncontroller language (e.g., Python) but unfamiliar\nwith the database access language (e.g., SQL).\n2 4 6 80.4\n0.5\n0.6\n0.7\n0.8\nbeam size\nexact match accuracy\nT5QL-Base T5-Base\nPICARD-Base PICARD-3B\nFigure 1: Exact-match accuracy of the highest scor-\ning prediction as a function of beam size on the Spider\ndevelopment set. Our method, T5QL, significantly im-\nproves upon T5-Base and is superior to PICARD-Base.\nPICARD-3B remains the SOTA for very large LMs, i.e.,\nPICARD-3B uses T5-3B which is ≈13xlarger than\nT5-Base. Results for PICARD-Base and PICARD-3B\nare straight (dashed) lines since Scholak et al. (2021)\nonly report results in the setting using database content\nfor a single point, namely beam search with 4 beams.\nGenerating SQL from NL is challenging because\nthe NL query might be ambiguous (e.g., columns\nfrom different tables can have the same name).\nFuthermore, obtaining labelled pairs of NL queries\nto SQL is hard, time-consuming, and requires la-\nbellers that are proficient in SQL. In recent years,\nbenchmark datasets have been used by developers\nto evaluate their methods, namely Spider (Raffel\net al., 2019) and CoSQL (Yu et al., 2019).\nPICARD (Scholak et al., 2021) is the current\nSOTA method, i.e., the highest ranked method on\nSpider. It is built on top of T5 (Raffel et al., 2019),\na general purpose LLM. As proven by Merrill et al.\n(2021), LLMs are ungrounded and thus can gener-\nate any token at any given step, which may result in\ninvalid SQL; thus, to improve upon T5, PICARD\n276\nprunes the search tree in order to avoid generating\ninvalid SQL. However, since PICARD fully prunes\nbranches during beam search, it is not guaranteed\nto always generate an answer. Another major issue\nwith PICARD is that it needs a very large LM to\nachieve good performance: PICARD gets ≈75.5%\nexact match (EM) accuracy in Spider’s develop-\nment set when using T5-3B, but only ≈66.6%\nwhen using the smaller T5-Base.\nHere, we propose T5QL, a novel SQL generation\nmethod that achieves 69.3% EM on Spider develop-\nment set using T5-Base instead of the ≈13x larger\nT5-3B. T5QL uses constrained decoding to ensure\nthat it always generates valid SQL, and it always\ngenerates an answer. Our main contributions are:\n1. Narrow the gap between large and small LMs\n(Figure 1). With beam size equal to 4 and\nusing T5-Base, T5QL achieves 69.3% EM\naccuracy on Spider, versus 66.6% obtained by\nPICARD. PICARD with T5-3B is still SOTA\n(75.5%) but it requires much larger GPUs,\nwhich are expensive and thus not available for\nregular practitioners.\n2. Propose a constrained decoding method that\nalways generates valid SQL, except for infre-\nquent model hallucinations. In Appendix A.1\nwe show one such case.\n3. Propose a novel ranker model for SQL gen-\neration. This model re-ranks the generator\nmodel’s predictions after beam search, boost-\ning EM on Spider for larger beam sizes (e.g.,\n8 beams) from 67.9% to 69.6%.\nThe remainder of the paper is organized as fol-\nlows. Section 2 presents SOTA for SQL genera-\ntion. Section 3 describes T5QL’s main components,\nnamely constrained decoding and the ranker. Sec-\ntion 4 shows our results. Finally, Section 5 con-\ncludes our work.\n2 State-of-the-art\nAutomated program generation has long been one\nof the major goals of computer science. Various\nprogram synthesis tools have been proposed that\ngenerate SQL from code fragments (Cheung et al.,\n2012) or pairs of input-output examples (Orvalho\net al., 2020). However, code fragments might not\nbe readily available if the developer does not write\ncode or does not want to, and creating enough input-\noutput examples for the program synthesis tool to\nbe effective might be cumbersome. Other tools\ngenerate SQL from NL which is more developer-\nfriendly (Yaghmazadeh et al., 2017).\nThe complexity of generating SQL from NL\nvaries with the length and complexity of the SQL\nquery and the size of the database schema. Thus,\nin order to properly evaluate and compare methods’\nperformance, multiple benchmark datasets have\nbeen proposed, namely Spider (Raffel et al., 2019),\nSpider-SSP (Shaw et al., 2021), and CoSQL (Yu\net al., 2019). We describe these benchmarks in de-\ntail in Section 4.2 and discuss how they relate to\nour research questions (enumerated at the start of\nSection 4).\nThe current SOTA for SQL generation (i.e., the\nmethods that achieve the highest performance on\nbenchmark datasets) comprises DL methods. DL\nmethods for code generation avoid the complex-\nity of traditional program synthesis and, thus, are\ngenerally faster during generation (Parisotto et al.,\n2016; Hayati et al., 2018; Sun et al., 2019).\nRatSQL’s authors argue that predicting SQL di-\nrectly from NL is hard and can be made easier by\ninstead predicting an intermediate representation\n(IR) that is more similar to NL than SQL is (Wang\net al., 2019; Gan et al., 2021). With this insight,\nthey obtained SOTA results on Spider. However,\ntheir IR is not capable of representing all SQLs\nand, thus, for some queries the correct SQL is not\nobtainable, leading to a loss of EM accuracy. Other\napproaches were built on top of RatSQL with good\nresults (Zhao et al., 2021; Shi et al., 2020; Yu et al.,\n2020). One of the major disadvantages of these\nmethods is that, since they use custom architec-\ntures, they cannot leverage pre-trained LLMs in\ntheir decoding step. Being able to leverage LLMs\nis beneficial since they can be used for multiple\ntasks. For example, Xie et al. (2022) unifies struc-\ntured knowledge grounding tasks into a text-to-text\nformat and are thus able to train the same model\nfor different tasks.\nTo the best of our knowledge, Shaw et al. (2021)\nwere the first to propose a method that uses an\nLLM, namely T5, and evaluate it on Spider. They\nconcluded that their method had good predictive\ncapabilities, but sometimes generated syntactically\nincorrect SQL and had lower precision in out-of-\ndistribution examples. Since T5 is ungrounded,\nit cannot be guaranteed to always generate valid\nSQL; the same is true for other LLMs (Merrill et al.,\n2021). In order to address the issue, Xiao et al.\n277\nGener at or \n(T5)\nRank er \n(CodeBERT)\nConstr ained\nDecoder\nSQL\nGr ammar\n> from User [NEXT]\nNEXT: - select\n      - as X\n      - join\n        ...\nAccount\n- userID\n- country\nUser\n- ID\n- name\n- birthdate\n- country\n1: from User select name, country\n2: from User select * order by country\n   ...\nfrom User  \nselect *   \norder by country\n2\n1\n3\n4 5\nG iv e me t he users \nsor t ed b y countr y\nFigure 2: T5QL model architecture. T5QL receives as input an NL query and a database schema (step 1). Then, the\ngenerator model, T5, consults the constrained decoder to know which tokens are valid (step 2) and predicts the next\ntoken (step 3). This step is done iteratively. The generation is done using beam search, thus producing a set of k\ncandidates which are given as input to the ranker model (step 4). Finally, the ranker model ranks all candidates and\na final prediction is outputted by T5QL (step 5).\n(2016) propose a method that constrains the output\ngeneration based on grammatical rules. They also\ncompare a model trained with constraints and verify\nthat using the constraints only during inference\nimproves the model.\nTargeting code generation specifically, Scholak\net al. (2021) propose PICARD, a method that con-\nstrains the model generation by removing wrong\noutputs during beam search. By doing so, PICARD\nis the current SOTA in the Spider benchmark. How-\never, they report that PICARD did not generate any\nSQL for 2% of the queries. Poesia et al. (2022) im-\nprove LLM performance in the few-shot setting by\nintroducing two components, one that selects the\nexamples to be given to the model and another that\nconstrains the generation of syntactically correct\nSQL. However, fine-tuned models (e.g., PICARD)\nstill perform better in the general task than their\nmodel, which was trained in the few-shot setting.\nIn this work, we use one model to generate\ncandidates, a generator, and another to re-rank\nthem, a ranker. This choice is motivated by recent\nwork (Chen et al., 2021) where the authors show\nthat a re-ranking method boosted performance for\ncode generation. Regarding semantic parsing more\nconcretely, Ye et al. (2022) use a ranker model\nto select candidates, and then a fine-tuned model\ngenerates the final output; their model shows good\ngeneralization capabilities and outperforms previ-\nous methods for question answering on knowledge\ngraphs. More recently, Krishna et al. (2022) argue\nthat when current LLMs are given a prefix prompt\nthey can often generate text that is incoherent with\nthe prefix. They propose a ranker model that scores\nthe generator’s candidates for an input prefix and\nobtain results that outperform earlier models in\nboth automatic and human evaluation.\n3 Method\nWe start this section by presenting an overview\nof our method and its architecture (Figure 2). Then,\nwe focus on each of its main components, namely\nconstrained decoding and the ranker. Finally, we\ndiscuss the scoring function and evaluation metrics.\n3.1 Overview\nOur method outputs the corresponding SQL query\nfor a given NL query and a database schema. The\ndatabase schema comprises a list of tables and their\nrespective columns. Figure 2 shows a simple NL\nquery, \"Give me the users sorted by country\", and\na toy database schema with only two tables, User\nand Account. The generator, T5, receives the NL\nand the database schema as input and, starting with\nan empty string, it iteratively predicts the next to-\nken. However, unlike regular T5, the next token\nprediction is limited by the constrained decoder\nto only consider tokens that form a valid SQL\nquery up to that point. For example, if the cur-\nrent query is \"from User\", the next valid tokens\ninclude \"select\", \"as X\", and \"join\", but do not\ninclude \"from\" or \"User\". We discuss why we\ninvert the from and the select statements in Sec-\ntion 3.2. We use beam search to generate multiple\ncandidate queries, which are given as input to the\nranker model.\n278\n3.2 Constrained decoding\nWe use constrained decoding to limit which tokens\nare considered by the generator to make the next\ntoken prediction. In order to enforce valid tokens,\nwe build a context-free grammar (CFG) of SQL\nstatements. Our constrained decoding method, de-\nscribed in Algorithm 1, is similar to the one pro-\nposed by Poesia et al. (2022): for each decoding\nstep, given the current generation P, T5QL finds\nthe maximum parsable prefix P∗, this means that\nall SQL tokens in the prefix P∗ have valid syntax\n(lines 2–5). Then, using the lookahead feature of\nthe parser, T5QL tokenizes all possible suffixes\nfor P∗ and adds them to trie T (lines 6–10). Fi-\nnally, T5QL computes possible generation tokens\nby searching the possibles suffixes forP in T (lines\n11–12).\nAlgorithm 1Constrained decoding\n1: procedure NEXT TOKEN (P, T) ▷P is the\ncurrent SQL generation and T the current trie\n2: P∗ ←FIND PARSABLE PREFIX (P)\n3: S ←GETPARSER STATE(P∗)\n4: N ←PARSER NEXT TOKENS (S)\n5: N∗ ←FILTER WRONG TOKENS (S,N)\n6: for nin N∗ do\n7: C ←P∗ + n\n8: CT ←SENTENCE TOKENIZER (C)\n9: T ←ADDTOTRIE (T,CT )\n10: end for\n11: PT ←SENTENCE TOKENIZER (P)\n12: returnGETCHILDREN (T,P T )\n13: end procedure\nWe note that, while our grammar is context free,\nour constrained decoding method uses context to\nmake decisions: FILTER WRONG TOKENS (line 5\nof Algorithm 1) constrains the SQL generation by\nonly allowing the generation of columns that are\ndefined in the from statement and by mapping table\naliases to the original tables. We should point out\nthat, while this is currently not performed by our\nmethod, we could extend constrained decoding to\nenforce more rules, such as only allowing tables to\nbe joined using valid foreign keys or limiting the\nwhere statement to only have conditions that have\nthe proper return type given the column types (e.g.,\nif a column \"X\" is of type string, \"X > 10\" is not a\nvalid generation).\nNext, we focus on the grammar. For brevity,\nwe only show higher-level statements below; the\nentire grammar is shown in our Codalab page 1.\nStatements inside square brackets indicate that they\nare optional (e.g., a SQL query can have an empty\nwhere statement).\n⟨sql⟩ |= ⟨expr⟩\n⟨expr⟩ |= ⟨query⟩ |\n⟨expr⟩union ⟨expr⟩ |\n⟨expr⟩intersect ⟨expr⟩ |\n⟨expr⟩except ⟨expr⟩\n⟨query⟩ |= from ⟨from-expr⟩\nselect ⟨select-expr⟩\n[where ⟨where-expr⟩]\n[group by ⟨groupby-expr⟩]\n[having ⟨having-expr⟩]\n[order by ⟨orderby-expr⟩]\n[limit ⟨limit-expr⟩]\nOur grammar only supports SQL select state-\nments since our focus are queries that retrieve data\nfrom a database. These select statements can be\na single query or contain subqueries joined by\nunions, intersects, and excepts. We note that the\nfrom and the select statements are inverted. This is\ndone because, besides restricting T5 to only gen-\nerate syntactically correct SQL, we also restrict\nit to only generate SQL with valid table names\n(i.e., tables that exist in the database schema) and\nvalid column names (i.e., columns that exist in the\ndatabase schema for the given table). To restrict\nthe generation to only valid columns, it is helpful\nto first know the valid tables, which are obtained\nin the from statement. Thus, T5QL first parses the\nfrom statement and stores the selected tables; then,\nwhen the select statement is parsed, T5QL already\nknows what columns are valid since they had to ap-\npear in the selected tables (e.g., from the example\nfrom Figure 2, if the current query is \"from User\nselect\", then \"user.ID\" and \"user.name\" are\nvalid token predictions while \"account.country\"\nand \"account.userId\" are not).\nFor a given query and database schema pair, we\naugment the grammar shown previously with two\nextra rules specifying the valid tables and the valid\ncolumns. For the example from Figure 2, we would\nadd the following production rules:\n1https://worksheets.codalab.org/worksheets/\n0x0049b642db90440e9eaf9cf6a850b4c9\n279\n⟨table-name⟩ |= user | account\n⟨column-name⟩ |= user.id | user.name |\nuser.birthdate |\nuser.name |\nuser.country |\naccount.userId |\naccount.country\nWhen a table has an an alias, we add one ex-\npression for the alias and another for the original\ntable table (e.g., for a column \"alias1.columnA\",\nwe add two expressions to the 〈column-name〉\nrule: \"alias1.columnA\" and \"tableX.columnA\",\nassuming that alias1 corresponds to tableX).\nThe grammar is given as input to the Lark\nparser2. We use Lark since it is one of the fastest\nparsers for Python, and it includes a look-ahead\nfeature that we require.\n3.3 Ranker\nWe use beam search to generate a set ofkcandidate\nqueries and employ a ranker model to choose the\nbest option among the kcandidates. We hypothe-\nsize that splitting the task of SQL generation into\ntwo tasks, (1) SQL candidates generation and (2)\nSQL candidate ranking, boosts the performance of\nthe complete task since each model is only focused\non a simpler task.\nWe use a trained generator model to generate the\ndataset to train the ranker model. The T5 model\ndescribed in Section 3.2 samples 16 SQL queries\nfor each input (NL query and database schema pair)\nin the training dataset using beam search. From the\n16 generated SQLs we sample the 12 with lowest\ntree edit distance (TED) (discussed in Section 3.5)\nto guarantee that we select hard negative examples.\nIf the generator model does not predict the correct\nSQL in any of the 12 SQLs samples, we discard\nthe one with the highest TED and add the correct\nSQL as one of the samples. Using the same sam-\npling strategy (i.e., based on TED), we sample an\nadditional two SQLs from the training dataset per-\ntaining to the same database as the input, for a total\nof 14 SQLs for each input.\nFor the ranker model, we fine-tune CodeBERT\n(Feng et al., 2020) in a cross encoder setting. The\n2https://github.com/lark-parser/lark\nranker is given pairs of NL and SQL and predicts\nthe probability of the pairs being correct, i.e., the\nSQL corresponding to the NL. We also append the\nterminals found in the NL using the method pro-\nposed by Lin et al. (2020) to the final NL (e.g., for\nthe NL \"People from ’France’\", the NL is trans-\nformed into \"People from ’France’ | France\").\n3.4 Scoring Function\nSimilarly to Yee et al. (2019), we compute the final\nprediction score for a given input by combining the\ngenerator’s probability score with the ranker’s prob-\nability score using the linear combination shown\nin Equation 1, where t is the length of the SQL,\nand λ is a tunable weight. In order to compare\nthe generator’s probability p(y|x) with the ranker’s\nprobability p(x,y), we scale the generator’s proba-\nbility by t.\n1\nt log p(y|x) +λlog p(x,y) (1)\n3.5 Evaluation metrics\nThe most commonly used evaluation metrics for\nSQL comparison are EM and execution match\n(EX). EM checks if two SQLs are syntactically\nequivalent, while EX checks if running two SQLs\nyields the same output. While desirable, EX is\nmore computationally expensive than EM since it\nrequires running the SQL statements, which might\nnot even be possible if we do not have access to the\ndatabase content. When measuring the method’s\nperformance, it is also relevant to highlight if it also\npredicts terminal values or not; T5QL generates the\nfull SQL query, including terminal values.\nSince EM is binary, its value might not be very\ninformative for the user nor the model. Partial\nmatches sub-divide the comparison to only portions\nof the SQL statement, such as thefrom clause or the\nwhere clause. Thus, one SQL prediction might be\nwrong in multiple parts of the query, and this more\ngranular information can be useful to improve the\nmodel. However, these measures are still coarse;\nthus, we use the TED in some experiments (namely\nin the ranker) when we want more information on\nthe difference (or distance) between two SQLs.\nIn order to compute the TED between two SQL\nstatements, we transform each SQL statement into\na tree and use APTED 3 to compute the TED be-\ntween two trees. Due to SQL’s semantics, we first\nnormalize the SQL to a canonical representation\n3https://github.com/DatabaseGroup/apted\n280\n2 4 6 80.4\n0.5\n0.6\n0.7\n0.8\nbeam size\nexact match accuracyOracle T5QL\nT5QL wo/ Ranker T5\nT5QL Ranker Score PICARD-3B\nFigure 3: EM accuracy in Spider’s development set by\nbeam size. All methods use T5-Base as their LM except\nfor PICARD-3B which uses T5-3B. The performance\nof PICARD-3B is shown as a straight line since the\nauthors only report results on the development set using\ndatabase content for a single point (beam search with 4\nbeams). The Oracle plot shows the performance ceiling\nfor T5QL, i.e., the performance of T5QL using a perfect\nranker that always outputs the correct SQL if the gener-\nator offers it as one of the candidates after beam search.\n(e.g., sort the list of tables in the select alphabet-\nically, transform left joins into right joins). Then\nmodify APTED to guarantee that the TED is mean-\ningful (e.g., the cost of removing a terminal and\ncolumn name should be the same).\n4 Experiments\nWe start by describing the experimental setup in\nSection 4.1. Then, we detail each dataset and the\nrelevant evaluation metrics in Section 4.2. Then,\neach subsequent section (Section 4.3–4.6) tries to\nanswer each of the following research questions:\nQ1. Does constrained decoding improve the gen-\nerator’s performance?\nQ2. Does T5QL have compositional generaliza-\ntion capabilities?\nQ3. Does T5QL generalize to the conversational\nsetting?\nQ4. Instead of using a very large generator, can\nwe improve performance using a ranker?\n4.1 Experimental setup\nFor our experiments we use a G4DN Extra Large\nAWS machine, which has an NVIDIA T4 Tensor\nCore GPU installed and 4 CPU-cores. We make\nour code available in our public Codalab page45.\n4.2 Datasets\nWe evaluate T5QL on three benchmark datasets:\nSpider (Raffel et al., 2019), Spider-SSP (Shaw\net al., 2021), and CoSQL (Yu et al., 2019).\nSpider comprises 10,181 NL and database\nschemas pairs, on 200 different database schemas.\nEvaluation on Spider consists of two main leader-\nboards: EX with terminal values and EM without\nterminal values. At the time of writing, PICARD\nis the current SOTA method on both leaderboards.\nSpider-SSP is a different splitting of the Spi-\nder dataset, with the aim of testing compositional\ngeneration instead of cross-database generalization,\ni.e., in the original Spider data split, a database\nschema seen in train is not seen in eval or test.\nSplits in the Spider-SSP dataset are made in three\ndifferent fashions: random split, a split based on\nsource length, and a split based on Target Maxi-\nmum Compound Divergence (TMCD). The goal\nhere is to evaluate if the model can have good per-\nformance on queries that it has not seen in training.\nWhile Spider consists of a single NL and do-\nmain model pair mapped into a single SQL query,\nCoSQL consists of a conversational dataset with\nmultiple iterations of NL plus data model being\nmapped to a SQL query. The goal of CoSQL is\nto simulate a user progressively exploring a data\nmodel. CoSQL contains 4,298 interactions and\n≈12,000 questions, on the same 200 data models\nused in Spider. Evaluation is done using EM with-\nout terminal values and reported using two differ-\nent metrics: question match (QM) and interaction\nmatch (IM). QM evaluates if all SQLs are correctly\npredicted, while IM evaluates if the questions for\nthe same interaction are correctly predicted.\n4.3 Q1. Constrained decoding\nLMs are unconstrained and thus can generate any\ntoken at any given time. For SQL generation, LMs\nmay generate SQL that are syntactically incorrect,\nwhich impact their performance.\nHere, we compare the performance of an uncon-\nstrained LM against T5QL without the ranker com-\nponent. Both methods use the same LM, namely\nT5-Base, and are trained using the same training\n4https://worksheets.codalab.org/worksheets/\n0x0049b642db90440e9eaf9cf6a850b4c9\n5We will make the code available in github after the blind\nreview process is finalized.\n281\nconfiguration; the only difference is that T5QL uses\nconstrained decoding as described in Section 3.2.\nBoth methods serialize the database schema as a\nstring and append it to the source sequence sim-\nilarly to Suhr et al. (2020). Similarly to Scholak\net al. (2021), we train both methods for a maximum\nof 512 training epochs with mini batch size of 5,\n205 gradient accumulation steps, with a learning\nrate of 1e−4, and an adafactor optimizer with ep-\nsilon set as 1e−6. We evaluate the models using\nbeam search with 1, 2, 4, and 8 beams. Contrary to\nScholak et al. (2021), we report results for a batch\nsize of 1025 instead of 2048 since it lead to better\nresults in our case.\nFigure 3 shows the performance of several meth-\nods and those results are discussed in this subsec-\ntion and in the next ones. All methods use T5-Base\nas its LLM, except for PICARD which is the cur-\nrent SOTA and uses T5-3B, a much larger LM.\nFrom Figure 3, we observe that T5 achieves\n≈55.1% EM accuracy using one beam, and its\nperformance does not improve with the beam size.\nOur method, T5QL, without the ranker compo-\nnent (i.e., T5QL wo/ Ranker in Figure 3) achieves\n≈65.7% EM accuracy using one beam, a gain of\n≈10.6pp, which is a relative gain of ≈19.2%.\nUsing 2 and 4 beams, we improve T5QL’s perfor-\nmance to ≈67.6% and ≈68%, a gain of ≈1.9pp\nand ≈2.3pp, respectively, when compared against\nT5QL using only one beam. We observe a loss of\nperformance when using 8 beams. These results\nhighlight the advantage of using constrained de-\ncoding for SQL generation: by using a CFG to\nguarantee that the LM always generates valid SQL,\nwe improve the model’s performance.\n4.4 Q2. Compositional generalization\nCompositional generalization of LLMs has at-\ntracted attention in recent years. Shaw et al. (2021)\npropose Spider-SSP, a dataset that can be used to\nmeasure the compositional generalization of SQL\ngeneration methods. In this section we use Spider-\nSSP to evaluate if constraint decoding increases the\ncompositional generalization capabilities of T5QL.\nShaw et al. (2021) already reported that T5-Base\nmodel struggles in most splitting strategies, partic-\nularly when using length-based split and TMCD\nsplit; we reproduce those results in Table 1 in rows\nT5-Base and T5-3B. We note that, in their experi-\nments, the predicted SQL follows the convention\nof predicting first the select statement and then the\nfrom statement. As discussed in Section 3.2, T5QL\nfirst predicts the from statement and then the select\nstatement. Thus, we evaluate two different models:\nT5-Base, which is similar to the model evaluated\nby Shaw et al. (2021), and T5QL-Base wo/ CD\nwhich is T5QL without the constrained decoding\ncomponent (and without the ranker). We compare\nthese models against T5QL-Base and T5-3B; the\nlatter also predicts the select statement first.\nWe observe that T5QL-Base wo/ CD obtains sig-\nnificantly higher EM than T5-Base, namely for the\nTMCD split where there is a gain of 22pp, which\nis a 52% relative gain. These results highlight that\npredicting the tables before predicting the columns\nseems to help the model. This result corroborates\nthe results obtained by Lin et al. (2020), which\nuse a representation similar to ours. We also verify\nthat T5QL-Base slightly, but consistently, improves\nupon the results obtained by T5QL-Base wo/ CD\nfor all splitting strategies, namely in TMCD where\nthere is a gain of 2pp. Finally, we conclude that our\nstrategy narrows the performance gap between the\nperformance of methods using small LMs (i.e., T5-\nBase) and very large LMs (i.e., T5-3B) by compar-\ning the performance of T5QL-Base against T5-3B.\n4.5 Q3. Generalize to conversational setting\nOften users might want to explore their data with-\nout having to write SQL. Thus, a conversational\nsetting where user’s iteratively ask questions to an\nAI is particularly interesting. Yu et al. (2019) pro-\npose a dataset comprised of multiple question-SQL\npairs, each consisting of several user interactions.\nThey evaluate SQL generation methods using QM\nand IM. In this section we use CoSQL to evaluate\nif constrained decoding increases the performance\nof T5SQL in the conversational setting.\nWe observe gains of≈7.9% and ≈5.5% in QM\nSpider-SSP\nModel Rand. Templ. Len. TMCD\nT5-Base 76.5 45 .3 42 .5 42 .3\nT5QL-Base wo/ CD 84.7 58 .3 50 .6 64 .4\nT5QL-Base 85.7 61 .1 54 .4 65 .9\nT5-3B 85.6 64 .8 56 .7 69 .6\nTable 1: EM accuracy in the Spider-SSP dataset using\ndifferent splitting strategies. T5QL-Base wo/ CD (i.e.,\nwithout constrained decoding) and T5QL-Base adopt\nthe strategy of predicting SQL with the from statement\nbefore the select statement, while T5-Base and T5-3B\ndo the opposite, which is the default.\n282\nCoSQL\nModel QM IM\nT5QL-Base wo/ CD 42.8 14 .8\nT5QL-Base 50.7 20 .3\nPICARD - 3B 56.9 24 .2\nTable 2: QM and IM in the CoSQL development set.\nand IM, respectively, when we add constrained de-\ncoding to T5QL-Base. We observe that PICARD-\n3B is still SOTA for the task, but the gap is sig-\nnificantly narrower. This is further evidence that\nconstrained decoding can improve the performance\nof LMs in multiple SQL generation tasks.\n4.6 Q4. Ranker\nCurrent SOTA methods, such as PICARD, use\nbeam search to find the best candidate and out-\nput it as the final prediction. Here we test whether\nwe can boost predictive performance by, instead\nof using the beam-selected best candidate as the\nfinal prediction, having a ranker that finds the best\ncandidate among the list of candidate predictions\nfound by the generator.\nOur first step to validate this hypothesis is to run\nbeam search for multiple beam sizesk, namely 1, 2,\n4, and 8, and measure the accuracy@k. In our set-\nting, the accuracy@k can be regarded as an oracle\nranker than can always find the correct candidate\nif it is present in the list of candidate generations.\nFrom Figure 3 we observe that this oracle could\nachieve 78.2% EM accuracy with 8 beams, sur-\npassing the performance of PICARD-3B but using\nT5-Base instead of T5-3B, which is highly desir-\nable due to T5-3B’s expensive nature in terms of\nGPU costs. Thus, our goal here is to build a ranker\nmodel that can boost the performance of T5QL\nwithout the ranker (T5QL wo/ Ranker in Figure 3)\nand approximate it to the oracle’s performance.\nWe note that the ranker model should be of a\ncomparable size to the generator, i.e., fit in the\nsame GPU. Otherwise, the advantage of using a\nsmall LM as the generator is lost since we assume\nthat the practitioner has hardware that can fit the\nlarger ranker, which might not be true. Here we\nuse T5-Base as the generator and CodeBERT as\nthe ranker, which are of comparable size.\nTo train the ranker model, we first create a\ndataset following the steps described in Section 3.5.\nThen, we fine-tune a CodeBERT model for 50,000\ntraining steps, using a batch size of 32 and 1 gra-\ndient accumulation step, with a 1e−3 learning rate\nand an AdamW optimizer with weight decay of\n1e−2 and a linear schedule with warmup of 1% of\nthe steps. We use Equation 1 to score the generated\nSQL; we conduct hyperparameter tuning for λand\nconclude that λ= 2e−2 performs best.\nWe analyze if combining the generator’s score\nwith the ranker’s score is superior to using each\nof the score’s individually. From Figure 3 we con-\nclude that combining the ranker model’s score with\nthe generator model’s score (i.e., the T5QL plot)\nimproves the best EM from 67.9% to 69.3% when\ncompared against T5QL without the ranker score.\nFurthermore, we also observe that using only the\nranker score (i.e., the T5QL Ranker Score plot)\nleads to a drop in performance even when com-\npared against T5QL wo/ Ranker. This effect is\nmore noticeable for larger beam sizes, which indi-\ncates that the ranker model struggles to differentiate\nthe correct SQL from the wrong SQL.\nFrom these experiments, we conclude that the\nranker boosts the performance of the generator.\nHowever, the ranker’s score needs to be combined\nwith the generator’s score to guarantee that the\nranker’s score does not completely dominate the\ngenerator’s predictions. We should also note that\nthere is a very large gap between our ranker and\nthe oracle, which leaves room for future research\nto improve the ranker model. We believe that this a\npromising line of research that can further narrow\nthe gap between the performance between small\nLMs and large LMs.\nFinally, we run T5SQL on Spider’s test set and\nobtain 66.8% EX and 65.9% EM. These results\nrank among the top-10 best models in terms of EX,\nand as the 22nd best in terms of EM6, whilst using\nsmall models. Small models have the advantage of\nbeing less computationally expensive and allowing\nmore easily for the use of ensemble methods.\n5 Conclusion\nHere we put forward T5QL, a new method for\nSQL generation with SOTA performance on bench-\nmark datasets when using small LMs. T5QL uses\nconstrained decoding to improve predictive perfor-\nmance and also to guarantee that the generated SQL\nis always valid. Futhermore, we complement the\ngenerator model with a ranker model that is capa-\nble of choosing the best candidate SQL from a pool\nof a few candidates.\n6https://yale-lily.github.io/spider\n283\nReferences\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nAlvin Cheung, Armando Solar-Lezama, and Samuel\nMadden. 2012. Inferring sql queries using program\nsynthesis. arXiv preprint arXiv:1208.2013.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,\nXiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020.\nCodeBERT: A Pre-Trained Model for Program-\nming and Natural Languages. arXiv e-prints, page\narXiv:2002.08155.\nYujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver,\nJohn R Woodward, John Drake, and Qiaofu Zhang.\n2021. Natural sql: Making sql easier to infer\nfrom natural language specifications. arXiv preprint\narXiv:2109.05153.\nShirley Anugrah Hayati, Raphael Olivier, Pravalika Av-\nvaru, Pengcheng Yin, Anthony Tomasic, and Graham\nNeubig. 2018. Retrieval-based neural code genera-\ntion. arXiv preprint arXiv:1808.10025.\nAishwarya Kamath and Rajarshi Das. 2018. A\nsurvey on semantic parsing. arXiv preprint\narXiv:1812.00978.\nKalpesh Krishna, Yapei Chang, John Wieting, and Mo-\nhit Iyyer. 2022. Rankgen: Improving text gener-\nation with large ranking models. arXiv preprint\narXiv:2205.09726.\nTriet HM Le, Hao Chen, and Muhammad Ali Babar.\n2020. Deep learning for source code modeling and\ngeneration: Models, applications, and challenges.\nACM Computing Surveys (CSUR), 53(3):1–38.\nXi Victoria Lin, Richard Socher, and Caiming Xiong.\n2020. Bridging textual and tabular data for cross-\ndomain text-to-sql semantic parsing. arXiv preprint\narXiv:2012.12627.\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A Smith. 2021. Provable limitations of acquir-\ning meaning from ungrounded form: What will future\nlanguage models understand? Transactions of the\nAssociation for Computational Linguistics, 9:1047–\n1060.\nPedro Orvalho, Miguel Terra-Neves, Miguel Ven-\ntura, Ruben Martins, and Vasco Manquinho. 2020.\nSquares: a sql synthesizer using query reverse en-\ngineering. Proceedings of the VLDB Endowment ,\n13(12):2853–2856.\nEmilio Parisotto, Abdel-rahman Mohamed, Rishabh\nSingh, Lihong Li, Dengyong Zhou, and Pushmeet\nKohli. 2016. Neuro-symbolic program synthesis.\narXiv preprint arXiv:1611.01855.\nAmir Pnueli and Roni Rosner. 1989. On the synthesis\nof a reactive module. In Proceedings of the 16th\nACM SIGPLAN-SIGACT symposium on Principles of\nprogramming languages, pages 179–190.\nGabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Ti-\nwari, Gustavo Soares, Christopher Meek, and Sumit\nGulwani. 2022. Synchromesh: Reliable code gen-\neration from pre-trained language models. arXiv\npreprint arXiv:2201.11227.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. Picard: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. arXiv preprint arXiv:2109.05093.\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and\nKristina Toutanova. 2021. Compositional general-\nization and natural language variation: Can a se-\nmantic parsing approach handle both? ArXiv,\nabs/2010.12725.\nPeng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu,\nAlexander Hanbo Li, Jun Wang, Cicero Nogueira\ndos Santos, and Bing Xiang. 2020. Learning Con-\ntextual Representations for Semantic Parsing with\nGeneration-Augmented Pre-Training. arXiv e-prints,\npage arXiv:2012.10309.\nAlane Laughlin Suhr, Kenton Lee, Ming-Wei Chang,\nand Pete Shaw. 2020. Exploring unexplored gen-\neralization challenges for cross-database semantic\nparsing.\nZeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li,\nand Lu Zhang. 2019. A grammar-based structural\ncnn decoder for code generation. In Proceedings of\nthe AAAI conference on artificial intelligence , vol-\nume 33, pages 7055–7062.\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,\nand Neel Sundaresan. 2020. Intellicode compose:\nCode generation using transformer. In Proceedings\nof the 28th ACM Joint Meeting on European Software\nEngineering Conference and Symposium on the Foun-\ndations of Software Engineering, pages 1433–1443.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2019. Rat-sql:\nRelation-aware schema encoding and linking for text-\nto-sql parsers. arXiv preprint arXiv:1911.04942.\nChunyang Xiao, Marc Dymetman, and Claire Gardent.\n2016. Sequence-based structured prediction for se-\nmantic parsing. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1341–1350,\nBerlin, Germany. Association for Computational Lin-\nguistics.\n284\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir R. Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. Unifiedskg:\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models. CoRR,\nabs/2201.05966.\nNavid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and\nThomas Dillig. 2017. Type-and content-driven syn-\nthesis of sql queries from natural language. arXiv\npreprint arXiv:1702.01168.\nXi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou,\nand Caiming Xiong. 2022. Rng-kbqa: Generation\naugmented iterative ranking for knowledge base ques-\ntion answering. In ACL.\nKyra Yee, Yann Dauphin, and Michael Auli. 2019.\nSimple and effective noisy channel modeling for\nneural machine translation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5696–5701, Hong Kong,\nChina. Association for Computational Linguistics.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\narXiv preprint arXiv:1704.01696.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\nWang, Yi Chern Tan, Xinyi Yang, Dragomir Radev,\nRichard Socher, and Caiming Xiong. 2020. Grappa:\ngrammar-augmented pre-training for table semantic\nparsing. arXiv preprint arXiv:2009.13845.\nTao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, et al. 2019. Cosql: A conversational\ntext-to-sql challenge towards cross-domain natural\nlanguage interfaces to databases. arXiv preprint\narXiv:1909.05378.\nLiang Zhao, Hexin Cao, and Yunsong Zhao. 2021. GP:\nContext-free Grammar Pre-training for Text-to-SQL\nParsers. arXiv e-prints, page arXiv:2101.09901.\nA SQL generation analysis\nIn this section we analyse in detail the predictions\ngenerated by T5QL. In Appendix A.1 we measure\nhow often T5QL outputs valid SQLs and give an\nexample of one invalid SQL. In Appendix A.2 we\nshow an example of how constraining column name\ngeneration can boost performance.\nA.1 Valid SQL generation\nFirst, we check if T5QL using constrained decod-\ning can still generate unparsable SQL. We obtain\nT5QL-Base’s predictions in Spider’s development\nset for beam sizes of 1, 2, 4 and 8. We observe that:\n• T5QL never generates an unparsable SQL for\nthe top-1 beam when the beam size >1.\n• Invalid SQL is generated when the LM (i.e.,\nT5) enters a loop, as can be seen in Listing 1.\nSince the SQL length is limited, T5QL outputs\nthe incomplete (and invalid) SQL. The loop,\neven if abnormal, is valid SQL syntax, e.g, an\naverage of averages.\n• For larger beam sizes (e.g, 8) we saw that\nthe aforementioned model hallucinations are\nmainly present in the lower scored beams.\nListing 1: Invalid SQL generated by T5QL. For space\nconcerns we abbreviate the generated SQL.\nfrom stadium select name , capacity\norder by avg ( avg ( avg ( avg (\navg ( avg ( avg ( avg ( avg ( avg (\navg ( avg ( avg ( avg ( avg ( avg (\navg ( avg ( avg ( avg ( max ( avg (\nmin ( min ( min ( min ( min ( max (\nmax ( max ( max ( max ( max ( max (\nmax ( max ( max ( max ( max ( max (\nmax ( max ( max ( max ( max ( max (\nmax ( max ( max ( max ( ...\nNext, we analyse whether model size reduces\nthe number of invalid SQL generated by T5QL.\nWe obtain the predictions in Spider’s development\nset using T5QL-Base and T5QL-Large with and\nwithout constrained decoding. We report results of\nthe four methods using 4 beams.\nWe observe that increasing the size of the model\nalso increases the ability of the model to gener-\nate parsable SQL: T5QL-Base wo/ CD generates\n≈20% invalid SQLs, while T5QL-Large wo/ CD\ngenerates only ≈5% invalid SQLs (Figure 4). No-\ntice, however, that 5% is still a substantial amount\nof invalid SQLs. On the other hand, when using\nconstrained decoding, T5QL always produces valid\n285\nT5QL-Base wo/ CD\nT5QL-Base\nT5QL-Large wo/ CD\nT5QL-Large\n80\n85\n90\n95\n100\nmodel\npercentage of parsable sql\nFigure 4: Percentage of parsable SQL, in the Spider’s\ndevelopment set, in each model configuration. All meth-\nods use beam search with 4 beams and we report results\nfor the first beam.\nSQLs when considering only the top-1 beam of\nbeam search with 4 beams; this is true for both\nT5QL-Base and T5QL-Large.\nA.2 Enforce existing table and column names\nFinally, we analyse what is the impact of constrain-\ning the table and column names during SQL gen-\neration. When T5QL does not constrain column\nand table names, it can generate examples such as\nthe one in Listing 2 where \"song_id\" is a column\nname that does not exist in the schema. When con-\nstraining column and table names, T5QL always\ngenerates existing column and table names, and, in\nthis case, predicts the correct SQL (Listing 3).\nListing 2: Invalid SQL generated by T5QL wo/ CD. In\nthis case the T5QL generated an non-existing column.\nfrom singer as t1 join\nsinger_in_concert as t2 on\nt1. song_id = t2. song_id\nselect t1.name , count ( * )\ngroup by t1. song_id\nListing 3: Valid and correct SQL generated by T5QL\nwith CD for the same example as Listing 2.\nfrom singer as t1 join\nsinger_in_concert as t2 on\nt1. singer_id = t2. singer_id\nselect t1.name , count ( * )\ngroup by t1. singer_id\n1 2 3 40.4\n0.5\n0.6\n0.7\nbeam size\nexact match accuracyBase wo/ CD Large wo/ CD\nBase wo/ R Large wo/ R\nBase Large\nFigure 5: Comparison of EM accuracy in Spider’s de-\nvelopment between different model configurations. The\n\"Base\" model refers to T5QL-Base with constraint de-\ncoding and reranking; models \"wo/ CD\" are the mod-\nels without constraint decoding nor reranking, whereas\nmodels \"wo/ R\" are the models without reranking.\nB Larger models\nExperiments shown for our proposed method,\nT5QL, used T5-Base as the generator LM. We\nmake this choice since our focus is to show that\nsmall LMs can have good performance even when\ncompared against very large LMs. Nevertheless,\nevaluating if the proposed techniques, namely con-\nstrained decoding and reranking, scale to larger\nLMs is an interesting research question. Thus, we\nevaluate whether constrained decoding and rerank-\ning improve the performance of T5SQL-Large.\nFrom Figure 5 we observe that the performance\nof T5QL-Base (i.e., Base) is superior to T5-Large\n(i.e., Large wo/CD) for 2–4 beams. When we add\nthe constrained decoding component to T5-Large\n(i.e., Large wo/ R), the performance is significantly\nsuperior. This results highlights the importance of\nadding constrained decoding for SQL generation.\nHowever, we do not observe gains of adding the\nreranker model to T5-Large (i.e., Large), which\nwe observed in T5-Base. This might indicate, as\nwe pointed out in Section 3.3, that finding better\nreranking strategies is an interesting research path.\nWe do not include results for T5QL-3B since our\nmain goal in this work is to increase performance\nusing multiple smaller components and domain-\naware techniques (e.g., constrained decoding) in-\nstead of relying on very large models. Furthermore,\ncomputing results for T5-3B is very costly in terms\nof money and time.\n286",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8697993755340576
    },
    {
      "name": "SQL",
      "score": 0.8055820465087891
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6870210766792297
    },
    {
      "name": "Parsing",
      "score": 0.6663138270378113
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5584487915039062
    },
    {
      "name": "Programming language",
      "score": 0.475010484457016
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4500666856765747
    },
    {
      "name": "Grammar",
      "score": 0.44581979513168335
    },
    {
      "name": "Natural language processing",
      "score": 0.3985063433647156
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": []
}