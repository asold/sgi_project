{
  "title": "TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up",
  "url": "https://openalex.org/W3166513219",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2366525249",
      "name": "Jiang, Yifan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744557519",
      "name": "Chang, Shiyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354751725",
      "name": "Wang, Zhangyang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963200935",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2758188475",
    "https://openalex.org/W2963873275",
    "https://openalex.org/W3034291999",
    "https://openalex.org/W3121661546",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2118858186",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2593414223",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2981721547",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2970176896",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3009624218",
    "https://openalex.org/W2964013315",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3016355810",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2989221291",
    "https://openalex.org/W3034720584",
    "https://openalex.org/W2617573776",
    "https://openalex.org/W3139445856",
    "https://openalex.org/W3134689216",
    "https://openalex.org/W3002944878",
    "https://openalex.org/W2883334176",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2502312327",
    "https://openalex.org/W2963312584",
    "https://openalex.org/W3148140980",
    "https://openalex.org/W2962879692",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W2687693326",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3035170495",
    "https://openalex.org/W3035687950",
    "https://openalex.org/W2796322794",
    "https://openalex.org/W2953327099",
    "https://openalex.org/W3035872583",
    "https://openalex.org/W1710476689",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W3135404760",
    "https://openalex.org/W2750699642",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2785678896",
    "https://openalex.org/W967544008",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2962892300",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W2953030256",
    "https://openalex.org/W3111551570",
    "https://openalex.org/W2766527293",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964259506",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2173520492",
    "https://openalex.org/W2981660215",
    "https://openalex.org/W2810518847",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W2751842161",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2787223504",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W3128976935",
    "https://openalex.org/W2963330667",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W3043547428",
    "https://openalex.org/W2951523806",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W2949864127",
    "https://openalex.org/W2983541695"
  ],
  "abstract": "The recent explosive interest on transformers has suggested their potential to become powerful \"universal\" models for computer vision tasks, such as classification, detection, and segmentation. While those attempts mainly study the discriminative models, we explore transformers on some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs). Our goal is to conduct the first pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed TransGAN, consists of a memory-friendly transformer-based generator that progressively increases feature resolution, and correspondingly a multi-scale discriminator to capture simultaneously semantic contexts and low-level textures. On top of them, we introduce the new module of grid self-attention for alleviating the memory bottleneck further, in order to scale up TransGAN to high-resolution generation. We also develop a unique training recipe including a series of techniques that can mitigate the training instability issues of TransGAN, such as data augmentation, modified normalization, and relative position encoding. Our best architecture achieves highly competitive performance compared to current state-of-the-art GANs using convolutional backbones. Specifically, TransGAN sets new state-of-the-art inception score of 10.43 and FID of 18.28 on STL-10, outperforming StyleGAN-V2. When it comes to higher-resolution (e.g. 256 x 256) generation tasks, such as on CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual examples with high fidelity and impressive texture details. In addition, we dive deep into the transformer-based generation models to understand how their behaviors differ from convolutional ones, by visualizing training dynamics. The code is available at https://github.com/VITA-Group/TransGAN.",
  "full_text": "TransGAN: Two Pure Transformers Can Make One\nStrong GAN, and That Can Scale Up\nYifan Jiang1, Shiyu Chang2,3, Zhangyang Wang1\n1University of Texas at Austin\n2UC Santa Barbara 3MIT-IBM Watson AI Lab\n{yifanjiang97,atlaswang}@utexas.edu, chang87@ucsb.edu\nAbstract\nThe recent explosive interest on transformers has suggested their potential to\nbecome powerful “universal\" models for computer vision tasks, such as classi-\nﬁcation, detection, and segmentation. While those attempts mainly study the\ndiscriminative models, we explore transformers on some more notoriously difﬁ-\ncult vision tasks, e.g., generative adversarial networks (GANs). Our goal is to\nconduct the ﬁrst pilot study in building a GAN completely free of convolutions,\nusing only pure transformer-based architectures. Our vanilla GAN architecture,\ndubbed TransGAN, consists of a memory-friendly transformer-based generator\nthat progressively increases feature resolution, and correspondingly a multi-scale\ndiscriminator to capture simultaneously semantic contexts and low-level textures.\nOn top of them, we introduce the new module of grid self-attention for alleviating\nthe memory bottleneck further, in order to scale up TransGAN to high-resolution\ngeneration. We also develop a unique training recipe including a series of tech-\nniques that can mitigate the training instability issues of TransGAN, such as data\naugmentation, modiﬁed normalization, and relative position encoding. Our best\narchitecture achieves highly competitive performance compared to current state-\nof-the-art GANs using convolutional backbones. Speciﬁcally, TransGAN sets the\nnew state-of-the-art inception score of 10.43 and FID of 18.28 on STL-10. It also\nreaches the inception score of 9.02 and FID of 9.26 on CIFAR-10, and 5.28 FID on\nCelebA 128×128, respectively: both on par with the current best results. When it\ncomes to higher-resolution (e.g. 256 ×256) generation tasks, such as on CelebA-\nHQ and LSUN-Church, TransGAN continues to produce diverse visual examples\nwith high ﬁdelity and reasonable texture details. In addition, we dive deep into\nthe transformer-based generation models to understand how their behaviors differ\nfrom convolutional ones, by visualizing training dynamics. The code is available\nat: https://github.com/VITA-Group/TransGAN.\n1 Introduction\nGenerative adversarial networks (GANs) have gained considerable success on numerous tasks [1–7].\nUnfortunately, GANs suffer from the notorious training instability, and numerous efforts have been\ndevoted to stabilizing GAN training, introducing various regularization terms [8–11], better losses\n[1, 12–14], and training recipes [ 15, 16]. Among them, one important route to improving GANs\nexamines their neural architectures. [17, 8] reported a large-scale study of GANs and observed that\nwhen serving as (generator) backbones, popular neural architectures perform comparably well across\nthe considered datasets. Their ablation study suggested that most of the variations applied in the\nResNet family resulted in very marginal improvements. Nevertheless, neural architecture search\n(NAS) was later introduced to GANs and suggests enhanced backbone designs are also important\nfor improving GANs, just like for other computer vision tasks. Those works are consistently able to\ndiscover stronger GAN architectures beyond the standard ResNet topology [18–20]. Other efforts\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2102.07074v4  [cs.CV]  9 Dec 2021\nFigure 1: Representative visual examples synthesized by TransGAN, without using convolutional\nlayers. (a) The synthesized visual examples on CelebA-HQ ( 256 ×256) dataset. (b) The linear\ninterpolation results between two latent vectors, on CelebA-HQ (256 ×256) dataset.\ninclude customized modules such as self-attention [21], style-based generator [22], and autoregressive\ntransformer-based part composition [23].\nHowever, one last “commonsense\" seems to have seldomly been challenged: using convolutional\nneural networks (CNNs) as GAN backbones. The original GAN [ 24, 25] used fully-connected\nnetworks and can only generate small images. DCGAN [26] was the ﬁrst to scale up GANs using\nCNN architectures, which allowed for stable training for higher resolution and deeper generative\nmodels. Since then, in the computer vision domain, every successful GAN relies on CNN-based\ngenerators and discriminators. Convolutions, with the strong inductive bias for natural images,\ncrucially contribute to the appealing visual results and rich diversity achieved by modern GANs.\nCan we build a strong GAN completely free of convolutions?This is a question not only arising\nfrom intellectual curiosity, but also of practical relevance. Fundamentally, a convolution operator\nhas a local receptive ﬁeld, and hence CNNs cannot process long-range dependencies unless passing\nthrough a sufﬁcient number of layers. However, that is inefﬁcient, and could cause the loss of feature\nresolution and ﬁne details, in addition to the difﬁculty of optimization. Vanilla CNN-based models are\ntherefore inherently not well suited for capturing an input image’s “global\" statistics, as demonstrated\nby the beneﬁts from adopting self-attention [21] and non-local [27] operations in computer vision.\nMoreover, the spatial invariance possessed by convolution poses a bottleneck on its ability of adapting\nto spatially varying/heterogeneous visual patterns, which also motivates the success of relational\nnetwork [28], dynamic ﬁlters [29, 30] and kernel prediction [31] methods.\n1.1 Our Contributions\nThis paper aims to be the ﬁrst pilot study to build a GAN completely free of convolutions, using\nonly pure transformer-based architectures. We are inspired by the recent success of transformer\narchitectures in computer vision [32–34]. Compared to parallel generative modeling works [21, 23,\n35] that applied self-attention or transformer encoder in conjunction with CNN-based backbones, our\ngoal is more ambitious and faces several daunting gaps ahead. First and foremost, although a pure\ntransformer architecture applied directly to sequences of image patches can perform very well on\nimage classiﬁcation tasks [34], it is unclear whether the same way remains effective in generating\nimages, which crucially demands the spatial coherency in structure, color, and texture, as well as the\nrichness of ﬁne details. The handful of existing transformers that output images have unanimously\nleveraged convolutional part encoders [ 23] or feature extractors [ 36, 37]. Moreover, even given\nwell-designed CNN-based architectures, training GANs is notoriously unstable and prone to mode\ncollapse [15]. Training vision transformers are also known to be tedious, heavy, and data-hungry [34].\nCombining the two will undoubtedly amplify the challenges of training.\nIn view of those challenges, this paper presents a coherent set of efforts and innovations towards\nbuilding the pure transformer-based GAN architectures, dubbed TransGAN. A naive option may\ndirectly stack multiple transformer blocks from raw pixel inputs, but that would scale poorly due\nto memory explosion. Instead, we start with a memory-friendly transformer-based generator by\ngradually increasing the feature map resolution in each stage. Correspondingly, we also improve\nthe discriminator with a multi-scale structure that takes patches of varied size as inputs, which\n2\nbalances between capturing global contexts and local details, in addition to enhancing memory\nefﬁciency more. Based on the above generator-discriminator design, we introduce a new module\ncalled grid self-attention, that alleviates the memory bottleneck further when scaling up TransGAN\nto high-resolution generation (e.g. 256 ×256).\nTo address the aforementioned instability issue brought by both GAN and Transformer, we also\ndevelop a unique training recipe in association with our innovative TransGAN architecture, that\neffectively stabilizes its optimization and generalization. That includes showings the necessity of data\naugmentation, modifying layer normalization, and replacing absolute token locations with relative\nposition encoding. Our contributions are outlined below:\n• Novel Architecture Design: We build the ﬁrst GAN using purely transformers and no\nconvolution. TransGAN has customized a memory-friendly generator and a multi-scale\ndiscriminator, and is further equipped with a new grid self-attention mechanism. Those\narchitectural components are thoughtfully designed to balance memory efﬁciency, global\nfeature statistics, and local ﬁne details with spatial variances.\n• New Training Recipe: We study a number of techniques to train TransGAN better, includ-\ning leveraging data augmentation, modifying layer normalization, and adopting relative\nposition encoding, for both generator and discriminator. Extensive ablation studies, discus-\nsions, and insights are presented.\n• Performance and Scalability: TransGAN achieves highly competitive performance com-\npared to current state-of-the-art GANs. Speciﬁcally, it sets the new state-of-the-art inception\nscore of 10.43 and FID score of 18.28 on STL-10. It also reaches competitive 9.02 inception\nscore and 9.26 FID on CIFAR-10, and 5.28 FID score on CelebA 128 ×128, respectively.\nMeanwhile, we also evaluate TransGAN on higher-resolution (e.g., 256 ×256) generation\ntasks, where TransGAN continues to yield diverse and impressive visual examples.\n2 Related Works\nGenerative Adversarial Networks. After its origin, GANs quickly embraced fully convolutional\nbackbones [26], and inherited most successful designs from CNNs such as batch normalization,\npooling, (Leaky) ReLU and more [ 38–40, 18]. GANs are widely adopted in image translation\n[3, 4, 41], image enhancement [ 7, 42, 43], and image editing [ 44, 45]. To alleviate its unstable\ntraining, a number of techniques have been studied, including the Wasserstein loss [46], the style-\nbased generator [22], progressive training [16], lottery ticket [47], and spectral normalization [48].\nTransformers in Computer Vision. The original transformer was built for NLP [49], where the\nmulti-head self-attention and feed-forward MLP layer are stacked to capture the long-term correlation\nbetween words. A recent work [34] implements highly competitive ImageNet classiﬁcation using\npure transformers, by treating an image as a sequence of 16 ×16 visual words. It has strong\nrepresentation capability and is free of human-deﬁned inductive bias. In comparison, CNNs exhibit a\nstrong bias towards feature locality, as well as spatial invariance due to sharing ﬁlter weights across\nall locations. However, the success of original vision transformer relies on pretraining on large-scale\nexternal data. [50, 51] improve the data efﬁciency and address the difﬁculty of optimizing deeper\nmodels. Other works introduce the pyramid/hierarchical structure to transformer [52–54] or combine\nit with convolutional layers [55, 56]. Besides image classiﬁcation task, transformer and its variants\nare also explored on image processing [ 37], point cloud [57], semantic segmentation [58], object\ndetection [32, 59] and so on. A comprehensive review is referred to [60].\nTransformer Modules for Image Generation. There exist several related works combining the\ntransformer modules into image generation models, by replacing certain components of CNNs. [61]\nﬁrstly formulated image generation as autoregressive sequence generation, for which they adopted\na transformer architecture. [ 62] propose sparse factorization of the attention matrix to reduce its\ncomplexity. While those two works did not tackle the GANs, one recent (concurrent) work [ 23]\nused a convolutional GAN to learn a codebook of context-rich visual parts, whose composition is\nsubsequently modeled with an autoregressive transformer architecture.The authors demonstrated\nsuccess in synthesizing high-resolution images. However, the overall CNN architecture remains in\nplace (including CNN encoder/decoder for the generators, and a fully CNN-based discriminator), and\nthe customized designs (e.g, codebook and quantization) also limit their model’s versatility. Another\nconcurrent work [35] employs a bipartite self-attention on StyleGAN and thus it can propagate latent\nvariables to the evolving visual features, yet its main structure is still convolutional, including both the\n3\nCTransformer \nBlocks\n2x UpScale\nTransformer\nBlocks\n2x UpScale\nGrid Transformer Blocks\nReal / Fake ?\nMLP\nNoise Input\nGenerator Discriminator\nLinear Unflatten\n(256 x 256) x\n(8 x 8) x C\nHead\n(16 x 16) x C\n256 x 256 x 3\n2x AvgPool\nGrid Transformer Blocks\nCLS\nConcatenate\nGrid Transformer \nBlocks(128 x 128) x \nTransformer Blocks\n2x AvgPool\nConcatenate\nTransformer \nBlocks\nx 2\nx 2\nH\n2P\nW\n2P\nC\n2X X\nH\n4P\nW\n4PX X\nH\nP\nW\nP\nC\n4X X\nC\n16\nC\n4\nLinearLinearLinear\nTransformer \nBlocks\n(             )\n(             )\n(             )\nFigure 2: The pipeline of the pure transform-based generator and discriminator of TransGAN. We take\n256×256 resolution image generation task as a typical example to illustrate the main procedure. Here\npatch size pis set to 32 as an example for the convenience of illustration, while practically the patch\nsize is normally set to be no more than 8 ×8, depending on the speciﬁc dataset. Grid Transformer\nBlocks refers to the transformer blocks with the proposed grid self-attention. Detailed architecture\nconﬁgurations are included in Appendix B.\ngenerator and discriminator. To our best knowledge, no other existing work has tried to completely\nremove convolutions from their generative modeling frameworks.\n3 Technical Approach: A Journey Towards GAN with Pure Transformers\nIn this section, we start by introducing the memory-friendly generator and multi-scale discriminator,\nequipped with a novel grid self-attention. We then introduce a series of training techniques to stabilize\nits training procedure, including data augmentation, the modiﬁed normalization, and injecting relative\nposition encoding to self-attention.\nTo start with, we choose the transformer encoder [49] as our basic block and try to make minimal\nchanges. An encoder is a composition of two parts. The ﬁrst part is constructed by a multi-head\nself-attention module and the second part is a feed-forward MLP with GELU non-linearity. The\nnormalization layer is applied before both of the two parts. Both parts employ residual connection.\n3.1 Memory-friendly Generator\nThe task of generation poses a high standard for spatial coherency in structure, color, and texture, both\nglobally and locally. The transformer encoders take embedding token words as inputs and calculate\nthe interaction between each token recursively. [63, 34]. The main dilemma here is: what is the right\n“word\" for image generation tasks? If we similarly generate an image in a pixel-by-pixel manner\nthrough stacking transformer encoders, even a low-resolution image (e.g. 32 ×32) can result in an\nexcessively long sequence (1024), causing the explosive cost of self-attention (quadratic w.r.t. the\nsequence length) and prohibiting the scalability to higher resolutions. To avoid this daunting cost,\nwe are inspired by a common design philosophy in CNN-based GANs, to iteratively upscale the\nresolution at multiple stages [25, 16]. Our strategy is hence to increase the input sequence and reduce\nthe embedding dimension gradually .\nFigure 2 (left) illustrates a memory-friendly transformer-based generator that consists of multiple\nstages. Each stage stacks several transformer blocks. By stages, we gradually increase the feature\n4\nmap resolution until it meets the target resolution H ×W. Speciﬁcally, the generator takes the\nrandom noise as its input, and passes it through a multiple-layer perceptron (MLP) to a vector of\nlength H0 ×W0 ×C. The vector is reshaped into a H0 ×W0 resolution feature map (by default we\nuse H0 = W0 = 8), each point a C-dimensional embedding. This “feature map\" is next treated as a\nlength-64 sequence of C-dimensional tokens, combined with the learnable positional encoding.\nTo scale up to higher-resolution images, we insert an upsampling module after each stage, consisting\nof a reshaping and resolution-upscaling layer. For lower-resolution stages (resolution lower than\n64 ×64), the upsampling module ﬁrstly reshapes the 1D sequence of token embedding back to a\n2D feature map Xi ∈RHi×Wi×C and then adopts the bicubic layer to upsample its resolution\nwhile the embedded dimension is kept unchanged, resulting in the output X\n′\ni ∈R2Hi×2Wi×C. After\nthat, the 2D feature map X\n′\ni is again reshaped into the 1D sequence of embedding tokens. For\nhigher-resolution stages, we replace the bicubic upscaling layer with the pixelshuffle module,\nwhich upsamples the resolution of feature map by2×ratio and also reduces the embedding dimension\nto a quarter of the input. This pyramid-structure with modiﬁed upscaling layers mitigates the memory\nand computation explosion. We repeat multiple stages until it reaches the target resolution (H,W ),\nand then we will project the embedding dimension to 3 and obtain the RGB image Y ∈RH×W×3.\n3.2 Multi-scale Discriminator\nUnlike the generator which synthesizes precise pixels, the discriminator is tasked to distinguish\nbetween real/fake images. This allows us to treat it as a typical classiﬁer by simply tokenizing the\ninput image in a coarser patch-level [34], where each patch can be regarded as a “word\". However,\ncompared to image recognition tasks where classiﬁers focus on the semantic differences, the dis-\ncriminator executes a simpler and more detail-oriented task to distinguish between synthesized and\nreal. Therefore, the local visual cues and artifacts will have an important effect on the discriminator.\nPractically, we observe that the patch splitting rule plays a crucial role, where large patch size\nsacriﬁces low-level texture details, and smaller patch size results in a longer sequence that costs more\nmemory. The above dilemma motivates our design of multi-scale discriminator below.\nAs shown in Figure 2 (right), a multi-scale discriminator is designed to take varying size of patches\nas inputs, at its different stages. We ﬁrstly split the input images Y ∈RH×W×3 into three different\nsequences by choosing different patch sizes (P, 2P, 4P). The longest sequence (H\nP ×W\nP ) ×3 is\nlinearly transformed to (H\nP ×W\nP ) ×C\n4 and then combined with the learnable position encoding to\nserve as the input of the ﬁrst stage, where C\n4 is the embedded dimension size. Similarly, the second\nand third sequences are linearly transformed to ( H\n2P ×W\n2P ) ×C\n4 and ( H\n4P ×W\n4P ) ×C\n2 , and then\nseparately concatenated into the second and third stages. Thus these three different sequences are\nable to extract both the semantic structure and texture details. Similar to the generator, we reshape\nthe 1D-sentence to 2D feature map and adopt Average Pooling layer to downsample the feature\nmap resolution, between each stage. By recursively forming the transformer blocks in each stage, we\nobtain a pyramid architecture where multi-scale representation is extracted. At the end of these blocks,\na [cls] token is appended at the beginning of the 1D sequence and then taken by the classiﬁcation\nhead to output the real/fake prediction.\n3.3 Grid Self-Attention: A Scalable Variant of Self-Attention for Image Generation\nSelf-attention allows the generator to capture the global correspondence, yet also impedes the\nefﬁciency when modeling long sequences/higher resolutions. That motivates many efﬁcient self-\nattention designs in both language [ 64, 65] and vision tasks [ 66, 67]. To adapt self-attention for\nhigher-resolution generative tasks, we propose a simple yet effective strategy, named Grid Self-\nAttention, tailored for high-resolution image generation.\nAs shown in Figure 3, instead of calculating the correspondence between a given token and all other\ntokens, the grid self-attention partitions the full-size feature map into several non-overlapped grids,\nand the token interactions are calculated inside each local grid. We add the grid self-attention on\nhigh-resolution stages (resolution higher than 32 ×32) while still keeping standard self-attention in\nlow-resolution stages, shown as Figure 2, again so as to strategically balance local details and global\nawareness. The grid self-attention shows surprising effectiveness over other efﬁcient self-attention\nforms [64, 67] in generative tasks, as compared later in Section 4.1.\nOne potential concern might arise with the boundary artifact between each grid. We observe that\nwhile the artifact indeed occurs at early training stages, it gradually vanishes given enough training\n5\n(a) Standard Self-Attention (b) Grid Self-Attention\nStage i - 1\n(H x W)\nStage i\n(2H x 2W)\nStage i + 1\n(4H x 4W)\nStage i - 1\n(H x W)\nStage i\n(2H x 2W)\nStage i + 1\n(4H x 4W)\nFigure 3: Grid Self-Attention across different transformer stages. We replace Standard Self-Attention\nwith Grid Self-Attention when the resolution is higher than 32 ×32 and the grid size is set to be\n16 ×16 by default.\niterations and training data, while producing nicely coherent ﬁnal results. We think this is owing\nto the larger, multi-scale receptive ﬁeld of the discriminator that requires generated image ﬁdelity\nin different scales. For other cases where the large-scale training data is hard to obtain, we discuss\nseveral solutions on Sec. 4.6.\n3.4 Exploring the Training Recipe\nData Augmentation. The transformer-based architectures are known to be highly data-hungry due\nto removing human-designed bias. Particularly in image recognition task [34], they were inferior to\nCNNs until much larger external data [68] was used for pre-training. To remove this roadblock, data\naugmentation was revealed as a blessing in [50], which showed that different types of strong data\naugmentation could lead us to data-efﬁcient training for vision transformers.\nWe follow a similar mindset. Traditionally, training CNN-based GANs hardly refers to data augmenta-\ntion. Recently, there is an interest surge in the few-shot GAN training, aiming to match state-of-the-art\nGAN results with orders of magnitude fewer real images [69, 70]. Contrary to this “commonsense\"\nin CNNs, data augmentation is found to be crucial in transformer-based architectures, even with\n100% real images being utilized. We show that simply using differential augmentation [ 69] with\nthree basic operators {Translation, Cutout, Color} leads to surprising performance improvement\nfor TransGAN, while CNN-based GANs hardly beneﬁt from it. We conduct a concrete study on the\neffectiveness of augmentation for both transformer and CNNs: see details in Section 4.2\nRelative Position Encoding. While classical transformers [ 49, 34] used deterministic position\nencoding or learnable position encoding, the relative position encoding [71] gains increasing popular-\nity [72, 28, 52, 73], by exploiting lags instead of absolute positions. Considering a single head of\nself-attention layer,\nAttention(Q,K,V ) =softmax((QKT\n√dk\nV) (1)\nwhere Q,K,V ∈R(H×W)×C represent query, key, value matrices,H,W,Cdenotes the height, width,\nembedded dimension of the input feature map. The difference in coordinate between each query and\nkey on H axis lies in the range of [−(H−1),H −1], and similar for W axis. By simultaneously\nconsidering both H and W axis, the relative position can be represented by a parameterized matrix\nM ∈R(2H−1)×(2W−1). Per coordinate, the relative position encoding Eis taken from matrix M and\nadded to the attention map QKT as a bias term, shown as following,\nAttention(Q,K,V ) =softmax(((QKT\n√dk\n+ E)V) (2)\nCompared to its absolute counterpart, relative position encoding learns a stronger “relationship\"\nbetween local contents, bringing important performance gains in large-scale cases and enjoying\nwidespread use ever since. We also observe it to consistently improve TransGAN, especially on\nhigher-resolution datasets. We hence apply it on top of the learnable absolute positional encoding for\nboth the generator and discriminator.\nModiﬁed Normalization. Normalization layers are known to help stabilize the deep learning training\nof deep neural networks, sometimes remarkably. While both the original transformer [ 49] and its\nvariants [52, 54] by default use the layer normalization, we follow previous works [75, 16] and replace\nit with a token-wise scaling layer to prevent the magnitudes in transformer blocks from being too\nhigh, describe as Y = X/\n√\n1\nC\n∑C−1\ni=0 (Xi)2 + ϵ, where ϵ= 1e−8 by default, X and Y denote the\ntoken before and after scaling layer, Crepresents the embedded dimension. Note that our modiﬁed\nnormalization resembles local response normalization that was once used in AlexNet [75]. Unlike\n6\nTable 1: Unconditional image generation results on CIFAR-10, STl-10, and CelebA ( 128 ×128)\ndataset. We train the models with their ofﬁcial code if the results are unavailable, denoted as “*”,\nothers are all reported from references.\nMethods CIFAR-10 STL-10 CelebA\nIS↑ FID↓ IS↑ FID↓ FID↓\nWGAN-GP [1] 6.49 ± 0.09 39.68 - - -\nSN-GAN [48] 8.22 ± 0.05 - 9.16 ± 0.12 40.1 -\nAutoGAN [18] 8.55 ± 0.10 12.42 9.16 ± 0.12 31.01 -\nAdversarialNAS-GAN [18] 8.74 ± 0.07 10.87 9.63 ± 0.19 26.98 -\nProgressive-GAN [16] 8.80 ± 0.05 15.52 - - 7.30\nCOCO-GAN [74] - - - - 5.74\nStyleGAN-V2 [69] 9.18 11.07 10.21* ± 0.14 20.84* 5.59*\nStyleGAN-V2 + DiffAug. [69] 9.40 9.89 10.31*± 0.12 19.15* 5.40*\nTransGAN 9.02 ± 0.12 9.26 10.43 ± 0.16 18.28 5.28\nother “modern\" normalization layers [76–78] that need afﬁne parameters for both mean and variances,\nwe ﬁnd that a simple re-scaling without learnable parameters sufﬁces to stabilize TransGAN training\n– in fact, it makes TransGAN train better and improves the FID on some common benchmarks, such\nas CelebeA and LSUN-Church.\n4 Experiments\nDatasets We start by evaluating our methods on three common testbeds: CIFAR-10 [ 79], STL-\n10 [80], and CelebA [ 81] dataset. The CIFAR-10 dataset consists of 60k 32 ×32 images, with\n50k training and 10k testing images, respectively. We follow the standard setting to use the 50k\ntraining images without labels. For the STL-10 dataset, we use both the 5k training images and\n100k unlabeled images, and all are resized to 48 ×48 resolution. For the CelebA dataset, we use\n200k unlabeled face images (aligned and cropped version), with each image at 128 ×128 resolution.\nWe further consider the CelebA-HQ and LSUN Church datasets to scale up TransGAN to higher\nresolution image generation tasks. We use 30k images for CelebA-HQ [16] dataset and 125k images\nfor LSUN Church dataset [82], all at 256 ×256 resolution.\nImplementation We follow the setting of WGAN [46], and use the WGAN-GP loss [1]. We adopt a\nlearning rate of 1e−4 for both generator and discriminator, an Adam optimizer with β1 = 0and\nβ2 = 0.99, exponential moving average weights for generator, and a batch size of 128 for generator\nand 64 for discriminator, for all experiments. We choose DiffAug. [69] as basic augmentation strategy\nduring the training process if not specially mentioned, and apply it to our competitors for a fair\ncomparison. Other popular augmentation strategies ([70, 10]) are not discussed here since it is beyond\nthe scope of this work. We use common evaluation metrics Inception Score (IS) [15] and Frechet\nInception Distance (FID) [ 83], both are measured by 50K samples with their ofﬁcial Tensorﬂow\nimplementations 12 . All experiments are set with 16 V100 GPUs, using PyTorch 1.7.0. We include\ndetailed training cost for each dataset in Appendix D. We focus on the unconditional image generation\nsetting for simplicity.\n4.1 Comparison with State-of-the-art GANs\nCIFAR-10. We compare TransGAN with recently published results by unconditional CNN-based\nGANs on the CIFAR-10 dataset, shown in Table 1. Note that some promising conditional GANs [21,\n8] are not included, due to the different settings. As shown in Table 1, TransGAN surpasses the\nstrong model of Progressive GAN [16], and many other latest competitors such as SN-GAN [48],\nAutoGAN [18], and AdversarialNAS-GAN [19], in terms of inception score (IS). It is only next to\nthe huge and heavily engineered StyleGAN-v2 [40]. Once we look at the FID results, TransGAN is\neven found to outperform StyleGAN-v2 [40] with both applied the same data augmentation [69].\nSTL-10. We then apply TransGAN on another popular benchmark STL-10, which is larger in scale\n(105k) and higher in resolution (48x48). We compare TransGAN with both the automatic searched\nand hand-crafted CNN-based GANs, shown in Table 1. Different from the results on CIFAR-10, we\nﬁnd that TransGAN outperforms all current CNN-based GAN models, and sets new state-of-the-art\nresults in terms of both IS and FID score. This is thanks to the fact that the STL-10 dataset size is 2×\n1https://github.com/openai/improved-gan/tree/master/inception_score\n2https://github.com/bioinf-jku/TTUR\n7\nCIFAR-10 \n32 x 32\nSTL-10 \n48 x 48\nCelebA \n128 x 128\nCelebA-HQ & Church \n256 x 256\nFigure 4: Representative visual results produced by TransGAN on different datasets, as resolution\ngrows from 32 ×32 to 256 ×256. More visual examples are included in Appendix F.\nTable 2: The effectiveness of Data Augmentation on both CNN-based GANs and TransGAN. We use\nthe full CIFAR-10 training set and DiffAug [69].\nMethods WGAN-GP AutoGAN StyleGAN-V2 TransGAN\nIS ↑ FID ↓ IS ↑ FID ↓ IS ↑ FID ↓ IS ↑ FID ↓\nOriginal 6.49 39.68 8.55 12.42 9.18 11.07 8.36 22.53\n+ DiffAug [69] 6.29 37.14 8.60 12.72 9.40 9.89 9.02 9.26\nTable 3: The ablation study of proposed techniques in three common dataset CelebA( 64 ×64),\nCelebA(128 ×128, and LSUN Church(256 ×256)). “OOM” represents out-of-momery issue.\nTraining Conﬁguration CelebA CelebA LSUN Church\n(64x64) (128x128) (256x256)\n(A). Standard Self-Attention 8.92 OOM OOM\n(B). Nyström Self-Attention [64] 13.47 17.42 39.92\n(C). Axis Self-Attention [67] 12.39 13.95 29.30\n(D). Grid Self-Attention 9.89 10.58 20.39\n+ Multi-scale Discriminator 9.28 8.03 15.29\n+ Modiﬁed Normalization 7.05 7.13 13.27\n+ Relative Position Encoding 6.14 6.32 11.93\n(E). Converge 5.01 5.28 8.94\nlarger than CIFAR-10, suggesting that transformer-based architectures beneﬁt much more notably\nfrom larger-scale data than CNNs.\nCelebA (128x128). We continue to examine another common benchmark: CelebA dataset (128×128\nresolution). As shown in Table 1, TransGAN largely outperforms Progressive-GAN [16] and COCO-\nGAN [74], and is slightly better than the strongest competitor StyleGAN-v2 [40], by reaching a FID\nscore of 5.28. Visual examples generated on CIFAR-10, STL-10, and CelebA (128 ×128) are shown\nin Figure 4, from which we observe pleasing visual details and diversity.\n4.2 Scaling Up to Higher-Resolution\nWe further scale up TransGAN to higher-resolution (256 ×256) generation, including on CelebA-\nHQ [16] and LSUN Church [82]. These high-resolution datasets are signiﬁcantly more challenging\ndue to their much richer and detailed low-level texture as well as the global composition. Thanks\nto the proposed multi-scale discriminator, TransGAN produces pleasing visual results, reaching\ncompetitive quantitative results with 10.28 FID on CelebA-HQ 256 ×256 and 8.94 FID on LSUN\nChurch dataset, respectively. As shown in Figure 4, diverse examples with rich textures details are\nproduced. We discuss the memory cost reduction brought by the Grid Self-Attention in Appendix E.\n4.3 Data Augmentation is Crucial for TransGAN\nWe study the effectiveness of data augmentation for both CNN-based GANs and Our TransGAN.\nWe apply the differentiable augmentation [69] to all these methods. As shown in Table 2, for three\nCNN-based GANs, the performance gains of data augmentation seems to diminish in the full-data\nregime. Only the largest model, StyleGAN-V2, is improved on both IS and FID. In sharp contrast,\nTransGAN sees a shockingly large margin of improvement: IS improving from 8.36 to 9.02 and FID\n8\nMSG-GANOurs\nEpoch 1 15 40 Epoch 200\n TransGAN Interpolation\nFigure 5: Left: training dynamic with training epochs for both TransGAN and MSG-GAN on\nCelebA-HQ (256 ×256). Right: Interpolation on latent space produced by TransGAN.\nimproving from 22.53 to 9.26. This phenomenon suggests that CIFAR-10 is still “small-scale \" when\nﬁtting transformers; it re-conﬁrms our assumption that transformer-based architectures are much\nmore data-hungry than CNNs, and that can be helped by stronger data augmentation.\n4.4 Ablation Study\nTo further evaluate the proposed grid self-attention, multi-scale discriminator, and unique training\nrecipe, we conduct the ablation study by separately adding these techniques to the baseline method\nand report their FID score on different datasets. Due to the fact that most of our contributions are\ntailored for the challenges brought by higher-resolution tasks, we choose CelebA and LSUN Church\nas the main testbeds, with details shown in Table 3. We start by constructing our memory-friendly\nwith vanilla discriminator as our baseline method (A), both applied with standard self-attention. The\nbaseline method achieves relatively good results with 8.92 FID on CelebA (64 ×64) dataset, however,\nit fail on higher-resolution tasks due to the memory explosion issue brought by self-attention. This\nmotivates us to evaluate two efﬁcient form of self-attention, (B) Nyström Self-Attention [64]\nand (C) Axis Self-Attention [67]\nBy replacing all self-attention layers in high-resolution stages (feature map resolution higher than\n32 ×32) with these efﬁcient variants, both two methods (B)(C) are able to produce reasonable results.\nHowever, they still show to be inferior to standard self-attention, even on the64×64 resolution dataset.\nBy adopting our proposed Grid Self-Attention (D), we observe a signiﬁcant improvement on\nboth three datasets, reaching 9.89, 10.58, 20.39 FID on CelebA 64 ×64, 128 ×128 and LSUN\nChurch 256 ×256, respectively. Based on the conﬁguration (D), we continue to add the proposed\ntechniques, including the multi-scale discriminator, modiﬁed normalization, and relative position\nencoding. All these three techniques signiﬁcantly improve the performance of TransGAN on three\ndatasets. At the end, we train our ﬁnal conﬁguration (E) until it converges, resulting in the best FID\non CelebA 64 ×64 (5.01), CelebA 128 ×128 (5.28), and LSUN Church 256 ×256 (8.94).\n4.5 Understanding Transformer-based Generative Model\nWe dive deep into our transformer-based GAN by conducting interpolation on latent space and\ncomparing its behavior with CNN-based GAN, through visualizing their training dynamics. We\nchoose MSG-GAN [84] for comparison since it extracts multi-scale representation as well. As shown\nin Figure 5, the CNN-based GAN quickly extracts face representation in the early stage of training\nprocess while transformer only produces rough pixels with no meaningful global shape due to missing\nany inductive bias. However, given enough training iterations, TransGAN gradually learns informative\nposition representation and is able to produce impressive visual examples at convergence. Meanwhile,\nthe boundary artifact also vanishes at the end. For the latent space interpolation, TransGAN continues\nto show encouraging results where smooth interpolation are maintained on both local and global\nlevels. More high-resolution visual examples will be presented in Appendix F.\n4.6 Analyzing the Failure Cases and Improving High-resolution Tasks\nWhile TransGAN shows competitive or even better results on common low-resolution benchmarks,\nwe still see large improvement room of its performance on high-resolution synthesis tasks, by\nanalyzing the failure cases shown in Appendix C. Here we discuss several alternatives tailored for\nhigh-resolution synthesis tasks, as potential remedies to address these failure cases. Speciﬁcally,\nwe apply the self-modulation [85, 22, 35] to our generator and use cross-attention [53, 86] to map\nthe latent space to the global region. Besides, we replace the current 2×upsampling layer, and\ninstead ﬁrstly upsample it to 4×lager resolution using bicubic interpolation, and then downsample\nit back to 2×larger one. This simple modiﬁcation not only helps the cross-boundary information\ninteraction, but also help enhances the high-frequency details [87]. Moreover, an overlapped patch\n9\nsplitting strategy for discriminator can slightly improve the FID score. Additionally, we follow the\nprevious work [22, 40] to conduct noise injection before the self-attention layer, which is found to\nfurther improve the generation ﬁdelity and diversity of TransGAN. By applying these techniques to\nour high-resolution GAN frameworks, we observe additional improvement on both qualitative and\nquantitative results, e.g., the FID score on CelebA 256 ×256 dataset is further improved from 10.26\nto 8.93.\n5 Conclusions, Limitation, and Discussions of Broad Impact\nIn this work, we provide the ﬁrst pilot study of building GAN with pure transformers. We have\ncarefully crafted the architectures and thoughtfully designed training techniques. As a result, the\nproposed TransGAN has achieved state-of-the-art performance across multiple popular datasets, and\neasily scales up to higher-resolution generative tasks. Although TransGAN provides an encouraging\nstarting point, there is still a large room to explore further, such as achieving state-of-the-art results\non 256 ×256 generation tasks or going towards extremely high resolution generation tasks (e.g.,\n1024 ×1024), which would be our future directions.\nBroader Impact. The proposed generative model can serve as a data engine to alleviate the challenge\nof data collection. More importantly, using synthesized image examples helps avoid privacy concerns.\nHowever, the abuse of advanced generative models may create fake media materials, which demands\ncaution in the future.\nAcknowledgements\nWe would like to express our deepest gratitude to the MIT-IBM Watson AI Lab, in particular John\nCohn for generously providing us with the computing resources necessary to conduct this research. Z\nWang’s work is in part supported by an IBM Faculty Research Award, and the NSF AI Institute for\nFoundations of Machine Learning (IFML).\nReferences\n[1] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved\ntraining of wasserstein gans. In Advances in neural information processing systems, pages 5767–5777,\n2017.\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\n[3] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional\nadversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 1125–1134, 2017.\n[4] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using\ncycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer\nvision, pages 2223–2232, 2017.\n[5] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli\nShechtman. Toward multimodal image-to-image translation. In Advances in neural information processing\nsystems, pages 465–476, 2017.\n[6] Shuai Yang, Zhangyang Wang, Zhaowen Wang, Ning Xu, Jiaying Liu, and Zongming Guo. Controllable\nartistic text style transfer via shape-matching gan. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 4442–4451, 2019.\n[7] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and\nZhangyang Wang. Enlightengan: Deep light enhancement without paired supervision. IEEE Transactions\non Image Processing, 30:2340–2349, 2021.\n[8] Karol Kurach, Mario Lu ˇci´c, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. A large-scale study\non regularization and normalization in gans. In International Conference on Machine Learning, pages\n3581–3590. PMLR, 2019.\n[9] Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative\nadversarial networks through regularization. In Advances in neural information processing systems, pages\n2018–2028, 2017.\n10\n[10] Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative\nadversarial networks. arXiv preprint arXiv:1910.12027, 2019.\n[11] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually\nconverge? arXiv preprint arXiv:1801.04406, 2018.\n[12] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least\nsquares generative adversarial networks. In Proceedings of the IEEE international conference on computer\nvision, pages 2794–2802, 2017.\n[13] Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard gan. arXiv\npreprint arXiv:1807.00734, 2018.\n[14] Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. Mmd gan: Towards\ndeeper understanding of moment matching network. In Advances in neural information processing systems,\npages 2203–2213, 2017.\n[15] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\ntechniques for training gans. arXiv preprint arXiv:1606.03498, 2016.\n[16] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved\nquality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n[17] Mario Lucic, Karol Kurach, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Are gans created\nequal? a large-scale study. In Proceedings of the 32nd International Conference on Neural Information\nProcessing Systems, pages 698–707, 2018.\n[18] Xinyu Gong, Shiyu Chang, Yifan Jiang, and Zhangyang Wang. Autogan: Neural architecture search for\ngenerative adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision,\npages 3224–3234, 2019.\n[19] Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, and Shuicheng Yan. Adversarialnas: Adversarial neural\narchitecture search for gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5680–5689, 2020.\n[20] Yuan Tian, Qin Wang, Zhiwu Huang, Wen Li, Dengxin Dai, Minghao Yang, Jun Wang, and Olga Fink. Off-\npolicy reinforcement learning for efﬁcient and effective gan architecture search. In European Conference\non Computer Vision, pages 175–192. Springer, 2020.\n[21] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial\nnetworks. In International conference on machine learning, pages 7354–7363. PMLR, 2019.\n[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n4401–4410, 2019.\n[23] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis.\narXiv preprint arXiv:2012.09841, 2020.\n[24] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014.\n[25] Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a\nlaplacian pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.\n[26] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n[27] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 7794–7803, 2018.\n[28] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 3464–3473, 2019.\n[29] Yu-Syuan Xu, Shou-Yao Roy Tseng, Yu Tseng, Hsien-Kai Kuo, and Yi-Min Tsai. Uniﬁed dynamic\nconvolutional network for super-resolution with variational degradations. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 12496–12505, 2020.\n[30] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\n11\n[31] Ben Mildenhall, Jonathan T Barron, Jiawen Chen, Dillon Sharlet, Ren Ng, and Robert Carroll. Burst\ndenoising with kernel prediction networks. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2502–2510, 2018.\n[32] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. arXiv preprint arXiv:2005.12872, 2020.\n[33] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transformations for\nvideo inpainting. In ECCV. Springer, 2020.\n[34] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[35] Drew A Hudson and C Lawrence Zitnick. Generative adversarial transformers. arXiv preprint\narXiv:2103.01209, 2021.\n[36] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture transformer network\nfor image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5791–5800, 2020.\n[37] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv preprint arXiv:2012.00364,\n2020.\n[38] Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. A review on generative adversarial\nnetworks: Algorithms, theory, and applications. arXiv preprint arXiv:2001.06937, 2020.\n[39] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial\nnetworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 8207–8216, 2020.\n[40] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and\nimproving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8110–8119, 2020.\n[41] Haotao Wang, Shupeng Gui, Haichuan Yang, Ji Liu, and Zhangyang Wang. Gan slimming: All-in-one gan\ncompression by a uniﬁed optimization framework. In European Conference on Computer Vision, pages\n54–73. Springer, 2020.\n[42] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-\nresolution using a generative adversarial network. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4681–4690, 2017.\n[43] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. Deblurgan-v2: Deblurring (orders-\nof-magnitude) faster and better. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 8878–8887, 2019.\n[44] Xi Ouyang, Yu Cheng, Yifan Jiang, Chun-Liang Li, and Pan Zhou. Pedestrian-synthesis-gan: Generating\npedestrian data in real scene and beyond. arXiv preprint arXiv:1804.02047, 2018.\n[45] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image\ninpainting with contextual attention. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 5505–5514, 2018.\n[46] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875,\n2017.\n[47] Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang. Ultra-data-efﬁcient gan training:\nDrawing a lottery ticket ﬁrst, then training it toughly. arXiv preprint arXiv:2103.00397, 2021.\n[48] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for\ngenerative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\nsystems, pages 5998–6008, 2017.\n12\n[50] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[51] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper\nwith image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[53] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer\nfor image classiﬁcation. arXiv preprint arXiv:2103.14899, 2021.\n[54] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-\nscale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint\narXiv:2103.15358, 2021.\n[55] Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, and Xiaojun Chang.\nBossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search.\narXiv preprint arXiv:2103.12424, 2021.\n[56] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. Cotr: Efﬁciently bridging cnn and transformer\nfor 3d medical image segmentation. arXiv preprint arXiv:2103.03024, 2021.\n[57] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer. arXiv preprint\narXiv:2012.09164, 2020.\n[58] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n[59] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n[60] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,\nChunjing Xu, Yixing Xu, et al. A survey on visual transformer. arXiv preprint arXiv:2012.12556, 2020.\n[61] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. Image transformer. In International Conference on Machine Learning, pages 4055–4064. PMLR,\n2018.\n[62] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[63] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[64] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas\nSingh. Nystromformer: A nystrom-based algorithm for approximating self-attention. arXiv preprint\narXiv:2102.03902, 2021.\n[65] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\n[66] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? arXiv preprint arXiv:2102.05095, 2021.\n[67] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner. Colorization transformer. arXiv preprint\narXiv:2102.04432, 2021.\n[68] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness\nof data in deep learning era. In Proceedings of the IEEE international conference on computer vision ,\npages 843–852, 2017.\n[69] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-\nefﬁcient gan training. arXiv preprint arXiv:2006.10738, 2020.\n[70] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training\ngenerative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020.\n13\n[71] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.\narXiv preprint arXiv:1803.02155, 2018.\n[72] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\narXiv preprint arXiv:1910.10683, 2019.\n[73] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, AM Dai,\nMD Hoffman, and D Eck. Music transformer: Generating music with long-term structure (2018). arXiv\npreprint arXiv:1809.04281, 2018.\n[74] Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-Tzong Chen.\nCoco-gan: Generation by parts via conditional coordinating. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 4512–4521, 2019.\n[75] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. Advances in neural information processing systems, 25:1097–1105, 2012.\n[76] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[77] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015.\n[78] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient\nfor fast stylization. arXiv preprint arXiv:1607.08022, 2016.\n[79] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[80] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature\nlearning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics,\npages 215–223. JMLR Workshop and Conference Proceedings, 2011.\n[81] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), 2015.\n[82] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Con-\nstruction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint\narXiv:1506.03365, 2015.\n[83] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint\narXiv:1706.08500, 2017.\n[84] Animesh Karnewar and Oliver Wang. Msg-gan: Multi-scale gradients for generative adversarial networks.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7799–\n7808, 2020.\n[85] Ting Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative adversarial\nnetworks. arXiv preprint arXiv:1810.01365, 2018.\n[86] Long Zhao, Zizhao Zhang, Ting Chen, Dimitris N Metaxas, and Han Zhang. Improved transformer for\nhigh-resolution gans. arXiv preprint arXiv:2106.07631, 2021.\n[87] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAlias-free generative adversarial networks. arXiv preprint arXiv:2106.12423, 2021.\nA Implementation of Data Augmentation\nWe mainly follow the way of differentiable augmentation to apply the data augmentation on our\nGAN training framework. Speciﬁcally, we conduct {Translation, Cutout, Color} augmentation\nfor TransGAN with probability p, while pis empirically set to be {1.0, 0.3, 1.0}. However, we ﬁnd\nthat Translation augmentation will hurt the performance of CNN-based GAN when 100% data is\nutilized. Therefore, we remove it and only conduct {Cutout, Color} augmentation for AutoGAN.\nWe also evaluate the effectiveness of stronger augmentation on high-resolution generative tasks (E.g.\n256 ×256), including random-cropping, random hue adjustment, and image filtering.\nMoreover, we ﬁnd image filtering helps remove the boundary artifacts in a very early stage of\ntraining process, while it takes longer training iterations to remove it in the original setting.\n14\nB Detailed Architecture Conﬁgurations\nWe present the speciﬁc architecture conﬁgurations of TransGAN on different datasets, shown in\nTable 4, 5, 6, 7. For the generator architectures, the “Block” represents the basic Transformer Block\nconstructed by self-atention, Normalization, and Feed-forward MLP. “Grid Block” denotes the Trans-\nformer Block where the standard self-attention is replaced by the propose Grid Self-Attention,\nwith grid size equals to 16. Upsampling layer represents Bicubic Upsampling by default. The\n“input_shape\" and “output_shape\" denotes the shape of input feature map and output feature map,\nrespectively. For the discriminator architectures, we use “Layer Flatten” to represent the process of\npatch splitting and linear transformation. In each stage, the output feature map is concatenated with\nanother different sequence, as described in Sec. 3.2. In the ﬁnal stage, we add another CLS token and\nuse a Transformer Block to build correspondence between CLS token and extracted representation.\nIn the end, only the CLS token is taken by the Classiﬁcation Head for predicting real/fake. For\nlow-resolution generative tasks (e.g., CIFAR-10 and STL-10), we only split the input images into two\ndifferent sequences rather than three and only two stages are built as well.\nC Failure Cases Analysis\nSince TransGAN shows inferior FID scores compared to state-of-the-art ConvNet-based GAN\non high-resolution synthesis tasks, we try to visualize the failure cases of TransGAN on CelebA-\nHQ 256 ×256 dataset, to better understand its drawbacks. As shown in Fig. 6, We pick several\nrepresentative failure examples produced by TransGAN. We observe that most failure examples are\nfrom the “wearing glasses” class and side faces, which indicates that TransGAN may also suffer from\nthe imbalanced data distribution issue, as well as the issue of insufﬁcient training data. We believe\nthis could be also a very interesting question and will explore it further in the near future.\nFigure 6: Analyzing the failure cases produced by TransGAN on High-resolution synthesis tasks.\nTable 4: Architecture conﬁguration of TransGAN on CIFAR-10 dataset.\nGenerator\nStage Layer Input Shape Output Shape\n- MLP 512 (8×8)×1024\n1\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\n2\nPixelShufﬂe (8×8)×1024 (16×16)×256\nBlock (16×16)×256 (16×16)×256\nBlock (16×16)×256 (16×16)×256\nBlock (16×16)×256 (16×16)×256\nBlock (16×16)×256 (16×16)×256\n3\nPixelShufﬂe(16×16)×256 (32×32)×64\nBlock (32×32)×64 (32×32)×64\nBlock (32×32)×64 (32×32)×64\n- Linear Layer(32×32)×64 32×32×3\nDiscriminator\nStage Layer Input Shape Out Shape\n- Linear Flatten 32×32×3 (16×16)×192\n1\nBlock (16×16)×192 (16×16)×192\nBlock (16×16)×192 (16×16)×192\nBlock (16×16)×192 (16×16)×192\nAvgPooling (16×16)×192 (8×8)×192\nConcatenate (8×8)×192 (8×8)×384\n2\nBlock (8×8)×384 (8×8)×384\nBlock (8×8)×384 (8×8)×384\nBlock (8×8)×384 (8×8)×384\n-\nAddCLSToken (8×8)×384 (8×8 + 1)×384\nBlock (8×8 + 1)×384 (8×8 + 1)×384\nCLS Head 1×384 1\nD Training Cost\nWe include the training cost of TransGAN on different datasets, with resolutions across from32 ×32\nto 256 ×256, shown in Table 8. The largest experiment costs around 3 days with 32 V100 GPUs.\nE Memory Cost Comparison\nWe compare the GPU memory cost between standard self-attention and grid self-attention. Our\ntestbed is set on Nvidia V100 GPU with batch size set to 1, using Pytorch V1.7 environment. We\nevaluate the inference cost of these two architectures, without calculating the gradient. Since the\noriginal self-attention will cause out-of-memory issue even when batch size is set to 1, we reduce the\nmodel size on (256 ×256) resolution tasks to make it ﬁt GPU memory, and apply the same strategy\non 128 ×128 and 64 ×64 architectures as well. When evaluating the grid self-attention, we do not\nreduce the model size and only modify the standard self-attention on the speciﬁc stages where the\n15\nTable 5: Architecture conﬁguration of TransGAN on STL-10 dataset.\nGenerator\nStage Layer Input Shape Output Shape\n- MLP 512 (12×12)×1024\n1\nBlock (12×12)×1024 (12×12)×1024\nBlock (12×12)×1024 (12×12)×1024\nBlock (12×12)×1024 (12×12)×1024\nBlock (12×12)×1024 (12×12)×1024\nBlock (12×12)×1024 (12×12)×1024\n2\nPixelShufﬂe(12×12)×1024 (24×24)×256\nBlock (24×24)×256 (24×24)×256\nBlock (24×24)×256 (24×24)×256\nBlock (24×24)×256 (24×24)×256\nBlock (24×24)×256 (24×24)×256\n3\nPixelShufﬂe(24×24)×256 (48×48)×64\nBlock (48×48)×64 (48×48)×64\nBlock (48×48)×64 (48×48)×64\n- Linear Layer(48×48)×64 48×48×3\nDiscriminator\nStage Layer Input Shape Out Shape\n- Linear Flatten 48×48×3 (16×16)×192\n1\nBlock (24×24)×192 (24×24)×192\nBlock (24×24)×192 (24×24)×192\nBlock (24×24)×192 (24×24)×192\nAvgPooling (24×24)×192 (12×12)×192\nConcatenate (12×12)×192 (12×12)×384\n2\nBlock (12×12)×384 (12×12)×384\nBlock (12×12)×384 (12×12)×384\nBlock (12×12)×384 (12×12)×384\n-\nAddCLSToken (12×12)×384 (12×12 + 1)×384\nBlock (12×12 + 1)×384 (12×12 + 1)×384\nCLS Head 1×384 1\nTable 6: Architecture conﬁguration of TransGAN on CelebA (128 ×128) dataset.\nGenerator\nStage Layer Input Shape Output Shape\n- MLP 512 (8×8)×1024\n1\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\n2\nUpsampling (8×8)×1024 (16×16)×1024\nBlock (16×16)×1024 (16×16)×1024\nBlock (16×16)×1024 (16×16)×1024\nBlock (16×16)×1024 (16×16)×1024\nBlock (16×16)×1024 (16×16)×1024\n3\nPixelShufﬂe(16×16)×1024 (32×32)×256\nBlock (32×32)×256 (32×32)×256\nBlock (32×32)×256 (32×32)×256\nBlock (32×32)×256 (32×32)×256\nBlock (32×32)×256 (32×32)×256\n4\nPixelShufﬂe(32×32)×256 (64×64)×64\nGrid Block (64×64)×64 (64×64)×64\nGrid Block (64×64)×64 (64×64)×64\nGrid Block (64×64)×64 (64×64)×64\nGrid Block (64×64)×64 (64×64)×64\n5\nPixelShufﬂe (64×64)×64 (128×128)×16\nGrid Block (128×128)×16 (128×128)×16\nGrid Block (128×128)×16 (128×128)×16\nGrid Block (128×128)×16 (128×128)×16\nGrid Block (128×128)×16 (128×128)×16\n- Linear Layer(128×128)×16 128×128×3\nDiscriminator\nStage Layer Input Shape Out Shape\n- Linear Flatten 128×128×3 (32×32)×96\n1\nBlock (32×32)×96 (32×32)×96\nBlock (32×32)×96 (32×32)×96\nBlock (32×32)×96 (32×32)×96\nAvgPooling (32×32)×96 (16×16)×96\nConcatenate (16×16)×96 (16×16)×192\n2\nBlock (16×16)×192 (16×16)×192\nBlock (16×16)×192 (16×16)×192\nBlock (16×16)×192 (16×16)×192\nAvgPooling (16×16)×192 (8×8)×192\nConcatenate (8×8)×192 (8×8)×384\n3\nBlock (8×8)×192 (8×8)×384\nBlock (8×8)×384 (8×8)×384\nBlock (8×8)×384 (8×8)×384\n-\nAddCLSToken (8×8)×384 (8×8 + 1)×384\nBlock (8×8 + 1)×384 (8×8 + 1)×384\nCLS Head 1×384 1\nresolution is larger than 32 ×32, and replace it with the proposed Grid Self-Attention. As shown in\nin Figure 7, even the model size of the one that represents the standard self-attention is reduced, it\nstill costs signiﬁcantly larger GPU memory than the proposed Grid Self-Attention does.\nF Visual Examples\nWe include more high-resolution visual examples on Figure 8,9. The visual examples produced by\nTransGAN show impressive details and diversity.\n16\nTable 7: Architecture conﬁguration of TransGAN on CelebA ( 256 ×256) and LSUN Church\n(256 ×256) dataset.\nGenerator\nStage Layer Input Shape Output Shape\n- MLP 512 (8×8)×1024\n1\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\nBlock (8×8)×1024 (8×8)×1024\n2\nUpsampling (8×8)×1024 (16×16)×1024\nBlock (16×16)×1024 (16×16)×1024\nBlock (16×16)×1024 (16×16)×1024\nBlock (16×16)×1024 (16×16)×1024\nBlock (16×16)×1024 (16×16)×1024\n3\nUpsampling(16×16)×1024 (32×32)×1024\nBlock (32×32)×1024 (32×32)×1024\nBlock (32×32)×1024 (32×32)×1024\nBlock (32×32)×1024 (32×32)×1024\nBlock (32×32)×1024 (32×32)×1024\n4\nPixelShufﬂe(32×32)×1024 (64×64)×256\nGrid Block (64×64)×256 (64×64)×256\nGrid Block (64×64)×256 (64×64)×256\nGrid Block (64×64)×256 (64×64)×256\nGrid Block (64×64)×256 (64×64)×256\n5\nPixelShufﬂe(64×64)×256 (128×128)×64\nGrid Block(128×128)×64 (128×128)×64\nGrid Block(128×128)×64 (128×128)×64\nGrid Block(128×128)×64 (128×128)×64\nGrid Block(128×128)×64 (128×128)×64\n6\nPixelShufﬂe(128×128)×64 (256×256)×16\nGrid Block(256×256)×16 (256×256)×16\nGrid Block(256×256)×16 (256×256)×16\nGrid Block(256×256)×16 (256×256)×16\nGrid Block(256×256)×16 (256×256)×16\n- Linear Layer(256×256)×16 256×256×3\nDiscriminator\nStage Layer Input Shape Out Shape\n- Linear Flatten 256×256×3 (64×64)×96\n1\nBlock (64×64)×96 (64×64)×96\nBlock (64×64)×96 (64×64)×96\nGrid Block (64×64)×96 (64×64)×96\nAvgPooling (64×64)×96 (32×32)×96\nConcatenate (32×32)×96 (32×32)×192\n2\nBlock (32×32)×192 (32×32)×192\nBlock (32×32)×192 (32×32)×192\nBlock (32×32)×192 (32×32)×192\nAvgPooling (32×32)×192 (16×16)×192\nConcatenate (16×16)×192 (16×16)×384\n3\nBlock (16×16)×192 (16×16)×384\nBlock (16×16)×384 (16×16)×384\nBlock (16×16)×384 (16×16)×384\n-\nAddCLSToken (16×16)×384 (16×16 + 1)×384\nBlock (16×16 + 1)×384 (16×16 + 1)×384\nCLS Head 1×384 1\nTable 8: Training Conﬁguration\nDataset Size Resolution GPUs Epochs Time\nCIFAR-10 50k 32 ×32 2 500 2.6 days\nSTL-10 105k 48 ×48 4 200 2.0 days\nCelebA 200k 64 ×64 8 250 2.4 days\nCelebA 200k 128 ×128 16 250 2.1 days\nCelebA-HQ 30k 256 ×256 32 300 2.9 days\nLSUN Church 125k 256 ×256 32 120 3.2 days\n0\n10000\n20000\n30000\n64x64 128x128 256x256\nStandard Grid\nMemory Cost\nFigure 7: Memory cost comparison between standard self-attention and grid self-attention\n17\nFigure 8: Latent Space Interpolation on CelebA (256 ×256) dataset.\n18\nFigure 9: High-resolution representative visual examples on CelebA (256 ×256) dataset.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7540917992591858
    },
    {
      "name": "Transformer",
      "score": 0.7119719982147217
    },
    {
      "name": "High fidelity",
      "score": 0.5550141334533691
    },
    {
      "name": "Discriminative model",
      "score": 0.5208916664123535
    },
    {
      "name": "Grid",
      "score": 0.4747229814529419
    },
    {
      "name": "Architecture",
      "score": 0.4706529676914215
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46282631158828735
    },
    {
      "name": "Bottleneck",
      "score": 0.4568529725074768
    },
    {
      "name": "Discriminator",
      "score": 0.4479144811630249
    },
    {
      "name": "Generative grammar",
      "score": 0.4166374206542969
    },
    {
      "name": "Convolutional neural network",
      "score": 0.41190004348754883
    },
    {
      "name": "Computer engineering",
      "score": 0.4107154309749603
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3689943850040436
    },
    {
      "name": "Electrical engineering",
      "score": 0.15901529788970947
    },
    {
      "name": "Embedded system",
      "score": 0.12060385942459106
    },
    {
      "name": "Detector",
      "score": 0.11486586928367615
    },
    {
      "name": "Voltage",
      "score": 0.11387768387794495
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 263
}