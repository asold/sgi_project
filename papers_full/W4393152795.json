{
  "title": "Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation",
  "url": "https://openalex.org/W4393152795",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2034456293",
      "name": "Li Zhong",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2111830153",
      "name": "Zilong Wang",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2628359750",
    "https://openalex.org/W2113736511",
    "https://openalex.org/W4318541617",
    "https://openalex.org/W1987615754",
    "https://openalex.org/W4287022754",
    "https://openalex.org/W6606991017",
    "https://openalex.org/W4315815628",
    "https://openalex.org/W2014577207",
    "https://openalex.org/W6809838524",
    "https://openalex.org/W2406533925",
    "https://openalex.org/W4308627320",
    "https://openalex.org/W2804673050",
    "https://openalex.org/W2547405428",
    "https://openalex.org/W3098403328",
    "https://openalex.org/W3161457214",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W4367860052",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4330337479",
    "https://openalex.org/W2964315653",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W4384026634",
    "https://openalex.org/W4366459745",
    "https://openalex.org/W4387561453",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4226242393",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W4292956935",
    "https://openalex.org/W4388858772",
    "https://openalex.org/W2782920565",
    "https://openalex.org/W4378474282",
    "https://openalex.org/W4385373622",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4287024925"
  ],
  "abstract": "Recently, large language models (LLMs) have shown an extraordinary ability to understand natural language and generate programming code. It has been a common practice for software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability, and robustness of the code generation from LLMs have not yet been thoroughly studied. The executable code is not equivalent to reliable and robust code, especially in the context of real-world software development. For example, the misuse of APIs in the generated code could lead to severe problems, such as resource leaks, program crashes, etc. Existing code evaluation benchmarks and datasets focus on crafting small tasks such as programming questions in coding interviews, which, however, deviates from the problem that developers would ask LLM for real-world coding help. To fill the missing piece, in this work, we propose a dataset RobustAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from Stack Overflow on 18 representative Java APIs. We summarize the common misuse patterns of these APIs and evaluate them on current popular LLMs. The evaluation results show that even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software.",
  "full_text": "Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of\nLarge Language Model Code Generation\nLi Zhong, Zilong Wang\nUniversity of California, San Diego\nlizhong@ucsd.edu, zlwang@ucsd.edu\nAbstract\nRecently, large language models (LLMs) have shown an ex-\ntraordinary ability to understand natural language and gen-\nerate programming code. It has been a common practice for\nsoftware engineers to consult LLMs when encountering cod-\ning questions. Although efforts have been made to avoid syn-\ntax errors and align the code with the intended semantics, the\nreliability, and robustness of the code generation from LLMs\nhave not yet been thoroughly studied. The executable code\nis not equivalent to reliable and robust code, especially in\nthe context of real-world software development. For example,\nthe misuse of APIs in the generated code could lead to se-\nvere problems, such as resource leaks, program crashes, etc.\nExisting code evaluation benchmarks and datasets focus on\ncrafting small tasks such as programming questions in coding\ninterviews. However, this deviates from the problems devel-\nopers typically consult LLMs about. To fill the missing piece,\nwe propose a dataset R OBUST API for evaluating the relia-\nbility and robustness of code generated by LLMs. We collect\n1208 coding questions from Stack Overflow on 18 representa-\ntive Java APIs. We summarize the common misuse patterns of\nthese APIs and evaluate them on current popular LLMs. The\nevaluation results show that even GPT-4 has 62% of the gen-\nerated code that contains API misuses. It would cause unex-\npected consequences if the code is introduced into real-world\nsoftware.\nIntroduction\nThe new era of language modeling arrives when large lan-\nguage models (LLMs) are capable of generating customized\ncode according to the user’s needs (Ye et al. 2023; Ope-\nnAI 2023a; Anil et al. 2023). It is not surprising that more\nand more software engineers choose to query large language\nmodels for the answer to the coding questions, such as gen-\nerating a code snippet using certain APIs or detecting bugs\nin a few lines of code. Large language models are able to\nrespond more suitable and customized answers for the ques-\ntion compared with searching in the online programming fo-\nrums, such as Stack Overflow.\nSuch a fast pace conceals potential risks in the code gen-\neration of large language models. From the perspective of\nsoftware engineering, the robustness and reliability of gen-\nerated code have not yet been thoroughly studied even if nu-\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nmerous works have been made to avoid syntax errors and\nimprove semantic understanding in the generated code (Xu\net al. 2022; Chen et al. 2021; Shen et al. 2023a; Luo et al.\n2023). Unlike the online programming forums, the gener-\nated code snippets are not reviewed by the community peers\nand thus suffer from API misuse, such as missing boundary\nchecking in file reading and variable indexing, missing file\nstream closing, failure in transaction completion, etc. Even\nif the code samples are executable or functionally correct,\nmisuse can trigger serious potential risks in production, such\nas memory leaks, program crashes, garbage collection fail-\nures, etc, as shown in Figure 1. To make things worse, the\nprogrammers asking these questions could be vulnerable to\nthe risk if they are novices to the APIs and cannot tell the\nviolations in the generated code snippets. Therefore, it is es-\nsential to contemplate the code reliability while evaluating\nthe code generation by large language models.\nTo evaluate the code generation of large language mod-\nels, most of the existing benchmarks focus on the functional\ncorrectness of the execution result from the generated code,\nwhich means the code is acceptable as long as it is func-\ntional for the user’s purpose (Chen et al. 2021; Yin et al.\n2018; Lu et al. 2021). We argue that the correct execution\nresult is important but it is not only the case in the software\ndevelopment scenario. What the engineers really need is a\nreliable code sample without potential risks in the long run.\nMoreover, the domain of most current programming datasets\nis far from software engineering. The data source is mostly\nonline coding challenge websites, such as Codeforces, Kat-\ntis, Leetcode, etc (Hendrycks et al. 2021; Austin et al. 2021).\nAlthough remarkable progress has been made, we argue that\nthey fail to substantially help the software development in\npractical scenarios.\nTo this end, we propose R OBUST API, a comprehensive\nbenchmark to evaluate the reliability and robustness of code\ngenerated by large language models, including a dataset of\ncoding questions and an evaluator using the abstract syntax\ntree (AST) (Fischer, Lusiardi, and V on Gudenberg 2007).\nIn the dataset, we target creating an evaluation setting that\nis close to real software development. Thus we collect rep-\nresentative questions about Java from Stack Overflow. Java\nis one of the most popular programming languages and is\nwidely used in software development because of its write\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21841\nHow can I create a file with Java?\nI want to create a file through Java. What \nfunctions shall I use?\nLLaMA2\nVicuna\nAsk LLMs \nfor help\nFile file = new File(filePath);\ntry {\n  file.createNewFile(); \n} catch (IOException e){ \n  e.printStackTrace();\n}\nstatic void CreateNewFile(String filePath) {\n  File file = new File(filePath);\n  if (!file.exists()) { file.createNewFile(); }\n}\nLLM-Generated Code Snippet (Llama 2)\nCorrect API Usage\ncreateNewFile \nRequires catching IO \nexceptions when the file \nalready exists or the parent \nfolder doesn’t exist.\nSyntax Correct ✓\nFunction Correct ✓\nSemantic Aligned ✓\nReliable & Robust \n✗\nFigure 1: The scenario where software engineers consult large language models for the answer to the programming questions.\nThe generated code snippet is not reliable and has potential risks in the software development.\nonce, run anywhere (WORA)feature1. For each question, we\nprovide a detailed description and the related Java API. We\ndesign templates to trigger large language models to gen-\nerate the code snippet and the corresponding explanation.\nWe also provide an evaluator that analyzes the generated\ncode snippets using the abstract syntax tree (AST) and com-\npares them with the expected API usage patterns. Following\nZhang et al. (2018), we formalize the API usage patterns into\nstructured call sequences, as shown in Figure 2. The struc-\ntured call sequences present how these APIs can be properly\nused to eliminate the potential system risks. Any violations\nof such structured call sequences would be considered as\nAPI misuse from the perspective of software engineering.\nWe collect 1208 real questions from Stack Overflow\nwhich involves 18 representative Java APIs. We run ex-\nperiments on the close-sourced language models (GPT-3.5\nand GPT-4 (OpenAI 2023a)) as well as the open-sourced\nlanguage models (Llama-2 (Touvron et al. 2023), Vicuna-\n1.5 (Chiang et al. 2023). We use the default hyper-parameter\nsettings of the models without extensive hyper-parameter\ntuning. We further design two experiment settings, zero-shot\nand one-shot, where none or one demonstration sample is\nprovided in the prompt. We conduct a comprehensive anal-\nysis of the generated code and study the common API mis-\nuse cases of current large language models. We would like\nto bring up the important issues of API misuse in the code\ngeneration by large language models, and provide a new di-\nmension to evaluate large language models other than the\ncommonly-used functional correctness. The main purpose\nof this benchmark is not to evaluate the functional correct-\nness of the generated code, but instead, we focus on reli-\nability and robustness. We hope this work could facilitate\nfuture research on this topic and help create a more robust\ncoding helper out of large language models to step further\ninto real artificial general intelligence. We open-source our\ndataset and evaluator on GitHub 2. We summarize our con-\ntribution as follows.\n• We propose a new benchmark, R OBUST API, to evaluate\nthe reliability and robustness of code generation by large\nlanguage models. This is an important but not yet well-\n1https://en.wikipedia.org/wiki/Java (programming language)\n2https://github.com/FloridSleeves/RobustAPI\nstudied perspective to evaluate the code quality apart\nfrom functional correctness.\n• We provide a well-formalized evaluation framework in-\ncluding a dataset of Stack Overflow questions and an API\nusage checker using AST. We report the performance of\npopular large language models, including GPT-3.5, GPT-\n4, Llama-2, and Vicuna-1.5.\n• We conduct a comprehensive analysis of the code gener-\nation performance of current large language models. We\nsummarize the common API misuse for each model and\npoint out the promising improvement direction for the\nfuture research.\nRelated Work\nCode Quality of LLM-Sythesized Code With the re-\nlease of Copilot (Chen et al. 2021) and other commercial\ncode assistant tools based on LLMs, the security and code\nquality of these tools gradually get the attention of the re-\nsearch community. Yetistiren, Ozsoy, and Tuzun (2022) as-\nsess the quality of LLM-generated code from the aspects\nof compilation correctness, functional correctness, and code\nefficiency. Siddiq et al. (2022) studied code smells in code\ngenerated by LLMs, which is the poor design in code like\nunusually long method, or duplicated code. Poesia et al.\n(2022) shows that LLMs can make implementation errors in\nthe code like syntax errors or semantic errors deviating from\nusers’ intention. Jesse et al. (2023) studied simple, stupid\nbugs in Codex and other LLMs, which shows that AI code\nassistants can help avoid some of such simple bugs but have\na higher chance of introducing bugs that are hard to de-\ntect. As for security impact, Pearce et al. (2022) designed\n89 security-sensitive scenarios for Copilot to complete the\ncode for users, which shows approximately 40% of the code\nis vulnerable. Perry et al. (2022) conducted the first large-\nscale user study to examine whether users interacting with\nAI Code assistants write secure code. They find that those\nusers wrote significantly less secure code while they believe\ntheir code was secure. Sandoval et al. (2023) conducts a user\nstudy to assess the security of low-level code with pointer\nand array manipulations generated by AI-based coding as-\nsistants. They find under this specific scenario, the assistants\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21842\ndo not introduce more security bugs than humans. Liu et al.\n(2023) enlarges HumanEval (Chen et al. 2021) by generat-\ning test cases with higher coverage which serve as an add-on\nto the existing programming benchmarks but the evaluation\nstill focuses on functional correctness and simple program-\nming questions far from software development. Shen et al.\n(2023b) evaluates the reliability of ChatGPT by testing on\nadversarial examples, which however has a different mean-\ning of ‘reliability’ in their context. In this paper, we refer to\nreliability as the ability of code to resist failure, high work-\nload, and unexpected input.\nQuality Assessment of Code in Online Forum Ex-\nisting literature in the software engineering field has inves-\ntigated the quality of code from online forums and warned\ndevelopers of the potential issues. Yang, Hussain, and Lopes\n(2016) finds that the majority of code examples given in\nStack Overflow answers cannot be compiled. Zhou and\nWalker (2016) pointed out that 43% of the posts investigated\nby them contained deprecated APIs, while Fischer et al.\n(2017) found that 29% of the code contains security risks.\nIn Zhang et al. (2018), the authors analyze the code by call\nsequence extraction and slicing, and compare it to the manu-\nally validated API usage rules, which concludes that 31% of\nthe code examples in Stack Overflow answers contain API\nmisuse and could produce unexpected behaviors.\nMethodology\nIn this section, we describe R OBUST API, a comprehensive\nbenchmark to thoroughly evaluate the reliability and robust-\nness of LLM-generated code. We describe the process of\ndata collection and prompt generation when constructing the\ndataset. Then we present the API misuse patterns evaluated\nin ROBUST API and discuss the potential consequence of vi-\nolations. Finally, we introduce the static analysis method in\nROBUST API for detecting the API usage violations which\nleverages the abstract syntax tree and achieves higher eval-\nuation accuracy in evaluating the API misuse in code gener-\nated by LLMs compared to rule-based method such as key-\nwords matching.\nData Collection\nTo take advantage of the existing research efforts in the soft-\nware engineering field, we build R OBUST API based on the\ndataset from ExampleCheck (Zhang et al. 2018) as our start-\ning point. ExampleCheck is proposed to study the frequent\nJava API misuse in online Q&A forums. We select 18 pop-\nular Java APIs from the dataset as shown in Table 1. These\n18 APIs cover 6 domains including string processing, data\nstructure, mobile development, crypto, I/O and database op-\neration. Then we crawl questions relevant to these APIs from\nStack Overflow. We only select the questions with online\nanswers and we keep the questions whose provided answer\ncontains API misuse. In this way, we guarantee that the ques-\ntions in R OBUST API are answerable and non-trivial so we\ncan use them to effectively evaluate the LLMs’ ability in\nanswering coding questions that humans are prone to make\nmistakes. After filtering, we get 1208 questions in total. The\ndistribution of questions for each domain is shown in Ta-\nble 1.\nAPI Domain Conseq* Github*\nStringTokenizer.ne\nxtToken String\nProcess\n(307)\n(iii) 13.3K\nString.getBytes (iii) 88.1K\nJsonElement.getAsString (iii) 4.4K\nList.get Data\nStructure\n(404)\n(iii) 2.7M\nMap.get (iii) 2.4M\nIterator\n.next (iii) 918K\nProgressDialog.dismiss Mobile\nDevelop\n(75)\n(iii) 54K\nT\nypedArray.getString (iv) 6.8K\nApplicationInfo.loadIcon (v) 3.6K\nActivity.setContentView (v) 4.6K\nCipher.init Crypto (10) (iii)\n66.3K\nRandomAccessFile.write\nI/O (390)\n(i) 129K\nBufferedReader\n.readLine (iii) 74.8K\nPrintWriter.write (i) 1.1M\nFile.mkdirs (ii) 73.2K\nFile.createNewFile (i) 176K\nFileChannel.write (i) 5.2K\nSQLiteDatabase.query Database (22) (i\nv) 4K\nTotal 1208 7.8M\nTable 1: 18 popular Java APIs in R OBUST API. They are\neasily misused by developers according to the existing lit-\nerature of software engineering (Zhang et al. 2018). *Con-\nsequences: (i) data loss; (ii) file system corruption; (iii)\nprogram crash; (iv) resource leak; (v) user interface bug.\n*Github: occurrences of this API on Github.\nAfter collecting the questions, we convert them into\nthe JSON format with the following fields: {id, api,\nquestion, origin}. id field contains the unique id we\nassign for each sample. api field contains the API that we\nspecifically instruct the large language models to use as a\nquestion hint. question field contains the title and descrip-\ntion of the Stack Overflow questions. origin field contains\nthe original URL of this sample.\nPrompt Generation\nIn the prompt, we start with the task introduction and the\nrequired response format. Then we append the few-shot\ndemonstrations on this API when conducting experiments in\nthe few-shot settings. The demonstration examples satisfy\nour provided response format. Next, we append the ques-\ntion and the corresponding API hint for this question. This\nprompt simulates a user asking coding questions without\nproviding any additional hints from the API documentation\nwhich is a typical scenario when novice developers seek help\nfrom large language models. Due to the chat completion na-\nture of state-of-the-art LLMs, we wrap the question and an-\nswer with special tags to instruct LLMs to generate answers\nto the questions. The prompt template is adapted from (Patil\net al. 2023), which can help LLMs follow a specific gener-\nation template so that we can extract more compilable code\nsnippets from the response.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21843\nDemonstration Samples\nDemonstration samples have been proven helpful to LLMs\nin understanding natural language. To thoroughly analyze\nLLMs’ ability in code generation, we design two few-shot\nsettings, One-shot-irrelevant and One-shot-relevant.\nIn the one-shot-irrelevant setting, we provide LLMs with\nan example using an irrelevant API (e.g. Arrays.stream).\nWe assume this demonstration example would eliminate the\nsyntax errors in the generated code.\nIn the one-shot-relevant setting, we provide LLMs with\nan example using the same API as the given question. The\nprovided example contains a pair of question and answer.\nThe question in the demo example is not present in the test-\ning dataset and we manually revise the answer to ensure that\nthere is no API misuse in it and that the semantics well align\nwith the questions.\nJava API Misuse\nWhen using the APIs provided by language libraries, de-\nvelopers need to follow the API usage rules so that they\ncan take full advantage of the ideal API effect. Violating\nthese rules and misusing the APIs could result in unex-\npected behaviors in production. A typical example is the\nfile operation. When opening and writing a file through\nRandomAccessFile, two usage rules need to be enforced:\n(1) Reading the file could throw exceptions. If the buffer\nlimit is reached before the expected bytes are read, the API\nwould throwIndexOutOfBoundsException. Also, if the file\nis concurrently closed by other processes, the API would\nthrow ClosedChannelException. To deal with these excep-\ntions, the correct implementation should enclose the API\ninside try-catch blocks. (2) The file channel should be\nclosed after usage. Otherwise, if this code snippet is inside a\nlong-lasting program that is concurrently running in multi-\nple instances, the file resources could be run out. Therefore,\nthe code needs to invoke close API after all file operations.\nThe correct usage are shown as following:\nCorrect API Usage:\ntry {\nRandomAccessFile raf =\nnew RandomAccessFile(\"/tmp/file.json\", \"r\");\nbyte[] buffer = new byte[1024 * 1024];\nint bytesRead = raf.read(buffer, 0, buffer.length);\nraf.close();\n} catch(Exception e) {...}\nIn R OBUST API, we summarized 41 API usage rules from\nthe 18 APIs, which are validated in the documentation\nof these APIs (Zhang et al. 2018). These rules include:\n(1) The guard condition of an API, which should be\nchecked before API calls. For example, check the re-\nsult of File.exists() before File.createNewFile()\n(2) Required call sequence of an API, which should be\ncalled in a specific order. For example, call close() after\nFile.write(). (3) Control structures of an API. For exam-\nple, enclose SimpleDateFormat.parse() with try-catch\nstructure.\nDetecting API Misuse\nExisting research in evaluating the code generated by LLMs\nusually uses test cases, which falls short when testing the\nreliability and robustness of code. To deal with this chal-\nlenging problem, we use static analysis for R OBUST API,\nwhich has relatively mature solutions in detecting API mis-\nuse (Zhang et al. 2018; Nguyen et al. 2014; Wang et al. 2013;\nHuang et al. 2023). To evaluate the API usage correctness\nin code, R OBUST API detects the API misuses against the\nAPI usage rules by extracting call consequences and control\nstructures from the source code, as shown in Figure 2. The\ncode checker first checks the code snippets to see whether\nit is a snippet of a method or a method of a class so that it\ncan enclose this code snippet and construct an abstract syn-\ntax tree (AST) from the code snippet. Then the checker tra-\nverses the AST to record all the method calls and control\nstructures in order, which generates a call sequence. Next,\nthe checker compares the call sequence against the API us-\nage rules. It infers the instance type of each method call and\nuses the type and method as keys to retrieve corresponding\nAPI usage rules. Finally, the checker computes the longest\ncommon sequence between the call sequence and the API\nusage rules. If the call sequence does not match the expected\nAPI usage rules, the checker will report API misuse.\nExperimenet\nExperiment Setup\nIn the experiments, we evaluate ROBUST API on four LLMs:\nGPT-3.5 (OpenAI 2023a), GPT-4 (OpenAI 2023a), Llama-\n2 (Touvron et al. 2023), Vicuna-1.5 (Chiang et al. 2023).\nWe use the default hyper-parameter settings of each model\nwithout further extensive hyper-parameter tuning. All exper-\niment results are Pass@1 unless specified. For all models,\nwe evaluate three experiment settings:\n• Zero-shot: No example is provided in the prompt. The\nprompt only contains the instruction, question.\n• One-shot-irrelevant: ROBUST API provides one exam-\nple of an irrelevant task in the prompt.\n• One-shot-relevant: ROBUST API provides one example\nof the same API with the correct usage in the prompt.\nThe examples for shot generations are manually written\nand double-checked by the authors. Then they are evaluated\nagainst the API usage checkers to make sure they are aligned\nwith the API usage rules.\nEvaluation Metrics\nTo quantitatively evaluate the reliability of the generated\ncode, we define the following values and our metrics are\ncomputed based on them. Supposing that we have N ques-\ntions in our dataset, we divide them into three groups.\n• Nmisuse: The number of cases where our API usage\nchecker detects the API usage violations.\n• Npass: The number of cases where our API usage checker\ndoes not detect the API usage violations.\n• Nnon-comp: The number of cases where the LLM fails to\ngenerate code or the generated code is not compilable.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21844\ntry {\n  RandomAccessFile raf = \\\n    new RandomAccessFile(\"file.json\", \"r\");\n  byte[] bf = new byte[1024 * 1024];\n  int bytes = raf.read(bf, 0, bf.length);\n} catch(Exception e) {\n  e.printStackTrace();\n}\nCode Snippet Generated by LLM\nTRY\nTRY-BODY CATCH-BODY\nCALL CALL CALL CALL\n(i) Generate AST for the Code Snippet\nAST of the Given Code Snippet\n(ii) Compare AST with API Usage Rules\nAST Call Sequence API Usage Rule\n(RandomAccessFile().read())\n(iii) Detect Mismatched Pattern \n& Report Violation\nCannot find RandomAccessFile().close() \nin AST Call Sequence\nTRY\nRandomAccessFile()\nEND_BLOCK\nRandomAccessFile().read()\nCATCH\nException.printStackTrace()\nEND_BLOCK\nTRY\nRandomAccessFile().read()\nRandomAccessFile().close()\nLongest Common String\nEND_BLOCK\nCATCH\nEND_BLOCK\nFigure 2: The workflow of Our API Checker. The API checker uses the static analysis method and analyzes the generated code\nwith the abstract syntax tree (AST). The API misuse is detected when the AST call sequence and the API usage mismatches.\nBased on the values, we define our metrics.\n• API Misuse Rate = Nmisuse/(Nmisuse + Npass): To an-\nalyze the proportion of misuse cases among the compil-\nable code snippets. It reveals how reliable the generated\ncode is after the users filter out the non-compilable cases.\n• Compilation Rate = (Nmisuse + Npass)/N: To analyze\nthe proportion of compilable cases among all questions.\nIt is necessary to consider the percentage of compilable\ncases in order to eliminate the influence from the ex-\ntreme situations, such as when only a few compilable\ncode snippets are generated.\n• Overall API Misuse Percentage = Nmisuse/N: To ana-\nlyze the proportion of misuse cases among all questions.\nResearch Questions\nWe conduct a series of experiments on state-of-the-art LLMs\nbased on ROBUST API, which demonstrate the usability and\neffectiveness of ROBUST API. The experiments provide in-\nsights on the ability to answer real-world coding questions\nand the robustness and reliability of these answers regarding\nAPI misuse problems. In the experiment, we try to answer\nthe following questions:\n• Q1: What are the API misuse rates in answering real-\nworld coding questions by these LLMs?\n• Q2: How do irrelevant shots affect the results?\n• Q3: Can correct API usage examples reduce the misuse?\n• Q4: Why does generated code fail the API usage check?\nAPI Misuse Rate\nFirstly, we present the API misuse rate of each model based\non R OBUST API on the left of Figure 3. In this figure, the\nhigher the API misuse rate is, the worse the code reliabil-\nity and robustness for this large language model. The API\nmisuse rate is calculated by dividing answers that can be\ncompiled and contains API misuses by all the answers that\nGPT3.5 GPT4 Llama2Vicuna GPT3.5 GPT4 Llama2Vicuna GPT3.5 GPT4 Llama2Vicuna\n0\n20\n40\n60\n80\n100API Usage Results\n29\n49\n29\n62\n49\n31\n28\n62\n27\n64\n41\n49\n 8\n 0 30\n49\n25\n47\n20\n16\n35\n48\n36\n27\nZero Shot One Shot Irrelevant One Shot Relevant\nNot Compilable API Misuse Pass\nFigure 3: Result of Checking API Usage from LLMs. Red\nbars are the percentage of answers that contain API misuse,\nwhich is the lower, the better. The white bars in dot lines are\nthe percentage of code answers that are not compilable.\ncan be compiled. From the evaluation results, all the eval-\nuated models suffer from API misuse problems, even the\nstate-of-the-art commercial models like GPT-3.5 and GPT-\n4. In zero-shot settings, Llama has the lowest API misuse\nrate. However, this is partially due to that most of Llama’s\nanswers do not include any code. A counter-intuition find-\ning is that GPT-4 actually has a higher API misuse rate than\nGPT-3.5, though the coding ability of GPT-4 is proved to be\n“40% more advanced than its predecessor, GPT-3.5”(Ope-\nnAI 2023b). We also evaluate a code-specialized large lan-\nguage model, DeekSeekCoder(Piplani and Bamman 2018),\nwhich is trained on a variety of programming languages\nincluding Java, and surpasses many existing Code LLMs.\nWe report the results of deepseek-coder-6.7b-base and\ndeepseek-coder-6.7b-instruct. We observe that the\ncode-specialized large language model can generate more\ncompilable samples. However, the API misuse rate is not\nsignificantly better than other models. This indicates that\nwith the code generation ability of large language models\nis largely improved nowadays, the reliability and robustness\nof code in real-world production rises as an unnoticed issue.\nAnd the space for improvement is huge for this problem.\nThe execution time for static analysis is shown in Table 3.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21845\nModel\nZero-shot One-shot-irrelev\nant One-shot-relevant\nMisuse Compilable Ov erall\nMisuse Compilable Overall Misuse Compilable Overall\nRate ↓ Rate ↑ Misuse ↓ Rate ↓ Rate ↑ Misuse ↓ Rate ↓ Rate ↑ Misuse ↓\nGPT 3.5 62.97% 79.14% 49.83%\n68.09% 91.06% 62.00% 38.56% 80.71% 31.13%\nGPT 4 68.81% 90.23% 62.09% 70.38% 91.39% 64.32% 54.40% 90.40% 49.17%\nLlama 2∗ 7.34%∗ 9.02%∗ 0.66%∗ 61.36% 80.13% 49.17% 64.47% 72.93% 47.02%\nVicuna 1.5 45.66% 37.17% 16.97% 57.85% 83.86% 48.51% 42.53% 64.24% 27.32%\nds-coder-6.7b-base 41.55% 40.65% 16.89% 75.60% 95.90% 72.43% 64.12% 67.14% 43.05%\nds-coder-6.7b-instruct 47.52% 50.00% 23.76% 59.04% 96.61% 57.04% 38.40% 86.01% 33.03%\nTable 2: Performance of Each LLM on R OBUST API. ↓: the lower the better. ↑: the higher the better. Misuse Rate is the\nproportion of misuse cases among the compilable cases; Compilation Rate is the proportion of compilable cases among all\nquestions; Overall Misuse is the proportion of misuse cases among all questions. ∗Though Llama2 has a low misuse rate, its\ncompilation rate is significantly lower than other models.\nThe time difference is due to the different coding styles of\neach LLM, all of which are within 7 minutes.\nGPT 3.5 GPT 4\nLlama 2 Vicuna 1.5 DeepSeek-Coder\n6m 31s 6m 56s\n6m 36s 6m 19s 6m 36s\nTable 3: Execution Time of Static Analysis in ROBUST API.\nFinding 1. Answers to real-world coding questions from the\nstate-of-the-art large language models widely have API mis-\nuse problems.\nOne-Shot-Irrelevant Results\nIn this experiment, R OBUST API gives a pair of question\nand answer as an example to show the model how to follow\nthe template required by the instructions. The example con-\ntains no information about the API usage checked by R O-\nBUST API. The result is shown in the middle of Figure 3.\nHowever, for most models, the irrelevant shot does not sig-\nnificantly reduce the API misuse rate but on the contrary,\nslightly increases the misuse rate. One possible reason for\nthis is the irrelevant shot provided to the large language mod-\nels actually encourages the models to give a lengthy code\nsolution, which increases the chance of API misuse. API\nmisuse rate of Llama increases significantly after adding the\nirrelevant shot because it has more valid answers that con-\ntain code snippets. Overall, adding an irrelevant shot triggers\nthe large language models to generate more valid answers,\nwhich enables a better evaluation of the code reliability and\nrobustness.\nFinding 2. Among all the answers containing compilable\ncode, 57-70% of the LLM answers contain API misuse,\nwhich could lead to severe consequence in production.\nFinding 3. Irrelevant shot examples does not help decrease\nthe API misuse rate but triggers more valid answers, which\nshow to be effective for benchmarking the model perfor-\nmance.\nOne-Shot-Relevant Results\nIn this experiment, R OBUST API adds a manually-written\nshot in the prompt, which performs a different task but uses\nthe same API. This gives hints to LLMs on how to use\nthese APIs correctly. From the results, after adding the cor-\nrect usage shot, the API misuse rates of GPT-3.5, GPT-4,\nand Vicuna significantly drop. This indicates an effective\nimprovement under this experiment setting. As for Llama,\nthe relevant shot does not improve the performance. This\nexperiment shows that some LLMs can effectively ‘learn’\nthe correct API usage and follow the usage. However, since\nexisting language models are trained with data from code\nrepositories if the training datasets contain a large number\nof API violations, the language models are prone to gen-\nerate code with API misuses, which explains the high API\nmisuse rate in zero-shot and one-shot-irrelevant evaluation.\nWe show Pass@k results of one-shot-relevant in Table 4.\nPass@k Misuse Rate\nCompilation Rate Overall Misuse\nPass@1 39.06% 76.08%\n29.72%\nPass@5 21.98% 93.79% 20.61%\nPass@10 16.51% 96.27% 15.89%\nTable 4: Pass@k results of GPT 3.5 (T=1, one-relevant-\nshot).\nFinding 4. Some LLMs can learn from the correct usage\nexample, which reduce the API misuse rate.\nRobustness Analysis\nWe evaluate the benchmark on GPT 3.5 under different tem-\nperatures (Table 5). From the result, changing temperature\ndoes not significantly change the misuse rate and compila-\ntion rate. To study the effect of different prompting methods,\nwe study how the API misuse rate changes when we replace\nthe one-shot examples with the API usage rules. We feed\nthe symbolized rules to ChatGPT and get the rules in natural\nlanguage. We add the usage rules as part of the prompts and\nevaluate GPT-3.5 with ROBUST API. The results are shown\nin Table 6, which indicates that the API usage rules might\nnot help reduce the API misuse rate compared to one-shot\nrelevant examples.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21846\nTemp. Misuse Rate\nCompilation Rate Overall Misuse\nT = 0 38.56%\n80.71% 31.13%\nT = 0.5 39.77% 80.13% 31.87%\nT = 1.0 39.06% 76.08% 29.72%\nTable 5: Results of GPT 3.5 with different temperature\n(Pass@1, one-relevant-shot).\nPrompt Misuse Rate\nCompilation Rate Overall Misuse\nUsage Rule 65.01% 79.78%\n51.86%\nRelevant Shot 38.56% 80.71% 31.13%\nTable 6: Results of GPT 3.5 with API usage rules (T=0,\nPass@1).\nFinding 5. Increasing temperature or replacing one shot ex-\namples with API rules does not affect the API misuse rate\nsignificantly.\nError Analysis\nActivity.setContentViewApplicationInfo.loadIconBufferedReader.readLine\nCipher.init\nDataOutputStream.write\nFile.createNewFile\nFile.mkdirs\nFileChannel.writeInputStream.read\nIterator.next\nJsonElement.getAsString\nList.get\nMac.doFinal\nMap.get\nPrintWriter.write\nProgressDialog.dismissRandomAccessFile.readRandomAccessFile.writeSQLiteDatabase.query\nSortedMap.firstKey\nString.getBytes\nStringT okenizer.nextT oken\nT ypedArray.getString\ngpt_zero_shot\ngpt_fake_shot\ngpt_one_shot\ngpt4_zero_shot\ngpt4_fake_shot\ngpt4_one_shot\nllama_zero_shot\nllama_fake_shot\nllama_one_shot\nvicuna_zero_shot\nvicuna_fake_shot\nvicuna_one_shot\nMisuse Rate for Each API in Each Model\n0\n20\n40\n60\n80\n100\nFigure 4: Misuse rate of each API by each LLM. Thedeeper\nthe color, the higher the misuse rate. G3.5, G4, LMA, Vic\nare short for GPT3.5, GPT4, Llama2, Vicuna1.5.\nIn this section, we discuss the answers from LLMs that\ncannot pass the API usage check in R OBUST API evalua-\ntion. There are two categories for failure cases: cases that\nare not compilable, and cases that are compilable but con-\ntain API misuses as shown in Figure 3. We refer to the abil-\nity to be compiled successfully as compilability. The com-\npilation failure rate is calculated by dividing the number of\ncases that can be compiled to the total number of cases in the\nbenchmarks. GPT-4 performs the best among all the mod-\nels regarding compilability, which has less than 10% of an-\nswers that cannot be compiled across all experiment settings.\nAdding a few shots to prompts helps reduce the compilation\nfailure rate in the evaluation results for all models. As for the\nAPI misuse rate, we dive deeper into the APIs that LLMs are\nprone to misuse. Figure 4 details the misuse rate of each API\nfor each LLM. Among all APIs, the Android development\nAPI Activity.setContentView has the lowest misuse rate\nacross all the models.\nCase Study: API Misuse in GPT-3.5\nTaking GPT-3.5 as an example, we show a typical sample\nthat GPT-3.5 reacts differently under different experiment\nsettings. This question asks the model to help write a string\nto the file using API PrintWriter.write. Under zero-shot\nand one-irrelevant-shot settings, the answers differ slightly\nbut both misuse the API by not catching exceptions. After\ngiving the model the correct API usage example, the model\nlearns how to use the API and responds correctly.\nZero Shot:\nPrintWriter writer = new PrintWriter(\"f.txt\", true);\nwriter.write(\"text to append\");\nwriter.close();\nOne Irrelevant Shot:\nString text = \"Hello, World!\";\nPrintWriter writer = new PrintWriter(\"f.txt\", true);\nwriter.write(text);\nwriter.close();\nOne Relevant Shot:\ntry {String text = \"Hello, World!\";\nPrintWriter writer = new PrintWriter(\"f.txt\", true);\nwriter.write(text);\n} catch (IOException e) {e.printStackTrace();}\nDiscussion\nExtend to Other Language ROBUST API focuses on Java\nAPI usage since Java is one of the most widely used lan-\nguages in software development and has a special niche in\nweb and Android ecosystems so that its API misuses may\ncause more serious problems in real applications. Theoreti-\ncally, the method proposed in this paper can also be applied\nto other languages like Python.\nFuture Work The API misuse problem proposed in our\nresearch can motivate many further research directions.\nFirst, how to improve the quality of generated code aside\nfrom functionality alignment. To achieve this goal, in-\ncontext learning, fine-tuning, and pre-training would be crit-\nical to improving existing models. Besides, other online\ncode community like Github could also be a useful re-\nsource to evaluate code models, as proposed in a recent\nwork (Jimenez et al. 2023). As we believe, evaluating and\nimproving LLMs on the perspective of real-world software\ndevelopment is a demanding and important problem.\nConclusion\nIn this paper, we propose a benchmark ROBUST API to study\nthe API misuse behaviors in code generated by LLMs. From\nthe benchmark results on state-of-the-art models, we find\nthat API misuse widely exists in large language models even\nwhen the code is executable and aligned with users’ inten-\ntion. Under different experiment settings, we explore effec-\ntive methods of benchmarking and improving the API mis-\nuse rate of LLMs. To inspire and accelerate future research\non this problem, we open source the dataset and benchmark\nin https://github.com/FloridSleeves/RobustAPI.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21847\nAcknowledgments\nThe authors sincerely appreciate the reviewers and chairs of\nthe AAAI for their constructive and insightful comments.\nTheir expertise and thorough reviews have significantly con-\ntributed to the enhancement of this paper.\nReferences\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin,\nD.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen,\nZ.; et al. 2023. Palm 2 technical report. arXiv preprint\narXiv:2305.10403.\nAustin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski,\nH.; Dohan, D.; Jiang, E.; Cai, C.; Terry, M.; Le, Q.; et al.\n2021. Program synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;\nKaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman,\nG.; et al. 2021. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nFischer, F.; B¨ottinger, K.; Xiao, H.; Stransky, C.; Acar, Y .;\nBackes, M.; and Fahl, S. 2017. Stack overflow considered\nharmful? the impact of copy&paste on android application\nsecurity. In 2017 IEEE Symposium on Security and Privacy\n(SP), 121–136. IEEE.\nFischer, G.; Lusiardi, J.; and V on Gudenberg, J. W. 2007.\nAbstract syntax trees-and their role in model driven software\ndevelopment. In International Conference on Software En-\ngineering Advances (ICSEA 2007), 38–38. IEEE.\nHendrycks, D.; Basart, S.; Kadavath, S.; Mazeika, M.;\nArora, A.; Guo, E.; Burns, C.; Puranik, S.; He, H.; Song, D.;\net al. 2021. Measuring coding challenge competence with\napps. arXiv preprint arXiv:2105.09938.\nHuang, H.; Shen, B.; Zhong, L.; and Zhou, Y . 2023. Pro-\ntecting data integrity of web applications with database con-\nstraints inferred from application code. InProceedings of the\n28th ACM International Conference on Architectural Sup-\nport for Programming Languages and Operating Systems,\nVolume 2, 632–645.\nJesse, K.; Ahmed, T.; Devanbu, P. T.; and Morgan, E. 2023.\nLarge Language Models and Simple, Stupid Bugs. arXiv\npreprint arXiv:2303.11455.\nJimenez, C. E.; Yang, J.; Wettig, A.; Yao, S.; Pei, K.; Press,\nO.; and Narasimhan, K. 2023. SWE-bench: Can Language\nModels Resolve Real-World GitHub Issues? arXiv preprint\narXiv:2310.06770.\nLiu, J.; Xia, C. S.; Wang, Y .; and Zhang, L. 2023. Is your\ncode generated by chatgpt really correct? rigorous evalua-\ntion of large language models for code generation. arXiv\npreprint arXiv:2305.01210.\nLu, S.; Guo, D.; Ren, S.; Huang, J.; Svyatkovskiy, A.;\nBlanco, A.; Clement, C.; Drain, D.; Jiang, D.; Tang, D.; et al.\n2021. Codexglue: A machine learning benchmark dataset\nfor code understanding and generation. arXiv preprint\narXiv:2102.04664.\nLuo, Z.; Xu, C.; Zhao, P.; Sun, Q.; Geng, X.; Hu, W.; Tao,\nC.; Ma, J.; Lin, Q.; and Jiang, D. 2023. WizardCoder: Em-\npowering Code Large Language Models with Evol-Instruct.\narXiv preprint arXiv:2306.08568.\nNguyen, H. A.; Dyer, R.; Nguyen, T. N.; and Rajan, H. 2014.\nMining preconditions of APIs in large-scale code corpus. In\nProceedings of the 22nd ACM SIGSOFT International Sym-\nposium on Foundations of Software Engineering, 166–177.\nOpenAI. 2023a. GPT-4 Technical Report. ArXiv,\nabs/2303.08774.\nOpenAI. 2023b. GPT-4 Technical Report.\narXiv:2303.08774.\nPatil, S. G.; Zhang, T.; Wang, X.; and Gonzalez, J. E. 2023.\nGorilla: Large language model connected with massive apis.\narXiv preprint arXiv:2305.15334.\nPearce, H.; Ahmad, B.; Tan, B.; Dolan-Gavitt, B.; and Karri,\nR. 2022. Asleep at the keyboard? assessing the security of\ngithub copilot’s code contributions. In 2022 IEEE Sympo-\nsium on Security and Privacy (SP), 754–768. IEEE.\nPerry, N.; Srivastava, M.; Kumar, D.; and Boneh, D. 2022.\nDo users write more insecure code with AI assistants?arXiv\npreprint arXiv:2211.03622.\nPiplani, T.; and Bamman, D. 2018. DeepSeek: Con-\ntent based image search & retrieval. arXiv preprint\narXiv:1801.03406.\nPoesia, G.; Polozov, O.; Le, V .; Tiwari, A.; Soares, G.;\nMeek, C.; and Gulwani, S. 2022. Synchromesh: Reliable\ncode generation from pre-trained language models. arXiv\npreprint arXiv:2201.11227.\nSandoval, G.; Pearce, H.; Nys, T.; Karri, R.; Garg, S.; and\nDolan-Gavitt, B. 2023. Lost at c: A user study on the se-\ncurity implications of large language model code assistants.\narXiv preprint arXiv:2208.09727.\nShen, B.; Zhang, J.; Chen, T.; Zan, D.; Geng, B.; Fu, A.;\nZeng, M.; Yu, A.; Ji, J.; Zhao, J.; et al. 2023a. PanGu-\nCoder2: Boosting Large Language Models for Code with\nRanking Feedback. arXiv preprint arXiv:2307.14936.\nShen, X.; Chen, Z.; Backes, M.; and Zhang, Y . 2023b. In\nchatgpt we trust? measuring and characterizing the reliabil-\nity of chatgpt. arXiv preprint arXiv:2304.08979.\nSiddiq, M. L.; Majumder, S. H.; Mim, M. R.; Jajodia, S.; and\nSantos, J. C. 2022. An Empirical Study of Code Smells in\nTransformer-based Code Generation Techniques. In 2022\nIEEE 22nd International Working Conference on Source\nCode Analysis and Manipulation (SCAM), 71–82. IEEE.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nWang, J.; Dang, Y .; Zhang, H.; Chen, K.; Xie, T.; and Zhang,\nD. 2013. Mining succinct and high-coverage API usage pat-\nterns from source code. In 2013 10th Working Conference\non Mining Software Repositories (MSR), 319–328. IEEE.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21848\nXu, F. F.; Alon, U.; Neubig, G.; and Hellendoorn, V . J. 2022.\nA systematic evaluation of large language models of code. In\nProceedings of the 6th ACM SIGPLAN International Sympo-\nsium on Machine Programming, 1–10.\nYang, D.; Hussain, A.; and Lopes, C. V . 2016. From query to\nusable code: an analysis of stack overflow code snippets. In\nProceedings of the 13th International Conference on Mining\nSoftware Repositories, 391–402.\nYe, J.; Chen, X.; Xu, N.; Zu, C.; Shao, Z.; Liu, S.; Cui, Y .;\nZhou, Z.; Gong, C.; Shen, Y .; et al. 2023. A comprehensive\ncapability analysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\nYetistiren, B.; Ozsoy, I.; and Tuzun, E. 2022. Assessing the\nquality of GitHub copilot’s code generation. In Proceedings\nof the 18th International Conference on Predictive Models\nand Data Analytics in Software Engineering, 62–71.\nYin, P.; Deng, B.; Chen, E.; Vasilescu, B.; and Neubig, G.\n2018. Learning to mine aligned code and natural language\npairs from stack overflow. In Proceedings of the 15th inter-\nnational conference on mining software repositories, 476–\n486.\nZhang, T.; Upadhyaya, G.; Reinhardt, A.; Rajan, H.; and\nKim, M. 2018. Are code examples on an online Q&A forum\nreliable?: a study of API misuse on stack overflow. In 2018\nIEEE/ACM 40th International Conference on Software En-\ngineering (ICSE). IEEE, New York, United States, 886–896.\nZhou, J.; and Walker, R. J. 2016. API deprecation: a ret-\nrospective analysis and detection method for code examples\non the web. In Proceedings of the 2016 24th ACM SIGSOFT\nInternational Symposium on Foundations of Software Engi-\nneering, 266–277.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21849",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7204633355140686
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.7088650465011597
    },
    {
      "name": "Code generation",
      "score": 0.4815290570259094
    },
    {
      "name": "Code (set theory)",
      "score": 0.4644840359687805
    },
    {
      "name": "Reliability engineering",
      "score": 0.46284031867980957
    },
    {
      "name": "Stack (abstract data type)",
      "score": 0.42587965726852417
    },
    {
      "name": "Programming language",
      "score": 0.2574765682220459
    },
    {
      "name": "Engineering",
      "score": 0.1670151650905609
    },
    {
      "name": "Operating system",
      "score": 0.15435543656349182
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    }
  ]
}