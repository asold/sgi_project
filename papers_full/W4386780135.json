{
  "title": "Improving diagnosis and prognosis of lung cancer using vision transformers: a scoping review",
  "url": "https://openalex.org/W4386780135",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2186945331",
      "name": "Ali Hazrat",
      "affiliations": [
        "Hamad bin Khalifa University",
        "Qatar Foundation"
      ]
    },
    {
      "id": "https://openalex.org/A4307539343",
      "name": "Mohsen, Farida",
      "affiliations": [
        "Qatar Foundation",
        "Hamad bin Khalifa University"
      ]
    },
    {
      "id": "https://openalex.org/A4213655685",
      "name": "Shah, Zubair",
      "affiliations": [
        "Hamad bin Khalifa University",
        "Qatar Foundation"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2024492396",
    "https://openalex.org/W2807800408",
    "https://openalex.org/W2988799679",
    "https://openalex.org/W4205164650",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2900954917",
    "https://openalex.org/W4205739105",
    "https://openalex.org/W2525884435",
    "https://openalex.org/W2909244736",
    "https://openalex.org/W2608231518",
    "https://openalex.org/W2893531431",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4308081867",
    "https://openalex.org/W3192812714",
    "https://openalex.org/W4293163051",
    "https://openalex.org/W4308578764",
    "https://openalex.org/W3161958962",
    "https://openalex.org/W2981841914",
    "https://openalex.org/W4360939568",
    "https://openalex.org/W4310668250",
    "https://openalex.org/W4286355566",
    "https://openalex.org/W4284713627",
    "https://openalex.org/W4281257868",
    "https://openalex.org/W3211647829",
    "https://openalex.org/W4288809219",
    "https://openalex.org/W4221080555",
    "https://openalex.org/W4312713525",
    "https://openalex.org/W4313031477",
    "https://openalex.org/W4312599853",
    "https://openalex.org/W3026063058",
    "https://openalex.org/W4312468136",
    "https://openalex.org/W4224308582",
    "https://openalex.org/W4283466931",
    "https://openalex.org/W3161200432",
    "https://openalex.org/W4281400392",
    "https://openalex.org/W4309433391",
    "https://openalex.org/W4309073588",
    "https://openalex.org/W3201782492",
    "https://openalex.org/W4312337217",
    "https://openalex.org/W4225426831",
    "https://openalex.org/W4286377440",
    "https://openalex.org/W4303613786",
    "https://openalex.org/W4289444084",
    "https://openalex.org/W4301392676",
    "https://openalex.org/W4297460160",
    "https://openalex.org/W4310464206",
    "https://openalex.org/W4281850275",
    "https://openalex.org/W4310672790",
    "https://openalex.org/W3203898052",
    "https://openalex.org/W3204764952",
    "https://openalex.org/W4226120250",
    "https://openalex.org/W4312523470",
    "https://openalex.org/W4283816523",
    "https://openalex.org/W4286587621",
    "https://openalex.org/W2891378911",
    "https://openalex.org/W2560438049",
    "https://openalex.org/W4281683654",
    "https://openalex.org/W3100819185",
    "https://openalex.org/W3105282616",
    "https://openalex.org/W3105100200"
  ],
  "abstract": null,
  "full_text": "RESEARCH ARTICLE Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, \nsharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The \nCreative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available \nin this article, unless otherwise stated in a credit line to the data.\nAli et al. BMC Medical Imaging          (2023) 23:129 \nhttps://doi.org/10.1186/s12880-023-01098-z\nBMC Medical Imaging\n*Correspondence:\nHazrat Ali\nhaali2@hbku.edu.qa\nZubair Shah\nzshah@hbku.edu.qa\nFull list of author information is available at the end of the article\nAbstract\nBackground Vision transformer-based methods are advancing the field of medical artificial intelligence and cancer \nimaging, including lung cancer applications. Recently, many researchers have developed vision transformer-based AI \nmethods for lung cancer diagnosis and prognosis.\nObjective This scoping review aims to identify the recent developments on vision transformer-based AI methods \nfor lung cancer imaging applications. It provides key insights into how vision transformers complemented the \nperformance of AI and deep learning methods for lung cancer. Furthermore, the review also identifies the datasets \nthat contributed to advancing the field.\nMethods In this review, we searched Pubmed, Scopus, IEEEXplore, and Google Scholar online databases. The search \nterms included intervention terms (vision transformers) and the task (i.e., lung cancer, adenocarcinoma, etc.). Two \nreviewers independently screened the title and abstract to select relevant studies and performed the data extraction. \nA third reviewer was consulted to validate the inclusion and exclusion. Finally, the narrative approach was used to \nsynthesize the data.\nResults Of the 314 retrieved studies, this review included 34 studies published from 2020 to 2022. The most \ncommonly addressed task in these studies was the classification of lung cancer types, such as lung squamous \ncell carcinoma versus lung adenocarcinoma, and identifying benign versus malignant pulmonary nodules. Other \napplications included survival prediction of lung cancer patients and segmentation of lungs. The studies lacked clear \nstrategies for clinical transformation. SWIN transformer was a popular choice of the researchers; however, many other \narchitectures were also reported where vision transformer was combined with convolutional neural networks or UNet \nmodel. Researchers have used the publicly available lung cancer datasets of the lung imaging database consortium \nand the cancer genome atlas. One study used a cluster of 48 GPUs, while other studies used one, two, or four GPUs.\nConclusion It can be concluded that vision transformer-based models are increasingly in popularity for developing \nAI methods for lung cancer applications. However, their computational complexity and clinical relevance are \nimportant factors to be considered for future research work. This review provides valuable insights for researchers in \nImproving diagnosis and prognosis of lung \ncancer using vision transformers: a scoping \nreview\nHazrat Ali1*, Farida Mohsen1 and Zubair Shah1*\nPage 2 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nIntroduction\nLung cancer is a highly prevalent and fatal form of can -\ncer globally [ 1, 2]. Over the last few decades, medical \nimaging techniques have played an increasingly vital \nrole in diagnosing, prognosis, survival prediction, and \nearly detection of lung cancer, eventually aiding in effec -\ntive cure and prevention. Such techniques make use \nof lung computed tomography (CT), X-rays, positron \nemission tomography (PET), and magnetic resonance \nimaging (MRI). Traditionally, medical images in clini -\ncal work have been interpreted and analyzed by trained \nradiologists who use their expertise and experience to \nmake accurate diagnoses. However, the manual interpre -\ntation of medical images can be time-consuming, prone \nto human error, and affected by intra-observer as well as \ninter-observer variability.\nArtificial intelligence (AI) methods, particularly deep \nlearning models, have played a vital role in automating \nimage processing in the past few years and have been \ngaining increasing attention in medical imaging [ 3, 4]. \nAI methods dominated by convolutional neural net -\nworks (CNNs) [ 5, 6] have revolutionized the realm of \nmedical imaging with their capability of learning com -\nplex representations enabling the automated diagno -\nsis of diseases and the detection of abnormalities. They \nhave demonstrated remarkable improvements in various \nmedical imaging applications and modalities, including \nMRI [7, 8], CT [ 9], endoscopy [ 10], and radiography [ 11, \n12], to name a few. However, the advent of transformers \napprised researchers of CNNs’ major drawback, i.e., the \ninability to capture long-range dependencies such as the \nextraction of contextual information and the non-local \ncorrelation of objects.\nRecently, Dosovitskiy et al. [13] sought to apply the suc-\ncess of transformers in natural language processing to \nimage processing. They developed a vision transformer \nto capture long-term dependencies within an image by \ntreating image classification as a sequence prediction \ntask for a series of image patches. On several benchmark \ndatasets, the vision transformer and its derived instances \ndemonstrated state-of-the-art (SOTA) performance \nand gained popularity in several computer vision tasks, \nincluding classification [ 13], segmentation [ 14], and \ndetection [ 15]. The use of vision transformers has also \nbeen cross-pollinated into the medical image field, where \nthey are used for image segmentation [16], synthesis [17], \nand disease diagnosis, resulting in SOTA performances. \nFor lung cancer imaging applications, the use of vision \ntransformers has gained attention for different applica -\ntions, including cancer classification, tumor segmenta -\ntion, nodule detection, and survival prediction. Much \nnew vision transformer-based AI methods for lung can -\ncer imaging applications have recently been published by \nresearchers.\nOur scoping review aims to present a comprehensive \noverview of the recent studies that developed vision \ntransformer-based AI methods for lung cancer imag -\ning. While there are a few related reviews in the litera -\nture [18–21]; they differ in their focus and coverage. For \nexample, the review in [ 18] covers the applications of \nvision transformers in medical imaging; however, it is not \nspecific to lung cancer imaging applications and covers \nmany different medical imaging applications. Similarly, \nthe reviews in [ 19, 20] cover other AI methods for can -\ncer imaging but do not include vision transformers, while \nthe review in [ 21] covers AI methods for lung cancer \napplications but covers only pathology imaging and does \nnot cover all the imaging modalities. Besides, it does not \ncover the recent developments of vision transformers for \nlung cancer imaging, as the review was published much \nearlier. To the best of our knowledge, no review study \nfocuses specifically on the utilization of vision trans -\nformers for medical imaging in lung cancer. Therefore, \nour review is the first comprehensive review that focuses \nspecifically on the use of vision transformers for medical \nimaging in lung cancer, providing a thorough overview of \nthe current state of the field. Table  1 shows the literature \nreview comparison.\nThe primary aim of our scoping review is to synthesize \nscientific literature by answering the following research \nquestions, as listed in Fig. 1C.\nWe are confident that this review will provide a com -\nprehensive text on the recent developments in vision \ntransformer-based lung cancer imaging applications.\nResults\nSearch results\nThe search retrieved 314 studies. However, 92 studies \nwere duplicates that we removed. We removed 183 stud -\nies according to the exclusion/inclusion criteria in the \ntitle and abstract screening phase. In the remaining 39 \nstudies, we removed eight more studies after the full-text \nreading phase, as they did not fulfill the inclusion crite -\nria. We added three additional studies through forward/\nbackward referencing. Finally, we included 34 unique \nstudies [24–57] in the review (also see Appendix 1 for all \nthe field of AI and healthcare to advance the state-of-the-art in lung cancer diagnosis and prognosis. We provide an \ninteractive dashboard on lung-cancer.onrender.com/.\nKeywords Adenocarcinoma, Artificial Intelligence, Convolutional neural networks, Deep learning, Diagnosis, Lung \nCancer, Medical imaging, Segmentation, Survival prediction, Vision Transformers\nPage 3 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nthe included studies). Figure  1 shows the flowchart for \nthe different phases of the study selection and the num -\nber of studies retained in each phase. Readers may access \nan interactive dashboard on lung-cancer.onrender.com/. \n(Loading may take up to 60 s)\nDemographics of the included studies\nIn the included studies, half of the studies (n = 17) were \njournal articles, while 16 studies were published in con -\nference proceedings. Only one study was published as \na thesis. Most of the studies (n = 28) were published in \n2022, four studies were published in 2021, and only two \nstudies were published in 2020. Of the articles published \nin 2022, five studies were published in June, five were \npublished in September, and four were published in \nAugust. Of the four studies published in 2021, no study \nwas published in the first eight months. The included \nstudies were published by researchers from seven differ -\nent countries (first-author country affiliation). Research -\ners from China published almost two-third (n = 21) of \nthe studies, while researchers from the USA published \napproximately one-fourth (n = 8) of the studies. Research-\ners from India, Saudi Arabia, Pakistan, Canada, and \nSouth Korea published one study each. Figure  2 shows a \nsummary of the year-wise, and Fig.  3 shows the country-\nwise demographics of the included studies. Table  2 sum-\nmarizes the demographics of the included studies.\nMain tasks addressed in the studies\nIn the 34 studies included in this review, one-third of \nstudies (n = 11) [24–35] performed classification of dif -\nferent types of lung cancers. Nearly half of the studies \n(n = 15) [ 35, 43, 45–57] used vision transformer-based \nmodels to predict the growth of tumors or the course \nof cancer. Of these, eight studies [ 35, 43, 48, 53–57] \ndeveloped vision transformer-based models for survival \nprediction of lung cancer patients. Six studies [ 36–43] \naddressed the task of segmentation of tumor or lung nod-\nules. One study [ 44] performed lung nodule detection. \nFew studies performed more than one task. For example, \none study [35] performed the classification of lung cancer \ntypes and reported performance for survival prediction \ntoo. Similarly, one study [ 42] performed segmentation \nof lungs and detection of nodules, and one study [ 43] \nreported segmentation of lungs and survival prediction \nof patients. Figure  4 shows a mapping of the different \ntasks addressed in the included studies.\nTable 1 Literature comparison with previous review studies\nReference Year Scope and coverage Differences with our review\nTransformers in Medi-\ncal Image Analysis [18]\nAugust\n2022\nIt focuses on the use of transformers for various medical imaging \napplications.\nIt is not specific to lung cancer imaging.\nIt does not cover many recent studies on vision transformers in lung \ncancer imaging.\nOur review focuses specifically on the \nuse of vision transformers for lung cancer \nimaging.\nOur review covers many recent studies \npublished in later 2022.\nArtificial intelligence \nin lung cancer: cur-\nrent applications and \nperspectives [19]\nNo-\nvember \n2022\nIt focuses on the current state of AI in lung cancer.\nIt covers traditional machine learning and deep learning methods for \nlung nodule detection and segmentation.\nIt does not cover vision transformer-based approaches.\nOur review focuses specifically on the \nuse of vision transformers for lung cancer \nimaging.\nArtificial intelligence \ntechniques for cancer \ndetection in medical \nimage processing: A \nreview [20]\nMay \n2021\nIt focuses on a broad range of AI methods.\nIt covers many different types of cancer imaging applications.\nIt is not specific to vision transformers.\nIt is not specific to lung cancer.\nIt does not cover many recent studies.\nOur review focuses specifically on the use \nof vision transformers for medical imaging \nin lung cancer.\nOur review is specific to vision transformers\nOur review is specific to lung cancer.\nOur review covers recent studies.\nArtificial Intelligence in \nLung Cancer Pathology \nImage Analysis [21]\nNo-\nvember \n2019\nIt focuses on AI methods for pathology image analysis of lung \ncancer.\nIt does not cover vision transformers.\nIt does not cover recent studies.\nOur review covers the use of transformers \nfor medical imaging in lung cancer.\nOur review covers different imaging modal-\nities, including pathology and CT scans.\nOur review is specific to vision transformers.\nOur review covers recent studies.\nRecent advances of \nTransformers in medi-\ncal image analysis: A \ncomprehensive review \n[22]\nMarch \n2023\nIt focuses on transformers for various medical imaging applications.\nIt is not specific to lung cancer imaging and covers many studies on \nCOVID-19.\nIt does not cover many recent studies on vision transformers in lung \ncancer imaging.\nOur review focuses specifically on the \nuse of vision transformers for lung cancer \nimaging.\nOur review covers many recent studies \npublished in later 2022.\nMachine Learning for \nLung Cancer Diag-\nnosis, Treatment, and \nPrognosis\n [23]\nOc-\ntober \n2022\nIt covers different machine learning and deep learning techniques.\nIt does not cover any study on vision transformer applications.\nIt is not specific to imaging modality and covers studies on different \ndata modalities for lung cancer.\nOur review focuses on vision transformer \napplications in lung cancer imaging.\nOur review focuses on imaging data for \nlung cancer.\nPage 4 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nKey implementation details\nIn the included studies, vision transformers were com -\nbined with CNNs, UNet, or graph networks. In the \nincluded studies, seven studies [ 25, 28, 34, 37, 51, 57] \ncombined vision transformers with CNNs, three studies \n[36, 38, 52] used vision transformers in combination with \nUNet model, one study [ 41] combined both CNN and \nUNet with vision transformer, one study combined vision \ntransformer with ResNet model. One study [ 24] com -\nbined the mask R-CNN model with a vision transformer \nto perform segmentation followed by classification. Two \nstudies [ 48, 55] explored the use of graph networks in \ncombination with vision transformers. Six studies [24, 28, \n32, 46, 49, 50] used SWIN transformer as their backbone \ntransformer architecture.\nAlmost half of the studies (n = 18) [24, 26–28, 30, 31, 33, \n35–37, 41, 42, 45–49, 53] reported that their implemen -\ntation was in Pytorch framework, while one study [ 25] \nreported the use of TensorFlow and Keras frameworks. \nThe remaining studies did not specify the framework \nused.\nFig. 1 ( a) Search terms used. (b) The PRISMA-ScR flowchart for the selection of the included studies. (c) Research questions\n \nPage 5 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nThree studies [ 24, 26, 40] reported the use of a single \nNvidia RTX 2080Ti GPU that usually comes with 11 GB \nmemory, while one study [ 53] reported the use of four \nNvidia RTX 2080Ti GPUs with 12 GB memory. Two stud-\nies [25, 36] reported the use of Nvidia P100 GPU, where \nthe authors in [25] accessed the GPU via the Kaggle com-\nputational platform. Four studies [28, 39, 42, 57] reported \nthe use of Nvidia V100 GPUs. Of these, one study [ 39] \nused four GPUs, one study [ 42] used two GPUs, and one \nstudy [57] used a single V100 GPU. Three studies [ 27, 33, \n37] used a single Nvidia RTX3090 GPU, while one study \n[41] used a combination of two Nvidia RT3090 GPUs. \nThree studies [45, 54, 55] used a single Nvidia GTX 1080 \nor 1080Ti GPU with 11 GB memory. One study [47] used \nNvidia Titan-XP GPU. The largest number of GPUs usage \nwas reported by [ 44], who used 48 Nvidia V100 GPUs. \nThe remaining studies did not provide information on \nGPU usage.\nTypes of data used in the studies\nIn the included studies, 22 studies reported the publicly \navailable use of data, six studies reported experiments \non privately collected data, and six studies used both \npublic and private datasets. In the included studies, 23 \ndeveloped models for 2D image data while 11 developed \nmodels for volumetric data. Nearly two-third (n = 21) of \nthe included studies used computed tomography (CT) \nscans of lung, while one-third (n = 11) studies used histo -\npathology or whole slide images of lungs. One study used \nPET, while another used CT and MRI scans. Table 3 sum-\nmarizes the use of types of data in the included studies. \nTable 2 Demographics of the inlcuded studies\nYear Year Month Number of studies\n2022 (n = 28) January 1\nMarch 1\nApril 1\nMay 1\nJune 5\nJuly 2\nAugust 4\nSeptember 5\nOctober 5\nNovember 2\nDecember 1\n2021 (n = 4) September 1\nOctober 2\nDecember 1\n2020 (n = 2) January 1\nApril 1\nCountries Country Number of studies\nChina 21\nUSA 8\nCanada 1\nIndia 1\nSaudi Arabia 1\nSouth Korea 1\nPakistan 1\nType of publication Venue Number of studies\nJournal 17\nConference 16\nThesis 1\nFig. 3 Country-wise number of publications\n \nFig. 2 Summary of year-wise and month-wise number of publications\n \nPage 6 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nFigure 5 shows the number of studies that used different \nmodalities of data. Figure  6 shows the Venn diagram for \nthe number of studies using public versus private data.\nDatasets used in the studies\nIn the included studies, six studies [ 30–32, 40, 44, 52] the \nLung Imaging Database Consortium (LIDC-IDRI) data -\nset, five studies [27, 28, 54–56] used The Cancer Genome \nAtlas (TCGA) datasets, four studies [ 34, 36–38] used the \nLUNA16 dataset. Table  4 summarizes the datasets used \nin the included studies along with the URL for the pub -\nlicly available datasets.\nEvaluation metrics\nThe most commonly used evaluation metrics in the \nincluded studies were accuracy and area under the \nROC curve (AUC), each reported in 16 studies. Other \npopular metrics were specificity reported in 11 stud -\nies, sensitivity reported in nine studies, dice similarity \ncoefficient reported in seven studies, and concordance \nindex reported in six studies. Both precision and recall \nTable 3 Model parameter estimates with the entire student \nsample\nUsed by\nAvailability \nof dataset\nPublic data (n = 22) [25–28], [30–32], [34–40], [42, \n44], [51–56]\nPrivate data (n = 6) [24, 29, 33, 41, 46, 49]\nPublic and private data \n(n = 6)\n[43, 45, 47, 48, 50, 57]\nDimension-\nality of data\n2D models (2D data) \n(n = 23)\n[24], [26–28], [30–32], [34–37], \n[40, 41], [47–50], [52–57]\n3D models (volumetric \ndata) (n = 11)\n[25, 29, 33, 38, 39], [42–46], \n[51]\nModality of \nimage data\nCT (n = 21) [29, 30, 32, 33, 33, 34], \n[36–42], [44–52]\nHistopathology (n = 11) (n = 11) [24–28], [35], [53–57]\nPET (n = 1) [43]\nCT and MRI (n = 1) [39]\nFig. 4 Different tasks addressed in the included studies. The main tasks included classification of lung cancer types, segmentation of lungs, survival pre-\ndiction for cancer patients, and prediction of course of the disease\n \nPage 7 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nmeasures were reported in five studies each. Other met -\nrics were the F1 score, mean absolute error, mean abso -\nlute error, root mean square error, and Kappa score, each \nreported by one study. Figure  7 summarizes the number \nof studies using different evaluation metrics.\nAlmost one-third of studies (n = 11) reported splitting \nthe data into training, validation, and test sets, while five \nreported splitting the data into training and test sets only. \nSimilarly, eight studies used a 5-fold cross-validation \nscheme, while six used a 10-fold cross-validation scheme \nto evaluate the performance of their methods.\nIn the included studies, only nine studies [ 26–28, 35, \n37] provided a GitHub link for the implementation code.\nData preprocessing\nIn the included studies, only 19 out of 34 provided some \ninformation on the data preprocessing. Of these, ten \nstudies [26, 28, 30–32, 44–46, 56, 57] reported that they \nhave done patch extraction. The patches are extracted \nwith fixed sizes such as 96 × 96 × 96 volumetric CT \npatches in [ 44] or 32 × 32 patches from 2D images in \n[30]. These patches are usually extracted with fixed sizes \nto help in reducing computational overload and load \nthe samples in the memory. Six studies [ 25, 29, 32, 34, \n37, 48] reported that they resized or reshaped the input \nimage data into fixed dimensions such as 128 × 128 pixels \nin [25] or resized in the spatial domain such as 1 mm × \n1 mm × 1 mm in [ 34] or 64 mm ×64 mm×36 mm in [ 48]. \nThree studies [31, 38, 42] reported that they applied nor -\nmalization to the data, for example, by transforming the \nvalues in the [0, 1] range. Five studies [ 30, 33, 34, 36, 38] \nreported different image augmentation techniques such \nas random rotation, random flipping, random affine, \nrandom shearing, zooming, horizontal and vertical flip -\nping, and shifting, applied to the data before using the \ndata for model training. In 15 studies [ 24, 27, 35, 39–\n41, 43, 47, 49–55], no details are provided on the data \npreprocessing.\nDeep learning approaches such as unsupervised and \nself-supervised learning eliminate the need for man -\nual feature engineering and feature selection. So, the \nincluded studies do not mention feature selection meth -\nods as they extract features using deep learning.\nDiscussion\nPrinciple results\nThis study provides an overview of recent literature on \nthe utilization of vision transformer-based artificial intel -\nligence models for enhancing the diagnosis, prognosis, \nand classification of lung cancer. In the review, we did \nnot find any studies before 2020. This is not surprising \nas vision transformers were proposed in 2017, and their \nuse in medical imaging has recently gained popularity. \nMost of these studies were published in 2022, reflecting \nthe growing interest in developing vision transformer-\nbased approaches for lung cancer applications. However, \nthe diversity of authors was limited, as researchers from \nChina or the United States authored 85% of the studies.\nThe popularity of vision transformers for classification \ntasks has driven a majority of the studies reviewed in \nthis work to employ them for classifying different types \nof lung cancers. The classification tasks included separat -\ning lung squamous cell carcinoma from lung adenocar -\ncinoma, identifying benign versus malignant pulmonary \nnodules, and determining the invasiveness of lung adeno-\ncarcinomas. The studies also used vision transformers \nto predict lung cancer’s severity or growth, thus aiding \nin survival predictions for patients. Some of the studies \nwere limited to segmenting lung nodules.\nVision transformers effectively capture the long-range \ncontext in the input data, while CNNs tend to excel in \nFig. 6 Venn diagrams showing the contribution of public versus private \ndatasets used in the included studies\n \nFig. 5 Different data modalities used in the included studies\n \nPage 8 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \ncapturing short-range dependencies. This is why many \nstudies reviewed in this work combined vision transform-\ners and CNNs, either through cascade or parallel connec-\ntions or by incorporating vision transformer attention \nmechanisms into CNNs. Researchers integrated vision \ntransformers with UNet or Mask R-CNN models for seg -\nmentation tasks. Due to its inherent benefits, the SWIN \ntransformer was frequently used as the backbone archi -\ntecture for lung cancer imaging applications. The most \npopular framework for implementing vision transformer-\nbased models in the studies was Pytorch, with all but one \nstudy (that used Tensorflow and Keras) reporting the use \nof Pytorch as the implementation framework.\nTable 4 Datasets used in the included studies\nDataset name URL Used by\nLC25000 https://github.com/tampapath/lung_colon_image_set [24]\nNational Lung Screening Trial (NLST) https://www.cancer.gov/types/lung/research/nlst [26, 47, 50]\nNon-small cell lung cancer (NSCLC) https://www.cancer.gov/about-nci/organization/ccg/research/\nstructural-genomics/tcga/studied-cancers/lung-adenocarcinoma\n[27, 48]\nLung Squamous Cell Carcinoma (TCGA-LUSC) https://wiki.cancerimagingarchive.net/pages/viewpage.\naction?pageId=16056484\n[44, 54, 55]\nLung Adenocarcinoma (TCGA-LUAD) https://www.cancerimagingarchive.net/collections/tcga-luad/ [28, 53, 56]\nLung Imaging Database Consortium (LIDC-IDRI) https://wiki.cancerimagingarchive.net/pages/viewpage.\naction?pageId=1966254\n[30–32], [40, 44, 52]\nLUNA16 https://luna16.grand-challenge.org/Data/ [34, 36, 37, 44]\nTianchi Lung Nodule Detection dataset https://tianchi.aliyun.com/competition/entrance/231601/introduction [34]\nCbioportal https://www.cbioportal.org/ [35]\nMedical Segmentation Decathlon http://medicaldecathlon.com/ [42]\nLUNG1 https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics [43]\nLUNGx https://wiki.cancerimagingarchive.net/display/Public/\nSPIE-AAPM+Lung+CT+Challenge\n[44]\nNSCLC Radiogenomics https://wiki.cancerimagingarchive.net/display/Public/\nNSCLC+Radiogenomics\n[27, 39, 48]\nNLSTt (derived from NLST) https://github.com/liaw05/STMixer [51]\nShanghai pulmonary hospital Private [33, 48]\nHuadong Hospital dataset Private [45]\nWest China Hospital of Sichuan University Private [46]\nShanxi Provincial People’s Hospital Private [47, 50]\nCHCAMS Private [57]\nFig. 7 Evaluation metrics used in the included studies. DSC: dice similarity coefficient. MAE: mean absolute error. RMSE: root mean square error\n \nPage 9 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nVision transformer-based models, in general, are com -\nputationally demanding. The computational demands of \nvision transformer models were addressed in the studies, \nwith some utilizing multiple GPUs and one using a clus -\nter of 48 GPUs, while others demonstrated that imple -\nmentation on a single GPU was feasible.\nSince the most commonly addressed task was the clas -\nsification of lung cancers; hence, the studies reported \naccuracy and area under the ROC curve. The concor -\ndance index was a common evaluation metric for stud -\nies that reported survival prediction. In machine learning \nmodels, it is common to split the data in training, vali -\ndation, and test sets (or training and test sets); however, \nsome studies did not specify the evaluation mechanism \nand data split.\nPractical and research implications\nIn developing AI models based on vision transform -\ners, the availability of public datasets plays a crucial \nrole. More than two-thirds of the studies utilized pub -\nlicly available datasets for lung cancer imaging analysis. \nTo encourage further growth in this field, it is impera -\ntive to have a rich resource of large-scale public datasets \nfor lung cancer. In our review, the most commonly used \nimaging modality for lung cancer analysis was CT scans, \nfollowed by histopathology images. The use of PET and \nMRI was found to be less common. The Lung Imaging \nDatabase Consortium (LIDC) and The Cancer Genome \nAtlas (TCGA) offer extensive datasets for lung cancer \n(and other cancers) for researchers to utilize.\nDespite the promising outcomes of vision transformer-\nbased AI methods in analyzing lung cancer, they have \nlimitations, such as the reliance on a significant amount \nof computational resources, including clusters of numer -\nous GPUs, which may not be accessible to many research \nlaboratories. Moreover, their practical implementa -\ntion in a clinical setting remains unverified. Hence, \nthere is a pressing need to advance toward developing \ncomputationally efficient training methods for vision \ntransformers. The analyzed studies failed to furnish a \ncomprehensive understanding of the interpretability of \nvision transformer-based models. This information is \ncritical in applications such as predicting the survival of \nlung cancer patients, as it provides a deeper insight into \nthe progression of the disease and the related risk fac -\ntors. Additionally, the majority of studies (73%) did not \nprovide access to their implementation code, hindering \nthe ability of other researchers to reproduce the results or \nbuild upon the vision transformer-based models for lung \ncancer analysis. The absence of such links further reduces \nthe reproducibility of the reported studies.\nIn our review, studies from China and USA domi -\nnated the literature where the healthcare tools are \nadvanced, and thus, the new techniques can, in general, \nbe integrated with less effort. However, there is a lack of \nstudies from developing countries. However, there is a \nscarcity of studies from developing countries. It is antici -\npated that increased contributions from these countries \nwould aid in addressing the challenges of lung cancer in \nunderdeveloped economies, where the disease is more \nprevalent due to socio-economic reasons.\nThe included studies greatly varied in how they \nreported the different datasets’ usage or the number of \nimages for training, validation, or test sets. For example, \nmany studies reported the values of accuracy, sensitiv -\nity, specificity, or AUC, the number of samples in the test \nset varied between them, or the cross-validation strat -\negy differed (or was even absent) in some of the studies. \nAccordingly, this review does not provide a quantitative \nsummary of the results reported in the included studies \nfor two reasons. First, the review aimed to identify the \nrecent AI methods that used vision transformers for lung \ncancer imaging applications. Second, the review included \nmany studies that vary in how they report quantitative \nmetrics for the outcomes or how they organize their data; \nhence, establishing a direct summarization of the results \nis not practical. We believe that future systematic reviews \nshould also cover the clinical relevance of the methods \nfor lung cancer applications. This review did not find any \nimplementation of vision transformers-based methods \nfor mobile devices. Mobile devices will carry a significant \nrole in transforming cancer care, and porting of highly \naccurate and effective strategies for cancer diagnosis and \nclassification to mobile devices will open new dimensions \nin future digital healthcare by facilitating ease of use \nand accessibility. The included studies were inconsistent \nin reporting the training time required for the model. \nFor example, the reviewers could not find this informa -\ntion in most of the studies or did not compare how the \nmodels would behave on different hardware and whether \nthe training/inference time would see a major reduc -\ntion. It is expected that providing web-based demos for \nthe proposed models, in general, will increase the inter -\nest of doctors, physicians, and students in exploring the \npotential of vision transformers for lung cancer appli -\ncations. However, this review did not find web-based \nplatforms that used vision transformers for lung cancer \napplications.\nStrengths and limitations\nStrengths\nWith the recent popularity of vision transformer-based \nAI methods in medical imaging, there has been a grow -\ning interest in reviews on the topic [ 18, 58, 59, 22, 23]. \nHowever, we did not find any previous review on \nvision transformers for lung cancer imaging. This is the \nfirst review covering the classification, diagnosis, and \nPage 10 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nprognosis applications of vision transformers for lung \ncancer imaging.\nIn this review, we have summarized the key vision \ntransformer-based methods for lung cancer applications \nthat will help the readers and the researchers to iden -\ntify the potential opportunities and related challenges in \ndeveloping advanced methods for lung cancer analysis. \nIn the review, we followed the guidelines of the PRISMA-\nScR [ 60]. We included the most relevant studies from \npopular scientific databases that cover technical and \nhealthcare literature. We overcame bias in study selec -\ntion by adapting an independent selection mechanism \nof studies by two reviewers that a third reviewer vali -\ndated. We identified the key areas and gaps in the vision \ntransformer-based methods to which researchers may \ncontribute. To the best of our knowledge, this is the first \ncomprehensive review that explores the role of vision \ntransformers in improving lung cancer classification and \nprognosis. Furthermore, it covers the most recent stud -\nies reported by the researchers. Hence, this review will be \nbeneficial for researchers and practitioners interested in \nthe transformation of digital healthcare, in general, and \nlung cancer, in particular.\nLimitations\nSince this review covered imaging-based applications \nonly, clinical factors and living habits of lung cancer \npatients were not covered in the included studies, which \nwould otherwise provide key information in the course \nof the disease. We did not evaluate the code as this was \nbeyond the scope of this review. Since the included stud -\nies varied in terms of the datasets, or the number of sam -\nples/patients used, it was impossible to establish a direct \ncomparison of their performances on the classification \nor prognosis of lung cancer. This review does not pro -\nvide a discussion on the training delays due to two rea -\nsons. Firstly, such information was not provided in the \nincluded studies. Secondly, different research groups may \nvary greatly in their access to computational resources \nand GPUs. We understand that the interest in using and \ndeveloping newer architectures of vision transformer-\nbased AI methods for lung cancer imaging is growing \nrapidly. Hence, we cannot rule out the possibility that \nseveral other studies may come out while this work is \nbeing drafted, despite our best efforts to include the most \nrecent studies until December 2022. This review covers \nstudies published in English, so, relevant studies in other \nlanguages (if any) are not included.\nMethods\nIn the review, we followed the PRISMA-ScR (Preferred \nReporting Items for Systematic Reviews and Meta-Analy-\nses) [60] guidelines to perform the study search and syn -\nthesis of the data.\nSearch strategy\nAn extensive search of scientific databases, includ -\ning PubMed, Scopus, IEEE Xplore, Google Scholar, and \nMEDLINE (via PubMed), was conducted to identify \nthe relevant studies. The study search was performed \non December 21, 2022. Reference list checking was also \nperformed for additional relevant studies. Only the first \n150 relevant studies from Google Scholar were consid -\nered for the review, as search results beyond this number \nrapidly lost relevance and were not pertinent to the scop-\ning review topic. The search terms were defined through \nconsultation with domain experts and on the basis of \nthe previous literature. The search terms included terms \nbased on the target anatomy (e.g., lung cancer) and the \nintervention (e.g., transformers). The detailed search \nstrings used in the study can be found in Appendix 2.\nSearch eligibility criteria\nIn this scoping review, we focused on exploring the recent \nadvancements and applications of vision transformers in \nlung cancer medical imaging. We analyzed studies pub -\nlished until December 2022 in English that involved the \nutilization of vision transformers for various purposes \nrelated to lung cancer imaging, such as classification \nof lung cancer types, prediction of the cancer growth, \ndetection of nodule, survival prediction of lung cancer \npatients, and segmentation of lungs. Studies that used \nany medical imaging modality such as MRI, CT, X-ray, \nand histopathology images were considered. Only origi -\nnal research published in peer-reviewed journals, confer -\nence proceedings, or book chapters was considered.\nStudies that did not use vision transformers specifically \nbut utilized other deep learning methods, such as CNNs \nand Generative Adversarial Networks (GANs), were \nexcluded from the review. Additionally, studies that used \ntransformers for non-imaging data, such as text data and \nelectronic health records (EHRs), were excluded. More -\nover, studies that used transformers for cancers other \nthan lung cancer were also excluded. Studies identified \nas non-English text, review articles, preprints, editori -\nals, proposals, conference abstracts, commentaries, and \neditor letters were also excluded. No restrictions were in \nplace on the country of publications, the models’ com -\nplexity, the reported methods’ performance, and the \nmodality of imaging data.\nStudy selection and data extraction\nWe used the Rayyan web-based review management \ntool [61] to conduct the initial screening and study selec -\ntion process. One reviewer (H.A.) performed the litera -\nture search. After eliminating duplicates, two reviewers \n(F.M.) and (H.A.) independently screened the titles \nand abstracts of the studies to identify eligible studies. \nThe studies that successfully passed the initial title and \nPage 11 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nabstract screening were selected for the full-text screen -\ning phase. Any disagreements during the process were \nresolved through discussion and through validation by \na third reviewer (Z.S.). An evidence form was created \nand tested on three studies to establish a systematic and \nprecise data extraction process (also see Appendix 3). \nData extracted from the studies included the titles, first \nauthor’s name, publication date and venue, the coun -\ntry of the first author’s institution, the study application, \nthe imaging type, the transformer type, the data source \n(public or private), data size, the validation methods, and \nthe evaluation metrics. Additionally, information regard -\ning the required hardware resources was also extracted. \nMoreover, the studies’ challenges and suggested solutions \nwere extracted, along with the challenges encountered \nand proposed solutions in the studies. Two reviewers \n(F.M. and H.Z.) conducted the data extraction, and any \ndiscrepancies were resolved through discussions and \nmutual consensus.\nData synthesis\nWe followed a narrative approach to synthesize the data \nextracted from the included studies. We categorized the \ndata in terms of the specific tasks addressed in them, \nsuch as classification of lung cancer type, prediction of \nthe course of cancer, survival prediction of the cancer \npatients, and segmentation of lungs. Based on the models \ndeveloped in the included studies, we categorized them \ninto those using 3D models and those using 2D models. \nWe also cataloged the studies based on the use of pub -\nlic versus privately developed datasets, the method of \nvalidation of the results, and the reproducibility of the \nresults.\nConclusion\nIn this work, we undertook a scoping review of 34 stud -\nies investigating the development and implementa -\ntion of AI methods in lung cancer imaging, specifically \nusing vision transformer models. Our review work indi -\ncates that vision transformer-based methods have been \ndeveloped for the classification of lung cancer types and \nsurvival prediction of lung patients. Most reported meth -\nods have achieved performance propelling forward the \nfield of AI for lung cancer imaging. The included stud -\nies evaluated the performance in terms of accuracy, the \narea under the ROC curve, and the concordance index. \nAdditionally, we cataloged publicly available datasets for \nlung cancer imaging. Despite these advancements, we \nalso identified areas for improvement, such as reduc -\ning model complexity, bridging the gap between clinical \npractice and vision transformer-based AI methods, and \nincreasing geographical diversity in published studies. \nMoreover, there is an urgent need to develop explainable \nvision transformer models for lung cancer imaging, as \nthis will enhance the trust and acceptance of these meth -\nods among all stakeholders. We anticipate that our find -\nings will provide a valuable reference text for researchers \nand students in the interdisciplinary fields of medical \nAI and cancer imaging. Vision transformers struggle to \ngeneralize well when data is limited. To improve vision \ntransformers’ generalization for lung cancer imaging, we \nadvocate for the acquisition of larger and more diverse \ndatasets of lung images with different modalities. While \ncombining vision transformers with CNNs is common, a \nsimple cascade arrangement might not effectively capture \nlocal and global features crucial for lung nodule detection \nand classification. Moreover, the parallel use of vision \ntransformers and CNNs necessitates carefully filtering \nredundant information to overcome computational over-\nhead. Future research must focus on developing pipe -\nlines that optimize the complementary performance of \nthese architectures. Likewise, multimodal AI techniques \nhave been proven effective in healthcare data; thus, \nvision transformers-based pipelines should be explored \nfor processing lung cancer imaging data of multiple \nmodalities, such as CT and PET. Lung nodules and large \ntumors are usually available in only few of the samples. \nSo, data imbalance remains a challenge as most publicly \navailable datasets come with a smaller number of large \ntumors. Thus, multi-institutional and multi-center col -\nlaborative efforts are needed to ensure large and diverse \ndata of lung cancer that can help in better model gener -\nalization. Furthermore, research efforts should prioritize \naddressing the explainability and interpretability of vision \ntransformers’ performance in identifying tumor-related \nimaging components or discerning significant features \ninfluencing the model’s prognosis behavior. Consider -\ning the resource-intensive nature of transformer archi -\ntectures, we urge the development of resource-efficient \nimplementation methods for vision transformers-based \napproaches. By doing so, we can advance toward clinical \ntranslation and real-time integration of vision transform -\ners-based methods in lung cancer care.\nAbbreviations\nAI  Artificial Intelligence\nAUC  Area under ROC curve\nCNN  Convolutional Neural Networks\nCT  Computed Tomography\nDSC  Dice similarity coefficient\nGPU  Graphics Processing Unit\nLIDC  Lung Imaging Database Consortium\nMAE  Mean absolute error\nPET  Positron Emission Tomography\nRMSE  Root mean square error\nSOTA  State-of-the-art\nTCGA  The Cancer Genome Atlas\nWSI  Whole slide imaging\nPage 12 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nSupplementary Information\nThe online version contains supplementary material available at https://doi.\norg/10.1186/s12880-023-01098-z.\nAppendix 1: List of included studies\nAppendix 2: Search strings\nAppendix 3: Data extraction form\nAcknowledgements\nNot applicable.\nAuthors’ contributions\nH. A. contributed to the conception, design, literature search, data selection, \ndata synthesis, data extraction, and drafting. F. M. contributed to the data \nsynthesis, data extraction, and drafting. Z. S. contributed to the drafting and \ncritical revision of the manuscript. All authors gave their final approval and \naccepted accountability for all aspects of the work.\nFunding\nOpen Access funding provided by Qatar National Library.\nData Availability\nAll data generated or analysed during this study are included in this published \narticle and its supplementary information files.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1College of Science and Engineering, Hamad Bin Khalifa University, Qatar \nFoundation, Doha, Qatar\nReceived: 29 March 2023 / Accepted: 5 September 2023\nReferences\n1. Cruz CSD, Tanoue LT, Matthay RA. Lung cancer: epidemiology, etiology, and \nprevention. Clin Chest Med. 2011;32(4):605–44.\n2. de Groot PM, Wu CC, Carter BW, Munden RF. The epidemiology of lung \ncancer. Transl Lung Cancer Res. 2018;7(3):220.\n3. Lewis SJ, Gandomkar Z, Brennan PC. Artificial Intelligence in medical imaging \npractice: looking to the future. J Med Radiat Sci. 2019;66(4):292–5.\n4. Rajpurkar P , Chen E, Banerjee O, Topol EJ. AI in health and medicine. Nat Med. \n2022;28(1):31–8.\n5. Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep \nconvolutional neural networks. Commun ACM. 2017;60(6):84–90.\n6. Goodfellow I, Bengio Y, Courville A. Deep learning. MIT press; 2016.\n7. Lundervold AS, Lundervold A. An overview of deep learning in medical imag-\ning focusing on MRI. Z Für Med Phys. 2019;29(2):102–27.\n8. Ali H, et al. The role of generative adversarial networks in brain MRI: a scoping \nreview. Insights Imaging. 2022;13(1):1–15.\n9. Würfl T, Ghesu FC, Christlein V, Maier A. “Deep learning computed tomogra-\nphy,” in Medical Image Computing and Computer-Assisted Intervention-MICCAI \n2016: 19th International Conference, Athens, Greece, October 17–21, 2016, \nProceedings, Part III 19, Springer, 2016, pp. 432–440.\n10. Min JK, Kwak MS, Cha JM. Overview of deep learning in gastrointestinal \nendoscopy. Gut Liver. 2019;13(4):388.\n11. Lakhani P , Sundaram B. Deep learning at chest radiography: automated clas-\nsification of pulmonary tuberculosis by using convolutional neural networks. \nRadiology. 2017;284(2):574–82.\n12. Iqbal T, Ali H. Generative adversarial network for medical images (MI-GAN). J \nMed Syst. 2018;42:1–11.\n13. Dosovitskiy A et al. “An image is worth 16x16 words: Transformers for image \nrecognition at scale,” ArXiv Prepr. ArXiv201011929, 2020.\n14. Zheng S et al. “Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers,” in IEEE/CVF conference on computer \nvision and pattern recognition, 2021, pp. 6881–6890.\n15. Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. “End-to-\nend object detection with transformers,” in Computer Vision–ECCV 2020: 16th \nEuropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, \nSpringer, 2020, pp. 213–229.\n16. Gao X et al. “COVID-VIT: Classification of Covid-19 from 3D CT chest images \nbased on vision transformer model,” in 2022 3rd International Conference on \nNext Generation Computing Applications (NextComp), IEEE, 2022, pp. 1–4.\n17. Watanabe S, Ueno T, Kimura Y, Mishina M, Sugimoto N. Generative image \ntransformer (GIT): unsupervised continuous image generative and transform-\nable model for [123 I] FP-CIT SPECT images. Ann Nucl Med. 2021;35:1203–13.\n18. He K, et al. Transformers in medical image analysis: a review. Intell Med. \n2022;3(1):59–78. https://doi.org/10.1016/j.imed.2022.07.002.\n19. Chassagnon G et al. Artificial intelligence in lung cancer: current applications \nand perspectives, Jpn J Radiol, pp. 1–10, 2022.\n20. Kaur C, Garg U. “Artificial intelligence techniques for cancer detection in \nmedical image processing: A review,” Mater. Today Proc, 2021.\n21. Wang S, et al. Artificial intelligence in lung cancer pathology image analysis. \nCancers. 2019;11(11):1673.\n22. Xia K, Wang J. Recent advances of Transformers in medical image analysis: a \ncomprehensive review. MedComm–Future Med. 2023;2(1):e38. https://doi.\norg/10.1002/mef2.38.\n23. Li Y, Wu X, Yang P , Jiang G, Luo Y. Machine learning for Lung Cancer diagnosis, \ntreatment, and prognosis. Genomics Proteom Bioinf. 2022;20(5):850–66.\n24. Chen Y, Feng J, Liu J, Pang B, Cao D, Li C. Detection and classification of Lung \nCancer cells using swin transformer. J Cancer Ther. 2022;13(7):464–75.\n25. Aitazaz T, Tubaishat A, Al-Obeidat F, Shah B, Zia T, Tariq A. Transfer learning \nfor histopathology images: an empirical study. Neural Comput Appl. 2022. \nhttps://doi.org/10.1007/s00521-022-07516-7.\n26. Zheng Y, et al. A graph-transformer for whole slide image classification. \nIEEE Trans Med Imaging. 2022;41(11):3003–15. https://doi.org/10.1109/\nTMI.2022.3176598.\n27. Shao Z, Bian H, Chen Y, Wang Y, Zhang J, Ji X. “Transmil: Transformer based \ncorrelated multiple instance learning for whole slide image classification,” in \nAdvances in neural information processing systems, 2021, pp. 2136–2147.\n28. Wang X, et al. Transformer-based unsupervised contrastive learning for \nhistopathological image classification. Med Image Anal. 2022;81. https://doi.\norg/10.1016/j.media.2022.102559.\n29. Heidarian S. “Capsule Network-based COVID-19 Diagnosis and Transformer-\nbased Lung Cancer Invasiveness Prediction via Computerized Tomography \n(CT) Images,” Doctoral thesis, Concordia University, 2022.\n30. Liu D, Liu F, Tie Y, Qi L, Wang F. Res-trans networks for lung nodule clas-\nsification. Int J Comput Assist Radiol Surg. 2022;17(6):1059–68. https://doi.\norg/10.1007/s11548-022-02576-5.\n31. Wang R, Zhang Y, Yang J. TransPND: A Transformer Based Pulmonary \nNodule Diagnosis Method on CT Image, vol. 13535. in Lecture Notes in \nComputer Science (including subseries Lecture Notes in Artificial Intel-\nligence and Lecture Notes in Bioinformatics), vol. 13535. 2022. https://doi.\norg/10.1007/978-3-031-18910-4_29.\n32. Wu P , Chen J, Wu Y. “Swin Transformer based benign and malignant pulmo-\nnary nodule classification,” in Proceedings of SPIE - The International Society for \nOptical Engineering, 2022. https://doi.org/10.1117/12.2656809.\n33. Xiong Y, Du B, Xu Y, Deng J, She Y, Chen C. “Pulmonary Nodule Classification \nwith Multi-View Convolutional Vision Transformer,” in 2022 International Joint \nConference on Neural Networks (IJCNN), 2022, pp. 1–7. https://doi.org/10.1109/\nIJCNN55064.2022.9892716.\n34. Yang J, Deng H, Huang X, Ni B, Xu Y. “Relational Learning Between Multiple \nPulmonary Nodules via Deep Set Attention Transformers,” in 2020 IEEE 17th \nInternational Symposium on Biomedical Imaging (ISBI), 2020, pp. 1875–1878. \nhttps://doi.org/10.1109/ISBI45749.2020.9098722.\n35. Chen RJ et al. “Scaling Vision Transformers to Gigapixel Images via Hierarchical \nSelf-Supervised Learning,” in 2022 IEEE/CVF Conference on Computer Vision and \nPage 13 of 13\nAli et al. BMC Medical Imaging           (2023) 23:129 \nPattern Recognition (CVPR), New Orleans, LA, USA: IEEE, 2022, pp. 16144–\n16155. https://doi.org/10.1109/CVPR52688.2022.01567.\n36. Dhamija T, Gupta A, Gupta S, Anjum R, Katarya, Singh G. Semantic segmenta-\ntion in medical images through transfused convolution and transformer \nnetworks, Appl Intell, 2022.\n37. Alahmadi MD. Medical image segmentation with learning semantic and \nglobal contextual representation. Diagnostics. 2022;12(7). https://doi.\norg/10.3390/diagnostics12071548.\n38. Guo D, Terzopoulos D. “A Transformer-Based Network for Anisotropic 3D \nMedical Image Segmentation,” in 2020 25th International Conference on \nPattern Recognition (ICPR), 2021, pp. 8857–8861. https://doi.org/10.1109/\nICPR48806.2021.9411990.\n39. Jiang J, Tyagi N, Tringale K, Crane C, Veeraraghavan H. Self-supervised 3D \nAnatomy Segmentation Using Self-distilled Masked Image Transformer (SMIT), \nvol. 13434. in Lecture Notes in Computer Science (including subseries \nLecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), \nvol. 13434. 2022. https://doi.org/10.1007/978-3-031-16440-8_53.\n40. Wang S, Jiang A, Li X, Qiu Y, Li M, Li F. DPBET: a dual-path lung nodules \nsegmentation model based on boundary enhancement and hybrid \ntransformer. Comput Biol Med. 2022;151:106330. https://doi.org/10.1016/j.\ncompbiomed.2022.106330.\n41. Xie H, Chen Z, Deng J, Zhang J, Duan H, Li Q. “Automatic segmentation of the \ngross target volume in radiotherapy for lung cancer using transresSEUnet 2.5 \nD Network,” J. Transl. Med, vol. 20, no. 1, pp. 1–12, Nov. 2022.\n42. Yang D, Myronenko A, Wang X, Xu Z, Roth HR, Xu D. “T-AutoML: Automated \nmachine learning for lesion segmentation using transformers in 3d medical \nimaging,” presented at the Proceedings of the IEEE/CVF international confer-\nence on computer vision, 2021, pp. 3962–3974.\n43. Dao D-P et al. “Survival Analysis based on Lung Tumor Segmentation using \nGlobal Context-aware Transformer in Multimodality,” in 2022 26th Interna-\ntional Conference on Pattern Recognition (ICPR), 2022, pp. 5162–5169. https://\ndoi.org/10.1109/ICPR56361.2022.9956406.\n44. Niu C, Wang G. Unsupervised contrastive learning based transformer \nfor lung nodule detection. Phys Med Biol. 2022;67(20). https://doi.\norg/10.1088/1361-6560/ac92ba.\n45. Zhao W, et al. GMILT: a Novel Transformer Network that can noninvasively \npredict EGFR Mutation Status. IEEE Trans Neural Netw Learn Syst. 2022;1–15. \nhttps://doi.org/10.1109/TNNLS.2022.3190671.\n46. Shao J, et al. Radiogenomic System for non-invasive identification of \nmultiple actionable mutations and PD-L1 expression in Non-Small Cell Lung \nCancer based on CT images. Cancers. 2022;14(19). https://doi.org/10.3390/\ncancers14194823.\n47. Wang H, et al. Static–dynamic coordinated transformer for Tumor Longitudi-\nnal Growth Prediction. Comput Biol Med. 2022;148. https://doi.org/10.1016/j.\ncompbiomed.2022.105922.\n48. Lian J, et al. Early stage NSCLS patients’ prognostic prediction with multi-\ninformation using transformer and graph neural network model. eLife. \n2022;11. https://doi.org/10.7554/eLife.80547.\n49. Ma X, Xia L, Chen J, Wan W, Zhou W. Development and validation of a deep \nlearning signature for predicting lymph node metastasis in lung adenocarci-\nnoma: comparison with radiomics signature and clinical-semantic model. Eur \nRadiol. 2022. https://doi.org/10.1007/s00330-022-09153-z.\n50. Song P et al. “MSTS-Net: malignancy evolution prediction of pulmonary \nnodules from longitudinal CT images via multi-task spatial-temporal self-\nattention network,” Int. J. Comput. Assist. Radiol. Surg, pp. 1–9, Nov. 2022.\n51. Fang J et al. “Siamese Encoder-based Spatial-Temporal Mixer for Growth \nTrend Prediction of Lung Nodules on CT Scans,” in International Conference \non Medical Image Computing and Computer-Assisted Intervention, Singapore: \nSpringer, Sep. 2022, pp. 484–494.\n52. Wang H, Zhu H, Ding L. “Accurate Classification of Lung Nodules on CT Image \nBased on TransUnet,” Front. Public Health, p. 4664, Dec. 2022.\n53. Chen RJ et al. “Multimodal Co-Attention Transformer for Survival Prediction \nin Gigapixel Whole Slide Images,” in Proceedings of the IEEE/CVF International \nConference on Computer Vision, 2021, pp. 3995–4005.\n54. Huang Z, Chai H, Wang R, Wang H, Yang Y, Wu H. “Integration of patch fea-\ntures through self-supervised learning and transformer for survival analysis \non whole slide images,” in International Conference on Medical Image Comput-\ning and Computer-Assisted Intervention, Strasbourg, France: Springer, 2021, \npp. 561–570.\n55. Wang R, Huang Z, Wang H, Wu H, Biomedicine. (BIBM), IEEE, 2021, \npp. 757–760.\n56. Li C, Zhu X, Yao J, Huang J. “Hierarchical Transformer for Survival Prediction \nUsing Multimodality Whole Slide Images and Genomics,” in 26th International \nConference on Pattern Recognition (ICPR), Montreal, QC, Canada: IEEE, 2022, \npp. 4256–4262.\n57. Shen Y et al. “Explainable Survival Analysis with Convolution-Involved Vision \nTransformer,” in Proceedings of the AAAI Conference on Artificial Intelligence, \n2022, pp. 2207–2215.\n58. Shamshad F et al. Transformers in medical imaging: a survey, ArXiv Prepr \nArXiv220109873, 2022.\n59. Akinyelu AA, Zaccagna F, Grist JT, Castelli M, Rundo L. Brain tumor diagnosis \nusing machine learning, convolutional neural networks, Capsule neural \nnetworks and Vision Transformers, Applied to MRI: a Survey. J Imaging. \n2022;8(8):205.\n60. Tricco AC, et al. PRISMA extension for scoping reviews (PRISMA-ScR): checklist \nand explanation. Ann Intern Med. 2018;169(7):467–73.\n61. Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan—a web and \nmobile app for systematic reviews. Syst Rev. 2016;5:1–10.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations. ",
  "topic": "Lung cancer",
  "concepts": [
    {
      "name": "Lung cancer",
      "score": 0.7156898975372314
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5496786236763
    },
    {
      "name": "Medicine",
      "score": 0.5115407705307007
    },
    {
      "name": "Convolutional neural network",
      "score": 0.497113972902298
    },
    {
      "name": "Computer science",
      "score": 0.49413779377937317
    },
    {
      "name": "Inclusion and exclusion criteria",
      "score": 0.49113819003105164
    },
    {
      "name": "Machine learning",
      "score": 0.452680766582489
    },
    {
      "name": "Adenocarcinoma",
      "score": 0.4492826759815216
    },
    {
      "name": "Medical physics",
      "score": 0.3890477418899536
    },
    {
      "name": "Pathology",
      "score": 0.23916468024253845
    },
    {
      "name": "Cancer",
      "score": 0.22612443566322327
    },
    {
      "name": "Internal medicine",
      "score": 0.1815587878227234
    },
    {
      "name": "Alternative medicine",
      "score": 0.09764793515205383
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210144839",
      "name": "Hamad bin Khalifa University",
      "country": "QA"
    },
    {
      "id": "https://openalex.org/I92528248",
      "name": "Qatar Foundation",
      "country": "QA"
    }
  ],
  "cited_by": 41
}