{
    "title": "Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions",
    "url": "https://openalex.org/W3046715528",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Huang, Yu-Siang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222713164",
            "name": "Yang, Yi-Hsuan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2405656250",
        "https://openalex.org/W2963408210",
        "https://openalex.org/W3034758887",
        "https://openalex.org/W2144001972",
        "https://openalex.org/W2963032576",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2076769328",
        "https://openalex.org/W2962699318",
        "https://openalex.org/W2963575853",
        "https://openalex.org/W2809621972",
        "https://openalex.org/W625810977",
        "https://openalex.org/W2963557407",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2113409081",
        "https://openalex.org/W2566969886",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2906214917",
        "https://openalex.org/W2112349754",
        "https://openalex.org/W2971351900",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W4928837",
        "https://openalex.org/W2771265026",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2087870700",
        "https://openalex.org/W2051219610",
        "https://openalex.org/W2962046549",
        "https://openalex.org/W2990559773",
        "https://openalex.org/W2920538220",
        "https://openalex.org/W2123282787",
        "https://openalex.org/W2902458379",
        "https://openalex.org/W2572771584",
        "https://openalex.org/W3154236293",
        "https://openalex.org/W2766802526",
        "https://openalex.org/W2991108091",
        "https://openalex.org/W2111007352",
        "https://openalex.org/W2963681776",
        "https://openalex.org/W2963551352",
        "https://openalex.org/W2919624000",
        "https://openalex.org/W2527926565"
    ],
    "abstract": "A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.",
    "full_text": "Pop Music Transformer: Beat-based Modeling and Generation of\nExpressive Pop Piano Compositions\nYu-Siang Huang\nTaiwan AI Labs & Academia Sinica\nTaipei, Taiwan\nyshuang@ailabs.tw\nYi-Hsuan Yang\nTaiwan AI Labs & Academia Sinica\nTaipei, Taiwan\nyhyang@ailabs.tw\nABSTRACT\nA great number of deep learning based models have been recently\nproposed for automatic music composition. Among these models,\nthe Transformer stands out as a prominent approach for generating\nexpressive classical piano performance with a coherent structure\nof up to one minute. The model is powerful in that it learns abstrac-\ntions of data on its own, without much human-imposed domain\nknowledge or constraints. In contrast with this general approach,\nthis paper shows that Transformers can do even better for music\nmodeling, when we improve the way a musical score is converted\ninto the data fed to a Transformer model. In particular, we seek to\nimpose a metrical structure in the input data, so that Transform-\ners can be more easily aware of the beat-bar-phrase hierarchical\nstructure in music. The new data representation maintains the flex-\nibility of local tempo changes, and provides hurdles to control the\nrhythmic and harmonic structure of music. With this approach, we\nbuild a Pop Music Transformer that composes Pop piano music\nwith better rhythmic structure than existing Transformer models.\nCCS CONCEPTS\n• Applied computing →Sound and music computing .\nKEYWORDS\nAutomatic music composition, transformer, neural sequence model\nACM Reference Format:\nYu-Siang Huang and Yi-Hsuan Yang. 2020. Pop Music Transformer: Beat-\nbased Modeling and Generation of Expressive Pop Piano Compositions.\nIn 28th ACM International Conference on Multimedia (MM ’20), October\n12–16, 2020, Seattle, WA, USA.. ACM, New York, NY, USA, 9 pages. https:\n//doi.org/10.1145/3394171.3413671\n1 INTRODUCTION\nMusic is sound that’s organized on purpose on many time and\nfrequency levels to express different ideas and emotions. For ex-\nample, the organization of musical notes of different fundamental\nfrequencies (from low to high) influences the melody, harmony\nand texture of music. The placement of strong and weak beats over\ntime, on the other hand, gives rise to the perception of rhythm [28].\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM ’20, October 12–16, 2020, Seattle, WA, USA.\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7988-5/20/10. . . $15.00\nhttps://doi.org/10.1145/3394171.3413671\nMIDI-like [30] REMI (this paper)\nNote onset Note-On\n(0–127)\nNote-On\n(0–127)\nNote offset Note-Off\n(0–127)\nNote Duration\n(32th note multiples; 1–64)\nTime grid Time-Shift\n(10–1000ms)\nPosition (16 bins; 1–16)\n& Bar (1)\nTempo changes ✗ Tempo\n(30–209 BPM)\nChord ✗ Chord\n(60 types)\nTable 1: The commonly-used MIDI-like event representa-\ntion [23, 30] versus the proposed beat-based one, REMI. In\nthe brackets, we show the ranges of the type of event.\nRepetition and long-term structure are also important factors that\nmake a musical piece coherent and understandable.\nBuilding machines that can compose music like human beings\nis one of the most exciting tasks in multimedia and artificial in-\ntelligence [5, 7, 13, 18, 26, 27, 31, 44, 45]. Among the approaches\nthat have been studied, neural sequence models, which consider\nmusic as a language, stand out in recent work [9, 23, 30, 32] as a\nprominent approach with great potential. 1 In doing so, a digital\nrepresentation of a musical score is converted into a time-ordered\nsequence of discrete tokens such as the Note-On events. Sequence\nmodels such as the Transformer [39] can then be applied to model\nthe probability distribution of the event sequences, and to sample\nfrom the distribution to generate new music compositions. This\napproach has been shown to work well for composing minute-long\npieces of classical piano music with expressive variations in note\ndensity and velocity [23], in the format of a MIDI file.2\nWe note that there are two critical elements in the aforemen-\ntioned approach—the way music is converted into discrete tokens\nfor language modeling, and the machine learning algorithm used to\nbuild the model. Recent years have witnessed great progress regard-\ning the second element, for example, by improving the self-attention\nin Transformers with “sparse attention” [8], or introducing a recur-\nrence mechanism as in Transformer-XL for learning longer-range\n1However, whether music and language are related is actually debatable from a musi-\ncology point of view. The computational approach of modeling music in the same way\nas modeling natural language therefore may have limitations.\n2Therefore, the model generates the pitch, velocity (dynamics), onset and offset time\nof each note, including those involved in the melody line, the underlying chord pro-\ngression, and the bass line, etc in an expressive piano performance.\narXiv:2002.00212v3  [cs.SD]  10 Aug 2020\n(a) Transformer-XL ×MIDI-like (‘Baseline 1’)\n(b) Transformer-XL ×REMI (with Tempo and Chord)\nFigure 1: Examples of piano rolls and ‘downbeat probability\ncurves’ (cf. Section 5.3) for music generated by an adapta-\ntion of the state-of-the-art model Music Transformer [23],\nand the proposed model. We can see clearer presence of\nregularly-spaced downbeats in the probability curve of (b).\ndependency [11, 12]. However, no much has been done for the first\none. Most work simply follows the MIDI-like event representation\nproposed by [30] to set up the “vocabulary” for music.\nAs shown in Table 1, the MIDI-like representation, for the case\nof modeling classical piano music [9, 23], uses Note-On events to\nindicate the action of hitting a specific key of the piano keyboard,\nand Note-Off for the release of the key. This representation has its\nroots in MIDI [22], a communication protocol that also uses Note-\nOn and Note-Off messages to transmit data of real-time musical\nperformance. Unlike words in human language, note messages in\nMIDI are associated with time. To convert a score into a sequence,\n[30] uses additionally the Time-Shift (∆T ) events to indicate the\nrelative time gap between events (rather than the absolute time of\nthe events), thereby representing the advance in time. This MIDI-\nlike representation is general and represents keyboard-style music\n(such as piano music) fairly faithfully, therefore a good choice as the\nformat for the input data fed to Transformers. A general idea here\nis that a deep network can learn abstractions of the MIDI data that\ncontribute to the generative musical modeling on its own. Hence,\nhigh-level (semantic) information of music [4], such as downbeat,\ntempo and chord, are not included in this MIDI-like representation.\nHowever, we note that when humans compose music, we tend\nto organize regularly recurring patterns and accents over ametrical\nstructure defined in terms of sub-beats, beats, and bars [10]. Such a\nstructure is made clear on a score sheet or a MIDI file with notation\nof the time signature and bar lines, but is implicit in the MIDI-like\nevent representation. A sequence model has to recover the metrical\nstructure on its own from the provided sequence of tokens. When it\ncomes to modeling music genres that feature the use of steady beats,\nwe observe that the Transformer [23] has a hard time learning the\nregularity of beats over time, as shown in Figure 1a.\nMusic Transcription\nAudio MIDI\nBeat Tracking\nChord Recognition G:maj,\nA:min,\n…\nChord\nBar, Position, Chord,\nNote Velocity, Note On,\nNote Duration, …\nEvent\nSequential Modeling\nBar, Position, Chord,\nNote Velocity, Note On,\nNote Duration, …\nEvent\n MIDI\nTraining Stage\nInference Stage\nFigure 2: The diagram of the proposed beat-based music\nmodeling and generation framework. The training stage\nentails the use of music information retrieval (MIR) tech-\nniques, such as automatic transcription, to convert an au-\ndio signal into an event sequence, which is then fed to a\nsequence model for training. At inference time, the model\ngenerates an original event sequence, which can then be con-\nverted into a MIDI file for playback.\nTo remedy this, and to study whether Transformer-based compo-\nsition models can benefit from the addition of some human knowl-\nedge of music, we propose REMI, which stands for revamped MIDI-\nderived events, to represent MIDI data following the way humans\nread them. Specifically, we introduce the Bar event to indicate the\nbeginning of a bar, and thePosition events to point to certain loca-\ntions within a bar. For example, Position (9/16)indicates that we\nare pointing to the middle of a bar, which is quantized to 16 regions\nin this implementation (see Section 3 for details). The combination\nof Position and Bar therefore provides an explicit metrical grid to\nmodel music, in a beat-based manner. This new data representation\ninforms models the presence of a beat-bar hierarchical structure\nin music and empirically leads to better rhythmic regularity in the\ncase of modeling Pop music, as shown in Figure 1b.\nWe note that, while the incorporation of the bars is not new in the\nliterature of automatic music composition [6, 20, 38], it represents\na brand new attempt in the context of Transformer-based music\nmodeling. This facilitates extensions of the Transformer, such as\nthe Compressive Transformer [33], Insertion Transformer [37], and\nTree Transformer [40], to be studied for this task in the future, since\nthe models have now clearer ideas of segment boundaries in music.\nMoreover, we further explore adding other supportive musical\ntokens capturing higher-level information of music. Specifically, to\nmodel the expressive rhythmic freedom in music (e.g., tempo ru-\nbato), we add a set ofTempo events to allow for local tempo changes\nper beat. To have control over the chord progression underlying the\nmusic being composed, we introduce the Chord events to make\nthe harmonic structure of music explicit, in the same vein of adding\nthe Position and Bar events for rhythmic structure.\nIn achieving all these, the proposed framework for automatic\nmusic composition has to integrate audio- and symbolic-domain\nmusic information retrieval (MIR) techniques such as downbeat\ntracking [2, 3, 16] and chord recognition [17, 35], which differenti-\nates it from existing approaches. Moreover, we use automatic music\ntranscription [1, 21, 25] to prepare the MIDI data, so the model\nlearns from expressive piano performances . We show in Figure\n2 an illustration of the overall framework. Table 1 compares REMI\nBar, Position (1/16), Chord (C major),  Position (1/16), Tempo Class (mid), Tempo Value (10), Position (1/16), Note Velocity (16), Note On (60), Note Duration (4), Position (5/16), ⋯⋯Tempo Value (12), Position (9/16), Note Velocity (14), Note On (67), Note Duration (8), Bar\nFigure 3: An example of a REMI event sequence and the\ncorresponding music fragment in staff notation. The Bar,\nPosition and Tempo-related events entail the use of audio-\ndomain downbeat and beat tracking algorithms [3] (see\nSection 3.3), and the Chord events the use of a symbolic-\ndomain chord recognition algorithm (see Section 3.5).\nwith the commonly-adopted MIDI-like representation, and Figure\n3 gives an example of a REMI event sequence.\nIn our experiment (see Section 5), we use Transformer-XL [11]\nto learn to compose Pop piano music using REMI as the underlying\ndata representation. We conduct both objective and subjective evalu-\nation comparing the proposed model against the Music Transformer\n[23], showing that we are able to improve the state-of-the-art with\nsimple yet major modifications in the data presentation, rather than\nwith sophisticated re-design of the neural network architecture.\nFor reproducibility, the code, data and pre-trained model are\nmade publicly available.3\n2 RELATED WORK\nRecent neural network-based approaches for automatic music com-\nposition can be broadly categorized into two groups. Image-based\napproaches such as MidiNet [41] and MuseGAN [13] use an image-\nlike representation such as the piano roll to represent a score as a\nmatrix of time steps and MIDI pitches, and then use convolution-\nbased operations to generate music. It is easier for such approaches\nto learn the rhythmic structure of music, as the time steps cor-\nresponding to the beats and bar lines are clear in such matrices.\nLanguage-based approaches such as the Music Transformer [23],\non the other hand, may learn the temporal dependency between\nmusical events such asNote-Ons better. Yet, due to the limits of the\nMIDI-like representation outlined in Section 1, existing work may\nfall short in the rhythmic aspect. The proposed approach combines\nthe advantage of the image- and language-based approaches by\nembedding a metrical grid in the event representation.\nThe idea of designing events for music metadata is similar to\nCTRL [24], which provided more explicit controls for text genera-\ntion. However, recent work in neural sequence modeling of music\nmostly adopt the same MIDI-like event representation proposed\nby [30], or its extensions. For example, extra events denoting the\ncomposer, instrumentation and global tempo are used in MuseNet\n[32] for conditioned generation, and events specifying the Note-\nOn and Note-Off of different instruments are used to achieve\nmulti-instrument music composition [ 12]. However, to our best\nknowledge, the use of Time-Shift to mark the advance in time has\n3https://github.com/YatingMusic/remi\nbeen taken for granted thus far. While this approach has its own\nmerits, we endeavor to explore alternative representations in this\npaper, again in the specific context of Transformer-based modeling.\n3 NEW EVENT-BASED REPRESENTATION OF\nMUSIC\nIn this section, we discuss at length how ‘REMI’ is different from\nthe commonly-adopted ‘MIDI-like’ representation (cf. Table 1), and\nhow we design the proposed events.\nAs discussed in [30], a score to be converted into events can be\neither a MIDI score with no expressive dynamics and timing, or a\nMIDI performance that has been converted from an expressive audio\nrecording by means of, for example, a MIDI keyboard, or automatic\nmusic transcription [1]. We consider the latter case in this paper,\nbut the discussion below can also be generalized to the former case.\n3.1 Note-On and Note Velocity\nThe collection of 128 Note-On events indicates the onset of MIDI\npitches from 0 (C-1) to 127 (G9), and Note Velocityindicates the\nlevel of dynamics (which correspond to perceptual loudness) of the\nnote event.4 Both the MIDI-like and REMI representations have\nthese two types of events.\n3.2 Note-Off versus Note Duration\nIn REMI, we use Note Durationevents in replacement of the\nNote-Off events. Specifically, we represent each note in a given\nscore with the following three consecutive tokens: aNote Velocity\nevent, a Note-On event, and a Note Durationevent. There are\nadvantages in doing so:\n•In MIDI-like, the duration of a note has to be inferred from\nthe time gap between a Note-On and the corresponding\nNote-Off, by accumulating the time span of theTime Shift\nevents in between. In REMI, note duration is made explicit,\nfacilitating modeling the rhythm of music.\n•In MIDI-like, a Note-On event and the corresponding Note-\nOff are usually several events apart.5 As a result, a sequence\nmodel may find it difficult to learn that Note-On and Note-\nOff must appear in pairs, generating dangling Note-On\nevents without the correspondingNote-Off. We would then\nhave to use some heuristics (e.g., maximal note duration) to\nturn off a note in post-processing. With REMI, we are free of\nsuch an issue, because the sequence model can easily learn\nfrom the training data that a Note Velocityevent has to be\nfollowed by a Note-On event, and then right away a Note\nDuration event.\n3.3 Time-Shift versus Position & Bar\nEmpirically, we find that it is not easy for a model to generate music\nwith steady beats using the Time-Shift events. When listening to\nthe music generated, the intended bar lines drift over time and the\nrhythm feels unstable. We attribute this to the absence of a metrical\nstructure in the MIDI-like representation. Mistakes in Time-Shift\n4Following [30], in our implementation we quantize note velocity into 32 levels, giving\nrise to 32 different Note Velocityevents.\n5For example, in our implementation, there are on average 21.7±15.3 events between\na pair of Note-On and Note-Off, when we adopt the MIDI-like event representation.\nwould lead to accumulative error of timing in the inference phase,\nwhich is not obvious in the training phase due to the common use\nof teacher forcing strategy [14] in training recurrent models.\nTo address this issue, we propose to use the combination of Bar\nand Position events instead. Both of them are readily available\nfrom the musical scores, or from the result of automatic music\ntranscription (with the help of a beat and downbeat tracker [ 3]);\nthey are simply discarded in the MIDI-like representation. While\nBar marks the bar lines, Position points to one of the Q possible\ndiscrete locations in a bar. HereQ is an integer that denotes the time\nresolution adopted to represent a bar. For example, if we consider a\n16-th note time grid as [23, 34], Q = 16. Adding Bar and Position\ntherefore provides a metrical context for models to “count the beats”\nand to compose music bar-after-bar.67\nWe note that there are extra benefits in usingPosition & Bar, in-\ncluding 1) we can more easily learn the dependency (e.g., repetition)\nof note events occurring at the same Position (⋆/Q) across bars; 2)\nwe can add bar-level conditions to condition the generation process\nif we want; 3) we have time reference to coordinate the generation\nof different tracks for the case of multi-instrument music.\n3.4 Tempo\nIn an expressive musical performance, the temporal length (in sec-\nonds) of each bar may not be the same. To account for such local\nchanges in tempo (i.e., beats per minute; BPM), we add Tempo\nevents every beat (i.e., atPosition (1/Q), Position (\u0000 Q\n4 +1\u0001 /Q), etc).\nIn this way, we have a flexible time grid for expressive rhythm.8 Un-\nlike the Position events, information regrading the Tempo events\nmay not be always available in a MIDI score. But, for the case of\nMIDI performances, one can derive such Tempo events with the\nuse of an audio-domain tempo estimation function [3].\n3.5 Chord\nAs another set of supportive musical tokens, we propose to encode\nthe chord information into input events. Specifically, chords are\ndefined as any harmonic set of pitches consisting of multiple notes\nsounding together or one after another. A chord consists of a root\nnote and a chord quality [ 29]. Here, we consider 12 chord roots\n(C,C#,D,D#,E,F,F#,G,G#,A,A#,B) and five chord qualities\n(major, minor, diminished, augmented, dominant), yielding 60 possi-\nble Chord events, covering the triad and seventh chords. However,\nthe set of Chord events can be further expanded as long as there is\na way to get these chords from a MIDI score or a MIDI performance\nin the data preparation process. We note that the Chord events\nare just “symbols”— the notes are still generated with the Note-on\nevents after them.\nFollowing the time grid of REMI, each Tempo or Chord event is\npreceded by a Position event.\n6We find in our implementation that models learn the meaning of Bar and Position\nquickly—right after a few epochs the model knows that, e.g.,Position (9/Q) cannot go\nbefore Position (3/Q), unless there is a Bar in between. No postprocessing is needed\nto correct errors. See Figure 4 for the learning curve seen in our implementation.\n7We note that the idea of using Bar and Position has been independently proposed\nbefore [19], though in the context of RNN-based not Transformer-based models.\n8As exemplified in Figure 3, we use a combination of Tempo Classevents (low, mid,\nhigh) and Tempo Valueevents to represent local tempo values of 30–209 BPM.\nWe note that music composed by a model using the MIDI-like\nrepresentation also exhibits the use of chords, as such note combi-\nnations can be found in the training data by the sequence model\nitself. However, by explicitly generating Tempo and Chord events,\ntempo and chord become controllable.\n4 PROPOSED FRAMEWORK\nAgain, a diagram of the proposed beat-based music modeling and\ngeneration framework can be found in Figure 2. We provide details\nof some of the major components below.\n4.1 Backbone Sequence Model\nThe Transformer [39] is a neural sequence model that uses self-\nattention to bias its prediction of the current token based on a\nsubset of the past tokens. This design has been shown effective\nfor modeling the structure of music. For example, with the help\nof a relative-positional encoding method [36], Music Transformer\nis claimed in [23] to be able to compose expressive classical piano\nmusic with a coherent structure of up to one minute.\nTransformer-XL [11] extends Transformer by introducing the\nnotion of recurrence and revising the positional encoding scheme.\nThe recurrence mechanism enables the model to leverage the infor-\nmation of past tokens beyond the current training segment, thereby\nlooking further into the history. Theoretically, Transformer-XL\ncan encode arbitrarily long context into a fixed-length representa-\ntion. Therefore, we adopt Transformer-XL as our backbone model\narchitecture.9\nIn the training process, each input sequence is divided into “seg-\nments” of a specific length (set to 512 events in our implementation).\nGiven such segments, the overall computational procedure for an\nN -layer Transformer-XL with M heads can be summarized as:\n˜hn−1\nτ = [stop_gradient(hn−1\nτ−1)◦hn−1\nτ ], (1)\nqn\nτ, kn\nτ, vn\nτ = hn−1\nτ W⊤\nq , ˜hn−1\nτ W⊤\nk , ˜hn−1\nτ W⊤\nv , (2)\nan\nτ, i = masked_softmax(qn⊤\nτ, i kn\nτ, i + R)vn\nτ, i , (3)\nan\nτ = [an\nτ, 1 ◦an\nτ, 2 ◦... ◦an\nτ,m]⊤Wn\no , (4)\non\nτ = layernorm(an\nτ + hn−1\nτ ), (5)\nhn\nτ = max(0, on\nτWn\n1 + bn\n1 )Wn\n2 + bn\n2 , (6)\nwhere hnτ denotes the n-th layer hidden features for the τ -th seg-\nment, R denotes the relative positional encodings designed for the\nTransformer-XL [11], an\nτ, i indicates the attention features from\nthe i-th head, and q, k, v denote the query, key and values in the\ncomputation of self-attention [39], respectively. The main differ-\nence between Transformer and Transformer-XL lies in the usage\nof the features from the segments prior to the current segment (i.e.,\nthe segments are from the same input MIDI sequence of a music\npiece) when updating the model parameters based on that current\nsegment. This mechanism brings in longer temporal context and\nspeeds up both the training and inference processes.\n9We have implemented both Transformer- and Transformer-XL based models and\nfound the latter composes Pop piano music with better temporal coherence percep-\ntually. However, as the use of Transformer-XL for automatic music composition has\nbeen reported elsewhere [12], we omit such an empirical performance comparison\nbetween the XL and non-XL versions.\nChord Required Gain\n1 point\nDeduct\n1 point\nDeduct\n2 points\nMajor 0, 4 7 2, 5, 9 1, 3, 6, 8, 10\nMinor 0, 3 7 2, 5, 8 1, 4, 6, 9, 11\nDiminished 0, 3, 6 9 2, 5, 10 1, 4, 7, 8, 11\nAugmented 0, 4, 8 - 2, 5, 9 1, 3, 6, 7, 10\nDominant 0, 4 ,7, 10 - 2, 5, 9 1, 3, 6, 8, 11\nTable 2: The proposed rule-based scoring criteria for estab-\nlishing the Chord events via pitch intervals in the chro-\nmatic scale.\nTransformer-like sequence models learn the dependency among\nelements (i.e., events here) of a sequence. Therefore, although we\nhave quite a diverse set of events featuring different characteristics\nin the proposed REMI representation, we opt for the simple ap-\nproach of using a single Transformer-XL to model all these events\nat once. In other words, we do not consider the alternative, possibly\nmore computationally intensive, approach of establishing multiple\nTransformer models, one for a non-overlapping subset of the events,\nfollowed by some mechanisms to communicate between them.\n4.2 Beat Tracking and Downbeat Tracking\nTo create the Bar events, we employ the recurrent neural network\nmodel proposed by [3] to estimate from the audio files the position\nof the ‘downbeats, ’ which correspond to the first beat in each bar.\nThe same model is used to track the beat positions to create the\nTempo events [2]. We obtain the tick positions between beats by\nlinear interpolation, and then align the note onsets and offsets to the\nnearest tick. To eliminate the imprecision of transcription result,\nwe further quantize the onset and offset times according to the\nassumed time grid (e.g., to the 16-th note when Q = 16).\n4.3 Chord Recognition\nWe establish the 60 Chord events described in Section 3.5 by de-\nsigning a heuristic rule-based symbolic-domain chord recognition\nalgorithm to the transcribed MIDI files. First, we compute binary-\nvalued “chroma features” [17] for each tick, to represent the activity\nof 12 different pitch classes ignoring the pitch’s octave. Then, we\nuse a sliding window to assign “likelihood scores” to every active\nnote for each 2-beat and 4-beat segment. After summarizing the\nchroma features of the current segment, we consider every note in a\nsegment as a candidate root note of theChord of that segment, and\ncalculate its pitch intervals to all the other notes in that segment.\nThe look-up table shown in Table 2 is employed to assign likelihood\nscores to a pair of root note and chord quality based on the pitch\nintervals. Each chord quality has its required set of pitch intervals,\nand scoring functions. Finally, we recursively label the segments\nby the Chord symbol with the highest likelihood score.\n5 EVALUATION\nWe report both objective and subjective evaluations aiming to vali-\ndate the effectiveness of the proposed model over the Music Trans-\nformer [23] for the case of composing expressive Pop piano music.\nIn what follows, we present the training data we used to train both\n0 100 200 300\nEpoch\n0\n1\n2\n3\n4\n5Average loss\nNote On\nNote Off\nTime Shift\nNote Velocity\n(a) Baseline 1\n0 50 100\nEpoch\n0\n1\n2\n3\n4\n5\n6Average loss\nNote On\nNote Duration\nNote Velocity\nChord\nTempo Class\nTempo Value\nBar\nPosition (b) REMI\nFigure 4: Average cross-entropy loss for different types of\nevents as the training process proceeds. (a) An adapted ver-\nsion of the Music Transformer [23]; (b) the proposed model.\nour model and variants of the the Music Transformer, then some\nimplementation details, and finally the performance study.\n5.1 Dataset\nWe intend to evaluate the effectiveness of the proposed approach\nfor Pop piano composition, as Pop is a musical genre that features\nsalient rhythmic structures. In doing so, we collect audio files of\nPop piano music from the Internet. A total number of 775 pieces\nof piano music played by different people is collected, amounting\nto approximately 48 hours’ worth of data. They are covers 10 of\nvarious Japanese anime, Korean popular and Western popular songs,\nplaying only with the piano. We then apply “Onsets and Frames”\n[21], the state-of-the-art approach for automatic piano transcription,\nto estimate the pitch, onset time, offset time and velocity of the\nmusical notes of each song, converting the audio recordings into\nMIDI performances.\nWe select the Pop piano covers such that they are all in 4/4\ntime signature, simplifying the modeling task. Accordingly, a bar is\ncomposed of four beats. Furthermore, following [23], we consider\nthe 16-th note time grid and quantize each bar intoQ = 16 intervals.\nIn our preliminary experiments, we have attempted to use a finer\ntime grid (e.g., 32-th or 64-th note) to reduce quantization errors\nand to improve the expression of music (e.g., to include triplets,\nswing, mirco-timing variations, etc). However, we found that this\nseems to go beyond the capability of the adopted Transformer-XL\narchitecture. The generated compositions using a finer time grid\ntends to be fragmented and not pleasant to listen to. This is an\nissue that has to be addressed in the future. We stick withQ = 16\nhereafter.\nDue to copyright restrictions, we plan to make the training data\npublicly available not as audio files but as the transcribed MIDI files\nand the converted REMI event sequences.\n10A ‘cover’ is a new recording by someone other than the original artist or composer\nof a commercially released song.\nMethod Note offset Time grid Tempo Chord Beat Downbeat Downbeat\nSTD STD salience\nBaseline 1 Note-Off Time-Shift (10-1000ms) 0.0968 0.3561 0.1033\nBaseline 2 Duration Time-Shift (10-1000ms) 0.0394 0.1372 0.1651\nBaseline 3 Duration Time-Shift (16th-note multiples) 0.0396 0.1383 0.1702\nREMI\nDuration Position & Bar ✓ ✓ 0.0386 0.1376 0.2279\nDuration Position & Bar ✓ 0.0363 0.1265 0.1936\nDuration Position & Bar ✓ 0.0292 0.0932 0.1742\nDuration Position & Bar 0.0199 0.0595 0.1880\nTraining data 0.0607 0.2163 0.2055\nTable 3: Quantitative comparison of different models for Pop piano composition, evaluating how the composed music exhibit\nrhythmic structures. We report the average result across songs. For all the three objective metrics, the values are the closer to\nthose of the ‘Training data’ shown in the last row the better—the training data are the transcribed and 16-th note quantized\nversion of the 775 Pop songs. ‘Baseline 1’ represents a Transformer-XL version of the Music Transformer, adopting a MIDI-like\nevent representation. The other two baselines are its improved variants. We highlight the two values closest to those of the\ntraining data in bold.\nBaseline 1\nBaseline 3\nREMI\nFigure 5: Examples of the generated piano rolls of different models. These are 12 bars generated to continue a given prompt\nof 4-bar human piano performance. We show the result for three different prompts (from left to right). The thicker vertical\nlines indicate the bar lines; best viewed in color.\n5.2 Baselines & Model Settings\nWe consider three variants of the Music Transformer [23] as the\nbaselines in our evaluation. The first one, dubbed Baseline 1, fol-\nlows fairly faithfully the model settings of [ 23], except that we\nuse Transformer-XL instead of Transformer (for fair comparison\nwith our model). The other two differ from the first one only in the\nadopted event representation. While Baseline 1 employs exactly\nthe MIDI-like representation, Baseline 2 replaces Note-Off by\nNote Duration, and Baseline 3 further modifies the time steps\ntaken in Time-Shift from multiples of 10ms to multiples of the\n16-th note. In this way, Baseline 3 has a time grid similar to that of\nREMI, making Baseline 3 a strong baseline.\nFor either the baselines or the proposed model adopting the REMI\nrepresentation, we train a single Transformer-XL with N = 12 self-\nattention layers and M = 8 attention heads. The length of the\ntraining input events (i.e., the segment length) and the recurrence\nlength (i.e., the length of the segment “cached” [11]) are both set\nto 512. The total number of learnable parameters is approximately\n41M. Training a model with an NVIDIA V100 with mini-batch size\nof 16 till the training loss (i.e., cross-entropy) reaches a certain\nlevel takes about 9 hours. We find that Baseline 1 needs a smaller\nlearning rate and hence longer training time. For processing the\nMIDI files, we use the miditoolkit. 11\nFigure 4 shows the training loss of different event types as the\ntraining unfolds. Figure 4a shows that Baseline 1 struggles the most\nfor learning Time-Shift. Moreover, Note-Off has higher loss than\nNote-On, suggesting that they are not recognized as event pairs.\nIn contrast, Figure 4b shows that the Position events in REMI are\neasy to learn, facilitating learning the rhythm of music.12\n5.3 Objective Evaluation\nThe objective evaluation assesses the rhythmic structure of the\ngenerated compositions. Specifically, we employ each model to\nrandomly generate 1,000 sequences, each with 4,096 events, using\nthe temperature-controlled stochastic sampling method with top-\nk [24]. We convert the generated sequences into MIDI and then\n11https://github.com/YatingMusic/miditoolkit\n12According to [21, 25], piano transcription models do better in estimating note onsets\nand pitches, than offsets and velocities. This may be why our model has higher loss\nfor Note Durationand Note Velocity.\nrender them into audio via a piano synthesizer.13 Then, we apply\nthe joint beat and downbeat tracking model of [3] to the resulting\naudio clips.14\nThe model [3] has two components. The first one estimates the\nprobability, or salience, of observing beats and downbeats for each\ntime frame via a recurrent neural network (RNN). The second one\napplies a dynamic Bayesian network (DBN) to the output of the RNN\nto make binary decisions of the occurrence of beats and downbeats.\nWe use the output of the RNN to calculate the downbeat salience,\nand the output of the DBN for the beat STD and downbeat STD.\nSpecifically, given a piece of audio signal, the model [ 3] returns\na time-ordered series of ( τ B\nt , sB\nt ), where τ B\nt denotes the estimate\ntime (in seconds) for the t-th beat by the DBN, and sB\nt ∈[0, 1]the\nsalience associated with that beat estimated by the RNN. It also\nreturns similarly a series of (τ D\nt , sD\nt ) for the downbeats; see Figure 1\nfor examples of such series. From the tracking results, we calculate\nthe following values for each audio clip:\n•Beat STD : the standard deviation of (τ B\nt −τ B\nt−1), which is\nalways positive, over the beat estimates for that clip.\n•Downbeat STD: similarly the standard deviation of (τ D\nt −\nτ D\nt−1)over the downbeat estimates for that clip. Both beat\nSTD and downbeat STD assess the consistency of the rhythm.\n•Downbeat salience : the mean of sD\nt over the downbeat\nestimates for that clip, indicating the salience of the rhythm.\nWe report the average values across the audio clips, assuming that\nthese values are closer to the values calculated from the training\ndata (which have been quantized to the 16-th note grid) the better.15\nTables 3 shows the result of the baselines and a few variants\n(ablated versions) of the REMI-based model. From Beat STD and\nDownbeat STD, we see that the result of Baseline 1, which resembles\nMusic Transformer [23], features fairly inconsistent rhythm, echo-\ning the example shown in Figure 1a. The STD is lower when Note\nDuration is used in place of Note-Off, highlighting the effect of\nNote Durationin stabilizing the rhythm. Interestingly, we see\nthat the REMI models without the Tempo events have much lower\nSTD than the training data, suggesting that Tempo is important for\nexpressive rhythmic freedom.\nFrom downbeat salience, the REMI models outnumber all the\nbaselines, suggesting the effectiveness of Position & Bar. More-\nover, the gap between the result of Baseline 1 and Baseline 2 further\nsupports the use of the Note Durationevents.\n5.4 Subjective Evaluation\nThe best way to evaluate a music composition model remains today\nto be via a listening test. To have a common ground to compare\ndifferent models, we ask the models to continue the same given\nprompt. For doing so, we prepare an additional set of 100 Pop piano\nperformances (that have no overlaps with the training set), process\nthem according to the procedures described in Section 5.1, and take\nthe first four bars from each of them as the prompts. The following\nthree models are asked to generate 16 bars continuing each prompt,\n13https://github.com/YatingMusic/ReaRender\n14We choose to use the model of [3] for it achieved state-of-the-art performance for\nbeat and downbeat tracking for a variety of musical genres, especially for Pop music.\n15Small beat/downbeat STD implies that the music is too rigid, whereas large STD\nimplies that the rhythm is too random. We want the rhythm to be stable yet flexible.\nBaseline 1 Baseline 3 REMI\n‘pros’ group 1.77 2.03 2.20\n‘non-pros’ group 1.66 1.90 2.44\nall participants 1.73 1.98 2.28\nTable 4: The average scores for the subjective preference test\non a three-point scale from 1 (like the least) to 3 (like the\nmost).\nPairs Wins Losses p-value\nREMI Baseline 1 103 49 5.623e-5\nREMI Baseline 3 92 60 0.0187\nBaseline 3 Baseline 1 90 62 0.0440\nTable 5: The result of pairwise comparison from the user\nstudy, with the p-value of the Wilcoxon signed-rank test.\nand got evaluated in an online listening test: ‘Baseline 1, ’ ‘Baseline 3’\nand ‘REMI without Chord. ’ We do not useChord here to make the\nrhythmic aspect the major point of difference among the models.\nWe distribute the listening test over our social circles globally\nand solicit the response from 76 participants. 51 of them understand\nbasic music theory and have the experience of being an amateur\nmusician, so we consider them as professionals (denoted as ‘pros’).\nA participant has to listen to two randomly picked sets of samples\nand evaluate, for each set, which sample they like the most and the\nleast, in any evaluation criteria of their choice. Each set contains\nthe result of the three models (in random order) for a given prompt.\nTable 4 shows the aggregated scores, and Table 5 the result\nof broken-down pairwise comparisons, along with the p-value of\nthe Wilcoxon signed-rank test. We see that REMI is preferred by\nboth pros and non-pros, and that the difference between REMI and\nBaseline 1 is significant (p < 0.01).\nFigure 5 provides examples of the generated continuations. From\nthe optional verbal feedbacks of the participants, the music gen-\nerated by REMI are perceptually more pleasing and are more in\nline with the prompt. Audio examples can be found at our demo\nwebsite.16\n5.5 Controllable Chord and Tempo\nFinally, we demonstrate the controllability of Tempo and Chord of\nour model. To achieve this, we can simply force the model not to\ngenerate specific events by masking out the corresponding probabil-\nities of the model output. Figure 6 shows the piano rolls, (optionally)\nthe Tempo and Chord events generated by our model under dif-\nferent conditions. Figure 6b shows that the model selects chords\nthat are harmonically close to F:minor when we prohibit it from\ngenerating F:minor. Figure 6c shows controlling the Tempo Class\naffects not only the Tempo Valuebut also the note events.\n16https://drive.google.com/drive/folders/1LzPBjHPip4S0CBOLquk5CNapvXSfys54\n110\n122\nF\nmin\nF\nmin\nF\nmin/A#\nF\nmin\nF\nmin\nF\nmin\nF\nmin\nC\nmin/G\nF\nmin\nF\nmin\nF\nmin\nD#\nmaj/G\nF\nmin\nF\nmin\nF\nmin/D#\nF\nmin/C#\nC#\nmaj\nC#\nmaj\n C3\n C5\n C7\n(a) A human-made musical piece containing the prompt used be-\nlow\nF\nmin\nF\nmin\nF\nmin\nF\nmin\nA#\nmin\nG#\nmaj\nA#\nmin\nA#\nmin\nC#\nmaj\nG#\nmaj\nG#\nmaj\nG#\nmaj\nA#\nmin\nA#\nmin\nA#\nmin\nG#\nmaj\nC\naug\nC#\nmaj\nG\ndom\nE\nmin\nD#\naug\nA#\nmin\nG#\nmaj\n C3\n C5\n C7\n(b) The result when we enforce no F:minor chord after the 4th\nbar\n110\n178\n C3\n C5\n C7\n(c) The result when we enforce using Tempo Class (high)\nFigure 6: An example illustrating how we can condition the\nTempo and Chord events (the first four bars is the prompt).\n6 CONCLUSION\nIn this paper, we have presented REMI, a novel MIDI-derived event\nrepresentation for sequence model-based composition of expressive\npiano music. Using REMI as the underlying data representation, we\nhave also built a Pop Music Transformer that outperforms that state-\nof-the-art Music Transformer for composing expressive Pop piano\nmusic, as validated in objective and subjective studies. We take this\nas a proof-of-concept that it is beneficial to embed prior human\nknowledge of music through including components of music infor-\nmation retrieval (MIR) techniques, such as downbeat estimation\nand chord recognition, to the overall generative framework. For\nfuture work, we plan to incorporate other high-level information\nof music, such as grooving [15] and musical emotion [42], to learn\nto compose additional tracks such as the guitar, bass, and drums,\nto further improve the resolution of the time grid, to condition the\ngenerative process with other multimedia input such as the lyrics\n[43] or a video clip [44], and also to study other model architectures\nthat can model longer sequences (e.g., [33]) and thereby learn better\nthe long-term structure in music.\n7 ACKNOWLEDGEMENT\nThe authors would like to thank Wen-Yi Hsiao (Taiwan AI Labs)\nfor helping with processing the MIDI data and with rendering the\nMIDIs to audios for subjective study.\nREFERENCES\n[1] Emmanouil Benetos, Simon Dixon, Zhiyao Duan, and Sebastian Ewert. 2018.\nAutomatic music transcription: An overview. IEEE Signal Processing Magazine\n36, 1 (2018), 20–30.\n[2] Sebastian Böck, Filip Korzeniowski, Jan Schlüter, Florian Krebs, and Gerhard\nWidmer. 2016. Madmom: A new python audio and music signal processing\nlibrary. In Proc. ACM Multimedia . 1174âĂŞ1178.\n[3] Sebastian Böck, Florian Krebs, and Gerhard Widmer. 2016. Joint beat and down-\nbeat tracking with recurrent neural networks. In Proc. Int. Soc. Music Information\nRetrieval Conf. 255–261.\n[4] Dmitry Bogdanov, J. Serrà, Nicolas Wack, and Perfecto Herrera. 2009. From\nlow-level to high-level: Comparative study of music similarity measures. In Proc.\nIEEE International Symposium on Multimedia .\n[5] Paolo Bottoni, Anna Labella, Stefano Faralli, Mario Pierro, and Claudio Scoz-\nzafava. 2006. Interactive composition, performance and music generation through\niterative structures. In Proc. ACM Multimedia . 189âĂŞ192.\n[6] Jean-Pierre Briot, Gaëtan Hadjeres, and François Pachet. 2019. Deep Learning\nTechniques for Music Generation, Computational Synthesis and Creative Systems .\nSpringer.\n[7] Gong Chen, Yan Liu, Sheng-hua Zhong, and Xiang Zhang. 2018. Musicality-\nnovelty generative adversarial nets for algorithmic composition. In Proc. ACM\nMultimedia. 1607âĂŞ1615.\n[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long\nsequences with sparse Transformers. arXiv preprint arXiv:1904.10509 (2019).\n[9] Kristy Choi, Curtis Hawthorne, Ian Simon, Monica Dinculescu, and Jesse Engel.\n2020. Encoding musical style with transformer autoencoders. In Proc. Int. Conf.\nMachine Learning .\n[10] Grosvenor W Cooper, Grosvenor Cooper, and Leonard B Meyer. 1963. The\nRhythmic Structure of Music . University of Chicago Press.\n[11] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-\nLength context. In Proc. Annual Meeting of the Association for Computational\nLinguistics. 2978–2988.\n[12] Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garrison W. Cottrell, and\nJulian McAuley. 2019. LakhNES: Improving multi-instrumental music generation\nwith cross-domain pre-training. In Proc. Int. Soc. Music Information Retrieval Conf.\n685–692.\n[13] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. 2018.\nMuseGAN: Multi-track sequential generative adversarial networks for symbolic\nmusic generation and accompaniment. In Proc. AAAI . 34–41.\n[14] Kenji Doya. 1992. Bifurcations in the learning of recurrent neural networks 3.\nlearning (RTRL) 3 (1992), 17.\n[15] Jan Frühauf, Reinhard Kopiez, and Friedrich Platz. 2013. Music on the timing\ngrid: The influence of microtiming on the perceived groove quality of a simple\ndrum pattern performance. Musicae Scientiae 17, 2 (2013), 246–260.\n[16] Magdalena Fuentes, Brian McFee, C. HÃľlÃĺne Crayencour, Slim Essid, and\nPablo Juan Bello. 2018. Analysis of common design choices in deep learning\nsystems for downbeat tracking. In Proc. Int. Soc. Music Information Retrieval Conf.\n106–112.\n[17] Takuya Fujishima. 1999. Realtime chord recognition of musical sound: A system\nusing common Lisp. In Proc. International Computer Music Conf. 464–467.\n[18] Lorenzo Gatti, Gözde Özbal, Oliviero Stock, and Carlo Strapparava. 2017. Auto-\nmatic generation of lyrics parodies. In Proc. ACM Multimedia . 485âĂŞ491.\n[19] Benjamin Genchel, Ashis Pati, and Alexander Lerch. 2019. Explicitly conditioned\nmelody generation: A case study with interdependent RNNs. In Proc. Computer\nSimulation of Music Creativity Conf.\n[20] Gaëtan Hadjeres, François Pachet, and Frank Nielsen. 2017. DeepBach: A Steer-\nable Model for Bach chorales generation. In Proc. Int. Conf. Machine Learning .\n1362–1371.\n[21] Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin\nRaffel, Jesse Engel, Sageev Oore, and Douglas Eck. 2018. Onsets and Frames:\nDual-objective piano transcription. In Proc. Int. Soc. Music Information Retrieval\nConf. 50–57.\n[22] Jim Heckroth. 1998. A tutorial on MIDI and wavetable music synthesis. Applica-\ntion Note, CRYSTAL a division of CIRRUS LOGIC (1998).\n[23] Cheng-Zhi A. Huang et al . 2019. Music Transformer: Generating music with\nlong-term structure. In Proc. Int. Conf. Learning Representations .\n[24] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and\nRichard Socher. 2019. CTRL: A conditional transformer language model for\ncontrollable generation. arXiv preprint arXiv:1909.05858 (2019).\n[25] Jong Wook Kim and Juan Pablo Bello. 2019. Adversarial learning for improved\nonsets and frames music transcription. InProc. Int. Soc. Music Information Retrieval\nConf. 670–677.\n[26] Chien-Hung Liu and Chuan-Kang Ting. 2017. Computational intelligence in mu-\nsic composition: A survey.IEEE Transactions on Emerging Topics in Computational\nIntelligence 1, 1 (2017), 2–15.\n[27] Qi Lyu, Zhiyong Wu, and Jun Zhu. 2015. Polyphonic music modelling with\nLSTM-RTRBM. In Proc. ACM Multimedia . 991âĂŞ994.\n[28] Jason Martineau. 2008. The Elements of Music: Melody, Rhythm, and Harmony .\nBloomsbury Publishing USA.\n[29] B. McFee and J. P. Bello. 2017. Structured training for large-vocabulary chord\nrecognition. In Proc. Int. Soc. Music Information Retrieval Conf. 188–194.\n[30] Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and Karen Simonyan.\n2018. This time with feeling: Learning expressive musical performance. Neural\nComputing and Applications (2018).\n[31] François Pachet. 2016. A Joyful Ode to automatic orchestration.ACM Transactions\non Intelligent Systems and Technology 8, 2 (2016).\n[32] Christine McLeavy Payne. 2019. MuseNet. OpenAI Blog (2019).\n[33] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Tim-\nothy P. Lillicrap. 2020. Compressive Transformers for long-range sequence\nmodelling. In Proc. Int. Conf. Learning Representations .\n[34] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck.\n2018. A hierarchical latent vector model for learning long-term structure in\nmusic. In Proc. Int. Conf. Machine Learning . 4361–4370.\n[35] Ricardo Scholz and Geber Ramalho. 2008. Cochonut: Recognizing complex chords\nfrom MIDI guitar sequences. In Proc. Int. Soc. Music Information Retrieval Conf.\n27–32.\n[36] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with\nrelative position representations. In Proc. Conf. the North American Chapter of\nthe Association for Computational Linguistics . 464–468.\n[37] Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019. Insertion\nTransformer: Flexible sequence generation via insertion operations. In Proc. Int.\nConf. Machine Learning . 5976–5985.\n[38] Bob L. Sturm, JoÃčo F. Santos, Oded Ben-Tal, and Iryna Korshunova. 2016. Music\ntranscription modelling and composition using deep learning. In Proc. Conf.\nComputer Simulation of Musical Creativity .\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. Advances in Neural Information Processing Systems . 5998–6008.\n[40] Yaushian Wang, Hung-Yi Lee, and Yun-Nung Chen. 2019. Tree Transformer:\nIntegrating tree structures into self-attention. In Proc. Conf. Empirical Methods in\nNatural Language Processing . 1061–1070.\n[41] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. 2017. MidiNet: A convolutional\ngenerative adversarial network for symbolic-domain music generation. In Proc.\nInt. Soc. Music Information Retrieval Conf. 324–331.\n[42] Yi-Hsuan Yang and Homer H. Chen. 2011. Music Emotion Recognition . CRC\nPress.\n[43] Yi Yu and Simon Canales. 2019. Conditional LSTM-GAN for melody generation\nfrom lyrics. arXiv preprint arXiv:1908.05551 (2019).\n[44] Yi Yu, Zhijie Shen, and Roger Zimmermann. 2012. Automatic music soundtrack\ngeneration for outdoor videos from contextual sensor information. In Proc. ACM\nMultimedia. 1377âĂŞ1378.\n[45] Hongyuan Zhu et al. 2018. XiaoIce Band: A melody and arrangement generation\nframework for Pop music. In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery\n& Data Mining . 2837âĂŞ2846."
}