{
  "title": "Classification based Grasp Detection using Spatial Transformer Network",
  "url": "https://openalex.org/W2791026634",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2679240406",
      "name": "Park, Dongwon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753053892",
      "name": "Chun, Se Young",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963326767",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2123435073",
    "https://openalex.org/W2964161785",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2523315006",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2951548327",
    "https://openalex.org/W2572996265",
    "https://openalex.org/W2005824379",
    "https://openalex.org/W2041376653",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2118262422",
    "https://openalex.org/W3112422759",
    "https://openalex.org/W1999156278",
    "https://openalex.org/W2165603175",
    "https://openalex.org/W1892339738"
  ],
  "abstract": "Robotic grasp detection task is still challenging, particularly for novel objects. With the recent advance of deep learning, there have been several works on detecting robotic grasp using neural networks. Typically, regression based grasp detection methods have outperformed classification based detection methods in computation complexity with excellent accuracy. However, classification based robotic grasp detection still seems to have merits such as intermediate step observability and straightforward back propagation routine for end-to-end training. In this work, we propose a novel classification based robotic grasp detection method with multiple-stage spatial transformer networks (STN). Our proposed method was able to achieve state-of-the-art performance in accuracy with real- time computation. Additionally, unlike other regression based grasp detection methods, our proposed method allows partial observation for intermediate results such as grasp location and orientation for a number of grasp configuration candidates.",
  "full_text": "Classiﬁcation based Grasp Detection using Spatial Transformer Network\nDongwon Park1 and Se Young Chun 1,†\nAbstract— Robotic grasp detection task is still challenging,\nparticularly for novel objects. With the recent advance of\ndeep learning, there have been several works on detecting\nrobotic grasp using neural networks. Typically, regression based\ngrasp detection methods have outperformed classiﬁcation based\ndetection methods in computation complexity with excellent\naccuracy. However, classiﬁcation based robotic grasp detection\nstill seems to have merits such as intermediate step observability\nand straightforward back propagation routine for end-to-end\ntraining. In this work, we propose a novel classiﬁcation based\nrobotic grasp detection method with multiple-stage spatial\ntransformer networks (STN). Our proposed method was able\nto achieve state-of-the-art performance in accuracy with real-\ntime computation. Additionally, unlike other regression based\ngrasp detection methods, our proposed method allows partial\nobservation for intermediate results such as grasp location and\norientation for a number of grasp conﬁguration candidates.\nI. INTRODUCTION\nRobotic grasping of novel objects is still a challenging\nproblem. It requires to perform robotic grasp detection,\ntrajectory planning and execution. Detecting robotic grasp\nfrom imaging sensors is a crucial step for successful grasp-\ning. There have been numerous works on robotic grasp\ndetection (or synthesis). In large, grasp synthesis is divided\ninto analytical or empirical (or data-driven) methods [1] for\nknown, familiar or novel objects [2].\nMachine learning approaches for robotic grasp detection\nhave utilized data to learn discriminative features for a suit-\nable grasp conﬁguration and to yield excellent performance\non generating grasp locations [3], [4]. Since deep learning\nhas been successful in computer vision applications such as\nimage classiﬁcation [5], [6] and object detection [7], [8],\nthese powerful tools have applied to robotic grasp detection\nof location and orientation [9], [10].\nThere are two types of machine learning approaches in\nrobotic grasp detection of location and orientation. One\nis classiﬁcation based approach that trains classiﬁers to\ndiscriminate graspable points and orientations for local im-\nage patches [3], [4], [9], [11]. In general, sliding window\napproach generates numerous candidates for robotic grasp\nconﬁgurations with different location, orientation and scale\n(size of gripper). Then, these image patches are fed into\nmachine learning (or deep learning) classiﬁers to yield the\n*This work was supported by the Technology Innovation Program or\nIndustrial Strategic Technology Development Program (10077533, Devel-\nopment of robotic manipulation algorithm for grasping/assembling with the\nmachine learning using visual and tactile sensing information) funded by\nthe Ministry of Trade, Industry & Energy (MOTIE, Korea).\n1Dongwon Park and Se Young Chun are with Department of Electrical\nEngineering, Ulsan National Institute of Science and Technology (UNIST),\nUlsan, Republic of Korea. †Email: sychun@unist.ac.kr\nFig. 1. An example of a robotic grasp detection with ﬁve-dimensional\ngrasp representation for a banana. Green lines are two plates of a gripper\nwhose size is h, yellow lines are the distance between two plate grippers\nfor grasping, red point is the center location of grasp rectangle (x,y), and\nred angle θ is the orientation of the grasp rectangle.\nscores of graspability (the higher score, the better graspa-\nbility). Finally, one candidate image patch with the highest\nscore will be selected. The location, orientation and size\nof that image patch will be the ﬁnal grasp detection result.\nUnfortunately, this approach is in general slow due to brute-\nforce sliding window. Note that classiﬁer based robotic grasp\ndetection is similar to grasp detection methods using 3D\ngrasp simulators [12], [13].\nThe other is regression based approach that trains a model\n(parametric or non-parametric, neural network or probability\ndistribution) to yield robotic grasp detection parameters for\nlocation and orientation directly [10], [14], [15]. Typical ﬁve-\ndimensional robotic grasp parameters are shown in Fig. 1,\nwhere the width ( w), height ( h), location ( x,y) and and\norientation ( θ) of a grasp rectangle [11], [9]. Note that\nwith additional depth and surface norm information, these\nﬁve parameters can be transformed into seven-dimensional\nrobotic grasp representation [11]. Since all grasp parameters\nare directly estimated from a single image (or a set of multi-\nmodal images), no sliding window is required. Regression\nbased robotic grasp detection methods are usually much\nfaster than classiﬁcation based methods in term of computa-\ntion [10]. However, fair comparison between regression and\nclassiﬁcation based grasp detection does not seem to be well\ninvestigated for different computation platforms.\nIn this paper, we propose a novel classiﬁcation based\nrobotic grasp detection method using multiple-stage spatial\ntransformer networks (STN). Unlike other black-box regres-\nsion based grasp detection methods, multiple-stage STN of\nour proposed method allows partial observation of interme-\ndiate grasp results such as grasp location and orientation,\nfor a number of candidates for grasp. We will show that\nour proposed method achieves state-of-the-art performance\nin terms of both accuracy and computation complexity. The\narXiv:1803.01356v1  [cs.CV]  4 Mar 2018\nFig. 2. Classiﬁcation based robotic grasp detection procedure [9]. Sliding window brute-force search of candidates consumes lots of computation time\nfor different location, orientation, and scale. However, intermediate steps are clearly visible so that good or bad candidates can be observable.\ncontribution of this paper is as follows:\n• A novel multiple-stage STN network is proposed using\nthe original STN [16] and the deep residual network\n(ResNet) [6] instead of brute force sliding window.\nIntermediate grasp results are now partially observable\nin our proposed network.\n• A new classiﬁcation based robotic grasp detection\nmethod using our multiple-stage STN is proposed for\nreal-time detection with excellent accuracy. End-to-\nend training strategy was investigated for our proposed\nmethod with high resolution images.\n• Extensive comparison of our proposed method with\nother methods on the same platform was performed with\nthe same size of input image for fair comparison.\nII. RELATED WORK\nA. Deep Learning based Object Detection\nThere have been much research on object detection using\ndeep learning. Object detection is simply to identify the\nlocation and the class of an object (or objects).\nThere have been several classiﬁer based object detection\nmethods proposed such as region-based convolutional neural\nnetwork (R-CNN) [17], fast R-CNN [18] and faster R-\nCNN [7]. The original R-CNN uses a classiﬁer for a local\npatch of an image with sliding window on the image to\nidentify where and what the object is [17]. Due to time-\nconsuming sliding window with heavy CNN operations, this\nmethod is known to be slow. This sliding window approach\nis similar to the work of Lenz et al. in robotic grasp detection\nproblem [9]. Fast R-CNN signiﬁcantly reduced computation\ncost of R-CNN by having sliding window not on an image,\nbut on a feature space [18]. However, due to a large amount\nof object detection candidates, it was still not a real-time\nprocessing. Faster R-CNN proposed region proposal network\n(RPN) to reduce the amount of object detection candidates\nso that it signiﬁcantly improved computation time [7]. RPN\ngenerates candidate rectangles of object detection selectively\nso that faster R-CNN maintains (or improves) detection\nperformance while reduces computation.\nSeveral regressor based object detection methods have also\nbeen proposed such as you only look once (YOLO) [19],\nsingle shot multibox detector (SSD) [20] and YOLO9000 [8].\nInstead of evaluating many object detection candidate win-\ndows, these methods process an image only once to estimate\nobject detection rectangles directly so that fast computation\nis able to be achieved. YOLO used a pre-trained AlexNet [5]\nto estimate the location and class of multiple objects [19].\nSSD further developed regression based object detection to\nincorporate intermediate CNN features for object detection\nand improved accuracy and computation speed [20]. Re-\ncently, YOLO9000 extended the original YOLO signiﬁcantly\nto classify 9000 classes of objects with fast computation and\nhigh accuracy [8]. These approaches are similar to the work\nof Redmon and Angelova [10] in robotic grasp detection.\nB. Deep Learning based Robotic Grasp Detection\nData-driven robotic grasp detection for novel object has\nbeen investigated extensively [2]. Before deep learning, there\nhave been some works on grasp location detection using\nmachine learning techniques [3]. Saxena et al. proposed a\nmachine learning method to rank the best graspable location\nfor all candidate image patches from different locations.\nJiang et al. proposed to use a ﬁve-dimensional robotic grasp\nrepresentation as also shown in Fig. 1 and further improved\na machine learning method to rank the best graspable image\npatch whose representation includes orientation and gripper\ndistance among all candidates [11].\nSince the advent of deep learning [21], robotic grasp detec-\ntion using deep learning has been investigated for improved\naccuracy. Lenz et al. proposed a sparse auto-encoder (SAE)\nto train the network to rank the best graspable image patch\nwith multi-modal information (RGB color, depth, and calcu-\nlated surface norm) and to apply to robotic grasp detection\nusing sliding window [9]. However, due to time-consuming\nsliding window process, this method was slow (13.5 sec per\nimage). Moreover, this work was not further extended with\nsimple modiﬁcation using recent activation functions such\nas ReLU (Rectiﬁer Linear Unit), which could potentially\nimprove performance signiﬁcantly [5]. This method can be\ncategorized into classiﬁcation based grasp detection method.\nThis type of methods allows to observe intermediate steps of\ngrasp detection by showing many candidates with good or\nbad graspability. Fig. 2 illustrates an example of classiﬁcation\nbased robotic grasp detection pipeline.\nThen, Redmon and Angelova proposed a real-time robotic\ngrasp detection using modiﬁed AlexNet [5], [10]. A pre-\nFig. 3. Regression based robotic grasp detection pipeline [10]. One shot evaluation for the whole image is possible to directly generate grasp conﬁguration\nsuch as location, orientation, and scale. However, intermediate steps are not observable.\ntrained AlexNet was modiﬁed to estimate robotic grasp\nparameters directly for local windows so that no sliding win-\ndows is necessary. This approach signiﬁcantly improved the\nperformance of grasp detection in accuracy and computation\ntime over the work of Lenz et al. . To incorporate depth\ninformation without changing the network structure much,\nblue channel was used for depth. No sliding window seems\nto contribute to the speed up of this method signiﬁcantly and\nrecent deep network seems to help achieving state-of-the-art\naccuracy. This method can be categorized into regression\nbased grasp detection method. Unfortunately, this type of\nregression based methods do not allow to observe interme-\ndiate steps of grasp detection. Fig. 3 illustrates an example\nof regression based robotic grasp detection procedure.\nRecently, Asif et al. proposed a object recognition and\ngrasp detection using hierarchical cascaded forests using\nfeatures extracted from deep learning [15]. This work fo-\ncused on improving accuracy of tasks, rather on improv-\ning computation time. Wang et al. proposed a real-time\nclassiﬁcation based grasp detection method using two-stage\napproach [22]. This method utilized a stacked SAE for\nclassiﬁcation, which is similar to the work of Lenz et al., but\nwith much more efﬁcient grasp candidate generation. This\nmethod utilized several prior information and pre-processing\nto reduce the search space of grasp candidates such as\nobject recognition result and the graspability of previously\nevaluated image patches. It also reduced the number of grasp\nparameters to estimate such as height ( h) for known gripper\nand the orientation ( θ) that could be analytically calculated\nfrom surface norm. This model does not support end-to-end\nlearning for candidate estimation block. Kumra and Kanan\nproposed a real-time regression based grasp detection method\nusing ResNet for multimodal information (RGB-D). Two\npre-trained ResNet-50 networks were used to extract features\nfor RGB color image and for depth image, respectively [14].\nThen, a neural network with 3 fully connected layers merged\nthese feature vectors to yield grasp conﬁguration such as\nwidth, height, orientation and location. End-to-end training\noptimized the whole network. However, due to regression\nbased approach, intermediate steps are not observable.\nIII. PROPOSED METHOD\nA. Multiple-Stage STN for Robotic Grasp Detection\nInstead of slow sliding window for generating many grasp\ncandidates, we propose a multi-stage STN for generating a\nnumber of highly selective robotic grasp candidates. This\napproach seems similar to RPN in [7], but RPN has never\nused in robotic grasp detection and can not deal with different\norientation. STN can explicitly encode spatial transformation\nincluding orientation [16]. Our approach is also different\nfrom the work of Wanget al. [22] that used prior information,\nwhile our approach is fully data-driven and can support end-\nto-end training for ﬁne tuning.\nSTN consists of localization network to generate transfor-\nmation parameters, grid generators with the output transfor-\nmation parameters and sampler to generate warped images\nor feature maps as shown in Fig. 4 [16]. We modiﬁed the\noriginal STN by constructing a new localization network\nusing residual blocks [6]. STN can generate all necessary\nrobotic grasp conﬁguration in one network like regression\nbased grasp detection method. However, we adopt multiple-\nstage approach to generate a number of locations, then to\nestimate proper orientation for each location, and lastly to\nﬁne tune for locations and scale (width and height). Note\nthat this procedure seems similar to human grasp detection.\nHumans usually ﬁnd possible grasp locations ﬁrst, then\nestimate orientation and scale information. Our multiple-\nstage STN is illustrated in Fig. 5. Note that since STN is\ndifferentiable, it is possible to implement back propagation\nfor our proposed multiple-stage STN.\nOne of the potential advantages in our multiple-stage\nSTN approach is that intermediate grasp detection steps\nare partially observable. A number of generated candidates\nare observable at each stage of our proposed STN. This\nfeature was practically helpful for us to correct for potential\nerrors in grasp detection during our experiment. In addition,\nit is possible for our proposed multiple-stage STN to be\ntrained end-to-end. This is not only helpful for training\nrobotic grasp detection with ground truth data with labels, but\nalso potentially useful for end-to-end learning from robotic\ncontrol systems so that the whole network can be improved\nover trial and error of robots [23].\nFig. 4. A typical STN structure proposed in [16]. Localization network\ngenerates transformation parameters. Then, STN transforms an input image\nor feature map using the output parameters. It is possible to back propagate\nthis network since it is differentiable.\nFig. 5. Our proposed classiﬁcation based robotic grasp detection pipeline. One shot evaluation for the whole image is possible to generate a number of\npotential grasp candidates so that it is fast as well as intermediate steps are partially observable.\nB. Classiﬁcation based Grasp Detection using STN\nOur proposed grasp detection network using multiple-\nstage STN is illustrated in Fig. 5. After pre-processing,\nSTNCrop identiﬁes a number of possible grasp locations and\ngenerates a set of locations (x,y). Then, for each location,\nSTNrotation is applied to ﬁnd appropriate orientation θ\nfor graspable areas. Lastly, STNScale,Crop determines the\nwidth and height for gripper distance and size as well as\nadditional locations (δx,δy) for ﬁne tuning. Each cropped\nimage patch is fed into grasp classiﬁer to generate the score\nof graspability. Finally, max pooling select the best graspable\nconﬁguration among selective candidates.\nFor training this proposed network, ﬁrst of all, each\ncomponent was trained using ground truth data: three STN\nnetworks as well as grasp classiﬁer. Then, for end-to-end\nlearning, we focus on training the end block (grasp classiﬁer)\nﬁrst, and then train the second end block ( STNScale,Crop )\nwhile ﬁxing the previous block, and so on. This ﬁne tuning\nstep was able to improve accuracy further. Note that unlike\nprevious works using 224 ×224 input images, our proposed\nnetwork has relatively high resolution input images with\n400 ×400, roughly 3 times more pixels than previous cases.\nIV. EXPERIMENTAL RESULTS\nMost robotic grasp detection works compared their meth-\nods with other previous approaches based on the reported\nresults in papers. While the accuracy comparison may be\nreasonable, computation complexity comparison may require\nmore careful approach than that due to many crucial factors\nsuch as input image size and used GPU spec. In this\nwork, we reproduced all the results of previous methods for\ncomparison on the same platform with the same size data.\nA. Dataset\nWe trained and tested our proposed classiﬁcation based\nrobotic grasp detection method using multiple-stage STN\nwith the Cornell grasp detection dataset [9]. This dataset\ncontains 855 images (RGB color and depth) of 240 different\nobjects with the ground truth labels of a few graspable\nrectangles and a few not-graspable rectangles. To train our\ngrasp classiﬁers to assign low graspability score, we addition-\nally generated a number of background image patches with\nalmost all white color. Note that we used cropped images\nwith 400 × 400 instead of 224 × 224, which is relatively\nhigh resolution than the resolution previous works used. It\nwas possible since our proposed method does not require\npre-training with massive dataset such as ImageNet.\nB. Training\nWe split the Cornell grasp dataset into training set and\ntest set (4:1) with image-wise spliting. To train individual\nblocks of our proposed network as shown in Fig. 4, ground\ntruth labels were transformed into appropriate values for each\nSTN block. We set the output of STNCrop to be 8 so that 4\ncandidate locations can be generated for potential graspable\nareas. To generate the initial ground truth for this network,\nwe put the top right ground truth label as the ﬁrst output of\nthis network and put zero if there are not enough graspable\nlabels. Note that no data augmentation and no pre-training\nwere used.\nC. Evaluation\nThe same metric for accuracy was used as in [9], [10],\n[14]. When the difference between the output orientation and\nFig. 6. Examples of the Cornell grasp detection dataset [9].\nFig. 7. One result of our proposed grasp detection method. Top left ﬁgure\nis the output of the ﬁrst STN (4 locations) and top right is the output of\nthe second STN with rotation. Then, the bottom right is the output of the\nlast STN with scaling and ﬁne tuned location. The bottom left is the best\ngraspable conﬁguration among 4 outputs of the last STN.\nthe ground truth orientation is less than 30 o, the Jaccard\nindex was measured between the output rectangle and the\nground truth rectangle. When the Jaccard index is more than\n25%, the output grasp conﬁguration is considered as a good\ngrasp and otherwise bad.\nD. Implementation\nThe original Lenz et al. ’s result was reproduced using\nthe MATLAB code provided by the authors [9] (called\nclassiﬁcation with SAE). We optimized the original code\nfurther so that similar computation speed was able to be\nachieved for larger input images with 400 × 400. We also\nimplemented the work of Lenz et al. using Tensorﬂow and\nreplaced SAE with 4-layer CNN (called classiﬁcation with\nCNN). For direct regression based grasp detection method\nthat is similar to [10], we implemented it using Tensorﬂow\nbased ResNet-32 [6]. Unlike the original work using 3\nchannels (RG and Depth) [10], we implemented the ResNet-\n32 with 7 channel input so that all multimodal information\ncan be used (RGB color, depth, 3 channel surface norm).\nThus, no pre-training was performed for this implementation.\nLastly, our proposed methods using single stage STN and\nmultiple stage STN were implemented using Tensorﬂow. All\nalgorithms were tested on the platform with a single GPU\n(NVIDIA GeForce GTX 1080 Ti), a single CPU (Intel i7-\n7700K 4.20GHz) and 32 GB memory.\nE. Qualitative Results of Proposed Method\nFig. 7 shows an example of the step-by-step result using\nour proposed classiﬁcation based grasp detection method\nusing multiple-stage STN. The ﬁrst ﬁgure illustrates detected\n4 candidate locations from the ﬁrst STN, then rotated can-\ndidates from the second STN for each output of the ﬁrst\nSTN, scaled and location adjusted candidates from the third\nSTN, and ﬁnally chosen one best graspable rectangle from\nthe grasp classiﬁer. Fig. 8 illustrates another example of the\nresult from our proposed method for an object that is difﬁcult\nFig. 8. Another result of our proposed grasp detection method for a difﬁcult\nobject to detect grasp conﬁguration. Top left ﬁgure is the output of the ﬁrst\nSTN (4 locations) and top right is the output of the second STN with\nrotation. Then, the bottom right is the output of the last STN with scaling\nand ﬁne tuned location. The bottom left is the best graspable conﬁguration\namong 4 outputs of the last STN.\nto detect robotic grasp conﬁguration with a single output\nregression model [10]. Thanks to a number of candidates that\nwere generated from the ﬁrst STN, this case also successfully\nidentify robotic grasp conﬁguration. Note that intermediate\nsteps are observable partially (for 4 candidates in this case)\nso that it is easier to identify problems for the network\nthan black-box models. Note that this partially observable\nfeature was helpful to design and train our proposed network\nproperly.\nF . Comparison Results for Proposed Method\nFig. 9 illustrates the grasp conﬁguration outputs of classi-\nﬁcation (SAE), classiﬁcation (CNN), regression (CNN), and\nproposed multiple stage STN based grasp detection methods.\nAs also reported in [10], regression based grasp detection\nmethod yielded average of all good candidates, while clas-\nFig. 9. Four comparison results from classiﬁcation based grasp detection\nusing sliding window with SAE and CNN, regression based grasp detection\nand the proposed classiﬁcation based grasp detection using multi-stage STN.\nTABLE I\nPERFORMANCE FOR DIFFERENT METHODS IN ACCURACY & SPEED . ALL\nMEASURED ON THE SAME PLATFORM WITH THE SAME SIZE INPUT .\nMethod Accuracy (%) Time / Image\nClassiﬁcation (SAE) 76.00 13 sec\nClassiﬁcation (CNN) 82.53 13 sec\nRegression (CNN) 70.67 11.3 msec\nOur Single Stage STN 71.30 13.6 msec\nOur Multiple Stage STN 89.60 23.0 msec\nsiﬁcation based methods yielded good grasp conﬁgurations.\nFig. 10 shows that classiﬁcation (SAE) and regression based\nmethods yielded relatively poor grasp conﬁgurations, while\nour proposed method yielded good grasp conﬁgurations.\nTable I shows that this superior performance of our proposed\nmethod is not just for a few images, but for all test images\nin general. Our method achieved state-of-the-art performance\nwith real-time processing speed.\nV. DISCUSSION AND CONCLUSION\nIn this paper, we proposed a novel classiﬁcation based\nrobotic grasp detection method using our multiple stage STN\nand demonstrated that our proposed method achieved state-\nof-the-art performance in accuracy and real-time computa-\ntion time for relatively high resolution images. Our proposed\nmethod also has merits such as easy integration with robot\ncontrol algorithm for end-to-end training, partially observ-\nable intermediate steps, and easy training without requiring\npre-training with massive amount of data such as ImageNet.\nIn our results, the accuracy for regression based grasp\ndetection method using ResNet-32 can be further improved\nwhen using pre-training. However, in many massive image\ndataset such as ImageNet, depth images are not available\nso that it makes challenging to pre-train a model with\nmultimodal information. Regression based grasp detection\nmethods have merit in terms of computation speed, while\nour proposed method is relatively easy to train without pre-\ntraining.\nFig. 10. Four comparison results from classiﬁcation based grasp detection\nusing sliding window with SAE and CNN, regression based grasp detection\nand the proposed classiﬁcation based grasp detection using multi-stage STN.\nREFERENCES\n[1] A. Sahbani, S. El-Khoury, and P. Bidaud, “An overview of 3D\nobject grasp synthesis algorithms,” Robotics and Autonomous Systems,\nvol. 60, no. 3, pp. 326–336, Mar. 2012.\n[2] J. Bohg, A. Morales, T. Asfour, and D. Kragic, “Data-Driven Grasp\nSynthesis—A Survey,” IEEE Transactions on Robotics, vol. 30, no. 2,\npp. 289–309, Mar. 2014.\n[3] A. Saxena, J. Driemeyer, and A. Y . Ng, “Robotic grasping of novel\nobjects using vision,” The International Journal of Robotics Research,\nvol. 27, no. 2, pp. 157–173, Feb. 2008.\n[4] J. Bohg and D. Kragic, “Learning grasping points with shape context,”\nRobotics and Autonomous Systems , vol. 58, no. 4, pp. 362–377, Apr.\n2010.\n[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in Neural\nInformation Processing Systems 25 , 2012, pp. 1097–1105.\n[6] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for\nImage Recognition,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2016, pp. 770–778.\n[7] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-\ntime object detection with region proposal networks,” in Advances in\nNeural Information Processing Systems 28 , 2015, pp. 91–99.\n[8] J. Redmon and A. Farhadi, “YOLO9000: Better, Faster, Stronger,”\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2017, pp. 6517–6525.\n[9] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting robotic\ngrasps,” The International Journal of Robotics Research , vol. 34, no.\n4-5, pp. 705–724, Apr. 2015.\n[10] J. Redmon and A. Angelova, “Real-time grasp detection using con-\nvolutional neural networks,” in IEEE International Conference on\nRobotics and Automation (ICRA) , 2015, pp. 1316–1322.\n[11] Y . Jiang, S. Moseson, and A. Saxena, “Efﬁcient grasping from RGBD\nimages: Learning using a new rectangle representation,” in IEEE\nInternational Conference on Robotics and Automation (ICRA) , 2011,\npp. 3304–3311.\n[12] A. T. Miller, S. Knoop, H. I. Christensen, and P. K. Allen, “Automatic\ngrasp planning using shape primitives,” in IEEE International Confer-\nence on Robotics and Automation (ICRA) , 2003, pp. 1824–1829.\n[13] B. Le ´on, S. Ulbrich, R. Diankov, G. Puche, M. Przybylski, A. Morales,\nT. Asfour, S. Moisio, J. Bohg, J. Kuffner, and R. Dillmann, “Open-\nGRASP: A toolkit for robot grasping simulation,” in Simulation,\nModeling, and Programming for Autonomous Robots , 2010, pp. 109–\n120.\n[14] S. Kumra and C. Kanan, “Robotic grasp detection using deep con-\nvolutional neural networks,” in IEEE International Conference on\nIntelligent Robots and Systems (IROS) , 2017, pp. 769–776.\n[15] U. Asif, M. Bennamoun, and F. A. Sohel, “RGB-D Object Recognition\nand Grasp Detection Using Hierarchical Cascaded Forests,” IEEE\nTransactions on Robotics , vol. 33, no. 3, pp. 547–564, May 2017.\n[16] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\n“Spatial transformer networks,” in Advances in Neural Information\nProcessing Systems 28 , 2015, pp. 2017–2025.\n[17] R. Girshick, J. Donahue, and T. Darrell, “Rich feature hierarchies\nfor accurate object detection and semantic segmentation,” in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) ,\n2014, pp. 580–587.\n[18] R. Girshick, “Fast R-CNN,” in IEEE International Conference on\nComputer Vision (ICCV) , 2015, pp. 1440–1448.\n[19] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You Only Look\nOnce: Uniﬁed, Real-Time Object Detection,” in IEEE Conference on\nComputer Vision and Pattern Recognition , 2016, pp. 779–788.\n[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\nA. C. Berg, “SSD: Single Shot MultiBox Detector,” in European\nConference on Computer Vision (ECCV) , 2016, pp. 21–37.\n[21] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature, vol.\n521, no. 7553, pp. 436–444, May 2015.\n[22] Z. Wang, Z. Li, B. Wang, and H. Liu, “Robot grasp detection\nusing multimodal deep convolutional neural networks,” Advances in\nMechanical Engineering, vol. 8, no. 9, pp. 1–12, Sept. 2016.\n[23] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training\nof deep visuomotor policies,” Journal of Machine Learning Research ,\nvol. 17, pp. 1–40, Apr. 2016.",
  "topic": "GRASP",
  "concepts": [
    {
      "name": "GRASP",
      "score": 0.9561936855316162
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7477450966835022
    },
    {
      "name": "Computer science",
      "score": 0.7434316873550415
    },
    {
      "name": "Observability",
      "score": 0.6351528167724609
    },
    {
      "name": "Computation",
      "score": 0.6287571787834167
    },
    {
      "name": "Transformer",
      "score": 0.5114316344261169
    },
    {
      "name": "Deep learning",
      "score": 0.4745821952819824
    },
    {
      "name": "Machine learning",
      "score": 0.44381392002105713
    },
    {
      "name": "Robot",
      "score": 0.4420970678329468
    },
    {
      "name": "Artificial neural network",
      "score": 0.42531126737594604
    },
    {
      "name": "Computer vision",
      "score": 0.3635866641998291
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3619316816329956
    },
    {
      "name": "Engineering",
      "score": 0.14123883843421936
    },
    {
      "name": "Algorithm",
      "score": 0.08777397871017456
    },
    {
      "name": "Voltage",
      "score": 0.07728734612464905
    },
    {
      "name": "Mathematics",
      "score": 0.0648573637008667
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Applied mathematics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}