{
  "title": "From promise to practice: challenges and pitfalls in the evaluation of large language models for data extraction in evidence synthesis",
  "url": "https://openalex.org/W4405636647",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2555150227",
      "name": "Gerald Gartlehner",
      "affiliations": [
        "RTI International",
        "Universität für Weiterbildung Krems"
      ]
    },
    {
      "id": "https://openalex.org/A3094506338",
      "name": "Leila Kahwati",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A4223519657",
      "name": "Barbara Nussbaumer-Streit",
      "affiliations": [
        "Universität für Weiterbildung Krems"
      ]
    },
    {
      "id": "https://openalex.org/A2101481273",
      "name": "Karen Crotty",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A2591835652",
      "name": "Rainer Hilscher",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A1639937765",
      "name": "Shannon Kugley",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A2128664431",
      "name": "Meera Viswanathan",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A1746725942",
      "name": "Ian Thomas",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A4321561139",
      "name": "Amanda Konet",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A2152352490",
      "name": "Graham Booth",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A1965550992",
      "name": "Robert Chew",
      "affiliations": [
        "RTI International"
      ]
    },
    {
      "id": "https://openalex.org/A2555150227",
      "name": "Gerald Gartlehner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3094506338",
      "name": "Leila Kahwati",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223519657",
      "name": "Barbara Nussbaumer-Streit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101481273",
      "name": "Karen Crotty",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2591835652",
      "name": "Rainer Hilscher",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1639937765",
      "name": "Shannon Kugley",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128664431",
      "name": "Meera Viswanathan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1746725942",
      "name": "Ian Thomas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4321561139",
      "name": "Amanda Konet",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152352490",
      "name": "Graham Booth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965550992",
      "name": "Robert Chew",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4367368990",
    "https://openalex.org/W4398144060",
    "https://openalex.org/W4386520339",
    "https://openalex.org/W4391993820",
    "https://openalex.org/W4398201968",
    "https://openalex.org/W4391836235",
    "https://openalex.org/W4392343921",
    "https://openalex.org/W4391723759",
    "https://openalex.org/W4391536321",
    "https://openalex.org/W3169933308",
    "https://openalex.org/W2770117783",
    "https://openalex.org/W4399812563",
    "https://openalex.org/W4394966486",
    "https://openalex.org/W2787894218",
    "https://openalex.org/W4311313465",
    "https://openalex.org/W2013745961",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2964179635"
  ],
  "abstract": null,
  "full_text": "BMJ Evidence- Bas ed Medicine Month 2024 |  volume 0 | number 0 | 1\nFrom promise to practice: challenges and pitfalls in \nthe evaluation of large language models for data \nextraction in evidence synthesis\nGerald Gartlehner    ,1,2 Leila Kahwati    ,1 \nBarbara Nussbaumer- \nStreit    ,2 Karen Crotty,1 \nRainer Hilscher,1 Shannon Kugley,1 Meera Viswanathan    ,1 \nIan Thomas,1 Amanda Konet,1 Graham Booth,1 Robert Chew1\nAnalysis\n1RTI International, Research \nTriangle Park, North Carolina, \nUSA\n2Department for Evidence- \nb\nased Medicine and \nEvaluation, University for \nContinuing Education Krems, \nKrems, Austria\nCorrespondence to: \nDr Gerald Gartlehner; \n \ngerald.\n \ng\nartlehner@\n \ndonau-\n \nuni.\n \nac.\n \nat\n10.1136/bmjebm-2024-113199\nTo cite: Gartlehner G, \nKahwati L, Nussbaumer-\n \nStr\neit B, et al. BMJ Evidence-\n \nBas\ned Medicine Epub ahead \nof print: [please include Day \nMonth Year]. doi:10.1136/\nbmjebm-2024-113199\n© Author(s) (or their \nemployer(s)) 2024. Re-\n use \npermitted under CC \nB\nY-\n NC. No commercial \nre-\n use. See rights and \npermissions. Published by \nBMJ Group.\nBackground\nThe introduction of generative large language \nmodels (LLMs) has led to the exploration of their \nuse in evidence synthesis tasks, such as literature \nsearches,\n1 screening,2 3 risk of bias assessment, 4 5 \ndata extraction,6–9 statistical analysis8 and writing \nplain language summaries. 10 Of these tasks, data \nextraction (ie, transferring data from reports \nof primary studies into standardised tables) is a \ncrucial, yet time-\n consuming11 and error-  prone12 \nstep in the evidence synthesis process.\nUnlike earlier natural language processing \ntechnologies used for data extraction, LLMs do not \nrequire labelled training data, making them acces-\nsible to users without a technical background. To \ndate, few studies have assessed the use of LLMs \nfor data extraction, and these studies have yielded \nmixed results compared with human reference \nstandards.\n1 6 7 13 14\nIn this commentary, we discuss the challenges \nand pitfalls researchers face when assessing the \nperformance of an LLM for data extraction in vali-\ndation studies. Our insights stem from an initial \nproof of concept study,\n7 and ongoing workflow \nvalidation study focused on employing an LLM \nfor semi-\n automated data extraction.15 These chal-\nlenges include selecting an appropriate reference \nstandard, avoiding data contamination, choosing \nsuitable outcome metrics and defining what qual-\nifies as an error. We will also reflect on prompt \nengineering and the practical challenges associ-\nated with using LLMs in validation research.\nThe choice of validation design \ndetermines the relevance of results\nEvaluating LLMs for data extraction in evidence \nsynthesis involves two distinct approaches that \ninvestigators must consider at the outset of plan-\nning a validation study: model validation or work-\nflow validation in real-\n world review settings.\nModel \nvalidation studies examine the perfor-\nmance and reliability of an LLM under specific, \ncontrolled conditions. These studies are essen-\ntial for evaluating the model’s ability to extract \ndifferent types of data and identifying its strengths \nand limitations. They determine whether data \nextraction using an LLM is feasible and typically \nrely on carefully controlled datasets to compare \nhuman versus machine performance. If the selec-\ntion of example texts is representative, model vali-\ndation studies can provide generalisable results.\n16\nIn contrast, workflow validation studies inte-\ngrate the LLM into the workflow of an ongoing \nsystematic review, providing a detailed perspective \non the model’s practical effectiveness, efficiency \nand utility. These studies can be designed as a study \nwithin a review (SWAR),\n17 allowing researchers to \nobserve and document the model’s performance in \nreal-\n world scenarios, extending beyond theoret\n-\nical capabilities or controlled environments.\nModel validation studies are less expensive and \nserve as necessary prerequisites for workflow vali-\ndation studies. While workflow validation studies \nare more complex, they provide richer insights \nand ultimately determine the utility of an LLM. \nFor instance, an LLM might match human accu-\nracy in data extraction tasks yet prove unsuitable \nfor practical application due to the time required \nto successfully integrate it into the workflow. A \nlimitation of workflow validation studies is their \npotentially restricted generalisability. This limita-\ntion arises from the unique characteristics and \npotential biases present in the specific study publi-\ncations used for a given review. The performance \nand correctness of an LLM in extracting data from \none set of documents may not necessarily trans-\nlate to equal performance across different domains \nor study designs.\nThe choice of the reference standard \nmay lead to benchmark bias or data \ncontamination\nHuman- led data extractions usually serve as the \nreference standard to evaluate an LLM’s perfor\n-\nmance in data extraction. However, two potential \nbiases—benchmark bias and data contamination—\nneed to be considered.\nBenchmark bias, a form of information or clas-\nsification bias,\n18 arises when the reference used \nin a validation study is imperfect. In the case of \nhuman-\n led data extraction, research indicates that \nup to 63% of studies extracted by humans contain \nat least one data extraction error\n.12 In our recent \nproof-\n of-\n concept study\n, Claude 2 made one major \nand five minor errors (see definitions in table 1) \nbut detected 21 minor errors in the human-\n led \nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://ebm.bmj.com/Downloaded from 20 December 2024. 10.1136/bmjebm-2024-113199 on BMJ EBM: first published as \nBMJ Evidence- Bas ed Medicine Month 2024 |  volume 0 | number 0 | 2\nAnalysis\nreference standard. 7 Thus, considering human data extraction \nfrom published systematic reviews as the gold standard intro-\nduces bias by presuming discrepancies are errors made by the \nLLM, potentially underestimating the model’s capabilities. There-\nfore, refining the reference standard in validation studies of data \nextraction is essential.\nIn validation studies that do not use a labelled corpus (ie, a \ndataset where each data item has been annotated with specific \nlabels, serving as a reference to assess the accuracy), one effective \nmethod is verifying each discrepancy between human and LLM \nextractions against the original study report, ideally by a blinded \nadjudicator. This approach can expose human errors, allowing for \ncorrections in the reference standard and providing a more accu-\nrate assessment of LLM performance.\nData contamination occurs if the dataset used for performance \nevaluation also contributed to training the LLM.\n19 For example, \nusing data from open-\n access systematic reviews to assess an \nLLM’s data extraction capability risks contamination if the model \npreviously encountered and ‘memorised’ this data during training. \nThis could artificially enhance its performance on familiar mate\n-\nrial. The practical challenge for investigators is that developers of \nLLMs typically do not disclose their training data. However, it is \nconceivable that databases with numerous open-\n access system\n-\natic reviews, such as the Cochrane Library, may be used to train \nLLMs. Similarly, publicly available labelled corpora of study data, \nsuch as Evidence-\n based Medicine-\n Natural Language Processing,20 \ncould be used, either intentionally or unintentionally, for LLM \ntraining.\nAlthough the extent to which data contamination biases LLM \nevaluations is uncertain, investigators can implement several \nstrategies to minimise this risk. These strategies include using \ndatasets from unpublished reviews, subscription-\n based \nsystematic \nreviews or studies published after the LLM’s most recent training \nupdate. For instance, as of this writing, OpenAI’s GPT-\n 4o \nhas been \ntrained on data up to October 2023, and Claude 3 up to August \n2023. Datasets from open-\n access reviews published after an \nLLM’s training date should not be subject to data contamination. \nHowever\n, these strategies may, in turn, limit the generalisability \nof the validation study’s results. Additionally, the risk of data \ncontamination is likely minimal for workflow validation studies, \nwhich typically involve a prospective comparison of extracted \ndata, reducing the likelihood of prior model exposure to the data.\nChallenges in defining incorrect data extractions \nand adjudicating differences between extracted data \nelements\nEven with a reference standard corrected for human errors, adju-\ndicating differences between a fully human extraction process and \none assisted by an LLM remains challenging. The definition of \n‘correct’ can vary even among human extractors. Data extraction \noften involves decisions about format, style, relevance, the level \nof detail and reasonable inferences from ambiguous data.\nValidation studies typically focus on correctness of extraction \nof individual data elements, which is easiest to assess for data \nelements that are narrowly defined. Extractions of data elements \nthat are defined broadly (eg, ‘study population’) may vary between \nhumans and LLMs, but these are usually not factual errors, \ninstead, they are justifiable interpretations of the contents of the \noriginal study report. A similar variability in data extractions \ncan also be seen when comparing human-\n to-\n human extractions. \nOne way to approach this issue is to conceptualise differences in \ndata extractions as concordant and discordant rather than simply \ncorrect and incorrect. Our team has employed the following \nworking \ndefinition for concordance: Concordance is factual \ncongruence of extracted data items, even if there are variations \nin style, presentation or length between the two data extractions.\nAnother challenge is that data extractors are often required to \nclassify, summarise or record data in ways that involve judgement. \nFor instance, identifying the active ingredients of a behavioural \nintervention or classifying a specific outcome into a domain (eg, \nquality of life, pain, function). In such cases, the ‘correct’ answer \nis often determined by the judgement of the most senior reviewer, \nnot the study report. Additionally, extractors sometimes calculate \nvalues based on data reported in study reports, such as deter-\nmining the percentage of women in the study population from the \nreported frequency of women and the overall sample size. These \ncalculations occasionally require judgement, such as deciding \nwhich number to use as a denominator for the overall study popu-\nlation (eg, number randomised or number analysed).\nTable 1 C lassification of differences when assessing LLM-  a ssisted data extraction processes\nType of difference Definitions\nMissed or omitted data Data that were available in the study report but were either missed or omitted by the extraction process.\nFabricated data Data that were not available in the study report were either inaccurately filled in by human data extractors or \nerroneously generated (hallucinated) by the LLM and not revised/removed by the human conducting the correctness \ncheck.\nMisallocated data Data that were provided in the study report were allocated to the wrong data element field.\nIncorrect calculations Mathematically incorrect calculations of data elements based on information provided in the study report, including \nrounding errors.\nDifference in level of detail The level of detail is the only difference and is not relevant to drawing conclusions.\nOther Optional ‘other’ field if none of the above apply\nPotential impact of difference\nMajor error This error significantly compromises the correctness of the data, and, if uncorrected, could lead to erroneous \nconclusions; for example, grossly incorrect calculations, misallocated data that results in a different interpretation or \nuncorrected hallucinations of the LLM that result in a new or different interpretation of the data.\nMinor error This error is less severe than a major error and may or may not impact interpretation of the existing data; for example, \nsmall calculation errors or rounding errors that do not critically affect the data’s overall utility,\nInconsequential difference This difference would not impact the interpretation of the data; for example, additional or alternative language \ndescribing study population or the intervention that doesn’t inherently alter the meaning.\nLLM, large language model.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://ebm.bmj.com/Downloaded from 20 December 2024. 10.1136/bmjebm-2024-113199 on BMJ EBM: first published as \nBMJ Evidence- Bas ed Medicine Month 2024 |  volume 0 | number 0 | 3\nAnalysis\nOur approach to assessing correctness of data extraction has \nevolved from merely counting the ‘errors’ made by an LLM-\n \nassisted process to evaluating the factual concordance of the data.\nWhen extracted data do not factually agree, we consult the \noriginal study report to adjudicate the difference. W\ne also classify \nthe type and severity of the difference to enhance our prompt \nengineering and provide better guidance to human and LLM \nextractors for future extractions. Our current classification scheme \nis depicted in table 1.\nSelecting appropriate outcomes and defining the unit \nof analysis\nThe choice of validation design—whether model validation or \nworkflow validation—determines the types of outcomes that can \nbe assessed. Model validation studies primarily focus on correct-\nness of data extraction. In contrast, the prospective design of \nworkflow validation studies allows for the evaluation of addi-\ntional relevant outcomes which determine the utility of a data \nextraction tool, such as task time or the downstream effects of \ndata extraction errors on conclusions.\nAccuracy and unit of analysis\nVarious metrics are available to quantify correctness of an LLM’s \ndata extraction. Table 2 summarises the advantages and disadvan-\ntages of four commonly used metrics for assessing the machine \nlearning performance: precision, recall, accuracy and F1 score. \nBecause the performance of LLMs can vary substantially across \ndata types, an overall quantitative evaluation is usually less \nmeaningful than an evaluation by data category (eg, participants \ncharacteristics, outcomes) or individual items.\n9\nClosely related is the choice of the unit of analysis. Data items \nextracted for evidence synthesis vary in complexity, ranging \nfrom simple single numbers (eg, trial registration number) to \nmore complex elements that consist of multiple values (eg, effect \nestimates with 95% CIs). Consequently, composite data elements \nare more susceptible to extraction errors than single-\n value data \nelements. For instance, consider a risk ratio with a 95% CI. If the \npoint estimate and the upper and lower bounds of the CI are incor\n-\nrect, these would constitute three separate errors and potentially \ndistort the results. However, if the unit of analysis is defined as \nthe composite data item (ie, the point estimate with the 95% CI), \nthese multiple incorrect values would be counted as a single error. \nTherefore, defining data items and the appropriate unit of anal-\nysis in the study protocol is important to ensure consistent error \ncounts across data elements.\nInter-rater reliability\nKappa- type statistics (eg, Cohen’s kappa or Gwet’s Agreement \nCoefficient 1 (A\nC1)) are useful metrics for assessing inter-\n rater \nagreement. These statistics assume that some proportion of the \nobserved agreement between raters is due to chance rather than \ntrue agreement. In validation studies, chance agreement can be \nproblematic when extractors choose between predefined cate\n-\ngories, such as distinguishing between randomised and non-\n \nrandomised \nstudies. Although kappa-\n type \nstatistics can be applied \nto qualitative data that vary in levels of detail, such as descriptions \nof study populations, they often require simplification of complex \ninformation into categories. This process and the resulting statis-\ntics often fail to capture the nuances of interpretation that are \ncrucial in interpretation of qualitative data. Additionally, chance \nagreements are not a concern when extracting numerical data \nfrom study reports. This is because the probability of randomly \nguessing the exact numerical value that matches the true data is \nextremely low.\nTime spent on task\nA primary objective of employing LLMs for data extraction is \nto enhance the efficiency of the systematic review process. Time \nTable 2 C ommonly used metrics for quantifying LLM accuracy when used for data extraction\nMetric Definition Range of results Strength Limitation\nPrecision (=positive \npredictive value)\nThe accuracy of an LLM on \nthe data items for which it \nreturned an extracted value.\n \nTP(\nTP+FP\n)\n \n0–1 (wher\ne 0 is poor and \n1 is excellent); results can \nalso be expressed as a \npercentage\nWidely used Can be misleading if considered \nindependently.\nDoes not take missed data \ninto consideration and can be \nhigh even if the LLM missed \ninformation on a substantial \nnumber of data elements (ie, it has \nlow recall).\nRecall (=sensitivity) The ability of an LLM to \ncorrectly extract available \ndata items.\n \nTP(\nTP+FN\n)\n \n0–1 (wher\ne 0 is poor and \n1 is excellent); results can \nalso be expressed as a \npercentage\nWidely used Can be misleading if considered \nindependently.\nDoes not reflect the extent of \nfabricated data for items for \nwhich no data were available (ie, \nhallucinated data).\nAccuracy (=percent \nagreement)\nThe percentage of correct \ndata extractions, out of all \ndata elements.\n \n(\nTP+TN\n)\n(\nTP+FP+TN+FN\n) × 100\n \n0–100 (wher\ne 0 is poor and \n100 is excellent); results \ncan also be expressed as a \ndecimal\nStraightforward summary of an \nLLMs performance in correctly \nextracting data and correctly \nidentifying elements for which no \ndata are reported.\nFails to distinguish between \nfabricated data and missed data.\nMay not be a reliable metric when \ndealing with imbalanced data \ndistributions.\nF1 score An evaluation metric that \ncombines the precision and \nrecall (via harmonic mean) \ninto a single statistic.\n \n2 ∗\n(\nPrecision∗Recall\n)\n(\nPrecision+Recall\n)\n \n0–1 (wher\ne 0 is poor and 1 \nis excellent)\nThe greater the disparity \nbetween precision and \nrecall scores, the lower the \nF1 score\nEnsures that both fabricated data \n(false positives) and missed data \n(false negatives) are considered \nseparately.\nMore sensitive to imbalanced data \nthan accuracy.\nNot informative about the \ndistribution of errors, as it \nprovides a single value that \nsummarises the model’s \nperformance across both precision \nand recall.\nFN, false negatives: the number of data items missed or incorrectly extracted by the LLM from the full text publication; FP , false positives: the numberof data \nitems for which the LLM provided fabricated data when no data were available in the full text publication (i.e., hallucinated data); LLM, large language model; TN, \ntruenegatives: the number of data items that the LLM correctly identified as not available in the full text publication; TP , true positives: the number of data items \ncorrectly extracted bythe LLM from the full text publication.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://ebm.bmj.com/Downloaded from 20 December 2024. 10.1136/bmjebm-2024-113199 on BMJ EBM: first published as \nBMJ Evidence- Bas ed Medicine Month 2024 |  volume 0 | number 0 | 4\nAnalysis\nspent on task is a relevant outcome for evaluating this effi-\nciency. Although LLMs demonstrate rapid data extraction capa-\nbilities from study reports, additional time is needed for prompt \nengineering—a requirement absent in human-\n only data extrac\n-\ntion approaches. To accurately assess whether integrating LLMs \nenhances efficiency, it is essential to measure the time invested \nin all associated tasks. Some tasks, such as prompt engineering, \nare time-\n intensive only once and may become less demanding as \nprompt libraries develop.\nImpact on conclusions: determining downstream effects of data \nextraction inaccuracies\nThe consequences of inaccuracies in different data elements \ncan vary, potentially impacting a review’s conclusions differ-\nently. For instance, the omission of data from a critical outcome \ncould significantly sway the overall conclusion. Conversely, \nthe absence of data from a single, inconsequential study might \nhave a negligible impact on the overarching results and conclu-\nsions. Therefore, in-\n depth case studies comparing different ways \nof incorporating LLMs with human-\n only methodologies should \nextend beyond evaluating the correctness of data extraction \nalone. They should delve into how any inaccuracies might affect \nthe synthesis (eg, meta-\n analyses) and the ultimate conclusions of \na systematic review\n.\nThe prompts determine the output of the LLM\nPrompt engineering involves designing text inputs (prompts) \nwith the objective of accurate and succinct output from LLMs. \nThe prompt provided to an LLM directly impacts response correct-\nness, completeness and format. For data extraction, if the prompt \ndoes not contain sufficient information and context to complete \na task, the LLM is more likely to generate incomplete, erroneous \nor hallucinated responses. For example, prompts for LLM data \nextraction tend to include instructions, field definitions and the \ndocument text as part of the prompt. Models with shorter context \nlengths may not be able to fit the entire document in the prompt, \nrequiring additional text parsing that can reduce performance.\n13\nWhen developing prompts for LLM-  assisted data extraction, \niteration on the prompt text is necessary for obtaining accu\n-\nrate results. 9 Before extracting information, researchers should \nconduct a pilot phase in which prompts are developed, tested and \nevaluated on a subset of articles. This will prevent the temptation \nto develop unique prompts for each article that may not generalise \nwell to new articles.\nAlthough the pilot phase can be done informally, setting up an \nevaluation framework will help researchers quantitatively deter-\nmine whether changes to prompts are improving outcomes or \nnot. At a minimum, the evaluation criteria used in the pilot phase \nshould mimic the criteria intended for the full study to provide the \nresearch team with the feedback necessary to iterate on prompts. \nOther criteria that may be useful to consider testing for in the pilot \nphase include response format, length and level of detail.\nTest-retest reliability\nTest- retest reliability is a crucial component of model validation \nstudies for LLMs, as it measures output consistency over time. \nT\no ensure accurate test-\n retest assessments in data extraction, it \nis vital to use the same prompts and study reports for the data \nextraction process.\nLLMs \ninherently incorporate stochastic elements, enabling \nthem to produce varied and creative responses. They predict the \nlikelihood of word sequences using the context from preceding \nwords. During this process, LLMs frequently employ sampling \nmethods to select subsequent words in a sequence. Consequently, \neven with identical prompts, the model can produce different \noutputs on different runs.\n9\nFor example, in our proof-  of-  concept study , testing an LLM’s \nperformance in data extraction, the number of errors was similar \nbetween the original run and a repeated run 4\n weeks later with the \nsame prompts and study reports. However\n, the errors occurred in \ndifferent data elements in five out of six cases.7\nPractical challenges\nValidation studies for data extraction also face various practical \nobstacles. One significant obstacle is the rapid pace of LLM devel-\nopment. By the time a study is completed, the LLM under eval-\nuation may have been replaced by a newer model. To address \nthis, investigators can design workflow validation as an adaptive \nSWAR, allowing the option to switch to a new model if it offers \nadvantages such as a larger context window. Second, LLM rate \nrestrictions, which vary based on factors like institutional traffic, \ncan severely restrict workflow. Subscription-\n based versions of the \nLLM or Application Programming Interface usually help mitigate \nthese rate restrictions. Third, human variation—differences in \nsystematic review experience, data extraction detail, team differ\n-\nences in validating data extractions and proficiency in engineering \nprompts—can significantly impact validation study results. Varia-\ntion in human expertise and team differences should be carefully \nconsidered when interpreting workflow validation study results. \nLarge SWARs with multiple review teams can provide insights into \nthe variability of the human reference standard. Furthermore, the \nchoice of the topic for validation can impact results. Randomised \ntrials of simple pharmacologic interventions may be easier for \nboth humans and machines to accurately extract than studies of \nbehavioural or complex implementation interventions, or non-\n \nrandomised designs. The quality of reporting in study publications \ncan significantly affect the accuracy of data extraction, both by \nhumans and machines. However\n, it is important to explore vari-\nations in correctness and utility across the spectrum of evidence \nsynthesis topics and study designs.\nContributors\n GG, KC, MV and LK contributed to \nconceptualisation. GG and KC contributed to funding acquisition. \nGB contributed to project administration. GG, LK, BN-\n S, RC and \nAK contributed to writing original draft. KC, MV\n, LK, RH, BN-\n S, \nSK, IT\n, AK and GB contributed to review and revisions of draft. \nWe used AI for editing purposes.\nFunding\n The underlying research for this commentary was \nsupported by internal resources of RTI International (https://\nwww\n.rti.org) through the Innovation Fund, and Cochrane Austria \n(BNS). Effort from LK and MV was supported by the RTI Fellows \nProgram.\nCompeting interests\n The authors declare no competing interests. \nThey emphasise that they have no financial investments in \ncompanies developing LLMs or commercial software using LLMs, \nnor do they collaborate with LLM providers.\nP\natient consent for publication\n Not applicable.\nEthics approval\n Not applicable.\nProvenance and peer review\n Not commissioned; externally peer \nreviewed.\nOpen access\n This is an open access article distributed in \naccordance with the Creative Commons Attribution Non \nCommercial (CC B\nY-\n NC 4.0) license, which permits others to \nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://ebm.bmj.com/Downloaded from 20 December 2024. 10.1136/bmjebm-2024-113199 on BMJ EBM: first published as \nBMJ Evidence- Bas ed Medicine Month 2024 |  volume 0 | number 0 | 5\nAnalysis\ndistribute, remix, adapt, build upon this work non- commercially , \nand license their derivative works on different terms, provided \nthe original work is properly cited, appropriate credit is given, \nany changes made indicated, and the use is non-\n commercial. \nSee: http://creativecommons.org/licenses/by-nc/4.0/.\nORCID iDs\nGerald Gartlehner \nhttp://orcid.org/0000-0001-5531-3678\nLeila Kahwati http://orcid.org/0000-0001-6963-2848\nBarbara Nussbaumer-\n \nStreit http://orcid.org/0000-0001-7622-\n \n843X\nMeera Viswanathan \nhttp://orcid.org/0000-0003-3486-8126\nReferences\n 1 Qureshi R , Shaughnessy D, Gill KAR, et al. Are ChatGPT and large \nlanguage models “the answer” to bringing us closer to systematic review \nautomation? Syst Rev 2023;12:72. \n \n2\n Tran \nV-\n \nT, Gartlehner G, Yaacoub S, et al. Sensitivity and Specificity \nof Using GPT-\n 3.5 Turbo Models for Title and Abstract Screening in \nSystematic Reviews and Meta-\n analyses. \nAnn Intern Med 2024;177:791–9. \n \n3\n Cai X\n, Geng Y, Du Y, et al. Utilizing chatgpt to select literature for meta-\n \nanalysis shows workload reduction while maintaining a similar recall level \nas manual curation. \nEpidemiology [Preprint] 2023. \n \n4\n Hasan B\n, Saadi S, Rajjoub NS, et al. Integrating large language models in \nsystematic reviews: a framework and case study using ROBINS-\n I for risk of \nbias assessment. \nBMJ EBM 2024;29:394–8. \n \n5\n Lai H\n, Ge L, Sun M, et al. Assessing the Risk of Bias in Randomized \nClinical Trials With Large Language Models. JAMA Netw Open \n2024;7:e2412687. \n \n6\n Dagdelen J\n, Dunn A, Lee S, et al. Structured information extraction from \nscientific text with large language models. Nat Commun 2024;15:1418. \n \n7\n Gartlehner G\n, Kahwati L, Hilscher R, et al. Data extraction for evidence \nsynthesis using a large language model: A proof-\n of-\n concept study\n. Res \nSynth Methods 2024;15:576–89. \n \n8\n Reason T\n, Benbow E, Langham J, et al. Artificial Intelligence to \nAutomate Network Meta-\n Analyses: Four Case Studies to Evaluate the \nP\notential Application of Large Language Models. Pharmacoecon Open \n2024;8:205–20. \n \n9\n Graziozi S\n, Campbell F, Kapp C, et al. Exploring the use of a large \nlanguage model for data extraction in systematic reviews. 2024. Available: \nhttps://arxiv.org/abs/2405.14445 [Accessed 12 Sep 2024].\n \n10 Ovelman C\n, Kugley S, Gartlehner G, et al. The use of a large language \nmodel to create plain language summaries of evidence reviews in \nhealthcare: A feasibility study. Cochrane Ev Synth Methods 2024;2:e12041. \n \n11 Nussbaumer-\n \nStreit B, Ellen M, Klerings I, et al. Resource use during \nsystematic review production varies widely: a scoping review. J Clin \nEpidemiol 2021;139:287–96. \n \n12\n Mathes T\n, Klaßen P, Pieper D. Frequency of data extraction errors and \nmethods to increase data extraction quality: a methodological review. BMC \nMed Res Methodol 2017;17:152. \n 13  Konet A , Thomas I, Gartlehner G, et al. Performance of two large language \nmodels for data extraction in evidence synthesis. Res Synth Methods \n2024;15:818–24. \n \n14\n Mahmoudi H\n, Chang D, Lee H, et al. A Critical Assessment of Large \nLanguage Models for Systematic Reviews: Utilizing ChatGPT for Complex \nData Extraction. SSRN J 2024. \n \n15\n Gartlehner G\n. SWAR 28: Semi-\n automated data extraction for evidence \nsyntheses using Claude 2. 2024.\n \n16\n Trevor Hastie RT\n, Friedman J. The Elements of Statistical Learning. 2nd \nedn. New York, NY: Springer, 2009.\n \n17\n Devane D\n, Burke NN, Treweek S, et al. Study within a review (SWAR). J \nEvid Based Med 2022;15:328–32. \n \n18\n Schmidt RL\n, Factor RE. Understanding sources of bias in diagnostic \naccuracy studies. Arch Pathol Lab Med 2013;137:558–65. \n \n19\n Carlini N\n, Ippolito D, Jagielski M, et al. Quantifying memorization across \nneural language models. 2022.\n \n20\n Nye B\n, Li JJ, Patel R, et al. A corpus with multi-\n level annotations of \npatients, interventions and outcomes to support language processing \nfor medical literature. Proceedings of the 56th Annual Meeting of the \nAssociation for Computational Linguistics (V\nolume 1); Stroudsburg, PA, \nUSA, Melbourne, Australia. \nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://ebm.bmj.com/Downloaded from 20 December 2024. 10.1136/bmjebm-2024-113199 on BMJ EBM: first published as ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5221310257911682
    },
    {
      "name": "Data science",
      "score": 0.46893930435180664
    },
    {
      "name": "Data extraction",
      "score": 0.4507350027561188
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.4142783284187317
    },
    {
      "name": "Management science",
      "score": 0.3281603455543518
    },
    {
      "name": "Engineering",
      "score": 0.2019406259059906
    },
    {
      "name": "MEDLINE",
      "score": 0.18577969074249268
    },
    {
      "name": "Political science",
      "score": 0.15642541646957397
    },
    {
      "name": "Chemistry",
      "score": 0.1526777148246765
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I132976966",
      "name": "Universität für Weiterbildung Krems",
      "country": "AT"
    },
    {
      "id": "https://openalex.org/I180297670",
      "name": "RTI International",
      "country": "US"
    }
  ]
}