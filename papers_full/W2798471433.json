{
    "title": "Token-level and sequence-level loss smoothing for RNN language models",
    "url": "https://openalex.org/W2798471433",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2798605389",
            "name": "Maha Elbayad",
            "affiliations": [
                "Laboratoire Jean Kuntzmann",
                "Centre Inria de l'Université Grenoble Alpes",
                "Université Grenoble Alpes",
                "Laboratoire d'Informatique de Grenoble",
                "Institut polytechnique de Grenoble",
                "Centre National de la Recherche Scientifique"
            ]
        },
        {
            "id": "https://openalex.org/A2237966808",
            "name": "Laurent Besacier",
            "affiliations": [
                "Laboratoire d'Informatique de Grenoble",
                "Institut polytechnique de Grenoble",
                "Université Grenoble Alpes",
                "Centre Inria de l'Université Grenoble Alpes",
                "Laboratoire Jean Kuntzmann",
                "Centre National de la Recherche Scientifique"
            ]
        },
        {
            "id": "https://openalex.org/A2145238836",
            "name": "Jakob Verbeek",
            "affiliations": [
                "Institut polytechnique de Grenoble",
                "Centre National de la Recherche Scientifique",
                "Université Grenoble Alpes",
                "Laboratoire d'Informatique de Grenoble",
                "Laboratoire Jean Kuntzmann",
                "Centre Inria de l'Université Grenoble Alpes"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2950304420",
        "https://openalex.org/W2964091467",
        "https://openalex.org/W1987937363",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2775304348",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963248296",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2041334360",
        "https://openalex.org/W2131494463",
        "https://openalex.org/W2963443335",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2950635152",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963223306",
        "https://openalex.org/W2508728158",
        "https://openalex.org/W2594978815",
        "https://openalex.org/W2962970071",
        "https://openalex.org/W2133459682",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W2963962369",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2964325005",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2158349948",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2963084599",
        "https://openalex.org/W4293714597",
        "https://openalex.org/W2419597278",
        "https://openalex.org/W2581377246",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W3098341425",
        "https://openalex.org/W2552161745",
        "https://openalex.org/W1850531616",
        "https://openalex.org/W2626177496",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2952840881",
        "https://openalex.org/W2953022248",
        "https://openalex.org/W2951590222",
        "https://openalex.org/W4297798436",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2953022181",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2171361956",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2952122856",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2409027918",
        "https://openalex.org/W2302086703",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2210838531",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W854541894",
        "https://openalex.org/W2176263492",
        "https://openalex.org/W2950178297",
        "https://openalex.org/W2560645892",
        "https://openalex.org/W2407414618",
        "https://openalex.org/W2575842049",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2031489346",
        "https://openalex.org/W648786980",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W4301230920",
        "https://openalex.org/W2250473257"
    ],
    "abstract": "Despite the effectiveness of recurrent neural network language models, their\\nmaximum likelihood estimation suffers from two limitations. It treats all\\nsentences that do not match the ground truth as equally poor, ignoring the\\nstructure of the output space. Second, it suffers from \"exposure bias\": during\\ntraining tokens are predicted given ground-truth sequences, while at test time\\nprediction is conditioned on generated output sequences. To overcome these\\nlimitations we build upon the recent reward augmented maximum likelihood\\napproach \\\\ie sequence-level smoothing that encourages the model to predict\\nsentences close to the ground truth according to a given performance metric. We\\nextend this approach to token-level loss smoothing, and propose improvements to\\nthe sequence-level smoothing approach. Our experiments on two different tasks,\\nimage captioning and machine translation, show that token-level and\\nsequence-level loss smoothing are complementary, and significantly improve\\nresults.\\n",
    "full_text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2094–2103\nMelbourne, Australia, July 15 - 20, 2018.c⃝2018 Association for Computational Linguistics\n2094\nToken-level and sequence-level loss smoothing for RNN language models\nMaha Elbayad1,2 Laurent Besacier1 Jakob Verbeek2\nUniv. Grenoble Alpes, CNRS, Grenoble INP, Inria, LIG, LJK, F-38000 Grenoble France\n1 firstname.lastname@univ-grenoble-alpes.fr\n2 firstname.lastname@inria.fr\nAbstract\nDespite the effectiveness of recurrent neu-\nral network language models, their max-\nimum likelihood estimation suffers from\ntwo limitations. It treats all sentences that\ndo not match the ground truth as equally\npoor, ignoring the structure of the out-\nput space. Second, it suffers from “ex-\nposure bias”: during training tokens are\npredicted given ground-truth sequences,\nwhile at test time prediction is conditioned\non generated output sequences. To over-\ncome these limitations we build upon the\nrecent reward augmented maximum likeli-\nhood approach i.e. sequence-level smooth-\ning that encourages the model to predict\nsentences close to the ground truth accord-\ning to a given performance metric. We\nextend this approach to token-level loss\nsmoothing, and propose improvements to\nthe sequence-level smoothing approach.\nOur experiments on two different tasks,\nimage captioning and machine translation,\nshow that token-level and sequence-level\nloss smoothing are complementary, and\nsigniﬁcantly improve results.\n1 Introduction\nRecurrent neural networks (RNNs) have recently\nproven to be very effective sequence modeling\ntools, and are now state of the art for tasks such\nas machine translation (Cho et al., 2014; Sutskever\net al., 2014; Bahdanau et al., 2015), image caption-\ning (Kiros et al., 2014; Vinyals et al., 2015; Ander-\nson et al., 2017) and automatic speech recognition\n(Chorowski et al., 2015; Chiu et al., 2017).\nThe basic principle of RNNs is to iteratively\ncompute a vectorial sequence representation, by\napplying at each time-step the same trainable func-\ntion to compute the new network state from the\nprevious state and the last symbol in the sequence.\nThese models are typically trained by maximizing\nthe likelihood of the target sentence given an en-\ncoded source (text, image, speech).\nMaximum likelihood estimation (MLE), how-\never, has two main limitations. First, the training\nsignal only differentiates the ground-truth target\noutput from all other outputs. It treats all other\noutput sequences as equally incorrect, regardless\nof their semantic proximity from the ground-truth\ntarget. While such a “zero-one” loss is probably\nacceptable for coarse grained classiﬁcation of im-\nages, e.g. across a limited number of basic ob-\nject categories (Everingham et al., 2010) it be-\ncomes problematic as the output space becomes\nlarger and some of its elements become semanti-\ncally similar to each other. This is in particular the\ncase for tasks that involve natural language gener-\nation (captioning, translation, speech recognition)\nwhere the number of possible outputs is practically\nunbounded. For natural language generation tasks,\nevaluation measures typically do take into account\nstructural similarity, e.g. based on n-grams, but\nsuch structural information is not reﬂected in the\nMLE criterion. The second limitation of MLE is\nthat training is based on predicting the next token\ngiven the input and preceding ground-truth output\ntokens, while at test time the model predicts condi-\ntioned on the input and the so-fargenerated output\nsequence. Given the exponentially large output\nspace of natural language sentences, it is not obvi-\nous that the learned RNNs generalize well beyond\nthe relatively sparse distribution of ground-truth\nsequences used during MLE optimization. This\nphenomenon is known as “exposure bias” (Ran-\nzato et al., 2016; Bengio et al., 2015).\nMLE minimizes the KL divergence between a\ntarget Dirac distribution on the ground-truth sen-\ntence(s) and the model’s distribution. In this pa-\n2095\nper, we build upon the “loss smoothing” approach\nby Norouzi et al. (2016), which smooths the Dirac\ntarget distribution over similar sentences, increas-\ning the support of the training data in the output\nspace. We make the following main contributions:\n•We propose a token-level loss smooth-\ning approach, using word-embeddings, to\nachieve smoothing among semantically sim-\nilar terms, and we introduce a special proce-\ndure to promote rare tokens.\n•For sequence-level smoothing, we propose to\nuse restricted token replacement vocabular-\nies, and a “lazy evaluation” method that sig-\nniﬁcantly speeds up training.\n•We experimentally validate our approach on\nthe MSCOCO image captioning task and the\nWMT’14 English to French machine trans-\nlation task, showing that on both tasks com-\nbining token-level and sequence-level loss\nsmoothing improves results signiﬁcantly over\nmaximum likelihood baselines.\nIn the remainder of the paper, we review the ex-\nisting methods to improve RNN training in Sec-\ntion 2. Then, we present our token-level and\nsequence-level approaches in Section 3. Experi-\nmental evaluation results based on image caption-\ning and machine translation tasks are laid out in\nSection 4.\n2 Related work\nPrevious work aiming to improve the generaliza-\ntion performance of RNNs can be roughly divided\ninto three categories: those based on regulariza-\ntion, data augmentation, and alternatives to maxi-\nmum likelihood estimation.\nRegularization techniques are used to increase\nthe smoothness of the function learned by the\nnetwork, e.g. by imposing an ℓ2 penalty on the\nnetwork weights, also known as “weight decay”.\nMore recent approaches mask network activations\nduring training, as in dropout (Srivastava et al.,\n2014) and its variants adapted to recurrent mod-\nels (Pham et al., 2014; Krueger et al., 2017). In-\nstead of masking, batch-normalization (Ioffe and\nSzegedy, 2015) rescales the network activations\nto avoid saturating the network’s non-linearities.\nInstead of regularizing the network parameters or\nactivations, it is also possible to directly regular-\nize based on the entropy of the output distribution\n(Pereyra et al., 2017).\nData augmentation techniques improve the ro-\nbustness of the learned models by applying trans-\nformations that might be encountered at test time\nto the training data. In computer vision, this is\ncommon practice, and implemented by, e.g., scal-\ning, cropping, and rotating training images (Le-\nCun et al., 1998; Krizhevsky et al., 2012; Paulin\net al., 2014). In natural language processing, ex-\namples of data augmentation include input noising\nby randomly dropping some input tokens (Iyyer\net al., 2015; Bowman et al., 2015; Kumar et al.,\n2016), and randomly replacing words with sub-\nstitutes sampled from the model (Bengio et al.,\n2015). Xie et al. (2017) introduced data augmenta-\ntion schemes for RNN language models that lever-\nage n-gram statistics in order to mimic Kneser-\nNey smoothing of n-grams models. In the con-\ntext of machine translation, Fadaee et al. (2017)\nmodify sentences by replacing words with rare\nones when this is plausible according to a pre-\ntrained language model, and substitutes its equiv-\nalent in the target sentence using automatic word\nalignments. This approach, however, relies on the\navailability of additional monolingual data for lan-\nguage model training.\nThe de facto standard way to train RNN lan-\nguage models is maximum likelihood estimation\n(MLE) (Cho et al., 2014; Sutskever et al., 2014;\nBahdanau et al., 2015). The sequential factoriza-\ntion of the sequence likelihood generates an ad-\nditive structure in the loss, with one term corre-\nsponding to the prediction of each output token\ngiven the input and the preceding ground-truth\noutput tokens. In order to directly optimize for\nsequence-level structured loss functions, such as\nmeasures based on n-grams like B LEU or CIDER ,\nRanzato et al. (2016) use reinforcement learn-\ning techniques that optimize the expectation of a\nsequence-level reward. In order to avoid early con-\nvergence to poor local optima, they pre-train the\nmodel using MLE.\nLeblond et al. (2018) build on the learn-\ning to search approach to structured prediction\n(Daumé III et al., 2009; Chang et al., 2015) and\nadapts it to RNN training. The model generates\ncandidate sequences at each time-step using all\npossible tokens, and scores these at sequence-level\nto derive a training signal for each time step. This\nleads to an approach that is structurally close to\nMLE, but computationally expensive. Norouzi\net al. (2016) introduce a reward augmented maxi-\nmum likelihood (RAML) approach, that incorpo-\n2096\nrates a notion of sequence-level reward without\nfacing the difﬁculties of reinforcement learning.\nThey deﬁne a target distribution over output sen-\ntences using a soft-max over the reward over all\npossible outputs. Then, they minimize the KL di-\nvergence between the target distribution and the\nmodel’s output distribution. Training with a gen-\neral reward distribution is similar to MLE train-\ning, except that we use multiple sentences sam-\npled from the target distribution instead of only the\nground-truth sentences.\nIn our work, we build upon the work of\nNorouzi et al. (2016) by proposing improvements\nto sequence-level smoothing, and extending it to\ntoken-level smoothing. Our token-level smooth-\ning approach is related to the label smoothing ap-\nproach of Szegedy et al. (2016) for image clas-\nsiﬁcation. Instead of maximizing the probability\nof the correct class, they train the model to pre-\ndict the correct class with a large probability and\nall other classes with a small uniform probabil-\nity. This regularizes the model by preventing over-\nconﬁdent predictions. In natural language gen-\neration with large vocabularies, preventing such\n“narrow” over-conﬁdent distributions is impera-\ntive, since for many tokens there are nearly inter-\nchangeable alternatives.\n3 Loss smoothing for RNN training\nWe brieﬂy recall standard recurrent neural net-\nwork training, before presenting sequence-level\nand token-level loss smoothing below.\n3.1 Maximum likelihood RNN training\nWe are interested in modeling the conditional\nprobability of a sequence y = (y1,...,y T) given\na conditioning observation x,\npθ(y|x) =\nT∏\nt=1\npθ(yt|x,y<t), (1)\nwhere y<t = ( y1,...,y t−1), the model parame-\nters are given byθ, and xis a source sentence or an\nimage in the contexts of machine translation and\nimage captioning, respectively.\nIn a recurrent neural network, the sequence yis\npredicted based on a sequence of states ht,\npθ(yt|x,y<t) = pθ(yt|ht), (2)\nwhere the RNN state is computed recursively as\nht =\n{\nfθ(ht−1,yt−1,x) for t∈{1,..T},\ngθ(x) for t= 0. (3)\nThe input is encoded by gθ and used to initialize\nthe state sequence, and fθ is a non-linear function\nthat updates the state given the previous stateht−1,\nthe last output token yt−1, and possibly the input\nx. The state update function can take different\nforms, the ones including gating mechanisms such\nas LSTMs (Hochreiter and Schmidhuber, 1997)\nand GRUs (Chung et al., 2014) are particularly ef-\nfective to model long sequences.\nIn standard teacher-forced training, the hidden\nstates will be computed by forwarding the ground\ntruth sequence y∗i.e. in Eq. (3), the RNN has ac-\ncess to the true previous token y∗\nt−1. In this case\nwe will note the hidden states h∗\nt.\nGiven a ground-truth target sequence y∗, maxi-\nmum likelihood estimation (MLE) of the network\nparameters θamounts to minimizing the loss\nℓMLE(y∗,x) = −ln pθ(y∗|x) (4)\n= −\nT∑\nt=1\nln pθ(y∗\nt|h∗\nt). (5)\nThe loss can equivalently be expressed as the KL-\ndivergence between a Dirac centered on the target\noutput (with δa(x) = 1 at x= aand 0 otherwise)\nand the model distribution, either at the sequence-\nlevel or at the token-level:\nℓMLE(y∗,x) = DKL\n(\nδy∗||pθ(y|x)\n)\n(6)\n=\nT∑\nt=1\nDKL\n(\nδy∗\nt||pθ(yt|h∗\nt)\n)\n. (7)\nLoss smoothing approaches considered in this pa-\nper consist in replacing the Dirac on the ground-\ntruth sequence with distributions with larger sup-\nport. These distributions can be designed in such\na manner that they reﬂect which deviations from\nground-truth predictions are preferred over others.\n3.2 Sequence-level loss smoothing\nThe reward augmented maximum likelihood ap-\nproach of Norouzi et al. (2016) consists in replac-\ning the sequence-level Dirac δy∗ in Eq. (6) with a\ndistribution\nr(y|y∗) ∝exp r(y,y∗)/τ, (8)\nwhere r(y,y∗) is a “reward” function that mea-\nsures the quality of sequence yw.r.t. y∗, e.g. met-\nrics used for evaluation of natural language pro-\ncessing tasks can be used, such as BLEU (Pap-\nineni et al., 2002) or CIDE R (Vedantam et al.,\n2097\n2015). The temperature parameter τ controls\nthe concentration of the distribution around y∗.\nWhen m >1 ground-truth sequences are paired\nwith the same input x, the reward function can\nbe adapted to ﬁt this setting and be deﬁned\nas r(y,{y∗(1),...,y ∗(m)}). The sequence-level\nsmoothed loss function is then given by\nℓSeq(y∗,x) = DKL\n(\nr(y|y∗)||pθ(y|x)\n)\n= H(r(y|y∗)) −Er[ln pθ(y|x)] , (9)\nwhere the entropy term H(r(y|y∗)) does not de-\npend on the model parameters θ.\nIn general, expectation in Eq. (9) is intractable\ndue to the exponentially large output space, and\nreplaced with a Monte-Carlo approximation:\nEr[−ln pθ(y|x)] ≈−\nL∑\nl=1\nln pθ(yl|x). (10)\nStratiﬁed sampling. Norouzi et al. (2016) show\nthat when using the Hamming or edit distance as a\nreward, we can sample directly from r(y|y∗) us-\ning a stratiﬁed sampling approach. In this case\nsampling proceeds in three stages. (i) Sample a\ndistance dfrom {0,...,T }from a prior distribu-\ntion on d. (ii) Uniformly select dpositions in the\nsequence to be modiﬁed. (iii) Sample the dsubsti-\ntutions uniformly from the token vocabulary.\nDetails on the construction of the prior distri-\nbution on d for a reward based on the Hamming\ndistance can be found in Appendix A.\nImportance sampling. For a reward based on\nBLEU or CIDER , we cannot directly sample from\nr(y|y∗) since the normalizing constant, or “parti-\ntion function”, of the distribution is intractable to\ncompute. In this case we can resort to importance\nsampling. We ﬁrst sample L sequences yl from\na tractable proposal distribution q(y|y∗). We then\ncompute the importance weights\nωl ≈ r(yl|y∗)/q(yl|y∗)∑L\nk=1 r(yk|y∗)/q(yk|y∗)\n, (11)\nwhere r(yk|y∗) is the un-normalized reward distri-\nbution in Eq. (8). We ﬁnally approximate the ex-\npectation by reweighing the samples in the Monte\nCarlo approximation as\nEr[−ln pθ(y|x)] ≈−\nL∑\nl=1\nωlln pθ(yl|x). (12)\nIn our experiments we use a proposal distribu-\ntion based on the Hamming distance, which al-\nlows for tractable stratiﬁed sampling, and gener-\nates sentences that do not stray away from the\nground truth.\nWe propose two modiﬁcations to the sequence-\nlevel loss smoothing of Norouzi et al. (2016):\nsampling to a restricted vocabulary (described in\nthe following paragraph) and lazy sequence-level\nsmoothing (described in section 3.4).\nRestricted vocabulary sampling. In the strati-\nﬁed sampling method for Hamming and edit dis-\ntance rewards, instead of drawing from the large\nvocabulary V, containing typically in the order of\n104 words or more, we can restrict ourselves to a\nsmaller subset Vsub more adapted to our task. We\nconsidered three different possibilities for Vsub.\nV: the full vocabulary from which we sample\nuniformly (default), or draw from our token-level\nsmoothing distribution deﬁned below in Eq. (13).\nVrefs: uniformly sample from the set of tokens\nthat appear in the ground-truth sentence(s) associ-\nated with the current input.\nVbatch: uniformly sample from the tokens that\nappear in the ground-truth sentences across all in-\nputs that appear in a given training mini-batch.\nUniformly sampling from Vbatch has the effect\nof boosting the frequencies of words that appear\nin many reference sentences, and thus approxi-\nmates to some extent sampling substitutions from\nthe uni-gram statistics of the training set.\n3.3 Token-level loss smoothing\nWhile the sequence-level smoothing can be di-\nrectly based on performance measures of inter-\nest such as BLEU or CIDEr, the support of the\nsmoothed distribution is limited to the number\nof samples drawn during training. We propose\nsmoothing the token-level Diracs δy∗\nt in Eq. (7) to\nincrease its support to similar tokens. Since we\napply smoothing to each of the tokens indepen-\ndently, this approach implicitly increases the sup-\nport to an exponential number of sequences, un-\nlike the sequence-level smoothing approach. This\ncomes at the price, however, of a naive token-level\nindependence assumption in the smoothing.\nWe deﬁne the smoothed token-level distribu-\ntion, similar as the sequence-level one, as a soft-\nmax over a token-level “reward” function,\nr(yt|y∗\nt) ∝exp r(yt,y∗\nt)/τ, (13)\n2098\nwhere τ is again a temperature parameter. As\na token-level reward r(yt,y∗\nt) we use the cosine\nsimilarity between yt and y∗\nt in a semantic word-\nembedding space. In our experiments we use\nGloVe (Pennington et al., 2014); preliminary ex-\nperiments with word2vec (Mikolov et al., 2013)\nyielded somewhat worse results.\nPromoting rare tokens. We can further im-\nprove the token-level smoothing by promoting rare\ntokens. To do so, we penalize frequent tokens\nwhen smoothing over the vocabulary, by subtract-\ning βfreq(yt) from the reward, where freq(·) de-\nnotes the term frequency and β is a non-negative\nweight. This modiﬁcation encourages frequent to-\nkens into considering the rare ones. We experi-\nmentally found that it is also beneﬁcial for rare\ntokens to boost frequent ones, as they tend to\nhave mostly rare tokens as neighbors in the word-\nembedding space. With this in mind, we deﬁne a\nnew token-level reward as:\nrfreq(yt,y∗\nt) = r(yt,y∗\nt) (14)\n−βmin\n(freq(yt)\nfreq(y∗\nt),freq(y∗\nt)\nfreq(yt)\n)\n,\nwhere the penalty term is strongest if both tokens\nhave similar frequencies.\n3.4 Combining losses\nIn both loss smoothing methods presented above,\nthe temperature parameter τ controls the concen-\ntration of the distribution. As τ gets smaller the\ndistribution peaks around the ground-truth, while\nfor large τ the uniform distribution is approached.\nWe can, however, not separately control the spread\nof the distribution and the mass reserved for the\nground-truth output. We therefore introduce a sec-\nond parameter α ∈[0,1] to interpolate between\nthe Dirac on the ground-truth and the smooth dis-\ntribution. Using ¯α = 1 −α, the sequence-level\nand token-level loss functions are then deﬁned as\nℓα\nSeq(y∗,x) = αℓSeq(y∗,x) + ¯αℓMLE(y∗,x) (15)\n= αEr[ℓMLE(y,x)] + ¯αℓMLE(y∗,x)\nℓα\nTok(y∗,x) = αℓTok(y∗,x) + ¯αℓMLE(y∗,x) (16)\nTo beneﬁt from both sequence-level and token-\nlevel loss smoothing, we also combine them by ap-\nplying token-level smoothing to the different se-\nquences sampled for the sequence-level smooth-\ning. We introduce two mixing parameters α1 and\nα2. The ﬁrst controls to what extent sequence-\nlevel smoothing is used, while the second controls\nto what extent token-level smoothing is used. The\ncombined loss is deﬁned as\nℓα1,α2\nSeq, Tok(y∗,x,r ) = α1Er[ℓTok(y,x)] + ¯α1ℓTok(y∗,x)\n= α1Er[α2ℓTok(y,x) + ¯α2ℓMLE(y,x)]\n+ ¯α1(α2ℓTok(y∗,x) + ¯α2ℓMLE(y∗,x)).\n(17)\nIn our experiments, we use held out validation\ndata to set mixing and temperature parameters.\nAlgorithm 1 Sequence-level smoothing algorithm\nInput: x,y∗\nOutput: ℓα\nseq(x,y∗)\nEncode xto initialize the RNN\nForward y∗in the RNN to compute the hidden states h∗\nt\nCompute the MLE loss ℓMLE(y∗,x)\nfor l∈{1,...,L }do\nSample yl ∼r(˙|y∗)\nif Lazy then\nCompute ℓ(yl,x) = −∑\ntlog pθ(yl\nt|h∗\nt)\nelse\nForward yl in the RNN to get its hidden states hl\nt\nCompute ℓ(yl,x) = ℓMLE(yl,x)\nend if\nend for\nℓα\nSeq(x,y∗) = ¯αℓMLE(y∗,x) + α\nL\n∑\nlℓ(yl,x)\nLazy sequence smoothing. Although sequence-\nlevel smoothing is computationally efﬁcient com-\npared to reinforcement learning approaches (Ran-\nzato et al., 2016; Rennie et al., 2017), it is slower\ncompared to MLE. In particular, we need to for-\nward each of the samples yl through the RNN in\nteacher-forcing mode so as to compute its hidden\nstates hl\nt, which are used to compute the sequence\nMLE loss as\nℓMLE(yl,x) = −\nT∑\nt=1\nln pθ(yl\nt|hl\nt). (18)\nTo speed up training, and since we already forward\nthe ground truth sequence in the RNN to evaluate\nthe MLE part of ℓα\nSeq(y∗,x), we propose to use the\nsame hidden states h∗\nt to compute both the MLE\nand the sequence-level smoothed loss. In this case:\nℓlazy(yl,x) = −\nT∑\nt=1\nln pθ(yl\nt|h∗\nt) (19)\nIn this manner, we only have a single instead of\nL+ 1 forwards-passes in the RNN. We provide\nthe pseudo-code for training in Algorithm 1.\n2099\nWithout attention\nLoss Reward Vsub BLEU-1 BLEU-4 CIDER\nMLE 70.63 30.14 93.59\nMLE +γH 70.79 30.29 93.61\nTok Glove sim 71.94 31.27 95.79\nTok Glove sim rfreq 72.39 31.76 97.47\nSeq Hamming V 71.76 31.16 96.37\nSeq Hamming Vbatch 71.46 31.15 96.53\nSeq Hamming Vrefs 71.80 31.63 96.22\nSeq, lazy Hamming V 70.81 30.43 94.26\nSeq, lazy Hamming Vbatch 71.85 31.13 96.65\nSeq, lazy Hamming Vrefs 71.96 31.23 95.34\nSeq CIDE R V 71.05 30.46 94.40\nSeq CIDE R Vbatch 71.51 31.17 95.78\nSeq CIDE R Vrefs 71.93 31.41 96.81\nSeq, lazy CIDE R V 71.43 31.18 96.32\nSeq, lazy CIDE R Vbatch 71.47 31.00 95.56\nSeq, lazy CIDE R Vrefs 71.82 31.06 95.66\nTok-Seq Hamming V 70.79 30.43 96.34\nTok-Seq Hamming Vbatch 72.28 31.65 96.73\nTok-Seq Hamming Vrefs 72.69 32.30 98.01\nTok-Seq CIDE R V 70.80 30.55 96.89\nTok-Seq CIDE R Vbatch 72.13 31.71 96.92\nTok-Seq CIDE R Vrefs 73.08 32.82 99.92\nWith attention\nBLEU-1 BLEU-4 CIDER\n73.40 33.11 101.63\n72.68 32.15 99.77\n73.49 32.93 102.33\n74.01 33.25 102.81\n73.12 32.71 101.25\n73.26 32.73 101.90\n73.53 32.59 102.33\n73.29 32.81 101.58\n73.43 32.95 102.03\n73.53 33.09 101.89\n73.08 32.51 101.84\n73.50 33.04 102.98\n73.42 32.91 102.23\n73.55 33.19 102.94\n73.18 32.60 101.30\n73.92 33.10 102.64\n73.68 32.87 101.11\n73.86 33.32 102.90\n73.56 33.00 101.72\n73.31 32.40 100.33\n73.61 32.67 101.41\n74.28 33.34 103.81\nTable 1: MS-COCO ’s test set evaluation measures.\n4 Experimental evaluation\nIn this section, we compare sequence prediction\nmodels trained with maximum likelihood (MLE)\nwith our token and sequence-level loss smoothing\non two different tasks: image captioning and ma-\nchine translation.\n4.1 Image captioning\n4.1.1 Experimental setup.\nWe use the MS-COCO datatset (Lin et al., 2014),\nwhich consists of 82k training images each anno-\ntated with ﬁve captions. We use the standard splits\nof Karpathy and Li (2015), with 5k images for val-\nidation, and 5k for test. The test set results are\ngenerated via beam search (beam size 3) and are\nevaluated with the MS-COCO captioning evalu-\nation tool. We report CIDE R and BLEU scores\non this internal test set. We also report results ob-\ntained on the ofﬁcial MS-COCO server that ad-\nditionally measures METEOR (Denkowski and\nLavie, 2014) and ROUGE-L (Lin, 2004). We ex-\nperiment with both non-attentive LSTMs (Vinyals\net al., 2015) and the ResNet baseline of the state-\nof-the-art top-down attention (Anderson et al.,\n2017).\nThe MS-COCO vocabulary consists of 9,800\nwords that occur at least 5 times in the training\nset. Additional details and hyperparameters can\nbe found in Appendix B.1.\n4.1.2 Results and discussion\nRestricted vocabulary sampling In this sec-\ntion, we evaluate the impact of the vocabulary\nsubset from which we sample the modiﬁed sen-\ntences for sequence-level smoothing. We exper-\niment with two rewards: CIDE R , which scores\nw.r.t. all ﬁve available reference sentences, and\nHamming distance reward taking only a single ref-\nerence into account. For each reward we train our\n(Seq) models with each of the three subsets de-\ntailed previously in Section 3.2, Restricted vocab-\nulary sampling.\nFrom the results in Table 1 we note that for the\ninattentive models, sampling from Vrefs or Vbatch\nhas a better performance than sampling from the\nfull vocabulary on all metrics. In fact, using\nthese subsets introduces a useful bias to the model\nand improves performance. This improvement is\nmost notable using the CIDE R reward that scores\ncandidate sequences w.r.t. to multiple references,\nwhich stabilizes the scoring of the candidates.\nWith an attentive decoder, no matter the re-\nward, re-sampling sentences with words fromVref\nrather than the full vocabulary Vis better for both\nreward functions, and all metrics. Additional ex-\nperimental results, presented in Appendix B.2, ob-\ntained with a BLEU-4 reward, in its single and\n2100\nBLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDER SPICE\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nGoogle NIC+(Vinyals et al., 2015) 71.3 89.5 54.2 80.2 40.7 69.4 30.9 58.7 25.4 34.6 53.0 68.2 94.3 94.6 18.2 63.6\nHard-Attention (Xu et al., 2015) 70.5 88.1 52.8 77.9 38.3 65.8 27.7 53.7 24.1 32.2 51.6 65.4 86.5 89.3 17.2 59.8\nATT-FCN+(You et al., 2016) 73.1 90.0 56.5 81.5 42.4 70.9 31.6 59.9 25.0 33.5 53.5 68.2 94.3 95.8 18.2 63.1\nReview Net+(Yang et al., 2016) 72.0 90.0 55.0 81.2 41.4 70.5 31.3 59.7 25.6 34.7 53.3 68.6 96.5 96.9 18.5 64.9\nAdaptive+(Lu et al., 2017) 74.8 92.0 58.4 84.5 44.4 74.4 33.6 63.7 26.4 35.9 55.0 70.5 104.2 105.9 19.7 67.3\nSCST:Att2all+†(Rennie et al., 2017) 78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7 - -\nLSTM-A3+†◦(Yao et al., 2017) 78.7 93.7 62.7 86.7 47.6 76.5 35.6 65.2 27.0 35.4 56.4 70.5 116 118 - -\nUp-Down+†◦(Anderson et al., 2017) 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5 - -\nOurs: Tok-Seq CIDER 72.6 89.7 55.7 80.9 41.2 69.8 30.2 58.3 25.5 34.0 53.5 68.0 96.4 99.4 - -\nOurs: Tok-Seq CIDER+ 74.9 92.4 58.5 84.9 44.8 75.1 34.3 64.7 26.5 36.1 55.2 71.1 103.9 104.2 - -\nTable 2: MS-COCO ’s server evaluation . (+) for ensemble submissions, (†) for submissions with CIDEr\noptimization and (◦) for models using additional data.\nmultiple references variants, further corroborate\nthis conclusion.\nLazy training. From the results of Table 1, we\nsee that lazy sequence-level smoothing is compet-\nitive with exact non-lazy sequence-level smooth-\ning, while requiring roughly equivalent training\ntime as MLE. We provide detailed timing results\nin Appendix B.3.\nOverall For reference, we include in Table 1\nbaseline results obtained using MLE, and our im-\nplementation of MLE with entropy regularization\n(MLE+γH) (Pereyra et al., 2017), as well as the\nRAML approach of Norouzi et al. (2016) which\ncorresponds to sequence-level smoothing based on\nthe Hamming reward and sampling replacements\nfrom the full vocabulary (Seq, Hamming, V)\nWe observe that entropy smoothing is not able\nto improve performance much over MLE for the\nmodel without attention, and even deteriorates for\nthe attention model. We improve upon RAML\nby choosing an adequate subset of vocabulary for\nsubstitutions.\nWe also report the performances of token-level\nsmoothing, where the promotion of rare tokens\nboosted the scores in both attentive and non-\nattentive models.\nFor sequence-level smoothing, choosing a task-\nrelevant reward with importance sampling yielded\nbetter results than plain Hamming distance.\nMoreover, we used the two smoothing schemes\n(Tok-Seq) and achieved the best results with\nCIDE R as a reward for sequence-level smoothing\ncombined with a token-level smoothing that pro-\nmotes rare tokens improving CIDE R from 93.59\n(MLE) to 99.92 for the model without attention,\nand improving from 101.63 to 103.81 with atten-\ntion.\nQualitative results. In Figure 1 we showcase\ncaptions obtained with MLE and our three vari-\nants of smoothingi.e. token-level (Tok), sequence-\nlevel (Seq) and the combination (Tok-Seq). We\nnote that the sequence-level smoothing tend to\ngenerate lengthy captions overall, which is main-\ntained in the combination. On the other hand, the\ntoken-level smoothing allows for a better recogni-\ntion of objects in the image that stems from the\nrobust training of the classiﬁer e.g. the ’cement\nblock’ in the top right image or the carrots in the\nbottom right. More examples are available in Ap-\npendix B.4\nComparison to the state of the art. We com-\npare our model to state-of-the-art systems on the\nMS-COCO evaluation server in Table 2. We sub-\nmitted a single model (Tok-Seq, CIDE R , Vrefs)\nas well as an ensemble of ﬁve models with differ-\nent initializations trained on the training set plus\n35k images from the dev set (a total of 117k im-\nages) to the MS-COCO server. The three best\nresults on the server (Rennie et al., 2017; Yao\net al., 2017; Anderson et al., 2017) are trained in\ntwo stages where they ﬁrst train using MLE, be-\nfore switching to policy gradient methods based\non CIDEr. Anderson et al. (2017) reported an in-\ncrease of 5.8% of CIDE R on the test split after\nthe CIDE R optimization. Moreover, Yao et al.\n(2017) uses additional information about image\nregions to train the attributes classiﬁers, while An-\nderson et al. (2017) pre-trains its bottom-up atten-\ntion model on the Visual Genome dataset (Krishna\net al., 2017). Lu et al. (2017); Yao et al. (2017)\nuse the same CNN encoder as ours (ResNet-\n152), (Vinyals et al., 2015; Yang et al., 2016) use\nInception-v3 (Szegedy et al., 2016) for image en-\ncoding and Rennie et al. (2017); Anderson et al.\n2101\nFigure 1: Examples of generated captions with the baseline MLE and our models with attention.\n(2017) use Resnet-101, both of which have similar\nperformances to ResNet-152 on ImageNet classi-\nﬁcation (Canziani et al., 2016).\n4.2 Machine translation\n4.2.1 Experimental setup.\nFor this task we validate the effectiveness of our\napproaches on two different datasets. The ﬁrst is\nWMT’14 English to French, in its ﬁltered version,\nwith 12M sentence pairs obtained after dynami-\ncally selecting a “clean” subset of 348M words\nout of the original “noisy” 850M words (Bahdanau\net al., 2015; Cho et al., 2014; Sutskever et al.,\n2014). The second benchmark is IWSLT’14 Ger-\nman to English consisting of around 150k pairs\nfor training. In all our experiments we use the at-\ntentive model of (Bahdanau et al., 2015) The hy-\nperparameters of each of these models as well as\nany additional pre-processing can be found in Ap-\npendix C.1\nTo assess the translation quality we report the\nBLEU-4 metric.\n4.2.2 Results and analysis\nLoss Reward Vsub WMT’14 IWSLT’14\nMLE 30.03 27.55\ntok Glove sim 30.16 27.69\ntok Glove sim rfreq 30.19 27.83\nSeq Hamming V 30.85 27.98\nSeq Hamming Vbatch 31.18 28.54\nSeq BLEU-4 Vbatch 31.29 28.56\nTok-Seq Hamming Vbatch 31.36 28.70\nTok-Seq BLEU-4 Vbatch 31.39 28.74\nTable 3: Tokenized BLEU score on WMT’14\nEn-Fr evaluated on the news-test-2014 set. And\nTokenzied, case-insensitive BLEU on IWSLT’14\nDe-En.\nWe present our results in Table 3. On both\nbenchmarks, we improve on both MLE and\nRAML approach of Norouzi et al. (2016) (Seq,\nHamming, V): using the smaller batch-vocabulary\nfor replacement improves results, and using im-\nportance sampling based on BLEU-4 further\nboosts results. In this case, unlike in the cap-\ntioning experiment, token-level smoothing brings\nsmaller improvements. The combination of both\nsmoothing approaches gives best results, similar\nto what was observed for image captioning, im-\nproving the MLE BLEU-4 from 30.03 to 31.39 on\nWMT’14 and from 27.55 to 28.74 on IWSLT’14.\nThe outputs of our best model are compared to\nthe MLE in some examples showcased in Ap-\npendix C.\n5 Conclusion\nWe investigated the use of loss smoothing ap-\nproaches to improve over maximum likelihood es-\ntimation of RNN language models. We gener-\nalized the sequence-level smoothing RAML ap-\nproach of Norouzi et al. (2016) to the token-\nlevel by smoothing the ground-truth target across\nsemantically similar tokens. For the sequence-\nlevel, which is computationally expensive, we in-\ntroduced an efﬁcient “lazy” evaluation scheme,\nand introduced an improved re-sampling strat-\negy. Experimental evaluation on image captioning\nand machine translation demonstrates the comple-\nmentarity of sequence-level and token-level loss\nsmoothing, improving over both the maximum\nlikelihood and RAML.\nAcknowledgment. This work has been par-\ntially supported by the grant ANR-16-CE23-0006\n“Deep in France” and LabEx PERSYV AL-Lab\n(ANR-11-LABX-0025-01).\n2102\nReferences\nP. Anderson, X. He, C. Buehler, D. Teney, M. John-\nson, S. Gould, and L. Zhang. 2017. Bottom-\nup and top-down attention for image captioning\nand visual question answering. arXiv preprint\narXiv:1707.07998.\nD. Bahdanau, K. Cho, and Y . Bengio. 2015. Neural\nmachine translation by jointly learning to align and\ntranslate. In ICLR.\nS. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. 2015.\nScheduled sampling for sequence prediction with re-\ncurrent neural networks. In NIPS.\nS. Bowman, L. Vilnis, O. Vinyals, A. Dai, R. Jozefow-\nicz, and S. Bengio. 2015. Generating sentences from\na continuous space. In CoNLL.\nA. Canziani, A. Paszke, and E. Culurciello. 2016. An\nanalysis of deep neural network models for practical\napplications. arXiv preprint arXiv:1605.07678.\nK.-W. Chang, A. Krishnamurthy, A. Agarwal,\nH. Daumé III, and J. Langford. 2015. Learning to\nsearch better than your teacher. In ICML.\nC.-C. Chiu, T. Sainath, Y . Wu, R. Prabhavalkar,\nP. Nguyen, Z. Chen, A. Kannan, R.-J. Weiss, K. Rao,\nE. Gonina, N. Jaitly, B. Li, J. Chorowski, and\nM. Bacchiani. 2017. State-of-the-art speech recog-\nnition with sequence-to-sequence models. arXiv\npreprint arXiv:1712.01769.\nK. Cho, B. van Merrienboer, Ç. Gülçehre, D. Bah-\ndanau, F. Bougares, H. Schwenk, and Y . Bengio.\n2014. Learning phrase representations using RNN\nencoder-decoder for statistical machine translation.\nIn Empirical Methods in Natural Language Process-\ning.\nJ. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and\nY . Bengio. 2015. Attention-based models for speech\nrecognition. In NIPS.\nJ. Chung, C. Gulcehre, K. Cho, and Y . Bengio. 2014.\nEmpirical evaluation of gated recurrent neural net-\nworks on sequence modeling. In NIPS Deep Learn-\ning Workshop.\nH. Daumé III, J. Langford, and D. Marcu. 2009.\nSearch-based structured prediction. Machine Learn-\ning, 75(3):297–325.\nM. Denkowski and A. Lavie. 2014. Meteor universal:\nLanguage speciﬁc translation evaluation for any tar-\nget language. In Workshop on statistical machine\ntranslation.\nM. Everingham, L. van Gool, C. Williams, J. Winn,\nand A. Zisserman. 2010. The pascal visual object\nclasses (VOC) challenge. IJCV, 88(2):303–338.\nM. Fadaee, A. Bisazza, and C. Monz. 2017. Data aug-\nmentation for low-resource neural machine transla-\ntion. In ACL.\nK. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep resid-\nual learning for image recognition. In CVPR.\nS. Hochreiter and J. Schmidhuber. 1997. Long short-\nterm memory. Neural Computation, 9(8):1735–\n1780.\nS. Ioffe and C. Szegedy. 2015. Batch normalization:\nAccelerating deep network training by reducing in-\nternal covariate shift. In ICML.\nM. Iyyer, V . Manjunatha, J. Boyd-Graber, and\nH. Daumé III. 2015. Deep unordered composition\nrivals syntactic methods for text classiﬁcation. In\nACL.\nA. Karpathy and Fei-Fei Li. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In CVPR.\nD. Kingma and J. Ba. 2015. Adam: A method for\nstochastic optimization. In ICLR.\nR. Kiros, R. Salakhutdinov, and R. Zemel. 2014. Mul-\ntimodal neural language models. In ICML.\nR. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata,\nJ. Kravitz, S. Chen, Y . Kalantidis, L.-J. Li,\nD. Shamma, M. Bernstein, and L. Fei-Fei. 2017. Vi-\nsual genome: Connecting language and vision us-\ning crowdsourced dense image annotations. IJCV,\n123(1):32–73.\nA. Krizhevsky, I. Sutskever, and G. Hinton. 2012. Im-\nagenet classiﬁcation with deep convolutional neural\nnetworks. In NIPS.\nD. Krueger, T. Maharaj, J. Kramár, M. Pezeshki,\nN. Ballas, N. Ke, A. Goyal, Y . Bengio,\nH. Larochelle, A. Courville, and C. Pal. 2017.\nZoneout: Regularizing RNNs by randomly preserv-\ning hidden activations. In ICLR.\nA. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury,\nI. Gulrajani, V . Zhong, R. Paulus, and R. Socher.\n2016. Ask me anything: Dynamic memory net-\nworks for natural language processing. In ICML.\nR. Leblond, J.-B. Alayrac, A. Osokin, and S. Lacoste-\nJulien. 2018. SeaRnn: Training RNNs with global-\nlocal losses. In ICLR.\nY . LeCun, L. Bottou, Y . Bengio, and P. Haffner. 1998.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, pages 2278–2324.\nC.-Y . Lin. 2004. Rouge: a package for automatic eval-\nuation of summaries. In ACL Workshop Text Sum-\nmarization Branches Out.\nT.-Y . Lin, M. Maire, S. Belongie, L. Bourdev, R. Gir-\nshick, J. Hays, P. Perona, D. Ramanan, P. Dollár,\nand C. Zitnick. 2014. Microsoft COCO: common\nobjects in context. In ECCV.\n2103\nJ. Lu, C. Xiong, D. Parikh, and R. Socher. 2017.\nKnowing when to look: Adaptive attention via a vi-\nsual sentinel for image captioning. In CVPR.\nT. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013.\nEfﬁcient estimation of word representations in vec-\ntor space. In ICLR.\nM. Norouzi, S. Bengio, Z. Chen, N. Jaitly, M. Schus-\nter, Y . Wu, and D. Schuurmans. 2016. Reward aug-\nmented maximum likelihood for neural structured\nprediction. In NIPS.\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.\nBLEU: a method for automatic evaluation of ma-\nchine translation. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics.\nM. Paulin, J. Revaud, Z. Harchaoui, F. Perronnin, and\nC. Schmid. 2014. Transformation pursuit for image\nclassiﬁcation. In CVPR.\nM. Pedersoli, T. Lucas, C. Schmid, and J. Verbeek.\n2017. Areas of attention for image captioning. In\nICCV.\nJ. Pennington, R. Socher, and C. Manning. 2014.\nGloVe: Global vectors for word representation. In\nEmpirical Methods in Natural Language Process-\ning.\nG. Pereyra, G. Tucker, J. Chorowski, L. Kaiser, and\nG. Hinton. 2017. Regularizing neural networks by\npenalizing conﬁdent output distributions. In ICLR.\nV . Pham, T. Bluche, C. Kermorvant, and J. Louradour.\n2014. Dropout improves recurrent neural networks\nfor handwriting recognition. In Frontiers in Hand-\nwriting Recognition.\nM. Ranzato, S. Chopra, M. Auli, and W. Zaremba.\n2016. Sequence level training with recurrent neural\nnetworks. In ICLR.\nS. Rennie, E. Marcheret, Y . Mroueh, J. Ross, and\nV . Goel. 2017. Self-critical sequence training for\nimage captioning. In CVPR.\nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov. 2014. Dropout: A simple\nway to prevent neural networks from overﬁtting.\nJMLR.\nI. Sutskever, O. Vinyals, and Q. Le. 2014. Sequence to\nsequence learning with neural networks. In NIPS.\nC. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and\nZ. Wojna. 2016. Rethinking the inception architec-\nture for computer vision. In CVPR.\nR. Vedantam, C. Zitnick, and D. Parikh. 2015. CIDEr:\nConsensus-based image description evaluation. In\nCVPR.\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015.\nShow and tell: A neural image caption generator. In\nCVPR.\nZ. Xie, S. Wang, J. Li, D. Lévy, A. Nie, D. Jurafsky, and\nA. Ng. 2017. Data noising as smoothing in neural\nnetwork language models. In ICLR.\nK. Xu, J. Ba, R. Kiros, K. Cho, A. Courville,\nR. Salakhutdinov, R. Zemel, and Y . Bengio. 2015.\nShow, attend and tell: Neural image caption genera-\ntion with visual attention. In ICML.\nZ. Yang, Y . Yuan, Y . Wu, R. Salakhutdinov, and W. Co-\nhen. 2016. Encode, review, and decode: Reviewer\nmodule for caption generation. In NIPS.\nT. Yao, Y . Pan, Y . Li, Z. Qiu, and T. Mei. 2017. Boost-\ning image captioning with attributes. In ICLR.\nQ. You, H. Jin, Z. Wang, C. Fang, and J. Luo. 2016.\nImage captioning with semantic attention. In CVPR."
}