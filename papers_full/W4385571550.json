{
  "title": "Augmenting Large Language Model Translators via Translation Memories",
  "url": "https://openalex.org/W4385571550",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3120903733",
      "name": "Yongyu Mu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A3120689759",
      "name": "Abudurexiti Reheman",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2518305282",
      "name": "Zhiquan Cao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2955406401",
      "name": "Yu-Chun Fan",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2099334605",
      "name": "Bei Li",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2168056687",
      "name": "Li Yinqiao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A1983914940",
      "name": "Tong Xiao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2122370144",
      "name": "Chun-Liang Zhang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2496766346",
      "name": "Jingbo Zhu",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4318903120",
    "https://openalex.org/W3170427498",
    "https://openalex.org/W4245202262",
    "https://openalex.org/W4287597717",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4317547647",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W4382202554",
    "https://openalex.org/W3174160883",
    "https://openalex.org/W3034881347",
    "https://openalex.org/W2963829526",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2951166594",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2788330850",
    "https://openalex.org/W2113104171",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W1872443190",
    "https://openalex.org/W4310831983"
  ],
  "abstract": "Using translation memories (TMs) as prompts is a promising approach to in-context learning of machine translation models. In this work, we take a step towards prompting large language models (LLMs) with TMs and making them better translators. We find that the ability of LLMs to \"understand\" prompts is indeed helpful for making better use of TMs. Experiments show that the results of a pre-trained LLM translator can be greatly improved by using high-quality TM-based prompts. These results are even comparable to those of the state-of-the-art NMT systems which have access to large-scale in-domain bilingual data and are well tuned on the downstream tasks.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 10287–10299\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAugmenting Large Language Model Translators via Translation Memories\nYongyu Mu1∗, Abudurexiti Reheman1∗, Zhiquan Cao1, Yuchun Fan1,\nBei Li1, Yinqiao Li1, Tong Xiao1,2†, Chunliang Zhang1,2 and Jingbo Zhu1,2\n1NLP Lab, School of Computer Science and Engineering,\nNortheastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\nlixiaoyumu9@gmail.com rexiti_neu@outlook.com\n{xiaotong,zhujingbo}@mail.neu.edu.cn\nAbstract\nUsing translation memories (TMs) as prompts\nis a promising approach to in-context learning\nof machine translation models. In this work, we\ntake a step towards prompting large language\nmodels (LLMs) with TMs and making them\nbetter translators. We find that the ability of\nLLMs to “understand” prompts is indeed help-\nful for making better use of TMs. Experiments\nshow that the results of a pre-trained LLM trans-\nlator can be greatly improved by using high-\nquality TM-based prompts. These results are\neven comparable to those of the state-of-the-art\nNMT systems which have access to large-scale\nin-domain bilingual data and are well tuned on\nthe downstream tasks.\n1 Introduction\nMarrying the world of translation memory (TM)\nand the world of neural machine translation (NMT)\nis a challenging but interesting problem in natural\nlanguage processing (NLP). Previous work along\nthis line of research either requires architecture\nchanges of NMT models and/or additional train-\ning (Gu et al., 2018; Bulté and Tezcan, 2019; Xu\net al., 2020; Hossain et al., 2020; He et al., 2021)\nor constructing translation knowledge base from\nTM (Zhang et al., 2018; Khandelwal et al., 2021;\nMeng et al., 2022).\nMore recently, researchers have been aware\nof the strength of prompting techniques for one-\nshot/few-shot machine translation (Vilar et al.,\n2022; Agrawal et al., 2022; Zhang et al., 2023). In\nparticular, Reheman et al. (2023) investigated one-\nshot learning methods for NMT by simply viewing\nTMs as prompts. The result of their work is a\nstronger NMT system that works in the same way\nas usual but can be prompted when TMs are avail-\nable. Interestingly, they found that the ability of\nNMT models to “understand” prompts plays an\n∗Equal contribution.\n†Corresponding author.\nMethod w/o-arch-change w/o-base few-shot\nZhang et al. (2018) yes\nHe et al. (2021) yes\nKhandelwal et al. (2021) yes\nReheman et al. (2023) yes yes one-shot\nTMPLM(our) yes yes yes\nTable 1: Methods of using TM for better MT. w/o-arch-\nchange = without architecture changes or training, w/o-\nbase = without constructing translation knowledge base\nfrom TM, and few-shot = few-shot learning.\nimportant role in this type of system. Prompts are\nstill difficult to use if NMT systems are weak.\nIn this work, we take a step forward. We treat\nlarge language models (LLMs) as machine transla-\ntion systems and prompt them with TMs (see Table\n1 for a comparison of different methods). This is in\npart motivated by recent developments of LLMs:\none of the most powerful properties of LLMs is\ntheir ability to understand and respond to complex\ninstructions and questions (Ouyang et al., 2022;\nThoppilan et al., 2022). We show that this abil-\nity is crucial for in-context learning of TM-based\nprompts, and LLM-based translation systems can\nbe greatly improved by using simple instruction-\nlike prompts. To this end, we propose Translation\nMemory Prompting for large Language Models,\nnamely TMP LM - a simple but effective approach\nto injecting TMs into LLM translators.\nWe experiment with our method on a GPT-based\nLLM (text-davinci-003*). On translation\ntasks ranging over multiple languages and domains,\nTM-based prompting improves the LLM-based\ntranslation system by 20 to 30 BLEU points, show-\ning better performance than a well-tuned, large-\nscale, in-domain NMT system on most of the tasks.\nWe also compare different kinds of prompt tem-\nplates and discuss some interesting issues, such as\nthe role of prompting in treating LLMs as transla-\ntors.\n*We will refer to it as davinci-003 later in the paper.\n10287\nf(·) : What is the translation of \" \" from src-lang to tgt-lang? Only translation results\nare required.\nx\nINSTRUCTION\nfref(·): If the translation of \" \" from src-lang to tgt-lang is \" \" and the translation ofx1\ntm y1\ntm\n\" \" from src-lang to tgt-lang is \" \", then what is the translation of \" \" from\nsrc-lang to tgt-lang? Only translation results are required.\nx2\ntm y2\ntm x\nCODE\nf(·) : [src-lang]=[ ] [ tgt-lang]=x\nfref(·): [src-lang]=[ ] [ tgt-lang]=[ ] [ src-lang]=[ ] [ tgt-lang]=[ ] [ src-lang]=\n[ ] [ tgt-lang]=\nx1\ntm y1\ntm x2\ntm y2\ntm\nx\nFigure 1: Two styles of template. f(·) denotes a template by which we represent the input sentence as the input of\nthe translation model (such as LLM in this figure). fref(·) is a new template involving outputs of a TM (k = 2 in\nthis example). x in red stands for the sentence that needs to be translated. xtm in blue and ytm in green stand for\nthe source and target sentence found in the TM, respectively. Both src-lang and tgt-lang need to be replaced by the\nnames of the source and target language.\n2 Prompting Methods\nTM is a database that contains the bilingual transla-\ntion history of professional translators. It is usually\nused to help the translation of the test sentence by\nproviding similar sentence pairs, which may have\ntranslation hints, such as similar sentence patterns,\nphrases, lexicons, terminologies, or other transla-\ntion knowledge. Either an NMT model or an LLM\nneed to dig out those hints and ignore the irrelevant\ncontent. This motivates us to investigate prompt-\ning LLMs with TMs benefiting from their dazzling\nability of “understand” prompts.\nSuppose we have a TM database that retains a\ncollection of pairs of sentences. Given a source-\nlanguage sentence x, the database returns k most\nsimilar sentences Xtm = {x1\ntm, ...,xk\ntm}along\nwith their corresponding translations Ytm =\n{y1\ntm, ...,yk\ntm}. Now suppose we have a pre-\ntrained translation model (either an NMT model or\nan LLM) that takes x in some way and outputs a\ntranslation y, written as\ny = Trans( f(x)) (1)\nwhere Trans(·) denotes the translation model, and\nf(·) denotes a template by which we representx as\nthe input of Trans(·). For example, if Trans(·) is\nan NMT model, f(x) = x; if Trans(·) is a genera-\ntive LLM, f(x) could be an instruction involving\nx.\nWe then wish to use this model to generate a\nnew translation y′by considering (Xtm, Ytm) as\ninstances for reference. This can be written as\ny′ = Trans( fref(x, Xtm, Ytm)) (2)\nHere fref(x, Xtm, Ytm) is a new template involv-\ning (Xtm, Ytm).\nIn this work, we focus on the case in which a\npowerful generative LLM (such as ChatGPT) is\nused to perform translation. The input of Trans(·)\ncould be an instruction or question-like text, and\nso we can design fref(·) in many different ways.\nIn Figure 1, we present two types of templates:\nthe instruction-style template and the code-style\ntemplate. These designs come from a considera-\ntion of the human instruction tuning and the code\ntraining used in developing davinci-003. For a\nmore extensive discussion of template design, see\nAppendix B.2.\nIt is worth emphasizing that, while we restrict\nourselves to TM-based prompts in experiments, we\ncan apply this general approach to deal with other\nknowledge about translation. As a simple exam-\nple, we can extend (Xtm, Ytm) to term or phrase\ntranslations. Also, when some MT systems are\navailable, we can make use of automatic transla-\ntions from other systems to define prompts.\n3 Experiments\n3.1 Data and LLM Setup\nWe tested our method (denoted by TMP LM) on\nthree widely-used datasets of TM: DGT-TM (Stein-\nberger et al., 2012), JRC-Acquis (JRC-A) (Stein-\nberger et al., 2006) and the multi-domain dataset\ndescribed in (Aharoni and Goldberg, 2020). To\nensure a fair comparison, we adopted the same\npreprocessing steps as outlined in Reheman et al.\n(2023) for data cleanup and training/testing data\nsplit.\n10288\nData\nWMT19 200M WMT21 4B davinci-003 175B\nNMT NMT+TM NMT NMT+TM LLM LLM+TM LLM+TM(zero-shot) (one-shot) (few-shot)\nDGT-TM de → en 45.40 54.03 (+8.63) 51.62 69.39 (+17.77) 38.89 66.90 (+28.01) 69.99(+31.10)\nen → de 39.03 44.77 (+5.74) 42.48 60.09 (+17.61) 29.00 57.39 (+28.39) 62.02(+33.02)\nJRC-A de → en 45.90 50.95 (+5.05) 51.72 62.99 (+11.27) 40.75 62.23 (+21.48) 65.55(+24.80)\nen → de 40.10 43.41 (+3.31) 41.71 56.21 (+14.50) 29.83 55.01 (+25.18) 57.30(+27.47)\nTable 2: BLEU scores of NMT models and LLMs on the DGT-TM and JRC-A dataset. WMT19 200M indicates\nWMT19 champion models (Ng et al., 2019), containing 200 million parameters. WMT21 4B indicates WMT21\nchampion models (Tran et al., 2021) trained by multi language-pairs data containing 4 billion parameters. One-shot\nand few-shot represent the results of TMP LM with k = 1 and k = 5, respectively. The BLEU improvements are\nreported in subscripts. See Table 6 for the COMET-22 version.\ncs→en\ncs→it\nro→en\nen→cs\nit→cs\nen→ro\nde→en\nen→de20\n30\n40\n50\n60\n70\nes→en\nfr→en\nde→fr\nen→es\nen→fr\nfr→de\nit→en\nen→it20\n30\n40\n50\n60\n70\nFigure 2: Comparison of LLM w/o and w/ TMs (one-\nshot) on 8 language-pairs from JRC-A. Points in deep\nand light color stand for the BLEU scores of LLM w/o\nand w/ TM, respectively.\nFor LLMs, we chose the davinci-003 model\ndeveloped by OpenAI because it is currently one\nof the state-of-the-art generative LLMs. The model\nwas configured with default values of all param-\neters, except that the sampling temperature was\nset to 0. In the experiments, we used the code-\nstyle template and set k to 5 by default. The\nquality of translations was mainly evaluated using\nmulti-bleu.perl from Moses†. In addition,\nfollowing the recommend of using neural network-\nbased metrics in machine translation evaluation\n(Freitag et al., 2022), we also used COMET-22 ‡\n(wmt22-COMET-da) (Rei et al., 2022) to make a\ncomplementary evaluation. See more details about\ndata processing in Appendixes A.3 and A.4.\n3.2 Baselines\nWe re-implemented Reheman et al. (2023)’s\nmethod which augments NMT systems via TM-\nbased one-shot learning. For NMT systems, we\nchose two champion models in WMT: Facebook’s\nWMT19 en ↔de models (Ng et al., 2019) and\nWMT21 multilingual models (Tran et al., 2021).\nThese WMT models were all trained on large-scale\n†http://www.statmt.org/moses/\n‡https://github.com/Unbabel/COMET\nw/o TM w/ TM\n29.00 50.00\n(a) different models\nGPT3\ndavinci-003\nw/o TM w/ TM\n29.00 50.00\n(b) different template styles\nde →enen →de\ninstruction\ncode\nBLEU\nFigure 3: Experiments on two impacts including differ-\nent LLMs and different template styles.\nbilingual data and are improved by using a series\nof techniques, such as back-translation and fine-\ntuning. As a second baseline, we chose the kNN-\nMT model (Khandelwal et al., 2021) because it is a\nvery strong model for TM and NMT combination.\n3.3 Translation Quality\nMain Results. Table 2 shows BLEU scores on\nthe DGT-TM and JRC-A datasets. We see, first of\nall, that TMP LM achieves the best result among\nall the systems. When TMs are not involved, the\nperformance of LLMs is 10 BLEU points lower\nthan that of the NMT baselines. But, when armed\nwith TMs, LLMs obtain very large BLEU improve-\nments. The few-shot learning+LLM system even\noutperforms the strong NMT+TM baseline on all\nof the test sets. Also, by comparing the results of\nWMT19 200M models and WMT21 4B models,\nwe see that larger models help more for making\nuse of TM (see Section 3.4 for more discussions).\nBesides, one-shot learning can give satisfactory re-\nsults for TMP LM indicating that the most similar\nTM provides the most helpful translation hints. In\nAppendix B.4 we will see that few-shot learning\nyields BLEU gains in a long-tail manner.\nMulti-language Experiments. We test TMP LM\non more languages and run our system on data of\n7 extra language pairs (i.e., 14 directions) from\n10289\nDomain kNN-MT\nWMT19 200M WMT21 4B davinci-003 175B\nNMT NMT+TM NMT NMT+TM LLM LLM+TM LLM+TM(zero-shot) (one-shot) (few-shot)\nIT 45.82 38.09 40.63 (+2.54) 38.41 46.61 (+8.20) 20.53 47.46 (+26.93) 51.03(+30.50)\nMedical 54.35 41.14 45.78 (+4.64) 47.94 55.36 (+7.42) 37.37 58.54 (+21.17) 60.40(+23.03)\nKoran 19.45 17.11 17.53 (+0.42) 23.33 19.27(−4.06) 17.59 18.80 (+1.21) 20.55(+2.96)\nLaw 61.78 45.92 48.97 (+3.05) 51.60 59.97 (+8.37) 41.04 61.85 (+20.81) 64.92(+23.88)\nTable 3: Comparison of the NMT models and the kNN-MT model on the multi-domain dataset by BLEU. The\nCOMET-22 version can be found in Table 7.\n30.00 40.00 50.00 60.00 70.00\nfew-shot\n(TM)\nfew-shot\n(out-domain)\nfew-shot\n(in-domain)\nzero-shot\n69.99\n42.45\n44.56\n38.89 de →en\n20.00 30.00 40.00 50.00 60.00 70.00\nfew-shot\n(TM)\nfew-shot\n(out-domain)\nfew-shot\n(in-domain)\nzero-shot\n62.02\n30.63\n31.91\n29.00 en →de\nFigure 4: BLEU scores of different prompting strategies\non the DGT-TM dataset. In-domain and out-domain\nrepresent demonstrations randomly selected from the\nTM database of the DGT-TM dataset and newstest2017,\nrespectively. TM represents top- k similar translation\nmemories (i.e., demonstrations) retrieved from the TM\ndatabase of the DGT-TM dataset.\nJRC-Acquis. From Figure 2, we see consistent\nimprovements over all the language pairs. Even\nfor non-English tasks, TMP LM can still achieve\nsignificant BLEU improvements. See Table 8 in\nAppendix B.3 for complete experimental results.\nMulti-domain Experiments. Table 3 shows\nBLEU results on the multi-domain dataset. Again,\nthe TMP LM system is robust to the domain shift.\nIt performs best on three of the four domains.\n3.4 Language Understanding Matters Most\nWe then investigate an interesting issue: what kind\nof ability do large models have to make better use\nof TM-based prompts? There are possibly three rea-\nsons, including the abilities oftranslating, logically\nreasoning and language understanding. However,\nas seen from Table 2, the baseline LLMs are not\nstrong translation systems and their BLEU scores\nare generally 10 points lower than the NMT sys-\ntems. The translation ability of LLMs does not turn\nout to be important in TM-based prompting. Note\nthat davinci-003 is a successor of GPT3 and\nis trained on additional large-scale code data. It\nhas been pointed out that training LLMs on code\ndata can lead to a strong ability of logical reasoning\n(Liang et al., 2022). As seen in Figure 3 (a), how-\never, no big difference between davinci-003\nand GPT3 in BLEU performance. On the other\nhand, davinci-003 has a significant ability to\ndeal with instructions because it is tuned by using\nfeedback to human instructions. Such a property\nmakes davinci-003 a better text processor, and\nthus a stronger translation system that works with\nvarious prompts. Therefore, it is the ability of lan-\nguage understanding that boosts LLMs’ translation\nperformance when prompted with TMs.\n3.5 Template Styles\nIn Figure 3 (b), we compare the performance be-\ntween the code-style and instruction-style tem-\nplates on the DGT-TM en-de and de-en tasks. For\nsystems without TMs, the instruction-style tem-\nplate shows similar performance as the code-style\ntemplate. However, when TMs are used, the code-\nstyle template is better in most cases. In Appendix\nB.2, we test more templates and see a similar phe-\nnomenon that simpler templates work better.\n3.6 Prompting with randomly selected\ndemonstrations\nWe also compare the performance of TMP LM with\nthe conventional few-shot, i.e., prompting LLM\ntranslators with randomly selected high quality\ndemonstrations (Vilar et al., 2022; Agrawal et al.,\n2022; Zhang et al., 2023; Moslem et al., 2023;\nHendy et al., 2023). We conduct experiments on\nthe DGT-TM dataset, with demonstrations selected\nfrom the TM database of the DGT-TM dataset (in-\ndomain) and newstest2017 (out-domain), respec-\ntively. In Figure 4, we see, TMP LM exceeds the\nconventional few-shot by about 30 BLEU points\nindicating that LLM can benefit from TMs much\nmore than the conventional few-shot itself. It also\ndemonstrates the valid information hinted by TMs\nas explained in Section 2.\n10290\n0 0.20.40.60.81.0\n20.0032.5045.0057.5070.00\nWMT19 200M\n(a) DGT-TM de →en (b) IT domain de →en\n0 0.20.40.60.81.0\n0 0.250.500.751.00WMT21 4B\nBLEU\nproportion\nFigure 5: BLEU scores as functions of thresholds of\nusing similar sentences in TMs on the DGT-TM and IT\ndomain data. The left y-axis represents the BLEU scores\nof prompting LLMs with the translation results from\nNMT systems, and the x-axis represents the similarity\n(i.e., the FMS in Appendix A.1) thresholds by which we\nhave a trade-off between using TMs and NMT results\nas prompts (1 means that we only use TMs as prompts,\nand 0 means that we only use NMT outputs as prompts).\nDeep and light red curves represent the performance\nof the LLMs when working with the WMT19 200M\nand WMT21 4B systems. Blue curves represent the\nproportion of the use of TMs (see the right y-axis).\n3.7 Combining TMs and NMT results\nTo examine the impact of high-quality transla-\ntions on prompting LLMs, we replace the retrieved\nTM with the translation result of the WMT19 and\nWMT21 NMT systems when the TM’s similarity\nis not high enough. We conducted experiments on\nthe DGT-TM de →en data and the IT data in the\nmulti-domain dataset because the sentence similar-\nity distributes differently on them (see Appendix\nA.1). In Figure 5, we can see that the performance\ndeclines as more NMT translation results replace\nthe TM results in prompting. This demonstrates\nthat the quality of translations plays an important\nrole in prompting LLMs. We also see that the per-\nformance on DGT-TM declines faster than that on\nIT domain. We attribute this to the better transla-\ntion quality of the NMT models on the DGT-TM\ndataset.\nThere is an interesting finding that the method of\nprompting LLMs with the NMT results cannot sur-\npass the NMT system itself, while the BLEU scores\nof prompting LLMs with TMs are always better\nthan those of the TMs. It indicates that LLMs in-\ndeed process the prompting texts rather than simply\noutputting the prompting texts.\n4 Conclusion\nWe have proposed TMP LM, an in-context learning\nmethod to prompt TMs for LLMs. By incorporat-\ning TMs into tailored templates, LLMs with TM-\nPLM outperforms the state-of-the-art NMT models\nwith TM prompting. We have also demonstrated\nthat the ability of language understanding plays an\nimportant role in prompting LLMs with TMs.\nLimitations\nThe similarity of TMs is an important factor in-\nfluencing the translations of TMP LM. However,\nhigh-similarity TMs are not always available in\npractical applications. It is worth studying methods\nto make use of relatively low-similarity translations\nin LLM-based translation systems.\nAcknowledgements\nThis work was supported in part by the National\nScience Foundation of China (No. 62276056), the\nNational Key R&D Program of China, the China\nHTRD Center Project (No. 2020AAA0107904),\nthe Natural Science Foundation of Liaoning\nProvince of China (2022-KF-16-01), the Yunnan\nProvincial Major Science and Technology Special\nPlan Projects (No. 202103AA080015), the Funda-\nmental Research Funds for the Central Universities\n(Nos. N2216016, N2216001, and N2216002), and\nthe Program of Introducing Talents of Discipline\nto Universities, Plan 111 (No. B16009).\nReferences\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nZettlemoyer, and Marjan Ghazvininejad. 2022. In-\ncontext examples selection for machine translation.\nCoRR, abs/2212.02437.\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 7747–7763. Associa-\ntion for Computational Linguistics.\nAndrzej Bialecki, Robert Muir, and Grant Ingersoll.\n2012. Apache lucene 4. In Proceedings of the SI-\nGIR 2012 Workshop on Open Source Information Re-\ntrieval, OSIR@SIGIR 2012, Portland, Oregon, USA,\n16th August 2012, pages 17–24. University of Otago,\nDunedin, New Zealand.\nBram Bulté and Arda Tezcan. 2019. Neural fuzzy re-\npair: Integrating fuzzy matches into neural machine\ntranslation. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 1800–1809. Association\nfor Computational Linguistics.\n10291\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge F. Foster, Alon Lavie, and André F. T. Mar-\ntins. 2022. Results of WMT22 metrics shared task:\nStop using BLEU - neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference on\nMachine Translation, WMT 2022, Abu Dhabi, United\nArab Emirates (Hybrid), December 7-8, 2022, pages\n46–68. Association for Computational Linguistics.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor\nO. K. Li. 2018. Search engine guided neural machine\ntranslation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 5133–5140. AAAI Press.\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\nLemao Liu. 2021. Fast and accurate neural machine\ntranslation with translation memory. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 3170–3180. Associa-\ntion for Computational Linguistics.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Has-\nsan Awadalla. 2023. How good are GPT models\nat machine translation? A comprehensive evaluation.\nCoRR, abs/2302.09210.\nNabil Hossain, Marjan Ghazvininejad, and Luke Zettle-\nmoyer. 2020. Simple and effective retrieve-edit-\nrerank text generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2020, Online, July 5-10, 2020,\npages 2532–2538. Association for Computational\nLinguistics.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021. OpenReview.net.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei\nSun, Tianwei Zhang, and Jiwei Li. 2022. Fast nearest\nneighbor machine translation. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\nDublin, Ireland, May 22-27, 2022 , pages 555–565.\nAssociation for Computational Linguistics.\nYasmin Moslem, Rejwanul Haque, and Andy Way. 2023.\nAdaptive machine translation with large language\nmodels. CoRR, abs/2301.13294.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nfair’s WMT19 news translation task submission. In\nProceedings of the Fourth Conference on Machine\nTranslation, WMT 2019, Florence, Italy, August 1-2,\n2019 - Volume 2: Shared Task Papers, Day 1, pages\n314–319. Association for Computational Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nAbudurexiti Reheman, Tao Zhou, Yingfeng Luo,\nDi Yang, Tong Xiao, and Jingbo Zhu. 2023. Prompt-\ning neural machine translation with translation mem-\nories. arXiv preprint arXiv:2301.05380.\nRicardo Rei, José G. C. de Souza, Duarte M. Alves,\nChrysoula Zerva, Ana C. Farinha, Taisiya Glushkova,\nAlon Lavie, Luísa Coheur, and André F. T. Martins.\n2022. COMET-22: unbabel-ist 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation, WMT\n2022, Abu Dhabi, United Arab Emirates (Hybrid),\nDecember 7-8, 2022, pages 578–585. Association for\nComputational Linguistics.\nRalf Steinberger, Andreas Eisele, Szymon Klocek,\nSpyridon Pilos, and Patrick Schlüter. 2012. DGT-\nTM: A freely available translation memory in 22\nlanguages. In Proceedings of the Eighth Interna-\ntional Conference on Language Resources and Eval-\nuation, LREC 2012, Istanbul, Turkey, May 23-25,\n2012, pages 454–459. European Language Resources\nAssociation (ELRA).\nRalf Steinberger, Bruno Pouliquen, Anna Widiger,\nCamelia Ignat, Tomaz Erjavec, Dan Tufis, and Dániel\nVarga. 2006. The jrc-acquis: A multilingual aligned\nparallel corpus with 20+ languages. In Proceedings\nof the Fifth International Conference on Language\nResources and Evaluation, LREC 2006, Genoa, Italy,\nMay 22-28, 2006, pages 2142–2147. European Lan-\nguage Resources Association (ELRA).\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation, LREC 2012, Istanbul, Turkey, May 23-\n25, 2012, pages 2214–2218. European Language Re-\nsources Association (ELRA).\n10292\nChau Tran, Shruti Bhosale, James Cross, Philipp Koehn,\nSergey Edunov, and Angela Fan. 2021. Facebook ai’s\nWMT21 news translation task submission. In Pro-\nceedings of the Sixth Conference on Machine Transla-\ntion, WMT@EMNLP 2021, Online Event, November\n10-11, 2021, pages 205–215. Association for Com-\nputational Linguistics.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2022. Prompt-\ning palm for translation: Assessing strategies and\nperformance. arXiv preprint arXiv:2211.09102.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nTong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li. 2012.\nNiutrans: An open source toolkit for phrase-based\nand syntax-based machine translation. In The 50th\nAnnual Meeting of the Association for Computational\nLinguistics, Proceedings of the System Demonstra-\ntions, July 10, 2012, Jeju Island, Korea, pages 19–24.\nThe Association for Computer Linguistics.\nJitao Xu, Josep Maria Crego, and Jean Senellart. 2020.\nBoosting neural machine translation with similar\ntranslations. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 1580–1590.\nAssociation for Computational Linguistics.\nBiao Zhang, Barry Haddow, and Alexandra Birch. 2023.\nPrompting large language model for machine transla-\ntion: A case study. arXiv preprint arXiv:2301.07069.\nJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-\nham Neubig, and Satoshi Nakamura. 2018. Guiding\nneural machine translation with retrieved translation\npieces. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2018, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 1 (Long Papers), pages\n1325–1335. Association for Computational Linguis-\ntics.\nA Detailed Experimental Setup\nA.1 Retrieval of Similar Sentences\nFollowing Reheman et al., 2023, we adopt a word-\nlevel fuzzy matching strategy, with the numbers\nand punctuation marks removed. Specifically, we\nfirst use the search engine Apache Lucene (Bialecki\net al., 2012) to acquire the Top 500 similar TMs\nfrom TM database, then rerank the most similar\nTM by using the length normalized Levenshtein\nDistance, given by\nFMS(X, S) = 1 − LD(X, S)\nmax(|X|, |S|) (3)\nwhere FMS(·, ·) denotes the Fuzzy Match Score,\nLD(·, ·) denotes the word level Levenshtein Dis-\ntance, and |·| denotes the length of a sentence.\nA.2 Details of Datasets\nDatasets and their language directions used in our\nexperiments are listed here.\n• The DGT-TM dataset(Tiedemann, 2012),\nwhich is bidirectional in English-German;\n• The JRC-Acquis (JRC-A) dataset(Steinberger\net al., 2006), which includes 8 language\npairs and 16 directions: English-German,\nEnglish-French, German-French, English-\nItalian, English-Romanian, English-Spanish,\nEnglish-Czech, and Czech-Italian;\n• The multi-domain dataset (Aharoni and Gold-\nberg, 2020), which includes 4 domains in the\nGerman to English direction: Medical, Law,\nIT, and Koran.\nThe statistics of these TM and the corresponding\nsimilarity ratios of retrieved sentences in the FMS\nmetric are shown in Table 4.\nA.3 Data Pre-processing\nFor the DGT-TM, JRC-A and multi-domain\ndatasets, we clean the data using the scripts pro-\nvided by Reheman et al. (2023)’s work. To con-\nstruct the test set and TM database for the DGT-\nTM and JRC-A datasets, we process each language\ndirection separately. Specifically, we randomly ex-\ntract 3,000 sentence pairs from each dataset as the\ntest set, and use the remaining sentence pairs as\nthe TM database. For the multi-domain dataset,\nwe use its original test set as our test set and its\noriginal training set as the TM database. We use\nthe FMS algorithm on the split data to obtain the\nTM corresponding to the test set. In particular, for\nthe few-shot experiments, we retrieved the k most\nsimilar sentence pairs from the TM database for\neach test sentence.\nFinally, we replace the escaped characters in\nthe dataset and use Moses§ decoder detokenizer to\nrecover the tokenized data before feeding it to the\ndavinci-003 system.\nA.4 Data Post-processing\ndavinci-003 always generates redundant sym-\nbols at the beginning and end of sentences, in-\ncluding: ‘\"’, ‘ \\n’, ‘[’, ‘]’, and other escaped\n§http://www.statmt.org/moses/\n10293\nDataset Lang Domain TM scale FMS\n[0, 0.2) [0.2, 0.4) [0.4, 0.6) [0.6, 0.8) [0.8, 1.0)\nDGT-TM En-De - 3.1M 2% 23% 16% 17% 42%\nDe-En - 3.1M 4% 26% 17% 17% 36%\nJRC-A\nEn-De - 423K 6% 33% 18% 13% 30%\nDe-En - 423K 6% 33% 18% 15% 28%\nEn-Fr - 424K 3% 34% 19% 14% 30%\nFr-En - 424K 3% 33% 19% 15% 30%\nDe-Fr - 846K 9% 34% 16% 12% 29%\nFr-De - 846K 8% 34% 16% 12% 30%\nEn-It - 433K 7% 32% 18% 14% 29%\nIt-En - 433K 7% 32% 17% 16% 28%\nEn-Ro - 273K 7% 39% 21% 14% 19%\nRo-En - 273K 6% 37% 22% 15% 20%\nEn-Es - 432K 2% 34% 20% 16% 28%\nEs-En - 432K 2% 34% 20% 16% 28%\nEn-Cs - 681K 12% 33% 17% 12% 26%\nCs-En - 681K 13% 32% 15% 13% 27%\nCs-It - 714K 11% 31% 17% 14% 27%\nIt-En - 714K 12% 32% 16% 13% 27%\nmulti-domain\nDe-En IT 223K 14% 18% 28% 26% 14%\nDe-En Koran 18K 2% 26% 33% 28% 11%\nDe-En Law 467K 8% 31% 18% 14% 28%\nDe-En Medical 248K 7% 23% 20% 17% 33%\nWMT14 En-De - 4.5M 18% 68% 12% 1% 1%\nWMT19 De-En - 30M 13% 65% 19% 2% 1%\nTable 4: TMs and proportions of the retrieved sentences in different ranges of FMS.\ncharacters. The occurrences of these charac-\nters is regular and can be removed uniformly\nby scripts. Consequently, before scoring, we\nuse NiuTrans (Xiao et al., 2012) word segmen-\ntation tool for Chinese and Moses decoder’s\ntokenizer.perl for all other languages. Fi-\nnally we use multi-bleu.perl for scoring.\nA.5 More Prompt Templates\nWe try a large number of prompt templates, as\nshown in Table 5. Without special specification, the\ninstruction-style template with TM is the #1, and\nwithout TM is the #2, and the code-style template\nwith TM is the #17, and without TM is the #18. In\nparticular, in the multi-language experiment, we\nuse the instruction-style template. The template\nfor all of the few-shot experiments is obtained by\nincreasing the number of TMs in #17.\nPunctuation has a significant impact on the gen-\neration results. For example, using template #13,\nif the source sentence ends with ‘:’, it will lead\nthe model to continue generating words but not\nstop in an appropriate number of decoding steps.\nMeanwhile, although many templates have a simi-\nlar form, their performance still differs. We believe\nthat adding a strong boundary signal to the tem-\nplates helps the model to know where to end.\nB More Experimental Results\nB.1 Evaluation by COMET-22\nExcept for the BLEU scores, we also provide the\nCOMET-22 scores as seen in Table 6 and Table 7.\nWe can see that despite LLM’s poor performance\non zero-shot, prompting LLM with a few TMs\ncan achieve significant improvement. On the other\nhand, the few-shot learning+LLM system can still\noutperform the strong NMT+TM baseline in most\ncases.\nB.2 Performance of Different Prompt\nTemplates\nIn order to explore the effect of using differ-\nent prompt templates on the performance of\ndavinci-003, we use 20 prompt templates in\nthe de →en direction of the DGT-TM dataset for\nexperiments. Seen from table 5, the code-style tem-\nplate is better than the instruction-style template in\nmost cases.\nB.3 Experiments on More languages\nWe perform multi-lingual experiments on the JRC-\nA dataset, and in these experiments, we use the\ninstruction-style template as shown in Figure 1. Ta-\nble 8 shows the complete experiment results for the\n10294\nNo. Prompt Template With Sample BLEUTM\n1\nIf the translation of \"xtm\" fromsrc-lang\nYes\nIf the translation of \"I have an apple.\" from English\n63.97totgt-langis \"ytm\", then what is the to German is \"Ich habe einen Apfel.\" then what is the\ntranslation of \"x\" fromsrc-langtotgt-lang? translation of \"I have an orange.\" from English to German?\nOnly translation results are required. Only translation results are required.\n2\nWhat is the translation of \"x\"\nNo\nWhat is the translation of \"I have an apple.\"\n38.38fromsrc-langtotgt-lang? from English to German?\nOnly translation results are required. Only translation results are required.\n3\nIf \"xtm\" translated intotgt-langis \"ytm\",\nYes\nIf \"I have an apple.\" translated into German\n61.9then what is the translation of \"x\" is \"Ich habe einen Apfel.\" then what is the\nshould be if translated intotgt-lang? translation of \"I have an orange.\" should be if translated\nOnly translation results are required. into German? Only translation results are required.\n4\nWhat is the translation of \"x\"\nNo\nWhat is the translation of \"I have an apple.\"\n37.93should be if translated intotgt-lang? should be if translated into German?\nOnly translation results are required. Only translation results are required.\n5\nIf [xtm] translated intotgt-langis [ytm],\nYes\nIf [I have an apple.] translated into German\n61.78then what is the translation of [x] is [Ich habe einen Apfel.] then what is the\nshould be if translated intotgt-lang? translation of [I have an orange.] should be if translated\nOnly translation results are required. into German? Only translation results are required.\n6\nTranslatesrc-langtotgt-lang.\\n\nYes\nTranslate English to German.\\n\n65.25\n[src-lang]: [xtm]\\n [English]: [I have an apple.]\\n\n[tgt-lang]: [ytm]\\n [German]: [Ich habe einen Apfel.]\\n\n[src-lang]: [x]\\n [English]: [I have an orange.]\\n\n[tgt-lang]: [German]:\n7\nTranslatesrc-langtotgt-lang.\\n\nYes\nTranslate English to German.\\n\n66.02\n[src-lang]=[xtm]\\n [English]=[I have an apple.]\\n\n[tgt-lang]=[ytm]\\n [German]=[Ich habe einen Apfel.]\\n\n[src-lang]=[x]\\n [English]=[I have an orange.]\\n\n[tgt-lang]= [German]=\n8\nTranslatesrc-langtotgt-lang.\nYes\nTranslate English to German.\n66.08[src-lang]=[xtm] [tgt-lang]=[ytm] [English]=[I have an apple.] [German]=[Ich habe einen Apfel.]\n[src-lang]=[x] [tgt-lang]= [English]=[I have an orange.] [German]=\n9\nTranslatesrc-langtotgt-lang.\\n\nYes\nTranslate English to German.\\n\n66.20[src-lang]=[xtm] [tgt-lang]=[ytm]\\n [English]=[I have an apple.] [German]=[Ich habe einen Apfel.]\\n\n[src-lang]=[x] [tgt-lang]= [English]=[I have an orange.] [German]=\n10 ifsrc-lang= [xtm] thentgt-lang= [ytm]; Yes if English = [I have an apple.] then German = [Ich habe einen Apfel.];66.75ifsrc-lang= [x] thentgt-lang= if English = [I have an orange.] then German =\n11 src-lang=\"xtm\" tgt-lang=\"ytm\" Yes English=\"I have an apple.\" German=\"Ich habe einen Apfel.\" 66.28src-lang=\"x\"tgt-lang= English=\"I have an orange.\" German=\n12 src-lang=[xtm] tgt-lang=[ytm] Yes English=[I have an apple.] German=[Ich habe einen Apfel.] 65.37src-lang=[x]tgt-lang= English=[I have an orange.] German=\n13 [src-lang] xtm [tgt-lang] ytm Yes [English] I have an apple. [German] Ich habe einen Apfel. 58.47[src-lang] x [tgt-lang] [English] I have an orange. [German]\n14 [src-lang]: [xtm] [tgt-lang]: [ytm] Yes [English]: [I have an apple.] [German]: [Ich habe einen Apfel.]65.54[src-lang]: [x] [tgt-lang]: [English]: [I have an orange.] [German]:\n15 [src-lang]: [x] [tgt-lang]: No [English]: [I have an orange.] [German]: 39.83\n16 [src-lang] = [xtm] [tgt-lang] = [ytm] Yes [English] = [I have an apple.] [German] = [Ich habe einen Apfel.]66.45[src-lang] = [x] [tgt-lang] = [English] = [I have an orange.] [German] =\n17 [src-lang]=[xtm] [tgt-lang]=[ytm] Yes [English]=[I have an apple.] [German]=[Ich habe einen Apfel.]66.90[src-lang]=[x] [tgt-lang]= [English]=[I have an orange.] [German]=\n18 [src-lang]=[x] [tgt-lang]= No [English]=[I have an orange.] [German]= 38.89\n19 {src-lang}={xtm} {tgt-lang}={ytm} Yes {English}={I have an apple.} {German}={Ich habe einen Apfel.}65.48{src-lang}={x} {tgt-lang}= {English}={I have an orange.} {German}=\n20 {[src-lang]=[xtm]} {[tgt-lang]=[ytm]} Yes {[English]=[I have an apple.]} {[German]=[Ich habe einen Apfel.]}63.32{[src-lang]=[x]} {[tgt-lang]=} {[English]=[I have an orange.]} {[German]=}\nTable 5: Comparison of prompt templates in one-shot TM (i.e., k = 1). Abbreviations are same as Figure 1.\nmulti-language experiment. Great BLEU improve-\nments are obtained on these datasets.\nB.4 Impact of k\nTo explore the effect of k on the performance of\ndavinci-003 in the few-shot experiments, we\nconduct experiments with k from 1 to 9 in both\ndirections of the DGT-TM dataset. Figure 6 shows\na long-tail performance gain as k increases.\n10295\nData\nWMT19 200M WMT21 4B davinci-003 175B\nNMT NMT+TM NMT NMT+TM LLM LLM+TM LLM+TM(zero-shot) (one-shot) (few-shot)\nDGT-TM de → en 85.99 87.28 (+1.29) 87.10 89.28 (+2.18) 83.86 88.74 (+4.88) 89.47(+5.61)\nen → de 85.52 86.91 (+1.39) 86.89 89.01 (+2.12) 82.24 88.52 (+6.28) 89.44(+7.20)\nJRC-A de → en 85.85 85.80 (−0.05) 86.68 87.70 (+1.02) 84.15 87.79 (+3.64) 88.46(+4.31)\nen → de 86.53 86.25 (−0.28) 87.39 88.88(+1.49) 84.03 88.20 (+4.17) 88.84(+4.81)\nTable 6: COMET-22 scores of NMT models and LLMs on the DGT-TM and JRC-A dataset.\nDomain\nWMT19 200M WMT21 4B davinci-003 175B\nNMT NMT+TM NMT NMT+TM LLM LLM+TM LLM+TM(zero-shot) (one-shot) (few-shot)\nIT 83.04 83.87 (+0 .83) 83.54 85.09 (+1 .55) 72.44 86.05 (+13 .61) 87.27 (+14 .83)\nMedical 83.30 83.61 (+0 .31) 84.92 84.97 (+0 .05) 80.76 84.97 (+4 .21) 86.63 (+5 .87)\nKoran 72.42 72.00 (−0.42) 75.09 72.23 (−2.86) 73.35 73.65 (+0 .30) 74.34 (+0 .99)\nLaw 85.80 85.53 (−0.27) 86.75 87.23 (+0 .48) 83.30 87.49 (+4 .19) 88.47 (+5 .17)\nTable 7: COMET-22 scores of NMT models and LLMs on the multi-domain dataset.\nLang Cs-En Cs-It De-En De-Fr\nDirection → ← → ← → ← → ←\nw/o TM 58.58 52.58 47.93 47.22 59.74 53.56 53.52 50.08\nw/ TM 37.50 28.02 27.02 22.85 36.34 28.74 34.78 28.93\n∆ +21.08 +24.56 +20.91 +24.37 +23.40 +24.82 +18.74 +21.15\nLang Es-En Fr-En It-En Ro-En\nDirection → ← → ← → ← → ←\nw/o TM 61.25 59.18 64.45 64.60 61.80 57.71 59.66 50.18\nw/ TM 41.89 37.10 44.06 43.78 41.13 33.53 41.37 27.06\n∆ +19.36 +22.08 +20.39 +20.78 +20.67 +24.18 +18.29 +23.12\nTable 8: Experiment results on 8 language-pairs from JRC-A.\nData NMT FMS\nSystem 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nDGT-TM\nen→de WMT19 57.39 57.53 57.70 58.02 57.71 57.08 56.07 54.73 53.05 48.67 37.06\nde→en 66.90 66.90 66.88 66.34 65.63 64.30 62.70 61.04 58.45 54.15 44.21\nde→en WMT21 66.90 66.90 67.06 67.02 66.74 65.84 64.58 63.08 60.59 57.33 49.21\nIT de→en WMT19 47.46 44.48 43.85 43.30 40.78 36.92 35.78 32.55 29.27 28.17 26.65\nde→en WMT21 47.46 44.01 43.19 42.31 37.53 34.35 33.31 30.17 27.23 26.25 24.90\nLaw de→en WMT19 61.85 61.84 61.67 60.89 59.61 58.04 56.75 55.04 53.00 50.10 44.12\nde→en WMT21 61.85 61.83 62.00 61.99 61.39 60.34 59.32 57.98 56.31 53.93 47.56\nMedical de→en WMT19 58.54 58.45 58.32 58.05 57.25 55.34 54.06 52.62 50.03 47.01 38.85\nde→en WMT21 58.54 58.45 58.32 58.11 57.30 55.44 54.14 52.74 20.15 47.14 38.97\nTable 9: Performance of replacing the low-matching part of TMs at different thresholds of FMS with the translation\nresults from NMT. For example, FMS 0.2 in first row means that TMs with FMS less than 0.2 are replaced by NMT\ntranslation results.\nB.5 Impact of Orders of TM results\nTo observe the effect of constructing the prompt\ntemplate with different TMs similarity orders on\nthe performance in the few-shot experiments, we\nconstructed two types of prompt templates in the\nDGT-TM dataset with a few-shot sample size of\n5. One is arranged in descending order of TMs\nsimilarity, and the TM adjacent to the sentence to\n10296\n2 3 4 5 6 7 8 9\n60.00\n62.00\n64.00\n66.00\n68.00\n70.00\nk\nBLEU\nen2de de2en\nFigure 6: BLEU scores of different k on the DGT\ndataset\nLang Direction Descending Ascending\nde →en 69.99 70.01\nen →de 62.02 62.30\nTable 10: The performance comparison of different\ntemplates which is constructed based on the similarity\nof TM when the number of few-shot samples is 5.\nModel TM WMT14 WMT19\nEn2De De2En\nTransformer-base w/o TM 27.59 39.67\nw/ TM 21.86 40.22\ntext-davinci-003 w/o TM 29.58 40.85\nw/ TM 28.11 36.63\nTable 11: Comparison of performance on WMT dataset.\nbe translated is the lowest similarity. The other one\nis arranged in ascending order of TMs similarity,\nand the TM adjacent to the sentence to be translated\nis the highest similarity. The results are shown in\nTable 10.\nB.6 Performance on the WMT Datasets\nWe conduct experiments on WMT14 en →de and\nWMT19 de →en directions. We use the same\nmethod as that used on the multi-domain dataset to\nprocess these two benchmarks. It is worth noting\nthat the data obtained on these two benchmarks\nhave a low similarity of TMs, as shown in Table\n11. Table 11 shows the performance of the LLM\nand baseline models on the WMT14 en →de and\nWMT19 de →en datasets.\nModel BLEU\ntext-davinci-003 66.90\ndavinci(GPT3) 65.48\ntext-curie-001 42.30\ntext-babbage-001 37.72\ntext-ada-001 14.65\nTable 12: Comparison of performance with different\nsize models on DGT-TM de →en.\nB.7 Performance of Different Sized Models\nMoreover, we conduct experiments using\n“small” models such as text-curie-001 and\ntext-babbage-001. But their performance is\nfar away behind davinci-003 whose outputs\ncontain null in lines sometimes. We attribute this\nto the lack of emergent abilities of big models (Wei\net al., 2022). The results are shown in Table 12.\n10297\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitation In Page 5\n□\u0013 A2. Did you discuss any potential risks of your work?\nLimitation In Page 5\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection1, in Page 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 3.2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n10298\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3.1\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nWe draw conclusions on the basis of experiments conducted on a wide range of datasets and language\npairs.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix A\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n10299",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8074634671211243
    },
    {
      "name": "Machine translation",
      "score": 0.7395379543304443
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5706706047058105
    },
    {
      "name": "Translation (biology)",
      "score": 0.545292317867279
    },
    {
      "name": "Natural language processing",
      "score": 0.5322492122650146
    },
    {
      "name": "Language model",
      "score": 0.5244781374931335
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5189014077186584
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.46732962131500244
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4516626000404358
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3294515609741211
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}