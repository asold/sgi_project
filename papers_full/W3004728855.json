{
  "title": "Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss",
  "url": "https://openalex.org/W3004728855",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2044492937",
      "name": "Zhang Qian",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2005904254",
      "name": "Lu Han",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2114627456",
      "name": "Sak, Hasim",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2745035099",
      "name": "Tripathi, Anshuman",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2921529615",
      "name": "McDermott, Erik",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3204082710",
      "name": "Koo Stephen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2587552697",
      "name": "Kumar Shankar",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2941814890",
    "https://openalex.org/W1560013842",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2577366047",
    "https://openalex.org/W2108677974",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2981696585",
    "https://openalex.org/W3008174054",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W2972451902",
    "https://openalex.org/W2491408735",
    "https://openalex.org/W2802023636",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W2746192915",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2981857663",
    "https://openalex.org/W2963414781",
    "https://openalex.org/W2913718171",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1494198834"
  ],
  "abstract": "In this paper we present an end-to-end speech recognition model with Transformer encoders that can be used in a streaming speech recognition system. Transformer computation blocks based on self-attention are used to encode both audio and label sequences independently. The activations from both audio and label encoders are combined with a feed-forward layer to compute a probability distribution over the label space for every combination of acoustic frame position and label history. This is similar to the Recurrent Neural Network Transducer (RNN-T) model, which uses RNNs for information encoding instead of Transformer encoders. The model is trained with the RNN-T loss well-suited to streaming decoding. We present results on the LibriSpeech dataset showing that limiting the left context for self-attention in the Transformer layers makes decoding computationally tractable for streaming, with only a slight degradation in accuracy. We also show that the full attention version of our model beats the-state-of-the art accuracy on the LibriSpeech benchmarks. Our results also show that we can bridge the gap between full attention and limited attention versions of our model by attending to a limited number of future frames.",
  "full_text": "TRANSFORMER TRANSDUCER: A STREAMABLE SPEECH RECOGNITION MODEL\nWITH TRANSFORMER ENCODERS AND RNN-T LOSS\nQian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, Shankar Kumar\n{zhaqian, luha, hasim, anshumant, erikmcd, kookaburra, shankarkumar}@google.com\nGoogle Inc., USA\nABSTRACT\nIn this paper we present an end-to-end speech recognition model\nwith Transformer encoders that can be used in a streaming speech\nrecognition system. Transformer computation blocks based on self-\nattention are used to encode both audio and label sequences indepen-\ndently. The activations from both audio and label encoders are com-\nbined with a feed-forward layer to compute a probability distribution\nover the label space for every combination of acoustic frame position\nand label history. This is similar to the Recurrent Neural Network\nTransducer (RNN-T) model, which uses RNNs for information en-\ncoding instead of Transformer encoders. The model is trained with\nthe RNN-T loss well-suited to streaming decoding. We present re-\nsults on the LibriSpeech dataset showing that limiting the left context\nfor self-attention in the Transformer layers makes decoding compu-\ntationally tractable for streaming, with only a slight degradation in\naccuracy. We also show that the full attention version of our model\nbeats the-state-of-the art accuracy on the LibriSpeech benchmarks.\nOur results also show that we can bridge the gap between full at-\ntention and limited attention versions of our model by attending to a\nlimited number of future frames.\nIndex Terms— Transformer, RNN-T, sequence-to-sequence,\nencoder-decoder, end-to-end, speech recognition\n1. INTRODUCTION\nIn the past few years, models employing self-attention [1] have\nachieved state-of-art results for many tasks, such as machine trans-\nlation, language modeling, and language understanding [1, 2]. In\nparticular, large Transformer-based language models have brought\ngains in speech recognition tasks when used for second-pass re-\nscoring and in ﬁrst-pass shallow fusion [3]. As typically used in\nsequence-to-sequence transduction tasks [4, 5, 6, 7, 8], Transformer-\nbased models attend over encoder features using decoder features,\nimplying that the decoding has to be done in a label-synchronous\nway, thereby posing a challenge for streaming speech recognition\napplications. An additional challenge for streaming speech recog-\nnition with these models is that the number of computations for\nself-attention increases quadratically with input sequence size. For\nstreaming to be computationally practical, it is highly desirable that\nthe time it takes to process each frame remains constant relative to\nthe length of the input. Transformer-based alternatives to RNNs\nhave recently been explored for use in ASR [9, 10, 11, 12].\nFor streaming speech recognition models, recurrent neural net-\nworks (RNNs) have been the de factochoice since they can model\nthe temporal dependencies in the audio features effectively [13]\nTHIS IS THE FINAL VERSION OF THE PAPER SUBMITTED TO\nTHE ICASSP 2020 ON OCT 21, 2019.\nFig. 1. RNN/Transformer Transducer architecture.\nwhile maintaining a constant computational requirement for each\nframe. Streamable end-to-end modeling architectures such as the\nRecurrent Neural Network Transducer (RNN-T) [14, 15, 16], Re-\ncurrent Neural Aligner (RNA) [17], and Neural Transducer [18]\nutilize an encoder-decoder based framework where both encoder\nand decoder are layers of RNNs that generate features from audio\nand labels respectively. In particular, the RNN-T and RNA models\nare trained to learn alignments between the acoustic encoder features\nand the label encoder features, and so lend themselves naturally to\nframe-synchronous decoding.\nSeveral optimization techniques have been evaluated to enable\nrunning RNN-T on device [16]. In addition, extensive architecture\nand modeling unit exploration has been done for RNN-T [15]. In\nthis paper, we explore the possibility of replacing RNN-based au-\ndio and label encoders in the conventional RNN-T architecture with\nTransformer encoders. With a view to preserving model streama-\nbility, we show that Transformer-based models can be trained with\nself-attention on a ﬁxed number of past input frames and previous\nlabels. This results in a degradation of performance (compared to\nattending to all past input frames and labels), but then the model\nsatisﬁes a constant computational requirement for processing each\nframe, making it suitable for streaming. Given the simple archi-\ntecture and parallelizable nature of self-attention computations, we\nobserve large improvements in training time and training resource\nutilization compared to RNN-T models that employ RNNs.\nThe RNN-T architecture 1 (as depicted in Figure 1) is a neural\nnetwork architecture that can be trained end-to-end with the RNN-\n1We use ”RNN-T architecture” or ”RNN-T model” interchangeably in\nthis paper to refer to the neural network architecture described in Eq. (3), and\nEq. (4), and ”RNN-T loss”, deﬁned in Eq. (5), to refer to the loss used to train\nthis architecture.\narXiv:2002.02562v2  [eess.AS]  14 Feb 2020\nT loss to map input sequences (e.g. audio feature vectors) to tar-\nget sequences (e.g. phonemes, graphemes). Given an input se-\nquence of real-valued vectors of length T, x = (x1,x2,...,x T ),\nthe RNN-T model tries to predict the target sequence of labels y =\n(y1,y2,...,y U ) of length U.\nUnlike a typical attention-based sequence-to-sequence model,\nwhich attends over the entire input for every prediction in the output\nsequence, the RNN-T model gives a probability distribution over the\nlabel space at every time step, and the output label space includes\nan additional null label to indicate the lack of output for that time\nstep — similar to the Connectionist Temporal Classiﬁcation (CTC)\nframework [19]. But unlike CTC, this label distribution is also con-\nditioned on the previous label history.\nThe RNN-T model deﬁnes a conditional distribution P(z|x)\nover all the possible alignments, where\nz = [(z1,t1),(z2,t2),..., (zU ,tU )]\nis a sequence of (zi,ti) pairs of length U, and (zi,ti) represents an\nalignment between output label zi and the encoded feature at time\nti. The labels zi can optionally be blank labels (null predictions).\nRemoving the blank labels gives the actual output label sequence y,\nof length U.\nWe can marginalize P(z|x) over all possible alignments z to\nobtain the probability of the target label sequence y given the input\nsequence x,\nP(y|x) =\n∑\nz∈Z(y,T)\nP(z|x), (1)\nwhere Z(y,T) is the set of valid alignments of lengthT for the label\nsequence.\n2. TRANSFORMER TRANSDUCER\n2.1. RNN-T Architecture and Loss\nIn this paper, we present all experimental results with the RNN-T\nloss [14] for consistency, which performs similarly to the monotonic\nRNN-T loss [20] in our experiments.\nThe probability of an alignment P(z|x) can be factorized as\nP(z|x) =\n∏\ni\nP(zi|x,ti,Labels(z1:(i−1))), (2)\nwhere Labels(z1:(i−1)) is the sequence of non-blank labels in\nz1:(i−1). The RNN-T architecture parameterizes P(z|x) with an\naudio encoder, a label encoder, and a joint network. The encoders\nare two neural networks that encode the input sequence and the tar-\nget output sequence, respectively. Previous work [14] has employed\nLong Short-term Memory models (LSTMs) as the encoders, giving\nthe RNN-T its name. However, this framework is not restricted to\nRNNs. In this paper, we are particularly interested in replacing the\nLSTM encoders with Transformers [1, 2]. In the following, we refer\nto this new architecture as the Transformer Transducer (T-T). As in\nthe original RNN-T model, the joint network combines the audio\nencoder output at ti and the label encoder output given the previous\nnon-blank output label sequence Labels(z1:(i−1)) using a feed-\nforward neural network with a softmax layer, inducing a distribution\nover the labels. The model deﬁnes P(zi|x,ti,Labels(z1:(i−1))) as\nfollows:\nJoint =Linear(AudioEncoderti (x))+\nLinear(LabelEncoder(Labels(z1:(i−1))))) (3)\nP(zi|x,ti,Labels(z1:(i−1))) =\nSoftmax(Linear(tanh(Joint))), (4)\nwhere each Linear function is a different single-layer feed-forward\nneural network, AudioEncoderti (x) is the audio encoder output at\ntime ti, and LabelEncoder(Labels(z1:(i−1))) is the label encoder\noutput given the previous non-blank label sequence.\nTo compute Eq. (1) by summing all valid alignments naively\nis computationally intractable. Therefore, we deﬁne the forward\nvariable α(t,u) as the sum of probabilities for all paths ending at\ntime-frame t and label position u. We then use the forward algo-\nrithm [14, 21] to compute the last alpha variable α(T,U ), which\ncorresponds to P(y|x) deﬁned in Eq. (1). Efﬁcient computation of\nP(y|x) using the forward algorithm is enabled by the fact that the\nlocal probability estimate (Eq. (4)) at any given label position and\nany given time-frame is not dependent on the alignment [14]. The\ntraining loss for the model is then the sum of the negative log prob-\nabilities deﬁned in Eq. (1) over all the training examples,\nloss = −\n∑\ni\nlog P(yi|xi) =−\n∑\ni\nα(Ti,Ui), (5)\nwhere Ti and Ui are the lengths of the input sequence and the output\ntarget label sequence of the i-th training example, respectively.\n2.2. Transformer\nThe Transformer [1] is composed of a stack of multiple identical\nlayers. Each layer has two sub-layers, a multi-headed attention layer\nand a feed-forward layer. Our multi-headed attention layer ﬁrst\napplies LayerNorm, then projects the input to Query, Key, and\nValue for all the heads [2]. The attention mechanism is applied\nseparately for different attention heads. The attention mechanism\nprovides a ﬂexible way to control the context that the model uses.\nFor example, we can mask the attention score to the left of the\ncurrent frame to produce output conditioned only on the previous\nstate history. The weight-averaged Values for all heads are con-\ncatenated and passed to a dense layer. We then employ a residual\nconnection on the normalized input and the output of the dense\nlayer to form the ﬁnal output of the multi-headed attention sub-\nlayer (i.e. LayerNorm(x) + AttentionLayer(LayerNorm(x)),\nwhere x is the input to the multi-headed attention sub-layer).\nWe also apply dropout on the output of the dense layer to pre-\nvent overﬁtting. Our feed-forward sub-layer applies LayerNorm\non the input ﬁrst, then applies two dense layers. We use ReLu\nas the activation for the ﬁrst dense layer. Again, dropout to\nboth dense layers for regularization, and a residual connection\nof normalized input and the output of the second dense layer (i.e.\nLayerNorm(x) + FeedForwardLayer(LayerNorm(x)), where x\nis the input to the feed-forward sub-layer) are applied. See Figure 2\nfor more details.\nNote that LabelEncoder states do not attend toAudioEncoder\nstates, in contrast to the architecture in [1]. As discussed in the Intro-\nduction, doing so poses a challenge for streaming applications. In-\nstead, we implement AudioEncoder and LabelEncoder in Eq. (3),\nwhich are LSTMs in conventional RNN-T architectures [14, 16, 15],\nusing the Transformers described above. In tandem with the RNN-T\narchitecture described in the previous section, the attention mecha-\nnism here only operates within AudioEncoder or LabelEncoder,\ncontrary to the standard practice for Transformer-based systems. In\naddition, so as to model sequential order, we use the relative posi-\ntional encoding proposed in [2]. With relative positional encoding,\nthe encoding only affects the attention score instead of the Values\nbeing summed. This allows us to reuse previously computed states\nFig. 2. Transformer encoder architecture.\nTable 1. Transformer encoder parameter setup.\nInput feature/embedding size 512\nDense layer 1 2048\nDense layer 2 1024\nNumber attention heads 8\nHead dimension 64\nDropout ratio 0.1\nrather than recomputing all previous states and getting the last state\nin an overlapping inference manner when the number of frames or la-\nbels that AudioEncoder or LabelEncoder processed is larger than\nthe maximum length used during training (which would again be\nintractable for streaming applications). More speciﬁcally, the com-\nplexity of running one-step inference to get activations at time tis\nO(t), which is the computation cost of attending totstates and of the\nfeed-forward process for the current step when using relative posi-\ntional encoding. On the other hand, with absolute positional encod-\ning, the encoding added to the input should be shifted by one whent\nis larger than the maximum length used during training, which pre-\ncludes re-use of the states, and makes the complexity O(t2). How-\never, even if we can reduce the complexity fromO(t2) to O(t) with\nrelative positional encoding, there is still the issue of latency grow-\ning over time. One intuitive solution is to limit the model to attend to\na moving window W of states, making the one-step inference com-\nplexity constant. Note that training or inference with attention to lim-\nited context is not possible for Transformer-based models that have\nattention from Decoder to Encoder, as such a setup is itself trying\nto learn the alignment. In contrast, the separation ofAudioEncoder\nand LabelEncoder, and the fact that the alignment is handled by a\nseparate forward-backward process, within the RNN-T architecture,\nmakes it possible to train with attention over an explicitly speciﬁed,\nlimited context.\n3. EXPERIMENTS AND RESULTS\n3.1. Data\nWe evaluated the proposed model using the publicly available Lib-\nriSpeech ASR corpus [24]. The LibriSpeech dataset consists of 970\nTable 2. Comparison of WERs for Hybrid (streamable), LAS (e2e),\nRNN-T (e2e & streamable) and Transformer Transducer models\n(e2e & streamable) on LibriSpeech test sets.\nModel Param No LM (%) With LM (%)\nsize clean other clean other\nHybrid [22] - - - 2.26 4.85\nLAS[23] 361M 2.8 6.8 2.5 5.8\nBiLSTM RNN-T 130M 3.2 7.8 - -\nFullAttn T-T (Ours) 139M 2.4 5.6 2.0 4.6\nTable 3. Limited left context per layer for audio encoder.\nAudio Mask Label Mask WER (%)\nleft right left Test-clean Test-other\n10 0 20 4.2 11.3\n6 0 20 4.3 11.8\n2 0 20 4.5 14.5\nhours of audio data with corresponding text transcripts (around 10M\nword tokens) and an additional 800M word token text only dataset.\nThe paired audio/transcript dataset was used to train T-T models and\nan LSTM-based baseline. The full 810M word tokens text dataset\nwas used for standalone language model (LM) training. We ex-\ntracted 128-channel logmel energy values from a 32 ms window,\nstacked every 4 frames, and sub-sampled every 3 frames, to produce\na 512-dimensional acoustic feature vector with a stride of 30 ms.\nFeature augmentation [23] was applied during model training to pre-\nvent overﬁtting and to improve generalization, with only frequency\nmasking (F = 50, mF = 2) and time masking (T = 30, mT = 10).\n3.2. Transformer Transducer\nOur Transformer Transducer model architecture has 18 audio and 2\nlabel encoder layers. Every layer is identical for both audio and la-\nbel encoders. The details of computations in a layer are shown in\nFigure 2 and Table 1. All the models for experiments presented\nin this paper are trained on 8x8 TPU with a per-core batch size\nof 16 (effective batch size of 2048). The learning rate schedule\nis ramped up linearly from 0 to 2.5e−4 during ﬁrst 4K steps, it\nis then held constant till 30K steps and then decays exponentially\nto 2.5e−6 till 200K steps. During training we also added a gaus-\nsian noise(µ = 0,σ = 0.01) to model weights [25] starting at 10K\nsteps. We train this model to output grapheme units in all our exper-\niments. We found that the Transformer Transducer models trained\nmuch faster ( ≈ 1 day) compared to the an LSTM-based RNN-T\nmodel (≈3.5 days), with a similar number of parameters.\n3.3. Results\nWe ﬁrst compared the performance of Transformer Transducer (T-\nT) models with full attention on audio to an RNN-T model using a\nbidirectional LSTM audio encoder. As shown in Table 2, the T-T\nmodel signiﬁcantly outperforms the LSTM-based RNN-T baseline.\nWe also observed that T-T models can achieve competitive recogni-\ntion accuracy with existing wordpiece-based end-to-end models with\nsimilar model size. To compare with systems using shallow fusion\n[19, 26] with separately trained LMs, we also trained a Transformer-\nbased LM with the same architecture as the label encoder used in\nT-T, using the full 810M word token dataset. This Transformer LM\nFig. 3. Transformer context masking for the y7 position (left=2,\nright=1)\nTable 4. Limited right context per layer for audio encoder.\nAudio Mask Label Mask WER (%)\nleft right left Test-clean Test-other\n512 512 20 2.4 5.6\n512 10 20 2.7 6.6\n512 6 20 2.8 6.9\n512 2 20 3.0 7.7\n10 0 20 4.2 11.3\n(6 layers; 57M parameters) had a perplexity of2.49 on the dev-clean\nset; the use of dropout, and of larger models, did not improve either\nperplexity or WER. Shallow fusion was then performed using that\nLM and both the trained T-T system and the trained bidirectional\nLSTM-based RNN-T baseline, with scaling factors on the LM out-\nput and on the non-blank symbol sequence length tuned on the Lib-\nriSpeech dev sets. The results are shown in Table 2 in the “With LM”\ncolumn. The shallow fusion result for the T-T system is competitive\nwith corresponding results for top-performing existing systems.\nNext, we ran training and decoding experiments using T-T mod-\nels with limited attention windows over audio and text, with a view\nto building online streaming speech recognition systems with low\nlatency. Similarly to the use of unidirectional RNN audio encoders\nin online models, where activations for time t are computed with\nconditioning only on audio frames before t, here we constrain the\nAudioEncoder to attend to the left of the current frame by mask-\ning the attention scores to the right of the current frame. In order\nto make one-step inference for AudioEncoder tractable (i.e. to\nhave constant time complexity), we further limit the attention for\nAudioEncoder to a ﬁxed window of previous states by again mask-\ning the attention score. Due to limited computation resources, we\nused the same mask for different Transformer layers, but the use\nof different contexts (masks) for different layers is worth exploring.\nThe results are shown in Table 3, where N in the ﬁrst two columns\nindicates the number of states that the model uses to the left or right\nof the current frame. As we can see, using more audio history gives\nthe lower WER, but considering a streamable model with reasonable\ntime complexity for inference, we experimented with a left context\nof up to 10 frames per layer.\nTable 5. Limited left context per layer for label encoder.\nAudio Mask Label Mask WER (%)\nleft right left Test-clean Test-other\n10 0 20 4.2 11.3\n10 0 4 4.2 11.4\n10 0 3 4.2 11.4\n10 0 2 4.3 11.5\n10 0 1 4.4 12\nTable 6. Results for limiting audio and label context for streaming.\nAudio Mask Label Mask WER (%)\nleft right left Test-clean Test-other\n512 512 20 2.4 5.6\n10 2 2 3.6 10\n10 0 20 4.2 11.3\nSimilarly, we explored the use of limited right context to allow\nthe model to see some future audio frames, in the hope of bridg-\ning the gap between a streamable T-T model (left = 10, right = 0)\nand a full attention T-T model (left = 512, right = 512). Since we\napply the same mask for every layer, the latency introduced by us-\ning right context is aggregated over all the layers. For example, in\nFigure 3, to produce y7 from a 3-layer Transformer with one frame\nof right context, it actually needs to wait for x10 to arrive, which\nis 90 ms latency in our case. To explore the right context impact\nfor modeling, we did comparisons with ﬁxed 512 frames left con-\ntext per layer to compared with full attention T-T model. As we can\nsee from Table 4, with right context of 6 frames per layer (around\n3.2 secs of latency), the performance is around 16% worse than full\nattention model. Compared with streamable T-T model, 2 frames\nright context per layer (around 1 sec of latency) brings around 30%\nimprovements.\nIn addition, we evaluated how the left context used in the T-\nT LabelEncoder affects performance. In Table 5, we show that\nconstraining each layer to only use three previous label states yields\nthe similar accuracy with the model using 20 states per layer. It\nshows very limited left context for label encoder is good engough\nfor T-T model. We see a similar trend when limiting left label states\nwhile using a full attention T-T audio encoder.\nFinally, Table 6 reports the results when using a limited left con-\ntext of 10 frames, which reduces the time complexity for one-step\ninference to a constant, with look-ahead to future frames, as a way\nof bridging the gap between the performance of left-only attention\nand full attention models.\n4. CONCLUSIONS\nIn this paper, we presented the Transformer Transducer model, em-\nbedding Transformer based self-attention for audio and label encod-\ning within the RNN-T architecture, resulting in an end-to-end model\nthat can be optimized using a loss function that efﬁciently marginal-\nizes over all possible alignments and that is well-suited to time-\nsynchronous decoding. This model achieves a new state-of-the-art\naccuracy on the LibriSpeech benchmark, and can easily be used for\nstreaming speech recognition by limiting the audio and label context\nused in self-attention. Transformer Transducer models train signiﬁ-\ncantly faster than LSTM based RNN-T models, and they allow us to\ntrade recognition accuracy and latency in a ﬂexible manner.\n5. REFERENCES\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin, “Attention is all you need,” in Advances in neural\ninformation processing systems, 2017, pp. 5998–6008.\n[2] Zihang Dai, Zhilin Yang, Yiming Yang, William W Co-\nhen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov, “Transformer-xl: Attentive language models beyond a\nﬁxed-length context,” in Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 2019, p.\n29782988.\n[3] Kazuki Irie, Albert Zeyer, Ralf Schlter, and Hermann Ney,\n“Language Modeling with Deep Transformers,” in Proc. In-\nterspeech, 2019, pp. 3905–3909.\n[4] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-transformer: A\nno-recurrence sequence-to-sequence model for speech recog-\nnition,” in Proc. ICASSP, 04 2018, pp. 5884–5888.\n[5] Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus\nM¨uller, and Alex Waibel, “Very deep self-attention net-\nworks for end-to-end speech recognition,” CoRR, vol.\nabs/1904.13377, 2019.\n[6] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li,\nDerek F. Wong, and Lidia S. Chao, “Learning deep transformer\nmodels for machine translation,” CoRR, vol. abs/1906.01787,\n2019.\n[7] “Syllable-based sequence-to-sequence speech recognition with\nthe transformer in mandarin chinese,” in Proc. Interspeech\n2018. pp. 791–795, ISCA.\n[8] Abdelrahman Mohamed, Dmytro Okhonko, and Luke Zettle-\nmoyer, “Transformers with convolutional context for ASR,”\nCoRR, vol. abs/1904.11660, 2019.\n[9] Daniel Povey, Hossein Hadian, Pegah Ghahremani, Ke Li, and\nSanjeev Khudanpur, “A time-restricted self-attention layer\nfor asr,” 2018 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pp. 5874–5878,\n2018.\n[10] Linhao Dong, Feng Wang, and Bo Xu, “Self-attention aligner:\nA latency-control end-to-end model for asr using self-attention\nnetwork and chunk-hopping,” 2019 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP),\nMay 2019.\n[11] Matthias Sperber, Jan Niehues, Graham Neubig, Sebastian\nStker, and Alex Waibel, “Self-attentional acoustic models,”\nProc. Interspeech, Sep 2018.\n[12] Emiru Tsunoo, Yosuke Kashiwagi, Toshiyuki Kumakura, and\nShinji Watanabe, “Towards online end-to-end transformer au-\ntomatic speech recognition,” arXiv:1910.11871, 2019.\n[13] Has ¸im Sak, Andrew Senior, and Francoise Beaufays, “Long\nShort-Term Memory Recurrent Neural Network Architectures\nfor Large Scale Acoustic Modeling,” in Proc. Interspeech,\n2014.\n[14] Alex Graves, “Sequence transduction with recurrent neural\nnetworks,” in Proceedings of the 29th International Confer-\nence on Machine Learning, 2012.\n[15] Kanishka Rao, Has ¸im Sak, and Rohit Prabhavalkar, “Exploring\narchitectures, data and units for streaming end-to-end speech\nrecognition with rnn-transducer,” in 2017 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU).\nIEEE, 2017, pp. 193–199.\n[16] Yanzhang (Ryan) He, Rohit Prabhavalkar, Kanishka Rao, Wei\nLi, Anton Bakhtin, and Ian McGraw, “Streaming small-\nfootprint keyword spotting using sequence-to-sequence mod-\nels,” in Automatic Speech Recognition and Understanding\n(ASRU), 2017 IEEE Workshop on, 2017.\n[17] Has ¸im Sak, Matt Shannon, Kanishka Rao, and Franoise Bea-\nufays, “Recurrent neural aligner: An encoder-decoder neural\nnetwork model for sequence to sequence mapping,” in Proc.\nInterspeech, 2017, pp. 1298–1302.\n[18] Navdeep Jaitly, David Sussillo, Quoc V Le, Oriol Vinyals, Ilya\nSutskever, and Samy Bengio, “A neural transducer,” arXiv\npreprint arXiv:1511.04868, 2015.\n[19] Alex Graves, Santiago Fern ´andez, Faustino Gomez, and J¨urgen\nSchmidhuber, “Connectionist temporal classiﬁcation: La-\nbelling unsegmented sequence data with recurrent neural net-\nworks,” in Proc. ICML, 2006.\n[20] Anshuman Tripathi, Han Lu, Hasim Sak, and Hagen Soltau,\n“Monotonic Recurrent Neural Network Transducer and De-\ncoding Strategies,” in Proc. ASRU, 2019.\n[21] L. R. Rabiner and B.-H. Juang, Fundamentals of Speech\nRecognition, PTR Prentice-Hall, Inc., Englewood Cliffs, New\nJersey 07632, 1993.\n[22] Yongqiang Wang, Abdelrahman Mohamed, Duc Le, Chunxi\nLiu, Alex Xiao, Jay Mahadeokar, Hongzhao Huang, An-\ndros Tjandra, Xiaohui Zhang, Frank Zhang, Christian Fue-\ngen, Geoffrey Zweig, and Michael L. Seltzer, “Transformer-\nbased acoustic modeling for hybrid speech recognition,”\narXiv:1910.09799, 2019.\n[23] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D Cubuk, and Quoc V Le, “Specaugment: A\nsimple data augmentation method for automatic speech recog-\nnition,” arXiv preprint arXiv:1904.08779, 2019.\n[24] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev\nKhudanpur, “Librispeech: an asr corpus based on public do-\nmain audio books,” in Proc. ICASSP. IEEE, 2015, pp. 5206–\n5210.\n[25] Alex Graves, “Practical variational inference for neural net-\nworks,” in Advances in neural information processing systems,\n2011, pp. 2348–2356.\n[26] Jan Chorowski and Navdeep Jaitly, “Towards better decoding\nand language model integration in sequence to sequence mod-\nels,” in Proc. Interspeech, 2017, pp. 523–527.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7178434133529663
    },
    {
      "name": "Encoder",
      "score": 0.7015066146850586
    },
    {
      "name": "Transformer",
      "score": 0.6783966422080994
    },
    {
      "name": "Decoding methods",
      "score": 0.6580770015716553
    },
    {
      "name": "Speech recognition",
      "score": 0.6515536308288574
    },
    {
      "name": "Recurrent neural network",
      "score": 0.640120804309845
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3791056275367737
    },
    {
      "name": "Artificial neural network",
      "score": 0.2572096884250641
    },
    {
      "name": "Algorithm",
      "score": 0.23223242163658142
    },
    {
      "name": "Engineering",
      "score": 0.11201614141464233
    },
    {
      "name": "Electrical engineering",
      "score": 0.0865824818611145
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}