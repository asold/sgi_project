{
  "title": "Transformer-Based Approach Towards Music Emotion Recognition from Lyrics",
  "url": "https://openalex.org/W3118881387",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4287898836",
      "name": "Agrawal, Yudhik",
      "affiliations": [
        "International Institute of Information Technology, Hyderabad"
      ]
    },
    {
      "id": null,
      "name": "Shanker, Ramaguru Guru Ravi",
      "affiliations": [
        "International Institute of Information Technology, Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A2320847550",
      "name": "Alluri, Vinoo",
      "affiliations": [
        "International Institute of Information Technology, Hyderabad"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3062715998",
    "https://openalex.org/W2615509168",
    "https://openalex.org/W2623026530",
    "https://openalex.org/W2963119602",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2087407704",
    "https://openalex.org/W4249666919",
    "https://openalex.org/W6600225990",
    "https://openalex.org/W3021397509",
    "https://openalex.org/W2031081012",
    "https://openalex.org/W2509286793",
    "https://openalex.org/W1984061769",
    "https://openalex.org/W3042725465",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W3104667238",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2900823314",
    "https://openalex.org/W2023109038",
    "https://openalex.org/W2149628368",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2045558466",
    "https://openalex.org/W2005422315",
    "https://openalex.org/W2753709519"
  ],
  "abstract": null,
  "full_text": "Transformer-based approach towards music\nemotion recognition from lyrics\nYudhik Agrawal[0000−0003−3827−6857], Ramaguru Guru Ravi\nShanker[0000−0002−5251−3655], and Vinoo Alluri [0000−0003−3689−1039]\nInternational Institute of Information Technology, Hyderabad, India\n{yudhik.agrawal,ramaguru.guru}@research.iiit.ac.in,\nvinoo.alluri@iiit.ac.in\nAbstract. The task of identifying emotions from a given music track has\nbeen an active pursuit in the Music Information Retrieval (MIR) commu-\nnity for years. Music emotion recognition has typically relied on acoustic\nfeatures, social tags, and other metadata to identify and classify music\nemotions. The role of lyrics in music emotion recognition remains under-\nappreciated in spite of several studies reporting superior performance of\nmusic emotion classiﬁers based on features extracted from lyrics. In this\nstudy, we use the transformer-based approach model using XLNet as the\nbase architecture which, till date, has not been used to identify emotional\nconnotations of music based on lyrics. Our proposed approach outper-\nforms existing methods for multiple datasets. We used a robust method-\nology to enhance web-crawlers’ accuracy for extracting lyrics. This study\nhas important implications in improving applications involved in playlist\ngeneration of music based on emotions in addition to improving music\nrecommendation systems.\nKeywords: Music Emotion Recognition · Lyrics · Valence-Arousal ·\nTransformers.\n1 Introduction\nInformation retrieval and recommendation, be it related to news, music, prod-\nucts, images, amongst others, is crucial in e-commerce and on-demand content\nstreaming applications. With the staggering increase in paid subscribers for mu-\nsic streaming platforms over the years, and especially in these Covid times [1],\nMIR systems have increased need and relevancy. Music Emotion Recognition\nhas gained prominence over the recent years in the ﬁeld of MIR, albeit relying\non acoustic features [11,29] and social tags [6] to identify and classify music\nemotions. Lyrics have been largely neglected despite the crucial role they play in\nespecially eliciting emotions [14], a vital factor contributing to musical reward\n[25], in addition to reﬂecting user traits and tendencies [34] which in turn are\nrelated to musical preferences [26]. Despite a handful of studies reporting the\nsuperior performance of music emotion classiﬁers based on features extracted\nfrom lyrics than audio [16,38], the role of lyrics in music emotion recognition\nremains under-appreciated.\narXiv:2101.02051v1  [cs.IR]  6 Jan 2021\n2 Y. Agrawal et al.\nAnalyzing lyrics and its emotional connotations using advanced Natural Lan-\nguage Processing (NLP) techniques would make for a natural choice. However,\nNLP in MIR has been used for topic modelling [20], identifying song structure\nvia lyrics [13], and mood classiﬁcation [16]. In the context of Music emotion\nrecognition [23,38], typically traditional NLP approaches have been used, which\nare limited to word-level representations and embeddings, as opposed to more\nmodern NLP techniques that are based on context and long-term dependencies\nsuch as transformers [10,40]. Lyrics can be treated as narratives rather than in-\ndependent words or sentences, which therefore renders the use of transformers a\nnatural choice in mining aﬀective connotations. In this study, we use transformer\nmodel which, till date, has not been used for identifying emotional connotations\nof music based on lyrics.\n2 Related Work\nAnalyzing aﬀective connotations from text, that is, sentiment analysis, has been\nactively attempted in short contexts like reviews [4,30], tweets [3,7], news arti-\ncles [35] amongst others with limited application to lyrics. Sentiment analysis\nhas come a long way from its inception based on surveys and public opinions [21]\nto use of linguistic features like character n-grams [15], bag-of-words [4] and lex-\nicons like SentiWordNet [27] to state-of-the-art that employ context-based ap-\nproaches [10,33] for capturing the polarity of a text. The task of sentiment anal-\nysis has been approached using several deep learning techniques like RNN [7,31],\nCNN [7], and transformers [10,18] and have shown to perform remarkably better\nthan traditional machine-learning methods [19].\nMusic emotion classiﬁcation using lyrics has been performed based on tradi-\ntional lexicons [16,17]. The lexicons not only have very limited vocabulary but\nalso the values have to be aggregated without using any contextual informa-\ntion. In recent years the use of pre-trained models like GloVe [32], ELMO [33],\ntransformers [10,37] are fast gaining importance for large text corpus has shown\nimpressive results in downstream several NLP tasks. Authors in [9,2] perform\nemotion classiﬁcation using lyrics by applying RNN model on top of word-level\nembedding. The MoodyLyrics dataset [5] was used by [2] who report an impres-\nsive F1-score of 91.00%. Recurrent models like LSTMs work on Markov’s princi-\nple, where information from past steps goes through a sequence of computations\nto predict a future state. Meanwhile, the transformer architecture eschews recur-\nrence nature and introduces self-attention, which establishes longer dependency\nbetween each step with all other steps. Since we have direct access to all the\nother steps (self-attention) ensures negligible information loss. In this study, we\nemploy Multi-task setup, using XLNet as the base architecture for classiﬁca-\ntion of emotions and evaluate the performance of our model on several datasets\nthat have been organized by emotional connotations solely based on lyrics. We\ndemonstrate superior performance of our transformer-based approach compared\nto RNN-based approach [9,2]. In addition, we propose a robust methodology for\nextracting lyrics for a song.\nTransformer-based approach towards music emotion recognition from lyrics 3\n3 Methodology\n3.1 Datasets\nMoodyLyrics [5]: This dataset comprises 2595 songs uniformly distributed\nacross the 4 quadrants of the Russell’s Valence-Arousal (V-A) circumplex model\n[36] of aﬀect where emotion is a point in a two-dimensional continuous space\nwhich has been reported to suﬃciently capture musical emotions [12]. Valence\ndescribes pleasantness and Arousal represents the energy content. The authors\nused a combination of existing lexicons such as ANEW, WordNet, and WordNet-\nAﬀect to assign the V-A values at a word-level followed by song-level averaging\nof these values. These were further validated by using subjective human judg-\nment of the mood tags from AllMusic Dataset [24]. Finally, the authors had\nretained songs in each quadrant only if their Valence and Arousal values were\nabove speciﬁc thresholds, thereby rendering them to be highly representative of\nthose categories.\nMER Dataset [24]: This dataset contains 180 songs distributed uniformly\namong the 4 emotion quadrants of the 2-D Russell’s circumplex model. Sev-\neral annotators assigned the V-A values for each song solely based on the lyrics\ndisplayed without the audio. The Valence and Arousal for each song were com-\nputed as the average of their subjective ratings. Also, this dataset was reported\nto demonstrate high internal consistency making it highly perceptually relevant.\n3.2 Lyrics Extraction\nDue to copyright issues, the datasets do not provide lyrics, however, the URLs\nfrom diﬀerent lyric websites are provided in each of the datasets. In order to mine\nthe lyrics, one approach is to write a crawler for each of the websites present\nin the datasets. However, some of those URLs were broken. Hence, in order to\naddress this concern, we provide a robust approach for extracting lyrics using\nthe Genius website. All the existing APIs, including Genius API require the\ncorrect artist and track name for extracting the lyrics. However, if the artist or\ntrack names are misspelled in the dataset, the API fails to extract the lyrics. We\nhandled this issue by introducing a web crawler to obtain the Genius website\nURL for the lyrics of the song instead of hard-coding the artist and track name\nin Genius API. Using the web crawler, we were able to considerably improve the\nnumber of songs extracted from 60% - 80% for the diﬀerent datasets to ∼99%\nfor each dataset.\n3.3 Proposed Architecture\nWe describe a deep neural network architecture that, given the lyrics, outputs the\nclassiﬁcation of Emotion Quadrants, in addition to Valence and Arousal Hemi-\nspheres. The entire network is trained jointly on all these tasks using weight-\nsharing, an instance of multi-task learning. Multi-task learning acts as a regu-\nlarizer by introducing inductive bias that prefers hypotheses explaining all the\n4 Y. Agrawal et al.\ntasks. It overcomes the risk of overﬁtting and reduces the model’s ability to ac-\ncommodate random noise during training while achieving faster convergence[41].\nWe use XLNet [40] as the base network, which is a large bidirectional trans-\nformer that uses improved training methodology, larger data and more computa-\ntional power. XLNet improves upon BERT [10] by using the Transformer XL [8]\nas its base architecture. The added recurrence to the transformer enables the\nnetwork to have a deeper understanding of contextual information.\nL yrics XLNet \nT ransformer \nSequence \nSummary \nQuadrant \nLabels \nV alence \nLabels \nArousal \nLabels \nQ1 \nQ2 \nQ3 \nQ4 \nFig. 1. Overview of our method\nThe XLNet transformer Model outputs raw hidden states, which are then\npassed on to SequenceSummary block, which computes a single vector summary\nof a sequence of hidden states, followed by one more hidden Fully-Connected\n(FC) layer which encodes the information into a vector of length 8. This layer\nﬁnally branches out into three complementary tasks via a single FC layer on\ntop for classiﬁcation of Quadrant, Valence, and Arousal separately. As we feed\ninput data, the entire pre-trained XLNet model and the additional untrained\nclassiﬁcation layers are trained for all three tasks. We use the following loss\nfunction to train our network.\nL= (λ1 ∗LQ) + (λ2 ∗LV ) + (λ3 ∗LA) (1)\nwhere LQ,LV , and LA represents the classiﬁcation loss on Quadrants, Valence,\nand Arousal, respectively.\nImplementation Details We use the AdamW optimizer [22] with an initial\nlearning rate of 2e−5 and a dropout regularization with a 0.1 discard probability\nfor the layers. We use Cross-Entropy Loss for calculating loss. A batch size of\n8 was used. We also restrict the length of the lyrics to 1024 words. Lyrics of\nmore than 99% of the songs had less than 1024 words. We leverage the rich\ninformation of pre-trained (XLNet-base-cased) model as they are trained on\nbig corpora. As the pre-trained model layers already encode a rich amount of\ninformation about language, training the classiﬁer is relatively inexpensive [37].\nWe also run our network on single-task classiﬁcation and compare the results as\npart of our ablation study in a later section.\nTransformer-based approach towards music emotion recognition from lyrics 5\n4 Experiment & Results\n4.1 Evaluation Measures\nFor evaluating the eﬀectiveness of our proposed model, we use the standard\nrecall, precision, and F1 measures. We provide results for both macro-averaged\nF1 and micro-averaged F1. The micro-average F1 is also the classiﬁer’s overall\naccuracy. We use Macro-averaged F1(F1-score) [39] as given in Equation 2. The\nscores are ﬁrst computed for the binary decisions for each individual category\nand then are averaged over categories.\nF1x = 2 PxRx\nPx + Rx\n; F1 = 1\nn\n∑\nx\nF1x (2)\nwhere F1x, Px, Rx denote F1-score, precision and recall with respect to class x.\nThis metric is signiﬁcantly more robust towards the error type distribution as\ncompared to the other variants of the Macro-averaged F1 [28].\n4.2 Results\nWe use multi-task setup to compare our performance on various datasets. For a\nfair evaluation of our method, we use the data splits for respective datasets, as\nmentioned in respective studies. All the results reported hereon are the average\nof multiple data splits. Tables 1 and 2 compares the results of our approach\non MoodyLyrics and MER dataset respectively. These results demonstrate the\nfar superior performance of our method when compared to studies that have\nattempted the same task.\nTable 1. Results of classiﬁcation by Quadrants on MoodyLyrics dataset.\nApproach Accuracy Precision Recall F1-score\nNaive Bayes [2] 83.00% 87.00% 81.00% 82.00%\nBiLSTM + Glove [2] 91.00% 92.00% 90.00% 91.00%\nOur Method 94.78% 94.77% 94.75% 94.77%\nWe also compare the performance of our approach by validating on an ad-\nditional dataset, the AllMusic dataset comprising 771 songs provided by [24].\nWe follow the same procedure of training on the MER dataset and evaluat-\ning on the AllMusic dataset as mentioned by the authors. We get an improved\nF1-score of 75.40% compared to their reported 73.60% on single-task Quadrant\nclassiﬁcation in addition to improved Accuracy of 76.31% when compared to\nthe reported Accuracy of 74.25%, albeit on a subset of the AllMusic dataset,\nin[5]. Our Multi-task method demonstrated comparable F1-score and accuracy\nof 72.70% and 73.95% when compared to our single-task Quadrant classiﬁcation.\n6 Y. Agrawal et al.\nTable 2. Results of classiﬁcation on MER dataset.\nClassiﬁcation Approach Accuracy Precision Recall F1-score\nQuadrant CBF + POS tags, Structural\nand Semantic features [24] - - - 80.10%\nQuadrant Our Method 88.89% 90.83% 88.75% 88.60%\nValence CBF + POS tags, Structural\nand Semantic features [24] - - - 90.00%\nValence Our Method 94.44% 92.86% 95.83% 93.98%\nArousal CBF + POS tags, Structural\nand Semantic features [24] - - - 88.30%\nArousal Our Method 88.89% 90.00% 90.00% 88.89%\nTable 3. Ablation Study on MoodyLyrics\nClassiﬁcation Accuracy F1-score\nMulti-Task Single-Task Multi-Task Single-Task\nQuadrant 94.78% 95.68% 94.77% 95.60%\nValence 95.73% 96.51% 95.67% 96.46%\nArousal 94.38% 94.38% 94.23% 94.35%\nAblation Study:Owing to its large size and quadrant representativeness of the\nMoodyLyrics dataset, we perform extensive analysis with diﬀerent architecture\ntypes and sequence lengths. In the initial set of experiments, we aimed to ﬁnd\nthe best model where we compared our baseline model with BERT transformer\nwith same sequence length of 512, which resulted in inferior performance of an\nF1-score down by around 1.3%. We also compare the performance of our baseline\nmodel with our multi-task setup. Table 3 shows that we perform similar to our\nbaseline method, but we saw a huge improvement in training speed as the latter\nconverge faster. This also requires training diﬀerent tasks from scratch every\ntime, which makes it ineﬃcient.\n5 Conclusion\nIn this study, we have demonstrated the robustness of our novel transformer-\nbased approach for music emotion recognition using lyrics on multiple datasets\nwhen compared to hitherto used approaches. Our multi-task setup helps in faster\nconvergence and reduces model overﬁtting, however, the single-task setup per-\nforms marginally better albeit at the expense of computational resources. This\nstudy can help in improving applications like playlist generation of music with\nsimilar emotions. Also, hybrid music recommendation systems, which utilize\npredominantly acoustic content-based and collaborative ﬁltering approaches can\nfurther beneﬁt from incorporating emotional connotations of lyrics for retrieval.\nThis approach can be extended in future to multilingual lyrics.\nTransformer-based approach towards music emotion recognition from lyrics 7\nReferences\n1. Spotify hits 130 million subscribers amid Covid-19, https://www.bbc.com/news/\ntechnology-52478708\n2. Abdillah, J., Asror, I., Wibowo, Y.F.A., et al.: Emotion classiﬁcation of song lyrics\nusing bidirectional lstm method with glove word representation weighting. Jurnal\nRESTI (Rekayasa Sistem Dan Teknologi Informasi) 4(4), 723–729 (2020)\n3. Agarwal, A., Xie, B., Vovsha, I., Rambow, O., Passonneau, R.J.: Sentiment analysis\nof twitter data. In: Proceedings of the workshop on language in social media (LSM\n2011). pp. 30–38 (2011)\n4. Barry, J.: Sentiment analysis of online reviews using bag-of-words and lstm ap-\nproaches. In: AICS. pp. 272–274 (2017)\n5. C ¸ ano, E., Morisio, M.: Moodylyrics: A sentiment annotated lyrics dataset. In:\nProceedings of the 2017 International Conference on Intelligent Systems, Meta-\nheuristics & Swarm Intelligence. pp. 118–124 (2017)\n6. C ¸ ano, E., Morisio, M., et al.: Music mood dataset creation based on last. fm tags.\nIn: 2017 International Conference on Artiﬁcial Intelligence and Applications, Vi-\nenna, Austria (2017)\n7. Cliche, M.: Bb twtr at semeval-2017 task 4: Twitter sentiment analysis with cnns\nand lstms. arXiv preprint arXiv:1704.06125 (2017)\n8. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R.:\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860 (2019)\n9. Delbouys, R., Hennequin, R., Piccoli, F., Royo-Letelier, J., Moussallam, M.: Music\nmood detection based on audio and lyrics with deep neural net. arXiv preprint\narXiv:1809.07276 (2018)\n10. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n11. Eerola, T., Lartillot, O., Toiviainen, P.: Prediction of multidimensional emotional\nratings in music from audio using multivariate regression models. In: Ismir. pp.\n621–626 (2009)\n12. Eerola, T., Vuoskoski, J.K.: A comparison of the discrete and dimensional models\nof emotion in music. Psychology of Music 39(1), 18–49 (2011)\n13. Fell, M., Nechaev, Y., Cabrio, E., Gandon, F.: Lyrics segmentation: Textual\nmacrostructure detection using convolutions. In: Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics. pp. 2044–2054 (2018)\n14. Greasley, A., Lamont, A.: Musical preferences. Oxford handbook of music psychol-\nogy pp. 263–281 (2016)\n15. Han, Q., Guo, J., Schuetze, H.: Codex: Combining an svm classiﬁer and character\nn-gram language models for sentiment analysis on twitter text. In: Second Joint\nConference on Lexical and Computational Semantics (* SEM), Volume 2: Pro-\nceedings of the Seventh International Workshop on Semantic Evaluation (SemEval\n2013). pp. 520–524 (2013)\n16. Hu, X., Downie, J.S.: When lyrics outperform audio for music mood classiﬁcation:\nA feature analysis. In: ISMIR. pp. 619–624 (2010)\n17. Hu, Y., Chen, X., Yang, D.: Lyric-based song emotion detection with aﬀective\nlexicon and fuzzy clustering method. In: ISMIR (2009)\n18. Huang, Y.H., Lee, S.R., Ma, M.Y., Chen, Y.H., Yu, Y.W., Chen, Y.S.:\nEmotionx-idea: Emotion bert–an aﬀectional model for conversation. arXiv preprint\narXiv:1908.06264 (2019)\n8 Y. Agrawal et al.\n19. Kansara, D., Sawant, V.: Comparison of traditional machine learning and deep\nlearning approaches for sentiment analysis. In: Advanced Computing Technologies\nand Applications, pp. 365–377. Springer (2020)\n20. Kleedorfer, F., Knees, P., Pohle, T.: Oh oh oh whoah! towards automatic topic\ndetection in song lyrics. In: Ismir. pp. 287–292 (2008)\n21. Knutson, A.L.: Japanese opinion surveys: the special need and the special diﬃcul-\nties. Public Opinion Quarterly 9(3), 313–319 (1945)\n22. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017)\n23. Malheiro, R., Panda, R., Gomes, P., Paiva, R.: Music emotion recognition from\nlyrics: A comparative study. 6th International Workshop on Machine Learning\nand Music (MML13). Held in . . . (2013)\n24. Malheiro, R., Panda, R., Gomes, P., Paiva, R.P.: Emotionally-relevant features\nfor classiﬁcation and regression of music lyrics. IEEE Transactions on Aﬀective\nComputing 9(2), 240–254 (2016)\n25. Mas-Herrero, E., Marco-Pallares, J., Lorenzo-Seva, U., Zatorre, R.J., Rodriguez-\nFornells, A.: Individual diﬀerences in music reward experiences. Music Perception:\nAn Interdisciplinary Journal 31(2), 118–138 (2012)\n26. Melchiorre, A.B., Schedl, M.: Personality correlates of music audio preferences for\nmodelling music listeners. In: Proceedings of the 28th ACM Conference on User\nModeling, Adaptation and Personalization. pp. 313–317 (2020)\n27. Ohana, B., Tierney, B.: Sentiment classiﬁcation of reviews using sentiwordnet. In:\n9th. it & t conference. vol. 13, pp. 18–30 (2009)\n28. Opitz, J., Burst, S.: Macro f1 and macro f1. arXiv preprint arXiv:1911.03347 (2019)\n29. Panda, R., Malheiro, R., Rocha, B., Oliveira, A., Paiva, R.P.: Multi-modal music\nemotion recognition: A new dataset, methodology and comparative analysis. In:\nInternational Symposium on Computer Music Multidisciplinary Research (2013)\n30. Pang, B., Lee, L.: A sentimental education: Sentiment analysis using subjectivity\nsummarization based on minimum cuts. arXiv preprint cs/0409058 (2004)\n31. Patel, A., Tiwari, A.K.: Sentiment analysis by using recurrent neural network. In:\nProceedings of 2nd International Conference on Advanced Computing and Software\nEngineering (ICACSE) (2019)\n32. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre-\nsentation. In: Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP). pp. 1532–1543 (2014)\n33. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,\nZettlemoyer, L.: Deep contextualized word representations. arXiv preprint\narXiv:1802.05365 (2018)\n34. Qiu, L., Chen, J., Ramsay, J., Lu, J.: Personality predicts words in favorite songs.\nJournal of Research in Personality 78, 25–35 (2019)\n35. Raina, P.: Sentiment analysis in news articles using sentic computing. In: 2013\nIEEE 13th International Conference on Data Mining Workshops. pp. 959–962.\nIEEE (2013)\n36. Russell, J.A.: A circumplex model of aﬀect. Journal of personality and social psy-\nchology 39(6), 1161 (1980)\n37. Sun, C., Qiu, X., Xu, Y., Huang, X.: How to ﬁne-tune bert for text classiﬁcation?\nIn: China National Conference on Chinese Computational Linguistics. pp. 194–206.\nSpringer (2019)\n38. Xia, Y., Wang, L., Wong, K.F.: Sentiment vector space model for lyric-based song\nsentiment classiﬁcation. International Journal of Computer Processing Of Lan-\nguages 21(04), 309–330 (2008)\nTransformer-based approach towards music emotion recognition from lyrics 9\n39. Yang, Y., Liu, X.: A re-examination of text categorization methods. In: Proceed-\nings of the 22nd annual international ACM SIGIR conference on Research and\ndevelopment in information retrieval. pp. 42–49 (1999)\n40. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:\nGeneralized autoregressive pretraining for language understanding. In: Advances\nin neural information processing systems. pp. 5753–5763 (2019)\n41. Zhang, Y., Yang, Q.: An overview of multi-task learning. National Science Review\n5(1), 30–43 (2018)",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I64189192",
      "name": "International Institute of Information Technology, Hyderabad",
      "country": "IN"
    }
  ]
}