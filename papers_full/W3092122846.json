{
  "title": "Transformer Transducer: One Model Unifying Streaming and Non-streaming Speech Recognition",
  "url": "https://openalex.org/W3092122846",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2745035099",
      "name": "Tripathi, Anshuman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1785740175",
      "name": "Kim Jae-Young",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2044492937",
      "name": "Zhang Qian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005904254",
      "name": "Lu Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114627456",
      "name": "Sak, Hasim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3015686596",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2606722458",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3004728855",
    "https://openalex.org/W2291513470",
    "https://openalex.org/W1533416326",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1566256432",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W1828163288"
  ],
  "abstract": "In this paper we present a Transformer-Transducer model architecture and a training technique to unify streaming and non-streaming speech recognition models into one model. The model is composed of a stack of transformer layers for audio encoding with no lookahead or right context and an additional stack of transformer layers on top trained with variable right context. In inference time, the context length for the variable context layers can be changed to trade off the latency and the accuracy of the model. We also show that we can run this model in a Y-model architecture with the top layers running in parallel in low latency and high latency modes. This allows us to have streaming speech recognition results with limited latency and delayed speech recognition results with large improvements in accuracy (20% relative improvement for voice-search task). We show that with limited right context (1-2 seconds of audio) and small additional latency (50-100 milliseconds) at the end of decoding, we can achieve similar accuracy with models using unlimited audio right context. We also present optimizations for audio and label encoders to speed up the inference in streaming and non-streaming speech decoding.",
  "full_text": "TRANSFORMER TRANSDUCER: ONE MODEL UNIFYING STREAMING AND\nNON-STREAMING SPEECH RECOGNITION\nAnshuman Tripathi, Jaeyoung Kim, Qian Zhang, Han Lu, Hasim Sak\n{anshumant, jaeykim, zhaqian, luha, hasim}@google.com\nGoogle Inc., USA\nABSTRACT\nIn this paper we present a Transformer-Transducer model architec-\nture and a training technique to unify streaming and non-streaming\nspeech recognition models into one model. The model is composed\nof a stack of transformer layers for audio encoding with no looka-\nhead or right context and an additional stack of transformer layers\non top trained with variable right context. In inference time, the con-\ntext length for the variable context layers can be changed to trade off\nthe latency and the accuracy of the model. We also show that we can\nrun this model in a Y-model architecture with the top layers running\nin parallel in low latency and high latency modes. This allows us to\nhave streaming speech recognition results with limited latency and\ndelayed speech recognition results with large improvements in ac-\ncuracy (20% relative improvement for voice-search task). We show\nthat with limited right context (1-2 seconds of audio) and small addi-\ntional latency (50-100 milliseconds) at the end of decoding, we can\nachieve similar accuracy with models using unlimited audio right\ncontext. We also present optimizations for audio and label encoders\nto speed up the inference in streaming and non-streaming speech de-\ncoding.\nIndex Terms— Transformer, RNN-T, sequence-to-sequence,\nencoder-decoder, end-to-end, speech recognition\n1. INTRODUCTION\nPast research has shown that having access to future audio con-\ntext to encode the current audio frame in neural network models\nsigniﬁcantly improves speech recognition accuracy [1, 2, 3, 4].\nBidirectional LSTMs take advantage of future audio context, how-\never the model can only be run when the entire audio is available.\nIn the past few years, models employing self-attention mechanism\nhave achieved state-of-art results for sequence modeling tasks [5, 6].\nTransformer models encode an input sequence by running self-\nattention mechanism over left and right context window of each\ninput in sequence. In speech recognition models the future audio\ncontext can be encoded by specifying a limited right context [4].\nSince the right context is limited (unlike bidirectional LSTMs), this\nallows transformer models with future audio context to recognize\nspeech in streaming fashion with some limited delay. This makes\ntransformer models desirable especially for applications that can\nafford a higher streaming latency for better recognition quality.\nEven in low latency streaming speech recognition systems (e.g.\nvoice-search, dictation), the usability of the system depends on accu-\nracy of the ﬁnal recognized result. The accuracy of ﬁnal results can\nbe improved by applying n-best or lattice re-scoring techniques. For\nlongform audio the extra latency introduced by rescoring models is\nnot bounded and the accuracy improvements are also limited due to\na lack of diversity in the n-best hypotheses. To address this problem,\nin this paper, we propose decoding speech using two transformer\nmodels in parallel - one with smaller right context for low-latency\nstreaming recognition results and one with larger right context for\nﬁnal results. Unlike rescoring this works well for both short and\nlong utterances. We show that it gives about 20% relative word error\nrate improvements over the low-latency results for voice-search task.\nThe extra latency introduced by larger right context decoding is also\nbounded because of limited right context.\nWe further show that we can unify low-latency and high-latency\nmodels by training a single model that can decode speech in either\nlow latency or high latency mode. We do this by varying right con-\ntext for transformer layers (variable context layers) during training.\nBy using only ﬁnal few layers as variable context layers (referred\nto as Y-model architecture), and applying decoding optimizations\npresented in this paper, we show that it is possible to do efﬁcient\nparallel decoding for high latency and low latency modes using one\nASR model.\n2. TRANSFORMER TRANSDUCERS\n2.1. Architecture\nFig. 1. Transformer Transducer architecture.\nTransformer Transducer [4] is a model architecture that can be\ntrained with end-to-end RNN-T loss [7] using transformer based au-\ndio encoder and label encoder. As shown in ﬁgure 1, T-T model\npredicts a probability distribution over the label space at every time\nstep. The probability of an alignment P(z|x) can be factorized as\nP(z|x) =\n∏\ni\nP(zi|x, ti, Labels(z1:(i−1))), (1)\narXiv:2010.03192v1  [cs.SD]  7 Oct 2020\nFig. 2. Transformer encoder architecture.\nwhere Labels(z1:(i−1)) is the sequence of non-blank labels in\nz1:(i−1). In T-T architecture P(z|x) is parameterized with an audio\nencoder, a label encoder, and a joint network. The model deﬁnes\nP(zi|x, ti, Labels(z1:(i−1))) as follows:\nJoint =Linear(AudioEncoderti (x))+\nLinear(LabelEncoder(Labels(z1:(i−1))))) (2)\nP(zi|x, ti, Labels(z1:(i−1))) =\nSoftmax(Linear(tanh(Joint))), (3)\nwhere each Linear function is a different single-layer feed-forward\nneural network, AudioEncoderti (x) is the audio encoder output at\ntime ti, and LabelEncoder(Labels(z1:(i−1))) is the label encoder\noutput given the previous non-blank label sequence.\nMore details about each transformer layer is shown in ﬁgure 2. It\ncontains normalization layer, masked multi-head attention layer with\nrelative position encoding, residual connection, stacking/unstacking\nlayer and feed forward layer. The residual connections are applied\nwith normalized input to the output of the attention layer or feed for-\nward layers. The stacking/unstacking layer can be used to change\nframe rate for each transformer layer which helps to speed up train-\ning and inference. For further optimization, the label encoder can\nalso be a bigram label embedding model.\n2.2. Variable Context Training\nIn self attention block of transformer architecture we compute the\nself attention over entire input sequence and then mask it based on\nthe left and right context of the layer. In variable context training\nwe keep the left context of the layers constant and sample a right\ncontext length from a given distribution. The sampled right context\nlength decides the mask for the self attention. In our experiments\nwe found that randomly sampling right context length for each layer\nindependently led to unstable training, and we had to limit the num-\nber of right context conﬁgurations. During training we specify right\ncontext conﬁguration as a list of right contexts for each transformer\nlayer in the encoder ex: [0]×15 + [8]×5 speciﬁes a right context of\n0 for ﬁrst 15 layers and right context of 8 frames for last ﬁve layers.\nDuring training we uniformly sample a right context conﬁguration\nfrom all these available conﬁgurations. We show in section 4.3.2\nthat models trained in this way can be used with any right context\nconﬁgurations used for training.\n2.3. Y-model\nVariable context training allows us to train transformer layers that\nare capable of using different right context from input, effectively at\ninference. We apply this technique to train a model that has input\nfollowed by several initial layers trained with zero right context and\nﬁnal few layers trained as variable context layers. With this model\nwe can do speech recognition with different future audio context (de-\npending on the right context used at inference time). This can be\nuseful if an application can afford a higher latency for better quality\nresults. We can also use this model for low latency speech recog-\nnition by running two parallel decoders using the same model, one\nwith no or very small right context and one with larger right context.\nSince we keep the right context of shared layers constant (at zero)\nwe only need to recompute the activation for the ﬁnal layers with\nvariable context. We call this a Y-model because we have parallel\ndecoding branches running with different right context. We call the\ndecoding branch with smaller right context ’low latency branch’ and\ndecoding branch with higher right context ’high latency branch’\n2.3.1. Inference and Recognition Latency\nDuring inference we stream partial recognition results based on low\nlatency branch (for better model responsiveness) and when the ut-\nterance ends we replace the recognition results with the results from\nhigh latency branch (for better ASR quality). Depending on the right\ncontext used for high latency branch (2.4sec in our experiments), it is\nalways behind the low latency branch in decoding by a ﬁnite amount.\nWhen the user utterance ends we just need to run high latency branch\ndecoding for the remaining lookahead (e.g. 2.4 secs) and show the\nﬁnal results. Since we have access to all the remaining audio context\nwe can process it very quickly (no need to wait for streaming), by\nbatching all the available context. The additional latency to show\nthese ﬁnal results at end of the utterance includes run time for com-\nputing activation for audio encoder layers with variable context in\ndelayed branch and decoding. In section 4.3.2 we show that this can\nbe done very quickly for lookahead audio context of 2.4 seconds,\nhence resulting in very small additional latency.\n2.4. Constrained Alignment Training\nTransformer-Transducer minimizes RNN-T loss which contains all\nthe alignment paths for each label sequence. Although optimizing\nlabel likelihoods should provide advantage on improving ASR per-\nformance, a model can have high alignment delay because there’s no\nmechanism to restrict prediction delay in optimizing RNN-T loss.\nThe high alignment delay happens especially for a streaming model\nwhich does not have a right context because the model tries to im-\nprove prediction accuracy by looking ahead future frames.\nConstrained alignment training was originally proposed in [8].\nIt puts constraints on RNN-T loss by masking out high delay align-\nment paths from reference alignments. To ﬁnd reference alignments,\nwe used full-attention non-streaming Transformer-Transducer model\nas a reference alignment model because it showed almost no delay.\nBased on the reference alignment, we put constrained window on\nword-boundaries and any word label path outside of the constrained\nwindow is masked out from the RNN-T loss. We applied constrained\nalignment training on Y-model and its evaluation results are in sec-\ntion 4.3.2.\n3. INFERENCE OPTIMIZATIONS\nIn this section, we talk about the optimizations and implementations\nwe did for speeding up the Transformer encoder in streaming and\nnon-streaming applications and the label encoder used for speech\ndecoding. This is required to make parallel decoding feasible for\nY-model.\n3.1. Streaming Transformer Audio Encoder\nStreaming audio encoding is the process of taking audio frame(s) and\nprevious states as inputs, and outputting the corresponding encoded\nfeature(s) and next states. Unlike RNN/LSTM based audio encoder,\nTransformer encoder has no time dependency while encoding audio.\nIn other word, to encode thei-th frame, RNN/LSTM based encoders\nneed to ﬁrst encode from the 0-th to the i −1-th frame before en-\ncoding the i-th frame, whereas, Transformer encoder can encode all\nthe i frames at the same time in parallel. We refer to the way of\nencoding a batch of time steps in parallel ”batch step” inference. In\nthe later section, we will discuss the effectiveness of batching more\ntime frames in terms of inference speed.\n3.2. Non-streaming Transformer Audio Encoder\nIn non-streaming inference, ideally we can run the Transformer en-\ncoder exactly the same way it is run during training time. However,\nin practice, because of the O(T2) memory consumption in attention\nmatrix computation, the Transformer encoder can be very memory\nexpensive. As showed in [4], limited context Transformer encoder\nprovides about the same quality as inﬁnite context encoder. Based\non this model architecture, we can compute the attention matrix for\na few Query blocks at a time, and each Query block only attends\nto a limited set of Keys, which makes the memory consumption\nconstant. We refer to this inference method as ”query slicing”.\n3.3. Decoder Optimizations\nIn the transformer transducer model, we use a label encoder as an\nauto-regressive model to encode predicted label history. The label\nencoder output is combined with the audio encoder output using\na dense layer before the softmax function. The decoding speed is\nhighly dependent on the computational complexity of label encoder\nnetwork because it is run for every hypothesis. For RNN-T models,\nit has been shown that the label history context length can be re-\nduced signiﬁcantly without affecting the accuracy of the models [9].\nIn this paper, we experimented with two models for label encoding.\nThe ﬁrst one is a transformer model with limited label context length\nas described in [4]. The other one is an embedding model with a bi-\ngram label context. The embedding model learns a weight vector\nof d dimension for each possible bigram label context, where d is\nthe dimension of audio and label encoder outputs. The total number\nof parameters is N2 ∗d where N is the vocabulary size for the la-\nbels. The learned weight vector is simply used as the embedding of\nbigram label context in the T-T model. Since this is a simple embed-\nding lookup based on label bigrams, the runtime for label encoder is\nvery fast.\nTo further speed up the inference, we maintain a cache to store\nand reuse the computed label encoder outputs for the limited label\ncontexts seen so far in decoding since during decoding the model\nwill encode the same label history multiple times for different hy-\npothesis.\nLabel encoder WER RTF\n40 grapheme context transformer 4.8 0.3\n3 grapheme context transformer 4.8 0.02\n2 grapheme emb lookup 4.9 0.01\nTable 1. WER and RT factors for different label encoders.\n4. EXPERIMENTS AND RESULTS\n4.1. Data\nFor our experiments we use 30K hours of speech data from voice-\nsearch application. The test set we use consists of 14K V oice Search\nutterances with duration less than 5.5 seconds long. The training and\ntest sets are all anonymized and hand-transcribed. The input speech\nwave-forms are framed using a 32 msec window with 10 msec shift.\nWe use 128 dimension logmel energy features and use as acoustic\nfeatures after stacking 4 of them and subsampling by a factor of 3,\nresulting in 30msec features. During training we perform specaug-\nment [10] on the acoustic features.\n4.2. Limited Context Decoding\nTabel 1 shows WER and benchmarking results for different label en-\ncoder architectures. From the results we see that there is not much\ndifference in WER when the context is reduce for the label encoder.\nSimilar results have been reported before in [9]. For the case of lim-\nited context of 3 graphemes, we see a huge improvement in speed\nof decoder over the case when label encoder has 40 labels. This\nis because of the label encoder output caching as explained in sec-\ntion 3.3. For label context 2 it is even faster since the model itself is\njust a lookup table.\n4.3. Y-model results\nWe present results on two Y-architecture models:\n4.3.1. Y-Model1\nThe audio encoder has 20L of transformers with all the layers trained\nusing variable context with following possible right context conﬁgu-\nrations: [0] ×19 + [4], [2] ×20, [4] ×20, and [32] ×20. In all these\nmodes model is trained with an output delay of 4 frames. When\nevaluating the model with 240ms lookahead we use the right context\nconﬁguration of [0] ×19 + [4], when evaluating with 1.2s lookahead\nwe use [2] ×20 and for 2.4 sec look ahead we use the conﬁguration\nof [4] ×20\n4.3.2. Y-Model2\nThe audio encoder has 20L of transformers with ﬁrst 15 layers\ntrained with no right context and last 5 layers trained with vari-\nable context with following possible right context conﬁgurations:\n[0] ×19 + [4], [0] ×15 + [8]×5 and [0] ×15 + [16]×5. In\nall these modes model is trained with an output delay of 4 frames.\nWhen evaluating the model with 240ms lookahead we use the right\ncontext conﬁguration of [0] ×19 + [4], when evaluating with 1.2s\nlookahead we use [0] ×15 + [8]×5 and for 2.4 sec look ahead we\nuse the conﬁguration of [0] ×15 + [16]×5.\nModel Lookahead WER Alignment Delay\nFull context model 34 sec 4.8 60msec\nLeft context model 240 msec 6.1 982msec\nY model1 240msec 6.1 883msec\n1.2sec 5.2 780msec\n2.4sec 5.1 764msec\nY model2 240msec 5.3 767msec\n1.2sec 5.0 -\n2.4sec 5.0 742msec\nTable 2. WER with different models and lookaheads.\nModel Lookahead WER Alignment Delay\nY model2 240msec 6.5 119msec\n2.4sec 4.9 74msec\nTable 3. WER with different models and lookaheads with con-\nstrained alignment training.\nTable 2 shows baselines without constrained alignment training.\nWe can see that Y-architecture with a high latency branch of 2.4\nsec is very close to the full attention model performance. The same\nmodel in low latency mode has much better accuracy than our best\nstreaming model with no right context. We also show the delay in\nword alignment for all the model evaluations. Since the low latency\nbranch of Y-model2 is still delaying the output by 767 msec, it can\nget much better performance as it still looks ahead 240 msec + 767\nmsec by delaying the words. Another interesting observation is that\n2.4 sec lookahead mode for Y model also delays the word align-\nments, although the full attention model does not delay. This is ex-\npected since delaying words helps low latency mode and does not\naffect high latency mode.\nTable 3 shows the results of Y-model with constrained RNN-T\nloss, we can see that the words are now predicted much earlier, but\nthis leads to degradation in quality of low latency branch. The qual-\nity for the higher latency mode is still same since it is not affected by\nword delays much.\nThe alignment delay is deﬁned as the mean word alignment dif-\nference between reference non-streaming model and the constrained\nalignment Y model:\nD = 1\nN\n∑\n(Tref\ni −TY\ni ) (4)\nwhere Tref\ni is an alignment time for the ith word from the reference\nmodel, TY\ni is an alignment time for the ith word from the Y model\nand N is the total number of words. For the low-latency mode of\nY model, we used extra 4 right context at the last layer to reduce\nWER loss. Moreover, there is 4 output delay applied to Y model.\nTherefore, besides alignment delay, Y model has extra 240 msec for\nthe low latency mode.\nTable 3 shows constrained alignment model. Comparing with\nunconstrained Y model 2 at Table 2, alignment delay signiﬁcantly\nimproved: 767 msec to 119 msec and 742 msec to 74 msec for low\nand high latency modes, respectively. Moreover, constrained align-\nment high-latency mode did not show any WER loss because the\nhigh-latency mode already has enough right context and restricting\nits prediction delay did not hurt its performance. However, for the\nnumber of time batch steps\nrun time per 100 sec audio (sec)0\n1\n2\n3\n4\n5\n2 4 6 8\ntrain query slicing batch step\nFig. 3. Inference speed for batch of 8 utterance with different modes\non TPU.\nNum cpu cores used\nTotal decoding time in min\n0\n1\n2\n3\n4\n5\n5 10 15 20 25 30\n3 label ctx transformer decoder bigrapheme emb lookup decoder\nFig. 4. Inference speed for query slicing mode on CPU with a 36\nminutes audio.\nlow-latency mode, WER degraded due to the reduced look-ahead\nframes.\nFor Y model, more accurate high-latency mode always comes\nto correct any error from the low-latency mode. Therefore, reducing\nalignment delay is more important for the low-latency mode, while\nmaintaining high quality performance is the main concern for the\nhigh latency mode. The constrained alignment training is well suited\nfor this.\n4.4. Inference Benchmarks\nIn Figure3, we benchmark the time taken to encode 100 seconds\naudio with different modes on a single TPU [11] core with respect\nto the number of time steps the encoders run at a time. We can see\nthat the inference speed is faster when we encode more time steps\nat a single inference run for all the modes because it allows better\nparallelization. We can also see that training mode is faster than\nquery slicing mode, and batch step mode is the slowest among them\nbecause of extra overheads for handling the query block, and states.\nIn high latency scenarios, where the encoder encodes 120 frames at\na time, the encoding times are even faster. It takes 0.3 second, 0.6\nsecond, and 1.8 seconds to encode 100 seconds audio with training\nmode, query slicing mode and batch step mode respectively.\nIn Figure.4, we benchmark query slicing inference mode on\ndesktop CPU for a 36 minutes long audio. We can see that this au-\ndio can be recognized in less than 3 minutes with 8 CPU cores (i.e.\nless than 8% real time factor). The recognition time can be further\nimproved with a bigrapheme embedding lookup decoder with slight\nWER regression.\n5. CONCLUSIONS\nWe describe a method to train a Transformer-Transducer model that\nallows training a single model that can decode speech in both low\nlatency and high latency modes. We also propose Y-model trained\nwith variable right context at penultimate layers as an efﬁcient solu-\ntion for unifying low-latency and high-latency decoding modes. The\nlow latency mode can be used for streaming recognition results while\nthe ﬁnal recognition results are obtained from high-latency mode.\nWe show that with a small future audio context of 2.4 sec, we can\nget very similar accuracy to a model with full audio context. We also\nshow that the extra latency at the end of decoding from high latency\nmode can be made very small with parallel encoding of audio frames\nand decoder optimizations.\n6. REFERENCES\n[1] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural\nnetworks,” IEEE Transactions on Signal Processing, vol. 45,\nno. 11, pp. 2673–2681, 1997.\n[2] Alex Graves, Santiago Fern ´andez, and J ¨urgen Schmidhuber,\n“Bidirectional lstm networks for improved phoneme classiﬁ-\ncation and recognition,” in Artiﬁcial Neural Networks: Formal\nModels and Their Applications – ICANN 2005 , Włodzisław\nDuch, Janusz Kacprzyk, Erkki Oja, and Sławomir Zadro ˙zny,\nEds., Berlin, Heidelberg, 2005, pp. 799–804, Springer Berlin\nHeidelberg.\n[3] H. Sak, A. Senior, K. Rao, O. ˙Irsoy, A. Graves, F. Beau-\nfays, and J. Schalkwyk, “Learning acoustic frame labeling for\nspeech recognition with recurrent neural networks,” in 2015\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2015, pp. 4280–4284.\n[4] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik\nMcDermott, Stephen Koo, and Shankar Kumar, “Transformer\ntransducer: A streamable speech recognition model with trans-\nformer encoders and rnn-t loss,” 2020.\n[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin, “Attention is all you need,” in Advances in neural\ninformation processing systems, 2017, pp. 5998–6008.\n[6] Zihang Dai, Zhilin Yang, Yiming Yang, William W Co-\nhen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov, “Transformer-xl: Attentive language models beyond a\nﬁxed-length context,” in Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics , 2019, p.\n2978–2988.\n[7] Alex Graves, “Sequence transduction with recurrent neural\nnetworks,” in Proceedings of the 29th International Confer-\nence on Machine Learning, 2012.\n[8] A. Senior, H. Sak, F. de Chaumont Quitry, T. Sainath, and\nK. Rao, “Acoustic modelling with cd-ctc-smbr lstm rnns,” in\n2015 IEEE Workshop on Automatic Speech Recognition and\nUnderstanding (ASRU), 2015, pp. 604–609.\n[9] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein,\n“Rnn-transducer with stateless prediction network,” inICASSP\n2020 - 2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2020, pp. 7049–\n7053.\n[10] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D Cubuk, and Quoc V Le, “Specaugment: A\nsimple data augmentation method for automatic speech recog-\nnition,” arXiv preprint arXiv:1904.08779, 2019.\n[11] Jouppi et al, “In-datacenter performance analysis of a tensor\nprocessing unit,” in Proceedings of the 44th Annual Interna-\ntional Symposium on Computer Architecture , New York, NY ,\nUSA, 2017, ISCA ’17, p. 1–12, Association for Computing\nMachinery.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.817752480506897
    },
    {
      "name": "Latency (audio)",
      "score": 0.7078172564506531
    },
    {
      "name": "Decoding methods",
      "score": 0.6724220514297485
    },
    {
      "name": "Speech recognition",
      "score": 0.6711530089378357
    },
    {
      "name": "Transformer",
      "score": 0.6520968675613403
    },
    {
      "name": "Inference",
      "score": 0.5994588136672974
    },
    {
      "name": "Encoder",
      "score": 0.5718023180961609
    },
    {
      "name": "Real-time computing",
      "score": 0.3362817168235779
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3122643232345581
    },
    {
      "name": "Algorithm",
      "score": 0.12204957008361816
    },
    {
      "name": "Engineering",
      "score": 0.09232106804847717
    },
    {
      "name": "Telecommunications",
      "score": 0.08595135807991028
    },
    {
      "name": "Voltage",
      "score": 0.07978051900863647
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}