{
    "title": "VHF Speech Enhancement Based on Transformer",
    "url": "https://openalex.org/W4210682693",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5066962274",
            "name": "Xue Han",
            "affiliations": [
                "Dalian Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A5063170334",
            "name": "Mingyang Pan",
            "affiliations": [
                "Dalian Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A5012884230",
            "name": "Zheng-zhong Li",
            "affiliations": [
                "Dalian Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A5019069670",
            "name": "Haipeng Ge",
            "affiliations": [
                "Dalian Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A5086547689",
            "name": "Zongying Liu",
            "affiliations": [
                "Dalian Maritime University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3135188703",
        "https://openalex.org/W3174904875",
        "https://openalex.org/W3175140823",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W2128653836",
        "https://openalex.org/W1995536493",
        "https://openalex.org/W2121973264",
        "https://openalex.org/W2078528584",
        "https://openalex.org/W2963453742",
        "https://openalex.org/W2897371647",
        "https://openalex.org/W6756251360",
        "https://openalex.org/W3044712972",
        "https://openalex.org/W2605589342",
        "https://openalex.org/W2962843322",
        "https://openalex.org/W3122728801",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3129077738",
        "https://openalex.org/W3095820034",
        "https://openalex.org/W2964089206",
        "https://openalex.org/W3161950572",
        "https://openalex.org/W3096893582",
        "https://openalex.org/W6737664043",
        "https://openalex.org/W2963242190",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W1552314771",
        "https://openalex.org/W2963341071"
    ],
    "abstract": "To solve the poor quality of Very high frequency (VHF) speech communication in the navigation field, a VHF speech enhancement model based on an improved transformer (VHFSE) is proposed in this paper. The long-term and short-term noise are the reasons for the poor quality of VHF voice communication. VHFSE can reduce these two aspects of noise. We select the Two-stage Transformer based Neural Network (TSTNN) as the baseline. The Transformer structure pays attention to global information and parallel computing, which can reduce the long-term noise. In order to strengthen the ability of the model to reduce short-term noise, we add CNN module to the transformer according to the ability of revolutionary neural networks (CNN) to extract local information. Meanwhile, to improve the real-time performance, this study employs the lightweight convolution module (Depthwise Separable Convolution) to efficiency of VHF speech communication. Experimental results show that the proposed model VHFSE obtains the highest PESQ and STOI values than other compared modules. Besides, we apply the self-built dataset in our proposed model. The spectrum diagram shows that our model has the best enhancement effect on navigation VHF speech.",
    "full_text": "Received 25 September 2021; revised 15 December 2021; accepted 26 January 2022. Date of publication 31 January 2022;\ndate of current version 21 February 2022.\nDigital Object Identifier 10.1109/OJITS.2022.3147816\nVHF Speech Enhancement Based on Transformer\nXUE HAN, MINGYANG PAN , ZHENGZHONG LI, HAIPENG GE, AND ZONGYING LIU\nNavigation College, Dalian Maritime University, Dalian 116026, China\nCORRESPONDING AUTHOR: M. PAN (e-mail: panmingyang@dlmu.edu.cn)\nThis work was supported in part by the Fundamental Research Funds for the Central Universities under Grant 3132019400.\nABSTRACT To solve the poor quality of Very high frequency (VHF) speech communication in the\nnavigation field, a VHF speech enhancement model based on an improved transformer (VHFSE) is\nproposed in this paper. The long-term and short-term noise are the reasons for the poor quality of\nVHF voice communication. VHFSE can reduce these two aspects of noise. We select the Two-stage\nTransformer based Neural Network (TSTNN) as the baseline. The Transformer structure pays attention to\nglobal information and parallel computing, which can reduce the long-term noise. In order to strengthen\nthe ability of the model to reduce short-term noise, we add CNN module to the transformer according to\nthe ability of revolutionary neural networks (CNN) to extract local information. Meanwhile, to improve\nthe real-time performance , this study employs the lightweight convolution module (Depthwise Separable\nConvolution) to efficiency of VHF speech communication. Experimental results show that the proposed\nmodel VHFSE obtains the highest PESQ and STOI values than other compared modules. Besides, we\napply the self-built dataset in our proposed model. The spectrum diagram shows that our model has the\nbest enhancement effect on navigation VHF speech.\nINDEX TERMS Depthwise separable convolution, speech enhancement, transformer, VHF.\nI. INTRODUCTION\nI\nN RECENT years, to meet global security, efficiency, and\nenvironmental protection requirements, the state acceler-\nates the deployment of “Internet plus government services.”\nThe maritime system takes the construction of “smart mar-\nitime” as the guide to promote maritime informatization. To\nrealize intelligent ships, an increasing number of computer\ntechnologies are applied in the field of navigation. For exam-\nple, Du et al. [1] combined the contour stress algorithm with\nthe navigation sign classification model to improve the classi-\nfication accuracy of navigation signs. Han et al. [2] proposed\na faster and more accurate ship detection model, ShipYOLO.\nShan et al. [3] proposed an offshore target detection algo-\nrithm based on electronic image stabilization technology.\nHowever, there is still a lack of research on vessel traf-\nfic service (VTS) informatization. As a tool of ship traffic\nmanagement, the intellectual development of VTS is the\ninevitable trend. VHF is the most important tool of communi-\ncation for maritime mobile services. VHF not only carries out\nship distress, emergency, safety communications, and daily\nThe review of this article was arranged by Associate Editor Chi-Hua\nChen.\nbusiness communication, but is also an essential communica-\ntion tool for searching and rescuing operation, coordination\nand evading between ships and VTS. Ensuring VHF com-\nmunication quality is essential for the efficiency and safety\nof ship sailing. However, the disturbed VHF communica-\ntion affects the communication between VTS and captains, it\nmakes VTS inefficient and has great potential safety hazards.\nAt the same time, improving the VHF speech quality assists\non realizing navigation speech recognition and navigation\nlanguage analysis in the future work. The VHF frequency\nband is unique, and its spectrum diagram is shown in Fig. 1.\nIn the real sense, VHF is easy to be attacked by many\nunknown interferences. Therefore, the VHF speech not only\nhas the same kind of noise for a long time, but it also has\nspecial short-term noise. Reducing long-term noise requires\nthe model to combine global information, while reducing\nshort-term noise requires focusing on local information. The\ntransformer is good at extracting global features, and CNN\nis suitable for local features [4]. According to this par-\nticularity of VHF speech, we propose an improved VHF\nspeech enhancement model based on a transformer (VHFSE),\nwhich adds the convolution module. This structure pays more\nattention to local features under the situation of the naviga-\ntion VHF speech. The proposed model improves the speech\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n146 VOLUME 3, 2022\nFIGURE 1. Navigation VHF speech spectrum.\nenhancement evaluation index of the model and maintains\nthe real-time performance of the model.\nOur contributions can be summarized as follows: 1. To\nsolve the navigation VHF speech enhancement task by using\nthe deep learning method. 2. To propose a VHFSE model for\nreducing long-term and short-term noise in real VHF speech.\n3. To solve the real-time problem of speech enhancement by\nadding a lightweight CNN module in the transformer. 4. Our\nproposed model can reduce long-term and short-term noise.\nIt meets the characteristics of maritime VHF speech.\nThis paper is organized as follows: Section II introduces\nthe related work of speech enhancement; Section III pro-\nvides a new method of speech enhancement. We add the\nCNN module to the transformer to make it more suitable\nfor VHF scene speech. Section IV is the introduction of\nthe experiment and the analysis of the experimental results.\nConclusions are drawn in Section V .\nII. RELATED WORK\nSpeech enhancement is mainly used to improve the quality\nof speech communication in a noisy environment. Speech\nenhancement methods are divided into the traditional digital\nsignal processing and the deep learning methods. Traditional\nmethods include spectral subtraction [5], Wiener filtering [6],\nminimum mean square error method [7], etc. Compared\nwith the traditional speech enhancement methods, the deep\nlearning methods have the advantages in the field of speed\nand computation complexity. It shows better processing\nefficiency in a non-stationary environment.\nVarious network models are used in speech enhancement\ntasks. Such as, Xu et al. [8] proposed DNN for speech\nenhancement, which improved common music artifacts in\nthe traditional speech enhancement methods. Compared\nwith DNN, Park and Lee [9] learnt the local features\nwith fewer parameters than the fully connected networks.\nTan et al. [10] systematically aggregated the context to\nexpand the receptive field through expended convolution.\nMacartney and Weyde [11] applied wave-u-net architecture\nto enhance speech and reduce the number of hidden layers.\nYuanet al. [12] combined a convolutional neural network and\nthe attention mechanism for the speech separation. They used\nconvolutional neural networks to extract low-dimensional\nfeatures of speech signals, and attention mechanisms were\nused to reduce the loss of sequence information. DNN and\nCNN were the beginning of speech enhancement using deep\nlearning methods.\nCNN and DNN cannot sufficiently describe the spectrum\nrelationship among the continuous frame speech signals.\nRNN uses the previous input to predict the current state,\nwhich is more suitable for capturing the speech spectrum’s\ninter-frame relationship. Sun et al. [13] proposed a regres-\nsion LSTM-RNN method, which directly map the noisy\nspeech features in the clean speech features and achieved\na better denoising effection than DNN. Zhao et al. [14]\nproposed an end-to-end speech enhancement model based\non CNN and RNN, which generalized visible and invisible\nnoise. Sun et al. [15] combined the advantages of CNN and\nRNN with attention mechanism, focused on the relationship\nbetween different features and the further optimized speech\nseparation performance. The application of RNN promoted\nthe development of speech enhancement. After the trans-\nformer appearances, its speech enhancement effect exceeds\nthe RNN. Because the transformer only uses the attention\nmechanism, which solves the long-term dependence of RNN\nand the problem of vanishing gradient or exploding gradient.\nTransformer [16] was proposed for solving the problem of\ntext length dependency in the NLP field. Due to the attention\nmechanism, it operated in parallel and extracted the global\ninformation. Compared with RNN, the transformer improves\nthe training speed and prediction accuracy [16]. Recently,\nthe transformer is also widely used in speech processing.\nYu et al. [17] proposed SETransformer by using the advan-\ntages of LSTM and multi-head attention mechanism. Its\nexperimental results demonstrated SETransformer obtained\nbetter denoising performance than a standard transformer and\nLSTM model. Nicolson and Paliwal [18] combined masked\nmulti-head attention with DNN, which was suitable for the\ncausal speech enhancement. Sperber et al. [19] improved\nself-attention to make it more suitable for an acoustic model.\nThere was a robust evidence in the article that the speed\nof the self-attention model was faster. Wang et al. [20]\nproposed a two-stage transformer neural network (TSTNN)\nfor end-to-end speech enhancement. This method under\nbenchmarks obtained much better performance in the speech\nenhancement than others. However, the processing effect of\nTSTNN did not perform well in the navigation VHF speech\nwith local noise. Therefore, this study adds a CNN module\nin the transformer of the TSTNN for extracting the local\nnoise in VHF speech. The details of specific methods will\nbe introduced in the following chapter.\nIII. VHFSE\nWe introduce the overall structure of VHFSE model in\nSection III-A. The transformer block in VHFSE model\nVOLUME 3, 2022 147\nHAN et al.: VHF SPEECH ENHANCEMENT BASED ON TRANSFORMER\nFIGURE 2. The overall architecture of VHFSE.\nand the method of model improvement are described\nin Section III-B. Section III-C introduces the calculation\nprocess of the model in the form of Pseudocode.\nA. SYSTEM OVERVIEW\nThe overall structure of the VHFSE model is shown in Fig. 2.\nThe model is mainly divided into four phases, including\nEncoder, Transformer Module, Masks, and Decoder. The\nencoder contains two convolution layers. The first convolu-\ntion layer increases by 64 channels. The second convolution\nlayer halves the frame size. Transformer Module is the\nfeature extraction part of the VHFSE. It includes four trans-\nformer blocks (Fig. 3) for learning the local and global\nfeatures. The specific structure of this part is described in\nSection III-B The Masks phase obtains the mask through\ntwo-way binary convolution and nonlinear operation. In the\nphase of decoder, reconstructs the features are reconstructed\nby the convolution of the Dilated-Dense block and Sub-pixel.\nThen, the output matrix is proposed by the LayerNorm and\nPReLU to transform features to be 1 * 1 convolution. Finally,\nthe model obtains the enhanced speech waveform.\nB. TRANSFORMER BLOCK\nThe specific structure of the transformer block is shown in\nFig. 3. It uses a dual channel converter [21]. One is for\nthe local information extraction and the other one is for the\nglobal information extraction. First, Intra-transformer local\nblock models the local chunk independently. Then, this study\nuses Inter-transformer to summarize all blocks’ information\nfor obtaining the global dependencies. The same information\nis extracted in two different ways. And their outputs are\ncombined. According to this approach, the processed output\nfrom transformer has the high dimensional and rich features.\nTherefore, this study proposes an Improved Transformer. The\nFIGURE 3. Transformer Block.\nspecific structure of the Improved Transformer is shown in\nFig. 4.\nThe proposed model is developed based on the feature\nextraction part of TSTNN. Before introducing our proposed\nmethod, we firstly represent the original TSTNN (baseline),\nwhich is shown in Fig. 4 (1). The input of TSTNN learns the\nglobal information through the Multi-Head Attention. Then\nthe normalization of data is realized through Layer Norm.\nFinally, TSTNN converts the full connection layer in the\nFeed-forward Network into GRU. This structure obtains the\nposition information.\nIn this study, we propose a speech enhancement model\nnamed VHF Speech Enhancement(VHFSE). Its structure is\nshown in Fig. 4 (2). To extract the special short-term noise\nin the navigation VHF speech, this study adds a convolu-\ntion module based on the baseline. CNN shares the weights\nand have the locally receptive fields. It has a strong abil-\nity to capture details. Therefore, this convolution module\nenhances the extraction ability of the local features in the\nmodel. Meanwhile, to keep the reasoning speed of the model,\ninspired by [22], we apply pointwise (PW) and depthwise\n(DW) to replace the traditional convolution operation. DW\nand PW has few parameters, low operation cost, which dose\nnot change the real-time performance. Next, we introduce\nthe specific calculation process of the improved transformer.\nProcessed by encoder, the following phase is the Multi-\nHead Attention block. In this block, first obtain the input\nqueries (Q), keys (K), and values (V), then use formula\n(1) and formula (2) to obtain the attention of each head and\nuse formula (3) to connect the attention of all heads in series\nand project linearly again to obtain the output of this block.\nAttention(Q,K,V) = softmax\n( QK\nT\n√dk\n)\nV (1)\n148 VOLUME 3, 2022\nFIGURE 4. Improved Transformer.\nheadi = Attention\n(\nQWQ\ni ,KWK\ni ,VWV\ni\n)\n(2)\nMultiHead(Q,K,V) = Concat(head1,..., headn)WO (3)\nwhere i = 1,2,..., h, WQ\ni ,WK\ni ,WV\ni ∈ Rd×d/h represents the\nlinear transformation matrix of queries, keys, and values,\nrespectively. WO ∈ Rd×d is the linear transformation matrix.\nh is the number of attention layers, which we set to 4.\nAfter the Multi-Head Attention calculation, the data enters\nthe Convolution Module we added. In the Convolution\nModule, we use depthwise separable convolution instead\nof standard convolution. The standard convolution operation\ncombines the filters and inputs into a new set of outputs in\none step. Adding the standard convolution to the model will\nincrease the model parameters. However, depthwise separa-\nble convolution is to decompose the standard convolution\ninto the depthwise convolution and the pointwise convolu-\ntion. This structure can significantly reduce the number of\nparameters and calculations. Depthwise convolution uses ker-\nnels on each depth or channel of the input volume. Instead of\nusing kernels simultaneously on all input volumes in a stan-\ndard convolution operation. This process can be expressed\nby formula (4).\nˆG\nk,l,m =\n∑\ni,j\nˆKk,l,m · Fk+i+1,l+j-1,m (4)\nwhere ˆK is the depthwise convolution, and the convolution\nkernel is (DK,DK,1,M), where mth convolution kernels are\napplied to the mth channel in F to generate the mth chan-\nnel output on ˆG. Inspired by [4], we use GLU and swish\nactivation functions in the convolution module.\nC. THE PROPOSED MODEL VHFSE\nIf we denote the speech mixture by x, The input speech\nvector x is divided into nframes blocks, in which there are\nframe_size frames. The dimension of the speech vector is\ntransformed as [batch_size,channels,nframes,frame_size].\nThe input speech vector x first goes through the encoder\nphase. The encoder uses Conv2d to increase the number\nof channels to 64. Then, the Dilated − Dense increases the\nreceptive field of CNN. Finally, in order to reduce the amount\nof subsequent calculations, this phase uses another Conv2d to\nreduce the frame_size. LayerNorm and PReLU is calculated\nafter each Conv2d.\nThe second phase is the Transformer Module. Firstly, this\nphase calculates the multi-head attention. Then, the pointwise\nand the depthwise is calculated. The activation functions in\nthis part are GLU and swish, respectively. The output of the\nmulti-head attention is x\nrln. This phase changes the dimen-\nsion of the xrln. Then, repeat the step 14 to 27. The next step\nis to combine xo with xcln and xrln. Finally, the transformer\nmodule repeats 4 times to get the output.\nThe third phase is Masks. This phase applies a two-path\nConv2d and nonlinearity operation to obtain xtan and xsig.\nThe next step is to multiply the two results as the input\nVOLUME 3, 2022 149\nHAN et al.: VHF SPEECH ENHANCEMENT BASED ON TRANSFORMER\nof Conv2d and PReLU to obtain xprelu. Finally, the xmm is\ncomputed by the multiplication between the xprelu and the\noutput of the encoder.\nThe Decoder phase employs the Dilated − Dense and\nthe Sub − pixelConv to reconstruct the xmm into enhanced\nspeech feature. After the LayerNorm, PReLU and Conv2d,\nthe enhanced speech vector Z is calculated. The Pseudocode\nof our proposed model VHFSE is shown in Algorithm 1.\nIV. EXPERIMENTAL RESULTS AND DISCUSSIONS\nA. DATASETS\nThis study uses the AISHELL [23] open-source Chinese\nspeech data set. The speech in this dataset is clean. We\nextract 17 hours of the speech data set from AISHELL as\nthe training set, and extract 1.5 hours of the speech data set\nas the test set. Thirty kinds of pure noise are intercepted\nby us from the real VHF scene speech. When establishing\nthe training set, we choose 25 kinds of noise as the training\nset noise from the noise set. The noise is randomly added\nto the clean speech. This method is also used in the test\nset. When adding the noise, we set the signal-noise ratio to\n0dB or 5dB, or 10dB.\nFurthermore, we produce a dataset named VHFV oice.\nVHFV oice is taken from the real scene navigational VHF\nvoice, with a duration of 25 minutes and a sampling rate\nof 16000. The dataset is used to test the performance of the\nmodel in the real scene, with only noisy speech.\nB. EXPERIMENTAL SETUP\nThis study conducts experiments in an NVIDIA V100 envi-\nronment. The sampling rate of each speech signal is 16KHz,\nthe size of each frame is 512 samples, and it overlaps 256 sam-\nples. Most of the VHF real scene speech duration is distributed\naround 4 seconds. So, we intercept or pad all speech to 4 sec-\nonds. This study trains the model with 200 epochs, optimizes\nit with Adam [24], and applies gradient clipping with an L2\nnorm of 5 to avoid gradient explosion during training. This\nexperiment linearly increases the learning rate during training\nand then attenuate it by 0.98 every two epochs. As shown\nin formula (5), (6), (7), the loss function combines the time\ndomain and the time-frequency domain [20]. In formula (5), F\nand T represent the number of frequency bins and the number\nof time frames. X and ˆX represent the spectrum of the clean\nspeech waveform and the spectrum of the enhanced speech\nwaveform. r and i represent the real and imaginary parts of\ncomplex variable. In formula (6), X and ˆX are samples of\nclean speech and enhanced speech. In formula (7), α is set\nto 0.2 in our experiment.\nl oss\nF = 1\nTF\nT−1∑\nt=0\nF−1∑\nf =0\n[\n(|Xr(t,f )| + |Xi(t,f )|)\n−\n( ⏐⏐\n⏐ˆX\nr(t,f )\n⏐⏐\n⏐ +\n⏐⏐\n⏐ˆX\ni(t,f )\n⏐⏐\n⏐\n)]\n(5)\nloss\nT = 1\nN\nN-1∑\ni=0\n(\nxi −ˆxi\n)2 (6)\nloss = α*lossF + (1-α)lossT (7)\nAlgorithm 1 VHFSE\nRequire: the training features (noisy speech), X.\nEnsure: the target value(enhanced speech), Z.\nEncoder:\n1: x ∈ Rb,c,n,f (b:batch_size,c:channels,n:nframes,f :frame_size)\n2: xc = Conv2d(x) (change the channels from 1 to 64)\n3: xp = PReLU(LayerNorm(xc))\n4: xd = Dilated − Dense(xp)\n5: xc = Conv2d(xd) (change the nframes from 512 to 256,\nwhich can reduce the amount of subsequent calculations)\n6: xp = PReLU(LayerNorm(xc))\nTransformerModule:\n7: for n ∈{ 1, ...,4} do\n8: x ∈ Rb,c,n,f\n9: Calculate the multi-head attention of xp by formula\n(1)(2)(3),to obtain xr\n10: xr = xr + xp\n11: xrn = LayerNorm(xr)\n12: xrp = pointwise(xrn)\n13: xrg = GLU(xrp)\n14: xrd = depthwise(xrg)\n15: xrs = swish(xrd)\n16: xrpp = pointwise(xrs)\n17: xrpp = xrpp + xr\n18: xrl = LayerNorm(xrpp)\n19: xrr = GRU(xrl)\n20: xrre = ReLU(xrr)\n21: xrli = Linear(xrre)\n22: xrli = xrli + xrl\n23: xrln = LayerNorm(xrli)\n24: xrln ∈ Rb,c,n,f (Change the dimension)\n25: Calculate the multi-head attention of xrln by formula\n(1)(2)(3), to obtain xcol\n26: Repeat steps 14 to 27\n27: xcol ∈ Rb,c,n,f (Change the dimension)\n28: xcln = LayerNorm(xcol)\n29: xo = xo + xcln + xrln\n30: end for\nMasks:\n31: xprelu = PReLU(xo)\n32: xmc = Conv2d(Conv2d(xprelu))\n33: xtan = tanh(xmc)\n34: xsig = sigmoid(xmc)\n35: xm = xtan × xsig\n36: xprelu = PReLU(Conv2d(xm))\n37: xmm = xprelu × xp\nDecoder:\n38: xd = Dilated − Dense(xmm)\n39: xsub = Sub − pixelConv(xd)\n40: Z = Conv2d(PReLU(LayerNorm(xsub)))\n41: return Z\nWe select speech quality perception assessment\n(PESQ) [25] and short-term objective intelligibility\n(STOI) as speech quality assessment criteria. The value\n150 VOLUME 3, 2022\nFIGURE 5. Spectrum Diagram.\nTABLE 1. Evaluation.\nTABLE 2. Average speech enhancement time.\nrange of PESQ is [ −0.5, 4.5]. The larger the PESQ value,\nthe better the speech quality; The value range of STOI is\n[0, 1]. The larger the STOI value, the higher the speech\nintelligibility.\nC. EXPERIMENTAL RESULTS\nTable 1 lists the PESQ and STOI evaluation values obtained\nafter using different models to enhance the test set. In this\ntable, the SEGAN [26] is a model that applies Generative\nAdversarial Networks to speech enhancement. It operates\nat the waveform level and trains the model end-to-end.\nThe Wave-U-Net [11] operates directly in the time domain,\nallows integrated modeling of phase information, and can\nconsider large time contexts. TSTNN [20] is the base-\nline model of this paper, which is described in detail in\nSection III-B. VHFSE is our improved model based on\nTSTNN. It can be seen from the table that the PESQ and\nSTOI values of our model are higher than those of the other\nmodels. These experimental results verify the effectiveness\nof the model improvement in this paper.\nTable 2 lists the average time of speech enhancement\nfor each model. The noise reduction speed of the SEGAN\nmodel is 0.3 seconds slower than our model VHFSE. The\nWave-U-Net model has the fastest noise reduction speed,\nbut its accuracy is lower. The speed of our model is only\n0.01 second slower than the TSTNN model. Compared with\nthe baseline, the real-time performance of our improved\nmodel changes little. It can be concluded that adding a\nconvolution module to the transformer model can increase\nthe model’s accuracy and will not change the real-time\nperformance of the model.\nIn addition to the experiments on the test set, to intu-\nitively compare the effects of our improved modelwe select\nthe speech in VHFV oice dataset to test the enhancement\neffect of the model. Fig. 5 is a speech spectrum diagram,\nin which (1) shows the linear frequency power spectrum\nand the logarithmic frequency power spectrum of speech\nin the VHFV oice dataset taken from the real scene, (2) shows\nthe TSTNN (baseline) enhancement result, and (3) shows the\nVHFSE (our model) enhancement result. The red box con-\ntains the special local noise of this speech. From the figure,\nthe denoising effect of the figure (3) on the local noise in the\nred box is better than the figure (2). Therefore, our model\nhas a better denoising ability for local noise.\nV. CONCLUSION\nIn this paper, the characteristics of VHF speech are obtained by\nanalyzing navigation VHF speech in real scenes. Navigation\nVHF speech has not only long-term global noise but also\nshort-term local noise. According to the characteristics of VHF\nspeech, we propose a navigation VHF speech enhancement\nmodel (VHFSE) based on the transformer. Our model adds a\nCNN module to the transformer in the feature extraction part\nof the baseline to improve the extraction effect of the model\non local information. At the same time, we use lightweight\nconvolution instead of the standard convolution to ensure the\nreal-time performance of the model. Experiments show that\nour VHFSE model effectively solves the problem of excessive\nVHF speech noise. Compared with other methods, our model\nhas the highest PESQ and STOI values.\nThe method proposed in this paper fills the gap of using\ndeep learning methods to solve the problems of VHF speech\nnoise. At the same time, this method also solves the data\nproblems for our future research on VHF speech recognition\nand navigation language analysis.\nREFERENCES\n[1] Y . Du, S. Sun, S. Qiu, S. Li, M. Pan, and C.-H. Chen, “Intelligent\nrecognition system based on contour accentuation for navigation\nmarks,” Wireless Commun. Mobile Comput. , vol. 2021, pp. 1–11,\nMar. 2021.\nVOLUME 3, 2022 151\nHAN et al.: VHF SPEECH ENHANCEMENT BASED ON TRANSFORMER\n[2] X. Han, L. Zhao, Y . Ning, and J. Hu, “ShipYOLO: An enhanced\nmodel for ship detection,” J. Adv. Transp. , vol. 2021, Jun. 2021,\nArt. no. 1060182.\n[3] X. Shan, M. Pan, D. Zhao, D. Wang, F.-J. Hwang, and C.-H. Chen,\n“Maritime target detection based on electronic image stabilization\ntechnology of shipborne camera,” IEICE Trans. Inf. Syst. , vol. E104.D,\nno. 7, pp. 948–960, 2021.\n[4] A. Gulati et al. , “Conformer: Convolution-augmented transformer\nfor speech recognition,” in Proc. INTERSPEECH Conf. , 2020,\npp. 5036–5040.\n[5] S. Boll, “Suppression of acoustic noise in speech using spectral sub-\ntraction,” IEEE Trans. Acoust., Speech, Signal Process. , vol. 27, no. 2,\npp. 113–120, Apr. 1979.\n[6] J. Lim and A. Oppenheim, “All-pole modeling of degraded speech,”\nIEEE Trans. Acoust., Speech, Signal Process. , vol. 26, no. 3,\npp. 197–210, Jun. 1978.\n[7] Y . Ephraim and D. Malah, “Speech enhancement using a min-\nimum mean-square error log-spectral amplitude estimator,” IEEE\nTrans. Acoust., Speech, Signal Process. , vol. 33, no. 2, pp. 443–445,\nApr. 1985.\n[8] Y . Xu, J. Du, L.-R. Dai, and C.-H. Lee, “An experimental study on\nspeech enhancement based on deep neural networks, ” IEEE Signal\nProcess. Lett. , vol. 21, no. 1, pp. 65–68, Jan. 2014.\n[9] S. R. Park and J. Lee, “A fully convolutional neural network for speech\nenhancement,” in Proc. INTERSPEECH Conf. , 2017, pp. 1993–1997.\n[10] K. Tan, J. Chen, and D. Wang, “Gated residual networks with\ndilated convolutions for monaural speech enhancement,” IEEE/ACM\nTrans. Audio, Speech, Language Process. , vol. 27, no. 1, pp. 189–198,\nJan. 2019.\n[11] C. Macartney and T. Weyde, “Improved speech enhancement with the\nwave-U-net,” 2018, arXiv:1811.11307.\n[12] C. M. Yuan, X. M. Sun, and H. Zhao, “Speech separation using\nconvolutional neural network and attention mechanism,” Discr. Dyn.\nNat. Soc. , vol. 2020, no. 6, pp. 1–10, 2020.\n[13] L. Sun, J. Du, L.-R. Dai, and C.-H. Lee “Multiple-target deep\nlearning for LSTM-RNN based speech enhancement,” in Proc. Hands-\nFree Speech Commun. Microphone Arrays (HSCMA) Conf. , 2017,\npp. 136–140.\n[14] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, “Convolutional-recurrent\nneural networks for speech enhancement,” in Proc. IEEE Int. Conf.\nAcoust. Speech Signal Process. (ICASSP) Conf. , 2018, pp. 2401–2405.\n[15] C. Sun et al., “A convolutional recurrent neural network with attention\nframework for speech separation in monaural recordings,” Sci. Rep. ,\nvol. 11, no. 1, pp. 1–14, 2021.\n[16] A. Vaswani et al. , “Attention is all you need,” in Proc. Adv. Neural\nInf. Process. Syst. Conf. , 2017, pp. 5998–6008.\n[17] W. Yu, J. Zhou, H. Wang, and L. Tao, “SETransformer: Speech\nenhancement transformer,” Cogn. Comput. , vol. 2021, pp. 1–7,\nFeb. 2021.\n[18] A. Nicolson and K. Paliwal, “Masked multi-head self-attention for\ncausal speech enhancement,” Speech Commun. , vol. 125, no. 3,\npp. 80–95, 2020.\n[19] M. Sperber, J. Niehues, G. Neubig, S. Stüker, and A. Waibel, “Self-\nattentional acoustic models,” in Proc. INTERSPEECH Conf. , 2018,\npp. 3723–3727.\n[20] K. Wang, B. He, and W. Zhu, “TSTNN: Two-stage transformer based\nneural network for speech enhancement in the time domain,” in Proc.\nIEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP) Conf. , 2021,\npp. 7098–7102.\n[21] J. Chen, Q. Mao, and D. Liu, “Dual-path transformer network: Direct\ncontext-aware modeling for end-to-end monaural speech separation,”\nin Proc. INTERSPEECH Conf. , 2020, pp. 2642–2646.\n[22] A. G. Howard et al. , “MobileNets: Efficient convolutional neural\nnetworks for mobile vision applications,” 2017, arXiv:1704.04861.\n[23] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “AISHELL-1: An open-\nsource mandarin speech corpus and a speech recognition baseline,”\nin Proc. 20th Conf. Oriental Chapter Int. Coordinating Committee\nSpeech Databases Speech I/O Syst. Assess. (O-COCOSDA) Conf. ,\n2017, pp. 1–5.\n[24] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” 2014. arXiv:1412.6980.\n[25] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra “Perceptual\nevaluation of speech quality (PESQ)-a new method for speech quality\nassessment of telephone networks and codecs,” in Proc. IEEE Int.\nConf. Acoust. Speech Signal Process. , 2001, pp. 749–752.\n[26] S. Pascual, A. Bonafonte, and J. Serrà, “SEGAN: Speech enhancement\ngenerative adversarial network,” in Proc. INTERSPEECH Conf. , 2017,\npp. 3642–3646.\nXUE HAN received the bachelor’s degree in intelli-\ngent science and technology from Dalian Maritime\nUniversity, Dalian, China, in 2020, where she is\ncurrently the master’s degree in transport engi-\nneering. Her research interests are mainly on\nintelligent waterway transportation systems and\nspeech enhancement.\nMINGYANG PAN received the Ph.D. degree in\ntraffic information engineering and control from\nDalian Maritime University, Dalian, China, in\n2004.\nHe is a Professor with the Navigation College,\nDalian Maritime University, where he is also the\nDirector of the Technical Institute of Navigation.\nHis research activities are in the fields of elec-\ntronic chart display and information system, digital\nwaterway system, and intelligent waterway trans-\nportation system.\nZHENGZHONG LI received the bachelor’s degree in\ntraffic engineering from the Shandong University\nof Technology, Zibo, China, in 2019. He is cur-\nrently pursuing the master’s degree in transporta-\ntion engineering with Dalian Maritime University,\nDalian, China. His research interests are mainly\non multicamera vedio stiching and water target\ndetection.\nHAIPENG GE received the bachelor’s degree from\nDalian Maritime University in 2019, where he\nis currently pursuing the master’s degree. His\nresearch interests include data analysis and com-\nputer vision.\nZONGYING LIU received the B.Eng. degree in tex-\ntile engineering and financial management from\nQiqihar University, China, in 2009, and the M.S.\nand Ph.D. degrees in artificial intelligence from the\nUniversity of Malaya in 2015 and 2019, respec-\ntively.\nHe is currently a Lecturer with the Faculty\nof Navigation, Dalian Maritime University. His\ncurrent research interests include time series\nprediction, random projection algorithms, reservoir\ncomputing, machine learning, deep learning, and\noptimization algorithm.\n152 VOLUME 3, 2022"
}