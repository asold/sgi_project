{
  "title": "Non-Autoregressive Text Generation with Pre-trained Language Models",
  "url": "https://openalex.org/W3132164968",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226224972",
      "name": "Su, Yixuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2364924703",
      "name": "Cai, Deng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967442656",
      "name": "WANG Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287531418",
      "name": "Vandyke, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2160704708",
      "name": "Baker Simon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231753118",
      "name": "Li, Piji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231855369",
      "name": "Collier, Nigel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2962969034",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2218641061",
    "https://openalex.org/W2962974924",
    "https://openalex.org/W2995404354",
    "https://openalex.org/W2251656952",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2804268364",
    "https://openalex.org/W2990389671",
    "https://openalex.org/W2971119378",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963536265",
    "https://openalex.org/W2741938760",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2251654079",
    "https://openalex.org/W2988975212"
  ],
  "abstract": "Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. Lastly, to further increase the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component.",
  "full_text": "Non-Autoregressive Text Generation with Pre-trained Language Models\nYixuan Su♦ Deng Cai♥ Yan Wang♠ David Vandyke♣\nSimon Baker♦ Piji Li♠ Nigel Collier♦\n♦Language Technology Lab, University of Cambridge\n♥The Chinese University of Hong Kong\n♠Tencent AI Lab\n♣Apple\n{ys484,sb895,nhc30}@cam.ac.uk\nthisisjcykcd@gmail.com, dvandyke@apple.com\n{brandenwang,pijili}@tencent.com\nAbstract\nNon-autoregressive generation (NAG) has re-\ncently attracted great attention due to its fast\ninference speed. However, the generation qual-\nity of existing NAG models still lags behind\ntheir autoregressive counterparts. In this work,\nwe show that BERT can be employed as the\nbackbone of a NAG model to greatly improve\nperformance. Additionally, we devise mech-\nanisms to alleviate the two common prob-\nlems of vanilla NAG models: the inﬂexibil-\nity of preﬁxed output length and the condi-\ntional independence of individual token predic-\ntions. Lastly, to further increase the speed ad-\nvantage of the proposed model, we propose a\nnew decoding strategy, ratio-ﬁrst, for applica-\ntions where the output lengths can be approx-\nimately estimated beforehand. For a compre-\nhensive evaluation, we test the proposed model\non three text generation tasks, including text\nsummarization, sentence compression and ma-\nchine translation. Experimental results show\nthat our model signiﬁcantly outperforms exist-\ning non-autoregressive baselines and achieves\ncompetitive performance with many strong au-\ntoregressive models. In addition, we also con-\nduct extensive analysis experiments to reveal\nthe effect of each proposed component.1\n1 Introduction\nAutoregressive generation (AG) models achieve\nstate-of-the-art performance on a wide range of\ntext generation tasks, such as machine transla-\ntion (Vaswani et al., 2017) and text summarization\n(Rush et al., 2015). Such models generate a token\nsequence in a left-to-right, token-by-token fashion.\nThe prediction for the next token is conditioned on\nall previously generated tokens. This characteris-\ntic makes it impossible to parallelize the computa-\ntional overhead for token predictions in different\n1All related code, data, and models can be found in\nhttps://github.com/yxuansu/NAG-BERT.\npositions, which leads to a relatively high latency\nin inference. On the other hand, non-autoregressive\ngeneration (NAG) models (Gu et al., 2018) have\nemerged as a promising alternative due to their fast\ninference speed. NAG models omit the sequential\ndependencies within the output-side sequence and\npredict tokens in all positions simultaneously once\nthe output length has been determined beforehand.\nWhile NAG models enjoy full parallelism and faster\ninference, the generation quality of NAG models\noften lags behind their autoregressive counterparts.\nIn this work, we explore the potential of large-\nscale pre-trained language models for improving\nthe performance of non-autoregressive generation.\nSpeciﬁcally, we utilize BERT (Devlin et al., 2019)\nas the backbone for NAG modelling and extend\nthe architecture of BERT with a CRF output layer\n(Lafferty et al., 2001; Sun et al., 2019) for better\ncapturing the output-side dependencies.\nIn addition, we analyze two signiﬁcant limita-\ntions that NAG models currently suffer from: (1)\nthe inﬂexibility of preﬁxed output length, and (2)\nthe conditional independence of individual token\npredictions. Accordingly, we devise two solutions\nto these two problems.\nFirst, prior NAG models require the output\nlength to be determined before token generation,\nthus an extra module for output length prediction\nis always required. Nevertheless, the most likely\nlength from the prediction module is not neces-\nsarily the best-suited one for the token generation\nmodel. To this end, previous works (Gu et al., 2018;\nMa et al., 2019) usually rely on length-parallel de-\ncoding (LPD) (Wei et al., 2019) for performance\nenhancement; that is, generating and re-ranking the\nresults from different output length candidates. In\nthis work, we propose a simple and elegant decod-\ning mechanism that lets the model determine the\noutput length on-the-ﬂy. Speciﬁcally, our model\ndynamically adjusts the output sequence length via\narXiv:2102.08220v1  [cs.CL]  16 Feb 2021\nemitting an [eos] token at any output position\nto indicate the ending of the generated sequence.\nTherefore, we can avoid the additional efforts of\noutput length prediction and results re-ranking.\nSecond, most existing NAG models assume the\ntoken predictions in different positions are condi-\ntionally independent. As a consequence, they often\ntend to generate results that are ungrammatical with\nrepetitions (Wang et al., 2019b). To alleviate this\nproblem, we propose a context-aware learning ob-\njective which impels the model to output different\ntokens at adjacent positions, thereby reducing the\npossibility of repetitive generation.\nFurthermore, for tasks like text summarization,\nthe output sequence (summary) is known to be\nshorter than the source sequence (article). In such\ncases, to further improve the model’s inference ef-\nﬁciency, we introduce a new ratio-ﬁrst decoding\nstrategy. Speciﬁcally, instead of performing infer-\nence on all source-side hidden states, ratio-ﬁrst gen-\nerates the result only based on a subset of source\nhidden states. The subset size is jointly determined\nby the source length T and a predeﬁned ratio α\nthat is set based on our prior knowledge from the\ndata statistics. In the experiments, we show that\nratio-ﬁrst can signiﬁcantly improve the inference\nspeed while maintaining the generation quality.\nWe evaluate the proposed model on three typical\ntext generation tasks, including text summarization,\nsentence compression and machine translation. Ex-\nperimental results show that our model signiﬁcantly\noutperforms many strong non-autoregressive base-\nlines, and even performs competitively with several\nstrong autoregressive models. In addition, we con-\nduct extensive analysis experiments to study the\neffect of individual proposed components.\nIn summary, our contributions are: (1) We pro-\npose a novel framework that utilizes BERT for text\ngeneration under the non-autoregressive generation\nparadigm; (2) We propose a decoding mechanism\nthat allows the model to dynamically determine the\noutput length, and a new context-aware learning\nobjective that reduces errors stemming from the\noutput-side conditional independence assumption;\n(3) We introduce a ratio-ﬁrst decoding strategy that\nfurther improve the model’s inference efﬁciency.\n2 Background\nAutoregressive generation (AG) models generate\nsequences based on a left-to-right factorization. As\nshown in Figure 1, given the source sequence X,\nFigure 1: (a) Autoregressive; (b) Non-Autoregressive\nthe target sequence Y with length T′is generated\nvia a chain of conditional probabilities based on\nthe left-to-right sequential dependencies as:\np(Y|X) =\nT′\n∏\ni=1\np(yi|y<i,X), (1)\nwhere y<i denotes the tokens before the i-th step.\nThis property of autoregressive factorization makes\nthe generation process hard to be parallelized as\nthe result is generated token by token.\nUnlike AG models, non-autoregressive (NAG)\nmodels generate sequences without modelling the\noutput-side dependencies. As shown in Figure 1,\ngiven the prespeciﬁed output length T′, the proba-\nbility of the target sequence Y is then modelled as:\np(Y|X) =\nT′\n∏\ni=1\np(yi|X,i,T ′). (2)\nWith this conditional independence assumption,\nNAG models can fully parallelize their generation\nprocess, which signiﬁcantly improves the inference\nspeed. However, it has been shown that, the choice\nof the prespeciﬁed output length has a notable im-\npact on the model’s generation quality (Gu et al.,\n2018). In addition, the removal of output-side se-\nquential dependency also causes the generation\nquality of NAG models to be inferior to their au-\ntoregressive counterparts (Wang et al., 2019b).\n3 Proposed Model\nIn this section, we give a detailed explanation of the\nproposed model. First, we describe how to utilize\nBERT as a non-autoregressive generation model.\nThen we discuss the decoding mechanism which\nallows the model to determine the output length\ndynamically. Finally, we introduce the new ratio-\nﬁrst decoding strategy which further improves the\nmodel’s decoding efﬁciency.\nFigure 2: The overall illustration of the proposed model: During training, the model parameters are only updated\non the positions of the target sequence. During inference, once the decoded trajectory (colored in red) gets into the\n[eos] state, it will only transit to the[eos] state in the remaining steps. The ﬁnal result is obtained by removing\nthe generated [eos] tokens from the entire decoded trajectory.\n3.1 Model Architecture\nThe architecture of the proposed model is presented\nin Figure 2, in which the embedding layer and\nthe stack of transformer layers are initialized with\nBERT (Devlin et al., 2019).\nInput Representation Following the setup of\nBERT, we ﬁrst append a [cls] and a [sep]\ntoken on both sides of the source sequence. Then\nwe attach a number of [pad] tokens at the end\nof source sequence to make its length equal to the\npredeﬁned maximum size (e.g., 256). Thus we can\nmake sure the source length is longer than or equal\nto the output length. As a special case, for tasks\nlike text summarization where the source is known\nto be longer than the target, we do not attach the\n[pad] tokens when constructing the input.\nTransformer Layers Given the source sequence\nX, it is processed by a stack of N transformer\n(Vaswani et al., 2017) layers. Formally, the Multi-\nHead Attention is deﬁned as MultiHead(Q,K,V),\nwhere Q, K, V denotes the query, key and value re-\nspectively. The computation of the ﬁrst transformer\nlayer is then deﬁned as:\nV(1) = MultiHead(E(X),E(X),E(X)), (3)\nO(1) = FFN(V(1)), (4)\nFFN(x) = max(0,xW1 + b1)W2 + b2, (5)\nwhere E(X) = TE(X)+ PE(X) in which TE(·)\ndenotes the token embedding and PE(·) denotes\nthe position embedding. For other layers:\nV(n) = MultiHead(O(n−1),O(n−1),O(n−1)),\n(6)\nO(n) = FFN(V(n)), (7)\nwhere n = 2,...,N and N is the total number of\ntransformer layers. The ﬁnal sequence representa-\ntion H ∈RT×dmodel is the output states of BERT\nfrom the last layer, where T is the source sequence\nlength and dmodel is the model size.\nCRF Layer Then, H is passed through a linear-\nchain CRF (Lafferty et al., 2001). Under the CRF\nframework, the likelihood of the target sequence Y\nwith length T′is then modelled as:\nPCRF(Y|X) = eS(X,Y)\n∑\nY′eS(X,Y′)\n= 1\nZ(X) exp(\nT′\n∑\ni=1\nΦyi (hi) +\nT′\n∑\ni=2\nt(yi−1,yi)),\n(8)\nwhere Z(X) is the normalizing factor and Φyi (hi)\ndenotes the label score of yi at position i. In prac-\ntice, Φ is parameterized by a neural network that\nmaps the BERT output state hi into the label (vo-\ncabulary) space. The t(yi−1,yi) = Tyi−1,yi de-\nnotes the transition score from label yi−1 to yi\nwhere T ∈R|V|×|V|is the transition matrix.\nApproximation In the context of text genera-\ntion, the size of the label space (vocabulary size)\n|V|is typically large, e.g., 32k. Therefore, it is in-\ntractable to directly model the transition matrix T\nand the normalizing factor Z(X). To this end, we\nadopt the techniques proposed by Sun et al. (2019)\nto approximate these two terms. Speciﬁcally, the\nfull transition matrix is approximated by the prod-\nuct of two low-rank matrices T = E1ET\n2 , where\nE1,E2 ∈R|V|×d and dis much smaller than |V|.\nTo compute the normalizing factor Z(X), at each\ntime step, instead of searching through all possi-\nble paths, the number of candidates is heuristically\ntruncated to a predeﬁned beam size k. We refer\nreaders to the original paper for further details.\n3.2 Output Length Determination\nIn this section, we describe how to let the model\ndetermine the output sequence length by itself.\nOur basic idea is that we want the model to dy-\nnamically stop generation via emitting a special\n[eos] token. To achieve this, during training,\nwe manually append two consecutive [eos] to-\nkens to the end of the target sequence, as shown\nin the top left part of Figure 2. In this way,\nthe model can learn a deterministic transition be-\nhaviour between two [eos] states, meaning that\nt([eos],[eos]) = maxv∈Vt([eos],v). This\nis because, during training, the model never sees a\ntransition ([eos], v), where v̸= [eos].\nDuring inference, the result ˜Y is acquired as\n˜Y = arg maxY′S(X,Y′), where the CRF scor-\ning function S(X,Y′) in Equation (8) can be de-\ncomposed as:\nS(X,Y′) =\nT∑\ni=1\nΦy′\ni\n(hi) +\nT∑\ni=2\nt(y′\ni−1,y′\ni)\n= Φy′\n1\n(h1)\n  \ninitial state\n+\nT∑\ni=2\n{\nlabel score  \nΦy′\ni\n(hi) +\ntransition score  \nt(y′\ni−1,y′\ni)\n  \nstate transition\n}.\n(9)\nOnce the decoded trajectory enters the [eos]\nstate, the state transition term in S(X,Y′)\nwill be dominated by the transition score term\nt([eos],[eos]). As a result, the model will\nkeep transitioning to [eos] in the remaining steps.\nAn example is provided in the right part of Figure 2,\nfrom which we can see that, at step 5, the decoded\ntrajectory enters the [eos] state and remains at it\nin the rest of the generation process. In this way, our\nmodel can dynamically control the length of output\nsequence by entering the [eos] state during the\ngeneration process. After the entire generation pro-\ncess is completed, the ﬁnal output sequence can be\nobtained by removing all generated [eos] tokens.\n3.3 Ratio-First Decoding\nWe note that the outputs of BERT can be divided\ninto two subsets. The ﬁrst subset ranges from the\nbeginning to the position where the ﬁrst [eos]\nis emitted, and the second subset is the rest. For\nexample, in Figure 2, the ﬁrst subset are those cor-\nresponding to the output sequence “y(1) y(2) y(3)\ny(4) [eos]”. As for the second part, we can see\nthat it has little effect on the ﬁnal output and remov-\ning it should not change the result. This indicates\nthat it sufﬁces to only consider the beginning part\nof BERT outputs for improving the inference speed.\nEspecially, for tasks like summarization where the\ntarget is known to be shorter than the source se-\nquence, we are safe to only use the ﬁrst [α·T]\noutputs of BERT to perform inference. Here T de-\nnotes the source length, α∈(0.0,1.0) is set based\non the data statistics and [·] is the integer rounding\noperation. Formally, given the source sequence X,\nthe ratio-ﬁrst decoding is deﬁned as\n˜Y = arg max\nY′\nF(X,Y′,α),\n= arg max\nY′\n{\n[α·T]∑\ni=1\nΦy′\ni\n(hi) +\n[α·T]∑\ni=2\nt(y′\ni−1,y′\ni)}.\n(10)\nWhen α= 1.0, ratio-ﬁrst degenerates to the stan-\ndard decoding strategy in CRF-based models.\nIt should be noted that, [α·T] only constrains\nthe maximum length of the generated result, and\nthe actual output length (after removing the gener-\nated [eos] tokens) is still decided by the model\nitself. In the experiment section, we demonstrate\nthat ratio-ﬁrst can notably improve the inference\nspeed whilst maintaining the generation quality.\n4 Learning\nDue to the conditional independence approxima-\ntion on output tokens, NAG models often tend to\ngenerate repeated tokens (Wang et al., 2019b). One\nway to alleviate this problem is to introduce im-\nplicit dependencies on the output side. In this work,\nwe propose to use the unlikelihood formulation\nof Welleck et al. (2020) in the context of NAG,\nwhere we deﬁne the set of negative candidate as\nthe surrounding tokens within a predeﬁned context\nwindow c. Formally, given the source sequence\nX and the target sequence Y with length T′, the\nproposed context-aware objective is deﬁned as:\nLCA(Y|X) = −\nT′\n∑\ni=1\n{log pθ(yi|hi; X) + lCA(i)},\nlCA(i) =\nj=i+c∑\nj=i−c,yj ̸=yi\nlog(1.0 −pθ(yj|hi; X)),\n(11)\nwhere hi is the model output state at position i.\nAt position i, the proposed objective maximizes\nthe probability of token yi while minimizing the\nprobabilities of the surrounding tokens. In this way,\nit discourages the model from generating repetitive\ntokens at different time steps.\nThe overall learning objective is then deﬁned as\nLCRF = −log PCRF(Y|X),\nL= LCRF + λ·LCA, (12)\nwhere λcontrols the importance of different loss\nterms and PCRF(Y|X) is described in Equation (8).\n5 Related Work\nNon-Autoregressive generation was ﬁrst intro-\nduced by Gu et al. (2018) to reduce the inference\nlatency in machine translation. Recent works in this\narea have investigated ways to mitigate the trade-\noff between the decoding speed and generation\nquality. Gu et al. (2018) utilized fertility as latent\nvariables for better translation performance. Wang\net al. (2019b) proposed two auxiliary objectives\nfor better modelling the output states and solving\nthe under-translation problem. To better model the\nintermediate alignments between source and target\nsides, Ma et al. (2019) proposed a model based\non the generative ﬂow framework. Ghazvininejad\net al. (2019) proposed to use a masked language\nobjective to train the NAG model. During infer-\nence, starting from a fully masked sequence, the\noutput is generated in an iterative reﬁnement man-\nner. Recently, Sun et al. (2019) proposed to incor-\nporate a conditional random ﬁeld into the decoder\nof a NAG model for better modelling the output-\nside dependencies. Our work is different from prior\nworks in two aspects: (1) we directly utilize a pre-\ntrained language model (BERT) to perform non-\nautoregressive generation; (2) our model can dy-\nnamically generate the output sequence without the\nneed of prespeciﬁed output length.\n6 Experiments\nWe evaluate the proposed model on three typical\ntext generation tasks: (1) text summarization; (2)\nsentence compression and (3) machine translation.\n6.1 Experimental Setup\nWe implement the proposed model with PyTorch\n(Paszke et al., 2017). The BERT model we use is\nthe Huggingface implementation (Wolf et al., 2019)\n(bert-base-uncased). To approximate the transition\nmatrix in the CRF layer, we set the dimension d\nof matrices E1 and E2 as 32. For the normalizing\nfactor Z(X), we set the predeﬁned beam size kas\n256. As for the overall learning objective, we set\nthe window size cas 3 and λas 1.0. In training, we\nuse Adam optimizer (Kingma and Ba, 2015). To\nmeasure the relative speedup, we follow the stan-\ndard setup which runs inference for each individual\nexample separately. The model’s inference speed\nis computed by averaging the results of test cases.\nFor a fair comparison, we measure the inference\nspeed of all models on the same platform.\n6.2 Text Summarization\nText summarization aims to automatically generate\na compact summary that retains the most important\ncontent of the original text document (Nenkova and\nMcKeown, 2012). In this experiment, we use the\nGigawords dataset (Rush et al., 2015) as our bench-\nmark. For evaluation, standard metrics including\nROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L\n(R-L) (Lin, 2004) are reported.\nWe compare our model with several representa-\ntive and the latest NAG models, including NAG-\nNMT (Gu et al., 2018), NAR-REG (Wang et al.,\n2019b) and NAG-CRF (Sun et al., 2019). Follow-\ning previous works, during training, we train a\nlength predictor to predict the output length. Dur-\ning inference, for each NAG baseline, we adopt\nthe length-parallel decoding strategy (LPD-k) (Wei\net al., 2019), that is, generating k results using\nthe top-kpossible output length predictions from\nthe length predictor. The results are then re-ranked\nby a transformer model to get the ﬁnal ouput. In\nthe experiment, we report the results of different\nNAG baselines using LPD- 9 decoding. In addi-\ntion, to better examine the effect of using BERT\nin NAG models, we add a BNAG-CRF baseline\nwhich adopts the same structure of the NAG-CRF\nmodel but using BERT as the encoder. We also\ncompare our model with several strong autoregres-\nsive models, which are Luong-NMT (Luong et al.,\n2015), Pointer-Generator (See et al., 2017), DRGD\n(Li et al., 2017) and Concept Pointer (Wang et al.,\n2019a). To measure the relative inference speedup,\nwe include transformer as a baseline model.\nThe results are shown in Table 1, from which\nwe can see that, by using length-parallel decod-\ning, the performance of all NAG baselines can be\nnotably improved. However, such procedure signif-\nicantly increases the inference latency. In contrast,\nModels R-1 R-2 R-L Speedup\nAutoregressive\nLuong-NMT 33.10 14.45 30.71 -\nPointer-Generator 35.98 15.99 33.33 -\nDRGD 36.25 17.61 33.55 -\nConcept Pointer 36.62 16.40 33.98 -\nTransformer (b= 4) 35.74 16.97 33.43 1.00 ×\nNon-Autoregressive\nNAG-NMT 27.20 8.96 25.58 9.31×\n+LPD-9 29.76 10.03 28.04 5.28 ×\nNAR-REG 28.56 9.79 26.83 8.64 ×\n+LPD-9 31.23 11.14 29.55 4.74 ×\nNAG-CRF 30.29 12.61 28.71 8.07 ×\n+LPD-9 32.91 14.31 31.03 4.32 ×\nBNAG-CRF 32.63 14.32 30.82 6.13 ×\n+LPD-9 34.56 16.10 32.76 3.21 ×\nOurs (α= 0.3) 34.67 16.13 32.81 9.31×\nOurs (α= 1.0) 35.05 16.48 33.28 6.72×\nTable 1: Results on Gigawords dataset, where bin the\ntransformer baseline stands for beam search size.\nour model can self-determine the output length\nwithout any re-ranking process. As shown in the\nresults, our model outperforms the best NAG base-\nline (with LPD) and achieves performances that are\ncomparable with several strong AG models.\nComparing the results of BNAG-CRF and NAG-\nCRF, we can see that incorporating BERT as en-\ncoder helps to improve the model performance.\nNonetheless, our model still outperforms BNAG-\nCRF with LPD-9 decoding. This is because the\ndynamic length decoding mechanism allows our\nmodel to generate results with optimal length, lead-\ning to stronger model performances.\nFinally, we analyze the proposed ratio-ﬁrst de-\ncoding. From the results, we observe a moderate\nperformance drop when using ratio-ﬁrst (α= 0.3).\nIt comes from the fact that, for some input doc-\numents with length T, the reference summary is\nlonger than [α·T]. In such cases, ratio-ﬁrst fails\nto generate the complete reference summary, lead-\ning to the drop of performance. On the other hand,\nwe can see that, ratio-ﬁrst can notably improve\nthe inference speedup. With α = 0.3, our model\nachieves the highest inference speedup while still\noutperforms all compared NAG models.\n6.3 Sentence Compression\nSentence compression aims at compressing a long\nsentence into a short one by deleting redundant\nwords. In this experiment, we use the Google sen-\ntence compression dataset (Filippova and Altun,\n2013) as our benchmark. For evaluation, we use\nModels F1 R-1 R-2 R-L Speedup\nAutoregressive\nBi-LSTM-Dep 82.3 81.5 74.1 81.3 -\nTagger 82.8 81.1 72.4 80.9 -\nTagger+ILP 79.0 76.1 64.6 75.8 -\nHiSAN-Dep 82.7 82.1 74.9 81.9 -\nHiSAN 83.2 82.9 75.8 82.7 -\nTransformer (b= 4) 82.4 82.0 74.6 81.8 1.00 ×\nNon-Autoregressive\nNAG-NMT 72.5 72.1 59.9 71.8 10.71×\n+LPD-9 73.8 73.6 61.0 73.1 6.09 ×\nNAG-REG 73.7 73.1 61.5 73.0 10.00 ×\n+LPD-9 75.6 75.1 63.4 74.9 5.49 ×\nNAG-CRF 75.1 74.4 66.8 74.2 9.41 ×\n+LPD-9 77.3 76.5 69.0 76.3 5.04 ×\nBNAG-CRF 77.1 76.2 68.9 76.0 7.21 ×\n+LPD-9 79.3 78.5 71.7 78.2 3.91 ×\nOurs (α= 0.7) 79.5 79.0 72.1 78.7 10.00 ×\nOurs (α= 1.0) 80.7 80.3 73.6 80.1 8.42×\nTable 2: Results on sentence compression task\nthe standard token-kept-F1 (F1) score. In addition,\nWe also report the results of other standard metrics\nincluding ROUGE-1, ROUGE-2 and ROUGE-L.\nWe compare the proposed model with the same\nNAG baselines as in the previous experiment. We\nalso compare our model with several strong autore-\ngressive models, including Bi-LSTM-Dep (Filip-\npova et al., 2015), Tagger and Tagger+ILP (Wang\net al., 2017), HiSAN-Dep and HiSAN (Kamigaito\net al., 2018). To measure the inference speedup, we\ninclude transformer as a baseline model.\nThe results are presented in Table 2, from which\nwe see that our model outperforms the best reported\nNAG baseline (with LPD) in terms of both the gen-\neration quality and inference speed. Comparing\nwith the strong autoregressive models, our model\ncan achieve competitive performance with a over\n8.42×inference speed up. We also report the re-\nsults of our model using the ratio-ﬁrst decoding\nstrategy. By setting αas 0.7, it achieves a 10.00×\ninference speedup while still outperforming other\ncompared NAG baselines.\n6.4 Machine Translation\nMachine translation aims at translating text from\nthe source language to the target language. In this\ntask, we use the IWSLT14 German-to-English (DE-\nEN) dataset as our benchmark. Following previous\nworks, we use the sequence-level knowledge distil-\nlation (Gu et al., 2018) during training. For evalu-\nation, we report results in BLEU scores (Papineni\net al., 2002). In this experiment, we use the BERT\nmodel in German language.\nWe compare our model with a range of strong\nModels BLEU Speedup( ×)\nAutoregressive\nLSTM-based 28.53 -\nCNN-based 32.84 -\nTransformer (b= 4) 33.31 1.00\nNon-Autoregressive\nENAG-E 24.13 (27.30) 15.08 (7.39)\nENAG-P 25.09 (28.60) 14.48 (7.24)\nNAG-REG 23.89 (28.04) 16.45 (9.05)\nNAG-NMT 23.04 (26.79) 13.92 (7.24)\nNAG-CRF 26.39 (29.21) 11.74 (6.03)\nBNAG-CRF 26.73 (29.67) 9.42 (5.01)\nOurs (α= 0.8) 29.71 13.92\nOurs (α= 1.0) 30.45 11.31\nTable 3: Results on IWSLT14 De-En dataset. The num-\nbers in () are results using length-parallel decoding.\nBERT CRF R-1 R-2 R-L\n✓ ✓ 35.05 16.48 33.28\n× ✓ 32.41 14.19 30.53\n✓ × 32.16 11.33 30.34\n× × 27.02 8.81 25.25\nTable 4: Ablation study on Gigawords dataset.\nNAG models, including NAG-NMT (Gu et al.,\n2018), ENAG-E and ENAG-P (Guo et al., 2019),\nNAG-REG (Wang et al., 2019b), NAG-CRF (Sun\net al., 2019) and BNAG-CRF. For each NAG\nbaseline, we also report the results using LPD-\n9 decoding. In addition, we compare our model\nwith several strong autoregressive models, includ-\ning LSTM-based (Wu et al., 2016), CNN-based\n(Gehring et al., 2017) and transformer model.\nThe results are shown in Table 3, from which\nwe see that our model outperforms the best NAG\nbaseline (with LPD) in terms of both the generation\nquality and inference speedup. Additionally, we\nalso report the results using the ratio-ﬁrst decoding.\nBy setting αas 0.8, the inference speedup can be\nfurther boosted to 13.92×while the generation\nquality is still higher than the best NAG baseline.\n6.5 Further Analysis\nIn this section, we present further discussions and\nempirical analysis of the proposed model.\nBERT & CRF To quantify the importance of\neach component (BERT & CRF) of our model, we\nevaluate the performance on Gigawords dataset by\nremoving each component iteratively.\nThe results are shown in Table 4, from which\nwe can see that by removing any of these compo-\nModels rep- 1 rep-2 rep-3 rep-4 R-L\nw/o CA 6.897 2.640 0.741 0.295 32.89\nOurs 5.786 1.978 0.427 0.106 33.28\nTransformer 4.329 1.348 0.267 0.089 33.43\nTable 5: Evaluation results on n-gram repetitions.\nnents, the overall performance decreases. By re-\nmoving BERT from the model, we observe notable\ndrop across all metrics. This shows that the knowl-\nedge of BERT is an important factor of the model’s\nstrong performance. Comparing with results in Ta-\nble 1, it still outperforms vanilla NAG-CRF and\nperforms comparably with NAG-CRF using LPD\ndecoding, which demonstrates the merit of the pro-\nposed dynamic length decoding mechanism. An-\nother interesting ﬁnding is that, by only removing\nthe CRF layer, the most notable drop is observed on\nthe bigram-level metric (ROUGE-2). This shows\nthat the bigram-level dependencies on the output\nside are mainly captured by the CRF module. In\naddition, by removing both BERT and CRF, all\nmetrics further decrease. This conﬁrms that each\nof these two components positively contributes to\nthe model’s overall performance.\nContext-Aware Objective In this part, we study\nthe effect of the context-aware objective. As de-\nscribed in Equation (11), it aims at alleviating the\nproblem of repetitive generation. To give a quantita-\ntive analysis, we use the measurement of sentence-\nlevel repetition (Welleck et al., 2020) to compute\nthe ratio of duplicate n-grams (rep-n) in the gener-\nated result. This metric is deﬁned as\nrep-n(Y) = 100 ×(1.0 −|unique n-grams(Y)|\n|n-grams(Y)| ).\n(13)\nFor each generated result, rep-nis 0.0 when it has\nno repeating n-grams. The ﬁnal result is computed\nby averaging over the entire evaluation set.\nWe conduct experiments on Gigawords dataset\nto evaluate the n-gram repetitions ranging from\nuni-gram to 4-gram. The results are shown in Table\n5, where w/o CA means the model is trained with-\nout using context-aware objective and R-L denotes\nthe model’s ROUGE-L score. Additionally, we also\nshow the results from transformer model for a di-\nrect comparison. Comparing the two variants of our\nmodel, we see that training with context-aware ob-\njective leads to a 42% drop on rep-3 metric (0.427\nvs 0.741) and a 64% drop on rep-4 metric (0.106\nvs 0.295). The ROUGE-L results also indicate that\nModels Ours Length-Parallel Decoding\n(α= 1.0) LPD- 1 LPD-5 LPD-10\nBLEU 30.45 27.15 29.62 30.37\nSpeedup(×) 11.31 11.84 8.92 6.01\nTable 6: Results comparison on IWSLT14 dataset\nthe reduction in token repetition can effectively\nimprove the model generation quality.\nDynamic Length Determination Next, we ex-\namine the importance of the model’s ability to dy-\nnamically determine the length of the generated\noutput. To this end, we train another model vari-\nant by removing the two [eos] tokens from the\ntarget sequence. In this way, the model is not able\nto self-determine the output length throughout the\ngeneration process. To perform inference, we use\nlength-parallel decoding (LPD) with different num-\nber of length candidates. Formally, for each length\ncandidate l, the model generates the result ˜Y as\n˜Y = arg max\nY′\n{\nl∑\ni=1\nΦy′\ni\n(hi) +\nl∑\ni=2\nt(y′\ni−1,y′\ni)}.\n(14)\nThe ﬁnal result is acquired by re-ranking the gener-\nated results with a transformer model.\nWe conduct experiments on the IWSLT14 DE-\nEN dataset in which we try a different number of\nlength candidates, including top-1, top-5 and top-\n10. The results are shown in Table 6, from which\nwe can see, as the number of length candidates in-\ncreases, the model performance increases as well.\nThe reason is that a larger candidates set is more\nlikely to contain the best-suited length for the gen-\neration model, leading to better performance. How-\never, such decoding procedure inevitably increases\nthe required computation overhead. We can see\nthat, when setting kas 10, the inference speedup\ndecreases from 11.84×to 6.01×. In contrast, our\nproposed model is able to determine the optimal\noutput length by itself. Without any re-ranking pro-\ncess, it outperforms the model with LPD- 10 de-\ncoding and achieves the inference speedup that is\ncomparable with the model using LPD-1 decoding.\nRatio-First Decoding We are also interested in\nthe effect of the ratio-ﬁrst decoding strategy. To\nprovide a quantitative analysis, we perform infer-\nence on the Gigawords dataset using ratio-ﬁrst with\ndifferent α. The experimental results with differ-\nent αare presented in Figure 3. It can be observed\nthat, when αreaches 0.3, the model approximately\nFigure 3: Experiment results on Gigawords dataset us-\ning ratio-ﬁrst decoding with different α.\nFigure 4: The distribution of target/source length ratio\nof the training and test set in Gigawords dataset.\nachieves its optimal performance. At the same time,\na notable improvement can be observed in terms of\nthe inference speedup (6.72×→ 9.31×).\nNow we illustrate why the near optimal perfor-\nmance can be achieved when α reaches 0.3. In\nFigure 4, we present the distribution of the tar-\nget/source length ratio of every data instance in the\nGigawords dataset. We can see that, for most cases,\nthe ratio between the target length T′and source\nlength T is less than 0.3. Recall the deﬁnition of\nratio-ﬁrst decoding in Equation (10), the [α·T]\nconstrains the maximum length of the generated\nresult. Therefore, once we have a prior knowledge\non the data statistic, we can easily choose a proper\nα that both improves the inference speed whilst\nmaintaining the generation quality. In this case, a\nproper αcould be 0.3 which is demonstrated by\nthe results in Figure 3 and 4. By setting different\nα, ratio-ﬁrst provides us an explicit way to control\nthe balance between the inference speed and the\ngeneration quality. This property of ratio-ﬁrst is\nespecially favorable in real-life scenarios where the\ninference speed is the highest concern.\n7 Conclusion\nIn this work, we explored the potential of BERT\nin various text generation tasks under the NAG\nframework. To address problems from NAG mod-\nels previously having a preﬁxed output length, we\ndevised a decoding mechanism which enables the\nmodel to determine the output length dynamically.\nTo reduce errors stemming from the assumption\nof conditional independence of output tokens, we\nproposed a context-aware objective as well as us-\ning a CRF decoding. Furthermore, to maximize the\ninference speed advantage of our model, we intro-\nduced a ratio-ﬁrst decoding strategy. We evaluated\nour model on three benchmark datasets and the\nresults show that our model signiﬁcantly outper-\nforms many strong NAG baselines and performs\ncomparably to many strong AG models.\nAcknowledgments\nThe authors wish to thank Jialu Xu, Guanlin Li,\nXing Wang for their insightful discussions and sup-\nport. Many thanks to our anonymous reviewers for\ntheir suggestions and comments.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186.\nKatja Filippova, Enrique Alfonseca, Carlos A. Col-\nmenares, Lukasz Kaiser, and Oriol Vinyals. 2015.\nSentence compression by deletion with lstms. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2015, Lisbon, Portugal, September 17-21, 2015 ,\npages 360–368.\nKatja Filippova and Yasemin Altun. 2013. Overcom-\ning the lack of parallel data in sentence compres-\nsion. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2013, 18-21 October 2013, Grand Hy-\natt Seattle, Seattle, Washington, USA, A meeting of\nSIGDAT, a Special Interest Group of the ACL, pages\n1481–1491.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning, ICML 2017, Sydney, NSW, Australia, 6-11\nAugust 2017, pages 1243–1252.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel\ndecoding of conditional masked language models.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 6111–\n6120.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O. K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In 6th\nInternational Conference on Learning Representa-\ntions, ICLR 2018, Vancouver, BC, Canada, April 30\n- May 3, 2018, Conference Track Proceedings.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu,\nand Tie-Yan Liu. 2019. Non-autoregressive neu-\nral machine translation with enhanced decoder in-\nput. In The Thirty-Third AAAI Conference on Artiﬁ-\ncial Intelligence, AAAI 2019, The Thirty-First Inno-\nvative Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2019, The Ninth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - Febru-\nary 1, 2019, pages 3723–3730.\nHidetaka Kamigaito, Katsuhiko Hayashi, Tsutomu Hi-\nrao, and Masaaki Nagata. 2018. Higher-order syn-\ntactic attention network for longer sentence compres-\nsion. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2018, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 1 (Long Papers), pages\n1716–1726.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth Inter-\nnational Conference on Machine Learning (ICML\n2001), Williams College, Williamstown, MA, USA,\nJune 28 - July 1, 2001, pages 282–289.\nPiji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017.\nDeep recurrent generative decoder for abstractive\ntext summarization. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2017, Copenhagen, Denmark,\nSeptember 9-11, 2017, pages 2091–2100.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Proc. ACL workshop on\nText Summarization Branches Out, page 10.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2015, Lisbon, Portu-\ngal, September 17-21, 2015, pages 1412–1421.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard H. Hovy. 2019. Flowseq: Non-\nautoregressive conditional sequence generation with\ngenerative ﬂow. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Con-\nference on Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7,\n2019, pages 4281–4291.\nAni Nenkova and Kathleen R. McKeown. 2012. A sur-\nvey of text summarization techniques. In Mining\nText Data, pages 43–76.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA, pages 311–318.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in pytorch.\nIn NIPS-W.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2015, Lisbon, Portugal,\nSeptember 17-21, 2015, pages 379–389.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1073–1083.\nZhiqing Sun, Zhuohan Li, Haoqing Wang, Di He,\nZi Lin, and Zhi-Hong Deng. 2019. Fast structured\ndecoding for sequence models. In Advances in Neu-\nral Information Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 3011–3020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nLiangguo Wang, Jing Jiang, Hai Leong Chieu,\nChen Hui Ong, Dandan Song, and Lejian Liao. 2017.\nCan syntax help? improving an lstm-based sentence\ncompression model for new domains. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2017, Vancou-\nver, Canada, July 30 - August 4, Volume 1: Long Pa-\npers, pages 1385–1393.\nWenbo Wang, Yang Gao, Heyan Huang, and Yuxi-\nang Zhou. 2019a. Concept pointer network for\nabstractive summarization. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 3074–3083.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang\nZhai, and Tie-Yan Liu. 2019b. Non-autoregressive\nmachine translation with auxiliary regularization. In\nThe Thirty-Third AAAI Conference on Artiﬁcial In-\ntelligence, AAAI 2019, The Thirty-First Innovative\nApplications of Artiﬁcial Intelligence Conference,\nIAAI 2019, The Ninth AAAI Symposium on Edu-\ncational Advances in Artiﬁcial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - Febru-\nary 1, 2019, pages 5377–5384.\nBingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang\nLin, and Xu Sun. 2019. Imitation learning for non-\nautoregressive neural machine translation. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 1304–1312.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.9152111411094666
    },
    {
      "name": "Computer science",
      "score": 0.7517127990722656
    },
    {
      "name": "Automatic summarization",
      "score": 0.742840051651001
    },
    {
      "name": "Conditional independence",
      "score": 0.583615243434906
    },
    {
      "name": "Inference",
      "score": 0.5272851586341858
    },
    {
      "name": "Language model",
      "score": 0.5017495155334473
    },
    {
      "name": "Machine translation",
      "score": 0.5006613731384277
    },
    {
      "name": "Security token",
      "score": 0.49455568194389343
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45756345987319946
    },
    {
      "name": "STAR model",
      "score": 0.45146799087524414
    },
    {
      "name": "Decoding methods",
      "score": 0.43194496631622314
    },
    {
      "name": "Independence (probability theory)",
      "score": 0.42013904452323914
    },
    {
      "name": "Speech recognition",
      "score": 0.35458430647850037
    },
    {
      "name": "Machine learning",
      "score": 0.3258657157421112
    },
    {
      "name": "Algorithm",
      "score": 0.3142247796058655
    },
    {
      "name": "Econometrics",
      "score": 0.24503737688064575
    },
    {
      "name": "Time series",
      "score": 0.2152199149131775
    },
    {
      "name": "Statistics",
      "score": 0.17185211181640625
    },
    {
      "name": "Mathematics",
      "score": 0.1405215859413147
    },
    {
      "name": "Autoregressive integrated moving average",
      "score": 0.11821147799491882
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ],
  "cited_by": 10
}