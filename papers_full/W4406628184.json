{
    "title": "Large language models for data extraction from unstructured and semi-structured electronic health records: a multiple model performance evaluation",
    "url": "https://openalex.org/W4406628184",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2584010616",
            "name": "Vasileios Ntinopoulos",
            "affiliations": [
                "Triemli Hospital",
                "University Hospital of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A76783178",
            "name": "Hector Rodriguez Cetina Biefer",
            "affiliations": [
                "University Hospital of Zurich",
                "Triemli Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2619703182",
            "name": "Igor Tudorache",
            "affiliations": [
                "Triemli Hospital",
                "University Hospital of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2117359010",
            "name": "Nestoras Papadopoulos",
            "affiliations": [
                "Triemli Hospital",
                "University Hospital of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2773507945",
            "name": "Dragan Odavic",
            "affiliations": [
                "University Hospital of Zurich",
                "Triemli Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A732818030",
            "name": "Petar Risteski",
            "affiliations": [
                "Triemli Hospital",
                "University Hospital of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2530001983",
            "name": "Achim Haeussler",
            "affiliations": [
                "Triemli Hospital",
                "University Hospital of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A331187783",
            "name": "Omer Dzemali",
            "affiliations": [
                "University Hospital of Zurich",
                "Triemli Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2515682654",
        "https://openalex.org/W2783114168",
        "https://openalex.org/W3211024994",
        "https://openalex.org/W4401700474",
        "https://openalex.org/W4386958277",
        "https://openalex.org/W4365211688",
        "https://openalex.org/W4386836900",
        "https://openalex.org/W6839218079",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4399854538",
        "https://openalex.org/W2905367128",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4391876619",
        "https://openalex.org/W4394778349",
        "https://openalex.org/W2328768668",
        "https://openalex.org/W3039053697",
        "https://openalex.org/W2770117783",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4283703423"
    ],
    "abstract": "Objectives We aimed to evaluate the performance of multiple large language models (LLMs) in data extraction from unstructured and semi-structured electronic health records. Methods 50 synthetic medical notes in English, containing a structured and an unstructured part, were drafted and evaluated by domain experts, and subsequently used for LLM-prompting. 18 LLMs were evaluated against a baseline transformer-based model. Performance assessment comprised four entity extraction and five binary classification tasks with a total of 450 predictions for each LLM. LLM-response consistency assessment was performed over three same-prompt iterations. Results Claude 3.0 Opus, Claude 3.0 Sonnet, Claude 2.0, GPT 4, Claude 2.1, Gemini Advanced, PaLM 2 chat-bison and Llama 3-70b exhibited an excellent overall accuracy &gt;0.98 (0.995, 0.988, 0.988, 0.988, 0.986, 0.982, 0.982, and 0.982, respectively), significantly higher than the baseline RoBERTa model (0.742). Claude 2.0, Claude 2.1, Claude 3.0 Opus, PaLM 2 chat-bison, GPT 4, Claude 3.0 Sonnet and Llama 3-70b showed a marginally higher and Gemini Advanced a marginally lower multiple-run consistency than the baseline model RoBERTa (Krippendorffâ€™s alpha value 1, 0.998, 0.996, 0.996, 0.992, 0.991, 0.989, 0.988, and 0.985, respectively). Discussion Claude 3.0 Opus, Claude 3.0 Sonnet, Claude 2.0, GPT 4, Claude 2.1, Gemini Advanced, PaLM 2 chat bison and Llama 3-70b performed the best, exhibiting outstanding performance in both entity extraction and binary classification, with highly consistent responses over multiple same-prompt iterations. Their use could leverage data for research and unburden healthcare professionals. Real-data analyses are warranted to confirm their performance in a real-world setting. Conclusion Claude 3.0 Opus, Claude 3.0 Sonnet, Claude 2.0, GPT 4, Claude 2.1, Gemini Advanced, PaLM 2 chat-bison and Llama 3-70b seem to be able to reliably extract data from unstructured and semi-structured electronic health records. Further analyses using real data are warranted to confirm their performance in a real-world setting.",
    "full_text": null
}