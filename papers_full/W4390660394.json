{
  "title": "A Novel Transformer Model With Multiple Instance Learning for Diabetic Retinopathy Classification",
  "url": "https://openalex.org/W4390660394",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2054230213",
      "name": "Yaoming Yang",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A2164067188",
      "name": "Zhili Cai",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A2137253636",
      "name": "Shuxia Qiu",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A1982578470",
      "name": "Peng Xu",
      "affiliations": [
        "China Jiliang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2791011305",
    "https://openalex.org/W3048958604",
    "https://openalex.org/W2965191493",
    "https://openalex.org/W2899170201",
    "https://openalex.org/W4362603432",
    "https://openalex.org/W4200585179",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6787972765",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3165183789",
    "https://openalex.org/W4308279659",
    "https://openalex.org/W6799920274",
    "https://openalex.org/W3167044851",
    "https://openalex.org/W3204146347",
    "https://openalex.org/W3202476840",
    "https://openalex.org/W4280578375",
    "https://openalex.org/W2110119381",
    "https://openalex.org/W2053784471",
    "https://openalex.org/W2010792435",
    "https://openalex.org/W3201905155",
    "https://openalex.org/W1969496006",
    "https://openalex.org/W2557738935",
    "https://openalex.org/W1966206262",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3159265489",
    "https://openalex.org/W6847512970",
    "https://openalex.org/W4309778771",
    "https://openalex.org/W4285160408",
    "https://openalex.org/W4386494539",
    "https://openalex.org/W4385521858",
    "https://openalex.org/W4315796330",
    "https://openalex.org/W2979377551",
    "https://openalex.org/W2626629100",
    "https://openalex.org/W2983395335",
    "https://openalex.org/W2889859380",
    "https://openalex.org/W3126755857",
    "https://openalex.org/W4311550933",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3193873731"
  ],
  "abstract": "Diabetic retinopathy (DR) is an irreversible fundus retinopathy. A deep learning-based automated DR diagnosis system can save diagnostic time. While Transformer has shown superior performance compared to Convolutional Neural Network (CNN), it typically requires pre-training with large amounts of data. Although Transformer-based DR diagnosis method may alleviate the problem of limited performance on small-scale retinal datasets by loading pre-trained weights, the size of input images is restricted to <inline-formula> <tex-math notation=\"LaTeX\">$224\\times 224$ </tex-math></inline-formula>. The resolution of retinal images captured by fundus cameras is much higher than <inline-formula> <tex-math notation=\"LaTeX\">$224\\times 224$ </tex-math></inline-formula>, reducing resolution in training will result in the loss of valuable information. In order to efficiently utilize high-resolution retinal images, a new Transformer model with multiple instance learning (TMIL) is proposed for DR classification. A multiple instance learning approach is firstly applied on the retinal images to segment these high-resolution images into <inline-formula> <tex-math notation=\"LaTeX\">$224\\times 224$ </tex-math></inline-formula> image patches. Subsequently, Vision Transformer (ViT) is used to extract features from each patch. Then, Global Instance Computing Block (GICB) is designed to calculate the inter-instance features. After introducing global information from GICB, the features are used to output the classification results. When using high-resolution retinal images, TMIL can load pre-trained weights of Transformer without being affected by weight interpolation on model performance. Experimental results using the APTOS dataset and the Messidor-1 dataset demonstrate that TMIL achieves better classification performance and reduces inference time by 62&#x0025; compared with that directly inputting high-resolution images into ViT. And TMIL shows highest classification accuracy compared with the current state-of-the-art results. The code will publicly available at <uri>https://github.com/CNMaxYang/TMIL</uri>.",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nA Novel Transformer Model with Multiple \nInstance Learning for Diabetic Retinopathy \nClassification \nYaoming Yang1, Zhili Cai1, Shuxia Qiu1,2, and Peng Xu1,2 \n1College of Science, China Jiliang University, Hangzhou 310018, China  \n2Key Laboratory of Intelligent Manufacturing Quality Big Data Tracing and Analysis of Zhejiang Province, Hangzhou  310018, P.R. China  \nCorresponding author: Peng Xu (e-mail: xupeng@cjlu.edu.cn). \n‚ÄúThis work was supported by the National Natural Science Foundation of China [grant numbers 51876196].‚Äù  \nABSTRACT Diabetic retinopathy (DR) is an irreversible fundus retinopathy. A deep learning -based \nautomated DR diagnosis sys tem can save  diagnostic time. While Transformer has shown superior \nperformance compared to Convolutional Neural Network (CNN), it typically requires pre-training with large \namounts of data. Altho ugh Transformer-based DR diagnosis method may alleviate the problem of limited \nperformance on small -scale retinal datasets by loading pre -trained weights, the size of input images is \nrestricted to 224√ó 224. The resolution of retinal images captured by fundu s cameras is much higher than \n224√ó 224, reducing resolution in training will result in the loss of valuable information. In order to efficiently \nutilize high-resolution retinal images, a new Transformer model with multiple instance learning (TMIL) is \nproposed for DR classification. A multiple instance learning approach is firstly applied on the retinal images \nto segment these high -resolution images into 224√ó 224 image patches. Subsequently, Vision Transformer \n(ViT) is used to extract features from each patch. Then, Global Instance Computing Block (GICB) is designed \nto calculate the inter-instance features. After introducing global information from GICB, the features are used \nto output the classification results. When using high-resolution retinal images, TMIL can load pre -trained \nweights of Transformer without being affected by weight interpolation on model performance. Experimental \nresults using the APTOS dataset and the Messidor -1 dataset demonstrate that TMIL achieves better \nclassification performance and re duces inference time by 62% compared with that directly inputting high -\nresolution images into ViT. And TMIL shows highest classification accuracy compared with the current state-\nof-the-art results. The code will publicly available at https://github.com/CNMaxYang/TMIL. \nINDEX TERMS Vision Transformer, Multiple instance learning, Diabetic retinopathy, High -resolution \nfundus retinal images, Medical image classification \nI. INTRODUCTION \nDiabetic retinopathy (DR) is a retinal vasculopathy caused by \ndiabetes, has led to a great number of blindness cases among \nthe working-age population worldwide [1]. Since the damage \nto vision caused by DR is irreversible, early detection and \ntreatment are crucial for preserving  DR patients' eyesight. \nWith the rapid development of CNN, it has been successfully \napplied for the detection and classification of DR [2-4].  \nRecently, Transformer has shown superior performance to \nCNN on natural images [5], and therefore has been introduced \nin the field of medical image [6]. Kumar et al. [7] tested CNN, \nTransformer and MLP on the APTOS dataset, and they found \nthat the DR classification performance of Transformer is  \nbetter than that of CNN. While the cost of acquiring and \nannotating medical images is high, and the dataset is limited. \nThus, the classification performance of ViT may be weaker \nthan that of CNN due to the lack of inductive biases [5]. \nAlthough transfer learning can alleviate this issue, in order to \navoid missing pre-trained weights, the resolution of medical \nimages needs to be adjusted to match the resolution of the pre-\ntrained images (e.g., 224√ó 224). However, medical images \ntypically have a higher resolution than the resolution of natural \nimages used for pre-training [8-10]. For DR, ophthalmologists \ncan identify lesion features such as hemorrhages, hard \nexudates, microaneurysms, and more in high-definition retinal \nimages, as depicted in Figure 1. However, some lesion  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nFIGURE 1. A schematic diagram of DR lesion features, sourced from  \n[11]. \nfeatures are extremely small within the image, as seen in \nFigure 1, where the microaneurysm lesion feature occupies \njust a few pixels, making it easy to overlook. Further image \ncompression in such instances is likely to result in the loss of \nthese features, which can have an adverse impact on the \nmodel's classification capabilities. Moreover, the information \nloss caused by compressing medical images is not conducive \nto alleviating the problem of limited training data, and it also \nhampers fine-tuning the model on medical image datasets. \nFurthermore, inputting high -resolution medical images into \nViT will lead to a larger sequence length, which significantly \nincreases computational time for the self-attention mechanism. \nIn order to use higher resolu tion images while avoiding a \nsignificant increase in network inference time, a new \nTransformer-based multiple instance model (TMIL) is \nproposed for DR classification in this paper. As illustrated in \nFigure 2, a single retinal image is  treated as a \"bag\" and the \nimage is uniformly divided into non -overlapping image \npatches (instances). Then, ViT -Small is used to extract \nfeatures from each instance, followed by adding positional \nembeddings. The GICB is proposed to aggregate features from \ndifferent instances and obtain global features, which are then \nused to generate the final prediction for the bag. The main \ncontributions of this study are listed as follows: \n‚ö´ A simple and efficient multiple instance model along \nwith a concise GICB is developed for the high-resolution \nretinal fundus images. \n‚ö´ The model achieves an average accuracy of 85.6%, an \naverage Area Under Curve (AUC) of 95.6%, an average \nKappa score of 91.5%, and an average sensitivity of 73.7% \nfor the five -level classification of DR on  the APTOS \ndataset. On the Messidor-1 dataset, the model achieves \nan average accuracy of 93.1%, an average AUC of \n97.4%, and an average sensitivity of 86.9% for the \nreferable DR classification. The result outperforms that \ndirectly inputting high-resolution images into ViT, and \nthe accuracy surpasses the state-of-the-art results.  \n‚ö´ Compared to the original ViT, the proposed model not \nonly achieves better performance but also significantly \nreduces inference time by approximately 62%. \n \n \n \n \n \nFIGURE 2. The architecture of proposed TMIL for DR classification.  The model initially uniformly divides the image into patches (instances) and then \nemploys the weight-shared ViT-Small to extract features for each instance. After fea ture extraction, these features are input into the GICB to \ncomplement global features, and finally, a MLP is utilized to output the classification results.  \n \n...\n nput image\n896 896  \n iT  ma  \n eatures\n  oba  \n nstan e \n omputing \n  o k\n16  8 \n  oba  \n nstan e \n omputing \n  o k\n      r  \n osition \n mbedding\n ayer orm  ttention    \n iT  ma   iT  ma   iT  ma  ...\n eight sharing\n at hes  nstan es \n16          \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nII. RELATED WORK \nA. TRANSFORMER IN DIABETIC RETINOPATHY \nDOMAIN \nMohan et al. [12] tested networks such as ViT, ResNet101, \nand Inceptionv3 on IDRiD and EyePACS datasets. They \nfound that ViT outperformed CNN like ResNet101 and \nInceptionv3 i n terms of classification performance. \nMeanwhile, Matsoukas et al.  [13] tested the classification \nperformance of ResNet50 and DeiT-S on APTOS, ISIC, and \nCBIS-DDSM. They found that ViT pre-trained on ImageNet \nperforms comparably to CNN with limited data. However, \nViT outperforms CNN by further pre-training ViT using self-\nsupervised methods based upon ImageNet pre -training. The \nDR grading and lesion detection are generally treated as \nseparate tasks, whic h restricts model performance and \npractical deployment. Sun et al. [14] proposed a lesion-aware \ntransformer (LAT) model that can simultaneously perform \nlesion detection and DR prediction on retinal fundus images. \nIt has been shown to achieve better performance than the state-\nof-the-art methods for DR grading and lesion detection. \nB. TRANSFORMER WITH MULTIPLE INSTANCE \nLEARNING IN MEDICAL IMAGE \nYu et al. [15] proposed a multiple instance model called MIL-\nVT for the classification of DR. The model builds upon ViT-\n ma   and in orporates a designed ‚Äú    head‚Äù, and one of the \nclassification result was obtained from the Class Token of \nViT-Small. Each of the rest token, except for the Class Token, \nwas treated as an instance and input to the MIL head. A bag \nprediction was finally output by the MIL head. Myronenko et \nal. [16] applied the multiple instance learning for the \nclassification of histology whole slide images (WSI). They \nembedded a Transformer block to capture the dependencies \namong instances because the tumor grading may depend on \nspecific patterns in different locations within the WSI. They \nalso introduced an instance -wise loss function based on \ninstance pseudo-labels and achieved state-of-the-art results on \nthe PANDA challenge dataset. Qian et al. [17] presented a \nmodel named Swin -MIL based on the Swin Transformer. \nThey also recognized that the independence among instances \nin MIL restricts the model's p erformance due to the lack of \ninter-instance correlation. Therefore, they introduced the \nTransformer into MIL to capture global dependencies and got \nstate-of-the-art results in weakly supervised segmentation \nmethods on a colon cancer dataset. \nIII. METHODOLOGY \nA. MULTIPLE INSTANCE LEARNING \nMultiple instance learning was first introduced by Dietterich \net al. in 1997 [18] to predict the activity of drug molecules. \nThey took the molecule conformation as an instance and all \nthe conformations of the same molecule as a bag. If any \ninstance in the bag had a class label of 1, the bag was labeled \nas 1. If all instances in a bag had a class label of 0, the bag was \nlabeled as 0. The ultimate goal of multiple instance learning is \nto train a multiple instance model that can classify bags. This \napproach is currently known as the Bag-Space (BS) paradigm \n[19]. In addition to the BS paradigm, there are also the \nInstance-Space (IS) paradigm and the Embedded-Space (ES) \nparadigm [20]. In the IS paradigm, instance predictions are \nobtained first, followed by bag predictions. While, in the ES \nparadigm, all instances are embedded into a low-dimensional \nrepresentation, and bag predictions are generated based on \nthese low-dimensional representations [21]. In this work, the \nES paradigm is used to construct the multiple instance model \nin order to leverage the global information of bags as much as \npossible. \nB. VISION TRANSFORMER \nThe ViT is a type of Transformer model applied in the field of \ncomputer vision. Dosovitskiy et al. [5] modified the original \nTransformer architecture by removing the decoder and fine -\ntuning the model details to make it suitable for natural images. \nThey named this model as ViT and achieved state -of-the-art \nclassification results on the ImageNet dataset, which \noutperforms CNN-based approaches. \nThe present Transformer model is based on ViT-Small [5], \nand the architecture of ViT -Small is illustrated in Figure 3. \nThe original structure of this model consists of 12 Transformer \nblocks with an embedding vector dimension of 384. The patch \nsize is 16√ó 16√ó3, and the input image size is 224√ó 224√ó 3. The \nmodel divides the input image into non -overlapping image \npatches and encodes them into 384-dimensional vectors. After \nencoding, a learnable position embedding vector of the same \ndimension is added to these vectors. Then these vectors are \ninput to the T ransformer blocks for further encoding. Each \nTransformer block consists of 2 LayerNorm layers, 1 multi -\nhead attention module, and an MLP with two fully connected \nlayers. Finally, a simple fully connected layer is used to output \nthe classification results.  \nIn the proposed TMIL approach, the fully connected layer \nused for the final classification in ViT-Small is removed, and \nthe remaining structure is retained for extracting instance \nfeatures. \nC. GLOBAL INSTANCE COMPUTING BLOCK \nThe instances are obtained throug h segmentation on the \noriginal image, which may result in instances with mild lesion \nfeatures or without any lesions being present in the bag with \nsevere lesion. In order to avoid the influence of specific \ninstances and utilize the global information of instances within \na bag, a relatively simple GICB is designed here to perform \ncomputations on the features extracted from different \ninstances. As shown in Figure 2, this module consists of a \nLayerNorm layer, a multi-head self-attention mechanism, and \na single -layer fully connected layer. The multi -head self -\nattention mechanism is used to compute the information \namong features extracted from different instances. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n \n \nFIGURE 3. The architecture of ViT-Small. \n \nAnd the global information is introduced accordingly. \nSubsequently, a non -linear transformation is introduced \nthrough the fully connected layer. The multi -head self -\nattention mechanism can be calculated by: \nAttention(ùëÑ, ùêæ, ùëâ) = softmax (ùëÑùêæùëá\n‚àöùëëùëò\n) ùëâ (1) \nMultiHead(ùëÑ, ùêæ, ùëâ) =\nùê∂ùëúùëõùëêùëéùë°(head1, head2, ‚Ä¶ , headh)ùëäùëÇ (2) \nEquation (1) represents a standard ‚Äú  a ed  ot -Product \n ttention‚Äù, where Q, K and V are Query Key and Value, \nrespectively. These three components are derived from the \noutput of the previous layer, hence referred to as self-attention. \nIn (2), h represents the number of heads in the multi -head \nattention mechanism. In the ViT -Small model used in this \nstudy, the value of h is set to be 6. The value of a single head \nis obtained by (1). Once the calculations for the 6 heads are \ncompleted, the results from each head are concatenated and \nmapped to the vector dimension set by the model using the WO \nmapping. \nD. MODEL ARCHITECTURE \nFirstly, the original image is divided into 16 non-overlapping \nimage patches. Then, ViT -Small is used to extract features \nfrom these 16 patches individually. After the extraction, the \n‚Äú  assifi ation token‚Äù from ea h of these 16 features are \nconcatenated t ogether to form a new feature ùëãùêπùëíùëéùë°ùë¢ùëüùëíùë† ‚àà\n‚Ñù16√ó384 . Next, a learnable position embedding ùê∏ùëùùëúùë† ‚àà\n‚Ñù16√ó384 is added to this feature as the input to the GICB, as \nshown in (3). The classification results can be obtained by \n...\n nput image\n         \n at hes\n196 16 16  \n inear  ro e tion of   attened  at hes\n196... 195 1 \nTransformer   o k\nTransformer   o k\n... epth 1 \n orm\n orm\n u ti  ead\n ttention\n   \n     ass\n at h    osition  mbedding\ndi ision\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \npassing this input through t wo G ICBs and a simple fully \nconnected layer. The two GICBs are determined by (4)-(7). \nùëã = ùëãùêπùëíùëéùë°ùë¢ùëüùëíùë† + ùê∏ùëùùëúùë†    ùëã ‚àà ‚Ñù16√ó384(3) \nùê¥1 = MSA1(LN(ùëã)) + ùëã     ùê¥1 ‚àà ‚Ñù16√ó384(4) \nùëÄ1 = MLP1(ùê¥1)      ùëÄ1 ‚àà ‚Ñù16√ó128(5) \nùê¥2 = MSA2(LN(ùëÄ1)) + ùëÄ1  ùê¥2 ‚àà ‚Ñù16√ó128(6) \nùëÄ2 = MLP2(ùê¥2)          ùëÄ2 ‚àà ‚Ñù16√ó1(7) \nwhere the dimension of MSA1  is 384, and it has 6 heads. \nMSA2 has a dimension of 128 and 4 heads. \nIV. EXPERIMENTS AND RESULTS \nA. DATASETS AND EXPERIMENTAL SETUP \nIn this study, the APTOS [22] and Messidor-1 [23] datasets \nwere used to test the proposed model. The APTOS consists of \n5,590 color fundus retinal images. However, only 3,662 \nimages have corresponding DR lesion grades, and were \ndivided into five classes. Thus, only these labeled 3,662 color \nfundus retinal images in APTOS were used for training and \ntesting the model in order to implement five -class \nclassification of DR. The Messidor -1 contains 1,200 color \nfundus retinal images, which were specified as four classes [24, \n25]. Due to the limited number of images in the Messidor -1 \nand the insufficient samples of certain class, the samples with \nannotations of grade 0 and 1 were taken as normal samples, \nwhile grade 2 and grade 3 were considered as referable DR. \nThat is binary classification was performed on Messidor-1. \nIn the APTOS, 10% of the fundus retinal images were \nrandomly selected as the test set. 80% of the remaining images \nwere randomly chosen as the training set, and 20% were used \nas the validation set. All images were cropped to remove black \nregions and uniformly resized to 896√ó 896 pixels. Image \naugmentation included random horizontal flips and random \nrotations within the range of -180 degrees to +180 degrees. \nThe optimizer was AdamW with a weight decay of 0.3. The \nmaximum learning rate was set to 1√ó 10-4, with a learning rate \nwarm-up for the first 10 epochs. The learning rate was reduced \nby a factor of 0.5 at epochs 20, 35, and 60. The maximum \nnumber of epochs was set to be 110. The loss function was the \ncross-entropy loss with label smoothing of 0.05 [26]. For the \nMessidor-1, the data set partitioning and hyperparameter \nsetting were similar to that with the APTOS. The binary cross-\nentropy loss function was used for DR prediction, and the \nmaximum learning rate was set to be 1√ó 10-5. \nAll experiments in this s tudy were conducted on an \nNVIDIA RTX 3090 GPU with 24GB of VRAM. The model \nwith the highest accuracy on the validation set was saved \nduring training and tested on the test set. The main evaluation \nmetrics include accuracy, AUC, Kappa score and sensitivity. \nTo avoid the randomness during model training, all \nexperiments were repeated five times and the results were \naveraged. Additionally, all models were set to evaluation \nmode with autograd computation disabled and the batch size \nwas uniformly set to be 16 in comparing the inference time of \nthe models. \nB. ABLATION EXPERIMENT \nThe experimental results on APTOS dataset are presented in \nTable 1. If the images are firstly compressed to 224√ó 224 and \nthen inputted into ViT, the ACC, AUC, Kappa and sensitivity \nof ViT  (224) are 83.5%, 94.6%, 88.5% and 67.3%, \nrespectively. When the high-resolution images were directly \ninput into ViT for classification, the ACC, AUC, Kappa and \nsensitivity of ViT (896) are 83.9%, 94.6%, 90.3% and 68.3%, \nrespectively. All the performance evaluation metrics of ViT  \n(869) has been improved compared that of ViT  (224). The \nhigh-resolution images with more information are conducive \nto mitigate overfitting, which takes a positive impact on the \nmode ‚Äôs performan e.  owe er,  9   position embedding \nweights were missing in directly inputting the 896√ó 896 \nimages into the pre-trained ViT because the sequence lengths \nof 896√ó 896 and 224√ó 224 images are respectively 3136 and \n196. Although missing weights can be obtained through \ninterpolation, when there are too many missing weights, the \ninterpolated weights may have a negative impa ct on the \nmodel's classification performance. The present results \nindicate that the positive impact of high-resolution images is \nlarger than the negative influence of weight interpolation. \nIt can be also found that the multiple instance learning \napproach without GICB and with a simple fully connected \n ayer for   assifi ation  T    1  doesn‚Äôt impro e the mode ‚Äôs \nprediction accuracy and only slightly improves the AUC. \nHowever, the multiple instance learning approach with GICB \n(TMILv2) significantly improves t he classification \nperformance. The accuracy, Kappa score and sensitivity of \nTMILv2 are 85.3%, 91.5% and 73.2%, respectively. In \nTMILv3, the positional relationship between instances was  \n \nTABLE 1. THE RESULT OF ABLATION STUDY ON APTOS DATASET (MEAN ¬± STANDARD DEVIATION, UNIT: %) \n ode   mpro ement      assifi ation \n      osition  mbedding  esidua   onne tion      U  Kappa  en \n iT             8 .5¬± .8 9 .6¬± .  88.5¬±1.6 67. ¬± .  \n iT  896        8 .9¬±1.6 9 .6¬±1.6 9 . ¬±1.  68. ¬± .5 \nT    1       8 . ¬± .9 95.7¬± .5 9 . ¬±1.  67.5¬±1.5 \nT      ‚àö     85. ¬± .5 9 .9¬± .8 91.5¬±0.7 7 . ¬±1.6 \nT      ‚àö ‚àö   8 .5¬±1.  96.0¬±0.9 91.5¬±0.8 7 .1¬± .1 \nT      ‚àö ‚àö ‚àö 85.6¬±1.4 95.6¬± .7 9 .7¬± .6 73.7¬±1.6 \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nTABLE 2. THE RESULT OF ABLATION STUDY ON MESSIDOR-1 DATASET (MEAN ¬± STANDARD DEVIATION, UNIT: %). \n ode   mpro ement r     assifi ation \n      osition  mbedding  esidua   onne tion      U   en \n iT             81¬±1.1 88.5¬±1.5 76.7¬± .  \n iT  896        87.5¬± .6 9 .8¬±1.  78.1¬± .6 \nT    1       91. ¬± .  95.9¬± .6 8 .7¬±5.  \nT      ‚àö     9 . ¬± .  97.4¬±0.5 8 .1¬± .  \nT       ‚àö ‚àö   93.1¬±2.2 97¬± .  86.9¬±4.0 \nT       ‚àö ‚àö ‚àö 93.1¬±0.9 97. ¬± .  86.9¬±2.3 \n \ntaken into account and the learnable position embeddings were \nintroduced after extracting features with ViT to facilitate the \n    ‚Äôs uti ization of positiona  information. This \nimprovement allows the model to maintain a Kappa score of \n91.5% while further increases the AUC to 96.0%. In TMILv4, \nthe residual connections were introduced in GICB, which \nfurther improves the classification performance. It can be seen \nthat the accuracy, AUC, Kappa score and sensitivity of \nTMILv4 reach up to 85.6%, 95.6%, 90.7% and  73.7%, \nrespectively. A brief analysis on the DR classification at \nvarious levels for both TMILv4 and ViT was also conducted. \nThe confusion matrices for ViT (224), ViT (896) and TMILv4 \non APTOS dataset are summarized in Table 3. Compared with \nViT, TMILv4 indicates a slight decrease in sensitivity for DR \nlevel 2, while shows improvement in sensitivity for DR levels \n1, 3 and 4. \nTABLE 3. CONFUSION MATRIX ON APTOS. \n \n redi ted  abe  \n   1       \nTrue  abe    177   1     98.  % 1.67% \n1   19 1      51. 5%  8.65% \n  1 11 8      8 .8 % 16.16% \n      7 11 1 57.89%   .11% \n      1    1   8. 8% 51.7 % \n 97. 5% 55.88% 7 .17% 68.75% 8 . 5%   .75%   .1 %  7.8 %  1. 5% 17.65% \n iT       \n \n \n redi ted  abe  \n   1       \nTrue  abe    18          1  %  % \n1   16 17       .  % 56.76% \n  1 8 8  6 1 8 .8 % 16.16% \n      5 1  1 68.  %  1.58% \n    1 1    1    .8 % 55.17% \n 97. % 6 % 7 .9 % 59. 9% 86.67%   .7%  6%  9. 6%   .91% 1 .  % \n iT  896  \n \n  redi ted  abe     1       \nTrue  abe    178         98.89% 1.11% \n1      1      6 .16%  7.8 % \n    9 79 6 5 79.8%   . % \n        1    7 .68%  6.  % \n      8   18 6 . 7%  7.9 % \n 98.89% 67.65% 79% 6 .87% 66.67%  1.11%   . 5%  1%  9.1 %   .  % \nT      \n \n ubsequent y, the mode ‚Äôs binary   assifi ation \nperformance was tested on the Messidor -1 dataset, and the \nresults are summarized in Table 2. It is consistent with that on \nAPTOS dataset that inputting higher-resolution retinal images \ninto ViT improves the classification performance. Compared \nwith the binary classification of ViT (224), the accuracy of \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nViT (896) is increased from 81% to 87.5%, and AUC and \nsensitivity are enhanced to 93.8% and 78.1%, respectively. \nUsing the multipl e instance learning approach without the \nGICB and a single -layer fully connected layer for \nclassification (TMILv1) based on Messidor-1 dataset further \nincreases the accuracy, AUC and sensitivity to 91.2%, 95.9% \nand 83.7% respectively. While, the accuracy and AUC can be \nfurther increased to 92.2% and 97.4% by introducing GICB \nmodule in the multiple instance learning approach (TMILv2). \nAlthough the introduction of position embeddings (TMILv3) \ndoesn‚Äôt resu t in an impro ement in  U , the a  ura y and \nsensitivity can be increased to 93.1% and 86.9%, respectively. \nOnly AUC is slightly increased to 97.2% by introducing \nresidual connections in the GICB (TMILv4).  The confusion \nmatrices for ViT  (224), ViT  (896) and TMILv4 on \nMESSIDOR dataset are presented in Table 4. It suggests that \nutilizing high-resolution images can enhance classification \nperformance. And TMIL demonstrates further improvement \nin classification performance by using high-resolution images. \nTABLE 4. CONFUSION MATRIX ON MESSIDOR. \nTrue  abe    redi ted  abe     1 \n  57 1  8 .61% 17. 9% \n1 11  8 77.55%   . 5% \n 8 .8 % 76%  16.18%   % \n iT       \n \nTrue  abe    redi ted  abe     1 \n  67   97.1%  .9% \n1 1   7 75.51%   . 9% \n 8 .81% 9 .87%  15.19% 5.1 % \n iT  896  \n \nTrue  abe    redi ted  abe     1 \n  67   97.1%  .9% \n1 6    87.76% 1 .  % \n 91.78% 95.56%  8.  %  .  % \nT      \nC. COMPARISON TO STATE-OF-THE-ART METHODS \nThe DR classification result of TMILv4 on APTOS dataset \nwas compared with that by state-of-the-art methods. Vives-\nBoix et al. [27] assessed the capability of CNN models for DR \nclassification on APTOS dataset. They showed that ResNet50 \nachieves an accuracy of 71.4% and sensitivity of 62.3%, while \nVGG-16 demonstrates an accuracy of 72.7% and sensitivity \nof 65.9%. And they improved ResNet50 and VGG16, and \nproposed AmResNet50 and AmVGG16 by enhancing the \nweight updating mechanism during the backpropagation \nprocess of CNN. The accuracy and sensitivity of AmResNet50 \nare 72.7% and 65.4%, while the accuracy and sensitivity of \nAmVGG16 are 78.6% and 72%, respectively. Although the \nsensitivity of AmVGG16 is close to tha t of TMIL, the \naccuracy of AmVGG16 is still much lower than that of TMIL. \nMao et al. [28] modified the decoder structure of MAE and \nintroduced a contrastive learning task to propose S DMAE. \nThey achieved an accuracy of 83.1% on the APTOS dataset. \nYue et al. [29] introduced ADCNet, which utilizes attention \nmechanisms to capture a significant amount of lesion \nperception information. They subsequently employ an \nattention-driven aggregation strategy to acquire a large \nnumber of DR -related features. Their ADCNet  achieved an \naccuracy of 83.4%, an AUC of 94.26%, a Kappa score of \n74.8%, and a sensitivity of 67.7% on the APTOS dataset, with \noverall classification performance slightly lower than TMILv4. \nThe Texture Attention Network proposed by Alahmadi [30] \nachieves an accuracy of 85.1% and a Kappa score of 88.1% \non the APTOS dataset. Feng et al.  [31] proposed a graph \nneural network-based model and achieved an accuracy of 85.5% \nand a Kappa score of 90.6% on the APTOS dataset. While the \nproposed TMILv4 method achieves an accuracy of 85.6% and \na Kappa score of 90.7%. And also, the accur acy of the \nproposed method surpasses that of the MIL -VT method \nproposed by Yu et al. [15]. It should be noted that MIL-VT, in \naddition to utilizing ImageNet pre -trained weights, also \nundergoes pre-training on over 340,000 retinal images. While, \nonly the ImageNet pre -trained weights were required for \nTMILv4. The higher Kappa score of the MIL-VT method may \nbe attributed to the fact that the weights pre-trained on a large-\nscale dataset of retinal images are better than that pre-trained \non natural images. \nTABLE 5. DR CLASSIFICATION RESULTS BY DIFFERENT METHODS ON \nAPTOS DATASET (UNIT: %) \n ode       U  Kappa  en \n    16 [ 7] 7 .7     65.9 \n es et5  [ 7] 71.      6 .  \n m   16 [ 7] 78.6     7  \n m es et5  [ 7] 7 .7     65.  \nT   O   O  [  ] 8 .1     8 .1 \n      [ 8] 8 .1       \n    et [ 9] 8 .  9 . 6 7 .8 67.7 \nTexture  ttention  etwork [  ] 85.1   88.1 92 \n ode  based on     [ 1] 85.5   9 .6   \n     T [15] 85.5 97.9 92   \nT      85.6 95.6 9 .7 7 .7 \n \nThe proposed TMIL v4 method on Messidor -1 dataset \nachieves highest accuracy. Mukherjee et al. [33] performed \nunaltered classical CNN models on Messidor-1 dataset. \nSimilar to observations by Vives-Boix et al.  [27], the DR \nclassification performance of unmodified CNN models was \ngenerally low. The accuracy of Xception and MobileNet -\nV3_large are 80.3% and 81.9%, respectively. Hajabdollahi et \nal. [34] simplified the network structure and reduced network \nparameters through layer pruning based on VGG -16. The \nmodified VGG-16 achieves an accuracy of 92% and an AUC \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nof 96.1%. Both of these metrics were lower than those \nachieved by TMILv4 . And the AUC of TMILv4 are higher \nthan that of Zoom-in-net method proposed by Wang et al. [35], \nCANet method proposed by Li et al. [36] and AFN method \nproposed by Lin et al. [37]. Sun et al. [14] did not report the \naccuracy of LAT on the Messidor-1 dataset. Although a higher \nAUC of LAT was reported, the DR classification needs to be \nused in conjunction of lesion detection. In  comparison, the \nproposed TMIL focuses on DR classification and has a \nrelatively simple structure. \nTABLE 6. REFERABLE DR CLASSIFICATION RESULTS BY DIFFERENT \nMETHODS ON MESSIDOR-1 DATASET (UNIT: %) \n ode       U  \n       [ 8] 75 7  \nX eption [  ] 8 .  91.  \n obi e et     arge [  ] 81.9 9 .1 \nZoom in net [ 5] 91.1 95.7 \n odified     16 [  ] 9  96.1 \n   et [ 6] 9 .6 96.  \n    [ 7]   96.8 \n  T [1 ]   98.7 \nT      93.1 97.  \nD. COMPARISON OF INFERENCE TIME BETWEEN VIT \nAND TMIL ON HIGH-RESOLUTION IMAGES \nTABLE 7. VIT AND TMIL INFERENCE TIME ON DIFFERENT DATASETS \n ode   ataset  nferen e time  unit: \nse ond . \n iT   TO  test set 1 .   \nT      TO  test set  .71 \n iT  essidor 1 test set  .9  \nT     essidor 1 test set 1.5  \nThe inference time of TMIL and ViT  on high -resolution \nimages were tested under the same conditions. And the \ninference time on different datasets are shown in Table 7. It \ndemonstrates that the proposed TMIL achieves higher \nclassification performance on high-resolution retinal images \nwhile reduces inference time by 62%. This indicates that the \nproposed TMIL has significant advantages over the original \nViT in fast patient screening. \n \nFIGURE 4. Schematic of ViT self-attention computation, where each \nvector is computed with every vector. This diagram omits steps such as \nvector mapping and MLP. \n \nThe faster inference time achieved by TMIL is attributed to \nthe reduced sequence length resulting from the multiple \ninstance learning approach. \nMore specifically, in (1), Q, K, and V are all matrices with \ndimensions ùëõ √ó ùëë. Here, n represents the sequence length, and \nd denotes the vector dimension (in the ViT-Small model used \nin this study, d corresponds to 384). Formula 1 r equires \ncomputations between an ùëõ √ó ùëë matrix and a ùëë √ó ùëõ matrix to \nobtain an ùëõ √ó ùëõ matrix, which is subsequently operated on \nwith an ùëõ √ó ùëë matrix. This results in a time complexity of \nùëÇ(ùëõ2ùëë) , where the computational complexity is squared \nrelative to the sequence length. \nIn this study, with input images of size 896√ó 896, as \nillustrated in Figure 4, directly inputting into ViT produces a \nsequence length of 3136. The Transformer must perform \ncomputations on 3136 vectors. \n \nFIGURE 5. Schematic of TMIL self-attention computation, where the \nimage is divided into 16 instances and the Transformer computes only \nwithin each instance. This diagram omits steps such as vector mapping \nand MLP. \n \nIn TMIL, as shown in Figure 5, an 896√ó 896 image is \ndivided into 16 instances, each of size 224√ó 224. The \nTransformer only needs to process individual instances, with \na significantly reduced sequence length of only 196 for each \ninstance. \nAs a result, the reduced sequence length of 196, compared \nto the original 3136 sequence  length, leads to a substantial \nreduction in inference time. \nV. CONCLUSION \nIn this work, a Transformer-based multiple instance model is \ndeveloped for diabetic retinopathy classification on high -\nresolution fundus retinal images. The proposed TMIL adopts \nthe co ncept of multiple instance learning, where the high-\nresolution images are divided into several instances and ViT \nis employed to extract features within each instance. Then a \nGlobal Instance Computation Block (GICB) is designed to \ncompute features from diff erent instances and obtain global \ninformation by using attention mechanisms. Testing on the \npublicly available APTOS and Messidor -1 datasets \ndemonstrates that the proposed TMIL model significantly \nreduces inference time and enhances disease classification \naccuracy compared to the original ViT. This suggests that the \nproposed multiple instance approach can be directly applied \non high -resolution medical images and avoids the loss of \nlesion details caused by image overcompression. Furthermore, \n...\n nput image\n896 896  \n at hes: 1 6 16 16  \ndi ision\n...\nTransformer   o k\n...\n nput image\n896 896  \n nstan es:16          \ndi ision\n...\nTransformer   o k\n...\n...\n at hes:196 16 16   at hes:196 16 16  \n...\nTransformer   o k\n...\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nit can alleviate the computational demands of ViT caused by \nexcessively long sequences. However, it should be pointed out \nthat there still exists a false -negative rate exceeding 30% of \nTMIL, although the sensitivity of TMIL for DR levels 1 and 4 \nis enhanced. The image augmentation can be employed to \nenhance lesion features and thereby enhance the sensitivity in \ndetecting DR levels 1 and 4  in the future . And continuing \nefforts can be made  on refining the GICB in order to take \nbetter advantage of global information and achieve higher \ndisease classification performance.  Additionally, the \napplication of the proposed multiple instance method on 3D \nmedical images will be another research topic. \n \n \n \nREFERENCES \n[1] N. H. Cho  et al., \"IDF Diabetes Atlas: Global estimates of diabetes \nprevalence for 2017 and projections for 2045,\" Diabetes Research and \nClinical Practice, vol. 138, pp. 271-281, 2018. \n[2] R. Sarki, K. Ahmed, H. Wang, and Y. Zhang, \"Automatic detection of \ndiabetic eye disease through deep learning using fundus images: a \nsurvey,\" IEEE Access, vol. 8, pp. 151133-151149, 2020. \n[3] N. Asiri, M. Hussain, F. Al Adel, and N. Alzaidi, \"Deep learning based \ncomputer-aided diagnosis systems for diabetic retinopathy: A survey,\" \nArtificial Intelligence in Medicine, vol. 99, p. 101701, 2019. \n[4] K. B. Nielsen, M. L. Lautrup, J. K. Andersen, T. R. Savarimuthu, and \nJ. Grauslund, \"Deep learning ‚Äìbased algorithms in screening of \ndiabetic retinopathy: A systematic review of diagnostic performance,\" \nOphthalmology Retina, vol. 3, no. 4, pp. 294-304, 2019. \n[5] A. Dosovitskiy et al., \"An image is worth 16x16 words: Transformers \nfor image recognition at scale,\" arXiv preprint arXiv:2010.11929, \n2020. \n[6] F. Shamshad  et al. , \"Transformers in medical imaging: A survey,\" \narXiv preprint arXiv:2201.09873, 2022. \n[7] N. S. Kumar and B. R. Karthikeyan, \"Diabetic Retinopathy Detection \nusing CNN, Transformer and MLP based Architectures,\" in Proc. \nInternational Symposium on Intelligent Signal Processing and \nCommunication Systems (ISPACS), 2021, pp. 1-2.  \n[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: \nA large-scale hierarchical image database,\" in Proc. IEEE Conference \non Computer Vision and Pattern Recognition, 2009, pp. 248-255.  \n[9] A. Krizhevsky and G. Hinton, \"Learning multiple layers of f eatures \nfrom tiny images,\" 2009. \n[10] T.-Y. Lin et al., \"Microsoft coco: Common objects in context,\" in Proc. \nComputer Vision ‚ÄìECCV 2014: 13th European Conference, Zurich, \nSwitzerland, September 6-12, 2014, Part V 13, 2014, pp. 740-755.  \n[11] W. L. Alyoubi,  M. F. Abulkhair, and W. M. Shalash, \"Diabetic \nretinopathy fundus image classification and lesions localization \nsystem using deep learning,\" Sensors, vol. 21, no. 11, p. 3704, 2021. \n[12] N. J. Mohan, R. Murugan, T. Goel, and P. Roy, \"ViT -DR: Vision \nTransformers in Diabetic Retinopathy Grading Using Fundus \nImages,\" in Proc. IEEE 10th Region 10 Humanitarian Technology \nConference (R10-HTC), 2022, pp. 167-172.  \n[13] C. Matsoukas, J. F. Haslum, M. S√∂ derberg, and K. Smith, \"Is it time \nto replace cnns with transformers for medical images?\" arXiv preprint \narXiv:2108.09038, 2021. \n[14] R. Sun, Y. Li, T. Zhang, Z. Mao, F. Wu, and Y. Zhang, \"Lesion-aware \ntransformers for diabetic retinopathy grading,\" in Proc. IEEE/CVF \nConference on Computer Vision and Pattern Recogniti on, 2021, pp. \n10938-10947.  \n[15] S. Yu  et al. , \"Mil -vt: Multiple instance learning enhanced vision \ntransformer for fundus image classification,\" in Proc. Medical Image \nComputing and Computer Assisted Intervention‚ÄìMICCAI 2021: 24th \nInternational Conference,  Strasbourg, France, September 27 ‚Äì\nOctober 1, 2021, Part VIII 24, 2021, pp. 45-54.  \n[16] A. Myronenko, Z. Xu, D. Yang, H. R. Roth, and D. Xu, \"Accounting \nfor dependencies in deep learning based multiple instance learning for \nwhole slide imaging,\" in Proc. Medical Image Computing and \nComputer Assisted Intervention ‚ÄìMICCAI 2021: 24th International \nConference, Strasbourg, France, September 27‚ÄìOctober 1, 2021, Part \nVIII 24, 2021, pp. 329-338.  \n[17] Z. Qian  et al. , \"Transformer based multiple instance learning for  \nweakly supervised histopathology image segmentation,\" in Proc. \nMedical Image Computing and Computer Assisted Intervention ‚Äì\nMICCAI 2022: 25th International Conference, Singapore, September \n18‚Äì22, 2022, Part II, 2022, pp. 160-170.  \n[18] T. G. Dietterich, R. H. Lathrop, and T. Lozano -P√© rez, \"Solving the \nmultiple instance problem with axis -parallel rectangles,\" Artificial \nIntelligence, vol. 89, no. 1-2, pp. 31-71, 1997. \n[19] V. Cheplygina, D. M. Tax, and M. Loog, \"Multiple instance learning  \nwith bag dissimilarities,\" Pattern Recognition, vol. 48, no. 1, pp. 264-\n275, 2015. \n[20] J. Amores, \"Multiple instance classification: Review, taxonomy and \ncomparative study,\" Artificial Intelligence, vol. 201, pp. 81-105, 2013. \n[21] H. Li  et al. , \"DT -MIL: deformable transformer for multi -instance \nlearning on histopathological image,\" in  Proc. Medical Image \nComputing and Computer Assisted Intervention‚ÄìMICCAI 2021: 24th \nInternational Conference, Strasbourg, France, September 27 ‚Äì\nOctober 1, 2021, Part VIII 24, 2021, pp. 206-216.  \n[22] M. Karthik, Sohier Dane. APTOS 2019 Blindness Detection [Online] \nAvailable: https://kaggle.com/competitions/aptos2019-blindness-\ndetection \n[23] E. Decenc i√® re et al. , \"Feedback on a publicly distributed image \ndatabase: the Messidor database,\" Image Analysis & Stereology, vol. \n33, no. 3, pp. 231-234, 2014. \n[24] V. Gulshan  et al. , \"Development and validation of a deep learning \nalgorithm for detection of diabe tic retinopathy in retinal fundus \nphotographs,\" Jama, vol. 316, no. 22, pp. 2402-2410, 2016. \n[25] C. P. Wilkinson  et al. , \"Proposed international clinical diabetic \nretinopathy and diabetic macular edema disease severity scales,\" \nOphthalmology, vol. 110, no. 9, pp. 1677-1682, 2003. \n[26] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \n\"Rethinking the inception architecture for computer vision,\" in Proc. \nof the IEEE Conference on Computer Vision and Pattern Recognition, \n2016, pp. 2818-2826.  \n[27] V. Vives -Boix and D. Ruiz -Fern√° ndez, \"Diabetic retinopathy \ndetection through convolutional neural networks with synaptic \nmetaplasticity,\" Computer Methods and Programs in Biomedicine, vol. \n206, p. 106094, 2021. \n[28] J. Mao, H. Zhou, X. Yin, Y. C. Xu, and B . N. Rui, \"Masked \nautoencoders is an effective solution to transformer data -hungry,\" \narXiv preprint arXiv:2212.05677, 2022. \n[29] G. Yue, Y. Li, T. Zhou, X. Zhou, Y. Liu, and T. Wang, \"Attention -\nDriven Cascaded Network for Diabetic Retinopathy Grading from \nFundus Images,\" Biomedical Signal Processing and Control, vol. 80, \np. 104370, 2023. \n[30] M. D. Alahmadi, \"Texture attention network for diabetic retinopathy \nclassification,\" IEEE Access, vol. 10, pp. 55522-55532, 2022. \n[31] M. Feng, J. Wang, K. Wen, and J.  Sun, \"Grading of Diabetic \nRetinopathy Images Based on Graph Neural Network,\" IEEE Access, \n2023. \n[32] W. Wong, F. H. Juwono, and C. Capriono, \"Diabetic Retinopathy \nDetection and Grading: A Transfer Learning Approach Using \nSimultaneous Parameter Optimization and Feature -Weighted ECOC \nEnsemble,\" IEEE Access, 2023. \n[33] N. Mukherjee and S. Sengupta, \"Comparin g deep feature extraction \nstrategies for diabetic retinopathy stage classification from fundus \nimages,\" Arabian Journal for Science and Engineering, pp. 1-20, 2023. \n[34] M. Hajabdollahi, R. Esfandiarpoor, K. Najarian, N. Karimi, S. Samavi, \nand S. R. Sorous hmehr, \"Hierarchical pruning for simplification of \nconvolutional neural networks in diabetic retinopathy classification,\" \nin Proc. 2019 41st Annual International Conference of the IEEE \nEngineering in Medicine and Biology Society (EMBC), 2019, pp. 970-\n973.  \n[35] Z. Wang, Y. Yin, J. Shi, W. Fang, H. Li, and X. Wang, \"Zoom-in-net: \nDeep mining lesions for diabetic retinopathy detection,\" in Proc. \nMedical Image Computing and Computer Assisted Intervention‚àí \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nMICCAI 2017: 20th International Conference, Quebec City,  QC, \nCanada, September 11-13, 2017, Part III 20, 2017, pp. 267-275.  \n[36] X. Li, X. Hu, L. Yu, L. Zhu, C.-W. Fu, and P.-A. Heng, \"CANet: cross-\ndisease attention network for joint diabetic retinopathy and diabetic \nmacular edema grading,\" IEEE Transactions on Medical Imaging, vol. \n39, no. 5, pp. 1483-1493, 2019. \n[37] Z. Lin et al., \"A framework for identifying diabetic retinopathy based \non anti-noise detection and attention-based fusion,\" in Proc. Medical \nImage Computing and Computer Assisted Intervention‚ÄìMICCAI 2018: \n21st International Conference, Granada, Spain, September 16 -20, \n2018, Part II 11, 2018, pp. 74-82.  \n[38] E. Abdelmaksoud, S. El -Sappagh, S. Barakat, T. Abuhmed, and M. \nElmogy, \"Automatic diabetic retinopathy grading system based on \ndetecting multiple retinal lesions,\" IEEE Access, vol. 9, pp. 15939 -\n15960, 2021. \n \n \n \nYaoming Yang  was born in Yuhuan, Zhejiang, \nChina in 1998. He received the B.E. degree  in \ncomputer science and te chnology from the \nCollege of Modern Science and Technology, \nChina Jiliang University, Hangzhou, China. He is \ncurrently pursuing the M.S. degree in electronic \nscience and technology at China Jiliang University, \nCollege of Science, Hangzhou, China. \nHis research interests include image processing, \nmachine learning, and artificial intelligence. \n \n \nZhili Cai was born in Shanghai, China in 1999. \nShe received the B.E. degree in computer science \nand technology from Shanghai Business School, \nShanghai, China in 2021. She is currently pursuing \nthe M.S. degree in electronic information at China \nJiliang University, College of Science, Hangzhou, \nChina. \nHer research interests include image processing, \nmachine learning, artificial intelligence and \nporous media. \n \n \nShuxia Qiu  was born in China in 1980. She \nreceived the B.S. degree in physics from Qufu \nNormal University, Qufu, China in 2004, the M.S. \ndegree in theoretical physics from Huazhong \nUniversity of Science and Technology, Wuhan, \nChina in 2007 and the Ph.D. degree in engineering \nthermophysics from University of Shanghai for \nScience and Technology, Shanghai, China in 2021.  \nFrom 2009 to 2021, she was an engineer at \nChina Jiliang University (CJLU), College of \nScience. Since 2022, she has been a senior engineer at CJLU. She is the \nauthor of more than 30 articles. Her research interest includes enhanced heat \nand mass transfer, porous media, CFD. \n \n \nPeng Xu was born in China in 1981. He received \nthe B.S. degree in physics from Qufu Normal \nUniversity, and Ph.D. degree in condensed matter \nphysics from Huazhong University of Science and \nTechnology, Wuhan, China in 2008. \nFrom 2007 to 2009, he was a visiting scholar \nwith Transport Processes Research (TPR) group of \nNational University of Singapore. And he was a \nvisiting professor with Mine Mul tiphysics group \nof McGill University from Jun to Dec in 2015 . \nSince 2018, he has been a Full Professor of Applied Physics at China Jiliang \nUniversity (CJLU). He serves as the Deputy Dean of College of Science in \nCJLU and Editorial Board of FRACTALS. The research interests includes \nheat and mass transfer in porous media, image processing, multiscale and \nmultiphysics modeling, fractal geometry and its applications in engineering, \nsludge dewatering and drying etc. He is the author of two books, one book \nchapter and more than 100 journal papers. \nDr. Xu was ranked as the  or d‚Äôs Top  %   ientists. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3351473\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Diabetic retinopathy",
  "concepts": [
    {
      "name": "Diabetic retinopathy",
      "score": 0.6272113919258118
    },
    {
      "name": "Computer science",
      "score": 0.6187523007392883
    },
    {
      "name": "Transformer",
      "score": 0.4391847252845764
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39799806475639343
    },
    {
      "name": "Machine learning",
      "score": 0.342532217502594
    },
    {
      "name": "Diabetes mellitus",
      "score": 0.25249183177948
    },
    {
      "name": "Medicine",
      "score": 0.20932933688163757
    },
    {
      "name": "Engineering",
      "score": 0.07124045491218567
    },
    {
      "name": "Voltage",
      "score": 0.06798994541168213
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Endocrinology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55538621",
      "name": "China Jiliang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210123185",
      "name": "Zhejiang Lab",
      "country": "CN"
    }
  ],
  "cited_by": 30
}