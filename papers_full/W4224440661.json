{
  "title": "Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering",
  "url": "https://openalex.org/W4224440661",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2528705731",
      "name": "Yu Jung Heo",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2343139254",
      "name": "Eun Sol Kim",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2145811783",
      "name": "Woo Suk Choi",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A4227092556",
      "name": "Byoung-Tak Zhang",
      "affiliations": [
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3154297739",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W3034552680",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W3104007871",
    "https://openalex.org/W2963521239",
    "https://openalex.org/W2963448850",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2970988759",
    "https://openalex.org/W2997062749",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2964051675",
    "https://openalex.org/W1894439495",
    "https://openalex.org/W3034862985",
    "https://openalex.org/W2950898568",
    "https://openalex.org/W2083897630",
    "https://openalex.org/W3035272542",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2996919866",
    "https://openalex.org/W3100071090",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2769099080",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W2969985801",
    "https://openalex.org/W3105305165",
    "https://openalex.org/W2963477107",
    "https://openalex.org/W102708294",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2891394954",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W2966317026",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3114713146",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2783905278",
    "https://openalex.org/W2950457956",
    "https://openalex.org/W2964303913",
    "https://openalex.org/W3035520037",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3035454069",
    "https://openalex.org/W2952792693",
    "https://openalex.org/W2945365184",
    "https://openalex.org/W2964051877"
  ],
  "abstract": "Knowledge-based visual question answering (QA) aims to answer a question which requires visually-grounded external knowledge beyond image content itself. Answering complex questions that require multi-hop reasoning under weak supervision is considered as a challenging problem since i) no supervision is given to the reasoning process and ii) high-order semantics of multi-hop knowledge facts need to be captured. In this paper, we introduce a concept of hypergraph to encode high-level semantics of a question and a knowledge base, and to learn high-order associations between them. The proposed model, Hypergraph Transformer, constructs a question hypergraph and a query-aware knowledge hypergraph, and infers an answer by encoding inter-associations between two hypergraphs and intra-associations in both hypergraph itself. Extensive experiments on two knowledge-based visual QA and two knowledge-based textual QA demonstrate the effectiveness of our method, especially for multi-hop reasoning problem. Our source code is available at https://github.com/yujungheo/kbvqa-public.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 373 - 390\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nHypergraph Transformer: Weakly-Supervised Multi-hop Reasoning\nfor Knowledge-based Visual Question Answering\nYu-Jung Heo1,4, Eun-Sol Kim2, Woo Suk Choi1, and Byoung-Tak Zhang1,3\n1 Seoul National University 2 Department of Computer Science, Hanyang University\n3 AI Institute (AIIS), Seoul National University 4 Surromind\nyjheo@bi.snu.ac.kr, eunsolkim@hanyang.ac.kr, {wschoi, btzhang}@bi.snu.ac.kr\nAbstract\nKnowledge-based visual question answering\n(QA) aims to answer a question which re-\nquires visually-grounded external knowledge\nbeyond image content itself. Answering com-\nplex questions that require multi-hop reason-\ning under weak supervision is considered as\na challenging problem since i) no supervision\nis given to the reasoning process and ii) high-\norder semantics of multi-hop knowledge facts\nneed to be captured. In this paper, we intro-\nduce a concept of hypergraph to encode high-\nlevel semantics of a question and a knowl-\nedge base, and to learn high-order associations\nbetween them. The proposed model, Hyper-\ngraph Transformer, constructs a question hy-\npergraph and a query-aware knowledge hy-\npergraph, and infers an answer by encoding\ninter-associations between two hypergraphs\nand intra-associations in both hypergraph it-\nself. Extensive experiments on two knowledge-\nbased visual QA and two knowledge-based\ntextual QA demonstrate the effectiveness of\nour method, especially for multi-hop reason-\ning problem. Our source code is available\nat https://github.com/yujungheo/\nkbvqa-public.\n1 Introduction\nVisual question answering (VQA) is a semantic rea-\nsoning task that aims to answer questions about vi-\nsual content depicted in images (Antol et al., 2015;\nZhu et al., 2016; Hudson and Manning, 2019), and\nhas become one of the most active areas of research\nwith advances in natural language processing and\ncomputer vision. Recently, researches for VQA\nhave advanced, from inferring visual properties on\nentities in a given image, to inferring commonsense\nor world knowledge about those entities (Wang\net al., 2017, 2018; Marino et al., 2019; Shah et al.,\n2019; Zellers et al., 2019).\nIn this paper, we focus on the task which is\ncalled knowledge-based visual question answering,\nFigure 1: An example of knowledge-based visual ques-\ntion answering. The rectangles and arrows between the\nrectangles represent the entities and relations from KB.\nTo answer the given question, the multiple reasoning\nevidences (marked as orange) are required.\nwhere a massive number of knowledge facts from\na general knowledge base (KB) is given with an\nimage-question pair. To answer the given question\nas shown in Figure 1, a model should understand\nthe semantics of the given question, link visual enti-\nties appearing in the given image to the KB, extract\na number of evidences from the KB and predict an\nanswer by aggregating semantics of both the ques-\ntion and the extracted evidences. Following these,\nthere are two fundamental challenges in this task. i)\nTo answer a complex question, multi-hop reasoning\nover multiple knowledge evidences is necessary. ii)\nLearning a complex reasoning process is difficult\nespecially in a condition where only QA is pro-\nvided without extra supervision on how to capture\nany evidence from the KB and infer based on them.\nThat is, the model should learn which knowledge\nfacts to be attended to and how to combine them\nto infer the correct answer on its own. Following\nthe previous work (Zhou et al., 2018), we call this\nsetting under weak supervision.\nUnder weak supervision, previous studies pro-\nposed memory-based methods (Narasimhan and\nSchwing, 2018; Shah et al., 2019) and graph-based\n373\nmethods (Narasimhan et al., 2018; Zhu et al., 2020)\nto learn to selectively focus on necessary pieces of\nknowledge. The memory-based methods represent\nknowledge facts in a form of memory and calculate\nsoft attention scores of each memory with respect\nto a question. Then, it infers an answer by attending\nto knowledge evidence with high attention scores.\nOn the other hand, to explicitly consider relational\nstructure between knowledge facts, graph-based\nmethods construct a query-aware knowledge graph\nby retrieving facts from KB and perform graph rea-\nsoning for a question. These methods mainly adopt\nan iterative message passing process to propagate\ninformation between adjacent nodes in the graph.\nHowever, it is difficult to capture multi-hop rela-\ntionships containing long-distance nodes from the\ngraph due to the well-known over-smoothing prob-\nlem, where repetitive message passing process to\npropagate information across long distance makes\nfeatures of connected nodes too similar and undis-\ncriminating (Li et al., 2018; Wang et al., 2020).\nTo address the above limitation, we propose a\nnovel method, Hypergraph Transformer, which ex-\nploits hypergraph structure to encode multi-hop re-\nlationships and transformer-based attention mecha-\nnism to learn to pay attention to important knowl-\nedge evidences for a question. We construct a ques-\ntion hypergraph and a knowledge hypergraph to\nexplicitly encode high-order semantics present in\nthe question and each knowledge fact, and capture\nmulti-hop relational knowledge facts effectively.\nThen, we perform hyperedge matching between the\ntwo hypergraphs by leveraging transformer-based\nattention mechanism. We argue that introducing the\nconcept of hypergraph is powerful for multi-hop\nreasoning problem in that it can encode high-order\nsemantics without the constraint of length and learn\ncross-modal high-order associations.\nThe main contributions of this paper can be sum-\nmarized as follows. i) We propose Hypergraph\nTransformer which enhances multi-hop reasoning\nability by encoding high-order semantics in the\nform of a hypergraph and learning inter- and intra-\nhigh-order associations in hypergraphs using the\nattention mechanism. ii) We conduct extensive ex-\nperiments on two knowledge-based VQA datasets\n(KVQA and FVQA) and two knowledge-based tex-\ntual QA datasets (PQ and PQL) and show superior\nperformances on all datasets, especially multi-hop\nreasoning problem. iii) We qualitatively observe\nthat Hypergraph Transformer performs robust in-\nference by focusing on correct reasoning evidences\nunder weak supervision.\n2 Related Work\nKnowledge-based visual question answering\n(Wang et al., 2017, 2018; Shah et al., 2019; Marino\net al., 2019; Sampat et al., 2020) proposed bench-\nmark datasets for knowledge-based visual question\nanswering that requires reasoning about an image\non the basis of facts from a large-scale knowledge\nbase (KB) such as Freebase (Bollacker et al., 2008)\nor DBPedia (Auer et al., 2007). To solve the task,\ntwo pioneering studies (Wang et al., 2017, 2018)\nsuggested logical parsing-based methods which\nconvert a question to a KB logic query using pre-\ndefined query templates and execute the generated\nquery on KB for searching an answer. Since then\ninformation retrieval-based methods which retrieve\nknowledge facts associated with a question and con-\nduct semantic matching between the facts and the\nquestion are introduced. (Narasimhan and Schwing,\n2018; Shah et al., 2019) proposed memory-based\nmethods that represent knowledge facts in the form\nof memory and calculate soft attention scores of\nthe memory with a question. (Narasimhan et al.,\n2018; Zhu et al., 2020) represented the retrieved\nfacts as a graph and performed graph reasoning\nthrough message passing scheme utilizing graph\nconvolution. However, these methods are compli-\ncated to encode inherent high-order semantics and\nmulti-hop relationships present in the knowledge\ngraph. Therefore, we introduce a concept of hy-\npergraph and propose transformer-based attention\nmechanism over hypergraphs.\nMulti-hop knowledge graph reasoningis a pro-\ncess of sequential reasoning based on multiple evi-\ndences of a knowledge graph, and has been broadly\nused in various downstream tasks such as ques-\ntion answering (Lin et al., 2019; Saxena et al.,\n2020; Han et al., 2020b,a; Yadati et al., 2021), or\nknowledge-enhanced text generation (Liu et al.,\n2019; Moon et al., 2019; Ji et al., 2020). Recent\nresearches have introduced the concept of hyper-\ngraph for multi-hop graph reasoning (Kim et al.,\n2020; Han et al., 2020b,a; Yadati et al., 2019, 2021;\nSun et al., 2020). These models have a similar moti-\nvation to the Hypergraph Transformer proposed in\nthis paper, but core operations are vastly different.\nThese models mainly update node representations\nin the hypergraph through a message passing pro-\ncess using graph convolution operation. On the\n374\nFigure 2: The overview of Hypergraph Transformer. (a) Entity linking module links concepts from query (a\ngiven image and a question) to KB. (b) Query-aware knowledge hypergraphHk and question hypergraph Hq are\nconstructed by multi-hop graph walk. (c) Two hyperedge sets are fed into the guided-attention and self-attention\nblocks to learn inter- and intra-association in them. The joint representation is used to predict an answer.\ncontrary, our method update node representations\nvia hyperedge matching of hypergraphs instead of\nmessage passing scheme. We argue that this update\nprocess effectively learns the high-order seman-\ntics inherent in each hypergraph and the high-order\nassociations between two hypergraphs.\n3 Method\n3.1 Notation\nTo capture high-order semantics inherent in the\nknowledge sources, we adopt the concept of hyper-\ngraph. Formally, directed hypergraph H = {V, E}\nis defined by a set of nodes V = {v1, ..., v|V|}\nand a set of hyperedges E = {h1, ..., h|E|}. Each\nnode is represented as a w-dimensional embed-\nding vector, i.e., vi ∈ Rw. Each hyperedge con-\nnects an arbitrary number of nodes and has partial\norder itself, i.e., hi = {v′\n1 ⪯ ... ⪯ v′\nl} where\nV′ = {v′\n1, ..., v′\nl} is a subset of V and ⪯ is a binary\nrelation which denotes an element ( v′\ni) precedes\nthe other ( v′\nj) in the ordering when v′\ni ⪯ v′\nj. A\nhyperedge is flexible to encode different kinds of\nsemantics in the underlying graph without the con-\nstraint of length.\n3.2 Entity linking\nAs shown in Figure 2(a), entity linking module first\nlinks concepts from query (a given image-question\npair) to knowledge base. We detect visual concepts\n(e.g., objects, attributes, person names) in a given\nimage and named entities in a given question. The\nsemantic labels of visual concepts or named enti-\nties are then linked with knowledge entities in the\nknowledge base using exact keyword matching.\n3.3 Hypergraph construction\nQuery-aware knowledge hypergraph A knowl-\nedge base (KB), a vast amount of general knowl-\nedge facts, contains not only knowledge facts re-\nquired to answer a given question but also unneces-\nsary knowledge facts. Thus, we construct a query-\naware knowledge hypergraph Hk = {Vk, Ek} to\nextract related information for answering a given\nquestion. It consists of a node setVk and hyperedge\nset Ek, which represent a set of entities in knowl-\nedge facts and a set of hyperedges, respectively.\nEach hyperedge connects the subset of vertices\nV′k ⊂ Vk.\nWe consider a huge number of knowledge facts\nin the KB as a huge knowledge graph, and construct\na hypergraph by traversing the knowledge graph.\nSuch traversal, called graph walk, starts from the\nnode linked from the previous module (see section\n3.2) and considers all entity nodes associated with\nthe start node. We define a triplet as a basic unit\nof graph walk to preserve high-order semantics in-\nherent in knowledge graph, i.e., every single graph\nwalk contains three nodes {head, predicate, tail},\nrather than having only one of these three nodes. In\naddition to the triplet-based graph walks, a multi-\nhop graph walk is proposed to encode multiple\nrelational facts that are interconnected. Multi-hop\ngraph walk connects multiple facts by setting the\narrival node (tail) of the preceding walk as the start-\ning (head) node of the next walk, thus,n-hop graph\nwalk combines n facts as a hyperedge.\n375\nQuestion hypergraph We transform a question\nsentence into a question hypergraph Hq consisting\nof a node setVq and a hyperedge setEq. We assume\nthat each word unit (a word or named entity) of\nthe question is defined as a node, and has edges\nto adjacent nodes. For question hypergraph, each\nword unit is used as a start node of a graph walk.\nThe multi-hop graph walk is conducted in the same\nmanner as the knowledge hypergraph. A n-gram\nphrase is considered as a hyperedge in the question\nhypergraph (see Figure 2(b)).\n3.4 Reasoning with attention mechanism\nTo consider high-order associations between knowl-\nedge and question, we devise structural semantic\nmatching between the query-aware knowledge hy-\npergraph and the question hypergraph. We intro-\nduce an attention mechanism over two hypergraphs\nbased on guided-attention (Tsai et al., 2019) and\nself-attention (Vaswani et al., 2017). As shown\nin Figure 2(c), the guided-attention blocks are\nintroduced to learn correlations between knowl-\nedge hyperedges and question hyperedges by inter-\nattention mechanism, and then intra-relationships\nof in knowledge or question hyperedges are trained\nwith the following self-attention blocks. The details\nof two modules, guided-attention blocks and self-\nattention blocks, are described as below. Note that\nwe use Q, K, and V for query, key, value, andq, k\nas subscripts to represent question and knowledge,\nrespectively.\nGuided-attention To learn inter-association be-\ntween two hypergraphs, we first embed a knowl-\nedge hyperedge and a question hyperedge as fol-\nlows: ek = ϕk ◦ fk(hk) ∈ Rd, eq = ϕq ◦ fq(hq) ∈\nRd where h[·] is a hyperedge in E[·]. Here, f[·] is a\nhyperedge embedding function and ϕ[·] is a linear\nprojection function. The design and implementa-\ntion of f[·] are not constrained (e.g., any pooling\noperation or any learnable neural networks), but\nwe use a simple concatenation operation of node\nrepresentations in a hyperedge as f[·]. The repre-\nsentations of hyperedges in the same hypergraph\n(e.g., ek, eq) are packed together into a matrix Ek\nand Eq.\nWe define the knowledge hyperedgesEk and the\nquestion hyperedges Eq as a query and key-value\npairs, respectively. We set a queryQk = EkWQk ,\na key Kq = EqWKq , and a value Vq = EqWVq ,\nwhere all projection matrices W[·] ∈ Rd×dv are\nlearnable parameters. Then, scaled dot product at-\ntention using the query, key, and value is calculated\nas Attention(Qk, Kq, Vq) = softmax(\nQkKT\nq√dv\n)Vq\nwhere dv is the dimension of the query and the key\nvector. In addition, the guided-attention which uses\nthe question hyperedges as query and the knowl-\nedge hyperedges as key-value pairs is performed in\na similar manner: Attention(Qq, Kk, Vk).\nSelf-attention The only difference between\nguided-attention and self-attention is that the same\ninput is used for both query and key-value within\nself-attention. For example, we set query, key, and\nvalue based on the knowledge hyperedges Ek, and\nthe self-attention for knowledge hyperedges is con-\nducted by Attention(Qk, Kk, Vk). For question\nhyperedges Eq, self-attention is performed in a\nsimilar manner: Attention(Qq, Kq, Vq).\nFollowing the standard structure of the trans-\nformer, we build up guided-attention block and self-\nattention block where each block consists of each\nattention operation with layer normalization, resid-\nual connection, and a single feed-forward layer.\nBy passing the guided-attention blocks and self-\nattention blocks sequentially, representations of\nknowledge hyperedges and question hyperedges\nare updated and finally aggregated to single vector\nrepresentation as zk ∈ Rdv and zq ∈ Rdv , respec-\ntively.\n3.5 Answer predictor\nTo predict an answer, we first concatenate the rep-\nresentation zk and zq obtained from the attention\nblocks and feed into a single feed-forward layer\n(i.e., R2dv 7→ Rw) to make a joint representation\nz. We then consider two types of answer predictor:\nmulti-layer perceptron and similarity-based answer\npredictor. Multi-layer perceptron as an answer clas-\nsifier p = ψ(z) is a prevalent for visual question\nanswering problems. For similarity-based answer,\nwe calculate a dot product similarity p = zCT\nbetween z and answer candidate set C ∈ R|A|×w\nwhere |A| is a number of candidate answers and w\nis a dimension of representation for each answer.\nThe most similar answer to the joint representation\nis selected as an answer among the answer candi-\ndates. For training, we use only supervision from\nQA pairs without annotations for ground-truth rea-\nsoning paths. To this end, cross-entropy between\nprediction p and ground-truth t is utilized as a loss\nfunction.\n376\nModel Original (ORG) Paraphrased (PRP) Mean1-hop 2-hop 3-hop 1-hop 2-hop 3-hop\nBLSTM - - - - - - 51.0\nMemNN (Sukhbaatar et al., 2015) - - - - - - 59.2\nGCN (Kipf and Welling, 2017) 65.7 67.4 66.9 65.8 67.5 67.0 66.7\nGGNN (Li et al., 2016) 72.9 74.5 74.0 72.9 74.6 74.1 73.8\nMemNN† (Sukhbaatar et al., 2015) 78.1 77.8 76.1 78.0 78.1 76.0 77.3\nHAN (Kim et al., 2020) 77.5 77.5 77.2 77.1 77.4 76.9 77.3\nBAN (Kim et al., 2018) 83.5 84.0 83.7 83.7 84.3 83.8 83.8\nOurs 88.1 90.2 91.0 87.8 90.5 90.7 89.7\nTable 1: QA accuracy on oracle setting in KVQA under weak supervision. ORG and PRP are a type of question and\n1-hop, 2-hop, and 3-hop are the number of graph walks to construct a knowledge hypergraph. The performance of\nBLSTM and MemNN is reported in (Shah et al., 2019) and we re-implemented MemNN† for a fair comparison.\n4 Experimental Settings\n4.1 Datasets\nIn this paper, we evaluate our model across vari-\nous benchmark datasets: Knowledge-aware VQA\n(KVQA) (Shah et al., 2019), Fact-based VQA\n(FVQA) (Wang et al., 2018), PathQuestion (PQ)\nand PathQuestion-Large (PQL) (Zhou et al., 2018).\nKVQA, a large-scale benchmark dataset for com-\nplex VQA, contains 183,007 pairs for 24,602\nimages from Wikipedia and corresponding cap-\ntions, and provides 174,006 knowledge facts for\n39,414 unique named entities based on Wiki-\ndata (Vrande ˇci´c and Krötzsch, 2014) since it re-\nquires world knowledge beyond visual content.\nKVQA consists of two types of questions: orig-\ninal (ORG) and paraphrased (PRP) question gen-\nerated from the original question via the online\nparaphrasing tool. FVQA, a representative dataset\nfor commonsense-enabled VQA, considers exter-\nnal knowledge about common nouns depicted in a\ngiven image, and contains 5,826 QA pairs for 2,190\nimages and 4,216 unique knowledge facts from\nDBPedia (Auer et al., 2007), ConceptNet (Liu and\nSingh, 2004), and WebChild (Tandon et al., 2014).\nThe last two datasets, PQ and PQL, focus on evalu-\nating multi-hop reasoning ability in the knowledge-\nbased textual QA task. PQ and PQL contain 7,106\nand 2,625 QA pairs on 4,050 and 9,844 knowledge\nfacts from the subset of Freebase (Bollacker et al.,\n2008), respectively. The detailed statistics of the\ndatasets are shown in Appendix A.\n4.2 Implementation details\nEach node in the knowledge hypergraph and\nthe question hypergraph is represented as a 300-\ndimensional vector (i.e., w = 300) initialized using\nGloVe (Pennington et al., 2014). Random initial-\nization is applied when a word for a node does not\nexist in the vocabulary of GloVe. Mean pooling is\napplied when a node consists of multiple words.\nFor entity linking for KVQA, we apply the well-\nknown pre-trained models for face identification:\nRetinaFace (Deng et al., 2020) for face detection\nand ArcFace (Deng et al., 2019) for face feature\nextraction. For all datasets, we follow the experi-\nmental settings as in previous works. We use the\nsimilarity-based answer predictor for KVQA, and\nMLP for the others. We adopt Adam (Kingma and\nBa, 2015) to optimize all learnable parameters in\nthe model. We describe details of the experimental\nsettings and the tuned hyperparameters for each\ndataset in Appendix D.\n5 Quantitative Results\n5.1 Knowledge-aware visual question\nanswering\nWe compare the proposed model, Hypergraph\nTransformer, with other comparative state-of-the-\nart methods. We report performances on original\n(ORG) and paraphrased (PRP) questions accord-\ning to the number of graph walk. For compara-\ntive models, three kinds of methods are consid-\nered, which are graph-based, memory-based and\nattention-based networks. The detailed description\nabout the comparative models is described in Ap-\npendix E. To evaluate a pure reasoning ability of\nthe models regardless of the performance of entity\nlinking, we first conduct experiments in the ora-\ncle setting which ground-truth named entities in an\nimage are given.\nAs shown in Table 1, our model outperforms\ncomparative models with a large margin across\n377\nPathQuestion PathQuestion-Large\nPQ-2H PQ-3H PQ-M PQL-2H PQL-3H PQL-M\nSeq2Seq (Sutskever et al., 2014) 89.9 77.0 - 71.9 64.7 -\nMemNN (Sukhbaatar et al., 2015) 89.5 79.2 86.8 61.2 53.6 55.8\nKV-MemNN (Miller et al., 2016) 91.5 79.4 85.2 70.5 63.4 68.6\nIRN (Zhou et al., 2018) 96.0 87.7 - 72.5 71.0 -\nEmbed (Bordes et al., 2014b) 78.7 48.3 - 42.5 22.5 -\nSubgraph (Bordes et al., 2014a) 74.4 50.6 - 50.0 21.3 -\nMINERV A (Das et al., 2018) 75.9 71.2 73.1 71.8 65.7 66.9\nIRN-weak (Zhou et al., 2018) 91.9 83.3 85.8 63.0 61.8 62.4\nSRN (Qiu et al., 2020) 96.3 89.2 89.3 78.6 77.5 78.3\nOurs 96.4 90.3 89.5 90.5 77.9 (*) 94.5\n(*) For PQL-3H-More data (2x QA pairs on the same KB as PQL-3H), our model shows 95.4% accuracy.\nTable 2: Accuracy on PathQuestion (PQ) and PathQuestion-Large (PQL). 2H and 3H represent the number of\nmulti-hops in ground-truth reasoning paths to answer given questions, and M represents the mixture of 2H and 3H.\nThe models in the first block employ a ground-truth reasoning path as extra supervision (i.e., fully-supervised), and\nthe models in the second block including our model are under weak supervision.\nall settings. From the results, we find that the at-\ntention mechanism between question and knowl-\nedge is crucial for complex QA. Since GCN (Kipf\nand Welling, 2017) and GGNN (Li et al., 2016)\nencode question and knowledge graph separately,\nthey do not learn interactions between question and\nknowledge. Thus, GCN and GGNN show quite\nlow performance under 74% mean accuracy. On\nthe other hand, MemNN † (Weston et al., 2015),\nHAN (Kim et al., 2020), and BAN (Kim et al.,\n2018) achieve comparatively high performance be-\ncause MemNN† adopts question-guided soft atten-\ntion over knowledge memories. HAN and BAN\nutilize multi-head co-attention between question\nand knowledge.\nEntity linking setting We also present the exper-\nimental results on the entity linking setting where\nthe named entities are not provided as the oracle\nsetting, but detected by the module as described in\nSection 3.2. As shown in Table 7 of Appendix E,\nour model shows the best performances for both\noriginal and paraphrased questions. For all compar-\native models, we use the same knowledge hyper-\ngraph extracted by the 3-hop graph walk. In entity\nlinking setting, the constructed knowledge hyper-\ngraph can be incomplete and quite noisy due to the\nundetected entities or misclassified entity labels.\nHowever, Hypergraph Transformer shows robust\nreasoning capacity over the noisy inputs. Here, we\nremark that the upper bound of QA performance is\n72.8% due to the error rate of entity linking module.\nWe expect that the performance will be improved\nwhen the entity linking module is enhanced.\n5.2 Fact-based visual question answering\nWe conduct experiments on Fact-based Visual\nQuestion Answering (FVQA) as an additional\nbenchmark dataset for knowledge-based VQA. Dif-\nferent from KVQA focusing on world knowledge\nfor named entities, FVQA considers commonsense\nknowledge about common nouns in a given image.\nHere, we assume that the performance of entity\nlinking is perfect, and evaluate the pure reasoning\nability of our model. As shown in Table 8 of Ap-\npendix D, Hypergraph Transformer shows compa-\nrable performance in both top-1 and top-3 accuracy\nin comparison with the state-of-the-art methods.\nWe confirm that our model works effectively as\na general reasoning framework without consider-\ning characteristics of different knowledge sources\n(i.e., Wikidata for KVQA, DBpedia, ConceptNet,\nWebChild for FVQA).\n5.3 PathQuestion and PathQuestion-Large\nTo verify multi-hop reasoning ability of our model,\nwe conduct experiments on PathQuestion (PQ) and\nPathQuestion-Large (PQL). PQ and PQL datasets\nhave annotations of a ground-truth reasoning path\nto answer a given question. Specifically, {PQ,\nPQL}-{2H, 3H} denotes a split of PQ and PQL\nwith respect to the number of hops in ground-truth\nreasoning paths (i.e., 2-hop or 3-hop). {PQ, PQL}-\nM is a mixture of the 2-hop and 3-hop questions\nin both dataset, and used to evaluate the more gen-\neral scenario where the number of reasoning path\n378\nModel Inputs Original (ORG) Paraphrased (PRP) MeanKnowledge Question 1-hop 2-hop 3-hop 1-hop 2-hop 3-hop\n(a) SA Word Word 79.4 79.6 77.6 77.1 77.7 77.7 78.2\n(b) SA+GA Word Word 80.9 82.3 81.5 80.7 82.2 81.8 81.6\n(c) SA+GA Word Hyperedge 82.1 84.2 82.8 81.1 83.5 82.3 82.7\n(d) SA+GA Hyperedge Word 87.0 89.9 88.9 87.3 89.7 89.2 88.7\n(e) SA+GA Hyperedge Hyperedge 88.1 90.2 91.0 87.8 90.5 90.7 89.7(Ours)\n(f) Ours-SA Hyperedge Hyperedge 85.2 88.8 88.3 85.0 88.3 88.4 87.1\n(g) Ours-GA Hyperedge Hyperedge 82.6 83.6 85.0 82.7 83.6 84.9 83.7\nTable 3: (a-e) Validation for the effectiveness of using hypergraph. Here, we compare the results with respect to\nthe different types of the input format (i.e., Single Word or Hyperedge) used to represent knowledge and question\nwhich are fed into the attention mechanism. (e-g) Ablation study for attention blocks of Hypergraph Transformer.\nGA and SA are abbreviations of guided-attention and self-attention, respectively.\nrequired to answer a given question is unknown.\nThe experimental results on diverse split of PQ\nand PQL datasets are provided in Table 2. The first\nsection in the table includes fully-supervised mod-\nels which require a ground-truth path annotation as\nan additional supervision. The second section con-\ntains weakly-supervised models learning to infer\nthe multi-hop reasoning paths without the ground-\ntruth path annotation. Hypergraph Transformer is\ninvolved in the weakly-supervised models because\nit only exploits an answer as a supervision.\nOur model shows comparable performances on\nPQ-{2H, 3H, M} to the state-of-the-art weakly-\nsupervised model, SRN. Especially, Hypergraph\nTransformer shows significant performance im-\nprovement (78.6% → 90.5% for PQL-2H, 78.3%\n→ 94.5% for PQL-M) on PQL. We highlight that\nPQL is more challenging dataset than PQ in that\nPQL not only covers more knowledge facts but also\nhas fewer QA instances. We observe that the accu-\nracy on PQL-3H is relatively lower than the other\nsplits. This is due to the insufficient number of\ntraining QA pairs in PQL-3H. When we use PQL-\n3H-More which has twice more QA pairs (1031 →\n2062) on the same knowledge base as PQL-3H, our\nmodel achieves 95.4% accuracy.\n6 Validation for Hypergraph Transformer\nWe verify the effectiveness of each module in\nHypergraph Transformer. To analyze the perfor-\nmances of the variants in our model, we use KVQA\nwhich is a representative and large-scale dataset for\nknowledge-based VQA. Here, we mainly focus on\ntwo aspects: i) effect of hypergraph and ii) effect of\nattention mechanism. To evaluate a pure reasoning\nability of the models, we conduct experiments in\nthe oracle setting.\n6.1 Effect of hypergraph\nTo analyze the effectiveness of hypergraph-based\ninput representation, we conduct comparative ex-\nperiments on the different types of input formats\nfor Transformer architecture. Here, we consider the\ntwo types of input format, which are single-word-\nunit and hyperedge-based representations. Com-\npared to hyperedge-based inputs considering mul-\ntiple relational facts as a input token, single-word-\nunit takes every entity and relation tokens as sepa-\nrate input tokens. We note that using single-word-\nunit-based input format for both knowledge and\nquestion is the standard settings for the Transformer\nnetwork and using hyperedge-based input format\nfor both is the proposed model, Hypergraph Trans-\nformer. We set the Transformer (SA+GA) as a back-\nbone model, and present the results in Table 3(b-e).\nWhen hypergraph-based representations are used\nfor both knowledge and question, the results show\nthe best performance across all settings over ques-\ntion types (ORG and PRP) and a number of graph\nwalk (1-hop, 2-hop, and 3-hop). As shown in Table\n3, the mean accuracy of QA achieves 89.7% when\nboth are encoded using hyperedges, while using\nsingle-word-unit-based representation causes per-\nformance to drop to 81.6%. Especially, when we\nconvert the one of both hyperedge-level representa-\ntion to single-word-unit-based representation, the\nmean accuracy of QA is 82.7% and 88.7%, respec-\ntively. These results validate that it is meaningful\nto consider not only knowledge but also question\nas hypergraphs.\n379\nFigure 3: Qualitative analysis on effectiveness of using hypergraph as input format to Transformer architecture.\nHere, we visualize attention maps for Hypergraph Transformer and the Transformer (SA+GA). All attention scores\nare averaged over multi-heads and multi-layers. Each x and y axis represent indices of question and knowledge\nhyperedges in Hypergraph Transformer, and indices of question and knowledge word in Transformer (SA+GA). In\nthe attention maps, the dark colors represent high values. The hyperedges with high attention scores are visualized.\nEffect of multi-hop graph walkWe compare\nthe performances with different number of graph\nwalks used to construct a knowledge hypergraph\n(i.e., 1-hop, 2-hop, and 3-hop). All models except\nours show slightly lower performance on the 3-hop\ngraph than on the 2-hop graph. We observe that\nthe number of extracted knowledge facts increases\nwhen the number of graph walk increases, and un-\nnecessary facts for answering a given question are\nusually included. Nonetheless, our model shows ro-\nbust reasoning performance when a large and noisy\nknowledge facts are given.\n6.2 Effect of attention mechanism\nTo investigate the impacts of each attention block\n(i.e., GA and SA), ablation studies are shown in\nTable 3(e-g). The scores across all settings drop\nwhen GA or SA is removed. Particularly, the mean\naccuracy of QA is decreased by 6.0% (89.7% →\n83.7%), 2.6% (89.7% → 87.1%) for cutting out\nthe GA and the SA block, respectively. Based on\nthe two experiments, we identify that not only the\nguided-attention which captures inter-relationships\nbetween question and knowledge but also the self-\nattention which learns intra-relationship in them\nare crucial to the complex QA. To sum up, Hyper-\ngraph Transformer takes graph-level inputs, i.e.,\nhyperedge, and conducts semantic matching be-\ntween hyperedges by the attention mechanism. Due\nto the two characteristics, the model shows better\nreasoning performance focusing on the evidences\nnecessary for reasoning under weak supervision.\n7 Qualitative Analysis\nFigure 3 provides the qualitative analysis on ef-\nfectiveness of using a hypergraph as an input for-\nmat to Transformer architecture. We present the\nattention map from the guided-attention block,\nand visualize top- k attended knowledge facts or\nentities with the attention scores. In the first ex-\nample, both model, Hypergraph Transformer and\nTransformer (SA+GA), infer the correct answer,\nQ5075293. Our model responds by focusing on\n{second ⪯ from ⪯ left} phrase of the ques-\ntion and four facts having a left relation among\n86 knowledge hyperedges. In comparison, Trans-\nformer (SA+GA) strongly attends to the knowledge\nentities which appear repetitive in the knowledge\nfacts. Especially, the model attends to Q3476753,\nQ290666 and Ireland with the high attention score\n0.237, 0.221, and 0.202. In the second example, our\nmodel attends to the correct knowledge hyperedges\nconsidering the multi-hop facts about place of birth\nof the people shown in the given image, and infers\n380\nthe correct answer. On the other hand, Transformer\n(SA+GA) strongly attends to the knowledge entity\nof person (Q2439789) presented in the image with\nundesired attention score 0.788. The second and\nthird attended knowledge entities are the other per-\nson (Q7141361) and Iran. Transformer (SA+GA)\nfails to focus on the multi-hop facts required to\nanswer the given question and predicts the answer\nwith the wrong number at the end.\n8 Discussion and Conclusion\nIn this paper, we proposed Hypergraph Transformer\nfor multi-hop reasoning over knowledge graph un-\nder weak supervision. Hypergraph Transformer\nadopts hypergraph-based representation to encode\nhigh-order semantics of knowledge and questions\nand considers associations between a knowledge\nhypergraph and a question hypergraph. Here, each\nnode representation in the hypergraphs is updated\nby inter- and intra-attention mechanisms in two hy-\npergraphs, rather than by iterative message passing\nscheme. Thus, Hypergraph Transformer can mit-\nigate the well-known over-smoothing problem in\nthe previous graph-based methods exploiting the\nmessage passing scheme. Extensive experiments\non various datasets, KVQA, FVQA, PQ, and PQL\nvalidated that Hypergraph Transformer conducts\naccurate inference by focusing on knowledge evi-\ndences necessary for question from a large knowl-\nedge graph. Although not covered in this paper,\nan interesting future work is to construct heteroge-\nneous knowledge graph that includes more diverse\nknowledge sources (e.g. documents on web).\nAcknowledgements\nWe would like to thank Woo Young Kang, Kyoung-\nWoon On, Seonil Son, Gi-Cheon Kang, Christina\nBaek, Junseok Park, Min Whoo Lee, Hwiyeol Jo\nand Sang-Woo Lee for their helpful comments and\ndiscussion. This work was partly supported by the\nIITP (2015-0-00310-SW.StarLab/20%, 2017-0-\n01772-VTT/20%, 2019-0-01371-BabyMind/10%,\n2021-0-02068-AIHub/10%, 2021-0-01343-\nGSAI/10%, 2020-0-01373/10%) grants, the KIAT\n(P0006720-ILIAS/10%) grant funded by the\nKorean government, and the Hanyang University\n(HY-202100000003160/10%).\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: visual question an-\nswering. In 2015 IEEE International Conference\non Computer Vision, ICCV 2015, Santiago, Chile,\nDecember 7-13, 2015, pages 2425–2433. IEEE Com-\nputer Society.\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\nDbpedia: A nucleus for a web of open data. In The\nsemantic web, pages 722–735. Springer.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collabo-\nratively created graph database for structuring human\nknowledge. In Proceedings of the 2008 ACM SIG-\nMOD international conference on Management of\ndata, pages 1247–1250.\nAntoine Bordes, Sumit Chopra, and Jason Weston.\n2014a. Question answering with subgraph embed-\ndings. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 615–620, Doha, Qatar. Association\nfor Computational Linguistics.\nAntoine Bordes, Jason Weston, and Nicolas Usunier.\n2014b. Open question answering with weakly super-\nvised embedding models. In Joint European confer-\nence on machine learning and knowledge discovery\nin databases, pages 165–180. Springer.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings\nof the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nLuke Vilnis, Ishan Durugkar, Akshay Krishnamurthy,\nAlex Smola, and Andrew McCallum. 2018. Go for a\nwalk and arrive at the answer: Reasoning over paths\nin knowledge bases using reinforcement learning. In\n6th International Conference on Learning Represen-\ntations, ICLR 2018, Vancouver, BC, Canada, April\n30 - May 3, 2018, Conference Track Proceedings .\nOpenReview.net.\nJiankang Deng, Jia Guo, Evangelos Ververas, Irene\nKotsia, and Stefanos Zafeiriou. 2020. Retinaface:\nSingle-shot multi-level face localisation in the wild.\nIn 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2020, Seattle, WA,\nUSA, June 13-19, 2020, pages 5202–5211. IEEE.\nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos\nZafeiriou. 2019. Arcface: Additive angular margin\nloss for deep face recognition. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n381\n2019, Long Beach, CA, USA, June 16-20, 2019, pages\n4690–4699. Computer Vision Foundation / IEEE.\nJiale Han, Bo Cheng, and Xu Wang. 2020a. Open\ndomain question answering based on text enhanced\nknowledge graph with hyperedge infusion. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1475–1481, Online. Association\nfor Computational Linguistics.\nJiale Han, Bo Cheng, and Xu Wang. 2020b. Two-phase\nhypergraph based reasoning with dynamic relations\nfor multi-hop KBQA. In Proceedings of the Twenty-\nNinth International Joint Conference on Artificial\nIntelligence, IJCAI 2020, pages 3615–3621. ijcai.org.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 6700–6709. Computer Vision Founda-\ntion / IEEE.\nHaozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan\nZhu, and Minlie Huang. 2020. Language generation\nwith multi-hop reasoning on commonsense knowl-\nedge graph. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 725–736, Online. Association\nfor Computational Linguistics.\nEun-Sol Kim, Woo-Young Kang, Kyoung-Woon On,\nYu-Jung Heo, and Byoung-Tak Zhang. 2020. Hyper-\ngraph attention networks for multimodal learning. In\n2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 14569–14578. IEEE.\nJin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.\n2018. Bilinear attention networks. In Advances\nin Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018,\nMontréal, Canada, pages 1571–1581.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nQimai Li, Zhichao Han, and Xiao-Ming Wu. 2018.\nDeeper insights into graph convolutional networks\nfor semi-supervised learning. In Proceedings of the\nThirty-Second AAAI Conference on Artificial Intelli-\ngence, (AAAI-18), the 30th innovative Applications\nof Artificial Intelligence (IAAI-18), and the 8th AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 3538–3545. AAAI Press.\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and\nRichard S. Zemel. 2016. Gated graph sequence\nneural networks. In 4th International Conference\non Learning Representations, ICLR 2016, San Juan,\nPuerto Rico, May 2-4, 2016, Conference Track Pro-\nceedings.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. KagNet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2829–2839, Hong Kong,\nChina. Association for Computational Linguistics.\nHugo Liu and Push Singh. 2004. Conceptnet—a practi-\ncal commonsense reasoning tool-kit. BT technology\njournal, 22(4):211–226.\nZhibin Liu, Zheng-Yu Niu, Hua Wu, and Haifeng Wang.\n2019. Knowledge aware conversation generation\nwith explainable reasoning over augmented graphs.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1782–\n1792, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. OK-VQA: A visual\nquestion answering benchmark requiring external\nknowledge. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 3195–3204. Com-\nputer Vision Foundation / IEEE.\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1400–1409, Austin, Texas. Associ-\nation for Computational Linguistics.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. OpenDialKG: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845–854, Florence, Italy. Associa-\ntion for Computational Linguistics.\nMedhini Narasimhan, Svetlana Lazebnik, and Alexan-\nder G. Schwing. 2018. Out of the box: Reasoning\nwith graph convolution nets for factual visual ques-\ntion answering. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS\n2018, December 3-8, 2018, Montréal, Canada, pages\n2659–2670.\n382\nMedhini Narasimhan and Alexander G Schwing. 2018.\nStraight to the facts: Learning knowledge base re-\ntrieval for factual visual question answering. In Pro-\nceedings of the European conference on computer\nvision (ECCV), pages 451–468.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nYunqi Qiu, Yuanzhuo Wang, Xiaolong Jin, and Kun\nZhang. 2020. Stepwise reasoning for multi-relation\nquestion answering over knowledge graph with weak\nsupervision. In WSDM ’20: The Thirteenth ACM In-\nternational Conference on Web Search and Data Min-\ning, Houston, TX, USA, February 3-7, 2020, pages\n474–482. ACM.\nShailaja Keyur Sampat, Yezhou Yang, and Chitta Baral.\n2020. Visuo-linguistic question answering (VLQA)\nchallenge. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 4606–4616,\nOnline. Association for Computational Linguistics.\nApoorv Saxena, Aditay Tripathi, and Partha Talukdar.\n2020. Improving multi-hop question answering over\nknowledge graphs using knowledge base embeddings.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 4498–\n4507, Online. Association for Computational Lin-\nguistics.\nSanket Shah, Anand Mishra, Naganand Yadati, and\nPartha Pratim Talukdar. 2019. KVQA: knowledge-\naware visual question answering. In The Thirty-Third\nAAAI Conference on Artificial Intelligence, AAAI\n2019, The Thirty-First Innovative Applications of\nArtificial Intelligence Conference, IAAI 2019, The\nNinth AAAI Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019 , pages 8876–\n8884. AAAI Press.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks. In\nAdvances in Neural Information Processing Systems\n28: Annual Conference on Neural Information Pro-\ncessing Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada, pages 2440–2448.\nZequn Sun, Chengming Wang, Wei Hu, Muhao Chen,\nJian Dai, Wei Zhang, and Yuzhong Qu. 2020. Knowl-\nedge graph alignment network with gated multi-\nhop neighborhood aggregation. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 222–229.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Sys-\ntems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Mon-\ntreal, Quebec, Canada, pages 3104–3112.\nNiket Tandon, Gerard de Melo, Fabian M. Suchanek,\nand Gerhard Weikum. 2014. Webchild: harvesting\nand organizing commonsense knowledge from the\nweb. In Seventh ACM International Conference on\nWeb Search and Data Mining, WSDM 2014, New\nYork, NY, USA, February 24-28, 2014, pages 523–\n532. ACM.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ. Zico Kolter, Louis-Philippe Morency, and Rus-\nlan Salakhutdinov. 2019. Multimodal transformer\nfor unaligned multimodal language sequences. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6558–\n6569, Florence, Italy. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78–85.\nPeng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and\nAnton van den Hengel. 2018. Fvqa: Fact-based vi-\nsual question answering. IEEE transactions on pat-\ntern analysis and machine intelligence, 40(10):2413–\n2427.\nPeng Wang, Qi Wu, Chunhua Shen, Anthony R.\nDick, and Anton van den Hengel. 2017. Explicit\nknowledge-based reasoning for visual question an-\nswering. In Proceedings of the Twenty-Sixth Interna-\ntional Joint Conference on Artificial Intelligence, IJ-\nCAI 2017, Melbourne, Australia, August 19-25, 2017,\npages 1290–1296. ijcai.org.\nXu Wang, Shuai Zhao, Jiale Han, Bo Cheng, Hao Yang,\nJianchang Ao, and Zhenzi Li. 2020. Modelling\nlong-distance node relations for KBQA with global\ndynamic graph. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 2572–2582, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nJason Weston, Sumit Chopra, and Antoine Bordes. 2015.\nMemory networks. In 3rd International Conference\non Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceed-\nings.\nNaganand Yadati, Madhav Nimishakavi, Prateek Yadav,\nVikram Nitin, Anand Louis, and Partha P. Talukdar.\n2019. Hypergcn: A new method for training graph\nconvolutional networks on hypergraphs. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 1509–1520.\n383\nNaganand Yadati, Dayanidhi R S, Vaishnavi S, Indira\nK M, and Srinidhi G. 2021. Knowledge base ques-\ntion answering through recursive hypergraphs. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 448–454, Online. As-\nsociation for Computational Linguistics.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Visual\ncommonsense reasoning. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019, pages\n6720–6731. Computer Vision Foundation / IEEE.\nMantong Zhou, Minlie Huang, and Xiaoyan Zhu. 2018.\nAn interpretable reasoning network for multi-relation\nquestion answering. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 2010–2022, Santa Fe, New Mexico, USA. As-\nsociation for Computational Linguistics.\nYuke Zhu, Oliver Groth, Michael S. Bernstein, and\nLi Fei-Fei. 2016. Visual7w: Grounded question an-\nswering in images. In 2016 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV , USA, June 27-30, 2016, pages\n4995–5004. IEEE Computer Society.\nZihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu,\nand Qi Wu. 2020. Mucko: Multi-layer cross-modal\nknowledge reasoning for fact-based visual question\nanswering. In Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artificial Intelligence,\nIJCAI 2020, pages 1097–1103. ijcai.org.\n384\nAppendix. This supplementary material provides\nadditional information not described in the main\ntext due to the page limit. The contents of this\nappendix are as follows: In Section A, we show\nthe detailed statistics for the diverse splits of four\nbenchmark datasets, i.e., KVQA, FVQA, PQ and\nPQL. In Section B and C, we present the additional\nquantitative and qualitative analyses on KVQA and\nPQ datasets, respectively. In Section D, we describe\nthe experimental details for each dataset. In Section\nE, we depict the implementation details of compar-\native models for KVQA.\nA Data Statistics\nThe diverse split statistics for four benchmark\ndatasets, KVQA (Shah et al., 2019), FVQA (Wang\net al., 2018), PQ and PQL (Zhou et al., 2018), are\nshown in Table 4. Here, we highlight four aspects as\nfollows: 1) KVQA dataset covers the large number\nof entities (at least 5 times more) and knowledge\nfacts (at least 17 times more) than FVQA, PQ and\nPQL. 2) PQ and PQL datasets have annotations of\na ground-truth reasoning path to answer a given\nquestion. 2H and 3H denote the number of hops\n(i.e., 2-hop and 3-hop) in ground-truth reasoning\npaths. Also, M denotes a mixture of the 2H and 3H\nquestions. 3) PQL covers more knowledge facts\nincluding a large number of entities and relations\nthan PQ, but has fewer QA pairs. 4) PQL-3H has\na quite limited number of QA pairs (1,031). PQL-\n3H-More has twice more QA pairs (2,062) with the\nsame number of entities, relations, knowledge facts\nand answers as PQL-3H.\nB Additional Analysis on KVQA\nHere, we analyze more in-depth on KVQA dataset\nconcerning i) categories of question, and ii) types\nof answer selector. All models are under the same\nsetting of ORG+3-hop reported in Table 1.\nB.1 Analysis on question categories\nWe analyze QA performances over different ques-\ntion categories in Table 5. Hypergraph Transformer\nachieves the best accuracy in all categories ex-\ncept Multi-hop (slightly low at second-best). Our\nmodel shows notable strengths especially on com-\nplex problems such as Comparison, Multi-entity\nor Subtraction. To draw inferences for these ques-\ntion categories, the model needs to attend to mul-\ntiple knowledge facts related to a given question,\nand conducts multi-hop reasoning based on the\nfacts. Also, our model shows significant improve-\nment in spatial question compared to other models.\nWhereas spatial question is quite simple, it is re-\nquired to understand a correct spatial relationship\nbetween multiple entities in a given image. Ex-\namples of QA on diverse question categories are\ndepicted in Figure 4. Answers, inferred by five com-\nparative models and the proposed model, are pre-\nsented with corresponding image and question. The\nqualitative results indicate that our model draws\nreasonable inferences across diverse question cate-\ngories.\nB.2 Effect of similarity-based answer selector\nTo validate the impact of similarity-based answer\nselector, we replace the similarity-based answer se-\nlector (SIM) with a multi-layer perceptron (MLP).\nWe first note that KVQA dataset includes a large\nnumber of unique answers (19,360), and contains a\nlot of zero-shot and few-shot answers in test phase.\nAs shown in Table 6, the MLP fails to infer zero-\nshot answers which are not appeared in the training\nphase at all. Besides, the performance difference\nbetween SIM and MLP in one-shot answer (ap-\npeared in the only one time in training phase) is\nmore than 18%. The MLP uses 17% more parame-\nters than SIM because KVQA has a large number\nof answer candidates (19,360). When the number\nof candidate answers increases, the MLP needs\nmore parameters, but SIM does not. To sum up,\nthe similarity-based answer selector (SIM) con-\ntributes to infer few-shot and zero-shot answers in\nparameter-efficient manner.\nC Qualitative Analysis on PathQuestion\nFigure 5 shows the qualitative analysis of Hyper-\ngraph Transformer and Transformer (SA+GA) on\nPathQuestion. In Figure 5(a), Hypergraph Trans-\nformer attends to the second question hyperedge\n{the ⪯ ethnicity ⪯ of} and the fourth knowledge\nhyperedge {Alice Betty Stern ⪯ children ⪯\nOtto Frank ⪯ ethnicity ⪯ Germans} to reason\nbased on the multi-hop evidence about ethnicity.\nOn the other hand, Transformer (SA+GA) focuses\non the third question word ethnicity correctly, but\nattends to Otto Frank, Jew, Male with the high\nattention score 0.461, 0.242, and 0.204, not the\nexact knowledge entity, Germans. In Figure 5(b),\nboth model, Hypergraph Transformer and Trans-\nformer (SA+GA), fail to infer the correct answer.\nThe predicted answer of Hypergraph Transformer\n385\nKVQA FVQA PQ-2H PQ-3H PQ-M PQL-2H PQL-3H PQL-M\n# Entities 39,414 3,391 1,057 1,837 2,257 5,035 6,506 6,506\n# Relations 18 13 14 14 14 364 412 412\n# Knowledge facts 174,006 4,216 1,211 2,839 4,050 4,247 5,597 9,844\n# Words 63,164 6,663 1,180 1,929 2,407 5,505 7,001 7,034\n# QA pairs 183,007 5,826 1,908 5,198 7,106 1,594 1,031 2,625\n# Answers 19,360 500 305 1,009 1,107 380 292 438\n(*) PQL-3H-More has twice more QA pairs (2,062) with the same number of entities, relations, knowledge facts and answers as\nPQL-3H.\nTable 4: Statistics of four benchmark datasets: Knowledge-aware Visual Question Answering (KVQA), Fact-based\nVisual Question Answering (FVQA), PathQuestion (PQ) and PathQuestion-Large (PQL).\nBool Comp. Multi\nentity\nMulti\nhop\nMulti\nrelation 1-hop 1-hop\nsubtract Spatial Subtract.\nMemNN 75.1 50.5 43.5 53.2 45.2 61.0 - 48.1 40.5\nGCN 86.8 87.7 87.7 96.7 77.7 61.4 53.7 29.4 37.7\nGGNN 86.6 88.8 88.6 95.1 90.0 70.4 55.2 32.6 26.1\nHAN 98.1 93.8 93.6 98.2 92.8 73.5 51.5 29.6 29.0\nBAN 98.5 94.8 94.5 99.3 98.6 81.2 56.7 39.1 39.2\nOurs 99.1 96.9 96.8 99.2 99.3 89.9 73.3 90.1 42.4\nTable 5: Analysis of QA accuracy over different question categories of original (ORG) questions in oracle setting. All\nmodels use 3-hop graph reported in Table 1. Comp. and Subtract. are abbreviations of Comparison and Subtraction.\nThe best performance of each question type is highlighted in bold.\nis wrong even though it attends correctly to the\nfirst knowledge hyperedge {Wallace Reid ⪯\nspouse ⪯ Dorothy Davenport ⪯ parents ⪯\nHarry Davenport ⪯ cause of death ⪯\nMyocardial Infarction}. However, Transformer\n(SA+GA) attends to only the second and seventh\nword ( Dorothy Davenport ) and the fourth and\nninth word ( Harry Davenport ) in knowledge\nwith high attention score, not the answer entity,\nMyocardial Infarction. We consider that the reason\nwhy Hypergraph Transformer failed to infer the\ncorrect answer despite focusing on the exact\nknowledge fact is that the correct answer word\n(Myocardial Infarction) appears rarely in QA pairs.\nD Experimental details\nD.1 Knowledge-aware VQA\nWe follow the experimental settings suggested\nin (Shah et al., 2019). For entity linking, we ap-\nply well-known pre-trained models for face iden-\ntification: RetinaFace (Deng et al., 2020) for face\ndetection and ArcFace (Deng et al., 2019) for face\nfeature extraction. We first assign a name of the\ndetected faces with the label of the closest distance\ncompared to all of the face embeddings of 18,880\nnamed entities. In addition, we refine a list of de-\ntected named entities by matching the associated\nimage caption (i.e., Wikipedia caption). By doing\nso, we obtain the result of entity linking with top-1\nprecision 65.0% and top-1 recall 72.8%. QA per-\nformances in the entity linking setting on KVQA\nare shown in Table 7. Here, we note that BLSTM\nand MemNN of the first section in the table are\nbased on the different entity linking modules with\ntop-1 precision 81.1% and top-1 recall 82.2%1. It is\nmore accurate than ours around 9.4% in the recall\nmetric.\nD.2 Fact-based VQA\nWe follow the experimental settings suggested\nin (Wang et al., 2018). Following the paper, the\ndataset provides five splits of train and test data. We\nreport the average accuracy of five repeated runs\non different data split: 76.55 as top-1 accuracy (av-\nerage of 76.93, 75.92, 76.24, 76.16, and 77.50) and\n82.20 as top-3 accuracy (average of 82.90, 81.45,\n81.70, 81.74 and 83.20). The experimental results\nare shown in Table 8.\n1The code for the entity linking module has not been re-\nleased publicly. As such, we implement the module based on\nthe open-source: https://github.com/deepinsight/insightface.\nWe use the pre-trained model named retinaface-mnet025-v2\nand LResNet100E-IR,ArcFace@ms1m-refine-v2.\n386\nFigure 4: Qualitative results on KVQA dataset. GCN, GGNN, MemNN†, HAN, BAN and our model infer answers\nto a question about a given image. Green and red marks indicate correct and incorrect answers, respectively.\nOriginal (ORG) Paraphrased (PRP)\nZero-shot One-shot Multi-shot ALL Zero-shot One-shot Multi-shot ALL\nMLP 0.0 78.3 87.2 76.0 0.0 76.9 86.8 75.6\nSIM 93.9 96.7 90.1 91.0 92.4 96.3 89.9 90.7\nTable 6: Analysis for answer selector with the frequency of answers in the test split. SIM and MLP represent\nsimilarity-based answer selector and multi-layer perceptron.\nModel ORG PRP Mean\nBLSTM 48.0 27.2 37.6\nMemNN 50.2 34.2 42.2\nGCN 48.9 48.2 48.5\nGGNN 50.9 50.9 50.9\nMemNN† 54.0 53.9 54.0\nHAN 53.4 53.3 53.3\nBAN 59.6 60.0 59.8\nTransformer (SA) 57.5 58.9 58.3\nTransformer (SA+GA) 60.4 59.8 60.1\nOurs 62.0 62.8 62.4\nTable 7: QA accuracy on entity linking setting in KVQA.\nThe performances of BLSTM and MemNN are reported\nin (Shah et al., 2019).\nAccuracy\n@1 @3\nHuman 77.99 -\nLSTM-Q+I (Pre-VQA) 24.98 30.30\nHie-Q+I (Pre-VQA) 43.14 59.44\nFVQA-Top3-QQmaping 56.91 64.65\nSTTF-Q+VConcept 62.20 75.60\nRC (pre-SQuAD) 62.94 70.08\nOut of the Box 69.35 80.25\nMucko 73.06 85.94\nOurs 76.55 82.20\nTable 8: Accuracy on Fact-based Visual Question An-\nswering (FVQA). Top-1 and top-3 accuracy are used as\nevaluation metrics.\n387\nFigure 5: Qualitative analysis on effectiveness of using hypergraph as input format to Transformer architecture. Here,\nwe visualize attention maps (Attention(Qk, Kq, Vq) and Attention(Qq, Kk, Vk)) for Hypergraph Transformer and\nthe Transformer (SA+GA). All attention scores are averaged over multi-heads and multi-layers. Each x and y axis\nrepresent indices of question and knowledge hyperedges in Hypergraph Transformer, and indices of question and\nknowledge word in Transformer (SA+GA). In the attention maps, the dark colors represent high values. We also\nvisualize the top-3 attended knowledge hyperedges in Hypergraph Transformer, and top-3 attended knowledge fact\nin Transformer (SA+GA) with the attention score.\nD.3 PathQuestion and PathQuestion-Large\nWe follow the same experimental settings sug-\ngested in (Zhou et al., 2018). Following the paper,\nwe split the dataset into train, validation, and test\nsets with a proportion of 8:1:1, and report the aver-\nage accuracy of five repeated runs on different data\nsplit.\nE Implementation Details of\nComparative Models for KVQA\nFor comparative models for KVQA, three kinds\nof methods are considered, which are graph-based,\nmemory-based and attention-based networks.\nGraph-based networks. Graph convolutional\nnetworks (GCN) (Kipf and Welling, 2017) and\ngated graph neural networks (GGNN) (Li et al.,\n2016) are representative models of graph-based\nneural networks. Both learn node representations\nof a knowledge and question graph (not a hyper-\ngraph), propagating information between neighbor-\nhoods. After propagation, node representations in\na graph are aggregated to encode a graph-level rep-\nresentation. Joint representation is obtained based\non the two graph representations.\nMemory-based networks. Memory network\n(MemNN) (Weston et al., 2015) is a de facto base-\nline for fact-based question answering. Each fact\nis embedded into a memory slot, and soft attention\n388\nis calculated between memory slots and a given\nquestion. Joint representation is obtained based on\nthe attention.\nAttention-based networks. Bilinear attention\nnetworks (BAN) (Kim et al., 2018) and hypergraph\nattention networks (HAN) (Kim et al., 2020) con-\nsider interactions between knowledge and question\nbased on co-attention mechanism. BAN calculates\nsoft attention scores between knowledge entities\nand question words. Meanwhile, HAN employs\nstochastic graph walk in a knowledge and ques-\ntion graph to encode high-order semantics (e.g.,\nknowledge facts and question phrases), and consid-\ners attention scores between knowledge facts and\nquestion phrases. Joint representation is obtained\nbased on the attention as well. The more implemen-\ntation details of the above comparative models is\ndescribed as follows.\nE.1 Graph convolutional networks\nThe knowledge and question graph are encoded\nseparately by two graph convolutional networks\n(GCN) (Kipf and Welling, 2017). Each GCN model\nconsists of two propagation layers and a sum pool-\ning layer across the nodes in the graph. The op-\neration of the propagation layer is as follows:\nf(H(l), A) = σ( ˆD−1\n2 ˆA ˆD−1\n2 H(l)W(l)) where\nˆA = A + I, A is an adjacency matrix of the graph,\nI is an identity matrix, D is a degree matrix of A,\nW(l) is the model parameters ofl-th layer, andH(l)\nis the representations of the graph in the l-th layer.\nHere, H(0) is the word embeddings of each entity\nin the knowledge and question graph. After propa-\ngation and aggregation phase, the knowledge and\nquestion graph representations are obtained. Then,\nthe two graph representations are concatenated and\nfed into a single layer feed-forward layer to get\njoint representation.\nE.2 Gated graph neural networks\nAs the same as graph convolutional networks,\nthe knowledge and question graph are encoded\nseparately by two gated graph neural networks\n(GGNN). Each GGNN model consists of three\ngated recurrent propagation layers and a graph-\nlevel aggregator. Motivated by Gated Recurrent\nUnits (Cho et al., 2014), GGNN adopts a update\ngate and a reset gate to renew each node’s hid-\nden state. The detailed equation of gated recur-\nrent propagation is as follows: h(1)\nv = [xT\nv , 0]T\nwhere xv is the v-th word embedding of each en-\ntity in the knowledge and question graph, a(t)\nv =\nAT\nv: [h(t−1)T\n1 · ··h(t−1)T\n|V| ]T + b where the matrix\nA determines how nodes in the graph communi-\ncate each other and b is a bias vector. Then, the\nupdate gate and reset gate are computed as follows:\nzt\nv = σ(Wza(t)\nv + Uzh(t−1)\nv ), rt\nv = σ(Wra(t)\nv +\nUrh(t−1)\nv ) where σ is a logistic sigmoid function,\nand W[·] and U[·] are learnable parameters. Finally,\nthe hidden states of nodes in the given graph are\nupdates as h(t)\nv = (1− zt\nv) ⊙ h(t−1)\nv + zt\nv ⊙ ˜h(t)\nv\nwhere ˜h(t)\nv = tanh(Wha(t)\nv + Uh(rt\nv ⊙ h(t−1)\nv ).\nAfter the propagation phase, the nodes in the\ngraph are aggregated to a graph-level represen-\ntation as hG = tanh( P\nv∈V σ(i(h(T)\nv , xv)) ⊙\ntanh(j(h(T)\nv , xv)) where i and j are a single layer\nfeed-forward layer, respectively. Then, the two ag-\ngregated graph representations are concatenated\nand fed into another single layer feed-forward layer\nto get joint representation of question and knowl-\nedge graph.\nE.3 Memory networks\nWe reproduce end-to-end memory net-\nworks (Sukhbaatar et al., 2015) proposed as\na baseline model in (Shah et al., 2019). First,\nwe use Bag-of-words (BoW) representation for\nknowledge facts and a question. The soft attention\nover the knowledge facts and the given question\nis computed as follows: pij = softmax(qT\ni−1mij)\nwhere m is the embeddings of knowledge facts,i is\na number of layer and j is an index of knowledge\nfacts. The output representation of i-th layer is\nOi = P\nj pijoij where o is the another embeddings\nof knowledge facts different from m. The updated\nquestion representation is qk+1 = Ok+1 + qk, and\nbased on the output representation and question\nrepresentation, answer is predicted as follows:\nˆa = softmax(f(OK + qK−1)) where f is a single\nlayer feed-forward layer. Here, we set up the model\nas three layers with adjacent and layer-wise weight\ntying.\nE.4 Bilinear attention networks\nBilinear attention networks exploit a multi-head\nco-attention mechanism between knowledge and\nquestion. BAN calculates soft attention scores be-\ntween knowledge entities and question words as\nfollows: A = softmax(Wh◦(MqWq)(MkWk)⊤)\nwhere Mq, Mk are a row-wise concatenated ques-\ntion words and knowledge entities, W[·] is learn-\n389\nable matrices, and ◦ is element-wise multiplication.\nBased on the attention map A, the joint feature is\nobtained as follows: zi = (MqWq)i\n⊤A(MkWk)i\nwhere the subscript i denotes the i-th index of\ncolumn vectors in each matrix. For multi-head at-\ntention, the attended outputs with different heads\nare concatenated and fed into a single layer feed-\nforward layer to make a final representation. Here,\nwe use four attention heads as multi-head.\nE.5 Hypergraph attention networks\nThe model architecture and detailed operation of\nhypergraph attention networks are similar to that\nof BAN. The difference between BAN and HAN\nis the abstraction level of the input. For HAN, the\nhyperedges sampled by stochastic graph walk are\nfed into the co-attention mechanism. What HAN\nand our model have in common is introducing a\nhypergraph to consider high-order relationships in\nquestion graph and knowledge graph. Both models\nshare the similar motivation, but the core opera-\ntions are quite different. Especially, HAN employs\nstochastic graph walk to construct question and\nknowledge hypergraph. Due to the randomness of\nthe stochasticity, misinformed or incomplete hyper-\nedges can be extracted.\nE.6 Transformer Variants\nThe model architectures of Transformer (SA) and\nTransformer (SA+GA) presented in this paper are\nthe same as Hypergraph Transformer. The only\ndifference is the abstraction level of input. The\nTransformer (SA) and Transformer (SA+GA) take\nsingle-word-unit as input tokens, and Hypergraph\nTransformer takes hyperedges as input tokens. Fol-\nlowing (Vaswani et al., 2017; Tsai et al., 2019), we\napply positional embeddings to the input sequence\nof both models. We stack two guided-attention\nblocks and three self-attention blocks, respectively.\nEach attention block has multi-head attention with\nfour attention heads followed by layer normaliza-\ntion, residual connections and a single multi-layer\nperceptron. We set the dropout applied on the token\nembedding weights, query and key-value embed-\nding weights, attention weights and residual con-\nnections from 0.05 to 0.2. We minimize negative\nlog-likelihood using Adam optimizer (Kingma and\nBa, 2015) with an initial learning rate from 1e − 4\nto 1e − 5 with batch size from 128 to 256. All\ntransformer variant models described in this paper\nhave the same fixed-number of sequence length as\nfollows: 300 for 1-hop, 1,000 for 2-hop and 1,800\nfor 3-hop graphs.\n390",
  "topic": "Hypergraph",
  "concepts": [
    {
      "name": "Hypergraph",
      "score": 0.9252835512161255
    },
    {
      "name": "Computer science",
      "score": 0.7410932183265686
    },
    {
      "name": "Question answering",
      "score": 0.7210270762443542
    },
    {
      "name": "Knowledge base",
      "score": 0.5698744058609009
    },
    {
      "name": "ENCODE",
      "score": 0.45652735233306885
    },
    {
      "name": "Information retrieval",
      "score": 0.4312503933906555
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4147588014602661
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.4112875163555145
    },
    {
      "name": "Transformer",
      "score": 0.4109008014202118
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39754000306129456
    },
    {
      "name": "Natural language processing",
      "score": 0.37234628200531006
    },
    {
      "name": "Mathematics",
      "score": 0.13499081134796143
    },
    {
      "name": "Discrete mathematics",
      "score": 0.08016088604927063
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I4575257",
      "name": "Hanyang University",
      "country": "KR"
    }
  ]
}