{
    "title": "Towards a Language Model for Temporal Commonsense Reasoning",
    "url": "https://openalex.org/W3216327107",
    "year": 2021,
    "authors": [
        {
            "id": null,
            "name": "Ochanomizu University, Japan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2128917267",
            "name": "Mayuko Kimura",
            "affiliations": [
                "Ochanomizu University"
            ]
        },
        {
            "id": "https://openalex.org/A4316237531",
            "name": "Lis Kanashiro Pereira",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Ochanomizu University, Japan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1982119413",
            "name": "Ichiro Kobayashi",
            "affiliations": [
                "Ochanomizu University"
            ]
        },
        {
            "id": null,
            "name": "Ochanomizu University, Japan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963159690",
        "https://openalex.org/W4287813862",
        "https://openalex.org/W2970780738",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2963797084",
        "https://openalex.org/W2971236147",
        "https://openalex.org/W3113303810",
        "https://openalex.org/W4322614701",
        "https://openalex.org/W1967936132",
        "https://openalex.org/W3034602344",
        "https://openalex.org/W2157275230",
        "https://openalex.org/W3035507081",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2945290257"
    ],
    "abstract": "Temporal commonsense reasoning is a challenging task as it requires temporal knowledge usually not explicitly stated in text.In this work, we propose an ensemble model for temporal commonsense reasoning.Our model relies on pre-trained contextual representations from transformer-based language models (i.e., BERT), and on a variety of training methods for enhancing model generalization: 1) multistep fine-tuning using carefully selected auxiliary tasks and datasets, and 2) a specifically designed temporal task-adaptive pre-trainig task aimed to capture temporal commonsense knowledge.Our model greatly outperforms the standard fine-tuning approach and strong baselines on the MC-TACO dataset.",
    "full_text": null
}