{
  "title": "Open-ended Long Text Generation via Masked Language Modeling",
  "url": "https://openalex.org/W4385571101",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2123592475",
      "name": "Xiaobo Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2990396704",
      "name": "Zecheng Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126452540",
      "name": "Juntao Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3126267552",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W4287327373",
    "https://openalex.org/W3035289598",
    "https://openalex.org/W4285241415",
    "https://openalex.org/W3098295156",
    "https://openalex.org/W3115582371",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2970574558",
    "https://openalex.org/W4285180072",
    "https://openalex.org/W3102401511",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2115613106",
    "https://openalex.org/W3037337776",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W3000840023",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4377079846",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3174264667",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3169714379",
    "https://openalex.org/W2963248348",
    "https://openalex.org/W2130942839"
  ],
  "abstract": "Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated OPen-ended Long Text Generation (Open-LTG).However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.To improve inference efficiency, we alternatively explore the potential of the pre-trained masked language models (MLMs) along with a representative iterative non-autoregressive (NAR) decoding strategy for Open-LTG.Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling.To enhance the long text generation capability of MLMs, we introduce two simple yet effective strategies for the iterative NAR model: dynamic sliding window attention (DSWA) and linear temperature decay (LTD). It can alleviate long-distance collapse problems and achieve longer text generation with a flexible trade-off between performance and inference speedup.Experiments on the storytelling and multi-paragraph opinionated article writing tasks show that pre-trained MLMs can achieve more than 3 × → 13 × speedup with better performance than strong AR models.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 223–241\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nOpen-ended Long Text Generation via Masked Language Modeling\nXiaobo Liang∗ Zecheng Tang∗ Juntao Li† Min Zhang\nSoochow University\n{xbliang3, zctang}@stu.suda.edu.cn,\n{ljt,minzhang}@suda.edu.cn\nAbstract\nPre-trained autoregressive (AR) language mod-\nels such as BART and GPTs have dominated\nOpen-ended Long Text Generation (Open-\nLTG). However, the AR nature will decrease\nthe inference efficiency along with the increase\nof generation length, which hinder their ap-\nplication in Open-LTG. To improve inference\nefficiency, we alternatively explore the poten-\ntial of the pre-trained masked language models\n(MLMs) along with a representative iterative\nnon-autoregressive (NAR) decoding strategy\nfor Open-LTG. Our preliminary study shows\nthat pre-trained MLMs can merely generate\nshort text and will collapse for long text mod-\neling. To enhance the long text generation\ncapability of MLMs, we introduce two sim-\nple yet effective strategies for the iterative\nNAR model: dynamic sliding window attention\n(DSWA) and linear temperature decay (LTD).\nIt can alleviate long-distance collapse problems\nand achieve longer text generation with a flex-\nible trade-off between performance and infer-\nence speedup. Experiments on the storytelling\nand multi-paragraph opinionated article writing\ntasks show that pre-trained MLMs can achieve\nmore than 3 ×→ 13 ×speedup with better\nperformance than strong AR models. Our code\nis available at GitHub*.\n1 Introduction\nPre-trained language models (PLMs) like\nBART (Lewis et al., 2020) and GPTs (Radford\net al.; Radford et al.; Brown et al., 2020) have\nachieved remarkable progress in Open-LTG.\nThrough modeling languages from left to right,\nthey can autoregressively “create” fluent and gram-\nmatical content. With the further enhancement of\nplanning strategies (Hua and Wang, 2020; Hu et al.,\n2022) or high-level representation learning (Guan\n∗Equal Contribution\n†Corresponding Author\n*https://github.com/dropreg/OpenLTG-\nMLM\nModel Type Iter Tokens/s\nBART base AR - 151.3\nBART base + Planning † AR - 5.8\nBERT-CRF† NAR 0 2,597.4\nRoBERTa base NAR 0 1,561.2\n1 1,068.9\n4 505.2\nTable 1: Inference speed of each model with a single\nGPU (NVIDIA A10040GB). For a fair comparison, we\nforce all models to generate 200 tokens. The models\nlabeled with †are implemented with the Hugging Face\nplatform, while the rest are implemented with Fairseq.\net al., 2021a), pre-trained AR language models can\nachieve promising Open-LTG. However, the low\ninference efficiency of AR impedes their usability\nin real-world applications. Table 1 presents the\ninference speed of a few typical AR language\nmodels. We can see that BART (Lewis et al., 2020)\nrequires at least 1.3 seconds to generate a story\nwith 200 tokens on the powerful NVIDIA A100\nGPU, and extra planning (Hua and Wang, 2020)\ncan make the inference process even slower (more\nthan 30 seconds to create a 200-tokens story). In\ngreat contrast with AR models, NAR models (e.g.,\nBERT-CRF (Su et al., 2021)) can generate more\nthan 12 stories with the same length within one\nsecond, but their effectiveness in open-ended long\ntext generation has not been proven yet.\nThe high inference efficiency of NAR models is\nat the sacrifice of output dependency modeling, in\nwhich each generation is executed in parallel (Xiao\net al., 2022). Thus, NAR models are mainly ex-\nplored and utilized for text generation tasks with\nadequate input information to predict each output\ntoken of different positions and extra correlations\nto constrain the generation process, e.g., neural\nmachine translation (Gu et al., 2018; Huang et al.,\n2022), summarization (Qi et al., 2021; Agrawal and\nCarpuat, 2022), sentence compression (Su et al.,\n2021), dialogue generation (Zou et al., 2021), and\nconstrained story-ending generation (Yang et al.,\n223\n2021). To the best of our knowledge, none of the\nexisting research explores Open-LTG with NAR\nmodels, particularly based on pre-trained MLMs.\nWe fill this gap by first conducting a prelim-\ninary study to calibrate the potential and limita-\ntions of a pre-trained MLM, i.e., RoBERTa (Liu\net al., 2019)†, on two story generation corpora, i.e.,\nROCStories (ROC) (Mostafazadeh et al., 2016) and\nWritingPrompts (WP) (Fan et al., 2018). To achieve\nconditional generation, we simply use RoBERTa as\nboth the encoder and the decoder with mixed atten-\ntion (He et al., 2018) to achieve encoder-decoder\ncross-attention. Through experiments, we found\nthat: (1) pre-trained MLMs can achieve competi-\ntive performance in the iterative NAR fashion for\nopen-ended short text generation (e.g., a paragraph\nwith around 40 tokens), (2) pre-trained MLMs fail\nto model Open-LTG (with about 140 tokens on av-\nerage), which will generate uninformative content\nwith high-frequency and repeated tokens (e.g., “.”\nand “,”). Furthermore, we offer three possible rea-\nsons for the attention mechanism of MLMs and\ninference strategy to explain the collapse of the\niterative NAR model based on pre-trained MLMs\nfor the Open-LTG scenario.\nInspired by the above observations, we introduce\ntwo improvement strategies: Dynamic Sliding Win-\ndow Attention (DSW A) and linear temperature de-\ncay strategy (LTD) to maintain more informative\ncontext content in the iterative NAR generation. As\na result, iterative NAR models based on pre-trained\nMLMs can achieve much longer text generation\nthan the vanilla setting. Experiments on two Open-\nLTG tasks (i.e., storytelling and multi-paragraph\nopinionated article writing) with four widely-used\ndatasets demonstrate that the pre-trained MLM can\nachieve better performance (BLEU score, ROUGE\nscore, BERT score, and Perplexity) than multi-\nple strong AR models without extra post-training,\nstructure modification, or using more model param-\neters. Importantly, our approach can speed up the\ninference process due to non-autoregressive prop-\nerties, making the pre-trained MLM as a promis-\ning candidate for the Open-LTG community. The\nRoBERTa base achieves more than 3 ×→ 13 ×\nwith better performance to the competitive BART.\n†MLMs can achieve iterative NAR generation with the\nmask-predict inference strategy (Ghazvininejad et al., 2019).\n2 Related Work\nLong Text Generation Text generation tasks can\nbe classified into two categories: directed genera-\ntion and open-end generation. The directed genera-\ntion (Sutskever et al., 2014; Li et al., 2015; Vaswani\net al., 2017) for long text scenarios has long source\nthan the target, which is also constrained by source\nsequence, e.g., neural machine translation and sum-\nmarization. These tasks aim to solve the quadratic\ngrowth requirement of the memory and computa-\ntional of the self-attention mechanism. The open-\nended generation task (Guo et al., 2018; Tan et al.,\n2020; Goldfarb-Tarrant et al., 2020; Hua and Wang,\n2020; Orbach and Goldberg, 2020; Hu et al., 2022)\ndesire to generate more freedom content and has\nrecently become a promising research direction.\nPrevious works have explored multiple generation\nstrategies to generate high-quality and fluent text,\ne.g., planning then generating (Guo et al., 2018;\nTan et al., 2020; Goldfarb-Tarrant et al., 2020;\nHua and Wang, 2020; Orbach and Goldberg, 2020;\nHu et al., 2022) and introducing external knowl-\nedge (Guan et al., 2020; Xu et al., 2020). Although\nthe above strategies enable the model to achieve\nsignificant advances, time-consuming is still a crit-\nical issue that hinders their usage in real-world\napplications (Guan et al., 2021a; Tan et al., 2020).\nIterative Non-autoregressive Generation Non-\nautoregressive (NAR) model breaks the sequential\ndependencies from front to back for parallel text\ngeneration (Gu et al., 2018; Guo et al., 2020; Sa-\nharia et al., 2020). Furthermore, the iterative-based\nNAR model (Lee et al., 2018; Gu et al., 2019;\nChi et al., 2021) can achieve comparable perfor-\nmance with the AR model. The typical CMLM\nmodel (Ghazvininejad et al., 2019) can generate\nfluent results conditioned on the predictions from\nthe previous iteration instead of previous tokens:\nP(Yt|X) = P(Yt|Yt−1,X) (1)\nBenefiting from this, the iterative NAR model is\nmore flexibly compared with the AR model, which\ncan easily generate consistent and controllable text\nfor each iteration step. To the best of our knowl-\nedge, the iterative NAR model has never been used\nto solve open-ended generation. Especially, we in-\nvestigate its usability for the long text scenario, i.e.,\ntarget lengths between 100 and 400, which is still\nunder-explored in the directed generation tasks.\n224\nFigure 1: The overview of MLM for text generation.\n(We concatenate the hidden states of Xand Yas the\nkey and value of the mixed-attention mechanism.)\n3 Preliminary Study\nWe first present the training and inference paradigm\nof utilizing the pre-trained MLMs for Open-LTG\n(§ 3.1), e.g., BERT or RoBERTa. Then, we study\nthe significant collapse problem in a long text gen-\neration scenario by conducting preliminary experi-\nments on two datasets with different target lengths\n(§ 3.2). Finally, we investigate the reason for the\nabove issues with an exhaustive case study and\nexploration tests to motivate our method design\n(§ 3.3), where the model can generate text in non-\nautoregressive manner to speed up the inference.\n3.1 Text Generation via Pre-trained MLMs\nPre-trained MLMs are typically used as the encoder\nto extract the representations of sentences instead\nof generating texts. Previous works (Dong et al.,\n2019; Wang et al., 2019) have indicated that the\nMLM encoder can support text generation tasks via\nattention masks or Gibbs sampling. In contrast, we\nintroduce mixed attention and parameter sharing to\nthe encoder-based model to solve the sequence to\nsequence tasks, as shown in Figure 1.\nModel Training Given the parallel text gener-\nation dataset D={(X,Y)}|D|, we can feed the\nsource Xinto the MLM encoder to obtain the rep-\nresentation Hl\nsrc of l-th layer. Concretely, each\nlayer comprises two sub-layers, including one self-\nattention layer and one feed-forward layer:\n¯Hl\nsrc = Self-ATTN(Hl−1\nsrc ) + Hl−1\nsrc\nHl\nsrc = FFN( ¯Hl\nsrc) + ¯Hl\nsrc.\n(2)\nThen, we random mask Y= {y1,y2,··· ,y|Y|}to\nobtain corrupted target YM = {y1,m2,··· ,m|Y|}\n(mis the symbol of mask token “<mask>”). As be-\nfore, we can obtain the representationHl\ntgt by using\nthe shared parameter MLM encoder and then try\nto recover the masked sequence, where the mixed-\nattention mechanism (He et al., 2018) is applied to\naggregate the source HL\nsrc and the target Hl\ntgt:\n¯Hl\ntgt = Mixed-ATTN(Hl−1\ntgt ,HL\nsrc) + Hl−1\ntgt\nHl\ntgt = FFN( ¯Hl\ntgt) + ¯Hl\ntgt.\n(3)\nMixed-attention does not break the original atten-\ntion mechanism, which only utilizes the target hid-\nden states as query vector and the concatenated\nvector of source and target hidden states as key\nand value. It is worth noting that this approach is\navailable for transformer encoder models without\nadditional parameters.\nSpecifically, we uniformly mask 1 to n(target\nlength) tokens fromYfor model training. The train-\ning objective is thus to minimize the conditional\nMLM loss like the pre-training stage:\nLMLM = −\nM∑\ni=1\nlog P(yi|X,YM)\nP(yj|X,YM) = exp(utgt/T)∑\n|u′\ntgt|exp(u\n′\ntgt/T),\n(4)\nwhere Mis the number of masked tokens, utgt\nis the output logit, and T is the temperature to\nre-estimate the final probability.\nModel Inference We use an iterative refinement\nstrategy to generate text like CMLM (Ghazvinine-\njad et al., 2019). In particular, We use the fully\nmasked sequence {m1,m2,··· ,mn}to initialize\nthe target sequence and predict all masked tokens\nat the first step. Then, we iteratively regenerate the\nlow-confidence tokens at the subsequent iteration\nsteps to obtain better performance. For Open-LTG,\nwe utilize the nucleus sampling (Holtzman et al.,\n2019) decoding strategy instead of beam search.\nLength Prediction It is necessary to obtain the\ntarget length to initialize the full mask sequence\nas model input before inference. Specifically, we\nprovide two strategies: 1) Fixed Length, which\ninitializes the target length according to the average\nlength of the validation set or human experience. 2)\nPrediction Module, which uses the mean-pooling\nlayer followed by one classification layer to predict\nthe target length by feeding HL\nsrc into them:\nP(Ltgt|X) =Softmax(WL(Mean-Pooling(HL\nsrc))), (5)\nwhere Ltgt is the target length, and WL is the learn-\nable parameter. Specifically, we will adjust Ltgt\naccording to the specific offset, which is the param-\neter based on the validation dataset.\n225\nFigure 2: The iterative inference process of typical good and bad cases, randomly sampled from ROC and WP. The\nhistogram refers to the output distributions (Iter=1) across candidate tokens for a randomly picked position.\nData Model B-1 B-2 R-1 R-2 Dist Rep\nROC BART 30.06 14.37 22.37 2.42 3.93 79.07\nRoBERTa30.89 14.36 25.01 3.48 5.24 73.42\nWP BART 29.69 10.26 24.34 2.20 0.47 90.15\nRoBERTa15.80 5.21 10.08 0.84 8.48 17.08\nTable 2: The performance on WP and ROC.\n3.2 Extensive Trials\nStudy Settings We use Writing Prompt (WP)\nand ROC Stories (ROC) datasets to conduct exper-\niments for validating whether pre-trained MLMs\ncan work better on Open-LTG tasks. In particular,\nthese two datasets have different lengths for target\nsentences, i.e., the average length of WP is 140 and\nROC is 40, and more details are given in Section 5\nand Appendix A. We choose RoBERTa base (Liu\net al., 2019) as our backbone model and use BLEU,\nROUGE, Distinct, and Lexical Repetition metrics\nfor evaluation. During inference, we set nucleus\nsampling hyper-parameter top-p=0.9, temperature\nT=1.0, and limit the maximum iteration steps to 6\nfor ROC and 8 for WP.\nResults As shown in Table 2, For the ROC\ndataset, the RoBERTa base model obtains com-\nparable performance with BART. However, the\ngeneration quality significantly decreases for the\nWP dataset, which involves much longer targets.\nSpecifically, most of the generated results are made\nup of duplicated function words or punctuations,\ne.g., “it”, “to”, “the”, and “.”, etc, which makes the\nmodel outputs unreadable and meaningless. One\nintuitive question isWhat causes the collapse prob-\nlem in Open-LTG when using pre-trained MLMs?\n3.3 Analysis and Possible Improvements\nWe show typical good case and bad case in Fig-\nure 2, which are randomly selected from the ROC\nand WP datasets respectively to demonstrate the\ngeneration process. For each iterative refinement\nstep of bad case, the informative tokens will be re-\nplaced by the placeholder token “<mask>” and are\nreplaced by the function words at the subsequent\nsteps. Thus it is unable to generate fluent results\nlike good case. According to this observation, we\ntry to provide some possible explanations for the\naforementioned collapse issues:\n1) The most intuitive reason is that the function\nwords are often located at the front of the output\ndistribution, which dominates the high probability\nregion, causing the informative tokens hard to be\nsampled.. The output distribution trained with the\nROC dataset contains more prompt-related tokens\nthan WP, e.g., the “swim” and “water” in the top\n50 candidates of ROC output, as shown in Figure 2\n(distribution histogram). Worse still, the function\nwords dominate the high probability regions (from\n35% to 45%) for the bad case and lead to terrible\ninitialization at the first iteration step.\n2) The iterative refinement mechanism depends\non the token confidence of generated sequences,\nand it is easier for the low-confidence but infor-\n226\nData RecurrentB-1 B-2 R-1 R-2 Dist Rep\nWP\n1 15.80 5.21 10.08 0.84 17.08 94.25\n2 22.42 8.70 16.81 2.14 34.82 83.87\n4 26.91 10.67 21.32 2.81 50.32 35.93\nTable 3: The performance of different recurrent steps.\nmative tokens to be masked. In fact, the iterative\nrefinement mechanism is designed for directed gen-\neration tasks, e.g., neural machine translation or\nsummarization, which usually apply theargmax op-\neration to sample results, and the evaluation of con-\nfidence is reasonable in different iterations. Never-\ntheless, we use the nucleus sampling strategy for\ninference in Open-LTG, which leads to the low-\nconfidence tokens with high priority being masked.\n3) The massive absent context tokens suffer a\nmore serious multi-modality problem on long text\ngeneration in early iteration steps. As a result, the\nmodel is inclined to generate duplicated tokens due\nto the multi-modal output distribution. Although it-\nerative refinement can provide additional context to\nalleviate this issue, the model still cannot generate\nthe expected results. The possible explanationis\nthat the self-attention layer needs the context token\nas key-value pairs to calculate the token represen-\ntation. Unfortunately, the massive uninformative\nmask tokens (“<mask>”) in context lead to model\ncollapse steadily worsening in the following itera-\ntion steps. Thus, we utilize the recurrent generation\nmechanism for model training and inference to re-\nduce the context dependency, which can also flexi-\nbly control the maximum length of the generated\nsequence (please refer to the Appendix B for more\ndetails about the model architectures and experi-\nments). The results are shown in Table 3. We can\nobserve that the model can gradually improve its\nperformance as the recurrent steps increase, demon-\nstrating that informative context dependency is the\nimplicit reason for the model collapse.\nImprovements Based on the above analysis and\nfindings, we categorize these critical factors into\ntwo types: the defects of attention mechanism\nand inappropriate inference strategies. In partic-\nular, we believe that each token should not pays\nattention to all context information, and most to-\nkens only need the neighbor tokens’ information to\nrepresent the hidden states and predict the results.\nTherefore, we will change the self-attention mech-\nanism of the pre-trained MLMs so that each tokens\ncan attend to the restricted neighbors. Besides,\nFigure 3: The overview of sliding window attention.\nwe will adjust the confidence score of the output\ndistributions to keep the informative tokens in sub-\nsequent iteration steps instead of being masked.\n4 Method\nIn this section, we propose two simple yet effective\nstrategies for attention mechanism and inference\nto mitigate the model collapse problems: Dynamic\nSliding Window Attention (DSWA) and Linear\nTemperature Decay (LTD). These designs do not\nbreak the paradigm of MLM so that it can flexibly\nadapt to the pre-trained models.\n4.1 Dynamic Sliding Window Attention\nWe first introduced the sliding window mecha-\nnism (Beltagy et al., 2020) for the self-attention\nlayer to adjust each token’s attention pattern, which\nalso ensures that the top layer’s token representa-\ntions can have a large receptive field, similar to\nCNN (Wu et al., 2018). Figure 3 illustrates the\nattention mask of the mixed attention layer of pre-\ntrained MLMs. It is worth noting that the key-value\npairs consist of two parts: the source representa-\ntion of the last layer (with green background) and\nthe target representation of the current layer (with\nyellow background):\n¯Hl\ntgt = Mixed-ATTN(Win(Hl−1\ntgt ),HL\nsrc) + Hl−1\ntgt\nHl\ntgt = FFN( ¯Hl\ntgt) + ¯Hl\ntgt,\n(6)\nwhere the operation Win(◦) employs a fixed-size\nwindow to select the neighbor token representa-\ntions. Meanwhile, the query can attend all source\nsequence hidden states and the target sequence hid-\nden states in the window, stemming the impact of\nmassive absent context.\n227\nFigure 4: Re-estimate the output distribution by LTD.\nDynamic Schedule Intuitively, it is not essential\nto use a fixed receptive field for each layer, e.g., the\ntop layer may need to reduce the receptive field to\nperform prediction. Thus, we propose a dynamic\nschedule strategy for the inference stage to adjust\nthe window size Swin of each layer:\nSwin = max(αmin,L−i\nL ∗αmax) ∗Sfix, (7)\nwhere iis the current layer number, Lis the max\nlayer number of pre-trained MLM encoder, Sfix is\nthe fixed window size for model training, and the\nαmin and αmax is the lower and upper bound of\ncoefficient hyper-parameter selected from [0,1].\nWith this strategy, we can alleviate the multi-\nmodality problem by restricting the model to attend\nto the tokens in the window instead of the whole\nsequence, thus degenerating the multi-modal distri-\nbution into a uni-modal distribution. As a bonus,\nthe top-pcandidates of output distribution can con-\ntain more informative tokens.\n4.2 Linear Temperature Decay\nTo further improve the effectiveness of sampling,\nwe use the confidence-based iterative refinement\nby adjusting the temperature with linear schedule:\nP(yi|X,Win(YM)) = exp(ul/T)∑\nl′ exp(ul′ /T),\nT = β∗(1 − t\nT),\n(8)\nwhere β is hyper-parameter, t ∈ {0,··· ,T}is\nthe current iteration step, and T is the maximum\niteration step. Actually, the output distributions\nwill be flattened when T >1, and become sharp\nwhen T <1. Therefore, by applying this strategy,\nwe can penalize the distribution from peaked to flat\nin the former iteration steps and encourage it from\nflat to peaked in the later steps. The aforementioned\nprocess is shown in Figure 4.\n4.3 Training and Inference\nGiven the parallel data, we use vanilla self-attention\nto obtain source sentence representation and sliding\nwindow mixed-attention with fixed window size to\ngenerate the target during the training stage. Dur-\ning the inference, we apply DSWA to the mixed-\nattention layer and LTD to sample the results ac-\ncording to the probability distributions.\nBesides, the model uses the ground truth tokens\nas context to predict the masked tokens during the\ntraining stage and applies the randomly sampled\ntokens as context during the inference stage. This\ndiscrepancy makes the model only refine a frac-\ntion of the low confidence tokens, which causes\nthe degeneration in practice. Thus, we update all\ntarget tokens according to model predictions at\neach iteration step by utilizing the SMART mecha-\nnism (Ghazvininejad et al., 2020).\n5 Experiments\n5.1 Settings\nDatasets We conduct experiments on three Open-\nLTG tasks, i.e., storytelling (ROC (Mostafazadeh\net al., 2016), WP (Fan et al., 2018), and WikiPlots\nand multi-paragraph opinionated article writing\n(OPINION (Hua and Wang, 2020)). For ROC\ndatasets, we follow (Guan et al., 2021b) to mask\nall the names with specific placeholders to improve\nthe generation ability. We fine-tune the model us-\ning our approach without additional corpus. More\ndetails are illustrated in Appendix A.\nImplementation & Baselines We utilize the pre-\ntrained RoBERTa base‡ as our backbone model and\nimplement all experiments with the open library\nFairseq toolkit§ (Ott et al., 2019). In addition, we\nalso compare our method with the strong baselines,\ne.g., the widely-used AR models like BART (Lewis\net al., 2020), HINT (Guan et al., 2021b) for story-\ntelling tasks, and PAIR (Hua and Wang, 2020) for\nmulti-paragraph level text generation task. It is\nworth noting that the layer and model parameters\nof RoBERTa (125M) are close to BART (140M),\nso it can be used to compare the inference speed\ndirectly. For the inference stage, we set the max\niteration step as 6 for ROC and 8 for others. We set\nthe hyper-parameter αmin=0.125, αmax=0.75, and\nwindow size Swin equals 64. We set top-p=0.9 for\nall baseline models, set β=1.6 for ROC and 1.8 for\nWP and WikiPlots, and set β=1.5 for OPINION.\n‡https://dl.fbaipublicfiles.com/\nfairseq/models/roberta.base.tar.gz\n§https://github.com/facebookresearch/\nfairseq\n228\nData Model BLEU ROUGE Repetation Distinct BERT Score PPL SpeedupB-1(↑) B-2(↑) R-1(↑) R-2(↑) R-L(↑) LR-n(↓) SR-n SR-m D-4(↑) P( ↑) R( ↑) F1(↑)\nROC\nBERT-CRF 18.90 7.04 14.98 1.73 12.26 36.60 - - 33.11 74.07 71.32 72.65 - -\nHINT 32.97 16.91 25.54 3.87 18.48 5.96 73.93 45.27 57.9378.4077.14 77.74 26.16 -\nBART 30.06 14.37 22.37 2.42 15.52 3.93 69.53 40.04 79.07 76.34 76.83 76.57 65.21 1.0×\nOurs 33.22 17.08 26.82 3.9118.22 3.28 70.52 43.71 68.93 77.8678.23 78.0353.00 2.9×\nGround-Truth - - - - - 2.50 70.74 40.99 46.46 - - - 53.35 -\nWP\nBERT-CRF 18.50 7.42 17.70 2.30 12.91 83.80 - - 8.58 71.50 66.38 68.82 - -\nHINT 22.44 8.38 18.66 1.69 11.71 26.05 80.56 46.50 36.92 71.23 67.72 69.38 14.18 -\nBART 29.29 9.96 23.57 1.98 12.04 0.73 74.92 33.82 90.38 71.64 71.38 71.50 88.74 1.0×\nOurs 32.80 11.65 26.67 2.43 12.970.73 78.67 35.29 86.7072.17 72.09 72.1285.88 6.4×\nGround-Truth - - - - - 0.45 80.23 34.36 49.23 - - - 55.39 -\nWikiPlots\nBERT-CRF 16.33 6.42 18.41 1.64 12.24 78.28 - - 29.80 63.27 65.53 64.37 - -\nHINT 19.86 8.61 19.36 2.14 10.98 9.86 70.42 50.49 55.16 72.28 68.36 70.18 15.63 -\nBART 27.15 10.51 22.63 2.45 11.42 1.58 75.88 44.41 92.60 71.24 73.61 72.36 68.63 1.0×\nOurs 30.06 12.39 25.88 3.55 12.624.50 79.06 41.16 83.9771.74 73.64 72.6361.36 13.3×\nGround-Truth - - - - - 0.98 75.13 46.72 91.71 - - - 40.88 -\nTable 4: Performance on ROC Stories, Writing Prompt, and WikiPlots.\nEvaluation Metrics We utilize BLEU (B-n) (Pa-\npineni et al., 2002), ROUGE (R-n) (Lin, 2004),\nLexical Repetition (LR- n, 4-gram repetition for\nn-times) (Shao et al., 2019), Semantic Repetition\n(SR-n, average top-nsemantic similarity between\nany two sentences) (Guan et al., 2021b)¶, average\nsemantic overlap (S-m, average semantic similar-\nity of all the sentences), Distinct (D- n) (Li et al.,\n2016) and BERTScore (Zhang et al., 2019) for the\nstorytelling task. As for the multi-paragraph opin-\nionated articles writing, we utilize B-n, R-n, and\nMETEOR (Banerjee and Lavie, 2005) to evalu-\nate the results. The settings of nare mainly due\nto the length of the generated text and details are\nillustrated in each subsection below. We report\nthe LR-2 and SR-1 for ROC stories and LR-5 and\nSR-10 for WP to reflect the lexical and semantic\nrepetition of the generation texts. We also report\nthe Repetition and Distinct scores of ground truth\nas a reference. We calculate the perplexity (PPL)\nusing GPT2 (Radford et al.) for each model, which\nis the most common fluency metric.\n5.2 Main Results\nTable 4 summarize the evaluation results on each\nstorytelling test set. We choose the appropriate\ncheckpoint based on the repetition and distinct com-\nparison with the ground truth of the validation set.\nWe can observe that our approach achieves better\nperformance on all datasets than the strong base-\nline model. Especially, The text generated by the\nRoBERTa model has high-quality and fluent results,\nwhich have high BLEU, ROUGE, BERT scores,\n¶https://huggingface.co/sentence-\ntransformers/bert-base-nli-mean-tokens\nModel Refine ARGGEN\nBLEU-4 ROUGE-L METEOR\nPAIRfull\n/enc-3734.09/32.59* 55.42/49.39* 32.74/50.63*\n/enc-3336.09/34.42* 56.86/50.82* 33.30/51.39*\nOurs /enc-3731.42 53.55 55.58\n/enc-3337.76 59.24 59.70\nTable 5: Results of the OPINION dataset. The data\nnoted with * represent our implementation.\nand lower perplexity, demonstrating the effective-\nness of our model.\nFor the OPINION dataset, we use the specific\nplans to initialize the model input and then try to\ngenerate the missing text according to PAIRfull\nsettings, where these special plans are extracted\nfrom the ground truth. The results are shown in Ta-\nble 5. The PAIR results are based on BART, the AR\nmodel, so it has high quality even without refine-\nment. Our model achieves better results than PAIR\nwhen using iterative refinement, demonstrating that\nas a masked language model, RoBERTa is more\nsuitable to complete the planning sequence than an\nAR model. In addition, we found that the model\nworks better without dynamic sliding window at-\ntention, because the additional context information\nprovided a good initialization to the model.\n5.3 Ablation Results\nWe conduct the ablation study in Table 6 to evaluate\nthe effectiveness of each inference strategy. We can\nobserve the performance drops when without using\nany strategy, and this phenomenon is significant for\nlonger WP datasets. In particular, the results are\nmore in tune with the current prompt benefit from\nthe DSW A, such as better BLEU and ROUGE, and\n229\nData Model B-1 R-L Rep Dist PPL\nROC\nOurs 33.22 18.22 3.28 68.93 53.00\nw/o DSW A 32.12 17.67 3.71 68.53 48.87\nw/o LTD 33.04 17.73 11.29 69.66 78.07\nw/o ALL 31.86 16.96 14.49 67.30 67.75\nWP\nOurs 32.80 12.97 0.73 86.70 85.88\nw/o DSW A 29.37 12.31 0.90 86.07 86.95\nw/o LTD 29.80 13.88 17.80 64.53 63.08\nw/o ALL 12.95 6.60 90.58 32.15 17.69\nTable 6: Ablation study of different inference strategies.\nFigure 5: Inference speed for different datasets.\nthe model generates more repetition text without\nLTD. Thus, the DSWA and LTD are crucial for\nOpen-LTG, which can reduce the context depen-\ndencies to predict the output distribution better, and\nimprove the confidence score for each iteration step\nto adopt the open-ended scenarios.\n6 Analysis and Discussion\n6.1 Speedup for Inference\nFigure 5 illustrate the generation speed with the\nNVIDIA A100 GPU, which all run with the batch\nsize equal to 1 on each test dataset. Our model can\nspeed up the inference from 3×to 13 ×with differ-\nent target lengths, i.e., from 133 token/s to 391 to-\nken/s for the ROC dataset, from 137 token/s to 882\ntoken/s for the WP dataset, and from 132 token/s to\n1753 token/s for the WikiPlots dataset. Although\nthe smaller iteration step can further accelerate the\nspeed, the perplexity drops significantly.\n6.2 Length Prediction\nWe validate the different length prediction strate-\ngies on the WP dataset, as shown in Table 7. We\ninitialize the full mask sequence with ground truth\nlength to inference. For the prediction method, we\nselect the specific offset according to the validation\nset, e.g., −20 for WP and −100 for WikiPlots. Be-\nsides, the prediction modules work better for short\ntext dataset ROC with offset 0. We also found\nthat the fixed strategy obtained comparable perfor-\nStrategy Length B-1 R-L LR-n D-4 PPL\nGround-Truth 157.42 33.21 13.17 0.67 86.92 86.86\nFixed 153.51 32.80 12.97 0.90 86.70 85.88\nPrediction 155.55 31.96 12.94 0.63 86.53 85.56\nTable 7: Length prediction of different strategies.\nMetrics Win Loss Tie κ\nFluency 38.0 35.0 27.0 0.55\nCoherence 39.5 30.5 30.0 0.44\nRelevance 47.5 23.5 29.0 0.61\nTable 8: Human evaluation results on mixed dataset. κ\ndenotes Fleiss’ kappa value.\nmance with a slight drop, even the prediction is\nalso a viable choice for the inference stage.\n6.3 Human Evaluation\nFor human evaluation, we compare our method\nwith strong baseline BART. We sample 100 cases\nfrom the model outputs on three different datasets\nin total. We hire three annotators to give their pref-\nerences (win, loss and tie) for three evaluation cri-\nteria: fluency, coherence, and relevance, which re-\nflect the intra-sentence linguistic quality (Xu et al.,\n2020), inter-sentence relatedness & causal depen-\ndency and consistency of the generation results,\nrespectively. More details are illustrated in Ap-\npendix C. We apply the Fleiss’ kappa (Fleiss, 1971)\nto measure the agreement among three annotators,\nand the results are listed in Table 8, where we report\nthe percentage(%) of each preference when com-\nparing with BART model. We can observe that our\nmethod can achieve better performance on three\ncriteria when comparing with the BART model, es-\npecially for the relevance criterion, which indicates\nthat such a NAR generation paradigm can mitigate\nthe inconsistent issues of long text generation tasks.\nIt is worth noting that all the inter-annotator agree-\nments are either moderate (κ∈[0.4,0.6]) or sub-\nstantial (κ∈[0.6,0.8]). Besides, we also plot the\ndetailed percentage for ROC, WP, and WikiPlots\non Figure 6, which can clearly exhibit the discrete\ndistributions across three datasets. The fluency and\ncoherence of the sentence generated by our mod-\nels obviously decreased as the length increased,\nsimilar to the BART model. We will improve the\ntext quality and overall fluency and solve the above\nproblems for Open-LTG scenarios in future work.\n230\nFigure 6: Discrete distribution for different datasets.\n7 Conclusion\nThis paper explores Open-LTG with NAR models\nbased on pre-trained MLMs. We first examined\nthe potential and limitations of MLMs along with\nthe iterative NAR inference for open-ended text\ngeneration and observed that MLMs would col-\nlapse for Open-LTG. Through extensive study and\nanalysis, we found the reason is the inappropri-\nate attention mechanism and inference strategies,\nand introduced two simple strategies to alleviate\nsuch a problem, i.e., dynamic sliding window at-\ntention and linear temperature decay. Experiments\ndemonstrate that our model achieves competitive\nperformance and significant speedup. We hope\nour research can make pre-trained MLMs as new\ncandidates for the Open-LTG community.\n8 Limitation\nAlthough our NAR approach can generate fluent\nand meaningful text, it inevitably suffers from the\ntypical generation problems like in the AR fashion:\n(1) off-prompt: the provided prompt is very short,\nwhich causes the model can not focus on meaning-\nful content and generate reasonable text. Besides,\nthe model usually simply copy prompt text to gen-\nerate results instead of planning reasonable content,\nsuch as the case 3 as shown in Table 13 in Ap-\npendix D. (2) incoherent between sentences: When\nthe model is initialized, it does not consider the\nlogical order between sentences, so it can only rely\non the training data to learn automatically. We will\nconsider how to generate a suitable initialization\nto help the model generate coherence results. Our\npaper’s primary concern focuses on accelerating\nthe generation speed, and we will put how to solve\nthese problems in future work.\nEthics Statement\nOur method heavily relies on the pre-trained lan-\nguage models, e.g., RoBERTa, which may inherit\nthe problematic biases (Radford et al.). We have\nattempted to mitigate these issues by conducting\nexperiments on comparatively innocuous story gen-\neration and opinion generation tasks. Furthermore,\nwe have replaced all the names in those corpora\nwith special placeholders. Although some mea-\nsures are taken to mitigate the problematic biases,\nsuch issues cannot be solved completely. Thus,\nwe urge the users to carefully examine the gener-\nation results and cautiously apply our method in\nreal-world applications. Additionally, it is worth\nnoting that all the corpora used in our experiments\nare only for scientific research.\nAs for the human evaluation process, we resort\nto open source web library Django || to build our\nown human evaluation interface. Before releasing\nthe human evaluation cases, we carefully check that\nthere is no private information or other problematic\nbiases in the cases. Besides, we did not collect per-\nsonal information or ask the annotators about their\nprivate information during the annotation process.\nWe hired three annotators and paid each of them\n$0.29 for each case comparison. The payment is\nreasonable since there are only 100 cases for anno-\ntation, and it would cost average 4 hours for one to\nfinish all the comparisons.\nAcknowledgements\nThis work is supported by the National Science\nFoundation of China (NSFC No. 62206194), the\nNatural Science Foundation of Jiangsu Province,\nChina (Grant No. BK20220488), and JSS-\nCBS20210661. This work is also supported by\nBeijing Academy of Artificial Intelligence (BAAI).\nReferences\nSweta Agrawal and Marine Carpuat. 2022. An imita-\ntion learning curriculum for text editing with non-\nautoregressive models. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7550–\n7563.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved cor-\nrelation with human judgments. In Proceedings of\nthe acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summariza-\ntion, pages 65–72.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\n||https://www.djangoproject.com\n231\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nEthan A Chi, Julian Salazar, and Katrin Kirchhoff. 2021.\nAlign-refine: Non-autoregressive speech recognition\nvia iterative realignment. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1920–1927.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. Advances in Neural Information Process-\ning Systems, 32.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 6112–6121.\nMarjan Ghazvininejad, Omer Levy, and Luke Zettle-\nmoyer. 2020. Semi-autoregressive training im-\nproves mask-predict decoding. arXiv preprint\narXiv:2001.08785.\nSeraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph\nWeischedel, and Nanyun Peng. 2020. Content plan-\nning for neural story generation with aristotelian\nrescoring. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4319–4338.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In International Confer-\nence on Learning Representations.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019. Lev-\nenshtein transformer. Advances in Neural Informa-\ntion Processing Systems, 32.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93–108.\nJian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wen-\nbiao Ding, and Minlie Huang. 2021a. Long text\ngeneration by modeling sentence-level and discourse-\nlevel coherence. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6379–6393, Online. Association for\nComputational Linguistics.\nJian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wen-\nbiao Ding, and Minlie Huang. 2021b. Long text\ngeneration by modeling sentence-level and discourse-\nlevel coherence. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6379–6393.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu,\nand Jun Wang. 2018. Long text generation via adver-\nsarial training with leaked information. In Proceed-\nings of the AAAI conference on artificial intelligence,\nvolume 32.\nJunliang Guo, Linli Xu, and Enhong Chen. 2020.\nJointly masked sequence-to-sequence model for non-\nautoregressive neural machine translation. meeting\nof the association for computational linguistics.\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo\nChen, and Tie-Yan Liu. 2018. Layer-wise coordina-\ntion between encoder and decoder for neural machine\ntranslation. Advances in Neural Information Process-\ning Systems, 31.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nZhe Hu, Hou Pong Chan, Jiachen Liu, Xinyan Xiao,\nHua Wu, and Lifu Huang. 2022. Planet: Dynamic\ncontent planning in autoregressive transformers for\nlong-form text generation. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2288–\n2305.\nXinyu Hua and Lu Wang. 2020. Pair: Planning and\niterative refinement in pre-trained transformers for\nlong text generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 781–793.\nXiao Shi Huang, Felipe Perez, and Maksims V olkovs.\n2022. Improving non-autoregressive translation mod-\nels without distillation. In International Conference\non Learning Representations.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative refinement. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1173–1182.\n232\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand William B Dolan. 2016. A diversity-promoting\nobjective function for neural conversation models.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119.\nJiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.\nA hierarchical neural autoencoder for paragraphs and\ndocuments. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1106–1115.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849.\nEyal Orbach and Yoav Goldberg. 2020. Facts2story:\nControlling text generation by key facts. In Pro-\nceedings of the 28th International Conference on\nComputational Linguistics, pages 2329–2345.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nWeizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu\nChen, Dayiheng Liu, Kewen Tang, Houqiang Li,\nJiusheng Chen, Ruofei Zhang, et al. 2021. Bang:\nBridging autoregressive and non-autoregressive gen-\neration with large scale pretraining. In International\nConference on Machine Learning, pages 8630–8639.\nPMLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language mod-\nels are unsupervised multitask learners.\nChitwan Saharia, William Chan, Saurabh Saxena, and\nMohammad Norouzi. 2020. Non-autoregressive ma-\nchine translation with latent alignments. empirical\nmethods in natural language processing.\nZhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu,\nand Xiaoyan Zhu. 2019. Long and diverse text gen-\neration with planning-based hierarchical variational\nmodel. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3257–3268.\nYixuan Su, Deng Cai, Yan Wang, David Vandyke, Si-\nmon Baker, Piji Li, and Nigel Collier. 2021. Non-\nautoregressive text generation with pre-trained lan-\nguage models. In Proceedings of the 16th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: Main Volume, pages 234–\n243.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nBowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric P\nXing, and Zhiting Hu. 2020. Progressive generation\nof long text.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Kyunghyun Cho, and CIFAR Azrieli Global\nScholar. 2019. Bert has a mouth, and it must speak:\nBert as a markov random field language model.\nNAACL HLT 2019, page 30.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2018. Pay less attention with\nlightweight and dynamic convolutions. In Interna-\ntional Conference on Learning Representations.\nYisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li,\nMin Zhang, Tao Qin, and Tie-yan Liu. 2022. A\nsurvey on non-autoregressive generation for neural\nmachine translation and beyond. arXiv preprint\narXiv:2204.09269.\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul\nPuri, Pascale Fung, Animashree Anandkumar, and\n233\nBryan Catanzaro. 2020. Megatron-cntrl: Control-\nlable story generation with external knowledge using\nlarge-scale language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2831–2845.\nKexin Yang, Wenqiang Lei, Dayiheng Liu, Weizhen\nQi, and Jiancheng Lv. 2021. Pos-constrained paral-\nlel decoding for non-autoregressive generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 5990–\n6000.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\nYicheng Zou, Zhihua Liu, Xingwu Hu, and Qi Zhang.\n2021. Thinking clearly, talking fast: Concept-guided\nnon-autoregressive generation for open-domain di-\nalogue systems. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2215–2226.\n234\nA Dataset and Pre-processing\nDataset Input Reference#Train #Valid #Test\nROC 9.01 37.66 176,688 9,816 9,818\nWP 25.51 141.60 53,516 4,000 4,000\nWikiPlots3.41 354.8 102,936 5,000 5,000\nOPINION17.88 104.36 42,462 6,480 7,562\nTable 9: Statistic of datasets.\nThe statistic of each dataset is shown in ta-\nble 9, and we provide the download address of\nOPINION **, ROCStories, WritingPrompts ††, and\nWikiPlots ‡‡. In particular, we have to pre-process\nthe dataset to ensure RoBERTa can handle each\nsample. We first use the NLTK tokenizer to split\neach sample into individual sentences, generally\naccording to punctuation as a separator. Then, we\ncollect the segment with a pre-defined segment\nnumber Kto make the different pieces hold com-\nparable lengths. Finally, we truncate the sample\nwith a sequence length over 512 to satisfy the\nBERT maximum length limitation. Furthermore,\nwe also provide the library version or link informa-\ntion, which is used in our paper: Transformers ==\nv4.0.0, NLTK == v3.5, and evaluation scripts§§.\nB Recurrent Segment Generation\nAs shown in Figure 7, to gradually increase the con-\ntext during the decoding stage, we divide the one-\npass parallel decoding into multiple decoding steps.\nSpecifically, we split the target Yinto multiple\nsegments {S1,S2,··· ,SK}, where each segment\nconsists of multiple tokens/sentences by specifying\nthe length of each segment. Then, the model will\ngenerate those segments incrementally, ensuring\nthat each decoding step depends on the previously\ngenerated context to provide adequate information.\nIn other words, we introduce NAR to generate each\nsegment and use recurrent segment generation to\nkeep segment-level coherence. Meanwhile, the\nmodel can obtain a flexible decoding paradigm by\nmanipulating the length of the segments, e.g., the\nmodel can achieve one-pass decoding when set-\nting the segment as the whole target sequence and\n**https://drive.google.com/file/d/\n1gs_4fJj3U6Mrt8ekNIoDHRwSUc9WQbzp/view\n††https://drive.google.com/drive/\nfolders/1i_2YfzpDnfuLyyctOyDabn3Br0OcK1Tj\n?usp=sharing\n‡‡https://github.com/markriedl/\nWikiPlots\n§§https://huggingface.co/evaluate-\nmetric\nachieve AR decoding (same as BART) when set-\nting the segment as one single token.\nConcretely, we feed the input text Xinto the\nBERT model to obtain the representationHsrc. We\nthen concatenate the hidden states of the input and\npreviously generated context segments to feed them\ninto the decoder mixed-attention layer and generate\nthe k-th segment:\n¯Hl\nSk = Mixed-ATTN(Hl−1\nSk ,HL\nsrc, ˜HL\nS<k) + Hl−1\nSk\nHl\nSk = FFN( ¯Hl\nSk) + ¯Hl\nSk,\n(9)\nwhere ˜HL\nS<k is the representation of the previous\nsegment using the ground truth instead of the gen-\neration results HL\nS<k to guarantee the reliability of\nthe context information. The model recovers the k-\nth masked segment and calculates the cross-entropy\nof those masked tokens SM as the the MLM loss:\nLMLM = −\nK∑\nk=1\n|SM|∑\nj=1\nlog P(Sk\nj |X,S<k,Sk\nj\\M), (10)\nwhere Sk\nj\\M is the observed tokens of k-th segment.\nBesides, we will select a segment number before\nmodel training and then use it to split the training\ndata, ensuring the same number of segments for\ntraining and inference in the experiment.\nC Human Evaluation\nDataset #Num Length\nROC 40 40\nWP 35 140\nWikiPlots 25 350\nTable 10: Statistic of human evaluation data, where\n#Num denotes the number of cases in human evaluation\ndataset.\nWe show the human evaluation interface in Fig-\nure 8 that was built using the python web library\nDjango ¶¶. To test the generation ability between\nour method and the strong AR model (BART) in\ndifferent generation tasks, we sample cases for dif-\nferent tasks. The statistic of sampled evaluation\ndatasets is shown in Table 10. In each compari-\nson, each annotator is shown with one model in-\nput (prompt) and two outputs generated from two\nmodels, namely “Text A” and “Text B”. Then, the\nannotators are asked to select the better text in each\ncomparison in terms of fluency, coherence, and\nrelevance. In case of a situation where annotators\n¶¶https://www.djangoproject.com\n235\nFigure 7: The overview of recurrent segment generation. The green blocks and arrows represent the generation\nresults and the corresponding flow directions.\nFigure 8: Human Evaluation Interface\nthink two texts are hard to distinguish, the “Tie”\nchoice is allowed. We can ensure that each annota-\ntor is independent during their annotation process\nand the total annotation process is fair.\nD Case Study\nWe randomly selected some cases from different\ndatasets to facilitate the evaluation, which was gen-\nerated by the BART and our model. Table 11 illus-\ntrates the results on the ROC dataset, and we can\nsee that our model results are close to the prompt\ntext benefit from the NAR fashion. For example,\ntopic case 2 is about “candy\", the BART generates\nthe sentence with fruit “grapes,\" instead, our model\ngenerates the “chocolate,\" and the whole sentence\nis close to the topic candy. Furthermore, our model\ncan generate a high correlation for different sen-\ntences, such as “plants, seeds and watered. finally, i\nhad a beautiful garden.\" in case 11. We also provide\nthe results of WP and WikiPlots for Table 12 and\nTable 13. Although these results are relatively un-\ngrammatical and incoherent, the pre-trained MLM\n(RoBERTa) achieves competitive performance as\nBART.\nBesides, the results have some grammar errors\nfor our models, e.g., “when i got home i went to\nthe kitchen.” in case 10. The possible explanation\nis that the non-autoregressive model may generate\ngrammatically incorrect sentences during the iter-\nation refinement procedure due to multi-modality\nproblems. We will add grammar corrections for\neach iteration in future work to help the model\nproduce better results.\n236\nCaseType Text\n1\nPromptthe man made music .\nBARThe put his name on a paperpha pamphlet . someone subscribed . he sold it . his name got popular .\nOurs it was for a dance to a his friends . they invited him to the studio to play his music .\nit played over and over reminding the man of his past . the new song did n’t become a success , but very popular .\n2\nPrompti had an intense craving for candy .\nBARTshe wanted me to buy grapes . some of the ingredients did not fall in pretty . i could not make sauces however .\ni have decided to take up cooking .\nOurs i went to the grocery store to buy some chocolate . i went to the store and found one very empty bag of candy .\nhe took care of it for a few minutes . when i got home , i was out of candy .\n3\nPrompti sat down in the quiet room .\nBARTi took a turn for the head held high . i felt ill around my shoulders . i closed my eyes and got out of bed .\ni had flung smoke at reality .\nOurs there was a very clean room . i could n’t find my phone . i was scared and felt like something was going to happen !\ni stood up and took a breath and closed the door . i was home and the i was happy .\n4\nPrompti went to the store to buy breakfast food .\nBARTi returned to the store for breakfast and the cost for food was much too much . he asked me to try in the big eggs .\nto my dismay , it was expired . in the Mangrove , the eggs in the pan were all burnt .\nOurs i bought cereal and oatmeal eggs , but i was sad because they were expensive . then i thought about how much i cared about breakfast .\ni brought it home to my wife . when i got back home , i gave her breakfast .\n5\nPromptthe court date was set for earlier this month .\nBART\ni managed to get my client into court on time , so my dad ordered [FEMALE] to get up early . the day of the release ,\ni went to sleep and woke up early . i missed getting to sit with him over breakfast .\nhowever , it turns out she was finally able to get back to court this week .\nOurs the clerk announced the date had been postponed for the week . he went to the courthouse the day before the court .\nhe so told her to stop smoking that she called the police for help . it now appeared that the trial was postponed .\n6\nPrompti decided to ride my bike to the park .\nBARTi immediately rode my bike across the road . i picked up the first cyclist to get on my bike .\ni waited two minutes and then waited five minutes . i decided that it was time to turn my bike .\nOurs as i got on my bike i made a plans for the day . after going to the park , i decided to ride my bike home .\ni rode around the park until i a my home . i pulled over and put my bike back .\n7\nPrompti opened up a new bank account last month .\nBARTone of the accounts i opened would not have any cash . when i opened the account , i had fun with some old stock .\ni couldnt wait to check out the shares in subsequent months . i also realized how busy i was without new passwords .\nOurs i was able to put almost everything i need in my account . i looked a website of how to make money now .\ni intend to pay out to that account once a while . i have made a lot of money with this account .\n8\nPromptwhen we were young my brother and i made spears out of newspapers .\nBARTwe must have been 3 or 4 in the Arist . unable to get my spears down i put them into a ball .\nwe used fuses to pick screws . [MALE] liked his five foot tall six foot Luther beast .\nOurs one year we went to a field in an open field . i told us to would sell them when we got home early .\nwe practiced the spears until it was dark . afterwards , my mother took me to the hospital to treat my wounds .\n9\nPrompti had a nearly new keyboard .\nBARTi was torturing it with it . i was torturing it with it .\nthen i was so excited i ran to the bathroom . i rushed my new keyboard away and it barely fit my needs .\nOurs it had a great keyboard . when i got my new keyboard , there were black stains all over it .\ni was so upset that i decided to throw my keyboard away . when i put it back on , it was working great at work .\n10\nPromptone day i was really hungry .\nBARTit was almost time to eat so i checked the fridge to see what was there . i saw a bag of raw steak and wondered how went there .\nthe cheapest way to eat was to come up with a replacement . i picked a different restaurant later that day .\nOurs i got dressed and went to something to eat . i was very hungry and went to the store and bought a lot of fruit .\nwhen i got home i went to the kitchen . i got some cheese and bread and ate the turkey .\n11\nPrompti decided to start a garden .\nBARTi researched the crop and the dates . i grew tomatoes . he had just for most of his meals ,\nso he was out of milk . [FEMALE] garden is excellent at thinking about the future .\nOurs i planted some seeds online from the local garden store . i planted the seedlings in the soil and started planting .\ni gave in with what to do of the plants , and seeds and watered . finally , i had a beautiful garden .\nTable 11: Representative ROC examples for BART and Pre-trained MLMs (RoBERTa)\n237\nCaseType Text\n1 Promptcaffeine is made illegal and is now rated schedule i by the dea .\nBART\nwith ’conservatives ’ is for ’conservatives ’ . that is basis of the term . the name is the republic . remnants of the religion were\nwiped from the lips of most . hegemony . democracy was a jungle . every entity was put to a test . to put it gently ,\na ban would be imposed on the world , forcing every person who had taken part to create some form of protein to join the majority\nof their species . food was a main reason why the republic was a flourishing , independent nation .\ngovernments rushed to clamp down on caffeine , showing how virtually any government could kill them .\nwe feed billions of people who needed one of their pills through cigarettes . protests had come and gone but nothing remained for bobby .\nOurs\n\" i went to the coffee shop last month ago and saw that . i was a kid at the time . sometimes i would daydream about my older brother ,\nat least male , alive as rain fell on the roof . and a can of coke . thanks . he gave this to me to have a copy of every book i ever picked up .\ni were in the parking section , and as i looked down , i smelled it ! no wonder what my headphones do ...\ni grabbed some plastic cups and placed them on the coffee table , along with a white sheet on the way to the office . i ordered a joe .\nnow it had been time to go . jesus christ when one of my friends said to me , \" come on . just save me the milk ! \"\n2 Prompta magical mirror shows your reflection and your future soulmate . you only see your reflection .\nBART\nnight would always come first . that ’s how every man usually looks . one point is this .\" wait a minute !\nwhere do amateur artists live ? \" \" in outer space , \" the mirror roamed through the neighborhood , staring intently at the road-lights .\nhe moved his feet a few feet so that even the hardest parts would stay put . \" i ’m sitting in your shit hole ,\nfound a pot in your room .\" said you .\" i sure am .\" ACL looked at me with a cheery smile ,\n\" i could n’t tell . i think my dad was an atheist . they got acquainted the same way because he always wore glasses .\npeople ca n’t wrap their wrists around idiots or say they ’re complaining .\nOurs\nshe ’s staring me by the lenses of a painting a gagged man tells me the only noise that comes to my mind is question .\nquestion . my head burns with fear . i wonder when i will walk into the mirror , how long my a full of hope and love\nhas faded back and forth ? do you tell you a lie , because i ’ll find you out . and if you ask you three i ’ll have to look at light brown .\ni look ca n’t really help it . being able to empathize with what i ’m going through i can a myself . she i must have be having children .\nbut what i want is that ? the first girl since last time came around . do n’t care because i love her , the way i love her .\n3 Promptit ’s 3016 , cryogenic people are being revived for the first time .\nBART\n\" knock ! knock ! \" i gasped as the glass door cracked open , revealing a thin version of me . a lot of blood , especially from the infection .\ni quickly grabbed my utensil and held it close against my mouth , thumb plungingikhail ’s face back down toward his mouth .\nthe kitchen was booked and locked , all but impossible to see . smitherin turned to me , his hand outstretched .\" fix anything ,\n\" i sighed , slowly closing the deal . my fingers were too large , uncomfortable , and my left arm sank behind his neck .\ni pressed a silent raise , still gripped tight , until my ankles buckled beneath me .\" fine , we have her back ,\n\" i managed , leaning closer for another variation .\nOurs\nthe massive tremors of the ringing in my s ears , beating furiously against my heart for a moment . i leaned back and against the floor ,\ni was here , finally . the last one left these days behind still wonder at night , as to what this all meant . i felt cruel at best ,\nbut least in a place like this we could talk . the , the people were sitting on an space vessel in space and had at least days left .\nnot good enough to write the process in a journal . i felt as comfortable as i could as they ’ new body was brought back for a while .\neverything was normal , no important , unknown secrets . but the thing about cryogenic people , they took you in and out .\nwe knew the ways , but still some were left .\n4 Promptyou are visited by your guardian angel but he/she is an asshole .\nBART\ncon the morning , there was a clear wind coming through thecloud ( um.. sunny ? is thatreally what it is ! ) down into the woods .\nthe last thing i remember was climbing up the steps of my home . my eyelids began to flutter nervously , i rubbed my eyes ,\nmoaning beneath my own robotic fingers . the line rose from a distant object , inviting me to focus .\nmy reflexive positioning prevented me from climbing back down . again , i thought i had recieved enough therapy to ensure such\nthings had not happened . would it be the last ? or was it the previous trick we played ?\ni sighed and climbed up into my favorite bed of the house ; the big one that i recently occupied .\nOurs\nher eyes met mine cold metal waiting eyes . with a blink a moment later i was replaced with a deep , dark , earthy smell filled the room .\nin it was that glorious smile on my face as i stared at my beautiful toes and joined in a whirl when there was eternity .\nher blue , emerald green eyes and foot gave away those evil , sly grin on the face .\nwhat little thing i remember was all the smells that tasted like steaming water . little touch did make me turn as white as a bird of heart .\nmake or worse . a tear ran its way down my body , taking note of my surroundings . i looked to i .\ni was facing a woman with a pink dress . my body seemed to lower itself into a numb state of the moment .\n5 Promptwayne enterprises goes bankrupt , forcing batman to pursue justice through more economical means .\nBART\nfire batman watched that shutter of his fist hit the ground , shattering him . his furious scream sounded as he shifted a small pile of broken\npaint on the side of the building . wallet , phone , controller , and was buttoned down , lying upon the floor in the middle of his .\ndabble , dabble . he closed his eyes and attempted to try and make sense of the implications . to splinter himself forever ,\nhe needed to pull together a means of escape , happiness , and serenity that would bring him back to reality . his floor was exposed to\nthe current rut in which the cash machine and carton of barks had gone , making him fix his entire apartment . questions about\nthe earlier charges ? complying ? no . get assistance ! james felt himself shaking and looking around , like the sky was blue\nOurs\nafter 30 hours of work was out again . batman sighed , walking down the street . he was in the hesian era and ,\nin fact , the father of batman, after all . a friend a friend and big money ? eh ? \" waggin ’ death , a bitch . the last batman of the world\nsat at his table with the latest edition of \" the slash of day . \" was the last show he ever watched .\nwhen he looked into that camera , batman called ready excited . his life had been locked behind race car running lights .\ntwirling \" do n’t go out \" out to ground batman was him out of the room . \" roger .\nTable 12: Representative WP examples for BART and Pre-trained MLMs (RoBERTa)\n238\nCaseType Text\n1 PromptMagic Wonderland\nBART\nDuring in 2021, Gavin Fleming (Marshall Hall), his son Duff Jackson (Hayden Farther), and his and Maggie Peterson (McCullough)explore mythical lands, and come across an ancient legend concerning the Wall, an artifact that may be more mythical than whatthe descendants of the Elemental Box, purchased and locked in deep holes in the human body. As the Dunder Mifflin alternateuniverse endlessly and alone, Gavin meets Belle Duncan (Brassman), a beautiful young woman who haunts Slater’s dreamsand constantly challenges him to battle her, so aided by a new creature called the X-Rex (Amaradurorylum), Gavin is advised to fightbeast-like creatures around the wall and defend the Wall from destruction when the beast enters. The beast grows into a giant male humanoid,known in his form as the Castle Unbound, a winged humanoid among the fantastical creatures. Gavin adores Belle,and tries to spend time with her after she has recovered. Belle suggests to Gavin that he turn his back on the villain and encounter theWolfrunner. He reluctantly agrees. Gavin does not agree, and Emma begins to welcome him only after a battle of wits. Throughout an attackof beasts, Liam Kane (Lyle Marvin) bursts through the defences of the Wall, following the guard staff, and scares everyone away.He forces Gavin to alert the guards to the camp’s presence. However, those he has encountered are unaware that he is a friend of EmmaPeterson, a Japanese-Australian employee who suffers from nightmares and dreams of superpowers and past matches\nOurs\nThe story follows the aia and his best friend playing the game Magic Wonderland, a game they play in \" Under the Tree \" .According to the legend, magic creatures also exist in a parallel world called the Wonderland. Coming out of which is a large house consideredto be haunted, but he is allowed to stay. After his magic over he goes for a while and does strange things about it. The world he went to isabout to happen something that happens to everyone. While waiting at the French manor he (Derek) learns of the kingdom of Aiaia and all ofthe other characters were left with a little guide on how to use the magic of all the the creatures in the world. When the new year came,all the people on the island decided that that would be A lot of fun for the two boys to explore the Ahaia’s fairy world. They decide toblow up the castle. On their way to school, they foolishly (missed) the black magic, and was had to move in. But Aia and his help make Aiaa magical man and a beast, making it difficult to figure out what he is about to do, and why. But now this evil plan has been set up,the story remains the same. Aia is more or less lost in a quest to find a world similar like his sister’s, too. They had their pet bird, a greendragon. Then, so that everyone did perfectly normal. When theia ran out of money, Aia decided to the home. The events are the as to if it wasin Aia’s life his father did not love him, after all, leaving him money to look after him. Also, he was to find out who is behind the magical fairyworld, and they will be in love until the end of their lives, and then the world from there. During the game, Aia giving one another a kiss.\n2 PromptBeyond Apollo\nBART\nSavika (Saurashtra Prakash) is a demon hunter of Lore Love (Madhuravalli who has set off for Chennai) and doesn’t want to interact with women.Tensely wanting to save her own girl, he approaches her in a customized john vehicle. Upon hearing about the coming of the eye,he enlists Glyndar (Urba Rao), the last man he knows and a high society man called Ramesh (Isha Kher). They meet in the limelight After feelingsick when he asks her to go out, he decides to travel to Seta village by car as his long distance companions. There, he enlists the help of an attractivewoman named Kadeb (Jaswini Gopal), and is immediately attracted to her for her beauty. At Seta, Gadeb unwittingly breaks into Kadeb’s celland steals money from him. A quarrel ensues between Gadeb and Skylady, an official in Seta who is in charge of the operation, over the case.During the meeting, Skylady and Gadeb beat Gadeb up and gave him her pocket money and dancing lessons. Gadec sees this and flees while Skyladytakes a cab in a hurry. She then steals weapons and goes off with Kadeb. Nightfall starts and Gadec runs into Kadeb, who secretly intends to stealthe money that Gadeb gave her to sell to her heir. He is shocked when Gadeb offers her a way out. Skylady\nOurs\nThe crew of Apollo is one person after another living in the O’ Beel family. That is, from the time they, on the planet Bumblebee, 12-year-old Roxiis about to be the pilot of an orphon-based spaceship. So, the crew of Apollo decides to be a rescue mission. Back at Earth, the crew is ready to leavefor the moon. On the station are OX-O-s- that, like themselves, can travel, using the help of space suits from the isle, stored in special’s as year 3031,a hundred years away, when the moon is built, so he and Fifi decide to see if they can find out about the ship. At the same time, a new memberis inducted into the crew, and completes the planning for further exploration. Then the nanobots appear and begin saying \" Enter into space.This I’ll do she replies in Just but not only Number One even after the end of Apollo, that is, not yet. . the planeto has been (andarently) transferredto a planet we came from called Dusty. They must go back to Apollo. Who cannot and why they did not abandon her. One’s afraid that oneis coming. Soon they decide to join at first for friends, but erupts start to be the ship. The Zesti wants to take Shoxi home, tries to stop him.But they refuse to see him again until one of them becomes a crew member and, he says, it was the only time he went out to take to work.However, it turns out that it was nothing but a very old man called R. who is tired of himself having a affair with their and their beau.However, without them all„ Six must deal with his very father, Olaf, and being a space pilot, who as a result has plans for the future.\n3 PromptMacbett\nBART\nYoungRecently released gangster, Ronnie Abbett, pairs up with Jake, an older lieutenant in the Marines. Instead of killing him by torpedo,he eventually exposes him in the hands of an army of locals who want to hold him prisoner even for one night. The drug lord is especiallyantagonistic as, near the end of the film, the \"likely\" blood of the terrorist murders in a bar kills him. The gang tries to punish the gangsters,making them excited over the pretense of love. Frankantly, the gangsters’ leader tries to coerced Ronnie to help them, while alcohol,drugs andreedness win him over by tricking him into accepting his debt. Adoption of drugs greatly affects Ronnie, and he complainsto his alcoholic brother about Daniel, who promptly kills him and tells Ronnie’s mother to stay away from him. Ronnie tries to be supportiveof Mike, who is working at Seagraves. The rest of the gang, including Mike, are led by a man named Dan, who is actually Ronnie’s adoptive fatherand enjoys side-play with Dan when they go out. However, Danny and Mike are against the most recent gang activities. Hell saves Mike’s lifeand Jim, a family friend, helps him out. The meanwhile, the new shift surgeons start robbing the bars and poor performers practice hollows,sending mugs on the streets, hitting people who cried out Loud at the climax so much as collapsing. They later see Mond Roger Lewis(Bruce Mancini), the bartender’s brother who supposedly does coconut liquor in a bar fight, surrounded by relatives\nOurs\nMacbettruns a small coffee shop on the grounds of his father’s farm. Mary and her are go to Scotland where JohnMacbetthad his first meetingwith Sir Andrew Macbett’s family and other things.Macbett, however, has a lot of respect for the character of \" \"Macbett\" . In the plotan man, a woman, and the manor, \" Teneggi.Macbett.Macbettat the funeral, and we learns that Mac’s father, Nail, Sr. died in an accident.John Pendleton was rich but he had nothing worth good for, but not evenMacbett’sdistant relatives, one of them Mary.Mary both do want to go see Jack Nelly and Celia’s father a little man (JohnMacbett). Later on, Mary and everyone, includingMacbettand Mary, in. They sell the house and sell it to Servant’s the next. who, after having watching the news; had been a party called forMacbett Macbett, who came here, while a other people get killed inside.Macbettdecides that meeting with Clint and Denegan has starteda new life Mac. S. Eton, who wasMacbett’sold friend, and fell in love with her.Macbettused to not fallen in love with Mary and that becausehe was in so much that was Keley’s land. In the end, Mary died when he was a child. We also find that his wife, Carol, doesn’t want to get marriedany more, after having a child.Macbetthad a son,Macbett.Macbett.Macbettand Scenein time with Mary and the rest of their family,except for Mary who is up withMacbett. MacMacbettsaysI don’t know what to do \" . Jack replies \" int \" .Overly without any memory of who he was is really dead not only in but but but but his two brothers.\nTable 13: Representative WikiPlots examples for BART and Pre-trained MLMs (RoBERTa)\n239\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n240\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n241",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.794257402420044
    },
    {
      "name": "Computer science",
      "score": 0.7676041126251221
    },
    {
      "name": "Inference",
      "score": 0.7261300086975098
    },
    {
      "name": "Autoregressive model",
      "score": 0.692222535610199
    },
    {
      "name": "Language model",
      "score": 0.5539954900741577
    },
    {
      "name": "Paragraph",
      "score": 0.5519992113113403
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49695876240730286
    },
    {
      "name": "Decoding methods",
      "score": 0.49136772751808167
    },
    {
      "name": "Natural language processing",
      "score": 0.41325029730796814
    },
    {
      "name": "Speech recognition",
      "score": 0.3877156972885132
    },
    {
      "name": "Algorithm",
      "score": 0.33589863777160645
    },
    {
      "name": "Machine learning",
      "score": 0.33525627851486206
    },
    {
      "name": "Parallel computing",
      "score": 0.11959749460220337
    },
    {
      "name": "Mathematics",
      "score": 0.08531242609024048
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    }
  ],
  "institutions": []
}