{
  "title": "Codified audio language modeling learns useful representations for music information retrieval",
  "url": "https://openalex.org/W3180663620",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4309798964",
      "name": "Castellon, Rodrigo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221397544",
      "name": "Donahue, Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3166964279",
      "name": "Liang, Percy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2296751288",
    "https://openalex.org/W2962942158",
    "https://openalex.org/W2584032004",
    "https://openalex.org/W2991108091",
    "https://openalex.org/W3093494400",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964051853",
    "https://openalex.org/W2127870748",
    "https://openalex.org/W3155776249",
    "https://openalex.org/W2922565841",
    "https://openalex.org/W2407685581",
    "https://openalex.org/W2963550089",
    "https://openalex.org/W3143852976",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W2906658932",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W335809467",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2765325162",
    "https://openalex.org/W3139211892",
    "https://openalex.org/W3029858316",
    "https://openalex.org/W1989445502",
    "https://openalex.org/W1960352584",
    "https://openalex.org/W2952502547",
    "https://openalex.org/W2990244497",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2136129419",
    "https://openalex.org/W2133824856",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2988736778",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3095513901",
    "https://openalex.org/W2191779130",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2137619888",
    "https://openalex.org/W2964274466",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W1556219185",
    "https://openalex.org/W2295991281",
    "https://openalex.org/W2794150026",
    "https://openalex.org/W3010903955",
    "https://openalex.org/W2019360207",
    "https://openalex.org/W2765119701",
    "https://openalex.org/W2023001347",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2964307104",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2896197082",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3000400453",
    "https://openalex.org/W2962904371",
    "https://openalex.org/W3021164770",
    "https://openalex.org/W2293255527",
    "https://openalex.org/W2955142818",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W3102568015"
  ],
  "abstract": "We demonstrate that language models pre-trained on codified (discretely-encoded) music audio learn representations that are useful for downstream MIR tasks. Specifically, we explore representations from Jukebox (Dhariwal et al. 2020): a music generation system containing a language model trained on codified audio from 1M songs. To determine if Jukebox's representations contain useful information for MIR, we use them as input features to train shallow models on several MIR tasks. Relative to representations from conventional MIR models which are pre-trained on tagging, we find that using representations from Jukebox as input features yields 30% stronger performance on average across four MIR tasks: tagging, genre classification, emotion recognition, and key detection. For key detection, we observe that representations from Jukebox are considerably stronger than those from models pre-trained on tagging, suggesting that pre-training via codified audio language modeling may address blind spots in conventional approaches. We interpret the strength of Jukebox's representations as evidence that modeling audio instead of tags provides richer representations for MIR.",
  "full_text": "CODIFIED AUDIO LANGUAGE MODELING LEARNS USEFUL\nREPRESENTATIONS FOR MUSIC INFORMATION RETRIEV AL\nRodrigo Castellon*\nStanford University\nChris Donahue*\nStanford University\nPercy Liang\nStanford University\nABSTRACT\nWe demonstrate that language models pre-trained on cod-\niﬁed (discretely-encoded) music audio learn representa-\ntions that are useful for downstream MIR tasks. Speciﬁ-\ncally, we explore representations from Jukebox [1]: a mu-\nsic generation system containing a language model trained\non codiﬁed audio from 1M songs. To determine if Juke-\nbox’s representations contain useful information for MIR,\nwe use them as input features to train shallow models on\nseveral MIR tasks. Relative to representations from con-\nventional MIR models which are pre-trained on tagging,\nwe ﬁnd that using representations from Jukebox as in-\nput features yields 30% stronger performance on average\nacross four MIR tasks: tagging, genre classiﬁcation, key\ndetection, and emotion recognition. For key detection, we\nobserve that representations from Jukebox are consider-\nably stronger than those from models pre-trained on tag-\nging, suggesting that pre-training via codiﬁed audio lan-\nguage modeling may address blind spots in conventional\napproaches. We interpret the strength of Jukebox’s repre-\nsentations as evidence that modeling audio instead of tags\nprovides richer representations for MIR.\n1. INTRODUCTION\nIt is conventional in MIR1 to pre-train models on large la-\nbeled datasets for one or more tasks (commonly tagging),\nand reuse the learned representations for different down-\nstream tasks [2–10]. Such transfer learning approaches\ndecrease the amount of labeled data needed to perform\nwell on downstream tasks, which is particularly useful\nin MIR where labeled data for many important tasks is\nscarce [11, 12]. Historically-speaking, improvement on\ndownstream tasks is enabled by ﬁnding ever-larger sources\nof labels for pre-training—in chronological order: tags [3],\nmetadata [5, 7, 9, 10], and recently, co-listening data [9].\nHowever, it stands to reason that directly modeling mu-\nsic audio (as opposed to labels) could yield richer repre-\n* Equal contribution\n1 MIR has a broad deﬁnition, but in this paper “MIR” refers speciﬁ-\ncally to making discriminative predictions on music audio.\n© Rodrigo Castellon, Chris Donahue, Percy Liang. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Rodrigo Castellon, Chris Donahue, Percy\nLiang, “Codiﬁed audio language modeling learns useful representations\nfor music information retrieval”, in Proc. of the 22nd Int. Society for\nMusic Information Retrieval Conf., Online, 2021.\nsentations. Recently, contrastive learning [13] has been\nproposed as an MIR pre-training strategy which learns rep-\nresentations from audio [14], but this paradigm has yet to\nexceed the performance of label-based pre-trained models\non downstream tasks.\nOutside of the discriminative MIR landscape, a recent\nsystem called Jukebox [1] demonstrated promising per-\nformance for generating music audio. To achieve this\nresult, Jukebox leverages recent architectural develop-\nments from natural language processing (NLP) by codi-\nfying audio—encoding high-rate continuous audio wave-\nforms into lower-rate discrete sequences which can be fed\nin directly to NLP models. Speciﬁcally, Jukebox trains\na Transformer [15, 16] language model , an autoregres-\nsive generative model, on codiﬁed audio from 1M songs.\nPurely for convenience, we refer to Jukebox’s training pro-\ncedure as codiﬁed audio language modeling (CALM).\nWhile Jukebox already demonstrates that CALM is use-\nful for music generation, in this work we demonstrate that\nCALM is also useful as a pre-training procedure for dis-\ncriminative MIR tasks. To this end, we repurpose Jukebox\nfor MIR by ﬁrst using it to extract audio feature represen-\ntations, and then training shallow models (probes [18, 19])\non downstream tasks using these features as input (Fig-\nure 1). Relative to representations from models pre-trained\nwith tagging, we ﬁnd that representations from Jukebox are\n30% more effective on average when used to train probes\non four downstream MIR tasks: tagging, genre classiﬁ-\ncation, key detection, and emotion recognition. We also\nobserve that representations from Jukebox are much more\nuseful for key detection than those from models pre-trained\non tagging, which suggests that CALM pre-training may\nbe particularly beneﬁcial for tasks which have little to do\nwith tagging. This simple setup of training shallow models\non representations from Jukebox is even competitive with\npurpose-built state-of-the-art methods on several tasks.\nTo facilitate reproducibility and encourage further in-\nvestigation of these representations and tasks [11], we re-\nlease all of our code for this project, alongside images for\nDocker containers which provide full provenance for our\nexperiments. 2 We note that, while CALM pre-training at\nthe scale of Jukebox requires substantial computational re-\nsources, our post hoc experiments with Jukebox only re-\nquire a single commodity GPU with 12 GB memory.\n2 Code: https://github.com/p-lambda/jukemir\nContainers: https://hub.docker.com/orgs/jukemir\nAll experiments reproducible on the CodaLab platform:\nhttps://worksheets.codalab.org/worksheets/\n0x7c5afa6f88bd4ff29fec75035332a583\narXiv:2107.05677v1  [cs.SD]  12 Jul 2021\n6\n9\n 7\n 2\n 0\n 0\n 6\n 5\n8\n…\n6\n 5\n 2\n 5\n 2\n 0\n 6\n0\n \n \n7\nFigure 1 . Conventional MIR pre-training ( left) trains convolutional neural networks on audio spectrograms using\nmanually-annotated labels from tagging datasets. In contrast, CALM MIR pre-training ( middle) involves training a lan-\nguage model on codiﬁed audio, which has been previously explored for music generation [17, 1]—here, we propose to use\nit for discriminative MIR tasks. To determine if CALM pre-training is effective for MIR, we probe for information about\nparticular MIR tasks (right) in resultant representations. Speciﬁcally, we extract features from the learned language model\nfor the audio in small, task-speciﬁc labeled datasets, and use these features to train shallow probing models on each task.\n2. CALM PRE-TRAINING\nCALM was ﬁrst proposed by van den Oord et al. and\nused for unconditional speech generation [20]. As input,\nCALM takes a collection of raw audio waveforms (and op-\ntionally, conditioning metadata), and learns a distribution\np(audio |metadata). To this end, CALM adopts a three-\nstage approach: (1) codify a high-rate continuous audio\nsignal into lower-rate discrete codes, (2) train a language\nmodel on the resulting codiﬁed audio and optional meta-\ndata, i.e., learn p(codiﬁed audio |metadata), and (3) de-\ncode sequences generated by the language model to raw\naudio. 3 The original paper [20] also proposed a strat-\negy for codifying audio called the vector-quantized vari-\national auto-encoder (VQ-V AE), and the language model\nwas a WaveNet [21]. Within music, CALM was ﬁrst used\nby Dieleman et al. for unconditional piano music genera-\ntion [17], and subsequently, Dhariwal et al. used CALM\nto build a music generation system called Jukebox [1] with\nconditioning on genre, artist, and optionally, lyrics.\nDespite promising results on music audio generation,\nCALM has not yet been explored as a pre-training strat-\negy for discriminative MIR. We suspect that effective mu-\nsic audio generation necessitates intermediate representa-\ntions that would also contain useful information for MIR.\nThis hypothesis is further motivated by an abundance of\nprevious work in NLP suggesting that generative and self-\nsupervised pre-training can yield powerful representations\nfor discriminative tasks [22–25].\nTo explore this potential, we repurpose Jukebox for\nMIR. While Jukebox was designed only for generation,\nits internal language model was trained on codiﬁed au-\ndio from a corpus of 1.2M songs from many genres and\n3 This third stage is not necessary for transfer learning.\nartists, making its representations potentially suitable for a\nmultitude of downstream MIR tasks. Jukebox consists of\ntwo components—the ﬁrst is a small ( 2M parameters) VQ-\nV AE model [20] that learns to codify high-rate (44.1 kHz),\ncontinuous audio waveforms into lower-rate ( ∼345 Hz),\ndiscrete code sequences with a vocabulary size of 2048\n(11 bits). The second component is a large ( 5B parame-\nters) language model that learns to generate codiﬁed audio\nusing a Transformer decoder—an architecture originally\ndesigned for modeling natural language [15, 16]. By train-\ning on codiﬁed audio (as in [17, 1]) instead of raw audio\n(as in [21, 16]), language models are (empirically) able to\nlearn longer-term structure in music, while simultaneously\nusing signiﬁcantly less memory to model the same amount\nof audio.\nLike conventional MIR models which pre-train on tag-\nging and/or metadata, Jukebox also makes use of genre\nand artist labels during training, providing them as con-\nditioning information to allow for increased user control\nover the music generation process. Hence, while CALM\nin general is an unsupervised strategy that does not require\nlabels, transfer learning from Jukebox speciﬁcally should\nnot be considered an unsupervised approach (especially for\ndownstream tasks like genre detection). However, by mod-\neling the audio itself instead of modeling the labels (as in\nconventional MIR pre-training), we hypothesize that Juke-\nbox learns richer representations for MIR tasks than con-\nventional strategies.\n3. EXTRACTING SUITABLE REPRESENTATIONS\nFROM JUKEBOX\nHere we describe how we extract audio representations\nfrom Jukebox which are suitable as input features for\n0 10 20 30 40 50 60 70\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Validation Performance\nTagging (Magnatagatune)\nGenre classification (GTZAN)\nEmotion recognition (Emomusic)\nKey detection (Giantsteps)\nAverage\nFigure 2 . Normalized validation performance of linear\nmodels trained on representations from speciﬁc layers of\nJukebox across four downstream MIR tasks. On average,\nthe strongest representations for these tasks come from the\nmiddle of Jukebox.\ntraining shallow models. While several pre-trained Juke-\nbox models exist with different sizes and conditioning in-\nformation, here we use the 5B-parameter model without\nlyrics conditioning (named “5b”), which is a sparse trans-\nformer [15, 16] containing 72 layers. Each layer yields\n4800-dimensional activations for each element in the codi-\nﬁed audio sequence, i.e., approximately 345 times per sec-\nond. To extract representations from this model for a par-\nticular audio waveform, we (1) resample the waveform to\n44.1kHz, (2) normalize it, (3) codify it using the Jukebox\nVQ-V AE model, and (4) input the codiﬁed audio into the\nlanguage model, interpreting its layer-wise activations as\nrepresentations. Jukebox was trained on∼24-second audio\nclips (codiﬁed audio sequences of length 8192)—we feed\nin this same amount of audio at a time when extracting rep-\nresentations. In addition to the genre and artist condition-\ning ﬁelds mentioned previously, Jukebox expects two ad-\nditional ﬁelds: total song length and clip offset—to ensure\nthat representations only depend on the input audio, we\nsimply pass in “unknown” for artist and genre, one minute\nfor song length, and zero seconds for clip offset. 4\nThe Jukebox language model yields an unwieldy\namount of data—for every 24-second audio clip, it emits\n24 ×345 ×72 ×4800 numbers, i.e., over 10GB if stored\nnaively as 32-bit ﬂoating point. We reduce the amount of\ndata by mean pooling across time, a common strategy in\nMIR transfer learning [4, 8], which aggregates more than\n10GB of activations to around 1MB (72 ×4800).\n3.1 Layer selection\nWhile pooling across time dramatically reduced the di-\nmensionality of Jukebox’s outputs, training shallow clas-\nsiﬁers on 72 ×4800 features is still computationally ex-\npensive. To further reduce the dimensionality, we use only\none of the layers from Jukebox—the middle layer ( 36)—\nyielding a total of 4800 features per 24 second audio clip.\n4 We observed in initial experiments that passing in ground-truth\nconditioning information had little effect on downstream performance.\nHence, we elected to pass in placeholder metadata to maintain the typical\ntype signature for audio feature extraction (audio as the only input).\nTask Size Metrics #Out\nTagging [31] 25860 AUC/AP 50\nGenre classiﬁcation [32] 930 Accuracy 10\nKey detection [33] 1763 Score 24\nEmotion recognition [34] 744 A/V R2 2\nTable 1. Basic information about the four tasks we con-\nsider in this work, including the size of each task-speciﬁc\ndataset in terms of number of labeled examples, relevant\nmetrics for each task, and the number of model outputs re-\nquired for each dataset.\nUnlike conventional pre-training, where the strongest rep-\nresentations for transfer learning typically lie at the end\nof the model [26], the strongest representations from pre-\ntrained language models tend to lie towards the middle\nof the network [27–30]. To conﬁrm this observation in\nour context, we trained linear models using representations\nfrom different layers of Jukebox on our downstream MIR\ntasks—average performance indeed peaked at the middle\nlayers (Figure 2).\nIn addition to using the middle layer, we experimented\nwith two other layer selection strategies: (1) sub-sampling\nlayers across the network, and (2) selecting relevant lay-\ners in a task-speciﬁc fashion. 5 We found that the simplest\nstrategy of using only the middle layer was equally effec-\ntive and more computationally practical 6 than the other\ntwo layer selection strategies.\n4. DOWNSTREAM TASK DESCRIPTIONS\nWe select four downstream MIR tasks to constitute a\nbenchmark for comparing different audio feature repre-\nsentations: (1) tagging, (2) genre classiﬁcation, (3) key\ndetection, and (4) emotion recognition. A summary of\nthe datasets used for each task appears in Table 1. These\ntasks were selected to cover a wide range of dataset sizes\n(744 examples for emotion recognition vs. 26k examples\nfor tagging) and subjectivity (emotion recognition is more\nsubjective vs. key detection is more objective). Addition-\nally, each task has an easily-accessible dataset with stan-\ndard evaluation criteria. We describe each of these tasks\nand metrics below.\n4.1 Tagging\nTagging involves determining which tags from a ﬁxed\nset of tags apply to a particular song. Categories of\ntags include genre (e.g., jazz), instrumentation (e.g., vio-\nlin), emotions (e.g., happy), and characteristics (e.g., fast).\nThere are two large datasets for tagging, which both con-\ntain human-annotated tags for 30-second clips: MagnaTa-\ngATune [31] (MTT) which contains around 26k clips,\nand a tagged subset of 240k clips from the Million Song\n5 This procedure selected layers that were the most jointly informative\nin a greedy fashion, measured by task performance with a linear probe.\n6 While the entirety of Jukebox does not ﬁt on a single commodity\nGPU with 12GB memory, the ﬁrst 36 layers do ﬁt.\nDataset [35] (MSD). While both datasets contain a large\nvocabulary of tags, typical usage involves limiting the vo-\ncabulary to the 50 most common tags in each.\nBecause it is the largest non-proprietary MIR dataset,\nMSD is commonly used for pre-training models for trans-\nfer learning. To mitigate an unfair advantage of methods\nwhich pre-train on MSD, we use MTT instead of MSD to\nbenchmark representations on tagging performance. While\nboth datasets are superﬁcially similar (choosing from 50\ntags for 30-second clips), their label distributions are quite\ndifferent: MSD is skewed towards genre tags, while MTT\nis skewed towards instrumentation tags.\nWe use the standard ( 12:1:3) train, validation, and test\nsplit for MTT [3]. Additionally, we report both common\nmetrics (both are macro-averaged over tags as is conven-\ntional): area under the receiver operating characteristic\ncurve (MTTAUC), and average precision (MTT AP). 7 We\nnote that inconsistencies in handling unlabeled examples\nfor past work on MTT have been observed [36]—some\nwork discards examples without top- 50 tags during train-\ning, evaluation, or both. In this work, we do not discard\nany examples.\n4.2 Genre classiﬁcation\nGenre classiﬁcation involves assigning the most appropri-\nate genre from a ﬁxed list for a given song. For this task,\nwe report accuracy on the GTZAN dataset [37], which\ncontains 30-second clips from10 distinct genres. We adopt\nthe “fault-ﬁltered” split from [32] which addresses some of\nthe reported issues with this dataset [38]. We note that this\ntask has a high degree of overlap with tagging, as tagging\ndatasets typically have a number of genres within their tag\nvocabulary. In fact, seven of ten genres in GTZAN are\npresent in the tag list of MSD.\n4.3 Key detection\nKey detection involves predicting both the scale and tonic\npitch class for the underlying key of a song. We inves-\ntigate the Giantsteps-MTG and Giantsteps datasets [33]\nwhich include songs in major and minor scales for all\npitch classes, i.e., a 24-way classiﬁcation task. As in past\nwork [39], we use the former for training and the latter\nfor testing. Because no standard validation split exists\nfor Giantsteps-MTG, we follow [32] and create an artist-\nstratiﬁed 4:1 split for training and validation, which we in-\nclude in our codebase for reproducibility. The music in this\ndataset is all electronic dance music, and the clips are two\nminutes in length. We report the typical weighted score\nmetric for Giantsteps (GS): an accuracy measure which\ngives partial credit for reasonable mistakes such as predict-\ning the relative minor key for the major ground truth [40].\n4.4 Emotion recognition\nEmotion recognition involves predicting human emotional\nresponse to a song. Data is collected by asking hu-\n7 Most past work refers to the quantity of average precision as area\nunder the precision-recall curve.\nRepresentation Pre-training strategy Dimensions\nCHROMA N/A 72\nMFCC N/A 120\nCHOI [4] MSD Tagging [3] 160\nMUSI CNN [8] MSD Tagging [3] 4194\nCLMR [14] Contrastive [13] 512\nJUKEBOX [1] CALM [20] 4800\nTable 2. Basic statistics about the six representations we\nexamine in this work.\nmans to report their emotional response on a two di-\nmensional valence-arousal plane [41], where valence in-\ndicates positive versus negative emotional response, and\narousal indicates emotional intensity. We use the Emomu-\nsic dataset [34], which contains 744 clips of 45 seconds\nin length. We investigate the static version of this task\nwhere original time-varying annotations are averaged to-\ngether to constitute a clip-level annotation. Because this\ndataset does not have a standard split, it is difﬁcult to di-\nrectly compare with past work. To simplify comparison\ngoing forward, we created an artist-stratiﬁed split of Emo-\nmusic, which is released in our codebase. We take the\nhighest reported numbers from past work to characterize\n“state-of-the-art” performance, though we note that these\nnumbers are not directly comparable to our own due to\ndiffering splits. We report the coefﬁcient of determination\nbetween the model predictions and human annotations for\narousal (EmoA) and valence (EmoV).\n5. PROBING EXPERIMENTS\nHere we describe our protocol for probing for information\nabout MIR tasks in representations from Jukebox and other\npre-trained models, i.e., measuring performance of shal-\nlow models trained on these tasks using different represen-\ntations as input features. We borrow the term “probing”\nfrom analogous investigations in NLP [19, 42, 43], how-\never such methodology is common in transfer learning for\nMIR [2–5, 7–10].\n5.1 Descriptions of representations\nIn addition to probing representations from Jukebox (an\nexemplar of CALM pre-training), we probe four additional\nrepresentations which are emblematic of three other MIR\npre-training strategies (Table 2). Before pre-training, hand-\ncrafted features were commonplace in MIR—as archetypal\nexamples, we probe constant-Q chromagrams (C HROMA )\nand Mel-frequency cepstral coefﬁcients (MFCC), ex-\ntracted with librosa [49] using the default settings. As\nin [4], we concatenate the mean and standard deviation\nacross time of both the features and their ﬁrst- and second-\norder discrete differences. We also probe two exam-\nples of the current conventional paradigm which pre-trains\non tagging using MSD: a convolutional model proposed\nby Choi et al. [4] (C HOI ), and a more modern convo-\nlutional model from [8] (M USI CNN). Finally, we com-\nTags Genre Key Emotion\nApproach MTT AUC MTTAP GTZAN GS Emo A EmoV Average\n(No pre-training) Probing CHROMA 77.6 18 .5 32 .8 56 .5 29 .3 5 .9 38 .7\n(No pre-training) Probing MFCC 85.8 30 .2 44 .8 14 .6 47 .9 26 .5 38 .7\n(Tagging) Probing CHOI [4] 89.7 36 .4 75 .9 13 .1 67 .3 43 .4 51 .9\n(Tagging) Probing MUSI CNN [8] 90.6 38 .3 79 .0 12 .8 70 .3 46 .6 53 .7\n(Contrastive) Probing CLMR [14] 89.4 36 .1 68 .6 14 .9 67 .8 45 .8 50 .8\n(CALM) Probing JUKEBOX [1] 91.5 41 .4 79 .7 66 .7 72 .1 61 .7 69 .9\nState-of-the-art [9, 8, 6, 44–46] 92.0 38.4 82.1 79 .6 70.4* 55.6* 72.5*\nPre-trained [9, 14, 6, 45, 45, 47] 92.0 35.9 82.1 75.8 67 .1* 55.6* 70.8*\nFrom scratch [8, 8, 48, 44, 44, 39] 90.7 38 .4 65 .8 74 .3 70 .4* 50.0* 66.2*\nTable 3. Comparing performance of probes on representations from a model pre-trained with CALM to other pre-\ntrained MIR models (top section) to reported state-of-the-art performance (bottom section) across four tasks: (1) tagging\n(MTTAUC/MTTAP), (2) genre classiﬁcation (GTZAN), (3) key detection (GS), and (4) emotion recognition (EmoA/EmoV).\nFor all six metrics, the max score is 100 and higher is better—see Section 4 for a full description of tasks/metrics. For each\nmetric, the best probing-based approach and the best approach overall are bolded. We also report an average score across\nall four tasks; tasks with multiple evaluation metrics are averaged beforehand. On all metrics, probing J UKEBOX is more\neffective than probing representations from other pre-trained models. Probing J UKEBOX is competitive with task-speciﬁc\nstate-of-the-art approaches for all tasks/metrics except key detection (GS). Note that the ordering of citations in the bottom\nsection corresponds to respective column ordering. * indicates that past work on Emomusic evaluates on different subsets\nof the dataset than our work and hence numbers are not directly comparable—see Section 4.4 for details.\npare to a recently-proposed strategy for MIR pre-training\ncalled contrastive learning of musical representations[14]\n(CLMR), though we note that the only available pre-\ntrained model from this work was trained on far less audio\n(a few thousand songs) than the other pre-trained models\n(CHOI , MUSI CNN, and J UKEBOX ).\nAll of these strategies operate at different frame\nrates, i.e., they produce a different number of representa-\ntion vectors for a ﬁxed amount of input audio. To handle\nthis, we follow common practice of mean pooling repre-\nsentations across time [4, 8]. While C HROMA , MFCC,\nand CLMR produce a single canonical representation per\nframe, we note that the other three produce multiple repre-\nsentations per frame, i.e., the outputs of individual layers\nin each model. For C HOI , we concatenate all layer repre-\nsentations together, which was shown to have strong per-\nformance on all downstream tasks in [4]. For M USI CNN,\nwe concatenate together the mean and max pool of three-\nsecond windows (before mean pooling across these win-\ndows), i.e., the default conﬁguration for that approach. For\nJUKEBOX , we use the middle layer of the network as moti-\nvated in Section 3.1. By using a single layer, we also miti-\ngate the potential of a superﬁcial dimensionality advantage\nfor J UKEBOX , as this induces a dimensionality similar to\nthat of M USI CNN ( 4800 and 4194 respectively; see Ta-\nble 2).\nUnlike other representations which operate on short\ncontext windows, C HOI and J UKEBOX were trained on\nlong windows of 29 seconds and 24 seconds of audio re-\nspectively. Accordingly, for the three datasets with short\nclips (tagging, genre classiﬁcation, and emotion recogni-\ntion all have clips between 30 and 45 seconds in length),\nwe adopt the policy from [4] and simply truncate the clips\nto the ﬁrst window when computing representations for\nCHOI and J UKEBOX . Because clips from the key detec-\ntion dataset are much longer (two minutes), we split the\nclips into 30-second windows for all methods and train\nprobes on these shorter windows. At test time, we ensem-\nble window-level predictions into clip-level predictions be-\nfore computing the score.\n5.2 Probing protocol\nTo probe representations for relevant information about\ndownstream MIR tasks, we train shallow supervised mod-\nels (linear models and one-layer MLPs) on each task us-\ning these representations as input features. As some repre-\nsentations may require different hyperparameter conﬁgura-\ntions for successful training, we run a grid search over the\nfollowing hyperparameters ( 216 total conﬁgurations) for\neach representation and task (24 total grid searches), using\nearly stopping based on task-speciﬁc metrics computed on\nthe validation set of each task:\n• Feature standardization: {off, on}\n• Model: {Linear, one-layer MLP with 512 hidden\nunits}\n• Batch size: { 64, 256}\n• Learning rate: { 1e-5, 1e-4, 1e-3}\n• Dropout probability: { 0.25, 0.5, 0.75}\n• L2 regularization: {0, 1e-4, 1e-3}\nWhile we use this same hyperparameter grid for all\ntasks, the learning objective varies by task (cross-entropy\nfor genre classiﬁcation and key detection, independent bi-\nnary cross-entropy per tag for tagging, and mean squared\nerror for emotion recognition) as does the number of probe\noutputs (Table 1). Some tasks have multiple metrics—we\nearly stop on MTT AUC for tagging as it is a more com-\nmon metric than MTTAP, and on the average of EmoA and\nEmoV for emotion recognition. We take the model with the\nbest early stopping performance from each grid search and\ncompute its performance on the task-speciﬁc test set.\n6. RESULTS AND DISCUSSION\nIn Table 3, we report performance of all representations\non all tasks and metrics, as well as average performance\nacross all tasks. Results are indicative that CALM is a\npromising paradigm for MIR pre-training. Speciﬁcally, we\nobserve that probing the representations from J UKEBOX\n(learned through CALM pre-training) achieves an average\nof 69.9, which is 30% higher relative to the average of the\nbest representation pre-trained with tagging (M USI CNN\nachieves an average of53.7). Performance of JUKEBOX on\nall individual metrics is also higher than that of any other\nrepresentation. Additionally, J UKEBOX achieves an aver-\nage performance that is 38% higher than that of CLMR.\nRepresentations from all pre-trained models outperform\nhand-crafted features (C HROMA and MFCC) on average.\nNote that these results are holistic comparisons across dif-\nferent model architectures, model sizes, and amounts of\npre-training data (e.g., CLMR was trained on far less data\nthan JUKEBOX ), and hence not sufﬁcient evidence to claim\nthat CALM is the “best” music pre-training strategy in gen-\neral.\nWe also observe that J UKEBOX contains substantially\nmore information relevant for key detection than other\nrepresentations. While C HROMA (spectrogram projected\nonto musical pitch classes) contains information relevant\nto key detection by design, all other representations besides\nJUKEBOX yield performance on par with that of a majority\nclassiﬁer (outputting “F minor” for every example scores\n15.0)—hence, these representations contain almost no in-\nformation about this task. For models pre-trained with tag-\nging (CHOI and MUSI CNN), intuition suggests that this is\nbecause none of the tags in MSD relate to key signature.\nFor CLMR, we speculate that the use of transposition as a\ndata augmentation strategy also results in a model that con-\ntains little useful information about key signature. While\ntagging and CLMR were not designed with the intention\nof supporting transfer to key detection, we argue that it is\ngenerally desirable to have a uniﬁed music representation\nwhich performs well on a multitude of downstream MIR\ntasks. Hence, we interpret the comparatively stronger per-\nformance of J UKEBOX on key detection as evidence that\nCALM pre-training addresses blind spots present in other\nMIR pre-training paradigms.\nIn the bottom section of Table 3, we also report state-of-\nthe-art performance for purpose-built methods on all tasks,\nwhich is further broken down by models which use any\nform of pre-training (including pre-training on additional\ntask-speciﬁc data as in [47]) vs. ones that are trained from\nscratch. Surprisingly, we observe that probing JUKEBOX is\ncompetitive with state-of-the-art for all tasks except for key\ndetection, and achieves an average only 4% lower relative\nto that of state-of-the-art. On tagging, probing J UKEBOX\nachieves similar MTTAUC to a strategy which pre-trains on\na proprietary dataset of 10M songs using supervision [9].\nWe interpret the strong performance of this simple probing\nsetup as evidence that CALM pre-training is a promising\npath towards models that are useful for many MIR tasks.\nWe believe that CALM pre-training is promising for\nMIR not just because of the strong performance of an ex-\nisting pre-trained model (Jukebox), but also because there\nare numerous avenues which may yield further improve-\nments for those with the data and computational resources\nto explore them. Firstly, CALM could be scaled up to\npre-train even larger models on more data (Jukebox was\ntrained on 1M songs, while Spotify has an estimated 70M\nsongs in its catalog). In [50], it is observed that increasing\nmodel and dataset size yields predictable improvements to\ncross-entropy for language modeling in NLP, an insight\nwhich may also hold for CALM pre-training for MIR. Sec-\nondly, we anticipate that ﬁne-tuning a model pre-trained\nwith CALM would outperform our probing setup. Finally,\ntaking a cue from related ﬁndings in NLP, we speculate that\nCALM pre-training with a bidirectional model and masked\nlanguage modeling (as in BERT [23]) would outperform\nthe generative setup of Jukebox (that of GPT [51]).\n7. RELATED WORK\nTransfer learning has been an active area of study in MIR\nfor over a decade. An early effort seeking to replace\nhand-crafted features used neural networks to automati-\ncally extract context-independent features from unlabeled\naudio [52] and used those features for a supervised learn-\ning task. Other early efforts focused on learning shared\nembedding spaces between audio and metadata [53, 2] or\ndirectly using outputs from pre-trained tagging models for\nmusic similarity judgements [54].\nThe predominant strategy for MIR pre-training us-\ning large tagging datasets was ﬁrst proposed by\nvan den Oord et al. 2014 [3]. This work pre-trained deep\nneural networks on MSD and demonstrated promising per-\nformance on other tagging and genre classiﬁcation tasks.\nChoi et al. 2017 [4] pre-trained on MSD but using a con-\nvolutional neural network and also explored a more diverse\narray of downstream tasks—we use their pre-trained model\nas one of our baselines. More recent improvements use the\nsame approach with different architectures [6,8], the latest\nof which is another one of our baselines.\nOther strategies for MIR transfer learning have been\nproposed. Some work pre-trains on music metadata (e.g.,\nartist, album) instead of tags [5, 7]. In contrast to the man-\nual annotations required for tagging-based pre-training,\nmetadata is much cheaper to obtain, but performance of\npre-training on metadata is comparable to that of pre-\ntraining on tagging. Kim et al. 2020 [10] improve over\nChoi et al. 2017 [4] using a multi-task approach that pre-\ntrains on both tags and metadata. Huang et al. [9] demon-\nstrate that metadata can be combined with proprietary co-\nlistening data for pre-training on 10M songs to achieve\nstate-of-the-art performance on MTT—probing represen-\ntations from CALM pre-training on 1M songs achieves\ncomparable performance on MTT (Table 3). Finally, con-\ntrastive learning [13] has been proposed as a strategy for\nMIR pre-training [55,56,14]—we compare to such a model\nfrom Spijkervet and Burgoyne 2021 [14].\nWhile CALM has not previously been explored for\nMIR transfer learning, it has been explored for other pur-\nposes. van den Oord et al. 2017 [20] ﬁrst proposed CALM\nand used it for unconditional speech generation. Varia-\ntions of CALM have been used as pre-training for speech\nrecognition [57, 58] and urban sound classiﬁcation [59].\nCALM has also been explored for music generation [17,1].\nCALM is related to past work on language modeling of\nraw (i.e., not codiﬁed) waveforms [21,60,61], which tends\nto be less effective for capturing long-term dependencies\ncompared to modeling codiﬁed audio. Language models\nhave also been used extensively for modeling symbolic\nmusic [62–64], including some work on pre-training on\nlarge corpora of scores for transfer learning [65, 66].\n8. CONCLUSION\nIn this work we demonstrated that CALM is a promis-\ning pre-training strategy for MIR. Compared to conven-\ntional approaches, CALM learns richer representations by\nmodeling audio instead of labels. Moreover, CALM al-\nlows MIR researchers to repurpose NLP methodology—\nhistorically, repurposing methodology from another ﬁeld\n(computer vision) has provided considerable leverage for\nMIR. Finally, CALM suggests a direction for MIR re-\nsearch where enormous models pre-trained on large music\ncatalogs break new ground on MIR tasks, analogous to on-\ngoing paradigm shifts in other areas of machine learning.\n9. ACKNOWLEDGEMENTS\nWe would like to thank Nelson Liu, Mina Lee, John He-\nwitt, Janne Spijkervet, Minz Won, Jordi Pons, Ethan Chi,\nMichael Xie, Ananya Kumar, and Glen Husman for helpful\nconversations about this work. We also thank all reviewers\nfor their helpful feedback.\n10. REFERENCES\n[1] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,\nand I. Sutskever, “Jukebox: A generative model for\nmusic,”arXiv:2005.00341, 2020.\n[2] P. Hamel, M. Davies, K. Yoshii, and M. Goto, “Trans-\nfer learning in MIR: Sharing learned latent representa-\ntions for music audio classiﬁcation and similarity,” in\nISMIR, 2013.\n[3] A. van den Oord, S. Dieleman, and B. Schrauwen,\n“Transfer learning by supervised pre-training for\naudio-based music classiﬁcation,” in ISMIR, 2014.\n[4] K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Trans-\nfer learning for music classiﬁcation and regression\ntasks,” inISMIR, 2017.\n[5] J. Park, J. Lee, J. Park, J.-W. Ha, and J. Nam,\n“Representation learning of music using artist labels,”\narXiv:1710.06648, 2017.\n[6] J. Lee, J. Park, K. L. Kim, and J. Nam, “SampleCNN:\nEnd-to-end deep convolutional neural networks using\nvery small ﬁlters for music classiﬁcation,”Applied Sci-\nences, 2018.\n[7] J. Lee, J. Park, and J. Nam, “Representation learning\nof music using artist, album, and track information,”\narXiv:1906.11783, 2019.\n[8] J. Pons and X. Serra, “musicnn: Pre-trained convolu-\ntional neural networks for music audio tagging,”ISMIR\nLate-breaking Demos, 2019.\n[9] Q. Huang, A. Jansen, L. Zhang, D. P. Ellis, R. A.\nSaurous, and J. Anderson, “Large-scale weakly-\nsupervised content embeddings for music recommen-\ndation and tagging,” inICASSP, 2020.\n[10] J. Kim, J. Urbano, C. C. Liem, and A. Hanjalic, “One\ndeep music representation to rule them all? A com-\nparative analysis of different representation learning\nstrategies,”Neural Computing and Applications, 2020.\n[11] B. McFee, J. W. Kim, M. Cartwright, J. Salamon, R. M.\nBittner, and J. P. Bello, “Open-source practices for\nmusic signal processing research: Recommendations\nfor transparent, sustainable, and reproducible audio re-\nsearch,”IEEE Signal Processing Magazine, 2018.\n[12] W. Chen, J. Keast, J. Moody, C. Moriarty, F. Villalo-\nbos, V . Winter, X. Zhang, X. Lyu, E. Freeman, J. Wang\net al., “Data usage in MIR: history & future recommen-\ndations,” inISMIR, 2019.\n[13] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,\n“A simple framework for contrastive learning of visual\nrepresentations,” inICML, 2020.\n[14] J. Spijkervet and J. A. Burgoyne, “Contrastive learning\nof musical representations,”arXiv:2103.09410, 2021.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n“Attention is all you need,”arXiv:1706.03762, 2017.\n[16] R. Child, S. Gray, A. Radford, and I. Sutskever,\n“Generating long sequences with sparse transformers,”\narXiv:1904.10509, 2019.\n[17] S. Dieleman, A. van den Oord, and K. Simonyan, “The\nchallenge of realistic music generation: modelling raw\naudio at scale,” inNIPS, 2018.\n[18] G. Alain and Y . Bengio, “Understanding inter-\nmediate layers using linear classiﬁer probes,”\narXiv:1610.01644, 2016.\n[19] D. Hupkes, S. Veldhoen, and W. Zuidema, “Visual-\nisation and ‘diagnostic classiﬁers’ reveal how recur-\nrent and recursive neural networks process hierarchical\nstructure,” Journal of Artiﬁcial Intelligence Research ,\n2018.\n[20] A. van den Oord, O. Vinyals, and K. Kavukcuoglu,\n“Neural discrete representation learning,”\narXiv:1711.00937, 2017.\n[21] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan,\nO. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu, “WaveNet: A generative model for\nraw audio,”arXiv:1609.03499, 2016.\n[22] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner,\nC. Clark, K. Lee, and L. Zettlemoyer, “Deep contextu-\nalized word representations,” inNorth American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, 2018.\n[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“BERT: Pre-training of deep bidirectional transformers\nfor language understanding,” inNorth American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, 2018.\n[24] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, “Language models are unsupervised mul-\ntitask learners,”OpenAI Blog, 2019.\n[25] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian,\nZ. Yang, and J. Tang, “GPT understands, too,”\narXiv:2103.10385, 2021.\n[26] M. D. Zeiler and R. Fergus, “Visualizing and under-\nstanding convolutional networks,” in European Con-\nference on Computer Vision, 2014.\n[27] N. F. Liu, M. Gardner, Y . Belinkov, M. E. Peters, and\nN. A. Smith, “Linguistic knowledge and transferabil-\nity of contextual representations,” arXiv:1903.08855,\n2019.\n[28] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan,\nand I. Sutskever, “Generative pretraining from pixels,”\nin ICML, 2020.\n[29] E. A. Chi, J. Hewitt, and C. D. Manning, “Finding uni-\nversal grammatical relations in multilingual bert,” in\nAssociation for Computational Linguistics, 2020.\n[30] A. Rogers, O. Kovaleva, and A. Rumshisky, “A primer\nin BERTology: What we know about how BERT\nworks,” Transactions of the Association for Computa-\ntional Linguistics, 2020.\n[31] E. Law, K. West, M. I. Mandel, M. Bay, and J. S.\nDownie, “Evaluation of algorithms using games: The\ncase of music tagging.” inISMIR, 2009.\n[32] C. Kereliuk, B. L. Sturm, and J. Larsen, “Deep learning\nand music adversaries,” IEEE Transactions on Multi-\nmedia, 2015.\n[33] P. Knees, Á. Faraldo Pérez, H. Boyer, R. V ogl, S. Böck,\nF. Hörschläger, M. Le Goff et al. , “Two data sets\nfor tempo estimation and key detection in electronic\ndance music annotated from user corrections,” in IS-\nMIR, 2015.\n[34] M. Soleymani, M. N. Caro, E. M. Schmidt, C.-Y . Sha,\nand Y .-H. Yang, “1000 songs for emotional analysis\nof music,” in ACM International Workshop on Crowd-\nsourcing for Multimedia, 2013.\n[35] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and\nP. Lamere, “The Million Song Dataset,” in ISMIR,\n2011.\n[36] M. Won, A. Ferraro, D. Bogdanov, and X. Serra, “Eval-\nuation of CNN-based automatic music tagging mod-\nels,”arXiv:2006.00751, 2020.\n[37] G. Tzanetakis and P. Cook, “Musical genre classiﬁca-\ntion of audio signals,” IEEE Transactions on Speech\nand Audio Processing, 2002.\n[38] B. L. Sturm, “The GTZAN dataset: its contents, its\nfaults, their effects on evaluation, and its future use,”\narXiv:1306.1461, 2013.\n[39] F. Korzeniowski and G. Widmer, “End-to-end musical\nkey estimation using a convolutional neural network,”\nin European Signal Processing Conference, 2017.\n[40] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. Ellis, “mir_eval: A trans-\nparent implementation of common mir metrics,” in IS-\nMIR, 2014.\n[41] A. Huq, J. P. Bello, and R. Rowe, “Automated music\nemotion recognition: A systematic evaluation,” Jour-\nnal of New Music Research, 2010.\n[42] A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni, “What you can cram into a single vec-\ntor: Probing sentence embeddings for linguistic prop-\nerties,”arXiv:1805.01070, 2018.\n[43] J. Hewitt and C. D. Manning, “A structural probe\nfor ﬁnding syntax in word representations,” in North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\n2019.\n[44] F. Weninger, F. Eyben, and B. Schuller, “On-line\ncontinuous-time music mood regression with deep re-\ncurrent neural networks,” inICASSP, 2014.\n[45] E. Koh and S. Dubnov, “Comparison and analysis of\ndeep audio embeddings for music emotion recogni-\ntion,”arXiv:2104.06517, 2021.\n[46] Pioneer, “rekordbox v3.2.2,” 2015. [Online]. Avail-\nable: http://www.cp.jku.at/datasets/giantsteps/\n[47] J. Jiang, G. G. Xia, and D. B. Carlton, “MIREX 2019\nsubmission: Crowd annotation for audio key estima-\ntion,”MIREX, 2019.\n[48] F. Medhat, D. Chesmore, and J. Robinson, “Masked\nconditional neural networks for audio classiﬁcation,”\nin International Conference on Artiﬁcial Neural Net-\nworks, 2017.\n[49] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\nE. Battenberg, and O. Nieto, “librosa: Audio and music\nsignal analysis in python,” in Python in Science Con-\nference, 2015.\n[50] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown,\nB. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei, “Scaling laws for neural language mod-\nels,”arXiv:2001.08361, 2020.\n[51] A. Radford, K. Narasimhan, T. Salimans, and\nI. Sutskever, “Improving language understanding by\ngenerative pre-training,”OpenAI Blog, 2018.\n[52] P. Hamel and D. Eck, “Learning features from music\naudio with deep belief networks,” inISMIR, 2010.\n[53] J. Weston, S. Bengio, and P. Hamel, “Multi-tasking\nwith joint semantic spaces for large-scale music anno-\ntation and retrieval,” Journal of New Music Research ,\n2011.\n[54] K. Seyerlehner, M. Schedl, R. Sonnleitner, D. Hauger,\nand B. Ionescu, “From improved auto-taggers to im-\nproved music similarity measures,” in International\nWorkshop on Adaptive Multimedia Retrieval, 2012.\n[55] X. Favory, K. Drossos, T. Virtanen, and X. Serra,\n“Learning contextual tag embeddings for cross-modal\nalignment of audio and tags,”arXiv:2010.14171, 2020.\n[56] A. Ferraro, X. Favory, K. Drossos, Y . Kim, and D. Bog-\ndanov, “Enriched music representations with multiple\ncross-modal contrastive learning,” IEEE Signal Pro-\ncessing Letters, 2021.\n[57] A. Baevski, M. Auli, and A. Mohamed, “Effectiveness\nof self-supervised pre-training for speech recognition,”\narXiv:1911.03912, 2019.\n[58] A. Baevski, H. Zhou, A. Mohamed, and M. Auli,\n“wav2vec 2.0: A framework for self-supervised learn-\ning of speech representations,” arXiv:2006.11477,\n2020.\n[59] P. Verma and J. Smith, “A framework for contrastive\nand generative learning of audio representations,”\narXiv:2010.11459, 2020.\n[60] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain,\nJ. Sotelo, A. Courville, and Y . Bengio, “SampleRNN:\nAn unconditional end-to-end neural audio generation\nmodel,” inICLR, 2017.\n[61] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury,\nN. Casagrande, E. Lockhart, F. Stimberg, A. van den\nOord, S. Dieleman, and K. Kavukcuoglu, “Efﬁcient\nneural audio synthesis,” inICML, 2018.\n[62] D. Eck and J. Schmidhuber, “Finding temporal struc-\nture in music: Blues improvisation with LSTM recur-\nrent networks,” inIEEE Workshop on Neural Networks\nfor Signal Processing, 2002.\n[63] I. Simon and S. Oore, “Performance RNN: Generating\nmusic with expressive timing and dynamics,” 2017.\n[Online]. Available: https://magenta.tensorﬂow.org/\nperformance-rnn\n[64] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer,\nI. Simon, C. Hawthorne, A. M. Dai, M. D. Hoffman,\nM. Dinculescu, and D. Eck, “Music transformer,” in\nICLR, 2019.\n[65] C. Donahue, H. H. Mao, Y . E. Li, G. W. Cottrell, and\nJ. McAuley, “LakhNES: Improving multi-instrumental\nmusic generation with cross-domain pre-training,” in\nISMIR, 2019.\n[66] H.-T. Hung, C.-Y . Wang, Y .-H. Yang, and H.-M.\nWang, “Improving automatic jazz melody generation\nby transfer learning techniques,” inAsia-Paciﬁc Signal\nand Information Processing Association Annual Sum-\nmit and Conference, 2019.",
  "topic": "Key (lock)",
  "concepts": [
    {
      "name": "Key (lock)",
      "score": 0.7864437699317932
    },
    {
      "name": "Computer science",
      "score": 0.7652890682220459
    },
    {
      "name": "Language model",
      "score": 0.5185145139694214
    },
    {
      "name": "Natural language processing",
      "score": 0.4966135621070862
    },
    {
      "name": "Representation (politics)",
      "score": 0.48757022619247437
    },
    {
      "name": "Music information retrieval",
      "score": 0.4542364478111267
    },
    {
      "name": "Speech recognition",
      "score": 0.40897297859191895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3691120743751526
    },
    {
      "name": "Musical",
      "score": 0.07871457934379578
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": []
}