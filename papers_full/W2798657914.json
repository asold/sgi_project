{
  "title": "Syllable-Based Sequence-to-Sequence Speech Recognition with the Transformer in Mandarin Chinese",
  "url": "https://openalex.org/W2798657914",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2357468579",
      "name": "Zhou Shiyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743815191",
      "name": "Dong, Linhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2067510015",
      "name": "Xu Shuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1937328933",
      "name": "Xu, Bo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2515801922",
    "https://openalex.org/W2749514303",
    "https://openalex.org/W2963920996",
    "https://openalex.org/W2530876040",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2514969556",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2513938599",
    "https://openalex.org/W2750499125",
    "https://openalex.org/W2775766866",
    "https://openalex.org/W2963882470",
    "https://openalex.org/W1526236009",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2167200294",
    "https://openalex.org/W854541894",
    "https://openalex.org/W2962826786",
    "https://openalex.org/W2290401751",
    "https://openalex.org/W2786031273",
    "https://openalex.org/W2627092829",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2775304348",
    "https://openalex.org/W2888226713",
    "https://openalex.org/W1553004968"
  ],
  "abstract": "Sequence-to-sequence attention-based models have recently shown very promising results on automatic speech recognition (ASR) tasks, which integrate an acoustic, pronunciation and language model into a single neural network. In these models, the Transformer, a new sequence-to-sequence attention-based model relying entirely on self-attention without using RNNs or convolutions, achieves a new single-model state-of-the-art BLEU on neural machine translation (NMT) tasks. Since the outstanding performance of the Transformer, we extend it to speech and concentrate on it as the basic architecture of sequence-to-sequence attention-based model on Mandarin Chinese ASR tasks. Furthermore, we investigate a comparison between syllable based model and context-independent phoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese. Additionally, a greedy cascading decoder with the Transformer is proposed for mapping CI-phoneme sequences and syllable sequences into word sequences. Experiments on HKUST datasets demonstrate that syllable based model with the Transformer performs better than CI-phoneme based counterpart, and achieves a character error rate (CER) of \\emph{$28.77\\%$}, which is competitive to the state-of-the-art CER of $28.0\\%$ by the joint CTC-attention based encoder-decoder network.",
  "full_text": "Syllable-Based Sequence-to-Sequence Speech Recognition with the\nTransformer in Mandarin Chinese\nShiyu Zhou1,2, Linhao Dong1,2, Shuang Xu1, Bo Xu1\n1Institute of Automation, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences\n{zhoushiyu2013, donglinhao2015, shuang.xu, xubo}@ia.ac.cn\nAbstract\nSequence-to-sequence attention-based models have recently\nshown very promising results on automatic speech recognition\n(ASR) tasks, which integrate an acoustic, pronunciation and\nlanguage model into a single neural network. In these models,\nthe Transformer, a new sequence-to-sequence attention-based\nmodel relying entirely on self-attention without using RNNs\nor convolutions, achieves a new single-model state-of-the-art\nBLEU on neural machine translation (NMT) tasks. Since\nthe outstanding performance of the Transformer, we extend it\nto speech and concentrate on it as the basic architecture of\nsequence-to-sequence attention-based model on Mandarin Chi-\nnese ASR tasks. Furthermore, we investigate a comparison be-\ntween syllable based model and context-independent phoneme\n(CI-phoneme) based model with the Transformer in Mandarin\nChinese. Additionally, a greedy cascading decoder with the\nTransformer is proposed for mapping CI-phoneme sequences\nand syllable sequences into word sequences. Experiments on\nHKUST datasets demonstrate that syllable based model with\nthe Transformer performs better than CI-phoneme based coun-\nterpart, and achieves a character error rate (CER) of 28.77%,\nwhich is competitive to the state-of-the-art CER of 28.0% by\nthe joint CTC-attention based encoder-decoder network.\nIndex Terms: ASR, multi-head attention, syllable based acous-\ntic modeling, sequence-to-sequence\n1. Introduction\nExperts have shown signiﬁcant interest in the area of sequence-\nto-sequence modeling with attention [1, 2, 3, 4] on ASR tasks\nin recent years. Sequence-to-sequence attention-based models\nintegrate separate acoustic, pronunciation and language models\nof a conventional ASR system into a single neural network [5]\nand do not make the conditional independence assumptions as\nin standard hidden Markov based model [6].\nSequence-to-sequence attention-based models are com-\nmonly comprised of an encoder, which consists of multiple re-\ncurrent neural network (RNN) layers that model the acoustics,\nand a decoder, which consists of one or more RNN layers that\npredict the output sub-word sequence. An attention layer acts\nas the interface between the encoder and the decoder: it selects\nframes in the encoder representation that the decoder should at-\ntend to in order to predict the next sub-word unit [5]. However,\nRNNs maintain a hidden state of the entire past that prevents\nparallel computation within a sequence. In order to reduce se-\nquential computation, the model architecture of the Transformer\nhas been proposed in [7]. This model architecture eschews re-\ncurrence and instead relies entirely on an attention mechanism\nThe research work is supported by the National Key Research and\nDevelopment Program of China under Grant No. 2016YFB1001404.\nto draw global dependencies between input and output, which\nallows for signiﬁcantly more parallelization and achieves a new\nsingle-model state-of-the-art BLEU on NMT tasks [7]. Since\nthe outstanding performance of the Transformer, this paper fo-\ncuses on it as the basic architecture of sequence-to-sequence\nattention-based model on Mandarin Chinese ASR tasks.\nRecently various modeling units of sequence-to-sequence\nattention-based models have been studied on English ASR\ntasks, such as graphemes, CI-phonemes, context-dependent\nphonemes and word piece models [1, 5, 8]. However, few\nrelated works have been explored by sequence-to-sequence\nattention-based models on Mandarin Chinese ASR tasks. As\nwe know, Mandarin Chinese is a syllable-based language and\nsyllables are their logical unit of pronunciation. These syllables\nhave a ﬁxed number (around 1400 pinyins with tones are used in\nthis work) and each written character corresponds to a syllable.\nIn addition, syllables are a longer linguistic unit, which reduces\nthe difﬁculty of syllable choices in the decoder by sequence-to-\nsequence attention-based models. Moreover, syllables have the\nadvantage of avoiding out-of-vocabulary (OOV) problem.\nDue to these advantages of syllables, we are concerned\nwith syllables as the modeling unit in this paper and investigate\na comparison between CI-phoneme based model and syllable\nbased model with the Transformer on Mandarin Chinese ASR\ntasks. Moreover, Since we investigate the comparison between\nCI-phonemes and syllables, these CI-phoneme sequences or\nsyllable sequences from the Transformer have to be converted\ninto word sequences for the performance comparison in terms\nof CER. The conversion from CI-phoneme sequences or sylla-\nble sequences to word sequences can be regarded as a sequence-\nto-sequence task, which is modeled by the Transformer in this\npaper. Then we propose a greedy cascading decoder with the\nTransformer to maximize the posterior probability Pr(W|X)\napproximately. Experiments on HKUST datasets reveal that\nthe Transformer performs very well on Mandarin Chinese ASR\ntasks. Moreover, we experimentally conﬁrm that syllable based\nmodel with the Transformer can outperform CI-phoneme based\ncounterpart, and achieve a CER of 28.77%, which is compet-\nitive to the state-of-the-art CER of 28.0% by the joint CTC-\nattention based encoder-decoder network [9].\nThe rest of the paper is organized as follows. After an\noverview of the related work in Section 2, Section 3 describes\nthe proposed method in detail. we then show experimental re-\nsults in Section 4 and conclude this work in Section 5.\n2. Related work\nSequence-to-sequence attention-based models have shown very\nencouraging results on English ASR tasks [1, 8, 10]. How-\never, it is quite difﬁcult to apply it to Mandarin Chinese ASR\ntasks. In [11], Chan et al. proposed Character-Pinyin sequence-\narXiv:1804.10752v2  [eess.AS]  4 Jun 2018\nto-sequence attention-based model on Mandarin Chinese ASR\ntasks. The Pinyin information was only used during training for\nimproving the performance of the character model. Instead of\nusing joint Character-Pinyin model, [12] directly used Chinese\ncharacters as network output by mapping the one-hot charac-\nter representation to an embedding vector via a neural network\nlayer.\nIn this paper, we are concerned with syllables as the mod-\neling unit. Acoustic models using syllables as the modeling\nunit have been investigated for a long time [13, 14, 15]. Gana-\npathiraju et al. have ﬁrst shown that syllable based acoustic\nmodels can outperform context dependent phone based acous-\ntic models with GMM [14]. Wu et al. experimented on sylla-\nble based context dependent Chinese acoustic model and dis-\ncovered that context dependent syllable based acoustic models\ncan show promising performance [15]. Qu et al. [13] explored\nthe CTC-SMBR-LSTM using syllables as outputs and veriﬁed\nthat syllable based CTC model can perform better than CI-\nphoneme based CTC model on Mandarin Chinese ASR tasks.\nInspired by [13], we extend their work from CTC based models\nto sequence-to-sequence attention-based models.\nUsing syllables as the modeling unit, it is natural to con-\nsider the conversion from Chinese syllable sequences to Chi-\nnese word sequences as a task of labelling unsegmented se-\nquence data. Liu et al. [16] proposed RNN based supervised\nsequence labelling method with CTC algorithm to achieve a di-\nrect conversion from syllable sequences to word sequences.\n3. System overview\n3.1. Transformer model\nThe Transformer model architecture is the same as sequence-to-\nsequence attention-based models except relying entirely on self-\nattention and position-wise, fully connected layers for both the\nencoder and decoder [7]. The encoder maps an input sequence\nof symbol representations x = (x1,...,x n) to a sequence of con-\ntinuous representations z = (z1,...,z n). Given z, the decoder\nthen generates an output sequence y = (y1,...,y m) of symbols\none element at a time.\n3.1.1. Multi-head attention\nAn attention function maps a query and a set of key-value pairs\nto an output, where the query, keys, values, and output are all\nvectors. The output is computed as a weighted sum of the val-\nues, where the weight assigned to each value is computed by a\ncompatibility function of the query with the corresponding key\n[7]. Scaled dot-product attention is adopted as the basic atten-\ntion function in the Transformer, which describes (1):\nAttention(Q,K,V ) =softmax\n(QKT\n√dk\n)\nV (1)\nWhere the dimension of query Q and key K are the same, which\nare dk, and dimension of value V is dv.\nInstead of performing a single attention function, the Trans-\nformer employs the multi-head attention (MHA) which projects\nthe queries, keys and values htimes with different, learned lin-\near projections to dk, dk and dv dimensions. On each of these\nprojected versions of queries, keys and values, the basic atten-\ntion function is performed in parallel, yielding dv-dimensional\noutput values. These are concatenated and projected again, re-\nsulting in the ﬁnal values. The equations can be represented as\nMulti-Head\nAttention\nK V Q\nPositional\nEncoding\nFeed\nForward\nAdd & Norm\nAdd & Norm\nMasked\nMulti-Head\nAttention\nK V Q\nPositional\nEncoding\nOutput\nEmbedding\nMulti-Head\nAttention\nAdd & Norm\nAdd & Norm\nOutputs\n(shifted right)\nFeed\nForward\nAdd & Norm\nK V Q\nLinear\nOutput\nProbabilities\nSoftmax\nN×\nN×\nFbank\nDim & Norm\nFigure 1: The architecture of the ASR Transformer.\nfollows [7]:\nMultiHead(Q,K,V ) =Concat(head1,...,head h)WO (2)\nwhereheadi = Attention\n(\nQWQ\ni ,KW K\ni ,VW V\ni\n)\n(3)\nWhere the projections are parameter matrices WQ\ni ∈\nRdmodel×dk , WK\ni ∈Rdmodel×dk , WV\ni ∈Rdmodel×dv , WO ∈\nRhdv×dmodel , h is the number of heads, and dmodel is the\nmodel dimension.\nMHA behaves like ensembles of relatively small attentions\nto allow the model to jointly attend to information from dif-\nferent representation subspaces at different positions, which is\nbeneﬁcial to learn complicated alignments between the encoder\nand decoder.\n3.1.2. Transformer model architecture\nThe architecture of the ASR Transformer is shown in Figure 1,\nwhich stacks MHA and position-wise, fully connected layers\nfor both the encode and decoder. The encoder is composed of\na stack of N identical layers. Each layer has two sub-layers.\nThe ﬁrst is a MHA, and the second is a position-wise fully\nconnected feed-forward network. Residual connections are em-\nployed around each of the two sub-layers, followed by a layer\nnormalization. The decoder is similar to the encoder except in-\nserting a third sub-layer to perform a MHA over the output of\nthe encoder stack. To prevent leftward information ﬂow and\npreserve the auto-regressive property in the decoder, the self-\nattention sub-layers in the decoder mask out all values corre-\nsponding to illegal connections. In addition, positional encod-\nings [7] are added to the input at the bottoms of these encoder\nand decoder stacks, which inject some information about the\nrelative or absolute position in the sequence to make use of the\norder of the sequence.\nSince our ASR experiments use 80-dimensional log-Mel\nﬁlterbank features, we explore a linear transformation with a\nlayer normalization to convert the input dimension to the model\ndimension dmodel for dimension matching, which is marked out\nby a dotted line in Figure 1.\n3.2. Greedy cascading decoder with the Transformer\nAs syllables and CI-phonemes are investigated in this paper, the\nCI-phoneme sequences or syllable sequences have to be con-\nverted into word sequences using a lexicon during beam-search\ndecoding.\nThe speech recognition problem can be deﬁned as the prob-\nlem of ﬁnding word sequence W that maximizes posterior prob-\nability Pr(W|X) given observation X, and can transform as\nfollows [17].\n˜W = argmax\nW\nPr(W|X) (4)\n= argmax\nW\n∑\ns\nPr(W|s)Pr(s|X) (5)\n≈ argmax\nW\nPr(W|s)Pr(s|X) (6)\nHere, Pr(s|X) is the probability from observation X to sub-\nword unit sequence s, Pr(W|s) is the the probability from sub-\nword unit sequence sto word sequence W.\nAccording to equation (6), we propose that both Pr(s|X)\nand Pr(W|s) can be regarded as sequence-to-sequence trans-\nformations, which can be modeled by sequence-to-sequence\nattention-based models, speciﬁcally the Transformer is used in\nthe paper.\nThen, the greedy cascading decoder with the Transformer\nis proposed to directly estimate equation (6). First, the best sub-\nword unit sequence s is calculated by the Transformer from\nobservation X to sub-word unit sequence with beam size β.\nAnd then, the best word sequence W is chosen by the Trans-\nformer from sub-word unit sequence to word sequence with\nbeam size γ. Through cascading two sequence-to-sequence\nattention-based models, we assume that equation (6) can be ap-\nproximated.\nIn this work we employ β = 13and γ = 6.\n4. Experiment\n4.1. Data\nThe HKUST corpus (LDC2005S15, LDC2005T32), a corpus\nof Mandarin Chinese conversational telephone speech, is col-\nlected and transcribed by Hong Kong University of Science and\nTechnology (HKUST) [18], which contains 150-hour speech,\nand 873 calls in the training set and 24 calls in the test set. All\nexperiments are conducted using 80-dimensional log-Mel ﬁlter-\nbank features, computed with a 25ms window and shifted every\n10ms. The features are normalized via mean subtraction and\nvariance normalization on the speaker basis. Similar to [19, 20],\nat the current frame t, these features are stacked with 3 frames\nto the left and downsampled to a 30ms frame rate.\n4.2. Training\nWe perform our experiments on the base model and big model\n(i.e. D512-H8 and D1024-H16 respectively) of the Transformer\nfrom [7]. The basic architecture of these two models is the same\nbut different parameters setting. Table 1 lists the experimental\nparameters between these two models. The Adam algorithm\n[21] with gradient clipping and warmup is used for optimiza-\ntion. During training, label smoothing of value ϵls = 0.1 is\nemployed [22].\nTable 1: Experimental parameters conﬁguration.\nmodel N dmodel h dk dv warmup\nD512-H8 6 512 8 64 64 4000 steps\nD1024-H16 6 1024 16 64 64 12000 steps\nFirst, for the Transformer from observation X to sub-word\nunit sequence, 118 CI-phonemes without silence (phonemes\nwith tones) are employed in the CI-phoneme based experi-\nments and 1384 syllables (pinyins with tones) in the sylla-\nble based experiments. Extra tokens (i.e. an unknown token\n(<UNK>), a padding token (<PAD>), and sentence start and\nend tokens ( <S>/<\\S>)) are appended to the outputs, mak-\ning the total number of outputs 122 and 1388 respectively in\nthe CI-phoneme based model and syllable based model. Sec-\nond, for the Transformer from sub-word unit sequence to word\nsequence, we collect all words from the training data together\nwith appended extra tokens and the total number of outputs is\n28444. In our experiments, we only train the Transformer from\nsub-word unit sequence to word sequence by the base model.\nStandard tied-state cross-word triphone GMM-HMMs are\nﬁrst trained with maximum likelihood estimation to generate\nCI-phoneme alignments on training set and test set for han-\ndling multiple pronunciations of the same word in Mandarin\nChinese. we then generate syllable alignments through these\nCI-phoneme alignments according to the lexicon. Finally, we\nproceed to train the Transformer with these alignments.\nIn order to verify the effectiveness of the greedy cascading\ndecoder proposed in this paper, the CI-phoneme and syllable\nalignments on test data are converted into word sequences using\nthe trained Pr(W|s) models. We can get a CER of 4.70% on\nthe CI-phoneme based model and 4.15% on the syllable based\nmodel respectively, which are the lower bounds of our exper-\niments. If sub-word unit sequences, calculated by the Trans-\nformer from observation X to sub-word unit sequence s, can\napproximate to these corresponding alignments, our experimen-\ntal results can approach the lower bounds using the greedy cas-\ncading decoder.\nFigure 2 visualizes the self-attention alignments in the en-\ncoder layer and the vanilla attention alignments in the encoder-\ndecoder layer by Tensorﬂow [23]. As can be seen in the ﬁgure,\nboth self-attention matrix and vanilla attention matrix appear\nvery localized, which let us to understand how changing the at-\ntention window inﬂuences the CER.\n4.3. Results of CI-phoneme and syllable based model\nOur results are summarized in Table 2. As can be seen in the\ntable, CI-phoneme and syllable based model with the Trans-\nformer can achieve competitive results on HKUST datasets in\nterms of CER. It reveals that the Transformer is very suitable\nfor the ASR task since its powerful sequence modeling capa-\nbility, although it relies entirely on self-attention without us-\ning RNNs or convolutions. Furthermore, we note here that the\nCER of syllable based model outperforms that of correspond-\ning CI-phoneme based model. The results suggest that the sub-\nword unit of syllables is a better modeling unit in sequence-\nto-sequence attention-based models on Mandarin Chinese ASR\ntasks compared to the sub-word unit of CI-phonemes. It vali-\nFigure 2: Self-attention (top) of encoder-encoder that both x-\naxis and y-axis represent input frames. Vanilla attention (bot-\ntom) of encoder-decoder that the x-axis represents input frames\nand y-axis corresponds to output labels.\ndates the conclusion proposed on CTC based model [13]. Fi-\nnally, it is obvious that the big model always performs better\nthan the base model no matter on the CI-phoneme based model\nor syllable based model. Therefore, our further experiments are\nconducted on the big model.\nWe further generate more training data by linearly scaling\nthe audio lengths by factors of 0.9 and 1.1 (speed perturb.)\n[9]. It can be observed that syllable based model with speed\nperturb becomes better and achieves the best CER of 28.77%\ncompared to without it. However, CI-phoneme based model\nwith speed perturb becomes very slightly worse than without\nit. The interpretation of this phenomenon is that syllables have\na longer duration and more invariance than CI-phonemes, so\nsmall speed perturb would not affect the pronuciation of syl-\nlables too much, instead of providing more useful and various\ntraining data. However, small speed perturb might have more\nimpact on the pronuciation of CI-phonemes due to the short du-\nration.\nTable 2: Comparison of CI-phoneme and syllable based model\nwith the Transformer on HKUST datasets in CER (%).\nsub-word unit model CER\nCI-phonemes\nD512-H8 32.94\nD1024-H16 30.65\nD1024-H16 (speed perturb) 30.72\nSyllables\nD512-H8 31.80\nD1024-H16 29.87\nD1024-H16 (speed perturb) 28.77\n4.4. Comparison with previous works\nIn Table 3, we compare our experimental results to other model\narchitectures from the literature on HKUST datasets. First, we\ncan ﬁnd that the result of CI-phoneme based model with the\nTransformer is comparable to the best result by the deep mul-\ntidimensional residual learning with 9 LSTM layers in hybrid\nsystem [24], and the syllable based model with the Transformer\nprovides over a 6% relative improvement in CER compared to\nit. Moreover, the CER 28.77% of syllable based model with\nthe Transformer is comparable to the CER 28.9% by the joint\nCTC-attention based encoder-decoder network [9] when no ex-\nternal language model is used, but slightly worse than the CER\n28.0% by the joint CTC-attention based encoder-decoder net-\nwork with separate RNN-LM, which is the state-of-the-art on\nHKUST datasets to the best of our knowledge.\nTable 3: CER (%) on HKUST datasets compared to previous\nworks.\nmodel CER\nLSTMP-9×800P512-F444 [24] 30.79\nCTC-attention+joint dec. (speed perturb., one-pass)\n+VGG net\n+RNN-LM (separate) [9]\n28.9\n28.0\nCI-phonemes-D1024-H16 30.65\nSyllables-D1024-H16 (speed perturb) 28.77\n4.5. Comparison of different frame rates\nFinally, table 4 compares different frame rates on CI-phoneme\nand syllable based model with the Transformer. It indicates that\nthe performance of CI-phoneme and syllable based model with\nthe Transformer decreases as the frame rate increases. The de-\ncreasing rate is relatively slow from 30ms to 50ms, but deteri-\norates rapidly from 50ms to 70ms. Thus, it shows that frame\nrate between 30ms and 50ms performs relatively well on CI-\nphoneme and syllable based model with the Transformer.\nTable 4: Comparison of different frame rates on HKUST\ndatasets in CER (%).\nmodel frame rate CER\nCI-phonemes-D1024-H16\n(speed perturb)\n30ms\n50ms\n70ms\n30.72\n31.68\n33.96\nSyllables-D1024-H16\n(speed perturb)\n30ms\n50ms\n70ms\n28.77\n29.36\n32.22\n5. Conclusions\nIn this paper we applied the Transformer, a new sequence trans-\nduction model based entirely on self-attention without using\nRNNs or convolutions, to Mandarin Chinese ASR tasks and\nveriﬁed its effectiveness on HKUST datasets. Furthermore, we\ncompared syllables and CI-phonemes as the modeling unit in\nsequence-to-sequence attention-based models with the Trans-\nformer in Mandarin Chinese. Our experimental results demon-\nstrated that syllable based model with the Transformer performs\nbetter than CI-phoneme based counterpart on HKUST datasets.\nWhat is more, a greedy cascading decoder with the Transformer\nis proposed to maximize Pr(W|s)Pr(s|X) and then posterior\nprobability Pr(W|X) can be maximized. Experimental results\non CI-phoneme and syllable based model veriﬁed the effective-\nness of the greedy cascading decoder.\n6. Acknowledgements\nThe authors would like to thank Chunqi Wang for insightful\ndiscussions on training and tuning the Transformer.\n7. References\n[1] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Goninaet al., “State-\nof-the-art speech recognition with sequence-to-sequence models,”\narXiv preprint arXiv:1712.01769, 2017.\n[2] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Ben-\ngio, “Attention-based models for speech recognition,” in Ad-\nvances in neural information processing systems, 2015, pp. 577–\n585.\n[3] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y . Ben-\ngio, “End-to-end attention-based large vocabulary speech recog-\nnition,” in Acoustics, Speech and Signal Processing (ICASSP),\n2016 IEEE International Conference on. IEEE, 2016, pp. 4945–\n4949.\n[4] W. Chan, N. Jaitly, Q. V . Le, and O. Vinyals, “Listen, attend and\nspell. arxiv preprint,” arXiv preprint arXiv:1508.01211 , vol. 1,\nno. 2, p. 3, 2015.\n[5] R. Prabhavalkar, T. N. Sainath, B. Li, K. Rao, and N. Jaitly, “An\nanalysis of attention in sequence-to-sequence models,,” in Proc.\nof Interspeech, 2017.\n[6] H. A. Bourlard and N. Morgan, Connectionist speech recognition:\na hybrid approach. Springer Science & Business Media, 2012,\nvol. 247.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n6000–6010.\n[8] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, “A comparison of sequence-to-sequence models for\nspeech recognition,” inProc. Interspeech, 2017, pp. 939–943.\n[9] T. Hori, S. Watanabe, Y . Zhang, and W. Chan, “Advances in joint\nctc-attention based end-to-end speech recognition with a deep cnn\nencoder and rnn-lm,”arXiv preprint arXiv:1706.02737, 2017.\n[10] Y . Zhang, W. Chan, and N. Jaitly, “Very deep convolutional net-\nworks for end-to-end speech recognition,” in Acoustics, Speech\nand Signal Processing (ICASSP), 2017 IEEE International Con-\nference on. IEEE, 2017, pp. 4845–4849.\n[11] W. Chan and I. Lane, “On online attention-based speech recog-\nnition and joint mandarin character-pinyin training.” in INTER-\nSPEECH, 2016, pp. 3404–3408.\n[12] C. Shan, J. Zhang, Y . Wang, and L. Xie, “Attention-based end-to-\nend speech recognition on voice search.”\n[13] Z. Qu, P. Haghani, E. Weinstein, and P. Moreno, “Syllable-based\nacoustic modeling with ctc-smbr-lstm,” 2017.\n[14] A. Ganapathiraju, J. Hamaker, J. Picone, M. Ordowski, and G. R.\nDoddington, “Syllable-based large vocabulary continuous speech\nrecognition,”IEEE Transactions on speech and audio processing,\nvol. 9, no. 4, pp. 358–366, 2001.\n[15] H. Wu and X. Wu, “Context dependent syllable acoustic model for\ncontinuous chinese speech recognition,” in Eighth Annual Con-\nference of the International Speech Communication Association ,\n2007.\n[16] Y . Liu, J. Hua, X. Li, T. Fu, and X. Wu, “Chinese syllable-to-\ncharacter conversion with recurrent neural network based super-\nvised sequence labelling,” in Signal and Information Process-\ning Association Annual Summit and Conference (APSIPA), 2015\nAsia-Paciﬁc. IEEE, 2015, pp. 350–353.\n[17] N. Kanda, X. Lu, and H. Kawai, “Maximum a posteriori based de-\ncoding for ctc acoustic models.” in Interspeech, 2016, pp. 1868–\n1872.\n[18] Y . Liu, P. Fung, Y . Yang, C. Cieri, S. Huang, and D. Graff,\n“Hkust/mts: A very large scale mandarin telephone speech cor-\npus,” in Chinese Spoken Language Processing. Springer, 2006,\npp. 724–735.\n[19] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and accurate\nrecurrent neural network acoustic models for speech recognition,”\narXiv preprint arXiv:1507.06947, 2015.\n[20] A. Kannan, Y . Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, “An analysis of incorporating an external lan-\nguage model into a sequence-to-sequence model,” arXiv preprint\narXiv:1712.01996, 2017.\n[21] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,”arXiv preprint arXiv:1412.6980, 2014.\n[22] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Re-\nthinking the inception architecture for computer vision,” in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 2818–2826.\n[23] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,\nG. S. Corrado, A. Davis, J. Dean, M. Devin et al., “Tensorﬂow:\nLarge-scale machine learning on heterogeneous distributed sys-\ntems,”arXiv preprint arXiv:1603.04467, 2016.\n[24] Y . Zhao, S. Xu, and B. Xu, “Multidimensional residual learning\nbased on recurrent neural networks for acoustic modeling,” Inter-\nspeech 2016, pp. 3419–3423, 2016.",
  "topic": "Mandarin Chinese",
  "concepts": [
    {
      "name": "Mandarin Chinese",
      "score": 0.7938311100006104
    },
    {
      "name": "Transformer",
      "score": 0.7734156250953674
    },
    {
      "name": "Computer science",
      "score": 0.7344313859939575
    },
    {
      "name": "Speech recognition",
      "score": 0.7149546146392822
    },
    {
      "name": "Encoder",
      "score": 0.4851987659931183
    },
    {
      "name": "Syllable",
      "score": 0.4776686131954193
    },
    {
      "name": "Recurrent neural network",
      "score": 0.45661064982414246
    },
    {
      "name": "Pronunciation",
      "score": 0.4467681348323822
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44109857082366943
    },
    {
      "name": "Machine translation",
      "score": 0.4400781989097595
    },
    {
      "name": "Hidden Markov model",
      "score": 0.419038861989975
    },
    {
      "name": "Sequence (biology)",
      "score": 0.41215047240257263
    },
    {
      "name": "Natural language processing",
      "score": 0.3638148307800293
    },
    {
      "name": "Artificial neural network",
      "score": 0.3597908616065979
    },
    {
      "name": "Linguistics",
      "score": 0.08707153797149658
    },
    {
      "name": "Voltage",
      "score": 0.0777561366558075
    },
    {
      "name": "Engineering",
      "score": 0.07692903280258179
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 28
}