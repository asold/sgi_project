{
  "title": "U-Net Transformer: Self and Cross Attention for Medical Image Segmentation",
  "url": "https://openalex.org/W3134449767",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2044457849",
      "name": "Olivier Petit",
      "affiliations": [
        "Conservatoire National des Arts et Métiers",
        "Centre d'Etudes et De Recherche en Informatique et Communications"
      ]
    },
    {
      "id": "https://openalex.org/A2056492599",
      "name": "Nicolas Thome",
      "affiliations": [
        "Conservatoire National des Arts et Métiers",
        "Centre d'Etudes et De Recherche en Informatique et Communications"
      ]
    },
    {
      "id": "https://openalex.org/A2765726630",
      "name": "Clément Rambour",
      "affiliations": [
        "Conservatoire National des Arts et Métiers",
        "Centre d'Etudes et De Recherche en Informatique et Communications"
      ]
    },
    {
      "id": "https://openalex.org/A3201298742",
      "name": "Loic Themyr",
      "affiliations": [
        "Centre d'Etudes et De Recherche en Informatique et Communications",
        "Conservatoire National des Arts et Métiers"
      ]
    },
    {
      "id": "https://openalex.org/A2096690687",
      "name": "Toby Collins",
      "affiliations": [
        "Institut de Recherche contre les Cancers de l’Appareil Digestif"
      ]
    },
    {
      "id": "https://openalex.org/A2160719952",
      "name": "Luc Soler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2044457849",
      "name": "Olivier Petit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2056492599",
      "name": "Nicolas Thome",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2765726630",
      "name": "Clément Rambour",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3201298742",
      "name": "Loic Themyr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096690687",
      "name": "Toby Collins",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2160719952",
      "name": "Luc Soler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2913736247",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2891451067",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2963794428",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W3017153481",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2891511539",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2896457183"
  ],
  "abstract": null,
  "full_text": "U-Net Transformer: Self and Cross Attention for\nMedical Image Segmentation\nOlivier Petit1,2, Nicolas Thome1, Clement Rambour1, and Luc Soler 2\n1 CEDRIC - Conservatoire National des Arts et Metiers, Paris, France\n2 Visible Patient SAS, Strasbourg, France\nolivier.petit@visiblepatient.com\nAbstract. Medical image segmentation remains particularly challeng-\ning for complex and low-contrast anatomical structures. In this paper,\nwe introduce the U-Transformer network, which combines a U-shaped\narchitecture for image segmentation with self- and cross-attention from\nTransformers. U-Transformer overcomes the inability of U-Nets to model\nlong-range contextual interactions and spatial dependencies, which are\narguably crucial for accurate segmentation in challenging contexts. To\nthis end, attention mechanisms are incorporated at two main levels: a\nself-attention module leverages global interactions between encoder fea-\ntures, while cross-attention in the skip connections allows a ﬁne spatial\nrecovery in the U-Net decoder by ﬁltering out non-semantic features.\nExperiments on two abdominal CT-image datasets show the large per-\nformance gain brought out by U-Transformer compared to U-Net and\nlocal Attention U-Nets. We also highlight the importance of using both\nself- and cross-attention, and the nice interpretability features brought\nout by U-Transformer.\nKeywords: Medical Image Segmentation· Transformers · Self-attention\n· Cross-attention · Spatial layout · Global interactions\n1 Introduction\nOrgan segmentation is of crucial importance in medical imaging and computed-\naided diagnosis, e.g. for radiologists to assess physical changes in response to a\ntreatment or for computer-assisted interventions.\nCurrently, state-of-the-art methods rely on Fully Convolutional Networks\n(FCNs), such as U-Net and variants [11,2,9,21]. U-Nets use an encoder-decoder\narchitecture: the encoder extracts high-level semantic representations by using\na cascade of convolutional layers, while the decoder leverages skip connections\nto re-use high-resolution feature maps from the encoder in order to recover lost\nspatial information from high-level representations.\nDespite their outstanding performances, FCNs suﬀer from conceptual limita-\ntions in complex segmentation tasks,e.g. when dealing with local visual ambigui-\nties and low contrast between organs. This is illustrated in Fig 1a) for segmenting\narXiv:2103.06104v2  [eess.IV]  12 Mar 2021\n2 O. Petit et al.\nFig. 1.Global context is crucial for complex organ segmentation but cannot be cap-\ntured by vanilla U-Nets with a limited receptive ﬁeld, i.e. blue cross region in a) with\nfailed segmentation in c). The proposed U-Transformer network represents full image\ncontext by means of attention maps b), which leverage long-range interactions with\nother anatomical structures to properly segment the complex pancreas region in d).\nthe blue cross region corresponding to the pancreas with U-Net: the limited re-\nceptive ﬁeld framed in red does not capture suﬃcient contextual information,\nmaking the segmentation fail, see Fig 1c).\nIn this paper, we introduce the U-Transformer network, which leverages the\nstrong abilities of transformers [15] to model long-range interactions and spatial\nrelationships between anatomical structures. U-Transformer keeps the inductive\nbias of convolution by using a U-shaped architecture, but introduces attention\nmechanisms at two main levels, which help to interpret the model decision.\nFirstly, a self-attention module leverages global interactions between semantic\nfeatures at the end of the encoder to explicitly model full contextual information.\nSecondly, we introduce cross-attention in the skip connections to ﬁlter out non-\nsemantic features, allowing a ﬁne spatial recovery in the U-Net decoder.\nFig 1b) shows a cross-attention map induced by U-Transformer, which high-\nlights the most important regions for segmenting the blue cross region in Fig 1a):\nour model leverages the long-range interactions with respect to other organs\n(liver, stomach, spleen) and their positions to properly segment the whole pan-\ncreas region, see Fig 1d). Quantitative experiments conducted on two abdom-\ninal CT-image datasets show the large performance gain brought out by U-\nTransformer compared to U-Net and to the local attention in [13].\nRelated Work.Transformers [15] have witnessed increasing success in the last\nﬁve years, started in natural language processing with text embeddings [3]. A pio-\nneer use of transformers in computer vision is non-local networks [17], which com-\nbine self-attention with a convolutional backbone. Recent applications include\nobject detection [1], semantic segmentation [20,16], and image classiﬁcation [4].\nRecent works also focus on approximating self-attention mechanisms [6,19,7] to\ncircumvent the high memory demand in transformers. All these approaches limit\nthe use of transformers to self-attention. In contrast, U-Transformer use both\nself- and cross-attention, the latter being leveraged for improving the recovery\nof ﬁne spatial and semantic information.\nU-Transformer: Self and Cross Attention for Medical Image Segmentation 3\nAttention models for medical image segmentation have also been used re-\ncently [18,10,12,13,14]. [18,8] create attention maps combining local and global\nfeatures with a simple attention module, and [14] successfully applies the Dual\nattention network in [5] in diﬀerent segmentation contexts. Despite the relevance\nof these works, they do not leverage the recent improvements obtained by trans-\nformers to model full range interactions. Attention U-Net [13] uses gating signal\nin the skip connection, which acts as cross-attention. However, the attention\nweight maps are computed from local information only. In contrast, our cross-\nattention module incorporates rich region interactions and spatial information.\n2 The U-Transformer Network\nAs mentioned in Section 1, encoder-decoder U-shaped architectures lack global\ncontext information to handle complex medical image segmentation tasks. We\nintroduce the U-Transformer network, which augments U-Nets with attention\nmodules built from multi-head transformers. U-Transformer models long-range\ncontextual interactions and spatial dependencies by using two types of attention\nmodules (see Fig 2): Multi-Head Self-Attention (MHSA) and Multi-Head Cross-\nAttention (MHCA). Both modules are designed to express a new representation\nof the input based on its self-attention in the ﬁrst case (cf. 2.1) or on the attention\npaid to higher level features in the second ( cf. 2.2).\nFig. 2. U-Transformeraugments U-Nets with transformers to model long-range con-\ntextual interactions. The Multi-Head Self-Attention (MHSA) module at the end of the\nU-Net encoder gives access to a receptive ﬁeld containing the whole image (shown in\npurple), in contrast to the limited U-Net receptive ﬁeld (shown in blue). Multi-Head\nCross-Attention (MHCA) modules are dedicated to combine the semantic richness in\nhigh level feature maps with the high resolution ones coming from the skip connections.\n2.1 Self-attention\nThe MHSA module is designed to extract long range structural information from\nthe images. To this end, it is composed of multi-head self-attention functions as\ndescribed in [15] positioned at the bottom of the U-Net as shown in Figure 2.\nThe main goal of MHSA is to connect every element in the highest feature map\n4 O. Petit et al.\nwith each other, thus giving access to a receptive ﬁeld including all the input\nimage. The decision for one speciﬁc pixel can thus be inﬂuenced by any input\npixel. The attention formulation is given in Equation 1. A self-attention module\ntakes three inputs, a matrix of queries Q ∈Rn×dk , a matrix of keys K ∈Rn×dk\nand a matrix of values V ∈Rn×dk .\nAttention(Q, K, V ) = softmax(QKT\n√dk\n)V = AV (1)\nA line of the attention matrix A ∈Rn×n corresponds to the similarity of a\ngiven element in Q with respect to all the elements in K. Then, the attention\nfunction performs a weighted average of the elements of the value V to account\nfor all the interactions between the queries and the keys as illustrated in Figure\n3. In our segmentation task, Q, K and V share the same size and correspond\nto diﬀerent learnt embedding of the highest level feature map denoted by X in\nFigure 3. The embedding matrices are denoted as Wq, Wk and Wv. The atten-\ntion is calculated separately in multiple heads before being combined through\nanother embedding. Moreover, to account for absolute contextual information,\na positional encoding is added to the input features. It is especially relevant for\nmedical image segmentation, where the diﬀerent anatomical structures follow a\nﬁxed spatial position. The positional encoding can thus be leveraged to capture\nabsolute and relative position between organs in MHSA.\nFig. 3. MHSA module: the input tensor is embedded into a matrix of queries Q,\nkeys K and values V . The attention matrix A in purple is computed based on Q and\nK. (1) A line of A corresponds to the attention given to all the elements in K with\nrespect to one element in Q. (2) A column of the value V corresponds to a feature\nmap weighted by the attention in A.\n2.2 Cross-attention\nThe MHSA module allows to connect every element in the input with each other.\nAttention may also be used to increase the U-Net decoder eﬃciency and in par-\nticular enhance the lower level feature maps that are passed through the skip\nconnections. Indeed, if these skip connections insure to keep a high resolution\nU-Transformer: Self and Cross Attention for Medical Image Segmentation 5\ninformation they lack the semantic richness that can be found deeper in the net-\nwork. The idea behind the MHCA module is to turn oﬀ irrelevant or noisy areas\nfrom the skip connection features and highlight regions that present a signiﬁ-\ncant interest for the application. Figure 4 shows the cross-attention module. The\nMHCA block is designed as a gating operation of the skip connection S based on\nthe attention given to a high level feature map Y . The computed weight values\nare then re-scaled between 0 and 1 through a sigmoid activation function. The\nresulting tensor, denoted Z in Figure 4, is a ﬁlter where low magnitude elements\nindicate noisy or irrelevant areas to be reduced. A cleaned up version of S is\nthen given by the Hadamard product Z ⊙S. Finally, the result of this ﬁltering\noperation is concatenated with the high level feature tensor Y .\nFig. 4. MHCA module: the value of the attention function corresponds to the skip\nconnection S weighted by the attention given to the high level feature map Y . This\noutput is transformed into a ﬁlter Z and applied to the skip connection.\n3 Experiments\nWe evaluate U-Transformer for abdominal organ segmentation on the TCIA\npancreas public dataset, and an internal multi-organ dataset.\nAccurate pancreas segmentation is particularly diﬃcult, due to its small size,\ncomplex and variable shape, and because of the low contrast with the neigh-\nboring structures, see Fig 1. In addition, the multi-organ setting assesses how\nU-transformer can leverage attention from multi-organ annotations.\n6 O. Petit et al.\nExperimental setupThe TCIA pancreas dataset3 contains 82 CT-scans with\npixel-level annotations. Each CT-scan has around 181 ∼466 slices of 512 ×512\npixels and a voxel spacing of ([0 .66 ∼0.98] ×[0.66 ∼0.98] ×[0.5 ∼1.0]) mm3.\nWe also experiment with an Internal Multi-Organ (IMO) dataset composed\nof 85 CT-scans annotated with 7 classes: liver, gallbladder, pancreas, spleen,\nright and left kidneys, and stomach. Each CT-scan has around 57 ∼500 slices\nof 512 ×512 pixels and a voxel spacing of ([0 .42 ∼0.98] ×[0.42 ∼0.98] ×\n[0.63 ∼4.00])mm3.\nAll experiments follow a 5-fold cross validation, using 80% of images in train-\ning and 20% in test. We use the Tensorﬂow library to train the model, with Adam\noptimizer (10−4 learning rate, exponential decay scheduler).\nWe compare U-Transformer to the U-Net baseline [11] and Attention U-\nNet [13] with the same convolutional backbone for fair comparison. We also\nreport performances with self-attention only (MHSA, section 2.1), and the cross-\nattention only (MHCA, section 2.2). U-Net has ∼30M parameters, the overhead\nfrom U-transformer is limited (MHSA ∼5M, each MHCA block ∼2.5M).\n3.1 U-Transformer performances\nTable 1 reports the performances in Dice averaged over the 5 folds, and over or-\ngans for IMO. U-Transformer outperforms U-Net by 2.4pts on TCIA and 1.3pts\nfor IMO, and Attention U-Net by 1.7pts for TCIA and 1.6pts for IMO. The\ngains are consistent on all folds, and paired t-tests show that the improvement\nis signiﬁcant with p−values < 3% for every experiment.\nTable 1.Results for each method in Dice similarity coeﬃcient (DSC, %)\nDataset U-Net [11] Attn U-Net [13] MHSA MHCA U-Transformer\nTCIA 76.13 (± 0.94) 76,82 ( ± 1.26) 77.71 (± 1.31) 77.84 (± 2.59) 78.50 (± 1.92)\nIMO 86.78 (± 1.72) 86.45 ( ± 1.69) 87.29 (± 1.34) 87.38 (± 1.53) 88.08 (± 1.37)\nFigure 5 provides qualitative segmentation comparison between U-Net, At-\ntention U-Net and U-Transformer. We observe that U-Transformer performs\nbetter on diﬃcult cases, where the local structures are ambiguous. For example,\nin the second row, the pancreas has a complex shape which is missed by U-Net\nand Attention U-Net but U-Transformer successfully segments the organ.\nIn Table 1, we can see that the self-attention (MHSA) and cross-attention\n(MCHA) alone already outperform U-Net and Attention U-Net on TCIA and\nIMO. Since MCHA and Attention U-Net apply attention mechanisms at the\nskip connection level, it highlights the superiority of modeling global interactions\nbetween anatomical structures and positional information instead of the simple\nlocal attention in [13]. Finally, the combination of MHSA and MHCA in U-\nTransformer shows that the two attention mechanisms are complementary and\ncan collaborate to provide better segmentation predictions.\n3 https://wiki.cancerimagingarchive.net/display/Public/Pancreas-CT\nU-Transformer: Self and Cross Attention for Medical Image Segmentation 7\nFig. 5.Segmentation results for U-Net [11], Attention U-Net [13] and U-Transformer\non the multi-organ IMO dataset (ﬁrst row) and on TCIA pancreas (second row).\nTable 2 details the results for each organ on the multi-organ IMO dataset.\nThis further highlights the interest of U-Transformer, which signiﬁcantly out-\nperforms U-Net and Attention U-Net for the most challenging organs: pancreas:\n+3.4pts, gallbladder: +1.3pts and stomach: +2.2pts. This validates the capacity\nof U-Transformer to leverage multi-label annotations to drive the interactions\nbetween anatomical structures, and use easy organ predictions to improve the\ndetection and delineation of more diﬃcult ones. We can note that U-Transformer\nis better for every organ, even the liver which has a high score> 95% with U-Net.\nTable 2.Results on IMO in Dice similarity coeﬃcient (DSC, %) detailed per organ.\nOrgan U-Net [11] Attn U-Net [13] MHSA MHCA U-Transformer\nPancreas 69.71 (± 3.74) 68.65 (± 2.95) 71.64 (± 3.01) 71.87 (± 2.97) 73.10 (± 2.91)\nGallbladder76.98 (± 6.60) 76.14 (± 6.98) 76.48 (± 6.12) 77.36 (± 6.22) 78.32 (± 6.12)\nStomach 83.51 (± 4.49) 82.73 (± 4.62) 84.83 (± 3.79) 84.42 (± 4.35) 85.73 (± 3.99)\nKidney(R) 92.36 (± 0.45) 92.88 (± 1.79) 92.91 (± 1.84) 92.98 (± 1.70) 93.32 (± 1.74)\nKidney(L) 93.06 (± 1.68) 92.89 (± 0.64) 92.95 (± 1.30) 92.82 (± 1.06) 93.31 (± 1.08)\nSpleen 95.43 (± 1.76) 95.46 (± 1.95) 95.43 (± 2.16) 95.41 (± 2.21) 95.74 (± 2.07)\nLiver 96.40 (± 0.72) 96.41 (± 0.52) 96.82 (± 0.34) 96.79 (± 0.29) 97.03 (± 0.31)\n3.2 U-Transformer analysis and properties\nPositional encoding and multi-level MHCA. The Positional Encoding\n(PE) allows to leverage the absolute position of the objects in the image. Table 3\nshows an analysis of its impact, on one fold on both datasets. For MHSA, the\nPE improves the results by +0.7pt for TCIA and +0.6pt for IMO. For MHCA,\nwe evaluate a single level of attention with and without PE. We can observe an\nimprovement of +1.7pts for TCIA and +0.6pt for IMO between the two versions.\n8 O. Petit et al.\nFig. 6.Cross-attention maps for the yellow-crossed pixel (left image).\nTable 3 also shows the favorable impact of using multivs single-level attention\nfor MHCA: +1.8pts for TCIA and +0.6pt for IMO. It is worth noting that\nAttention U-Net uses multi-level attention but remains below MHCA with a\nsingle level. Figure 6 shows attention maps at each level of U-Transformer: level\n3 corresponds to high-resolution features maps, and tends to focus on more\nspeciﬁc regions compared to the ﬁrst levels.\nTable 3. Ablation study on the positional encoding and multi-level on one fold of\nTCIA and IMO.\nMHSA MHCA\nU-Net Attn U-Net wo PE – w PE 1 lvl wo PE – 1 lvl w PE – multi-lvl w PE\nTCIA 76.35 77.23 78.17 78.90 77.18 78.88 80.65\nIMO 88.18 87.52 88.16 88.76 87.96 88.52 89.13\nFurther analysis.To further analyse the behaviour of U-Transformer, we evalu-\nate the impact of the number of attention heads for MHSA (appendix, Figure 7):\nmore heads lead to better performances, but the biggest gain comes from the\nﬁrst head (i.e. U-Net to MHSA). Finally, the evaluation of U-Transformer with\nrespect to the Hausdorﬀ distance (appendix, Table 4) follows the same trend\nthan with Dice score. This highlights the capacity of U-Transformer to reduce\nprediction artefacts by means of self- and cross-attention.\n4 Conclusion\nThis paper introduces the U-Transformer network, which augments a U-shaped\nFCN with Transformers. We propose to use self and cross-attention modules\nto model long-range interactions and spatial dependencies. We highlight the\nrelevance of the approach for abdominal organ segmentation, especially for small\nand complex organs. Future works could include the study of U-Transformer in\n3D networks, with other modalities such as MRI or US images, as well as for\nother medical image tasks.\nU-Transformer: Self and Cross Attention for Medical Image Segmentation 9\nReferences\n1. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European Conference on Computer\nVision. pp. 213–229. Springer (2020)\n2. C ¸ i¸ cek,¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3d u-net:\nLearning dense volumetric segmentation from sparse annotation. In: MICCAI. pp.\n424–432 (2016)\n3. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec-\ntional transformers for language understanding. CoRR abs/1810.04805 (2018),\nhttp://arxiv.org/abs/1810.04805\n4. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nInternational Conference on Learning Representations (2021)\n5. Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H.: Dual attention network for\nscene segmentation. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) (June 2019)\n6. Hu, P., Perazzi, F., Heilbron, F.C., Wang, O., Lin, Z., Saenko, K., Sclaroﬀ, S.: Real-\ntime semantic segmentation with fast attention. IEEE Robotics and Automation\nLetters 6(1), 263–270 (2020)\n7. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: Transformers\nin vision: A survey. arXiv preprint arXiv:2101.01169 (2021)\n8. Li, C., Tong, Q., Liao, X., Si, W., Sun, Y., Wang, Q., Heng, P.A.: Attention based\nhierarchical aggregation network for 3d left atrial segmentation. In: Statistical At-\nlases and Computational Models of the Heart. Atrial Segmentation and LV Quan-\ntiﬁcation Challenges. pp. 255–264 (2019)\n9. Milletari, F., Navab, N., Ahmadi, S.: V-net: Fully convolutional neural networks for\nvolumetric medical image segmentation. In: 2016 Fourth International Conference\non 3D Vision (3DV). pp. 565–571 (2016)\n10. Nie, D., Gao, Y., Wang, L., Shen, D.: Asdnet: Attention based semi-supervised deep\nnetworks for medical image segmentation. In: Frangi, A., Fichtinger, G., Schnabel,\nJ., Alberola-L´ opez, C., Davatzikos, C. (eds.) MICCAI 2018. pp. 370–378. Lecture\nNotes in Computer Science, Springer Verlag (2018)\n11. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: MICCAI. pp. 234–241 (2015)\n12. Roy, A.G., Navab, N., Wachinger, C.: Concurrent spatial and channel squeeze &\nexcitation in fully convolutional networks. In: MICCAI. vol. abs/1803.02579 (2018)\n13. Schlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glocker,\nB., Rueckert, D.: Attention gated networks: Learning to leverage salient\nregions in medical images. Medical Image Analysis 53 (02 2019).\nhttps://doi.org/10.1016/j.media.2019.01.012\n14. Sinha, A., Dolz, J.: Multi-scale self-guided attention for medical image segmenta-\ntion. IEEE Journal of Biomedical and Health Informatics pp. 1–1 (2020)\n15. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: NeurIPS. pp. 5998–6008 (2017)\n16. Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., Chen, L.C.: Axial-deeplab:\nStand-alone axial-attention for panoptic segmentation. In: European Conference\non Computer Vision. pp. 108–126 (2020)\n10 O. Petit et al.\n17. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-\nceedings of the IEEE conference on computer vision and pattern recognition. pp.\n7794–7803 (2018)\n18. Wang, Y., Deng, Z., Hu, X., Zhu, L., Yang, X., xu, X., Heng, P.A., Ni, D.: Deep\nattentional features for prostate segmentation in ultrasound. In: MICCAI (09 2018)\n19. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V.:\nNystr¨ omformer: A nystr¨ om-based algorithm for approximating self-attention. In:\nAAAI (2021)\n20. Ye, L., Rochan, M., Liu, Z., Wang, Y.: Cross-modal self-attention network for\nreferring image segmentation. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 10502–10511 (2019)\n21. Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: Unet++: A nested\nu-net architecture for medical image segmentation. In: Deep Learning in Medical\nImage Analysis and Multimodal Learning for Clinical Decision Support. pp. 3–11\n(2018)\n76,35%\n77,78%\n78,29% 78,24%\n78,90%\nHeads\nScores (DSC)\n76,00%\n77,00%\n78,00%\n79,00%\n0 2 4 6 8\nFig. 7.Evolution of the Dice Score on TCIA (fold 1) when the number of heads varies\nbetween 0 and 8 in MHSA.\nTable 4.Hausdorﬀ Distances (HD) for the diﬀerent models\nDataset U-Net Attn U-Net U-Transformer\nTCIA 13.61 ( ± 2.01) 12.48 (± 1.36) 12.34 (± 1.51)\nIMO 12.06 ( ± 1.65) 12.13 (± 1.58) 12.00 (± 1.32)",
  "concepts": [
    {
      "name": "Image segmentation",
      "score": 0.6209381818771362
    },
    {
      "name": "Computer science",
      "score": 0.5860477089881897
    },
    {
      "name": "Transformer",
      "score": 0.5027585029602051
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44266897439956665
    },
    {
      "name": "Segmentation",
      "score": 0.4419458210468292
    },
    {
      "name": "Computer vision",
      "score": 0.4397708475589752
    },
    {
      "name": "Electrical engineering",
      "score": 0.13123297691345215
    },
    {
      "name": "Engineering",
      "score": 0.12069100141525269
    },
    {
      "name": "Voltage",
      "score": 0.07472288608551025
    }
  ],
  "topic": "Image segmentation",
  "institutions": [
    {
      "id": "https://openalex.org/I4210145724",
      "name": "Centre d'Etudes et De Recherche en Informatique et Communications",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210151542",
      "name": "VLNComm (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210109354",
      "name": "Institut de Recherche contre les Cancers de l’Appareil Digestif",
      "country": "FR"
    }
  ],
  "cited_by": 22
}