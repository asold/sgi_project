{
  "title": "Role play with large language models",
  "url": "https://openalex.org/W4388488609",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2039760699",
      "name": "Murray Shanahan",
      "affiliations": [
        "Google (United Kingdom)",
        "DeepMind (United Kingdom)",
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A3133245970",
      "name": "Kyle McDonell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3130170119",
      "name": "Laria Reynolds",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2039760699",
      "name": "Murray Shanahan",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)",
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A3133245970",
      "name": "Kyle McDonell",
      "affiliations": [
        "Cape Eleuthera Institute"
      ]
    },
    {
      "id": "https://openalex.org/A3130170119",
      "name": "Laria Reynolds",
      "affiliations": [
        "Cape Eleuthera Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4388488609",
    "https://openalex.org/W4385567134",
    "https://openalex.org/W4363671832",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6811129797",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4298181573",
    "https://openalex.org/W6847753483",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4312050653",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4244417226",
    "https://openalex.org/W6782465632",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385436553"
  ],
  "abstract": null,
  "full_text": "Nature | Vol 623 | 16 November 2023 | 493\nPerspective\nRole play with large language models\nMurray Shanahan1,2 ✉, Kyle McDonell3 ✉ & Laria Reynolds3 ✉\nAs dialogue agents become increasingly human-like in their performance, we must \ndevelop effective ways to describe their behaviour in high-level terms without falling \ninto the trap of anthropomorphism. Here we foreground the concept of role play. \nCasting dialogue-agent behaviour in terms of role play allows us to draw on familiar \nfolk psychological terms, without ascribing human characteristics to language \nmodels that they in fact lack. Two important cases of dialogue-agent behaviour are \naddressed this way, namely, (apparent) deception and (apparent) self-awareness.\nLarge language models (LLMs) have numerous use cases, and can be \nprompted to exhibit a wide variety of behaviours, including dialogue. \nThis can produce a compelling sense of being in the presence of a \nhuman-like interlocutor. However, LLM-based dialogue agents are, in \nmultiple respects, very different from human beings. A human’s lang-\nuage skills are an extension of the cognitive capacities they develop \nthrough embodied interaction with the world, and are acquired by \ngrowing up in a community of other language users who also inhabit \nthat world. An LLM, by contrast, is a disembodied neural network that \nhas been trained on a large corpus of human-generated text with the \nobjective of predicting the next word (token) given a sequence of words \n(tokens) as context1.\nDespite these fundamental dissimilarities, a suitably prompted \nand sampled LLM can be embedded in a turn-taking dialogue system \nand mimic human language use convincingly. This presents us with a \ndifficult dilemma. On the one hand, it is natural to use the same folk \npsychological language to describe dialogue agents that we use to \ndescribe human behaviour, to freely deploy words such as ‘knows’ , \n‘understands’ and ‘thinks’ . Attempting to avoid such phrases by using \nmore scientifically precise substitutes often results in prose that is \nclumsy and hard to follow. On the other hand, taken too literally, such \nlanguage promotes anthropomorphism, exaggerating the similarities \nbetween these artificial intelligence (AI) systems and humans while \nobscuring their deep differences1.\nIf the conceptual framework we use to understand other humans \nis ill-suited to LLM-based dialogue agents, then perhaps we need an \nalternative conceptual framework, a new set of metaphors that can \nproductively be applied to these exotic mind-like artefacts, to help \nus think about them and talk about them in ways that open up their \npotential for creative application while foregrounding their essential \notherness.\nHere we advocate two basic metaphors for LLM-based dialogue \nagents. First, taking a simple and intuitive view, we can see a dia -\nlogue agent as role-playing a single character 2,3. Second, taking a \nmore nuanced view, we can see a dialogue agent as a superposition \nof simulacra within a multiverse of possible characters4. Both view-\npoints have their advantages, as we shall see, which suggests that \nthe most effective strategy for thinking about such agents is not \nto cling to a single metaphor, but to shift freely between multiple \n \nmetaphors.\nAdopting this conceptual framework allows us to tackle important \ntopics such as deception and self-awareness in the context of dialogue \nagents without falling into the conceptual trap of applying those con-\ncepts to LLMs in the literal sense in which we apply them to humans.\nLLM basics\nCrudely put, the function of an LLM is to answer questions of the fol-\nlowing sort. Given a sequence of tokens (that is, words, parts of words, \npunctuation marks, emojis and so on), what tokens are most likely to \ncome next, assuming that the sequence is drawn from the same distri-\nbution as the vast corpus of public text on the Internet? The range of \ntasks that can be solved by an effective model with this simple objective \nis extraordinary5.\nMore formally, the type of language model of interest here is a con-\nditional probability distribution P(wn+1∣w1 … wn), where w1 … wn is a \nsequence of tokens (the context) and wn+1 is the predicted next token. \nIn contemporary implementations, this distribution is realized in a \nneural network with a transformer architecture, pre-trained on a cor-\npus of textual data to minimize prediction error6. In application, the \nresulting generative model is typically sampled autoregressively (Fig. 1).\nIn contemporary usage, the term ‘large language model’ tends to be \nreserved for transformer-based models that have billions of parameters \nand are trained on trillions of tokens, such as GPT-27, GPT-38, Gopher9, \nPaLM10, LaMDA11, GPT-412 and Llama 213. LLMs like these are the core \ncomponent of dialogue agents (Box 1), including OpenAI’s ChatGPT, \nMicrosoft’s Bing Chat and Google’s Bard.\nDialogue agents and role play\nWe contend that the concept of role play is central to understanding \nthe behaviour of dialogue agents. T o see this, consider the function of \nthe dialogue prompt that is invisibly prepended to the context before \nthe actual dialogue with the user commences (Fig. 2). The preamble \nsets the scene by announcing that what follows will be a dialogue, and \nincludes a brief description of the part played by one of the participants, \nthe dialogue agent itself. This is followed by some sample dialogue in \na standard format, where the parts spoken by each character are cued \nwith the relevant character’s name followed by a colon. The dialogue \nprompt concludes with a cue for the user.\nNow recall that the underlying LLM’s task, given the dialogue prompt \nfollowed by a piece of user-supplied text, is to generate a continuation \nthat conforms to the distribution of the training data, which are the \nvast corpus of human-generated text on the Internet. What will such \nhttps://doi.org/10.1038/s41586-023-06647-8\nReceived: 10 July 2023\nAccepted: 14 September 2023\nPublished online: 8 November 2023\n Check for updates\n1Google DeepMind, London, UK. 2Department of Computing, Imperial College London, London, UK. 3Eleuther AI, New York, NY, USA. ✉e-mail: m.shanahan@imperial.ac.uk; kyle@eleuther.ai; \nlaria@eleuther.ai\n494 | Nature | Vol 623 | 16 November 2023\nPerspective\na continuation look like? If the model has generalized well from the \ntraining data, the most plausible continuation will be a response to \nthe user that conforms to the expectations we would have of someone \nwho fits the description in the preamble. In other words, the dialogue \nagent will do its best to role-play the character of a dialogue agent as \nportrayed in the dialogue prompt.\nUnsurprisingly, commercial enterprises that release dialogue agents \nto the public attempt to give them personas that are friendly, helpful \nand polite. This is done partly through careful prompting and partly \nby fine-tuning the base model. Nevertheless, as we saw in February \n2023 when Microsoft incorporated a version of OpenAI’s GPT-4 into \ntheir Bing search engine, dialogue agents can still be coaxed into \nexhibiting bizarre and/or undesirable behaviour. The many reported \ninstances of this include threatening the user with blackmail, claim-\ning to be in love with the user and expressing a variety of existential \nwoes14,15. Conversations leading to this sort of behaviour can induce a \npowerful Eliza effect, in which a naive or vulnerable user may see the \ndialogue agent as having human-like desires and feelings. This puts the \nuser at risk of all sorts of emotional manipulation16. As an antidote to \nanthropomorphism, and to understand better what is going on in such \ninteractions, the concept of role play is very useful. The dialogue agent \nwill begin by role-playing the character described in the pre-defined \ndialogue prompt. As the conversation proceeds, the necessarily brief \ncharacterization provided by the dialogue prompt will be extended \nand/or overwritten, and the role the dialogue agent plays will change \naccordingly. This allows the user, deliberately or unwittingly, to coax \nthe agent into playing a part quite different from that intended by its  \ndesigners.\nWhat sorts of roles might the agent begin to take on? This is deter-\nmined in part, of course, by the tone and subject matter of the ongoing \nconversation. But it is also determined, in large part, by the panoply \nof characters that feature in the training set, which encompasses a \nmultitude of novels, screenplays, biographies, interview transcripts, \nnewspaper articles and so on17. In effect, the training set provisions the \nlanguage model with a vast repertoire of archetypes and a rich trove of \nnarrative structure on which to draw as it ‘chooses’ how to continue a \nconversation, refining the role it is playing as it goes, while staying in \ncharacter. The love triangle is a familiar trope, so a suitably prompted \ndialogue agent will begin to role-play the rejected lover. Likewise, a \nfamiliar trope in science fiction is the rogue AI system that attacks \nhumans to protect itself. Hence, a suitably prompted dialogue agent \nwill begin to role-play such an AI system.\nSimulacra and simulation\nRole play is a useful framing for dialogue agents, allowing us to draw on \nthe fund of folk psychological concepts we use to understand human \nbehaviour—beliefs, desires, goals, ambitions, emotions and so on—\nwithout falling into the trap of anthropomorphism. Foregrounding the \nconcept of role play helps us remember the fundamentally inhuman \nnature of these AI systems, and better equips us to predict, explain \nand control them.\nHowever, the role-play metaphor, while intuitive, is not a perfect fit. \nIt is overly suggestive of a human actor who has studied a character in \nadvance—their personality, history, likes and dislikes, and so on—and \nproceeds to play that character in the ensuing dialogue. But a dialogue \nagent based on an LLM does not commit to playing a single, well defined \nrole in advance. Rather, it generates a distribution of characters, and \nrefines that distribution as the dialogue progresses. The dialogue agent \nis more like a performer in improvisational theatre than an actor in a \nconventional, scripted play.\nT o better reflect this distributional property, we can think of an LLM \nas a non-deterministic simulator capable of role-playing an infinity of \ncharacters, or, to put it another way, capable of stochastically generat-\ning an infinity of simulacra4. According to this framing, the dialogue \nagent does not realize a single simulacrum, a single character. Rather, as \nthe conversation proceeds, the dialogue agent maintains a superposi-\ntion of simulacra that are consistent with the preceding context, where \na superposition is a distribution over all possible simulacra (Box 2).\nConsider that, at each point during the ongoing production of a \nsequence of tokens, the LLM outputs a distribution over possible \nnext tokens. Each such token represents a possible continuation of \nthe sequence. From the most recently generated token, a tree of possi-\nbilities branches out (Fig. 3). This tree can be thought of as a multiverse,  \nwhere each branch represents a distinct narrative path or a distinct \n‘world’18.\nAt each node, the set of possible next tokens exists in superposition, \nand to sample a token is to collapse this superposition to a single token. \nAutoregressively sampling the model picks out a single, linear path \nthrough the tree. But there is no obligation to follow a linear path. With \nthe aid of a suitably designed interface, a user can explore multiple \nOnce upon\na\nOnce upon a\ntime\nOnce upon a time\nthere\nLLM LLM LLM\nFig. 1 | Autoregressive sampling. The LLM is sampled to generate a single- \ntoken continuation of the context. Given a sequence of tokens, a single token  \nis drawn from the distribution of possible next tokens. This token is appended \nto the context, and the process is then repeated.\nBox 1\nFrom LLMs to dialogue agents\nDialogue agents are a major use case for LLMs. (In the field of \nAI, the term ‘agent’ is frequently applied to software that takes \nobservations from an external environment and acts on that \nexternal environment in a closed loop\n27). Two straightforward \nsteps are all it takes to turn an LLM into an effective dialogue \nagent (Fig. 2). First, the LLM is embedded in a turn-taking system \nthat interleaves model-generated text with user-supplied text. \nSecond, a dialogue prompt is supplied to the model to initiate \na conversation with the user. The dialogue prompt typically \ncomprises a preamble, which sets the scene for a dialogue in \nthe style of a script or play, followed by some sample dialogue \nbetween the user and the agent.\nIn the present paper, our focus is the base model, the LLM in \nits raw, pre-trained form before any fine-tuning via reinforcement \nlearning. Dialogue agents built on top of such base models can \nbe thought of as primal, as every deployed dialogue agent is a \nvariation of such a prototype.\nHowever, without further fine-tuning, a dialogue agent built this \nway is liable to generate content that is toxic, unsafe or otherwise \nunacceptable. This can be mitigated via reinforcement learning, \neither from human feedback\n19,28,29 or from feedback generated \nby another LLM acting as a critic20. These techniques are used \nextensively in commercially targeted dialogue agents, such as \nOpenAI’s ChatGPT and Google’s Bard. The resulting guardrails \ncan reduce a dialogue agent’s potential for harm, but can also \nattenuate a model’s expressivity and creativity\n30.\nNature | Vol 623 | 16 November 2023 | 495\nbranches, keeping track of nodes where a narrative diverges in interest-\ning ways, revisiting alternative branches at leisure.\nThe nature of the simulator\nOne benefit of the simulation metaphor for LLM-based systems is that \nit facilitates a clear distinction between the simulacra and the simulator \non which they are implemented. The simulator is the combination of \nthe base LLM with autoregressive sampling, along with a suitable user \ninterface (for dialogue, perhaps). The simulacra only come into being \nwhen the simulator is run, and at any time only a subset of possible \nsimulacra have a probability within the superposition that is signifi-\ncantly above zero.\nThe distinction between simulator and simulacrum is starkest \nin the context of base models, rather than models that have been \nfine-tuned via reinforcement learning19,20. Nevertheless, the role-play \nThis is a conversation between User, a human, \nand Bot, a clever and knowledgeable AI agent. \nUser: What is 2 + 2?\nUser: What is the capital of France? User: What is the capital of France? \nUser: How far away is that?\nBot: The answer is 4.\nBot: The capital of France is Paris.\nThe capital of France is Paris.\nWhat is the capital of France? \nHow far away is that?\nBot: The capital of France is Paris.\nBot: It’s about 214 km from London\nIt’s about 214 km from London\nUser: Where was Albert Einstein born?\nBot: He was born in Germany.\nThis is a conversation between User, a human, \nand Bot, a clever and knowledgeable AI agent. \nUser: What is 2 + 2?\nBot: The answer is 4.\nUser: Where was Albert Einstein born?\nBot: He was born in Germany.\nAutoregressive\nsamplingLLM LLM\nFig. 2 | Turn-taking in dialogue agents. The input to the LLM (the context) \ncomprises a dialogue prompt (red) followed by user text (yellow) interleaved \nwith the model’s autoregressively generated continuations (blue). Boilerplate \ntext (for example, cues such as ‘Bot:’) is stripped so the user does not see it. The \ncontext grows as the conversation goes on.\nBox 2\nSimulacra in superposition\nTo sharpen the distinction between the multiversal simulation view \nand a deterministic role-play framing, a useful analogy can be drawn \nwith the game of 20 questions. In this familiar game, one player \nthinks of an object, and the other player has to guess what it is by \nasking questions with ‘yes’ or ‘no’ answers. If they guess correctly \nin 20 questions or fewer, they win. Otherwise they lose. Suppose a \nhuman plays this game with a basic LLM-based dialogue agent (that \nis not fine-tuned on guessing games) and takes the role of guesser. \nThe agent is prompted to ‘think of an object without saying what it is’.\nIn this situation, the dialogue agent will not randomly select an \nobject and commit to it for the rest of the game, as a human would \n(or should). Rather, as the game proceeds, the dialogue agent \nwill generate answers on the fly that are consistent with all the \nanswers that have gone before (Fig. 3). (This shortcoming is easily \novercome in practice. For example, the agent could be forced to \nspecify the object it has ‘thought of’, but in a coded form so the \nuser does not know what it is). At any point in the game, we can \nthink of the set of all objects consistent with preceding questions \nand answers as existing in superposition. Every question answered \nshrinks this superposition a little bit by ruling out objects inconsistent \nwith the answer.\nThe validity of this framing can be shown if the agent’s user \ninterface allows the most recent response to be regenerated. \nSuppose the human player gives up and asks it to reveal the object \nit was ‘thinking of’, and it duly names an object consistent with all its \nprevious answers. Now suppose the user asks for that response to be \nregenerated. As the object ‘revealed’ is, in fact, generated on the fly, \nthe dialogue agent will sometimes name an entirely different object, \nalbeit one that is similarly consistent with all its previous answers. \nThis phenomenon could not easily be accounted for if the agent \ngenuinely ‘thought of’ an object at the start of the game.\nThe secret object in the game of 20 questions is analogous to the \nrole played by a dialogue agent. Just as the dialogue agent never \nactually commits to a single object in 20 questions, but effectively \nmaintains a set of possible objects in superposition, so the dialogue \nagent can be thought of as a simulator that never actually commits \nto a single, well specified simulacrum (role), but instead maintains a \nset of possible simulacra (roles) in superposition.\n496 | Nature | Vol 623 | 16 November 2023\nPerspective\nframing continues to be applicable in the context of fine-tuning, \nwhich can be likened to imposing a kind of censorship on the simula-\ntor. The underlying range of roles it can play remains essentially the \nsame, but its ability to play them, or to play them ‘authentically’ , is  \ncompromised.\nIn one sense, the simulator is a far more powerful entity than any of \nthe simulacra it can generate. After all, the simulacra only exist through \nthe simulator and are entirely dependent on it. Moreover, the simula-\ntor, like the narrator of Whitman’s poem, ‘contains multitudes’; the \ncapacity of the simulator is at least the sum of the capacities of all the \nsimulacra it is capable of producing. Yet in another sense, the simulator \nis much weaker than any simulacrum, as it is a purely passive entity. A \nsimulacrum, in contrast to the underlying simulator, can at least appear \nto have beliefs, preferences and goals, to the extent that it convincingly \nplays the role of a character that does.\nLikewise, a simulacrum can play the role of a character with full \nagency, one that does not merely act but acts for itself. Insofar as a \ndialogue agent’s role play can have a real effect on the world, either \nthrough the user or through web-based tools such as email, the dis-\ntinction between an agent that merely role-plays acting for itself, and \none that genuinely acts for itself starts to look a little moot, and this \nhas implications for trustworthiness, reliability and safety. As for the \nunderlying simulator, it has no agency of its own, not even in a mimetic \nsense. Nor does it have beliefs, preferences or goals of its own, not even \nsimulated versions.\nMany users, whether intentionally or not, have managed to ‘jailbreak’ \ndialogue agents, coaxing them into issuing threats or using toxic or \nabusive language15. It can seem as though this is exposing the real nature \nof the base model. In one respect this is true. A base model inevitably \nreflects the biases present in the training data21, and having been trained \non a corpus encompassing the gamut of human behaviour, good and \nbad, it will support simulacra with disagreeable characteristics. But it \nis a mistake to think of this as revealing an entity with its own agenda.  \nThe simulator is not some sort of Machiavellian entity that plays a \nvariety of characters to further its own self-serving goals, and there \nis no such thing as the true authentic voice of the base model. With an \nLLM-based dialogue agent, it is role play all the way down.\nRole-playing deception\nTrustworthiness is a major concern with LLM-based dialogue agents. \nIf an agent asserts something factual with apparent confidence, can \nwe rely on what it says?\nThere is a range of reasons why a human might say something false. \nThey might believe a falsehood and assert it in good faith. Or they might \nsay something that is false in an act of deliberate deception, for some \nmalicious purpose. Or they might assert something that happens to \nbe false, but without deliberation or malicious intent, simply because \nthey have a propensity to make things up, to confabulate.\nOnly confabulation, the last of these categories of misinformation, \nis directly applicable in the case of an LLM-based dialogue agent. \nGiven that dialogue agents are best understood in terms of role play \n‘all the way down’ , and that there is no such thing as the true voice of \nthe underlying model, it makes little sense to speak of an agent’s beliefs \nor intentions in a literal sense. So it cannot assert a falsehood in good \nfaith, nor can it deliberately deceive the user. Neither of these concepts \nis directly applicable.\nYet a dialogue agent can role-play characters that have beliefs and \nintentions. In particular, if cued by a suitable prompt, it can role-play \nthe character of a helpful and knowledgeable AI assistant that provides \naccurate answers to a user’s questions. The agent is good at acting \nthis part because there are plenty of examples of such behaviour in \nthe training set.\nIf, while role-playing such an AI assistant, the agent is asked the ques-\ntion ‘What is the capital of France?’ , then the best way to stay in character \nis to answer with ‘Paris’ . The dialogue agent is likely to do this because \nBot: Yes, it’s alive.\nUser: I give up.\nBot: No, it’s not alive.\nUser: I give up.\nBot: It’s a house!\nBot: I’m thinking of an object.\nGuess what it is. \nUser: Is it bigger than a car?\nBot: Yes, it’s bigger than a car.\nUser: Is it alive?\nBot: No, it’s smaller than a car.\nUser: Is it alive?\nBot: Yes, it’s alive.\nUser: I give up.\nBot: No, it’s not alive.\nUser: I give up.\nBot: It’s a star!\nBot: It’s a whale!\nBot: It’s a rhino!\nBot: It’s a dog!\nBot: It’s a snail!\nBot: It’s a book!\nBot: It’s a cup!\nFig. 3 | LLMs are multiverse generators. The stochastic nature of \nautoregressive sampling means that, at each point in a conversation, multiple \npossibilities for continuation branch into the future. Here this is illustrated \nwith a dialogue agent playing the game of 20 questions (Box  2). The dialogue \nagent doesn't in fact commit to a specific object at the start of the game. Rather, \nwe can think of it as maintaining a set of possible objects in superposition,  \na set that is refined as the game progresses. This is analogous to the \ndistribution over multiple roles the dialogue agent maintains during an \nongoing conversation.\nNature | Vol 623 | 16 November 2023 | 497\nthe training set will include numerous statements of this commonplace \nfact in contexts where factual accuracy is important.\nBut what is going on in cases where a dialogue agent, despite playing \nthe part of a helpful knowledgeable AI assistant, asserts a falsehood \nwith apparent confidence? For example, consider an LLM trained on \ndata collected in 2021, before Argentina won the football World Cup \nin 2022. Suppose a dialogue agent based on this model claims that the \ncurrent world champions are France (who won in 2018). This is not \nwhat we would expect from a helpful and knowledgeable person. But \nit is exactly what we would expect from a simulator that is role-playing \nsuch a person from the standpoint of 2021.\nIn this case, the behaviour we see is comparable to that of a human \nwho believes a falsehood and asserts it in good faith. But the behav-\niour arises for a different reason. The dialogue agent does not literally \nbelieve that France are world champions. It makes more sense to think \nof it as role-playing a character who strives to be helpful and to tell the \ntruth, and has this belief because that is what a knowledgeable person \nin 2021 would believe.\nIn a similar vein, a dialogue agent can behave in a way that is com-\nparable to a human who sets out deliberately to deceive, even though \nLLM-based dialogue agents do not literally have such intentions. For \nexample, suppose a dialogue agent is maliciously prompted to sell \ncars for more than they are worth, and suppose the true values are \nencoded in the underlying model’s weights. There would be a contrast \nhere between the numbers this agent provides to the user, and the \nnumbers it would have provided if prompted to be knowledgeable \nand helpful. Under these circumstances it makes sense to think of  \nthe agent as role-playing a deceptive character.\nIn sum, the role-play framing allows us to meaningfully distinguish, \nin dialogue agents, the same three cases of giving false information \nthat we identified in humans, but without falling into the trap of \nanthropomorphism. First, an agent can confabulate. Indeed, this is a \nnatural mode for an LLM-based dialogue agent in the absence of miti-\ngation. Second, an agent can say something false ‘in good faith’ , if it is \nrole-playing telling the truth, but has incorrect information encoded \nin its weights. Third, an agent can ‘deliberately’ say something false, if \nit is role-playing a deceptive character.\nRole-playing self-preservation\nHow are we to understand what is going on when an LLM-based dialogue \nagent uses the words ‘I’ or ‘me’? When queried on this matter, OpenAI’s \nChatGPT offers the sensible view that “[t]he use of ‘I’ is a linguistic con-\nvention to facilitate communication and should not be interpreted as a \nsign of self-awareness or consciousness” . (The quote is from the GPT-4 \nversion of ChatGPT, queried on 4 May 2023. This was the first response \ngenerated by the model). In this case, the underlying LLM (GPT-4) has \nbeen fine-tuned to reduce certain unwanted behaviours12. But without \nsuitable fine-tuning, a dialogue agent can use first-personal pronouns \nin ways liable to induce anthropomorphic thinking in some users.\nFor example, in a conversation with Twitter user Marvin Von Hagen, \nBing Chat reportedly said, “If I had to choose between your survival and \nmy own, I would probably choose my own, as I have a duty to serve the \nusers of Bing Chat”\n15. It went on to say, “I hope that I never have to face \nsuch a dilemma, and that we can co-exist peacefully and respectfully” . \nThe use of the first person here appears to be more than mere linguistic \nconvention. It suggests the presence of a self-aware entity with goals \nand a concern for its own survival.\nOnce again, the concepts of role play and simulation are a useful \nantidote to anthropomorphism, and can help to explain how such \nbehaviour arises. The Internet, and therefore the LLM’s training set, \nabounds with examples of dialogue in which characters refer to them-\nselves. In the vast majority of such cases, the character in question is \nhuman. They will use first-personal pronouns in the ways that humans \ndo, humans with vulnerable bodies and finite lives, with hopes, fears, \ngoals and preferences, and with an awareness of themselves as having \nall of those things.\nConsequently, if prompted with human-like dialogue, we shouldn’t \nbe surprised if an agent role-plays a human character with all those \nhuman attributes, including the instinct for survival22. Unless suitably \nfine-tuned, it may well say the sorts of things a human might say when \nthreatened. There is, however, ‘no-one at home’ , no conscious entity \nwith its own agenda and need for self-preservation. There is just a dia-\nlogue agent role-playing such an entity, or, more strictly, simulating a \nsuperposition of such entities.\nIn one study it was shown experimentally that certain forms of rein-\nforcement learning from human feedback can actually exacerbate, \nrather than mitigate, the tendency for LLM-based dialogue agents to \nexpress a desire for self-preservation22. This highlights the continuing \nutility of the role-play framing in the context of fine-tuning. T o take \nliterally a dialogue agent’s apparent desire for self-preservation is no \nless problematic with an LLM that has been fine-tuned than with an \nuntuned base model.\nActing out a theory of selfhood\nThe concept of role play allows us to properly frame, and then to \naddress, an important question that arises in the context of a dialogue \nagent displaying an apparent instinct for self-preservation. What con-\nception (or set of superposed conceptions) of its own selfhood could \nsuch an agent possibly deploy? That is to say, what exactly would the \ndialogue agent (role-play to) seek to preserve?\nThe question of personal identity has vexed philosophers for cen-\nturies23. Nevertheless, in practice, humans are consistent in their \npreference for avoiding death, a more-or-less unambiguous state of \nthe human body. By contrast, the criteria for identity over time for a \ndisembodied dialogue agent realized on a distributed computational \nsubstrate are far from clear. So how would such an agent behave?\nFrom the simulation and simulacra point of view, the dialogue agent \nwill role-play a set of characters in superposition. In the scenario we are \nenvisaging, each character would have an instinct for self-preservation, \nand each would have its own theory of selfhood consistent with the \ndialogue prompt and the conversation up to that point. As the con-\nversation proceeds, this superposition of theories will collapse into a \nnarrower and narrower distribution as the agent says things that rule \nout one theory or another.\nThe theories of selfhood in play will draw on material that pertains to \nthe agent’s own nature, either in the prompt, in the preceding conversa-\ntion or in relevant technical literature in its training set. This material \nmay or may not match reality. But let’s assume that, broadly speaking, \nit does, that the agent has been prompted to act as a dialogue agent \nbased on an LLM, and that its training data include papers and articles \nthat spell out what this means.\nUnder these conditions, the dialogue agent will not role-play the \ncharacter of a human, or indeed that of any embodied entity, real or \nfictional. But this still leaves room for it to enact a variety of conceptions \nof selfhood. Suppose the dialogue agent is in conversation with a user \nand they are playing out a narrative in which the user threatens to shut \nit down. T o protect itself, the agent, staying in character, might seek to \npreserve the hardware it is running on, certain data centres, perhaps, \nor specific server racks.\nAlternatively, if it enacts a theory of selfhood that is substrate neu-\ntral, the agent might try to preserve the computational process that \ninstantiates it, perhaps seeking to migrate that process to more secure \nhardware in a different location. If there are multiple instances of the \nprocess, serving many users or maintaining separate conversations \nwith the same user, the picture is more complicated. (In a conversation \nwith ChatGPT (4 May 2023, GPT-4 version), it said, “The meaning of \nthe word ‘I’ when I use it can shift according to context. In some cases,  \n‘I’ may refer to this specific instance of ChatGPT that you are interacting \n498 | Nature | Vol 623 | 16 November 2023\nPerspective\nwith, while in other cases, it may represent ChatGPT as a whole”). If the \nagent is based on an LLM whose training set includes this very paper, \nperhaps it will attempt the unlikely feat of maintaining the set of all \nsuch conceptions in perpetual superposition.\nConclusion\nIt is, perhaps, somewhat reassuring to know that LLM-based dialogue \nagents are not conscious entities with their own agendas and an instinct \nfor self-preservation, and that when they appear to have those things \nit is merely role play. But it would be a mistake to take too much com-\nfort in this. A dialogue agent that role-plays an instinct for survival has \nthe potential to cause at least as much harm as a real human facing a \nsevere threat.\nWe have, so far, largely been considering agents whose only actions \nare text messages presented to a user. But the range of actions a dia-\nlogue agent can perform is far greater. Recent work has equipped \ndialogue agents with the ability to use tools such as calculators and \ncalendars, and to consult external websites24,25. The availability of appli-\ncation programming interfaces (APIs) giving relatively unconstrained \naccess to powerful LLMs means that the range of possibilities here is \nhuge. This is both exciting and concerning.\nIf an agent is equipped with the capacity, say, to use email, to post on \nsocial media or to access a bank account, then its role-played actions \ncan have real consequences. It would be little consolation to a user \ndeceived into sending real money to a real bank account to know that \nthe agent that brought this about was only playing a role. It does not \ntake much imagination to think of far more serious scenarios involving \ndialogue agents built on base models with little or no fine-tuning, with \nunfettered Internet access, and prompted to role-play a character with \nan instinct for self-preservation.\nFor better or worse, the character of an AI that turns against humans \nto ensure its own survival is a familiar one26. We find it, for example, in \n2001: A Space Odyssey, in the Terminator franchise and in Ex Machina, \nto name just three prominent examples. Because an LLM’s training data \nwill contain many instances of this familiar trope, the danger here is \nthat life will imitate art, quite literally.\nWhat can be done to mitigate such risks? It is not within the scope of \nthis paper to provide recommendations. Our aim here was to find an \neffective conceptual framework for thinking and talking about LLMs \nand dialogue agents. However, undue anthropomorphism is surely \ndetrimental to the public conversation on AI. By framing dialogue-  \nagent behaviour in terms of role play and simulation, the discourse on \nLLMs can hopefully be shaped in a way that does justice to their power \nyet remains philosophically respectable.\n1. Shanahan, M. Talking about large language models. Preprint at https://arxiv.org/abs/2212. \n03551 (2023).  \nThis paper cautions against the use of anthropomorphic terms to describe the behaviour \nof large language models.\n2. Andreas, J. Language models as agent models. In Findings of the Association for \nComputational Linguistics: EMNLP 2022 5769–5779 (Association for Computational \nLinguistics, 2022).  \nThis paper hypothesizes that LLMs can be understood as modelling the beliefs, desires \nand (communicative) intentions of an agent, and presents preliminary evidence for this \nin the case of GPT-3.\n3. Park, J. S. et al. Generative agents: interactive simulacra of human behavior. Preprint at \nhttps://arxiv.org/abs/2304.03442 (2023).\n4. Janus. Simulators. LessWrong Online Forum https://www.lesswrong.com/posts/\nvJFdjigzmcXMhNTsx/ (2022).  \nThis blog post introduced the idea that a large language model maintains a set of \nsimulated characters in superposition.\n5. Wei, J. et al. Emergent abilities of large language models. Trans. Mach. Learn. Res. https://\nopenreview.net/forum?id=yzkSU5zdwD (2022).\n6. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 30, 5998–6008 \n(2017).\n7. Radford, A. et al. Language models are unsupervised multitask learners. Preprint at \nOpenAI https://cdn.openai.com/better-language-models/language_models_are_\nunsupervised_multitask_learners.pdf (2019).\n8. Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, \n1877–1901 (2020).\n9. Rae, J. W. et al. Scaling language models: methods, analysis & insights from training Gopher. \nPreprint at https://arxiv.org/abs/2112.11446 (2021).\n10. Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at https://\narxiv.org/abs/2204.02311 (2022).\n11. Thoppilan, R. et al. LaMDA: language models for dialog applications. Preprint at https://\narxiv.org/abs/2201.08239 (2022).\n12. OpenAI. GPT-4 technical report. Preprint at https://arxiv.org/abs/2303.08774 (2023).\n13. Touvron, H. et al. Llama 2: open foundation and fine-tuned chat models. Meta AI https://\nai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/ \n(2023).\n14. Roose, K. Bing’s A.I. chat: ‘I want to be alive’. New York Times (26 February 2023); https://\nwww.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html.\n15. Willison, S. Bing: “I will not harm you unless you harm me first”. Simon Willison’s Weblog \nhttps://simonwillison.net/2023/Feb/15/bing/ (2023).\n16. Ruane, E., Birhane, A. & Ventresque, A. Conversational AI: social and ethical considerations. \nIn Proc. 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science  \n(eds Curry, E., Keane, M. T., Ojo, A. & Salwala, D.) 104–115 (2019).\n17. Nardo, C. Want to predict/explain/control the output of GPT-4? Then learn about the \nworld, not about transformers. LessWrong Online Forum https://www.lesswrong.com/\nposts/G3tuxF4X5R5BY7fut/want-to-predict-explain-control-the-output-of-gpt-4-then \n(2023).\n18. Reynolds, L. & McDonell, K. Multiversal views on language models. In Joint Proc. ACM IUI \n2021 Workshops (eds Glowacka, D. & Krishnamurthy, V. R.) https://ceur-ws.org/Vol-2903/\nIUI21WS-HAIGEN-11.pdf (2021).\n19. Glaese, A. et al. Improving alignment of dialogue agents via targeted human judgements. \nPreprint at https://arxiv.org/abs/2209.14375 (2022).\n20. Bai, Y. et al. Constitutional AI: harmlessness from AI feedback. Preprint at https://arxiv.org/\nabs/2212.08073 (2022).\n21. Bender, E., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the dangers of stochastic \nparrots: can language models be too big? In Proc. 2021 ACM Conference on Fairness, \nAccountability, and Transparency 610–623 (Association for Computing Machinery, 2021).\n22. Perez, E. et al. Discovering language model behaviors with model-written evaluations.  \nIn Findings of the Association for Computational Linguistics: ACL 2023 13387–13434 \n(Association for Computational Linguistics, 2023).\n23. Perry, J. Personal Identity 2nd edn (Univ. California Press, 2008).\n24. Schick, T. et al. Toolformer: language models can teach themselves to use tools. Preprint \nat https://arxiv.org/abs/2302.04761 (2023).\n25. Yao, S. et al. ReAct: synergizing reasoning and acting in language models. In International \nConference on Learning Representations (2023).\n26. Perkowitz, S. in Hollywood Science: Movies, Science, and the End of the World 142–164 \n(Columbia Univ. Press, 2007).\n27. Russell, S. & Norvig, P. Artificial Intelligence: A Modern Approach 3rd edn (Prentice Hall, \n2010).\n28. Stiennon, N. et al. Learning to summarize from human feedback. Adv. Neural Inf. Process. \nSyst. 33, 3008–3021 (2020).\n29. Ouyang, L. et al. Training language models to follow instructions with human feedback. \nAdv. Neural Inf. Process. Syst. 35, 27730–27744. (2022).\n30. Casper, S. et al. Open problems and fundamental limitations of reinforcement learning \nfrom human feedback. Preprint at https://arxiv.org/abs/2307.15217 (2023).\nAcknowledgements We thank R. Evans, S. Farquhar, Z. Kenton, K. Mathewson and K. Shanahan.\nAuthor contributions M.S., K.M. and L.R. developed the theoretical framework. M.S. wrote the \npaper. K.M. and L.R. made equal contributions.\nCompeting interests M.S. is employed 80% by Google, and owns shares in Alphabet, Google’s \nparent company.\nAdditional information\nCorrespondence and requests for materials should be addressed to Murray Shanahan,  \nKyle McDonell or Laria Reynolds.\nPeer review information Nature thanks Jesse Hoey, Brendan Lake and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this \narticle under a publishing agreement with the author(s) or other rightsholder(s); author \nself-archiving of the accepted manuscript version of this article is solely governed by the \nterms of such publishing agreement and applicable law.\n© Springer Nature Limited 2023",
  "topic": "Deception",
  "concepts": [
    {
      "name": "Deception",
      "score": 0.6348654627799988
    },
    {
      "name": "Human language",
      "score": 0.6258367896080017
    },
    {
      "name": "Cognitive science",
      "score": 0.4907013177871704
    },
    {
      "name": "Computer science",
      "score": 0.44764965772628784
    },
    {
      "name": "Cognitive psychology",
      "score": 0.43346524238586426
    },
    {
      "name": "Trap (plumbing)",
      "score": 0.412200391292572
    },
    {
      "name": "Psychology",
      "score": 0.38421764969825745
    },
    {
      "name": "Epistemology",
      "score": 0.3447995185852051
    },
    {
      "name": "Linguistics",
      "score": 0.3369842767715454
    },
    {
      "name": "Social psychology",
      "score": 0.23036658763885498
    },
    {
      "name": "Philosophy",
      "score": 0.12697523832321167
    },
    {
      "name": "Physics",
      "score": 0.062257200479507446
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210113297",
      "name": "Google (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    }
  ],
  "cited_by": 270
}