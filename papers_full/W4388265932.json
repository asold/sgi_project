{
  "title": "Exhaustive local chemical space exploration using a transformer model",
  "url": "https://openalex.org/W4388265932",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5013544797",
      "name": "Alessandro Tibo",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5089237271",
      "name": "Jiazhen He",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5030485362",
      "name": "Jon Paul Janet",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5077542460",
      "name": "Eva Nittinger",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5076975589",
      "name": "Ola Engkvist",
      "affiliations": [
        "AstraZeneca (Sweden)",
        "Chalmers University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2412446857",
    "https://openalex.org/W2023818227",
    "https://openalex.org/W2967422457",
    "https://openalex.org/W3029978182",
    "https://openalex.org/W4233253307",
    "https://openalex.org/W4287182533",
    "https://openalex.org/W3089428833",
    "https://openalex.org/W2753588101",
    "https://openalex.org/W2937307539",
    "https://openalex.org/W4360884546",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2999242200",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2786722833",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W3100358278",
    "https://openalex.org/W4220802400",
    "https://openalex.org/W4297632148",
    "https://openalex.org/W4214868967",
    "https://openalex.org/W3210331473",
    "https://openalex.org/W4242372416",
    "https://openalex.org/W2153693853",
    "https://openalex.org/W4297951436",
    "https://openalex.org/W4237624520",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W1991133427",
    "https://openalex.org/W4237801237",
    "https://openalex.org/W4230007416",
    "https://openalex.org/W4322713498",
    "https://openalex.org/W2975022113",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4234451290",
    "https://openalex.org/W4234959353",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3166396011"
  ],
  "abstract": "How many near-neighbors does a molecule have? This is a simple, fundamental, but unsolved question in chemistry. It is key for solving many important molecular optimization problems, for example in lead optimization in drug discovery. Generative models can sample virtual molecules from a vast theoretical chemical space, but so far have lacked explicit knowledge about molecular similarity. This means that a generative model needs to be guided by reinforcement learning or another learning mechanism to be able to sample a relevant chemical space. Correspondingly the generative model provide no mechanism for quantifying how completely it can sample a particular region of the chemical space. To overcome these limitations, a novel source-target molecular transformer model is proposed. The transformer model have a similarity kernel based regularization function. It has been trained on, to the best of our knowledge, the largest data set of molecular pairs so far consisting of ≥ 200 billion pairs. The regularization term enforces a direct relationship between the log-likelihood of generating a target molecule and its similarity to a given source molecule. The model is able to systematically sample compounds ordered by their log-likelihood and therefore by their similarity. In combination with a deterministic sampling strategy, beam search, it is possible for the first time to comprehensively explore the near-neighborhood around a specific compound. Our results show that the regularization term helps to substantially improve the correlation between the log-likelihood of generating a target compound and its similarity to the source compound. The resulting model is able to exhaustively sample a near-neighborhood around a drug-like molecule.",
  "full_text": "Exhaustive local chemical space exploration using\na transformer model\nAlessandro Tibo∗1, Jiazhen He1, Jon Paul Janet1, Eva Nittinger2,\nand Ola Engkvist1,3\n1Molecular AI, Discovery Sciences, R&D, AstraZeneca, Gothenburg,\nSweden\n2Medicinal Chemistry, Research and Early Development,\nRespiratory and Immunology (R&I), BioPharmaceuticals R&D\nAstraZeneca, Gothenburg, Sweden\n3Data Science and AI, Computer Science and Engineering,\nChalmers, Gothenburg, Sweden\nAbstract\nHow many near-neighbors does a molecule have? This is a simple, fun-\ndamental, but unsolved question in chemistry. It is key for solving many\nimportant molecular optimization problems, for example in lead optimiza-\ntion in drug discovery. Generative models can sample virtual molecules from\na vast theoretical chemical space, but so far have lacked explicit knowledge\nabout molecular similarity. This means that a generative model needs to\nbe guided by reinforcement learning or another learning mechanism to be\nable to sample a relevant chemical space. Correspondingly the generative\nmodel provide no mechanism for quantifying how completely it can sample\na particular region of the chemical space. To overcome these limitations,\na novel source-target molecular transformer model is proposed. The trans-\nformer model have a similarity kernel based regularization function. It has\nbeen trained on, to the best of our knowledge, the largest data set of molec-\nular pairs so far consisting of ≥ 200 billion pairs. The regularization term\nenforces a direct relationship between the log-likelihood of generating a tar-\nget molecule and its similarity to a given source molecule. The model is\nable to systematically sample compounds ordered by their log-likelihood and\ntherefore by their similarity. In combination with a deterministic sampling\nstrategy, beam search, it is possible for the first time to comprehensively ex-\nplore the near-neighborhood around a specific compound. Our results show\nthat the regularization term helps to substantially improve the correlation\nbetween the log-likelihood of generating a target compound and its similarity\n∗Corresponding Author: alessandro.tibo@astrazeneca.com\n1\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nto the source compound. The resulting model is able to exhaustively sample\na near-neighborhood around a drug-like molecule.\nKeywords: Molecular optimization, Transformer, Tanimoto similarity, Neg-\native log-likelihood, Beam search.\n1 Introduction\nThe so called similarity principle [1] - that structurally similar molecules share sim-\nilar properties - is a key concept in drug discovery and molecular design in general.\nThe main challenge in drug discovery is to find compounds with a combination of\ndesirable properties such as absorption, distribution, metabolism, elimination and\ntoxicity, safety and potency. Molecular optimization aims to address this challenge\nby exploiting the similarity principle, improving properties of molecules through\nsmall changes while still retaining or improving already desirable properties, for\nexample retaining affinity against a drug target while improving aqueous solubility.\nThe enormous sized “drug-like” chemical space is frequently discussed. One es-\ntimation based on the GDB-17 data set is that the chemical space contains 10 33\ncompounds [2]. There is no broadly accepted way to quantify how much of this vast\nspace is similar to a given compound of interest, in other words how dense is the\nchemical space. Further, although there exist methods that allow for local combi-\nnatorial modification of compounds based on reagents [3, 4] and based on matched\nmolecular pairs (MMP) [5], despite the key practical importance of this task, no ex-\nisting method is currently available that can systematically and exhaustively sample\nthis bespoke chemical space.\nIn recent years, the application of deep learning methods have had a dramatic\nimpact in the field of chemistry [6, 7] and drug discovery [8, 9, 10]. Advances in\nmachine learning, including transformers [11], which have already shown remark-\nable success in natural language processing [12, 13] and computer vision [14, 15, 16],\nare readily adapted to solve domain-specific problems including molecular design.\nMany different deep learning architectures have been proposed to explore the chem-\nical space. [17, 18] proposed recurrent neural networks, [19] investigated generative\nadversarial networks, and [20, 21] used variational autoencoders to sample new\nmolecules. All of these techniques allow sampling compounds from the chemi-\ncal space, but they do not naturally encode the localized search characteristic of\nmolecular optimization. The generated molecules need to be refined with e.g. rein-\nforcement learning approaches [22, 23] or via coupling the latent representation to\na predictive model.\nOthers [24] have been treating molecular optimization as a translation task\nbetween a source and a target molecule, inspired by natural language processing\n(NLP). These source-target based methods require a data set of molecular pairs for\ntraining. Inspired by the medicinal chemistry concept of matched molecular pairs,\nHe et al. [25] generated molecular pairs based on MMP, identical scaffolds, or a\nTanimoto similarity above a certain threshold. The set of pairs were used to train\ndifferent molecular transformer models that was used to explore the region of the\nchemical space relatively close to a source compound.\n2\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nA molecular transformer is able to generate the most precedented (probable)\nmolecular transformations from a specific source molecule. The precedence associ-\nated to the molecular transformation to a target molecule is learnt by the model\nduring the training phase, but the precedence is not necessarily related to the molec-\nular similarity between the source and target molecule, i.e. the model can generate\nprecedented target molecules which are very dissimilar to the source molecule. This\nbehaviour is not optimal in applications such as lead optimization where one would\nlike to be able to sample all similar compounds given a specific source molecule.\nMotivated by the fact that aforementioned approaches have only an intrinsic\nknowledge of the similarity between molecules, given by the way the molecular\npairs are constructed, we propose in this paper a novel framework to systemati-\ncally sample target molecules that are both precedented and similar to the source\nmolecule. We stress the fact that similarity alone is not enough as there exist target\nmolecules similar to a source molecule that should have low precedence, for example\nunstable target molecules or target molecules containing an unusual atomic element.\nTo improve on the current molecular transformer models, we have developed a\nnovel source-target molecular transformer model, trained at a large scale on 10 11\nmolecular pairs, that is able to pseudo-exhaustively sample the precedented near-\nneighborhood of a given molecule. We adopted the same molecular transformer\nmodel as proposed by [25] but included a new regularization term into the training\nprocess. This additional term penalizes the generated target compounds if their\nsimilarity to the source compound does not align with their assigned negative log-\nlikelihood (NLL), which is used as a proxy for precedence (probability). In contrast\nto many recent works which used [26] to train the models, we adopted PubChem [27]\nwhich contains 40 × more molecules than ChEMBL, and accordingly many more\npairs can be extracted and used for training.\nThe main contributions of this work can be summarized as:\n• we introduce a regularization term in the training loss for a source-target\nmolecular transformer which establishes a direct relationship between the\nprobability (NLL) of sampling a particular target molecule given a specific\nsource molecule and a similarity metric;\n• we use this method to train and release into the public domain a new foun-\ndational molecular transformer model trained on what is, to the best of\nour knowledge, the largest ever data set of molecular pairs assembled, com-\nprising of over 200 billion pairs. Some recent source-target models [24, 25]\nused 2 to 10 million pairs, while previous foundational chemical models such\nas ChemBERTa [28] are trained on around 100 million unique compounds\nand contrastive-learning baselines are available at the 10 million molecule\nscale [29];\n• our novel transformer model allow for the approximately exhaustive sampling\nby using beam search to identify all target molecules up to a given NLL for\na source molecule. This corresponds by construction to an approximately\ncomplete precedented near-neighborhood for a given source molecule.\n3\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nCC(Cc1ccc(O)cc1)NCC(O)C(N)=O\nDECODERENCODER\nTRANSFORMER\ntarget\nsource\ntarget1\nCC(Cc1ccc(Cl)cc1)NCC(O)C(N)=O\nCC(Cc1ccc(Cl)cc1)NCC(O)C(N)=S\ntarget2CC(Cc1ccc(Cl)cc1)NCC(F)C(N)=O\nFigure 1: The transformer model receives as input pairs of molecules consisting\nof a source molecule and a target molecule (represented as SMILES strings). The\nmodel is trained to transform a source molecule to a target molecule. At training\nmolecular pairs of source and target molecules are used to train the transformer\nmodel. At inference a source molecule is transformed into many target molecules\n(target1 and target2 in the figure). Red and blue arrows identify the flow during\ntraining and inference, respectively.\n2 Methods\n2.1 Transformer model with a regularized loss function\nA transformer [11, 24, 25] architecture trained on a SMILES (simplified molecular-\ninput line-entry system) string representation of a molecule was used. The purpose\nof the study is to compare a novel framework for training a molecular transformer\nwith a regularized loss function with existing unconstrained training framework [25].\nHowever, our framework can in general be applied to any transformer model based\non molecular pairs constructed with a similarity metric. Formally, we denote with\nX the chemical space, and with P = {(s, t) | s, t∈ X × X}the set of molecular\npairs constructed from X, s and t are the source and target molecules, respectively.\nEach element of sources and targets is a token (symbol) which takes values from a\nvocabulary (ordered set) V . Furthermore, we denote with fθ : X × X →[0, 1]|V |\nthe transformer with θ being the set of its parameters. To simplify, we denote\nwith the same symbol a molecule and its SMILES string representation. The trans-\nformer, fθ, consists of an encoder and a decoder which are simultaneously fed by\nsource molecules and target molecules, respectively. Source molecules and target\nmolecules are first tokenized and then converted into sequences of dense vector em-\nbeddings and passed over several stacked encoder and decoder layers, respectively.\nThe final encoder output is merged into the decoder through a multihead attention\nmechanism which captures the transformation from a source molecule to a target\nmolecule. Figure 1 shows the general architecture of our transformer model, more\ndetails about the transformer architecture can be found in [11, 24, 25].\nfθ models the probability p of the ℓ−th token of a targettiℓ given all the previous\n4\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nti,1:ℓ−1 = tiℓ−1, . . . , ti1 target tokens and source si compound, i.e. fθ(ti,1:ℓ−1, si)[tiℓ] =\np(tiℓ|ti,1:ℓ−1, si). The transformer’s parameters θ are then trained on P by min-\nimizing the negative log-likelihood (NLL) of the entire SMILES string, p(ti|si),\n(si, ti) ∈ Pfor all i = 1, . . . ,|P|, that is\nNLLi = −log p(ti|si) = −log\nLY\nℓ=1\np(tiℓ|tiℓ−1, . . . , ti1, si)\n= −\nLX\nℓ=1\nlog p(tiℓ|tiℓ−1, . . . , ti1, si) = −\nLX\nℓ=1\nlog fθ(ti,1:ℓ−1, si)[tiℓ], (1)\nwhere L denotes the number of tokens associated to ti, and [ tiℓ] denotes the index\nof vector fθ(ti,1:ℓ−1, si) corresponding to the token tiℓ. The NLL represents the\nprobability of transforming a given source molecule into a specific target molecule.\nThe NLL is always non-negative and the higher the NLL value, the less likely\na target molecule is to be generated. An NLL equal to 0.0 would imply that a\nspecific target molecule would have the probability of 1.0 to be generated from the\nsource molecule.\nThe loss in Equation 1 allows the transformer to learn but it arbitrarily associates\nnegative log-likelihood to target molecules. This behaviour is not ideal since during\ninference we would like the probability of generating a target molecule given a\nspecific source molecule, p(t|s), to be proportional to the similarity between the\nmolecules. To mitigate this issue, we introduce in Equation 2, a regularization term\nto the loss in Equation 1 which penalizes the NLL if the order relative to a similarity\nmetric is not respected.\nΩ((si, ti), (sj, tj)) = max(0, sign(κ(si, ti) − κ(sj, tj)) (NLLi − NLLj)), (2)\nwhere (si, ti) and (sj, tj) are two pairs fromP, and κ is an arbitrarily kernel function.\nThe Tanimoto similarity was chosen as κ but our framework is general so any valid\nkernel can be used. Note that the NLL is always non-negative in our context.\nDuring training, we sample a batch of source-target molecule pairs and we compute\nthe regularization term in Equation 2 for the all the pairs included in the batch.\nFigure 2 depicts an example of the ranking loss calculation. Finally, to train the\nmodel we propose the following loss function as a combination of Equations 1 and 2\nL = 1\n|P|\n|P|X\ni=1\nNLLi + λ\n|P|(|P| −1)\n|P|−1X\ni=1\n|P|X\nj=i+1\nΩ((si, ti), (sj, tj)), (3)\nwhere λ ≥ 0 is a hyper-parameter controlling the weight of the regularization term.\nConnected to the similarity is the notion of precedence. Precedence is learnt by\na transformer model based on the training data and represents the probability for\na source compound to be transformed into a specific target compound. The NLL\ncan be used as proxy for the precedence where a low NLL means a high precedence\nand vice versa. Figure 3 illustrates an example where a nitrogen atom in penicillin\nis replaced by a oxygen atom, phosphorus atom, or arsenic atom, respectively. It\nis important to notice that the Tanimoto similarities (calculated with the count\n5\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nFigure 2: Two pairs of molecules ( s1, t1), ( s2, t2) with Tanimoto similarity 0.71\nand 0.76, respectively are shown. In the left example NLL 2 > NLL1, therefore\nthe regularization term Ω (see Equation 2) is greater than 0. In the right example\nNLL1 > NLL2. In this case, Ω = 0 as the similarities and NLLs are in the same\norder.\nversion of ECFP4 fingerprints) between Penicillin and the modified compounds are\nexactly the same, while their NLLs are different, as the compound with an arsenic\natom has lower precedence compared to the compound with a phosphorus atom,\nand the compound with a phosphorus atom has lower precedence compared to\nthe compound with an oxygen atom and the compound with an oxygen has lower\nprecedence than penicillin. We can therefore anticipate that the compound with\nan oxygen atom will be retrieved with a much higher probability during inference\nthan the compounds with a phosphorous atom and arsenic atom, respectively.\n2.2 Approximately exhaustive sampling of the chemical space\nTwo different techniques can be used to sample target molecules with a molecular\ntransformer: multinomial sampling (used e.g. by [24, 25]) and beam search [30].\nMultinomial sampling allows fast generation of compounds distributed according\nto their NLL. Given a source compound s ∈ X, the length L of the tokens in\nthe SMILES string associated with s, and V the vocabulary, i.e. the set of possi-\nble tokens, the computationally complexity of multinomial sampling is O(L · |V |).\nHowever, multinomial sampling suffers from mode collapse i.e. the same target\ncompound might be sampled multiple times, and the method is not deterministic,\ni.e. different runs produce different target compounds. Beam search is, in con-\ntrast to multinomial sampling, deterministic but computationally more complex\nO(B · L · |V |), where B is the beam size. Beam search retains B unique SMILES\nstrings sorted by their corresponding NLL. Note that for both the techniques, the\ncomplexity of the underlying transformer model impacts the performance. This\ncomplexity arises because SMILES strings are generated iteratively by feeding the\ntransformer with n − 1 tokens to obtain the n−th. In fact, for multinomial sam-\npling, the model needs to compute the probabilities of each token in the vocabulary,\nwhile for beam search, we need to store the B SMILES subsequences with the most\nfavorable NLL. Similarly to multinomial sampling, we also need to compute the\nprobabilities of each token in the vocabulary for each subsequence. Note that beam\n6\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nPenicillin - CC1(C)SC2C(NC(=O)Cc3ccccc3)C(=O)N2C1C(=O)O\nS=0.726 - NLL=20.199S=1.000 - NLL=5.559 S=0.726 - NLL=9.144 S=0.726 - NLL=13.468\nFigure 3: Penicillin (used here as the source molecule) and its derivatives where a\nnitrogen atom (N) is replaced with an oxygen atom (O), phosphorus atom (P), and\narsenic atom (As), respectively. The molecular similarity between Penicillin and\nthe modified analogues are the same, while their corresponding NLLs (precedence)\nare different. The molecular transformation from penicillin to the derivative with\nan arsenic atom has lower precedence (higher NLL) than transforming penicillin to\nthe derivatives with phosphorus and oxygen, respectively. The derivative with an\noxygen atom has the highest precedence after penicillin. The Tanimoto similarity\nis denoted with the letter S on top of modified penicillin analogue in the lower part\nof the figure.\nsearch is an approximate exhaustive search and it might miss compounds with a\nfavorable NLL.\n2.3 Data preparation\nMolecular structures were downloaded from PubChem as SMILES strings. In total\n102, 419, 980 compounds were downloaded 1. The data set was pre-processed as\nfollows:\n• the SMILES strings were standardized using MolVS 2 including the following\nsteps: sanitize, remove hydrogens, apply normalization rules, re-ionize acids,\nand keep stereo chemistry;\n• all duplicate structures were removed;\n• all the compounds containing an isotopic label were removed.\nStarting from the set of preprocessed SMILES strings X (containing 102, 377, 734\nSMILES strings), we constructed a data set D = {(s, t) | s, t∈ P, κ(s, t) ≥ 0.5}\ncontaining all the pairs having a Tanimoto similarity κ(s, t) ≥ 0.50. The Tanimoto\nsimilarity is calculated with the RDKit [31] (version 2022.09.5) Morgan binary fin-\ngerprint (ECFP4) with radius equals to 2, 1024 bits, calculated from the SMILES\nstrings in X. The number of molecular pairs in D is 217 , 111, 386, 586, which is\n1PubChem dynamically grows. This number reflects the available compounds by end of De-\ncember 2021.\n2https://molvs.readthedocs.io/en/latest/\n7\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nonly 0 .002% of the over 10 16 possible molecular pairs). The Tanimoto similarity\nfor the molecular pairs can be computed efficiently by storing the fingerprints in a\nbinary matrix of size N ×128 bytes in the GPU memory. In this way, the Tanimoto\nsimilarity among all the possible pairs can be efficiently computed by utilizing GPU\nparallelism. For each SMILES string in X we can calculate the Tanimoto similarity\nat once against all the other SMILES strings. The calculation of all molecular pairs\ntook 10 days on 16 A100 GPUs with 40GB of memory.\nStarting from D, we constructed an additional data set Dc where we kept all\nthe pairs having a ECFP4 Tanimoto similarity with counts greater or equal than\n0.50. The major advantage of Morgan fingerprints with counts compared to their\nbinary counterpart is their ability to capture the frequency of a substructure within\na molecule. Using count fingerprints makes it possible to differentiate between\nmolecules having the same substructures but different frequency of them and there-\nfore a lower similarity value can be assigned when comparing the two molecules.\nFigure 4 shows an example of the Tanimoto similarity between two molecules with\nthe binary and count fingerprints. The Tanimoto similarity with count fingerprints\ncannot be computed as efficiently as for binary fingerprints, therefore we first con-\nstruct D and then refine it to obtain Dc, where for each pair D we recomputed\nthe Tanimoto similarity on fingerprints with count and kept only those with values\ngreater or equal than 0 .50. Dc contains 173, 911, 600, 788 pairs. In Section 3, we\nwill show results for both D and Dc.\nTransformers receive a sequence of integers, therefore each SMILES string is\nfirst tokenized and then translated into a specific integer number. The tokens are\ncollected into a dictionary where keys are tokens and values are integers, that is\nV = { *:0, ^:1, $:2, <UNK>:3, ..., 1:48, ..., C:60, Cl:61, ... } .\nNote that V contains 4 special tokens: * is the padding token used to enforce\nthe same length of all SMILES, ^ is the starting token, $ the ending token, and\n<UNK> is the unknown token used at inference time if a new token is observed. We\nconstructed V from D which contains 455 different tokens. Figure 10 in Appendix\nshows an example of the SMILES string tokenization procedure.\n2.4 Model training and sampling\nFour transformer models were trained on D and Dc and with and without ranking\nloss. Every model was trained for 30 epochs, 3 utilizing 8 A100 GPUs, with each\ntraining cycle lasting for 30 days. All models were trained following the same\nstrategy and using the same hyperparameters as [25], including a batch size of 128,\nAdam optimizer and the original learning rate schedule [11] with 4,000 warmup\nsteps. Due to the computational time required to train a model, we could not\noptimize λ (see Section 2.1). However, while not necessarily optimal, the value we\nchose for λ, i.e. λ = 10 already highlights (see Section 3) the benefits from using\nthe ranking loss when assessing the overall quality of the models.\n3During an epoch all the source-target molecular pairs in the training set are included once.\n8\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nFingerprints without counts\nFingerprints with counts\n1.0\n0.5\nc1cc2c3c(cccc3c1)C1CC1C2\nc1cc2c3c(cccc3c1)C1CC3Cc4cccc5cccc(c45)C3CC1C2\nFigure 4: An example where the Tanimoto similarity evaluated with binary ECFP4\nfingerprints gives a poor estimation of the true similarity. The two molecules are\ndifferent but using binary fingerprints gives a Tanimoto similarity of 1.0. Contrarily,\nthe Tanimoto similarity evaluated on count fingerprints gives a much more realistic\nvalue of 0.5.\nOnce trained, the models can be used to generate target molecules conditioned\non a source molecule by predicting one token at a time. Initially, the decoder pro-\ncesses the start token along with the encoder outputs to sample the next token from\nthe probability distribution over all the tokens in the vocabulary. The generation\nprocess iteratively continues by producing the next token from the encoder outputs\nand all the previous generated tokens until the end token is found or a predefined\nmaximum sequence length (128) is reached. To allow for the sampling of multiple\ntarget molecules, beam search is used (see Section 2.2), and unless otherwise stated,\na beam size of 1 , 000 was used.\n2.5 Evaluation setup\nThe model was evaluated on two publicly available data sets: the Therapeutic Tar-\nget Database (TTD) [32] and compound series from recent scientific publications\nextracted from the ChEMBL database [26]. TTD contains clinically investigated\ndrug candidates, which we used to investigate exhaustive sampling of the near-\nneighborhood for drug-like molecules. The compound series from publications con-\ntains only novel molecules that were not part of our training data, resulting in a\nout-of-distribution set of molecules that we cluster into chemical series based on the\npublication that they were extracted from.\nEach data set was pre-processed using the strategy described in Section 2.3, and\ncompounds that contained tokens not present in the vocabulary V (used to train\nthe models) were removed. A final filtering was applied to both data sets in order\nto remove peptides and other non drug-like small molecules. Only compounds that\nsatisfied all the following criteria were kept:\n• Lipinski rule of five compliant [33];\n• molecular weight larger than 300 Dalton;\n9\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n• less than eight ring systems.\nThe final TTD dat aset contains 821 compounds, while the ChEMBL data set\ncontains 2,685 compounds distributed in 200 series with 5 to 60 compounds in\neach series. Both curated data sets are released together with the code. Notably,\ncompounds from ChEMBL were selected to be distinct from both training sets D\nand Dc, ensuring no overlap in between the sets.\n2.6 Evaluation metrics\nTo evaluate the impact of ranking loss (see Equation 2) on a fully trained model\nwe considered several metrics (for all of them the higher the better):\n• Validity: the percentage of target compounds generated by the transformer\nmodel that are valid according to RDKit. Validityis calculated by averaging\nthe percentage of valid target compounds sampled for each source compound;\n• Uniqueness: the percentage of unique target compounds generated by the\ntransformer model. In order to evaluate the uniqueness, the generated valid\ntarget compounds are canonicalized with RDKit to identify duplicates;Unique-\nness is calculated by averaging the percentage of valid unique target com-\npounds sampled for each source compound;\n• Top Identical: the number of cases where the target compound with the\nlowest NLL is identical to the source compound. Top Identicalallows to\nevaluate whether the ranking loss forces the transformer model to generate\nthe source compound as the generated target compound with the lowest NLL.\nNote that, κ(s, s) = 1 for all possible source compounds s. Top Identical\nis calculated by averaging over the source compounds;\n• Rank Score: the Kendall’s tau score τ between the Tanimoto similarity and\nthe NLL for the top ten compounds sampled by beam search. The score mea-\nsures the correspondence between the two rankings. The score have values\nin [−1, 1] range, where the extremes denote perfect disagreement and agree-\nment, respectively. Given a source compound s and N generated compounds\nˆt1, . . . ,ˆtN from s, τ is computed as:\nτ = 2\nN(N − 1)\nX\ni<j\nsign(log p(ˆtj|s) − log p(ˆti|s)) sign(κ(s, ˆtj) − κ(s, ˆti)).\n• Correlation: the Pearson correlation coefficient between the Tanimoto\nsimilarity and the NLL, which measures the linear correlation between the\nTanimoto Similarity and the NLL. It have values in the [ −1, 1] range, where\nthe extremes denote perfect disagreement and agreement, respectively. Given\na set of pairs P = {(xi, yi) | xi ∈ R, yi ∈ R}N\ni=1 the Pearson correlation\ncoefficient is computed as:\nρ =\nPN\ni=1(xi − ¯x)(yi − ¯y)qPN\ni=1(xi − ¯x)2\nqPN\ni=1(yi − ¯y)2\n,\n10\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nwhere ¯x (and similarly ¯y) is the average of all the xi, i.e. ¯x = 1/N PN\ni=1 xi.\nCorrelation is calculated by averaging over all the sampled target com-\npounds.\n3 Results\nFour different transformer models on D and Dc with and without ranking loss were\ntrained and the impact of the regularization term during training and the count\nversion of the ECFP4 fingerprints were evaluated (see Equation 2 and Section 2.4\nfor training details).\n3.1 Impact of ranking loss for sampling similar molecules\nWe evaluated all the metrics described in Section 2.6 for the 821 compounds in\nthe TTD database. The results in Table 1 clearly show that the binary and count\nversions perform approximately the same in terms of Validity, Uniqueness, and\nTop Identical. The models with regularization term ( λ = 10) significantly im-\nprove the Top Identical, Rank Score, and Correlation metrics. Finally,\nthe model trained on Dc with λ = 10 outperforms all the other models on the\nRank Score, and Correlation metrics, showing that the ECFP4 fingerprint\nwith counts achieves superior results compared to the binary version. The models\nwere trained on RDKit canonicalized SMILES strings, however, the transformer\nmodels can also generate non-canonicalized valid SMILES strings. The uniqueness\nafter the molecules are canonicalized, remains close to 1.0 in all cases, showing that\nin most cases the canonicalized version of the SMILES string is the only one gen-\nerated. However once stereochemistry is removed ( NS in Table 1), the uniqueness\nfalls to 0 .5 − 0.6, which suggests that these models are generating approximately\ntwo stereoisomers for most target molecules. The increase from 0 − 0.3 to 0.6 − 0.9\nfor Top Identicalwhen removing stereo-chemistry indicates that the transformer\nmodels generate different stereo-isomers with similar NLL.\nFigures 5 reports the results of generated compounds corresponding to the peni-\ncillin. Overall, the ranking score is always higher for the generated molecules from\nthe regularized ( λ = 10) models. Furthermore, the distribution of the Tanimoto\nsimilarity is always shifted to the right for the λ = 10 case, confirming that the\nregularized models produces more compounds similar to the sources.\n3.2 Exhaustive sampling of the near-neighborhood\nThe correlation between the NLL and molecular similarity implies that sampling\nto a particular NLL threshold corresponds to exhaustively sampling of the near-\nneighborhood around a given source molecule. That is, given a source molecule of\ninterest, the proposed training method for the transformer allows a controlled and\napproximately complete enumeration of the local, precedented near-neighborhood.\nThe user can decide how large the chemical space should be sampled by varying the\nbeam size. To demonstrate this, we exhaustively sampled the near-neighborhood\nof all molecules in the TTD database with a large beam size of B = 30, 000. All of\n11\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nTable 1: Combined results for all compounds in the TTD database for beam size\nB = 1, 000 and the different transformer models. A higher value is the better for all\nthe columns, and the best results are highlighted in bold. D and Dc represent the\ntraining sets generated with ECFP4 fingerprints without and with counts, respec-\ntively. The sub-columns P, C, and NS under Uniqueness and Top Identical\ndenotes different type of post-processing applied to the generated target compounds.\nThe sub-columns are fraction unique SMILES strings ( P), fraction unique SMILES\nstrings after canonicalization ( C), and fraction unique SMILES strings after re-\nmoving stereo-chemical information and canonicalization ( NS). λ = 0 denotes the\nabsence and λ = 10 the presence of the regularization term when training the\ntransformer models.\nTask Validity Uniqueness Top Identical Rank Score CorrelationP C NS P C NS\nD, λ = 0 1.00 1.00 0.97 0.60 0.04 0.04 0.62 0.24 ±0.27 0.37 ±0.13\nD, λ = 10 0.99 1.00 0.95 0.53 0.31 0.30 0.93 0.35±0.25 0.56 ±0.17\nDc, λ = 0 1.00 1.00 0.97 0.59 0.06 0.07 0.66 0.29 ±0.25 0.39 ±0.14\nDc, λ = 10 0.99 1.00 0.95 0.53 0.31 0.31 0.93 0.44±0.24 0.60 ±0.19\nPenicillin - CC1(C)SC2C(NC(=O)Cc3ccccc3)C(=O)N2C1C(=O)O\n0.5 0.6 0.7 0.8 0.9 1.0\n6\n8\n10\n12\n14\nExample on \nNLL\n= 0 - Correlation=0.47\n0.5 0.6 0.7 0.8 0.9 1.0\n6\n8\n10\n12\n14\n= 10 - Correlation=0.62\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\n0\n1\n2\n3Density\nSimilarity distribution\n= 0\n= 10\n0.5 0.6 0.7 0.8 0.9 1.0\nT animoto Similarity\n6\n8\n10\n12\n14\nExample on c\nNLL\n= 0 - Correlation=0.39\n0.5 0.6 0.7 0.8 0.9 1.0\nT animoto Similarity\n6\n8\n10\n12\n14\n= 10 - Correlation=0.63\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\nT animoto Similarity\n0\n2\n4\n6Density\n= 0\n= 10\nFigure 5: Similarity (x-axis) and NLL (y-axis) of generated target compounds with\npenicillin as the source compound (depicted on top). Top and bottom scatter\nplots refer to D (fingerprints without counts) and Dc (fingerprints with counts),\nrespectively. The first two scatter plots from the left show the Tanimoto similarity\nagainst the NLL for the λ = 0 and λ = 10 models. The Correlation is always\nbetter for λ = 10 models. The plot to the right shows the distribution of the\nTanimoto similarity of the generated compounds for the two models.\nthe resulting generated target molecules with Tanimoto similarity greater or equal\nthan 0 .8 were extracted. We denote this set the near-neighborhood. Naturally,\nlarger molecules have a larger near-neighborhood, and therefore source molecules\n12\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nwere grouped based on their heavy atom count (HAC). Between O(10) and O(104)\non average near-neighbors for source molecules between 13 and 36 heavy atoms, re-\nspectively (Figure 6) were identified. The threshold of 0.8 is chosen for illustrative\npurposes and represents a reasonable choice of a neighborhood size.Figure 7 depicts\nthe estimated near-neighborhood size for HAC 13-36 for different similarity thresh-\nolds for beam size B = 30, 000. The transformer model based on count fingerprints\ntrained with λ = 10 always generate larger near-neighborhoods than the modell\ntrained with λ = 0.\nTo understand how close this is to a truly exhaustive sampling of the local\nchemical space, we created the GDB-12 database, which was extracted from the\nGDB-13 database. [34] The GDB-12 database enumerates all possible compounds\nbased on a set of rules up to 12 heavy atoms. The heavy atoms used are C, N,\nO, S and Cl. This is, to our knowledge, the most complete lower bound of the\npossible density of organic chemical space available. For each compound in GDB-\n12, we computed the number of neighbors inside GDB-12 up to the same similarity\nthreshold, and grouped them by the HAC.\nWe observe an exponential relationship between the HAC and the near-neighborhood\nsize for GDB-12 (Figure 6). Although the vocabulary set used for creating GDB-12\nis smaller than the vocabulary set, V , used to train the transformer models, the\nbehaviour of trained transformer models follow the same trend as shown by GDB-12\nfor larger HAC, with an approximate linear slope of 0.10 and intercept of 0.55 for\nboth GDB-12 and TTD for HAC 13-36 (i.e. neighborhood size ≈ 100.10 HAC+0.55).\nThis suggests that, while our sampling remains a lower bound on the full local chem-\nical space, it is approximately as complete as the enumerated near-neighborhood for\nthe GDB-12 database. More approximations of the neighborhood size for different\nsimilarity thresholds can be found in Figure 11 in Appendix.\nWe also compared the these numbers to the number of near-neighbors that could\nbe retrieved by searching PubChem with the same similarity cutoff and source\ncompounds. On average two orders of magnitude fewer near-neighbour compounds\ncompared to the number of near-neighbours generated with the transformer models\nwere retrieved. This highlights that the near-neighborhood, even for therapeutic\nmolecules in the TTD database, is relatively unexplored.\nFinally, we investigated the overlap between the generated near-neighborhood\ntarget molecules with the transformers and the similar molecules identified in Pub-\nChem. This was done to assess if the trained transformer models identifies the same\nnear-neighbours as was identified in Pubchem. As expected the trained transformer\nmodels are indeed able to retrieve most of the near-neighbours with approximately\n98% average recovery. As will be discussed in Section 3.3, the missing near-neighbors\nfrom Pubchem were not identified because of two reasons. First, beam search is\nan approximation of an exhaustive search, meaning that there is no guarantees to\nfind all the compounds below a certain NLL. Second, our model does not provide a\nperfect correlation between NLL and similarity, meaning that the NLL between two\nsimilar source and target compounds can be high (low precedence) if the molecular\ntransformation from the source molecule to the target molecule is not well repre-\nsented in the training set. Incorporation of the ranking loss and count fingerprints\nboth improve recovery, from 96.98% to 98.15% for models trained on D with λ = 0\n13\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35\nHeavy atom count\n1.0\n3.0\n10.0\n30.0\n100.0\n300.0\n1000.0\n3000.0\n10000.0Neighborhood size\n= 10\n= 0\nPubChem\nGDB-12*\nFigure 6: Average neighborhood size (y-axis in log scale), defined by a similarity\nthreshold of 0.8, to a source compound grouped by their number of heavy atoms (x-\naxis). The Tanimoto similarity was evaluated on ECFP4 fingerprints with counts.\nThe method trained with λ = 10 always outperforms the method with λ = 0.\nThe filling color surrounding solid lines represent the variance. Due to computa-\ntional complexity, the similarity on GDB-12* was computed on fingerprints without\ncounts. Also included in the figure is the size of the near-neighborhood retrieved\nfrom PubChem for each source compound. For HAC between 13 and 18, the neigh-\nborhood size was plotted explicitly since only a few source compounds were available\nin the TTD database, whereas for a HAC greater or equal than 19 the average and\nstandard deviation are depicted.\nand λ = 10, and from 97.19% to 98.38% for models trained on Dc.\n0.70 0.75 0.80 0.85 0.90\nSimilarity threshold\n0\n5000\n10000\n15000\n20000\n25000Neighborhood size\n13\n17\n22\n26\n31\n36\nHeavy atom count\nFigure 7: Neighborhood size in function of the similarity threshold for HAC 13-36\nand beam size B = 30, 000.\n14\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n3.3 Validation for novel compound series\nIt is also of interest to validate the novel transformer model for retrieving similar\ncompounds in a series of interest to a medicinal chemist. Therefore 200 chemical se-\nries from the recent literature were extracted from the ChEMBL database. The 200\nseries were used to evaluate if the trained transformer model can efficiently retrieve\nnear-neighbours of interest. This was done through retrospectively investigating if\nknown near-neighbours could be identified within the 200 chemical series. All the\ncompounds within the 200 series are different from the training set, having been\npublished after our training set was created. As an illustration, Figure 8 depicts a\nchemical series extracted from the ChEMBL database consisting of five compounds.\nHere, the series is represented as a graph where nodes are the compounds in the\nseries and edges depict the NLL for the transformation of the source molecule to\nthe target molecule and the Tanimoto similarity (calculated with ECFP4 count fin-\ngerprints) for the compound pairs. Note, that the Tanimoto similarity is symmetric\nbut the NLL is not, meaning that in general p(t|s) ̸= p(s|t), where s and t are the\nsource molecule and target molecule, respectively. The transformer used in this\nexperiment is trained with a regularization term and the ECFP4 count fingerprint.\nAs expected, the NLL is strongly correlated with similarity and the NLL is lower for\nsimilar compounds and higher for dissimilar compounds. The difference between\np(t|s) and p(s|t) is small when the compounds are similar. The lower-left plot in Fig-\nure 8 shows the correlation between the molecular similarity and the NLL evaluated\nfor all pairs in all of the 200 extracted chemical series. The correlation coefficient\nis 0.88, confirming the efficacy of the proposed ranking loss used in the training of\nthe transformer. The similarity between compounds in the chemical series can be\nas low as 0 .2, since the series assignment is solely based on that the compounds\nwere extracted from the same publication. We observed markedly-poorer correla-\ntion between the NLL and the similarity for compound pairs with low similarity.\nThe NLL decreases more rapidly for compound pairs with low similarity (deviations\nfrom the linear fit in the Figure 8). This is expected since our training of the trans-\nformer only includes source-target molecular transformations where the similarity\nis above ≥ 0.50, i.e. the model has not been trained on molecular transformations\nof pairs with a similarity below < 0.50 and accordingly these source-target molec-\nular transformations have a low probability to be generated. The lower-right plot\nin Figure 8 depicts the maximum NLL as a function of the beam size. Here, the\nNLL consistently increases as the beam size increases, allowing an exploration of\nan increasingly dissimilar chemical space.\nWe repeated the analysis for all of the 200 compound series, for each series we\nconsidered in-turn all of the compounds as source molecules. Table 2 shows the\nresults for beam sizes equal to 1,000, 5,000, and 10,000. To understand the results\nin Table 2 we need to introduce two cutoff thresholds, tnll and tsim, calculated for\neach series and each source compounds in the series. tnll is defined as the highest\nNLL associated to a source compound found by beam search, while tsim is defined\nas the similarity between the source compound and the target compound with the\nhighest NLL found by the beam search. The choice of tsim is reasonable as we report\naveraged results and therefore potential error would be cancelled out, as confirmed\n15\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nNLL=27.97 \nNLL=20.36\nSim=0.42\nNLL=28.46 \nNLL=19.77\nSim=0.41\nNLL=35.98 \nNLL=22.45\nSim=0.38NLL=36.42 \nNLL=23.20\nSim=0.37\nNLL=5.86 \nNLL=6.07\nSim=0.81\nNLL=13.26 \nNLL=9.69\nSim=0.71NLL=12.97 \nNLL=9.15\nSim=0.73\nNLL=14.17 \nNLL=9.92\nSim=0.69\nNLL=15.01 \nNLL=10.10\nSim=0.69\nNLL=6.56 \nNLL=6.58\nSim=0.80\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nT animoto Similarity\n0\n10\n20\n30\n40\n50\n60 NLL\nCorrelation=0.88\n0 1000 2000 3000 4000 5000\nBeam size\n6\n7\n8\n9\n10\n11\n12\n13\nSearch space\nFigure 8: To the top one of the series from ChEMBL consisting of five compounds\nand displayed as a graph, where the nodes depict molecules and the edges depict the\nNLL of the molecular transformation and the Tanimoto Similarity (Sim in the plot)\nbetween the two connected nodes (molecules). Note that the NLL is not symmetric\nbetween a compound pair. The arrows on top of NLL denotes the direction of the\nmolecular transformation from the source molecule. If s and t are connected then− − − →NLL denotes −log p(t|s) and ← − − −NLL denotes −log p(s|t). The lower-left plot depicts\nthe correlation between Tanimoto similarity (evaluated on fingerprints with counts)\nand the NLL for the molecular transformation for all the possible pairs contained\nin the 200 compound series. The red dots denotes all the pairs of the above series.\nThe lower-right plot illustrates the maximum NLL reachable with different beam\nsizes. The filling color surrounding solid lines represent the variance.\nby the low standard deviations reported in Table 2. We define as true positive\n(TP) the percentage of target molecules found by beam search, with true negative\n(TN) the percentage of target molecules not found by beam search where their\nsimilarity to the source molecule is lower than tsim, and with false negative (FN)\nthe percentage of remaining target molecules not found by beam search but should\nhave been found in an ideal scenario. There are two types of FN: either due to beam\nsearch being an approximation of an exhaustive search, i.e. target compounds that\nhave Tanimoto similarity greater than tsim and NLL lower than tnll (column FN-B\nin Table 2), or due to precedence i.e. target compounds that have a Tanimoto\nsimilarity greater than tsim and NLL greater than tnll (column FN-P in Table 2).\nIn the latter case the transformation between the source and target molecule is\nnot well precedented in the training set and accordingly the NLL for the molecular\ntransformation will be high. Table 2 reports mean and median, denoted with ¯ x\n16\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nand ˜x, respectively for TP, FN, and TN. The majority of the target compounds are\nretrieved correctly (TP + TN), while the FN are due to the limitations with beam\nsearch (slightly more) and low precedence (slightly less) for the transformation\nbetween the source and target molecule. We additionally calculated the rate of\nFN identified for beam size 1,000 shifting to TP when the beam size increased\nfrom 1,000 to 5,000, having a decrease from 4,69% to 1,82% for FN due to the\nlimitations of beam search, and from 3,41% to 3,14% for FN due to low precedence\nof the molecular transformation. Thus it is possible through increasing the beam\nsize to compensate for that the beam search is only an approximation of exhaustive\nsearch. We also computed the same metrics for a beam size of 10,000. A further\ndecline in FN due to the limitations with beam search was observed to 1.55%. FN\ndue to limited precedence of the molecular transformation decreased to 2.94% with\n10,000 beam size. The percentages of FN in Table 2 for a beam size of 1,000, 5,000\nand 10,000 show a decreasing trend with increased beam size. Thus it is shown that\nmost near neighbours to a compound in a compound series can be retrieved with a\ntransformer regularized with a similarity constraint during training.\nNLL = 10.07\nSim = 0.69  \nSource\nBS = 1000BS = 5000\nNLL = 5.29\nSim = 0.86  \nNLL = 13.76\nSim = 0.61  \nNLL = 17.23\nSim = 0.71  \nFigure 9: A chemical series with a source molecule and the retrieved target molecules\n(with green borders) with beam search for two different beam sizes (BS). The red\nvertical line represents the maximum NLL reachable by the two beam search sizes\nwhich is 11.90 and 12.63 for BS=1,000 and BS=5,000, respectively. The compounds\nto the right of the red vertical line can not be found with these beam sizes due to\nthe high NLL for the molecular transformation. The right-most compound has a\nsimilarity of 0.71 to the source compound but low precedence as the NLL is equal\nto 17.23. The low precedence for the molecular transformation might due to the azo\ngroup which is a relatively uncommon chemical substructure in the training set.\nFigure 9 highlights some limitations of the proposed method. A compound se-\nries with five compounds is used to illustrate the limitations. A compound was\nselected as source (the left-most in Figure 9) and the remaining are the target com-\npounds. Beam search with two different beam sizes, 1,000 and 5,000, was used and\nit was checked which of the target compounds was retrieved by the beam searches.\nThe maximum NLL reachable by beam search was 11.90 and 12.63 for beam sizes\nof 1,000, and 5,000, respectively. This means that the compounds (represented to\nthe right of the vertical red line in Figure 9) for which the NLL is above these\nnumbers cannot be retrieved as a larger beam size would be required. There are\ntwo reasons why a specific target compound is not found. The first is due to the\nbeam search algorithm being a trade-off between a greedy and an exhaustive search\n17\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nTable 2: The table gives an overview of how well near neighbours in a chemical series\nare retrieved with a transformer. True positive (TP - the higher the better), false\nnegative (FN - the lower the better), and true negative (TN) for beam sizes (BS)\nequal to 1,000, 5,000, and 10,000 are shown. Columns FN-B and FN-P, represents\nthe two types of false negatives. Either due to beam search being an approximation\nof exhaustive search or low precedence due the molecular transformation is not well\nrepresented in the training set, respectively. ¯tnll and ¯tsim represent the average tnll\nand tsim with standard deviations, respectively. ¯x and ˜x represent mean and median\nexpressed in percentages, respectively, over the 200 chemical series.\nBS ¯tnll ¯tsim\nTP FN TNFN-B FN-P\n¯x ˜x ¯x ˜x ¯x ˜x ¯x ˜x\n1,000 11.03 ±1.35 0.71±0.05 29.48 23.33 4.69 2.94 3.41 0.83 62.42 68.10\n5,000 12.49 ±1.64 0.68±0.06 40.31 35.56 3.77 1.79 3.64 0.95 52.28 56.25\n10,000 13.22±1.95 0.66±0.06 45.33 40.00 3.48 1.69 3.29 1.14 47.90 50.00\ncontrolled by the beam size. We notice that for beam size equals to 1,000 the sec-\nond compound in the series is not found even though its NLL is lower than 11.90.\nThis happens because there is no guarantee that beam search will always find all\nthe compounds below the maximum reachable NLL. The second reason is due to\nprecedence as our method does not provide a perfect correlation between NLL and\nsimilarity. A high NLL i.e. low precedence can occur for similar molecules if the\nmolecular transformation from the source molecule to the target molecule is not well\nrepresented in the training set. This might occur for instance for molecules con-\ntaining unusual functional groups. Figure 9 shows that the right-most compounds\nhas higher similarity than other two but its NLL is higher, meaning that has lower\nprecedence. The low precedence for the right-most compound might be due to the\nazo substructure. Unfortunately, determining which tokens are responsible for an\nincreased NLL in relation to a target compound is a complex task. This complexity\narises from the fact that the NLL is influenced by both the source molecule and the\ngenerated target tokens. It is possible that the NLL is spread across all the target\ntokens, or alternatively, it may remain low until a specific point and then sharply\ndrop with the introduction of the next token in the target compound. This is a\ngeneral limitation of transformer based architectures.\n4 Conclusions\nIn this paper we have introduced a novel strategy for training a source-target molec-\nular transformer that explicitly links the NLL for molecular transformation of a\nsource molecule to a target molecule with a similarity metric. The method has\nbeen applied in the context of molecular optimization. A transformer model have\nbeen trained on, to the best of our knowledge, the largest data set of molecular\npairs so far.\n18\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nThe resulting model exhibit the intended relationship between the molecular\nsimilarity and the corresponding NLL, with a strong correlation across similarity\nranges from 0.50 to 1.0 when evaluated for drug-like molecules, including molecules\nabsent from the extensive training set. We could demonstrate a clear benefit in\nterms of the correlation between molecular similarity and NLL when applying our\nnovel regularization method. The limitations of using a binary finger print vs a\ncount fingerprint for the transformer model have also been described.\nA model that exhibits this property can be used in novel applications for in-\nstance to estimate the density of the near-neighborhood chemical space. This was\npreviously not possible. In particular, we could demonstrate an approximately ex-\nhaustive enumeration of the local, precedented chemical space for a molecule of\ninterest. We showed that neighborhood sizes computed using this method scale in\na similar way as the GDB-12 database.\nWe also investigated how many of the similar compounds we could retrieve in\na chemical series extracted from the litterature. With a beam search of 10,000, it\nwas found that approximately 87% of the similar compounds could be retrieved.\nReasons that a similar compound couldn’t be retrieved is due to that beam search is\nan approximation of an exhaustive search or that the transformation from a source\ncompound to a target compound have low precedence, i.e. the transformation is\nnow well represented in the training set. This might happen for uncommon chemical\nsubstructures. Low precedence have been discussed in the context of modifying an\natom in penicillin.\nWe believe that a molecular transformer model trained with a regularization\nterm for the molecular similarity provide a completely novel way to a address the\nclassic question of how many near neighbors a given molecule has. The transformer\nmodel therefore hold great promise as a tool for local molecular optimization and\nefficient local chemical search in a more exhaustive and controllable manner than\npreviously possible.\nDeveloped software, trained transformer models, and curated data sets will be\npublicly released upon acceptance of the manuscript.\n5 Acknowledgement\nAll the authors express their gratitude to the Molecular AI department at As-\ntraZeneca Gothenburg for their valuable and insightful discussions pertaining to\nthe paper.\n6 Author contributions\nContributions A.T. contributed to the main part of the research and conducted the\nexperiments the research. A.T., J.H., J.P.J, E.N., and O.E. designed the experi-\nments and provided feedback results. A.T. wrote the manuscript with the support\nand feedback of all the authors.\n19\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nAppendix\nCCc1cccc1c\nc1cccc1c ^ c 1 c c c c 1 c $ * *\n^ C C c 1 c c c c 1 c $\n1, 450, 48, 450, 450, 450, 450, 48, 450, 2, 0, 0\n1, 60, 60, 450, 48, 450, 450, 450, 450, 48, 450, 2\nFigure 10: Example of tokenizations where SMILES strings are divided into tokens\nand transformed to integers. Start and end tokens are added to the SMILES along\nwith padding tokens to make the two sequences having equal length.\n15 20 25 30 35\n31\n100\n316\n1000\n3162\n10000\n31622Neighborhood size\nSimilarity threshold = 0.70\n100.05 HAC + 2.77\n15 20 25 30 35\n31\n100\n316\n1000\n3162\n10000\n31622\nSimilarity threshold = 0.75\n100.08 HAC + 1.74\n15 20 25 30 35\nHeavy atom count\n31\n100\n316\n1000\n3162\n10000\n31622\nSimilarity threshold = 0.80\n100.10 HAC + 0.55\n15 20 25 30 35\nHeavy atom count\n31\n100\n316\n1000\n3162\n10000\n31622Neighborhood size\nSimilarity threshold = 0.85\n100.10 HAC + 0.01\n15 20 25 30 35\nHeavy atom count\n31\n100\n316\n1000\n3162\n10000\n31622\nSimilarity threshold = 0.90\n100.07 HAC + 0.47\nFigure 11: Estimation of the near-neighborhood size N as a function of the heavy\natom count for different similarity thresholds and a beam size of 30 , 000. In each\nplot, the mean near-neighborhood size is depicted with a solid blue line, while its\nestimation N in dashed green line. The mean near-neighborhood size is calculated\nby averaging all the near-neighborhood sizes for a specific HAC. Note that the near-\nneighborhood size is defined as the number of compounds generated by beam search\nwith a beam size of 30 , 000 for which their similarity is greater or equal than the\nsimilarity threshold.\nReferences\n[1] Gerald Maggiora, Martin Vogt, Dagmar Stumpfe, and J¨ urgen Bajorath. Molec-\nular similarity in medicinal chemistry. Journal of Medicinal Chemistry, 57(8):\n3186–3204, 2014. doi: 10.1021/jm401411z.\n[2] Pavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of\nthe size of drug-like chemical space based on gdb-17 data.Journal of computer-\naided molecular design, 27:675–679, 2013.\n20\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n[3] Kyle D Konze, Pieter H Bos, Markus K Dahlgren, Karl Leswing, Ivan Tubert-\nBrohman, Andrea Bortolato, Braxton Robbason, Robert Abel, and Sathesh\nBhat. Reaction-based enumeration, active learning, and free energy calcula-\ntions to rapidly explore synthetically tractable chemical space and optimize\npotency of cyclin-dependent kinase 2 inhibitors. Journal of chemical informa-\ntion and modeling, 59(9):3782–3793, 2019.\n[4] Phani Ghanakota, Pieter H Bos, Kyle D Konze, Joshua Staker, Gabriel Mar-\nques, Kyle Marshall, Karl Leswing, Robert Abel, and Sathesh Bhat. Com-\nbining cloud-based free-energy calculations, synthetically aware enumerations,\nand goal-directed generative machine learning for rapid large-scale chemical\nexploration and optimization. Journal of Chemical Information and Modeling,\n60(9):4311–4325, 2020.\n[5] Andrew Dalke, Jerome Hert, and Christian Kramer. mmpdb: An open-source\nmatched molecular pair platform for large multiproperty data sets. Journal of\nchemical information and modeling, 58(5):902–910, 2018.\n[6] Connor W. Coley, Natalie S. Eyke, and Klavs F. Jensen. Autonomous discov-\nery in the chemical sciences part i: Progress. Angewandte Chemie Interna-\ntional Edition, 59(51):22858–22893, 2020. doi: https://doi.org/10.1002/anie.\n201909987.\n[7] O. Anatole von Lilienfeld and Kieron Burke. Retrospective on a decade of\nmachine learning for chemical discovery. Nature Communications, 11(1):4895,\nSep 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-18556-9. URL https:\n//doi.org/10.1038/s41467-020-18556-9.\n[8] Lu Zhang, Jianjun Tan, Dan Han, and Hao Zhu. From machine learning to\ndeep learning: progress in machine intelligence for rational drug discovery.\nDrug Discovery Today, 22(11):1680–1685, 2017. ISSN 1359-6446. doi: https:\n//doi.org/10.1016/j.drudis.2017.08.010.\n[9] Jessica Vamathevan, Dominic Clark, Paul Czodrowski, Ian Dunham, Edgardo\nFerran, George Lee, Bin Li, Anant Madabhushi, Parantu Shah, Michaela\nSpitzer, and Shanrong Zhao. Applications of machine learning in drug dis-\ncovery and development. Nature Reviews Drug Discovery, 18(6):463–477, Jun\n2019. ISSN 1474-1784. doi: 10.1038/s41573-019-0024-5.\n[10] Jon Paul Janet, Lewis Mervin, and Ola Engkvist. Artificial intelligence in\nmolecular de novo design: Integration with experiment. Current Opinion in\nStructural Biology, 80:102575, 2023. ISSN 0959-440X. doi: https://doi.org/10.\n1016/j.sbi.2023.102575.\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nAdvances in neural information processing systems, 30, 2017.\n21\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\npre-training of deep bidirectional transformers for language understanding.\nProceedings of the 2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies, 2019.\n[13] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrah-\nman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART:\ndenoising sequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension. Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 2020.\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. International Conference on Learning\nRepresentations, 2021.\n[15] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and Herv´ e J´ egou. Training data-efficient image transformers &\ndistillation through attention. International conference on machine learning,\n2021.\n[16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from natural language supervision.\nInternational conference on machine learning, 2021.\n[17] Francesca Grisoni, Michael Moret, Robin Lingwood, and Gisbert Schneider.\nBidirectional molecule generation with recurrent neural networks. Journal of\nchemical information and modeling, 60(3):1175–1183, 2020.\n[18] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller.\nGenerating focused molecule libraries for drug discovery with recurrent neural\nnetworks. ACS central science, 4(1):120–131, 2018.\n[19] Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for\nsmall molecular graphs. arXiv preprint arXiv:1805.11973, 2018.\n[20] Rafael G´ omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos´ e Miguel\nHern´ andez-Lobato, Benjam´ ın S´ anchez-Lengeling, Dennis Sheberla, Jorge\nAguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al´ an Aspuru-\nGuzik. Automatic chemical design using a data-driven continuous representa-\ntion of molecules. ACS central science, 4(2):268–276, 2018.\n[21] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational\nautoencoder for molecular graph generation. In International conference on\nmachine learning, pages 2323–2332. PMLR, 2018.\n22\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n[22] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen.\nMolecular de-novo design through deep reinforcement learning. Journal of\ncheminformatics, 9(1):1–14, 2017.\n[23] Thomas Blaschke, Josep Ar´ us-Pous, Hongming Chen, Christian Margreitter,\nChristian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov.\nReinvent 2.0: an ai tool for de novo drug design. Journal of chemical informa-\ntion and modeling, 60(12):5918–5922, 2020.\n[24] Jiazhen He, Huifang You, Emil Sandstr¨ om, Eva Nittinger, Esben Jannik Bjer-\nrum, Christian Tyrchan, Werngard Czechtizky, and Ola Engkvist. Molecu-\nlar optimization by capturing chemist’s intuition using deep neural networks.\nJournal of cheminformatics, 13(1):1–17, 2021.\n[25] Jiazhen He, Eva Nittinger, Christian Tyrchan, Werngard Czechtizky, Atanas\nPatronov, Esben Jannik Bjerrum, and Ola Engkvist. Transformer-based molec-\nular optimization beyond matched molecular pairs. Journal of cheminformat-\nics, 14(1):18, 2022.\n[26] ChEMBL. Chembl database version 32. http://doi.org/10.6019/CHEMBL.\ndatabase.32, 2023.\n[27] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He,\nQingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem\n2023 update. Nucleic Acids Research, 51(D1):D1373–D1380, 2023.\n[28] Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and\nBharath Ramsundar. ChemBERTa-2: Towards Chemical Foundation Mod-\nels. arXiv e-prints, art. arXiv:2209.01712, September 2022. doi: 10.48550/\narXiv.2209.01712.\n[29] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molec-\nular contrastive learning of representations via graph neural networks. Na-\nture Machine Intelligence, 4(3):279–287, Mar 2022. ISSN 2522-5839. doi:\n10.1038/s42256-022-00447-x.\n[30] Andrew Viterbi. Error bounds for convolutional codes and an asymptotically\noptimum decoding algorithm. IEEE transactions on Information Theory, 13\n(2):260–269, 1967.\n[31] Greg Landrum et al. RDKit: Open-source cheminformatics software. version\n2022.09.5. Journal of Cheminformatics, 8(1):33, 2016.\n[32] Ying Zhou, Yintao Zhang, Xichen Lian, Fengcheng Li, Chaoxin Wang, Feng\nZhu, Yunqing Qiu, and Yuzong Chen. Therapeutic target database update\n2022: facilitating drug discovery with enriched comparative data of targeted\nagents. Nucleic Acids Research, 50(D1):D1398–D1407, 2022.\n23\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n[33] Christopher A Lipinski, Franco Lombardo, Beryl W Dominy, and Paul J\nFeeney. Experimental and computational approaches to estimate solubility\nand permeability in drug discovery and development settings. Advanced drug\ndelivery reviews, 64:4–17, 2012.\n[34] Lorenz C Blum and Jean-Louis Reymond. 970 million druglike small molecules\nfor virtual screening in the chemical universe database gdb-13. Journal of the\nAmerican Chemical Society, 131(25):8732–8733, 2009.\n24\nhttps://doi.org/10.26434/chemrxiv-2023-v25xb ORCID: https://orcid.org/0000-0002-9070-740X Content not peer-reviewed by ChemRxiv. License: CC BY 4.0",
  "topic": "Chemical space",
  "concepts": [
    {
      "name": "Chemical space",
      "score": 0.7679307460784912
    },
    {
      "name": "Generative model",
      "score": 0.6975165605545044
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6168702244758606
    },
    {
      "name": "Computer science",
      "score": 0.5882473587989807
    },
    {
      "name": "Generative grammar",
      "score": 0.48499009013175964
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4502425193786621
    },
    {
      "name": "Kernel (algebra)",
      "score": 0.43220055103302
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4084767699241638
    },
    {
      "name": "Machine learning",
      "score": 0.3500095009803772
    },
    {
      "name": "Data mining",
      "score": 0.3298588991165161
    },
    {
      "name": "Mathematics",
      "score": 0.2570774555206299
    },
    {
      "name": "Drug discovery",
      "score": 0.20208248496055603
    },
    {
      "name": "Chemistry",
      "score": 0.14671674370765686
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0799809992313385
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210143795",
      "name": "AstraZeneca (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I66862912",
      "name": "Chalmers University of Technology",
      "country": "SE"
    }
  ],
  "cited_by": 11
}