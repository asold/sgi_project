{
  "title": "BERT Masked Language Modeling for Co-reference Resolution",
  "url": "https://openalex.org/W2972677162",
  "year": 2019,
  "authors": [
    {
      "id": null,
      "name": "Alfaro, Felipe",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ruiz Costa-Jussà, Marta",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Rodríguez Fonollosa, José Adrián",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2093664665",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2920114910",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "This paper explains the TALP-UPC participation for the Gendered Pronoun Resolution shared-task of the 1st ACL Workshop on Gender Bias for Natural Language Processing. We have implemented two models for mask language modeling using pre-trained BERT adjusted to work for a classification problem. The proposed solutions are based on the word probabilities of the original BERT model, but using common English names to replace the original test names",
  "full_text": "Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 76–81\nFlorence, Italy, August 2, 2019.c⃝2019 Association for Computational Linguistics\n76\nBERT Masked Language Modeling for Co-reference Resolution\nFelipe Alfaro Lois Jos ´e A. R. Fonollosa Marta R. Costa-juss `a\nTALP Research Center\nUniversitat Polit`ecnica de Catalunya, Barcelona\nfelipe.alfaro@est.fib.upc.edu\njose.fonollosa,marta.ruiz}@upc.edu\nAbstract\nThis paper explains the TALP-UPC partici-\npation for the Gendered Pronoun Resolution\nshared-task of the 1st ACL Workshop on Gen-\nder Bias for Natural Language Processing. We\nhave implemented two models for mask lan-\nguage modeling using pre-trained BERT ad-\njusted to work for a classiﬁcation problem.\nThe proposed solutions are based on the word\nprobabilities of the original BERT model, but\nusing common English names to replace the\noriginal test names.\n1 Introduction\nThe Gendered Pronoun Resolution task is a nat-\nural language processing task whose objective is\nto build pronoun resolution systems that identify\nthe correct name a pronoun refers to. It’s called\na co-reference resolution task. Co-reference res-\nolution tackles the problem of different elements\nof a text that refer to the same thing. Like for ex-\nample a pronoun and a noun, or multiple nouns\nthat describe the same entity. There are multiple\ndeep learning approaches to this problem. Neu-\nralCoref 1 presents one based on giving every pair\nof mentions (pronoun + noun) a score to represent\nwhether or not they refer to the same entity. In our\ncurrent task, this approach is not possible, because\nwe don’t have the true information of every pair of\nmentions, only the two names per entry.\nThe current task also has to deal with the prob-\nlem of gender. As the GAP researchers point\nout (Webster et al., 2018), the biggest and most\ncommon datasets for co-reference resolution have\na bias towards male entities. For example the\nOntoNotes dataset, which is used for some of the\nmost popular models, only has a 25% female rep-\nresentation (Pradhan and Xue, 2009). This creates\n1https://medium.com/huggingface/state-of-the-art-\nneural-coreference-resolution-for-chatbots-3302365dcf30\na problem, because any machine learning model\nis only as good as its training set. Biased training\nsets will create biased models, and this will have\nrepercussions on any uses the model may have.\nThis task provides an interesting challenge spe-\ncially by the fact that it is proposed over a gender\nneutral dataset. In this sense, the challenge is ori-\nented towards proposing methods that are gender-\nneutral and to not provide bias given that the data\nset does not have it.\nTo face this task, we propose to make use of the\nrecent popular BERT tool (Devlin et al., 2018).\nBERT is a model trained for masked language\nmodeling (LM) word prediction and sentence pre-\ndiction using the transformer network (Vaswani\net al., 2017). BERT also provides a group of pre-\ntrained models for different uses, of different lan-\nguages and sizes. There are implementations for\nit in all sorts of tasks, including text classiﬁcation,\nquestion answering, multiple choice question an-\nswering, sentence tagging, among others. BERT\nis gaining popularity quickly in language tasks,\nbut before this shared-task appeared, we had no\nawareness of its implementation in co-reference\nresolution. For this task, we’ve used an imple-\nmentation that takes advantage of the masked LM\nwhich BERT is trained for and uses it for a kind of\ntask BERT is not speciﬁcally designed for.\nIn this paper, we are detailing our shared-task\nparticipation, which basically includes descrip-\ntions on the use we gave to the BERT model and\non our technique of ’Name Replacement’ that al-\nlowed to reduce the impact of name frequency.\n2 Co-reference Resolution System\nDescription\n2.1 BERT for Masked LM\nThis model’s main objective is to predict a word\nthat has been masked in a sentence. For this exer-\n77\ncise that word is the pronoun whose referent we’re\ntrying to identify. This one pronoun gets replaced\nby the [MASKED] tag, the rest of the sentence is\nsubjected to the different name change rules de-\nscribed in section 2.2.\nThe text is passed through the pre-trained BERT\nmodel. This model keeps all of its weights intact,\nthe only changes made in training are to the net-\nwork outside of the BERT model. The resulting\nsequence then passes through what is called the\nmasked language modeling head. This consists of\na small neural network that returns, for every word\nin the sequence, an array the size of the entire\nvocabulary with the probability for every word.\nThe array for our masked pronoun is extracted and\nthen from that array, we get the probabilities of\nthree different words. These three words are : the\nﬁrst replaced name (name 1), the second replaced\nname (name 2) and the word none for the case of\nhaving none.\nThis third case is the strangest one, because the\nword none would logically not appear in the sen-\ntence. Tests were made with the original pronoun\nas the third option instead. But the results ended\nup being very similar albeit slightly worse, so the\nword none was kept instead. These cases where\nthere is no true answer are the hardest ones for\nboth of the models.\nWe experimented with two models.\nModel 1 After the probabilities for each word\nare extracted, the rest is treated as a classiﬁcation\nproblem. An array is created with the probabili-\nties of the 2 names and none ([name 1, name 2,\nnone]), where each one represents the probability\nof a class in multi-class classiﬁcation. This array\nis passed through a softmax function to adjust it to\nprobabilities between 0 and 1 and then the log loss\nis calculated. A block diagram of this model can\nbe seen in ﬁgure 1.\nModel 2 This model repeats the steps of model 1\nbut for two different texts. These texts are mostly\nthe same except the replacement names name 1\nand name 2 have been switched (as explained in\nthe section 2.2). It calculates the probabilities for\neach word for each text and then takes an aver-\nage of both. Then ﬁnally applies the softmax and\ncalculates the loss with the average probability of\neach class across both texts. A block diagram of\nthis model can be seen in ﬁgure 2.\n2.2 Name Replacement\nThe task contains names of individuals who are\nfeatured in Wikipedia, and some of these names\nare uncommon in the English language. As part of\nthe pre-processing for both models, these names\nare replaced. They are replaced with common En-\nglish names in their respective genders 2. If the\npronoun is female, one of two common English\nfemale names are chosen, same thing for the male\npronouns. In order to replace them in the text, the\nfollowing set of rules are followed.\n1. The names mentioned on the A and B\ncolumns are replaced.\n2. Any other instances of the full name as it ap-\npears on the A/B columns are replaced.\n3. If the name on the A/B column contains a ﬁrst\nname and a last name. Instances of the ﬁrst\nname are also replaced. Unless both entities\nshare a ﬁrst name, or the ﬁrst name of one is\ncontained within the other.\n4. Both the name and the text are converted to\nlowercase\nThis name replacement has two major beneﬁts.\nFirst, the more common male and female names\nwork better with BERT because they appear more\nin the corpus in which it is trained on. Secondly,\nwhen the word piece encoding splits certain words\nthe tokenizer can be conﬁgured so that our chosen\nnames are never split. So they are single tokens\n(and not multiple word pieces), which helps the\nway the model is implemented.\nBoth models (1 and 2 presented in the above\nsection) use BERT for Masked LM prediction\nwhere the mask always covers a pronoun, and\nbecause the pronoun is a single token (not split\ninto word pieces), it’s more useful to compare the\nmasked pronoun to both names, which are also\nboth single tokens (not multiple word pieces).\nBecause the chosen names are very common\nin the English language, BERT’s previous train-\ning might contain biases towards one name or the\nother. This can be detrimental to this model where\nit has to compare between only 3 options. So the\nalternative is the approach in model number 2. In\nmodel 2 two texts are created. Both texts are ba-\nsically the same except the names chosen as the\n2https://www.ef.com/wwen/english-resources/english-\nnames/\n78\nFigure 1: Model 1 representation.\nFigure 2: Model 2 representation.\nFigure 3: Example of a text present in the dataset and\nhow the word replacement was done for the model 2.\nreplacement names 1 and 2 are switched. So, as\nﬁgure 3 shows, we get one text with each name in\neach position.\nFor example lets say we get the text:\n”In the late 1980s Jones began working with\nDuran Duran on their live shows and then in the\nstudio producing a B side single “This Is How A\nRoad Gets Made”, before being hired to record the\nalbum Liberty with producer Chris Kimsey. ”,\nA is Jones and B is Chris Kimsey. For the name\nreplacement lets say we choose two common En-\nglish names like John and Harry. The new text\nproduced for model 1 (ﬁgure 1) would be some-\nthing like:\n”in the late 1980sharry began working with du-\nran duran on their live shows and then in the stu-\ndio producing a b side single “this is how a road\ngets made”, before being hired to record the album\nliberty with producerjohn. ”\nAnd for model 2 (ﬁgure 2) the same text would\nbe used for the top side and for the bottom side\nit would have the harry and john in the opposite\npositions.\n79\n3 Experimental Framework\n3.1 Task details\nThe objective of the task is that of a classiﬁca-\ntion problem. Where the output for every entry\nis the probability of the pronoun referencing name\nA, name B or Neither.\n3.2 Data\nThe GAP dataset (Webster et al., 2018) created by\nGoogle AI Language was the dataset used for this\ntask. This dataset consists of 8908 co-reference la-\nbeled pairs sampled from Wikipedia, also it’s split\nperfectly between male and female representation.\nEach entry of the dataset consists of a short text, a\npronoun that is present in the text and its offset and\ntwo different names (name A and name B) also\npresent in the text. The pronoun refers to one of\nthese two names and in some cases, none of them.\nThe GAP dataset doesn’t contain any neutral pro-\nnouns such as it or they.\nFor the two different stages of the competition dif-\nferent datasets were used.\n•For Stage 1 the data used for the submission\nis the same as the development set available\nin the GAP repository. The dataset used for\ntraining is the combination of the GAP vali-\ndation and GAP testing sets from the reposi-\ntory.\n•For Stage 2 the data used for submission was\nonly available through Kaggle 3 and the cor-\nrect labels have yet to be released, so we can\nonly analyze the ﬁnal log loss of each of the\nmodels. This testing set has a total of 12359\nrows, with 6499 male pronouns and 5860 fe-\nmale ones. For training, a combination of the\nGAP development, testing and validation sets\nwas used. And, as all the GAP data, it is\nevenly distributed between genders.\nThe distributions of all the datasets are shown in\ntable 1. It can be seen that in all cases, the None\noption has the least support by a large margin.\nThis, added to the fact that the model naturally is\nbetter suited to identifying names rather than the\nabsence of them, had a negative effect on the re-\nsults.\n3https://www.kaggle.com/c/gendered-pronoun-\nresolution/overview\nStage 1 Stage 2\nTrain Test Train\nName A 1105 874 1979\nName B 1060 925 1985\nNone 289 201 490\nTable 1: Dataset distribution for the datasets of stages\n1 and 2.\n3.3 Training details\nFor the BERT pre-trained weights, several models\nwere tested. BERT base is the one that produced\nthe best results. BERT large had great results\nin a lot of other implementations, but in this\nmodel it produced worse results while consuming\nmuch more resources and having a longer training\ntime. During the experiments the model had an\noverﬁtting problem, so the learning rate was tuned\nas well as a warm up percentage was introduced.\nAs table 2 shows, the optimal learning rate was\n3e −5 while the optimal with a 20% warm up.\nThe length of the sequences is set at 256, where it\nﬁts almost every text without issues. For texts too\nbig, the text is truncated depending on the offsets\nof each of the elements in order to not eliminate\nany of the names or the pronoun.\nAccuracy LossLearning RateWarmup mean min mean min0.00003 0.0 0.840167 0.83150.519565 0.4542530.2 0.844444 0.83400.502667 0.4423130.00004 0.0 0.822389 0.79700.556491 0.4735280.2 0.834000 0.79250.530862 0.4562230.00005 0.1 0.743500 0.74350.666750 0.6667500.00006 0.0 0.756333 0.70400.630707 0.5448410.2 0.802278 0.74650.587041 0.497051\nTable 2: Results of the tuning for both models. Min-\nimum and average Loss and Accuracy across all the\ntuning experiments performed.\nThe training was performed in a server with\nan Intel Dual Core processor and Nvidia Titan X\nGPUs, with approximately 32GB of memory. The\nrun time varies a lot depending on the model. The\naverage run time on the stage 1 dataset for model\n1 is from 1 to 2 hours while for model 2 it has a\nrun time of about 4 hours. For the training set for\nstage 2, the duration was 4 hours 37 minutes for\nmodel 1 and 8 hours 42 minutes for model 2. The\nﬁnal list of hyperparameters is in table 3.\n80\nParameter Value\nOptimizer Adam\nVocabulary Size 28996\nDropout 0.1\nSequence Length 256\nBatch Size 32\nLearning Rate 3e−5\nWarm Up 20%\nSteps Stage 1: 81 — Stage 2: 148\nEpochs 1\nGradient Accumulation Steps5\nTable 3: Hyperparameters for the model training\n4 Results\nTables 4 and 5 report results for models 1 and 2\nreported in section 2.1 for stage 1 of the compe-\ntition. Both models 1 and 2 have similar overall\nresults. Also both models show problems with the\nNone class, model 2 specially. We believe this is\nbecause our model is based on guessing the correct\nname, so the guessing of none is not as well suited\nto it. Also, the training set contains much less of\nthese examples, therefore making it even harder to\ntrain for them.\nPrecision Recall F1 Support\nA 0.83 0.87 0.85 874\nB 0.88 0.88 0.88 925\nNone 0.64 0.52 0.57 201\nAvg 0.83 0.84 0.84 2000\nTable 4: Model 1 results for the testing stage 1.\nPrecision Recall F1 Support\nA 0.81 0.86 0.83 874\nB 0.88 0.78 0.82 925\nNone 0.48 0.62 0.54 201\nAvg 0.81 0.80 0.80 2000\nTable 5: Model 2 results for the testing stage 1.\n4.1 Advantages of the Masked LM Model\nAs well as the Masked LM, other BERT imple-\nmentations were experimented with for the task.\nFirst, a text multi class classiﬁcation model (ﬁgure\n4) where the [CLS] tag is placed at the beginning\nof every sentence, the text is passed through a pre-\ntrained BERT and then the result from this label is\npassed through a feed forward neural network.\nAnd a multiple choice question answering\nmodel (ﬁgure 5), where the same text with the\nFigure 4: Model: BERT for text classiﬁcation\nFigure 5: Model: BERT for multiple choice answering\n[CLS] label is passed through BERT with different\nanswers and then the result these labels is passed\nthrough a feed forward neural network.\nThese two models, which were speciﬁcally de-\nsigned for other tasks had similar accuracy to\nthe masked LM but suffered greatly with the log\nloss, which was the competition’s metric. This\nis because in a lot of examples the difference be-\ntween the probabilities of one class and another\nwas minimal. This made for a model where each\nchoice had low conﬁdence and therefore the loss\nincreased considerably.\nAccuracy Loss\nBERT for Classiﬁcation 0.8055 0.70488\nBERT for Question Answering0.785 0.6782\nBERT for Masked LM 0.838 0.44231\nTable 6: Results for the tests with different BERT im-\nplementations.\n4.2 Name Replacement Results\nAs table 2.2 shows, name replacement consid-\nerably improved the model’s results. This is in\npart because the names chosen as replacements are\nmore common in BERT’s training corpora. Also,\na 43% of the names across the whole GAP dataset\nare made up of multiple words. So replacing these\nwith a single name makes it easier for the model\nto identify their place in the text.\n81\nAccuracy Loss\nModel 1 Original Names 0.782 0.7021\nModel 1 Name Replacement 0.838 0.4423\nTable 7: Results for the models with and without name\nreplacement.\n4.3 Competition results\nIn the ofﬁcial competition on Kaggle we placed\n46th, with the second model having a loss around\n0.301. As the results in table 8 show, the results of\nstage 2 were better than those of stage 1. And the\nsecond model, which had performed worse on the\nﬁrst stage was better in stage 2.\nModel 1 Model 2\nStage 1 0.44231 0.49607\nStage 2 0.31441 0.30151\nTable 8: Results for both models across both stages of\nthe competition\n5 Conclusions\nWe have proved that pre-trained BERT is useful\nfor co-reference resolution. Additionally, we have\nshown that our simple ’Name Replacement’ tech-\nnique was effective to reduce the impact of name\nfrequency or popularity in the ﬁnal decision.\nThe main limitation of our technique is that it\nrequires knowing the gender from the names and\nso it only makes sense for entities which have a\ndeﬁned gender. Our proposed model had great\nresults when predicting the correct name but had\ntrouble with with the none option.\nAs a future improvement it’s important to an-\nalyze the characteristics of these examples where\nnone of the names are correct and how the model\ncould be trained better to identify them, specially\nbecause they are fewer in the dataset. Further im-\nprovements could be made in terms of ﬁne-tuning\nthe weights in the actual BERT model.\nAcknowledgements\nThis work is also supported in part by the Span-\nish Ministerio de Econom ´ıa y Competitividad,\nthe European Regional Development Fund and\nthe Agencia Estatal de Investigaci ´on, through the\npostdoctoral senior grant Ram ´on y Cajal, con-\ntract TEC2015-69266-P (MINECO/FEDER,EU)\nand contract PCIN-2017-079 (AEI/MINECO).\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSameer S. Pradhan and Nianwen Xue. 2009.\nOntoNotes: The 90% solution. In Proceedings\nof Human Language Technologies: The 2009\nAnnual Conference of the North American Chapter\nof the Association for Computational Linguis-\ntics, Companion Volume: Tutorial Abstracts ,\npages 11–12, Boulder, Colorado. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran As-\nsociates, Inc.\nKellie Webster, Marta Recasens, Vera Axelrod, and Ja-\nson Baldridge. 2018. Mind the GAP: A balanced\ncorpus of gendered ambiguous pronouns. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:605–617.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.823441743850708
    },
    {
      "name": "Natural language processing",
      "score": 0.7027583718299866
    },
    {
      "name": "Pronoun",
      "score": 0.6514546871185303
    },
    {
      "name": "Language model",
      "score": 0.6500206589698792
    },
    {
      "name": "Resolution (logic)",
      "score": 0.6486689448356628
    },
    {
      "name": "Task (project management)",
      "score": 0.6426646709442139
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5562955141067505
    },
    {
      "name": "Word (group theory)",
      "score": 0.5507534146308899
    },
    {
      "name": "Natural language",
      "score": 0.46915900707244873
    },
    {
      "name": "Speech recognition",
      "score": 0.32409584522247314
    },
    {
      "name": "Linguistics",
      "score": 0.3120681643486023
    },
    {
      "name": "Engineering",
      "score": 0.060563087463378906
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9617848",
      "name": "Universitat Politècnica de Catalunya",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210150896",
      "name": "National Student Clearinghouse Research Center",
      "country": "US"
    }
  ],
  "cited_by": 6
}