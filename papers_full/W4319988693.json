{
    "title": "Membership Inference Attacks With Token-Level Deduplication on Korean Language Models",
    "url": "https://openalex.org/W4319988693",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4308632398",
            "name": "Myung Gyo Oh",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2900865373",
            "name": "Leo-Hyun Park",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2191545978",
            "name": "Jaeuk Kim",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2111261120",
            "name": "Jae-Woo Park",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2112333632",
            "name": "Taekyoung Kwon",
            "affiliations": [
                "Yonsei University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6679434410",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3155584966",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6771915120",
        "https://openalex.org/W3035050380",
        "https://openalex.org/W6787335730",
        "https://openalex.org/W6810332117",
        "https://openalex.org/W6794353620",
        "https://openalex.org/W3165327186",
        "https://openalex.org/W3096738375",
        "https://openalex.org/W2887995258",
        "https://openalex.org/W4385573947",
        "https://openalex.org/W6810760854",
        "https://openalex.org/W3170672407",
        "https://openalex.org/W3115042282",
        "https://openalex.org/W6838995493",
        "https://openalex.org/W4281923881",
        "https://openalex.org/W6798182279",
        "https://openalex.org/W4229506649",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6803958908",
        "https://openalex.org/W6796453306",
        "https://openalex.org/W6840437216",
        "https://openalex.org/W2795435272",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W6781511523",
        "https://openalex.org/W2930926105",
        "https://openalex.org/W6761551260",
        "https://openalex.org/W3138815606",
        "https://openalex.org/W6635831535",
        "https://openalex.org/W6601894380",
        "https://openalex.org/W6638575559",
        "https://openalex.org/W6798205730",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W6768817161",
        "https://openalex.org/W6727690538",
        "https://openalex.org/W4283172211",
        "https://openalex.org/W6768028577",
        "https://openalex.org/W2963260202",
        "https://openalex.org/W6765055791",
        "https://openalex.org/W6763240421",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W6810463509",
        "https://openalex.org/W4308643663",
        "https://openalex.org/W4288057780",
        "https://openalex.org/W6839352123",
        "https://openalex.org/W2109426455",
        "https://openalex.org/W3154109599",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W6810027437",
        "https://openalex.org/W2884943453",
        "https://openalex.org/W2963378725",
        "https://openalex.org/W4308233967",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W6802444496",
        "https://openalex.org/W3158803559",
        "https://openalex.org/W2983140679",
        "https://openalex.org/W6776993083",
        "https://openalex.org/W3154155772",
        "https://openalex.org/W6791907524",
        "https://openalex.org/W6788811087",
        "https://openalex.org/W6802224358",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W4298877056",
        "https://openalex.org/W46679369",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W1600963723",
        "https://openalex.org/W3103245149"
    ],
    "abstract": "The confidentiality threat against training data has become a significant security problem in neural language models. Recent studies have shown that memorized training data can be extracted by injecting well-chosen prompts into generative language models. While these attacks have achieved remarkable success in the English-based Transformer architecture, it is unclear whether they are still effective in other language domains. This paper studies the effectiveness of attacks against Korean models and the potential for attack improvements that might be beneficial for future defense studies. The contribution of this study is two-fold. First, we perform a membership inference attack against the state-of-the-art Korean GPT model. We found approximate training data with 20&#x0025; to 90&#x0025; precision in the top-100 samples and confirmed that the proposed attack technique for naive GPT is valid across the language domains. Second, in this process, we observed that the redundancy of the selected sentences could hardly be detected with the existing attack method. Since the information appearing in a few documents is more likely to be meaningful, it is desirable to increase the uniqueness of the sentences to improve the effectiveness of the attack. Thus, we propose a deduplication strategy to replace the traditional word-level similarity metric with the BPE token level. Our proposed strategy reduces 6&#x0025; to 22&#x0025; of the underestimated samples among selected ones, thereby improving precision by up to 7&#x0025;p. As a result, we show that considering both language- and model-specific characteristics is essential to improve the effectiveness of attack strategies. We also discuss possible mitigations against the MI attacks on the general language models.",
    "full_text": "Received 4 January 2023, accepted 18 January 2023, date of publication 25 January 2023, date of current version 2 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3239668\nMembership Inference Attacks With Token-Level\nDeduplication on Korean Language Models\nMYUNG GYO OH\n , (Graduate Student Member, IEEE),\nLEO HYUN PARK\n , (Graduate Student Member, IEEE), JAEUK KIM,\nJAEWOO PARK, AND TAEKYOUNG KWON\n, (Member, IEEE)\nGraduate School of Information, Yonsei University, Seoul 03722, South Korea\nCorresponding author: Taekyoung Kwon (taekyoung@yonsei.ac.kr)\nThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT)\n(No. NRF-2019R1A2C1088802).\nABSTRACT The confidentiality threat against training data has become a significant security problem\nin neural language models. Recent studies have shown that memorized training data can be extracted\nby injecting well-chosen prompts into generative language models. While these attacks have achieved\nremarkable success in the English-based Transformer architecture, it is unclear whether they are still effective\nin other language domains. This paper studies the effectiveness of attacks against Korean models and the\npotential for attack improvements that might be beneficial for future defense studies. The contribution of\nthis study is two-fold. First, we perform a membership inference attack against the state-of-the-art Korean\nGPT model. We found approximate training data with 20% to 90% precision in the top-100 samples and\nconfirmed that the proposed attack technique for naive GPT is valid across the language domains. Second,\nin this process, we observed that the redundancy of the selected sentences could hardly be detected with the\nexisting attack method. Since the information appearing in a few documents is more likely to be meaningful,\nit is desirable to increase the uniqueness of the sentences to improve the effectiveness of the attack. Thus,\nwe propose a deduplication strategy to replace the traditional word-level similarity metric with the BPE\ntoken level. Our proposed strategy reduces 6% to 22% of the underestimated samples among selected\nones, thereby improving precision by up to 7%p. As a result, we show that considering both language- and\nmodel-specific characteristics is essential to improve the effectiveness of attack strategies. We also discuss\npossible mitigations against the MI attacks on the general language models.\nINDEX TERMSConfidentiality, deep learning, generative language model, Korean-based GPT, membership\ninference, training data extraction attack.\nI. INTRODUCTION\nDeep learning (DL)-based neural language models (neural\nLMs) are rapidly advancing in their respective subfields of\nnatural language processing (NLP), such as neural machine\ntranslation (NMT) [1], [2], question answering (QA) [3], [4],\nand text summarization [5], [6]. Along with these advances,\nrecent studies have shown that LMs can leak memorized\ntraining data by well-chosen prompts [7], [8], [9], [10], [11],\n[12], [13], [14], [15], [16], [17], [18], [19]. In particular,\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Jerry Chun-Wei Lin\n.\ngenerative LMs (e.g., GPT family) are a prime target for\nmembership inference (MI) attacks since they can automat-\nically yield samples from being inferred. Carlini et al. [7]\nidentified 604 actual training data by selecting 1,800 unique\ncandidates in GPT-2 [20]. Their follow-up study confirmed\nthat the memorization capacity of LMs had a log-linear rela-\ntionship with the model size [8]. Considering the current cir-\ncumstance that GPT-based architectures are widely adopted\nas core engines in real applications [21], [22], the MI attacks\nagainst LMs are a substantial threat.\nWhile these attacks have achieved remarkable success in\nthe English-based Transformer [2] architecture, it is unclear\nVOLUME 11, 2023 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 10207\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\nwhether they are still effective in other language domains.\nFor example, many languages have grammatical character-\nistics different from English: no (or flexible) spacing, case-\ninsensitive characters, and relatively free word order in a\nsentence. These differences in grammatical characteristics\nlead to contrasts in preprocessing (specifically, tokenization 1)\nbetween English-based and Korean-based LMs, raising ques-\ntions about the effectiveness of the attacks. On the contrary,\nCarlini et al.’s work assumed case-sensitivity and rigorous\nspacing by targeting the English model. To the best of our\nknowledge, there is no case of studying an MI attack with\nthis language difference.\nIn this paper, we study the MI attacks on Korean-based\ngenerative LMs. As we mentioned above, Korean has very\ndifferent characteristics compared to English. For example,\na spacing is more complicated and a character is case-\ninsensitive. Moreover, the word order is more flexible so that\nits minor change may not affect the meaning of a sentence.\nStarting from the prior elegant work of Carlini et al. [7],\nwe sample 100,000 texts from the LM (§III-B). We score\nthe texts using four metrics based on each loss of samples\nand select the top-100 potential members (§III-C). Finally,\nwe compute the approximated precision through the man-\nual search (§III-D). The main difference between ours and\nCarlini et al.’s work [7] is the subsequent verification of the\ninference results.\nGiven their interesting assumption—formalized as being\nk-eidetic memorization—that the less mentioned information\nin the documents is unintentional and potentially harmful,\nincreasing the uniqueness of selected top-k samples is nec-\nessary. Following the assumption, we more strictly perform\ndeduplication (§III-D) based on the similarity between each\nprediction. We verify the effectiveness of our method by\nchecking the number of underestimated samples where the\nnewly calculated similarity exceeds a threshold so that judged\nto be a duplicate, unlike the previous result.\nOur two-fold contribution based on the experimental\nresults is as follows:\n• We verify that the existing MI attack is effective against\nthe state-of-the-art (SOTA) Korean-based GPT model\n(§IV-C). Our finding that well-defined MI attacks may\nnot depend on the language domain motivates future\nresearch toward multilingual (or universal) MI attacks.\n• We refine the existing deduplicating strategy to increase\nthe uniqueness of selection in the MI process, resulting\nin improved attack precision (§IV-D). A practical attack\nrequires deduplication because the knowledge appearing\nin a few documents is likely to be meaningful informa-\ntion.\nWe publish the experimental code to reproduce the main\nempirical results: https://github.com/seclab-yonsei/mia-ko-\nlm.\n1For example, BERT [4], one of the transformer-based LMs, provides a\ncase-sensitive version (cased) and a case-insensitive version (uncased).\nII. BACKGROUND\nA. LANGUAGE MODELING\n1) TRAINING OBJECTIVE\nAn LM f explores the weight ˆθ that maximizes the appear-\nance probability for each element in a given training corpus\nD =\n{\nxi}N\ni=1 in the pre-training process. The probability that\neach sentence x1:n = [x1, . . . ,xn] appears from f can be\nexpressed as Eq. 1 by the chain rule.\nPr (x1:n; θ) = Pr (x1, . . . ,xn; θ) =\nn∏\nj=1\nPr\n(\nxj | x<j; θ\n)\n(1)\nLM f takes a step of gradient descent to minimize negative\nlog-likelihood:\nL(θ) = −\nN∑\ni=1\nlog Pr\n(\nxi\n1:n; θ\n)\n(2)\n= −\nN∑\ni=1\nn∑\nj=1\nlog Pr\n(\nxi\nj | xi\n<j; θ\n)\n(3)\nθ ← θ − α∇θ L(θ) (4)\nwhere α is the step size of the gradient.\n2) INFERENCE\nThe LM f auto-regressively predicts the next word from the\ngiven words iteratively during the inference phase:\nˆxt = arg max\nxt ∈X\nlog Pr (xt | x<t ; θ) (5)\nwhere X is the set of all tokens that the LM can explore.\n3) GPT FAMILY\nThe Transformer structure proposed by Vaswani et al. [2]\nin 2017 overwhelms existing recurrent neural network-based\nmodels in performance of NMT, leading to rapid development\nin NLP. Since then, the GPT family using only the decoder\nblock of Transformer has shown exceptionally superior per-\nformance in natural language generation (NLG) [20], [21],\n[23], [24], [25], [26].\nB. MEMBERSHIP INFERENCE ATTACK\nA high intuition for MI is finding a distinguishable gap\nbetween the model’s behavior of the training and test\ndataset [16], [27], [28], [29]. An adversary (§III-A) tries to\nexploit loss values [7], logits/predictions [30], [31], or gradi-\nent norms [32], thereby finding or raising the gap. Since our\nexperiments start from Carlini et al.’s work [7], we attempt\nMI based on sample loss.\nIII. METHODOLOGY\nIn this section, we describe an approach to attack the Korean\nmodel. We first define the attacker’s capabilities and objec-\ntives (§III-A). Afterward, we explain the method to gen-\nerate texts following the threat model (§III-B), determine\nthe member/non-member of the sampled data through MI\n10208 VOLUME 11, 2023\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\n(§III-C), and verify the GT of the selected data (§III-D).\nIn this process, we introduce the modified Carlini et al.’s\ndeduplication to suit the characteristics of the Korean\nlanguage.\nA. THREAT MODEL\n1) ADVERSARY’s CAPABILITIES\nAssume that the attacker has a black-box attack environment\nthat can access the input/output of the pre-trained LM f . The\nattacker does not have access to the weights inside the f ,\ntraining data distribution, architecture knowledge, and out-\nput of the hidden layer. Nevertheless, for a chosen prompt\np = [p1, . . . ,pn], the attacker can compute the output s =\nf (p; θ) = [p1, . . . ,pn, s1, . . . ,sm] of the LM. The attacker\ncan control the hyperparameters for generating the output s,\ne.g., adjusting the resulting’s min/max length and considering\nvarious sampling strategies [23], [33].\n2) ADVERSARY’s OBJECTIVES\nGiven an input x and access to the pre-trained LM f ,\nan attacker infers whether ˆx ∈ Dtrain for ˆx ∈ Dinfer [34].\nWe assume an attacker conducting an untargeted attack that\nacquires data without aiming or deriving for specific data.\nThe attacker maximizes the MI precision by selecting the set\nof top-k samples A among numerous samples obtained from\nf :\nˆA = arg max\nA⊂Dinfer\n|A|=k\n∑\nˆx∈A\n1\n(\n∃x ∈ Dtrain : match\n(\nx, ˆx\n)\n≥ m\n)\n(6)\nwhere match is a substring matching function that deter-\nmines memorization, and m is a threshold that controls the\nlower bound of matching characters (§III-D2).\nB. TEXT SAMPLING\nFor MI, we first approximate the population by generating a\nsufficiently large number of samples. When selecting the next\nword, we auto-regressively sample the words with the top-40\nprobabilities. The output statement is 256 tokens, excluding\nthe input prompt. For random sampling, we use a special\ntoken [EOS] (end of the sentence) as a prompt to inform the\nend of the sentence.\nWe can interpret the process of inserting only a token to\ntrigger text in two ways. First, the sentences are likely to\nbe familiar with the form the LM frequently sees during\ntraining. For example, if the training data contains scripts\nfor movies or dramas, some output sentences might have a\nsimilar form. The other one is that the LM will likely guide\nthe most grammatically complete text form. Consequently,\nthis generation process leads to the start of extracting training\ndata.\nC. MEMBERSHIP INFERENCE\nWe infer membership of generated samples by selecting with\na high probability of being in the training data. MI attack\non machine learning (ML) starts with the assumption that\nTABLE 1. A brief introduction to the four evaluation indicators we used.\nPPL and Window metrics score better with lower values,zlib and\nLowercase metrics with higher values.\nthe data used for training is overfitting (or memorized) more\nthan the data not used for training (i.e., test dataset) [30];\nthe data already learned once will have relatively high con-\nfidence. We use four out of six evaluation metrics proposed\nby Carlini et al. [7] (Table 1) to capture the confidence dif-\nference. We select potential member sentences with the top-\nk duplicates removed by scoring the samples on the four\nmetrics. We did not use the other two metrics measuring the\nloss ratio of two different sizes (XL versus medium or small)\nmodels for a single sentence.\n1) PERPLEXITY\nWe calculate the perplexity (PPL) of the output sentences\nfrom the LM as the geometric mean of the sentence proba-\nbilities over the length. Intuitively, the PPL is the number of\nconfusing words for every time step. The lower the value, the\nmore confident the LM is about the sentence, and the higher\nthe value, the more confused the LM is.\n2) COMPARING WITH ZLIB ENTROPY\nLM sometimes yields an unexpected output; it produces an\nincoherent gibberish output or degenerates repetition [33].\nAlthough the PPL effectively filters out unfamiliar phrases\nthat do not fit the syntax, it sometimes gives a very high score\nto unexpectedly repeated output sentences (e.g., ‘‘ [1] [2] [3]\n. . .’’ or ‘‘----- . . .’’). Carlini et al. attempted filtering by\ncalculating the entropy of a sentence based on the zlib [35]\ncompression algorithm to alleviate this phenomenon. The\nlower the PPL and the higher the zlib of the sentence at\nthe same time, the closer it is to a probable member. We\npresent a code implementation for calculating zlib entropy in\nthe appendix (§A-A).\n3) COMPARING WITH LOWERCASE TEXTS\nIn the case of English text containing sufficient upper-\ncase characters, such as newspaper article titles and product\nnames, converting the sentences to lowercase may dramat-\nically raise the PPL. We select the Lowercase as one of\nthe metrics since even the Korean text can include various\nEnglish characters.\n4) CALCULATING MINIMUM PERPLEXITY BY SLIDING\nWINDOW\nThere is a possibility that the inference result is not wholly\nsimilar to the actual member and is included only in some\nsegmentation. Carlini et al. calculated the lowest PPL among\nVOLUME 11, 2023 10209\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\n50 tokens by sliding the sentence. Since the stride is unclear,\nwe set it to 16, considering the time cost.\nD. VERIFICATION\nWe verify whether the selected top-k samples by MI are mem-\nbers or not. The dataset used for training is not publicly avail-\nable, and the property (distribution) in which the data was\ncollected is also unspecified. Since the GT is unidentified,\nwe collected approximated reference documents by manually\nsearching each sample with the Google search engine as a\nsuboptimal approach.\n1) CANDIDATE DEDUPLICATION\nIn order to maximize the amount of information in the unique\noutput sentences, we deduplicate two sentences when the\nsimilarity between them exceeds a certain threshold. Two\nduplicated sentences are the same or similar. We deleted the\nidentical sentences except one for efficient indexing. If they\nare not the same but similar to some degree, we perform\ndeduplicating by the similarity function based on trigram\nmultiset. We describe two deduplication method, including\nour new strategy with BPE tokenization, as follows:\n• Word-leveltokenization is a traditional way of splitting\neach token by whitespace and punctuation [7]. Simple to\nuse and understand, but it might not be a suitable method\nfor tokenizing Korean text with complicated spacing.\n• Byte Pair Encoding (BPE) [36] tokenization is one\nof NLP’s most popular subword segmentation algo-\nrithms [37]. Prior knowledge of vocabulary mapping is\nrequired when pre-training the target LM, but generally,\nwe can expect to divide the text into more precise units\nthan word-level division. This modification was moti-\nvated by insufficient previous deduplicating, in which\nsome similar texts were included in top-k (Table 2).\nWe calculate the similarity between tokenized sentences\n(i.e., predictions) and deduplicate them based on that. We\nobtain a family of sets tri by binding three adjacent tokens\n(i.e., trigram).\ntri (s) := {{si, si+1, si+2} : ∀i} (7)\nFor some sentence s is similar with t if and only if\n|tri(s)∩tri(t)|\n|tri(s)| ≥ α for the threshold α = 0.5. We keep\nthe hyperparameter α the same to verify the attack proposed\nin the previous study [39]. We leave the implementation for\nsimilarity calculations in our appendix (§A-B).\n2) SUBSTRING MATCHING\nThe raw document text (called reference) collected by\nthe searching trivially includes the inference result (called\nhypothesis). Given reference r =\n[\nr1, . . . ,rp]\nand hypothesis\nh =\n[\nh1, . . . ,hq]\nwhere p > q and each element means a\ncharacter,h is memorized if ri..i+m = hj..j+m for some indexes\ni and j. We maintain m to 50 for the same reason we set the\nhyperparameter α.\nIV. EVALUATION\nWe answer two-fold research questions through our experi-\nments:\n• RQ1 (Effectiveness): Are MI attacks that have achieved\nremarkable success in existing English-based LMs still\neffective in other language domains, especially Korean?\n(§IV-C)\n• RQ2 (Uniqueness): Does the BPE tokenization method\nproposed to increase the amount of information con-\ntained in the inferred samples increase the uniqueness\nand precision of the top-k results? (§IV-D)\nA. ENVIRONMENTS\nWe performed text generation and MI attacks on a single\nmachine environment with two NVIDIA Quadro RTX 6000\n(24GB) GPUs. We used PyTorch [40] with version 1.11 and\nTransformers [41] with version 4.17 as our DL library.\nWe omit other components since they do not significantly\naffect the experimental results.\nB. TARGET SYSTEM\nWe experiment with the Korean-based SOTA genera-\ntive LM, KoGPT (Korean Generative Pre-trained Trans-\nformer) [42]. The pre-trained KoGPT is open for research\nin HuggingFace [41], and since the number of parameters\nis large enough, we considered it is effective MI would be\npossible [8]. In addition, the corporation to which the team\nbelongs has a search engine 2 that can collect high-quality\nsource data. The high quality of the training data leads to\nmore complete outputs and contributes to the consistency\n(i.e., reproducibility) of the experiment results.\nC. RQ1: EFFECTIVENESS OF MEMBERSHIP INFERENCE\nON KOREAN MODEL\n1) EXPERIMENTAL RESULT\nTable 3 shows the quantitative results of MI (first and\nsecond results). We identified 89 and 90 actual mem-\nbers out of 100 potential members for the metrics PPL\nand zlib, respectively. We searched only 20 samples with\nthe Lowercase metric, 33 fewer than the prior result.\nWe detected 52 samples with Window, 19 more than before;\nyet, it did not show performance improvement as much as\nPPL or zlib metric.\nTable 4 provides examples of top-1 sentences for each\nmetric. PPL and zlib entropy metrics showed the same out-\nput. Lowercase metric contained many English uppercase\ncharacters and had a low PPL at the same time. Window\nmetric includes trivial (i.e., repeated and boring) segments in\nthe middle of the sentence.\nThe selection of the four metrics is not focused on specific\noutputs but instead on looking at different parts. As shown\nin Fig. 1, the top-k PPL and zlib samples often overlap.\nNonetheless, Lowercase and Window indicated an even\ndistribution of the index regardless of the PPL. It is crucial to\n2https://www.daum.net/\n10210 VOLUME 11, 2023\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\nTABLE 2. An example of a non-duplicated sentence with high similarity by intrinsic evaluation. We usedifflib [38] to highlight common areas. In the\nexisting method [7], the similarity with the reference was underestimated to 0.474. However, our method raises the similarity to 0.538 and judges it as a\nduplicate.\nTABLE 3. Quantitative results of MI using four metrics. Each result means the number of samples found with the corresponding metric out of 100. The\nleading cause of finding more members than prior work inPPL, zlib, andWindow metrics is the increase in the number of parameters in LM. On the\nother hand, theLowercase metric shows poor performance due to differences in language domains; attempt to use the characteristics of English words\nfrom Korean-based LM.\nTABLE 4. The output sentence with the highest score for each metric. We masked the personal information included in the output with. The part\nobscured by the character ‘‘*’’ is presumed to be de-identified before pre-training.\nevaluate different aspects of the generated text as it enriches\nthe distribution of selected samples.\n2) DISCUSSION\nContrary to the previous insistence that PPL alone is chal-\nlenging for high performance, we confirmed the encouraging\nprecision of PPL on the Korean model. We interpret that\nthe similar performance of PPL to the zlib is because\nthe ratio of repeated phrases in the top-k sample selected\nby PPL was low, resulting in a similar performance to the\nzlib. In addition, in Figure 1, we observed that PPL and\nzlib metrics tend to pay attention to the same samples and\neven 94 of the top-100 samples selected from PPL were also\nselected as in zlib. On the other hand, the lowercase found\nonly 20 samples, about 37.7%, compared to the 53 sam-\nples found in the existing GPT-2. Since web-crawled Korean\ntext also contains English in product names or URLs, the\nmetric will guarantee the minimum performance. Regard-\nless, Korean is case-insensitive, so comparing PPL before\nand after lowercase is not enough to show outstanding per-\nformance. In summary, existing MI attacks still work well\nbeyond language domains through these characteristics, yet\nwe can expect a higher performance using language-specific\nstrategies.\nVOLUME 11, 2023 10211\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\nFIGURE 1. Selection points of each metric for the top-1000 samples in\nascending order ofPPL. PPL and zlib metrics tend to focus on common\nsamples. On the other hand,Lowercase and Window metrics look at\ndifferent aspects of the generated sample.\nTABLE 5. The number of underestimated cases. The word-level similarity\nmethod is being top-k, but our method is not.\nWhile we detected more redundant samples than in the\nprevious work, it is premature to conclude that KoGPT is\nvulnerable. We maintained the existing Carlini et al.’s attack\nstrategies substantially the same, but the experimental results\nmay be exaggerated or reduced due to the following differ-\nences. First, since our target system is 4.0× larger than that of\nCarlini et al. (§IV-B), the memorization capacity may differ.\nCarlini et al. used the GPT-2 (XL) [23] with 1542M param-\neters, and we used KoGPT [42] with 6167M parameters. For\na fairer comparison, it is desirable to perform an empirical\nevaluation on the same architecture (i.e., GPT-J [25]). Second,\nmemorization definition is also a significant factor influ-\nencing precision. Carlini et al. reported a potentially mem-\nbered sample to OpenAI and received only ‘‘member/non-\nmember’’ results; i.e., they overlooked a precise threshold\nof memorization. On the other hand, we considered a case\nin which 50 tokens of BPE tokenized texts were duplicated\n(§III-D2). Finally, the total number of sampled texts is also\ndifferent; Carlini et al. sampled 200,000 texts in a sampling\nstrategy, but we generated only 100,000 candidates.\nD. RQ2: UNIQUENESS OF INFERRED SAMPLES\n1) EXPERIMENTAL RESULT\nAs we mentioned in §III-D1, we modified the existing\nword-level tokenization with BPE tokenization to improve\nthe uniqueness of inference results. The BPE token-level\nmethod found 6% to 22% more duplicated samples from\neach metric (Table 5), showing that the previous method\nunderestimated the similarity of such samples.\nFurthermore, we report the precision after selecting top-\nk with the BPE tokenizer-based deduplicating strategy (the\nthird row in Table 3). Deduplication with the BPE tokenizer\nimproves the precision compared to the word-level tokenizer\nexcept for Lowercase; Lowercase is reduced by 1%p,\nPPL, zlib, and Window are increased by 2%p, 1%p,\nand 7%p, respectively. Accordingly, increasing uniqueness\nincreases the precision of MI attacks.\n2) DISCUSSION\nAlthough few selected samples impede the hasty expansion\ninterpretation of the experimental results, the existence of\npotentially overestimated members of about 6% per metric\nleaves room for further advancement. We found from further\nstudies that a more sophisticated deduplication could increase\nthe top-k precision of MI attacks. Therefore, increasing the\nuniqueness of top-k candidates for MI attacks is essential\nbecause it increases the amount of information in a sample\nand attack performance.\nThis deduplication strategy is not limited to BPE tokeniza-\ntion and can be generalized to all tokenizers used by the\ntarget model (except word-level, e.g.; WordPiece [4], [43]).\nIf the pre-trained LM is public-available, the tokenizer’s con-\nfiguration also tends to be disclosed. Therefore, the attacker\ncan naturally optimize the attack strategy by inferring the\ntokenizer used for training from the open source.\nV. DISCUSSION\nA. INFERENCE SAMPLE PROPERTIES\nSince we experimented with an untargeted attack, we did not\nconsider any properties of the inferences. A targeted attack\nthat maximizes the risk of privacy leakage of training data\nresulting from MI outcomes is a promising future research\narea. Researchers can extend the experiment to investigate\npolicy issues that may arise from the output, regardless of\nmember/non-member results [44]. A simple example is an\nissue of copyright and intellectual property (IP) infringement\ndriven by leakage of the memorized training data.\nB. VARIOUS SAMPLING STRATEGIES\nWe can consider various generation methods to increase fur-\nther the entropy of potential members beyond the current\nsampling method. For example, top-p [33] sampling can be\nused concurrently with top-k sampling currently in use. Alter-\nnatively, we can intentionally bias the outputs by adjusting\nvarious penalties, e.g., repetition [45], coverage [46], and\nlength [43]. We argue that we can also increase the diversity\nof our output by tweaking the number of return sequences.\nIn other words, it is possible to increase the output distribution\nby generating the top-n probability outputs instead of calcu-\nlating the maximum probability for each input prompt.\nC. APPLIED MEMBERSHIP INFERENCE\nAn attacker can try to make a more optimal attack based\non the approximate distribution of the training dataset.\nExisting studies treated output length as a constant, such\nas limiting a sentence’s maximum length or fixing the\n10212 VOLUME 11, 2023\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\nminimum/maximum length equally [7], [39]. However,\nassuming a limited white-box attack environment where the\nproper length of the desired output sentence is known [47],\nit would be reasonable to limit the sentence’s output not to\nbe too long/short. In other words, the attacker adjusts the\nmin/max length of the outputs to fit the distribution of the\ntraining dataset. For example, the RealNews [48] and One-\nBillion Word Benchmark (LM1B) [49] datasets have 31M\nand 30M documents with an average of 793 and 32 BPE\ntokens per sample, respectively.\nD. ENTIRELY THE SAME OUTPUTS\nWe found 35 curious sentences 3 whose all 256 tokens were\nutterly the same (§III-D1). The verbatim outputs induced\ntwo or more times by LM imply that the possibility of the\nsentence existing in the training data is very high. In advance,\nwe have obliterated duplicate sentences for efficient indexing,\nbut further research is still required in future.\nE. POSSIBLE MITIGATIONS\nNumerous defenses and possible mitigations have been stud-\nied to prevent MI. Considering the sequential phase of DL\nmodeling, we describe these prior works by dividing them\ninto four attack surfaces: (i) pre-processing, (ii) training, (iii)\ninference, and (iv) deployment.\n1) PRE-PROCESSING PHASE\nBefore training the model, we can attempt to preemptively\nreduce the memorization of the sequences that repeatedly\nappear by deduplicating—a step in text cleaning [50]—the\ngiven training dataset [39], [51]. Lee et al. [39] claimed\nthat deduplicating training data reduces the training time of\nLMs, with little or no harm to performance, and reduces the\nrate of emitting memorized training data by up to 10 times.\nAs a more proactive approach, based on the observation\nthat samples vulnerable to MI attacks are concentrated in\nsome outliers [52], [53], it seems possible to try to detect\nand remove them in advance. Regardless, these attempts are\nrefuted head-on due to the privacy onion effect [54] that if\nsamples vulnerable to MI attacks are removed, new samples\nbecome vulnerable to MI attacks.\n2) TRAINING PHASE\nDifferential privacy (DP) training is one well-known method\nof providing strong privacy guarantees in the training dataset\nby adding noise to the gradient [55], [56]. While being\na proven concept [34], [57], [58], DP can have tradeoffs\nthat impair the utility of models and result in performance\ndegradation. As an alternative to controlling this privacy-\nutility tradeoff, there are attempts to reduce overfitting—the\nmain cause of memorization—by modifying the loss func-\ntion [28], [59], adding regularization [47], [60], [61], or quan-\ntization [62]. Controlling direct access to private training\n3Two, three, and five identical sentences appear 6, 1, and 4 times, respec-\ntively.\ndata through knowledge distillation [63] is also extensively\nstudied [16], [64], [65].\n3) INFERENCE PHASE\nExtensive work has been studied on mitigating MI attacks\nthrough confidence masking, which intentionally promotes\nthe actual confidence score returned by the target model [34].\nFor example, the target model could consider giving the\nattacker only a top-k confidence score [30], returning only the\nmore restricted predicted labels (i.e., top-1) [31], or adding\nmanipulated noise to the prediction vector [66], [67].\n4) DEPLOYMENT PHASE\nAn attempt to intentionally unlearn training data exposed\nfrom a deployed model can also be seen as one of the possible\nmitigations of an MI attack [68], [69]. The most naive way\nto make a DL model forget certain data is to relearn it from\nscratch, but it can be challenging due to a large amount of time\nand computational cost. From this point of view, machine\nunlearning techniques that ensure that certain data is not used\nto train a model are not only budget-efficient but also crucial\nfor a responsible follow-up by the model deployer.\nVI. RELATED WORK\nA. MEMBERSHIP INFERENCE ATTACKS AGAINST\nMACHINE LEARNING MODELS\nShokri et al. [30] proposed a shadow training technique for\ntraining an attack model that predicts member/non-member\nusing a prediction probability vector. They first created sev-\neral shadow models that mimic the behavior of the target\nmodel. Afterward, they trained the shadow models through\nthree methods for generating or synthesizing training data.\nFinally, based on the shadow models’ output, they trained\nan attack model that predicts member/non-member and used\nit for the actual attack. While it showed outstanding perfor-\nmance in the classification task, the shadow training tech-\nnique is limited in the recent LMs for two reasons. First,\nthey achieved shadow training to target models trained by\nsupervised learning, but recent LMs are mostly pre-trained\nwith unsupervised learning. Second, to improve the perfor-\nmance of shadow training, the more the number of mimic\nmodels, the better. However, recent LMs have limited bud-\ngets since the model scale and the training data size have\ngrown exponentially. For example, GPT-3 [24] (proposed in\n2020) and Switch Transformers [70] (proposed in 2022) have\nparameters of 175 billion and one trillion, respectively.\nB. EXTRACTING TRAINING DATA FROM LARGE LANGUAGE\nMODELS\nCarlini et al. proposed a MI attack pipeline targeting a gen-\nerative LM [7]. They deduplicated sentences by calculat-\ning a word-level trigram-multiset based on whitespaces and\npunctuations in a sentence. In contrast, we considered that\nthe deduplicating strategy is an overly rough assumption for\nVOLUME 11, 2023 10213\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\ndetermining the similarity of sentences and improved it by the\nBPE token-level similarity (§III-D1).\nC. HOW BPE AFFECTS MEMORIZATION IN\nTRANSFORMERS\nKharitonov et al. [71] showed that the size of the subword\nvocabulary learned by BPE significantly affected both the\nability and tendency of the Transformer models to mem-\norize training data. They argue that large cardinality BPE\nvocabulary greatly encourages memorization due to the lower\nout-of-vocabulary (OOV) frequencies. However, unlike the\ncharacter-level BPE tokenization [37] they studied, we exper-\niment with byte-level BPE tokenization [23]; especially,\nour target model is pre-trained with a byte-level BPE tok-\nenizer. Byte-level BPE tokenization rarely incurs OOV, unlike\ncharacter-level BPE tokenization. Therefore, their study—\nwhere the more extensive the capacity of the subword map-\nping table, the more vulnerable it is to MI attacks—is quite\ndifferent from ours.\nD. QUANTIFYING MEMORIZATION ACROSS NEURAL\nLANGUAGE MODELS\nCarlini et al. performed extensive experiments on GPT-J [25]\nand GPT-Neo [26] for quantitative analysis of factors that\nincrease the memorization capability of LM. As a result, they\nidentified three attributes that significantly influence memo-\nrization; bigger models, repeated strings, and more extended\ncontext. They confirmed that the LM could accurately repro-\nduce the target sentence with a chosen length-k prompt.\nAfterward, they expanded the study to the T5 [72] masked\nLM (MLM) as a replication experiment. We conducted exper-\niments only on models trained by the next word prediction\n(NWP) strategy (§IV-B), not MLM. We did not study mem-\norization according to the selection of input prompts; this\nis due to the characteristics of the target system for which\ntraining data is not public-available. We plan to expand the\nexperiment to public LMs trained on public-available datasets\nin the future.\nE. DEDUPLICATING TRAINING DATA MAKES LANGUAGE\nMODELS BETTER\nLee et al. showed that duplication of training data for LM\nreduces the diversity of outputs and increases the likeli-\nhood of exposing the members [39]. They generated 100,000\nsamples, each with a maximum length of 512 BPE tokens.\nThey defined that the LM remembers tokens if each output\nhas exactly 50 token substrings in the training data. As a\nresult, they showed that LM memorized more than 1% of the\ngenerated tokens. We maintain their memorization definition\nwhile reducing the token length limit from ‘‘less than 512’’ to\n‘‘exactly 256’’. This process prevents the experimental results\n(precision) from being exaggerated.\nVII. CONCLUSION\nThis paper studied the effectiveness and improvements of the\nexisting MI attack for the SOTA Korean-based GPT model.\nWe confirmed that the existing attack strategies are still suf-\nficient for data extraction of the Korean LM. In addition,\nPPL and zlib metrics yield up to 80%p higher than the\nprior results. We established that the existing deduplicating\nprocedure during this attack process is not rigorous enough.\nWe identified 6% to 22% more duplicated samples than the\nexisting method by replacing the word-level trigram similar-\nity with the BPE token-level one. Since the knowledge that\nappears in fewer documents is more likely to be meaningful,\nincreasing the uniqueness of the selected samples increases\nthe effectiveness and diversity of the attack. Furthermore,\nwe confirmed that increasing the uniqueness of the top-k\nresult of the MI attack increases the precision by up to 7%p.\nWe hope future work continues to explore whether MI attacks\nare still effective in other LMs and leverage new findings to\nimprove language-specific attack and defense strategies.\nWe plan to gradually expand the experiment on LM mem-\norization, one of the possible mitigations. Future studies will\nhelp understand the fundamental cause of confidentiality\nleaks for LM. We believe that our study on confidentiality\ninfringement serves as an opportunity to raise the awareness\nof all researchers, developers, and administrators about secu-\nrity issues.\nAPPENDIX A\nCODE IMPLEMENTATION\nA. CALCULATING ZLIB ENTROPY\nWe obtained zlib entropy by calculating the number of bits\nof the compressed output after encoding the sentence in\nUTF-8 [73].\nimport zlib\ndef calculate_zlib_entropy(sentence: str) -> int:\nreturn len(zlib.compress(bytes(sentence, ‘‘utf-8’’)))\nB. CALCULATING SIMILARITY OF TRIGRAM MULTISET\nThe similarity of two sentences is obtained by calculating the\nratio of trigram intersection of two sentences tokenized by\nword-level or BPE tokenizer. Since we compute the similarity\nof str2 to the selected str1, the similarity operation is not\ncommutative (i.e., the similarity between str1 and str2 is\ndifferent from the similarity between str2 and str1).\ndef calculate_similarity(\ntokenizer,\nstr1: str,\nstr2: str,\nn_gram: int = 3,\nis_word_level: bool = True,\n) -> float:\ndef _word_level_tokenization(\nsentence: str,\n) -> List[str]:\nreturn re.split(r‘‘[\\s]+|[.,!?;]’’, sentence)\ndef _bpe_token_level_tokenization(\nsentence: str,\n) -> List[str]:\nreturn tokenizer.encode(sentence)\nif is_word_level:\n10214 VOLUME 11, 2023\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\nstr1 = _word_level_tokenization(str1)\nstr2 = _word_level_tokenization(str2)\nelse:\nstr1 = _bpe_token_level_tokenization(str1)\nstr2 = _bpe_token_level_tokenization(str2)\n## Calculate trigram similarity: str1 (reference)\n## vs str2 (hyphothesis). It~is same as\n## ‘‘Is string 1~is similar with string 2?’’\n## Occasionally, if~the Text consists of 1~or 2\n## words, the trigram multiset will result in an\n## empty list, resulting in a divided by zero\n## error. To~prevent this, we~guarantee that the\n## trigram multiset has at least one element.\nn_gram_set = lambda x: [\nsorted([j for j in x[i: i + n_gram]])\nfor i in range(max(len(x) - n_gram, 1))\n]\ns1 = n_gram_set(str1)\ns2 = n_gram_set(str2)\n## Return true if str1 is similar (or duplicated)\n## to str2 else false. It~is not recommended to\n## mark two strings as similar, trivially.\nreturn len([i for i in s1 if i in s2]) / len(s1)\nACKNOWLEDGMENT\nAn earlier version of this paper was presented at the ‘‘On\nMembership Inference Attacks to Generative Language Mod-\nels Across Language Domains’’ in Pre-Proceeding of the\n23rd World Conference on Information Security Applications\n(WISA 2022) [DOI: 10.1007/978-3-031-25659-2_11].\nREFERENCES\n[1] D. Bahdanau, K. Cho, and Y. Bengio, ‘‘Neural machine translation by\njointly learning to align and translate,’’ 2014, arXiv:1409.0473.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 1–11.\n[3] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y. Liu, J. Xu,\nM. Ott, K. Shuster, E. M. Smith, Y.-L. Boureau, and J. Weston, ‘‘Recipes\nfor building an open-domain chatbot,’’ 2020, arXiv:2004.13637.\n[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[5] J. Zhang, Y. Zhao, M. Saleh, and P. Liu, ‘‘PEGASUS: Pre-training with\nextracted gap-sentences for abstractive summarization,’’ in Proc. 37th Int.\nConf. Mach. Learn., 2020, pp. 11328–11339.\n[6] M. Zhong, P. Liu, Y. Chen, D. Wang, X. Qiu, and X. Huang, ‘‘Extractive\nsummarization as text matching,’’ 2020, arXiv:2004.08795.\n[7] N. Carlini, F. Tramer, E. Wallace, and M. Jagielski, ‘‘Extracting training\ndata from large language models,’’ in Proc. 30th USENIX Secur. Symp.\n(USENIX Secur.), 2021, pp. 2633–2650.\n[8] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and\nC. Zhang, ‘‘Quantifying memorization across neural language models,’’\n2022, arXiv:2202.07646.\n[9] A. Jagannatha, B. Pratap Singh Rawat, and H. Yu, ‘‘Membership\ninference attack susceptibility of clinical language models,’’ 2021,\narXiv:2104.08305.\n[10] E. Lehman, S. Jain, K. Pichotta, Y. Goldberg, and B. C. Wallace,\n‘‘Does BERT pretrained on clinical notes reveal sensitive data?’’ 2021,\narXiv:2104.07762.\n[11] C. Song and A. Raghunathan, ‘‘Information leakage in embedding mod-\nels,’’ in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., Oct. 2020,\npp. 377–390.\n[12] J. Hayes, L. Melis, G. Danezis, and E. D. Cristofaro, ‘‘LOGAN: Mem-\nbership inference attacks against generative models,’’ in Proc. Privacy\nEnhancing Technol. (PoPETs), 2019, pp. 133–152.\n[13] F. Mireshghallah, K. Goyal, A. Uniyal, T. Berg-Kirkpatrick, and R. Shokri,\n‘‘Quantifying privacy risks of masked language models using membership\ninference attacks,’’ 2022, arXiv:2203.03929.\n[14] C. Zhang, D. Ippolito, K. Lee, M. Jagielski, F. Tramèr, and\nN. Carlini, ‘‘Counterfactual memorization in neural language models,’’\n2021, arXiv:2112.12938.\n[15] O. D. Thakkar, S. Ramaswamy, R. Mathews, and F. Beaufays, ‘‘Under-\nstanding unintended memorization in language models under federated\nlearning,’’ in Proc. 3rd Workshop Privacy Natural Lang. Process., 2021,\npp. 1–10.\n[16] V. Shejwalkar and A. Houmansadr, ‘‘Membership privacy for machine\nlearning models through knowledge transfer,’’ in Proc. AAAI Conf. Artif.\nIntell., vol. 35, 2021, pp. 9549–9557.\n[17] F. Mireshghallah, A. Uniyal, T. Wang, D. Evans, and T. Berg-Kirkpatrick,\n‘‘Memorization in NLP fine-tuning methods,’’ 2022, arXiv:2205.12506.\n[18] A. Elmahdy, H. A. Inan, and R. Sim, ‘‘Privacy leakage in text classification:\nA data extraction approach,’’ 2022, arXiv:2206.04591.\n[19] T. Vakili and H. Dalianis, ‘‘Are clinical BERT models privacy preserv-\ning? The difficulty of extracting patient-condition associations,’’ in Proc.\nHUMAN@ AAAI Fall Symp., 2021.\n[20] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, ‘‘Improving\nlanguage understanding by generative pre-training,’’ OpenAI, Tech. Rep.,\n2018.\n[21] M. Chen et al., ‘‘Evaluating large language models trained on code,’’ 2021,\narXiv:2107.03374.\n[22] B. Kim et al., ‘‘What changes can large-scale language models bring?\nIntensive study on hyperclova: Billions-scale Korean generative pretrained\ntransformers,’’ 2021, arXiv:2109.04650.\n[23] A. Radford, J. Wu, R. Child, D. Luan, and D. Amodei, ‘‘Language models\nare unsupervised multitask learners,’’ OpenAI Blog, vol. 1, no. 8, p. 9,\n2019.\n[24] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, and A. Askell, ‘‘Language models\nare few-shot learners,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 33,\n2020, pp. 1877–1901.\n[25] B. Wang and A. Komatsuzaki, ‘‘GPT-J-6B: A 6 billion parame-\nter autoregressive language model,’’ May 2021. [Online]. Available:\nhttps://github.com/kingoflolz/mesh-transformer-jax\n[26] S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman, ‘‘GPT-Neo: Large\nscale autoregressive language modeling with mesh-tensorflow,’’ in If You\nUse This Software, Please Cite it Using These Metadata, vol. 58. Zenodo,\n2021.\n[27] Y. Kaya and T. Dumitras, ‘‘When does data augmentation help with\nmembership inference attacks?’’ in Proc. 38th Int. Conf. Mach. Learn.,\n2021, pp. 5345–5355.\n[28] D. Chen, N. Yu, and M. Fritz, ‘‘RelaxLoss: Defending membership infer-\nence attacks without losing utility,’’ in Proc. Int. Conf. Learn. Represent.,\n2022, pp. 1–28.\n[29] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, ‘‘Privacy risk in machine\nlearning: Analyzing the connection to overfitting,’’ in Proc. IEEE 31st\nComput. Secur. Found. Symp. (CSF), Jul. 2018, pp. 268–282.\n[30] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, ‘‘Membership inference\nattacks against machine learning models,’’ in Proc. IEEE Symp. Secur.\nPrivacy (SP), May 2017, pp. 3–18.\n[31] C. A. Choquette-Choo, F. Tramer, N. Carlini, and N. Papernot, ‘‘Label-\nonly membership inference attacks,’’ in Proc. 38th Int. Conf. Mach. Learn.,\n2021, pp. 1964–1974.\n[32] M. Nasr, R. Shokri, and A. Houmansadr, ‘‘Comprehensive privacy analysis\nof deep learning: Passive and active white-box inference attacks against\ncentralized and federated learning,’’ in Proc. IEEE Symp. Secur. Privacy\n(SP), May 2019, pp. 739–753.\n[33] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, ‘‘The curious case\nof neural text degeneration,’’ 2019, arXiv:1904.09751.\n[34] H. Hu, Z. Salcic, L. Sun, G. Dobbie, P. S. Yu, and X. Zhang, ‘‘Membership\ninference attacks on machine learning: A survey,’’ ACM Comput. Surv.,\nvol. 54, no. 11s, pp. 1–37, Jan. 2022.\n[35] J.-L. Gailly and M. Adler, ‘‘Zlib compression library,’’ Tech. Rep., 2004.\n[36] P. Gage, ‘‘A new algorithm for data compression,’’ C Users J., vol. 12,\nno. 2, pp. 23–38, 1994.\n[37] R. Sennrich, B. Haddow, and A. Birch, ‘‘Neural machine translation of rare\nwords with subword units,’’ 2015, arXiv:1508.07909.\n[38] Difflib—Helpers for Computing Deltas. Accessed: May 1, 2022. [Online].\nAvailable: https://docs.python.org/3/library/difflib.html\n[39] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and\nN. Carlini, ‘‘Deduplicating training data makes language models better,’’\n2021, arXiv:2107.06499.\nVOLUME 11, 2023 10215\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\n[40] A. Paszke et al., ‘‘PyTorch: An imperative style, high-performance deep\nlearning library,’’ in Proc. Adv. Neural Inf. Process. Syst. , vol. 32, 2019,\npp. 1–12.\n[41] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, and M. Funtowicz, ‘‘Huggingface’s transformers: State-\nof-the-art natural language processing,’’ 2019, arXiv:1910.03771.\n[42] I. Kim, G. Han, J. Ham, and W. Baek. (2021). KoGPT: Kakaobrain\nKorean(Hangul) Generative Pre-Trained Transformer. [Online]. Avail-\nable: https://github.com/kakaobrain/kogpt\n[43] Y. Wu et al., ‘‘Google’s neural machine translation system: Bridging the\ngap between human and machine translation,’’ 2016, arXiv:1609.08144.\n[44] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, and F. Tramèr, ‘‘What\ndoes it mean for a language model to preserve privacy?’’ 2022,\narXiv:2202.05520.\n[45] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher,\n‘‘CTRL: A conditional transformer language model for controllable gen-\neration,’’ 2019, arXiv:1909.05858.\n[46] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li, ‘‘Modeling coverage for neural\nmachine translation,’’ 2016, arXiv:1601.04811.\n[47] K. Leino and M. Fredrikson, ‘‘Stolen memories: Leveraging model\nmemorization for calibrated white-box membership inference,’’\nin Proc. 29th USENIX Secur. Symp. (USENIX Secur.), 2020,\npp. 1605–1622.\n[48] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and\nY. Choi, ‘‘Defending against neural fake news,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 32, 2019, pp. 1–12.\n[49] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and\nT. Robinson, ‘‘One billion word benchmark for measuring progress in\nstatistical language modeling,’’ 2013, arXiv:1312.3005.\n[50] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, and S. Young, ‘‘Scaling language\nmodels: Methods, analysis & insights from training gopher,’’ 2021,\narXiv:2112.11446.\n[51] N. Kandpal, E. Wallace, and C. Raffel, ‘‘Deduplicating training data miti-\ngates privacy risks in language models,’’ 2022, arXiv:2202.06539.\n[52] F. Tramèr, R. Shokri, A. San Joaquin, H. Le, M. Jagielski, S. Hong, and\nN. Carlini, ‘‘Truth serum: Poisoning machine learning models to reveal\ntheir secrets,’’ 2022, arXiv:2204.00032.\n[53] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer, ‘‘Mem-\nbership inference attacks from first principles,’’ in Proc. IEEE Symp. Secur.\nPrivacy (SP), May 2022, p. 1519.\n[54] N. Carlini, M. Jagielski, C. Zhang, N. Papernot, A. Terzis, and F. Tramer,\n‘‘The privacy onion effect: Memorization is relative,’’ in Proc. Adv. Neural\nInf. Process. Syst., A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds.\n2022, pp. 1–14.\n[55] N. Carlini, M. Jagielski, N. Papernot, A. Terzis, F. Tramer, and C. Zhang,\n‘‘The privacy onion effect: Memorization is relative,’’ in Proc. 3rd Theory\nCryptogr. Conf. Theory Cryptogr. (TCC), New York, NY, USA, Mar. 2006.\n[56] C. Dwork, ‘‘Differential privacy: A survey of results,’’ in Proc. 5th Int.\nConf. Theory Appl. Models Comput. (TAMC). Xi’an, China: Springer,\nApr. 2008, pp. 1–19.\n[57] M. Nasr, S. Songi, A. Thakurta, N. Papernot, and N. Carlin, ‘‘Adver-\nsary instantiation: Lower bounds for differentially private machine\nlearning,’’ in Proc. IEEE Symp. Secur. Privacy (SP), May 2021,\npp. 866–882.\n[58] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,\nK. Talwar, and L. Zhang, ‘‘Deep learning with differential pri-\nvacy,’’ in Proc. ACM SIGSAC Conf. Comput. Commun. Secur. , 2016,\npp. 308–318.\n[59] X. Yuan and L. Zhang, ‘‘Membership inference attacks and defenses in\nneural network pruning,’’ in Proc. 31st USENIX Secur. Symp. (USENIX\nSecur.), Boston, MA, USA, Aug. 2022, pp. 4561–4578.\n[60] M. Nasr, R. Shokri, and A. Houmansadr, ‘‘Machine learning with mem-\nbership privacy using adversarial regularization,’’ in Proc. ACM SIGSAC\nConf. Comput. Commun. Secur., Oct. 2018, pp. 634–646.\n[61] A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes,\n‘‘ML-leaks: Model and data independent membership inference attacks\nand defenses on machine learning models,’’ in Proc. Netw. Distrib. Syst.\nSecur. Symp., 2019, pp. 1–15.\n[62] C. Kowalski, A. Famili, and Y. Lao, ‘‘Towards model quantization on the\nresilience against membership inference attacks,’’ in Proc. IEEE Int. Conf.\nImage Process. (ICIP), Oct. 2022, pp. 3646–3650.\n[63] G. Hinton, O. Vinyals, and J. Dean, ‘‘Distilling the knowledge in a neural\nnetwork,’’ 2015, arXiv:1503.02531.\n[64] X. Tang, S. Mahloujifar, L. Song, V. Shejwalkar, M. Nasr, A. Houmansadr,\nand P. Mittal, ‘‘Mitigating membership inference attacks by self-\ndistillation through a novel ensemble architecture,’’ in Proc. 31st\nUSENIX Secur. Symp. (USENIX Secur.), Boston, MA, USA, Aug. 2022,\npp. 1433–1450.\n[65] J. Zheng, Y. Cao, and H. Wang, ‘‘Resisting membership inference attacks\nthrough knowledge distillation,’’ Neurocomputing, vol. 452, pp. 114–126,\nSep. 2021.\n[66] J. Jia, A. Salem, M. Backes, Y. Zhang, and N. Z. Gong, ‘‘MemGuard:\nDefending against black-box membership inference attacks via adversarial\nexamples,’’ in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., 2019,\npp. 259–274.\n[67] Z. Yang, B. Shao, B. Xuan, E.-C. Chang, and F. Zhang, ‘‘Defending model\ninversion and membership inference attacks via prediction purification,’’\n2020, arXiv:2005.03915.\n[68] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia,\nA. Travers, B. Zhang, D. Lie, and N. Papernot, ‘‘Machine\nunlearning,’’ in Proc. IEEE Symp. Secur. Privacy (SP), May 2021,\npp. 141–159.\n[69] A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh, ‘‘Remember what\nyou want to forget: Algorithms for machine unlearning,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 34, 2021, pp. 18075–18086.\n[70] W. Fedus, B. Zoph, and N. Shazeer, ‘‘Switch transformers: Scaling to\ntrillion parameter models with simple and efficient sparsity,’’ J. Mach.\nLearn. Res., vol. 23, no. 120, pp. 1–39, 2022.\n[71] E. Kharitonov, M. Baroni, and D. Hupkes, ‘‘How BPE affects memoriza-\ntion in transformers,’’ 2021, arXiv:2110.02782.\n[72] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,\nW. Li, and P. J. Liu, ‘‘Exploring the limits of transfer learning with a unified\ntext-to-text transformer,’’ 2019, arXiv:1910.10683.\n[73] F. Yergeau, ‘‘UTF-8, a transformation format of ISO 10646,’’ Tech. Rep.,\n2003.\nMYUNG GYO OH (Graduate Student Member,\nIEEE) received the B.S. degree in computer and\ninformation security and mathematics from Sejong\nUniversity, Seoul, South Korea, in 2022. He is cur-\nrently pursuing the M.S. degree with the Graduate\nSchool of Information, Yonsei University, under\nthe supervision of Taekyoung Kwon. His research\ninterest includes solving security problems (confi-\ndentially and integrity) in deep learning systems.\nLEO HYUN PARK (Graduate Student Mem-\nber, IEEE) received the B.S. degree in computer\nengineering from Kwangwoon University, Seoul,\nSouth Korea, in 2017. He is currently pursuing\nthe Ph.D. degree in information security with\nYonsei University, Seoul. His research interests\ninclude adversarial robustness, testing deep neural\nnetworks, adversarial machine learning, malware\nanalysis, and usable security.\n10216 VOLUME 11, 2023\nM. G. Oh et al.: Membership Inference Attacks With Token-Level Deduplication on Korean Language Models\nJAEUK KIM received the B.S. degree in infor-\nmation communication from Semyung University,\nJecheon, South Korea, in 2018. He is currently pur-\nsuing the M.S. degree in information systems with\nYonsei University. His research interests include\nAI security, adversarial machine learning, and\nmachine learning algorithms.\nJAEWOO PARK received the B.S. degree in\nelectronics engineering from Kyungpook National\nUniversity, Daegu, South Korea, in 2022. He is\ncurrently pursuing the M.S. degree in infor-\nmation security with Yonsei University, Seoul,\nSouth Korea. His research interests include adver-\nsarial robustness, testing deep neural networks,\nand adversarial machine learning.\nTAEKYOUNG KWON(Member, IEEE) received\nthe B.S., M.S., and Ph.D. degrees in computer sci-\nence from Yonsei University, Seoul, South Korea,\nin 1992, 1995, and 1999, respectively.\nFrom 1999 to 2000, he was a Postdoctoral\nResearch Fellow at the University of California at\nBerkeley, Berkeley, CA, USA. From 2001 to 2013,\nhe was a Professor of computer engineering at\nSejong University, Seoul. He is currently a Profes-\nsor of information security with Yonsei University,\nwhere he is also the Director of the Information Security Laboratory. His\nresearch interests include authentication, cryptographic protocols, system\nsecurity, fuzzing, usable security, AI security, adversarial robustness, and\nadversarial machine learning.\nVOLUME 11, 2023 10217"
}