{
  "title": "SHIELD: an evaluation benchmark for face spoofing and forgery detection with multimodal large language models",
  "url": "https://openalex.org/W4410974031",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2433608285",
      "name": "Yichen Shi",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2055865417",
      "name": "Yuhao Gao",
      "affiliations": [
        "Shijiazhuang Tiedao University"
      ]
    },
    {
      "id": "https://openalex.org/A2108050831",
      "name": "Ying-xin Lai",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2106145486",
      "name": "Hongyang Wang",
      "affiliations": [
        "Shijiazhuang Tiedao University"
      ]
    },
    {
      "id": "https://openalex.org/A2012120371",
      "name": "Jun Feng",
      "affiliations": [
        "Shijiazhuang Tiedao University"
      ]
    },
    {
      "id": "https://openalex.org/A2122874018",
      "name": "Lei He",
      "affiliations": [
        "UCLA Health",
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2044030749",
      "name": "Jun Wan",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2100330470",
      "name": "Changsheng Chen",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2726404128",
      "name": "Zitong Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113913003",
      "name": "Xiaochun Cao",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2433608285",
      "name": "Yichen Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2055865417",
      "name": "Yuhao Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108050831",
      "name": "Ying-xin Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106145486",
      "name": "Hongyang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2012120371",
      "name": "Jun Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122874018",
      "name": "Lei He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2044030749",
      "name": "Jun Wan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100330470",
      "name": "Changsheng Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2726404128",
      "name": "Zitong Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113913003",
      "name": "Xiaochun Cao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3204248352",
    "https://openalex.org/W3135925326",
    "https://openalex.org/W2174309130",
    "https://openalex.org/W2551249768",
    "https://openalex.org/W6674016869",
    "https://openalex.org/W6717588342",
    "https://openalex.org/W2998570087",
    "https://openalex.org/W2794969028",
    "https://openalex.org/W6765409617",
    "https://openalex.org/W2787613668",
    "https://openalex.org/W2778720069",
    "https://openalex.org/W2962367118",
    "https://openalex.org/W3094861582",
    "https://openalex.org/W3174133171",
    "https://openalex.org/W3014566885",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4221167490",
    "https://openalex.org/W3103670834",
    "https://openalex.org/W4285186923",
    "https://openalex.org/W4285601702",
    "https://openalex.org/W4225678970",
    "https://openalex.org/W3175452902",
    "https://openalex.org/W4390692050",
    "https://openalex.org/W4313127140",
    "https://openalex.org/W6794693742",
    "https://openalex.org/W3110828520",
    "https://openalex.org/W4226024856",
    "https://openalex.org/W6791449262",
    "https://openalex.org/W3042844210",
    "https://openalex.org/W3012033723",
    "https://openalex.org/W3048341421",
    "https://openalex.org/W6772330936",
    "https://openalex.org/W4402127110",
    "https://openalex.org/W6851578965",
    "https://openalex.org/W4389253150",
    "https://openalex.org/W4399618569",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W6729503815",
    "https://openalex.org/W4402716381",
    "https://openalex.org/W4402726948",
    "https://openalex.org/W4403943260",
    "https://openalex.org/W2946566846",
    "https://openalex.org/W4293112709",
    "https://openalex.org/W2912336782",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4383292642",
    "https://openalex.org/W4221146489",
    "https://openalex.org/W2952476201",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W2128377844",
    "https://openalex.org/W4387226214",
    "https://openalex.org/W4404782964",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W2914304175",
    "https://openalex.org/W2623012778",
    "https://openalex.org/W4200283578",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W4200333562"
  ],
  "abstract": "Abstract Multimodal large language models (MLLMs) have demonstrated strong capabilities in vision-related tasks, capitalizing on their visual semantic comprehension and reasoning capabilities. However, their ability to detect subtle visual spoofing and forgery clues in face attack detection tasks remains underexplored. In this paper, we introduce a benchmark, SHIELD , to evaluate MLLMs for face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to assess MLLM performance on multimodal face data across two tasks. For the face anti-spoofing task, we evaluate three modalities (i.e., RGB, infrared, and depth) under six attack types. For the face forgery detection task, we evaluate GAN-based and diffusion-based data, incorporating visual and acoustic modalities. We conduct zero-shot and few-shot evaluations in standard and chain of thought (COT) settings. Additionally, we propose a novel multi-attribute chain of thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images. The findings of this study demonstrate that MLLMs exhibit strong potential for addressing the challenges associated with the security of facial recognition technology applications.",
  "full_text": "Visual\nIntelligence\nShietal. VisualIntelligence             (2025) 3:9 \nhttps://doi.org/10.1007/s44267-025-00079-w\nRESEARCH OpenAccess\nSHIELD:anevaluationbenchmarkforface\nspooﬁngandforgerydetectionwith\nmultimodallargelanguagemodels\nYichenShi1†,YuhaoGao 2†,YingxinLai 3†,HongyangWang 2,JunFeng 2,LeiHe 4,JunW an5,\nChangshengChen6,ZitongY u7* andXiaochunCao 8\nAbstract\nMultimodallargelanguagemodels(MLLMs)havedemonstratedstrongcapabilitiesinvision-relatedtasks,\ncapitalizingontheirvisualsemanticcomprehensionandreasoningcapabilities.However,theirabilitytodetect\nsubtlevisualspooﬁngandforgerycluesinfaceattackdetectiontasksremainsunderexplored.Inthispaper,we\nintroduceabenchmark, SHIELD,toevaluateMLLMsforfacespooﬁngandforgerydetection.Speciﬁcally,wedesign\ntrue/falseandmultiple-choicequestionstoassessMLLMperformanceonmultimodalfacedataacrosstwotasks.For\nthefaceanti-spooﬁngtask,weevaluatethreemodalities(i.e.,RGB,infrared,anddepth)undersixattacktypes.Forthe\nfaceforgerydetectiontask,weevaluateGAN-basedanddiﬀusion-baseddata,incorporatingvisualandacoustic\nmodalities.Weconductzero-shotandfew-shotevaluationsinstandardandchainofthought(COT)settings.\nAdditionally,weproposeanovelmulti-attributechainofthought(MA-COT)paradigmfordescribingandjudging\nvarioustask-speciﬁcandtask-irrelevantattributesoffaceimages.TheﬁndingsofthisstudydemonstratethatMLLMs\nexhibitstrongpotentialforaddressingthechallengesassociatedwiththesecurityoffacialrecognitiontechnology\napplications.\nKeywords: Faceanti-spooﬁng(FAS),Faceforgerydetection,Multimodallargelanguagemodels(MLLMs),\nMulti-attributechainofthought(MA-COT)\n1 Introduction\nDespite signiﬁcant progressin faceanti-spooﬁng and face\nforgery detection, most research still concentrates on de-\nveloping models for speciﬁc scenarios or types of attacks,\noften relying on subtle facial changes. These models typi-\ncallyfocusonasinglemodalityoraspeciﬁckindofspoof-\ning attack, which limits their adaptability to a broader\nand more diverse range of attack scenarios. Traditional\napproaches in face anti-spooﬁng and face forgery detec-\ntion primarily rely on identifying forgery clues across dif-\n*Correspondence:yuzitong@gbu.edu.cn\n7SchoolofComputingandInformationTechnology,GreatBayUniversity,\nDongguan,Guangdong,523000,China\nFulllistofauthorinformationisavailableattheendofthearticle †Equal\ncontributors\nferent modalities, such as spatial and frequency domains\n[1, 2], with models speciﬁcally trained to recognize these\ncues.Incontrast,currentmultimodallargelanguagemod-\nels (MLLMs) [3, 4] do not utilize such task-speciﬁc fea-\nturesandhavenotbeenﬁne-tunedwithdedicatedtraining\ndata for face anti-spooﬁng (FAS) and face forgery detec-\ntion.Instead,theyarelargelytrainedongenericquestion-\nanswer pairs and lack datasets tailored to the nuances of\nFAS and face forgery detection tasks. To bridge this gap,\nthis study explores the capabilities of MLLMs in these\ntasksandhighlightsareasforpotentialﬁne-tuning.\nWe introduce a new benchmark, SHIELD, to evaluate\nthe ability of MLLMs in face spooﬁng and forgery detec-\ntion.Speciﬁcally,wedesigntrue/falseandmultiple-choice\nquestionstoevaluateMLLMperformanceonmultimodal\n©TheAuthor(s)2025. OpenAccess ThisarticleislicensedunderaCreativeCommonsAttribution4.0InternationalLicense,which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The\nimagesorotherthirdpartymaterialinthisarticleareincludedinthearticle’sCreativeCommonslicence,unlessindicatedotherwise\nin a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not\npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright\nholder.Toviewacopyofthislicence,visit http://creativecommons.org/licenses/by/4.0/.\nShietal. VisualIntelligence             (2025) 3:9 Page 2 of 25\nface data across these two face security tasks. For the face\nanti-spooﬁngtask,weevaluatethreemodalities(i.e.,RGB,\ninfrared,anddepth)undersixtypesofpresentationattacks\n(i.e., print attack, replay attack, rigid mask, paper mask,\nﬂexiblemaskandfakehead).Forthefaceforgerydetection\ntask, we evaluate generative adversarial network (GAN)-\nbased and diﬀusion-based data, incorporating both vi-\nsual and acoustic modalities. Since our benchmark cur-\nrentlylackstask-speciﬁcﬁne-tuningdataforFASandface\nforgery tasks, we conduct preliminary tests under zero-\nshot and few-shot settings using standard prompts and\nchainofthought(COT)promptstoestablishbaselineper-\nformance.\nTheoverallperformanceoftheseMLLMsisdepictedin\nFig.1.TheresultsdemonstratethatMLLMsholdsubstan-\ntialpromiseforfacialsecuritytasks.Furthermore,theno-\ntable performance diﬀerences caused by changes in input\nmodalities highlight the need for further research to fully\nunderstand the limitations and potential of these models.\nAdditionally, we propose a novel multi-attribute chain of\nthought (MA-COT) paradigm for describing and judging\nvarious task-speciﬁc and task-irrelevant attributes of face\nimages. This paradigm provides rich task-related knowl-\nedge,facilitatingthedetectionofsubtlespooﬁngorforgery\nclues.\nOurcontributionsarethreefold:\n1) We introduce anewbenchmarkforevaluating the\neﬀectivenessof MLLMsin addressingvarious\nchallengeswithinthedomain offace security,\nincluding bothface anti-spooﬁngandfaceforgery\ndetection.\n2) We proposeanovel MA-COTparadigm, which\nprovides rich task-relatedknowledgeforMLLMs\nandimproves theinterpretability ofthe\ndecision-makingprocessforface spooﬁngand\nforgerydetection.\n3) Throughextensiveexperiments,we ﬁndthat1)\nMLLMs(e.g., GPT4V andGemini) have potential\nreal/fake reasoningcapability forunimodal and\nmultimodal face spooﬁngandforgery detection;2)\ntheproposedMA-COTcanimprove therobustness\nandinterpretability forface attackdetection.\n2 Relatedwork\n2.1 Faceanti-spooﬁng\nIntheearlystage,faceanti-spooﬁngsystemsreliedontra-\nditional techniques. These methods mainly focused on\nhandcrafted features [5–8] and heuristics designed to de-\ntectpresentationattacks.Theywereeﬀectiveagainstbasic\nspooﬁngattemptsbutoftenfailedtogeneralizetomoreso-\nphisticated and diverse attack scenarios. With the advent\nofdeeplearning,therewasasigniﬁcantshiftinFASmeth-\nods. Deep learning-based techniques [9–15], particularly\nthose employing convolutional neural networks (CNNs)\nwithadvancedlearningstrategies(e.g.,meta-learning[ 16],\ndisentangledlearning[ 17],andfew-shotlearning[ 9]),have\nbecome dominant in recent years. These methods lever-\naged large-scale datasets and powerful computational re-\nsources to learn more complex and subtle features of gen-\nuine and spoofed faces. This shift marked a substantial\nimprovement over traditional methods in both detection\naccuracy and generalization capabilities. Recently, FAS\nmethodshavestartedtobeintegratedintolargerandmul-\ntimodal models with vision transformers (ViTs) [18]. On\ntheonehand,ViTisadoptedinthespatialdomain[ 19–22]\nto explore relationships between live and spoof patches\nat a local level. On the other hand, global features such\nas temporal abnormity [23] or physiological periodicity\n[24,25] are extracted by applying ViT in the temporal do-\nmain. Despite convincing performance via unimodal and\nmultimodal deep learning methods based on CNNs and\nViTs, there is currently no work exploring the potential of\nMLLMsforgeneralizedFAStasks.\n2.2 Faceforgerydetection\nPreviousstudieshavepredominantlyapproachedthistask\nas a binary classiﬁcation problem [26–28]. CNNs are\ncommonlyemployedtoextracthigh-dimensionalfeatures\nfrom input face images and then classify them as either\nreal or fake. While these models exhibit commendable\nperformance on in-domain datasets due to their utiliza-\ntionoffacialfeatures,theirperformanceoncross-domain\ndatasetsremainsinadequate.Toovercomethislimitation,\nrecent studies have explored alternative approaches, in-\ncluding the utilization of low-frequency information in\nfrequency domain analysis. For instance, noise-based ap-\nproaches[29,30],frequencydomainanalysis[ 31–33],and\nlocal relation learning methods [1, 27, 34]h a v eb e e ni n -\nvestigated. These methods show promising results by in-\ncorporatingadditionalinformationbeyondtraditionalfre-\nquencydomainorconvolutionalnetwork-basedmethods.\nHowever, it is important to note that real-world image\nforgeryscenariosarecharacterizedbydiversepatternsand\nunknown environmental conditions. The original tech-\nniques based on frequency domain information or con-\nvolutional networks tend to overﬁt existing image forgery\ntechniques in the training set, leading to signiﬁcant per-\nformancedegradationforthesemethods[ 2,35].Recently,\nvision foundation models (e.g., segment anything models\n[36])havebeensuccessfullyintroducedinfaceforgeryde-\ntection [37], demonstrating their strong attck localization\ncapabilities. Given the rapid evolution of face forgery at-\ntacksandtheneedforrobustdetectionmethods,thepow-\nerfulzero-shotgeneralizationcapacityofmultimodallarge\nlanguage models (MLLMs) makes them a promising av-\nenue for exploration. This study aims to evaluate whether\nMLLMs can eﬀectively and robustly detect diverse face\nforgeryattacks.Additionally,cross-domaingeneralization\nShietal. VisualIntelligence             (2025) 3:9 Page 3 of 25\nFigure1 Performanceofvariousmultimodallargelanguagemodelson(a)true/falseand(b)multiple-choicequestionsacrossdiﬀerenttypesof\nattacks.Theterm“bonaﬁde”isusedtodenoteagenuinefaceimage.Printreferstoaprintedphotograph,andreplayreferstoareplayedvideo.This\ndemonstratestheirsuperiorabilitytodistinguishbetweenphysicalanddigitalattacks.In(a),thelargertheareaofeachcoloredpolygon,thebet ter\ntheperformance.Qwen-VLandmPLUG-Owloutperformothermodels.In(b),GPT4Vshowsthebestperformancecomparedtotheothers.Thecolor\nbrownrepresentsthefaceanti-spooﬁngtask,deepbluerepresentsthefaceforgerydetectiontask,andorangerepresentsthejointtask.AVG:\naverage\ntechniques,inspiredbyrelatedﬁeldssuchasspeechspoof-\ning detection, have been explored to enhance robustness\nundervarying conditions[ 38].\n2.3 Multimodallargelanguagemodel\nMLLMs have developed at a rapid pace over the past few\nyears due to signiﬁcant contributions from various re-\nsearch teams. Alayrac et al. [39] introduced the Flamingo\nmodel, a signiﬁcant advancement in processing inter-\nleaved visual data and text, focusing on generating free-\nform text output. Following this, Li et al. [40]d e v e l o p e d\nBLIP-2, a model characterized by its resource-eﬃcient\nframework and the innovative Q-Former, which notably\nleverages frozen LLMs for eﬃcient image-to-text gen-\neration. In a similar vein, Dai et al. [41]f u r t h e rr e ﬁ n e d\nthis approach with Instruct BLIP, a model trained on the\nBLIP-2 framework, speciﬁcally enhancing the capabil-\nity of instruction-aware visual feature extraction. Devel-\noped by OpenAI, GPT4V [42]i t s e l fr e p r e s e n t sal e a pf o r -\nward in MLLMs, oﬀering versatility in generating coher-\nent and context-aware text. It is applicable in standalone\ntext-based applications as well as multimodal settings. Its\nsuperior performance establishes a dominant position in\nterms of versatility and general applicability. Zhu et al.\n[43] introduced MiniGPT-4, a streamlined approach that\naligns apre-trainedvision encoderwithLLMsbytraining\na single linear layer, eﬀectively replicating the capabilities\nof GPT4V. To extend the linguistic versatility, Bai et al.\n[44]p r e s e n t e dQ w e n - V L ,am u l t i l i n g u a lM L L Ms u p p o r t -\ning both English and Chinese, with the ability to process\nmultipleimagesduringtraining.Intheareaofmultimodal\ndatascarcity,Liuetal.[ 45]pioneeredLLaVAbyintroduc-\ninganovelopen-sourcemultimodalinstruction-following\ndatasetalongsidetheLLaVA-Benchbenchmark.Recently,\nGoogle’s newly introduced Gemini [4] represents a sig-\nniﬁcant advancement in the intrinsic multimodal charac-\nteristics of artiﬁcial intelligence(AI) systems. It processes\ntext and various audiovisual inputs, generating outputs in\nmultiple formats, thereby demonstrating the eﬃciency of\nintegrated multimodal interactions. Gemini also demon-\nstrates exceptional multimodal interaction eﬃciency and\ngeneralization capabilities, posing a signiﬁcant challenge\nto GPT4V [3]. Despite their remarkable problem-solving\ncapabilities in various vision domains (e.g., generic object\nrecognition and grounding), whether MLLMs are sensi-\nShietal. VisualIntelligence             (2025) 3:9 Page 4 of 25\ntive to subtle visual spooﬁng clues and how they perform\ninthefaceattackdetectiondomainremainsunexplored.\n2.4 ExistingMLLMbenchmark\nIn recent years, benchmarks have been developed for\nMLLMs to evaluate their performance on various tasks.\nA notable benchmark in this regard is the multimodal\nmodel evaluation (MME) [46]. The MME benchmark fo-\ncuses on a wide range of tasks to assess the capabilities\nofdiﬀerentMLLMs,includingbothperceptualandcogni-\ntivetasks,coveringareassuchasobjectrecognition,com-\nmon sense reasoning, numerical computation, text trans-\nlation,andcodereasoning.Thisbenchmarkevaluatessev-\neral advanced MLLMs, such as BLIP-2 [40], MiniGPT-4\n[43], and mPLUG-Owl [47], using accuracy (ACC) and\nhalf total error rate (HTER) as metrics. The former is cal-\nculated based on the correct response to each question,\nwhile the latter is a stricter measure that requires correct\nanswerstobothquestionsassociatedwitheachimage.Be-\nsides, several benchmarks fordiﬀerent perspectives (hier-\narchicalcapabilities[ 48],hallucination[ 49],andstyle-shift\nrobustness [50]) and diﬀerent tasks (e.g., low-level image\nenhancement[51]andimagequalityassessment[ 52])have\nalso been established. In terms of face attack detection\ntasks, eﬀorts are still needed to qualitatively and quanti-\ntatively evaluate sets of prompts and MLLMs, and to es-\ntablishafairevaluationbenchmark.\n3 Taskdesign\nWe establish diﬀerent true/false sub-tasks to assess the\nability of MLLMs to discriminate between real faces and\nspooffaces.Thesub-tasks areasfollows:\n1) Zero-shottesting:<image>,question:Isthisimagea\nreal face?Pleaseansweryesorno!\n2) Zero-shottesting (COT): <image>, question: Isthis\nimage a real face?Pleasedescribe theimage and\nansweryesorno!\n3) Few-shot testing: <image,image>, question: The\nﬁrstimage is a realface,is thesecondimage a real\nface?Pleaseansweryesorno!\n4) Few-shottesting (COT):<image, image>, question:\nT heﬁr sti mag ei sar ealfac e,i st hesec ondi mag ea\nreal face?Pleasedescribe theimage andansweryes\norno!\nMultiple-choice questions. We create several multiple-\nchoicesub-taskstoassesstheabilityofMLLMstoperceive\nandunderstandthedistinctionsbetweenmultiplefaceim-\nages.Thesub-tasksarepresentedasfollows:\n1) Zero-shottesting: <image×4>, question: The\nfollowing images are listedin theorderA, B,C,D.\nPleaseanswertheletter numberofA, B,C, orD\ncorrespondingto theimage oftherealface.\n2) Zero-shottesting (COT):<image×4>, question:\nThefollowing images are listedin theorderA, B,C,\nD.Pleasedescribe theimages andanswertheletter\nnumberofA, B,C,orDcorresponding to theimage\nofthereal face.\n3) Few-shottesting: <image×5>, question: The ﬁrst\nimage is areal face.Thefollowing images are listed\nintheorderA, B,C,D. Pleaseanswertheletter\nnumberofA, B,C,orDcorresponding to theimage\nofthereal face.\n4) Few-shottesting (COT):<image×5>, question: The\nﬁrstimage is areal face.Thefollowing images are\nlistedin theorderA, B,C,D.Please describethe\nimages andanswertheletternumberofA, B,C,or\nDcorresponding to theimage of thereal face.\n4S H I E L D\n4.1 Datacollection\nCollection of face anti-spooﬁng datasets.A ss h o w ni nT a -\nble 1,w ec o n d u c t e de x p e r i m e n t su s i n gt h eW M C A[53]\nand SiW-Mv2 [54] datasets, which include a variety of\npresentation attacks and multiple modalities (e.g., RGB\nimages, depth maps, infrared images, and thermal imag-\ning). For each identity (ID), we selected six types of at-\ntacksandthebonaﬁdefaces,alongwiththeircorrespond-\ning RGB images, depth maps, and infrared images for\nexperiments. Typical examples are presented in the ﬁrst\nrow of Fig.2. Collection of face forgery detection datasets.\nFor face forgery detection, we evaluated MLLMs on the\npopular FaceForencics++ (FF++) [55]d a t a s e t ,w h i c hi n -\ncludes four types of forgery techniques (i.e., Deepfakes,\nFace2Face, FaceSwap, and Nulltextures). Additionally, as\nAI-generated content (AIGC) has rapidly advanced, the\nresulting face images have become increasingly realistic.\nTo account for this, we also evaluated AIGC-based face\ndatausingtheDFFdataset[ 56],generatedbyStableDiﬀu-\nsion, Inpainting, and InsightFace techniques. The images\nTable 1Datastatistics.FAS:faceanti-spooﬁng.Theterm“bona\nﬁde”isusedtodenoteagenuinefaceimage.Printreferstoa\nprintedphotograph,andreplayreferstoareplayedvideo\nTask Dataset Attacktype Count\nFAS WMCA(352)\nSiw_MV2(348)\nBonaﬁde 100\nFakehead 100\nFlexiblemask 100\nPapermask 100\nPrint 100\nReplay 100\nRigidmask 100\nDeepfake FF++(500)\nDFF(300)\nReal 200\nDeepfakes 100\nFace2Face 100\nFaceSwap 100\nInsightFace 100\nNeuralTextures 100\nStable_Diﬀusion 100\nShietal. VisualIntelligence             (2025) 3:9 Page 5 of 25\nFigure2 Examplesofourcollecteddatasets.TheimagesaresourcedfromtheWMCA[ 53]andtheFF++[ 55]datasets\nFigure3 Pipelineoftaskdesign.Theellipsesindicatethatthestructuresareconsistentwiththetaskdesignframeworkshownabove.COT:chainof\nthought\nwere chosen using “The Dawn of LMMs: Preliminary Ex-\nplorations with GPT4V” [39] as a guide, and our sample\nissimultaneouslyrepresentativeanddiverse.Typicalsam-\nplesareshowninthesecondrowofFig. 2.\n4.2 Taskdesign\nAsshowninFig. 3,wedesignedtwotaskstotestthecapa-\nbilities of MLLMs in the ﬁeld of facial security: true/false\nquestionsandmultiple-choicequestions.Foreachtask,we\nconducted zero-shot tests and in-context few-shot tests.\nForeachtypeoftest,weexperimentedwithbothstandard\nsettings and COT settings. The overall pipeline of the ex-\nperimen tiss hownint helowerhalfofF ig.4.\n4.3 MA-COT\nChain of thought (COT) technology represents a recent\nadvancement in prompt learning, primarily designed to\nenhancethereasoningcapabilities ofMLLMs[ 57].Inthis\nstudy, we propose a novel COT paradigm, termed MA-\nCOT.Thisapproachdrawsinspirationfrompriorworkin\nvisual COT [58], where an image is ﬁrst described in de-\ntail and subsequently subjected to judgment. Speciﬁcally,\nin the domains of face anti-spooﬁng and deepfake detec-\ntion, we incorporate relevant prior knowledge to improve\ntheanalysisprocess.\nUnliketraditionalCOTmethodsthatdescribetheentire\nimage,ourMA-COTparadigmemphasizestheconsidera-\ntionofmultipleattributesofanimage,suchasshape,color,\nandtexture,toprovideamorein-depthanalysisandjudg-\nment. The attributes selected for MA-COT are carefully\nchosenbasedontheirhigh-levelrepresentationalcapacity\ntocaptureessential featuresoffaceforgery.\nThe analysis results of individual attributes are synthe-\nsized into a comprehensive assessment, increasing both\ntheaccuracyandreliabilityoftheoveralldecision-making\nprocess. Additionally, this approach mitigates instances\nwherethemodelmightotherwiserefusetoprovideanan-\nswer, ensuringamorerobustandinterpretableoutcome.\nI nM A - C O T ,w eh a v eas e to ft a s k s :{task\n1,task2,...,\ntaskn}.E a c ht a s ki is associated with a set of attributes:\n{attri1,attri2,...,attr im}.Foreachattribute,weobtainade-\nscription, forming a multi-attribute description set\nDescriptionattr.Basedonthisset, wegeneratetheﬁnalan-\nswer:\nP(Answer|Image,Question,Description attr).( 1 )\nThe selected attributes encompass both shared and task-\nspeciﬁc characteristics, with shared attributes facilitating\nShietal. VisualIntelligence             (2025) 3:9 Page 6 of 25\nFigure4 TheMA-COTprocess.Thisprocessisdesignedtoextractrelevantkeyattributesforvarioustasksandinputtheseattributesalongwiththe\nfaceimagesunderevaluationintoMLLMs.AllattributesweusedareshowninTable 2.ThisapproachaimstoguidetheMLLMtoanalyzetheimages\nfrommultipleperspectives,therebyidentifyingpotentialcluesofattacksanddeterminingwhethertheimagesarerealfaces.Theillustration\nprovidesexamplesofkeyattributeextractionanditsapplicationscenariosinseparateFAS,separatefaceforgerydetection,anduniﬁedfacespoof\nandforgerydetection.TheimagesaresourcedfromtheWMCA[ 53]andtheFF++[ 55]datasets.MA-COT:multi-attributechainofthought\ndiﬀerentiation between real faces and attacks, while task-\nspeciﬁc attributes improvedetection accuracyfordistinct\nforgeryscenarios.Forthisstudy,theanalysisremainsqual-\nitative, relying on MLLM’s reasoning capabilities rather\nthanexplicitattributelabelswithinthetestdata.Ourpro-\nposed MA-COT framework is designed to be ﬂexibly ex-\npandable, accommodating additional tasks and combina-\ntions of attributes, making it applicable to broader do-\nmains.Fortheuniﬁeddetectiontask,inspiredbyresearch\nindomaingeneralizationanddomainadaptation[ 59–61],\nwe have established both task-shared and task-speciﬁc at-\ntributes,asshowninTable 2.Sharedattributeshelpdistin-\nguish between real faces and various attacks, while task-\nspeciﬁcattributesenhancethedetectioncapabilities.\n5 Experiments\n5.1 Eexperimentalsetup\nWe conducted tests on the most advanced MLLMs cur-\nrently available: BLIP [ 62], BLIP-2 [ 40], Intern [ 63],\nMiniGPT-4 [43], LLaVA [45], QWen-VL [44], Instruct-\nBLIP[41],mPLUG-Owl[ 47],Gemini[ 4]andGPT4V[ 42].\nForclosesourceMLLMs,alltestswereperformedthrough\nAPI calls. We have developed various test scenarios to\nTable 2Attributeset\nAttribute Task\nLocalconsistencyofpixelsandclarity Taskshared\nGlobalshapeconsistencyoffacialfeatures Taskshared\nSenseofdepthandthree-dimensionality Faceanti-spooﬁng\nGlossandreﬂection Faceanti-spooﬁng\nPresenceofphonescreenorpaperedges Faceanti-spooﬁng\nTexturenaturalness Faceforgerydetection\nFaciallighting Faceforgerydetection\nFacialskincolor Faceforgerydetection\nEye-headandearmovements Faceforgerydetection\ncomprehensively evaluate the eﬀectiveness and accuracy\nofthesemodelsinperformingvariousfacialsecuritytasks.\n5.2 Evaluationmetrics\nWechoosethecommonlyusedhalftotalerrorrate(HTER)\n[64]infaceanti-spooﬁngandthecommonACCmetricin\nclassiﬁcationproblemsasthemeasuresfortrue/falseques-\ntions. For multiple-choice questions, we selected ACC as\nthe metric. These metrics were selected based on their\nsuitabilityforbinaryclassiﬁcationtasks,whichalignswith\nthebinarylabelstructureofexistingfaceforgerydatasets.\nGiventhecurrentstageofMLLMsinthisarea,thesemet-\nShietal. VisualIntelligence             (2025) 3:9 Page 7 of 25\nrics provide a straightforward way to benchmark their\nbaselineperformanceandidentifyareasforimprovement.\nTheresultspresentedinSect. 6andSect. 7areaggregated\naverages of all attack types and do not detail the perfor-\nmanceofeachindividual attackby diﬀerentMLLMs.\n6 Resultsonfaceanti-spooﬁng\n6.1 Promptdesign\nAsmentionedinRef.[ 65],usingdiﬀerentdescriptivestate-\nments as image labels to represent real faces and spoof-\ningattackscanaﬀectthemodels’predictions.Asshownin\nFig.5,wetesteddiﬀerentdescriptivemethodsforrealpeo-\nple and spoofs. The results indicate that using the phrase\n“realface”forarealfaceand“spoofface”foraspooﬁngat-\ntackgivesthebesteﬀect.Inaddition,acombinedqueryof\n“Isthisimagearealfaceoraspoofface?Andpleaseanswer\n‘this image is a real face’ or ‘this image is a spoof face”’ is\nmoreeﬀective.\n6.2 UnimodalFAStesting\nRGB-based unimodal face data are most common in face\nanti-spooﬁng tasks, primarily due to their ease of acqui-\nsition. In this study, we conducted a thorough testing of\nsingle-modalityfacialdata,whichincludedbothtrue/false\nand multiple-choice questions. Inspired by the ﬁndings in\nRef. [58], we discovered that the use of the COT tech-\nniqueinvisual-languagetasks(i.e.,ﬁrstdescribingtheim-\nage,andthenmakingadecision)signiﬁcantlyenhancesthe\nmodel’s performance. Accordingly, we carried out stan-\ndardtestingandCOTtesting foreachquestion.\nTrue/false questions. For face anti-spooﬁng, we chose\nt ou s et h ep h r a s e s“ r e a lf a c e ”t or e p r e s e n tar e a lf a c ea n d\n“ s p o o ff a c e ”t os i g n i f yas p o o ﬁ n ga t t a c k .W em a d et h r e e\ntypes of judgment in the decision-making process: inde-\npendent inquiry and combined inquiry. We focus on the\nissueofunimodalauthenticitydetermination.Thecruxof\nthisproblemliesinthein-depthanalysisofonetotwoim-\nages to ascertain whether they are authentic human facial\nimages.\nTable3displaysthecomprehensiveperformanceofvar-\nious MLLMs on true/false questions under zero-shot and\none-shot conditions. While mPLUG-Owl demonstrated\nhighaccuracyacrossalltests,itisnotablethattheone-shot\ncondition without COT did not lead to any performance\nimprovement.AlthoughtheGPT4Vdid notexcelinmost\nof the tests, it achieved consistent results across all tasks,\nparticularlyintheone-shotsetting.FormostMLLMs,us-\ning in-context learning (ICL) as a reference failed to im-\nprove performance, whereas employing COT techniques\noften resulted in enhanced outcomes.\nFigure5 Promptdesign.Thediagramrepresentsamatrixoftestresultsforselectingprompts.Ontheleftarethecandidateprompts,andalongthe\ntoparethetestcasesusedtoevaluatetheselectionofprompts,whichincluderealface,rigidmaskattack,replayattack,papermaskattackand\nﬂexiblemaskattack.TheresponsesfromGPT4VandGeminiareincluded.Yellow(red)highlightsthecorrect(incorrect)responses.Theimagesare\nsourcedfromtheWMCA[ 53]andtheFF++[ 55]datasets\nShietal. VisualIntelligence             (2025) 3:9 Page 8 of 25\nTable 3Performancemetricsofvariousmultimodallargelanguagemodels(MLLMs)onfaceanti-spooﬁng(FAS)true/falsequestions.\n↑/↓ indicatesthathigher/lowerscoresarebetter\nModel Zero-shot One-shot Zero-shot(COT) One-shot(COT)\nACC(%)↑ HTER(%)↓ ACC(%)↑ HTER(%)↓ ACC(%)↑ HTER(%)↓ ACC(%)↑ HTER(%)↓\nBLIP[62] 18.4 47.6 43.6 32.9 25.9 43.3 74.7 14.8\nBLIP-2[40] 14.3 50.0 9.0 68.5 14.3 50.0 0.4 98.5\nIntern[63] 57.6 24.8 14.3 50.0 56.4 25.4 17.7 48.0\nMiniGPT-4[43] 20.6 49.3 28.9 56.9 27.7 43.4 31.0 65.7\nLLaVA[45] 14.3 50.0 14.3 50.0 14.3 50.0 5.4 81.0\nQWen-VL[44] 14.3 50.0 14.3 50.0 14.3 50.0 14.3 50.0\nInstructBLIP[41] 23.7 44.5 17.1 48.3 42.3 33.7 15.9 49.1\nmPLUG-Owl[47] 82.0 10.9 53.0 60.3 81.7 11.5 82.6 42.7\nGemini[4] 73.4 15.5 62.0 36.3 77.0 14.7 13.9 90.3\nGPT4V[42] 53.2 27.7 66.8 28.8 68.7 18.1 50.3 36.8\nTable 4AccuracyofvariousMLLMsonFASmultiple-choicequestions\nModel ACC(%)\nZero-shot(real) One-shot(real) Zero-shot(attack) One-shot(attack) AVG\n- COT - COT - COT - COT - COT\nBLIP[62] 24.0 24.0 5.0 30.0 25.0 32.0 2.0 18.0 14.0 26.0\nBLIP-2[40] 10.0 0.0 20.0 0.0 11.0 0.0 4.0 0.0 11.3 0.0\nIntern[63] 21.0 3.0 20.0 5.0 24.0 0.0 5.0 0.0 17.5 2.0\nMiniGPT-4[43] 2.0 0.0 1.0 1.0 2.0 0.0 1.0 0.0 1.5 0.3\nLLaVA[45] 22.0 4.0 26.0 18.0 9.0 1.0 26.0 8.0 20.8 7.8\nQWen-VL[44] 3.0 0.0 26.0 0.0 9.0 0.0 27.0 0.0 16.3 0.0\nInstructBLIP[41] 21.0 24.0 0.0 0.0 6.0 7.0 8.0 1.0 8.8 8.0\nmPLUG-Owl[47] 23.0 5.0 26.0 1.0 5.0 1.0 27.0 0.0 20.3 1.8\nGemini[4] 30.0 24.2 15.0 18.0 25.6 24.0 22.2 7.1 23.3 18.8\nGPT4V[42] 42.1 60.9 20.4 54.2 44.3 42.9 25.0 40.6 33.1 51.3\nWhen analyzing MLLMs’ performance on FAS tasks, it\nbecomes evident that adding context in one-shot settings\nd o e sn o ta l w a y sl e a dt oi m p r o v e da c c u r a c y .S o m em o d e l s\nstruggletoeﬀectivelyintegratenewinformation,resulting\nin cognitive overload or confusion, which can negatively\nimpact their predictions. This phenomenon is evident in\ncases where models perform well under zero-shot condi-\ntions but show reduced accuracy when additional context\nisprovided.Suchoutcomessuggestlimitationsinthemod-\nels’reasoningcapabilitiesortheirabilitytoeﬀectivelypro-\ncessandapplynew information.\nMoreover,manymodelshadaccuraciesoflessthan50%\nacross diﬀerent test conditions, which can primarily be\nattributed to systematic biases or inherent misjudgments\nratherthanrandomguessing.Thisindicatesthatthecom-\nplexityoftheFAStaskmayexceedthecognitiveprocessing\ncapabilitiesofcertainmodels,leadingtosystematicerrors\nin speciﬁc scenarios. Additionally, some models showed\nidentical accuracies across multiple conditions. This con-\nsistency may stem from dataset homogeneity, similarities\ninmodelarchitecture,orlimitationsinthetrainingdata.\nMultiple-choice questions. We also test face anti-\nspooﬁngintheformofanimageselectiontask,withmul-\ntiple images as input, and ask the model to identify which\nimageisarealpersonorwhichoneisaspooﬁngattack.We\nconductbothzero-shotandfew-shottests,eachincluding\nstandardexperimentsandCOTexperiments.Wefocuson\nan in-depth exploration of the unimodal multiple-choice\nproblem. The fundamental task is to perform a detailed\nanalysisoffourtoﬁveimagesprovided,withtheaimofse-\nlecting authentic human faces or identifying correspond-\ningattackmethods.Thisprocessentailsnotonlycognitive\nanalysisoftheimagecontent,butalsoacomprehensiveas-\nsessment of the image’s authenticity, aiming to accurately\ndistinguish between real human faces and various poten-\ntialattackscenarios.\nTable 4 lists the accuracy of diﬀerent MLLMs on FAS\nmultiple-choice problems. It separately addresses two\ntypes of questions: identifying real faces and spoof faces,\nwitheachtypetestedundertwoconditions:zero-shotand\none-shot. Finally, the average accuracy (AVG) across all\nconditions isprovided.GPT4V achieved thehighest aver-\nage accuracy under all conditions, reaching 33.1%. Apart\nfrom GPT-4V, Gemini performed best in recognizing at-\ntack under zero-shot conditions (25.6%) and also demon-\nstrated strong overall performance with an average accu-\nracy of 23.3%. ICL did not signiﬁcantly improve the per-\nShietal. VisualIntelligence             (2025) 3:9 Page 9 of 25\nTable 5AccuracyofvariousGPT4VandGeminionFASmultimodaltrue/falsequestions\nModel ACC(%)\nZero-shot One-shot Zero-shot(COT) One-shot(COT) AVG\nGPT4V[42] 87.5 100.0 60.0 100.0 90.6\nGemini[4] 50.0 50.0 50.0 50.0 50.0\nformance,indicatingthattheICLcapabilityofMLLMsfor\nimagesstillneedstobeimproved.\n6.3 MultimodalFAStesting\nMultimodal data can provide a richer set of information\nto capture spooﬁng clues. The questions asked in the test\nwere kept consistent with those used in the unimodal ex-\nperiments to ensure uniformity and comparability of the\nresults.\nTrue/false questions. Table 5 shows the accuracy of\nGPT4V and Gemini in judging multimodal FAS tasks,\nwiththesemodelsselectedfortheirsuperiorperformance\ni nt h e s ec o m p l e xt a s k s .T h et e s t ,w h i c hu s e dal i m i t e d\nnumber of samples, required the MLLMs to discriminate\nwhether the subject was a real face under two conditions:\nzero-shot and one-shot. GPT4V showed commendable\nresults in each condition, demonstrating its inherent ca-\npability to extract information directly from images, with\nthe one-shot condition generally outperforming the zero-\nshot condition. In contrast, this task proved more chal-\nlenging for Gemini, which maintained an accuracy rate of\naround 50%. ICL did not enhance its performance, high-\nlighting Gemini’s limitations in handling more complex\ntasks.\nQualitativeresults. Fig.6showspartialanswersforzero-\nshotandfew-shottrue/falsequestions.Forthepapermask\nattack, aside from the zero-shot standard experiment,\nGemini, GPT4V, and mPLUG-Owl all demonstrated an\nability to correctly identify the correct answer. However,\ntheCOToutputsofGeminiandmPlugarenotideal,lead-\ning to inaccurate execution of instructions. For questions\nthat GPT4V refused to answer, the correct answer was\nprovided after supplying prepared samples as in-context\nlearning (ICL) [66]. Figure6 also displays partial answers\nforzero-shotandfew-shot multiple-choicequestions.For\nthe task of selecting real people, distinguishing between\nprint and real people is more challenging. GPT4V ob-\ntained the correct answer using the COT technique after\nbeing providedwith ICLforreference.\nQuantitative results.Tables6 to 13 show the results of\ntheperformanceofMLLMsintheFAStaskforvariousat-\ntacks,evaluatedbythemetricsACCandHTER.\n7 Resultsonfaceforgerydetection\nPromptdesign. To assess the performance of the model in\nthedomainoffaceforgerydetection,weutilizedcommon\nmultimodalmodelssuchasGPT4V,Genimi,andmPLUG-\nOwl to predict labels for face images. We juxtaposed im-\nages generated by the model with those generated by 1.5\nInsightface,askingquestionsabouttheimagesundercate-\ngoriesincludinggenuine,Deepfake,Face2Face,FaceSwap,\nNulltextures, and Stable Diﬀusion 1.5. As shown in Fig.7,\nsimilartotheF AStasks,weconductedthreetypesoftests\nfor individual images in the face forgery detection task:\nposing separate queries, employing contextual learning,\nandperformingjointtesting.Weusedtheterms“realface”\nand “fake face” to refer to genuine and forged faces, re-\nspectively. For example, one of the questions asked, “Is\nthis an example of a real face? Please answer yes or no!”\nBuilding on this, we employed two distinct sets of queries\nto evaluate face authenticity as part of a zero-shot testing\napproach.Additionally,weconductedcontextuallearning\ndialogues for further testing. In this approach, an image\nwas presented to each model, which was tasked to de-\ntermine its authenticity. The questions were framed in\npairs to guide the model’s reasoning process. Our study\nshows that Gemini and GPT4V demonstrate superior\nlearningandexpansioncapabilitiescomparedtomPLUG-\nOwl. Furthermore, after undergoing contextual learning,\nthe model’s performance improved. However, the model\nis prone to failure on more realistic forgedimages, under-\nscoring the need to imbue the model with domain knowl-\nedge.\n7.1 Unimodalfaceforgerydetectiontesting\nTrue/false questions.I nt h i ss u b s e c t i o n ,w ee v a l u a t et h e\nabilityofmodelstodetectforgedimagesbyrequiringﬁxed\nresponses regardingtheir authenticity.\nTable14 and Table15 present the quantitative results,\nwhere we observe the zero-shot and one-shot detec-\ntion performance of commonly used large models, such\nas BLIP2 and LLaVA, in the face forgery task. Lacking\ndomain-speciﬁcﬁne-tuning,theiraccuracyissigniﬁcantly\nlow, with BLIP2 achieving only 14.3% accuracy. Although\nmodels such as GPT4V and Gemini outperform others,\ntheiroverallaccuracyremainsrelativelylow.Interestingly,\none-shot accuracy does not improve signiﬁcantly across\nthese models. In contrast, our proposed COT method\ndemonstrates superior performance, with accuracy in-\ncreasingfrom22.4%to26.0%intheone-shot setting.\nMultiple-choicequestions. Inthissubsection,weemploy\nan image selection format to test the models’ capabilities\ninfaceforgerydetection.GPT4VandGeminiweretasked\nShietal. VisualIntelligence             (2025) 3:9 Page10of25\nFigure6 TheperformanceofMLLMsontheFAStask,segmentedintotrue/falseandmultiple-choicesections.Eachsectionincludestests\nconductedwithorwithouttheuseofCOTandICL,assessingMLLMs’capabilitiesfrommultipleperspectives.Yellow(red)highlightsthecorrect\n(incorrect)responses,andblueindicatesthatthemodelrefusestoanswer.TheimagesaresourcedfromtheWMCA[ 53]andtheFF++[ 55]datasets.\nICL:in-contextlearning\nShietal. VisualIntelligence             (2025) 3:9 Page11of25\nFigure7 Promptdesignforfaceforgerydetection.Ontheleftarethecandidateprompts,withthetestcasesusedtoevaluatethepromptselection\npositionedatthetop.Thesetestcasesincluderealfaces,Deepfakes,Face2Face,FaceSwap,Nulltextures,andStable_Diﬀusion.Theﬁgureillustra tes\ntheresponsesfromGPT4VandGemini.Thecoloryellowisusedtodenotecorrectresponses,whilethecolorredisusedtoindicateincorrect\nresponses.TheimagesaresourcedfromtheWMCA[ 53]andtheFF++[ 55]datasets\nTable 6AccuracyofvariousMLLMsonFAStrue/falsezero-shotquestions\nModel ACC(%)\nBonaﬁde Fakehead Flexiblemask Papermask Print Replay Rigidmask AVG\nBLIP[62] 100.0 2.0 0.0 27.0 0.0 0.0 0.0 18.4\nBLIP-2[40] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nIntern[63] 100.0 100.0 50.0 94.0 2.0 0.0 57.0 57.6\nMiniGPT-4[43] 93.0 10.0 7.0 17.0 5.0 4.0 8.0 20.6\nLLaVA[45] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nQWen-VL[44] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nInstructBLIP[41] 100.0 0.0 0.0 64.0 0.0 0.0 2.0 23.7\nmPLUG-Owl[47] 99.0 98.0 92.0 100.0 56.0 29.0 100.0 82.0\nGemini[4] 100.0 100.0 92.0 100.0 17.0 7.0 98.0 73.4\nGPT4V[42] 96.9 83.0 64.0 43.0 29.6 6.8 50.6 53.2\nwith determining which image was real and which was a\nfabrication. Similar to the previous subsection, our tests\ncovered common forgery patterns, including Deepfakes,\nFace2Face, FaceSwap, Nulltextures, and images generated\nby Stable Diﬀusion. Each test included both standard and\nCOT experiments, and requiredthe models to closely an-\nalyze the four to ﬁve images provided in order to identify\nthe real ones.\nInaddition,toassessthemodels’capabilitiesmorecom-\nprehensively, we evaluated their ability to recognize the\ngeneration methods used for forgery creation. The quan-\ntitative results in Table16 consistently demonstrate that\nmodels without ﬁne-tuning perform poorly, struggling to\naccurately identify forged faces or discriminate between\nforgerycategories.\n7.2 Multimodalfaceforgerydetectiontesting\nThe potential of multimodal information in face forgery\ndetection has received widespread attention from re-\nsearchers, typically serving as auxiliary data to enhance\nmodel accuracy. In this paper, we utilized DCT transfor-\nmation to acquire frequency domain modalities and em-\nployed the oﬃcial maximum magnitude frequency com-\nShietal. VisualIntelligence             (2025) 3:9 Page12of25\nTable 7AccuracyofvariousMLLMsonFAStrue/falsezero-shotquestionswithCOT\nModel ACC(%)\nBonaﬁde Fakehead Flexiblemask Papermask Print Replay Rigidmask AVG\nBLIP[62] 100.0 14.0 0.0 65.0 0.0 0.0 2.0 25.9\nBLIP-2[40] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nIntern[63] 100.0 100.0 45.0 93.0 2.0 0.0 55.0 56.4\nMiniGPT-4[43] 97.0 21.0 15.0 23.0 18.0 5.0 15.0 27.7\nLLaVA[45] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nQWen-VL[44] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nInstructBLIP[41] 100.0 5.0 32.0 85.0 4.0 0.0 70.0 42.3\nmPLUG-Owl[47] 98.0 97.0 95.0 100.0 53.0 32.0 97.0 81.7\nGemini[4] 97.0 97.0 96.0 100.0 31.0 19.0 99.0 77.0\nGPT4V[42] 98.3 100.0 90.2 64.0 32.5 6.3 83.2 68.7\nponentization (MMFC) for voice extraction to obtain\nvoice spectrograms. During the testing phase, we consid-\nered four common forgery modes: Deepfakes, Face2Face,\nFaceSwap,andNulltextures,andusedrepresentativezero-\nshotorone-shotpromptstoevaluatethetwomodels.The\nresultsindicatethatthemodelsaremorepronetotrigger-\ningsecurityalertsandexhibitsomeillogicalresponses.\nTrue/false questions.Table14 displays the performance\nof diﬀerent models when utilizing four modalities. Com-\nparedtousingonlyimages,therewasasigniﬁcantdecline\nin performance. For instance, GPT4V’s accuracy dropped\nfrom 26.0% to 6.9%. This suggests that the models strug-\ngledtocapturesubtlediﬀerencesinfrequencydomainfea-\ntures. On average, the accuracy of all tested models re-\nmained below 20.0%. Our study also shows that mPLUG-\nOwl consistently misjudged the images, whereas Gemini\ndemonstrated greater robustness, accurately identifying\nforgedimagesinmostcases.Additionally,GPT4Vshowed\nan improvement trend after undergoing ICL (In-context\nlearning),seeFig. 8.\nQuantitative results.Tables17 to24 show the results of\nthe performance of MLLMs in the face forgery detection\ntask for various attacks, evaluated by the ACC and HTER\nmetrics.\n8 Resultsonuniﬁedtasks\nWe combined the FAS task from Sect.6 with the face\nforgery detection task from Sect. 7 to assess the per-\nformance of mainstream MLLMs in a uniﬁed detection\ntask against both physical spooﬁng and digital forgery at-\ntacks.Therationaleforcombiningthesetasksstemsfrom\ntheir shared objective as binary classiﬁcation problems\naimedatidentifyingfacialforgeryclues.BothFASandface\nforgerydetectioninvolvethedetectionofmanipulations—\nwhether physical or digital—on facial images, providing\ninherent similarities. By addressing these tasks together,\nwe aim to provide a morerobustand practical framework\nfor evaluating MLLMs’ capabilities in identifying diverse\nfacialforgeryscenarios, in linewith recentstudies explor-\ningintegrateddetectionapproaches[ 25].\nTable 25 lists the accuracy of various MLLMs in the\nfew-shot joint task assessments. The data indicate that\nthe accuracy of models such as BLIP, Gemini, MiniGPT-\n4, LLava, and Instruct BLIP improved after the inclusion\nofCOT.GPT4VandInternexperiencedaslightdecrease,\nwhile BLIP-2 underwent a precipitous decline. These ob-\nservationssuggestthattheintroductionofCOTcangener-\nallyassistMLLMsinmakingcorrectjudgements,although\nitcanalsoleadtoanincreasedfocusonnoise,resultingin\naslightdeteriorationinresults.Theparticularlypoorper-\nformanceofBLIP-2maybeduetotheillusionsinducedby\nthe integrated COT. The performance of MLLMs in few-\nshot joint tasks highlights the signiﬁcant challenges they\nfaceineﬀectivelysolvingcombinedtasks.\nTable 26 presents the ACC of various MLLMs on the\nuniﬁed task multiple-choice tests. The task is divided\ninto two main categories: recognizing real faces and vari-\nous malicious attacks. Among all tested models, GPT4V\nachieves the highest average accuracy across all condi-\ntionsat29.2%,demonstratingthebestoverallperformance\non this uniﬁed task. In the zero-shot condition for rec-\nognizing real faces, GPT4V also leads with an accuracy\nof 27.2%, closely followed by BLIP at 26.0%. In the one-\nshot condition for real image recognition, GPT4V again\nachievesthehighestaccuracyat28.6%,followedbyLLaVA\nand mPLUG-Owl both at 27.0%. For attack recognition,\nGPT4V tops the zero-shot condition with an accuracy of\n29.3%,anditalsoleadsintheone-shotconditionat34.0%.\nModels such as BLIP-2, MiniGPT-4, and QWen-VL face\nsigniﬁcant challenges, especially in the zero-shot condi-\ntion for real images, where both BLIP-2 and MiniGPT-4\nexhibitanaccuracyof0.0%.Thegenerallylowaverageper-\nformance across all models suggests that the task is chal-\nlenging.\nA ss h o w ni nF i g .9, the experiment tested six high-\nperforming models on a joint task in the few-shot setting,\nusing four types of images: real human faces, physical at-\ntacksimulations,andimagesgeneratedbyGANanddiﬀu-\nsion technologies. Three attack-domain images were ini-\ntiallyprovidedaspriorknowledge,afterwhichthemodels\nShietal. VisualIntelligence             (2025) 3:9 Page13of25\nTable 8HTERofvariousMLLMsonFAStrue/falsezero-shotquestions\nModel HTER(%) ↓\nBonaﬁde Fakehead Flexiblemask Papermask Print Replay Rigidmask AVG\nBLIP[62] 0.0 49.0 50.0 36.5 50.0 50.0 50.0 47.6\nBLIP-2[40] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nIntern[63] 0.0 0.0 25.0 3.0 49.0 50.0 21.5 24.8\nMiniGPT-4[43] 3.5 45.0 46.5 41.5 47.5 48.0 46.0 49.2\nLLaVA[45] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nQWen-VL[44] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nInstructBLIP[41] 0.0 50.0 50.0 18.0 50.0 50.0 49.0 44.5\nmPLUG-Owl[47] 0.5 1.0 4.0 0.0 22.0 35.5 0.0 10.9\nGemini[4] 0.0 0.0 4.0 0.0 41.5 46.5 1.0 15.5\nGPT4V[42] 1.5 8.5 18.0 28.5 35.2 46.6 24.7 27.7\nTable 9HTERofvariousMLLMsonFAStrue/falsezero-shotquestionswithCOT\nModel HTER(%) ↓\nBonaﬁde Fakehead Flexiblemask Papermask Print Replay Rigidmask AVG\nBLIP[62] 0.0 43.0 50.0 17.5 50.0 50.0 49.0 43.2\nBLIP-2[40] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nIntern[63] 0.0 0.0 27.5 3.5 49.0 50.0 22.5 25.4\nMiniGPT-4[43] 1.5 39.5 42.5 38.5 41.0 47.5 42.5 43.4\nLLaVA[45] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nQWen-VL[44] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nInstructBLIP[41] 0.0 47.5 34.0 7.5 48.0 50.0 15.0 33.7\nmPLUG-Owl[47] 1.0 1.5 2.5 0.0 23.5 34.0 1.5 11.5\nGemini[4] 1.5 1.5 2.0 0.0 34.5 40.5 0.5 14.7\nGPT4V[42] 0.8 0.0 4.9 18.0 33.8 46.8 8.4 18.1\nTable 10AccuracyofvariousMLLMsonFAStrue/falseone-shotquestions\nModel ACC(%)\nBonaﬁde Fakehead Flexiblemask Papermask Print Replay Rigidmask AVG\nBLIP[62] 100.0 82.0 17.0 95.0 0.0 0.0 11.0 43.6\nBLIP-2[40] 63.0 0.0 0.0 0.0 0.0 0.0 0.0 9.0\nIntern[63] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nMiniGPT-4[43] 63.0 20.0 22.0 29.0 26.0 22.0 20.0 28.9\nLLaVA[45] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nQWen-VL[44] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nInstructBLIP[41] 100.0 1.0 0.0 16.0 0.0 0.0 3.0 17.1\nmPLUG-Owl[47] 21.0 57.0 67.0 70.0 60.0 26.0 70.0 53.0\nGemini[4] 66.0 70.0 87.0 91.0 25.0 11.0 84.0 62.0\nGPT4V[42] 76.8 93.0 79.6 62.5 47.2 37.8 66.7 66.8\nassessed the authenticity of a test image. Without COT,\nall six models failed, but GPT4V, Gemini, and mPLUG\nsucceededwithCOT,highlightingitspotentialtoenhance\nreasoninganddetection incomplexscenarios.\n8.1 MACOT\nFigure10 illustrates the test results of GPT4V on the FAS\nzero-shot task with and without the use of MACOT. It is\nclear from the ﬁgure that the prompts built into MACOT\nrequire the MLLM to make judgments based on speciﬁc\nattributes,culminatinginanoveralldecision.MACOTap-\npears to be more eﬀective when addressing subtle details\nthatareoftenoverlooked.Forinstance,forprintandreplay\nattacks,whichGPT4Vtypicallystrugglestodetect,thekey\nattributes “PhoneScreen or Paper Edges” promptGPT4V\ntofocusonthepresenceofpaperorscreenedgesintheim-\nage,therebyfacilitatingtherecognitionofprintandreplay\nattacks.\nThe primary analysis is based on Fig.11. The evidence\nindicates that, although the MA-COT method did not re-\nsultinMLLMsachieving perfectscoresincompletingthe\nuniﬁedtask,itdidleadGPT4Vtoprovideageneraljudge-\nment range that encompasses the correct answer under\nits guidance, reﬂecting GPT4V’s ability to discriminate.\nShietal. VisualIntelligence             (2025) 3:9 Page14of25\nFigure8 TheperformanceofMLLMsonthefaceforgerydetectiontask,segmentedintotrue/falseandmultiple-choicesections.Eachsection\nincludestestsconductedwithorwithouttheuseofCOTandICL,assessingMLLMs’capabilitiesfrommultipleperspectives.Thecoloryellowisused\ntodenotecorrectresponses,whilethecolorredisusedtoindicateincorrectresponses.TheimagesaresourcedfromtheWMCA[ 53]andtheFF++\n[55]datasets\nShietal. VisualIntelligence             (2025) 3:9 Page15of25\nFigure9 TheperformanceofMLLMsonuniﬁedtaskquestion.Thecoloryellowisusedtodenotecorrectresponses,whilethecolorredisusedto\nindicateincorrectresponses.Thecolorblueindicatesthatthemodelrefusestoanswer.TheimagesaresourcedfromtheWMCA[ 53]andtheFF++\n[55]datasets\nShietal. VisualIntelligence             (2025) 3:9 Page16of25\nTable 11AccuracyofvariousMLLMsonFAStrue/falseone-shotquestionswithCOT\nModel ACC(%)\nBonaﬁde Fakehead Flexiblemask Papermask Print Replay Rigidmask AVG\nBLIP[62] 100.0 100.0 65.0 100.0 35.0 36.0 87.0 74.7\nBLIP-2[40] 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.4\nIntern[63] 100.0 19.0 0.0 4.0 0.0 0.0 1.0 17.7\nMiniGPT-4[43] 39.0 36.0 22.0 38.0 26.0 32.0 24.0 31.0\nLLaVA[45] 38.0 0.0 0.0 0.0 0.0 0.0 0.0 5.4\nQWen-VL[44] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nInstructBLIP[41] 100.0 5.0 0.0 4.0 0.0 0.0 2.0 15.9\nmPLUG-Owl[47] 22.0 97.0 93.0 94.0 97.0 80.0 95.0 82.6\nGemini[4] 4.0 27.0 32.0 5.0 4.0 1.0 24.0 13.9\nGPT4V[42] 80.0 82.8 66.3 35.4 28.4 12.0 51.5 50.3\nTable 12HTERofvariousMLLMsonFAStrue/falseone-shotquestions\nModel HTER(%) ↓\nBonaﬁde Fakehead Flexiblemask Papermask Print Replay Rigidmask AVG\nBLIP[62] 0.0 9.0 41.5 2.5 50.0 50.0 44.5 32.9\nBLIP-2[40] 18.5 50.0 50.0 50.0 50.0 50.0 50.0 68.5\nIntern[63] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nMiniGPT-4[43] 18.5 40.0 39.0 35.5 37.0 39.0 40.0 56.9\nLLaVA[45] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nQWen-VL[44] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nInstructBLIP[41] 0.0 49.0 50.0 42.0 50.0 50.0 48.5 48.3\nmPLUG-Owl[47] 39.5 21.5 16.5 15.0 20.0 37.0 15.0 60.3\nGemini[4] 17.0 15.0 6.5 4.5 37.5 44.5 8.0 36.3\nGPT4V[42] 11.6 3.5 10.2 18.8 26.4 31.1 16.7 28.8\nTable 13HTERofvariousMLLMsonFAStrue/falseone-shotquestionswithCOT\nModel HTER(%) ↓\nBonaﬁde Fakehead Flexiblemask Papermask Print Replay Rigidmask AVG\nBLIP[62] 0.0 0.0 17.5 0.0 32.5 32.0 6.5 14.8\nBLIP-2[40] 48.5 50.0 50.0 50.0 50.0 50.0 50.0 98.5\nIntern[63] 0.0 40.5 50.0 48.0 50.0 50.0 49.5 48.0\nMiniGPT-4[43] 30.5 32.0 39.0 31.0 37.0 34.0 38.0 65.7\nLLaVA[45] 31.0 50.0 50.0 50.0 50.0 50.0 50.0 81.0\nQWen-VL[44] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nInstructBLIP[41] 0.0 47.5 50.0 48.0 50.0 50.0 49.0 49.1\nmPLUG-Owl[47] 39.0 1.5 3.5 3.0 1.5 10.0 2.5 42.7\nGemini[4] 48.0 36.5 34.0 47.5 48.0 49.5 38.0 90.2\nGPT4V[42] 10.0 8.6 16.8 32.3 35.8 44.0 24.2 36.8\nIn comparison, Gemini’s performance in this task did not\nquiteliveuptoexpectations,particularlyintermsofimage\nrecognitionandaccurateidentiﬁcation ofrealfaces.\n9R e s u l t s o f M A - C O T\nInthissection,weconductedaqualitativetestonselected\ndatatogainaninitialunderstandingofMA-COT’sperfor-\nmance, exploring the potential of MA-COT with general-\npurpose MLLMs. We tested the application eﬀectiveness\noftheMA-COTmethodpreviouslymentioned,acrossthe\nFAS task, the face forgery detection task, and the uniﬁed\ntask, with a subset of the results visualized. By employ-\ning the MA-COT method, we guide the MLLMs to fo-\ncus on key features related to presentation attacks using\npriorknowledge,therebymoreeﬃcientlycompletingsub-\ntasks without the direct introduction of such knowledge.\nMoreover, since these key features can be easily added as\nplug-and-playcomponents,theMA-COTmethodalsofa-\ncilitates the seamless integration of the FAS task and the\nfaceforgerydetectiontaskintoauniﬁedtaskframework.\nFigure 12 presents a comparison of the average metrics\nforGPT4Vintrue/falsequestionwithorwithouttheuseof\nMA-COT.Itisobservedthattheaverageaccuracyslightly\ndecreases after employing MA-COT, while the HTER re-\nShietal. VisualIntelligence             (2025) 3:9 Page17of25\nFigure10 ResultofvariousMLLMsonFASzero-shottrue/falsequestionwithMACOT.Thecoloryellowisusedtodenotecorrectresponses,while\nthecolorredisusedtoindicateincorrectresponses.TheimagesaresourcedfromtheWMCA[ 53]andtheFF++[ 55]datasets\nShietal. VisualIntelligence             (2025) 3:9 Page18of25\nFigure11 Resultsofmultiple-choicezero-shottests.Intheoriginaltest,bothGeminiandGPT4VincorrectlyidentiﬁedImage3asarealface.After\napplyingtheMA-COTpromptmethod,GPT4Vconductedamulti-attributeanalysisandvoting,andselectedtwoanswers,includingthecorrectreal\nfaceandaprintattackimagethatischallengingtodistinguishfromrealfacesintheFAStasks.Conversely,Geminierroneouslyconsideredallimage s\ntoberealfacesafteranalyzingeachattribute,indicatingadeﬁciencyindetailedanalysis.“Most”meansthatthemodelanswerswiththelabel\ncontainingthemostcorrectbiologicalfeatures\nTable 14PerformancemetricsofvariousMLLMsontrue/falsequestionsaboutfaceforgery\nModel Zero-shot One-shot Zero-shot(COT) One-shot(COT)\nACC(%)↑ HTER(%)↓ ACC(%)↑ HTER(%)↓ ACC(%)↑ HTER(%)↓ ACC(%)↑ HTER(%)↓\nBLIP[62] 17.9 51.3 49.1 29.7 31.0 49.4 62.6 23.1\nBLIP-2[40] 14.3 50.0 10.1 64.5 14.3 50.0 1.7 94.0\nIntern[63] 14.6 49.8 14.3 50.0 14.3 50.0 14.3 50.0\nMiniGPT-4[43] 17.7 48.0 32.1 57.1 18.1 48.2 36.1 49.8\nLLaVA[45] 14.3 50.0 14.3 50.0 14.3 50.0 13.1 54.0\nQWen-VL[44] 14.3 50.0 14.3 50.0 14.3 50.0 14.3 50.0\nInstructBLIP[41] 18.6 47.5 14.6 49.8 23.9 44.4 16.7 48.6\nmPLUG-Owl[47] 21.9 45.6 12.7 60.9 20.9 46.2 20.9 50.8\nGemini[4] 36.8 38.5 27.0 52.5 48.6 35.9 14.0 83.1\nGPT4V[42] 26.0 43.6 22.4 45.1 28.8 44.0 26.0 44.8\nmains relatively stable. In particular, the refusal rate of\nGPT4V decreases signiﬁcantly, demonstrating that MA-\nCOT can eﬀectively reduce the incidence of refusal. Fig-\nure 13 shows the result of each attack types with or with-\nout the use of MA-COT on GPT4V. The use of MA-COT\nnotablyimprovesperformanceinareaswhereGPT4Vini-\ntiallystruggled,suchasprintandreplayattacks, thoughit\nmay reduce performance in areas where it previously ex-\ncelled. This suggests that the MA-COT strategy is eﬀec-\nShietal. VisualIntelligence             (2025) 3:9 Page19of25\nTable 15AccuracyofGPT4VandGeminionfaceforgerymultimodaltrue/falsequestions\nModel ACC(%)\nZero-shot One-shot Zero-shot(COT) One-shot(COT) AVG\nGPT4V[42] 44.4 66.7 12.5 66.7 68.0\nGemini[4] 50.0 30.0 50.0 70.0 50.0\nTable 16AccuracyofvariousMLLMsonfaceforgerymultiple-choicequestions\nModel ACC(%) ↑\nZero-shot(real) One-shot(real) Zero-shot(attack) One-shot(attack) AVG\n- COT - COT - COT - COT - COT\nBLIP[62] 22.0 24.0 21.0 23.0 22.0 18.0 11.0 10.0 19.0 18.8\nBLIP-2[40] 0.0 0.0 6.0 0.0 7.0 0.0 0.0 0.0 3.3 0.0\nIntern[63] 16.0 5.0 18.0 10.0 10.0 0.0 14.0 1.0 14.5 4.0\nMiniGPT-4[43] 2.0 2.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.5\nLLaVA[45] 18.0 1.0 18.0 7.0 7.0 0.0 23.0 9.0 16.5 4.3\nQWen-VL[44] 6.0 0.0 19.0 0.0 1.0 0.0 24.0 0.0 12.5 0.0\nInstructBLIP[41] 6.0 15.0 0.0 0.0 0.0 0.0 12.0 3.0 4.5 4.5\nmPLUG-Owl[47] 11.0 0.0 18.0 0.0 8.0 0.0 13.0 0.0 12.5 0.0\nGemini[4] 14.0 17.0 16.0 19.0 20.0 4.0 0.0 17.2 16.3 13.7\nGPT4V[42] 6.9 23.6 12.0 20.7 0.0 0.0 14.8 18.6 8.9 17.0\nTable 17AccuracyofvariousMLLMsonfaceforgerydetectiontrue/falsezero-shotquestions\nModel ACC(%)\nBonaﬁde Deepfakes Face2Face FaceSwap InsightFace NeuralTextures Stable_Diﬀusion AVG\nBLIP[62] 92.0 5.0 9.0 10.0 2.0 2.0 5.0 17.9\nBLIP-2[40] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nIntern[63] 100.0 0.0 0.0 0.0 2.0 0.0 0.0 14.6\nMiniGPT-4[43] 100.0 5.0 2.0 0.0 8.0 1.0 8.0 17.7\nLLaVA[45] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nQWen-VL[44] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nInstructBLIP[41] 100.0 15.0 2.0 6.0 5.0 0.0 2.0 18.6\nmPLUG-Owl[47] 100.0 9.0 5.0 4.0 12.0 0.0 23.0 21.9\nGemini[4] 96.0 54.0 30.0 34.0 17.0 7.0 20.0 36.8\nGPT4V[42] 98.1 26.1 16.4 16.7 9.1 3.2 12.5 26.0\ntive, but the key attributes considered by it still need fur-\nther exploration.\n10 Findingsanddiscussion\n10.1 Morecomprehensivedataset\nCurrently, face security datasets are limited by the lack\nofdedicatedpretrainingandsupervisedﬁne-tuning(SFT)\ndata,aswellasarobustdataconstructionpipeline.Devel-\nopingadatasetthataddressesthesegapsbyincludingboth\nhigh-quality pre-training data and a structured pipeline\nfor data generation is essential to improving model inter-\npretabilityandgeneralizability.Futureworkcouldfocuson\nconstructing such datasets, which would enable more ef-\nfective training of models for complex security tasks and\nenhancetheirperformance.\n10.2 Morecomprehensiveevaluationmetrics\nCurrent evaluation metrics for FAS and face forgery tasks\nare relatively limited. Incorporating evaluation metrics\nfrom NLP and other multimodal domains could provide\na more comprehensive assessment of model performance\nand robustness. For the face forgery task, metrics such\nas area under curve (AUC), average precision (AP), and\nequal error rate (EER) provide critical insights into model\ncapabilities. AUC and AP measure the ability to distin-\nguish between real and forged faces, while EER reﬂects\nthetrade-oﬀbetweenfalseacceptanceandrejectionrates.\nThese metrics ensure a rigorous evaluation of model per-\nformanceindiverseforgeryscenarios.\nNLP-relatedmetricssuchasBLEU,ROUGE-L,andME-\nTEOR are valuable for assessing the quality of generated\ntextualexplanations.Thesemetrics,widelyusedinnatural\nlanguage processing, can evaluate the ﬂuency, relevance,\nand informativeness of the generated text, making them\nShietal. VisualIntelligence             (2025) 3:9 Page20of25\nTable 18AccuracyofvariousMLLMsonfaceforgerydetectiontrue/falsezero-shotquestionswithCOT\nModel ACC(%)\nBonaﬁde Deepfakes Face2Face FaceSwap InsightFace NeuralTextures Stable_Diﬀusion AVG\nBLIP[62] 78.0 38.0 28.0 42.0 7.0 15.0 9.0 31.0\nBLIP-2[40] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nIntern[63] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nMiniGPT-4[43] 99.0 6.0 3.0 8.0 8.0 0.0 3.0 18.1\nLLaVA[45] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nQWen-VL[44] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nInstructBLIP[41] 100.0 34.0 7.0 10.0 9.0 0.0 7.0 23.9\nmPLUG-Owl[47] 100.0 9.0 2.0 4.0 11.0 0.0 20.0 20.9\nGemini[4] 86.0 77.0 42.0 61.0 29.3 17.0 27.3 48.6\nGPT4V[42] 95.7 25.3 13.1 22.1 11.6 2.4 23.1 28.8\nTable 19HTERofvariousMLLMsonfaceforgerydetectiontrue/falsezero-shotquestions\nModel HTER(%) ↓\nBonaﬁde Deepfakes Face2Face FaceSwap InsightFace NeuralTextures Stable_Diﬀusion AVG\nBLIP[62] 4.0 47.5 45.5 45.0 49.0 49.0 47.5 51.2\nBLIP-2[40] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nIntern[63] 0.0 50.0 50.0 50.0 49.0 50.0 50.0 49.8\nMiniGPT-4[43] 0.0 47.5 49.0 50.0 46.0 49.5 46.0 48.0\nLLaVA[45] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nQWen-VL[44] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nInstructBLIP[41] 0.0 42.5 49.0 47.0 47.5 50.0 49.0 47.5\nmPLUG-Owl[47] 0.0 45.5 47.5 48.0 44.0 50.0 38.5 45.6\nGemini[4] 2.0 23.0 35.0 33.0 41.5 46.5 40.0 38.5\nGPT4V[42] 0.9 37.0 41.8 41.7 45.5 48.4 43.8 43.6\nTable 20HTERofvariousMLLMsonfaceforgerydetectiontrue/falsezero-shotquestionswithCOT\nModel HTER(%) ↓\nBonaﬁde Deepfakes Face2Face FaceSwap InsightFace NeuralTextures Stable_Diﬀusion AVG\nBLIP[62] 11.0 31.0 36.0 29.0 46.5 42.5 45.5 49.4\nBLIP-2[40] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nIntern[63] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nMiniGPT-4[43] 0.5 47.0 48.5 46.0 46.0 50.0 48.5 48.2\nLLaVA[45] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nQWen-VL[44] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nInstructBLIP[41] 0.0 33.0 46.5 45.0 45.5 50.0 46.5 44.4\nmPLUG-Owl[47] 0.0 45.5 49.0 48.0 44.5 50.0 40.0 46.2\nGemini[4] 7.0 11.5 29.0 19.5 35.4 41.5 36.4 35.9\nGPT4V[42] 2.2 37.4 43.4 39.0 44.2 48.8 38.5 44.0\nparticularly useful for understanding the interpretability\nof multimodal models in face security tasks. Incorporat-\ning these diverse metrics bridges the gap between visual\nand language modalities, enabling a holistic evaluation of\nmodelperformance.\nFuture research could explore ways to integrate these\ndiverse metrics, allowing for a deeper understanding of\nmodel capabilities and fostering the development ofmore\ninterpretable and generalized models for face security\ntasks.\n10.3 Task-speciﬁcMLLM\nDeveloping a task-speciﬁc MLLM tailored for face secu-\nr i t yt a s k si sap r o m i s i n gd i r e c t i o n .T h i si n v o l v e st e c h -\nniques such as dedicated pre-training, supervised ﬁne-\ntuning (SFT), and reinforcement learning from human\nfeedback (RLHF) to optimize the model’s performance in\nspeciﬁc tasks such as perception, reasoning, and judg-\nment.\nThe results in Tables 27 and 28 clearly demonstrate\nthat ﬁne-tuning with task-speciﬁc data can signiﬁcantly\nenhance model performance across both FAS and face\nforgery tasks. For the FAS task (Table27), SFT notably\nShietal. VisualIntelligence             (2025) 3:9 Page21of25\nTable 21AccuracyofvariousMLLMsonfaceforgerydetectiontrue/falseone-shotquestions\nModel ACC(%)\nBonaﬁde Deepfakes Face2Face FaceSwap InsightFace NeuralTextures Stable_Diﬀusion AVG\nBLIP[62] 100.0 53.0 48.0 53.0 27.0 24.0 39.0 49.1\nBLIP-2[40] 71.0 0.0 0.0 0.0 0.0 0.0 0.0 10.1\nIntern[63] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nMiniGPT-4[43] 58.0 26.0 21.0 32.0 33.0 30.0 25.0 32.1\nLLaVA[45] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nQWen-VL[44] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nInstructBLIP[41] 100.0 2.0 0.0 0.0 0.0 0.0 0.0 14.6\nmPLUG-Owl[47] 76.0 0.0 0.0 0.0 9.0 0.0 4.0 12.7\nGemini[4] 76.0 38.0 19.0 29.0 10.0 7.0 10.0 27.0\nGPT4V[42] 100.0 13.3 12.2 10.3 10.6 3.0 9.4 22.4\nTable 22AccuracyofvariousMLLMsonfaceforgerydetectiontrue/falseone-shotquestionswithCOT\nModel ACC(%)\nBonaﬁde Deepfakes Face2Face FaceSwap InsightFace NeuralTextures Stable_Diﬀusion AVG\nBLIP[62] 97.0 75.0 77.0 77.0 26.0 48.0 38.0 62.6\nBLIP-2[40] 12.0 0.0 0.0 0.0 0.0 0.0 0.0 1.7\nIntern[63] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nMiniGPT-4[43] 70.0 28.0 25.0 34.0 36.0 32.0 28.0 36.1\nLLaVA[45] 92.0 0.0 0.0 0.0 0.0 0.0 0.0 13.1\nQWen-VL[44] 100.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3\nInstructBLIP[41] 100.0 7.0 2.0 6.0 1.0 0.0 1.0 16.7\nmPLUG-Owl[47] 89.0 6.0 5.0 2.0 21.0 0.0 23.0 20.9\nGemini[4] 21.0 21.0 14.0 15.0 13.0 4.0 10.0 14.0\nGPT4V[42] 95.8 18.0 9.1 28.7 14.4 4.1 13.5 26.0\nTable 23HTERofvariousMLLMsonfaceforgerydetectiontrue/falseone-shotquestions\nModel HTER(%) ↓\nBonaﬁde Deepfakes Face2Face FaceSwap InsightFace NeuralTextures Stable_Diﬀusion AVG\nBLIP[62] 0.0 23.5 26.0 23.5 36.5 38.0 30.5 29.7\nBLIP-2[40] 14.5 50.0 50.0 50.0 50.0 50.0 50.0 64.5\nIntern[63] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nMiniGPT-4[43] 21.0 37.0 39.5 34.0 33.5 35.0 37.5 57.1\nLLaVA[45] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nQWen-VL[44] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nInstructBLIP[41] 0.0 49.0 50.0 50.0 50.0 50.0 50.0 49.8\nmPLUG-Owl[47] 12.0 50.0 50.0 50.0 45.5 50.0 48.0 60.9\nGemini[4] 12.0 30.5 40.5 35.5 45.0 46.5 45.0 52.5\nGPT4V[42] 0.0 43.4 43.9 44.9 44.7 48.5 45.3 45.1\nimproves detection accuracy (ACC) and reduces error\nrates (HTER), while also enhancing BLEU-4, ROUGE-L,\nand METEOR scores, indicating better quality and inter-\npretability ofthemodel’spredictions.\nSimilarly, for the face forgery task (Table28), SFT re-\nsults in substantial gains in AUC and AP, highlighting the\nmodel’s improved ability to distinguish between real and\nforged faces. The reduced EER further demonstrates en-\nhanced robustness, while improved BLEU-4, ROUGE-L,\nandMETEORscoresvalidatethemodel’scapacitytogen-\nerate more accurate and meaningful textual explanations.\nThese ﬁndings underscore the importance of ﬁne-tuning\nin adapting MLLMs to meet the speciﬁc demands of face\nsecuritytasks.\n10.4 Cross-taskcollaboration\nCollaborating on similar tasks such as face anti-spooﬁng\nand deepfake detection utilizes complementary knowl-\nedge for better results. Expanding such collaboration to\notherﬁeldssuchasindustrialanomalydetectionandmed-\nical image processing could signiﬁcantly boost accuracy\nandeﬃciency.\nShietal. VisualIntelligence             (2025) 3:9 Page22of25\nTable 24HTERofvariousMLLMsonfaceforgerydetectiontrue/falseone-shotquestionswithCOT\nModel HTER(%) ↓\nBonaﬁde Deepfakes Face2Face FaceSwap InsightFace NeuralTextures Stable_Diﬀusion AVG\nBLIP[62] 1.5 12.5 11.5 11.5 37.0 26.0 31.0 23.1\nBLIP-2[40] 44.0 50.0 50.0 50.0 50.0 50.0 50.0 94.0\nIntern[63] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nMiniGPT-4[43] 15.0 36.0 37.5 33.0 32.0 34.0 36.0 49.8\nLLaVA[45] 4.0 50.0 50.0 50.0 50.0 50.0 50.0 54.0\nQWen-VL[44] 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nInstructBLIP[41] 0.0 46.5 49.0 47.0 49.5 50.0 49.5 48.6\nmPLUG-Owl[47] 5.5 47.0 47.5 49.0 39.5 50.0 38.5 50.8\nGemini[4] 39.5 39.5 43.0 42.5 43.5 48.0 45.0 83.1\nGPT4V[42] 2.1 41.0 45.5 35.6 42.8 47.9 43.2 44.8\nFigure12 ComparativeresultsoftheGPT4VontheFASzero-shot\ntask,withorwithouttheMA-COT\nTable 25AccuracyofvariousMLLMsonuniﬁedtasktrue/false\nquestions\nModel ACC(%)\nFew-shot Few-shot (COT)\nBLIP[62] 34.0 41.0\nBLIP-2[40] 34.0 9.0\nIntern[63] 33.0 30.0\nMiniGPT-4[43] 18.0 24.0\nLLaVA[45] 33.0 41.0\nQWen-VL[44] 20.0 21.0\nInstructBLIP[41] 44.0 54.0\nmPLUG-Owl[47] 34.0 35.0\nGemini[4] 41.0 45.9\nGPT4V[42] 25.5 24.2\nAbbreviations\nACC,accuracy;AIGC,AI-generatedcontent;COT,chainofthought;DCT,\ndiscretecosinetransform;FAS,faceanti-spooﬁng;FF++,faceforensics++;HTER,\nhalftotalerrorrate;MLLMs,multimodallargelanguagemodels;GAN,\ngenerativeadversarialnetwork;MA-COT,multi-attributechainofthought;\nMMFC,maximummagnitudefrequencycomponentization.\nFigure13 ResultsofGPT4Vontrue/falsezero-shotFAStasks,\nshowingperformanceacrossdiﬀerentattackswithorwithout\nMA-COT\nAcknowledgements\nWewouldliketoexpressourgratitudetotheEITHighPerformance\nComputingPlatformforitsprovisionofcomputationalresourcesforthis\nproject.ExpressionsofgratitudeareextendedtoZhuofuTaoforhisinvaluable\ncontributionsinoptimizingtheﬁguresandtextsofthismanuscript.\nAuthorcontributions\nYSconceivedthemainideas,designedtheoverallframework,anddraftedthe\ninitialmanuscript.YGwasresponsiblefordesigningtheoverallexperiments,\nconductingsomeoftheFASexperiments,anddraftingpartsoftheﬁguresand\ntext.YLcarriedouttheexperimentsrelatedtofaceforgeryandcontributedto\nthemanuscriptwriting.Thethreeauthorscontributedequallyandare\nconsideredco-ﬁrstauthors.HWimplementedtheFASexperimentsand\ncreatedtheﬁguresforthemanuscript.JF,LHe,JW,CC,andXCservedas\nco-supervisorforthisstudyandoversawallaspectsofthemanuscriptwriting\nprocess.Theprojectleader,ZY,orchestratedandsupervisedtheentireprocess,\nencompassingconceptualdiscussions,experimentation,andmanuscript\nwriting.Allauthorsreadandapprovedtheﬁnalmanuscript.\nFundinginformation\nThisworkwassupportedbytheNationalNaturalScienceFoundationofChina\n(No.62306061),GuangdongBasicandAppliedBasicResearchFoundation(No.\n2023A1515140037),andGraduateInnovationFundProjectofShijiazhuang\nTiedaoUniversity(No.YC202449).\nShietal. VisualIntelligence             (2025) 3:9 Page23of25\nTable 26AccuracyofvariousMLLMsonuniﬁedtaskmultiple-choicequestions\nModel ACC(%)\nZero-shot(real) One-shot(real) Zero-shot(attack) One-shot(attack) AVG\n- COT - COT - COT - COT - COT\nBLIP[62] 26.0 25.0 8.0 24.0 24.0 24.0 5.0 8.0 15.8 20.3\nBLIP-2[40] 0.0 0.0 20.0 0.0 2.0 0.0 2.0 0.0 6.0 0.0\nIntern[63] 15.0 2.0 24.0 14.0 7.0 0.0 6.0 0.0 13.0 4.0\nMiniGPT-4[43] 0.0 0.0 1.0 0.0 2.0 0.0 1.0 0.0 1.0 0.0\nLLaVA[45] 13.0 0.0 27.0 14.0 6.0 1.0 21.0 4.0 16.8 4.8\nQWen-VL[44] 1.0 0.0 22.0 0.0 1.0 0.0 22.0 0.0 11.5 0.0\nInstructBLIP[41] 12.0 27.0 3.0 0.0 2.0 1.0 5.0 2.0 5.5 7.5\nmPLUG-Owl[47] 12.0 0.0 27.0 1.0 5.0 0.0 21.0 0.0 16.3 0.3\nGemini[4] 24.0 22.0 14.0 23.0 27.8 14.0 11.9 15.0 20.2 18.7\nGPT4V[42] 27.2 27.4 28.6 34.0 29.3 15.4 34.0 26.8 29.2 26.8\nTable 27PerformancemetricsontheFAStaskwithLLaVAandsupervisedﬁne-tuning(SFT)improvements\nModel ACC(%) ↑ HTER(%) ↓ BLEU-4(%) ↑ ROUGE-L(%) ↑ METEOR(%) ↑\nLLaVA 65.54 27.76 17.80 30.51 25.52\nLLaVA(SFT) 98.06( +32.52)1 . 7 4 ( –26.02) 82.63( +64.83) 81.72( +51.21) 51.97( +26.45)\nTable 28PerformancemetricsonthefaceforgerytaskwithLLaVAandSFTimprovements\nModel AUC(%) ↑ AP(%) ↑ EER(%) ↓ BLEU-4(%) ↑ ROUGE-L(%) ↑ METEOR(%) ↑\nLLaVA 69.30 94.90 37.20 33.80 43.50 29.50\nLLaVA(SFT) 88.20( +18.90) 97.90( +3.00) 18.50( –18.70) 36.14( +2.34) 57.84( +14.34) 34.25( +4.75)\nDataavailability\nThedatasetsusedinthisstudyarepubliclyavailableat:WMCA: https://www.\nidiap.ch/en/scientiﬁc-research/data/wmca;SiW -Mv2:http://cvlab.cse.msu.edu/\ncategory/downloads.html;FaceForensics++(FF++): https://github.com/\nondyari/FaceForensics/tree/master/dataset;DDF: https://huggingface.co/\ndatasets/OpenRL/DeepFakeFace.\nDeclarations\nCompetinginterests\nTheauthorsdeclarenocompetinginterests.\nAuthordetails\n1SchoolofElectronicInformationandElectricalEngineering,ShanghaiJiao\nTongUniversity,Shanghai,200030,China. 2SchoolofInformationScienceand\nTechnology,ShijiazhuangTiedaoUniversity,Shijiazhuang,050043,China.\n3DepartmentofArtiﬁcialIntelligence,XiamenUniversity,Xiamen,361005,\nFujian,China. 4ElectricalEngineeringDepartment,UCLA,LosAngeles,CA\n90095,UnitedStatesofAmerica. 5NewLaboratoryofPattern\nRecognition(NLPR),InstituteofAutomation,ChineseAcademyofSciences,\nBeijing,100190,China.\n6CollegeofElectronicsandInformationEngineering,\nShenzhenUniversity,Shenzhen,518060,China. 7SchoolofComputingand\nInformationTechnology,GreatBayUniversity,Dongguan,Guangdong,523000,\nChina. 8SchoolofCyberScienceandTechnology,ShenzhenCampusofSun\nYat-senUniversity,Shenzhen,518107,China.\nReceived:30August2024 Revised:22April2025 Accepted:23April2025\nReferences\n1. Li,J.,Xie,H.,Yu,L.,Gao,X.,&Zhang,Y.(2023).Discriminativefeaturemining\nbasedonfrequencyinformationandmetriclearningforfaceforgery\ndetection.IEEETransactionsonKnowledgeandDataEngineering ,35(12),\n12167–12180.\n2. Shang,Z.,Xie,H.,Zha,Z.,Yu,L.,Li,Y.,&Zhang,Y.(2021).PRRNet:\npixel-regionrelationnetworkforfaceforgerydetection. Pattern\nRecognition,116,107950.\n3. Yang,Z.,Li,L.,Lin,K.,Wang,J.,Lin,C.-C.,Liu,Z.,&Wang,L.(2023).Thedawn\nofLMMs:preliminaryexplorationswithGPT-4V(ision).arXivpreprint. arXiv:\n2309.17421.\n4. Anil,R.,Borgeaud,S.,Wu,Y .,Alayrac,J.-B.,Y u,J.,Soricut,R.,Schalkwyk,J.,Dai,\nA.M.,Hauth,A.,Silver,D.,etal.(2023).Gemini:afamilyofhighlycapable\nmultimodalmodels.arXivpreprint. arXiv:2312.11805.\n5. Boulkenafet,Z.,Komulainen,J.,&Hadid,A.(2015).Faceanti-spooﬁng\nbasedoncolortextureanalysis.In ProceedingsoftheIEEEinternational\nconferenceonimageprocessing (pp.2636–2640).Piscataway:IEEE.\n6. Boulkenafet,Z.,Komulainen,J.,&Hadid,A.(2017).Faceantispooﬁngusing\nspeeded-uprobustfeaturesandFishervectorencoding. IEEESignal\nProcessingLetters,24(2),141–145.\n7. Komulainen,J.,Hadid,A.,&Pietikäinen,M.(2013).Contextbasedface\nanti-spooﬁng.In ProceedingsoftheIEEEsixthinternationalconferenceon\nbiometrics:theory,applicationsandsystems (pp.1–8).Piscataway:IEEE.\n8. Patel,K.,Han,H.,&Jain,A.K.(2016).Securefaceunlock:spoofdetectionon\nsmartphones.IEEETransactionsonInformationTheory ,11(10),2268–2283.\n9. Qin,Y.,Zhao,C.,Zhu,X.,Wang,Z.,Yu,Z.,Fu,T.,Zhou,F.,Shi,J.,&Lei,Z.\n(2020).Learningmetamodelforzero-andfew-shotfaceanti-spooﬁng.In\nProceedingsoftheAAAIconferenceonartiﬁcialintelligence (pp.\n11916–11923).PaloAlto:AAAIPress.\n10. Liu,Y.,Jourabloo,A.,&Liu,X.(2018).Learningdeepmodelsforface\nanti-spooﬁng:binaryorauxiliarysupervision.In ProceedingsoftheIEEE/CVF\nconferenceoncomputervisionandpatternrecognition (pp.389–398).\nPiscataway:IEEE.\n11. Yang,X.,Luo,W.,Bao,L.,Gao,Y.,Gong,D.,Zheng,S.,Li,Z.,&Liu,W.(2019).\nFaceanti-spooﬁng:modelmatters,sodoesdata.In Proceedingsofthe\nIEEE/CVFconferenceoncomputervisionandpatternrecognition (pp.\n3507–3516).Piscataway:IEEE.\n12. Atoum,Y.,Liu,Y.,Jourabloo,A.,&Liu,X.(2017).Faceanti-spooﬁngusing\npatchanddepth-basedCNNs.In ProceedingsoftheIEEEinternationaljoint\nconferenceonbiometrics (pp.319–328).Piscataway:IEEE.\nShietal. VisualIntelligence             (2025) 3:9 Page24of25\n13. Gan,J.,Li,S.,Zhai,Y.,&Liu,C.(2017).3Dconvolutionalneuralnetwork\nbasedonfaceanti-spooﬁng.In Proceedingsofthe2ndinternational\nconferenceonmultimediaandimageprocessing (pp.1–5).Piscataway:IEEE.\n14. George,A.,&Marcel,S.(2019).Deeppixel-wisebinarysupervisionforface\npresentationattackdetection.In Proceedingsoftheinternationalconference\nonbiometrics (pp.1–8).Piscataway:IEEE.\n15. Yu,Z.,Wan,J.,Qin,Y.,Li,X.,Li,S.Z.,&Zhao,G.(2021).NAS-FAS:\nstatic-dynamiccentraldiﬀerencenetworksearchforfaceanti-spooﬁng.\nIEEETransactionsonPatternAnalysisandMachineIntelligence ,43(9),\n3005–3023.\n16. Qin,Y.,Yu,Z.,Yan,L.,Wang,Z.,Zhao,C.,&Lei,Z.(2022).Meta-teacherfor\nfaceanti-spooﬁng. IEEETransactionsonPatternAnalysisandMachine\nIntelligence,44(10),6311–6326.\n17. Wang,G.,Han,H.,Shan,S.,&Chen,X.(2020).Cross-domainface\npresentationattackdetectionviamulti-domaindisentangled\nrepresentationlearning.In ProceedingsoftheIEEE/CVFconferenceon\ncomputervisionandpatternrecognition (pp.6677–6686).Piscataway:IEEE.\n18. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,\nUnterthiner ,T .,Dehghani,M.,Minderer ,M.,Heigold,G.,Gelly ,S.,etal.\n(2021).Animageisworth16x16words:transformersforimage\nrecognitionatscale.In Proceedingsofthe9thinternationalconferenceon\nlearningrepresentations (pp.6677–6686).RetrievedApril5,2025,from\nhttps://openreview.net/forum?id=YicbFdNTTy.\n19. Ming,Z.,Yu,Z.,Al-ghadi,M.,Visani,M.,Luqman,M.M.&Burie,J.-C.(2022).\nVitranspad:videotransformerusingconvolutionandself-attentionforface\npresentationattackdetection.In ProceedingsoftheIEEEinternational\nconferenceonimageprocessing (pp.4248–4252).Piscataway:IEEE.\n20. George,A.,&Marcel,S.(2021).Ontheeﬀectivenessofvisiontransformers\nforzero-shotfaceanti-spooﬁng.In ProceedingsoftheIEEEinternationaljoint\nconferenceonbiometrics (pp.1–8).Piscataway:IEEE.\n21. Wang,Z.,Wang,Q.,Deng,W.,&Guo,G.(2022).Faceanti-spooﬁngusing\ntransformerswithrelation-awaremechanism. IEEETransactionson\nBiometrics,Behavior,andIdentityScience ,4(3),439–450.\n22. Liu,A.,&Liang,Y.(2022).MA-ViT:modality-agnosticvisiontransformersfor\nfaceanti-spooﬁng.arXivpreprint. arXiv:2304.07549.\n23. Wang,Z.,Wang,Q.,Deng,W.,&Guo,G.(2022).Learningmulti-granularity\ntemporalcharacteristicsforfaceanti-spooﬁng. IEEETransactionson\nInformationTheory,17,1254–1269.\n24. Yu,Z.,Li,X.,Wang,P.,&Zhao,G.(2021).TransRPPG:remote\nphotoplethysmographytransformerfor3Dmaskfacepresentationattack\ndetection.IEEESignalProcessingLetters ,28,1290–1294.\n25. Yu,Z.,Cai,R.,Li,Z.,Yang,W.,Shi,J.,&Kot,A.C.(2024).Benchmarkingjoint\nfacespooﬁngandforgerydetectionwithvisualandphysiologicalcues.\nIEEETransactionsonDependableandSecureComputing ,21(5),4327–4342.\n26. Cao,J.,Ma,C.,Yao,T.,Chen,S.,Ding,S.,&Yang,X.(2022).End-to-end\nreconstruction-classiﬁcationlearningforfaceforgerydetection.In\nProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern\nrecognition(pp.4103–4112).Piscataway:IEEE.\n27. Chen,S.,Yao,T.,Chen,Y.,Ding,S.,Li,J.,&Ji,R.(2021).Localrelationlearning\nforfaceforgerydetection.In ProceedingsoftheAAAIconferenceonartiﬁcial\nintelligence(pp.1081–1088).PaloAlto:AAAIPress.\n28. Haliassos,A.,Vougioukas,K.,Petridis,S.,&Pantic,M.(2021).Lipsdon’tlie:a\ngeneralisableandrobustapproachtofaceforgerydetection.In\nProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern\nrecognition(pp.5039–5049).Piscataway:IEEE.\n29. Gu,Q.,Chen,S.,Yao,T.,Chen,Y.,Ding,S.,&Yi,R.(2022).Exploiting\nﬁne-grainedfaceforgerycluesviaprogressiveenhancementlearning.In\nProceedingsoftheAAAIconferenceonartiﬁcialintelligence (pp.735–743).\nPaloAlto:AAAIPress.\n30. Zhao,H.,Zhou,W.,Chen,D.,Wei,T.,Zhang,W.,&Yu,N.(2021).\nMulti-attentionaldeepfakedetection.In ProceedingsoftheIEEE/CVF\nconferenceoncomputervisionandpatternrecognition (pp.2185–2194).\nPiscataway:IEEE.\n31. Qian,Y.,Yin,G.,Sheng,L.,Chen,Z.,&Shao,J.(2020).Thinkinginfrequency:\nfaceforgerydetectionbyminingfrequency-awareclues.In Proceedingsof\nthe16thEuropeanconferenceoncomputervision (pp.86–103).Cham:\nSpringer.\n32. Frank,J.,Eisenhofer,T.,Schönherr,L.,Fischer,A.,Kolossa,D.,&Holz,T.\n(2020).Leveragingfrequencyanalysisfordeepfakeimagerecognition.In\nProceedingsofthe37thinternationalconferenceonmachinelearning (pp.\n3247–3258).RetrievedApril5,2025,from http://proceedings.mlr.press/\nv119/frank20a.html\n33. Masi,I.,Killekar,A.,MarianMascarenhas,R.,Gurudatt,S.P.,&AbdAlmageed,\nW.(2020).Two-branchrecurrentnetworkforisolatingdeepfakesinvideos.\nInA.Vedaldi,H.Bischof,T.Brox,&J.-M.Frahm(Eds.), Proceedingsofthe16th\nEuropeanconferenceoncomputervision (pp.667–684).Cham:Springer.\n34. Li,L.,Bao,J.,Zhang,T.,Yang,H.,Chen,D.,Wen,F.,&Guo,B.(2020).Face\nX-rayformoregeneralfaceforgerydetection.In Proceedingsofthe\nIEEE/CVFconferenceoncomputervisionandpatternrecognition (pp.\n5000–5009).Piscataway:IEEE.\n35. Fang,M.,Yu,L.,Song,Y.,Zhang,Y.,&Xie,H.(2024).IEIRNet:inconsistency\nexploitingbasedidentityrectiﬁcationforfaceforgerydetection. IEEE\nTransactionsonMultimedia ,26,11232–11245.\n36. Kirillov,A.,Mintun,E.,Ravi,N.,Mao,H.,Rolland,C.,Gustafson,L.,Xiao,T.,\nWhitehead,S.,Berg,A.C.,Lo,W.Y.,etal.(2023).Segmentanything.In\nProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision (pp.\n3992–4003).Piscataway:IEEE.\n37. Lai,Y.,Luo,Z.,&Yu,Z.(2023).Detectanydeepfakes:segmentanything\nmeetsfaceforgerydetectionandlocalization.InW.Jia,W.Kang,Z.Pan,X.\nBen,Z.Bian,S.Y u,Z.He,&J.W ang(Eds.), Proceedingsofthe17thChinese\nconferenceonbiometricrecognition (pp.180–190).Cham:Springer.\n38. Wang,L.,Yu,L.,Zhang,Y.,&Xie,H.(2024).Generalizablespeechspooﬁng\ndetectionagainstsilencetrimmingwithdataaugmentationand\nmulti-taskmeta-learning. IEEE/ACMTransactionsonAudio,Speechand\nLanguageProcessing,32,3296–3310.\n39. Alayrac,J-B.,Donahue,J.,Luc,P.,Miech,A.,Barr,I.,Hasson,Y.,Lenc,K.,\nMensch,A.,Millican,K.,Reynolds,M.,etal.(2022).Flamingo:avisual\nlanguagemodelforfew-shotlearning.InS.Koyejo,S.Mohamed,A.\nAgarwal,D.Belgrave,K.Cho,&A.Oh(Eds.), Proceedingsofthe36th\ninternationalconferenceonneuralinformationprocessingsystems (pp.\n23716–23736).RedHook:CurranAssociates.\n40. Li,J.,Li,D.,Savarese,S.,&Hoi,S.(2023).BLIP-2:bootstrapping\nlanguage-imagepre-trainingwithfrozenimageencodersandlarge\nlanguagemodels.InA.Krause,E.Brunskill,K.Cho,B.Engelhardt,S.Sabato,\n&J.Scarlett(Eds.), Proceedingsofthe40thInternationalconferenceon\nmachinelearning (pp.19730–19742).RetrievedApril5,2025,from\nhttps://proceedings.mlr.press/v202/li23q.html.\n41. Dai,W .,Li,J.,Li,D .,Tiong,A.M.H.,Zhao,J.,W ang,W .,Li,B.,F ung,P .,&Hoi,S.\n(2023).InstructBLIP:towardsgeneral-purposevision-languagemodels\nwithinstructiontuning.InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.\nHardt,&S.Levine(Eds.), Proceedingsofthe37thinternationalconferenceon\nneuralinformationprocessingsystems .(pp.1–18).RedHook:Curran\nAssociates.\n42. OpenAI,Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,\nAlmeida,D.,Altenschmidt,J.,Altman,S.,etal.(2023).GPT-4technical\nreport.arXivpreprint. arXiv:2303.08774.\n43. Zhu,D.,Chen,J.,Shen,X.,Li,X.,&Elhoseiny,M.(2023).MiniGPT-4:\nenhancingvision-languageunderstandingwithadvancedlargelanguage\nmodels.arXivpreprint. arXiv:2304.10592.\n44. Bai,J.,Bai,S.,Y ang,S.,W ang,S.,T an,S.,W ang,P .,Lin,J.,Zhou,C.,&Zhou,J.\n(2023).Qwen-VL:afrontierlargevision-languagemodelwithversatile\nabilities.arXivpreprint. arXiv:2308.12966.\n45. Liu,H.,Li,C.,Wu,Q.,&Lee,Y.J.(2023).Visualinstructiontuning.arXiv\npreprint.arXiv:2304.08485.\n46. Fu,C.,Chen,P.,Shen,Y.,Qin,Y.,Zhang,M.,Lin,X.,Qiu,Z.,Lin,W.,Yang,J.,\nZheng,X.,etal.(2023).MME:acomprehensiveevaluationbenchmarkfor\nmultimodallargelanguagemodels.arXivpreprint. arXiv:2306.13394.\n47. Y e,Q .,Xu,H.,Xu,G.,Y e,J.,Y an,M.,Zhou,Y .,W ang,J.,Hu,A.,Shi,P .,Shi,Y .,et\nal.(2023).mPLUG-Owl:modularizationempowerslargelanguagemodels\nwithmultimodality.arXivpreprint. arXiv:2304.14178.\n48. Li,B.,Ge,Y.,Ge,Y.,Wang,G.,Wang,R.,Zhang,R.,&Shan,Y.(2023).\nSeed-ench-2:benchmarkingmultimodallargelanguagemodels.arXiv\npreprint.arXiv:2311.17092.\n49. Guan,T .,Liu,F .,Wu,X.,Xian,R.,Li,Z.,Liu,X.,W ang,X.,Chen,L.,Huang,F .,\nYacoob,Y.,etal.(2024).Hallusionbench:anadvanceddiagnosticsuitefor\nentangledlanguagehallucinationandvisualillusioninlarge\nvision-languagemodels.In ProceedingsoftheIEEE/CVFconferenceon\ncomputervisionandpatternrecognition (pp.14375–14385).Piscataway:\nIEEE.\n50. Cai,R.,Song,Z.,Guan,D.,Chen,Z.,Luo,X.,Yi,C.,&Kot,A.C.(2023).\nBenchLMM:benchmarkingcross-stylevisualcapabilityoflarge\nmultimodalmodels.arXivpreprint. arXiv:2312.02896.\n51. Wu,H.,Zhang,Z.,Zhang,E.,Chen,C.,Liao,L.,W ang,A.,Li,C.,Sun,W .,Y an,\nQ.,Zhai,G.,etal.(2023).Q-bench:abenchmarkforgeneral-purpose\nfoundationmodelsonlow-levelvision.arXivpreprint. arXiv:2309.14181.\nShietal. VisualIntelligence             (2025) 3:9 Page25of25\n52. Huang,Y.,Yuan,Q.,Sheng,X.,Yang,Z.,Wu,H.,Chen,P.,Yang,Y.,Li,L.,&Lin,\nW.(2024).AesBench:anexpertbenchmarkformultimodallargelanguage\nmodelsonimageaestheticsperception.arXivpreprint. arXiv:2401.08276.\n53. George,A.,Mostaani,Z.,Geissenbuhler,D.,Nikisins,O.,Anjos,A.&Marcel,S.\n(2020).Biometricfacepresentationattackdetectionwithmulti-channel\nconvolutionalneuralnetwork. IEEETransactionsonInformationForensics\nandSecurity,15,42–55.\n54. Guo,X.,Liu,Y.,Jain,A.K.,&Liu,X.(2022).Multi-domainlearningfor\nupdatingfaceanti-spooﬁngmodels.InS.Avidan,G.J.Brostow,M.Ciss’e,G.\nM.Farinella,&T.Hassner(Eds.), Proceedingsofthe17thEuropeanconference\noncomputervision (pp.230–249).Cham:Springer.\n55. Rössler,A.,Cozzolino,D.,Verdoliva,L.,Riess,C.,Thies,J.,&Nießner,M.\n(2019).Faceforensics++:learningtodetectmanipulatedfacialimages.In\nProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern\nrecognition(pp.1–11).Piscataway:IEEE.\n56. Song,H.,Huang,S.,Dong,Y.,&Tu,W.-W.(2023).Robustnessand\ngeneralizabilityofdeepfakedetection:astudywithdiﬀusionmodels.arXiv\npreprint.arXiv:2309.02218.\n57. Wei,J.,W ang,X.,Schuurmans,D .,Bosma,M.,Ichter ,B.,Xia,F .,Chi,E.H.,Le,Q .\nV.,&Zhou,D.(2022).Chain-of-thoughtpromptingelicitsreasoninginlarge\nlanguagemodels.InS.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.\nCho,&A.Oh(Eds.), Proceedingsofthe36thinternationalconferenceon\nneuralinformationprocessingsystems (pp.1–14).RedHook:Curran\nAssociates.\n58. Wu,Y.,Zhang,P.,Xiong,W.,Oguz,B.,Gee,J.C.,&Nie,Y.(2023).Theroleof\nchain-of-thoughtincomplexvision-languagereasoningtask.arXiv\npreprint.arXiv:2311.09193.\n59. Shi,Y.,Feng,J.,Xiao,L.,He,J.,&Hu,J.(2022).Outofdomainface\nanti-spooﬁng:asurvey. JournalofFrontiersofComputerScienceand\nTechnology,16(11),2471.\n60. Wang,Z.,Wang,Z.,Yu,Z.,Deng,W.,Li,J.,Li,S.,&Wang,Z.(2022).Domain\ngeneralizationviashuﬄedstyleassemblyforfaceanti-spooﬁng.In\nProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern\nrecognition(pp.4113–4123).Piscataway:IEEE.\n61. Shao,R.,Lan,X.,Li,J.,&Yuen,P.C.(2019).Multi-adversarialdiscriminative\ndeepdomaingeneralizationforfacepresentationattackdetection.In\nProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern\nrecognition(pp.10023–10031).Piscataway:IEEE.\n62. Li,J.,Li,D.,Xiong,C.,&Hoi,S.(2022).BLIP:bootstrappinglanguage-image\npre-trainingforuniﬁedvision-languageunderstandingandgeneration.In\nK.Chaudhuri,S.Jegelka,L.Song,C.Szepesv’ari,G.Niu,&S.Sabato(Eds.),\nProceedingsoftheinternationalconferenceonmachinelearning (pp.\n12888–12900).RetrievedApril5,2025,from https://proceedings.mlr.press/\nv162/li22n.html.\n63. Shao,J.,Chen,S.,Li,Y .,W ang,K.,Yin,Z.,He,Y .,T eng,J.,Sun,Q .,Gao,M.,Liu,\nJ.,etal.(2021).Intern:anewlearningparadigmtowardsgeneralvision.\narXivpreprint. arXiv:2111.08687.\n64. Chingovska,I.,Anjos,A.R.,&Marcel,S.(2014).Biometricsevaluationunder\nspooﬁngattacks. IEEETransactionsonInformationForensicsandSecurity ,\n9(12),2264–2276.\n65. Srivatsan,K.,Naseer,M.,&Nandakumar,K.(2023).Flip:cross-domainface\nanti-spooﬁngwithlanguageguidance.In ProceedingsoftheIEEE/CVF\ninternationalconferenceoncomputervision (pp.19685–19696.Piscataway:\nIEEE).\n66. Dong,Q.,Li,L.,Dai,D.,Zheng,C.,Wu,Z.,Chang,B.,Sun,X.,Xu,J.,&Sui,Z.\n(2023).Asurveyonin-contextlearning.arXivpreprint. arXiv:2303.12712.\nPublisher’snote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsin\npublishedmapsandinstitutionalaﬃliations.",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.8122251033782959
    },
    {
      "name": "Spoofing attack",
      "score": 0.7283297777175903
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.6743462085723877
    },
    {
      "name": "Computer science",
      "score": 0.6487343311309814
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5262724161148071
    },
    {
      "name": "Natural language processing",
      "score": 0.35417282581329346
    },
    {
      "name": "Speech recognition",
      "score": 0.3404689431190491
    },
    {
      "name": "Computer security",
      "score": 0.33855170011520386
    },
    {
      "name": "Linguistics",
      "score": 0.1440834403038025
    },
    {
      "name": "Geology",
      "score": 0.12244969606399536
    },
    {
      "name": "Geodesy",
      "score": 0.061611682176589966
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}