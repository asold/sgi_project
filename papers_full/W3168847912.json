{
    "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models",
    "url": "https://openalex.org/W3168847912",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2968490406",
            "name": "Ulme Wennberg",
            "affiliations": [
                "KTH Royal Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2069039611",
            "name": "Gustav Eje Henter",
            "affiliations": [
                "KTH Royal Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3097795905",
        "https://openalex.org/W2888541716",
        "https://openalex.org/W2991265431",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3021293129",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2973154008",
        "https://openalex.org/W3117312003",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2984582583",
        "https://openalex.org/W3152698349",
        "https://openalex.org/W3122515622",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3102094970",
        "https://openalex.org/W3035229828",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3155806510",
        "https://openalex.org/W3106061119",
        "https://openalex.org/W2963206679",
        "https://openalex.org/W3035206215"
    ],
    "abstract": "Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 130–140\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n130\nThe Case for Translation-Invariant Self-Attention\nin Transformer-Based Language Models\nUlme Wennberg Gustav Eje Henter\nDivision of Speech, Music and Hearing, KTH Royal Institute of Technology, Sweden\n{ulme, ghe}@kth.se\nAbstract\nMechanisms for encoding positional informa-\ntion are central for transformer-based language\nmodels. In this paper, we analyze the po-\nsition embeddings of existing language mod-\nels, ﬁnding strong evidence of translation in-\nvariance, both for the embeddings themselves\nand for their effect on self-attention. The de-\ngree of translation invariance increases dur-\ning training and correlates positively with\nmodel performance. Our ﬁndings lead us\nto propose translation-invariant self-attention\n(TISA), which accounts for the relative posi-\ntion between tokens in an interpretable fashion\nwithout needing conventional position embed-\ndings. Our proposal has several theoretical ad-\nvantages over existing position-representation\napproaches. Experiments show that it im-\nproves on regular ALBERT on GLUE tasks,\nwhile only adding orders of magnitude less po-\nsitional parameters.\n1 Introduction\nThe recent introduction of transformer-based lan-\nguage models by Vaswani et al. (2017) has set\nnew benchmarks in language processing tasks such\nas machine translation (Lample et al., 2018; Gu\net al., 2018; Edunov et al., 2018), question answer-\ning (Yamada et al., 2020), and information extrac-\ntion (Wadden et al., 2019; Lin et al., 2020). How-\never, because of the non-sequential and position-\nindependent nature of the internal components of\ntransformers, additional mechanisms are needed to\nenable models to take word order into account.\nLiu et al. (2020) identiﬁed three important crite-\nria for ideal position encoding: Approaches should\nbe inductive, meaning that they can handle se-\nquences and linguistic dependencies of arbitrary\nlength, data-driven, meaning that positional depen-\ndencies are learned from data, andefﬁcient in terms\nof the number of trainable parameters. Separately,\nShaw et al. (2018) argued for translation-invariant\npositional dependencies that depend on the relative\ndistances between words rather than their absolute\npositions in the current text fragment. It is also im-\nportant that approaches be parallelizable, and ide-\nally also interpretable. Unfortunately, none of the\nexisting approaches for modeling positional depen-\ndencies satisfy all these criteria, as shown in Table\n1 and in Sec. 2. This is true even for recent years’\nstate-of-the-art models such as BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019), ALBERT (Lan\net al., 2020), and ELECTRA (Clark et al., 2020),\nwhich require many positional parameters but still\ncannot handle arbitrary-length sequences.\nThis paper makes two main contributions: First,\nin Sec. 3, we analyze the learned position embed-\ndings in major transformer-based language models.\nSecond, in Sec. 4, we leverage our ﬁndings to pro-\npose a new positional-dependence mechanism that\nsatisﬁes all desiderata enumerated above. Experi-\nments verify that this mechanism can be used along-\nside conventional position embeddings to improve\ndownstream performance. Our code is available.\n2 Background\nTransformer-based language models (Vaswani\net al., 2017) have signiﬁcantly improved model-\ning accuracy over previous state-of-the-art models\nlike ELMo (Peters et al., 2018). However, the non-\nsequential nature of transformers created a need for\nother mechanisms to inject positional information\ninto the architecture. This is now an area of active\nresearch, which the rest of this section will review.\nThe original paper by Vaswani et al. (2017) pro-\nposed summing each token embedding with a posi-\ntion embedding, and then used the resulting embed-\nding as the input into the ﬁrst layer of the model.\nBERT (Devlin et al., 2019) reached improved per-\nformance training data-driven d-dimensional em-\n131\nInduct- Data- Parameter Translation Parallel Interpret-\nMethod ive? driven? efﬁcient? invariant? -izable? able?\nSinusoidal position embedding (Vaswani et al., 2017) \u0013 \u0017 \u0013 \u0017 \u0013 \u0017\nAbsolute position embedding (Devlin et al., 2019) \u0017 \u0013 \u0017 \u0017 \u0013 \u0017\nRelative position embedding (Shaw et al., 2018) \u0017 \u0013 \u0013 \u0013 \u0017 \u0017\nT5 (Raffel et al., 2020) \u0017 \u0013 \u0013 \u0013 \u0013 \u0013\nFlow-based (Liu et al., 2020) \u0013 \u0013 \u0013 \u0017 \u0017 \u0017\nSynthesizer (Tay et al., 2020) \u0017 \u0013 \u0013 \u0017 \u0013 \u0017\nUntied positional scoring (Ke et al., 2021) \u0017 \u0013 \u0017 \u0017 \u0013 \u0017\nRotary position embedding (Su et al., 2021) \u0013 \u0017 \u0013 \u0013 \u0013 \u0017\nTranslation-invariant self-attention (proposed) \u0013 \u0013 \u0013 \u0013 \u0013 \u0013\nTable 1: Characteristics of position-representation approaches for different language-modeling architectures.\nbeddings for each position in text snippets of at\nmost ntokens. A family of models have tweaked\nthe BERT recipe to improve performance, includ-\ning RoBERTa (Liu et al., 2019) and ALBERT (Lan\net al., 2020), where the latter has layers share the\nsame parameters to achieve a more compact model.\nAll these recent data-driven approaches are re-\nstricted to ﬁxed max sequence lengths of ntokens\nor less (typically n= 512). Longformer (Beltagy\net al., 2020) showed modeling improvements by\nincreasing nto 4096, suggesting that the cap on\nsequence length limits performance. However, the\nLongformer approach also increased the number\nof positional parameters 8-fold, as the number of\nparameters scales linearly with n; cf. Table 2.\nClark et al. (2019) and Htut et al. (2019) ana-\nlyzed BERT attention, ﬁnding some attention heads\nto be strongly biased to local context, such as the\nprevious or the next token. Wang and Chen (2020)\nfound that even simple concepts such as word-order\nand relative distance can be hard to extract from\nabsolute position embeddings. Shaw et al. (2018)\nindependently proposed using relative position em-\nbeddings that depend on the signed distance be-\ntween words instead of their absolute position, mak-\ning local attention easier to learn. They reached\nimproved BLEU scores in machine translation, but\ntheir approach (and reﬁnements by Huang et al.\n(2019)) are hard to parallelize, which is unattrac-\ntive in a world driven by parallel computing. Zeng\net al. (2020) used relative attention in speech syn-\nthesis, letting each query interact with separate\nmatrix transformations for each key vector, depend-\ning on their relative-distance offset. Raffel et al.\n(2020) directly model position-to-position interac-\ntions, by splitting relative-distance offsets into q\nbins. These relative-attention approaches all facili-\ntate processing sequences of arbitrary length, but\ncan only resolve linguistic dependencies up to a\nﬁxed predeﬁned maximum distance.\nTay et al. (2020) directly predicted both word\nand position contributions to the attention matrix\nwithout depending on token-to-token interactions.\nHowever, the approach is not inductive, as the size\nof the attention matrix is a ﬁxed hyperparameter.\nLiu et al. (2020) used sinusoidal functions with\nlearnable parameters as position embeddings. They\nobtain compact yet ﬂexible models, but use a neural\nODE, which is computationally unappealing.\nKe et al. (2021) showed that self-attention works\nbetter if word and position embeddings are untied\nto reside in separate vector spaces, but their pro-\nposal is neither inductive nor parameter-efﬁcient.\nSu et al. (2021) propose rotating each embed-\nding in the self-attention mechanism based on its\nabsolute position, thereby inducing translational\ninvariance, as the inner product of two vectors is\nconserved under rotations of the coordinate system.\nThese rotations are, however, not learned.\nThe different position-representation approaches\nare summarized in Table 1. None of them satisfy\nall design criteria. In this article, we analyze the po-\nsition embeddings in transformer models, leading\nus to propose a new positional-scoring mechanism\nthat combines all desirable properties (ﬁnal row).\n3 Analysis of Existing Language Models\nIn this section, we introspect selected high-proﬁle\nlanguage models to gain insight into how they have\nlearned to account for the effect of position.\n3.1 Analysis of Learned Position Embeddings\nFirst, we stack the position embeddings in the ma-\ntrix EP ∈Rn×d, and inspect the symmetric matrix\nP = EPET\nP ∈Rn×n, where Pi,j represents the\ninner product between the ith and jth embedding\nvectors. If inner products are translation invariant,\nPi,j will only depend on the difference between the\nindices, j −i, giving a Toeplitz matrix, a matrix\nwhere each diagonal is constant.\n132\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500\n(a) BERT base\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (b) RoBERTa base\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (c) ALBERT base v1\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (d) ALBERT xxlarge v2\nFigure 1: Heatmaps visualizing the matrix P = EP ET\nP of position-embedding inner products for different models.\nThe greater the inner product between the embeddings, the brighter the color. See appendix Figs. 4, 5 for more.\nFig. 1 visualizes the P-matrices for the position\nembeddings in a number of prominent transformer\nmodels, listed from oldest to newest, which also\nis in order of increasing performance. We note\nthat a clear Toeplitz structure emerges from left\nto right. Translation invariance is also seen when\nplotting position-embedding cosine similarities, as\ndone by Wang and Chen (2020) for transformer-\nbased language models and by Dosovitskiy et al.\n(2020) for 2D transformers modeling image data.\nIn Fig. 2 we further study how the degree of\nToeplitzness (quantiﬁed by R2, the amount of the\nvariance among matrix elements Pi,j explained by\nthe best-ﬁtting Toeplitz matrix) changes for differ-\nent ALBERT models. With longer training time\n(i.e., going from ALBERT v1 to v2), Toeplitzness\nincreases, as the arrows show. This is associated\nwith improved mean dev-set score. Such evolution\nis also observed in Wang and Chen (2020, Fig. 8).\n3.2 Translation Invariance in Self-Attention\nNext, we analyze how this translation invariance is\nreﬂected in self-attention. Recall that Vaswani et al.\n(2017) self-attention can be written as\natt(Q,K,V ) = softmax\n(\nQKT\n√dk\n)\nV, (1)\nand deﬁne position embeddings EP, word em-\nbeddings EW, and query and key transformation\nweight matrices WQ and WK. By taking\nQKT = (EW + EP)WQWT\nK(EW + EP)T (2)\nand replacing each row ofEW by the average word\nembedding across the entire vocabulary, we obtain\na matrix we call ˆFP that quantiﬁes the average ef-\nfect of EP on the softmax in Eq.(1). Plots of the re-\nsulting ˆFP for all 12 ALBERT-base attention heads\nin the ﬁrst layer are in appendix Fig. 8. Importantly,\nthese matrices also exhibit Toeplitz structure. Fig.\n3 graphs sections through the main diagonal for\nselected heads, showing peaks at short relative dis-\ntances, echoing Clark et al. (2019) and Htut et al.\n(2019). In summary, we conclude that position en-\ncodings, and their effect on softmax attention, have\nan approximately translation-invariant structure in\nsuccessful transformer-based language models.\n4 Proposed Self-Attention Mechanism\nWe now introduce our proposal for parameterizing\nthe positional contribution to self-attention in an ef-\nﬁcient and translation-invariant manner, optionally\nremoving the position embeddings entirely.\n4.1 Leveraging Translation Invariance for\nImproved Inductive Bias\nOur starting point is the derivation of Ke et al.\n(2021). They expand QKT while ignoring cross\nterms, yielding\nQKT ≈EWWQWT\nKET\nW + EPWQWT\nKET\nP, (3)\nan approximation they support by theory and em-\npirical evidence. They then “untie” the effects of\nwords and positions by using different W-matrices\nfor the two terms in Eq. (3). We agree with sepa-\n0.3 0.4 0.5 0.6 0.7\nDegree of Toeplitzness (R2)\n80\n82\n84\n86\n88\n90Average dev set result ALBERT base\nALBERT large\nALBERT xlarge\nALBERT xxlarge\nFigure 2: Scatterplot of the degree of Toeplitzness of\nP for different ALBERT models (v1 →v2) against av-\nerage performance numbers (from Lan et al.’s GitHub)\nover SST-2, MNLI, RACE, and SQuAD 1.1 and 2.0.\n133\n-5 -4 -3 -2 -1 0 1 2 3 4 5\nRelative Position, i j\n15\n10\n5\n0\n5\nPosition Score, f(i j)\n-5 -4 -3 -2 -1 0 1 2 3 4 5\nRelative Position, i j\n0.0\n0.1\n0.2\n0.3\n0.4Position Score, f(i j)\nFigure 3: Positional responses of select attention heads.\nLeft: Sections ( ˆFP )i,j through ˆFP of ALBERT base\nv2, varying j for 5 different i, keeping j= icentered.\nThe sections are similar regardless of i since ˆFP is\nclose to Toeplitz. Colors distinguish different heads.\nRight: TISA scoring functions, attending to similar po-\nsitions as heads on the left. Larger plots in Figs. 6, 7.\nrating these effects, but also see a chance to reduce\nthe number of parameters.\nConcretely, we propose to add a second term\nFP ∈Rn×n, a Toeplitz matrix, inside the parenthe-\nses of Eq. (1). FP can either a) supplement or b)\nreplace the effect of position embeddings on atten-\ntion in our proposed model. For case a), we simply\nadd FP to the existing expression inside the soft-\nmax, while for case b) a term √dkFP is inserted in\nplace of the term EPWQWT\nKET\nP in Eq. (3). This\nproduces two new self-attention equations:\natt=\n\n\n\nsoftmax\n(\nQKT\n√dk\n+FP\n)\nV a)\nsoftmax\n(QW KT\nW√dk\n+FP\n)\nVW b)\n(4)\nwhere the inputs QW, KW, and VW (deﬁned by\nQW = EWWQ, and similarly for KW and VW) do\nnot depend on the position embeddings EP. Case\na) is not as interpretable as TISA alone (case b),\nsince the resulting models have two terms, EP and\nFP, that share the task of modeling positional infor-\nmation. Our two proposals apply to any sequence\nmodel with a self-attention that follows Eq. (1),\nwhere the criteria in Table 1 are desirable.\n4.2 Positional Scoring Function\nNext, we propose to parameterize the Toeplitz ma-\ntrix FP using a positional scoring function fθ(·)\non the integers Z, such that (FP)i,j=fθ(j−i). fθ\ndeﬁnes FP-matrices of any size n. The value of\nfθ(j−i) directly models the positional contribution\nfor how the token at position iattends to position\nj. We call this translation-invariant self-attention,\nor TISA. TISA is inductive and can be simpliﬁed\ndown to arbitrarily few trainable parameters.\nLet k = j −i. Based on our ﬁndings for ˆFP\nin Sec. 3, we seek a parametric family {fθ}that\nallows both localized and global attention, without\ndiverging as |k|→∞ . We here study one family\nStandard Ke et al. (2021) TISA\nGeneral formula nd nd + 2d2 3SHL\nLongformer 3,145,728 4 ,325,376 2,160\nBERT/RoBERTa 393,216 1 ,572,864 2,160\nALBERT 65,536 98 ,304 2,160\nTable 2: Number of positional parameters for base mod-\nels of different language-model architectures and dif-\nferent positional information processing methods, with\nmax sequence length n ∈(512,4096), position em-\nbeddings of dimension d∈(128,768), S= 5 kernels,\nH=12 attention heads, and L=12 layers with distinct\nTISA positional scoring functions. Parameter sharing\ngives ALBERT lower numbers. TISA can be used\nalone or added to the counts in other columns.\nthat satisﬁes the criteria: the radial-basis functions\nfθ(k) =\n∑S\ns=1\nasexp\n(\n−|bs|(k−cs)2\n)\n. (5)\nTheir trainable parameters are θ= {as,bs,cs}S\ns=1,\ni.e., 3 trainable parameters per kernels. Since these\nkernels are continuous functions (in contrast to the\ndiscrete bins of Raffel et al. (2020)), predictions\nchange smoothly with distance, which seems intu-\nitively meaningful for good generalization.\nLin et al. (2019) found that word-order informa-\ntion in BERTs position embeddings gets increas-\ningly washed out from layer 4 onward. As sug-\ngested by Dehghani et al. (2019) and Lan et al.\n(2020), we inject positional information into each\nof the H heads at all L layers, resulting in one\nlearned function fθ(h,l) for each head and layer.\nThe total number of positional parameters of TISA\nis then 3SHL. As seen in Table 2, this is several\norders of magnitude less than the embeddings in\nprominent language models.\nThe inductivity and localized nature of TISA\nsuggests the possibility to rapidly pre-train models\non shorter text excerpts (small n), scaling up to\nlonger nlater in training and/or at application time,\nsimilar to the two-stage training scheme used by\nDevlin et al. (2019), but without risking the under-\ntraining artifacts visible for BERT at n >128 in\nFigs. 1 and 4. However, we have not conducted any\nexperiments on the performance of this option.\n5 Experiments\nThe main goal of our experiments is to illustrate\nthat TISA can be added to models to improve their\nperformance (Table 3a), while adding a minuscule\namount of extra parameters. We also investigate\nthe performance of models without position em-\n134\nTask Baseline S =1 3 5 ∆ ∆%\nSST-2 92.9 93.3 93.1 93.1 0.4 6.5%\nMNLI 83.8 84.1 84.4 84.8 1.0 5.9%\nQQP 88.2 88.0 88.3 88.3 0.1 1.2%\nSTS-B 90.3 90.4 90.0 90.4 0.1 1.5%\nCoLA 57.2 57.0 56.5 58.5 1.3 2.9%\nMRPC 89.6 90.1 89.0 90.1 0.5 5.3%\nQNLI 91.6 91.7 91.4 91.6 0.1 0.4%\nRTE 72.9 71.1 73.6 73.6 0.7 2.7%\n(a) ALBERT base v2 models with position embeddings\nTask Baseline S =1 3 5 ∆ ∆%\nSST-2 85.1 85.9 85.8 86.0 0.9 6.2%\nMNLI 78.8 80.9 81.4 81.6 2.8 13.4%\nQQP 86.3 86.2 86.5 86.8 0.5 3.4%\nSTS-B 89.0 89.0 89.1 89.1 0.1 0.3%\nMRPC 82.8 83.1 83.3 83.1 0.5 3.3%\nQNLI 86.6 87.2 87.4 87.7 1.1 7.8%\nRTE 62.1 61.7 62.5 62.8 0.7 1.9%\n(b) ALBERT base v2 models without position embeddings\nTable 3: GLUE task dev-set performance (median over\n5 runs) with TISA ( S kernels) and without (baseline).\n∆ is the maximum performance increase in a row and\n∆% is the corresponding relative error reduction rate.\nbeddings (Table 3b), comparing TISA to a bag-\nof-words baseline (S = 0). All experiments use\npretrained ALBERT base v2 implemented in Hug-\ngingface (Wolf et al., 2020). Kernel parameters\nθ(h) for the functions in Eq. (5) were initialized\nby regression to the ˆFP proﬁles of the pretrained\nmodel, (see Appendix C for details); example plots\nof resulting scoring functions are provided in Fig.\n3. We then benchmark each conﬁguration with and\nwithout TISA for 5 runs on GLUE tasks (Wang\net al., 2018), using jiant (Phang et al., 2020) and\nstandard dataset splits to evaluate performance.\nOur results in Table 3a show relative error reduc-\ntions between 0.4 and 6.5% when combining TISA\nand conventional position embeddings. These\ngains are relatively stable regardless of S. We also\nnote that Lan et al. (2020) report 92.9 on SST-2 and\n84.6 on MNLI, meaning that our contribution leads\nto between 1.3 and 2.8% relative error reductions\nover their scores. The best performing architecture\n(S=5), gives improvements over the baseline on 7\nof the 8 tasks considered and on average increases\nthe median F1 score by 0.4 points. All these gains\nhave been realized using a very small number of\nadded parameters, and without pre-training on any\ndata after adding TISA to the architecture. The\nonly joint training happens on the training data of\neach particular GLUE task.\nResults for TISA alone, in Table 3b, are not as\nstrong. This could be because these models are\nderived from an ALBERT model pretrained using\nconventional position embeddings, since we did\nnot have the computational resources to tune from-\nscratch pretraining of TISA-only language models.\nFigs. 3 and 6 plot scoring functions of different\nattention heads from the initialization described\nin Appendix C. Similar patterns arose consistently\nand rapidly in preliminary experiments on pretrain-\ning TISA-only models from scratch. The plots\nshow heads specializing in different linguistic as-\npects, such as the previous or next token, or multi-\nple tokens to either side, with other heads showing\nlittle or no positional dependence. This mirrors the\nvisualizations of ALBERT base attention heads in\nFigs. 3, 6, 7, 8 and the ﬁndings of Htut et al. (2019)\nand Clark et al. (2019) on BERT, but TISA makes\nthis directly visible in an interpretable model, with-\nout having to probe correlations in a black box.\nInterestingly, the ALBERT baseline on STS-B\nin Table 3a is only 1.3 points ahead of the bag-\nof-words baseline in Table 3b. This agrees with\nexperiments shufﬂing the order of words (Pham\net al., 2020; Sinha et al., 2021) ﬁnding that modern\nlanguage models tend to focus mainly on higher-\norder word co-occurrences, rather than word order,\nand suggests that word-order information is under-\nutilized in state-of-the-art language models.\n6 Conclusion\nWe have analyzed state-of-the-art transformer-\nbased language models, ﬁnding that translation-\ninvariant behavior emerges during training. Based\non this we proposed TISA, the ﬁrst positional infor-\nmation processing method to simultaneously sat-\nisfy the six key design criteria in Table 1. Exper-\niments demonstrate competitive downstream per-\nformance. The method is applicable also to trans-\nformer models outside language modeling, such as\nmodeling time series in speech or motion synthe-\nsis, or to describe dependencies between pixels in\ncomputer vision.\nAcknowledgments\nWe would like to thank Gabriel Skantze, Dmytro\nKalpakchi, Viktor Karlsson, Filip Cornell, Oliver\nÅstrand, and the anonymous reviewers for their\nconstructive feedback. This research was partially\nsupported by the Wallenberg AI, Autonomous Sys-\ntems and Software Program (W ASP) funded by the\nKnut and Alice Wallenberg Foundation.\n135\nReferences\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? An analysis of BERT’s attention. In Proc.\nBlackboxNLP@ACL, pages 276–286.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Pre-training trans-\nformers as energy-based cloze models. In Proc.\nEMNLP, pages 285–294.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2019. Univer-\nsal transformers. In Proc. ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. NAACL-HLT, pages 4171–4186.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2020. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. CoRR, abs/2010.11929.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Proc. EMNLP, pages 489–500.\nJiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho,\nand Victor O. K. Li. 2018. Meta-learning for\nlow-resource neural machine translation. In Proc.\nEMNLP, pages 3622–3631.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019. Do attention heads in\nBERT track syntactic dependencies?\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Ian Simon, Curtis Hawthorne, Noam\nShazeer, Andrew M. Dai, Matthew D. Hoffman,\nMonica Dinculescu, and Douglas Eck. 2019. Music\ntransformer. In Proc. ICLR.\nGuolin Ke, Di He, and Tie-Yan Liu. 2021. Rethink-\ning positional encoding in language pre-training. In\nProc. ICLR.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proc. EMNLP, pages 5039–5049.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Proc. ICLR.\nYing Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020.\nA joint neural model for information extraction with\nglobal features. In Proc. ACL, pages 7999–8009.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proc. BlackboxNLP@ACL, pages\n241–253.\nXuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and\nCho-Jui Hsieh. 2020. Learning to encode position\nfor transformer with continuous dynamical model.\nIn Proc. ICML, pages 6327–6335.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proc. NAACL, pages 2227–2237.\nThang M. Pham, Trung Bui, Long Mai, and Anh\nNguyen. 2020. Out of order: How important is the\nsequential order of words in a sentence in natural\nlanguage understanding tasks?\nJason Phang, Phil Yeres, Jesse Swanson, Haokun\nLiu, Ian F. Tenney, Phu Mon Htut, Clara Va-\nnia, Alex Wang, and Samuel R. Bowman. 2020.\njiant 2.0: A software toolkit for research on\ngeneral-purpose text understanding models. http:\n//jiant.info/.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. JMLR, 21(140):1–67.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proc. NAACL-HLT, pages 464–468.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional\nhypothesis: Order word matters pre-training for lit-\ntle.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and\nYunfeng Liu. 2021. Roformer: Enhanced trans-\nformer with rotary position embedding. CoRR,\nabs/2104.09864.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020. Synthesizer: Re-\nthinking self-attention in transformer models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. NIPS, pages 5998–6008.\n136\nDavid Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. 2019. Entity, relation, and event\nextraction with contextualized span representations.\nIn Proc. EMNLP-IJCNLP, pages 5784–5789.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Proc.\nBlackboxNLP@EMNLP, pages 353–355.\nYu-An Wang and Yun-Nung Chen. 2020. What do\nposition embeddings learn? An empirical study of\npre-trained language model positional encoding. In\nProc. EMNLP, pages 6840–6849.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proc. EMNLP System Demonstrations, pages\n38–45.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proc. EMNLP, pages 6442–\n6454.\nZhen Zeng, Jianzong Wang, Ning Cheng, and Jing\nXiao. 2020. Prosody Learning Mechanism for\nSpeech Synthesis System Without Text Length\nLimit. In Proc. Interspeech 2020, pages 4422–4426.\n137\nA Visualizing EPET\nP for Additional\nLanguage Models\nFig. 1 shows the inner product between different\nposition embeddings for the models BERT base\nuncased, RoBERTa base, ALBERT base v1 as well\nas ALBERT xxlarge v2. Leveraging our analysis\nﬁndings of translation invariance in the matrix of\nEPET\nP in these pretrained networks, we investigate\nthe generality of this phenomenon by visualizing\nthe same matrix for additional existing large lan-\nguage models. We ﬁnd that similar Toeplitz pat-\nterns emerge for all investigated networks.\nB Coefﬁcient of Determination R2\nThe coefﬁcient of determination, R2, is a widely\nused concept in statistics that measures what frac-\ntion of the variance in a dependent variable that can\nbe explained by an independent variable. Denoting\nthe Residual Sum of Squares, RSS, and Total Sum\nof Squares, TSS, we have that\nR2 = 1 −RSS\nTSS , (6)\nwhere R2 = 0 means that the dependent variable\nis not at all explained, and R2 = 1 means that\nthe variance is fully explained by the independent\nvariable.\nApplied to a matrix, A∈Rn×n, to determine its\ndegree of Toeplitzness, we get RSS by ﬁnding the\nToeplitz matrix, AT ∈Rn×n, that minimizes the\nfollowing expression:\nRSS = minAT\nn∑\ni=1\nn∑\nj=1\n(A−AT)2\ni,j (7)\nFurthermore, we can compute TSS as:\nTSS =\nn∑\ni=1\nn∑\nj=1\n\nAi,j −\n\n1\nn2\nn∑\ni=1\nn∑\nj=1\nAi,j\n\n\n\n\n2\n(8)\nC Extracting ALBERT positional scores\nIn order to extract out the positional contributions\nto the attention scores from ALBERT, we disentan-\ngle the positional and word-content contributions\nfrom equation (3), and remove any dependencies\non the text sequence through EW. We exchange\nEW ≈ EW, with the average word embedding\nover the entire vocabulary, which we call EW.\nFP ≈ 1√dk\n(EWWQWT\nKET\nP+ (9)\n+ EPWQWT\nKET\nW + EPWQWT\nKET\nP) (10)\n≈ 1√dk\n(EWWQWT\nKET\nP+ (11)\n+ EPWQWT\nKET\nW + EPWQWT\nKET\nP) (12)\nThis way, we can disentangle and extract the posi-\ntional contributions from the ALBERT model.\nInitialization of Position-Aware Self-Attention\nUsing this trick, we initialize FP with formula (12).\nSince FP is only generating the positional scores,\nwhich are independent of context, it allows for train-\ning a separate positional scorer neural network to\npredict the positional contributions in the ALBERT\nmodel. Updating only 2,160 parameters (see Ta-\nble 2) signiﬁcantly reduces the computational load.\nThis pretraining initialization scheme converges in\nless than 20 seconds on a CPU.\nRemoving Position Embeddings When remov-\ning the effect of position embeddings, we calculate\nthe average position embedding and exchange all\nposition embeddings for it. This reduces the varia-\ntion between position embeddings, while conserv-\ning the average value of the original input vectors\nEW+EP.\nExtracted Attention Score Contributions\nLeveraging our analysis ﬁndings of translation\ninvariance in large language models, we visualize\nthe scoring functions as a function of relative\ndistance offset between tokens. Fig. 3 shows the\nimplied scoring functions for 4 attention heads for\n5 different absolute positions. Figs. 6, 7 show all\n12 attention heads of ALBERT base v2 with TISA.\nD Number of Positional Parameters of\nLanguage Models\nIn the paper, deﬁne positional parameters as those\nmodeling only positional dependencies. In most\nBERT-like models, these are the position embed-\ndings only (typically n×dparameters). Ke et al.\n(2021) propose to separate position and content\nembeddings, yielding more expressive models with\nseparate parts of the network for processing sepa-\nrate information sources. In doing so, they intro-\nduce two weight matrices speciﬁc to positional in-\nformation processing, UQ∈Rd×d and UK∈Rd×d,\ntotaling nd+2d2 positional parameters.\n138\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500\n(a) BERT base uncased\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (b) BERT large uncased\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (c) BERT base cased\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (d) BERT large cased\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500\n(e) ELECTRA small\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (f) ELECTRA large\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (g) RoBERTa base\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (h) RoBERTa large\nFigure 4: Visualizations of the inner-product matrix P = EP ET\nP ∈Rn×n for different BERT, ELECTRA, and\nRoBERTa models. We see that ELECTRA and RoBERTa models show much stronger signs of translational invari-\nance than their BERT counterparts. Most BERT models follow the pattern noted by Wang and Chen (2020), where\nthe Toeplitz structure is much more pronounced for the ﬁrst 128 ×128 submatrix, reﬂecting how these models\nmostly were trained on 128-token sequences, and only scaled up to n = 512 for the last 10% of training (Devlin\net al., 2019). Position embeddings 385 through 512 of the BERT cased models show a uniform color, suggesting\nthat these embeddings are almost completely untrained.\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500\n(a) ALBERT base v1\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (b) ALBERT large v1\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (c) ALBERT xlarge v1\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (d) ALBERT xxlarge v1\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500\n(e) ALBERT base v2\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (f) ALBERT large v2\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (g) ALBERT xlarge v2\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (h) ALBERT xxlarge v2\nFigure 5: Visualizations of the inner-product matrix P = EP ET\nP ∈Rn×n for different ALBERT models (Lan\net al., 2020). We plot both v1 and v2 to show the progression towards increased Toeplitzness during training.\nHyperparameter Selection We performed a\nmanual hyperparameter search starting from the\nhyperparameters that the Lan et al. (2020) re-\nport in https://github.com/google-research/\nalbert/blob/master/run_glue.sh. Our hyper-\nparameter conﬁg ﬁles can be found with our code.\n139\n-10 -8 -6 -4 -2 0 2 4 6 8 10\nRelative Position, i j\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\nPosition Score, f(i j)\n-10 -8 -6 -4 -2 0 2 4 6 8 10\nRelative Position, i j\n0.0\n0.1\n0.2\n0.3\n0.4Position Score, f(i j)\nFigure 6: Positional responses of all attention heads. Sections through ˆFP of ALBERT base v2, aligned to the\nmain diagonal, (left) show similar proﬁles as the corresponding TISA scoring functions (right). Vertical axes differ\ndue to 1) the scaling factor √dk and 2) softmax being invariant to vertical offset.\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n10\n8\n6\n4\n2\n0\n(a) Attention head 1\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n50\n40\n30\n20\n10\n0\n10 (b) Attention head 2\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n40\n30\n20\n10\n0 (c) Attention head 3\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n250\n200\n150\n100\n50\n0 (d) Attention head 4\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n250\n200\n150\n100\n50\n0\n(e) Attention head 5\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n30\n20\n10\n0 (f) Attention head 6\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0 (g) Attention head 7\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n1\n0\n1\n2\n3 (h) Attention head 8\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\n(i) Attention head 9\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n2\n1\n0\n1\n2 (j) Attention head 10\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0 (k) Attention head 11\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n40\n30\n20\n10\n0\n10 (l) Attention head 12\nFigure 7: Rows from the positional attention matrices ˆFP for all ALBERT base v2 attention heads, centered on\nthe main diagonal. Note that the vertical scale generally differs between plots. The plots are essentially aligned\nsections through the matrices in Fig. 8, but zoomed in to show details over short relative distances since this is\nwhere the main peak(s) are located, and the highest values are by far the most inﬂuential on softmax attention.\nE Reproducibility\nExperiments were run on a GeForce RTX\n2080 machine with 8 GPU-cores. Each down-\nstream experiment took about 2 hours to run.\nDatasets and code can be downloaded from\nhttps://github.com/nyu-mll/jiant/blob/\nmaster/guides/tasks/supported_tasks.md\nand https://github.com/ulmewennberg/tisa.\n140\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500\n(a) Attention head 1\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (b) Attention head 2\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (c) Attention head 3\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (d) Attention head 4\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500\n(e) Attention head 5\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (f) Attention head 6\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (g) Attention head 7\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (h) Attention head 8\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500\n(i) Attention head 9\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (j) Attention head 10\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (k) Attention head 11\n0 100 200 300 400 500\n0\n100\n200\n300\n400\n500 (l) Attention head 12\nFigure 8: Values extracted from the positional attention matrices for all ALBERT base v2 ﬁrst-layer attention heads.\nSome heads are seen to be sensitive to position, while others are not. Note that these visualizations deliberately use\na different color scheme from other (red) matrices, to emphasize the fact that the matrices visualized here represent\na different phenomenon and are not inner products."
}