{
  "title": "Multi-Lingual Question Generation with Language Agnostic Language Model",
  "url": "https://openalex.org/W3176011152",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2478552254",
      "name": "Bingning Wang",
      "affiliations": [
        "Sohu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2004129631",
      "name": "Ting Yao",
      "affiliations": [
        "Sohu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097068550",
      "name": "Weipeng Chen",
      "affiliations": [
        "Sohu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2146806579",
      "name": "Jingfang Xu",
      "affiliations": [
        "Sohu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2110004139",
      "name": "Xiaochuan Wang",
      "affiliations": [
        "Sohu (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297801177",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2917436183",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1531374185",
    "https://openalex.org/W4229780102",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2994636820",
    "https://openalex.org/W2979736636",
    "https://openalex.org/W2610891036",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2971139875",
    "https://openalex.org/W3034630799",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2964278185",
    "https://openalex.org/W2994900646",
    "https://openalex.org/W1555380324",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3099756172",
    "https://openalex.org/W2949925034",
    "https://openalex.org/W19665345",
    "https://openalex.org/W4288106555",
    "https://openalex.org/W2890152674",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2891375654",
    "https://openalex.org/W2804292122",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2967827612",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2962897020",
    "https://openalex.org/W2952354317",
    "https://openalex.org/W2092029240",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2606333299",
    "https://openalex.org/W4245436919",
    "https://openalex.org/W2963355640",
    "https://openalex.org/W2757978590",
    "https://openalex.org/W2963667932",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2564256581",
    "https://openalex.org/W2951785908",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2016381774",
    "https://openalex.org/W2770872138",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2998653236"
  ],
  "abstract": "Question generation is the task of generating coherent and relevant question given context paragraph.Recently, with the development of large-scale question answering datasets such as SQuAD, the English question generation has been rapidly developed.However, for other languages such as Chinese, the available training data is limited, which hinders the development of question generation in the corresponding language.To investigate the multi-lingual question generation, in this paper, we develop a language-agnostic language model, which learns the shared representation from several languages in a single architecture.We propose an adversarial training objective to encourage the model to learn both language-specific and language-independent information.We utilize abundant monolingual text to improve the multi-lingual question generation via pre-training.With the languageagnostic language model, we achieve significant improvement in multi-lingual question generation over five languages.In addition, we propose a large-scale Chinese question generation dataset containing more than 220k human-generated questions to benefit the multi-lingual question generation research.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2262–2272\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2262\nMulti-Lingual Question Generation with\nLanguage Agnostic Language Model\nBingning Wang, Ting Yao, Weipeng Chen, Jingfang Xu and Xiaochuan Wang\nSogou Inc.\nBeijing, China\n{wangbingning,yaoting}@sogou-inc.com\nAbstract\nQuestion generation is the task of generating\ncoherent and relevant question given context\nparagraph. Recently, with the development of\nlarge-scale question answering datasets such\nas SQuAD, the English question generation\nhas been rapidly developed. However, for\nother languages such as Chinese, the avail-\nable training data is limited, which hinders\nthe development of question generation in the\ncorresponding language. To investigate the\nmulti-lingual question generation, in this pa-\nper, we develop a language-agnostic language\nmodel, which learns the shared representation\nfrom several languages in a single architec-\nture. We propose an adversarial training ob-\njective to encourage the model to learn both\nlanguage-speciﬁc and language-independent\ninformation. We utilize abundant monolingual\ntext to improve the multi-lingual question gen-\neration via pre-training. With the language-\nagnostic language model, we achieve signif-\nicant improvement in multi-lingual question\ngeneration over ﬁve languages. In addi-\ntion, we propose a large-scale Chinese ques-\ntion generation dataset containing more than\n220k human-generated questions to beneﬁt the\nmulti-lingual question generation research.\n1 Introduction\nQuestion Generation (QG), also known as learn-\ning to ask, has attracted a lot of research interest\nin recent years. QG is regarded as the dual task\nof machine reading comprehension (Yuan et al.,\n2017; Xiao et al., 2018). Rather than answering a\ngiven question, learning to ask a coherent, relevant,\nand non-trivial question also requires a deep under-\nstanding of the context (Davey and McBride, 1986;\nGraesser et al., 2010), providing a good testbed for\nnatural language understanding.\nConventional methods for question generation\nrely heavily on heuristic rules, and the standalone\ndependency parsing tool is needed to generate hand-\ncrafted templates (Mostow and Chen, 2009; Heil-\nman and Smith, 2010; Rus et al., 2010; Hussein\net al., 2014; Dhole and Manning, 2020). In re-\ncent years, with the development of deep learning\nand large-scale QA datasets, more and more neu-\nral network model has been proposed, which is\nalso referred as neural question generation. Neural\nQG shows great advantage compared with previ-\nous rule-based systems in terms of both ﬂuency and\ndiversity of the generated questions (Duan et al.,\n2017; Yuan et al., 2017).\nHowever, most progress in QG is made in En-\nglish. For other languages such as Hindi, the lack\nof large-scale QG data limits its development. Re-\ncently, multi-lingual and cross-lingual language un-\nderstanding has been studied in several NLP tasks,\nsuch as question answering (Liu et al., 2019; Cui\net al., 2019), summarization (Zhu et al., 2019), nat-\nural language inference (Conneau et al., 2018), etc.\nFor QG, Kumar et al. (2019) demonstrate that for\nlow-resource Hindi, incorporating the large-scale\nEnglish SQuAD (Rajpurkar et al., 2016) dataset\ncould boost the QG result a lot.\nFor multi-lingual QG, a key factor is to learn a\nmodel that could transfer knowledge across differ-\nent languages. In this paper, we propose a language-\nagnostic language model: it consists of the speciﬁc\nlow-level module for each language, and a shared\nhigh-level module for multi-lingual information\naggregation. Separating the language model into\ntwo levels enables us to learn the language-speciﬁc\ninformation in each language and the common in-\nformation shared among languages. In this way,\nthe knowledge in multi-lingual QG could be trans-\nferred via the high-level module.\nFor the language-agnostic language model, how-\never, the distributed representation of the low-level\nmodule could be easily mixed with the language in-\nformation, which makes the high-level module con-\n2263\ntain some unnecessary language-speciﬁc features\nthat are too speciﬁc to transfer across languages.\nInspired by previous works on transfer learning\n(Chen et al., 2017; Liu et al., 2017), we propose an\nadversarial training objective to decouple the low-\nlevel module with the high-level module, which\nprevents the private and shared latent feature spaces\nfrom interfering with each other, making the high-\nlevel module language-invariant, thus achieving\nbetter transferability for different languages.\nTo get a better initialization for our model, we de-\nvelop two self-supervised methods to pre-train our\nmodel on abundant monolingual text. We apply\nour model to ﬁve languages QG tasks that have\nhuman-labeled QG datasets. The experimental\nresults demonstrate that all languages QG could\nbeneﬁt from the multi-lingual training. Our mod-\nels surpass previous monolingual or multi-lingual\nQG methods by a large margin, even in zero-shot\nlearning where we had no training data in the low-\nresource languages, our model achieves satisfac-\ntory results by merely trained on English dataset,\nwhich shows a promising transferability of the pro-\nposed model.\nBesides, we also propose a large-scale Chinese\nQG dataset containing more than 220k human-\nlabeled questions. We hope the proposed Chi-\nnese dataset could beneﬁt the community for more\ncomprehensive multi-lingual QG research. The\ncodes and proposed datasets are available athttps:\n//github.com/benywon/LALM.\nOur contributions are summarized as follow:\n•We propose a novel language-agnostic language\nmodel which decouples the language speciﬁc\nand language independent information in QG.\n•The proposed model achieves signiﬁcant im-\nprovement over previous models in multi-lingual\nQG, and we analyze the transferability in multi-\nple languages.\n•We release a large-scale human labeled Chinese\nQG dataset containing more than 220k questions.\nTo our best knowledge, this is the largest speciﬁc\nquestion generation dataset so far.\n2 Related Work\nQuestion generation has received increasing at-\ntention from the research community. Traditional\nQG systems are mostly rule-based, which some-\ntimes utilizing off-the-shelf tools to get the syntac-\ntic structure, dependency relations, and semantic\nrole of the passage (Mostow and Chen, 2009; Heil-\nman and Smith, 2010). First, the target answers are\ngenerated using rules or semantic roles, next, low-\nquality questions are generated using hand-crafted\nrules or templates. Finally, the generated questions\nare ranked by features such as keyword matching\ndegree or sentence perplexity (Hussein et al., 2014).\nThe main drawbacks of these symbolic systems are\nthat the rules and templates are expensive to manu-\nally create, and lack diversity.\nWith the development of deep learning and large-\nscale question answering datasets, motivated by\nneural machine translation, Du et al. (2017) pro-\nposed a sequence to sequence (seq2seq) architec-\nture combined with attention mechanism, achiev-\ning a promising result on QA dataset SQuAD.\nSince then, many works have been proposed to\nextend the preliminary framework with rich fea-\ntures, such as named entity tags (Zhou et al., 2017)\nor answer position features (Duan et al., 2017), and\nincorporate copy mechanism to copy words from\nthe context paragraph (Song et al., 2018). Other\ntypes of models are also introduced such as graph\nneural networks (Chen et al., 2019) or Transformer\n(Scialom et al., 2019). However, most of these\nworks are focus on English QG and have not been\nvalidated in other languages.\nMulti-Lingual language generation . Duan\net al. (2019) translated documents as weakly su-\npervised training data for zero-shot multi-lingual\nabstractive summarization. Chi et al. (2019) pro-\nposed a multi-lingual pre-training method that can\ntransfer monolingual supervision signals to other\npre-trained languages. Zhu et al. (2019) adopt\nlarge-scale supervised data from existing monolin-\ngual summarization datasets via translation strategy\nto perform multi-lingual summarization. Kumar\net al. (2019) also proposed a multi-lingual question\ngeneration methods based on Transformer, they\nproposed a small Hindi QG dataset and improved\nthe QG result on Hindi by training with additional\nEnglish data.\nCompared with the previous multi-lingual meth-\nods, our method directly separates the language-\ndependent module and language-independent mod-\nule. We propose an adversarial decoupling module\nto improve the adaptive ability of the model. Be-\nsides, our model could be properly pre-trained by\nmonolingual data, which obviates the need to con-\nstruct the back-translation or pseudo-parallel data.\n2264\nEmbedding+LSTM\nMulti-Head Attention\nAttention Mask\nLinear+Softmax\n\u0000 x1, . . . , x|x|, sep, y1, . . . , y|y|\nNot mask\nMasked\nEnglish\nHindi\nChinese\nShared  \nBlock \nLinear\nLayerNorm\n\u0000 × L\nEmbedding+LSTM\n…\nEn\nZh\nFr\n0.3\n0.2\n0.4\nEn\n…\nmaximize\nmaximize\n( a ) ( b ) ( c )\nDiscriminator\n\u0000 x1,...,x|x|,sep,y1,...,y|y|\nLow-Level\nHigh-Level\nFigure 1: (a) The whole architecture of the proposed language-agnostic language model. It consists of the low-\nlevel language understanding module (Embedding+LSTM) and the high-level semantic understanding module\n(Transformer block), followed by a projection and softmax module. (b) The attention mask matrix M in the high-\nlevel module, Mi,j means whether the word in position icould attend to the word in position j. The gray cells\nare allowed to attend and the others are masked to forbid attention. (c) The adversarial decoupling module where\nthe discriminator tries to maximize the probability of the corresponding language while the generator (low-level\nmodule) tries to minimize it.\n3 Language-agnostic Language Model\nThe Language-agnostic language model (LALM)\nconsists of the low-level module and the high-level\nmodule, the whole architecture is illustrated in Fig-\nure 1(a) we describe it below.\n3.1 Low-Level Module\nThe low-level module is built to perform the basic\nlanguage understanding. In this paper, we adopt\nthe LSTM (Hochreiter and Schmidhuber, 1997) en-\ncoder as the low-level language understanding mod-\nule1. LSTM processes text in sequential order and\nembeds the language information into dense repre-\nsentations. We adopt the uni-directional LSTM in\nthis paper to make the model auto-regressive.\nFor the language-agnostic language models,\neach language has its speciﬁc word embeddings\nand speciﬁc low-level language understanding\nLSTMs. This is different from some previous\nmulti-lingual methods that a shared or aligned word\nembedding is utilized for different languages (Con-\nneau et al., 2018; Lample and Conneau, 2019). Sep-\narating the language understanding module enables\nus to model speciﬁc linguistic characteristics in dif-\nferent languages. In Section 4, we will show that\n1In fact, we also conduct experiments on adopting other\ntypes of models as the low-level module such as Transformer\nor GRU, but the result is not comparable with the LSTM.\nseparating the low-level module for each language\ncould beneﬁt a lot for multi-lingual QG.\n3.2 High-Level Module\nThe low-level module is built to perform the basic\nlinguistic understanding, and the high-level mod-\nule is built on top of the low-level module to per-\nform higher-level information aggregation, which\nrequires higher model capacity. In this paper, we\nuse the Transformer (Vaswani et al., 2017) model\nas the high-level module.\nThe Transformer, with the core building-block\ncalled multi-head attention, has shown great ad-\nvantages in representing languages in many NLP\ntasks. Current state-of-the-art models in natural\nlanguage understanding benchmark GLEU2 (Wang\net al., 2018) are almost Transformer-based. In this\npaper, we focus on QG which is a sequence-to-\nsequence problem, so we adopt the mask operation\nsimilar with (Dong et al., 2019), which is illustrated\nin Figure 1(b). For a pair of sequence (x,y) where\nx = x1,...,x |x|is the source, and y = y1,...,y |y|\nis the target, we concatenate them together with a\nspecial token <sep>, forming a single sequence\nwith length |x|+ |y|+ 1. We want all the positions\nin the source {1,2,..., |x|}to attend to each other\nso we can obtain the bi-directional representations\n2https://gluebenchmark.com/\n2265\nof the source, and all the positions in the target\n{|x|+ 1,..., |x|+ |y|+ 1}are forbidden to attend\nto future words:\nMi,j =\n{\n0, j ≤|x| or j ≤i,\n−∞, otherwise. (1)\nThis attention mask operation enables us to build\na causal language model that the generation of the\ncurrent word only depends on its previous words.\nTherefore, the probability of y could be denoted as:\np(y|x) =\n|y|∏\ni=1\np(yi|y<i,x) (2)\nAnd the loss for the whole model is the negative\nlog likelihood of the data:\nLNLL = −E\nx,y\n∑\nlog p(yi|y<i,x) (3)\n3.3 Adversarial Decoupling Module\nIn this paper, we want the representations of the\nlow-level module in different languages to con-\ntain no language-speciﬁc information that is inter-\nleaved with the high-level module. In this way,\nthe high-level module could focus on the semantic\nunderstanding shared across languages. We build\na discriminator on top of the low-level module to\ndetermine whether the output of low-level represen-\ntations contains the speciﬁc language information.\nThe discriminator is a bi-directional LSTM tak-\ning the output of the low-level module as input and\ntries to predict its language. Concretely, denote the\noutput of the low-level module is S ∈Rn,d where\nnis the sequence length (i.e. |x|+ |y|+ 1), and\ndis the hidden size of the low-level module. The\noutput of the discriminator can be represented as:\nH = bi-LSTM(S)\nh = Max-Pooling(H)\nˆh = MLP(h)\nˆy= Softmax(ˆh)\n(4)\nh ∈Rd is a pooled representations of the discrim-\ninator for classiﬁcation. ˆy is the language distri-\nbution in RC where Cis the number of languages.\nFor the discriminator, the target is to maximize the\nprobability of the corresponding language while\nthe low-level module (generator) tries to minimize\nit. Therefore, they form an adversarial training\nobjective that the low-level module must produce\nrepresentations without discriminative language in-\nformation. In this way, the discriminator acts as\nan adversarial decoupling module (ADM) to en-\ncourage the low-level module to generate language-\nagnostic representations.\nThe architecture of ADM is shown in Figure 1\n(c), and the loss function for the discriminator and\nlow-level module (generator) are:\nLD = −log p(ˆyi)\nLG = −log p(1 −ˆyi) (5)\nwhere ˆyi is the discriminator probability for the in-\nput language i. In fact, the objective of the genera-\ntor is to maximize the entropy of the discriminator’s\noutput to make it less conﬁdent of the language.\n3.4 Pre-training\nRecent works on NLP and language generation\nhave shown the great advantage of large-scale\npre-training (Devlin et al., 2018; Radford et al.,\n2019; Lewis et al., 2019; Roberts et al., 2020). In\nthis paper, we also pre-train our model in mas-\nsive multilingual text. Since our model is a se-\nquence to sequence architecture, we develop two\nself-supervised objectives for language generation\npre-training:\nDenoised Auto-Encoder(DAE): Most previous\nworks on natural language generation pre-training\nresort to DAE to initialize the model. In DAE, a\ncorrupted version of the original sentence is cre-\nated as the source and the model should reconstruct\nthe original sentence. In this paper, we adopt the\nsimilar noising strategy as Lewis et al. (2019): (1)\nToken Masking random tokens are sampled and\nreplaced with a special [MASK] token. (2) To-\nken Deletion randomly deletes several tokens in\nthe document. (3) Token Replacement randomly\nreplace some tokens with other tokens in the vo-\ncabulary. (4) Sentence Permutation randomly swap\nsome tokens in the sentence.\nNext Sentence Generation : One of the prob-\nlems of the DAE is that the input is always the cor-\nrupted sentence, which is not the case during ﬁne-\ntune, the pretrain-ﬁnetune discrepancy may hurt the\nperformance of the downstream tasks (Yang et al.,\n2019). Similar to Kiros et al. (2015) and Dong et al.\n(2019), we sample a consecutive segment in the\ntext and divide it into two parts, we treat the ﬁrst\nparts as the source and the second part as the target.\nThe objective is to generate the second part based\non the ﬁrst part.\n2266\n3.5 Question Generation Fine-tuning\nAfter pre-training, we suppose the low-level mod-\nule of our model has learned the multi-lingual lin-\nguistic information. Then the ﬁne-tuning objective\nis to adjust the high-level module for question gen-\neration. Therefore, in this phase, we ﬁx the low-\nlevel module, i.e. the word embedding, LSTM, and\noutput projection linear layer, and only update the\nparameter of the high-level module.\n4 Experiments\n4.1 Dataset\nThe question generation datasets are sometimes\ndirectly derived from the corresponding question\nanswering datasets. In the current question an-\nswering application, most multi-lingual datasets\nare automatically derived by translating from En-\nglish SQuAD (Asai et al., 2018). However, it may\nreduce the multi-lingual QG tasks to translation\ntasks if we use these datasets. Therefore, we con-\nsider four different language QG datasets that are\ndeveloped by the speciﬁc language speakers.\n•English (En) We use the SQuAD (Rajpurkar\net al., 2016) as the English question generation\ndataset. It is a standard machine reading com-\nprehension data consists of nearly 100k human-\nlabeled questions from Wikipedia.\n•Korean (Ko) We use the Korquad1.0 (Lim et al.,\n2019) as the Korean QG data. It consists of more\nthan 70,000 human-generated question-answer\npairs on Korean Wikipedia articles.\n•French (Fr) We adopt the French SQuAD-style\ndataset (d’Hoffschmidt et al., 2020) consisting of\nmore than 25k human-curated French questions.\n•Hindi (Hi) HiQuAD (Kumar et al., 2019) is\na speciﬁc Hindi QG dataset containing 6,555\nquestion-answer pairs. It was derived from the\nHindi storybook.\nSince the size of the QG dataset except English is\ncomparative small, so we propose a new large-scale\nQG dataset created by humans on Chinese (Zh).\nFirst of all, we collect nearly 3.5m passages from\nBaike3, a Chinese Wikipedia-like encyclopedia. To\nincrease the diversity of the selected paragraphs,\nwe cluster the passages based on the bag-of-words,\nthen we use Ward (Ward Jr, 1963) algorithm to\nselect the centroid in each cluster, which result in\nnearly 100k passages. We ask volunteers to ask no\n3https://baike.sogou.com/\nQG Pre-train\nTrain Dev/Test Name(Size)\nEn 86,635 8,965/8,964 enwiki(13.6Gb)\nZh 180,000 20,000/24,962 zhwiki(1.3Gb)\nKo 60,407 5,774/3,898 kowiki(608Mb)\nFr 20,731 3,188/2,189 frwiki(4.0Gb)\nHi 4,000 1,300/1,255 hiwiki(395Mb)\nTable 1: The statistics of the multi-lingual pre-training\ndata and question generation data.\nmore than 5 questions for each paragraph. Since\nwe did not give the speciﬁc answer candidates for\neach paragraph, the annotators were encouraged\nto ask more general and comprehensive questions.\nWe also ask other volunteers to check the quality\nand remove the questions that are either unanswer-\nable or contain grammar errors. Finally, we obtain\n224,962 question-paragraph pairs. We randomly se-\nlect 180k of them as the training data, 20k samples\nfor development, and the rest 24,962 for testing.\nWe name it LAB (Learning to Ask on Baike).\nWe adopt the 2020-05-20 data dumps of the\nWikipedia4 in the corresponding language as the\npre-training data. The details of the training data\nare shown in Table 1.\n4.2 Implementation Details\nIn all experiments, we tokenize the text with sen-\ntencepiece (Kudo and Richardson, 2018). For all\nlanguages datasets, we set the vocabulary size to\n30,000. We use the Adam (Kingma and Ba, 2014)\noptimizer with 5k warm-up steps and linearly de-\ncay the learning rate. β1,β2,ϵ was set to 0.9, 0.99\nand 10−6, respectively. For both pre-training and\nﬁne-tuning, the max learning rate was set to 10−4.\nThe batch size was 256 during pre-training and\n64 during ﬁne-tuning. We limit the max sequence\nlength to 512. For the adversarial decoupling mod-\nule training, following previous works of genera-\ntive adversarial networks (Goodfellow et al., 2014;\nSalimans et al., 2016), the update rate for discrimi-\nnator and generator was set to 1:10. For each of the\n4 noising strategies in pre-training, we set the sam-\nple probability to 0.1. Similar with Scialom et al.\n(2019) we do not provide the answer and direcetly\ngenerate questions based on the context. We use\nthree types of models:\nLALMshare is the shared language-agnostic lan-\n4https://dumps.wikimedia.org/\n2267\nTransformer NQG++ Multi-BERT CLQG XNLG LALM share LALMbase LALMlarge LALMlarge+ADM\nEn\nBLEU-4 14.03 15.09 17.19 17.63 19.98 20.96 21.95 23.50 24.94\nMETEOR 17.62 18.04 18.38 18.91 20.24 20.23 21.30 22.15 23.28\nROUGE 40.79 40.24 44.82 43.34 46.51 47.47 48.23 50.34 51.42\nZh\nBLEU-4 22.75 20.32 35.08 34.96 37.40 36.11 38.32 43.19 44.10\nMETEOR 17.24 18.95 26.10 26.54 27.13 27.28 27.99 32.38 33.04\nROUGE 30.14 29.87 38.46 40.11 42.15 43.25 44.49 45.16 46.40\nKo\nBLEU-4 7.11 7.95 10.35 8.97 - 11.93 12.19 12.58 12.93\nMETEOR 14.30 14.81 18.10 17.22 - 19.85 20.11 20.96 21.10\nROUGE 22.17 24.13 31.28 29.34 - 34.10 34.88 34.79 35.02\nFr\nBLEU-4 4.48 5.03 8.95 10.18 12.93 13.38 13.95 14.87 15.28\nMETEOR 13.05 13.19 15.91 16.28 18.37 17.75 18.20 18.84 19.92\nROUGE 32.17 31.66 39.34 41.23 40.96 41.15 42.80 43.11 44.51\nHi\nBLEU-4 9.77 10.10 23.15 20.24 - 30.35 32.21 34.02 35.19\nMETEOR 23.85 24.32 30.29 29.15 - 33.80 34.22 35.97 36.25\nROUGE 33.16 34.91 41.06 40.64 - 48.82 49.14 50.94 51.23\nTable 2: Main result of the multi-language QG. LALM share is similar with previous multi-lingual model that the\nparameters are shared across all languages. ADM represents the model trained with adversarial decoupling module.\nguage model. It is similar with the proposed model\nbut has no speciﬁc low-level LSTM for each lan-\nguage. That is, the low-level and high-level pa-\nrameters are both shared across different languages.\nThe hidden size was set to 768 and the layer size\nwas set to 12, and each layer consists of 12 heads.\nWe set the shared vocabulary size to 100,000.\nLALMbase is the base version of our model.\nIt has the same hidden size as LALM share. The\nlow-level module was single layer uni-directional\nLSTM with hidden size 768. LALMbase has nearly\n138m parameters, where nearly half of them are\nlow-level language understanding parameters.\nLALMlarge is the large version of our proposed\nmodel. The hidden size, layer size, and head size\nwere set to 1024,24,16, respectively. The low-\nlevel module was two-layer uni-directional LSTMs.\nLALMlarge has 548m parameters, where nearly a\nquarter of them are low-level module’s parameters.\n4.3 Criterion:\nFollowing previous works of QG (Zhou et al., 2017;\nChen et al., 2019), we adopt three widely used auto-\nmatic metrics for evaluation: BLEU, Meteor and\nRouge-L, which measure the n-gram similarities\nbetween the generated questions and real questions.\n4.4 Baselines\nWe adopt 5 baseline methods for comparison.\n□ Transformer (Vaswani et al., 2017; Scialom\net al., 2019) is the most widely used architecture\nin sequence-to-sequence learning. For each lan-\nguage, we train the correspondent Transformer\nmodel based on its training data. We set dropout\nratio to 0.4 to prevent overﬁtting.\n□ NQG++ (Zhou et al., 2017) is a popular neural\nQG model based on LSTM. It is enhanced with\nattention and copy mechanism5.\n□ Multi-BERT (Devlin et al., 2018) is a multi-\nlingual extension to the original BERT model. It\nwas trained on the multi-lingual wikipedia. All\nthe language shares the same vocabulary. We\nadopt the way same with R¨onnqvist et al. (2019)\nto extend BERT to language generation task.\n□ CLQG (Kumar et al., 2019) is a cross-lingual\nQG method based on Transformer. It is pre-\ntrained by denoising autoencoders along with\nback-translation. We use the public implemen-\ntation6 and adopt the same word tokenization as\nwell as pre-training data as our model.\n□ XNLG (Chi et al., 2019) is a multi-lingual lan-\nguage generation model that transfers monolin-\ngual supervision to all pre-trained languages. It\nwas trained with English, Chinese and French\ndatasets. We use their public pre-trained models7\nand ﬁne-tune on the three QG dataset.\n4.5 Multi-Lingual Question Generation\nTo evaluate the multi-lingual question generation\nability of the proposed methods, we assemble all\n5https://github.com/magic282/NQG\n6https://github.com/vishwajeet93/clqg\n7https://github.com/CZWin32768/XNLG\n2268\nBLEU-4 ROUGE\nZh Zh\nP\nF \u0013 \u0017 P\nF \u0013 \u0017\n\u0013 38.32 36.03 \u0013 44.49 41.73\n\u0017 – 34.22 \u0017 – 40.12\nEn En\nP\nF \u0013 \u0017 P\nF \u0013 \u0017\n\u0013 21.95 20.61 \u0013 48.23 47.15\n\u0017 - 17.93 \u0017 - 45.02\nTable 3: Multi-lingual and mono-lingual results for\nLALMbase. P denotes the pre-training and F denotes\nthe ﬁne-tuning, where \u0013denote the multi-lingual while\n\u0017denotes the mono-lingual training. For example, the\nupper right cell in each table denotes pre-training with\nmulti-lingual but ﬁnetuning with mono-lingual.\nQG data and train the LALM thereof. For Trans-\nformer and NQG++, we initialize the word embed-\ndings by fasttext multilingual word embeddings\n(Grave et al., 2018). The result is shown in Table 2.\nWe can see from the table that our model ex-\ncels at multi-lingual QG, achieving signiﬁcant im-\nprovement over previous methods in all languages.\nCompared with other architectures such as Trans-\nformer, we explicitly separate the low-level and the\nhigh-level module in the proposed model and use\nadversarial networks to decouple them. Therefore,\nthe shared high-level module is encouraged to learn\nmore common representations across different lan-\nguages, which is more transferable and beneﬁts the\ndownstream QG task a lot.\nBesides, we can see that if we don’t explic-\nitly separate the low and high-level parameters\n(LALMshare), the result drops a lot. We hypothesis\nthat different languages have different low-level\nlanguage information, such as lexical, syntactical,\netc. Embedding all language processing procedures\ninto a single model may make the model hard to\ndiscriminate the language-speciﬁc information.\nBesides, the model trained with the adversarial\ndecoupling module achieves further improvement,\nthe ADM may impose an implicit regularization\non the low-level module to make the representa-\ntions more abstract, and therefore encourage the\nhigh-level module to learn more common represen-\ntations (Chen et al., 2017; Liu et al., 2017).\n4.6 Human Evaluation\nThe automatic metrics are sometimes biased to-\nward a speciﬁc attribute of the generated question\n(Hosking and Riedel, 2019). So we conduct hu-\nman qualitative evaluation of the generated outputs.\nWe consider three aspects of the generated ques-\ntions: Fluency: Whether the generated questions\nare well-posed and natural, in terms of both gram-\nmar and semantic. Answerable: Whether the gen-\nerated questions could be answered by the context\nparagraph. Signiﬁcance: Whether the generated\nquestion is just a simple syntactical transformation\nof the paragraph sentence or trivial one that seems\nunlikely asked by human.\nWe randomly sample 50 generated questions\nfrom English and 50 from Chinese and ask three\nvolunteers to evaluate the sample quality. The re-\nsult is shown in Table 5. The result shows our\nproposed model is also excels at human evalua-\ntion, especially for signiﬁcance, which is some-\ntimes regarded as the most important factor in QG\n(Graesser et al., 2010). We also showcase some\noutputs of our model in Table 4. We can see that\nLALM could generate ﬂuent and sound questions.\n4.7 Multi-Lingual v.s. Mono-Lingual\nKumar et al. (2019) have found that in QG the per-\nformance of Hindi could be improved by training\nwith additional English data. In this section, we\nevaluate whether the multi-lingual is superior to\nthe mono-lingual QG. We focus on two aspects:\n(1) Pre-training. In contrast to the proposed multi-\nlingual pre-training, we adopt the mono-lingual\npre-training where we only pretrain on speciﬁc lan-\nguages8 and ﬁne-tune the QG models in the same\nlanguage. (2) Fine-tuning . Different from the\nsetup in Sec. 4.5 where we aggregate all languages\nQG data for training, we only ﬁne-tune the model\non speciﬁc language.\nWe experiment on English and Chinese with the\nLALM base model. The BLEU-4 and ROUGE-L\nscores are shown in Table 3. It is clear that for\nboth pre-training and ﬁne-tuning, the multi-lingual\ntraining improves the model a lot. Moreover, the\nmulti-lingual plays a more important role in pre-\ntraining than in ﬁne-tuning. We suppose that dur-\ning pre-training, multiple languages perform a type\nof regularization on the shared high-level module,\nwhile in ﬁne-tuning the language-dependent super-\n8Therefore, we omit the adversarial decoupling module\nsince it only takes effect on multi-lingual learning.\n2269\nTable 4: Some generated cases of the proposed model.\nEnglish\nContext: The United Methodist Church opposes conscription as incompatible with the teaching of Scripture. Therefore, the\nChurch supports and extends its ministry to those persons who conscientiously oppose all war, or any particular war, and\nwho therefore refuse to serve in the armed forces or to cooperate with systems of military conscription. However, the United\nMethodist Church also supports and extends its ministry to those persons who conscientiously choose to serve in the armed\nforces or to accept alternative service. The church also states that ”as Christians they are aware that neither the way of\nmilitary action, nor the way of inaction is always righteous before God. ”\nOriginal: The Church supports those persons who conscientiously oppose what?\nLALM: what does the church states after they oppose the construction ?\nChinese\nContext: 电桥平衡#四个电阻R0、R1、R2、Rx连成四边形,称为电桥的四个臂。四边形的一个对角线连有检流\n计,称为“桥”;四边形的另一对角线接上电源,称为电桥的“电源对角线”。E为线路中供电电源,学生实验用双路直\n流稳压电源,电压可在0-30V之间调节。R保护为较大的可变电阻,在电桥不平衡时取最大电阻作限流...\nOriginal: 什么是电桥平衡？\nLALM: 电桥平衡有什么用？\nFrench\nContext: Le seul quartier d’habitation `a avoir ´et´e fouill´e est situ´e sur le site du Merkes, `a l’est de la Voie processionnelle et\ndu complexe sacr´e, entre les anciens quartiers de Ka-dingirra, Eridu et Shuanna. Sa voirie est caract´eris´ee par des rues\n´etroites approximativement rectilignes et se coupant quasiment `a angles droits. Il s’agit peut-ˆetre de l’h´eritage d’un ancien\nplan orthogonal planiﬁ´e qui a ´et´e alt´er´e `a la suite de remaniements de constructions, courants en raison de l’alt´eration\nrapide des constructions en briques crues qui doivent r´eguli`erement ˆetre restaur´ees.\nOriginal: En quoi sont fabriqu´ees les habitations ?\nLALM: Quelles sont les caract´eristiques de la route ?\nKorean\nContext: g \u0000A©ᆼ(藏) ᅩ\" é¶s ᅵᄅ ᅡ ᅩ ¸Ô\u0005¦o ᅵ\u000fHw ᅵZ à Ô ᅩ\" é¶ É r1 l xᄋ ᅡr ᅵᄋ ᅡ\\ 0 Au ᅵô\u0014ÇV,ᆲ ᅩZ\u001a} É r ᅩ\" é¶s ᅵᄃ ᅡ. w ᅵZ à Ôᄌ ᅡu ᅵ½ ᅮ% i \u0003\nõᅪ×\u0005æ² D Gg \u0000Aᄒ ᅡs ᅵ$í\n(海省), Õ ᅳo ᅵ ᅩ \u001b\u0004 ¸ᄏ ᅡÃ »p ᅵØ Ô\\ \u001e\u0015\u00065 ge\u0012\u000fHw ᅵZ à Ô ᅩ\" é¶ É rz  ᆷ· ¡ ¤1000km, 1 l x\" f2500km\\ . á <# Qe\u0012\nÜ ¼ 9, Õ ᅳ¨î\nç\u0013HZ\u001a}s ᅵ\u000fH4500 p ᅵ' ᄀ ᅡ\u001fÅ\u0007\u000fHᄃ ᅡ. ’[ j> ᅨ_ ᅴt ᅵÔ\u0005æ’Ü ¼ ÐÔ\u0005¦wn =ëßᆫ\u0001p u[ j> ᅨ\\ \" fᄀ ᅡ©ᆼZ\u001a} ᅩß ¼ 9\u0016\u0004& h \u0003 É r  ᆨ250ëßᆫ\n¨î\n~½Ó~\u0002 ´ Ðp ᅵ' ᄂ ᅡ)aᄃ ᅡ. s ᅵ ᅩ\" é¶ É r \u001b\u0004 ¸-  ñÅ Òe\u0007¦Y Us ᅵà Ôü<Ä »ᄅ ᅡr ᅵᄋ ᅡe\u0007¦Y Us ᅵà Ôᄀ ᅡ \u001b\u0004Òqt@ /\\ Ø\u0005æ[\u001atᄒ ᅡ 9Òqt$í\n÷&% 3 \u0012Ü ¼ 9\nÕ ᅳõᅪ&ñ\n É rt ᅵ\u0000F K ¸ \u001b\u0004' ÷& ᅩe\u0012ᄃ ᅡ. s ᅵ ᅩ\" é¶ É ríßᆫÐoõᅪ è\u0000F K  ñÃ ºᄀ ᅡì\u001cr íô\u0014Ç ᅩ\" é¶_ ᅴ|\t\u0004 ¸Û ¼9 \\t ᅵ@ /\\\u0007¦+þA$í\nᄒ ᅡ ᅩe\u0012ᄃ ᅡ.\nô\u0014ÇK \u0017¨î\nç\u0013Hy©ᆼÃ º|¾Ó É r100mm\\ \" f300mm Ð, y©ᆼÃ º|¾Ó_ ᅴ@ /Â Òì\u001cr É rÄ º~ Ã Ì`\u0007¦s ᅵê\u001crᄃ ᅡ. Ä »3 l q \u001b\u0004[ þ t É r ᅩ\" é¶_ ᅴz  ᆷÂ Òx9 \f1 l xÂ Ò\n\u001eâ\n> ᅨ_ ᅴô\u0014ÇK \u00176> ᅢ\u0002 Z 4ᄀ ᅡ|¾Ó\" fo ᅵᄀ ᅡ? /o ᅵ\u000fH3 l q ít ᅵ\\ \" fÄ »3 l qÒqtÖ¸`\u0007¦Ä »t ᅵᄒ ᅡ ᅩe\u0012ᄃ ᅡ.\nOriginal: w ᅵZ à Ô ᅩ\" é¶_ ᅴ\u0016\u0004& h \u0003 É r?\nLALM: w ᅵZ à Ô ᅩ\" é¶ É r# Qn ᅵ\\ e\u0012_ þ vm ᅵᄁ ᅡ?\nFluency Answerable Signiﬁcance Ave.\nNQG++ 1.01 1.09 1.02 1.04\nMulti-BERT 1.22 1.19 1.23 1.21\nLALMbase 1.38 1.29 1.46 1.37\nTable 5: Human assessment of the generated questions\non English and Chinese. Each question was assigned\nto score in {0,1,2}which correspond to bad, ok and ex-\ncellent, respectively. The result is statistical signiﬁcant\nwith p< 0.05.\nvision of QG is more speciﬁc, which makes transfer\nlearning less useful.\n4.8 Zero-Shot Learning\nIn this section, we study the zero-shot multi-lingual\nlearning ability of our model. The previous Section\ndemonstrates that English SQuAD could strengthen\nother languages a lot. So we choose SQuAD as the\ntraining data and evaluate other languages. We only\nupdate the parameters of the high-level module for\nSQuAD without modifying the low-level language\nunderstanding module. Therefore, the replacement\nof the low-level module has little inﬂuence on the\nwhole architecture, making the zero-shot inference\navailable. We compare the zero-shot results of\nLALM base model with the supervised NQG++.\nThe result is shown in Table 7.\nWe can see that the zero-shot version of our\nLALM appears to have equaled or eclipsed the QG\nability of NQG++. It is an interesting result show-\ning our model could transfer the question genera-\ntion ability of English to other languages even with-\nout supervision. However, pure zero-shot learning\nis still struggle to achieve a good result, the super-\nvision from the target language is necessary.\n4.9 The Effect of Pre-training\nWe propose the self-supervised denoised auto-\nencoding and next sentence generation to pre-train\nthe model. In this section, we construct a model\nthat does not employ the pre-training but directly\nﬁne-tuned on the target data. The LALM hidden\nsize to 256 and layer and head numbers of 4 and\n8, respectively, to prevent overﬁtting. The results\nof English, Chinese, and Hindi are shown in Ta-\nble 6. The performance of our model drops a lot\nwithout pre-training. Especially, it barely performs\nwell for the low resource Hindi data because there\n2270\nEn Zh Hi\nB4 M R B4 M R B4 M R\nLALM 14.35 17.41 33.52 22.25 21.43 37.94 12.92 24.19 33.10\nLALM+ADM 15.94 18.24 36.24 24.10 22.05 38.78 14.13 23.77 34.24\nLALM+ADM+Pre-train 21.95 21.30 48.23 38.32 27.99 44.49 30.35 34.80 48.82\nTable 6: Ablation study of the pre-training. The three models are ﬁne-tuned on multi-lingual data.\nB1 B2 B3 B4 M R\nZh F 43.07 31.04 23.58 17.74 18.06 22.44\nZ 26.55 18.26 12.10 10.94 11.94 15.89\nKr F 25.20 15.34 10.71 5.07 14.35 16.42\nZ 20.55 11.17 8.32 5.95 13.32 16.77\nFr F 31.31 14.91 10.46 5.54 9.63 23.02\nZ 25.58 13.33 12.49 6.32 11.06 15.22\nHi F 30.15 20.42 12.30 9.03 23.47 32.84\nZ 24.10 15.77 12.54 10.89 26.42 33.01\nTable 7: Zero-shot multi-lingual evaluation. F denotes\nthe performance of NQG++ model, and Z denotes zero-\nshot result where we ﬁne-tune LALM base model on\nSQuAD and directly evaluate on other datasets.\nare only 4,000 training instances. Nevertheless,\nwhen trained with the adversarial decoupling mod-\nule, our model could achieve consistent improve-\nment, demonstrating that the ADM is good at multi-\nlingual transfer learning.\n5 Conclusion\nIn this paper, we propose a language-agnostic lan-\nguage model to deal with the multi-lingual question\ngeneration. The model consists of the low-level\nand the high-level module to explicitly represent\nthe language-dependent and language-independent\ninformation, respectively. We operate the attention\nmask matrix to ﬁt our model to the sequence to\nsequence learning. We propose an adversarial train-\ning mechanism to decouple the two-level modules,\nmaking the low-level module contains more ab-\nstractive representations and the high-level module\nlanguage-agnostic. We also proposed a large-scale\nChinese QG data containing more than 220k ques-\ntions. Experiments on ﬁve languages demonstrate\nour model achieves signiﬁcant improvements over\nprevious methods in multi-lingual QG. For future\nwork, we would like to apply our proposed model\nto other multi-lingual tasks such as summarization\nand question answering.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments. And we appereate the dedi-\ncated labeling efforts contributed by the annoators,\nwhich makes the large-scale Chinese QG datasets\navaliable for the community.\nReferences\nAkari Asai, Akiko Eriguchi, Kazuma Hashimoto, and\nYoshimasa Tsuruoka. 2018. Multilingual extractive\nreading comprehension by runtime machine transla-\ntion. arXiv preprint arXiv:1809.03275.\nXinchi Chen, Zhan Shi, Xipeng Qiu, and Xuan-Jing\nHuang. 2017. Adversarial multi-criteria learning for\nchinese word segmentation. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1193–1203.\nYu Chen, Lingfei Wu, and Mohammed J Zaki. 2019.\nReinforcement learning based graph-to-sequence\nmodel for natural question generation. arXiv\npreprint arXiv:1908.04942.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nLing Mao, and Heyan Huang. 2019. Cross-lingual\nnatural language generation via pre-training. arXiv\npreprint arXiv:1909.10481.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2475–2485.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2019. Cross-lingual\nmachine reading comprehension. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1586–1595.\nBeth Davey and Susan McBride. 1986. Effects\nof question-generation training on reading com-\nprehension. Journal of Educational Psychology ,\n78(4):256.\n2271\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMartin d’Hoffschmidt, Maxime Vidal, Wacim Belb-\nlidia, and Tom Brendl ´e. 2020. Fquad: French\nquestion answering dataset. arXiv preprint\narXiv:2002.06071.\nKaustubh D Dhole and Christopher D Manning.\n2020. Syn-qg: Syntactic and shallow seman-\ntic rules for question generation. arXiv preprint\narXiv:2004.08694.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, pages 13042–13054.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. pages 1342–1352.\nNan Duan, Duyu Tang, Peng Chen, and Ming Zhou.\n2017. Question generation for question answering.\nIn EMNLP, pages 866–874.\nXiangyu Duan, Mingming Yin, Min Zhang, Boxing\nChen, and Weihua Luo. 2019. Zero-shot cross-\nlingual abstractive sentence summarization through\nteaching generation and attention. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3162–3172.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680.\nArt Graesser, Yasuhiro Ozuru, and Jeremiah Sullins.\n2010. What is a good question?\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings\nof the International Conference on Language Re-\nsources and Evaluation (LREC 2018).\nMichael Heilman and Noah A. Smith. 2010. Good\nquestion! statistical ranking for question generation.\nIn HLT-NAACL.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nTom Hosking and Sebastian Riedel. 2019. Evaluating\nrewards for question generation models. In NAACL,\npages 2278–2283.\nHafedh Hussein, Mohammed Elmogy, and Shawkat\nGuirguis. 2014. Automatic english question genera-\ntion system based on template driven scheme.IJCSI,\n11(6):45.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. ICLR.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems ,\npages 3294–3302.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nVishwajeet Kumar, Nitish Joshi, Arijit Mukherjee,\nGanesh Ramakrishnan, and Preethi Jyothi. 2019.\nCross-lingual training for automatic question gen-\neration. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4863–4872.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nSeungyoung Lim, Myungji Kim, and Jooyoul Lee.\n2019. Korquad1. 0: Korean qa dataset for ma-\nchine reading comprehension. arXiv preprint\narXiv:1909.07005.\nJiahua Liu, Yankai Lin, Zhiyuan Liu, and Maosong\nSun. 2019. Xqa: A cross-lingual open-domain ques-\ntion answering dataset. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2358–2368.\nPengfei Liu, Xipeng Qiu, and Xuan-Jing Huang. 2017.\nAdversarial multi-task learning for text classiﬁca-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1–10.\nJack Mostow and Wei Chen. 2009. Generating instruc-\ntion automatically for the reading strategy of self-\nquestioning. In AIED.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\n2272\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910.\nSamuel R ¨onnqvist, Jenna Kanerva, Tapio Salakoski,\nand Filip Ginter. 2019. Is multilingual bert ﬂu-\nent in language generation? arXiv preprint\narXiv:1910.03806.\nVasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean,\nSvetlana Stoyanchev, and Cristian Moldovan. 2010.\nThe ﬁrst question generation shared task evaluation\nchallenge.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba,\nVicki Cheung, Alec Radford, and Xi Chen. 2016.\nImproved techniques for training gans. In Advances\nin neural information processing systems , pages\n2234–2242.\nThomas Scialom, Benjamin Piwowarski, and Jacopo\nStaiano. 2019. Self-attention architectures for\nanswer-agnostic neural question generation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6027–\n6032.\nLinfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang,\nand Daniel Gildea. 2018. Leveraging context infor-\nmation for natural question generation. In NAACL,\npages 569–574.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In International\nConference on Learning Representations.\nJoe H Ward Jr. 1963. Hierarchical grouping to opti-\nmize an objective function. Journal of the American\nstatistical association, 58(301):236–244.\nHan Xiao, Feng Wang, Yanjian Feng, and Jingyao\nZheng. 2018. Dual ask-answer network for ma-\nchine reading comprehension. arXiv preprint\narXiv:1809.01997.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nXingdi Yuan, Tong Wang, Caglar Gulcehre, Alessan-\ndro Sordoni, Philip Bachman, Sandeep Subrama-\nnian, Saizheng Zhang, and Adam Trischler. 2017.\nMachine comprehension by text-to-text neural ques-\ntion generation. arXiv preprint arXiv:1705.02012.\nQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,\nHangbo Bao, and Ming Zhou. 2017. Neural ques-\ntion generation from text: A preliminary study. In\nNLPCC.\nJunnan Zhu, Qian Wang, Yining Wang, Yu Zhou, Jiajun\nZhang, Shaonan Wang, and Chengqing Zong. 2019.\nNcls: Neural cross-lingual summarization. arXiv\npreprint arXiv:1909.00156.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8687613606452942
    },
    {
      "name": "Natural language processing",
      "score": 0.7191101312637329
    },
    {
      "name": "Text generation",
      "score": 0.6737574934959412
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6590642929077148
    },
    {
      "name": "Natural language generation",
      "score": 0.6238957047462463
    },
    {
      "name": "Language model",
      "score": 0.5829954147338867
    },
    {
      "name": "Adversarial system",
      "score": 0.5316883325576782
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5050569176673889
    },
    {
      "name": "Paragraph",
      "score": 0.5027143955230713
    },
    {
      "name": "Task (project management)",
      "score": 0.43123143911361694
    },
    {
      "name": "Question answering",
      "score": 0.4137149453163147
    },
    {
      "name": "Natural language",
      "score": 0.3103405237197876
    },
    {
      "name": "World Wide Web",
      "score": 0.09668636322021484
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}