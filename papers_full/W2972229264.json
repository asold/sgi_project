{
    "title": "Forecaster: A Graph Transformer for Forecasting Spatial and Time-Dependent Data",
    "url": "https://openalex.org/W2972229264",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1993907199",
            "name": "Li Yang",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Moura, Jos\\'e M. F.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2267734160",
            "name": "Moura, Jose M. F.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2919603742",
        "https://openalex.org/W2097581234",
        "https://openalex.org/W2528639018",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2962790412",
        "https://openalex.org/W2904832339",
        "https://openalex.org/W1574814276",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2132555912",
        "https://openalex.org/W2963165299",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2466579956",
        "https://openalex.org/W1982978808",
        "https://openalex.org/W2724789010",
        "https://openalex.org/W2117618130",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2075872298",
        "https://openalex.org/W2810861266",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963124587",
        "https://openalex.org/W2765323781",
        "https://openalex.org/W3103720336",
        "https://openalex.org/W2963669520",
        "https://openalex.org/W2963358464",
        "https://openalex.org/W2970066309"
    ],
    "abstract": "Spatial and time-dependent data is of interest in many applications. This task is difficult due to its complex spatial dependency, long-range temporal dependency, data non-stationarity, and data heterogeneity. To address these challenges, we propose Forecaster, a graph Transformer architecture. Specifically, we start by learning the structure of the graph that parsimoniously represents the spatial dependency between the data at different locations. Based on the topology of the graph, we sparsify the Transformer to account for the strength of spatial dependency, long-range temporal dependency, data non-stationarity, and data heterogeneity. We evaluate Forecaster in the problem of forecasting taxi ride-hailing demand and show that our proposed architecture significantly outperforms the state-of-the-art baselines.",
    "full_text": "Forecaster: A Graph Transformer for Forecasting\nSpatial and Time-Dependent Data\nYang Li and Jos´e M. F. Moura1\nAbstract. Spatial and time-dependent data is of interest in many\napplications. This task is difﬁcult due to its complex spatial depen-\ndency, long-range temporal dependency, data non-stationarity, and\ndata heterogeneity. To address these challenges, we propose Fore-\ncaster, a graph Transformer architecture. Speciﬁcally, we start by\nlearning the structure of the graph that parsimoniously represents the\nspatial dependency between the data at different locations. Based on\nthe topology of the graph, we sparsify the Transformer to account for\nthe strength of spatial dependency, long-range temporal dependency,\ndata non-stationarity, and data heterogeneity. We evaluate Forecaster\nin the problem of forecasting taxi ride-hailing demand and show that\nour proposed architecture signiﬁcantly outperforms the state-of-the-\nart baselines.\n1 Introduction\nSpatial and time-dependent data describe the evolution of signals\n(i.e., the values of attributes) at multiple spatial locations across time\n[39, 14]. It occurs in many domains, including economics [8], global\ntrade [10], environment studies [15], public health [20], or trafﬁc net-\nworks [16] to name a few. For example, the gross domestic product\n(GDP) of different countries in the past century, the daily temperature\nmeasurements of different cities for the last decade, and the hourly\ntaxi ride-hailing demand at various urban locations in the recent year\nare all spatial and time-dependent data. Forecasting such data allows\nto proactively allocate resources and take actions to improve the ef-\nﬁciency of society and the quality of life.\nHowever, forecasting spatial and time-dependent data is challeng-\ning — they exhibit complex spatial dependency, long-range temporal\ndependency, heterogeneity, and non-stationarity. Take the spatial and\ntime-dependent data in a trafﬁc network as an example. The data at a\nlocation (e.g., taxi ride-hailing demand) may correlate more with the\ndata at a geometrically remote location than a nearby location [16],\nexhibiting complex spatial dependency. Also, the data at a time in-\nstant may be similar to the data at a recent time instant, say an hour\nago, but may also highly correlate with the data a day ago or even\na week ago, showing strong long-range temporal dependency. Addi-\ntionally, the spatial and time-dependent data may be inﬂuenced by\nmany other relevant factors (e.g., weather inﬂuences taxi demand).\nThese factors are relevant information, shall be taken into account.\nIn other words, in this paper, we propose to perform forecasting with\nheterogeneous sources of data at different spatial and time scales and\nincluding auxiliary information of a different nature or modality. Fur-\nther, the data may be non-stationary due to unexpected incidents or\ntrafﬁc accidents [16]. This non-stationarity makes the conventional\n1 Carnegie Mellon University, USA\nEmail: {yangli1, moura}@andrew.cmu.edu\ntime series forecasting methods such as auto-regressive integrated\nmoving average (ARIMA) and vector autoregression (V AR), which\nusually rely on stationarity, inappropriate for accurate forecasting\nwith spatial and time-dependent data [16, 40].\nRecently, deep learning models have been proposed for forecast-\ning for spatial and time-dependent data [16, 35, 11, 36, 7, 38, 34, 40].\nTo deal with spatial dependency, most of these models either use pre-\ndeﬁned distance/similarity metrics or other prior knowledge like ad-\njacency matrices of trafﬁc networks to determine dependency among\nlocations. Then, they often use a (standard or graph) convolutional\nneural network (CNN) to better characterize the spatial dependency\nbetween these locations. These ad-hoc methods may lead to errors in\nsome cases. For example, the locations that are considered as being\ndependent (independent) may actually be independent (dependent)\nin practice. As a result, these models may encode the data at a loca-\ntion by considering the data at independent locations and neglecting\nthe data at dependent locations, leading to inaccurate encoding. Re-\ngarding temporal dependency, most of these models use recurrent\nneural networks (RNN), CNN, or their variants to capture the data\nlong-range temporal dependency and non-stationarity. But it is well\ndocumented that these networks may fail to capture temporal depen-\ndency between distant time epochs [9, 29].\nTo tackle these challenges, we propose Forecaster, a new deep\nlearning architecture for forecasting spatial and time-dependent data.\nOur architecture consists of two parts. First, we use the theory of\nGaussian Markov random ﬁelds [24] to learn the structure of the\ngraph that parsimoniously represents the spatial dependency between\nthe locations (we call such graph a dependency graph ). Gaussian\nMarkov random ﬁelds model spatial and time-dependent data as a\nmultivariant Gaussian distribution over the spatial locations. We then\nestimate the precision matrix of the distribution [6]. 2 The precision\nmatrix provides the graph structure with each node representing a\nlocation and each edge representing the dependency between two lo-\ncations. This contrasts prior work on forecasting — we learn from\nthe data its spatial dependency . Second, we integrate the depen-\ndency graph in the architecture of the Transformer [29] for fore-\ncasting spatial and time-dependent data. The Transformer and its ex-\ntensions [29, 3, 33, 4, 22] have been shown to signiﬁcantly outper-\nform RNN and CNN in NLP tasks, as they capture relations among\ndata at distant positions, signiﬁcantly improving the learning of long-\nrange temporal dependency [29]. In our Forecaster, in order to better\ncapture the spatial dependency, we associate each neuron in differ-\nent layers with a spatial location. Then, we sparsify the Transformer\nbased on the dependency graph: if two locations are not connected in\n2 The approach to estimate the precision matrix of a Gaussian Markov ran-\ndom ﬁeld (i.e., graphical lasso) can also be used with non-Gaussian distri-\nbutions [23].\narXiv:1909.04019v5  [cs.LG]  21 Feb 2020\nthe graph, we prune the connection between their associated neurons.\nIn this way, the state encoding for each location is only impacted by\nits own state encoding and encodings for other dependent locations.\nMoreover, pruning the unnecessary connections in the Transformer\navoids overﬁtting.\nTo evaluate the effectiveness of our proposed architecture, we ap-\nply it to the task of forecasting taxi ride-hailing demand in New York\nCity [28]. We pick 996 hot locations in New York City and fore-\ncast the hourly taxi ride-hailing demand around each location from\nJanuary 1st, 2009 to June 30th, 2016. Our architecture accounts for\ncrucial auxiliary information such as weather, day of the week, hour\nof the day, and holidays. This improves signiﬁcantly the forecast-\ning task. Evaluation results show that our architecture reduces the\nroot mean square error (RMSE) and mean absolute percentage error\n(MAPE) of the Transformer by 8.8210% and 9.6192%, respectively,\nand also show that our architecture signiﬁcantly outperforms other\nstate-of-the-art baselines.\nIn this paper, we present critical innovation:\n• Forecaster combines the theory of Gaussian Markov random ﬁelds\nwith deep learning. It uses the former to ﬁnd the dependency graph\namong locations, and this graph becomes the basis for the deep\nlearner forecast spatial and time-dependent data.\n• Forecaster sparsiﬁes the architecture of the Transformer based on\nthe dependency graph, allowing the Transformer to capture better\nthe spatiotemporal dependency within the data.\n• We apply Forecaster to forecasting taxi ride-hailing demand and\ndemonstrate the advantage of its proposed architecture over state-\nof-the-art baselines.\n2 Methodology\nIn this section, we introduce the proposed architecture of Forecaster.\nWe start by formalizing the problem of forecasting spatial and time-\ndependent data (Section 2.1). Then, we use Gaussian Markov ran-\ndom ﬁelds to determine the dependency graph among data at dif-\nferent locations (Section 2.2). Based on this dependency graph, we\ndesign a sparse linear layer, which is a fundamental building block\nof Forecaster (Section 2.3). Finally, we present the entire architecture\nof Forecaster (Section 2.4).\n2.1 Problem Statement\nWe deﬁne spatial and time-dependent data as a series of spatial sig-\nnals, each collecting the data at all spatial locations at a certain time.\nFor example, hourly taxi demand at a thousand locations in 2019\nis a spatial and time-dependent data, while the hourly taxi demand\nat these locations between 8 a.m. and 9 a.m. of January 1st, 2019\nis a spatial signal. The goal of our forecasting task is to predict the\nfuture spatial signals given the historical spatial signals and histori-\ncal/future auxiliary information (e.g., weather history and forecast).\nWe formalize forecasting as learning a function h(·) that maps T\nhistorical spatial signals and T+ T′historical/future auxiliary infor-\nmation to T′future spatial signals, as Equation (1):\n[\nxt−T+1, ··· , xt;\nat−T+1, ··· , at+T′\n]\nh(·)\n−→[ xt+1, ··· , xt+T′ ]\n(1)\nwhere xt is the spatial signal at time t, xt =[\nx1\nt , ··· , x N\nt\n]T\n∈ RN , with xi\nt the data at location i\nat time t; N the number of locations; at the auxiliary information at\ntime t, at ∈RP , P the dimension of the auxiliary information;3 and\nR is the set of the reals.\n2.2 Gaussian Markov Random Field\nWe use Gaussian Markov random ﬁelds to ﬁnd the dependency graph\nof the data over the different spatial locations. Gaussian Markov ran-\ndom ﬁelds model the spatial and time-dependent data{xt}as a mul-\ntivariant Gaussian distribution over N locations, i.e., the probability\ndensity function of the vector given by xt is\nf(xt) = |Q|\n(2π)\nN/2 exp\n(\n−1\n2 (xt −µ)T Q(xt −µ)\n)\n(2)\nwhere µ and Qare the expected value (mean) and precision matrix\n(inverse of the covariance matrix) of the distribution.\nThe precision matrix characterizes the conditional dependency be-\ntween different locations — whether the data xi\nt and xj\nt at the ith\nand jth locations depend on each other or not given the data at all\nthe other locations x−ij\nt (x−ij\nt =\n{\nxk\nt |k̸= i, j\n}\n). We can measure\nthe conditional dependency between locations iand j through their\nconditional correlation coefﬁcient Corr\n(\nxi\nt,xj\nt |x−ij\nt\n)\n:\nCorr\n(\nxi\nt,xj\nt |x−ij\nt\n)\n= − Qij√\nQiiQjj\n(3)\nwhere Qij is the ith, jth entry of Q. In practice, we set a threshold\non Corr\n(\nxi\nt,xj\nt |x−ij\nt\n)\n, and treat locations iand j as conditionally\ndependent if the absolute value of Corr\n(\nxi\nt,xj\nt |x−ij\nt\n)\nis above the\nthreshold.\nThe non-zero entries deﬁne the structure of the dependency graph\nbetween locations. Figure 1 shows an example of a dependency\ngraph. Locations 1 and 2 and locations 2 and 3 are conditionally de-\npendent, while locations 1 and 3 are conditionally independent. This\nprinciple example illustrates the advantage of Gaussian Markov ran-\ndom ﬁeld over ad-hoc pairwise similarity metrics — the former leads\nto parsimonious (sparse) graph representations.\n1\n2\n3\nFigure 1. An example of a simple dependency graph.\nWe estimate the precision matrix by graphical lasso [6], an L1-\npenalized maximum likelihood estimator:\nmin\nQ\ntr (SQ) −log det (Q) +λ∥Q∥1\ns.t., Q∈\n{\nQ= QT , Q≻0\n} (4)\nwhere Sis the empirical covariance matrix computed from the data:\nS = 1\nM−1\n∑M\nt=1 (xt −µ)T (xt −µ)\nµ = 1\nM\n∑M\nt=1 xt\n(5)\nwhere M is the number of time samples used to compute S.\n3 For simplicity, we assume in this work that different locations share the\nsame auxiliary information, i.e., at can impact xi\nt, for any i. However, it\nis easy to generalize our approach to the case where locations do not share\nthe same auxiliary information.\n2\n2.3 Building Block: Sparse Linear Layer\nWe use the dependency graph to sparsify the architecture of the\nTransformer. This leads to the Transformer better capturing the spa-\ntial dependency within the data. There are multiple linear layers in\nthe Transformer. Our sparsiﬁcation on the Transformer replaces all\nthese linear layers by the sparse linear layers described in this sec-\ntion.\nWe use the dependency graph to build a sparse linear layer. Fig-\nure 2 shows an example (based on the dependency graph in Figure 1).\nSuppose that initially thelth layer (of ﬁve neurons) is fully connected\nto the l+ 1th layer (of nine neurons). We assign neurons to the data\nat different locations (marked as ”1”, ”2”, and ”3” for locations 1, 2,\nand 3, respectively) and to the auxiliary information (marked as ”a”)\nas illustrated next. How to assign neurons is a design choice for users.\nIn this example, assign one neuron to each location and two neurons\nto the auxiliary information at the lth layer and assign two neurons\nto each location and three neurons to the auxiliary information at the\nl+ 1th layer. After assigning neurons, we prune connections based\non the structure of the dependency graph. As locations 1 and 3 are\nconditionally independent, we prune the connections between them.\nWe also prune the connections between the neurons associated with\nlocations and the auxiliary information to further simplify the archi-\ntecture.4 This way, the encoding for the data at a location is only im-\npacted by the encodings of itself and of its dependent locations, better\ncapturing the spatial dependency between locations. Moreover, prun-\ning the unnecessary connections between conditionally independent\nlocations helps avoiding overﬁtting.\n1\n1\n2\n2\n3\n3\na\na\na\n1\n 2\n 3\n a\na\n!+1thlayer\n!thlayer\nFigure 2. An example of a sparse linear layer based on the dependency\ngraph in Figure 1 (neurons marked as ”1”, ”2”, ”3”, and ”a” are for locations\n1, 2, and 3, and auxiliary information, respectively).\nOur sparse linear layer is similar to the state-of-the-art graph con-\nvolution approaches such as GCN [12] and TAGCN [5, 26] — all of\nthem transform the data based on the adjacency matrix of the graph.\nThe major difference is our sparse linear layer learns the weights for\nnon-zero entries of the adjacency matrix (equivalent to the weights\nof the sparse linear layer), considering that different locations may\nhave different strengths of dependency between each other.\n2.4 Entire Architecture: Graph Transformer\nForecaster adopts an architecture similar to that of the Transformer\nexcept for substituting all the linear layers in the Transformer with\nour sparse linear layer designed based on the dependency graph. Fig-\nure 3 shows its architecture. Forecaster employs an encoder-decoder\narchitecture [27], which has been widely adopted in sequence gen-\neration tasks such as taxi demand forecasting [16] and pose predic-\ntion [30]. The encoder is used to encode the historical spatial signals\nand historical auxiliary information; the decoder is used to predict\nthe future spatial signals based on the output of the encoder and the\n4 However, our architecture still allows the encodings for the data at different\nlocations (i.e., the encoding for the spatial signal) to consider the auxiliary\ninformation through the sparse multi-head attention layers in our architec-\nture, which we will illustrate in the Section 2.4.\nfuture auxiliary information. We omit what Forecaster shares with\nthe Transformer (e.g., positional encoding, multi-head attention) and\nemphasize only on their differences in this section. Instead, we pro-\nvide a brief introduction to multi-head attention in the appendix.\n2.4.1 Encoder\nAt each time step in the history, we concatenate the spatial signal\nwith its auxiliary information. This way, we obtain a sequence where\neach element is a vector consisting of the spatial signal and the aux-\niliary information at a speciﬁc time step. The encoder takes this se-\nquence as input. Then, a sparse embedding layer (consisting of a\nsparse linear layer with ReLU activation) maps each element of this\nsequence to the state space of the modeland outputs a new sequence.\nIn Forecaster, except for the sparse linear layer at the end of the de-\ncoder, all the layers have the same output dimension. We term this di-\nmension dmodel and the space with this dimension as the state space\nof the model. After that, we add positional encoding to the new se-\nquence, giving temporal order information to each element of the se-\nquence. Next, we let the obtained sequence pass through N stacked\nencoder layers to generate the encoding of the input sequence. Each\nencoder layer consists of a sparse multi-head attention layer and a\nsparse feedforward layer. These layers are the same multi-head at-\ntention layer and feedforward layer as in the Transformer, except that\nsparse linear layers, which reﬂect the spatial dependency between lo-\ncations, to replace linear layers within them. The sparse multi-head\nattention layer enriches the encoding of each element with the infor-\nmation of other elements in the sequence, capturing the long-range\ntemporal dependency between elements. It takes each element as a\nquery, as a key, and also as a value. A query is compared with other\nkeys to obtain the similarities between an element and other ele-\nments, and then these similarities are used to weight the values to\nSparse Multi-Head Attention\nSparse EmbeddingPositional Encoding\nSparse Feed Forward\nN x\n!\"#$∥&\"#$ '=−*+1,…,0 Sparse EmbeddingPositional Encoding\n!\"0$#1∥&\"0$ '=1,…,*′\nSparse Masked Multi-Head Attention\nSparse Multi-Head Attention\nSparse Feed Forward\nSparse Linear!\"0$'=1,…,*′\nN x\nvaluekeyquery\nquerykeyvalue\nquerykeyvalue\nEncoderDecoder\nFigure 3. Architecture of Forecaster (a ∥ b represents concatenating vector\na with vector b).\n3\nobtain the new encoding of the element. Note each query, key, and\nvalue consists of two parts: the part for encoding the spatial signal\nand the part for encoding the auxiliary information — both impact\nthe similarity between a query and a key. As a result, in the new en-\ncoding of each element, the part for encoding the spatial signal takes\ninto account the auxiliary information. The sparse feedforward layer\nfurther reﬁnes the encoding of each element.\n2.4.2 Decoder\nFor each time step in the future, we concatenate its auxiliary infor-\nmation with the (predicted) spatial signal one step before. Then, we\ninput this sequence to the decoder. The decoder ﬁrst uses a sparse\nembedding layer to map each element of the sequence to the state\nspace of the model, adds the positional encoding, and then passes it\nthrough Nstacked decoder layers to obtain the new encoding of each\nelement. Finally, the decoder uses a sparse linear layer to project\nthis encoding back and predict the next spatial signal. Similar to the\nTransformer, the decoder layer contains twosparse multi-head atten-\ntion layers and a sparse feedforward layer. The ﬁrst (masked) sparse\nmulti-head attention layer compares the elements in the sequence,\nobtaining a new encoding for each element. Like the Transformer,\nwe put a mask here such that an element is compared with only ear-\nlier elements in the sequence. This is because, in the inference stage,\na prediction can be made based on only the earlier predictions and the\npast history — information about later predictions are not available.\nHence, a mask needs to be placed here such that in the training stage\nwe also do the same thing as in the inference stage. The second sparse\nmulti-head attention layer compares each element of the sequence in\nthe decoder with the history sequence in the encoder so that we can\nlearn from the past history. If non-stationarity happens, the compari-\nson will tell the element is different from the historical elements that\nit is normally similar to, and therefore we should instead learn from\nother more similar historical elements, handling this non-stationarity.\nThe following sparse feedforward layer further reﬁnes the encoding\nof each element.\n3 Evaluation\nIn this section, we apply Forecaster to the problem of forecasting\ntaxi ride-hailing demand in Manhattan, New York City. We demon-\nstrate that Forecaster outperforms the state-of-the-art baselines (the\nTransformer [29] and DCRNN [16]) and a conventional time series\nforecasting method (V AR [19]).\n3.1 Evaluation Settings\n3.1.1 Dataset\nOur evaluation uses the NYC Taxi dataset [28] from 01/01/2009 to\n06/30/2016 (7.5 years in total). This dataset records detailed infor-\nmation for each taxi trip in New York City, including its pickup and\ndropoff locations. Based on this dataset, we select 996 locations with\nhot taxi ride-hailing demand in Manhattan of New York City, shown\nin Figure 4. Speciﬁcally, we compute the taxi ride-hailing demand at\neach location by accumulating the taxi ride closest to that location.\nNote that these selected locations are not uniformly distributed, as\ndifferent regions of Manhattan has distinct taxi demand. 5 We com-\npute the hourly taxi ride-hailing demand at these selected locations\n5 We use the following algorithm to select the locations. Our roadmap has\n5464 locations initially. Then, we compute the average hourly taxi demand\nat each of these locations. After that, we use a threshold (= 10) and an it-\n-74.02 -74 -73.98 -73.96 -73.94 -73.92\nLongitude (degree)\n40.7\n40.72\n40.74\n40.76\n40.78\n40.8\n40.82\n40.84\n40.86\n40.88Latitude (degree)\nSelected Locations\nRoad\nFigure 4. Selected locations in Manhattan.\nacross time. As a result, our dataset contains 65.4 million data points\nin total (996 locations ×number of hours in 7.5 years). As far as\nwe know, it is the largest (in terms of data points) and longest (in\nterms of time length) dataset in similar types of study. Our dataset\ncovers various types of scenarios and conditions (e.g., under extreme\nweather condition). We split the dataset into three parts — training\nset, validation set, and test set. Training set uses the data in the time\ninterval 01/01/2009 – 12/31/2011 and 07/01/2012 – 06/30/2015; val-\nidation set uses the data in 01/01/2012 – 06/30/2012; and the test set\nuses the data in 07/01/2015 –06/30/2016.\nOur evaluation uses hourly weather data from [32] to construct\n(part of) the auxiliary information. Each record in this weather data\ncontains seven entries — temperature, wind speed, precipitation, vis-\nibility, and the Booleans for rain, snow, and fog.\n3.1.2 Details of the Forecasting Task\nIn our evaluation, we forecast taxi demand for the next three hours\nbased on the previous 674 hours and the corresponding auxiliary in-\nformation (i.e., use a history of four weeks around;T = 674, T′= 3\nin Equation (1)). Instead of directly inputing this history sequence\ninto the model, we ﬁrst ﬁlter it. This ﬁltering is based on the fol-\nlowing observation: a future taxi demand correlates more with the\ntaxi demand at previous recent hours, the similar hours of the past\nweek, and the similar hours on the same weekday in the past several\nweeks. In other words, we shrink the history sequence and only input\nthe elements relevant to forecasting. Speciﬁcally, our ﬁltered history\nsequence contains the data for the following taxi demand (and the\ncorresponding auxiliary information):\n• The recent past hours: xt−i, i= 0,..., 5 ;\n• Similar hours of the past week: xt+i−j×24, i= −1,..., 5, j=\n1,.., 6 ;\n• Similar hours on the same weekday of the past several weeks:\nxt+i−j×24×7, i= −1,..., 5, j= 1,.., 4.\nerative procedure to down select to the 996 hot locations. This algorithm\nselects the locations from higher to lower demand. Every time when a lo-\ncation is added to the pool of selected locations, we compute the average\nhourly taxi demand at each of the locations in the pool by remapping the\ntaxi rides to these locations. If every location in the pool has a demand no\nless than the threshold, we will add the location; otherwise, remove it from\nthe pool. We reiterate this procedure over all the 5464 locations. This pro-\ncedure guarantees that all the selected locations have an average hourly taxi\ndemand no less than the threshold.\n4\n3.1.3 Evaluation Metrics\nSimilar to prior work [16, 7], we use root mean square er-\nror (RMSE) and mean absolute percentage error (MAPE) to\nevaluate the quality of the forecasting results. Suppose that for\nthe jth forecasting job ( j = 1 ,··· ,S), the ground truth\nis\n{\nxi(j)\nt |t= 1,··· ,T\n′\n, i= 1,··· ,N\n}\n, and the prediction is\n{\nˆxi\nt\n(j)\n|t= 1,··· ,T\n′\n, i= 1,··· ,N\n}\n, where N is the number of\nlocations, and T′ is the length of the forecasted sequence. Then\nRMSE and MAPE are:\nRMSE =\n√\n1\nST′N\nS∑\nj=1\nT′∑\nt=1\nN∑\ni=1\n(\nˆxi\nt\n(j)\n−xi(j)\nt\n)2\nMAPE = 1\nST′N\nS∑\nj=1\nT′\n∑\nt=1\nN∑\ni=1\n⏐⏐⏐⏐⏐⏐\nˆxi\nt\n(j)\n−xi(j)\nt\nxi(j)\nt\n⏐⏐⏐⏐⏐⏐\n(6)\nFollowing practice in prior work [7], we set a threshold on xi(j)\nt\nwhen computing MAPE: if xi(j)\nt <10, disregard the term associated\nit. This practice prevents small xi(j)\nt dominating MAPE.\n3.2 Models Details\nWe evaluate Forecaster and compare it against baseline models in-\ncluding V AR, DCRNN, and the Transformer.\n3.2.1 Our model: Forecaster\nForecaster uses weather (7-dimensional vector), weekday (one-\nhot encoding, 7-dimensional vector), hour (one-hot encoding, 24-\ndimensional vector), and a Boolean for holidays (1-dimensional vec-\ntor) as auxiliary information (39-dimensional vector). Concatenated\nwith a spatial signal (996-dimensional vector), each element of the\ninput sequence for Forecaster is a 1035-dimensional vector. Fore-\ncaster uses one encoder layer and one decoder layer (i.e., N = 1).\nExcept for the sparse linear layer at the end of the decoder, all the\nlayers of Forecaster use four neurons for encoding the data at each\nlocation and 64 neurons for encoding the auxiliary information and\nthus have 4048 neurons in total (i.e.,dmodel = 4×996+64 = 4048).\nThe sparse linear layer at the end has 996 neurons. Forecaster uses\nthe following loss function:\nloss (·) =η×RMSE2 + MAPE (7)\nwhere η is a constant balancing the impact of RMSE with MAPE,\nη= 8×10−3.\n3.2.2 Baseline model: Vector Autoregression\nVector autoregression (V AR) [19] is a conventional multivariant time\nseries forecasting method. It predicts the future endogenous variables\n(i.e., the spatial signal xt in our case) as a linear combination of the\npast endogenous variables and the current exogenous variables (i.e.,\nthe auxiliary information at in our case):\nˆ xt+1 = A1xt + ··· + Apxt−p+1 + Bat+1 (8)\nwhere xt ∈RN , at+1 ∈RP , Ai ∈RN×N , i = 1,...,p, B ∈\nRN×P . Matrices Ai and B are estimated during the training stage.\nOur implementation is based on Statsmodels[25], a standard Python\npackage for statistics.\n3.2.3 Baseline model: DCRNN\nDCRNN [16] is a deep learning model that models the dependency\nrelations between locations as a diffusion process guided by a pre-\ndeﬁned distance metric. Then, it leverages graph CNN to capture\nspatial dependency and RNN to capture the temporal dependency\nwithin the data.\n3.2.4 Baseline model: Transformer\nThe Transformer [29] uses the same input and loss function as Fore-\ncaster. It also adopts a similar architecture except that all the layers\nare fully-connected. For a comprehensive comparison, we evaluate\ntwo versions of the Transformer:\n• Transformer (same width): All the layers in this implementation\nhave the same width as Forecaster. The linear layer at the end of\ndecoder has a width of 996; other layers have a width of 4048 (i.e.,\ndmodel = 4048).\n• Transformer (best width): We vary the width of all the layers (ex-\ncept for the linear layer at the end of decoder which has a ﬁxed\nwidth of 996) from 64 to 4096, and pick the best width in perfor-\nmance to implement.\n3.3 Results\nOur evaluation of Forecaster starts by using Gaussian Markov ran-\ndom ﬁelds to determine the spatial dependency between the data at\ndifferent locations. Based on the method in Section 2.2, we can ob-\ntain a conditional correlation matrix where each entry of the matrix\nrepresents the conditional correlation coefﬁcient between two loca-\ntions. If the absolute value of an entry is less than a threshold, we will\ntreat the corresponding two locations as conditionally independent,\nand round the value of the entry to zero. This threshold can be chosen\nbased only on the performance on the validation set. Figure 5 shows\nthe structure of the conditional correlation matrix under a threshold\nof 0.1. We can see that the matrix is sparse, which means a location\ngenerally depends on just a few other locations other than all the lo-\ncations. We found that a location depends on only 2.5 other locations\non average. There are some locations which many other locations de-\npend on. For example, there is a location in Lower Manhattan which\n16 other locations depend on. This may be because there are many lo-\ncations with signiﬁcant taxi demand in Lower Manhattan, with these\n1 200 400 600 800 996\nLocation ID\n1\n200\n400\n600\n800\n996 Location ID\nFigure 5. Structure of the conditional correlation matrix (under a threshold\nof 0.1; each dot represents a non-zero entry).\n5\nTable 1. RMSE and MAPE of Forecaster and baseline models.\nMetrics Model Average Next step Second next step Third next step\nV AR 6.9991 6.4243 7.1906 7.3476\nDCRNN 5.3750 ± 0.0691 5.1627 ± 0.0644 5.4018 ± 0.0673 5.5532 ± 0.0758\nRMSE Transformer (same width) 5.6802 ± 0.0206 5.4055 ± 0.0109 5.6632 ± 0.0173 5.9584 ± 0.0478\nTransformer (best width) 5.6898 ± 0.0219 5.4066 ± 0.0302 5.6546 ± 0.0581 5.9926 ± 0.0472\nForecaster 5.1879 ± 0.0082 4.9629 ± 0.0102 5.2275 ± 0.0083 5.3651 ± 0.0065\nV AR 33.7983 31.9485 34.5338 34.9126\nDCRNN 24.9853 ± 0.1275 24.4747 ± 0.1342 25.0366 ± 0.1625 25.4424 ± 0.1238\nMAPE (%) Transformer (same width) 22.5787 ± 0.2153 21.8932 ± 0.2006 22.3830 ± 0.1943 23.4583 ± 0.2541\nTransformer (best width) 22.2793 ± 0.1810 21.4545 ± 0.0448 22.1954 ± 0.1792 23.1868 ± 0.3334\nForecaster 20.1362 ± 0.0316 19.8889 ± 0.0269 20.0954 ± 0.0299 20.4232 ± 0.0604\nlocations sharing a strong dependency. Figure 6 shows the top 400\nspatial dependencies. We see some long-range spatial dependency\nbetween remote locations. For example, there is a strong dependency\nbetween Grand Central Terminal and New York Penn Station, which\nare important stations in Manhattan with a large trafﬁc of passengers.\n-74.02 -74 -73.98 -73.96 -73.94 -73.92\nLongitude (degree)\n40.7\n40.72\n40.74\n40.76\n40.78\n40.8\n40.82\n40.84\n40.86\n40.88Latitude (degree)\nRoad\nTop 400 Connections\nGrand Central Terminal\nNew York Penn Station\nFigure 6. Top 400 dependency relations between locations.\nAfter determining the spatial dependency between locations, we\nuse the graph Transformer architecture of Forecaster to predict the\ntaxi demand. Table 1 contrasts the performance of Forecaster to other\nbaseline models. Here we run all the evaluated deep learning models\nsix times (using different seeds) and report the mean and the stan-\ndard deviation of the results. As V AR is not subject to the impact of\nrandom initialization, we run it once. We can see for all the evalu-\nated models, the RMSE and MAPE of predicting the next step are\nlower than that of predicting later steps (e.g., the third next step).\nThis is because, for all the models, the prediction of later steps is\nbuilt upon the prediction of the next step, and thus the error of the\nformer includes the error of the latter. Comparing the performance\nof these models, we can see the RMSE and MAPE of V AR is higher\nthan that of the deep learning models. This is because V AR does not\nmodel well the non-linearity and non-stationarity within the data; it\nalso does not consider the spatial dependencies between locations\nin the structure of its coefﬁcient matrices (matrices Ai and B in\nEquation (8)). Among the deep learning models, DCRNN and the\nTransformer perform similarly. The former captures the spatial de-\npendency within the data but does not capture well the long-range\ntemporal dependency, while the latter focuses on exploiting the long-\nrange temporal dependency but neglects the spatial dependency. As\nfor our method, Forecaster outperforms all the baseline methods at\nevery future step of forecasting. On average (over these future steps),\nForecaster achieves an RMSE of 5.1879 and a MAPE of 20.1362,\nwhich is 8.8210% and 9.6192% better than Transformer (best width),\nand 3.4809% and 19.4078% better than DCRNN. This demonstrates\nthe advantage of Forecaster in capturing both the spatial dependency\nand the long-range temporal dependency.\n4 Related Work\nTo our knowledge, this work is the ﬁrst (1) to integrate Gaussian\nMarkov Random ﬁelds with deep learning to forecast spatial and\ntime-dependent data, using the former to derive a dependency graph;\n(2) to sparsify the architecture of the Transformer based on the de-\npendency graph, signiﬁcantly improving the forecasting quality of\nthe result architecture. The most closely related work is a set of pro-\nposals on forecasting spatial and time-dependent data and the Trans-\nformer, which we brieﬂy review in this section.\n4.1 Spatial and Time-Dependent Data Forecasting\nConventional methods for forecasting spatial and time-dependent\ndata such as ARIMA and Kalman ﬁltering-based methods [18, 17]\nusually impose strong stationary assumptions on the data, which\nare often violated [16]. Recently, deep learning-based methods have\nbeen proposed to tackle the non-stationary and highly nonlinear na-\nture of the data [35, 38, 36, 7, 34, 16]. Most of these works consist\nof two parts: modules to capture spatial dependency and modules\nto capture temporal dependency. Regarding spatial dependency, the\nliterature mostly uses prior knowledge such as physical closeness be-\ntween regions to derive an adjacency matrix and/or pre-deﬁned dis-\ntance/similarity metrics to decide whether two locations are depen-\ndent or not. Then, based on this information, they usually use a (stan-\ndard or graph) CNN to characterize the spatial dependency between\ndependent locations. However, these methods are not good predic-\ntors of dependency relations between the data at different locations.\nRegarding temporal dependency, available works [35, 36, 7, 34, 16]\nusually use RNNs and CNNs to extract the long-range temporal de-\npendency. However, both RNN and CNN do not learn well the long-\nrange temporal dependency, with the number of operations used to\nrelate signals at two distant time positions in a sequence growing at\nleast logarithmically with the distance between them [29].\nWe evaluate our architecture with the problem of forecasting taxi\nride-hailing demand around a large number of spatial locations. The\n6\nproblem has two essential features: (1) These locations are not uni-\nformly distributed like pixels in an image, making standard CNN-\nbased methods [35, 34, 38] not good for this problem; (2) it is de-\nsirable to perform multi-step forecasting, i.e., forecasting at several\ntime instants in the future, this implying that the work mainly de-\nsigned for single-step forecasting [36, 7] is less applicable. DCRNN\n[16] is the state-of-the-art baseline satisfying both features. Hence,\nwe compare our architecture with DCRNN and show that our work\noutperforms DCRNN.\n4.2 Transformer\nThe Transformer [29] avoids recurrence and instead purely relies on\nthe self-attention mechanism to let the data at distant positions in a\nsequence to relate to each other directly. This beneﬁts learning long-\nrange temporal dependency. The Transformer and its extensions have\nbeen shown to signiﬁcantly outperform RNN-based methods in NLP\nand image generation tasks [29, 22, 3, 33, 4, 21, 13]. It has also been\napplied to graph and node classiﬁcation problems [1, 37]. However, it\nis still unknown how to apply the architecture of Transformer to spa-\ntial and time-dependent data, especially to deal with spatial depen-\ndency between locations. Later work [31] extends the architecture of\nTransformer to video generation. Even though this also needs to ad-\ndress spatial dependency between pixels, the nature of the problem\nis different from our task. In video generation, pixels exhibit spatial\ndependency only over a short time interval, lasting for at most tens\nof frames — two pixels may be dependent only for a few frames and\nbecome independent in later frames. On the contrary, in spatial and\ntime-dependent data, locations exhibit long-term spatial dependency\nlasting for months or even years. This fundamental difference of the\napplications that we consider enables us to use Gaussian Markov ran-\ndom ﬁelds to determine the dependency graph as basis for sparsifying\nthe Transformer. Child et al. [2] propose another sparse Transformer\narchitecture with a different goal of accelerating the multi-head atten-\ntion operations in the Transformer. This architecture is very different\nfrom our architecture.\n5 Conclusion\nForecasting spatial and time-dependent data is challenging due to\ncomplex spatial dependency, long-range temporal dependency, non-\nstationarity, and heterogeneity within the data. This paper proposes\nForecaster, a graph Transformer architecture to tackle these chal-\nlenges. Forecaster uses Gaussian Markov random ﬁelds to determine\nthe dependency graph between the data at different locations. Then,\nForecaster sparsiﬁes the architecture of the Transformer based on\nthe structure of the graph and lets the sparsiﬁed Transformer (i.e.,\ngraph Transformer) capture the spatiotemporal dependency, non-\nstationarity, and heterogeneity in one shot. We apply Forecaster to\nthe problem of forecasting taxi-ride hailing demand at a large number\nof spatial locations. Evaluation results demonstrate that Forecaster\nsigniﬁcantly outperforms state-of-the-art baselines (the Transformer\nand DCRNN).\nACKNOWLEDGEMENTS\nWe thank the reviewers. This work is partially supported by NSF\nCCF (award 1513936).\nREFERENCES\n[1] Benson Chen, Regina Barzilay, and Tommi Jaakkola, ‘Path-Augmented\nGraph Transformer Network’, inWorkshop on Learning and Reasoning\nwith Graph-Structured Data (ICML workshop), pp. 1–5, (2019).\n[2] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever, ‘Gen-\nerating Long Sequences with Sparse Transformers’, arXiv preprint\narXiv:1904.10509, (2019).\n[3] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov, ‘Transformer-XL: At-\ntentive Language Models beyond a Fixed-Length Context’, in Annual\nMeeting of the Association for Computational Linguistics (ACL) , pp.\n2978–2988, (2019).\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,\n‘Bert: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding’, in Annual Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), p. 41714186, (2019).\n[5] Jian Du, Shanghang Zhang, Guanhang Wu, Jos ´e M. F. Moura, and\nSoummya Kar, ‘Topology Adaptive Graph Convolutional Networks’,\narXiv preprint arXiv:1710.10370, 1–13, (2017).\n[6] Jerome Friedman, Trevor Hastie, and Robert Tibshirani, ‘Sparse Inverse\nCovariance Estimation with the Graphical Lasso’, Biostatistics, 9(3),\n432–441, (2008).\n[7] Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang, Jieping\nYe, and Yan Liu, ‘Spatiotemporal Multi-Graph Convolution Network\nfor Ride-Hailing Demand Forecasting’, in AAAI Conference on Artiﬁ-\ncial Intelligence, pp. 3656–3663, (2019).\n[8] Alfred Greiner, Willi Semmler, and Gang Gong, The Forces of Eco-\nnomic Growth: A Time Series Perspective, Princeton University Press,\n2016.\n[9] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J ¨urgen Schmid-\nhuber, Gradient Flow in Recurrent Nets: the Difﬁculty of Learning\nLong-Term Dependencies, A Field Guide to Dynamical Recurrent Neu-\nral Networks. IEEE Press, 2001.\n[10] Johannes Hofmann, Michael Gr ¨oßler, Manuel Rubio-S ´anchez, P-P\nPichler, and Dirk Joachim Lehmann, ‘Visual Exploration of Global\nTrade Networks with Time-Dependent and Weighted Hierarchical Edge\nBundles on GPU’,Computer Graphics Forum, 36(3), 273–282, (2017).\n[11] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena,\n‘Structural-RNN: Deep Learning on Spatio-Temporal Graphs’, inIEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pp.\n5308–5317, (2016).\n[12] Thomas N. Kipf and Max Welling, ‘Semi-Supervised Classiﬁcation\nwith Graph Convolutional Networks’, in International Conference on\nLearning Representations (ICLR), pp. 1–14, (2017).\n[13] Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and\nHannaneh Hajishirzi, ‘Text Generation from Knowledge Graphs with\nGraph Transformers’, in Annual Conference of the North American\nChapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies (NAACL-HLT), pp. 2284–2293, (2019).\n[14] Jie Li, Siming Chen, Kang Zhang, Gennady Andrienko, and Natalia\nAndrienko, ‘Cope: Interactive Exploration of Co-Occurrence Patterns\nin Spatial Time Series’,IEEE Transactions on Visualization and Com-\nputer Graphics, 25(8), 2554–2567, (2019).\n[15] Jie Li, Kang Zhang, and Zhao-Peng Meng, ‘Vismate: Interactive Visual\nAnalysis of Station-Based Observation Data on Climate Changes’, in\nIEEE Conference on Visual Analytics Science and Technology (VAST),\npp. 133–142, (2014).\n[16] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu, ‘Diffusion Convo-\nlutional Recurrent Neural Network: Data-Driven Trafﬁc Forecasting’,\nin International Conference on Learning Representations (ICLR) , pp.\n1–16, (2018).\n[17] Marco Lippi, Matteo Bertini, and Paolo Frasconi, ‘Short-Term Trafﬁc\nFlow Forecasting: An Experimental Comparison of Time-Series Anal-\nysis and Supervised Learning’,IEEE Transactions on Intelligent Trans-\nportation Systems, 14(2), 871–882, (2013).\n[18] Wei Liu, Yu Zheng, Sanjay Chawla, Jing Yuan, and Xie Xing, ‘Discov-\nering Spatio-Temporal Causal Interactions in Trafﬁc Data Streams’, in\nACM SIGKDD International Conference on Knowledge Discovery and\nData Mining (KDD), pp. 1010–1018, (2011).\n[19] Helmut L ¨utkepohl, New Introduction to Multiple Time Series Analysis,\nSpringer, 2005.\n[20] Daniel B Neill, ‘Expectation-Based Scan Statistics for Monitoring Spa-\n7\ntial Time Series Data’, International Journal of Forecasting , 25(3),\n498–517, (2009).\n[21] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam\nShazeer, Alexander Ku, and Dustin Tran, ‘Image Transformer’, in In-\nternational Conference on Machine Learning (ICML), pp. 4055–4064,\n(2018).\n[22] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever,\n‘Improving Language Understanding by Generative Pre-training’,Ope-\nnAI, (2018).\n[23] Pradeep Ravikumar, Martin J Wainwright, Garvesh Raskutti, and Bin\nYu, ‘High-Dimensional Covariance Estimation by Minimizing L1-\nPenalized Log-Determinant Divergence’, Electronic Journal of Statis-\ntics, 5, 935–980, (2011).\n[24] Havard Rue and Leonhard Held, Gaussian Markov Random Fields:\nTheory And Applications (Monographs on Statistics and Applied Prob-\nability), Chapman & Hall/CRC, 2005.\n[25] Skipper Seabold and Josef Perktold, ‘Statsmodels: Econometric and\nStatistical Modeling with Python’, in Python in Science Conference ,\npp. 57–61, (2010).\n[26] John Shi, Mark Cheung, Jian Du, and Jos ´e M. F. Moura, ‘Classiﬁcation\nwith Vertex-Based Graph Convolutional Neural Networks’, inAsilomar\nConference on Signals, Systems, and Computers (ACSSC) , pp. 752–\n756, (2018).\n[27] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, ‘Sequence to Sequence\nLearning with Neural Networks’, in Advances in Neural Information\nProcessing Systems (NIPS), pp. 3104–3112, (2014).\n[28] NYC Taxi and Limousine Commission, ‘Trip Record Data’, (2018).\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, ‘Atten-\ntion is All You Need’, in Advances in Neural Information Processing\nSystems (NIPS), pp. 5998–6008, (2017).\n[30] Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert,\n‘The Pose Knows: Video Forecasting by Generating Pose Futures’, in\nIEEE International Conference on Computer Vision (ICCV), pp. 3332–\n3341, (2017).\n[31] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He,\n‘Non-Local Neural Networks’, in IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pp. 7794–7803, (2018).\n[32] Weather Underground, ‘Historical Weather’, (2018).\n[33] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Rus-\nlan Salakhutdinov, and Quoc V Le, ‘XLNet: Generalized Autore-\ngressive Pretraining for Language Understanding’, arXiv preprint\narXiv:1906.08237, (2019).\n[34] Huaxiu Yao, Xianfeng Tang, Hua Wei, Guanjie Zheng, and Zhenhui Li,\n‘Revisiting Spatial-Temporal Similarity: A Deep Learning Framework\nfor Trafﬁc Prediction’, in AAAI Conference on Artiﬁcial Intelligence ,\npp. 5668–5675, (2019).\n[35] Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia, Siyu Lu,\nPinghua Gong, Jieping Ye, and Zhenhui Li, ‘Deep Multi-View Spatial-\nTemporal Network for Taxi Demand Prediction’, in AAAI Conference\non Artiﬁcial Intelligence, pp. 2588–2595, (2018).\n[36] Bing Yu, Haoteng Yin, and Zhanxing Zhu, ‘Spatio-Temporal Graph\nConvolutional Networks: A Deep Learning Framework for Trafﬁc Fore-\ncasting’, inInternational Joint Conference on Artiﬁcial Intelligence (IJ-\nCAI), pp. 3634–3640, (2018).\n[37] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyun-\nwoo J Kim, ‘Graph Transformer Networks’, inAdvances in Neural In-\nformation Processing Systems (NeurIPS), pp. 11960–11970, (2019).\n[38] Junbo Zhang, Yu Zheng, and Dekang Qi, ‘Deep Spatio-Temporal\nResidual Networks for Citywide Crowd Flows Prediction’, in AAAI\nConference on Artiﬁcial Intelligence, pp. 1655–1661, (2017).\n[39] Pusheng Zhang, Yan Huang, Shashi Shekhar, and Vipin Kumar, ‘Cor-\nrelation Analysis of Spatial Time Series Datasets: A Filter-and-Reﬁne\nApproach’, in Paciﬁc-Asia Conference on Knowledge Discovery and\nData Mining (PAKDD), pp. 532–544, (2003).\n[40] Ali Ziat, Edouard Delasalles, Ludovic Denoyer, and Patrick Gallinari,\n‘Spatio-Temporal Neural Networks for Space-Time Series Forecasting\nand Relations Discovery’, in IEEE International Conference on Data\nMining (ICDM), pp. 705–714, (2017).\nAppendix: Multi-Head Attention\nThe multi-head attention layer is a core component of the Trans-\nformer for capturing long-range temporal dependency within data.\nIt takes a query sequence\n{\nqt |qt ∈Rdmodel, t= 1,...,T\n}\n, a key\nsequence\n{\nkt |kt ∈Rdmodel, t= 1,...,T\n}\n, and a value sequence{\nvt |vt ∈Rdmodel, t= 1,...,T\n}\nas inputs, and outputs a new se-\nquence\n{\net |et ∈Rdmodel, t= 1,...,T\n}\nwhere each element of\nthe output sequence is impacted by the corresponding query and all\nthe keys and values, no matter how distant these keys and values are\nfrom the query in the temporal order, and thus captures the long-\nrange temporal dependency. The detailed procedure is as follows.\nFirst, the multi-head attention layer compares each query qt with\neach key kj to get their similarity α(h)\ntj from multiple perspectives\n(termed as multi-head; His the number of heads, h= 1,...,H ):\nα(h)\ntj = softmax\n(⣨\nWQ\n(h)qt, WK\n(h)kj\n⟩/√\ndmodel\nH\n)\n=\nexp\n(⣨\nWQ\n(h)qt, WK\n(h)kj\n⟩/√\ndmodel\nH\n)\nT∑\ni=1\nexp\n(⣨\nWQ\n(h)qt, WK\n(h)ki\n⟩/√\ndmodel\nH\n)\n(9)\nwhere α(h)\nij ∈(0, 1) is the similarity between qi and kj under head\nh, ∑T\nj=1 α(h)\nij = 1; WQ\n(h),WK\n(h) ∈R\ndmodel\nH ×dmodel are parameter\nmatrices for head hthat need to be learned;⟨·, ·⟩is the inner product\nbetween two vectors.\nIn our work, to balance the impact of spatial signals and auxiliary\ninformation on the prediction, we ﬁrst scaleqt and then use its scaled\nversion q′\nt instead in Equation (9) for computing the similarity α(h)\ntj .\nSuppose in qt and kt, the ﬁrst dsignal dimensions are for encoding\nspatial signals, and the next daux dimensions are for encoding auxil-\niary information, we compute q′\nt as:\nq′\nt = r ◦qt\nr =\n[ √\n1\n2 + daux\n2dsignal\n·1dsignal ,\n√\n1\n2 +\ndsignal\n2daux\n·1daux\n]T\n1d =\n[\n1 ··· 1\n]\n∈R1×d, d = dsignal or daux\n(10)\nwhere ◦is the Hadamard product.\nSecond, the multi-head attention layer uses these\nsimilarities as weights to generate a new sequence{\ne(h)\nt |e(h)\nt ∈Rdmodel, t= 1,...,T\n}\nfor each head h:\ne(h)\nt =\nT∑\nj=1\nα(h)\ntj WV\n(h)vj (11)\nwhere WV\n(h) ∈ R\ndmodel\nH ×dmodel is another parameter matrix for\nhead hthat needs to be learned.\nThird, the sequence under each head is concatenated and then used\nto generate the ﬁnal output sequence:\net = WO\n[\ne(1)\nt\nn\ne(2)\nt\nn\n···\nn\ne(H)\nt\n]\n(12)\nwhere WO ∈Rdmodel×dmodel is a parameter matrix that needs to\nbe learned; ·f·represents a concatenation of two vectors.\nIn summary, the multi-head attention layer needs to learn the pa-\nrameter matrices WQ\n(h), WK\n(h), WV\n(h), h= 1,··· ,H, and WO, which\nall can be treated as linear layers without bias. In our architecture, we\nuse sparse linear layers to replace these linear layers, capturing the\nspatial dependency between locations.\n8"
}