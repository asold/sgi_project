{
  "title": "Leveraging pre-trained language models for code generation",
  "url": "https://openalex.org/W4392293733",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2022679796",
      "name": "Ahmed Soliman",
      "affiliations": [
        "Al-Azhar University",
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A2464434837",
      "name": "Samir Shaheen",
      "affiliations": [
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A2977899394",
      "name": "Mayada Hadhoud",
      "affiliations": [
        "Zewail City of Science and Technology",
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A2022679796",
      "name": "Ahmed Soliman",
      "affiliations": [
        "Al-Azhar University",
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A2464434837",
      "name": "Samir Shaheen",
      "affiliations": [
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A2977899394",
      "name": "Mayada Hadhoud",
      "affiliations": [
        "Cairo University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2919115771",
    "https://openalex.org/W4206825193",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W3202039224",
    "https://openalex.org/W4212788259",
    "https://openalex.org/W4386570903",
    "https://openalex.org/W3202957193",
    "https://openalex.org/W2133708976",
    "https://openalex.org/W4389386718",
    "https://openalex.org/W3173485235",
    "https://openalex.org/W4200204829",
    "https://openalex.org/W4284696121",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W2963794306",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2890867094",
    "https://openalex.org/W2949215742",
    "https://openalex.org/W2997847174",
    "https://openalex.org/W3034976548",
    "https://openalex.org/W4317042421",
    "https://openalex.org/W3126095862",
    "https://openalex.org/W3174199721",
    "https://openalex.org/W3184995367",
    "https://openalex.org/W4285108092",
    "https://openalex.org/W4386576744",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2962936887",
    "https://openalex.org/W4236265809",
    "https://openalex.org/W2757361303",
    "https://openalex.org/W2964325845",
    "https://openalex.org/W2970490744",
    "https://openalex.org/W2963675284",
    "https://openalex.org/W3087300239",
    "https://openalex.org/W2060610732",
    "https://openalex.org/W4362733573",
    "https://openalex.org/W6600466347",
    "https://openalex.org/W4309700390",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W4321013654",
    "https://openalex.org/W3105388824"
  ],
  "abstract": "Abstract Code assistance refers to the utilization of various tools, techniques, and models to help developers in the process of software development. As coding tasks become increasingly complex, code assistant plays a pivotal role in enhancing developer productivity, reducing errors, and facilitating a more efficient coding workflow. This assistance can manifest in various forms, including code autocompletion, error detection and correction, code generation, documentation support, and context-aware suggestions. Language models have emerged as integral components of code assistance, offering developers the capability to receive intelligent suggestions, generate code snippets, and enhance overall coding proficiency. In this paper, we propose new hybrid models for code generation by leveraging pre-trained language models BERT, RoBERTa, ELECTRA, and LUKE with the Marian Causal Language Model. Selecting these models based on their strong performance in various natural language processing tasks. We evaluate the performance of these models on two datasets CoNaLa and DJANGO and compare them to existing state-of-the-art models. We aim to investigate the potential of pre-trained transformer language models to revolutionize code generation, offering improved precision and efficiency in navigating complex coding scenarios. Additionally, conducting error analysis and refining the generated code. Our results show that these models, when combined with the Marian Decoder, significantly improve code generation accuracy and efficiency. Notably, the RoBERTaMarian model achieved a maximum BLEU score of 35.74 and an exact match accuracy of 13.8% on CoNaLa, while LUKE-Marian attained a BLEU score of 89.34 and an exact match accuracy of 78.50% on DJANGO. Implementation of this work is available at https://github.com/AhmedSSoliman/Leveraging-Pretrained-Language-Models-for-Code-Generation .",
  "full_text": "Complex & Intelligent Systems (2024) 10:3955–3980\nhttps://doi.org/10.1007/s40747-024-01373-8\nORIGINAL ARTICLE\nLeveraging pre-trained language models for code generation\nAhmed Soliman 1,2 · Samir Shaheen 1 · Mayada Hadhoud 1,3\nReceived: 13 December 2023 / Accepted: 31 January 2024 / Published online: 29 February 2024\n© The Author(s) 2024\nAbstract\nCode assistance refers to the utilization of various tools, techniques, and models to help developers in the process of software\ndevelopment. As coding tasks become increasingly complex, code assistant plays a pivotal role in enhancing developer\nproductivity, reducing errors, and facilitating a more efﬁcient coding workﬂow. This assistance can manifest in various forms,\nincluding code autocompletion, error detection and correction, code generation, documentation support, and context-aware\nsuggestions. Language models have emerged as integral components of code assistance, offering developers the capability to\nreceive intelligent suggestions, generate code snippets, and enhance overall coding proﬁciency. In this paper, we propose new\nhybrid models for code generation by leveraging pre-trained language models BERT, RoBERTa, ELECTRA, and LUKE with\nthe Marian Causal Language Model. Selecting these models based on their strong performance in various natural language\nprocessing tasks. We evaluate the performance of these models on two datasets CoNaLa and DJANGO and compare them to\nexisting state-of-the-art models. We aim to investigate the potential of pre-trained transformer language models to revolutionize\ncode generation, offering improved precision and efﬁciency in navigating complex coding scenarios. Additionally, conducting\nerror analysis and reﬁning the generated code. Our results show that these models, when combined with the Marian Decoder,\nsigniﬁcantly improve code generation accuracy and efﬁciency. Notably, the RoBERTaMarian model achieved a maximum\nBLEU score of 35.74 and an exact match accuracy of 13.8% on CoNaLa, while LUKE-Marian attained a BLEU score of\n89.34 and an exact match accuracy of 78.50% on DJANGO. Implementation of this work is available at https://github.com/\nAhmedSSoliman/Leveraging-Pretrained-Language-Models-for-Code-Generation .\nKeywords Code generation · Code assistant · Language models · Marian model · RoBERTaMarian· LukeMarian\nAbbreviations\nRNN Recurrent neural networks\nSeq2Seq Sequence to sequence\nASTs Abstract syntax trees\nBERT Pre-training of deep bidirectional transformers\nfor language understanding\nB Ahmed Soliman\nahmed.shokry@engl.cu.edu.eg\nSamir Shaheen\nsshaheen@eng.cu.edu.eg\nMayada Hadhoud\nmayada.hadhoud@eng.cu.edu.eg\n1 Department of Computer Engineering, Cairo University,\nGiza, Egypt\n2 Department of Computer Engineering, Al-Azhar University,\nNasr City, Egypt\n3 School of Computational Sciences and Artiﬁcial Intelligence\n(CSAI), Zewail City of Science and Technology, 6th of\nOctober City, Giza 12578, Egypt\nRoBERTa A robustly optimized BERT pretraining approach\nGAN Generative adversarial networks\nMLM Masked language modeling\nLUKE Language understanding with knowledge-based\nembeddings\nAI Artiﬁcial intelligence\nIntroduction\nDeep Learning [ 1] is the most interesting ﬁeld in Artiﬁcial\nIntelligence that is inspired by biologic neural networks to\nrecognize patterns in the real world. These neural networks\nhave a remarkable ability to process and learn from massive\nvolumes of data. It is used in the image, video, text, and\nvoice analysis. The pre-trained language model is a neural\nnetwork that has been trained on unlabelled text data, such as\nin an unsupervised, task-agnostic way, to convert a sequence\nof input words into a context-dependent embedding. On the\n123\n3956 Complex & Intelligent Systems (2024) 10:3955–3980\nother hand, there is the notion of ﬁne-tuning the pre-trained\nmodel, which is the training of a previously initialized pre-\ntrained model using the weights of this pre-trained language\nmodel as the beginning weights during the upcoming training\nof other models.\nRecurrent neural networks (RNN) were proposed as the\nfoundation for the ﬁrst pre-trained language models [ 2]. Also,\nit is proved that pre-training RNN-based model on unlabeled\ndata and then ﬁne-tuning it on a speciﬁc task delivers better\nresults than training a randomly initialized model on such a\ntask directly. Two limitations exist with RNN-based sequence\nmodels. First, the processing manner of the tokens is in a\nsequential way which must process token after token, so\nRNNs don’t remember the non-sequential tokens perfectly.\nSecond, RNN-based models may fail to capture long-term\nrelationships between code tokens.\nPre-trained language models are neural networks designed\nfor various NLP tasks, utilizing a pre-train ﬁne-tuning\napproach. They are trained on vast text corpora and then\nﬁne-tuned for speciﬁc downstream tasks [ 3]. These models\nhave revolutionized NLP by providing accurate and efﬁcient\ntext representation.\nThe key advantage of using pre-trained models for code\ngeneration lies in their ability to generalize across various\ncodebases and programming languages. Instead of starting\nfrom scratch, pre-trained models already possess a substan-\ntial knowledge base, enabling them to better comprehend and\ngenerate code in a wide range of programming paradigms.\nThis generalization is crucial in dealing with multilingual\ncode generation tasks, as these models can seamlessly switch\nbetween different languages without the need for extensive\nlanguage-speciﬁc training.\nAnother signiﬁcant beneﬁt is the reduction in training time\nand resource requirements. Pre-trained models have been\nﬁne-tuned on vast corpora, allowing researchers and devel-\nopers to transfer this knowledge to speciﬁc code generation\ntasks with relatively small amounts of domain-speciﬁc data.\nFine-tuning a pre-trained model requires less computational\npower and data compared to training from scratch, which\nmakes it more accessible to the broader community.\nFurthermore, pre-trained models proved their capabili-\nties in capturing contextual dependencies and understanding\nthe surrounding context when generating code. This context\nawareness helps in producing more coherent and semanti-\ncally meaningful code snippets. Code generation tasks often\ninvolve complex syntactic structures, which pre-trained mod-\nels can effectively handle by learning intricate patterns and\ndependencies in code expressions.\nTransformers [ 4], a neural network architecture, play a\ncrucial role in pre-trained language models, allowing them to\nprocess input sequences of any length and handle long-range\ndependencies. The self-attention mechanism in transformers\nenables the model to weight the signiﬁcance of differ-\nent words or phrases, improving performance on various\nNLP tasks. Transformers, such as BERT, RoBERTa, and\nXLNet, have shown remarkable results in NLP tasks. ELMO\n[5] and ULMFit [ 6] are pre-trained language models that\nhave demonstrated improvement on several natural language\nunderstanding tasks. Pre-training transformers work in a\nself-supervised manner and achieve great success when ﬁne-\ntuned on downstream tasks like machine translation, question\nanswering, and text summarization.\nPre-trained language models, empowered by transform-\ners, have transformed the ﬁeld of NLP by offering effective\nand efﬁcient text representation. Their ability to be ﬁne-tuned\nfor speciﬁc tasks has made them a go-to solution for many\nNLP applications.\nSeveral leading companies, including OpenAI, Microsoft,\nGoogle, and HuggingFace, have introduced transformer-\nbased pre-trained language models featuring different archi-\ntectures through the models. Transformer models gener-\nally fall into three categories. The ﬁrst category com-\nprises encoder-only transformers, exempliﬁed by BERT,\nRoBERTa, ELECTRA, and Luke. The second type is repre-\nsented by decoder-only transformers, such as GPT-2, Llama,\nFalcon, and ChatGPT. The third category includes encoder–\ndecoder transformer models, like MarianMT, T5, and BART.\nIn encoder–decoder models, the encoder processes input\nsequences, utilizing embedding and attention mechanisms\nto construct an intermediate representation. Subsequently,\nthe decoder utilizes this representation to generate an out-\nput sequence. Notably, models like BERT and GPT have\ndemonstrated exceptional performance with minimal ﬁne-\ntuning, achieving state-of-the-art results across numerous\nnatural language understanding (NLU) tasks.\nTransformer-based language models, such as GPT-2,\nBERT, RoBERTa, and others, have surpassed the efﬁciency\nof traditional recurrent neural networks (RNNs). Lever-\naging this efﬁciency, these models can be pre-trained on\nextensive volumes of unlabeled text data. The massive pre-\ntrained encoder–decoder models have proven to signiﬁcantly\nenhance performance in various sequence-to-sequence appli-\ncations. Massive pre-trained encoder–decoder models have\nbeen proven to greatly improve performance on a range of\nsequence-to-sequence applications [ 7, 8]. Rothe et al. [ 9]\nproposed their work to avoid costly pre-training by construct-\ning the encoder–decoder model using pre-trained encoder\nand/or decoder-only checkpoints (e.g., BERT, GPT-2). This\nis referred to as leveraging pre-trained checkpoints.\nThe proposed process and pipelines for the AI code assis-\ntant system are shown in Fig. 1 and involve three distinct\nphases and pipelines that form a comprehensive approach\nto the code assistant system, encompassing code generation\nfrom natural language input to the corresponding code as out-\nput. Then linting and code analysis using Flake8, and ﬁnally\nthe error correction and reﬁning the generated code.\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3957\nFig. 1 Pipelines for the proposed code assistant system\nA signiﬁcant milestone in this paper is the evolution and\nemergence of the transformer language models, which have\ndemonstrated exceptional capabilities in capturing intricate\nlanguage nuances. These models, such as DistilRoBERTa,\nDistilBERT, ELECTRA, and LUKE, have found success\nin various natural language processing tasks. Harnessing\nthe power of pre-trained transformer language models in\ncombination with the Marian Decoder for code generation\nrepresents a novel approach to further enhance precision and\nefﬁciency in navigating intricate coding scenarios. We pro-\nposed and implemented new state-of-the-art models in the\ncode generation problem which employ the idea of lever-\naging pre-trained language models for sequence generation\ntasks to get more accurate results. Thanks to transformer\nmodels with multi-functions in the NLP ﬁeld such as machine\ntranslation, sentiment analysis, classiﬁcation, and other tasks.\nOur implementation depends on the transformer pre-trained\ncheckpoints for the encoder and Marian Decoder from Mar-\nian Neural Machine Translation model. All proposed models\nin our experiments are stack of six layers in the encoder and\nstack of six layers in the decode architecture.\nThis paper makes signiﬁcant contributions to the code gen-\neration problem through the following key ﬁndings:\n1. Proposing new hybrid models for code generation by\nleveraging pre-trained language models such as BERT,\nRoBERTa, and Marian. We propose novel hybrid models\nwith small sizes in the encoder and decoder to address the\ncode generation problem. The proposed models achieve\nhigh accuracy in the code generation task on the CoNaLa\nand DJANGO datasets.\n2. Highlighting the importance of pre-trained language mod-\nels. Our research emphasizes the necessity of pre-trained\nencoders in sequence formation tasks. Furthermore, we\ndemonstrate that weight sharing between the encoder and\nthe decoder is often beneﬁcial.\n3. Conducting Error Analysis and Reﬁning Generated Code.\nWe conduct a comprehensive error analysis of the gener-\nated code and reﬁne it to adhere to Python code standards.\nThis process ensures the produced code meets the require-\nments of quality and consistency.\n4. Ensuring code compliance and code reﬁnement with cod-\ning standards and best practices becomes imperative by\n123\n3958 Complex & Intelligent Systems (2024) 10:3955–3980\nusing techniques for error analysis, syntax correction, and\nenhancing the reliability and maintainability of the gen-\nerated code.\n5. Leveraging pre-trained language models such as Dis-\ntilRoBERTa with Marian Machine Translation Decoder\nyields a BLEU score of 35.74 and a ROUGE score of\n44.25 in the code generation task.\n6. Exploration of hybrid models, such as seq2seq or encoder–\ndecoder models, as potential solutions for code assistance.\nThese models, combining a language model encoder and\ndecoder, offer insights into enhancing the overall code\ngeneration process.\nThe rest of the paper is organized as follows: section two\ndelves into related work, thoroughly examining the litera-\nture on code generation challenges and distinguishing the\nproposed approach from existing methods. Section three\npresents the proposed hybrid models, detailing their architec-\ntures and design principles. Section four demonstrates error\ndetection and reﬁning the generated code using Flake8 and\nPython tools. Section ﬁve introduces datasets and the experi-\nmental setup, clarifying the selection process and parameters.\nSection six showcases comprehensive experimental results,\nproviding insights into the proposed models’ performance.\nAlso, this section analyzes the nature and frequency of errors\nand warnings in generated code and conducts a thorough\nevaluation, comparing the proposed models against state-\nof-the-art approaches. Finally, section seven concludes by\nsummarizing key ﬁndings, emphasizing the proposed mod-\nels’ impact, and suggesting avenues for future research.\nRelated work\nIn recent years, the advent of machine learning-based meth-\nods, particularly neural Seq2Seq models with attention\nmechanisms, has shown promising results in generating\ncode from natural language descriptions and pseudocode\n[10, 11]. These approaches have the potential to bridge\nthe gap between human-readable descriptions and machine-\nexecutable code, although they still face challenges in\nhandling variable-length code and ensuring syntactic and\nsemantic correctness [ 12]. Technology-aided learning has\nsigniﬁcant contributions to the understanding of technology-\nassisted language learning adaptive systems, offering valu-\nable insights for researchers, educators, and policy-makers\nin the ﬁeld [ 13]. Also, Plug-ins enhance the program, add\nassistance to the programmer and add more advancement\nand software capabilities to prevent the need of the starting\nfrom the scratch when developing [ 14].\nFurthermore, research has expanded to cater to speciﬁc\napplication domains, such as domain-speciﬁc language code\ngeneration, model-driven engineering, code-to-code trans-\nlation, and performance-driven code generation. Each of\nthese domains comes with its unique set of requirements\nand constraints, necessitating tailored solutions [ 15]. The\ngrowing availability of large-scale code datasets has also\ncontributed signiﬁcantly to advancements in code generation.\nResearchers have used various datasets, ranging from Python\ncode repositories to multilingual translation benchmarks,\nto train and evaluate their models [ 16]. Human evaluation\nremains a crucial component in assessing the quality and\ncorrectness of generated code, complementing traditional\nautomated evaluation metrics [ 17].\nPrevious studies demonstrate the continuous evolution\nand innovation in code generation techniques, driven by\nthe integration of machine learning, domain-speciﬁc knowl-\nedge, and large-scale datasets. However, challenges related\nto code quality, scalability, and adaptability to multiple lan-\nguages and programming paradigms persist. There are some\ndifﬁculties in this problem because the output has a well-\ndeﬁned structure and the domain, structure of the input, and\nthe output are not similar. There are various techniques used\nin code generation, including tree-based and deep learning-\nbased approaches. Also, semantic parsing-based techniques\nwere used in this task. Tree-based techniques involve the use\nof syntax trees to generate code, while deep learning tech-\nniques use neural networks to learn the mapping between\nnatural language descriptions and source code.\nTree-based techniques\nTree-based techniques in semantic parsing are task-driven\nmethods that convert natural language input into a formal,\nmachine-executable representation. These techniques often\nrepresent code as Abstract Syntax Trees (ASTs), serving\nas a syntactic tree representation capturing the structure of\nexpressions and the program’s control components. Demon-\nstrating effectiveness in code generation for speciﬁc domains\nover several decades [ 18], the goal of ASTs is to describe\nthe semantic structure of sentences in a computer language\nas trees. Semantic parsers, categorized into shallow and\ndeep semantic parsing, map natural language utterances into\nsemantic representations, such as logical forms or mean-\ning representations [ 19]. The use of tree-based methods in\nsemantic parsing offers advantages, including the ability to\naddress code generation challenges, enhance accuracy, and\nhandle various data types. However, challenges arise in rep-\nresenting code as ASTs, particularly in managing extensive\nnode counts and synchronicity issues in the generation pro-\ncess [ 20].\nResearchers employ sequence-to-tree models for code\ngeneration, where the tree represents the AST of the tar-\nget source code [ 18, 21–29]. These models aim to improve\nthe code snippet creation process using ASTs. The use of\ntree-based methods presents multiple advantages, addressing\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3959\ncode generation challenges by converting Natural Language\ninput into a corresponding AST. Tree-based approaches offer\nvisual intuitiveness, making complex predictive models more\ncomprehensible. However, challenges include difﬁculties in\nconsistently generating accurate code based on less common\ntraining data and synchronous deviations in output structure\n[20, 30].\nYin and Neubig [23] proposed a syntax-driven neural code\ngeneration technique, constructing an abstract syntax tree\nthrough actions from a probabilistic grammar model. Rabi-\nnovich et al. [ 24] introduced Abstract Syntax Networks for\ncode generation and semantic parsing, utilizing datasets like\nJOBS, GEO, and A TIS. In 2018, Yin and Neubig presented\nTRANX [25], parsing utterances into formal meaning repre-\nsentations using a transition system and probabilistic models.\nHowever, TRANX exhibited incoherence issues in genera-\ntion, as evidenced by a BLEU score of 24.30 with the CoNaLa\ndataset.\nDeep learning-based techniques\nThe generation of source code falls into the categories of\ntext-to-text or sequence-to-sequence, achievable through the\napplication of Deep Learning models for both development\nand maintenance. The integration of machine intelligence\ncapable of comprehending and constructing intricate soft-\nware structures holds signiﬁcant potential within Software\nEngineering [31]. Deep learning, a subset of artiﬁcial intelli-\ngence, offers a promising solution to alleviate the challenges\nassociated with manual code creation, demonstrating success\nin various domains [ 29, 32, 33].\nTransfer Learning has proven effective in ﬁne-tuning\npre-trained models for new tasks. By adapting pre-trained\nmodels to speciﬁc jobs, consistent outcomes and ﬁndings are\nachieved in the seq2seq code generation task [ 34–37]. Deep\nlearning techniques exhibit promise in generating code from\nnatural language descriptions, offering beneﬁts for complex\ndomains while requiring less manual effort compared to tree-\nbased techniques [ 38].\nV arious researchers have worked on code generation tasks\nusing datasets such as CoNaLa, DJANGO, A TIS, Code-\nSearchNet, and others. Notable models and methods include\nDong and Lapata’s syntax-driven neural code generation\n[22], Yin and Neubig’s reranking model [ 26], Shin et al.’s\nPATOIS [27], Sun et al.’s TreeGen [ 28], and Xu et al.’s deep\nlearning model with external knowledge incorporation [ 29].\nIn 2021, Dahal et al. conducted an analysis of tree-\nstructured architectures, evaluating text-to-tree, structured\ntree-to-tree, and linearized tree-to-tree models on constituency-\nbased parse trees [ 18]. Constrained decoding of language\nmodels by Shin et al. [ 21] demonstrated the paraphrasing of\nuser utterances into a regulated sublanguage for enhanced\nsemantic parsing.\nRecent contributions by Norouzi et al. [ 32] showcased\ntransformer-based seq2seq models competing with or out-\nperforming models speciﬁcally designed for code generation.\nBeau and Crabbé [ 34] proposed an encoder–decoder model\nusing BERT as an encoder and a grammar-based decoder,\nachieving a BLEU score of 34.2 on the CoNaLa dataset.\nSemantic parsing based techniques\nSemantic parsing, a subset of natural language process-\ning, involves translating natural language utterances into\nmachine-understandable logical forms. The overarching aim\nis to extract precise meaning, enabling machines to com-\nprehend and execute these utterances. Its applications span\nvarious domains, including machine translation, question\nanswering, ontology induction, automated reasoning, and\ncode generation [ 39].\nThe Context and V ariable Copying method in neural\nsemantic parsing for code generation integrates contex-\ntual information to enhance disambiguation. Utilizing an\nencoder–decoder system, this approach leverages program\ncontext during decoding. A two-step attention mechanism\naligns words in the language with environment identiﬁers,\nand a supervised copy mechanism replicates environment\ntokens, even if unseen during training [ 40]. Iyer et al. [ 41]\nintroduced an architecture-enhancing contextually relevant\ncode generation by incorporating programmatic context. An\nencoder–decoder system processes input utterances and envi-\nronment identiﬁers, employing a two-step attention mecha-\nnism for effective word-identiﬁer association.\nThe decoding process of neural semantic parsing mod-\nels is signiﬁcantly shaped by language structure. Techniques\nlike sequence-based knowledge base query generation [ 42]\nand enforcing type constraints in query generation [ 43] high-\nlight the importance of grammatical constraints. Ling et\nal. [ 44] presented a sequence-to-sequence code generation\napproach, incorporating generative and pointer models for\nkeyword copying from input. Yin and Neubig proposed\nleveraging abstract syntax trees (ASTs) for coherence in\ngeneral-purpose programming languages [ 23]. Also, Iyer\net al. [ 45] introduced idiom-based decoding to streamline\ngrammar-constrained semantic parsing systems. Shin et al.\n[27] mined code idioms to support high-level and low-level\nreasoning.\nAnother way in the semantic parsing techniques can be\nSketching using Pattern Recognition and Symbolic Rea-\nsoning Sketching in program synthesis involves articulating\nhigh-level descriptions through incomplete programs or\nsketches. Nye et al. [ 46] introduced Sketching systems with\na sketch generator and a program synthesizer. Shin et al. [ 27]\nin the same year incorporated mined code idioms into the\ngrammar for uniﬁed high-level and low-level reasoning.\n123\n3960 Complex & Intelligent Systems (2024) 10:3955–3980\nThrough Conversational Semantic Parsing, Dong et al.\n[47] introduced a method to enhance the interpretability\nof neural semantic parsers. Their approach focused on a\nconﬁdence modeling framework, which systematically char-\nacterizes different forms of uncertainty, including model,\ndata, and input uncertainties. By integrating these conﬁ-\ndence metrics as features in a regression model, a conﬁdence\nscore is generated. This score serves as a valuable tool for\nidentifying the sources of uncertainty in predictions, thereby\naugmenting the model’s overall comprehension. Importantly,\nthis framework lays the groundwork for conversational pro-\ngramming strategies, allowing models to engage users in\nclarifying discussions when confronted with uncertain or\nmissing information.\nAnother work from researchers [ 48, 49] delved into the\napplication of conversational AI within the domain of natural\nlanguage semantic parsing. This technique proves partic-\nularly advantageous in rectifying incomplete or inaccurate\nuser requests during the parsing process. Through establish-\ning an interactive dialog between the program and the user,\nconversational AI plays a pivotal role in addressing gaps and\nerrors in logical forms. The iterative exchanges not only ﬁll\nin missing details but also correct inaccuracies. Moreover,\nthese conversational interactions contribute signiﬁcantly to\nnarrowing down the search space, resulting in more precise\nand reliable output predictions. Polozov et al. [ 50] introduced\nthe FlashMeta framework, a neural network-independent\napproach for program synthesis. Parisotto et al. presented\nNeuro-symbolic program synthesis in 2016 [ 51]. DAPIP ,\na system for Programming-By-Example, was introduced in\n2017 [ 52]. DeepCoder and RobustFill were proposed in the\nsame year [ 53, 54].\nFurthermore, programming using Reinforcement Learn-\ning. Xu et al. [ 55] introduced Auto Assembler, leveraging\nreinforcement learning for autonomous generation of assem-\nbly instructions. Finally, programming from Description\nusing Rich Domain-Speciﬁc Languages. Semantic parsing\nmethods like Neural Program Search [ 56] and Language to\nLogical Form with Neural Attention [ 22] align with pro-\ngram synthesis from descriptions, mapping natural language\nto predeﬁned program structures [ 57].\nThis literature survey highlights the diverse techniques\nemployed in code generation, ranging from classical approaches\nto advanced deep learning models. While classical meth-\nods lay the foundation, modern deep learning approaches\ndemonstrate adaptability and effectiveness in addressing the\ncomplexities of code generation.\nProposed hybrid models\nLeveraging pre-trained models for sequence generation tasks\n[9], particularly in the context of code generation, has become\na transformative approach in recent years. Pre-trained mod-\nels are language models that have been extensively trained on\nlarge and diverse datasets to learn contextual representations\nof language. These models, such as GPT-3 [ 58], RoBERTa\n[59], and BERT [ 60], capture intricate patterns and relation-\nships in text data, making them adept at understanding the\nnuances of programming languages and code syntax.\nUsing pre-trained models for code generation does come\nwith a set of challenges. One signiﬁcant concern is the safety\nand reliability of generated code. Pre-trained models can\nsometimes produce incorrect, insecure, or inefﬁcient code,\nwhich poses risks in real-world applications. Careful consid-\neration of validation and veriﬁcation techniques is essential\nto ensure the safety and correctness of generated code.\nAnother challenge is the potential exposure of sensitive\ncode or intellectual property. Pre-trained models may inad-\nvertently memorize or expose conﬁdential code snippets\npresent in their training data, which can lead to privacy and\nsecurity issues. Researchers and practitioners must employ\nrobust techniques to prevent such data leakage. So, lever-\naging pre-trained models for code generation tasks offers a\npowerful and efﬁcient approach to address the complexities\nof the task. Also, these models continue in many applications.\nThe ability of pre-trained models to generalize across\nlanguages, context awareness, and reduced training require-\nments make them an invaluable resource for accelerating\nprogress in the ﬁeld of automated code generation. However,\nresponsible use, safety considerations, and privacy protec-\ntion measures are critical to fully harness the potential of\npre-trained models for code generation and ensure their\nsuccessful integration into real-world software development\nworkﬂows.\nWe leveraged pre-trained models to obtain various code\ngeneration models with different pre-trained encoders mod-\nels combined and integrated with Marian decoder as shown in\nFig. 2. Our proposed models are indicated with their encoders\nand decoders in Table 1, and Marian Decoder can be indicated\nfrom MarianCG paper [ 61].\nRoBERTaMarian model\nIn 2019, RoBERTa model was introduced by Facebook and\nit is based on Google BERT pre-trained model that was pro-\nposed before it in 2019. It was built on the BERT model\nby eliminating the next-sentence pretraining aim and train-\ning with considerably bigger mini-batches and learning rates.\nThis model is identical to BERT Model, except for a minor\nembedding adjustment and a setup for RoBERTa pre-trained\nmodels.\nRoBERTa is built on the same architecture as BERT, but\nit uses a byte-level BPE as a tokenizer (as does GPT-2) and a\ndifferent pretraining strategy. Token type ids do not exist in\nthe RoBERTa model. The architecture of the RoBERTa-base\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3961\nFig. 2 Leveraging pre-trained\nlanguage models for building\ncode generation models\nTable 1 Our proposed code\ngeneration models No. Model Model architecture\nEncoder model (encoder) Decoder model (decoder)\n1 RoBERTaMarian DistilRoBERTa Marian decoder\n2 BERTMarian DistilBERT Marian decoder\n3 ELECTAMarian ELECTRA Marian decoder\n4 LUKEMarian LUKE Marian decoder\nmodel has 12 layers. RoBERTa is pre-trained with the MLM\ntask (and without the NSP task).\nWe used the distilled version of the RoBERTa-base model\nwhich is called DistilRoBERTa and it has the training pro-\ncedure as DistilBERT [ 62]. The pre-trained DistilRoBERTa\nmodel has 6 layers, 768 dimensions, and 12 heads, with a total\nof 82 million parameters (compared to 125 million param-\neters for RoBERTa-base). DistilRoBERTa is twice as fast\nas the Roberta-base model. DistilRoBERTa model distin-\nguishes between English and english. It is a case-sensitive\nmodel. We combined DistilRoBERTa as an encoder with\nMarian Decoder as shown in Fig. 3.\nFigure 4 shows the architecture of the RoBERTaMarian\nmodel and the encoder architecture is declared and shown in\nFig. 5.\nRoBERTa pooler layer\nThe term “Roberta Pooler” pertains to the specialized layer\nwithin the RoBERTa model known as the pooler layer.\nRoBERTa represents a reﬁnement of the BERT (Bidirec-\ntional Encoder Representations from Transformers) model,\na widely used framework for tasks involving Transformer-\nBased Processing. Within the RoBERTa architecture, the\npooler layer holds the responsibility of condensing the infor-\nmation amassed from the encoder layers into a consistent\nand predetermined representation. This resultant representa-\ntion proves valuable for subsequent tasks like classiﬁcation\nor text generation. This is achieved by processing the hidden\nstates of the ﬁnal layer as input and subsequently applying\na pooling operation, often involving mean or max pooling\ntechniques, to derive a singular vector representation. This\nvector is subsequently suitable for utilization within a classi-\nﬁer or decoder layer to facilitate predictions or text creation.\nThe pooler layer within RoBERTa constitutes a fully con-\nnected layer equipped with learnable weights and biases.\nThese parameters are optimized during the model’s training\nphase to effectively capture the salient details from the input\nsequence and generate a purposeful representation aptly serv-\ning the task at hand. Typically, access to the pooler layer is\nfacilitated through the ‘pooler_output’ attribute embedded\nwithin the RoBERTa model’s output. This attribute holds\nthe outcome of the pooler layer’s operations, manifesting\nas a tensor characterized by dimensions (batch_size, hid-\nden_size). In this context, ‘batch_size’ denotes the number\nof input sequences within a batch, while ‘hidden_size’ signi-\nﬁes the dimensional extent of both the encoder layers and the\npooler layer. Notably, the incorporation of the pooler layer\nwithin RoBERTa is not obligatory and certain variations or\nimplementations might choose to omit it. Nevertheless, in the\nmajority of scenarios, the pooler layer is embraced due to its\npractical utility in generating a standardized representation\nof the input sequence\nBERTMarian model\nDistilBERT is a student version BERT as shown in Fig. 6.I ti s\nsmaller and quicker than BERT. The DistilBERT transformer\n123\n3962 Complex & Intelligent Systems (2024) 10:3955–3980\nFig. 3 RoBERTaMarian code generation model\nFig. 4 RoBERTaMarian model architecture\nmodel was created using the same basic design as BERT.\nThe pooler and token-type embeddings are removed, and the\nnumber of layers is reduced by a factor of two, resulting in\na smaller encoder and decoder with six layers. DistilBERT\nproved that variations in the tensor’s last dimension (hidden\nsize dimension) have a smaller impact on computation efﬁ-\nciency (for a ﬁxed parameters budget) than variations in other\nfactors such as the number of layers.\nAs a result, DistilBERT is meant to focus on reducing\nthe number of layers. It is self-supervised pre-trained on the\nsame corpus as a teacher using the BERT base model. It used\nan automatic procedure to produce inputs and labels from\nthose texts using the BERT base model. DistilBERT has 40%\nfewer parameters than BERT-base-uncased DistilBERT has\n40% less parameters than BERT-base-uncased. It runs 60%\nfaster while maintaining over 95% of BERT’s performance\non the GLUE language comprehension test benchmark.\nWe built a code generation model as shown in Fig. 7 that\ncontains DistilBERT as an encoder and Marian Decoder. Fig-\nure 8 shows the architecture of BERTMarian model and the\nencoder architecture is declared and shown in Fig. 9\nELECTRAMarian model\nIn 2020, ELECTRA [ 63] model was introduced by the Stan-\nford University in collaboration with Google, and it is a novel\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3963\nFig. 5 RoBERTaMarian encoder\nFig. 6 DistilBERT model architecture [ 62]\n123\n3964 Complex & Intelligent Systems (2024) 10:3955–3980\nFig. 7 BERTMarian code generation model\nFig. 8 BERTMarian model architecture\napproach for learning self-supervised language representa-\ntions. It is used to pre-train transformer networks with a small\namount of computation. ELECTRA models, like GAN dis-\ncriminators, are trained to identify “real” input tokens from\n“false” input tokens created by another neural network as\nshown in Fig. 10. ELECTRA provides impressive results at\na modest scale. On the SQuAD 2.0 dataset, ELECTRA pro-\nduces cutting-edge outcomes at big scale.\nThe ELECTRA pre-training approach involves training\ntwo neural networks, a generator G and a discriminator D,\neach consisting of an encoder that maps input tokens into con-\ntextualized vector representations. The generator is trained\nto perform masked language modeling (MLM), where it pre-\ndicts the original identities of masked-out tokens, while the\ndiscriminator is trained to distinguish between tokens in the\ndata and tokens replaced by generator samples. After pre-\ntraining, the generator is discarded, and the discriminator is\nﬁne-tuned on downstream tasks.\nThis pre-training model is more efﬁcient than Masked\nLanguage Modeling (MLM) because it was deﬁned across\nall input tokens rather than just the tiny selection that was\nmasked away. With the same model size, data, and computa-\ntion, the contextual representations acquired by ELECTRA\nmodel outperformed those trained by BERT. The improve-\nments are especially signiﬁcant for tiny models. We used\ngoogle electra-base-discriminator which has 6 hidden layers\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3965\nFig. 9 BERTMarian encoder\nFig. 10 ELECTRA model like\nGAN [ 63]\nas shown in Fig. 11. This model as an ELECTRA encoder\nwith Marian Decoder.\nFigure 12 shows the architecture of the ELECTRAMarian\nmodel and the encoder architecture is declared and shown in\nFig. 13.\nLUKEMarian model\nLUKE [ 64] is a transformer model that was trained using\na vast quantity of entity-annotated data acquired from\nWikipedia. It treats not just words but also entities as separate\ntokens and uses the transformer to construct intermediate and\noutput representations for all tokens. LUKE varies from pre-\nviously contextualized word representations (CWRs) and can\ndirectly simulate entity relationships since entities are repre-\nsented as tokens as shown in Fig. 14. LUKE is trained with a\nnovel pretraining task that is a simple extension of BERT’s\nmasked language model (MLM). The job entails masking\nentities at random by substituting them with [MASK] ones\nand training the model by predicting the originals of these\n123\n3966 Complex & Intelligent Systems (2024) 10:3955–3980\nFig. 11 ELECTRAMarian code generation model\nFig. 12 ELECTRAMarian model architecture\nmasked entities. RoBERTa was employed as the basic pre-\ntrained model and pre-train the model by simultaneously\nmaximizing the MLM objectives.\nAs shown in Fig. 15 we combined LUKE model with Mar-\nian decoder to construct our ﬁnal Seq2Seq hybrid model. This\nidea has some advantages for representing words by their val-\nues and their entities.\nLUKE is a contextualized representation created primarily\nfor entity-related activities. Using a vast quantity of entity-\nannotated corpus acquired from Wikipedia, LUKE is trained\nto predict randomly masked words and entities. LUKE rep-\nresents the word and its entity. The LUKE base model has 12\nhidden layers and the hidden size equals 768. It has 253 M\ntotal number of parameters. When LUKE is applied to the\ndownstream tasks, it computes representations of arbitrary\nentities in the text by employing [MASK] entities as inputs.\nIf the task includes entity annotation, the model generates\nentity representations based on the rich entity-centric infor-\nmation stored in the relevant entity embeddings. We used\nthe LUKE-base model as an encoder but only 6 layers from\nit. Combining this 6-layer LUKE-base model with Marian\nDecoder generated the LUKEMarian. Figure 16 shows the\narchitecture of the LUKEMarian model and the encoder\narchitecture is declared and shown in Fig. 17.\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3967\nFig. 13 ELECTRAMarian encoder\nFig. 14 LUKE outputs are contextualized representations for each word and entity [ 64]\n123\n3968 Complex & Intelligent Systems (2024) 10:3955–3980\nFig. 15 LUKEMarian code generation model\nFig. 16 LUKEMarian model architecture\nLUKE pooler layer\nThe pooler layer in the LUKE model (LUKE stands for \"Lan-\nguage Understanding with Knowledge-based Embeddings\")\nserves as a summarization layer that takes the contextu-\nalized representations of the input tokens and produces a\nﬁxed-length representation, often referred to as a pooled rep-\nresentation or a sentence-level representation.\nThe speciﬁc functionality of the pooler layer in LUKE\nincludes the following:\n Aggregating token representations: The pooler layer\ntakes the contextualized representations of the input\ntokens, typically obtained from the transformer encoder\nlayers, and aggregates them into a single representation.\nThis aggregation process summarizes the information\nfrom the individual tokens and captures the overall mean-\ning of the input sequence.\n Sentence-level encoding: The pooled representation pro-\nduced by the pooler layer encapsulates the contextual\ninformation from the entire input sequence. This repre-\nsentation is often used as a sentence-level encoding that\ncan be fed into downstream tasks, such as entity recog-\nnition or relation extraction.\n Semantic extraction: The pooler layer helps extract high-\nlevel semantic information from the input tokens. By\nsummarizing the token representations, it focuses on cap-\nturing the most salient features of the input sequence and\ndiscards less relevant or noisy information.\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3969\nFig. 17 LUKEMarian encoder\nOverall, the pooler layer in LUKE plays a crucial role\nin producing a ﬁxed-length representation that captures the\ncontextual information and semantic meaning of the input\nsequence. This pooled representation can then be used for\nvarious downstream tasks that require sentence-level under-\nstanding or knowledge extraction.\nThe proposed models share a foundational connection in\ntheir reliance on pre-trained language models for sequence\ngeneration tasks. DistilBERT, a streamlined version of BERT,\nserves as a rapid and efﬁcient encoder–decoder model. The\nreduction in layers and removal of certain components make\nit computationally efﬁcient while maintaining performance.\nIts self-supervised pre-training utilizes the BERT base model,\nemphasizing efﬁciency gains without sacriﬁcing effective-\nness.\nRoBERTa, an evolution from BERT by Facebook, demon-\nstrates a further enhancement in training methodology.\nEliminating next-sentence pretraining, utilizing larger mini-\nbatches and learning rates, and training on longer sequences\ncontribute to improved performance. DistilRoBERTa, a dis-\ntilled version, maintains efﬁciency with six layers, 768\ndimensions, and a case-sensitive approach, using a smaller\ndataset than its teacher model.\nELECTRA, an innovative model from Stanford and\nGoogle, introduces a novel approach to self-supervised learn-\ning. Its efﬁciency surpasses Masked Language Modeling\n(MLM), as it identiﬁes \"real\" input tokens from \"false\" ones\ncreated by another neural network. Its pre-training involves a\ndiscriminator distinguishing between genuine and generated\ntokens, showcasing effectiveness on large-scale datasets like\nSQuAD 2.0.\nLUKE, based on the RoBERTa model, takes a unique\napproach by treating entities as separate tokens. Its pre-\ntraining involves masking entities and predicting originals,\nproviding contextualized representations for both words and\nentities. LUKE’s 12 hidden layers and 768 hidden size con-\ntribute to a comprehensive model for entity-related tasks.\nThese models share the use of pre-trained language\nmodels, each introducing speciﬁc optimizations and train-\ning strategies. While DistilBERT emphasizes efﬁciency,\nRoBERTa focuses on enhanced training, ELECTRA intro-\nduces a discriminator approach, and LUKE extends BERT’s\nmasked language model to entities. Together, they repre-\nsent a spectrum of approaches, offering a versatile toolkit\nfor sequence generation tasks.\n123\n3970 Complex & Intelligent Systems (2024) 10:3955–3980\nError analysis and correction\nCode assistance is a rapidly advancing ﬁeld that combines\nthe power of artiﬁcial intelligence (AI) and machine learn-\ning with the expertise of human programmers to enhance\nand streamline the process of software development. This\ntechnology leverages intelligent algorithms and data-driven\nmodels to assist developers in writing code, improving code\nquality, and increasing overall productivity. Traditional soft-\nware development requires signiﬁcant manual effort to write\nand debug code, often resulting in time-consuming and error-\nprone processes.\nCode assistance aims to alleviate these challenges by\nproviding automated tools that analyze existing codebases,\nunderstand programming patterns, and generate sugges-\ntions or complete sections of code. The underlying concept\nbehind AI-assisted coding involves training machine learn-\ning models on large amounts of code repositories to learn\npatterns, syntax, and best practices. These models can then\nassist developers by providing code completion suggestions,\nidentifying potential bugs or vulnerabilities, and offering\noptimizations based on established coding standards.\nOne of the key advantages of code assistance is its ability\nto enhance developer productivity. By automating repetitive\ntasks and providing intelligent suggestions, developers can\nfocus on higher-level design decisions and problem-solving,\naccelerating the overall development process. Furthermore,\nAI-assisted coding can signiﬁcantly improve code quality. By\nanalyzing vast amounts of code, AI algorithms can identify\ncommon coding mistakes, detect potential bugs, and sug-\ngest code refactoring or optimizations. This helps in reducing\nerrors and enhancing the reliability, maintainability, and per-\nformance of software applications.\nAnother beneﬁt of code assistance is its potential to facili-\ntate knowledge transfer and skill-sharing within development\nteams. By capturing and analyzing coding patterns, best prac-\ntices, and successful code implementations, code assistance\ntools can assist less experienced developers in learning from\nthe collective expertise of their peers, ultimately elevating\nthe skill level of the entire team. However, it is important to\nnote that AI-assisted coding is not meant to replace human\nprogrammers. Rather, it serves as a powerful tool to aug-\nment their capabilities, improve efﬁciency, and enable them\nto tackle complex coding tasks more effectively.\nError analysis in the generated code\nCode Generation is an exciting area of research and develop-\nment that aims to bridge the gap between human language and\nprogramming languages. In this task, the input is the human\ndescription and speciﬁcations, and the output is Python code.\nTo accomplish this, we implemented the MarianCG model\nspeciﬁcally designed for the code generation task. However,\ngenerating code automatically can sometimes lead to linting\nissues and errors in the generated code. To address this, we\nemployed various tools and techniques to ensure the gener-\nated code meets the required standards.\nFirstly, we used Flake8,\n1 a popular Python linter, to ana-\nlyze the generated code and identify any potential issues.\nFlake8 examines the code for violations of style guidelines\nand common errors, providing feedback on areas that need\nimprovement. Additionally, we performed syntax analysis\non the generated code to verify its correctness as Python\ncode. This analysis involved checking for proper indentation,\ncorrect syntax usage, and overall adherence to the Python\nlanguage rules. Any modiﬁcations required to make the code\nsuitable as valid Python code was implemented.\nCode refining and correction\nWe employed several error correction tools and libraries to\nfurther enhance the quality and readability of the generated\ncode. One such tool is Autopep8,\n2 which automatically for-\nmats Python code according to deﬁned style guidelines. It can\nﬁx issues related to indentation, line length, and whitespace,\nresulting in cleaner and more consistent code.\nWe also utilized the Add-trailing-comma\n3 library, which\nadds trailing commas to Python lists and dictionaries. Trail-\ning commas can improve code readability and help prevent\nerrors, especially when modifying or extending these data\nstructures. For consistent code formatting, we employed\nYapf\n4 and Black, 5 both popular Python code formatters.\nThese tools automatically format code according to various\nstyle guides, such as PEP 8 and Google’s style guide. By\napplying these formatters, the generated code becomes eas-\nier to read and maintain. Y APF is a formatter for Python ﬁles\ndeveloped by Google. It is designed to be highly conﬁgurable\nand have a low impact on the code being formatted. Y APF\nreformats the code to follow the style guide speciﬁed in PEP\n8, but with a focus on readability and consistent formatting.\nY APF is available for installation through pip and can\nbe used as a Python module or as a command-line inter-\nface. It provides options for specifying the maximum line\nlength, variable naming style, indentation, and more. Y APF\ncan be used along with other Python code analysis tools\nlike Flake8 and PyLint to ensure consistent code quality. To\nensure proper import organization, we used Isort, a Python\nlibrary that sorts and formats imports in Python code. Isort\nhelps maintain a consistent and logical order for imports,\nimproving code organization and readability.\n1 https://ﬂake8.pycqa.org/en/latest/ .\n2 https://github.com/hhatto/autopep8.\n3 https://github.com/asottile/add-trailing-comma .\n4 https://github.com/google/yapf .\n5 https://github.com/psf/black.\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3971\nLastly, we incorporated Ruff, a Python library for code\nrefactoring, to enhance the readability and maintainability\nof the generated code. Ruff automatically performs code\ntransformations, such as simplifying complex expressions,\nextracting functions, and applying another refactoring to\nimprove the code’s structure and clarity.\nBy combining these error correction and code enhance-\nment techniques, the generated code from the human descrip-\ntion can be reﬁned, ensuring it meets the required Python cod-\ning standards and best practices. This approach contributes\nto the overall reliability, maintainability, and readability of\nthe codebase, facilitating efﬁcient software development. All\nthe complete process for the code generation, lining, analysis,\nand correction is shown in Fig. 18.\nSo, there are three phases to the code generation and AI-\nassistant code as follows:\n1. Code generation from natural language\n2. Linting and Error Analysis using Flake8\n3. Reﬁning the generated code and error correction using the\nrest of the tools.\nImplementation and experimental setup\nDatasets\nBy ﬁne-tuning the MarianMT transformer model on the\nCoNaLa and DJANGO datasets, we got MarianCG which\nis a new transformer model that was built on the pre-trained\ntransformer. We followed [ 29, 33, 61] in the CoNaLa dataset\nand selected the top mined samples depending on the prob-\nability that the NL-Code pair is correct.\nTraining the models can be done by using the CoNaLa-\ntrain and/or CoNaLa-mined datasets, then take the rewritten\nintent ﬁeld from the CoNaLa-test dataset as input and gen-\nerating output from it. The CoNaLa-Large dataset contains\nabout 26K different NL-Code. This dataset contains the\nCoNaLa-train and examples from CoNaLa-mined and the\n500 examples in CoNaLa-test to compare by the same bench-\nmarks as other state-of-the-art contributors. Also, we used\nDJANGO which contains 19K examples, and got the results.\nTable2 displays the datasets employed in each experiment,\nas well as the dataset size and number of records in the train,\nvalidation, and test sets of data.\nExperimental setup\nIn our experiments, we employed DJANGO and the CoNaLa\ndataset of 26K NL-Code pairings from CoNaLa-train and\nCoNaLa-mined. The testing data set contains 500 samples\nfrom CoNaLa-test that were compared using the same bench-\nmarks as other state-of-the-art contributors.\nTable 2 Datasets in each experiment and distribution of the data\nDataset Dataset size Dataset split\nTrain V alidation Test\nDJANGO 19K 16,000 1000 1805\nCoNaLa Large 26K 24,687 1237 500\nThe implementation of our generated models was done\nwith the CoNaLa dataset using Google’s Colab Pro. Our\ndevelopment was on Python and PyTorch, and thanks to\npre-trained transformer models where we used various pre-\ntrained language models from their model hub. For training,\nwe relied on HuggingFace’s trainer and the implementation\nof the learning rate scheduler with batch size equals 2. For\nthe implementation, we applied these parameters: Adam opti-\nmizer, Weight decay = 0.01, Learning rate = 1e\n− 5, Number\nof hidden layers for the encoder = 6, Number of hidden lay-\ners for the decoder = 6, a linear learning rate scheduler,\nwarmup ratio of 0.05 seed = 1995 early stopping, a length\npenalty of 0.9, and we used beam search with four beams for\ngeneration.\nExperimental results\nWe present the results of our proposed hybrid model which\nevaluated on the CoNaLa and DJANGO datasets. For the\nMarianCG experiments, we measured the model’s code gen-\neration performance by calculating metrics such as code\naccuracy and code similarity scores. The results showed that\nMarianCG achieved competitive code generation accuracy\non both datasets, demonstrating its effectiveness in generat-\ning syntactically correct code snippets.\nWe introduced our novel hybrid model, which leverages\nthe strengths of MarianCG while incorporating additional\ncontextual information from BERT embeddings and vari-\nous encoder-only models. The hybrid model signiﬁcantly\nimproved the quality of generated code, achieving higher\ncode accuracy and semantic relevance compared to Mari-\nanCG on both datasets. These ﬁndings demonstrate the\nefﬁcacy of our proposed hybrid approach for enhancing code\ngeneration performance, making it a promising solution for\ncode-related Transformer-Based Processing tasks.\nResults of the proposed models\nwe present four novel transformer-based architectures designed\nto excel in code generation tasks, speciﬁcally targeting\nthe Python programming language. Our proposed models\ncombine the strengths of popular pre-trained models like\nDistilBERT, Electra, and Luke, with the versatile Marian\n123\n3972 Complex & Intelligent Systems (2024) 10:3955–3980\nFig. 18 Code assistant process\ndecoder. We evaluate our models on two widely used datasets,\nCoNaLa and DJANGO, to demonstrate their performance\nand capabilities.\nOur ﬁrst model, RoBERTaMarian, integrates the Distil-\nROBERTa pre-trained model as the encoder and the Marian\ndecoder. It achieves impressive performance on both datasets,\nparticularly on DJANGO, where it attains a superior BLEU\nscore of 88.91, an exact match accuracy of 77.95%, Sacre-\nBLEU of 74.08, and a ROUGE score of 92.76. These\nresults demonstrate the model’s proﬁciency in generating\nhigh-quality code solutions that accurately match human-\ngenerated references.\nThe second model, BERTMarian, utilizes the DistilBERT\nencoder and Marian decoder. On the CoNaLa dataset, it\nachieves a BLEU score of 32.46, an exact match accuracy of\n12.4%, SacreBLEU of 29.48, and a ROUGE score of 43.95.\nSimilarly, on the DJANGO dataset, it attains a BLEU score\nof 56.55, an exact match accuracy of 76.78%, SacreBLEU of\n29.48, and a ROUGE score of 43.95. These results indicate\nthe model’s ability to generate code solutions that closely\nalign with human-written code while maintaining linguistic\nﬂuency and capturing critical code semantics.\nThe third model, ELECTRAMarian, combines the pow-\nerful ELECTRA encoder with the Marian decoder. On the\nCoNaLa dataset, it achieves a BLEU score of 30.18, an\nexact match accuracy of 10.0%, SacreBLEU of 27.15, and\na ROUGE score of 42.42. Meanwhile, on the DJANGO\ndataset, it attains a BLEU score of 53.02, an exact match\naccuracy of 65.32%, SacreBLEU of 58.16, and a ROUGE\nscore of 83.91. These results showcase the model’s versatil-\nity and high performance in generating code solutions that\naccurately match human-authored references while demon-\nstrating linguistic ﬂuency and capturing important code\nsemantics.\nLastly, the LUKEMarian model, which employs the\nLUKE encoder and Marian decoder, shows promising results\non both datasets. On the CoNaLa dataset, it achieves a BLEU\nscore of 29.83, an exact match accuracy of 7.6%, SacreBLEU\nof 25.32, and a ROUGE score of 39.84. On the DJANGO\ndataset, it attains a BLEU score of 89.34, an exact match accu-\nracy of 78.50%, SacreBLEU of 74.66, and a ROUGE score\nof 93.11. These results conﬁrm the model’s efﬁciency in gen-\nerating code solutions that closely align with human-written\ncode while maintaining linguistic ﬂuency and capturing crit-\nical code semantics.\nThese results pave the way for further advancements\nin code generation technology and its potential integration\ninto real-world programming and automation applications.\nTable3 shows the results and the performance metrics of our\nproposed models on the CoNaLa dataset. Also, Table 4 shows\nthe results of those proposed models on the DJANGO dataset.\nFigures 19 and 20 show these results for the proposed code\ngeneration models on the CoNaLa and DJANGO datasets\nrespectively. The ﬁndings show that RoBERTaMarian model\nproduced the highest BLEU score of 35.74.\nOur proposed transformer-based models, RoBERTaMar-\nian, BERTMarian, ELECTRAMarian, and LUKEMarian,\ndemonstrate exceptional performance in code generation\ntasks, particularly on the Python programming language.\nTheir ability to generate accurate, relevant, and contextually\nappropriate code outputs solidiﬁes their standing as cutting-\nedge solutions in the programming and Transformer-Based\nProcessing domain.\nResults evaluation\nThis paper unveiled a range of powerful encoder–decoder\nmodels for code generation, each demonstrating unique\nstrengths. RoBERTaMarian and BERTMarian stood out with\ntheir competitive performance on the CoNaLa dataset, while\nELECTRAMarian and LUKEMarian showcased impres-\nsive proﬁciency on the DJANGO dataset. Additionally,\nMarianCG served as a strong baseline, delivering promis-\ning results on both datasets. Understanding the trade-offs\nbetween these models allows us to tailor their application\nto various code generation scenarios, catering to the spe-\nciﬁc requirements of each task. When compared to other\nresearchers who worked on the code generation problem on\nCoNaLa, this result has the highest BLEU score. The second\nitem to mention is the highest ROUGE score or ROUGE-L\nresult, which indicates the LCS-based statistics. The longest\ncommon subsequence problem takes sentence-level struc-\ntural similarities into account and automatically selects the\nlongest co-occurring in sequence n-grams. The MarianCG\nmodel has the highest ROUGE score of 49.63. The results of\nthe state-of-the-art code generation models on the CoNaLa\ndataset are shown in the Table 5 and indicated in Fig. 21 for\nthe CoNaLa models.\nRoBERTaMarian is considered the ﬁrst among the models\nthat get accurate results of the generated code. This model\nrecords a BLEU score of 35.74 with the advantage of being\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3973\nTable 3 Performance metrics of all models on the CoNaLa dataset\nNo. Model Evaluation metrics\nBleu score Exact match accuracy (%) Sacrebleu Rouge score\n1 MarianCG 34.4291 10.2 30 .6776 49.6272\n2 RoBERTaMarian model 35.7365 13.8 31 .3025 44.2547\n3 BERTMarian 32.4618 12.4 29 .479 43.9467\n4 ELECTRAMarian 30.1819 10.0 27 .1495 42.4232\n5 LUKEMarian 29.8281 7.6 25 .3187 39.8431\nTable 4 Performance metrics of\nall models on the DJANGO\ndataset\nNo. Model Evaluation metrics\nBleu score Exact match accuracy (%) Sacrebleu Rouge score\n1M a r i a n C G 9 0 .41 81 .83 75 .906 94 .647\n2 RoBERTaMarian 88 .9123 77 .95 74 .083 92 .7351\n3 BERTMarian 56 .55 76 .676 64 .884 88 .692\n4 ELECTRAMarian 53 .02 65 .319 58 .155 83 .905\n5 LUKEMarian 89 .3424 78 .504 74 .6583 93 .1133\nFig. 19 Results and\nperformance metrics of our\nproposed models on the\nCoNaLa dataset\nFig. 20 Results and\nperformance metrics of our\nproposed models on the\nDJANGO dataset\n123\n3974 Complex & Intelligent Systems (2024) 10:3955–3980\nTable 5 State of the art code\ngeneration models on CoNaLa Rank Model Evaluation metrics Year\nBLEU score Exact match accuracy (%)\n1 RoBERTaMarian (ours) 35 .7365 13.8 2023\n2 MarianCG (ours) [ 61]3 4 .4291 10.2 2022\n3 TranX + BERT w/mined [ 34]3 4 .2 5.8 2022\n4B E R T + T A E [ 32]3 3 .41 – 2021\n5 BERTMarian (ours) 32 .4618 12.4 2023\n6 External knowledge with API 32 .26 – 2020\n+ Reranking [ 29]\n7 External knowledge with API [ 29]3 0 .69 – 2020\n8 BART W/mined [ 33]3 0 .55 – 2021\n9 ELECTRAMarian (ours) 30 .1819 10.0 2023\n10 Reranker [ 26]3 0 .11 – 2019\n11 LUKEMarian (ours) 29 .8281 7.6 2023\n12 BART base [ 33]2 6 .24 – 2021\n13 TranX [ 25]2 4 .3 – 2018\nfast, small and containing multi-head attention. Besides that,\nMarianCG is considered the second model to have a BLEU\nscore of 34.43. These are our models which record the ﬁrst\nand second in the code generation models results. Besides\nthat, our generated models have the number of BLEU score\nwhich are in the top-ranking code generation models.\nAlso, Table 6 shows the results of all state-of-the-art mod-\nels on DJANGO, and the performance metrics are indicated in\nFig. 22. Comparing all models that worked on the DJANGO\ndataset proved that our models got high values of BLEU\nscore such as MarianCG, LUKEMarian model, and RoBER-\nTaMarian model. These models got BLEU scores of 90.41,\n89.34, and 88.91 respectively.\nRoBERTaMarian model is the ﬁrst among the models that\nget accurate results of the generated code on the CoNaLa\ndataset. Also, the MarianCG model is ranked in the top mod-\nels for its accurate predictions in terms of BLEU score and\nexact match accuracy. This model has a smaller size archi-\ntecture. It has six layers in the encoder and six layers in the\ndecoder, whereas other models have larger model sizes.\nThe contrast analysis presented in this paper demonstrates\nthe validity of the proposed models by comparing their\nperformance with other state-of-the-art models on multiple\ndatasets and highlighting their strengths in terms of code\nquality, semantic similarity, and computational efﬁciency.\nThe proposed models (RoBERTaMarian, BERTMarian,\nELECTRAMarian, LUKEMarian, and MarianCG) are com-\npared with other models that have been proposed in the liter-\nature for code generation tasks, speciﬁcally on the CoNaLa\nand DJANGO datasets. The results show that the proposed\nmodels achieve competitive performance on both datasets,\nwith RoBERTaMarian and BERTMarian performing well on\nCoNaLa, and ELECTRAMarian and LUKEMarian perform-\ning well on DJANGO. This suggests that the proposed models\nare capable of generating high-quality code across different\ndomains and tasks.\nOn the CoNaLa dataset, RoBERTaMarian has the highest\nBLEU score among all models, and MarianCG has the high-\nest ROUGE score, indicating its ability to generate code with\nhigh semantic similarity to the input. This suggests that the\nproposed models are able to generate code that is not only\nsyntactically correct but also semantically meaningful.\nOn the DJANGO dataset, the proposed models achieve\nhigh BLEU scores, with MarianCG, LUKEMarian, and\nRoBERTaMarian ranking in the top positions. This suggests\nthat the proposed models are effective in generating code for\na variety of programming tasks and languages.\nAdditionally, the proposed models have advantages in\nterms of computational efﬁciency, with MarianCG having\na smaller model size than other models while still achieving\nhigh performance. This suggests that the proposed models\nare not only effective in generating high-quality code but also\nefﬁcient in terms of the computational resources required.\nOverall, the contrast analysis presented in this paper\ndemonstrates the validity of the proposed models by com-\nparing their performance with other state-of-the-art models\non multiple datasets and highlighting their strengths in terms\nof code quality, semantic similarity, and computational efﬁ-\nciency.\nError/warning analysis and refining\nCode assistant represents a powerful symbiosis between\nhuman programmers and AI technology, providing intel-\nligent tools that assist in code writing, error detection,\nand optimization [ 65, 66]. By leveraging AI algorithms\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3975\nTable 6 State-of-the-art code\ngeneration models on DJANGO Rank Model Evaluation metrics Year\nBLEU score Exact match accuracy (%)\n1 MarianCG (ours) [ 61] 90.41 81 .83 2022\n2 LUKEMarian (ours) 89.3424 78 .504 2023\n3 RoBERTaMarian (ours) 88.9123 77 .95 2023\n4 TranX + BERT w/mined [ 34] 79.86 81 .03 2022\n5B E R T + T A E [ 32]–8 1 .77 2021\n6 BERTMarian (ours) 56.55 76 .68 2023\n7 ELECTRAMarian (ours) 53.02 65 .32 2022\n8 Reranker [ 26]– 8 0 .2 2019\n9T r a n X [ 25]– 7 3 .7 2018\nFig. 21 All results of the state of the art models in the code generation problem with CoNaLa\nFig. 22 All results of the state-of-the-art models in the code generation on DJANGO with respect to accuracy\nand machine learning, developers can enhance productiv-\nity, improve code quality, and facilitate knowledge sharing\nwithin development teams. As the capabilities of code assis-\ntant tools advance, they hold the potential to transform the\nsoftware development landscape, making the process more\nefﬁcient, reliable, and collaborative. We conducted a compre-\n123\n3976 Complex & Intelligent Systems (2024) 10:3955–3980\nhensive error and warning analysis using Flake8, a powerful\ntool for code linting and static analysis. Flake8 helped iden-\ntify potential issues in the generated Python code, such as\nsyntax errors, undeﬁned variables, and code style violations.\nTo address these concerns and improve code quality, we\nemployed automatic code formatting tools such as Autopep8,\nwhich automatically corrected issues related to indentation,\nline length, and whitespace. This resulted in cleaner and more\nconsistent code, aligning with established style guidelines. In\naddition to Autopep8, we utilized the Add-trailing-comma\nlibrary to enhance code readability and reduce errors asso-\nciated with Python lists and dictionaries. By adding trailing\ncommas to these data structures, we ensured a consistent\nformat, especially when modifying or extending them dur-\ning the code generation process. To maintain uniform code\nformatting across the generated solutions, we integrated two\npopular Python code formatters, Yapf and Black. Both tools\nautomatically formatted the code according to various style\nguides, such as PEP 8 and Google’s style guide. By employ-\ning these formatters, the readability and maintainability of\nthe generated code signiﬁcantly improved, making it easier\nfor developers to comprehend and work with the code.\nYapf,\n6 developed by Google, demonstrated high con-\nﬁgurability and minimized its impact on the code being\nformatted. It strictly adhered to the PEP 8 style guide while\nemphasizing readability and consistent formatting. The tool\nprovided ﬂexible options for setting maximum line length,\nvariable naming style, and indentation, among others. Yapf\nseamlessly integrated with other Python code analysis tools,\nsuch as Flake8 and PyLint, to ensure comprehensive and\nconsistent code quality. To organize imports effectively, we\nimplemented Isort, a Python library that efﬁciently sorts and\nformats import statements in Python code. By utilizing Isort,\nwe maintained a logical and consistent order for imports,\nthereby enhancing code organization and readability. Lastly,\nwe incorporated Ruff, a powerful Python library for code\nrefactoring, to further enhance the readability and maintain-\nability of the generated code. Ruff automatically performed\ncode transformations, such as simplifying complex expres-\nsions, extracting functions, and applying various refactoring\ntechniques, resulting in improved code structure and clarity.\nThe error analysis and distributions of the code generated\nfrom MarianCG and RoBERTaMarian models are shown in\nTable7. Also, Figs. 23 and 24 show the distribution for these\nerrors and warnings in the generated code and after reﬁning.\nAfter using these tools, the error and warning analysis\non the MarianCG and RoBERTa models became less than\nbefore. By leveraging these tools and libraries in our code\ngeneration process, we achieved code solutions that not only\nmet high-quality standards but were also consistently format-\nted and easy to maintain. The combination of static analysis,\n6 https://github.com/google/yapf .\nlinting, formatting, and refactoring tools contributed signif-\nicantly to the overall success of our code generation models\nand reinforced the importance of code quality in modern soft-\nware development.\nConclusion\nThis paper has demonstrated the promising application of\npre-trained transformer language models in code genera-\ntion, speciﬁcally through the combination of DistilRoBERTa,\nELECTRA, and LUKE with the Marian Decoder. Our\nexperimental results show that these models can generate\nhigh-quality code, as measured by static error detection\nand refactoring. The RoBERTaMarian model achieved a\npeak BLEU score of 35.74 and an exact match accuracy\nof 13.8% on the CoNaLa dataset, while the LUKEMarian\nmodel attained a BLEU score of 89.34 and an exact match\naccuracy of 78.50% on the DJANGO dataset. These results\nindicate that pre-trained language models have the potential\nto revolutionize code generation, offering a viable solution\nfor converting human descriptions into executable code.\nHowever, several study limitations should be acknowl-\nedged. Firstly, the dataset sizes used in our experimentation\nwere relatively small compared to other code generation\ndatasets, which may limit the generalizability of our ﬁnd-\nings to larger, more diverse datasets. Also, it is one line\ncode generation and it is the start to be completed in the\nnear future to work with functions, classes and see the con-\nnected speech in the natural language to have integrated and\ncomplete program. Secondly, we explored four pre-trained\nlanguage models and one decoder, leaving room for inves-\ntigation of other models and combinations that may yield\nsuperior performance or different trade-offs between accu-\nracy and efﬁciency.\nDespite these limitations, our work opens up exciting\navenues for future research. One emerging topic is mul-\ntimodal code generation, which involves generating code\nbased on visual, audio, or video inputs. Integrating pre-\ntrained language models with computer vision or multimedia\nprocessing techniques could unlock new possibilities in this\narea. Another important direction is explainable AI and\ninterpretability, which is becoming increasingly relevant as\nAI systems become more pervasive. Incorporating attention\nmechanisms, saliency maps, or other interpretability tech-\nniques into pre-trained language models for code generation\nwould allow developers to understand how the models arrive\nat their output.\nMoreover, as AI models become increasingly relied upon\nin software development, understanding their robustness\nagainst adversarial attacks becomes crucial. Investigating\nattack strategies and developing corresponding defense\nmechanisms will ensure the security and trustworthiness of\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3977\nTable 7 Error and warning analysis on MarianCG and RoBERTaMarian models\nError/warning message MarianCG RoBERTaMarian\nUsing Flake8 After correction Using Flake8 After correction\n“undeﬁned name” 593 450 295 295\n“no newline at end of ﬁle” 438 0 230 0\n“.format(...) has unused arguments” 0 0 1 1\n“SyntaxError: invalid character” 62 62 270 64\n“ambiguous variable name ‘l”’ 3 3 2 2\n“missing whitespace around operator” 4 0 0 0\n“‘return’ outside function” 4 3 0 0\n“missing whitespace after ‘,”’ 3 0 5 0\n“test for membership should be ‘not in”’ 1 1 0 0\n“whitespace after ‘(”’ 0 0 1 0\nFig. 23 Error analysis in the\ncode generated from MarianCG\nthese models in practice. Additionally, human-AI collabo-\nration, where AI models assist humans in generating code,\ncould lead to novel approaches in software development,\nblending the strengths of both humans and machines. Finally,\nit is essential to acknowledge the ethical implications of AI\nmodels automating parts of software development. Questions\nregarding ownership, accountability, and potential biases in\nthe generated code necessitate open discussions, guidelines,\nand regulatory frameworks that promote responsible AI prac-\ntices in software engineering.\nThe complexity of the input text is a critical factor in\ndetermining the complexity of the code generation task. As\nthe input text becomes longer, more structured, and contains\nmore domain-speciﬁc vocabulary, the task becomes increas-\ningly challenging. This is because the model needs to capture\nlonger-range dependencies, handle ambiguity, and generate\ncoherent and accurate code that meets the requirements of\nthe given task.\nTo quantify the complexity of the future work in the input\ntext, we can consider several factors such as:\n Sentence length: Longer sentences with multiple clauses\nand phrases require more sophisticated encoding and\ndecoding mechanisms to capture the relationships between\ntokens, entities, and actions.\n Domain-speciﬁc vocabulary: Specialized domains such\nas programming languages, legal documents, or medical\nreports often contain technical jargon and terminology\nthat demand a deeper understanding of the subject matter.\n Ambiguity and uncertainty: Natural language is inher-\nently ambiguous, with words and phrases having multiple\nmeanings and contexts. The model must be able to resolve\n123\n3978 Complex & Intelligent Systems (2024) 10:3955–3980\nFig. 24 Error analysis in the generated code from RoBERTaMarian model\nthese ambiguities and generate appropriate code that sat-\nisﬁes the constraints of the task.\n Structural complexity: Code generation tasks may involve\ngenerating code that includes nested structures, loops,\nconditionals, and functions, which requires a deep under-\nstanding of programming concepts and syntax.\n The complexity of the input text directly impacts the\ncomplexity of the resulting code. For instance, a sim-\nple command-line interface (CLI) script may require\nless complex code than a web application with multi-\nple routes, user authentication, and database interactions.\nSimilarly, a code snippet that performs basic arithmetic\noperations may be simpler than one that implements\nmachine learning algorithms or performance computa-\ntions.\nIn conclusion, this paper demonstrates the potential of\npre-trained transformer language models in code generation\nand highlights opportunities for future research in emerging\ntopics such as multimodal code generation, explainable AI,\nadversarial attacks, and human-AI collaboration. Addressing\nthe study limitations and ethical implications will help ensure\nthe responsible adoption of these models in software devel-\nopment, ultimately enhancing the productivity and efﬁcacy\nof software engineers.\nAcknowledgements We would like to express our sincere gratitude\nto Prof. Mohamed Zaki for his invaluable guidance and mentorship\nthroughout this research. His expertise and support have been instru-\nmental in shaping the trajectory of this work.\nAuthor Contributions The authors contributed to this work as follows:\n[AS]: Conceptualization, methodology, investigation, formal analysis,\nwriting-original draft, visualization, Code implementation, experimen-\ntation, validation, data curation, and writing-review and editing. [SS]:\nSupervision. [MH]: Supervision.\nFunding Open access funding provided by The Science, Technology &\nInnovation Funding Authority (STDF) in cooperation with The Egyp-\ntian Knowledge Bank (EKB).\nData Availability Datasets: CoNaLa Dataset: https://huggingface.co/\ndatasets/AhmedSSoliman/CoNaLa-Large . DJANGO Dataset: https://\nhuggingface.co/datasets/AhmedSSoliman/DJANGO. Models are avail-\nable now at the huggingface hub, and can be used or tested and\nintegrated into any project. Y ou can ﬁnd these models and easily deal\nwith various datasets through the following links: RoBERTaMarian:\nhttps://www.huggingface.co/AhmedSSoliman/DistilRoBERTa-Marian\n-Model-on-CoNaLa . https://www.huggingface.co/AhmedSSoliman/Di\nstilRoBERTa-Marian-Model-on-DJANGO. BERTMarian: https://ww\nw.huggingface.co/AhmedSSoliman/DistilBERT-Marian-Model-on-Co\nNaLa. https://www.huggingface.co/AhmedSSoliman/DistilBERT-Mari\nan-Model-on-DJANGO. ELECTRAMarian: https://www.huggingface.\nco/AhmedSSoliman/ELECTRA-Marian-Model-on-CoNaLa . https://w\nww.huggingface.co/AhmedSSoliman/ELECTRA-Marian-Model-on-D\nJANGO. LUKEMarian: https://www.huggingface.co/AhmedSSoliman/\nLUKE-Marian-Model-on-CoNaLa . https://www.huggingface.co/Ahm\nedSSoliman/LUKE-Marian-Model-on-DJANGO .\nDeclarations\nConﬂict of interest Not applicable.\n123\nComplex & Intelligent Systems (2024) 10:3955–3980 3979\nEthics approval Not applicable.\nConsent to participate Not applicable.\nConsent for publication Not applicable.\nCode availability Implementation and availability of the code are in the\nfollowing repository: https://github.com/AhmedSSoliman/Leveraging-\nPretrained-Language-Models-for-Code-Generation .\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. LeCun Y , Bengio Y , Hinton G (2015) Deep learning. Nature\n521(7553):436–444\n2. Dai AM, Le QV (2015) Semi-supervised sequence learning. ArXiv\narXiv:1511.01432\n3. Elazar Y , Kassner N, Ravfogel S, Ravichander A, Hovy E, Schütze\nH, Goldberg Y (2021) Erratum: measuring and improving consis-\ntency in pretrained language models. Trans Assoc Comput Linguist\n9:1407. https://doi.org/10.1162/tacl_x_00455\n4. V aswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. In:\nAdvances in neural information processing systems, vol 30\n5. Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K,\nZettlemoyer L (2018) Deep contextualized word representations.\nArXiv arXiv:1802.05365\n6. Howard J, Ruder S (2018) Universal language model ﬁne-tuning for\ntext classiﬁcation. In: Annual meeting of the association for com-\nputational linguistics. https://api.semanticscholar.org/CorpusID:\n40100965\n7. Raffel C, Shazeer NM, Roberts A, Lee K, Narang S, Matena M,\nZhou Y , Li W, Liu PJ (2020) Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. ArXiv arXiv:1910.10683\n8. Lewis M, Liu Y , Goyal N, Ghazvininejad M, Mohamed A, Levy\nO, Stoyanov V , Zettlemoyer L (2019) Bart: Denoising sequence-to-\nsequence pre-training for natural language generation, translation,\nand comprehension. arXiv preprint arXiv:1910.13461\n9. Rothe S, Narayan S, Severyn A (2020) Leveraging pre-trained\ncheckpoints for sequence generation tasks. Trans Assoc Comput\nLinguist 8:264–280\n10. LeClair A, Jiang S, McMillan C (2019) A neural model for gen-\nerating natural language summaries of program subroutines. In:\nProceedings of the 41st international conference on software engi-\nneering. ICSE ’19. IEEE Press, pp 795–806. https://doi.org/10.\n1109/ICSE.2019.00087\n11. Gad W, Alokla A, Nazih W, Salem AB, Aref M (2021) Dlbt-\nDeep learning-based transformer to generate pseudo-code from\nsource code. CMC 70:3117–3123. https://doi.org/10.32604/cmc.\n2022.019884\n12. Alokla A, Gad W, Nazih W, Aref M, Salem AB (2022) Retrieval-\nbased transformer pseudocode generation. Mathematics 10(4):604.\nhttps://doi.org/10.3390/math10040604\n13. Kaur P , Kumar H, Kaushal S (2023) Technology-assisted language\nlearning adaptive systems: a comprehensive review. Int J Cogn\nComput Eng 4:301–313. https://doi.org/10.1016/j.ijcce.2023.09.\n002\n14. Javidpanah M, Javadpour A, Rezaei S (2021) ROOA: CloudIDE\nframework for extension development. Int J Cogn Comput Eng\n2:165–170. https://doi.org/10.1016/j.ijcce.2021.09.003\n15. Moss A, Muller H (2005) Efﬁcient code generation for a domain\nspeciﬁc language. In: Glück R, Lowry M (eds) Generative pro-\ngramming and component engineering. Springer, Berlin, pp 47–62\n16. Guizzo G, Zhang J, Sarro F, Treude C, Harman M (2023) Mutation\nanalysis for evaluating code translation. Empir Softw Eng 29:19\n17. Athiwaratkun B, Gouda SK, Wang Z, Li X, Tian Y , Tan M, Ahmad\nWU, Wang S, Sun Q, Shang M, Gonugondla SK, Ding H, Kumar\nV , Fulton N, Farahani A, Jain S, Giaquinto R, Qian H, Ramanathan\nMK, Nallapati R, Ray B, Bhatia P , Sengupta S, Roth D, Xiang B\n(2023) Multi-lingual evaluation of code generation models. arXiv\npreprint arXiv:2210.14868\n18. Dahal S, Maharana A, Bansal M (2021) Analysis of tree-structured\narchitectures for code generation. In: Findings of the association\nfor computational linguistics: ACL-IJCNLP 2021, pp 4382–4391\n19. Qin P , Tan W, Guo J, Shen B, Tang Q (2021) Achieving seman-\ntic consistency for multilingual sentence representation using an\nexplainable machine natural language parser (mparser). Appl Sci\n24:11699\n20. Tang Z, Shen X, Li C, Ge J, Huang L, Zhu Z, Luo B (2022) Ast-\ntrans: code summarization with efﬁcient tree-structured attention.\nIn: 2022 IEEE/ACM 44th international conference on software\nengineering (ICSE), pp 150–162\n21. Shin R, Lin CH, Thomson S, Chen C, Roy S, Platanios EA,\nPauls A, Klein D, Eisner J, V an Durme B (2021) Constrained\nlanguage models yield few-shot semantic parsers. arXiv preprint\narXiv:2104.08768\n22. Dong L, Lapata M (2016) Language to logical form with neural\nattention. arXiv preprint arXiv:1601.01280\n23. Yin P , Neubig G (2017) A syntactic neural model for general-\npurpose code generation. arXiv preprint arXiv:1704.01696\n24. Rabinovich M, Stern M, Klein D (2017) Abstract syntax net-\nworks for code generation and semantic parsing. arXiv preprint\narXiv:1704.07535\n25. Yin P , Neubig G (2018) Tranx: A transition-based neural abstract\nsyntax parser for semantic parsing and code generation. arXiv\npreprint arXiv:1810.02720\n26. Yin P , Neubig G (2019) Reranking for neural semantic parsing.\nIn: Proceedings of the 57th annual meeting of the association for\ncomputational linguistics\n27. Shin EC, Allamanis M, Brockschmidt M, Polozov A (2019) Pro-\ngram synthesis and semantic parsing with learned code idioms. In:\nAdvances in neural information processing systems, vol 32\n28. Sun Z, Zhu Q, Xiong Y , Sun Y , Mou L, Zhang L (2020) Treegen:\na tree-based transformer architecture for code generation. In: Pro-\nceedings of the AAAI conference on artiﬁcial intelligence, vol 34,\npp 8984–8991\n29. Xu FF, Jiang Z, Yin P , V asilescu B, Neubig G (2020) Incorporating\nexternal knowledge through pre-training for natural language to\ncode generation. arXiv preprint arXiv:2004.09015\n30. Lano K, Xue Q (2023) Code generation by example using symbolic\nmachine learning. SN Comput Sci 4:1–23\n31. Le THM, Chen H, Babar MA (2020) Deep learning for source code\nmodeling and generation. ACM Comput Surv (CSUR) 53:1–38\n32. Norouzi S, Tang K, Cao Y (2021) Code generation from natural\nlanguage with less prior knowledge and more monolingual data.\nIn: Proceedings of the 59th Annual meeting of the association for\n123\n3980 Complex & Intelligent Systems (2024) 10:3955–3980\ncomputational linguistics and the 11th international joint confer-\nence on natural language processing (volume 2: short papers), pp\n776–785\n33. Orlanski G, Gittens A (2021) Reading stackoverﬂow encourages\ncheating: adding question text improves extractive code generation.\narXiv preprint arXiv:2106.04447\n34. Beau N, Crabbé B (2022) The impact of lexical and grammati-\ncal processing on generating code from natural language. arXiv\npreprint arXiv:2202.13972\n35. Wang Z, Cuenca G, Zhou S, Xu FF, Neubig G (2022) Mconala: a\nbenchmark for code generation from multiple natural languages.\narXiv preprint arXiv:2203.08388\n36. Kusupati U, Ailavarapu VRT (2022) Natural language to code using\ntransformers. ArXiv arXiv:2202.00367\n37. Al-Hossami E, Shaikh S (2022) A survey on artiﬁcial intelli-\ngence for source code: a dialogue systems perspective. ArXiv\narXiv:2202.04847\n38. Ni P , Okhrati R, Guan S, Chang VI (2022) Knowledge graph and\ndeep learning-based text-to-graphQL model for intelligent medical\nconsultation chatbot. Inf Syst Front 2022:1–20\n39. Kamath A, Das R (2018) A survey on semantic parsing. ArXiv\narXiv:1812.00978\n40. Gu J, Lu Z, Li H, Li VOK (2016) Incorporating copying mechanism\nin sequence-to-sequence learning. ArXiv arXiv:1603.06393\n41. Iyer S, Konstas I, Cheung A, Zettlemoyer L (2018) Mapping\nlanguage to code in programmatic context. In: Conference on\nempirical methods in natural language processing. https://api.\nsemanticscholar.org/CorpusID:52125417\n42. Xiao C, Dymetman M, Gardent C (2016) Sequence-based\nstructured prediction for semantic parsing. In: Annual meet-\ning of the association for computational linguistics. https://api.\nsemanticscholar.org/CorpusID:16911296\n43. Krishnamurthy J, Dasigi P , Gardner M (2017) Neural semantic\nparsing with type constraints for semi-structured tables. In: Confer-\nence on empirical methods in natural language processing. https://\napi.semanticscholar.org/CorpusID:1675452\n44. Ling W, Blunsom P , Grefenstette E, Hermann KM, Kociský T,\nWang F, Senior AW (2016) Latent predictor networks for code\ngeneration. ArXiv arXiv:1603.06744\n45. Iyer S, Cheung A, Zettlemoyer L (2019) Learning program-\nmatic idioms for scalable semantic parsing. In: Conference on\nempirical methods in natural language processing. https://api.\nsemanticscholar.org/CorpusID:125969731\n46. Nye M, Hewitt LB, Tenenbaum JB, Solar-Lezama A (2019) Learn-\ning to infer program sketches. ArXiv arXiv:1902.06349\n47. Dong L, Quirk C, Lapata M (2018) Conﬁdence modeling for\nneural semantic parsing. In: Annual meeting of the associa-\ntion for computational linguistics. https://api.semanticscholar.org/\nCorpusID:13686145\n48. Chaurasia S, Mooney RJ (2017) Dialog for language to code.\nIn: International joint conference on natural language processing.\nhttps://api.semanticscholar.org/CorpusID:217279086\n49. Andreas J, Bufe J, Burkett D, Chen CC, Clausman J, Crawford\nJ, Crim K, DeLoach J, Dorner L, Eisner J, Fang H, Guo A, Hall\nDLW, Hayes KD, Hill K, Ho D, Iwaszuk W, Jha S, Klein D, Krish-\nnamurthy J, Lanman T, Liang P , Lin CH, Lintsbakh I, McGovern\nA, Nisnevich A, Pauls A, Petters D, Read B, Roth D, Roy S, Rusak\nJ, Short BA, Slomin D, Snyder B, Striplin S, Su Y , Tellman Z,\nThomson S, V orobev AA, Witoszko I, Wolfe J, Wray AG, Zhang\nY , Zotov A (2020) Task-oriented dialogue as dataﬂow synthesis.\nTrans Assoc Comput Linguist 8:556–571\n50. Polozov O, Gulwani S (2015) Flashmeta: a framework for inductive\nprogram synthesis. In: Proceedings of the 2015 ACM SIGPLAN\ninternational conference on object-oriented programming, sys-\ntems, languages, and applications\n51. Parisotto E, Mohamed A, Singh R, Li L, Zhou D, Kohli P (2017)\nNeuro-symbolic program synthesis. ArXiv arXiv:1611.01855\n52. Bhupatiraju S, Singh R, Mohamed Ar, Kohli P (2017) Deep api pro-\ngrammer: Learning to program with apis. ArXiv arXiv:1704.04327\n53. Balog M, Gaunt AL, Brockschmidt M, Nowozin S, Tar-\nlow D (2017) Deepcoder: learning to write programs. ArXiv\narXiv:1611.01989\n54. Devlin J, Uesato J, Bhupatiraju S, Singh R, Mohamed Ar, Kohli P\n(2017) Robustﬁll: Neural program learning under noisy i/o. ArXiv\narXiv:1703.07469\n55. Xu Y , Dai L, Singh U, Zhang K, Tu Z (2019) Neural program\nsynthesis by self-learning. ArXiv arXiv:1910.05865\n56. Polosukhin I, Skidanov A (2018) Neural program search: Solving\ndata processing tasks from description and examples. In: ICLR\n2018\n57. Li T, Zhang S, Li Z (2023) Sp-nlg: a semantic-parsing-guided nat-\nural language generation framework. Electronics 12:1772\n58. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P ,\nNeelakantan A, Shyam P , Sastry G, Askell A et al (2020) Language\nmodels are few-shot learners. arXiv preprint arXiv:2005.14165\n59. Liu Y , Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M,\nZettlemoyer L, Stoyanov V (2019) Roberta: A robustly optimized\nbert pretraining approach. ArXiv arXiv:1907.11692\n60. Devlin J, Chang MW, Lee K, Toutanova K (2019) Bert: Pre-training\nof deep bidirectional transformers for language understanding.\nArXiv arXiv:1810.04805\n61. Soliman AS, Hadhoud MM, Shaheen SI (2022) Mariancg: a code\ngeneration transformer model inspired by machine translation. J\nEng Appl Sci 69:1–23\n62. Sanh V , Debut L, Chaumond J, Wolf T (2019) Distilbert, a dis-\ntilled version of BERT: smaller, faster, cheaper and lighter. CoRR\narXiv:1910.01108\n63. Clark K, Luong MT, Le QV , Manning CD (2020) Electra: Pre-\ntraining text encoders as discriminators rather than generators.\nArXiv arXiv:2003.10555\n64. Yamada I, Asai A, Shindo H, Takeda H, Matsumoto Y (2020) Luke:\ndeep contextualized entity representations with entity-aware self-\nattention. In: EMNLP\n65. Ross SI, Martinez F, Houde S, Muller M, Weisz JD (2023) The\nprogrammer’s assistant: Conversational interaction with a large lan-\nguage model for software development. https://doi.org/10.1145/\n3581641.3584037\n66. Poldrack RA, Lu T, Beguš G (2023) Ai-assisted coding: Experi-\nments with gpt-4\nPublisher’s Note\nSpringer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8269518613815308
    },
    {
      "name": "Code generation",
      "score": 0.6892662048339844
    },
    {
      "name": "Language model",
      "score": 0.5636814832687378
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.535440981388092
    },
    {
      "name": "Workflow",
      "score": 0.5135424137115479
    },
    {
      "name": "Artificial intelligence",
      "score": 0.509935736656189
    },
    {
      "name": "Natural language processing",
      "score": 0.4829201400279999
    },
    {
      "name": "Code (set theory)",
      "score": 0.4668707549571991
    },
    {
      "name": "Source code",
      "score": 0.4541946053504944
    },
    {
      "name": "Machine learning",
      "score": 0.4341288208961487
    },
    {
      "name": "Code review",
      "score": 0.42973315715789795
    },
    {
      "name": "Natural language",
      "score": 0.4156954884529114
    },
    {
      "name": "Software",
      "score": 0.4044390022754669
    },
    {
      "name": "Software engineering",
      "score": 0.3501025438308716
    },
    {
      "name": "Programming language",
      "score": 0.32241055369377136
    },
    {
      "name": "Software development",
      "score": 0.26315146684646606
    },
    {
      "name": "Software quality",
      "score": 0.18508118391036987
    },
    {
      "name": "Database",
      "score": 0.14597639441490173
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}