{
  "title": "Probing the Role of Positional Information in Vision-Language Models",
  "url": "https://openalex.org/W4287854770",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287855476",
      "name": "Philipp J. Rösch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2310900135",
      "name": "Jindřich Libovický",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2798040152",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3173909648",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4285886409",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2563399268",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W3034336960",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3102094970",
    "https://openalex.org/W3120237956",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2560730294"
  ],
  "abstract": "In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object's depth as new feature for a better object localization in the space. Even though we were able to improve the model properties as defined by our probes, it only has a negligible effect on the downstream performance. Our results thus highlight an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1031 - 1041\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nProbing the Role of Positional Information in Vision-Language Models\nPhilipp J. Rösch1 and Jindˇrich Libovický2\n1Institute for Distributed Intelligent Systems, Bundeswehr University Munich, Germany\n2Faculty of Mathematics and Physics, Charles University, Czech Republic\nphilipp.roesch@unibw.de libovicky@ufal.mff.cuni.cz\nAbstract\nIn most Vision-Language models (VL), the\nunderstanding of the image structure is en-\nabled by injecting the position information (PI)\nabout objects in the image. In our case study\nof LXMERT, a state-of-the-art VL model, we\nprobe the use of the PI in the representation\nand study its effect on Visual Question An-\nswering. We show that the model is not ca-\npable of leveraging the PI for the image-text\nmatching task on a challenge set where only\nposition differs. Yet, our experiments with\nprobing conﬁrm that the PI is indeed present\nin the representation. We introduce two strate-\ngies to tackle this: ( i) Positional Information\nPre-training and ( ii) Contrastive Learning on\nPI using Cross-Modality Matching. Doing so,\nthe model can correctly classify if images with\ndetailed PI statements match. Additionally to\nthe 2D information from bounding boxes, we\nintroduce the object’s depth as new feature for\na better object localization in the space. Even\nthough we were able to improve the model\nproperties as deﬁned by our probes, it only has\na negligible effect on the downstream perfor-\nmance. Our results thus highlight an impor-\ntant issue of multimodal modeling: the mere\npresence of information detectable by a prob-\ning classiﬁer is not a guarantee that the infor-\nmation is available in a cross-modal setup.\n1 Introduction\nPre-trained Vision-Language models (Tan and\nBansal, 2019; Lu et al., 2019; Yu et al., 2021; Chen\net al., 2020) reached strong performance in many\nmultimodal tasks such as Visual Question Answer-\ning (Antol et al., 2015; Hudson and Manning, 2019;\nBigham et al., 2010) or Visual Inference (Johnson\net al., 2017; Suhr et al., 2019). All these models use\nthe Transformer architecture (Vaswani et al., 2017)\nand make use of several pre-training strategies like\nMasked Cross-Modality Language Modeling(MM)\nand Cross-Modality Matching (CMM) similar to\nmasked language modeling and next sentence pre-\ndiction (Devlin et al., 2019) in NLP.\nBecause the attention mechanism treats its inputs\nas unordered sets, Transformer-based NLP models\nneed to use position encodings to represent the mu-\ntual position of the tokens, so that the models can\ngrasp the sentence structure. The mutual position\nof objects is equally important for understanding\nthe structure of an image. VL models differ in how\nthey represent objects in the image, typically repre-\nsented as sets of object features and PI. Therefore,\nobject detectors are used to obtain bounding box\ninformation for all objects. In many models, the\nupper left and lower right corners of the object’s\nbounding box are used as 2D information to create\na learnable positional encoding. In addition to the\nspatial but ﬂat 2D values, we determine the depth\nof the objects in the image and make it available\nas an additional feature. Until now, VL models\nrecognize the objects on a ﬂat map but not in the\nreal three-dimensional context.\nWe found that the current LXMERT model is\ncapable of forwarding PI through the model but\nis not capable to use it to solve image-text match-\ning tasks where positional keywords are replaced\nby their counterparts. Introducing two new pre-\ntraining strategies, we target these unimodal and\nmultimodal evaluation schemes and improve prob-\ning results. Yet, we did not get any perfomance\nincrease on the downstream tasks. This is most\nlikely due to the small fraction of position-related\ntext in the pre-training corpus and suboptimal re-\nsults of the object detector. Regarding PI type, it\nseems to be sufﬁcient to input object center values\nwhich is far less than most VL model input today.\n2 Positional Information in VL Models\nIn NLP, the importance of word order is given great\nattention (Ke et al., 2021; Wang and Chen, 2020).\nDifferent methods exist, including analytical po-\nsition encodings (Vaswani et al., 2017), learnable\n1031\nPI Type Models\n∅ CLIP\nx1, y1, x2, y2 LXMERT, M4C\nx1, y1, x2, y2, a\nwh ViLBERT,\nUnicoder-VL,\nERNIE-ViL\nx1, y1, x2, y2, w, h OSCAR\nx1, y1, x2, y2, w, h, aUNITER\nTable 1: Positional information in Vision-Language\nmodels. Most models use the upper left and lower right\nof the object’s bounding box ( x1, y1, x2, y2). Some\nmodels add the absolute (a) or relative object area ( a\nwh )\nin combination with the image width ( w) and height\n(h). The object depth ( d) is not used and ∅denotes no\nPI.\nadditive embeddings (Devlin et al., 2019) or the rel-\native the attention query (Shaw et al., 2018). There\nis no equivalent research that would speciﬁcally\napproach PI in VL models. However, the position\nof the objects is considered in almost all common\nTransformer-based approaches.\nIn LXMERT (Tan and Bansal, 2019) the upper\nleft and lower right corners of the object are used\nto encode its position. The same is true for M4C\n(Hu et al., 2020). Other models also use the relative\narea fraction of the objects as an additional feature.\nAlthough the network should be able to determine\nthis feature, it is explicitly added, as in case of\nViLBERT (Lu et al., 2019), Unicoder-VL (Li et al.,\n2020a), and ERNIE-ViL (Yu et al., 2021). UNITER\n(Chen et al., 2020) uses – in addition to the objects’\ncorners – the absolute object area and the image\nwidth and height. OSCAR (Li et al., 2020b) uses\nbounding box and image height and width. Only\nCLIP (Radford et al., 2021) does not use PI, al-\nthough they use another pre-training concept. See\nTable 1 for an overview. To our knowledge, there\nis no structured analysis of PI in VL models.\nCurrent models use only 2D object information.\nBy introducing depth as a new feature, we represent\nobjects in the 3D space. This is not only important\nto be able to deﬁne the distances between objects\nbut also to have a more meaningful understanding\nof the object sizes. Using the area of the bounding\nbox without depth information does not add the ac-\ntual object size information since the sizes depend\non the depth localization of the object.\n3 Evaluation of Positional Information\nTo determine the capability of current models with\nregard to PI, we experiment with three evaluation\nmethods. Firstly, we perform an intrinsic evalua-\ntion to determine whether the PI passes through\nthe model. Secondly, we test if the models are ca-\npable of utilizing PI using the CMM task. Lastly,\nwe report extrinsic results for GQA downstream\ntask (Hudson and Manning, 2019) on different data\nsubsets. We report the results of the probing exper-\niment in Section 5.\nFor our experiments, we use four types of PI.\nAn empty set ( ∅) acts as a baseline. Object cen-\nter values ( x, y) act as a coarse identiﬁcation of\nwhere the object is located. Moreover, we evaluate\nx1, y1, x2, y2, which is the standard representation\nof bounding boxes and is also often used in VL\nmodels. This PI description contains information\nabout object width, height, and area. Therefore,\nwe ignore further settings that add these types to\nthe input in our evaluation. Since we are also inter-\nested in analyzing depth, we investigate the setting\nx1, y1, x2, y2, das well.\nMutual Position Evaluation. In the intrinsic\nevaluation task, we test if PI is forwarded through\nthe whole model. We use nine different pairwise\nclassiﬁers for different mutual positions, which are\napplied to all detected objects. LXMERT uses a\nﬁxed number of 36 objects as its input. This leads\nto a total number of 9 ×36 ×36 = 11, 664 classi-\nﬁcations for each input image.\nWe use six classiﬁers for 2D spatial relations\n(operate on X and Y coordinates) and three for\ndepth information (Z coordinate). The tasks are\n(1) whether the center of one object is more to the\nleft than that of another object, (2) the same if the\ncenter is closer to the bottom, (3) whether one ob-\nject is completely left of the other object (without\nan overlap), (4) and the same for being completely\nbelow the other object, (5) whether one object is\ncompletely inside the other bounding box, (6) and\nif there is no overlap in the X and Y dimension.\nRegarding depth, the model needs to correctly clas-\nsify (7) if one object is more in the foreground\nregarding the median value, (8) if one object is in\nbetween the inner 50% of the other object using\nall pixel values, and (9) if all depth values of one\nobject are signiﬁcantly smaller than the values of\nthe other object at a signiﬁcance level of 0.05 using\na t-test.\n1032\nOriginal caption: “A\nstudent works on an\nacademic paper at her\ndesk, computer screen\nglowing in the\nbackground.”\nFigure 1: Pre-training data with image and description\nwith a PI keyword. For contrastive evaluation the key-\nword is replaced by its counterpart (i.e. “foreground”).\nThese classiﬁcation tasks have the same inputs\nas the Masked Object Prediction task (see Sec-\ntion 4.1.1) and are also constructed in the same\nmanner (see Appendix A.1). An overview of the\nvisual pre-training tasks is provided in Figure 3. Be-\ncause this is a probing task, the classiﬁcation head\n(PI head) is not updated during pre-training. After\nthe training process has ﬁnished, all model parame-\nters are frozen and only the weights in the PI heads\nare updated for 1 epoch. The average accuracy of\nall 11,664 classiﬁcation tasks is reported on the MS\nCOCO validation dataset. In doing so, we evaluate\nthe unimodal capabilities of the model to forward\ninformation through the whole Transformer. The\ndetailed results are presented in Appendix A.6.\nContrastive Evaluation on PI using CMM.\nThe CMM classiﬁer can successfully match images\nand captions (91% accuracy on the balanced pre-\ntraining validation data). However, this says little\nabout the type of information considered during the\nclassiﬁcation. To better assess if PI is used by the\nmodel, we build a challenge set consisting of pairs\nof contrastive examples. We ﬁlter the validation\ndata for samples with keywords indicating spatial\nrelation between objects and only keep those which\nare replaceable by antonyms (see Appendix A.2).\nWe run two evaluation setups: (1) We replace all\nimage descriptions with a random caption of a dif-\nferent image (following the LXMERT pre-training\nstrategy). (2) We take the image and for all cap-\ntions we replace the PI keyword with its antonym,\ne.g., substitute background with foreground and\nvice versa. See Figure 1 for an example. This task\ndetermines if the model is able to understand PI\nin a multimodal fashion. In both cases, we only\nhave samples with “no match” ground truth values\n(which is our positive class)1, and consequently we\nreport recall only.\n1Hence, we have FP = TN = 0.\nDownstream Task Evaluation. Finally, we de-\ntermine the performance of the model on a down-\nstream task. We use GQA, since it is a carefully bal-\nanced image question answering dataset, where PI\nplays a role. We report the 1- and 5-best accuracy.\nMoreover, we evaluate (top 1) accuracy of data sub-\nsets where X, Y , and Z coordinates are important.\nWe do this by selecting questions where speciﬁc PI\nkeywords are present (see Appendix A.3).\nSince keyword search does not work perfectly\n(e.g., Which color is the bag on the back of the\nwoman?), we employ zero-shot text classiﬁcation\nusing a BART model 2 (Lewis et al., 2020). For\nzero-shot classiﬁcation we need a candidate label,\nwhich is used as input to determine if both texts\n(i.e. caption and candidate label) ﬁt together. We\nexperimented with different labels and found that\nthe simple keyword “position” works best for our\nuse case.\nDownstream evaluation is done on the GQAtest-\ndev split, which has 12,578 samples, hence, an\nchange of 0.1% is equivalent with approximately\n13 more correctly classiﬁed samples. For the sub-\nsets where X, Y , Z keywords are present, the dataset\nsize is 2,050, 1,203 and 1,349 respectively. For the\nzero-shot subset (indicated with P) the sample size\nis 1,349.\n4 Model and Data\n4.1 Model\nOur experiments are built upon LXMERT – a\nTransformer-based model with two separate en-\ncoders for image and text modality and one cross-\nencoder to join both. LXMERT was the only model\nin the top-3 leaderboard in both the VQA v2.0 2019\nand GQA 2019 challenge, which is why we use\nthis model as the basis for our work. Details are\nprovided in Section 4.1.1. A detailed description\nof how the object’s depth feature is determined is\nprovided in Section 4.1.2.\n4.1.1 Base Model\nLXMERT uses Faster R-CNN with ResNet-101\nfor the object detection task, originally introduced\nby Anderson et al. (2018). The object detec-\ntor is trained on Visual Genome (Krishna et al.,\n2017) predicting 1,600 objects with 400 different\nattributes (mostly adjectives). For LXMERT the\nmodel extracts the 36 most conﬁdent objects with\n2https://huggingface.co/facebook/\nbart-large-mnli\n1033\nthe region-of-interest features fj, the object class\ncj, attribute aj and the positional information pj,\nwhere j indicates the object indexes j = 1, . . . ,36.\nThe feature map (R36×2048) and the bounding box\ncoordinates (R36×4) are passed to two separate lin-\near models with weight matrix W and bias b. The\noutput is further processed by two layer normaliza-\ntions (LN) and ﬁnally both results are averaged:\nˆfj = LN(WF fj +bF ) ˆ pj = LN(WP pj +bP )\nvj = (ˆfj + ˆpj)/2\nThis leads to a uniﬁed embedding vj ∈R36×768\nrepresenting the content of the objects and the po-\nsitions at the same time. The image data is further\nprocessed in a BERT-style encoder.\nOn the language side, the text input is processed\nin a BERT-style encoder as well. Both outputs\nare merged in a cross-modality encoder (X-Enc)\nand passed to the output heads, where the losses\nfor each pre-training strategy are calculated. The\nLXMERT architecture can be investigated in Fig-\nure 2.\nThe same pre-training strategies are used,\nnamely Masked Cross-Modality Language Mod-\neling (MM), Cross-Modality Matching (CMM), Vi-\nsual Question Answering (VQA), and Masked Ob-\nject Prediction. The last one is composed of three\ntasks: two classiﬁcation tasks to predict the ob-\nject classes and attributes (ObjClassif, AttrClassif ),\nand a regression task to predict the feature vector\n(FeatRegr). See Tan and Bansal (2019) for all de-\ntails. Note that all pre-training strategies explicitly\nfocus on the object features fj, cj, and aj and not\non the PI. The same is true for other VL models\nlisted in Table 1. See Figure 3 for an illustration of\nall visual pre-training tasks.\nWe used the original implementation of\nLXMERT3 and only made minor changes. We\nintroduced dropout with p = 0.1 in the IQA head.\nFurther, we tested different training hyperparam-\neters to ﬁnd a good ratio between model perfor-\nmance and training time. Our ﬁnal pre-training\nmodel setup has a batch size of 2048 with a learn-\ning rate of 10−4 (with the same learning rate sched-\nuler), the ﬁne-tuning model has a batch size of\n32 and a learning rate of 10−5. Introducing Py-\nTorch’sDistributedDataParallel in the code and\nusing 8 instead of 4 GPUs reduced the pre-training\ntime from approximately 8.5 days to 41 hours. We\n3https://github.com/airsplay/lxmert/\nImg\nDepth Estimator\nObject DetectorVisEnc\nTxt LgnEnc\nX-Enc Targets\nFigure 2: Architecture of the LXMERT model (blue)\nwith depth information extension (gray). LXMERT\nuses object detection from Anderson et al. (2018) and\nhas 5 visual, 9 language and 5 cross-modality (X-Enc)\nlayers.\ndj\npj\nfj\nLXMERT\npj < pi, dj < di\nfj\ncj\naj\nFigure 3: Visual components for the pre-training phase\n(text components omitted). Input data ( fj, pj) to\nthe visual encoder and training targets ( fj, cj, aj) for\nLXMERT’s pre-training strategies are indicated in blue.\nOur additional depth data dj and PI pre-training labels\n(PIP) are colored gray.\nused the pre-training weights reported in the pa-\nper and not in the corresponding repository (see\nAppendix A.4).\n4.1.2 Depth Information\nThe datasets used for training LXMERT do not pro-\nvide any depth information. To obtain depth values\nfor each pixel in the image, we used MiDaS v2.14\n(Ranftl et al., 2020) – a state-of-the-art algorithm\nfor monocular depth estimations. It is trained on\ndiverse datasets from indoor and outdoor environ-\nments, containing static and dynamic images and\nimages of different quality. Hence, it ﬁts the var-\nious picture types in our datasets. See Figure 4a\nfor an original COCO image and Figure 4b for the\ndepth information provided by the MiDaS model.\nThe depth predictions from MiDaS can be any\nreal number. Large numbers indicate close objects,\nand small numbers refers to distant objects. We lin-\nearly normalized each pixel xi with 1 −xi−min(x)\nmax(x)\nto obtain 0 for the closest pixel and 1 for the most\ndistant one for each individual image.\nSince the rectangular bounding boxes do not sur-\nround the objects perfectly, we experimented with\nthe object’s center value, the mean and median as\nheuristic. We ﬁnally used the median, due to its\nrobustness. Furthermore, it would be conceivable\n4https://pytorch.org/hub/intelisl_\nmidas_v2/\n1034\n(a) Original image\n (b) Depth estimation\nFigure 4: We use a monocular depth estimator to obtain\na pixel-level depth prediction. We normalize the output\nthat 0 (yellow) indicates the value that is at the very\nfront and 1 for the furthest pixel (violet).\nto additionally take the standard deviation as a mea-\nsure for uncertainty if the object is on speciﬁc depth\nplane or spans over a larger distance. This issue can\nbe avoided with panoptic segmentation (Kirillov\net al., 2019), which we leave to the future work.\n4.2 Data\nFollowing the original LXMERT setup, our models\nare pre-trained using the MS COCO (Lin et al.,\n2014) and Visual Genome (VG; Krishna et al.,\n2017) data in conjunction with some Visual Ques-\ntion Answering task (VQA). There are in total\n9.18M image-caption pairs with 180K unique im-\nages. The average sentence length per caption is\n10.6 words for MS COCO and 6.2 words for VG.\nThe sentences are short and do not provide many\ndetails. Using 10 words, only the main occurrence\nof the image can be described. See examples in\nAppendix A.5.\nIn Table 2, we show the relative occurrence of\nPI keywords (see Appendix A.3). Pre-training data\ndo not have a lot of PI in the captions or questions.\nOnly Y keywords appear more often (11.2%) in\nMS COCO and X keywords in VQA (10.7%). This\nis different in GQA, which we use for downstream\nevaluation. In the train part, there are many X\nkeywords, but only a few Y and Z keywords. The\ndistribution in the testdev set is different. Here, the\nnumber of X, Y , and Z questions is high.\nLXMERT was also evaluated on VQA v2.0\n(Goyal et al., 2017) and NLVR2 (Suhr et al., 2019).\nVQA v2.0 has PI relations (under 3%), so we do\nnot run an analysis on this dataset. NLVR2 has po-\nsitional relations, but PI keywords are often part of\nthe left/right image assignment and do not indicate\nobjects within an image according to our deﬁnition\nof PI. Moreover, the presence of multiple images\nrules out a clean analysis of PI.\nDataset X Y Z\nMS COCO 2.9 11.2 6.5\nVG 3.4 3.8 4.6\nVQA 10.7 3.3 4.0\nGQA train 28.4 5.3 4.9\nGQA testdev 16.3 9.6 10.7\nTable 2: Occurrence of positional keywords in per-\ncent in pre-training (top lines) and downstream datasets\n(bottom lines).\n5 Probing Results\nThis section shows the results of the experiments\ndescribed in Section 3.\nMutual Position Evaluation. We determine\nwhether PI can be passed through the model us-\ning the classiﬁcations of the PI head. Results are\nshown in Table 3 (top lines). The accuracy is only\n80.0% for no PI, but over 88% for the remaining\ntypes. This conﬁrms that the model is able to for-\nward PI through the whole Transformer layer stack.\nInterestingly, the model is often capable of cor-\nrectly classifying the mutual position of objects,\nalthough PI is not used as the model input. This is\nmost likely due to the high correlation between the\nobject categories and their positions. For example,\n“shoes” are usually at the bottom and in the fore-\nground. The object detector is not powerful enough\nto detect small objects in the background in general.\n“Sky” and “clouds” are usually at the top and in the\nbackground of the image. Detected objects such as\n“kitchen” or “ofﬁce” often span the whole image\nwidth and therefore have their center in the middle\nof the X axis. The latent image representation fj\ncan be used as a proxy for object types.\nIn addition to that, we can see that with more\nPI the accuracy of this task increases by more than\neight percent points and has a peak at 89.7% for\nthe input setting x1, y1, x2, y2, d. Switching from\nobject centers to bounding boxes only has a minor\nimpact. Yet, adding depth improves accuracy on\nthe three Z related tasks (see Appendix A.6), which\nboost the overall performance.\nContrastive Evaluation on PI using CMM. To\nfurther evaluate the use of PI in VL models, we\ntest if the model can utilize the information using\nthe CMM task. Table 4 (top lines) shows that the\noriginal setting with dissimilar image-text pairs can\nbe predicted almost perfectly – the recall is always\nabove 96%. Hence, this pre-training strategy be-\n1035\nPI XYZ XY Z\nInput Acc Acc Acc\nProbing\n∅ 80.0 81.5 77.1\nx, y 88.5 92.1 81.1\nx1, y1, x2, y2 88.7 92.4 81.3\nx1, y1, x2, y2, d 89.7 92.2 84.7\nPre-training\n∅ 88.2 88.9 82.1\nx, y 91.6 94.4 86.0\nx1, y1, x2, y2 92.1 94.9 86.5\nx1, y1, x2, y2, d 93.9 94.8 92.2\nTable 3: Mutual Position Classiﬁcation Evaluation:\nMean accuracy of all 9 mutual classiﬁcation tasks\n(XYZ), 6 XY tasks, and 3 Z tasks for pre-trained\nmodels for different PI inputs. Upper lines for plain\nLXMERT and lower lines with our version (PIP, CL;\nsee Section 6).\nhaves as expected for the normal data provided.\nYet, the model cannot apply ﬁne-grained details\nfrom textual PI. It is not capable of correctly re-\njecting that, for example, “A student works on an\nacademic paper at her desk, computer screen glow-\ning in the foreground. ” does not ﬁt to the image\nfrom Figure 1. The recall is steadily below 2%.\nThe model is able to pass through PI in the vi-\nsual Transformer part but is not able to use it in a\ncross-modal fashion for solving problems. This is\nprobably due to the fact that ﬁne-grained matching\ndoes not play a role during pre-training. CMM\nis not constructed as indicated above (i.e., back-\nground vs. foreground) but to select completely\ndissimilar statements like \"A man sits before a light\nmeal served on the table of a travel trailer” to the\nimage in Figure 1. To overcome this problem, we\nneed more advanced negative sampling, i.e. cap-\ntions that are closer to the original image-text pairs.\nDownstream Task Evaluation. We evaluate\ndownstream performance on GQA testdev with\nfour different subsets targeting X, Y , Z keywords\nand general positional (P) samples. The results (in\nTable 5 top lines) reveal that using any type of PI is\nbetter or equally good than not using it (except for\nY in x1, y1, x2, y2). Although the improvements\nare small, they indicate that PI is indeed helpful in\nthis downstream task.\nThe best top 1 and X subset results are achieved\nby x, yinput type. This might be due to the fact\nthat most object relations are distinct and center\nvalues are sufﬁcient to track this relationship. For\nexample, the question “Is the boy in white left or\nPI Permuted Permuted\nInput caption PI words\nProbing\n∅ 97.4 1.4\nx, y 96.5 0.3\nx1, y1, x2, y2 96.8 1.7\nx1, y1, x2, y2, d 97.1 1.2\nPre-training\n∅ 96.8 78.1\nx, y 97.7 79.5\nx1, y1, x2, y2 97.7 79.3\nx1, y1, x2, y2, d 97.1 79.5\nTable 4: Contrastive Evaluation: Recall of the original\nCMM tasks with random captions (left) and text-image\npairs with substituted PI antonyms (right). Upper lines\nfor plain LXMERT and lower lines with our version\n(PIP, CL; see Section 6).\nFigure 5: Bounding box predictions for the 36 objects\nused in LXMERT. Descriptions contain predicted label\nand attribute with conﬁdence scores.\nright of the ball? ” is more common than asking\nambiguous questions, for example, where bound-\ning boxes intersect (“Is the left boy in yellow left or\nright of the ball?”, see Figure 5).\nThe PI input x1, y1, x2, y2, dreceived the best\nresults for the Y and Z subsets. Although the im-\nprovements are small, it shows that our new depth\nfeature can help solve the Z task. But also, the\nimprovement on Y can be attributed to the depth\ninput. Due to the graphical perspective, objects at\nthe top correlate with the background and objects\nat the bottom with the foreground (see Figure 4b).\nHere, object depth can act as a top/down proxy.\nFor the downstream evaluation, we need to keep\nin mind that the underlining object detector is not\nperfect. Therefore, we face the issue that objects\nasked for in the questions are not always a part\nof LXMERT’s visual input. Moreover, our con-\ntrastive evaluation scheme shows that LXMERT\nhas difﬁculties to properly matching image and\n1036\ntext representation in a multimodal fashion. This\ncan explain the small margin of improvements. The\nincrease of top-1 accuracy is not reﬂected in the\ntop-5 accuracy.\n6 From Probing to Pre-training\nIn the previous section, we evaluated the role of PI\nin pre-trained LXMERT. In this section, we use\nthe probing tasks as a part of model pre-training\nto improve weaknesses that we identiﬁed in the\nprevious section. Alongside the established strate-\ngies, we add two tasks to learn mutual positions\nand ﬁne-grained PI details in captions utilizing the\nCMM task. These strategies are elaborated below.\nPositional Information Pre-training (PIP).\nCurrently, all pre-training strategies rely on the\nvisual features ( fj, cj, aj) rather than on the PI.\nOnly in a small fraction of the pre-training captions\nand questions positional keywords are present, as\nTable 2 shows. Hence, we add a new pre-training\nstrategy which exclusively focuses on PI.\nWe take the PI head used in Mutual Position\nEvaluation and add it as a new classiﬁcation task\nwhich is updated during pre-training. We weight\nPIP by 10, since the initial loss is noticeably lower\nthan the losses of the other strategies. Until now\nonly visual representation of the object features,\nlabels and attributes was part of pre-training. Using\nPIP, we introduce an explicit unimodal connection\nbetween the PI input and the PI output, which was\nnot previously available (see Figure 3).\nContrastive Learning using CMM (CL). Dur-\ning pre-training in classical CMM in 50% of all\ncases the caption is replaced with another random\nimage description. This is similar to the main pre-\ntraining concept of CLIP. Yet, doing so, the model\nonly learns to distinguish dissimilar text and im-\nages. There are no small differences in the captions\nthat the model needs to be aware.\nIn line with Contrastive Evaluation on PI using\nCMM, we make CMM more complex. In 50% of\nall captions with PI keywords the word is replaced\nby its counterpart, so that it has to learn ﬁne-grained\nPI differences during pre-training. Dissimilar to\nPIP, this pre-training strategy only affects a small\nportion of the pre-training samples, since PI key-\nwords are rare. Yet, it operates on both modalities\nand hence is able to connect both data types. This\nidea can also be extended to other attributes (such\nas color, material, shape using VG’s Scene Graph).\nResults. Using both pre-training strategies, we\ntrain new models for all four PI input types. We\nassess the models using the same three evaluation\nschemes as the plain LXMERT model before.\nResults of Mutual Position Evaluation are shown\nin Table 3 (bottom lines). We observe an increase\nin accuracy for all input types. The largest is for\nthe empty input type with an accuracy of 88.2%,\nindicating the high correlation between feature fj\nand position pj. For the other versions improve-\nments are smaller. In Table 9 in the Appendix, the\naccuracies for each of the nine classiﬁcation tasks\nare displayed. The largest increase can be seen\nfor the empty input type with up to 23.1 percent\npoints for task (1) of the 9 mutual position classi-\nﬁcation tasks. For classiﬁcations based on depth,\nthe best improvements are 9.7 percent points for\ntask (7) and 8.0 percent point for task (9) utilizing\nx1, y1, x2, y2, d. This shows that the presence of\ndepth is useful as expected.\nIn the original LXMERT version, the probe on\nContrastive Evaluation on PI using CMM showed\nthat the model is not able to solve this task suc-\ncessfully. Recall was steadily below 2 percent. In-\ntroducing the CL pre-training strategy increases\nmatching accuracy to over 78 percent, as shown in\nTable 4 (bottom lines). In CMM, we are now able\nto perform matching between visual and textual\nrepresentations regarding PI. As a consequence,\nwe successfully force the model to connect both\ntypes in a multimodal manner.\nDownstream Task Evaluation. The third eval-\nuation is the downstream task and results are shown\nin Table 5 (bottom lines). In the two former probes,\nour extended pre-training helped the model to solve\nthese tasks. However, interestingly, this is not the\ncase for GQA evaluation. The best results for\nthe top 1 and subset tasks are obtained by plain\nLXMERT. Only in the (not ofﬁcial) best 5 accu-\nracy, evaluation our version achieves better results.\nOne reason for this may be that our PIP weight is\ntoo high and needs to be tuned in further studies.\nWe found that PI has much less impact on down-\nstream results as previously thought. Simple object\ncenters are often sufﬁcient. Bounding box data,\nwhich add object width, height and area, do not\nadd the desired information that the models can\nutilize. Adding depth is marginally useful on the Z\ntask, which suggests that this feature is useful.\n1037\nPI Input Top 1 X Y Z P Top 5\nProbing\n∅ 58.1 65.7 62.0 46.4 58.0 85.0\nx, y 59.4 69.6 62.0 49.6 60.2 85.0\nx1, y1, x2, y2 59.0 66.2 61.8 49.4 58.9 85.3\nx1, y1, x2, y2, d 58.6 66.0 62.4 50.0 58.4 85.1\nPre-training\n∅ 58.8 68.7 60.4 48.5 59.0 85.1\nx1, y1 58.8 68.7 60.4 48.5 59.0 85.1\nx1, y1, x2, y2, 58.7 67.6 61.5 48.3 58.6 85.4\nx1, y1, x2, y2, d 58.7 67.8 62.0 49.1 59.0 85.8\nTable 5: Evaluation on GQA testdev: Model comparison of plain LXMERT models (top lines) and our version\nwith PIP and CL pre-training (bottom lines) for different PI Input types. Evaluation on Top 1 and Top 5 accuracy,\nmoreover on subsets focusing on X, Y , and, Z keywords only and questions that focus on position (P) using zero-\nshot classiﬁcation. Underlined numbers indicate the best overall model per column and bold numbers indicate the\nbest model per block and column.\n7 Conclusions\nCurrent VL models make use of different PI in-\nputs without evaluating their impact. In our work,\nwe inspect the effect of such PI input types and\ninvestigate depth as a new input extension. In the\noriginal setting, the model is able to forward the\npositional information through the whole Trans-\nformer layer stack, but it cannot utilize it in the\ncontrastive evaluation and only marginally in the\ndownstream task. Overall, having any type of PI is\nhelpful, though object-center values are often sufﬁ-\ncient. However, object features fj are already good\nproxies for where objects are located. Because this\ncan be based on spurious correlations, we propose\npre-training methods that should make the model\nrely on PI directly.\nWe introduced two new pre-training strategies.\nFirstly, Positional Information Pre-training to en-\nsure that data is passed through the model prop-\nerly and does not need to rely on feature corre-\nlations. This operates on visual component only\nand increases performance on the corresponding\nintrinsic evaluation task. Moreover, we introduce\nContrastive Learning on PI using CMM. In doing\nso, we connect PI in the textual and visual modal-\nity. As a result, the model is now able to succeed\nin the contrastive evaluation task. However, these\nimprovements do not affect the downstream perfor-\nmance on GQA.\nIt is not enough to add different features\nunchecked, trusting they are properly utilized by\nthe Transformer. In line with BERTology (Rogers\net al., 2020; Clark et al., 2019; Tenney et al., 2019),\nstudies are important to understand better what\na model is capable. The same is true for pre-\ntraining strategies. It is not sufﬁcient to add new\npre-training strategies, although they look promis-\ning. With our probing experiments, we tried to re-\nceive a better understanding of the inner workings\nof LXMERT. We see the importance to investigate\ndifferences between general concepts and impact\non a downstream task.\nWe see two major issues for PI in VL models.\nFirstly, the pre-training data contains too little frac-\ntion of sentences with PI content. Hence, espe-\ncially the CL pre-training strategy has not enough\nsamples to learn from. Secondly, the used object\ndetector is not very powerful (see predictions in\nFigure 5). Newer detection models like VinVL\n(Zhang et al., 2021) might help to have a better im-\nage representation, which consequently leverages\nperformance regarding PI context.\nAcknowledgements\nThe authors gratefully acknowledge the computing\ntime granted by the Institute for Distributed Intel-\nligent Systems and provided on the GPU cluster\nMonacum One at the Bundeswehr University Mu-\nnich. The work at CUNI was funded by the Czech\nScience Foundation, grant no. 19-26934X.\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 6077–6086.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\n1038\nand Devi Parikh. 2015. VQA: Visual question an-\nswering. In Proceedings of the IEEE international\nconference on computer vision, pages 2425–2433.\nJeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg\nLittle, Andrew Miller, Robert C Miller, Robin\nMiller, Aubrey Tatarowicz, Brandyn White, Samual\nWhite, et al. 2010. Vizwiz: nearly real-time answers\nto visual questions. In Proceedings of the 23nd an-\nnual ACM symposium on User interface software\nand technology, pages 333–342.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: Universal image-text\nrepresentation learning. In European conference on\ncomputer vision, pages 104–120. Springer.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL-HLT.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nV in VQA matter: Elevating the role of image under-\nstanding in Visual Question Answering. In Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nRonghang Hu, Amanpreet Singh, Trevor Darrell, and\nMarcus Rohrbach. 2020. Iterative answer predic-\ntion with pointer-augmented multimodal transform-\ners for textvqa. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 9992–10002.\nDrew A Hudson and Christopher D Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In Pro-\nceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 6700–6709.\nJustin Johnson, Bharath Hariharan, Laurens van der\nMaaten, Li Fei-Fei, C. Lawrence Zitnick, and\nRoss B. Girshick. 2017. CLEVR: A diagnostic\ndataset for compositional language and elementary\nvisual reasoning. In 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2017,\nHonolulu, HI, USA, July 21-26, 2017 , pages 1988–\n1997. IEEE Computer Society.\nGuolin Ke, Di He, and Tie-Yan Liu. 2021. Rethinking\nPositional Encoding in Language Pre-training. In\nInternational Conference on Learning Representa-\ntions.\nAlexander Kirillov, Kaiming He, Ross Girshick,\nCarsten Rother, and Piotr Dollár. 2019. Panoptic\nsegmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 9404–9413.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma,\net al. 2017. Visual genome: Connecting language\nand vision using crowdsourced dense image anno-\ntations. International Journal of Computer Vision ,\n123(1):32–73.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 11336–\n11344.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu,\nPengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, Yejin Choi,\nand Jianfeng Gao. 2020b. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. ECCV 2020.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision –\nECCV 2014, pages 740–755, Cham. Springer Inter-\nnational Publishing.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. Advances in neural information processing\nsystems, 32.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision.\nRené Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. 2020. Towards ro-\nbust monocular depth estimation: Mixing datasets\nfor zero-shot cross-dataset transfer. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence\n(TPAMI).\n1039\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418–6428, Florence, Italy. Association for\nComputational Linguistics.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT Rediscovers the Classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYu-An Wang and Yun-Nung Chen. 2020. What do\nposition embeddings learn? an empirical study of\npre-trained language model positional encoding. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6840–6849, Online. Association for Computa-\ntional Linguistics.\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian,\nHua Wu, and Haifeng Wang. 2021. Ernie-vil:\nKnowledge enhanced vision-language representa-\ntions through scene graphs. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 35, pages 3208–3216.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. 2021. Vinvl: Making visual repre-\nsentations matter in vision-language models. CVPR\n2021.\nA Appendix\nA.1 PI Classiﬁcation Head\nThe PI head is build up in the same manner\nas the other visual heads, i.e. Dense →\nActivation → Layer Normalization\n→ Dropout → Dense.\nA.2 PI Antonyms\nFor Contrastive Evaluation, we replace some PI\nkeywords with its antonyms.\nWe substitute left with right, above with below,\nunder with over, foreground with background, be-\nfore with behind and vice versa.\nA.3 PI Keywords\nIn Table 6 we list all PI keywords used in our eval-\nuations.\nDim. Keywords\nX left, right, beside, besides, alongside, side\nY top, down, above, below, under, beneath,\nunderneath, over, beyond, overhead\nZ behind, front, rear, back, ahead, before,\nforeground, background, before, forepart,\nfar end, hindquarters\nTable 6: Overview of positional keywords regarding\ndimension.\nA.4 Pre-training Weights\nIn Table 7 we compare pre-training weights\nfrom LXMERT paper (Tan and Bansal, 2019)\nand the repository version ( https://github.\ncom/airsplay/lxmert/).\nVersion\nMLM\nCMM\nObjClassif\nAttrClassif\nFeatRegr\nVQA\nPaper 1 1 1 1 1 1\nRepository 1 1 6.6 6 .6 6 .6 1\nTable 7: Overview of pre-training weights in publica-\ntion and GitHub version.\nA.5 Text Examples\nIn Table 8 we provide examples from pre-training\nand downstream tasks with highlighted keywords.\n1040\nDataset Example Length\nMS COCO A very clean and well decorated empty bathroom 8\nA panoramic view of a kitchen and all of its appliances. 11\nSurfers waiting for the right wave to ride. 8\nTwo dogs are laying down next to each other. 9\nA red stop sign with a Bush bumper sticker under the word stop. 13\nVG separate kitchen areas in a home 6\nolder red V olkswagen Beetle car 5\na woman walking down the sidewalk 6\nA bag in the woman’sleft hand 7\nstones under wood bench 4\nGQA Are there both a television and a chair in the picture? 11\nThat car is what color? 5\nOn which side of the picture is the lamp? 9\nIs the table to the left or to the right of the appliance in the center? 16\nIs there a bookcase behind the yellow ﬂowers? 8\nTable 8: Text examples from different datasets with word counts. Italic stands for PI keywords that are wrongly\nselected and bold words are correctly detected.\nPI Input (1) (2) (3) (4) (5) (6) (7) (8) (9)\n∅ 65.0 84.1 82.1 89.9 95.6 72.3 77.7 75.3 78.4\nx, y 95.1 95.6 96.2 96.1 95.8 74.1 83.3 75.7 84.4\nx1, y1, x,2, y2 94.3 95.2 96.8 97.0 96.0 75.0 83.5 75.8 84.6\nx1, y1, x,2, y2, d 94.0 95.0 96.6 96.8 96.0 74.9 88.7 76.3 89.1\n∅ 88.1 89.4 92.6 93.5 95.9 74.1 83.9 77.7 84.8\nx, y 98.7 98.8 98.3 98.3 96.1 75.9 89.3 78.4 90.4\nx1, y1, x,2, y2 98.8 98.9 98.7 99.5 96.3 77.0 89.7 78.9 90.9\nx1, y1, x,2, y2, d 98.9 98.9 98.6 99.0 96.3 77.2 98.4 81.0 97.1\nTable 9: Average accuracy per classiﬁcation task (1-9) in Mutual Positional Evaluation for plain LXMERT (top\nlines) and our version (bottom lines).\nA.6 Mutual Positional Evaluation Details\nIn Table 9 we provide detailed results for all 9\nmutual PI tasks. Tasks (1)-(6) relate to X and Y\ncoordinates and tasks (7)-(9) to Z coordinates. The\nnumbering is explained in Section 3.\n1041",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7628868818283081
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6743674278259277
    },
    {
      "name": "Bounding overwatch",
      "score": 0.597450315952301
    },
    {
      "name": "Classifier (UML)",
      "score": 0.565596342086792
    },
    {
      "name": "Representation (politics)",
      "score": 0.4997272491455078
    },
    {
      "name": "Natural language processing",
      "score": 0.47383981943130493
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44658994674682617
    },
    {
      "name": "Matching (statistics)",
      "score": 0.44326573610305786
    },
    {
      "name": "Object (grammar)",
      "score": 0.44012778997421265
    },
    {
      "name": "Computer vision",
      "score": 0.3972618579864502
    },
    {
      "name": "Machine learning",
      "score": 0.34119588136672974
    },
    {
      "name": "Mathematics",
      "score": 0.10866329073905945
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40527276",
      "name": "Universität der Bundeswehr München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    }
  ],
  "cited_by": 6
}