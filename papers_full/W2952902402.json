{
  "title": "A Tensorized Transformer for Language Modeling",
  "url": "https://openalex.org/W2952902402",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Ma, Xindian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1800355547",
      "name": "Zhang Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1924112616",
      "name": "Zhang Shuai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2370849483",
      "name": "Duan, Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1567566553",
      "name": "Hou Yue-xian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1924050475",
      "name": "Song Da-wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102363648",
      "name": "Zhou Ming",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2901362701",
    "https://openalex.org/W2000045479",
    "https://openalex.org/W2058641082",
    "https://openalex.org/W2775020237",
    "https://openalex.org/W3104263599",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2279221249",
    "https://openalex.org/W1246381107",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W1539309091",
    "https://openalex.org/W2963689957",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W1996901117",
    "https://openalex.org/W2767321762",
    "https://openalex.org/W2000215628",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2559813832",
    "https://openalex.org/W2919207648",
    "https://openalex.org/W2410082850",
    "https://openalex.org/W2167215970",
    "https://openalex.org/W2963704562",
    "https://openalex.org/W2894175714",
    "https://openalex.org/W1963826206",
    "https://openalex.org/W2963838731",
    "https://openalex.org/W1993482030",
    "https://openalex.org/W2418388682",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W1798945469",
    "https://openalex.org/W2024165284",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2787560479"
  ],
  "abstract": "Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",
  "full_text": "A Tensorized Transformer for Language Modeling\nXindian Ma1, Peng Zhang1âˆ—, Shuai Zhang1,\nNan Duan2, Yuexian Hou1, Dawei Song3, Ming Zhou2\n1College of Intelligence and Computing, Tianjin University, Tianjin, China\n2Microsoft Research Asia, Beijing, China\n3School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China\n{xindianma, pzhang, szhang96, yxhou}@tju.edu.cn\n{nanduan, mingzhou}@microsoft.com\n{dwsong}@bit.edu.cn\nAbstract\nLatest development of neural models has connected the encoder and decoder\nthrough a self-attention mechanism. In particular, Transformer, which is solely\nbased on self-attention, has led to breakthroughs in Natural Language Processing\n(NLP) tasks. However, the multi-head attention mechanism, as a key component\nof Transformer, limits the effective deployment of the model to a resource-limited\nsetting. In this paper, based on the ideas of tensor decomposition and parameters\nsharing, we propose a novel self-attention model (namely Multi-linear attention)\nwith Block-Term Tensor Decomposition (BTD). We test and verify the proposed at-\ntention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-\nbillion) and a neural machine translation task (i.e., WMT-2016 English-German).\nMulti-linear attention can not only largely compress the model parameters but also\nobtain performance improvements, compared with a number of language modeling\napproaches, such as Transformer, Transformer-XL, and Transformer with tensor\ntrain decomposition.\n1 Introduction\nIn NLP, Neural language model pre-training has shown to be effective for improving many\ntasks [15, 29]. Transformer [37] is based solely on the attention mechanism, and dispensing with\nrecurrent and convolutional networks entirely. At present, this model has received extensive attentions\nand plays an key role in many neural language models, such as BERT [15], GPT [30] and Universal\nTransformer [13]. However, in Transformer based model, a lot of model parameters may cause prob-\nlems in training and deploying these parameters in a resource-limited setting. Thus, the compression\nof large neural pre-training language models has been an essential problem in NLP research.\nIn literature, there are some compression methods [21, 40, 17] proposed. When the vocabulary is\nlarge, the corresponding weight matrices can be enormous. Tensorized embedding (TE) [21] uses the\ntensor-train [28] to compress the embedding layers in Transformer-XL [10], but has not compressed\nthe attention layer. Recently, Block-Term Tensor Decomposition(BTD) [ 12] is used to compress\nrecurrent neural networks (RNNs) [40]. Ye et al. [40] propose a compact ï¬‚exible structure to deal\nwith the large number of model parameters instead by high dimensional inputs in training recurrent\nneural networks (RNNs). This method greatly reduces the parameters of RNNs and improves their\ntraining efï¬ciency. Still, the model only considers the input layer compression by the idea of low-rank\napproximation. On the other hand, some methods [17, 5] aim to develop a speciï¬c structure on its\nâˆ—Corresponding Author: Peng Zhang\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1906.09777v3  [cs.CL]  6 Nov 2019\nweight matrices and can reduce the parameters of the models. However, the new structure after\ncompressing can not be integrated into the model [37].\nIn Transformer, the multi-head attention is a key part and it is constructed by a large number\nof parameters. Speciï¬cally, Ashish et.al [ 37] compute the attention function on a set of queries\nsimultaneously, packed together into a matrix Q, while the keys and values are also packed together\ninto matrices Kand V, respectively. The attention function then adopts a no-linear functionsoftmax\nover two matrices Qand K. There are two challenges to ï¬nd a high-quality compression method to\ncompress the multi-head attention in Transformer.\nFirst, the self-attention function in Transformer is a non-linear function, which makes it difï¬cult\nto compress. In order to address this challenge, we ï¬rst prove that the output of the attention\nfunction of the self-attention model [ 37] can be linearly represented by a group of orthonormal\nbase vectors. Then, by initializing a low rank core tensor, we use Tucker-decomposition [35, 23] to\nreconstruct a new attention representation, where Q, Kand V can be considered as factor matrices.\nIn order to construct the multi-head mechanism and compress the model, we use the method of\nBlock-Term Tensor Decomposition (BTD), which is a combination of CP decomposition [ 6] and\nTucker decomposition [35]. The difference is that three factor matrices Q, Kand V are shared in\nconstructing each 3-order block tensor. This process can reduce many parameters.\nThe second challenge is that the attention model after compressing can not be directly integrated\ninto the encoder and decoder framework of Transformer [37, 10]. In order to address this challenge,\nthere are three steps as follows. First, the average of each block tensor can be computed; Second,\nmultiple matrices can be given by tensor split. Third, the concatenation of these matrices can serve as\nthe input to the next layer network in Transformer. After that, it can be integrated into the encoder\nand decoder framework of Transformer [37, 10] and trained end-to-end. Moreover, we also prove\nthat the 3-order tensor can reconstruct the scaled dot-product attention in Transformer by a sum on a\nparticular dimension.\nOur method combines two ideas which are the low-rank approximation and parameters sharing at\nthe same time. Therefore, it achieves the higher compression ratios. Although the self-attention (i.e.,\nscaled dot-product attention) in Transformer can be reconstructed, we do not consider reconstructing\nit and choose to split the 3-order tensor (the output of Multi-linear attention) which is helpful for\nimproving the accuracy in experiments.\nOur major contributions of this paper are as follows:\n1) It is proved that the output of scaled dot-product attention (considering as a function) can be\nlinearly represented by a group of orthonormal base vectors.\n2) A novel self-attention method, namely Multi-linear attention, is provided, which combines\ntwo compression ideas, parameters sharing and low-rank approximation, together.\n3) Multi-linear attention builds the strong connection between three factor matrices (pack a\nset of queries, keys and values, respectively ), enhancing the ability of capturing sufï¬cient\nattention information. We also prove our model can reconstruct the scaled dot-product\nattention in the original Transformer.\nIn order to validate the beneï¬ts of our model, we test it on two NLP tasks, namely language modeling\nand neural machine translation. In our experiments, the multi-head attention can be replaced by\nthe proposed model, namely multi-linear attention. We have observed that the standard Multi-head\nattention can be compressed with higher compression ratios on One-Billion dataset. As a result, we\nshow that multi-linear attention not only considerably reduces the number of parameters, but also\nachieve promising experiments results, especially in language modeling tasks.\n2 Preliminaries\nMulti-linear attention is carried out in this paper. The analysis of Multi-linear attention relies on these\nconcepts and results from the ï¬eld of tensor decomositon and multi-head attention. We cover below\nin Section 2.1 basic background on Block-Term tensor decomposition [12]. Then, we describe in\nSection 2.2 multi-head attention [37].\n2\nâ‰ˆ\n +\n +â‹¯\nğ’œğ’œ\nğ‘‘ğ‘‘1\nğ‘‘ğ‘‘2\nğ‘‘ğ‘‘3\nğ’³ğ’³1\n(1) ğ’³ğ’³ğ‘ƒğ‘ƒ\n(1)\nğ’³ğ’³1\n(3)\nğ’³ğ’³1\n(2)\nğ’³ğ’³ğ‘ƒğ‘ƒ\n(2)\nğ’³ğ’³ğ‘ƒğ‘ƒ\n(3)\nğ’¢ğ’¢1 ğ’¢ğ’¢ğ‘ƒğ‘ƒ\nğ‘‘ğ‘‘1 ğ‘‘ğ‘‘2\nğ‘‘ğ‘‘3\nğ‘…ğ‘…1\nğ‘…ğ‘…3\nğ‘…ğ‘…2\nğ‘‘ğ‘‘1\nğ‘…ğ‘…1\nğ‘‘ğ‘‘2\nğ‘…ğ‘…2\nğ‘…ğ‘…3\nğ‘‘ğ‘‘3\nï¿½1 ï¿½2\nï¿½3\nï¿½1 ï¿½2\nï¿½3\nFigure 1: The representation of Block-Term tensor decomposition for a 3-order tensor. A âˆˆ\nRd1Ã—d2Ã—d3 is a 3-order tensor, and can be approximated by P Tucker decomposition. P is the CP\nrank, and R1,R2,R3 are the Tucker rank, respectively. In this paper, we assume that R=R1=R2=R3.\n2.1 Tensor and Block-Term Tensor Decomposition\nTensor We use the Euler script letter Ato denote a tensor which can be thought of as a multi-array.\nThereby a vector and a matrix are a 1-order tensor and 2-order tensor, respectively. The element in a\nn-order tensor is denoted as Ad1,...,dn. In the geometric representation of a tensor, 3-order tensor can\nbe represented by a cube. After that, there is a related concept named tensorslice that will be used\nin this paper. Tensor and some other related concepts are showed in Supplementary Materials A.\nBlock-Term Tensor Decomposition (BTD)Block-Term tensor decomposition is a combination of\nCP decomposition [6] and Tucker decomposition [35]. Given a n-order tensor Aâˆˆ Rd1Ã—...Ã—dn. A\nhigh-order tensor can be decomposed into P block terms by the method named BTD. â€¢z is denoted as\nthe tenor-tensor product on the z-thorder [22] and zâˆˆ{1,...,d }. Each term contains â€¢z between a\ncore tensor Gi âˆˆRR1Ã—...Ã—Rd and dfactor matrices X(k)\ni âˆˆRdkÃ—Rk , where iâˆˆ[1,P] and kâˆˆ[1,d].\nThe formulation of BTD decomposition is as follows:\nA=\nPâˆ‘\ni=1\nGiâ€¢1X(1)\ni â€¢2X2\ni â€¢3 ... â€¢dX(d)\ni (1)\nwhere P is the CP rank, and d is the Core-order. In our work, the tensor is 3-order. Figure 1\ndemonstrates the example of how a 3-order tensor Acan be decomposed into P block terms.\n2.2 Multi-head Attention\nIn Transformer, the attention function is named as â€œScaled Dot-Product Attentionâ€. In practice,\nTransformer [37] processes query, keys and values as matrices Q, K, and V respectively. The\nattention function can be written as follows:\nAttention(Q,K,V ) =softmax(QKT\nâˆš\nd\n)V (2)\nwhere dis the number of columns of Qand K. In these work [37, 15, 10], they all use the multi-head\nattention, as introduced in [37],\nMultiHeadAttention(Q,K,V ) =Concat(head1,...,head k)WO\nwhereheadi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\n(3)\nwhere matrices WQ\ni and WK\ni âˆˆRdmodelÃ—d, WV\ni âˆˆRdmodelÃ—d and WO âˆˆRhdvÃ—dmodel. In practice,\ndv is equal to d. In this work [ 37], multiple groups of parameters ( WQ\ni , WK\ni and WV\ni ) are used,\nwhich results in a large number of redundant parameters.\n3 Tensorized Transformer\nIn this section, we ï¬rst build a Single-block attention in Figure 2 (left) based on the Tucker decompo-\nsition, a low-rank decomposition method. In this process, we prove that the self-attention function in\nTransformer can be represented by a linear function, i.e., a linear combination representation of a set\nof basic vectors.\n3\nğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ\nâ‹¯\n+ + +â‹¯\nâ„\nğ‘ ğ‘ ğ‘œğ‘œğ‘ ğ‘ ğ‘ ğ‘ ğ‘œğ‘œ ğ‘ğ‘ğ‘œğ‘œğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘œğ‘œ\nâ‹¯\nğ‘”ğ‘”1 ğ‘”ğ‘”2 ğ‘”ğ‘”â„\nğ‘ğ‘\nğ‘ğ‘\nğ‘ğ‘\nğ‘ğ‘\nğ‘ğ‘\nğ‘ğ‘\nâˆ—(1\nâ„)\nğ‘„ğ‘„\n ğ¾ğ¾\n ğ‘‰ğ‘‰\nLinear\n Linear\n Linear\nparameters \nsharing\nğ‘„ğ‘„ ğ¾ğ¾\nğ‘‰ğ‘‰\nğ‘‘ğ‘‘\nğ‘‘ğ‘‘\nğ‘…ğ‘…\nğ’¢ğ’¢\nğ‘…ğ‘…\nğ‘…ğ‘…\nğ‘…ğ‘…\nğ‘‡ğ‘‡1\nğ‘‡ğ‘‡2\nğ‘‡ğ‘‡ğ‘›ğ‘›\nğ‘Šğ‘Šğ‘‚ğ‘‚\nğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ\nğ‘ğ‘\nğ‘ğ‘\nğ‘‘ğ‘‘\nğ‘ğ‘\nFigure 2: (left) Single-block attention using Tucker decomposition. (right) Multi-linear attention\nbased on Block-Term tensor decomposition.\nIn order to compress the multi-head mechanism, we propose a multi-linear attention constructed by a\nBlock-Term tensor decomposition. This attention uses the idea of parameters sharing, i.e., sharing\nfactor matrices across multiple blocks, shown in Figure 2 (right). After that, the compression ratios\nand relatively lower complexity have been analyzed.\n3.1 Single-block Attention by Tucker Decomposition\nBefore building the Single-block attention, it is necessary to propose the theorem 3.1. The theorem is\nclosely related to attributes of Single-block attention function by Tucker decomposition [35].\nTheorem 3.1. Let e1,..., en be basis vectors from the vector space S. Assume that these vectors\ne1,..., en are linear independent and Q,K,V can be linearly represented by this set of basis vectors.\nThe output of the attention function in Eq. 2 can be represented by a linear combination of the set of\nthese basis vectors.\nAttention(Q,K,V ) = (e1,..., en)M, (4)\nwhere M âˆˆRnÃ—d is a coefï¬cient matrix, and dis a dimension of these matrices (i.e., Q, K, and V).\nProof. The proof can be found in Supplementary Materials B.\nIn Figure 2 (left), it is a schematic diagram about the Single-block attention. First, we assume that the\nquery, key and value can be mapped into three factor matrices of which are composed of three groups\nof orthogonal basis vectors. Three factor matrices are Q, K and V. After that, we can construct\na new attention (i.e., Single-block attention) by initializing a 3-order diagonal tensor (trainable)\nwhich is the G. In Figure 2 (left), Ris the rank about the tensor, N is the length of a sequence, and\ndis the dimension of matrix. The function of Single-block attention can be computed based on\nTucker-decomposition as follows:\nAttenTD (G; Q,K,V ) =Gâ€¢1Qâ€¢2Kâ€¢3V\n=\nIâˆ‘\ni=1\nJâˆ‘\nj=1\nMâˆ‘\nm=1\nGijmQi â—¦Kj â—¦Vm\n(5)\nwhere Gis a core tensor. i,j and mare the indexes of the core tensor. â—¦is the outer product. â€¢z is\nthe same deï¬nition in Eq. 1. Qi,Kj and Vk are column vectors from matrices Q,K and V, where\nQ âˆˆRNÃ—d, K âˆˆRNÃ—d and V âˆˆRNÃ—d, and N is the length of a sequence. In practice, we set\nI=J=M=R. The core tensor Gcan be deï¬ned as follows,\nGijm =\n{\nrand(0,1) i= j = m\n0 otherwise (6)\n4\nwhere the rand(0,1) is a random function, and the diagonal entries of core tensorGform the vector g.\nEach entry gr âˆˆ(0,1), râˆˆ{1,...,R }. We can consider g as the trainable weight. In experiments,\nwe compute the weight vector by softmax function (i.e., softmax(g)).\nAfter that, the output of Single-block attention function is a 3-order tensor which is given by linear\ncomputation. The Single-block attention (i.e., a 3-order tensor with Tucker decomposition) can\nreconstruct the Scaled Dot-Product attention in Eq. 2 by the summing over the tensor according to\nthe second index 2 (it can be seen as the coordinates in the vertical direction for a tensor), as proved\nin the following corollary. Note that in our model, we do not adopt the above reconstructing process.\nInstead, to obtain a new representation, we adopt the concat method after the tensor splitting (see\nSec. 3.2). We will further show the compression ability of the Single-block attention in Sec. 3.3.\nCorollary 1. Under the same conditions as in Theorem 3.1 and the value of N is equal to the value\nof d, Single-block attention representation Eq. 5 can reconstruct the Scaled Dot-Product attention in\nEq. 2 by the summing over the tensor (i.e., the output of Single-block attention function) according to\nthe second index. It holds that:\nAttention(Q,K,V )i,m =\nNâˆ‘\nj=1\nAttenTD (G; Q,K,V )i,j,m (7)\nwhere i, j and m are the indices of the Single-block attentionâ€™s output (i.e., a 3-order tensor).\nAttenTD (Â·) is the function of Single-block attention based on Tucker decomposition. iand mare\nthe indices of outputs (i.e., a matrix) from Eq. 2.\nProof. The proof can be found in Supplementary Materials C.\n3.2 Multi-Linear Attention by Block-Term Tensor Decomposition\nIn order to construct the multi-head mechanism and compress the parameters of multiple groups\nof mapping, we use a group of linear projections, and share the output from the linear projections.\nIn Figure 2(right), the learned linear projection can map queries, keys and values to three matrices\nwhich are composed of basis vectors. After that, we use the Block-Term tensor decomposition to\nbuild multi-head mechanism. In our work, our model is named as Multi-linear attention, which can\nbe formulated as follows:\nMultiLinear(G; Q,K,V ) =SplitConcat( 1\nh âˆ—(T1 + ... + Th))WO\nwhere Tj = AttenTD (Gj; QWq,KW k,VW v)\n(8)\nwhere the core tensor Gj is a diagonal tensor, and the number of parameter in Gj is equal to the rank\nof core tensor, j âˆˆ{1,...,h }. Gis the set of the core tensors. SplitConcat(Â·) is a function which\nachieves the concatenation after splitting for a 3-order tensor. Figure 2 (right) shows the basis idea\nabout the multi-linear attention. The WO is the parameter matrix which is a full connection layer\nand correlated to the output of Multi-linear attention. AttenTD (Â·) is the function of Single-block\nattention, which is a part of Multi-linear attention. Wq, WK and Wv are the parameters matrices\nwhich are shared in constructing Multi-linear attention.\nThe Multi-linear attention is a compression model. After compressing the multi-head attention in\nTransformer, it is to achieve a Tensorized Transformer. The Multi-linear attention can be incorporated\ninto Transformer architecture. A diagram which is about the incorporating of Multi-linear attention\nin partial Transformer structure is given in Supplementary Materials E.1.\n3.3 Analysis of Compression and Complexity\nCompression Our focus is on the compression of the multi-head mechanism in the multi-head\nattention of Transformer. Previous work [ 37] gets the multi-head attention by multiple groups of\nlinear mappings. We use three linear mappings for matrices Q, K and V, respectively. For the\noutput of three mappings, we choose to share them which are considered as three factor matrices in\nreconstructing the Multi-linear attention. This process is shown in Figure 2 (left). his the number of\nheads in [37], and dis the dimension of factor matrices. The compression ratios can be computed\n2If the coordinates of a 3-order tensor are i, jand m, jis the second index.\n5\nby (3 Ã—hÃ—d)/(3 Ã—d+ h). In practice, his normally set to 8, dis set to 512. In this case, the\ncompression ratios can achieve 8. In other words, we can reduce almost 8 times parameters in the\nattention layer. The details of the computing of compression ratios can be found in Supplementary\nMaterials D. The Transformer also contains other network layers, such as Position-wise feed forward\nnetwork and embedding layers et al. Therefore, for the compression ratios in whole Transformer, we\ncan compare it by the analysis of experimental results for model parameters.\nComplexity The time complexity of the attention function in Eq. 2 is O(N2d), N is the length of\na sequence, and dis the representation dimension. In Multi-linear attention, we can reorder the\ncomputations to receive the model complexity O(N3), where N is also the length of the sequence.\nThe minimum number of sequential operations in Multi-linear attention for different layers is\napproximately equal to the self-attention in Transformer [37].\n4 Related Work\nThe ï¬eld of language modeling has witnessed many signiï¬cant advances. Different from the archi-\ntectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language\nmodeling, the Transformer [37] and its variants [10, 15, 13] achieve excellent results in language\nmodeling processing. Transformer networks have a potential of learning long-term dependency, but\nare limited by a ï¬xed-length context in the setting of language modeling. Vaswani et al. [37] uses a\nsegment-level recurrence mechanism and a novel positional encoding scheme to resolve this question.\nBERT [15] is a kind of bidirectional encoder representations from transformers. It is designed to\npre-train deep bidirectional representation and obtains new SoTA on some NLP tasks. Although these\nmethods have achieved great results, a large number of parameters make it difï¬cult for the model to\nbe trained in limited resources. Transformer fails to generalize in many simple tasks, e.g. copying\nstring and logical inference [ 13]. Universal Transformers [ 13] propose a self-attentive recurrent\nsequence model which addresses this problem. This methods can increase the training speed. In\ntheir work, authors following weight sharing found in CNNs and RNNs, extend the Transformer\nwith a simple form of weight sharing that strikes an effective balance between induces and model\nexpressivity. This methods also uses a large number of parameters.\nTherefore, it is very important to consider how to reduce the amount of memory and computing they\nneed. As we know, existing model compression methods are mainly divided into parameter pruning\nand sharing [17], low rank approximation [31], knowledge transfer [5], and transferred convolutional\nï¬lters [9]. Currently, tensor decomposition methods are used to decompose a high-order tensor, which\ncan get different neural network language model structures [3, 1]. Besides, tensor decomposition\nmethods which adopts the idea of low rank approximation in most cases, have been successfully\napplied to neural networks compression. For example, in literature [14, 19], researchers approximate\na tensor by minimizing the reconstruction error of the original parameters on convolutional neural\nnetworks (CNNs). However, these approaches tend to accumulate errors when multiple layers are\ncompressed sequentially, and the output feature maps deviate far from the original values with the\nincrease of compressed layers. Our compression method uses the idea of parameters sharing in the\nconstructing of attention layers, and the size of output is same as the output from self-attention in\nTransformer which can effectively avoid these problems. Tensorizing Neural Networks [27] have\ncombined the idea of reshaping weights of fully-connected layers into high-dimensional tensors and\nrepresenting them in Tensor Train format [28]. This approach was later extended to convolutional [16]\nand recurrent neural networks [38]. Recently, in these work [8, 36], researchers introduce efï¬cient\ncompression methods for the embedding and softmax layers based on structured low rank matrix\napproximation. TT-embedding [21] aims to compression the larger embedding layer on Transformer-\nXL [10]. Sparse Transformer [2] adopts sparse techniques on the attention matrix and reduces its\nparameters. This work uses a sparse attention matrix by selecting the information on some positions\nin the attention matrix, but does not change the mechanism of the attention. Our method is different\nfrom these works, and combines two compression idea (low rank approximate and parameters sharing)\nto construct a tensorized Transformer.\nIn our work, we focus on the compression the multi-head attention in Transformer based the idea\nof parameters sharing. At the same time, we also combine low-rank approximate method to reduce\nparameters and computation complexity.\n6\n5 Experiments\nTransformer is a versatile and powerful modeling tool and widely is used in various natural language\nprocess tasks. In order to verify the effectiveness of our method (i.e., Multi-linear attention) replacing\nmulti-head attention in Transformer, we carry out two NLP tasks named language modeling (LM)\nand neural machine translation (NMT). Code3 for running experiments has been released, and the\nkey code which is about our method can be found in Supplementary Materials F.\n5.1 Language Modeling\nLanguage modeling is the task of predicting the next word in a sentence. This task is to estimate\nthe joint probability p(s) of a sentence of tokens s=(w1,...,w n). The resulting models can be\nused to generate text or further ï¬ne-tuned to solve other NLP tasks [30]. In this paper, we employ\nthe standard setting of predicting next token given the sequence of preceding tokens, based on the\nfunction p(s) =p(w1) âˆn\ni=2 p(wi|w1,...,w iâˆ’1). We chose three datasets in the order of small (i.e.,\nPTB), medium (i.e., WikiText-103) and large (i.e., One-Billion). Models are evaluated based on\nPerplexity (PPL), which is the average per-word log-probability. The lower the PPL, the better the\nmodel is.\nSpecially, we take Transformer, the open source state-of-the art language modeling architecture, and\nreplace the standard multi-head attention layers with our Multi-linear attention. Then, we test different\nmodel conï¬gurations on the PTB [ 26], WikiText-103 [25] and One-Billion Word benchmark [ 7]\ndatasets and report the results in Table 1 and Table 2.\nTable 1: Results (PPL) and model parameters with state-of-the-art results on One-Billion. Tensorized\nTransformer is our model. The core-1 is that the model use Single-block term tensor. Analogously,\nthe core-2 is that two block term tensor is used.\nModel Params Test PPL\nRNN-1024+9 Gram [7] 20B 51.3\nLSTM-2018-512 [20] 0.83B 43.7\nGCNN-14 bottleneck [11] â€“ 31.9\nLSTM-8192-1024+CNN Input [20] 1.04B 30.0\nHigh-Budget MoE [34] 5B 28.0\nLSTM+Mos [39] 113M 37.10\nTransformer+adaptive input [4] 0.46B 23.7\nTransformer-XL Base [10] 0.46B 23.5\nTransformer-XL Large [10] 0.8B 21.8\nTensorized Transformer core-1 0.16B 20.5\nTensorized Transformer core-2 0.16B 19.5\n5.2 Results and Details\nPTB has 929ktraining tokens, 73kvalidation words, and 82ktest words. The results is reported in\nTable 2. Similar to AWD-LSTM-MoS [39], we apply variational dropout and weight average to our\nmodel (i.e., Tensorized Transformer). In addition, we need to state that, our model only replaces the\nmulti-head attention using Multi-linear attention structure, and the other structures remain the same.\nWe compare the results of our model with other models. Our model achieves the comparable results\nwith SoTA when the number of core tensor is equal to two. However, our model size (i.e, model\nparameters) reduces by nearly half comparing with Transformer and Transformer-XL.\nWikiText-103 contains 267,735 unique tokens. The dataset is available word-level language modeling\nbenchmark with long-term dependency. It contains 103M training tokens from 28karticles, with an\naverage length of 3.6k tokens per article, which allows testing the ability of long-term dependency\nmodeling. As shown in Table 2, our model get the perplexity of 18.9, which is a comparable\nexperimental result with the previous SoTA perplexity 18.3 , which demonstrates the effectiveness of\nthe proposed attention architecture.\n3https://github.com/szhangtju/The-compression-of-Transformer\n7\nModel PTB WikiText-103\nParams Val PPL Test PPL Params Val PPL Test PPL\nLSTM+augmented loss [18] 24M 75.7 48.7 â€“ â€“ 48.7\nVariational RHN [41] 23M 67.9 65.4 â€“ â€“ 45.2\n4-layer QRNN [24] â€“ â€“ â€“ 151M â€“ 33.0\nAWD-LSTM-MoS [39] 22M 58.08 55.97 â€“ 29.0 29.2\nTransformer+adaptive input [4] 24M 59.1 57 247M 19.8 20.5\nTransformer-XL-Base [10] 24M 56.72 54.52 151M 23.1 24.0\nTransformer-XL-Large [10] â€“ â€“ â€“ 257M â€“ 18.3\nTransformer-XL+TT [21] 18 M 57.9* 55.4* 130M 23.61* 25.70*\nSparse Transformer [2] 14M 74.0* 73.1* 174M 38.98* 40.23*\nTensorized Transformer core-1 12M 60.5 57.9 85.3M 22.7 20.9\nTensorized Transformer core-2 12M 54.25 49.8 85.3M 19.7 18.9\nTable 2: Results and compression with state-of-the-art results on PTB and WikiText-103. â€™ âˆ’â€™\nindicates no reported results in that setting, â€™âˆ—â€™ indicates that the results is our own implementation.\nThe One-Billion Word benchmark is a large dataset derived from a news site. The dataset consists\nof 829,250,940 tokens over a vocabulary of 793,471 words. In this dataset, sentences are shufï¬‚ed\nand hence the context is limited. Consequently, this dataset mainly tests the ability of modeling only\nshort-term dependency. The comparison between Tensorized Transformer and the other methods\nare shown in Table 1. Although Tensorized Transformer is mainly designed to better compress\nTransformer or Transformer-XL model, it dramatically improves the single-model SoTA from21.8\nto 19.5. Speciï¬cally, Tensorized Transformer signiï¬cantly outperforms a contemporary method\nusing vanilla Transformers [37], suggesting that the advantage of the tensorized Transformer is also\ngeneralizable to modeling short sequences.\nTable 2 and Table 1 show that our model get the lower PPL than other models in three datasets.\nAn exciting observation is that our model has much fewer parameters. The model of Transformer-\nXL+TT [21] is a recent compression model with Tensor Train to compress the input embedding layers\nonly. Sparse Transformer [2] uses the method of sparse attention matrix to compress Transformer\nmodel. The results in Table 2 show that compared with Transformer-XL+TT, our method has much\nfewer parameters, and better language modeling performance. These results verify that our model\n(i.e., Multi-linear attention) is effective in language modeling tasks, and has performed well for\nthe model compression. Other details (such as hyperparameters and Hardware) can be found in\nSupplementary Materials E.\n5.3 Neural Machine Translation\nThe goal is to map an input sequence s= (x1,x2,...,x n) representing a phrase in one language, to\nan output sequence y = (y1,y2,...,y m) representing the same phrase in a different language. In\nthis task, we have trained the Transformer model [37] on WMT 2016 English-German dataset [33].\nSentences were tokenized using the SentencePiece 4. For our experiments, we have replaced each\nof the attention layers with Multi-linear attention in Encoder. For evaluation we used beam search\nwith a beam size of 5 and length penalty Î±=0.6. In this section, we only compared the results with\nTransformer [37]. Our results are summarized in Table 3. âˆ—indicates that the result is our own\nimplementation.\nIn Table 3, we select two baseline models. The Base-line [33] is ï¬rst model in WMT 2016 English-\nGerman dataset. For the other baseline, we use the basic Transformer architecture [37]. The BLEU\nscore is 34.5 for the basic architecture. We carry out two Tensorized Transformer structures, namely\ncore-1 and core-2 respectively. When Tensorized Transformer core-1 and core-2 are used, the BLEU\nscores are 34.10 and 34.91, which achieves better performance over Transformer. As for the reported\nmodel parameter size, our model uses less parameters.\n4https://github.com/google/sentencepiece\n8\nTable 3: Results and compression with Transformer on WMT-16 English-to-German translation.\nModel Params BLEU\nBase-line [33] â€“ 26.8\nLinguistic Input Featurec [32] â€“ 28.4\nAttentional encoder-decoder + BPE [33] â€“ 34.2\nTransformer [37] 52M 34.5*\nTensorized Transformer core-1 21M 34.10\nTensorized Transformer core-2 21.2M 34.91\n5.4 Discussion\nWe have shown the results on language modeling and neural machine translation tasks using the Multi-\nlinear attention. For the compression of the model parameters, although we report the parameters of\nthe whole model structure, our method mainly considers the compression of multi-head attention\nbut has not changed other layers in Transformer. Regarding the rationale for the improvements, in\nCorollary 1, we prove that the output of the original attention can be represented by summing over the\n3-order tensor. In Figure 2, we use a concat function over these matrices from tensor splitting. The\noperation of concat can model all values in the 3-order tensor, and thus captures more information\nthan sum operator. Another reason could be the alleviation of overï¬tting by reducing parameters. The\noverï¬tting will appear when the number of the core tensor is greater than 2. Besides, according to\nour experiments, relatively large dimensions of the word embedding can lead to overï¬tting, resulting\nin performance degradation. Therefore, our model requires a relatively small dimension of the\nembedding, compared with the original Transformer. In order for a more systematic evaluation, we\nreport more experiments and analyses in Supplementary Materials E.4.\n6 Conclusion and Further Work\nWe have proposed a novel self attention encoder layer, namely the Multi-linear attention, to compress\nthe original multi-head attention and derive a novel encoding scheme. Our main contribution lies in a\nstructure of Tensorized Transformer based on Block-Term tensor decomposition which is represented\nby the combination of a group of 3-order tensors, with low-rank approximation and parameters\nsharing ideas adopted. Compared with existing Transformer based methods, our model achieved\nhigher compression ratio and got better experimental results, particularly in language modeling task.\nThese evidences imply that our method can potentially be further applied to more NLP tasks with\nlimited resources.\nIn the future, we will continue to optimize the Tensorized Transformer framework and apply it in\nother NLP tasks. As we stated earlier, our model may suffer from overï¬tting when the number of\ncores is large in language modeling. In the future, we will explore the fundamental reasons that cause\nthe problem and tackle them within the Tensorized Transformer framework.\n7 Acknowledgement\nThis work is supported in part by the state key development program of China (grant No.\n2017YFE0111900, 2018YFC0831704), Natural Science Foundation of China (grant No. 61772363,\nU1636203), and the European Unions Horizon 2020 research and innovation programme under the\nMarie SkodowskaCurie grant agreement No.721321.\nReferences\n[1] Peng Zhang, Zhan Su, Lipeng Zhang, Benyou Wang, and Dawei Song. A quantum many-body wave\nfunction inspired language modeling approach. In Proceedings of the 27th ACM International Conference\non Information and Knowledge Management, pages 1303â€“1312. ACM, 2018.\n[2] Alec Radford Rewon Child, Scott Gray and Ilya Sutskever. Generating long sequences with sparse\ntransformer. arXiv preprint arXiv:1904.10509, 2019.\n9\n[3] Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, and Dawei Song. A generalized language\nmodel in tensor space. arXiv preprint arXiv:1901.11167, 2019.\n[4] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv\npreprint arXiv:1809.10853, 2018.\n[5] Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the\n12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535â€“541.\nACM, 2006.\n[6] J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling via an\nn-way generalization of â€œeckart-youngâ€ decomposition. Psychometrika, 35(3):283â€“319, 1970.\n[7] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling. Computer\nScience, 2013.\n[8] Patrick Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: Block-wise low-rank\napproximation for neural language model shrinking. In Advances in Neural Information Processing\nSystems, pages 10988â€“10998, 2018.\n[9] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on\nmachine learning, pages 2990â€“2999, 2016.\n[10] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ï¬xed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[11] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pages 933â€“941. JMLR. org, 2017.\n[12] Lieven De Lathauwer. Decompositions of a higher-order tensor in block termsâ€”part ii: Deï¬nitions and\nuniqueness. SIAM Journal on Matrix Analysis and Applications, 30(3):1033â€“1066, 2008.\n[13] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Åukasz Kaiser. Universal\ntransformers. Published at ICLR2019, 2018.\n[14] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure\nwithin convolutional networks for efï¬cient evaluation. In Advances in neural information processing\nsystems, pages 1269â€“1277, 2014.\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. 2018.\n[16] Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate tensorization:\ncompressing convolutional and fc layers alike. arXiv preprint arXiv:1611.03214, 2016.\n[17] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efï¬cient\nneural network. In Advances in neural information processing systems, pages 1135â€“1143, 2015.\n[18] Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiï¬ers: A loss\nframework for language modeling. arXiv preprint arXiv:1611.01462, 2016.\n[19] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with\nlow rank expansions. In Proceedings of the British Machine Vision Conference. BMVA Press, 2014.\n[20] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of\nlanguage modeling. arXiv preprint arXiv:1602.02410, 2016.\n[21] Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mirvakhabova, and Ivan Oseledets. Tensorized embedding\nlayers for efï¬cient model compression. arXiv preprint arXiv:1901.10787, 2019.\n[22] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications.SIAM review, 51(3):455â€“500,\n2009.\n[23] Guangxi Li, Jinmian Ye, Haiqin Yang, Di Chen, Shuicheng Yan, and Zenglin Xu. Bt-nets: simplifying\ndeep neural networks via block term decomposition. arXiv preprint arXiv:1712.05689, 2017.\n10\n[24] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling at\nmultiple scales. arXiv preprint arXiv:1803.08240, 2018.\n[25] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843, 2016.\n[26] TomÃ¡Å¡ Mikolov, Anoop Deoras, Stefan Kombrink, LukÃ¡Å¡ Burget, and JanË‡Cernock`y. Empirical evaluation\nand combination of advanced language modeling techniques. In Twelfth Annual Conference of the\nInternational Speech Communication Association, 2011.\n[27] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks.\nIn Advances in neural information processing systems, pages 442â€“450, 2015.\n[28] Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientiï¬c Computing, 33(5):2295â€“2317,\n2011.\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 2227â€“2237, 2018.\n[30] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language understanding paper. pdf, 2018.\n[31] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank\nmatrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE\ninternational conference on acoustics, speech and signal processing, pages 6655â€“6659. IEEE, 2013.\n[32] Rico Sennrich and Barry Haddow. Linguistic input features improve neural machine translation. arXiv\npreprint arXiv:1606.02892, 2016.\n[33] Rico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems for\nwmt 16. arXiv preprint arXiv:1606.02891, 2016.\n[34] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017.\n[35] Ledyard R Tucker. Some mathematical notes on three-mode factor analysis.Psychometrika, 31(3):279â€“311,\n1966.\n[36] Ehsan Variani, Ananda Theertha Suresh, and Mitchel Weintraub. West: Word encoded sequence transducers.\narXiv preprint arXiv:1811.08417, 2018.\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\nsystems, pages 5998â€“6008, 2017.\n[38] Yinchong Yang, Denis Krompass, and V olker Tresp. Tensor-train recurrent neural networks for video\nclassiï¬cation. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages\n3891â€“3900. JMLR. org, 2017.\n[39] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck:\nA high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.\n[40] Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe, Xinqi Chu, and Zenglin Xu. Learning\ncompact recurrent neural networks with block-term tensor decomposition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 9378â€“9387, 2018.\n[41] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint\narXiv:1611.01578, 2016.\n[42] Andrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari. Nonnegative matrix and tensor\nfactorizations: applications to exploratory multi-way data analysis and blind source separation . John\nWiley & Sons, 2009.\n11\nA Tensor and Tensor Slice\nAs introduced in [42], a tensor and the tensor slice can be deï¬ned as follows.\nDeï¬nition 1 (tensor). Let D1, D2, ... , DN âˆˆN denote index upper bounds. A tensor Aâˆˆ RD1,...,DN of order\nN is an N-way array where elements Ad1,d2,...,dn are indexed by dn âˆˆ{1,2,...,D n}for 1 â‰¤nâ‰¤N.\nThe concept of tensor slice is speciï¬ed as:\nDeï¬nition 2 (tensor slice). A tensor slice is a two-dimensional section (fragment) of a tensor, obtained by ï¬xing\nall indexes except for two indexes.\nB Theorem 3.1\nLet e1,..., en be basis vectors from the vector space S. Assume that these vectors e1,..., en are linear\nindependent and Q,K,V can be linearly represented by this set of basis vectors. The output of self-attention\nfunction in Eq. 2 (in the paper) can be represented by a linear combination of the set of these basis vectors.\nAttention(Q,K,V ) = (e1,..., en)M, (9)\nwhere M âˆˆRnÃ—d is a coefï¬cient matrix, and dis a dimension of these matrices (i.e., Q, K, and V).\nProof. If Q, Kand V âˆˆSpan(e1,..., en), the linear combination representation of matrices Q,Kand V can\nbe written as follows: ï£±\nï£²\nï£³\nQ= (e1,e2,..., en) (Î±1,Î±2,..., Î±d)\nK = (e1,e2,..., en) (Î²1,Î²2,..., Î²d)\nV = (e1,e2,..., en) (Î¾1,Î¾2,..., Î¾d)\n(10)\nThe self-attention function is written as follows [37]:\nAttention(Q,K,V ) = softmax(QKT\nâˆš\nd\n)V, (11)\nwhere QKT can be computed as follows:\nQKT = (e1,e2,..., en) (Î±1,Î±2,..., Î±d)(Î²1,Î²2,..., Î²d)T (e1,e2,..., en)T (12)\nAs a result, the input ofsoftmaxfunction is a product of coefï¬cient matrices(Î±1,..., Î±d) and (Î²1,..., Î²d)T .\nThen, we have\nsoftmax(QKT\nâˆš\nd\n) = (e1,..., en)softmax(A/\nâˆš\nd)(e1,..., en)T (13)\nwhere the matrix Ais equal to (Î±1,..., Î±d)(Î²1,..., Î²d)T . Therefore, the attention representation can be\nwritten as follows:\nsoftmax(QKT\nâˆš\nd\n)V = (e1,e2,..., en) softmax(A/\nâˆš\nd)(Î¾1,Î¾2,..., Î¾d)\n= (e1,e2,..., en) M\n(14)\nwhere the matrix M is equal to softmax(A/\nâˆš\nd)(Î¾1,Î¾2,..., Î¾d). The softmax(A/\nâˆš\nd) is to normalize the\ncoefï¬cient matrices of Qand K. It turns out that the output of the attention function [37] can be represented by\na linear combination of the set of basic vectors.\nAfter the proof, it is helpful to describe the basic idea. First, we consider that the self-attention function can be\nlinearly represented by a set of orthogonal basis vectors, when the input of softmax function is the product of\ntwo coefï¬cient matrices, (Î±1,Î±2,..., Î±d) and (Î²1,Î²2,..., Î²d)T , respectively. Second, in constructing the\nmulti-head mechanism, the matrices of basis vectors (e1,e2,..., en) can be shared.\nC Corollary 1\nUnder the same conditions as in Theorem 3.1 and the value of N is equal to the value of d, the Single-block\nattention representation Eq. 5 (in the paper) can reconstruct the Scaled Dot-Product attention in Eq. 2 (in the\npaper) by the summing over the tensor (i.e., the output of Single-block attention function) according to the\nsecond index. It holds that:\nAttention(Q,K,V )i,m =\nNâˆ‘\nj=1\nAttenTD (G; Q,K,V )i,j,m, (15)\n12\nğ‘–ğ‘–\nğ‘—ğ‘—\nğ‘˜ğ‘˜\nğ’œğ’œğ‘–ğ‘–,ğ‘—ğ‘—,ğ‘˜ğ‘˜\nâŸ¹\n+ + +\nğ’œğ’œğ‘–ğ‘–,1,ğ‘˜ğ‘˜ ğ’œğ’œğ‘–ğ‘–,2,ğ‘˜ğ‘˜ ğ’œğ’œğ‘–ğ‘–,ğ‘ğ‘,ğ‘˜ğ‘˜\nğ‘‹ğ‘‹ğ‘–ğ‘–,ğ‘˜ğ‘˜\nFigure 3: Tensor Ais a 3-order tensor, which represents the Single-block attention in the left. Ai,j,k\nis the entry of the tensor A. In the right, the graph represents that the summing of tensor slices which\nis from the tensor splitting in index j. This graph can help us to understand the main content of\ncorollary 1.\nwhere i, jand mare the indices of the Single-block attention output (i.e., a 3-order tensor). AttenTD (Â·) is the\nfunction of the Single-block attention based on Tucker decomposition. iand mare the indices of outputs (i.e., a\nmatrix) from Eq. 2 (in the paper).\nProof. In Theorem 3.1, we have proved the results about the attention function can be represented by a linear\ncombination of basis vectors. Therefore, we can represent the self-attention function in Eq. 2 (in the paper) by\nthe form as follows:\nAttention(Q,K,V ) = Î˜QKT V (16)\nwhere Î˜ is a normalization factor matrix, which can be used to replace the use of a sofmax function. We\nassume that Î˜ contains all the non-zero elements of the core tensor G. The self-attention in Eq. 2 (in the paper)\ncan be re-written as follows:\nXi,m =\nNâˆ‘\nk=1\nRâˆ‘\nr=1\nÎ˜i,mQi,rKk,rVk,m (17)\nwhere N is the length of a sentence, Xi,m = Attention(Q,K,V )i,m is the entry of the output from the\nself-attention, and Ris equal to d. Here the core tensor Gis same as that in Eq. 7 (in the paper). Then, the\nSingle-block attention (a 3-order tensor) can be represented as follows:\nAi,j,m =\nRâˆ‘\np\nRâˆ‘\nq\nRâˆ‘\nr\nGp,q,rQi,pKj,pVm,r (18)\nwhere Ais a 3-order tensor, which is equal to AttenTD (G; Q,K,V ). Accordingly, Ai,j,m is a entry in tensor\nAand is equal to AttentionTD i,j,m in Eq. 15. Next, we aim to prove Eq. 7 can be established. Therefore,\nwe need to establish the relation between Eq. 18 and Eq. 17. Since the core tensor Gis a special tensor (i.e.,\ndiagonal tensor), Eq. 18 can be written as follows:\nAi,j,m =\nRâˆ‘\nr=1\nGr,r,rQi,rKj,rVm,r (19)\nAfter that, we can compute the attention representation through adding to model k. For better understanding, we\ngive the graph representation in Figure 3.\nXi,m =\nRâˆ‘\nr=1\nNâˆ‘\nj=1\nGrrrQi,rKj,rVm,r\nThe corollary then holds.\nD Compression Ratio about Multi-Linear Attention\nIn order to compute the compression ratio, we need to compare multi-linear attention with multi-head attention.\nThe comparison chart has been given in Figure 4.\nIn Figure 4, each Linearfunction in multi-head attention is about a weight matrix W âˆˆRdmodelÃ—d, and all\nweight matrices in multi-head attention are different. In multi-linear attention, three weight matrices are used and\n13\nğ‘„ğ‘„\n ğ¾ğ¾\n ğ‘‰ğ‘‰\nLinear\n Linear\n Linear\nLinear\n Linear\n Linear\nğ‘„ğ‘„\n ğ¾ğ¾\n ğ‘‰ğ‘‰\nLinear\n Linear\n Linear\nâ€¦â€¦\nLinear\n Linear\n Linear\nâ€¦\nâ„\nğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š âˆ’â„ğ‘’ğ‘’ğ‘’ğ‘’\nğ‘’ğ‘’ ğ‘’ğ‘’ğ‘šğ‘šğ‘šğ‘šğ‘’ğ‘’ğ‘ğ‘ğ‘šğ‘šğ‘šğ‘šğ‘ğ‘ğ‘ğ‘\nâ„\nğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š âˆ’ğ‘šğ‘šğ‘šğ‘šğ‘ğ‘ğ‘’ğ‘’ğ‘’ğ‘’ğ‘™ğ‘™ğ‘’ğ‘’ğ‘šğ‘šğ‘šğ‘šğ‘’ğ‘’ğ‘ğ‘ğ‘šğ‘šğ‘šğ‘šğ‘ğ‘ğ‘ğ‘\n  d\ng1 g2 ghg2 gh-1\nFigure 4: A diagram about a comparison of parameters between multi-linear attention and multi-head\nattention.\nh(a number) weight vectors are used. Through the analysis about Figure 4, the compression ratio is computed\nas follows.\ncompressionratio = 3 Ã—hÃ—dmodel Ã—d\n3 Ã—dmodel Ã—d+ hÃ—d\n=3 Ã—hÃ—dmodel\n3 Ã—dmodel + h\n(20)\nIn practice, his equal to 8 and dis equal to 512. The compression ratio approximates 8 in this case. In our work,\nthe dimension of vector Gr is set as Rwhich is smaller than d, where dis the dimension of attention matrix.\nLow-rank Approximation for Model CompressionIn the paper, we have described that our method combines\ntwo compression ideas, namely low-rank approximation and parameters sharing. Parameters sharing can be\nunderstood through the description of Figure 4. In Multi-linear attention, the idea of low-rank decomposition\nalso has the function of model compression. We have proved that the Single-block attention can re-construct an\none-head self-attention in Transformer. In order to obtain the representation of a tensorized attention, we adopt\nthe tensor splitting and the concat function. After that, we consider that each tensor slice from tensor splitting\napproximates the output of the self-attention function Eq. 2 (in the paper). When we only focus on the idea of\nlow-rank approximation, the compression ratio can be computed by the form, NÃ—d\nNÃ—N , where N is the length of a\nsequence, dis the dimension of a matrix (also namely hidden size). N is smaller than d, normally.\nThrough combining the ideas of parameters sharing and low-rank approximation, by formally considering the\nrank R, the compression ratio of Multi-linear attention model can be computed as follows:\ncompressionratioR = 3 Ã—hÃ—dmodel Ã—d\n3 Ã—dmodel Ã—d+ RÃ—h, (21)\nwhere R is the rank of the core tensor G. The compression ratio will be larger when R is smaller. This\ncompressionratioR is the compression ratio associated with R. Rneed to be set in practice. In experiments,\nRcan be set to 18, which is smaller than dmodel.\nE Experiment\nE.1 Partial Structure about Tensorized Transformer\nin the paper, the multi-linear attention is proposed. In order to show that the process of incorporating multi-linear\nattention into Transformer, Figure 5 gives out some information about the structure.\nE.2 Experimental Details in Language Modeling\nNow, we report some details of experiments as a relevant supplementary material. Firstly, we use three\nweight matrices Wq,Wk and Wv to linearly project the queries, keys and values. The outputs from the linear\nprojections can be shared by htimes, where his the number of core tensors in our background (i.e., core-1(h=1),\ncore-2(h=2)). We use Block Term Tensor decomposition (BTD) to construct a new representation, namely\nMulti-linear attention, which is a 3-order tensor. For incorporating the proposed attention into the architecture of\nTransformer, we split the 3-order tensor, and then concat each matrix from the tensor. For other layers, we use\nthe same structure as vanilla-Transformer.\n14\nInput \nEmbedding\nPositional \nEncoding â¨\nMulti-Linear \nAttention\nLayer Normalization\nFeed Forward\nLayer Normalization Output\nğ‘„ğ‘„ ğ¾ğ¾ ğ‘‰ğ‘‰\nLinear Linear Linear\nâŠ˜ âŠ˜ âŠ˜\nâ‹¯\n+ +â‹¯+ âˆ—1\nâ„\nğ‘›ğ‘›\nğ‘›ğ‘›\nâ‹¯ ğ‘Šğ‘Šğ‘‚ğ‘‚âˆ—\nğ‘›ğ‘›\nparameters \nsharing\nMulti-Linear Attention\nFigure 5: A diagram which is about the incorporating of multi-linear attention in partial Transformer\nstructure. The parameters are shared in the constructing of each single-block attention.\nHardware\nWe trained our model on one machine with 2 NVIDIA P40 GPUs. For our base models, the hyperparameters are\ndescribed in Table 4. In addition, we set the dropout=0.3 in all datasets. The model is trained using 30 epochs\nin three datasets (PTB, WikiText-103 and One-Billion).\nTable 4: The hyperparameters in the Tensorized Transformers model\nDatasets dhead dff h L dk dv Test PPL\nPTB 256 2100 2 3 40 40 49.8\nWikiText-103 256 2100 2 6 40 40 18.9\nOne-Billion 1024 2100 2 6 40 40 19.5\nOptimizer We used the Adam optimizer and vary the learning rate over the course of training. The vary\nformula [37] is followed in our work. We also used the warmup_steps = 4000 . Label Smoothing is\nemployed with the value Ïµ=0.1.\nE.3 Experiment Details in Neural Machine Translation\nThe Tensorized Transformer also has been applied to Neural Machine Translation task. In this experiment, we\nuse the same setup with Transformer [37], and replace the multi-head attention with the proposed multi-linear\nattention in the encoder structure. In the decoder structure, we still use the multi-head attention for verifying the\neffectiveness of encoding a sentence. The model is trained in 1 NVIDA P40 GPUs.\nE.4 Experimental comparison\nFor a more detailed comparison, we design these experiments as follows. In this section, we mainly show the\nexperimental results on two language modeling datasets, i.e., PTB and WikiText-103. We show the value of\nperplexity, as well as FLOPs5 correspondingly.\nTo further compare the experimental results under the same size of parameters between Tensorized Transformer\nand the baseline model (i.e., Transformer-XL), we add some experiments in Table 5. In our paper, we use the\nTensorized Transformer of 12M on PTB dataset and the Tensorized Transformer of 85.5M on WikiText-103\ndataset. To achieve the same size, i.e., 12M for Transformer-XL on PTB, we can reduce the dimensions of\nQ,K,V from 40 to 26. To achieve Transformer-XL of 85.5M, we reduce the dimensions of word embedding\n5FLOPs:The number of ï¬‚oating-point operations\n15\nModel PTB WikiText-103\nParams FLOPS Test PPL Params FLOPS Test PPL\nTransformer-XL [10] 24M 11.5B 59.1 257M 996.5B 18.3\nTensorized Transformer 24M 5.4B 52.7 257M 312.0B 21.2\nTransformer-XL [10] â€“ â€“ â€“ 151M 126.5B 24.0\nTensorized Transformer â€“ â€“ â€“ 151M 83.4B 18.8\nTransformer-XL [10] 12M 4.5B 87.8 85.5M 22.0B 34.8\nTensorized Transformer 12M 0.75B 57.9 85.5M 17.3B 20.9\nTable 5: Experimental comparisons on PTB and WikiText-103.\nTable 6: The same hyperparameters in Tensorized Transformers and Transformer-XL,N is the length\nof sequence, and Lis the number of layers.\nDatasets Model dhead dmodel N L dropout Test PPL\nPTB Transformer-XL 40 256 30 3 0.3 81.2\nPTB Tensorized Transformer 40 256 30 3 0.3 50.2\nWikiText-103 Transformer-XL 40 256 80 6 0.1 34.86\nWikiText-103 Tensorized Transformer 40 256 80 6 0.1 19.9\nOne-Billion Transformer-XL 40 1024 100 6 0.1 43.6\nOne-Billion Tensorized Transformer 40 1024 100 6 0.1 26.7\nfrom 512 to 256. The experimental results are shown in Table 5. Our model gets better results and the lower\nFLOPS than Transformer-XL.\nOn the other hand, we can also increase the parameters of Tensorized Transformer to reach the parameters\nreported by Transformer-XL on PTB and WikiText-103 datasets. On PTB dataset, we can increase the number\nof layers from 3 to 7 to get the Tensorized Transformer of 24M. On WikiText-103 dataset, we increase the\nnumber of layers from 6 to 11 and the length of sequence from 80 to 120 to get the Tensorized Transformer of\n257M. We can increase the number of layers from 6 to 8 and the length of sequence from 80 to 100 to get the\nTensorized Transformer of 151M. After that, Tensorized Transformer achieves better results and lower FLOPS\nthan Transformer-XL. These results are shown in Table 5.\nIn addition, we also carry out experiments when Transformer-XL has the same hyperparameters with Tensorized\nTransformer. Experimental results are shown in Table 6. Table 6 shows that our model can get the better results\nthan Transformer-XL. Besides, on two datasets (i.e., PTB and WiliText-103), we also try to train our model\n(Tensorized Transformer) using larger dimension of word embedding (i.e., dmodel). If dmodel is larger than\n256 on PTB dataset and larger than 512 on WikiText-103 dataset, the overï¬tting will occur. For the overï¬tting\nproblem, we will investigate it in our future work.\nF Partial Code\nThe project have been achieved by pytorch. In this section, we give the partial code which is about our methods,\ni.e., Sing-block attention and Multi-linear attention. First, the class of Single-block attention is given as follows.\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport numpy as np\nclass SingleBlockAttention(nn.Module):\nâ€™â€™â€™Single block attentionâ€™â€™â€™\ndef __init__(self, Rank):\nsuper(SingleBlockAttention, self).__init__()\nself.softmax = nn.Softmax()\nself.R = Rank\ndef forward(self, q, k, v, mb_size,d):\nself.core = nn.Parameter(torch.FloatTensor(np.random.rand(self.R)))\nN = v.size(1)\nself.core = self.softmax(self.R)\ncore_tensor = torch.zeros(N,d,N).cuda()\nfor i in range(self.R):\n16\ncores_tensor[i][i][i] = self.core[i]\nfull_matrixs = []\nfor i in range(mb_size):\nfull_matrix_1 = torch.einsum(â€™pqk, ip,jq,kr->ijrâ€™, [core_tensor, q[i],\nk[i], v[i]]).contiguous()\nfull_matrixs.append(torch.sum(full_matrix_1, dim=1))\noutput = torch.stack(full_matrixs).cuda().float()\nreturn output\nEach Single block attention is a component of Multi-linear attention. Based on the Single block attention, the\nMulti-linear attention can be given as follows.\nclass MultiLinearAttention(nn.Module):\nâ€™â€™â€™ MultiLinearAttention â€™â€™â€™\ndef __init__(self, h, Rank, d, dropout=0.1):\nsuper(MultiLinearAttention, self).__init__()\nself.n_head = h # h is equal to 2 in our model\nself.d_k = d\nself.d_v = d\nself.w_q = nn.Parameter(torch.FloatTensor(d_model, d_k))\nself.w_k = nn.Parameter(torch.FloatTensor(d_model, d_k))\nself.w_v = nn.Parameter(torch.FloatTensor(d_model, d_v))\nself.Tattention = SingleCoreAttention(Rank)\nself.layer_norm = LayerNormalization(Rank)\nself.proj = Linear(self.n_head*d, Rank)\nself.dropout = nn.Dropout(dropout)\ninit.xavier_normal_(self.w_q)\ninit.xavier_normal_(self.w_k)\ninit.xavier_normal_(self.w_v)\ndef forward(self, q, k, v):\nd_k, d_v = self.d_k, self.d_v\nn_head = self.n_head\nresidual = q\nmb_size, len_q, d_model = q.size()\nmb_size, len_k, d_model = k.size()\nmb_size, len_v, d_model = v.size()\nq_s = q.repeat(1, 1).view(-1, d_model)\nk_s = k.repeat(1, 1).view(-1, d_model)\nv_s = v.repeat(1, 1).view(-1, d_model)\nif n_head > 1:\noutput_1 = self.Tattention(q_s, k_s, v_s, mb_size,d_v)\noutput_2 = self.Tattention(q_s, k_s, v_s, mb_size,d_v)\noutput = (output_1+output_2)*0.5\nelse:\nouput = self.Tattention(q_s, k_s, v_s, mb_size,d_v)\n# project back to residual size\noutputs = self.proj(outputs)\noutputs = self.dropout(outputs)\nreturn self.layer_norm(outputs + residual)\n17",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6005182862281799
    },
    {
      "name": "Computer science",
      "score": 0.48499342799186707
    },
    {
      "name": "Electrical engineering",
      "score": 0.1829620599746704
    },
    {
      "name": "Engineering",
      "score": 0.1487261950969696
    },
    {
      "name": "Voltage",
      "score": 0.08068987727165222
    }
  ],
  "institutions": []
}