{
  "title": "Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer",
  "url": "https://openalex.org/W4385574011",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5063559824",
      "name": "Dimitris Mamakas",
      "affiliations": [
        "Athens University of Economics and Business"
      ]
    },
    {
      "id": "https://openalex.org/A5086209824",
      "name": "Petros Tsotsi",
      "affiliations": [
        "Athens University of Economics and Business"
      ]
    },
    {
      "id": "https://openalex.org/A5069270736",
      "name": "Ion Androutsopoulos",
      "affiliations": [
        "Athens University of Economics and Business"
      ]
    },
    {
      "id": "https://openalex.org/A5079179621",
      "name": "Ilias Chalkidis",
      "affiliations": [
        null,
        "University of Copenhagen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093838622",
    "https://openalex.org/W4404815084",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3032232719",
    "https://openalex.org/W4385573298",
    "https://openalex.org/W3177382889",
    "https://openalex.org/W2578757680",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4287208198",
    "https://openalex.org/W3103764297",
    "https://openalex.org/W3176549216",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2890026792",
    "https://openalex.org/W4226275564",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3176443840",
    "https://openalex.org/W3176443126",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962910668",
    "https://openalex.org/W4254690863",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W3136888420",
    "https://openalex.org/W2964351105",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W4231934124",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2962854673",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3202026671",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963829982",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2119821739",
    "https://openalex.org/W4285200483",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Pre-trained Transformers currently dominate most NLP tasks. They impose, however, limits on the maximum input length (512 sub-words in BERT), which are too restrictive in the legal domain. Even sparse-attention models, such as Longformer and BigBird, which increase the maximum input length to 4,096 sub-words, severely truncate texts in three of the six datasets of LexGLUE. Simpler linear classifiers with TF-IDF features can handle texts of any length, require far less resources to train and deploy, but are usually outperformed by pre-trained Transformers. We explore two directions to cope with long legal texts: (i) modifying a Longformer warm-started from LegalBERT to handle even longer texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use TF-IDF representations. The first approach is the best in terms of performance, surpassing a hierarchical version of LegalBERT, which was the previous state of the art in LexGLUE. The second approach leads to computationally more efficient models at the expense of lower performance, but the resulting models still outperform overall a linear SVM with TF-IDF features in long legal document classification.",
  "full_text": "Proceedings of the Natural Legal Language Processing Workshop 2022, pages 130 - 142\nDecember 8, 2022 ©2022 Association for Computational Linguistics\nProcessing Long Legal Documents with Pre-trained Transformers:\nModding LegalBERT and Longformer\nDimitris Mamakas∗† Petros Tsotsi∗†\nIon Androutsopoulos† Ilias Chalkidis‡⋄\n† Department of Informatics, Athens University of Economics and Business, Greece\n‡Department of Computer Science, University of Copenhagen, Denmark\n⋄ Cognitiv+, Athens, Greece\nAbstract\nPre-trained Transformers currently dominate\nmost NLP tasks. They impose, however, lim-\nits on the maximum input length (512 sub-\nwords in BERT), which are too restrictive in\nthe legal domain. Even sparse-attention mod-\nels, such as Longformer and BigBird, which\nincrease the maximum input length to 4,096\nsub-words, severely truncate texts in three of\nthe six datasets of LexGLUE. Simpler linear\nclassiﬁers with TF-IDF features can handle\ntexts of any length, require far less resources to\ntrain and deploy, but are usually outperformed\nby pre-trained Transformers. We explore two\ndirections to cope with long legal texts: (i)\nmodifying a Longformer warm-started from\nLegalBERT to handle even longer texts (up to\n8,192 sub-words), and (ii) modifying Legal-\nBERT to use TF-IDF representations. The\nﬁrst approach is the best in terms of perfor-\nmance, surpassing a hierarchical version of\nLegalBERT, which was the previous state of\nthe art in LexGLUE. The second approach\nleads to computationally more e ﬃcient mod-\nels at the expense of lower performance, but\nthe resulting models still outperform overall a\nlinear SVM with TF-IDF features in long legal\ndocument classiﬁcation.\n1 Introduction\nTransformer-based models (Vaswani et al., 2017),\nlike BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), and their numerous o ﬀspring, cur-\nrently dominate most natural language processing\n(NLP) tasks. These models are pre-trained on very\nlarge corpora using generic tasks (e.g., masked to-\nken prediction) that do not require human annota-\ntions, and are then ﬁne-tuned (further trained) on\ntypically much smaller task-speciﬁc datasets with\nmanually annotated ground truth. The quadratic\ncomplexity of their attention mechanisms, how-\never, imposes limits on the maximum input length\n∗Equal contribution.\nFigure 1: Comparison of examined models presented\nin Section 3, considering averaged down-stream per-\nformance and eﬃciency (inference time) in LexGLUE\nlong document classiﬁcation tasks (ECtHR, SCOTUS).\n(512 sub-word tokens in BERT, RoBERTa), which\nare often too restrictive in the legal domain, where\nlonger documents are common. The same restric-\ntions apply to LegalBERT (Chalkidis et al., 2020),\na BERT variant pre-trained on legal corpora.\nEven the sparse-attention Longformer (Beltagy\net al., 2020), a well-known Transformer that in-\ncreases the maximum input to 4,096 sub-words,\nseverely truncates texts in three of the six datasets\n(see Fig. 2) of the LexGLUE legal NLP benchmark\n(Chalkidis et al., 2022). On the other hand, simpler\nlinear classiﬁers with TF-IDF features (Manning\net al., 2008), which were very common before deep\nlearning, can handle texts of any length, at least in\ntext classiﬁcation tasks, require far less resources\nto train and deploy, but are nowadays usually out-\nperformed by pre-trained Transformers.\nMotivated by these observations, we explore two\ndirections to better cope with long legal texts: (i)\nwe modify a Longformer warm-started from Legal-\nBERT to handle even longer texts (up to 8,192\nsub-words), a resource-intensive direction that fur-\nther increases the parameters and processing time\n130\nof large sparse-attention Transformer models; and\n(ii) we modify LegalBERT to use TF-IDF repre-\nsentations, which allows processing longer texts\nwithout increasing the model sizes. The ﬁrst ap-\nproach is the best overall in terms of performance,\nsurpassing a hierarchical version of LegalBERT\n(Chalkidis et al., 2021a), which was the previous\nstate of the art in LexGLUE. The second direction\nleads to computationally more eﬃcient models at\nthe expense of lower performance, but still out-\nperforms overall a linear Support Vector Machine\n(SVM) (Cortes and Vapnik, 1995) with TF-IDF\nfeatures in long document classiﬁcation.\n2 Related Work\n2.1 Long Document Processing\nTransformer-based models consist of stacked Trans-\nformer blocks (Vaswani et al., 2017). Each block\nbuilds a revised embedding (vector representation)\nfor each (sub-word) token of the input text, based\non the embeddings of the previous block, starting\nfrom an initial embedding layer that provides an\nembedding per vocabulary token. For a block with\na single attention head and an input n tokens long,\ngenerating a single revised token embedding in-\nvolves computing a weighted sum (weighted by at-\ntention scores) over the n token embeddings of the\nprevious block. Hence, O(n2) time is required to\ngenerate all the n revised token embeddings. With\nk attention heads, the complexity is O(k ·n2).\nSparse-attention variants of Transformers, like\nthose used in Longformer (Beltagy et al., 2020),\nETC (Ainslie et al., 2020), BigBird (Zaheer et al.,\n2020), generate each revised token embedding by\nattending (considering) only the previous block’s\nembeddings for the current, the l previous, and the\nl next tokens in the input text, i.e., the weighted\nsum is now over only 2 ·l + 1 (equal to 512 by\ndefault) token embeddings of the previous block.\nThe complexity becomes O(k ·n ·l), linear to n. To\nbetter capture long-distance dependencies, these\nmodels also use global attention. This involves\neither standard pseudo-tokens, such as the [cls]\ntoken at the beginning of each text, or additional\npseudo-tokens, e.g., [sep] tokens placed at the end\nof each paragraph. In both cases, these special\nglobal tokens are attended by, and attend all other\ntokens, allowing information to ﬂow across distant\ntokens, even when sparse attention is used.\nWe experiment with Longformer, a well-known\nand relatively simple sparse-attention Transformer,\nwhich can process texts up to 4,096 sub-words\nlong. ETC and Big Bird use the same maximum\ninput length and are very similar; one diﬀerence is\nthat they employ additional pre-training objectives\nfor the global tokens, whereas in Longformer the\nglobal tokens are not pre-trained. We also expand\nLongformer to process texts up to 8,192 sub-words\nlong, and we consider an ETC-like global attention\nscheme with additional [sep] tokens.\nHierarchical Transformers, e.g., SMITH (Yang\net al., 2020), use BERT (or other base models)\nto separately encode each paragraph or other seg-\nments of the input text that do not exceed the base\nmodel’s maximum input length. The generated\nparagraph embeddings (e.g., the embeddings of\n[cls] tokens placed at the beginning of each para-\ngraph) are then passed through additional stacked\nTransformer blocks, to allow interactions between\nthe paragraph embeddings. The resulting context-\naware paragraph embeddings can then be used to\nclassify individual paragraphs or to classify the en-\ntire text (e.g., using the ﬁrst paragraph embedding\nor by max-pooling over all paragraph embeddings).\nIn LexGLUE (Chalkidis et al., 2022) a simi-\nlar hierarchical model (Chalkidis et al., 2021a)\nwas used, with either generic BERT variants (e.g.,\nBERT, RoBERTa, DeBERTa) or LegalBERT as\nthe base model, in three of the benchmark’s tasks\n(ECtHR Task A and B, SCOTUS) where the av-\nerage text length was much higher than BERT’s\nmaximum length (Fig. 2). Unlike SMITH, the addi-\ntional paragraph-level Transformer blocks were not\npre-trained. We compare against this hierarchical\nvariant of LegalBERT on LexGLUE.\nRecurrent Transformers are another approach to\nhandle long texts (Dai et al., 2019; Yang et al.,\n2019; Ding et al., 2021). We do not consider them\nhere due to the latency that recurrency introduces.\nBag-of-Word (BoW) models typically represent\neach text as a (sparse) feature vector ⟨ f1, . . . ,f|V|\n⟩,\nwith one feature fi per vocabulary word. TF-IDF\nfeatures (Manning et al., 2008) are common. Given\na text n words long, each feature fi becomes:\nfi = TFi ·IDFi = ci\nn ·log N\n1 + di\n,\nwhere ci is the frequency of the i-th vocabulary\nword in the text, N is the number of documents in a\ncorpus (in text classiﬁcation this is often the train-\ning set), and di counts the documents of the corpus\n131\nFigure 2: Distribution of input text length, measured in BERT sub-word tokens, across the six LexGLUE datasets.\nCopied with permission from Chalkidis et al. (2022).\nthat contain the i-th vocabulary word.1 Averaging\nword embeddings (Jin et al., 2016; Brokos et al.,\n2016) with or without TF-IDF weighting, is also a\nBoW representation, but typically performs worse,\nsince averaging leads to very noisy representations.\nSuch BoW representations discard word order,\nbut are also insensitive to the length of the input\ntext, in the sense that the feature vector always con-\ntains |V|features. Combining TF-IDF feature vec-\ntors with linear classiﬁers leads to models that can\nhandle texts of any length and require far less re-\nsources to train compared to modern Transformer-\nbased models, at the expense of lower performance.\nBoW-BERT: Our attempts to combine TF-IDF fea-\ntures with BERT were inspired by the work of\nHessel and Schoﬁeld (2021), who reported that\nshuﬄing the words of each text during ﬁne-tuning\nled to a degradation of less than 5 p.p. (F1 or ac-\ncuracy) of BERT’s performance in most GLUE\ntasks (Wang et al., 2018). 2 The resulting model,\ncalled BoW-BERT, can be seen as operating on\nBoW representations, in the sense that word order\nis lost. Hessel and Schoﬁeld (2021) also reported\nthat BoW-BERT performed better than other BoW\nmodels on GLUE, including linear models with\nTF-IDF features. BoW-BERT’s word shu ﬄing,\nhowever, does not change the text length, hence it\ndoes not address BERT’s maximum input length\nlimit; IDF information is also not considered.\nBy contrast, we remove multiple occurrences of\nthe same word from each text; to incorporate TF-\nIDF information, we order the remaining words by\nTF-IDF and/or we add a TF-IDF embedding layer,\nboth discussed below.\n1In ‘sublinear’ TF-IDF, a logarithm is also applied to TF.\n2However, Hessel and Schoﬁeld (2021) also cite other\nwork that found word shuﬄing to have a larger impact on pre-\ntrained Transformers (and LSTMs) in other datasets and tasks.\nSinha et al. (2021) and Abdou et al. (2022) investigated the\neﬀect of word shuﬄing on pre-trained Transformers in more\ndetail, considering mostly word shuﬄing during pre-training.\n2.2 Applications in Legal NLP\nIn the early days of Deep Learning for legal NLP,\nthe community examined the use of the Hierarchi-\ncal Attention Network (HAN) of Yang et al. (2016)\nor simpler variants (hierarchical BILSTMs) to en-\ncode long documents in applications of legal judg-\nment prediction for Chinece (Zhong et al., 2018) or\nECtHR (Chalkidis et al., 2019a) court cases, show-\ncasing improvement over ﬂat RNN-based models,\nsuch as stacked BILSTMs followed by a single-\nhead attention layer (Xu et al., 2015). Hierarchical\nBILSTMs with self-attention were also employed\nby Chalkidis et al. (2018) for sequential sentence\nclassiﬁcation in order to identify obligations and\nprohibitions in contractual paragraphs.\nHierarchical variants of Transformers were ini-\ntially proposed by Chalkidis et al. (2019a). In\ntheir work, document paragraphs are encoded via a\nshared BERT encoder to produce paragraph embed-\ndings, which are then combined with max-pooling\nto form the ﬁnal document embedding. This model\noutperformed strong RNN-based methods such as\nthe Hierarchical Attention Network (HAN).\nLater on, Chalkidis et al. (2021a) presented a\nnew variant, where the paragraph embeddings are\nfed into additional stacked Transformer blocks, to\nallow cross-paragraph contextualization. This lat-\nter version has also been used in other legal NLP\napplications, by Niklaus et al. (2021, 2022) in judg-\nment prediction of Swiss court cases, using XLM-R\nas the underlying encoder, and by Chalkidis et al.\n(2022) for the long document classiﬁcation tasks\nof LexGLUE, using several alternative pre-trained\nTransformers, alongside Longformer. Xiao et al.\n(2021) released a Longformer pre-trained on Chi-\nnese legal corpora, which outperforms baselines in\nseveral legal NLP tasks. More recently, Dai et al.\n(2022) explored how tunable hyper-parameters of\nHierarchical Transformers and Longformer, such\n132\nas the size of the local window, aﬀect downstream\nperformance. In experiments on the ECtHR dataset,\nthey found that fewer but larger local windows\n(paragraphs), e.g., 8 ×512, instead of 32 ×128, in\nHierarchical Transformers improve performance.\nHierarchical Transformers are also used in the\nwork of Malik et al. (2021) in legal judgment pre-\ndiction of Indian court cases, where their best-\nperforming model uses XLNet (Yang et al., 2019)\nas the underlying paragraph encoder followed by\nstacked BiGRUs. Moreover, hierarchical Trans-\nformers similar to those of Malik et al. have been\nalso used by Kalamkar et al. (2022) for sequential\nlegal sentence classiﬁcation in order to segment\nIndian court cases into topical and coherent parts.\n3 Models Considered\nWe discuss models Chalkidis et al. (2022) eval-\nuated on LexGLUE as baselines, and models\nwe introduce. The LexGLUE baselines also in-\ncluded RoBERTa (Liu et al., 2019), DeBERTa (He\net al., 2021), BigBird (Zaheer et al., 2020), and\nCaseLaw-BERT (Zheng et al., 2021), which are\nnot considered here. Chalkidis et al. (2022) found\nRoBERTa and DeBERTa to be better than BERT on\nLexGLUE, but worse than LegalBERT; no legally\npre-trained variants of RoBERTa and DeBERTa\nare available. BigBird and CaseLaw-BERT were\nfound to be overall slightly worse than Longformer\nand LegalBERT, respectively, on LexGLUE.\n3.1 LexGLUE baselines\nTFIDF-SVM is a linear SVM with TF-IDF fea-\ntures for the top-K most frequent word n-grams of\nthe training set, where n ∈[1, 2, 3].3\nLegalBERT (Chalkidis et al., 2020) is BERT pre-\ntrained on English legal corpora (legislation, con-\ntracts, court cases). In the long document classiﬁca-\ntion tasks (see Table 1), we deploy its hierarchical\nvariant (Section 2) as in Chalkidis et al. (2022).\nLongformer (Beltagy et al., 2020). This is the orig-\ninal Longformer, discussed in Section 2. It extends\nthe maximum input length to 4,096 sub-word to-\nkens. Like BERT and RoBERTa, Longformer uses\nabsolute positional embeddings, i.e., there is a sep-\narate positional embedding for each token position\nup to the maximum input length. Longformer’s\npositional embeddings were warm-started from the\n512 positional embeddings of RoBERTa, cloning\n3K ∈[20k, 30k, 40k] is tuned per task on dev. data.\nthem 8 times (e.g., the embeddings of positions\n513–1024 were initialized to the same RoBERTa\npositional embeddings as positions 1–512). All the\nother parameters of Longformer (and RoBERTa)\nare not sensitive to token positions and were warm-\nstarted from the corresponding RoBERTa parame-\nters.4 After warm-starting, Longformer was further\npre-trained for 64k steps on generic corpora.\n3.2 Extensions of LegalBERT\nTFIDF-SRT-LegalBERT: This is LegalBERT,\nbut we remove duplicate sub-words from the input\ntext and sort the remaining ones by decreasing TF-\nIDF during ﬁne-tuning. Removing duplicate words\nis an attempt to avoid exceeding the maximum in-\nput length. In ECtHR, for example, the average text\nlength (in sub-words) drops from 1,619 to 1,120;\nin SCOTUS, from 5,953 to 1,636 (see Fig. 1). If\nthe new form of the text still exceeds the maximum\ninput length, we truncate it (keeping the ﬁrst 512\ntokens). Ordering sub-words by decreasing TF-\nIDF hopefully allows the model to learn to attend\nearlier sub-words (higher TF-IDF) more, utilizing\nBERT’s positional embeddings as TF-IDF ranking\nencodings. This is a BoW model, since the original\nword order of the input text is lost.\nTFIDF-SRT-EMB-LegalBERT: The same as the\nprevious model, except that we add a TF-IDF em-\nbedding layer (Fig. 3). We bucketize the distri-\nbution of TF-IDF scores of the training set and\nassign a TF-IDF embedding to each bucket. Dur-\ning ﬁne-tuning, we compute the TF-IDF score of\neach sub-word (before deduplication) and we add\nthe corresponding TF-IDF bucket embedding to\neach token’s input embedding when its positional\nembedding is also added. The TF-IDF bucket em-\nbeddings are initialized randomly and trained dur-\ning ﬁne-tuning. Hence, this model is informed both\nabout TF-IDF token ranking (via word re-ordering)\nand TF-IDF scores (captured by TF-IDF embed-\ndings). This is still a BoW model, since it ignores\nthe original word order, like the previous model.\nTFIDF-EMB-LegalBERT: The same as Legal-\nBERT, but we add the TF-IDF layer of the previous\nmodel. Token deduplication and ordering by TF-\nIDF scores are not included. This allows us to study\nthe contribution of the TF-IDF layer on its own by\ncomparing to the original LegalBERT. The result-\ning model is aware of word-order via its positional\n4E.g., the dense layers that produce the attention’s query,\nkey, value embeddings are the same for all token positions.\n133\nFigure 3: (a) TFIDF-SRT-EMB-LegalBERT, with sub-word token deduplication, re-ordering by TF-IDF, and TF-\nIDF embedding layer. (b) Longformer-8192-PAR extended to encode up to 8192 sub-word tokens, split into\nparagraphs separated by [sep] tokens. The original sequence (S ) of sub-words (W) is shown in the bottom. Super-\nscripts (W p) denote positioning in each sequence. Subscripts (W[id]) are the indices of the sub-words in the model’s\nvocabulary. In both models, the resulting contextualized [cls] token embedding is fed to a linear classiﬁer.\nembeddings (like BERT and LegalBERT). For long\ntexts, it addresses the maximum input length limi-\ntation via its hierarchical variant, which is similar\nto LegalBERT’s (Chalkidis et al., 2022).\n3.3 Extensions of Longformer\nLongformer-8192: This is the same as the origi-\nnal Longformer (Beltagy et al., 2020), which was\nwarm-started from RoBERTa (Section 3.1), but we\nextend the maximum input length to 8,192 sub-\nwords. We warm-start the positional embeddings\nfrom those of Longformer, cloning them once (posi-\ntions 4,097–8,192 get the same initial embeddings\nas positions 1–4,096). To keep the computational\ncomplexity under control, we decrease the local\nattention window size from 512 to 128 sub-words.5\nAll parameters, including positional embeddings,\nare updated during ﬁne-tuning, again as in the orig-\ninal Longformer. We did not perform any addi-\ntional pre-training, however, beyond that of the\noriginal Longformer, lacking computing resources.\nAll Longformer variants are aware of word order.\nLongformer-8192-PAR: This is the same as the\nprevious model, but we place a global token (Sec-\ntion 2), speciﬁcally a [sep] token, at the end of\neach paragraph (Fig. 3). By contrast, the original\nLongformer and Longformer-8192 use the single\n[cls] token at the beginning of the input text as a\nsingle global token for classiﬁcation tasks.6 As in\nthe previous model, we decrease the local attention\n5Table 4 shows that despite this counter-measure, the ex-\npansion to 8,192 sub-words leads to almost 2×inference time\nand 30% increase in memory.\n6Additional global tokens were used by Beltagy et al.\n(2020) in other tasks, e.g., question answering.\nwindow size from 512 to 128 sub-words.\nOur intuition was that using more global tokens,\nand synchronizing them with paragraph breaks\nwould allow information to ﬂow more easily across\nparagraphs, viewed as discourse segments. Pre-\nvious work by Zaheer et al. (2020) also suggests\nthat such ETC-like global attention layouts lead to\nbetter results. Again, all parameters are updated\nduring ﬁne-tuning, but we did not perform any ad-\nditional pre-training to better adjust the model to\nthe new global attention layout, lacking resources.\nLegalLongformer: Similar to Longformer, but\nwarm-started from LegalBERT. We clone the po-\nsitional embeddings of LegalBERT eight times to\ncover positions 1–4,096 (instead of 1–512 in Legal-\nBERT) and update them during ﬁne-tuning. All\nother parameters are also warm-started from Legal-\nBERT and are updated during ﬁne-tuning. Follow-\ning Beltagy et al. (2020), we warm-start the global\nattention parameters of LegalLongformer with the\n(local) attention parameters of LegalBERT. Again,\nno additional pre-training was performed.\nLegalLongformer-8192: Similar to Longformer-\n8192, but again warm-started from LegalBERT. In\nthis case, we clone the positional embeddings of\nLegalBERT 16 times to cover positions 1–8,192.\nAgain, no additional pre-training was performed.\nLegalLongformer-8192-PAR: The same as the\nprevious model, but with global tokens at the end\nof each paragraph, as in Longformer-8192-PAR.\n134\nDataset Source Text Length (Original/Unique) Instances ClassesAverage Maximum Train Dev. Test\nLongDocumentClassificationTasks\nECtHR (Task A)(Chalkidis et al., 2019a)1.6k (1.1k) 35.4k (27.2k) 9,000 1,000 1,000 10+1\nECtHR (Task B)(Chalkidis et al., 2021b)1.6k (1.1k) 35.4k (27.2k) 9,000 1,000 1,000 10+1\nSCOTUS (Spaeth et al., 2020) 6.0k (1.6k) 88.6k (12.1k) 5,000 1,400 1,400 14\nShortDocumentClassificationTasks\nEUR-LEX∗ (Chalkidis et al., 2021a)1.1k (341) 140.1k (10k) 55,000 5,000 5,000 100\nLEDGAR (Tuggener et al., 2020) 113 (65) 1.2k (484) 60,000 10,000 10,000 100\nUNFAIR-ToS (Lippi et al., 2019) 33 (25) 441 (181) 5,532 2,275 1,607 8+1\nTable 1: LexGLUE statistics. Lengths in sub-word tokens, before and after (in brackets) deduplication.∗EUR-LEX\nis treated as a short document task in our work, since using only the ﬁrst 512 tokens (what we do) has comparable\nperformance with using the full texts (Chalkidis et al., 2019b). +1 denotes an extra class for no-label instances.\n4 Experiments\n4.1 Datasets\nLexGLUE (Chalkidis et al., 2022) is a collection\nof six simpliﬁed English legal NLP datasets that\nare used to evaluate the performance of NLP meth-\nods across seven legal text understanding tasks.\nInspired by GLUE (Wang et al., 2018) and Su-\nperGLUE (Wang et al., 2019), LexGLUE was de-\nsigned to push towards generic-pretrained models\nthat can cope with multiple legal NLP tasks with\nlimited extra training (ﬁne tuning) for each one.\nHere, we experiment with six of the seven tasks\nof LexGLUE, excluding CaseHOLD (Zheng et al.,\n2021), a multiple choice question answering task\nabout holdings of US court cases. The other six\ntasks are all framed as text classiﬁcation problems.\nWhile our work targets the long document classiﬁ-\ncation tasks (ECtHR Tasks A and B, SCOTUS), we\nalso experiment with tasks that involve short texts\n(EUR-LEX, LEDGAR, UNFAIR-ToS), for com-\npleteness. Table 1 lists the sources of the datasets\nwe experiment with and provides key statistics. EC-\ntHR Task A and B require deciding which articles\nof the European Convention of Human Rights were\nviolated, or allegedly violated, respectively; both\ntasks use the same dataset in LexGLUE. SCOTUS\nrequires classifying opinions of the US Supreme\nCourt into issue areas (e.g., Criminal Procedure,\nCivil Rights). EUR-LEX requires labeling Euro-\npean laws with concepts from a European Union\ntaxonomy. LEDGAR requires assigning topical\ncategories to contract provisions. UNFAIR-ToS\nrequires detecting unfair terms in terms of service.\nConsult Chalkidis et al. (2022) and the work cited\nin Table 1 for further information.\n4.2 Evaluation measures\nFollowing Chalkidis et al. (2022), for each task we\nreport macro-F1 (m-F1), which assigns equal im-\nportance to all classes, and micro-F1 (µ-F1), which\nassigns more importance to frequent classes.\n4.3 Experimental setup\nAcross all experiments, we use Adam (Kingma\nand Ba, 2015) with initial learning rate 3e-5. We\ntrain models up to 20 epochs using early stopping,\nmonitoring µ-F1 on the development data. We run\nall experiments with 5 diﬀerent random seeds and\nreport test results for the seeds with the best devel-\nopment scores. For the TF-IDF bucket embedding\nlayer, we search in {16, 32, 64, 128}for the number\nof buckets that maximizesµ-F1 on the development\ndata, separately for each task.\n4.4 Experimental results\nTable 2 lists the test results of all models across the\nsix tasks considered. Table 3 aggregates the test\nresults over the three long-document classiﬁcation\ntasks (ECtHR Tasks A and B, SCOTUS) we are\nmainly interested in (see also see Table 1). We\nuse the harmonic mean over the scores of the three\ntasks, following Shavrina and Malykh (2021).\nBoW models: The results of the two BoW variants\nof LegalBERT (TFIDF-SRT-LegalBERT, TFIDF-\nSRT-EMB-LegalBERT) in Table 2 are mixed. In\nthe two ECtHR tasks and EUR-LEX, both mod-\nels outperform the TFIDF-SVM baseline, a much\nsimpler linear BoW model. Contrary, both mod-\nels are outperformed by TFIDF-SVM in SCOTUS\nand LEDGAR. In UNFAIR-ToS, the three models\nperform overall on par. While the original word\n135\nMethod ECtHR (A)*ECtHR (B)*SCOTUS* EUR-LEX LEDGAR UNFAIR-ToS\nµ-F1 m-F1 µ-F1 m-F1 µ-F1 m-F1 µ-F1 m-F1 µ-F1 m-F1 µ-F1 m-F1\nBoW models (word order lost)\nTFIDF-SVM 62.6 48.9 73.0 63.8 74.0 64.4 63.4 47.9 87.0 81.4 94.7 75.0\nTFIDF-SRT-LegalBERT 69.8 62.8 78.5 71.9 73.4 61.8 69.6 53.7 86.9 80.8 95.3 80.6\nTFIDF-SRT-EMB-LegalBERT68.7 63.1 79.0 72.5 73.9 63.6 69.7 53.9 86.5 80.3 95.8 78.7\nLegalBERT variants that retain word order\nLegalBERT 70.0 64.0 80.4 74.7 76.4 66.5 72.1 57.4 88.2 83.0 96.0 83.0\nTFIDF-EMB-LegalBERT 70.0 61.9 79.4 73.5 74.9 64.7 71.6 56.9 88.7 83.4 95.9 82.1\nLongformer variants (all retain word order)\nLongformer 69.9 64.7 79.4 71.7 72.9 64.0 71.6 57.7 88.2 83.0 95.5 80.9\nLongformer-8192 70.9 62.1 79.2 73.9 73.7 63.6 (Not considered for short-document tasks.)\nLongformer-8192-PAR 70.8 62.3 79.0 73.1 73.9 66.0 >>\nLegalLongformer 71.7 63.6 80.5 76.4 76.6 66.9 72.2 56.5 88.8 83.5 95.7 80.6\nLegalLongformer-8192 71.2 64.3 81.4 74.2 77.5 67.3 (Not considered for short-document tasks.)\nLegalLongformer-8192-PAR71.4 68.4 79.6 73.9 76.2 66.3 >>\nTable 2: Test results across LexGLUE tasks considered. In starred tasks, we use the hierarchical variant of Legal-\nBERT. We do not consider extended Longformers in short document classiﬁcation tasks (last three; see also Ta-\nble 1), which are included for completeness. Best scores per group are underlined, and best overall are in bold.\norder is lost in all three models, TFIDF-SVM re-\nlies on n-grams up to 3 words long, which allows\nit to retain local word order in features that repre-\nsent multi-word terms, like ‘civil rights’ or ‘federal\ntaxation’ in the case of SCOTUS. We suspect that\nsuch multi-word terms are more important in SCO-\nTUS and LEDGAR, which would explain the fact\nthat TFIDF-SVM outperforms the other two BoW\nmodels in these tasks. Future work could add a\nTFIDF-SVM variant with only unigram features\nto check this hypothesis; there should be a large\nperformance drop in the three tasks. One could\nalso explore ways to use TF-IDF information about\nn-grams (not just unigrams) in the BoW variants of\nLegalBERT.\nSwitching to the aggregated results of the long\ndocument tasks of Table 3, we observe that both\nBoW variants of LegalBERT outperform TFIDF-\nSVM. Table 3 also shows that TFIDF-SRT-EMB-\nLegalBERT (which includes the TF-IDF embed-\ndings layer) performs slightly better than TFIDF-\nSRT-LegalBERT in terms of m-F1 (1 p.p. improve-\nment), but there is almost no di ﬀerence in µ-F1,\nand the results of Table 2 show no clear winner\nbetween the two methods across tasks.\nLegalBERT variants that retain word order:\nTable 2 shows that adding the TF-IDF embeddings\nlayer to LegalBERT (TF-IDF-EMB-LegalBERT),\nwithout word deduplication and retaining the orig-\ninal word order, leads to lower performance in 5\nMethod µ-F1 m-F1\nTFIDF-SVM 69.5 58.1\nTFIDF-SRT-LegalBERT 73.7 65.2\nTFIDF-SRT-EMB-LegalBERT 73.6 66.1\nLegalBERT 75.4 68.1\nTFIDF-EMB-LegalBERT 74.6 66.3\nLongformer 73.9 66.6\nLongformer-8192 74.4 66.1\nLongformer-8192-PAR 74.4 66.8\nLegalLongformer 76.1 68.6\nLegalLongformer-8192 76.5 68.4\nLegalLongformer-8192-PAR 75.6 69.4\nTable 3: Test results aggregated (harmonic mean) over\nthe long-document classiﬁcation tasks (ECtHR Tasks A\nand B, SCOTUS) of LexGLUE. Best scores per group\nare underlined, and best overall are in bold.\nout of 6 tasks compared to the original LegalBERT;\nLEDGAR is the only exception, with small im-\nprovements. The aggregated results of Table 3\nalso show that TF-IDF-EMB-LegalBERT is worse\nthan the original LegalBERT. We can only hypoth-\nesize that TF-IDF-EMB-LegalBERT is in most\ncases unable to learn how to use the additional\ninformation from the additive TF-IDF embeddings,\nwhich are added only during ﬁne-tuning (they were\nnot present during pre-training). This hypothesis\n136\nis based on the positive (albeit small) impact of\nthe TF-IDF embeddings layer on LEDGAR, the\nlargest dataset with 60k training examples. All\nother datasets contain fewer than 10k training ex-\namples (Table 1), with the exception of EUR-LEX\n(55k), which does not support our hypothesis.\nGiven appropriate computing resources, one\ncould further pre-train TFIDF-EMB-LegalBERT\nto help it learn how to exploit the newly introduced\nTF-IDF embeddings. The same applies to both\nBoW variants of LegalBERT, although in that case\nappropriate BoW pre-training objectives should\nbe considered, since Masked Language Modeling\n(MLM) is not reasonable when the original word\norder is lost. Predicting the TF-IDF bucket id when\nmasked, or predicting masked words given their\nTF-IDF bucket ids seem better alternatives.\nLongformer variants: Comparing the original\nLongformer with Longformer-8192, a variant ca-\npable of processing even longer documents, the\nresults are mixed (Table 2) across the 3 long doc-\nument classiﬁcation tasks (ECtHR Tasks A and B,\nSCOTUS), i.e., µ-F1 is improved at the expense of\nm-F1, or vice-versa. Aggregating the results (Ta-\nble 3), we observe the very same trade-o ﬀ (+0.5\np.p. in µ-F1, -0.5 p.p. in m- F1). Considering the\nadditional global tokens in Longformer-8192-PAR,\nwe have comparable results in ECtHR tasks and\nimproved results in SCOTUS, the dataset with the\nlongest documents in LexGLUE (Table 1). Aggre-\ngating the results (Table 3), we observe that the\nextra global tokens do not improve µ-F1 further\n(74.4), but lead to the best m- F1 (66.8) of all the\nLongformer variants that have not been pre-trained\non legal corpora. Based on the aforementioned\nobservations, we believe that the additional posi-\ntional embeddings and adding more global tokens\nare in the right direction when seeking better long\ndocument performance with Longformer.\nMoving on to Longformer variants warm-started\nfrom LegalBERT, Table 2 shows that LegalLong-\nformer outperforms the original generic Long-\nformer (Beltagy et al., 2020) in most cases, which\nhighlights the importance of domain-speciﬁc mod-\nels as already noted in the literature (Chalkidis\net al., 2022; Zheng et al., 2021). We observe no-\ntable improvements in long document classiﬁcation\ntasks (ECtHR A and B, SCOTUS), with approx.\n+2.0 p.p. in both µ-F1 and m-F1 in the aggregated\nresults of Table 3. These results are impressive con-\nsidering that LegalLongformer was warm-started\nfrom LegalBERT, but no additional pre-training\nwas conducted; hence several parameters of the\nmodel (e.g., additional positional embeddings and\nglobal attention matrices) may be far from optimal.\nBy contrast, the original Longformer was warm-\nstarted from RoBERTa and was pre-trained for 64k\nadditional steps on generic long documents.\nConsidering the last two variants of LegalLong-\nformer (-8192, -8192-PAR), the results are mixed\n(trade-oﬀ between µ-F1 and m-F1 in Table 2, as\nwith the generic Longformer) and share the best\naggregated results across all examined methods in\nlong document classiﬁcations tasks (Table 3).\nBased on the above, we believe that the pro-\nposed extensions (warm-start from a legally pre-\ntrained model, additional positional embeddings,\nadditional global tokens) are in the right direc-\ntion, already producing better results compared\nto the generic Longformer, and state-of-the-art re-\nsults in several LexGLUE tasks (ECtHR A&B and\nLEDGAR). Given appropriate resources, one could\nfurther pre-train LegalLongformer-8192-PAR for\na limited number of steps (e.g., 64k) on long legal\ndocuments (e.g., the training subsets of ECtHR,\nand SCOTUS) to optimize the newly introduced\nparameters and expect further improvements.\n4.5 E ﬃciency considerations\nIn Table 4, we present important information with\nrespect to e ﬃciency. As expected, TFIDF-SVM\nhas the fewest parameters (200×fewer than Legal-\nBERT variants) and is substantially faster and less\nmemory-intensive compared to all other neural\nmethods, while achieving state-of-the-art results\nin two tasks (SCOTUS and EUR-LEX, Table 2).\nOur proposed BoW variants of LegalBERT are\nsubstantially less memory intensive; approx. 25%\nless GPU memory across the long document classi-\nﬁcation tasks (starred), and approx. 50% less GPU\nmemory across others with much shortened texts\ncompared to LegalBERT. The TF-IDF embeddings\ndo not aﬀect memory or inference time (storing and\nlooking up TF-IDF embeddings are negligible).\nConsidering LegalLongformer, we observe an\napprox. 50% increase in the number of parameters\nand approx. 25% increase in GPU memory. With\nrespect to inference time, there is a 10 ×increase\ncompared to LegalBERT models in long document\nprocessing tasks, and larger in the other tasks with\nmuch shorter documents, which makes hierarchical\nTransformers a faster alternative.\n137\nMethod Params. ECtHR* SCOTUS* EUR-LEX LEDGAR UNFAIR-ToS\nMem. Time Mem. Time Mem. Time Mem. Time Mem. Time\nBoW models (word order lost)\nTFIDF-SVM 0.5M 0.1 .001 0.1 .001 0.1 .001 0.1 .001 0.1 .001\nTFIDF-SRT-LegalBERT 110M 0.9 .012 0.9 .012 0.9 .012 0.9 .007 0.9 .007\nTFIDF-SRT-EMB-LegalBERT110M 0.9 .012 0.9 .012 0.9 .012 0.9 .007 0.9 .007\nLegalBERT variants that retain word order\nLegalBERT 110M 1.3 .014 1.3 .014 1.9 .012 1.9 .007 1.9 .007\nTFIDF-EMB-LegalBERT 110M 1.3 .014 1.3 .014 1.9 .012 1.9 .007 1.9 .007\nLongformer variants (all retain word order)\nLegalLongformer 148M 1.7 .164 1.7 .164 1.3 .033 1.3 .033 1.3 .033\nLegalLongformer-8192 151M 2.2 .318 2.2 .318 (Not considered for short-document tasks.)\nLegalLongformer-8192-PAR151M 2.2 .331 2.2 .331 >>\nTable 4: Model parameters, memory footprint (GBs/sample), and inference time (sec/sample). In starred tasks, we\nuse the hierarchical variant of LegalBERT. For ECtHR Tasks A and B, the information of this table is identical.\nMoving to the extensions of LegalLongformer\nthat are able to encode longer documents (Legal-\nLongformer8192) and use extra global tokens\n(LegalLongformer-8192-PAR), there is an approx.\n30% increase in GPU memory compared to the\nstandard Longformer (encoding up to 4,096 sub-\nwords), and 2×increase in inference time. In other\nwords, there is no free lunch when seeking perfor-\nmance improvements.\n5 Conclusions and Future Work\nConcluding, we presented BoW variants of Legal-\nBERT, which remove duplicate words and consider\nTF-IDF scores by reordering the remaining words\nand/or by employing a TF-IDF embedding layer.\nThese variants are more eﬃcient than the original\nLegalBERT and still overall outperform a TF-IDF-\nbased SVM in long legal document classiﬁcation.\nWe also modiﬁed Longformer to handle even\nlonger texts (up to 8,192 sub-words), use additional\nglobal tokens, and also showed the positive eﬀect of\nwarm-starting it from LegalBERT. Unlike the BoW\nmodels, this is a resource-intensive direction, with\nsubstantial improvements compared to the origi-\nnal Longformer (up to 4,096 sub-words, a single\nglobal token, warm-started from RoBERTa) in long\nlegal document classiﬁcation. The new LegalLong-\nformer (and its variants) are the new state of the art\nin the long document tasks of LexGLUE.\nIn future work, we would like to further pre-train\nthe proposed BoW variants of LegalBERT, Legal-\nLongformer, and variants on legal corpora, to help\nthem better optimize the newly introduced modiﬁ-\ncations (e.g., TF-IDF embeddings, additional posi-\ntional embeddings, updated attention scheme with\nadditional global tokens). We would also like to ex-\nperiment with long documents from other domains\n(e.g., long business documents).\nAcknowledgments\nThis research has been co-funded by the Euro-\npean Regional Development Fund of the European\nUnion and Greek national funds through the Opera-\ntional Program Competitiveness, Entrepreneurship\nand Innovation, under the call RESEARCH – CRE-\nATE – INNOV ATE (Τ2Ε/uni0394Κ-03849). This work is\nalso partly funded by the Innovation Fund Denmark\n(IFD)7 under File No. 0175-00011A.\nReferences\nMostafa Abdou, Vinit Ravishankar, Artur Kulmizev,\nand Anders Søgaard. 2022. Word order does matter\nand shuﬄed language models know it. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 6907–6919, Dublin, Ireland.\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268–284, Online.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\nCoRR, abs/2004.05150.\n7https://innovationsfonden.dk/en\n138\nGeorgios-Ioannis Brokos, Prodromos Malakasiotis,\nand Ion Androutsopoulos. 2016. Using centroids\nof word embeddings and word mover’s distance\nfor biomedical document retrieval in question an-\nswering. In Proceedings of the 15th Workshop\non Biomedical Natural Language Processing, pages\n114–118, Berlin, Germany. Association for Compu-\ntational Linguistics.\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos\nAletras. 2019a. Neural legal judgment prediction in\nEnglish. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 4317–4323, Florence, Italy. Association\nfor Computational Linguistics.\nIlias Chalkidis, Ion Androutsopoulos, and Achilleas\nMichos. 2018. Obligation and prohibition extrac-\ntion using hierarchical RNNs. In Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 254–259, Melbourne, Australia. Association\nfor Computational Linguistics.\nIlias Chalkidis, Emmanouil Fergadiotis, Prodromos\nMalakasiotis, and Ion Androutsopoulos. 2019b.\nLarge-scale multi-label text classiﬁcation on EU leg-\nislation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 6314–6322, Florence, Italy. Association\nfor Computational Linguistics.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online.\nIlias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapat-\nsanis, Nikolaos Aletras, Ion Androutsopoulos, and\nProdromos Malakasiotis. 2021a. Paragraph-level\nrationale extraction through regularization: A case\nstudy on European court of human rights cases. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 226–241, Online. Association for Computa-\ntional Linguistics.\nIlias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapat-\nsanis, Nikolaos Aletras, Ion Androutsopoulos, and\nProdromos Malakasiotis. 2021b. Paragraph-level\nrationale extraction through regularization: A case\nstudy on european court of human rights cases. In\nProceedings of the Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics, online.\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael\nBommarito, Ion Androutsopoulos, Daniel Martin\nKatz, and Nikolaos Aletras. 2022. Lexglue: A\nbenchmark dataset for legal language understanding\nin english. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4310–4330, Dubln, Ireland.\nCorinna Cortes and Vladimir Vapnik. 1995.\nSupport-vector networks. Maching Learning ,\n20(3):273–297.\nXiang Dai, Ilias Chalkidis, Sune Darkner, and\nDesmond Elliott. 2022. Revisiting transformer-\nbased models for long document classiﬁcation. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022 , Abu Dhabi, UAE. Associa-\ntion for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota.\nSiYu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2021.\nERNIE-Doc: A retrospective long-document model-\ning Transformer. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 2914–2927, Online.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: Decoding-\nenchanced BERT with disentangled attention. In\nInternational Conference on Learning Representa-\ntions.\nJack Hessel and Alexandra Schoﬁeld. 2021. How ef-\nfective is BERT without word ordering? implica-\ntions for language understanding and data privacy.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 2: Short Papers) , pages\n204–211, Online.\nPeng Jin, Yue Zhang, Xingyuan Chen, and Yunqing\nXia. 2016. Bag-of-embeddings for text classiﬁca-\ntion. In Proceedings of the Twenty-Fifth Interna-\ntional Joint Conference on Artiﬁcial Intelligence, IJ-\nCAI’16, page 2824–2830. AAAI Press.\nPrathamesh Kalamkar, Aman Tiwari, Astha Agarwal,\nSaurabh Karn, Smita Gupta, Vivek Raghavan, and\nAshutosh Modi. 2022. Corpus for automatic struc-\nturing of legal documents. In Proceedings of\nthe Thirteenth Language Resources and Evaluation\nConference, pages 4420–4429, Marseille, France.\nEuropean Language Resources Association.\n139\nD. P. Kingma and J. Ba. 2015. Adam: A method for\nstochastic optimization. In Proceedings of the 5th\nInternational Conference on Learning Representa-\ntions (ICLR), San Diego, CA, USA.\nMarco Lippi, Przemysław Pałka, Giuseppe Con-\ntissa, Francesca Lagioia, Hans-Wolfgang Mick-\nlitz, Giovanni Sartor, and Paolo Torroni. 2019.\nCLAUDETTE: an automated detector of potentially\nunfair clauses in online terms of service. Artiﬁcial\nIntelligence and Law, pages 117–139.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nVijit Malik, Rishabh Sanjay, Shubham Kumar Nigam,\nKripabandhu Ghosh, Shouvik Kumar Guha, Arnab\nBhattacharya, and Ashutosh Modi. 2021. ILDC\nfor CJPE: Indian legal documents corpus for court\njudgment prediction and explanation. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 4046–4062,\nOnline. Association for Computational Linguistics.\nChristopher D. Manning, Prabhakar Raghavan, and\nHinrich Sch¨utze. 2008. Introduction to Information\nRetrieval. Cambridge University Press, USA.\nJoel Niklaus, Ilias Chalkidis, and Matthias St ¨urmer.\n2021. Swiss-Court-Predict: A Multilingual Legal\nJudgment Prediction Benchmark. In Proceedings of\nthe 3rd Natural Legal Language Processing Work-\nshop Workshop, Online.\nJoel Niklaus, Matthias St ¨urmer, and Ilias Chalkidis.\n2022. An empirical study on cross-x transfer for le-\ngal judgment prediction. In Proceedings of the 2nd\nConference of the Asia-Paciﬁc Chapter of the Associ-\nation for Computational Linguistics (AACL-IJCNLP\n2022), Online. Association for Computational Lin-\nguistics.\nTatiana Shavrina and Valentin Malykh. 2021. How not\nto lie with a benchmark: Rearranging NLP learder-\nboards.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional\nhypothesis: Order word matters pre-training for lit-\ntle. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2888–2913, Online and Punta Cana, Domini-\ncan Republic.\nHarold J. Spaeth, Lee Epstein, Jeﬀrey A. Segal Andrew\nD. Martin, Theodore J. Ruger, and Sara C. Benesh.\n2020. Supreme Court Database, Version 2020 Re-\nlease 01. Washington University Law.\nDon Tuggener, Pius von D ¨aniken, Thomas Peetz, and\nMark Cieliebak. 2020. LEDGAR: A large-scale\nmulti-label corpus for text classiﬁcation of legal pro-\nvisions in contracts. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n1235–1241, Marseille, France. European Language\nResources Association.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, pages 6000–6010, Long Beach, California,\nUSA.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran As-\nsociates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nChaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu,\nand Maosong Sun. 2021. Lawformer: A pre-trained\nlanguage model for chinese legal long documents.\nAI Open, 2:79–84.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual atten-\ntion. In Proceedings of the 32nd International\nConference on Machine Learning , volume 37 of\nProceedings of Machine Learning Research , pages\n2048–2057, Lille, France. PMLR.\nLiu Yang, Mingyang Zhang, Cheng Li, Michael Ben-\ndersky, and Marc Najork. 2020. Beyond 512 tokens:\nSiamese multi-depth transformer-based hierarchical\nencoder for long-form document matching. In Pro-\nceedings of the 29th ACM International Conference\non Information and Knowledge Management , page\n1725–1734, Virtual Event, Ireland.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Proceedings of the\n33rd International Conference on Neural Informa-\ntion Processing Systems, Red Hook, NY , USA. Cur-\nran Associates Inc.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nattention networks for document classiﬁcation. In\n140\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 1480–1489, San Diego, California. Associa-\ntion for Computational Linguistics.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big Bird: Trans-\nformers for longer sequences. In Advances in Neu-\nral Information Processing Systems , pages 17283–\n17297, online.\nLucia Zheng, Neel Guha, Brandon R. Anderson, Pe-\nter Henderson, and Daniel E. Ho. 2021. When does\npretraining help? assessing self-supervised learning\nfor law and the CaseHOLD dataset. In International\nConference on Artiﬁcial Intelligence and Law.\nHaoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun\nXiao, Zhiyuan Liu, and Maosong Sun. 2018. Le-\ngal judgment prediction via topological learning.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 3540–3549, Brussels, Belgium. Association\nfor Computational Linguistics.\nA Additional Material\nIn Figures 4–5, we show boxplots of the aver-\nage text length (in sub-words) across LexGLUE\ndatasets before and after word deduplication.\n141\nFigure 4: Average text length (in sub-words) across LexGLUE datasets before word deduplication.\nFigure 5: Average text length (in sub-words) across LexGLUE datasets after word deduplication.\n142",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.764767050743103
    },
    {
      "name": "Transformer",
      "score": 0.7196205258369446
    },
    {
      "name": "Support vector machine",
      "score": 0.5644195079803467
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5546796321868896
    },
    {
      "name": "Natural language processing",
      "score": 0.40762054920196533
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37508147954940796
    },
    {
      "name": "Machine learning",
      "score": 0.36036068201065063
    },
    {
      "name": "Engineering",
      "score": 0.09505683183670044
    },
    {
      "name": "Electrical engineering",
      "score": 0.060814470052719116
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I73142707",
      "name": "Athens University of Economics and Business",
      "country": "GR"
    },
    {
      "id": "https://openalex.org/I124055696",
      "name": "University of Copenhagen",
      "country": "DK"
    }
  ]
}