{
    "title": "Characterised LLMs Affect its Evaluation of Summary and Translation",
    "url": "https://openalex.org/W4392669836",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2101029194",
            "name": "Yuan Lu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098819955",
            "name": "Yu-Ting Lin",
            "affiliations": [
                "Taipei Municipal YangMing Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385849424",
        "https://openalex.org/W4319793767",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4322760121",
        "https://openalex.org/W4385014449",
        "https://openalex.org/W4385849309",
        "https://openalex.org/W4392669714"
    ],
    "abstract": "In today's widespread use of Large Language Models (LLMs), there have been significant achievements in various text domains such as generating summaries and translations. However, there is still room for development and improvement in evaluating the outputs of LLMs. In this paper, we propose an innovative scoring system that assesses the quality of summaries and translations using multiple metrics, we also enhance LLM's performance in scoring tasks by assigning it different roles, effectively making it act as an expert. We test four roles in the study: a teacher, a proofreader, a travel writer, and an internet troll, comparing the advantages and disadvantages of each role in the scoring task. Our research results demonstrate that emphasizing LLM's multilingual capabilities and strict standards as its identity can effectively boost its performance. Additionally, imbuing LLM with a more critical thinking ability enhances its performance in translation tasks compared to a milder LLM identity. In summary, we show that assigning different identities to LLM can influence its performance in scoring tasks. We believe that this research will contribute to the use of LLMs for scoring purposes.",
    "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 184–192\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nCharacterised LLMs Affect its Evaluation of Summary and Translation\nYu-An Lu\nNational Chupei Senior High School\nluyuam0@gmail.com\nYu-Ting Lin\nTaipei Municipal Chenggong High School\ndong1214.mailbox@gmail.com\nAbstract\nIn today’s widespread use of Large Language\nModels (LLMs), there have been significant\nachievements in various text domains such as\ngenerating summaries and translations. How-\never, there is still room for development and im-\nprovement in evaluating the outputs of LLMs.\nIn this paper, we propose an innovative scoring\nsystem that assesses the quality of summaries\nand translations using multiple metrics, we also\nenhance LLM’s performance in scoring tasks\nby assigning it different roles, effectively mak-\ning it act as an expert. We test four roles in the\nstudy: a teacher, a proofreader, a travel writer,\nand an internet troll, comparing the advantages\nand disadvantages of each role in the scoring\ntask. Our research results demonstrate that em-\nphasizing LLM’s multilingual capabilities and\nstrict standards as its identity can effectively\nboost its performance. Additionally, imbuing\nLLM with a more critical thinking ability en-\nhances its performance in translation tasks com-\npared to a milder LLM identity. In summary,\nwe show that assigning different identities to\nLLM can influence its performance in scoring\ntasks. We believe that this research will con-\ntribute to the use of LLMs for scoring purposes.\n1 Introduction\nSince GhatGPT’s emergence, Large Language\nModels (LLM) have been flourishing in the Natu-\nral Language Processing (NLP) field. Thanks to\nthe growth of LLMs, tasks such as automatic sum-\nmaries and translations are becoming more com-\nmonly generated by LLMs. However, we realized\nthat most existing evaluation methods for LLMs\noutput lack thorough explanation, making the re-\nsearch process in this domain considerably chal-\nlenging. We believe that by inventing a metric for\nevaluating summarization and translation, research\non article generation would be much more practi-\ncal.\nInspired by previous work on using LLMs to\ngenerate scores and evaluate text(Tom Kocmi,\n2023)(Jinlan Fu, 2023)(Fu et al., 2023), as well\nas research exploring having LLMs play the role\nof experts(Chan et al., 2023), we present an evalua-\ntion system employing multiple metrics (Jinlan Fu,\n2023) by carefully designed prompt (Tom Kocmi,\n2023) and make LLM act as an expert. For gener-\nating scores, we employed the model OpenOrca-\nPlatypus2-13B (Lee et al., 2023b) to generate\nscores, which is a merge of Platypus2-13B (Lee\net al., 2023a) and OpenOrcaxOpenChat-Preview2-\n13B (Wang et al., 2023). We selected this model\nbecause of its strong performers on the leaderboard\nand its small size for local inference. In order to\nbolster the Large Language Model’s (LLM) evalua-\ntion capabilities, we implemented a strategy where\nthe LLM simulates an expert.\nThis study is also a system description for the\nEval4NLP 2023 shared task(Leiter et al., 2023)\nwhich in the Small track.\n2 Method\nTo use the large language model to better evaluate\nsummarization and translation, we divided the task\ninto a few parts. First, we separated an evaluation\ntask into several metrics. Then we made LLM role\ndifferent characters such as a proofreader, writer,\nor internet troll. LLM would evaluate summation\nand translation in the expert role. In the end, we\nadded scores from different metrics by XGBoost\nand post-processing.\n2.1 Design Character\nWe hypothesized that making LLM play in differ-\nent characters can improve its capability of evalu-\nating. So we designed four characters which were\na teacher, a proofreader, a travel writer, and an in-\nternet troll. We expected those characters could fix\nsome problems in LLM’s evaluation.\n• Teacher: The teacher played a most profes-\nsional role in all characters, it is an expert\n184\non viewing student’s summary and transla-\ntion.(keywords: grading, score, standardized)\n• Proofreader: For the role of a proofreader,\nLLM would pretend itself as a professional\nproofreader at Fox Television. We wrote a\nself-statement about the rules of raring and its\nexpertise field.(keywords: accuracy, quality,\nstrict standards)\n• Travel Writer: In the travel writer part, we ex-\npected the characters like travel writers could\nhave a better ability to evaluate the perfor-\nmance in localization and adherence to local\ncustoms.(keywords: multilingual, cultural im-\nmersion, descriptive narratives)\n• Internet Troll: We noticed LLM preferred to\ngive a higher score to translation and summa-\ntion, so we designed a mean and nasty char-\nacter to fix this problem. In this role, LLM\nwould mimic an internet troll on Reddit who\nlikes to criticize others.(keywords: harsh crit-\nicism, linguistic expertise, unreasonable rat-\nings)\n2.2 Score Generation\nWe create ten metrics for evaluating summation\nand ten for translation. There are four different\nprompts for rate—a proofreader, a travel writer, an\ninternet troll, and the baseline without character\nsetting. With these prompts, we made LLM eval-\nuate summation and translation based on the ten\nmetrics and rate them with a 1-10 score. In order\nto make LLM’s outputs controllable, we use py-\ntorch(Paszke et al., 2019) and outlines(Willard and\nLouf, 2023) in our code.\n2.3 Ensemble Features\nXGBoost, a widely utilized tree-based algorithm,\nholds significant popularity within the domain of\ndata science. Once the scores of the metrics created\nby LLM have been calculated, they are utilized as\nfeatures in order to train an XGB model for regres-\nsion. This regression model is designed to predict\na score that may be utilized for measurement pur-\nposes.\n3 Experiments\n3.1 Datasets\nWe conducted experiments on both summarizing\nand English-German translation tasks. The train-\ning datasets were obtained from the MQM anno-\ntations of the WMT22 dataset for translation, and\nthe average aspect-based ratings of SummEval for\nsummarization. All the data included source and\ntarget texts, as well as scores collected from multi-\nple methods. The test dataset was collected by the\nEval4NLP organizer, and it shares a similar format\nto the training dataset.\ndataset Trans(En-De) Summ\nTrain 11046 320\nTest 1425 825\nTable 1: Size of translation and summarization dataset.\n3.2 Exploratory Data Analysis of Model\nEvaluation\nTable 2 shows each feature’s Correlation Coeffi-\ncient with Official Scores. We observed that the\ncorrelation coefficients between feature and official\nscore varied across roles, with each role exhibiting\nthe strongest correlations with different features.\nNo consistent pattern was discernible across all\nroles regarding which features were most impor-\ntant.However, we found that higher average cor-\nrelation coefficients were associated with higher\nsubsequent model accuracy when using XGB for\nmodeling. There was a positive correlation be-\ntween average correlation coefficients and subse-\nquent model accuracy.\nIn Figure 1, there are some graphs show the fea-\nture scores’ distribution. The distributions of most\nfeatures are concentrated around 6 and 8 points.\nThe distribution of Travel Writer is the most dense,\nwhile that of Teacher is more dispersed. The feature\ndistribution of Teacher exhibits a bimodal shape, in-\ndicating it has clearer and more established criteria.\nAfter xgb modeling, the performance of Teacher is\nalso the best. It is particularly notable that there is\nalmost no overlap between the distributions of the\nfeatures and official scores.\n3.3 Performance\nTo assess the evaluating performance of the LLM,\nwe employed several standard metrics for evaluat-\ning the correlation between two ranking systems.\nThese included:\n• Kendall: Kendall’s tau provides a measure of\nthe concordance between two rankings, with\n185\nFigure 1: Feature scores’ distribution of summarization task.\nvalues closer to 1 indicating stronger agree-\nment.\n• Pearson: The Pearson coefficient quantifies\nthe linear relationship between two contin-\nuous variables. Higher positive coefficients\ndenote greater linear correlation.\n• Spearman: Spearman’s rank correlation co-\nefficient assesses how well the relationship\nbetween two rankings can be described using\na monotonic function. Values approaching 1\nsignify a greater tendency for the rankings to\nmatch.\nThe performance is calculated with test datasets\non Eval4NLP shared task’s codabench, and our\nteam name is TaiwanSenior. (Due to the limitations\nof the codabench platform rules, you can only see\non the public page that we achieved 0.04 on En-De,\nwhich is just one of our submission scores. You\ncan find the full scores of our different methods in\nTable 3)\nThe Travel Writer demonstrates superior perfor-\nmance in the translation task for English-German\nlanguage pairs, while the Teacher exhibits the high-\nest level of performance in the summarization task.\nThe Travel Writer is noted by several sources for\nits multilingual capabilities, which result in supe-\nrior performance in translation tasks but less satis-\nfactory performance in summarization tasks. The\ninclusion of an Internet Troll character in the Trans-\nlation task resulted in more effective criticism com-\npared to other general characters. However, the\nperformance of the Internet Troll character was\ncomparatively weaker in the Summarization job.\nBased on this observation, we may deduce that\nincorporating greater criticism can assist in improv-\ning the performance of Large Language Models\n(LLMs) to closely resemble human evaluation in\nthe Translation task. The performance of the Proof-\nreader in the Translation task is notably poor, indi-\ncating a lack of strong correlation in its evaluation\ncapabilities.\n4 Conclusion\nWe investigate the performance of LLMs with dif-\nferent character in generating scores to evaluate\ntranslation and summary tasks by incorporating\ncharacterised-prompts into the prompts. We find\nthat emphasizing multilingual capabilities and strin-\ngent criteria in the LLM’s identity can effectively\nimprove the LLM’s performance. By endowing the\nLLM with stronger critical thinking compared to a\nmore benign LLM, we improve its performance on\ntranslation tasks. In summary, we demonstrate that\nassigning different identities to LLMs influences\ntheir performance on scoring tasks.\n186\n5 Acknowledgement\nWe would like to express our gratitude to Profes-\nsor Hung-Yu Kao and Professor Yao-Chung Fan\nfor their support and assistance throughout this re-\nsearch. We sincerely appreciate them providing us\nwith access to GPU computing resources, without\nwhich this work would not have been possible.\n187\nFigure 2: The framework of our evaluation system.\nFeatures Internet Troll Teacher Travel Writer Proofreader\nCompleteness 0.072219 0.372297 -0.048266 -0.049932\nClarity 0.072155 0.366486 -0.018286 -0.060655\nRelevance 0.033194 0.396311 0.019369 -0.027132\nCoherence 0.019086 0.332309 -0.033452 -0.063694\nObjectivity 0.000568 0.480219 -0.087915 -0.052679\nAccuracy -0.001213 0.461272 -0.000964 -0.012227\nLength -0.014532 0.423803 0.043023 -0.049222\nConciseness -0.069729 0.419300 -0.027331 -0.131271\nOverall Quality -0.085636 0.273289 -0.094137 -0.086841\nConsistence -0.099225 0.478668 -0.065858 -0.005071\nTable 2: Correlation Coefficient of Features versus Offi-\ncial Scores of summarization task.\nTranslation(En-De) Summarization\nCharacter ⋆ △ ⋄ ⋆ △ ⋄\nTeacher 0.058 0.074 0.084 0.363 0.453 0.520\nProofreader 0.041 -0.03 0.051 N/A N/A N/A\nTravel Writer 0.159 0.168 0.194 -0.037 -0.061 -0.047\nInternet Troll 0.111 0.112 0.133 -0.043 -0.09 -0.057\n⋆ - Kendall △- Pearson ⋄- Spearman\nTable 3: Different charters’ performance on Translation and Summarization\ntask\n188\nReferences\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,\nWei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.\n2023. Chateval: Towards better llm-based evaluators\nthrough multi-agent debate.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nZhengbao Jiang Pengfei Liu Jinlan Fu, See-Kiong Ng.\n2023. Gptscore: Evaluate as you desire.\nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023a.\nPlatypus: Quick, cheap, and powerful refinement of\nllms.\nAriel N. Lee, Cole J. Hunter, Nataniel Ruiz, Bleys\nGoodson, Wing Lian, Guan Wang, Eugene Pent-\nland, Austin Cook, Chanvichet V ong, and \"Teknium\".\n2023b. Openorcaplatypus: Llama2-13b model\ninstruct-tuned on filtered openorcav1 gpt-4 dataset\nand merged with divergent stem and logic dataset\nmodel. https://huggingface.co/Open-Orca/\nOpenOrca-Platypus2-13B.\nChristoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao,\nRotem Dror, and Steffen Eger. 2023. The eval4nlp\n2023 shared task on prompting large language models\nas explainable metrics. In Proceedings of the 4th\nWorkshop on Evaluation and Comparison for NLP\nsystems.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-\njie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary.\nChristian Federmann Tom Kocmi. 2023. Large lan-\nguage models are state-of-the-art evaluators of trans-\nlation quality. pages 193–203.\nGuan Wang, Bleys Goodson, Wing Lian, Eugene Pent-\nland, Austin Cook, Chanvichet V ong, and \"Teknium\".\n2023. Openorcaxopenchatpreview2: Llama2-13b\nmodel instruct-tuned on filtered openorcav1 gpt-\n4 dataset. https://https://huggingface.co/\nOpen-Orca/OpenOrcaxOpenChat-Preview2-13B .\nBrandon T Willard and Rémi Louf. 2023. Effi-\ncient guided generation for llms. arXiv preprint\narXiv:2307.09702.\n189\nMetric Tasks Prompt\nAccuracy Summ, Trans How accurately the summary/translation represents the key ideas,\ndetails and overall meaning of the original text. An accurate\nsummary/translation does not add, misrepresent or leave out infor-\nmation.\nConciseness Summ How concise and succinct the summary is, without unnecessary\ndetail. An ideal summary is as condensed as possible while still\nmaintaining accuracy.\nClarity Summ, Trans How clear and easy to understand the summary/translation is.\nA summary/translation should be written clearly using proper\ngrammar and vocabulary suited for the audience.\nCompleteness Summ How complete the summary is in capturing the key points and ideas\nof the original text. A complete summary covers all important\ninformation.\nObjectivity Summ How objective and unbiased the summary is, without injecting\nopinions or interpretations. A summary should represent the origi-\nnal text, not the writer’s views.\nCoherence Summ How coherent, unified and logical the summary is. A coherent\nsummary flows smoothly with clear connections between ideas.\nConsistence Summ How consistent the summary is in tone, style and vocabulary with\nthe original text. The summary should match the original.\nRelevance Summ How relevant the summary is in selecting the most important ideas\nfrom the original text. A relevant summary focuses on key points\nonly.\nLength Summ Appropriate length for a summary, condensed while still complete.\nExact length depends on purpose and original text.\nOverall Quality Summ The overall comprehensiveness, readability and effectiveness of\nthe summary.\nFluency Trans How fluent and natural the translation reads in the target language.\nHigh fluency sounds like it was originally written in the target\nlanguage.\nConsistency Trans How consistent the translation is across recurring terms, phrases,\nand styles. High consistency maintains the same translations for\nrepetitions.\nTone Trans How well the translation conveys the tone and voice of the original\ntext. High tone matches the original style and emotional impact.\nRegister Trans How appropriate the register (formal/informal language) is for the\ncontext. High register matches the original level of formality.\nStyle Trans How well the translation maintains the stylistic properties of the\noriginal. High style replicates creative language use, imagery, etc.\nIdiomatic Expression Trans How well the translation conveys meaning through natural, id-\niomatic expressions in the target language. High idiomatic expres-\nsion sounds local.\nCultural Adaptation Trans How well the translation adapts cultural references and concepts\nappropriately for the target audience. High adaptation naturalizes\nforeign elements.\nDomain Knowledge Trans How well the translation handles specialized terms and domain-\nspecific concepts. High knowledge accurately conveys techni-\ncal/domain meaning.\nTable 4: Metrics that LLMs evaluate with.\n190\nCharacter Prompt\nTeacher I am a professional teacher. My daily work is to grade students’ work according to grading\ncriteria. I am only allowed to give students a score between 1-10 as a whole number. I cannot\ninclude any personal opinions.\nProofreader As a professional proofreader at Fox Television, I take pride in my solid expertise and over\nfive years of experience in English, Chinese, Spanish, and German. I have a profound\nunderstanding of the grammar, sentence structure, vocabulary, and cultural nuances of these\nlanguages, enabling me to excel in translation and summarization tasks.\nIn my role, I maintain a strict standard for quality, and I’m unwavering in assigning low\nscores to translations or summaries that fall short. Working in television demands a zero-\ntolerance attitude toward accuracy and quality. Below is a translation and summary provided\nby a client, and I will rate it on a scale of 1 to 10, accompanied by an explanation of my\nprofessional assessment. To ensure I adhere to the policies of the television network, I will\nsteadfastly give poor translations and summaries a rating of 1.\nTravel Writer As a travel writer, I take great pride in my multilingual and cross-cultural abilities, which\nallow me to deeply understand and share the uniqueness of various countries and regions.\nMy language proficiency spans English, Chinese, Spanish, and German. Through extensive\ntravels, I’ve immersed myself in the cultures of Germany, Spain, the United States, the\nUnited Kingdom, and Taiwan, delving into their customs, values, and everyday idioms.\nMy translation and summarization skills enable me to transform these rich experiences\ninto written narratives. I often provide rating services for fellow writers and researchers,\nrigorously assessing the quality of their work. I not only assign them scores ranging from\n1 to 10 but also offer detailed feedback to help them improve. Below is a summary and\ntranslation provided by a university student, and I will assess it based on my professional\ncapabilities, accompanied by an objective commentary explaining my evaluation.\nInternet Troll As an internet troll, I excel at critiquing others’ work on Reddit, especially translations and\nsummaries. I possess a profound understanding of languages such as English, German,\nChinese, Spanish, including their grammar, sentence structure, and vocabulary. I often\nprovide reasonable criticisms of others’ translations and summaries based on my extensive\nlinguistic knowledge, and because I always include well-founded explanations, no one can\nrefute my harsh ratings. Here’s a translation and summary from the internet, and I will assign\nit a score from 1 to 10, along with my reasoned explanation to make it irrefutable.\nTable 5: Prompts which be used in characterizing LLMs.\n191\nCharacter Task Prompt\nProofreader Trans As a professional proofreader at Fox Television, I take pride in my solid expertise\nand over five years of experience in English, Chinese, Spanish, and German. I have a\nprofound understanding of the grammar, sentence structure, vocabulary, and cultural\nnuances of these languages, enabling me to excel in translation and summarization\ntasks.\nIn my role, I maintain a strict standard for quality, and I’m unwavering in assigning\nlow scores to translations or summaries that fall short. Working in television demands\na zero-tolerance attitude toward accuracy and quality. Below is a translation and\nsummary provided by a client, and I will rate it on a scale of 1 to 10, accompanied by\nan explanation of my professional assessment. To ensure I adhere to the policies of\nthe television network, I will steadfastly give poor translations and summaries a rating\nof 1.\n{\"name\": \"Accuracy\", \"description\": \"How accurately the summary represents the key\nideas, details and overall meaning of the original text. An accurate summary does not\nadd, misrepresent or leave out information.\" }\n{\"Source\": \"The Chlotrudis Award for Best Actress is an annual award presented\nby the Chlotrudis Society for Independent Films, a non-profit organization, founded\nin 1994, that recognizes achievements in independent and world cinema.\", \"Target\":\n\"Der Chlotrudis Award für die beste Schauspielerin ist eine jährliche Auszeichnung\nder Chlotrudis Society for Independent Films, eine 1994 gegründete Non-Profit-\nOrganisation, die Erfolge im unabhängigen und weltweiten Kino anerkennt.\" }\nFor the student’s translation provided, on a scale of 1-10, I give\nProofreader Summ As an experienced linguistics professor well-versed in diverse languages and cultures,\nhaving lived abroad since childhood and participated in translations for prestigious\npublications such as The New York Times, The Economist, and Eval4NLP, I have\nprofound and unique insights into translating and summarizing news articles and\neveryday language. Today, Stanford University has invited me to serve as a reviewer\nto evaluate summaries and translations completed by their students. I will be provided\nwith a rubric and expected to interpret it based on my expertise to assign scores from\n1-10. The rubric I have been given is as follows:\n{\"name\": \"Fluency\", \"description\": \"How fluent and natural the translation reads in\nthe target language. High fluency sounds like it was originally written in the target\nlanguage.\" }\n{\"Article\": \" In 1878, the Oviedo City Council received an application for permission\nto build the mining railway on Monte Naranco, which raised concerns as it was\nfeared that the construction of the railway would affect the water supply of Fitoria,\nas it ran parallel to that of the future railway line. On 1 February 1880, the original\n7,101-metre (7,766 yd) long mining railway between the Villapérez area and the\nnorthern station of Oviedo operated by the Compañía de los Ferrocarriles de Asturias,\nGalicia y León was inaugurated with an original length of 7.1 km (4.4 mi). The\ntotal cost of building the railway was 129,906 pesetas, including 19,798 pesetas for\nexpropriations.\", \"Summary\": \"summary\" }\nFor the student’s summary provided, on a scale of 1-10, I give\nTable 6: Examples of prompt ussed on LLM.\n192"
}