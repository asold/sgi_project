{
  "title": "AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages",
  "url": "https://openalex.org/W4385574228",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3013057816",
      "name": "Bonaventure F. P. Dossou",
      "affiliations": [
        "McGill University",
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4208318288",
      "name": "Atnafu Lambebo Tonja",
      "affiliations": [
        "Instituto Politécnico Nacional"
      ]
    },
    {
      "id": "https://openalex.org/A5031390170",
      "name": "Oreen Yousuf",
      "affiliations": [
        "Uppsala University"
      ]
    },
    {
      "id": "https://openalex.org/A3092425591",
      "name": "Salomey Osei",
      "affiliations": [
        "Universidad de Deusto"
      ]
    },
    {
      "id": "https://openalex.org/A2288707658",
      "name": "Abigail Oppong",
      "affiliations": [
        "Ashesi University"
      ]
    },
    {
      "id": "https://openalex.org/A5014914162",
      "name": "Iyanuoluwa Shode",
      "affiliations": [
        "Montclair State University"
      ]
    },
    {
      "id": "https://openalex.org/A5029191755",
      "name": "Oluwabusayo Olufunke Awoyomi",
      "affiliations": [
        "College of Saint Rose"
      ]
    },
    {
      "id": "https://openalex.org/A3013461009",
      "name": "Chris Emezue",
      "affiliations": [
        "Technical University of Munich",
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W1956471287",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W4385572824",
    "https://openalex.org/W3105522431",
    "https://openalex.org/W4221155692",
    "https://openalex.org/W4224312738",
    "https://openalex.org/W2951786554",
    "https://openalex.org/W4289685901",
    "https://openalex.org/W4229014795",
    "https://openalex.org/W4226494779",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W571987451",
    "https://openalex.org/W4224001239",
    "https://openalex.org/W1526974435",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3117519745",
    "https://openalex.org/W4224255380",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4285233740",
    "https://openalex.org/W3103147437",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W4282915497",
    "https://openalex.org/W2785813126"
  ],
  "abstract": "Bonaventure F. P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei, Abigail Oppong, Iyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, Chris Emezue. Proceedings of The Third Workshop on Simple and Efficient Natural Language Processing (SustaiNLP). 2022.",
  "full_text": "AfroLM: A Self-Active Learning-based Multilingual Pretrained Language\nModel for 23 African Languages\nBonaventure F. P. Dossou1,2,∗, Atnafu Lambebo Tonja3,∗, Oreen Yousuf4,∗, Salomey Osei5,∗,\nAbigail Oppong6,∗, Iyanuoluwa Shode7,∗, Oluwabusayo Olufunke Awoyomi8,∗, Chris Chinenye Emezue1,9,∗\n∗Masakhane NLP, 1Mila Quebec AI Institute, Canada, 2 McGill University, Canada, 3Instituto Politécnico Nacional, Mexico,\n4Uppsala University, Sweden, 5Universidad de Deusto, Spain 6Ashesi University, Ghana, 7Montclair State University, USA,\n8The College of Saint Rose, USA, 9Technical University of Munich, Germany\nAbstract\nIn recent years, multilingual pre-trained lan-\nguage models have gained prominence due\nto their remarkable performance on numer-\nous downstream Natural Language Process-\ning tasks (NLP). However, pre-training these\nlarge multilingual language models requires\na lot of training data, which is not available\nfor African Languages. Active learning is a\nsemi-supervised learning algorithm, in which\na model consistently and dynamically learns\nto identify the most beneficial samples to train\nitself on, in order to achieve better optimization\nand performance on downstream tasks. Further-\nmore, active learning effectively and practically\naddresses real-world data scarcity. Despite all\nits benefits, active learning, in the context of\nNLP and especially multilingual language mod-\nels pretraining, has received little considera-\ntion. In this paper, we present AfroLM, a\nmultilingual language model pretrained from\nscratch on 23 African languages (the largest\neffort to date) using our novel self-active learn-\ning framework. Pretrained on a dataset signif-\nicantly (14x) smaller than existing baselines,\nAfroLM outperforms many multilingual pre-\ntrained language models (AfriBERTa, XLMR-\nbase, mBERT) on various NLP downstream\ntasks (NER, text classification, and sentiment\nanalysis). Additional out-of-domain sentiment\nanalysis experiments show thatAfroLM is able\nto generalize well across various domains. We\nrelease the code source, and our datasets used in\nour framework at https://github.com/\nbonaventuredossou/MLM_AL.\n1 Introduction\nWith the appearance of Transformer models\n(Vaswani et al., 2017), the field of Natural Lan-\nguage Processing (NLP) has seen the emergence of\npowerful multilingual pre-trained language models\n(MPLMs), such as mBERT (Devlin et al., 2018),\nXLM-RoBERTa (XML-R) (Conneau et al., 2019),\nand mT5 (Xue et al., 2021). These prominent mod-\nels have helped achieve state-of-the-art (SOTA)\nperformance in many downstream NLP tasks such\nas named entity recognition (NER) (Alabi et al.,\n2022a; Adelani et al., 2021a; Devlin et al., 2018;\nConneau et al., 2019), text classification (Kelechi\net al., 2021), and sentiment analysis (Alabi et al.,\n2022a; Adelani et al., 2021a; Devlin et al., 2018;\nConneau et al., 2019). However they usually re-\nquire a large amount of unlabeled text corpora\nfor good performance: mBERT was trained on\nWikipedia (2,500M words) and BookCorpus (Zhu\net al., 2015) (800M words) across 104 languages\n- 5 of which are African; mT5 supports 101 lan-\nguages (13 African) and XLM-R supports 100\nlanguages (8 African), and were trained on mC4\n(Xue et al., 2021) and CommonCrawl data (Wen-\nzek et al., 2019), respectively. This requirement\nfor large-scale datasets contrasts sharply with the\nscarcity of available text corpora for African lan-\nguages, which has pushed them into low-resource\nsettings and largely excluded them from the pre-\ntraining phase of these large pre-trained models\n(Joshi et al., 2020; Adelani et al., 2022a). This ex-\nclusion, leads very often to a poor performance on\nlanguages unseen during pre-training (Alabi et al.,\n2022a) which eventually leads to inability to carry\nout the required NLP task.\nActive learning is a semi-supervised machine\nlearning algorithm that makes use of only a few\ninitial training data points to achieve better per-\nformance of a given model M. The optimization\nis done by iteratively training M, and using an-\nother model N, usually referred to as the oracle,\nto choose new training samples that will help M\nfind better configurations while improving its per-\nformance (e.g., prediction accuracy). This makes\nactive learning a prevalent paradigm to cope with\ndata scarcity. The efficiency of active learning (i.e.\nits ability to produce better performance despite\nbeing trained on a smaller training data) has been\nproven in tasks such as biological sequence de-\nsign (Jain et al., 2022), chemical sampling (Smith\nLanguages Family Writing System African Region No of Speakers Initial # of Sentences Source Size (MB)\nAmharic (amh) Afro-Asiatic/Semitic Ge’ez script East 57M 655,079 ✥,✟,★ 279Afan Oromo (orm) Afro-Asiatic/Cushitic Latin script East 37.4M 50,105 ✟ 9.87Bambara (bam) NC/Manding Latin, Arabic(Ajami), N’ko West 14M 6,618 ✥ 1.00Ghomálá’ (bbj) NC/Grassfields Latin script Central 1M 4,841 ✥ 0.50\nÉwé (ewe) NC/Kwa Latin (Ewe alphabet) West 7M 5,615 ✥ 0.50Fon (fon) NC/V olta-Niger Latin script West 1.7M 5,448 ✥ 1.00Hausa (hau) Afro-Asiatic/Chadic Latin (Boko alphabet) West 63M 1,626,330 ✥,✟,★ 208\nIgbo (ibo) NC/V olta-Niger Latin (Önwu alphabet) West 27M 437,737 ✥,✟,★ 63Kinyarwanda (kin) NC/Rwanda-Rundi Latin script Central 9.8M 84,994 ➸,✟,✥ 37.70Lingala (lin) NC/Bang Latin script Central & East 45M 398,440 ✥ 45.90Luganda (lug) NC/Bantu Latin script (Ganda alphabet) East 7M 74,754 ✟,✥ 8.34Luo (luo) Nilo-Saharan Latin script East 4M 8,684 ✟ 1.29Mooré (mos) NC/Gur Latin script West 8M 27,908 ✥,✟ 5.05Chewa (nya) NC/Nyasa Latin script South & East 12M 8,000 ✥ 1.66Naija (pcm) English-Creole Latin script West 75M 345,694 ✥,✟,★ 101Shona (sna) NC/Bantu Latin script (Shona alphabet) Southeast 12M 187,810 ✥,✟ 32.80Swahili (swa) NC / Bantu Latin script (Roman Swahili alphabet) East & Central 98M 1,935,485 ✥,✟,★ 276Setswana (tsn) NC / Bantu Latin (Tswana alphabet) South 14M 13,958 ✥,✟ 2.21Akan/Twi (twi) NC / Kwa Latin script West 9M 14,701 ✥ 1.61Wolof (wol) NC / Senegambia Latin (Wolof alphabet) West 5M 13,868 ✟ 2.20Xhosa (xho) NC/Zunda Latin (Xhosa alphabet) South 20M 93,288 ✥,✟ 17.40Yor˚ ubá (yor) NC / V olta-Niger Latin (Yorùbá alphabet) West 42M 290,999 ✥,✟,★ 45.9isiZulu (zul) NC / Bantu Latin (Zulu alphabet) South 27M 194,562 ✥,✟ 33.70\nTable 1: Languages Corpora Details. Legends: (Adelani et al., 2022a) → ✥, (Alabi et al., 2022a) → ✟, (Kelechi\net al., 2021) → ★, (Niyongabo et al., 2020) → ➸.\net al., 2018), and Deep Bayesian (DB) approaches\non image data (Gal et al., 2017). Also, most of\nthe work on deep active learning focuses on image\nclassification with Convolutional Neural Networks\n(CNNs). It should be noted that active learning has\nbeen greatly explored and used to perform classi-\nfication tasks, but not in language generation and\nunderstanding, and this is what we hope to address.\nA study of active learning in the context of\nNLP has been carried out by (Siddhant and Lip-\nton, 2018). In their study, it is shown that active\nlearning with DB networks coupled with uncer-\ntainty measures and acquisition function outper-\nforms several i.i.d baselines. They showed that\nwith only 20% of samples labeled, their approach\nreached an accuracy of 98-99% on the Named En-\ntity Recognition (NER) task, while i.i.d tasks re-\nquired 50% of labelled data to achieve compara-\nble performance. In their study on clinical texts,\n(Chen et al., 2015) also proved that active learning\nalgorithms outperformed other learning methods.\n(Ein-Dor et al., 2020; Tonneau et al., 2022) on their\nworks with BERT model(s) (for n different lan-\nguages, there were n different BERT-based models)\nwent further by showing that active learning works\nwith a balanced and unbalanced dataset. They also\nshowed that the different active learning methods\nperformed relatively the same.\nIn our work, we fixed M=N (hence the title self-\nactive learning). In our framework, we give M\nthe ability to query itself, and use the knowledge\nacquired during each active learning round to con-\nstruct new data points (from existing ones) that will\nbe used for the next active learning round.\nWe considered a diverse set of 23 African lan-\nguages spread across the African continent. The\nselected languages are spoken in the south, cen-\ntral, east, and western regions of Africa. The lan-\nguages cover four language families: Afro-Asiatic\n(e.g., Amharic, Hausa, Afan Omoro), Niger-Congo\n(NC) (e.g., Yorùbá, Bambara, Fon), English-Creole\n(Naija) and Nilo-Saharan (Luo) (see Appendix A\nfor details). For each language, a dataset was col-\nlected from the news domain, which encompassed\nmany topics such as health, politics, society, sport,\nenvironment, etc.\nOur primary contribution to this work is our\nproposal of a self-active learning framework\nin which we pre-train the biggest Multilingual\nAfrican Language Model (for the number of lan-\nguages covered) to date, and we show that our\nsetup is very data-efficient and provides improve-\nments on downstream NLP tasks such as NER, text\nclassification, and sentiment analysis (even on out-\nof-domain experiments).\n2 Related Works on MPLMs for African\nLanguages\nLanguage adaptive fine-tuning (LAFT) is one of\nthe best approaches to adapt MPLMs to a new\nlanguage. This entails fine-tuning an MPLM on\nmonolingual texts of the said language with the\nsame pre-training objective. However, this can-\nnot be efficiently applied to African languages fac-\ning data-scarcity. (Alabi et al., 2022b) proposed a\nnew adaptation method called Multilingual adap-\ntive fine-tuning (MAFT), as an approach to adapt\nMPLMs to many African languages with a single\nFigure 1: Self-Active Learning Framework). The process is designed in 4 stages (fully explained and detailed in\nAlgorithm 1): (1) ■ Dataset split for current Active Learning round, (2) ■ Active Learning round training, (3) ■\nGeneration of new sentence samples for the current round, and (4) ■ Augmentation of the datasets of all languages.\nmodel. Their results show that MAFT is competi-\ntive to LAFT while providing a single model rather\nthan many models that are specific for individual\nlanguages. Nevertheless, Alabi et al. (2022b)’s ap-\nproach still works under the assumption that one\ndoes not need to train a model from scratch for lan-\nguages in the low-resource settings, as they could\nbenefit from high-resource languages. We find that\nthis is not always the case.\n(Kelechi et al., 2021) introduced AfriBERTa,\na multilingual language model trained on less\nthan 1GB of data from 11 African languages.\nTraining AfriBERTa from scratch showcased how\nAfrican languages can benefit from being included\nin the pre-training stage of MPLMs. AfriBERTa\nproduced competitive results compared to exist-\ning MPLMs (e.g., mBERT, XLM-R), and outper-\nformed them on text classification and NER tasks.\nRather than relying on high-resource languages for\ntransfer-learning, AfriBERTa leverages the linguis-\ntic similarity between languages with low-resource\nsettings to produce promising results. (Kelechi\net al., 2021) empirically demonstrates that this is\nmore beneficial to these languages and is crucial\nin assessing the viability of language models pre-\ntrained on small datasets.\n(Antoine and Niyongabo, 2022) went beyond\nthe linguistic taxonomy in creating KinyaBERT,\na morphology-aware language model for Kin-\nyarwanda. Trained on a 2.4GB corpus contain-\ning news articles from 370 websites registered\nbetween 2011 and 2021, KinyaBERT boasts a\nTransformer-like architecture that helps the repre-\nsentation of morphological compositionality. Their\nexperiments outperformed solid baseline results\nfor tasks such as NER and machine-translated\nGLUE on the Kinyarwanda language. These re-\nsults demonstrated the effectiveness of not relying\non transfer learning from high resource languages\nand rather explicitly incorporating morphological\ninformation of the African languages in their pre-\ntraining stage.\nIn the next section, we will describe our self-\nactive learning framework, and the core details of\nour approach.\n3 Self-Active Learning Framework\nIn this section, we describe our self-active learn-\ning framework (Figure 1). In Algorithm 1, we\npresent a single active learning loop. In our current\nwork, our model is trained only with a Masked Lan-\nguage Modeling (MLM) objective (Conneau et al.,\n2019; Conneau and Lample, 2019; Devlin et al.,\n2018). We plan to further incorporate Translation\nLanguage Modeling (TLM) objective to improve\ntranslations of low-resource languages with rela-\ntively few thousands of data points 1. This will be\nuseful for both supervised and unsupervised trans-\nlation (Adelani et al., 2022a; Conneau et al., 2019).\nWe used a shared Sentence Piece vocabulary\nwith 250, 000 BPE codes. The subword shared\n1https://github.com/facebookresearch/XLM\nvocabulary intends to improve alignment in the\nembedding space across languages (see languages\ndescription in Appendix A and corpora details in\nTable 1) that are linguistically similar in features\nsuch as script/alphabet, morphology, etc. (Conneau\net al., 2019), reflecting our focus languages. Addi-\ntionally, (Conneau et al., 2019) showed that scaling\nthe size of the shared vocabulary (e.g. from 36,000\nto 256,000) improved the performance of multilin-\ngual models on downstream tasks. Our vocabulary\nis defined jointly across all 23 languages and fixed\nduring training, as opposed to random training and\nheld-out dataset selection at each active learning\nround.\nThe motivations behind the randomness in the\nselection of the training and held-out datasets are:\n(1) to make efficient use of the limited dataset we\nhave, and (2) to expose the model step by step,\ninstead of simultaneously, to a variety of samples\nacross different news sub-domains. We believe\nthis would help in domain-shift adaptation and the\nrobustness of the model.\nAs extensively detailed in Algorithm 1, at each\nround we randomly select m sentences per lan-\nguage, from the held-out dataset of the language.\nFor a language, to generate a new sentences′, given\nan original sentences, we proceed as follows (more\ndetails can be found in Algorithm 1):\n1. select an initial ordered (left to right) set of\nwords from s as prompt,\n2. add a mask token at the end of the ordered set\nor sequence of words,\n3. query the model to predict the masked token,\n4. choose the best word, add it to the prompt,\n5. repeat 2-4 until we reach the length of s.\nThe process described above will produce m new\ndata points that will be added to the language\ndataset. The new dataset obtained is used to re-train\nthe model from scratch at the next active learning\nround.\n4 Experiments, Results and Discussion\nExperiments: We use the XLM-RoBERTa\n(XLM-R) architecture in our experiments based\non previous works utilizing the model to achieve\nstate-of-the-art performance in various downstream\ntasks. Following the work and results of (Kelechi\net al., 2021), we trained XLM-R-based models\nAlgorithm 1 Self-Active Learning Training Round\nRequire:\n• Masked Language Modeling (MLM) objective\nπθ with masking probability p = 0.15\n• V ocabularyV, Model M, Tokenizer T\n• Set of languages L = S\ni∈[1,23]{l}\n• Overall Dataset D = S\nl∈L Dl with Dl the\ndataset of language l\n• Training Dataset Dt with k% randomly se-\nlected sentences from Dl, l∈ L\n• Held-out Dataset H with 1 − k% samples for\neach language: H = S\nl∈L Hl\n• proportion t of words to successively mask in\na sentence (from left to right)\nEnsure:\n• Initialize M, and T with V\n• k ← 80\n• t ← 15\n• Train M with policy πθ\nGenerate set Gl of new samples for each lan-\nguage:\nfor l ∈ L do\nGl ← {}\n• Build Sl with m = |Hl| sentences randomly\nchosen from Hl ▷ we choose m this way to\ncope with small size datasets\nfor s ∈ Sl do\nn ← len(s), s = S\ni∈[1,n]{wi}\nts ←\n\u0006n∗t\n100\n\u0007\n+ 1\nprompt ← S\ni∈[1,n−ts]{wi}\nwhile ts ̸= 0do\nprompt ← prompt ∪ {<mask>}\nwp ← M(prompt): ▷ predicted\nmasked word\nprompt ← prompt ∪ {wp}\nts ← ts − 1\nend while\nGl ← Gl ∪ {prompt}\nend for\nDl ← Dl ∪ Gl ▷ new samples added to the\nlanguage dataset\nend for\nModel Hyper-parameters Values\nAfroLM-Large\nsequence maximum length 256\nhidden size 768\nattention heads 6\nhidden layers 10\nlearning rate 1e-4\nbatch size 32\n# of Parameters 264M\ntotal initial training examples 5,137,026\nvocabulary size 250,000\ngradient accumulation steps 8\nwarming steps 40,000\ntraining steps 500,000\nTable 2: Hyper-parameters summary\nfrom scratch. In our current work we trained\nthe model with 3 self-active learning rounds (we\nstopped at 3 due to computational resources). We\nused 80% and 20% of languages data for the train-\ning and held-out datasets respectively. We designed\n2 versions of AfroLM: AfroLM-Large (without\nself-active learning) and AfroLM-Large (with self-\nactive learning) with the hyper-parameters speci-\nfied in Table 2. All training experiments were done\nusing the HuggingFace Transformers library (Wolf\net al., 2019).\nAfroLM (without self-active learning) is one\nof our baselines. We trained an XLM-R model\non the entire dataset, and the held-out dataset was\njust used for evaluation. For AfroLM-Large mod-\nels, we used Google Cloud with a single 48GB\nNVIDIA A100 GPU. An active learning round took\n≈ 260 hours of training. We evaluated AfroLM-\nLarge models on three downstream tasks:\n• NER: we evaluated the performance of our\nmodel pre-trained using our self-active learn-\ning framework on the MasakhaNER dataset\n(Adelani et al., 2021a). The dataset contains\nten African languages: Amharic, Hausa, Igbo,\nKinyarwanda, Luganda, Luo, Nigerian Pid-\ngin, Swahili, Wolof, and Yorùbá. (Adelani\net al., 2021a; Alabi et al., 2022a) also pro-\nvided strong baselines with pre-trained lan-\nguage models like mBERT and XLM-R on\nMasakhaNER.\n• Text Classification: we tested our models\non Hausa and Yorùbá news text classification\ndataset from (Hedderich et al., 2020), where\nthe authors have also built strong baselines on\nmBERT and XLM-R models.\n• Sentiment Analysis: we tested the the out-\nof-domain performance of our model in two\ndomains different from news:\n1. Movies: we directly fine-tuned and\nevaluated AfroLM-Large on the YOSM\ndataset (Shode et al., 2022), which con-\ntains reviews of Yorùbá movies.\n2. Twitter → Movies: in this setup,\nwe finetuned on the training and valida-\ntion set of NaijaSenti (Muhammad et al.,\n2022), and evaluated on YOSM. Nai-\njaSenti contains human annotated tweets\nin Hausa, Yoruba, Igbo and Nigerian Pid-\ngin. However, we were not able to eval-\nuate AfroLM-Large on it because the\nauthors have not yet released the test set.\nResults & Discussion: Tables 1 and 3 show that\nour framework includes a large variety of African\nLanguages. Table 4, and Table 5 (with 11 addi-\ntional languages from MasakhaNER 2.0 dataset\n(Adelani et al., 2022b)) show the results of our\nmethod in comparison with other baselines on\nNER task. We can notice that AfroLM-Large\n(w/ AL) outperforms AfriBERTa-Large, mBERT\nand XLMR-base (≈ 2.5 TB of data); while being\npre-trained on significantly smaller dataset (≈ 0.73\nGB (80% of 0.91 GB initial dataset)). AfriBERTa-\nLarge has been pretrained from scratch on 11\nAfrican languages, while mBERT and XLMR-base\n(with existing pretrained weights) were finetuned\non the MasakhaNER dataset.\nTable 6 and Table 7 show that, on the text classi-\nfication and sentiment analysis tasks, our method\noutperforms many existing baselines. Additionally,\nout-of-domain experiments and analyses show that\nour method is robust and provides good results in\nout-of-domain settings.\nWhile AfroXLMR-base in average, slightly out-\nperforms our approach, it is important to notice that\nit has been pretrained on a dataset 14x bigger than\nour set. Furthermore, AfroLM-Large has been\ntrained on ≈ 0.73 GB of data (80% of 0.91 GB\ninitial dataset), which is less than the size of the\ncorpus used to train AfriBERTa (0.939 GB). This\nallows us to confidently affirm that our approach is\ndata-efficient, while being very competitive.\nIt is important to note that the margin of per-\nformance from AfroLM-Large (w/ AL) does not\ncome from the fact that it has been trained on more\nlanguages. Our results show that AfroLM-Large\n(w/ AL) outperforms models trained on signifi-\ncantly larger datasets and number of languages.\nLanguage In In In In In\nAfriBERTa? AfroLM? AfroXLMR mBERT? XLMR?\namh ✓ ✓ ✓ ✗ ✓\nhau ✓ ✓ ✓ ✗ ✓\nibo ✓ ✓ ✓ ✗ ✗\nkin ✓ ✓ ✓ ✗ ✗\nlug ✗ ✓ ✗ ✗ ✗\nluo ✗ ✓ ✗ ✗ ✗\npcm ✓ ✓ ✓ ✗ ✗\nswa ✓ ✓ ✓ ✓ ✓\nwol ✓ ✓ ✓ ✗ ✗\nyor ✓ ✓ ✓ ✓ ✗\nTable 3: Information about languages included in each language model. We can notice that AfroLM includes the\nmost of them.\nLanguage AfriBERTa-Large AfroLM-Large AfroLM-Large AfroXLMR-base mBERT XLMR-base(w/o AL) (w/ AL)\namh 73.82 43.78 73.84 76.10 00.00 70.96\nhau 90.17 84.14 91.09 91.10 87.34 87.44\nibo 87.38 80.24 87.65 87.40 85.11 84.51\nkin 73.78 67.56 72.84 78.00 70.98 73.93\nlug 78.85 72.94 80.38 82.90 80.56 80.71\nluo 70.23 57.03 75.60 75.10 72.65 75.14\npcm 85.70 73.23 87.05 89.60 87.78 87.39\nswa 87.96 74.89 87.67 88.60 86.37 87.55\nwol 61.81 53.58 65.80 67.40 66.10 64.38\nyor 81.32 73.23 79.37 82.10 78.64 77.58\navg 79.10 68.06 80.13 81.90 71.55 79.16\navg (excl. amh) 79.69 70.76 80.83 82.54 79.50 80.07\nTable 4: NER Performances: F1-scores on languages test sets after 50 epochs averaged over 5 seeds. These results\ncover all 4 tags in the MasakhaNER dataset: PER, ORG, LOC, DATE. XLM-R and mBERT results obtained from\n(Adelani et al., 2021b). AfroLM-Large (w/ AL) outperforms AfriBERTa, and the initial MasakhaNER baselines.\nThe bold numbers represent the performance of the model with the lowest pretrained data. AfroXMLR-base =\nXLMR-Large + MAFT (Alabi et al., 2022a) with 272M parameters. MAFT gives similar performance to individual\nLAFT models (Alabi et al., 2022a) (LAFT results in single model per language).\nMoreover, the comparison of AfroLM-Large (w/\nAL) to AfroLM-Large (w/o AL) shows a signifi-\ncant improvement in performance, which implies\nthat our self-active learning framework is efficient,\nand leads to a better performance. This is expected,\nbecause the idea of our self-active learning (and of\nactive learning in general) is that AfroLM consis-\ntently and dynamically, identifies during the train-\ning phase, the most beneficial sample(s) to learn\nfrom in order to boost the performance.\nIn our current algorithm, a sentence sample is\ngenerated by iterative next-token prediction: the\ngenerated sentence is the result of the concatenation\nof each best token. Diversity in sample generation\nand selection is paramount, and we believe, could\nimprove the performance of our framework. In\nthe limitation section (section 6), we propose a\nway of selecting diverse sentences (after sentence\ngeneration). We also propose a new weighted loss,\nthat we believe will be more balanced across the\nentire dataset.\n5 Future works and Conclusion\nIn conclusion, we propose AfroLM, a self-active\nlearning-based multilingual language model sup-\nporting 23 African Languages; the largest to date.\nOur language datasets are collected from the news\ndomain and span across different parts of the\nAfrican continent. Our experimental results on\nNLP downstream tasks (NER, text classification,\nand out-of-domain sentiment analysis), prove the\ndata-efficiency of AfroLM (as it has been trained\non a dataset 14x smaller than its competitors),\nand its competitiveness as it outperforms many\nMPLMs (AfriBERTa, mBERT, XLMR-base) while\nbeing very competitive to AfroXLMR-base. We\nModel bam bbj ewe fon mos nya sna tsn twi xho zul A VG\nMPLMs pre-trained on from scratch on African Languages\nAfriBERTa-Large 78.60 71.00 86.90 79.90 71.40 88.60 92.40 83.20 75.70 85.00 81.70 81.31\nAfroLM-Large (w/ AL) 80.40 72.91 88.14 80.48 72.14 90.25 94.46 85.38 77.89 87.50 86.31 83.26\nMPLMs adapted to African Languages\nAfroXLMR-base 79.60 73.30 89.20 82.30 74.40 91.90 95.70 87.70 78.90 88.60 88.40 84.55\nmBERT 78.90 60.60 86.90 79.90 71.40 88.60 92.40 86.40 75.70 85.00 81.70 80.68\nXLMR-base 78.70 72.30 88.50 81.90 72.70 89.90 93.60 86.10 78.70 87.00 84.60 83.09\nTable 5: NER Baselines on MasakhaNER2.0 (Adelani et al., 2022b). We compare MPLMs trained from scratch\non African languages, and MPLMs adapted to African Languages. The average of scores are over 5 runs. The bold\nnumbers represent the performance of the model with the lowest pretrained data.\nLanguage AfriBERTa-Large AfroLM-Large AfroLM-Large\n(w/o AL) (w/ AL)\nhau 90.86 85.57 91.00\nyor 83.22 75.30 82.90\nTable 6: Text Classification Performances: F1-scores on the languages test sets. The bold numbers represent\nthe performance of the model with the lowest pretrained data.\nModels Yoruba F1-score\nAfroLM-Large (w/o AL)\nMovies 83.12\nTwitter →Movies 41.28\nAfroLM-Large (w/ AL)\nMovies 85.40\nTwitter →Movies 68.70\nAfriBERTa-Large\nMovies 82.70\nTwitter →Movies 65.90\nTable 7: Out-Of-Domain Sentiment Analysis Performance: F1-scores on YOSM test set after 20 epochs averaged\nover 5 seeds. The bold numbers represent the performance of the model with the lowest pretrained data.\nalso show that AfroLM is also able to general-\nize across various domains. For future work, we\nintend to: (1) explore and understand the rela-\ntionship between the number of active learning\nsteps and the MPLMs performance on downstream\ntasks, and (2) integrate a new weighted loss, and\nmore diversity in new data points generation and\nselection as we explain in the limitation section\n(see section 6). Our datasets, and source code are\npublicly available at https://github.com/\nbonaventuredossou/MLM_AL.\n6 Limitations and Approach of Solution\nCurrently, the loss of the model across the train-\ning dataset (across all 23 languages), appears to\nbe the average of the individual (cross-entropy)\nlosses. Due to the disparate sizes of our corpora\nper language, the training will be biased toward the\nlanguages whose sizes predominate the training set.\nTherefore, we suggest a strategy to re-weight the\ncross entropy loss per language by the ratio of the\nsize of the dataset for that language to the size of\nthe entire training set:\nL = 1\nN\nX\nl\n|Dl\nD |Ll\nwhere |Dl\nD | is the weight of the training dataset of\nthe language l, Ll is the loss of the model on a\ngiven language l, and N is the total number of lan-\nguages (23 in our case). We believe this adjusts\nwell overall loss by using the right weighted loss of\neach language, which can be seen as their respec-\ntive contribution to the general loss.\nAnother limitation of our current framework is\nthat the samples that are generated from prompts\nmight not be diverse. Given a batch B of generated\nsamples, and a set S of initial samples, we want the\nsamples selected to be substantially different from\nthe majority of samples present in S. We think\nthat performing the following two steps will help\nto ensure this:\n1. Increasing the number of words, in a sentence,\nto be masked: this implies that the length\nof the prompt is shortened, and that we pro-\nvide less (or short) context in the input to our\nmodel. Long-range semantics is still a chal-\nlenge in natural language generation and un-\nderstanding, and large language models (GPT-\n2, DialoGPT) have insufficiently learned the\neffect of distant words on next-token predic-\ntion (Malkin et al., 2022). Therefore, we be-\nlieve that providing a short context will in-\ncrease the choices of the model and lead to\nthe generation of more various tokens. This\nhas been shown by (Malkin et al., 2022) where\nthey also introduced the coherence boosting\napproach to increase the focus of a language\nmodel on a long context.\n2. Using the Word Error Rate (WER) as a sim-\nple diversity measurement. The WER is an\nadaptation of the Levenshtein distance (also\ncalled edit distance), working at the word\nlevel instead of the phoneme level. Ideally,\nwe want high WER. Let W = S\ni∈[1,ts]{wi},\nthe set of words from a sentence s that we\ncut off for the next-token prediction loop de-\nscribed in section 3 and in Algorithm 1. Let\nW′ = S\ni∈[1,ts]{w′\ni}, the set of words pre-\ndicted by the model. Then, for a pair (s, s′) of\nthe original sentence and new generated sen-\ntence (s′ = prompt ∪ W′), we can define a\ndiversity score ds,s′ = WER (W, W′). Given\nthe definition of d, for a language l, we can\ndefine a diverse batch\nBl\ndiverse =\n[\ni∈[1,|Hl|]\n{s′\ni | dsi,s′\ni\n≥ t}\nwhere t is an hyper-parameter, representing\nan error threshold. t can be tuned because a\nsmall t will result in a less diverse batch, while\na very huge value will result in an empty or\nalmost empty batch.\n7 Ethics Statement\nAs any modern technology, machine learning al-\ngorithms are subject to potential dual good or bad\nusage. Our work is motivated by the desire of\nmaking AI (in general, NLP in particular) applica-\ntions to be inclusive to the low-resourced languages\n(which are the vast majority of existing living lan-\nguages), hence benefiting to humanity and society.\nWe strongly discourage bad and unethical use of\nour work (and its derivations).\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D’souza, Julia Kreutzer, Constantine Lig-\nnos, Chester Palen-Michel, Happy Buzaaba, Shruti\nRijhwani, Sebastian Ruder, Stephen Mayhew, Is-\nrael Abebe Azime, Shamsuddeen H. Muhammad,\nChris Chinenye Emezue, Joyce Nakatumba-Nabende,\nPerez Ogayo, Aremu Anuoluwapo, Catherine Gitau,\nDerguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-\nmam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,\nRubungo Andre Niyongabo, Jonathan Mukiibi, Ver-\nrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,\nGerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-\nwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel\nOyerinde, Clemencia Siro, Tobius Saul Bateesa,\nTemilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayo-\ndele Awokoya, Mouhamadane MBOUP, Dibora Ge-\nbreyohannes, Henok Tilaye, Kelechi Nwaike, De-\ngaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021a. MasakhaNER: Named Entity\nRecognition for African Languages. Transactions\nof the Association for Computational Linguistics ,\n9:1116–1131.\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig,\nDaniel D’souza, Julia Kreutzer, Constantine Lignos,\nChester Palen-Michel, Happy Buzaaba, Shruti Rijh-\nwani, Sebastian Ruder, et al. 2021b. Masakhaner:\nNamed entity recognition for african languages.\nTransactions of the Association for Computational\nLinguistics, 9:1116–1131.\nDavid Ifeoluwa Adelani, Jesujoba Oluwadara Alabi,\nAngela Fan, Julia Kreutzer, Xiaoyu Shen, Machel\nReid, Dana Ruiter, Dietrich Klakow, Peter Nabende,\nErnie Chang, Tajuddeen Gwadabe, Freshia Sackey,\nBonaventure F. P. Dossou, Chris Chinenye Emezue,\nColin Leong, Michael Beukman, Shamsuddeen Has-\nsan Muhammad, Guyo Dub Jarso, Oreen Yousuf,\nAndre Niyongabo Rubungo, Gilles HACHEME,\nEric Peter Wairagala, Muhammad Umair Nasir,\nBenjamin Ayoade Ajibade, Oluwaseyi Ajayi Ajayi,\nYvonne Wambui Gitau, Jade Abbott, Mohamed\nAhmed, Millicent Ochieng, Anuoluwapo Aremu,\nPerez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba\nKabore, Godson Koffi KALIPE, Derguene Mbaye,\nAllahsera Auguste Tapo, Victoire Memdjokam\nKoagne, Edwin Munkoh-Buabeng, Valencia Wagner,\nIdris Abdulmumin, and Ayodele Awokoya. 2022a. A\nfew thousand translations go a long way! leveraging\npre-trained models for african news translation. In\nNAACL-HLT.\nDavid Ifeoluwa Adelani, Graham Neubig, Sebastian\nRuder, Shruti Rijhwani, Michael Beukman, Chester\nPalen-Michel, Constantine Lignos, Jesujoba O. Al-\nabi, Shamsuddeen H. Muhammad, Peter Nabende,\nCheikh M. Bamba Dione, Andiswa Bukula, Roowei-\nther Mabuya, Bonaventure F. P. Dossou, Bless-\ning Sibanda, Happy Buzaaba, Jonathan Mukiibi,\nGodson Kalipe, Derguene Mbaye, Amelia Taylor,\nFatoumata Kabore, Chris Chinenye Emezue, An-\nuoluwapo Aremu, Perez Ogayo, Catherine Gitau,\nEdwin Munkoh-Buabeng, Victoire M. Koagne, Al-\nlahsera Auguste Tapo, Tebogo Macucwa, Vukosi\nMarivate, Elvis Mboning, Tajuddeen Gwadabe, Tosin\nAdewumi, Orevaoghene Ahia, Joyce Nakatumba-\nNabende, Neo L. Mokono, Ignatius Ezeani, Chia-\nmaka Chukwuneke, Mofetoluwa Adeyemi, Gilles Q.\nHacheme, Idris Abdulmumin, Odunayo Ogundepo,\nOreen Yousuf, Tatiana Moteu Ngoli, and Dietrich\nKlakow. 2022b. Masakhaner 2.0: Africa-centric\ntransfer learning for named entity recognition.\nJesujoba O Alabi, David Ifeoluwa Adelani, Marius Mos-\nbach, and Dietrich Klakow. 2022a. Multilingual lan-\nguage model adaptive fine-tuning: A study on african\nlanguages. arXiv preprint arXiv:2204.06487.\nJesujoba O Alabi, Adelani David Ifeoluwa, Mosbach\nMarius, and Klakow Dietrich. 2022b. Multilingual\nLanguage Model Adaptive Fine-Tuning: A case\nstudy on African Languages. COLING.\nNzeyimana Antoine and Rubungo Andre Niyongabo.\n2022. KinyaBERT:a Morphology-aware Kin-\nyarwanda Language Model. ACL.\nHounkpati B. C. Capo. 1991. A comparative phonology\nof Gbe. Foris Publications.\nYukun Chen, Thomas A. Lasko, Qiaozhu Mei, Joshua C.\nDenny, and Hua Xu. 2015. A study of active learning\nmethods for named entity recognition in clinical text.\nJournal of Biomedical Informatics, 58:11–18.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDavid M. Eberhard, Gary F. Simons, and Charles\nD. Fennig (eds.). 2020. Ethnologue: Languages of\nthe world. twenty-third edition.\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim. 2020.\nActive Learning for BERT: An Empirical Study. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7949–7962, Online. Association for Computa-\ntional Linguistics.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017.\nDeep bayesian active learning with image data. In\nProceedings of the 34th International Conference\non Machine Learning - Volume 70, ICML’17, page\n1183–1192. JMLR.org.\nMichael A. Hedderich, David Adelani, Dawei Zhu, Je-\nsujoba Alabi, Udia Markus, and Dietrich Klakow.\n2020. Transfer learning and distant supervision for\nmultilingual transformer models: A study on African\nlanguages. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2580–2591, Online. Association for\nComputational Linguistics.\nMoksh Jain, Emmanuel Bengio, Alex-Hernandez Gar-\ncia, Jarrid Rector-Brooks, Bonaventure F. P. Dossou,\nChanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kil-\ngour, Dinghuai Zhang, Lena Simine, Payel Das, and\nYoshua Bengio. 2022. Biological sequence design\nwith gflownets.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nOgueji Kelechi, Zhu Yuxin, and Lin Jimmy. 2021.\nSmall Data? No Problem! Exploring the Viabil-\nity of Pretrained Multilingual Language Models for\nLow-resourced Languages. EMNLP, pages 116–126.\nClaire Lefebvre and Anne-Marie Brousseau. 2002. A\ngrammar of Fongbe. Mouton de Gruyter.\nNikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2022.\nCoherence boosting: When your pretrained language\nmodel is not paying enough attention. InProceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8214–8236, Dublin, Ireland. Association for\nComputational Linguistics.\nShamsuddeen Hassan Muhammad, David Ifeoluwa Ade-\nlani, Sebastian Ruder, Ibrahim Said Ahmad, Idris\nAbdulmumin, Bello Shehu Bello, Monojit Choud-\nhury, Chris Chinenye Emezue, Saheed Salahudeen\nAbdullahi, Anuoluwapo Aremu, Alipio Jeorge, and\nPavel Brazdil. 2022. Naijasenti: A nigerian twitter\nsentiment corpus for multilingual sentiment analysis.\nRubungo Andre Niyongabo, Qu Hong, Julia Kreutzer,\nand Li Huang. 2020. KINNEWS and KIRNEWS:\nBenchmarking cross-lingual text classification for\nKinyarwanda and Kirundi. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics, pages 5507–5521, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nIyanuoluwa Shode, David Ifeoluwa Adelani, and Anna\nFeldman. 2022. YOSM: A NEW YORUBA SENTI-\nMENT CORPUS FOR MOVIE REVIEWS. In 3rd\nWorkshop on African Natural Language Processing.\nAditya Siddhant and Zachary C. Lipton. 2018. Deep\nbayesian active learning for natural language process-\ning: Results of a large-scale empirical study.\nJustin S. Smith, Benjamin Tyler Nebgen, Nick Lub-\nbers, Olexandr Isayev, and Adrian E. Roitberg. 2018.\nLess is more: sampling chemical space with ac-\ntive learning. The Journal of chemical physics, 148\n24:241733.\nManuel Tonneau, Dhaval Adjodah, Joao Palotti, Nir\nGrinberg, and Samuel Fraiberger. 2022. Multilingual\ndetection of personal employment status on Twitter.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 6564–6587, Dublin, Ireland.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2019. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In NAACL.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\nA Language Characteristics\nAmharic (amh) also called Amarinya or\nAmerigna, is a Semitic language, an official lan-\nguage of Ethiopia, and is also spoken in Eritrea.\nAmharic is written with a modified version of the\nGe’ez script, known as Fidel, consisting of 33 ba-\nsic characters, each of them with at least 7 vowel\nsequences. Unlike Central and Northwest Semitic\nlanguages such as Arabic, Hebrew and Assyrian\nAramaic, Amharic is written from left to right. The\nlanguage has a variety of local dialects, all of which\nare mutually intelligible. There are three major di-\nalects: Gondar, Gojjami, and Showa. There are\nspecially marked differences in pronunciation, vo-\ncabulary, and grammar between the northern Goj-\njami and the southern Showa dialects.\nAfan Oromo (oro) is an Afroasiatic language\nthat belongs to the Cushitic branch spoken by about\n30 million people in Ethiopia, Kenya, Somalia and\nEgypt, and it is the third largest language in Africa.\nThe Oromo people are the largest ethnic group in\nEthiopia and account for more than 40% of the pop-\nulation. They can be found all over Ethiopia, and\nparticularly in Wollega, Shoa, Illubabour, Jimma,\nArsi, Bale, Hararghe, Wollo, Borana and the south-\nwestern part of Gojjam2. Afan Oromo is written\nwith a Latin alphabet called Qubee. Like most other\nEthiopian languages, whether Semitic, Cushitic, or\nOmotic, Oromo has a set of ejective consonants,\nthat is, voiceless stops or affricates that are accom-\npanied by glottalization and an explosive burst of\nair. Afan Oromo has another glottalized phone\nthat is more unusual, an implosive retroflex stop,\n\"dh\" in Oromo orthography, a sound that is like\nan English \"d\" produced with the tongue curled\nback slightly and with the air drawn in so that a\nglottal stop is heard before the following vowel\nbegins. It is retroflex in most dialects, though it\nis not strongly implosive and may reduce to a flap\nbetween vowels3. In the Qubee alphabet, letters\ninclude the digraphs ch, dh, ny, ph, sh. Gemina-\ntion is not obligatorily marked for digraphs, though\nsome writers indicate it by doubling the first ele-\nment: qopphaa’uu ’be prepared’. Afan Oromo has\nfive vowel phonemes, i.e., sounds that can differ-\nentiate word meaning. They can be short or long.\nThe length of the vowel makes a difference in word\nmeaning e.g., laga ‘river’ and laagaa ‘roof of the\n2https://omniglot.com/writing/oromo.htm\n3https://en.wikipedia.org/wiki/Oromo_\nlanguage\nmouth’. Afan Oromo has 25 consonant phonemes,\ni.e., sounds that make a difference in word meaning.\nLike its close relative, Somali, native Oromo words\ndo not have the consonants /p/, /v/, and /z/.\nBambara (bam) is a Western Mande language\nwith about 14 million speakers mainly in Mali, and\nalso in Senegal, Niger, Mauritania, Gambia and\nCôte d’Ivoire. It is spoken principally among the\nBambara ethnic group in Mali, where it is the na-\ntional language and the most widely understood\none. Bambara is usually written with the Latin al-\nphabet, though the N’Ko and Arabic alphabets are\nalso used to some extent. It uses seven vowels a, e,\nE, i, o, O, and u each of which can be nasalized, pha-\nryngealized and murmured, giving a total number\nof 21 vowels.\nGhomalá’ (bbj) is a major Bamileke language\nspoken in Cameroon. It is spoken by an estimated\n1.1 million people in two main population groups.\nÉwé (ewe) is a language spoken in Togo and\nsoutheastern Ghana by approximately 20 million\npeople mainly in West Africa in the countries of\nGhana, Togo, and Benin. It is recognised as a\nnational language in Ghana, where English is the\nofficial language, and in Togo, where French is the\nofficial language. ’Ewe’ is also the name of the\ntribal group that speaks this language. Éwé has\nthree distinguishable dialects. Most of the differ-\nences among the dialects have to do with phonol-\nogy. All dialects are mutually intelligible. Éwé\nis written in the African reference alphabet, first\nproposed by a UNESCO-organized conference in\n1978. It is a version of the Latin alphabet adapted\nto represent Éwé sounds. Some sounds are rep-\nresented by two-letter sequences, e.g., dz, ts, gb,\nkp, ny. Éwé has seven oral and five nasal vowels.\nNasal vowels are produced by lowering the soft\npalate so that air escapes both through the mouth\nand the nose. Nasal vowels are marked by a tilde.\nFon (fon) also known as Fongbé is a native lan-\nguage of Benin Republic. It is spoken in average\nby 1.7 million people. Fon belongs to the Niger-\nCongo-Gbe languages family. It is a tonal, isolating\nand left-behind language according to (Joshi et al.,\n2020), with an Subject-Verb-Object (SVO) word\norder. Fon has about 53 different dialects, spoken\nthroughout Benin (Lefebvre and Brousseau, 2002;\nCapo, 1991; Eberhard et al., 2020). Its alphabet is\nbased on the Latin alphabet, with the addition of\nthe letters: ª , ¡ , ¢ , and the digraphs gb, hw, kp, ny,\nand xw. There are 10 vowels phonemes in Fon: 6\nsaid to be closed [i, u, ˜ı, ˜u], and 4 said to be opened\n[¢ , ª , a, ã]. There are 22 consonants (m, b, n, ¡ , p,\nt, d, c, j, k, g, kp, gb, f, v, s, z, x, h, xw, hw, w). Fon\nhas two phonemic tones: high and low. High is re-\nalized as rising (low–high) after a consonant. Basic\ndisyllabic words have all four possibilities: high-\nhigh, high-low, low-high, and low-low. In longer\nphonological words, like verb and noun phrases, a\nhigh tone tends to persist until the final syllable. If\nthat syllable has a phonemic low tone, it becomes\nfalling (high–low). Low tones disappear between\nhigh tones, but their effect remains as a downstep.\nRising tones (low–high) simplify to high after high\n(without triggering downstep) and to low before\nhigh (Lefebvre and Brousseau, 2002; Capo, 1991).\nHausa (hau) belongs to the West Chadic branch\nof the Afro-Asiatic language family. It is one of the\nlargest languages on the African continent, spoken\nas a first language by the original Hausa people\nand by people of Fula ancestry. Hausa is the major-\nity language of much of northern Nigeria and the\nneighboring Republic of Niger. In addition, there\nis a sizable Hausa-speaking community in Sudan4.\nIt has an alphabet of 29 letters containing 5 vow-\nels and 24 consonants. Hausa alphabet is a Latin\nscript/Roman alphabet/English letters except (x, v,\np, and q) and also added six extra letters (á, â, Î, sh,\nts and ¯ (Adelani et al., 2021b). Hausa is an agglu-\ntinative language, i.e., it adds suffixes to roots for\nexpressing grammatical relations without fusing\nthem into one unit, as is the case in Indo-European\nlanguages.\nÌgbò (ibo) is one of the largest languages of West\nAfrica, is spoken by 18 million people in Nigeria.\nIt belongs to the Benue-Congo group of the Niger-\nCongo language family. The language is thought to\nhave originated around the 9th century AD in the\narea near the confluence of the Niger and Benue\nrivers, and then spread over a wide area of south-\neastern Nigeria 5. Igbo is a national language of\nNigeria and is also recognised in Equatorial Guinea.\nIgbo is written in an expanded version of the Latin\nalphabet. Igbo is made up of many different di-\nalects which aren’t mutually intelligible to other\nIgbo speakers at times.\n4https://www.mustgo.com/worldlanguages/hausa/\n5https://www.mustgo.com/worldlanguages/igbo/\nKinyarwanda (kin) is part of the Bantu sub-\ngroup of the central branch of the Niger-Congo\nlanguage family. It is closely related to Kirundi,\nthe language of Burundi. The Rwanda language is\nmutually intelligible with Kirundi, which is spoken\nin neighboring Burundi6. It has only 18/19 conso-\nnants, as X and Q are not found in the alphabet. L\nis often replaced by R, but due to the appearance of\nimported words in the language, that is not always\nthe case. It has five vowel phonemes, i.e., sounds\nthat make a difference in word meaning.\nLingala (lin) is a Central Bantu language that be-\nlongs to the largest African languages phylum: the\nNiger-Congo. Lingala is spoken as a first, second,\nand third language primarily in the Democratic Re-\npublic of Congo (DRC), the Republic of Congo\n(Congo-Brazzaville), and in parts of five neighbor-\ning central African states: Northwestern Angola,\neastern Gabon, southern Central African Republic,\nand southwestern Sudan. The estimated number\nof speakers ranges from twenty to twenty five mil-\nlion7. It is written with the Latin alphabet. The\nseven vowels are represented by five symbols. The\northographic symbols ’e’ and ’o’ each represent\ntwo sounds. There are two tones in Lingala. High\ntone is represented with an acute accent, while low\ntone is unmarked.\nLuganda (lug) is a Bantu language spoken in the\nAfrican Great Lakes region. It is one of the major\nlanguages in Uganda and is spoken by more than\n10 million Baganda and other people principally in\ncentral Uganda including the capital Kampala of\nUganda. Its alphabet is composed of twenty-four\nletters; 18 consonants (b, p, v, f, m, d, t, l, r, n, z, s,\nj, c, g, k, ny, N), 5 vowels ( a, e, i, o, u) and 2 semi-\nvowels(w, y). Since the last consonant N) does\nnot appear on standard typewriters or computer\nkeyboards, it is often replaced by the combination\nng’. All consonants are pronounced as if with letter\n‘a’ or ‘ah’ at the end. For example, bah, cah, jah,\ngah, kah, mah, pah, lah, zah, e.t.c\nLuo (luo) are spoken by the Luo peoples in\nan area ranging from southern Sudan to south-\nern Kenya, with Dholuo extending into north-\nern Tanzania and Alur into the Democratic\nRepublic of the Congo. Luo has a CVC\nor VC structure—consonant/vowel/consonant or\nvowel/consonant. This is unlike Bantu languages,\n6https://nalrc.indiana.edu/doc/brochures/kinyarwanda.pdf\n7https://nalrc.indiana.edu/doc/brochures/lingala.pdf\nwhere words must end in a vowel. Luo language\nis therefore more similar to English articulation,\nwhile Bantu languages are more like Italian8.\nMooré (mos) is a Gur language of the Oti–V olta\nbranch and one of two official regional languages\nof Burkina Faso. It is the language of the Mossi\npeople, spoken by approximately 8 million people\nin Burkina Faso, plus another 1M+ in surround-\ning countries such as Ghana, Cote D’ivoire, Niger,\nMali and Togo as a native language, but with many\nmore L2 speakers. Mooré is spoken as a first or\nsecond language by over 50% of the Burkinabè\npopulation.\nChewa (nya) is a Bantu language spoken in\nmuch of Southern, Southeast and East Africa,\nnamely the countries of Malawi and Zambia, where\nit is an official language, and Mozambique and Zim-\nbabwe where it is a recognised minority language.\nChewa has five vowel sounds: /a, E, i, O, u/; these\nare written a, e, i, o, u.\nNaija (pcm) is an English-based creole language\nspoken as a lingua franca across Nigeria. The lan-\nguage is sometimes referred to as \"Pijin\" or Broken\n(pronounced \"Brokun\").\nShona (sna) is a Bantu language of the Shona\npeople of Zimbabwe. All syllables in Shona end\nin a vowel. Consonants belong to the next syllable.\nFor example, mangwanani (\"morning\") is syllabi-\nfied as ma.ngwa.na.ni; \"Zimbabwe\" is zi.mba.bwe.\nNo silent letters are used in Shona.\nSwahili (swa) also known by its native name\nKiswahili, is a Bantu language and the native lan-\nguage of the Swahili people native primarily to\nTanzania. Swahili has become a second language\nspoken by tens of millions in four African Great\nLakes countries (Kenya, DRC, Uganda, and Tan-\nzania), where it is an official or national language,\nwhile being the first language for many people in\nTanzania especially in the coastal regions of Tanga,\nPwani, Dar es Salaam, Mtwara and Lindi. Standard\nSwahili has five vowel phonemes: /a/, / E/, /i/, /O/,\nand /u/.\nSetswana (tsn) is a Bantu language spoken\nin Southern Africa by about 14 million people.\nSetswana is an official language and lingua franca\nof Botswana and South Africa.\n8https://owlcation.com/humanities/Luo-language-of-\nKenya-Conversation-Basics\nAkan/Twi is a dialect of the Akan language spo-\nken in southern and central Ghana by several mil-\nlion people, mainly of the Akan people, the largest\nof the seventeen major ethnic groups in Ghana. Twi\nexcludes consonants such as c, j, q, v, x and z. It has\n15 consonants and 7 vowels. Apart from [a], [e],\n[i], [o] and [u], Twi also has 2 additional vowels;\n[E] and [O].\nWolof (wol) is a language of Senegal, Mauritania,\nand the Gambia, and the native language of the\nWolof people. Wolof is the most widely spoken\nlanguage in Senegal, spoken natively by the Wolof\npeople (40% of the population) but also by most\nother Senegalese as a second language.\nXhosa (xho) also isiXhosa as an endonym, is a\nNguni language and one of the official languages of\nSouth Africa and Zimbabwe. The Xhosa language\nemploys 26 letters from the Latin alphabet. Xhosa\nhas an inventory of ten vowels: [a], [¢ e], [i], [ª o]\nand [u] written a, e, i, o and u in order, all occurring\nin both long and short. The /i/ vowel will be long\nin the penultimate syllable and short in the last\nsyllable.\nYorùbá (yor) has 25 Latin letters without the\nLatin characters (c, q, v, x and z) and with addi-\ntional letters (e., gb,s., o. ).Yorùbá is a tonal language\nwith three tones: low (\"\\\"), middle (\"—\", optional)\nand high (\"/\"). The Latin letters 〈c〉, 〈q〉, 〈v〉, 〈x〉,\n〈z〉 are not used as part of the official orthography\nof Standard Yorùbá, however, they exist in several\nYorùbá dialects. The tonal marks and underdots\nare referred to as diacritics and they are needed for\nthe correct pronunciation of a word. Yorùbá is a\nhighly isolating language and the sentence structure\nfollows subject-verb-object (Adelani et al., 2021b).\nZulu (zul) is the mother tongue of the Zulu\npeople, South’s Africa largest ethnic group, who\ncreated an empire in the 19th century.Zulu has\na 7-vowel system. Each vowel can be long or\nshort. Zulu has close to 50 consonants including\nclicks, ejectives and implosives. Clicks originated\nin Khoisan languages and then spread into some\nneighboring Bantu ones. In Zulu they have three\nplaces of articulation: central alveolar, lateral alveo-\nlar and palatal combined with five accompaniments\n(plain, aspirated, voiced, nasal, and voiced nasal).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7511659264564514
    },
    {
      "name": "Natural language processing",
      "score": 0.608629047870636
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5670667886734009
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5372471809387207
    },
    {
      "name": "Natural language",
      "score": 0.4777761697769165
    },
    {
      "name": "Linguistics",
      "score": 0.44414588809013367
    },
    {
      "name": "Programming language",
      "score": 0.3665952682495117
    },
    {
      "name": "Philosophy",
      "score": 0.07206133008003235
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210164802",
      "name": "Mila - Quebec Artificial Intelligence Institute",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I59361560",
      "name": "Instituto Politécnico Nacional",
      "country": "MX"
    },
    {
      "id": "https://openalex.org/I123387679",
      "name": "Uppsala University",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I136040515",
      "name": "Universidad de Deusto",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210117796",
      "name": "Ashesi University",
      "country": "GH"
    },
    {
      "id": "https://openalex.org/I166088655",
      "name": "Montclair State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I102420622",
      "name": "College of Saint Rose",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ],
  "cited_by": 22
}