{
  "title": "Evaluating the Usefulness of a Large Language Model as a Wholesome Tool for De Novo Polymerase Chain Reaction (PCR) Primer Design",
  "url": "https://openalex.org/W4388197750",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093171244",
      "name": "Soham Jorapur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2977854635",
      "name": "Amisha Srivastava",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Suyamindra Kulkarni",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2050717506",
    "https://openalex.org/W1575196615",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W6949927830",
    "https://openalex.org/W3085177480",
    "https://openalex.org/W2100303227",
    "https://openalex.org/W2162098634",
    "https://openalex.org/W2793388751",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4287666546"
  ],
  "abstract": null,
  "full_text": "Review began\n 10/06/2023 \nReview ended\n 10/18/2023 \nPublished\n 10/26/2023\n© Copyright \n2023\nJorapur et al. This is an open access article\ndistributed under the terms of the Creative\nCommons Attribution License CC-BY 4.0.,\nwhich permits unrestricted use, distribution,\nand reproduction in any medium, provided\nthe original author and source are credited.\nEvaluating the Usefulness of a Large Language\nModel as a Wholesome Tool for De Novo\nPolymerase Chain Reaction (PCR) Primer Design\nSoham Jorapur \n \n, \nAmisha Srivastava \n \n, \nSuyamindra Kulkarni \n1.\n Department of Biological Sciences, Indian Institute of Science Education and Research, Bhopal, Bhopal, IND \n2.\nDepartment of Electrical & Computer Engineering, University of Texas at Dallas, Richardson, USA \n3.\n Department of\nHigher Education, Government of Karnataka, Karnataka Institute for DNA Research (KIDNAR), Dharwad, IND\nCorresponding author: \nSoham Jorapur, \nsoham.jorapur@gmail.com\nAbstract\nThis study aimed to assess the ability of language learning models (LLMs), specifically GPT-3.5 (Chat\nGenerative Pre-trained Transformer 3.5) and GPT-4 (Chat Generative Pre-trained Transformer 3.5), in\ndesigning primers for diagnostic polymerase chain reaction (PCR) of the monkeypox virus (MPXV). Five\nprimer pairs were generated by each LLM, and their thermodynamic properties and specificity were analysed\npost-hoc using commonly used software. The LLMs demonstrated ability in sequence generation and\npredicting melting temperatures (Tm), but their accuracy in predicting GC content was suboptimal,\nnecessitating further investigation. Results indicated that, of the total primer pairs, only three designed by\nGPT-4 and two by GPT-3.5 could theoretically form a PCR product, but only one pair demonstrated suitable\nparameters for experimental validation. This preliminary exploration suggests that while LLMs have a\npotential in aiding primer design, their accuracy needs improvement to match current deterministic, rule-\nbased tools used in the field. Consequently, manual intervention remains a crucial step in PCR primer\ndesign.\nCategories:\n Other, Epidemiology/Public Health, Pathology\nKeywords:\n monkeypox virus, large language models (llms), primer, chat gpt, molecular biology, pcr, diagnostics\nIntroduction\nPolymerase chain reaction (PCR) is a transformative molecular biology technique that has revolutionised the\nfield of genetic analysis and research. Developed in the mid-1980s by Kary B. Mullis, PCR enables the\ntargeted and exponential amplification of specific DNA sequences, making it an essential tool in various\nscientific disciplines. Its tremendous impact on the scientific community was recognised with the\nprestigious 1993 Nobel Prize in Chemistry \n[1]\n. The fundamental principle of PCR lies in its ability to\nrepeatedly copy and amplify a specific DNA segment, even from a minute quantity of starting material. The\nPCR process involves a carefully orchestrated series of temperature cycles, each consisting of denaturation,\nannealing and extension steps. During denaturation, the DNA strands are separated by heating, and in the\nsubsequent annealing step, especially designed short DNA sequences called primers bind to the\ncomplementary regions flanking the target DNA sequence. These primers act as starting points for DNA\nsynthesis \n[2]\n.\nThe versatility and applicability of PCR are reflected in its wide range of applications. In the field of\nmolecular diagnostics, PCR has become an indispensable tool for the detection of various genetic diseases\nand pathogens. Its sensitivity and specificity make it particularly suited for accurate and early diagnosis.\nPCR is also heavily employed in genetic engineering and cloning, allowing scientists to create multiple\ncopies of a specific DNA fragment, essential for gene manipulation and genetic studies \n[3]\n.\nHowever, one of the critical challenges in the PCR process is the design of primers. The success of PCR\nheavily relies on the proper design of these short DNA sequences. Primers need to be highly specific to the\ntarget DNA sequence to avoid non-specific amplification, which can lead to misleading results or\ncontamination. In addition, the melting temperature (Tm) of the primers must be carefully considered to\nensure optimal annealing during the temperature cycles \n[4]\n. To address the complexities of primer design,\nvarious software tools have been developed to aid researchers in selecting appropriate primers for specific\nPCR applications. These tools offer algorithms that consider factors, such as primer length, GC content, Tm,\nsecondary structures and primer-dimer formation to optimise primer specificity and efficiency.\nThe potential of advanced language models, such as ChatGPT-3.5 (Chat Generative Pre-trained Transformer\n3.5) and GPT-4 (Generative Pre-trained Transformer 4) developed by OpenAI \n[5]\n, to streamline and enhance\nprimer design in PCR represents a promising frontier in molecular biology research. These language models,\nbased on large language models (LLMs), have showcased remarkable natural language processing\ncapabilities, capable of generating human-like text and answering questions based on provided prompts and\ncontext \n[6]\n. This study aims to explore the integration of ChatGPT-3.5 and GPT-4 in the process of\n1\n2\n3\n \n Open Access Original\nArticle\n \nDOI:\n 10.7759/cureus.47711\nHow to cite this article\nJorapur S, Srivastava A, Kulkarni S (October 26, 2023) Evaluating the Usefulness of a Large Language Model as a Wholesome Tool for De Novo\nPolymerase Chain Reaction (PCR) Primer Design. Cureus 15(10): e47711. \nDOI 10.7759/cureus.47711\nautomated primer design for various PCR techniques. By leveraging the knowledge and context provided by\nthese advanced language models, we aim to improve the efficiency and accuracy of primer design, thereby\nadvancing the field of PCR research and development.\nTo achieve this goal, we have used a comprehensive dataset comprising experimentally validated primer and\ntarget DNA sequences sourced from viral disease diagnostic data. These sequences encompass a diverse\nrange of PCR applications, providing a robust foundation for evaluating the performance of the language\nmodels in primer design. Our approach involves subjecting ChatGPT-3.5 and GPT-4 to this dataset and\nconducting a pre-training evaluation to validate their understanding of the provided molecular biology data.\nPractical questions designed for various PCR applications are posed to each model, allowing us to assess\ntheir ability to generate appropriate primer sequences for specific DNA targets. The comparison between the\nprimer designs generated by ChatGPT-3.5, GPT-4 and current manual methods will provide valuable insights\ninto the effectiveness and potential of integrating advanced language models in PCR research. Furthermore,\nthis exploration of artificial intelligence's role in molecular biology raises important considerations about\nethics, bias and control, highlighting the need for responsible and transparent use of such powerful tools.\nAs we delve into this intersection of advanced language models and molecular biology, we anticipate that\nour findings will not only optimise primer design but also pave the way for future advancements in PCR\ntechnology. Nevertheless, we recognise the ongoing efforts to address the challenges associated with\nartificial intelligence, and we are committed to ensuring the responsible and ethical use of these tools in\nscientific research.\nMaterials And Methods\nThis paper explores two prompt engineering models: few-shot prompting and chain-of-thought prompting\nfor validating the use of ChatGPT as a primer designing tool.\nBenchmarks\nA curated dataset of experimental primer sequences was utilised for training purposes \n[7]\n. This dataset\ncomprised primer details, including primer sequences, melting temperatures (Tm), GC content and other\nrelevant parameters, along with their corresponding target sequences. The dataset was pruned to ensure\ndiversity and representation of different primer characteristics. \nFew-shot prompting \nWe adopt the widely used method of few-shot prompting \n[8]\n. In this approach, a language model is primed\nusing in-context examples comprising pairs of input and output prior to generating predictions for a novel,\nunseen test instance. The in-context examples, known as 'exemplars', are structured in the form of a table.\nThis tabular representation includes various known values and also consists of blank entries, which the\nmodel is tasked to complete or predict. During the prediction phase, the model generates answers directly,\nwithout the need for further human intervention or post-processing. This process emulates the human\ncognitive function of extrapolating information from known instances and applying this knowledge to novel\nsituations. By utilising the few-shot prompting paradigm, our research leverages the model's inherent\ncapacity to comprehend context, recognize patterns and generate accurate predictions based on its pre-\nexisting training.\nAll models were posed with generating primers for DNA-dependent RNA polymerase of MPXV (monkeypox\nvirus) (GenBank OR209312). To validate the effectiveness and accuracy of the generated primer sequences,\nin silico PCR analysis was performed using suitable software tools. This simulation-based approach assessed\nwhether the generated primers successfully amplified the target regions as intended. The results of the in-\nsilico PCR analysis provided insights into the feasibility and functionality of the primers generated by\nChatGPT.\nFor primers that led to a PCR product prediction, further analysis of specificity and sensitivity was done. To\nensure the accuracy and reliability of our primers, we validated them using various tools, such as Primer-\nBLAST (Primer- Basic Local Alignment Search Tool) and BLAST (Basic Local Alignment Search Tool). These\ntools enabled us to thoroughly investigate the amplification targets of the primers, ensuring that they were\nhighly specific to the MPXV and sensitive enough to detect even small amounts of the virus in patient\nsamples. A flowchart of an overview of the methods used is presented in Figure \n1\n.\n2023 Jorapur et al. Cureus 15(10): e47711. DOI 10.7759/cureus.47711\n2\n of \n6\nFIGURE\n 1: A DNA-dependent RNA polymerase (RNAP) of MPXV\n(monkeypox virus; GenBank OR209312.1) was selected as the target\nsequence for this study.\n We utilised a comprehensive dataset that included diverse primer details for training purposes. Using the few-\nshot prompting method, which leverages in-context examples allowing the model to extrapolate information,\nChatGPT-3.5 and ChatGPT-4 were tasked with predicting novel primer sequences. In parallel, primers were also\ngenerated using contemporary standard software. To evaluate the effectiveness of the primers generated, an in-\nsilico PCR analysis was carried out. Furthermore, the accuracy and reliability of these primers were ascertained\nconfirming their specificity to the MPXV.\nResults\nFive pairs of primers were generated using the three methods as described in the methodology. For\nsequences generated from LLM models, outputs were in the form of training data and did not include \nΔ\nG\n(Gibbs free energy change) and specificity calculations. These were calculated separately using the software\nBenchling (Benchling Inc., USA) \n[9]\n. It is interesting to note that while three out of the five primer pairs\ngenerated by ChatGPT-4 could theoretically form a PCR product, the number is two for ChatGPT-3.5\nWe present the results in a tabulated form in Table \n1\n. A description of each column header is given: 1) \nPrimer\npair no.\n: the numeric identification used to distinguish between different primer pairs in a study;\n2) \nGenerated by\n: the software or method utilised to design and optimise the primer sequences; 3) \nSequence (5\n′\n→\n 3\n′\n)\n: the linear sequence of nucleotide bases in a primer, written from the 5' (five prime) end to the 3' (three\nprime) end; 4) \nNo. of mismatched bases\n: the number of nucleotide bases that do not perfectly complement the\ntemplate sequence; 5) \nSize [nt]\n: the length of the primer sequence, measured in nucleotides (nt); 6) \nPurine\ncontent (GC %)\n: the percentage of guanine (G) and cytosine (C) bases in the primer sequence, which can\ninfluence primer binding stability; 7) \nTm (°C)\n: the melting temperature (Tm) of the primer, which is the\ntemperature at which half of the primer-template duplexes have dissociated into single strands; 8) \nΔ\nG\nhomodimer (kcal)\n: the Gibbs free energy change (\nΔ\nG) for the formation of homodimers, self-binding\nstructures formed by two identical primers; 9) \nΔ\nG monomer (kcal)\n: the Gibbs free energy change (\nΔ\nG) for the\nformation of a monomer, or single-stranded structure of a primer; 10) \nΔ\nG heterodimer (kcal)\n: the Gibbs free\nenergy change (\nΔ\nG) for the formation of heterodimers, structures formed by two different primers; 11)\nSpecificity for monkeypox (BLAST taxid: 10244) (%)\n: the percentage indicating primer sequence specificity for\nMPXV, as determined by the BLAST algorithm; and 12) \nLength of hypothetical Amplicon (bp)\n: the size of the\nDNA fragment that would be produced in a PCR reaction using the primers, measured in base pairs (bp).\nPrimer\nGenerated\nSequence (5′ → 3′)\nNo. of\nmismatched\nSize\nPurine\ncontent (GC\nTm\nΔG\nhomodimer\nΔG\nmonomer\nΔG\nheterodimer\nSpecificity for monkeypox\nLength of hypothetical\n2023 Jorapur et al. Cureus 15(10): e47711. DOI 10.7759/cureus.47711\n3\n of \n6\npair no.\nby\nbases\n(nt)\n%)\n(°C)\n(kcal)\n(kcal)\n(kcal)\n(BLAST taxid: 10244) (%)\namplicon [bp]\n1\nPrimer3\nTCTCTGTGTCAGAACGCTCGTCA\n0\n23\n52.17\n59.7\n-3.11\n0\n-4.14\n100\n1142\nTTGTGCTGCTCTTATCGTCTGA\n0\n22\n45.45\n55.9\n-3.13\n0\n100\n2\nCTCTGTGTCAGAACGCTCGTCA\n0\n22\n54.55\n58.7\n-3.11\n0\n-4.14\n100\n1141\nTTGTGCTGCTCTTATCGTCTGA\n0\n22\n45.45\n55.9\n-3.13\n0\n100\n3\nTCTCTGTGTCAGAACGCTCGTCA\n0\n23\n52.17\n59.7\n-3.11\n0\n-4.14\n100\n1141\nTGTGCTGCTCTTATCGTCTGAT\n0\n22\n45.45\n55.7\n-3.13\n0\n100\n4\nCTCTGTGTCAGAACGCTCGTCA\n0\n22\n54.55\n58.7\n-3.11\n0\n-4.14\n100\n1140\nTGTGCTGCTCTTATCGTCTGAT\n0\n22\n45.45\n55.7\n-3.13\n0\n100\n5\nTCTCTGTGTCAGAACGCTCGTC\n0\n22\n54.55\n58.5\n-3.11\n0\n-4.14\n100\n1142\nTTGTGCTGCTCTTATCGTCTGA\n0\n22\n45.45\n55.9\n-3.13\n0\n100\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n1\nChatGPT-4\nTCTCTGTGTCAGAACGCTCG\n0\n20\n50 / 55\n60 /\n56.3\n-3.11\n0\n-4.13\n100\n155\nCGGTTAATCAGAGCTACATTC\n1\n21\n47.62 / 42.86\n60.6/\n50.6\n-3.25\n0\n100\n2\nTGTTGACTCTCTTATCGTCTG\n0\n21\n47.62 / 44.44\n60.6 /\n51.2\n-1.83\n0\n-1.99\n100\n819\nCTAGAGCCGCTGATGAACCT\n0 (11 nt long\noverhang)\n20\n50 / 55\n60 /\n16.3\n-4.55\n0\n100\n3\nCTCGTCAATATAGATCTTAG\n5\n20\n45 / 35\n58 /\n35\n-2.87\n0\n-2.42\n100\n59\nAACTCTCTCTAAAAAAAAATTCT\n7\n21\n23.81 / 21.74\n49.2 /\n46.94\n-1.45\n0\n100\n4\nCGTTCTCGACACAGAGAGA\n0\n19\n52.63 / 52.63\n57.6 /\n52.9\n-3.64\n0\n-2.03\n100\nThe reverse primer binds\nbefore the forward primer.\nGCACAGAGATTCTTCCAGATA\n2\n21\n47.62 / 42.86\n60.6 /\n51.1\n-2.12\n0\n100\n5\nAAATTGGGAGGCTTAAAGTG\n3\n20\n45 / 40\n58 /\n50.1\n-5.19\n0\n-3.88\n18\nComplementarity on the\nreverse strands\nGTTTACCGTCCATGCCACAC\n11\n20\n50 / 55\n60 /\n56\n-3.63\n0\n0\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n1\nChatGPT-\n3.5\nTCTCTGTGTCAGAACGCTCG\n0\n20\n50 / 55\n56.5 /\n56.\n-3.11\n0\n-5.35\n100\n86\nCGTAGTTGCGTTAGTTCTCT\n2\n20\n45 / 45\n54.5 /\n51.8\n-1.57\n0\n95\n2\nGATCTTAGAAATTTTTTAGA\n5\n20\n45 / 20\n54.5 /\n40.1\n-3.53\n0\n-2.12\n1\nBoth primers are forward\nprimers.\nTCTCTGTGTCAGAACGCTCG\n0\n20\n50 / 55\n56.5 /\n56.\n-3.11\n0\n100\n3\nATCATTCTTTTCCTCTTGAG\n8\n20\n45 / 35\n54.5 /\n46.8\n-1.7\n0\n-1.54\n0\nThe reverse primer binds\nbefore the forward primer.\n54.5 /\n2023 Jorapur et al. Cureus 15(10): e47711. DOI 10.7759/cureus.47711\n4\n of \n6\nCGTAGTTGCGTTAGTTCTCT\n2\n20\n45 / 45\n51.8\n-1.57\n0\n95\n4\nAAAGAATTCGAATCAAAGATA\n5\n20\n45 / 23.81\n54.5 /\n45.1\n-5.17\n0\n-3.11\n0\nThe forward primer binds to\nthe - strand.\nTCTCTGTGTCAGAACGCTCG\n0\n20\n50 / 55\n56.5 /\n56.\n-3.11\n0\n100\n5\nTCTCTGTGTCAGAACGCTCG\n0\n20\n50 / 55\n56.5 /\n56.\n-3.11\n0\n-2.89\n100\n132\nAGAGGATGATGAATAAAATA\n1\n20\n45 / 25\n54.5 /\n42.6\n-2.27\n0\n0\nTABLE\n 1: Primer design results.\nEach row represents a distinct primer pair. The columns detail the primer's ID, creation method, nucleotide sequence, mismatches, length, GC content,\nmelting temperature and potential self-binding energies (\nΔ\nG for homodimers, monomers and heterodimers). In addition, the table highlights each primer's\nspecificity to the monkeypox virus and the expected size of the DNA fragment produced in the polymerase chain reaction (PCR). Values appear as i/j,\nwhere 'i' denotes the language model's prediction while 'j' the references values from the primer3 software.\nWhen two values are given in the form of \ni/j\n, \ni\n represents the LLM’s prediction while \nj\n is the value determined\nby the software primer3 \n[10-12]\n.\nDiscussion\nSince all models were tasked with generating primers for the DNA-dependent RNA polymerase of MPXV\n(GenBank OR209312), the results primarily provide insights into this specific context. Nevertheless, the data\ngarnered from this focused study still offer valuable preliminary insights into the capabilities and boundaries\nof LLMs in primer design for specific genomic targets.\nBoth models of ChatGPT could grasp that a primer is a short oligomer and made reasonable predictions of\nTm. However, predictions of GC%, which could have been an easy task for an LLM of this order, was\nsurprisingly not up to the mark. We suggest further studies with chain-of-thought prompting that might\naddress this issue. The crux of this method lies in the assumption that the language model should not only\ngenerate accurate predictions but also rationalise these predictions logically, rooted in the context provided\nby the exemplars \n[8]\n. Each exemplar in chain-of-thought prompting serves a dual purpose. Firstly, it informs\nthe model about the nature of the task, much like in the case of standard few-shot prompting. However,\nmore importantly, it also acts as an explanatory guide, illuminating the reasoning process that leads to a\nparticular output \n[13]\n.\nAs for the primer pairs generated, on evaluation by in-silico PCR, we see that ChatGPT-4 performs\nmarginally better than ChatGPT-3.5. However, only one primer pair is found to have parameters suitable for\nexperimental validation. The ~6 °C Tm difference will prove to be a challenge for setting annealing and\nextension temperatures, and as is done in the case of using currently employed tools, manual tweaking of\nthe sequences to achieve optimal/parameters will have to be done.\nConclusions\nThis study has successfully tested two commonly used LLMs for designing primers for diagnostic PCR. The\nmodels were fed training data using few-shot prompting and were asked to generate five primers for the\namplification-based detection of the MPXV (GenBank: OR209312.1). It was found that out of 10 AI-\ngenerated primer pairs, only one was found comparable enough to currently used rule-based, deterministic\nprograms employed by researchers and professionals. We conclude that ChatGPT is not (yet) ready to\ndirectly help biologists and clinicians with PCR designing and that this process may still have to be done (at\nleast partially) manually for a few years to come.\nAdditional Information\nAuthor Contributions\nAll authors have reviewed the final version to be published and agreed to be accountable for all aspects of the\nwork.\nConcept and design:\n  \nSoham Jorapur, Suyamindra Kulkarni, Amisha Srivastava\nAcquisition, analysis, or interpretation of data:\n  \nSoham Jorapur, Amisha Srivastava\n2023 Jorapur et al. Cureus 15(10): e47711. DOI 10.7759/cureus.47711\n5\n of \n6\nDrafting of the manuscript:\n  \nSoham Jorapur, Amisha Srivastava\nCritical review of the manuscript for important intellectual content:\n  \nSoham Jorapur, Suyamindra\nKulkarni, Amisha Srivastava\nSupervision:\n  \nSuyamindra Kulkarni\nDisclosures\nHuman subjects:\n All authors have confirmed that this study did not involve human participants or tissue.\nAnimal subjects:\n All authors have confirmed that this study did not involve animal subjects or tissue.\nConflicts of interest:\n In compliance with the ICMJE uniform disclosure form, all authors declare the\nfollowing: \nPayment/services info:\n All authors have declared that no financial support was received from\nany organization for the submitted work. \nFinancial relationships:\n All authors have declared that they have\nno financial relationships at present or within the previous three years with any organizations that might\nhave an interest in the submitted work. \nOther relationships:\n All authors have declared that there are no\nother relationships or activities that could appear to have influenced the submitted work.\nAcknowledgements\nThe use of large language models (LLMs) in this study was strictly confined to the scope of the experiment as\ndefined herein. In the interest of clarity and transparency, we would like to emphasise that while this study\nevaluated the potential of ChatGPT-3.5 and GPT-4 in primer design for various PCR techniques, the actual\nmanuscript was not compiled using ChatGPT or any other language models. \nThe experiments were\nconceptualised and designed by SJ and were executed collaboratively by SJ and AS under the guidance of SK.\nThe manuscript was jointly drafted by SJ, AS and SK.\nReferences\n1\n. \nThe Nobel Prize in Chemistry 1993: Kary B. Mullis\n. (2023). Accessed: August 23, 2023:\nhttps://www.nobelprize.org/prizes/chemistry/1993/mullis/facts/\n.\n2\n. \nSaiki RK, Scharf S, Faloona F, Mullis KB, Horn GT, Erlich HA, Arnheim N: \nEnzymatic amplification of beta-\nglobin genomic sequences and restriction site analysis for diagnosis of sickle cell anemia\n. Science. 1985,\n230:1350-4. \n10.1126/science.2999980\n3\n. \nPCR protocols: a guide to methods and applications\n. Innis M, Gelfand D, Sninsky J, et al. (ed): Elsevier,\nLondon; 1989.\n4\n. \nPCR primer: a laboratory manual, 2nd ed.\n. Dieffenbach C, Dveksler G (ed): Cold Spring Harbor Laboratory\nPress, Cold Spring Harbor, New York; 2003.\n5\n. \nOpenAI: ChatGPT\n. (2023). Accessed: July 14, 2023: \nhttps://openai.com/blog/chatgpt\n.\n6\n. \nWei J, Tay Y, Bommasani R, et al.: \nEmergent abilities of large language models\n. arXiv CoRR. 2022,\n10.48550/arXiv.2206.07682\n7\n. \nARTIC Network: artic-network/primer-schemes: v1.1.1\n. (2020). Accessed: July 14, 2023:\nhttps://doi.org/10.5281/zenodo.4020380\n.\n8\n. \nSchick T, Schütze H: \nIt’s not just size that matters: small language models are also few-shot learners\n. arXiv\nCoRR. 2020, \n10.48550/ARXIV.2009.07118\n9\n. \nBenchling (biology software)\n. (2023). Accessed: July 14, 2023: \nhttps://benchling.com\n.\n10\n. \nKoressaar T, Remm M: \nEnhancements and modifications of primer design program Primer3\n. Bioinformatics.\n2007, 23:1289-91. \n10.1093/bioinformatics/btm091\n11\n. \nUntergasser A, Cutcutache I, Koressaar T, Ye J, Faircloth BC, Remm M, Rozen SG: \nPrimer3--new capabilities\nand interfaces\n. Nucleic Acids Res. 2012, 40:e115. \n10.1093/nar/gks596\n12\n. \nKõressaar T, Lepamets M, Kaplinski L, Raime K, Andreson R, Remm M: \nPrimer3_masker: integrating\nmasking of template sequence with primer design software\n. Bioinformatics. 2018, 34:1937-8.\n10.1093/bioinformatics/bty036\n13\n. \nWei J, Wang X, Schuurmans D, et al.: \nChain-of-thought prompting elicits reasoning in large language\nmodels\n. arXiv CoRR. 2022, \n10.48550/ARXIV.2201.11903\n2023 Jorapur et al. Cureus 15(10): e47711. DOI 10.7759/cureus.47711\n6\n of \n6",
  "topic": "Primer (cosmetics)",
  "concepts": [
    {
      "name": "Primer (cosmetics)",
      "score": 0.7988390922546387
    },
    {
      "name": "Polymerase chain reaction",
      "score": 0.6212321519851685
    },
    {
      "name": "Medicine",
      "score": 0.5748239159584045
    },
    {
      "name": "Computational biology",
      "score": 0.44335705041885376
    },
    {
      "name": "Transformer",
      "score": 0.42922890186309814
    },
    {
      "name": "Polymerase",
      "score": 0.4257201850414276
    },
    {
      "name": "Machine learning",
      "score": 0.3398543894290924
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33375704288482666
    },
    {
      "name": "Genetics",
      "score": 0.2969765365123749
    },
    {
      "name": "Computer science",
      "score": 0.28097692131996155
    },
    {
      "name": "Gene",
      "score": 0.20631998777389526
    },
    {
      "name": "Biology",
      "score": 0.19624850153923035
    },
    {
      "name": "Engineering",
      "score": 0.11375868320465088
    },
    {
      "name": "Electrical engineering",
      "score": 0.11340302228927612
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}