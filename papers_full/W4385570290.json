{
    "title": "Retrieval-based Language Models and Applications",
    "url": "https://openalex.org/W4385570290",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2784921365",
            "name": "Akari Asai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2139505590",
            "name": "Se-Won Min",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2886587608",
            "name": "Zexuan Zhong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095803999",
            "name": "Danqi Chen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4206251287",
        "https://openalex.org/W3206547074",
        "https://openalex.org/W4385570688",
        "https://openalex.org/W4287332702",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W4288725442",
        "https://openalex.org/W4385573236",
        "https://openalex.org/W4312091849",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3102844651",
        "https://openalex.org/W4320813768",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W4311731003",
        "https://openalex.org/W4221164017",
        "https://openalex.org/W4284670538",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4286905627",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W4281629162",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W3155807546",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W4287649493",
        "https://openalex.org/W4221155916",
        "https://openalex.org/W4309217888",
        "https://openalex.org/W4385570444",
        "https://openalex.org/W3175863856",
        "https://openalex.org/W4309953147",
        "https://openalex.org/W4385570569",
        "https://openalex.org/W3212422886",
        "https://openalex.org/W3037722615",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4205694376",
        "https://openalex.org/W4385573102",
        "https://openalex.org/W3157700644"
    ],
    "abstract": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 6: Tutorial Abstracts, pages 41–46\nJuly 9, 2023 ©2023 Association for Computational Linguistics\nTutorial Proposal:\nRetrieval-based Language Models and Applications\nAkari Asai† Sewon Min† Zexuan Zhong‡ Danqi Chen‡\n† University of Washington ‡Princeton University\n{akari,sewon}@cs.washington.edu\n{zzhong,danqic}@cs.princeton.edu\n1 Description\nLanguage models (LMs) such as GPT-3 (Brown\net al., 2020) and PaLM (Chowdhery et al., 2022)\nhave shown impressive abilities in a range of natu-\nral language processing (NLP) tasks. However, re-\nlying solely on their parameters to encode a wealth\nof world knowledge requires a prohibitively large\nnumber of parameters and hence massive compute,\nand they often struggle to learn long-rail knowl-\nedge (Roberts et al., 2020; Kandpal et al., 2022;\nMallen et al., 2022). Moreover, these paramet-\nric LMs are fundamentally incapable of adapting\nover time (De Cao et al., 2021; Lazaridou et al.,\n2021; Kasai et al., 2022), often hallucinate (Shus-\nter et al., 2021), and may leak private data from the\ntraining corpus (Carlini et al., 2021). To overcome\nthese limitations, there has been growing interest\nin retrieval-based LMs (Guu et al., 2020; Khan-\ndelwal et al., 2020; Borgeaud et al., 2022; Zhong\net al., 2022; Izacard et al., 2022b; Min et al., 2022),\nwhich incorporate a non-parametric datastore (e.g.,\ntext chunks from an external corpus) with their\nparametric counterparts. Retrieval-based LMs can\noutperform LMs without retrieval by a large mar-\ngin with much fewer parameters (Mallen et al.,\n2022), can update their knowledge by replacing\ntheir retrieval corpora (Izacard et al., 2022b), and\nprovide citations for users to easily verify and eval-\nuate the predictions (Menick et al., 2022; Bohnet\net al., 2022).\nPreviously, retrieval and LMs have been studied\nmostly separately, and only recently researchers\nhave integrated them and built systems in which\nretrieval and LMs interact more organically, and a\nnumber of retrieval-based LMs have been proposed\ndue to growing interest. They differ in their neural\narchitectures (e.g., the granularity of retrieval units,\nhow to integrate retrieved information), learning\nalgorithms, and different uses in downstream ap-\nplications. In this tutorial, we aim to provide a\ncomprehensive and coherent overview of recent\nadvances in retrieval-based LMs. We will start\nby first providing preliminaries covering the foun-\ndations of LM (e.g., masked LMs, autoregressive\nLMs) and retrieval systems (e.g., nearest-neighbor\nsearch methods widely used in neural retrieval sys-\ntems; Karpukhin et al. 2020). We will then focus\non recent progress in architectures, learning ap-\nproaches, and applications of retrieval-based LMs.\nA taxonomy of architectures We introduce a\ntaxonomy of architectures of retrieval-based LMs\nbased on a variety of dimensions. Retrieval-based\nLMs can be categorized by the granularity of re-\ntrieved units stored in the datastore: either 1) a\nchunk of text (Borgeaud et al., 2022; Izacard et al.,\n2022b), or 2) a token (Khandelwal et al., 2020;\nZhong et al., 2022; Min et al., 2022), or 3) an en-\ntity mention (Févry et al., 2020; de Jong et al.,\n2022). We also plan to cover techniques for refin-\ning data stores and improving similarity search (He\net al., 2021; Alon et al., 2022). At the same time,\nretrieval-base LMs can be categorized based on\nhow the retrieved information is integrated with\nthe parametric encoder: 1) whether retrieved com-\nponents are concatenated with the original input\ntext (Lewis et al., 2020; Guu et al., 2020; Izacard\net al., 2022b), 2) whether the retrieved components\nare latent and integrated into the intermediate lay-\ners of Transformers (de Jong et al., 2022; Févry\net al., 2020; Borgeaud et al., 2022), or 3) distribu-\ntion of tokens from the retrieved components and\nthe LMs are interpolated (Khandelwal et al., 2020;\nZhong et al., 2022; Yogatama et al., 2021).\nScalable learning algorithms Then, we discuss\nthe training approaches of retrieval-based LMs.\nSince a retrieval datastore is typically very large,\nhow to train retrieval-based LMs effectively and\nefficiently remains challenging. We first discuss\npipelined approaches that train retrieval compo-\nnents and LMs separately, either through large-\n41\nscale pre-training (Izacard et al., 2022a) or multi-\ntask instruction tuning (Asai et al., 2022). Several\nother works train retrieval-based LMs with a fixed\nretrieval module (Borgeaud et al., 2022; Yogatama\net al., 2021). We then discuss joint training under\nreasonable resource requirements: either through\nin-batch approximations to a full datastore, or up-\ndating the datastore with updated parameters asyn-\nchronously. The former uses fractions of the full\ncorpus that are carefully designed during joint train-\ning (Zhong et al., 2022; de Jong et al., 2022; Min\net al., 2022). The latter, on the other hand, aims to\nuse full corpus during training with asynchronous\nindex update for every certain time steps (Izacard\net al., 2022b; Guu et al., 2020).\nAdaption to downstream tasksAfter discussing\nthe basic building blocks of retrieval-based LMs,\nwe show how retrieval-based LMs are adapted to\ndownstream applications. We first briefly summa-\nrize the two approaches to adapt a model to a new\ntask: zero-shot or few-shot prompting without any\nparameter updates (Shi et al., 2022; Wang et al.,\n2022), and fine-tuning on target task data (Lewis\net al., 2020). We then discuss methods designed\nto build more powerful retrieval-based LMs for\ncertain downstream tasks, such as dialogue (Shus-\nter et al., 2021), semantic parsing (Pasupat et al.,\n2021), and machine translation (Khandelwal et al.,\n2021; Zheng et al., 2021).\nUp to this point, our tutorial has mainly fo-\ncused on retrieving and integrating English plain\ntext. At this end, we will cover recent exten-\nsions of retrieval-based LMs beyond English text,\nincluding multilingual (Asai et al., 2021), multi-\nmodal (Chen et al., 2022; Yasunaga et al., 2022)\nand code (Parvez et al., 2021) retrieval. These\nworks often extend dense retrieval models to enable\nretrieval between heterogeneous input spaces (e.g.,\ncross-lingual, cross-modal) and have shown that\nreferring retrieved knowledge leads to knowledge-\nintensive generation.\nFinally, we will use an exercise to showcase the\neffectiveness of retrieval-based LMs. We con-\nclude our tutorial by discussing several important\nquestions and future directions, including (1) how\nwe can further improve the scalability of retrieval-\nbased LMs without sacrificing performance, (2)\nwhen retrieval-based LMs are particularly useful\nin the era of rapidly evolving LMs, and (3) what is\nnecessary to enable applications of retrieval-based\nLMs for more diverse domains.\n2 Tutorial Outline\n1. Introduction (15 minutes)\n• An overview of the tutorial\n• Why retrieval-based LMs?\n2. Preliminaries (15 minutes)\n• Language models: Auto-regressive LMs vs.\nmasked LMs\n• Dense retrieval methods\n• Approximate nearest neighbor search\n3. Retrieval-based LMs: A taxonomy of archi-\ntectures (40 minutes)\n• Granularity of datastore: tokens, entity men-\ntions, and chunks of text\n• How retrieved information is integrated: in-\ncorporation in the input layer, intermediate\nlayers, and the output layer\n4. Retrieval-based LMs: Scalable learning algo-\nrithms (40 minutes)\n• Pipelined training\n• Training with In-batch approximations\n• Joint training of retrieval and LMs with asyn-\nchronous updates of corpus\n5. Retrieval-based LMs: Downstream adapta-\ntions (40 minutes)\n• Adaptation methods: zero-shot/few-shot\nprompting and fine-tuning on downstream\ntasks\n• Downstream applications and task-specific\nmodifications (e.g., dialogue, semantic pars-\ning)\n6. Extensions beyond English text(10 minutes)\n• Multilingual retrieval-based LMs\n• Multimodal retrieval-based LMs\n• Code generation\n7. Demostration: An exercise to show retrieval-\naugmented LMs (10 minutes)\n8. Conclusions and future directions(10 min-\nutes)\n42\n3 Tutorial Information\nType of the tutorialCutting-edge.\nLength This is a 3-hour tutorial.\nTarget audience The tutorial will be accessi-\nble to anyone who has a basic knowledge of ma-\nchine learning and natural language processing. We\nthink the topic will be of interest to both NLP re-\nsearchers/students in academia and NLP practition-\ners in the industry.\nBreadth We estimate that 20% of the work cov-\nered in this tutorial will be by the presenters and\nthe remaining 80% by others. The papers we will\ncover are from both academia and industry.\nDiversity considerations. The speakers are from\ntwo academic institutions with an affiliation with\nan industry research group, including both a profes-\nsor and Ph.D. students. Three out of four speakers\nare female. The methods covered by our tutorials\ncan scale up to various languages or domains, and\nwe also briefly cover several papers focusing on\nmultilingual and expert-domain extensions of the\ncore frameworks. We will reach out to academic\ncommunities such as WiNLP1 and Masakhane2 to\nencourage them to attend our tutorial for participa-\ntion of diverse audiences. Since retrieval-based\nLMs are alternatives to LMs with a significantly\nlarge number of parameters, we expect this tutorial\nto be especially useful to researchers with mod-\nest resources who do no have access to very large\nmodels.\nAn estimate of the audience sizeGiven that lan-\nguage models are now used in a range of NLP tasks\nand retrieval-based approaches have been applied\nto diverse domains, we estimate that the number of\naudiences will be around 150+.\nVenues. We prefer ACL due to the growing in-\nterest in the area and the travel constraints of some\nof the speakers. EMNLP is our second preferred\nchoice, and we currently do not consider EACL.\nTechnical equipment. We would like to have\nInternet access to show online demos.\nOpen access We plan to make all teaching ma-\nterial available online and agree to allow the pub-\nlication of slides and video recordings in the ACL\nanthology.\n1http://www.winlp.org/\n2https://www.masakhane.io/\nEthical considerations Retrieval-based LMs are\noften more powerful and parameter-efficient than\nLMs, and do not require full re-training to update\nworld knowledge, which makes it more energy-\nefficient and can reduce carbon footprints. Prior\nwork also shows that referring to external world\nknowledge can reduce harmful biases and hallu-\ncinations, although retrieval-based LMs can still\nbe plausible sounding but incorrect or non-sensical\noutputs. We note that, as retrieval-based LMs may\nretrieve raw data from a corpus, which can leak\nprivacy-sensitive information, especially when they\nare built on top of a private corpus. We acknowl-\nedge this to caution those who manage to apply\nretrieval-based LMs to privacy-sensitive domains.\nPedagogical material We plan to do some short\nhands-on exercises to let the audience try different\nretrieval-based LMs with few-shot prompting using\nColab.\nPast tutorials.\n• ACL 2020 tutorial on Open-domain QA (Chen\nand Yih, 2020): This tutorial provides com-\nprehensive reviews of open-domain question\nanswering, some of which consist of a re-\ntriever and a generative model, while we fo-\ncus on the recent progress of architectures and\nlearning algorithms of retrieval-based LMs\nfor diverse NLP tasks, not limiting its focus\nto open-domain QA. Most of the papers will\nbe discussed in this tutorial have been pub-\nlished since the Open-domain QA tutorial\nthree years ago. Moreover, one of the instruc-\ntors, Danqi was an instructor of this ACL 2020\ntutorial.\n• SIGIR 2022 tutorial on Recent Advances in\nRetrieval-Augmented Text Generation (Cai\net al., 2022): This tutorial focuses mainly on\nrecent retrieval-augmented text generation ap-\nproaches with a focus on two applications:\ndialogue and machine translation. Our tuto-\nrial puts more emphasis on the architecture\nand learning methods of retrieval-based LMs\nthat can be applicable to diverse NLP tasks.\n4 Presenters\nAkari Asai Akari Asai is a Ph.D. student in the\nPaul G. Allen School of Computer Science & En-\ngineering at the University of Washington, advised\nby Prof. Hannaneh Hajishirzi. Her research lies\n43\nin natural language processing and machine learn-\ning. Her recent research focuses on question an-\nswering, retrieval-based LMs, multilingual NLP,\nand entity-aware representations. She received the\nIBM Fellowship in 2022. She is a lead organizer\nof the Workshop on Multilingual Information Ac-\ncess (NAACL 2022) and serves as an area chair in\nquestion answering at EACL 2023.\nSewon Min Sewon Min is a Ph.D. student in the\nPaul G. Allen School of Computer Science & En-\ngineering at the University of Washington, and a\nvisiting researcher at Meta AI. Her research spans\nquestion answering, representation and retrieval of\nfactoid knowledge, and language modeling. She\nwas a co-instructor and a co-organizer of multi-\nple tutorials and workshops at ACL, NAACL-HLT,\nEMNLP, NeurIPS and AKBC, including a tuto-\nrial on Few-Shot NLP with Pretrained Language\nModels (ACL 2022), a tutorial on NLP for Long Se-\nquences (NAACL-HLT 2021), and the Workshop\non Semiparametric Methods in NLP (ACL 2022).\nZexuan Zhong Zexuan Zhong is a Ph.D. student\nin the Department of Computer Science at Prince-\nton University, advised by Prof. Danqi Chen. His\nresearch interests lie in natural language processing\nand machine learning. His recent research focuses\non retrieval-based LMs, generalization of retrieval\nmodels, and efficient models in NLP. He received\na J.P. Morgan PhD Fellowship in 2022.\nDanqi Chen Danqi Chen is an Assistant Profes-\nsor of Computer Science at Princeton University\nand co-leads the Princeton NLP Group. Her re-\ncent research focuses on training, adapting, and\nunderstanding large LMs, and developing scalable\nand generalizable NLP systems for question an-\nswering, information extraction, and conversational\nagents. Danqi is a recipient of a Sloan Fellowship,\na Samsung AI Researcher of the Year award, out-\nstanding paper awards from ACL 2016, EMNLP\n2017 and ACL 2022, and multiple industry fac-\nulty awards. Danqi served as the program chair\nfor AKBC 2021 and (senior) area chairs for many\n*ACL conferences. She taught a tutorial on “Open-\ndomain Question Answering” at ACL 2020.\n5 Reading List\n• Unsupervised Dense Information Retrieval\nwith Contrastive Learning (Izacard et al.,\n2022a)\n• Task-aware Retrieval with Instructions (Asai\net al., 2022)\n• Atlas: Few-shot Learning with Retrieval Aug-\nmented Language Models (Izacard et al.,\n2022b)\n• Improving language models by retrieving\nfrom trillions of tokens (Borgeaud et al., 2022)\n• Mention Memory: incorporating textual\nknowledge into Transformers through entity\nmention attention (de Jong et al., 2022)\n• Generalization through Memorization: Near-\nest Neighbor Language Models (Khandelwal\net al., 2020)\n• Nonparametric Masked Language\nModel (Min et al., 2022)\n• Training Language Models with Memory\nAugmentation (Zhong et al., 2022)\n• kNN-Prompt: Nearest Neighbor Zero-Shot\nInference (Shi et al., 2022)\n• Neuro-Symbolic Language Modeling with\nAutomaton-augmented Retrieval (Alon et al.,\n2022)\nReferences\nUri Alon, Frank F. Xu, Junxian He, Sudipta Sen-\ngupta, Dan Roth, and Graham Neubig. 2022.\nNeuro-symbolic language modeling with automaton-\naugmented retrieval. In International Conference on\nMachine Learning (ICML), Baltimore, USA.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nAkari Asai, Xinyan Yu, Jungo Kasai, and Hanna Ha-\njishirzi. 2021. One question answering model for\nmany languages with cross-lingual dense passage\nretrieval. In Advances in Neural Information Pro-\ncessing Systems.\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al.\n2022. Attributed question answering: Evaluation and\nmodeling for attributed large language models. arXiv\npreprint arXiv:2212.08037.\n44\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nProceedings of the 39th International Conference on\nMachine Learning.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in neural information process-\ning systems.\nDeng Cai, Yan Wang, Lemao Liu, and Shuming Shi.\n2022. Recent advances in retrieval-augmented text\ngeneration. In Proceedings of the 45th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21).\nDanqi Chen and Wen-tau Yih. 2020. Open-domain\nquestion answering. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: Tutorial Abstracts, pages 34–37, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and\nWilliam W Cohen. 2022. Murag: Multimodal\nretrieval-augmented generator for open question an-\nswering over images and text. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. PaLM: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing.\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Fei Sha, and William W. Cohen. 2022. Mention\nmemory: incorporating textual knowledge into trans-\nformers through entity mention attention. In Interna-\ntional Conference on Learning Representations.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning.\nJunxian He, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2021. Efficient nearest neighbor lan-\nguage models. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language\nProcessing, Punta Cana, Dominican Republic.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022a. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022b. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodels struggle to learn long-tail knowledge. arXiv\npreprint arXiv:2211.08411.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. Realtime qa: What’s the answer right now?\narXiv preprint arXiv:2207.13332.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In International Conference\non Learning Representations.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,\nDevang Agrawal, Adam Liska, Tayfun Terzi, Mai\nGimenez, Cyprien de Masson d’Autume, Tomas Ko-\ncisky, Sebastian Ruder, et al. 2021. Mind the gap:\nAssessing temporal generalization in neural language\n45\nmodels. Advances in Neural Information Processing\nSystems.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Inves-\ntigating effectiveness and limitations of paramet-\nric and non-parametric memories. arXiv preprint\narXiv:2212.10511.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, et al. 2022. Teaching\nlanguage models to support answers with verified\nquotes. arXiv preprint arXiv:2203.11147.\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-\ntau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.\n2022. Nonparametric masked language modeling.\narXiv preprint arXiv:2212.01349.\nMd Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty,\nBaishakhi Ray, and Kai-Wei Chang. 2021. Retrieval\naugmented code generation and summarization. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 2719–2734, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nPanupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.\nControllable semantic parsing via retrieval augmen-\ntation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing.\nWeijia Shi, Julian Michael, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. Nearest neighbor zero-shot\ninference. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021.\nZhenhailong Wang, Xiaoman Pan, Dian Yu, Dong Yu,\nJianshu Chen, and Heng Ji. 2022. Zemi: Learn-\ning zero-shot semi-parametric language models from\nmultiple tasks. arXiv preprint arXiv:2210.00185.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen-tau Yih. 2022. Retrieval-\naugmented multimodal language modeling. arXiv\npreprint arXiv:2211.12561.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive semiparametric lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 9:362–373.\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang,\nBoxing Chen, Weihua Luo, and Jiajun Chen. 2021.\nAdaptive nearest neighbor machine translation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing.\n46"
}