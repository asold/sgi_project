{
    "title": "Combining Statistical Language Models via the Latent Maximum Entropy Principle",
    "url": "https://openalex.org/W2159863202",
    "year": 2005,
    "authors": [
        {
            "id": "https://openalex.org/A2000707899",
            "name": "Shaojun Wang",
            "affiliations": [
                "University of Alberta"
            ]
        },
        {
            "id": "https://openalex.org/A1817936516",
            "name": "Dale Schuurmans",
            "affiliations": [
                "University of Alberta"
            ]
        },
        {
            "id": "https://openalex.org/A2101370014",
            "name": "Fuchun Peng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2042121132",
            "name": "Yunxin Zhao",
            "affiliations": [
                "University of Missouri"
            ]
        },
        {
            "id": "https://openalex.org/A2000707899",
            "name": "Shaojun Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1817936516",
            "name": "Dale Schuurmans",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101370014",
            "name": "Fuchun Peng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2042121132",
            "name": "Yunxin Zhao",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6637295340",
        "https://openalex.org/W2118714763",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W4302155765",
        "https://openalex.org/W1989705153",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W2163306339",
        "https://openalex.org/W1549285799",
        "https://openalex.org/W1007293483",
        "https://openalex.org/W2001792610",
        "https://openalex.org/W2160842254",
        "https://openalex.org/W2049633694",
        "https://openalex.org/W4245668478",
        "https://openalex.org/W2134731454",
        "https://openalex.org/W7041975391",
        "https://openalex.org/W1597533204",
        "https://openalex.org/W7071444332",
        "https://openalex.org/W2114858359",
        "https://openalex.org/W4243333943",
        "https://openalex.org/W2132434674",
        "https://openalex.org/W2151752770",
        "https://openalex.org/W6632620551",
        "https://openalex.org/W2123893795",
        "https://openalex.org/W1996903695",
        "https://openalex.org/W2100506586",
        "https://openalex.org/W1995875735",
        "https://openalex.org/W2120340025",
        "https://openalex.org/W6684295031",
        "https://openalex.org/W2147030611",
        "https://openalex.org/W2009570821",
        "https://openalex.org/W2108408304",
        "https://openalex.org/W1734853756",
        "https://openalex.org/W2012026000",
        "https://openalex.org/W2795996653",
        "https://openalex.org/W161994183",
        "https://openalex.org/W1543028224",
        "https://openalex.org/W1978861152",
        "https://openalex.org/W1509803206",
        "https://openalex.org/W4231741839",
        "https://openalex.org/W2167661947",
        "https://openalex.org/W2068905009",
        "https://openalex.org/W4290649644",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W3129711340",
        "https://openalex.org/W1508165687",
        "https://openalex.org/W3049188988",
        "https://openalex.org/W2164469736",
        "https://openalex.org/W2096175520"
    ],
    "abstract": null,
    "full_text": "Machine Learning, 60, 229–250, 2005\n2005 Springer Science + Business Media, Inc. Manufactured in The Netherlands.\nCombining Statistical Language Models via the\nLatent Maximum Entropy Principle\nSHAOJUN W ANG swang@cs.ualberta.ca\nDALE SCHUURMANS dale@cs.ualberta.ca\nDepartment of Computing Science, University of Alberta, Canada\nFUCHUN PENG fuchun@cs.umass.edu\nDepartment of Computer Science, University of Massachusetts at Amherst, USA\nYUNXIN ZHAO zhaoy@missouri.edu\nDepartment of Computer Engineering and Computer Science, University of Missouri at Columbia, USA\nEditors: Dan Roth and Pascale Fung\nAbstract. We present a uniﬁed probabilistic framework for statistical language modeling which can simul-\ntaneously incorporate various aspects of natural language, such as local word interaction, syntactic structure\nand semantic document information. Our approach is based on a recent statistical inference principle we have\nproposed—the latent maximum entropy principle—which allows relationships over hidden features to be effec-\ntively captured in a uniﬁed model. Our work extends previous research on maximum entropy methods for language\nmodeling, which only allow observed features to be modeled. The ability to conveniently incorporate hidden vari-\nables allows us to extend the expressiveness of language models while alleviating the necessity of pre-processing\nthe data to obtain explicitly observed features. We describe efﬁcient algorithms for marginalization, inference and\nnormalization in our extended models. We then use these techniques to combine two standard forms of language\nmodels: local lexical models (Markov N-gram models) and global document-level semantic models (probabilistic\nlatent semantic analysis). Our experimental results on the Wall Street Journal corpus show that we obtain a 18.5%\nreduction in perplexity compared to the baseline tri-gram model with Good-Turing smoothing.\nKeywords: language modeling, N-gram models, latent semantic analysis, maximum entropy, latent variables\n1. Introduction\nIn the past decade, a vast amount of information has become available on the World Wide\nWeb, much of it in the form of natural language text. As the amount and signiﬁcance of\nthis data source grows, it will become increasingly important to ﬁnd ways to effectively\nexploit it to help us improve our understanding of the world. Thus problems of information\nextraction and knowledge discovery from massive textual data sets are destined to remain\nimportant. In our view, statistical language models that also model hidden features provide\na promising approach to robustly extracting information from natural language text.\nStatistical language modeling is concerned with determining the probability of natu-\nrally occurring word sequences in human natural language. Traditionally, the dominant\nmotivation for language modeling has come from the ﬁeld of speech recognition (Jelinek,\n1998), however statistical language models have recently become more widely used in\n230 S. W ANG ET AL.\nmany other application areas, such as information retrieval (Lafferty & Zhai, 2001), ma-\nchine translation (Brown et al., 1992), optical character recognition, spelling correction,\ndocument classiﬁcation (Peng, Schuurmans,& Wang, 2004), and bio-informatics (Durbin\net al., 1998).\nThere are various kinds of language models that can be used to capture different aspects\nof natural language regularity. The simplest and most successful language models are the\nMarkov chain (N-gram) source models, ﬁrst explored by Shannon in his seminal paper\n(Shannon, 1948). Such N-gram models effectively capture local lexical regularities in text.\nSubsequently, a wide variety of smoothing methods have been developed to address the\nproblem of estimating rare events for these models (Chen & Goodman, 1999). The resulting\nsmoothed N-gram language models have become a key component of state of the art speech\nrecognizers, by helping to resolve acoustic ambiguities by placing higher probability on\nmore likely word strings.\nWhile Markov chains are efﬁcient at encoding local word interactions, natural language\nclearly has a richer structure than can be conveniently captured by an N-gram model. For\nexample, attempting to increase the order of an N-gram to capture longer range dependen-\ncies in natural language immediately runs into the curse of dimensionality (Bengio et al.,\n2003; Jelinek, 1998; Rosenfeld, 2000).\nMany recent approaches have been proposed to capture and exploit different aspects of\nnatural language regularity with the goal of outperforming the simple N-gram model. For\nexample, the structural language model (Chelba & Jelinek, 2000; Roark, 2001) effectively\nexploits relevant syntactic regularities to improve the perplexity score of N-gram models,\nand the semantic language model (Bellegarda, 2000; Bengio et al., 2003; Hofmann, 2001)\nexploits document-level semantic regularities to acheive similar improvements. Although\neach of these language models outperforms simple N-grams, they each only capture speciﬁc\nlinguistic phenomena. None of them can simultaneously take into account the lexical\ninformation inherent in Markov chain models, the hierarchical syntactic tree structure\nin stochastic branching processes, and the semantic content in bag-of-words categorical\nmixture log-linear models— all in a uniﬁed probabilistic framework.\nSeveral techniques for combining language models have thus also been investigated. The\nmost commonly used method is simple linear interpolation (Chelba & Jelinek, 2000; Jelinek\n& Mercer, 1980; Roark, 2001; Rosenfeld, 1996), where each individual model is trained\nseparately and then combined by a weighted linear combination, where the weights are\ntrained using held out data. Even though this technique is simple and easy to implement it\ndoes not generally yield effective combinations because the linear additive form is too blunt\nto capture subtleties in each of the component models (Rosenfeld, 1996). Another approach\nis based on Jaynes’ maximum entropy (ME) principle (Jaynes, 1983). This approach has\nseveral advantages over other methods for statistical modeling, such as introducing less data\nfragmentation (as in decision tree learning), requiring fewer independence assumptions\n(as in naive Bayes models), and exploiting a principled technique for automatic feature\nweighting. The major weakness with maximum entropy methods, however, are that they can\nonly model distributions over explicitlyobserved features, whereas in natural language we\nencounter hidden semantic (Bellegarda, 2000; Hofmann, 2001) and syntactic information\n(Chelba & Jelinek, 2000) which we do not observe directly.\nCOMBINING STATISTICAL LANGUAGE MODELS 231\nOne way to encode constraints over hidden features in a maximum entropy model is to\nﬁrst pre-process the training corpus to obtain explicit values for all of the hidden features—\nsuch as recovering syntactic structure by running a parser, or recovering semantic content by\nusing a latent semantic indexer—and then incorporating statistics over explicitly measured\nfeatures as additional constraints in the model (Berger, Della Pietra, & Della Pietra, 1996;\nKhudanpur & Wu, 2000; Rosenfeld, 1996). However, doing so explicitly is not always\npossible, and even if attempted, sparse data problems almost always immediately arise\nin such complex models. Consequently, the perplexity improvements or word error rate\nreductions obtained are often minimal. In this paper we address the question: is it possible\nto exploit the hidden hierarchical structure of natural language within a maximum entropy\nmethod without resorting to explicit preliminary parsing or semantic analysis?\nRecently we proposed a latent maximum entropy(LME) principle (Wang, Schuurmans,\n& Zhao, 2003) which extends Jaynes’ maximum entropy principle to incorporate latent\nvariables. It is different from both Jaynes’ ME principle and maximum likelihood esti-\nmation, but often yields better estimates in the presence of hidden variables. Preliminary\nexperiments in (Wang, Schuurmans, & Zhao, 2003) showed that estimation based on the\nlatent maximum entropy principle generally yielded improved results over the maximum\nlikelihood principle when estimating latent variable models on small observed data samples.\nIn this paper, we show how our new estimation principle can be used for statistical\nlanguage modeling by training mixtures of exponential families with rich expressive power.\nBelow, we ﬁrst summarize the LME principle, its problem formulation, solution, and certain\nconvergence properties. Then we discuss how to use LME for language modeling. By\nproperly using factorization methods and exploiting the sparseness of tri-gram features, we\ncan demonstrate efﬁcient algorithms for feature expectation, inference and normalization for\ncombining Markov N-gram models with probabilistic latent semantic analysis (Hofmann,\n2001). We apply this model to Wall Street Journal data to obtain experimental results which\nsupport the utility of our approach. Finally, we discuss the advantages and limitations of\nour approach.\n2. The latent maximum entropy (LME) principle\nTo express a joint probability model, let X ∈ X denote the complete data, Y ∈ Y be the\nobserved incomplete data andZ ∈ Z be the missing data. That is,X = (Y, Z). For example,\nY might be observed natural language in the form of text, andX might be the text along with\nits missing syntactic and semantic information Z. The goal of maximum entropy is to ﬁnd\na probability model that matches certain constraints in the observed data while otherwise\nmaximizing entropy. When the data has both missing and observed components we extend\nthe maximum entropy principle to the latent maximum entropy principle as follows.\nLatent maximum entropy principle\nGiven features f\n1,..., fN specifying the properties we would like to match in the data,\nselect a joint model p∗ from the set of possible probability distributions that maximizes the\nentropy\n232 S. W ANG ET AL.\nmax\np\nH(p) =−\n∑\nx\np(x)l o gp(x)\nsubject to\n∑\nx\np(x) fi (x) =\n∑\ny\n˜p(y)\n∑\nz\np(z | y) fi (y, z); i = 1,..., N\nHere ˜p(y) is the empirical distribution of the set of observed components of the training\ndata, and p(z | y) encodes the hidden dependency structure into the statistical model.\nIntuitively, the constraints specify that we require the expectations of fi (X) in the joint\nmodel to match their empirical expectations on the incomplete data Y, taking into account\nthe structure of the implied dependence of the unobserved componentZ on Y. Note that the\nconditional distribution p(z | y) implicitly encodes the latent structure and is a nonlinear\nmapping of p(x). That is, p(z | y) = p(y, z)/∑\nz′∈Z p(y, z′) = p(x)/∑\nz∈Z p(x′) where\nx = (y, z) and x′ = (y, z′) by deﬁnition. Unfortunately, p(z | y) is a nonlinear function of\np(x), which raises the main technical challenges we face below.\nThe LME principle is strictly more general than the ME principle, and only becomes\nequivalent to ME in the special case when the features only depend on the observable data\nY. However, if the features depend on unobserved components of the data Z then ME only\nmodels the observed part of the data Y, and LME differs from ME (Wang, Schuurmans, &\nZhao, 2003).\nBelow we will apply the LME principle to the problem of combining language models.\nHowever, we ﬁrst consider a small improvement that will prove useful. In many statistical\nmodeling situations, the constraints used in the maximum entropy principle are subject\nto errors due to the empirical data, especially in a very sparse domain. One way to gain\nrobustness to these errors is to relax the constraints but add a penalty to the entropy of the\njoint model (Chen & Rosenfeld, 2000; Csiszar, 1996).\nRegularized LME (RLME) principle\nmax\np,a\nH(p) − U(a) =−\n∑\nx\np(x)l o gp(x) − U(a)( 1 )\nsubject to\n∑\nx\np(x) fi (x) =\n∑\ny\n˜p(y)\n∑\nz\np(z | y) fi (y, z) + ai ; i = 1, ...,N (2)\nHere a = (a1, ...,aN ), ai is the error for each constraint, and U : ℜ N →ℜ is a convex\nfunction (Chen & Rosenfeld, 2000; Csiszar, 1996) which has its minimum at 0. The function\nU penalizes errors in satisfying the constraints, and can be used to penalize deviations in\nthe more reliably observed constraints to a greater degree than deviations in less reliably\nobserved constraints.\nCOMBINING STATISTICAL LANGUAGE MODELS 233\n3. A training algorithm\nAssume for the time being that we have already selected the features f1, ..., fN that we\nwish to capture in the training data (we return to this issue in Section 4 below). We are\nnow left with the problem of solving the constrained optimization problem posed in (1)\nand (2). Note that due to the nonlinear mapping introduced by p(z | y) we have nonlinear\nconstraints (2) on p and the feasible set is no longer convex. So even though the objective\nfunction (1) is concave, no unique optimal solution can be expected. In fact, minima and\nsaddle points may exist.\nTo make progress, we ﬁrst restrict p(x) to be an exponential model, p\nλ(x) =\n/Phi1−1\nλ exp(∑\ni λi fi (x)), where /Phi1λ is a constant that ensures ∑\nx∈X pλ(x) = 1. This as-\nsumption makes it possible to formulate a practical iterative algorithm for ﬁnding feasible\nsolutions (below) to approximately satisfying the RLME principle. Our algorithmic strat-\negy then is to generate many feasible candidates (by restarting the iterative procedure at\ndifferent initial points), evaluate their regularized entropy and select the best model. The\nhardest part of this process is generating feasible solutions.\nThe key observation to ﬁnding feasible solutions is to note that they are intimately related\nto ﬁnding locally maximum a posteriori(MAP) solutions: Given a penalty functionU over\nerrors a, an associated prior U\n∗ on λ can be obtained by settingU∗ to the convex (Fenchel)\nconjugate (Borwein & Lewis, 2000) of U. Vice versa, given the convex conjugate cost\nfunction U∗, the corresponding penalty function U can be derived by using the property of\nFenchel biconjugation(Borwein & Lewis, 2000); that is, the conjugate of the conjugate of\na convex function is the original convex function,U = U∗∗.\nTo illustrate, consider a quadratic penalty U(a) = ∑ N\ni=1\n1\n2 σ2\ni a2\ni . Here the convex conju-\ngate U∗(λ) = ∑ N\ni=1\nλ2\ni\n2σ2\ni\ncan be determined by setting ai = λi\nσ2\ni\n; which speciﬁes a Gaussian\nprior on λ. A different example can be obtained by considering the Laplacian prior on λ,\nU∗(λ) =∥ λ∥1 = ∑ N\ni=1 | λi |, which leads to the penalty function\nU(a) =\n{\n0 ∥a∥∞ = maxN\ni=1 | ai |≤ 1\n∞ otherwise\nthat forces hard inequality constraints.\nNote that in each case, given a prior U∗, the standard MAP estimate maximizes the\npenalized log-likelihood R(λ) = ∑\ny ˜p(y)l o gpλ(y) − U∗(λ). Our key result is that locally\nmaximizing R(λ) is equivalent to satisfying the feasibility constraints (2) of the RLME\nprinciple.\nTheorem 1. Under the log-linear assumption, locally maximizing the posterior proba-\nbility of log-linear models on incomplete data is equivalent to satisfying the feasibility\nconstraints of the RLME principle. That is, the only distinction between MAP and RLME\nin log-linear models is that, among local maxima (feasible solutions), RLME selects the\nmodel with the maximum regularized entropy, whereas MAP selects the model with the\nmaximum posterior probability (Wang et al., 2004).\n234 S. W ANG ET AL.\nThat is, to ﬁnd feasible solutions it sufﬁces to ﬁnd models that maximize the penalized log-\nlikelihood (posterior) on observed data using standard iterative approaches. It is important\nto emphasize, however, that EM will only ﬁnd alternative feasible solutions, while the\nRLME and MAP principles will differ markedly in the feasible solutions they prefer. We\nillustrate this distinction below.\nTo ﬁnd feasible solutions, we use an iterative procedure, R-EM-IS, which employs an\nEM algorithm (Dempster, Laird, & Rubin, 1977) as an outer loop, but uses a nested GIS/IIS\nalgorithm (Berger, Della Pietra, & Della Pietra, 1996; Della Pietra, Della Pietra, & Lafferty,\n1997) to perform the internal M step. To derive this algorithm, ﬁrst decompose the penalized\nlog-likelihood function R(λ)i n t o\nR(λ) =\n∑\ny∈ ˜Y\n˜p(y)l o gpλ(y) − U∗(λ) = Q(λ,λ ′) + H(λ,λ ′)\nwhere Q(λ,λ ′) = ∑\ny∈ ˜Y ˜p(y) ∑\nz∈Z pλ′(z | y)l o gpλ(x) − U∗(λ), H(λ,λ ′) =\n−∑\ny∈ ˜Y ˜p(y) ∑\nz∈Z pλ′(z | y)l o gpλ(z | y). This is a standard decomposition used for\nderiving EM. For log-linear models, in particular, we have\nQ\n(\nλ,λ ( j))\n=− log(/Phi1λ) +\nN∑\ni=1\nλi\n(∑\ny∈ ˜Y\n˜p(y)\n∑\nz∈Z\nfi (x) pλ( j) (z | y)\n)\n− U∗(λ)( 3 )\nInterestingly, it turns out that maximizing Q(λ,λ ( j)) as a function of λ for ﬁxed λ( j) (the\nM step) is equivalent to solving another constrained optimization problem correspond-\ning to a maximum entropy principle; but a much simpler one than before (Wang et al.,\n2004).\nLemma 1. Maximizing Q(λ,λ ( j)) as a function ofλ for ﬁxedλ( j) is equivalent to solving\nmax\np,a\nH(p) − U(a) =−\n∑\nx\np(x)l o gp(x) − U(a)( 4 )\nsubject to\n∑\nx\np(x) fi (x) =\n∑\ny\n˜p(y)\n∑\nz\npλ( j) (z | y) fi (y, z) + ai ; i = 1, ...,N (5)\nIt is critical to realize that the new constrained optimization problem in Lemma 1 is much\neasier than maximizing (1) subject to (2) for log-linear models, because the right hand side\nof the constraints (5) no longer depends on λ but rather on the ﬁxed constants from the\nprevious iterationλ( j). This means that maximizing (4) subject to (5) with respect toλ is now\na convex optimization problem with linear constraints. The generalized iterative scaling\nalgorithm (GIS) (Darroch and Ratchliff, 1972) or improved iterative scaling algorithm\n(IIS) (Della Pietra, Della Pietra, and Lafferty, 1997) can be used to maximize Q(λ,λ\n( j))\neasily.\nCOMBINING STATISTICAL LANGUAGE MODELS 235\nFrom these observations, we can recover feasible log-linear models by using an algorithm\nthat combines EM with nested iterative scaling to calculate the M step. Assuming the\nGaussian prior, the explicit iterative procedure we obtain will be as follows.\nR-EM-IS algorithm\nEs t e p: Compute ∑\ny ˜p(y) ∑\nz pλ( j) (z | y) fi (y, z)f o ri = 1, ...,N.\nMs t e p: Perform K parallel updates of the parameter values λi for i = 1, ...,N by iterative\nscaling (GIS or IIS) as follows\nλ( j+s/K )\ni = λ( j+(s−1)/K )\ni + γ( j+s/K )\ni ; s = 1, ...,K (6)\nwhere γ( j+s/K )\ni satisﬁes\n∑\nx\npλ( j+(s−1)/K ) (x) fi (x)eγ( j+s/K )\ni f (x) + λ( j+(s−1)/K )\ni + γ( j+s/K )\ni\nσ2\ni\n=\n∑\ny\n˜p(y)\n∑\nz\npλ( j) (z | y) fi (y, z)( 7 )\nwhere f (x) = ∑ N\ni=1 fi (x). The value of γ( j+s/K )\ni can be obtained by bisection line search\nor solving the nonlinear Eq. (7) by Newton-Raphson iteration.\nA natural interpretation of this iterative procedure is that, if the right hand side of (2) is\nconstant, then the optimal solution pλ(x) is a log-linear model with parameters provided\nby GIS/IIS. Once we obtain pλ we can calculate the value of the right hand side of (2).\nIf this value matches the value previously assigned, then by the optimality condition we\nhave reached a stationary point of the penalized log-likelihood and a feasible solution of\nthe RLME problem; otherwise, we iterate until the constraints are met.\nProvided that the E and M steps can both be computed, R-EM-IS can be shown to\nconverge to a local maximum in penalized likelihood for log-linear models, and hence is\nguaranteed to yield feasible solutions to the RLME principle.\nTheorem 2. The R-EM-IS algorithm monotonically increases the penalized log-likelihood\nfunction R(λ), and all limit points of any R-EM-IS sequence{λ( j+s/K ), j ≥ 0},s = 1, ...,K,\nbelong to the set\n/Gamma1=\n{\nλ ∈ℜ N : ∂R(λ)\n∂λ = 0\n}\n(8)\nTherefore, R-EM-IS asymptotically yields feasible solutions to the RLME principle for\nlog-linear models (Wang et al., 2004).\nThus, R-EM-IS provides an effective means to ﬁnd feasible solutions to the RLME\nprinciple. (We note that Lauritzen, 1995 has suggested a similar algorithm for the unregu-\nlarized case, but did not supply a convergence proof. More recently, Riezler (1999) has also\nproposed an algorithm equivalent to setting K = 1 in EM-IS for the unregularized case.\n236 S. W ANG ET AL.\nHowever, we have foundK > 1 to be more effective in many situations, and regularization\nalso usually yields a substantial beneﬁt.)\nWe can now exploit the R-EM-IS algorithm to develop a practical approximation to the\nRLME principle.\nR-ME-EM-IS algorithm:\nInitialization: Randomly choose initial guesses for λ.\nR-EM-IS: Run R-EM-IS to convergence, to obtain feasible λ∗.\nEntropy calculation: Calculate the regularized entropy of pλ∗ .\nModel selection: Repeat the above steps several times to produce a set of distinct feasible\ncandidates. Choose the feasible candidate that achieves the highest regularized entropy.\nThis leads to a new estimation technique that we will compare to standard MAP esti-\nmation below. One apparent complication, ﬁrst, is that we need to calculate the entropies\nof the candidate models produced by R-EM-IS. However, it turns out that we do not need\nto calculate entropies explicitly because one can recover the entropy of feasible log-linear\nmodels simply as a byproduct of running R-EM-IS to convergence.\nCorollary 1. If λ∗ is feasible, then Q (λ∗,λ∗) =− H(pλ∗ ) + U(a∗) and R (λ∗) =\n−H(pλ∗ ) + U(a∗) + H(λ∗,λ∗).\nTherefore, at a feasible solution λ∗, we have already calculated the regularized entropy,\n−Q(λ∗,λ∗), in the M step of R-EM-IS.\nWhen there is no hidden variable, H(λ∗,λ∗) = 0, we have a unique solution R(λ∗) =\n−H(pλ∗ ) + U(a∗), the standard global optimal regularized maximum entropy model is\nequivalent to the global optimal MAP solution. When there are hidden variables, we have\nmultiple solutions which make things different. To draw a clear distinction between RLME\nand MAP, assume that the term H(λ\n∗,λ∗) is constant across different feasible solutions.\nThen MAP, which maximizes R(λ∗), will choose the model that has lowest regularized\nentropy, whereas RLME, which maximizes H(pλ∗ ) − U(a∗), will chose a model that has\nleast regularized likelihood. (Of course, H(λ∗,λ∗) will not be constant in practice and the\ncomparison between RLME and MAP is not so straightforward, but this example does\nhighlight their difference.) The fact that RLME and MAP are different raises the question\nof which method is the most effective when inferring a model from sample data. To address\nthis question we turn to a comparison.\n4. RLME for language modeling\nThe regularized latent maximum entropy principle can be used to model natural language\nin a principled way by combining different exponential models to obtain rich expressive\npower. In this section, we describe how to use the RLME principle to combine the tri-\ngram Markov model with probabilistic latent semantic analysis (PLSA) to obtain a better\nlanguage model.\nCurrently almost all maximum entropy language models use the conditional form ﬁrst\nproposed by Brown et al. for statistical machine translation (Brown et al., 1992). The main\nCOMBINING STATISTICAL LANGUAGE MODELS 237\nFigure 1. A graphical representation of the semantic tri-gram model, where the curve that connects the three\nword nodes together denotes the tri-gram feature. In this graphical representation, many arcs share the same\nparameters.\nreason for using the conditional model is to avoid enumerating all possible histories to\nperform inference. Here we use the joint probability model, but point out that once the\nset of features are selected, the problem of calculating the needed feature expectations\nand normalization terms becomes tractable by using proper factorization methods and\nexploiting the sparseness of tri-grams.\nCombining N-gram and PLSA models\nDeﬁne the complete data as x = (W\n2, W1, W0, D, T2, T1, T0), where W0, W1, W2 are the\ncurrent and two previous words, T2, T1, T0 are the hidden ‘topic’ values associated with\nthese words, and D is a document identiﬁer. Thus, y = (W2, W1, W0, D) is the observed\ndata and z = (T2, T1, T0) is unobserved. Typically the number of documents, words in\nthe vocabulary, and latent class variables are on the order of 100,000, 10,000 and 100,\nrespectively. A graphical representation of a semantic node interacting with a tri-gram is\nillustrated in Figure 1.\nFor the tri-gram portion of the model, all features are explicitly observed in the training\ndata, and the corresponding constraints can be modeled directly as follows.\n∑\nx\np(x)δ(W2 = wi , W1 = wj , W0 = wk ) =\n∑\nd\n˜p(d) ˜p(wi wj wk | d)\n∑\nx\np(x)\n1∑\nℓ=0\nδ(Wℓ+1 = wi , Wℓ = wj ) =\n∑\nd\n˜p(d)\n1∑\nℓ=0\n˜p(Wℓ+1 = wi , Wℓ = wj | d)\n∑\nx\np(x)\n2∑\nℓ=0\nδ(Wℓ = wi ) =\n∑\nd\n˜p(d)\n2∑\nℓ=0\n˜p(Wℓ = wi | d)( 9 )\nHere, for example, ˜p(wi wj wk | d) denotes the empirical distribution of the tri-grams\nactually seen in the training corpus, and δ(·) is an indicator that returns 1 if the event is\n238 S. W ANG ET AL.\nactive and 0 otherwise. Note theδ functions specify the features that the learned modelp(x)\nshould respect, which above are the tri-gram, bi-gram and uni-gram constraints respectively.\nFor the semantic (PLSA) portion of the model, the constraints involve the hidden topic\nvariables T and can be encoded by the more complex constraints\n∑\nx\np(x)\n2∑\nℓ=0\nδ(Tℓ = t, D = d) = ˜p(d)\n2∑\nℓ=0\n∑\nWℓ\n˜p(Wℓ | d)p(Tℓ = t | Wℓ , D = d)\n∑\nx\np(x)\n2∑\nℓ=0\nδ(Tℓ = t, Wℓ = wi ) =\n∑\nd\n˜p(d)\n2∑\nℓ=0\n˜p(Wℓ = wi | d)\n× p(Tℓ = t | Wℓ = wi , D = d) (10)\nAgain, these δ functions specify the features the learned model should respect. The ﬁrst\nequality imposes the | D |×| T | constraints between the document node and the topic\nnodes, and the second equality imposes the | W |×| T | constraints between the topic\nnodes and words.\nWe can now learn a probability model that simultaneously takes all of these information\nsources into account, by employing the RLME principle to ﬁnd the log-linear model\npλ(x) that maximizes entropy subject to satisfying all of the constraints. This model will\nencapsulate the N-gram and semantic models as special cases. Figure 1 gives a graphical\nrepresentation of the structure resulting from satisfying all of the imposed constraints. Note\nthat many of the components share the same parameters; namely, ( T\n2, D), ( T1, D), and\n(T0, D) are identical; (T2, W2), (T1, W1), and (T0, W0) are identical; (W2, W1) and (W1, W0)\nare identical; and (W2), (W1) and (W0) are identical.\nEfﬁcient feature expectation and inference\nThe computational bottleneck is calculating the feature expectations and normalization\nconstants needed to perform inference. Note that the full joint distribution is in the form\nof a product over exponential functions of features. The key idea for efﬁcient calculation\nis to “push” the sums in as far as possible when summing (marginalizing) out irrelevant\nterms. Since calculating feature expectations has the same computational cost as normal-\nization (Khudanpur & Wu, 2000), we only show how to do normalization efﬁciently here.\nThe normalization factor can be calculated efﬁciently by sum-product algorithm, that is,\nsumming over all the links at each time slice and passing through the trellis nodes with the\nproduct of the weight to the ongoing nodes we obtain\n/Phi1=\n∑\nw2,w1,w0,t2,t1,t0,d\n(eλw2 eλw1 eλw0 eλw2w1 eλw1w0 eλw2w1w0 eλw2t2 eλw1t1 eλw0t0 eλt2d eλt1d eλt0d )\n=\n∑\nw0\neλw0\n∑\nt0\neλw0t0\n∑\nw1\neλw1 eλw1wo\n∑\nt1\neλw1t1\n∑\nw2\neλw2 eλw1w2 eλw2w1w0\n∑\nt2\neλw2t2\n(∑\nd\neλt2d eλt1d eλt0 d\n)\n(11)\nCOMBINING STATISTICAL LANGUAGE MODELS 239\nSimultaneously to obtaining the normalization constant, we can also calculate all of the\nfeature expectations. For example, the expectation of a given tri-gram featurewi wj wk can\nbe calculated as\n∑\nx\np(x)δ(W2 = wi , W1 = wj , W0 = wk )\n= /Phi1−1eλwi eλwj eλwk eλwi wj eλwj wk eλwi wj wk\n×\n∑\nt0\neλwk t0\n∑\nt1\neλwj t1\n∑\nt2\neλwi t2\n×\n(∑\nd\neλt2d eλt1d eλt0 d\n)\n(12)\nSemantic smoothing\nTo make use of semantic similarity and subtle variation between words, we can introduce\nan additional node C between each topic node and word node. The ﬁrst set of feature\nconstraints in (10) can be augmented to incorporate this new cluster variableC as follows:\n∑\nx\np(x)\n2∑\nℓ=0\nδ(Tℓ = t,Cℓ == c, D = d)\n=\n∑\nd\n˜p(d)\n2∑\nℓ=0\n∑\nWℓ\n˜p(Wℓ | d)p(Tℓ = t,Cℓ == c | Wℓ , D = d)\nThe effect of these cluster nodes critically depends on the range of their variation. For\nexample, if all the words are grouped into a single class, then the model will be maximally\nsmoothed. On the other hand, if there are as many classes as words in the vocabulary, there\nwill be no smoothing effect at all. There is a trade-off between smoothing to reduce the\neffective number of parameters in the model, and non-smoothing to permit a more detailed\nmodel.\nAs a further extension which takes account of the semantic similarity and sub-topic\nvariation within each document and among documents, we can introduce additional nodeS\nbetween the topic nodes and the document node. Again, the second set of feature constraints\nas in (10) can be written analogously as follows:\n∑\nx\np(x)\n2∑\nℓ=0\nδ(Tℓ = t, Sℓ = s, Wℓ = wi )\n=\n∑\nd\n˜p(d)\n2∑\nℓ=0\n˜p(Wℓ = wi | d)p(Tℓ = t, Sℓ = s | Wℓ = wi , D = d)\n240 S. W ANG ET AL.\nFigure 2. A graphical representation of the extended PLSA tri-gram model, which includes word cluster\nvariables, C2,C1,C0, and topic cluster variables,S2, S1, S0. The cliques ofW −C −T and T − S − D correspond\nto the features in Section 4.3. In this graphical representation, many arcs share the same parameters.\nAgain the effect of node S critically depends on the range of its variation. If all the\ndocuments are grouped in a single cluster, then the model is the same as (10) and is over-\nsmoothed, and in the context of diverse discourse this could not capture the speciﬁc topics.\nOn the other hand, if there are as many clusters as documents in the corpus, there will be\nno smoothing effect at all. Again, we encounter a trade-off between smoothing to reduce\nparameters, versus non-smoothing to permit variation.\nNote that the beneﬁt of the maximum entropy combination method is that the cluster\nnodes behave like latent variables in a mixture model for “soft clustering”, instead of the\n“hard clusters” created by methods likeK-means used in Bellegarda (2000).\nFigure 2 shows an extended semantic smoothed version of the model that incorporates\nadditional word cluster variables within each topic, and additional topic cluster variables\nwith each document.\nComputation in testing\nTo evaluate the perplexity of our semantic tri-gram model on the observable portion of the\ntest document d, note that\np(w\n1 ...w |d|,d) =\n|d|∏\nℓ=1\npDℓ (wℓ ,dℓ | w1 ...w ℓ−1)\n=\n|d|∏\nℓ=1\n∑\nT2,T1,T0\npDℓ (wℓ ,dℓ , T2, T1, T0 | w1...wℓ−1)\n=\n|d|∏\nℓ=1\n∑\nT2,T1,T0\npDℓ (wℓ ,dℓ , T2, T1, T0 | wℓ−2,wℓ−1)\nwhere | d | denotes the length of the test document d. Since the representation for a docu-\nment of the test data is not contained in the original training corpus, we use similar “fold-in”\nCOMBINING STATISTICAL LANGUAGE MODELS 241\nheuristic approach as used in Hofmann (2001): the Lagrange multipliers corresponding to\nthe document-topic arcs are re-estimated by the same formulation as (10) while holding\nother parameters ﬁxed, where the empirical distribution is given by the current updated\ndocument history. To be more precise, let p\nD0 (x) to be the exponential distribution for\nFigure 1 with the estimated RLME or MAP Lagrange multipliers as weights, except that\nthe multipliers corresponding to the document-topic arcs are ﬁxed to be zero. Then the new\nmodel pDℓ will be the result of the following optimization\nmin\np,a\nKL (p, pDℓ−1 ) + U(a)\nsubject to\n∑\nx\np(x)\n2∑\nℓ=0\nδ(Tℓ = t, D = dℓ ) =\n2∑\nℓ=0\n˜p(dℓ )\n∑\nWℓ\n˜p(Wℓ | dℓ )p(Tℓ = t | Wℓ ,dℓ ) + at,d\nwhere KL (p,q) is the Kullback-Leiber divergence.\nSince our model provides the probability of complete data pDℓ (W2, W1, W0, Dℓ ,\nT2, T1, T0), the conditional probability pDℓ (W0, Dℓ , T2, T1, T0 | W2, W1) can be easily\nobtained by marginalization (and division).\n5. Experimental evaluation\nExperimental data sets and performance measure\nThe corpus used to train our model was taken from the WSJ portion of the NAB corpus and\nwas composed of about 150,000 documents spanning the years 1987 to 1989, comprising\napproximately 42 millions words. The vocabulary was constructed by taking the 20,000\nmost frequent words of the training data. We split another separate set of data consisting of\n325,000 words taken from the year 1989 into half, one half used as development data by\nrandom selection, another half for testing.\nThe statistics of the datasets are shown in Table 1.\nTo evaluate a language model we use the standard deﬁnitions of perplexity and entropy\non held-out test data. That is, given a test corpus s ={ d\n1,..., dL } and a language model\nTable 1. Data sets statistics.\nNo. of articles No. of sentences No. of words\nTrain 150,981 1,611,571 41,780,924\nDev 378 6904 157,312\nTest 379 6638 153,801\n242 S. W ANG ET AL.\ndeﬁning p(s) we calculate test perplexity and test entropy as\nPerplexity =\n|s|\n√\n1\np(s) =\n|s|\n\n√\nL∏\ni=1\n1\np(w1 ...w |di |,di )\n=\n|s|\n\n√\nL∏\ni=1\n|di |∏\nl=1\n1\np(wl ,di | wi ...w l−1)\nEntropy = log2 Perplexity\nwhere | s |= ∑ L\ni=1 | di | is the length of test corpus. The goal is to obtain small values\nof these measures. That is, the goal of language modeling is to predict the probability of\nnatural word sequences; or more simply, to put high probability on word sequences that\nactually occur (and low probability on word sequences that never occur).\nExperimental design\nTo serve as a baseline standard of performance, we use a conventional tri-gram model with\nGood-Turing back-off smoothing. Implementing this approach with the CMU language\nmodeling toolkit (Clarkson & Rosenfeld, 1997), we obtained a test perplexity score of 103.\nWe then implemented and compared a few simple versions of our RLME approach using\nthe models described above. Note that in all of the experiments below, to implement the\ninner R-EM-IS procedure we set the number of EM iterations to 5 and the number of\ninternal IIS loop iterations to 20. We also had to choose a variance for the Gaussian prior\nto serve as a regularizer, but simply chose the default value of σ\ni = 1 for every feature\ni = 1, ...,N in each case, and therefore performed no optimization of the regularization\nparameters.\nTo control for the effects of maximizing regularized entropy (RLME) verus maximizing a\nposteriori probability (MAP), we ﬁrst omitted the outer R-ME-EM-IS procedure and instead\njust initialized the parameters to zero and executed a single run of R-EM-IS, then perturb\nthe parameters randomly and run a single R-EM-IS to ﬁnd a single locally MAP model (or\nequivalently a single feasible model for the RLME principle). Then, using these results as a\ncontrol, we re-ran the procedures with the outer R-ME-EM-IS procedure re-introduced, to\nﬁnd higher regularized entropy (RLME) solutions and higher penalized likelihood (MAP)\nsolutions. Speciﬁcally, we used 20 random starting points for λ, ran R-EM-IS from each,\nand then selected the highest regularized entropy solution as the RLME estimate, and the\nhighest penalized maximum likelihood solution as the MAP estimate. Among all feasible\nsolutions, the one which has the lowest perplexity score on the development corpus is\ndenoted as best feasible solution.\nWe ﬁrst considered a simple model which only considered the features (and the con-\nstraints) from the simple tri-gram model. In this simple case, there are no hidden variables,\nand our estimation principles (MAP, RLME, a single run of R-EM-IS) all reduce to the same\nstandard (regularized) ME principle for completely observed data. Here we observed a per-\nplexity score of 112 on development data, which is slightly worse than the 109 perplexity\nCOMBINING STATISTICAL LANGUAGE MODELS 243\n60 80 100 120 140 160 180 200\n90\n95\n100\n105\nbest feasible solution\nRLME\nMAP\nNumber of topic clusters\nPerplexity\nFigure 3. Perplexity versus number of topics for the semantic tri-gram language model.\nobtained by training a tri-gram model with Good-Turing smoothing. Nevertheless, this\nresult shows that the parameter smoothing effect achieved by our (untuned) regularization\nprinciple is nearly as effective as using Good-Turing smoothing for N-gram models.1\nNext, we introduced the PLSA features to our model, setting the number of possible\nhidden topics to be| T |= 125. First, running R-EM-IS from a single starting point resulted\nin a model that achieved perplexity score 95 on the development corpus; comprising a 12.8%\nreduction in perplexity from the baseline tri-gram model. Then, using 20 runs from different\nstarting points to yield the RLME, MAP and best feasible solutions estimates, we obtained\nmodels with perplexities 92, 95 and 91 respectively; comprising respective 15.6, 10.6 and\n16.5% reductions in perplexity from the baseline tri-gram model. Figure 3 shows how the\nperplexities of these estimated models change as the number of topics is increased and\ndecreased, with the best perplexities achieved in each case when the number equals 125.\nTherefore, in the remaining experiments we ﬁx the number of hidden topics to be 125.\nFinally, we considered a few augmentations of the basic combined tri-gram, PLSA model\nillustrated in ﬁgure 1, which is shown in ﬁgure 2.\nFirst, when we add just the word cluster nodes to our model, we ﬁnd that the result is\nsensitive to the number of classes. For the single feasible model which is given by perturbing\nthe parameters of a single run of R-EM-IS from uniform initializing, then running a single\nR-EM-IS, we ﬁnd that when the class number is chosen to be 10, the perplexity achieved\nis now 93, which equals to the best result achieved by any technique above. However, if\nthe class number is set to 50, then the perplexity increases to 95, probably due to the huge\nincrease in parameters. For the MAP and RLME estimators we ﬁnd that we obtain similar\nresults. Figure 4 shows how the perplexity changes as the number of word clusters changes\nfor each of the best feasible solution, MAP and RLME estimates. The best perplexity\nachieved is 87 (by RLME) for a number of word clusters set to 30.\n244 S. W ANG ET AL.\n0 10 20 30 40 50 60\n85\n90\n95\n100\n105\n110\nbest feasible solution\nRLME\nMAP\nNumber of word clusters\nPerplexity\nFigure 4. Perplexity versus number of word clusters for the extended PLSA tri-gram language model.\nNext, when we add just the topic cluster nodes to the model, but omit the word cluster\nnodes, we also ﬁnd that the result depends on the number of topic clusters. For the single\nfeasible model which is given by perturbing the parameters of a single run of R-EM-IS\nfrom uniform initializing, then running a single R-EM-IS, we ﬁnd that when the cluster\nnumber is chosen to be 10, the perplexity achieved is 92, which is the same as the previous\nperplexity of 92 (with no cluster variables). However, if the cluster number is set to 30, the\nperplexity increases to 93. For the MAP and RLME estimators, again, we obtain similar\nresults. Figure 5 shows how the perplexity changes as the number of topic clusters changes\nfor each of the best feasible solution, MAP and RLME estimates. The best perplexity\nachieved is 90 (by RLME) when the number of topic clusters is set to 20.\nFinally, when we addboth the word cluster nodes and topic cluster nodes simultaneously\nto our model, we ﬁnd that the result is again sensitive to the number of classes, but notably\nimproved. When the number of word classes is 10 and the number of topic clusters is\n10, the perplexity achieved by the single run feasible model is now 91, which is the best\nperformance it achieves, and comprises about an 16.5% improvement over the baseline tri-\ngram model. Similarly, the MAP and RLME estimators show improved results. Figures 6\nand 7 shows how the perplexity changes as the number of topic clusters changes for each\nof the single feasible, MAP and RLME estimates. The best perplexity achieved is now 85\nby RLME when the number of word and topic clusters is set to 10 and 20 respectively.\nThis result is a substantial improvement over the baseline tri-gram model, comprising a\n22% reduction in perplexity. The best perplexity achieved is now 92 by MAP when both\nnumbers of word and topic clusters is set to 0. This result is also a substantial improvement\nover the baseline tri-gram model, comprising a 15.6% reduction in perplexity. The best\nperplexity achieved is now 84 by best feasible solution when the number of word and topic\nCOMBINING STATISTICAL LANGUAGE MODELS 245\nTable 2. Perplexity and reduction results on development corpus for the semantic tri-gram RLME, MAP and\nbest feasible solution models under various smoothing schemes.\nLanguage model RLME MAP Best feasible\nBaseline 108 (0.0%)\nSemantic tri-gram 92 (14.8%) 98 (9.2%) 91 (15.7%)\nSemantic tri-gram + Word\nsmoothing\n87 (19.4%) 91 (15.7%) 86 (20.3%)\nSemantic tri-gram + Topic\nsmoothing\n90 (16.7%) 95 (12.0%) 89 (17.5%)\nSemantic tri-gram + Word &\nTopic smoothing\n85 (21.3%) 94 (13.0%) 84 (22.2%)\n0 10 20 30 40 50 60\n85\n90\n95\n100\n105\n110\n115\n120\nbest feasible solution\nRLME\nMAP\nNumber of document clusters\nPerplexity\nFigure 5. Perplexity versus number of topic clusters for the extended PLSA tri-gram language model.\nclusters is set to 10 and 20 respectively. Table 2 summarizes the best results obtained in\neach case by RLME, MAP and best feasible solution models.\nOnce we use the development corpus to tune the optimal numbers of topic, word clusters,\nand topic clusters for the best feasible solution, MAP and RLME estimates, we calculate\nthe perplexity on test corpus. The perplexity score by the baseline tri-gram model is 103.\nWe obtain 84, 90 and 82 perplexity scores respectively by the tuned RLME, MAP and\nbest feasible solution. The corresponding perplexity reductions are 18.5, 12.6; and 20.4%.\nTable 3 summarizes the results on test corpus by these models.\n246 S. W ANG ET AL.\nTable 3. Perplexity results on test corpus by best RLME, MAP and best feasible solution models.\nLanguage model Test perplexity Reduction (%)\nBaseline 103\nRLME 84 18.5\nMAP 90 12.6\nBest feasible 82 20.4\n0 5 10 15 20 25 30 35 40 45 50\n80\n85\n90\n95\n100\n105\n110\nbest feasible solution\nRLME\nMAP\nNumber of document clusters\nPerplexity\nFigure 6. Perplexity versus number of word clusters for the extended PLSA tri-gram model when ﬁxing the\nnumber of topic clusters to 10.\n6. Discussion\nThe main observation to draw from this investigation is that extending language models to\nincorporate extra hidden features is conceptually very easy in the LME framework. All one\nhas to do is identify the features they wish to model and add the respective constraints to\nthe feasibility problem solved by R-EM-IS. Provided the E and M steps remain feasible,\nthis presents no practical barrier to extending the model.\nClearly, however, standard concerns such as over-ﬁtting avoidance remain inescapable\nissues when estimating model parameters from training data. Nevertheless, the above\nFigures 4– 7 seem to clearly indicate that RLME estimation consistently gives better results\nthan MAP estimation for this type of large scale parameter estimation from sparse data.\nRLME appears to cope more robustly when simultaneously estimating a large number of\nparameters in the presence of hidden variables and sparse training data.\nCOMBINING STATISTICAL LANGUAGE MODELS 247\n0 5 10 15 20 25 30 35 40 45 50\n85\n90\n95\n100\n105\n110\n115\nbest feasible solution\nRLME\nMAP\nNumber of word clusters\nPerplexity\nFigure 7. Perplexity versus number of word clusters for the extended PLSA tri-gram model when ﬁxing the\nnumber of topic clusters to 20.\n7. Extensions\nWe should note that there remain several avenues to improving the quality of the language\nmodels we are able to estimate from data. In language modeling research, there exists a\nbody of ad hoc tricks which are known to make signiﬁcant improvements. For example,\n(Bellegarda, 2000) proposes an ad hoc combination of tri-gram and latent semantic analysis\n(LSA) models, by using the perplexity formula\np(w\nℓ | w1 ...w ℓ−1)\n= p(wℓ | wℓ−2wℓ−1)pLSA (dℓ | wℓ )∑\nwi p(wi | wℓ−2wℓ−1)pLSA (dℓ | wi ) (13)\n=\np(wℓ | wℓ−2wℓ−1) pLSA (wℓ |dℓ )\npLSA (wℓ )\n∑\nwi p(wi | wℓ−2wℓ−1) pLSA (wi |dℓ )\npLSA (wi )\n(14)\nHere dℓ = (w1 ...w ℓ−1) is the current documentd’s history;pLSA(dℓ | wℓ ) is the probability\nof the current document history given the current wordwℓ ; pLSA(wℓ | dℓ ) is the probability\nof the current wordwℓ history given the current documentdℓ ; and pLSA(wℓ ) is the probability\nof current word wℓ .2 Each of these components is obtained by the latent semantic analysis.\nWe calculated the perplexity of Bellagarda’s model using the same training data and\ndevelop data considered above. The perplexity obtained by Bellegarda’s model in this\ncase is 99, which is an 9% reduction in perplexity compared to the baseline tri-gram\nmodel.\n248 S. W ANG ET AL.\nHowever, if one incorporates another ad hoc improvement from speech recognition\nresearch, a substantial futher reduction can be obtained. The idea is to strengthen the\ninﬂuence of the LSA portion of the model by raising its contribution to some power (here\n7) and renormalizing\np(w\nℓ | w1 ...w ℓ−1)\n= p(wℓ | wℓ−2wℓ−1)(pLSA (dℓ | wℓ ))7\n∑\nwi p(wi | wℓ−2wℓ−1)(pLSA (dℓ | wi ))7 (15)\nDoing so for Bellagarda’s model results in a suprising and dramatic perplexity value of\n86; almost equal to the best of our results using RLME above. Although we have yet\nto investigate such techniques in our RLME framework, given the potentially dramatic\nimprovements they potentially offer, it remains important future research to understand\nhow such improvements might be incorporated in a principled manner.\n8. Conclusion\nWe have applied our proposed RLME principle for estimating sophisticated mixed chain-\ntable graphical models of natural language, where local word interactions and global\nsemantic document information can be modeled by mixtures of exponential families in a\nuniﬁed framework. The parameters are estimated in the sense of maximum entropy over\na set of feasible solutions produced by an EM algorithm with nested iterative scaling.\nWhen modeling speciﬁc linguistic phenomena, the general framework reproduces standard\nmodels. However, our proposed approach allows one to model interactions among multiple\naspects of natural language simultaneously and automatically. This yields dramatic im-\nprovements over standard maximum a posteriori estimation in our experiments on natural\nlanguage modeling.\nAlthough we have shown the potential beneﬁts of RLME over standard MAP estimation\nfor large scale parameter estimation problems, the R-ME-EM-IS algorithm we use to ﬁnd\nsolutions is not a very sophisticated optimization algorithm, and is probably needlessly\ntime consuming as a result. It would be worthwhile to investigate more direct optimization\nmethods which simultaneously seek to achieve feasibility while maximizing the objective.\nThe experimental results also show that performance is sensitive to the number of word\nclusters and topic clusters. We are currently investigating techniques to automatically\ndetermine optimal word and topic cluster numbers.\nIn principle, our RLME approach provides a general statistical framework for incorpo-\nrating arbitrary aspects of natural language into a parametric model. Therefore, we could\nhope to incorporate syntactic structure encoded by probabilistic context free grammars\ninto the RLME framework. However, in practice, this raises some difﬁcult challenges. In\nparticular, these models appear to make the left hand side of the constraints (2) infeasible\nto calculate. It also appears to be difﬁcult to apply standard approximation techniques, such\nas loopy belief propagation and variational approximation (Wainwright & Jordan, 2003)\nto this problem. One possibility is to resort to computationally expensive Monte Carlo\nmethods, such as those employed by Abney (1997).\nCOMBINING STATISTICAL LANGUAGE MODELS 249\nAcknowledgments\nThis work is supported by AICML, MITACS, NSERC, and the Canada Research Chairs\nprogram. Fuchun Peng is also supported in part by the Center for Intelligent Information\nRetrieval. Any opinions, ﬁndings and conclusions or recommendations expressed in this\nmaterial are the author(s) and do not necessarily reﬂect those of the sponsors. Finally we\nthank the anonymous reviewers for a much improved version.\nNotes\n1. It turns out that regularization has a signiﬁcant effect in this case, because removing it leads to inferior results\n(perplexity scores over 150 (Chen & Rosenfeld, 2000). Therefore, we stick to our standard regularization\nparameters in all further experiments.\n2. p\nLSA (wℓ ) is not the standard unigram probability as obtained using the whole training corpus (as claimed in\nBellegarda (2000), one line below equation 27, page 1289). Intuitively, it is the weighted unigram averaged\nover possible semantic information.\nReferences\nAbney, S. (1997). Stochastic attribute-value grammars.Computational Linguistics, 23:4, 597–618.\nBellegarda, J. (2000). Exploiting latent semantic information in statistical language modeling. Proceedings of\nIEEE, 88:8, 1279–1296.\nBengio, Y ., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. Journal of\nMachine Learning Research, 3, 1137–1155.\nBerger, A., Della Pietra, S., & Della Pietra, V . (1996). A maximum entropy approach to natural language processing.\nComputational Linguistics, 22:1, 39–71.\nBorwein, J., & Lewis, A. (2000). Convex analysis and nonlinear optimization: Theory and examples.Springer.\nBrown, P., Della Pietra, S., Della Pietra, V ., Mercer, R., Nadas, A., & Roukos, S. (1992). A maximum entropy\nconstruction of conditional log-linear language and translation models using learned features and a generalized\nCsiszar algorithm. IBM Report.\nChelba, C., & Jelinek, F. (2000). Structured language modeling.Computer Speech and Language, 14:4, 283–332.\nChen, S., & Goodman, J. (1999). An empirical study of smoothing techniques for language modeling. Computer\nSpeech and Language, 13:4, 319–358.\nChen, S., & Rosenfeld, R. (2000). A survey of smoothing techniques for ME models.IEEE Trans. on Speech and\nAudio Processing, 8:1, 37–244.\nClarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using the CMU-Cambridge toolkit.Proceed-\nings of Eurospeech, 2707–2710.\nCsiszar, I. (1996). Maxent, mathematics, and information theory. In K. Hanson and R. Silver (Eds.), Maximum\nEntropy and Bayesian Methods(pp. 35–50). Kluwer Academic Publishers\nDarroch, J., & Ratchliff, D. (1972). Generalized iterative scaling for log-linear models.The Annals of Mathematical\nStatistics, 43:5, 1470–1480.\nDella Pietra, S., Della Pietra, V ., & Lafferty, J. (1997). Inducing features of random ﬁelds.IEEE Transactions on\nPattern Analysis and Machine Intelligence, 19:4, 380–393.\nDempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood estimation from incomplete data via the EM\nalgorithm. Journal of Royal Statistical Society, Series B, 39, 1–38.\nDurbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998). Biological sequence analysis: Probabilistic models of\nproteins and nucleic acids.Cambridge University Press.\nHofmann, T. (2001). Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42:1,\n177–196.\nJaynes, E. (1983). Papers on probability, statistics, and statistical physics. In R. Rosenkrantz & D. Reidel,\nPublishing Company.\n250 S. W ANG ET AL.\nJelinek, F., & Mercer, R. (1980). Interpolated estimation of Markov source parameters from sparse data. In E.\nGelsema, & L. Kanal, (Eds.), Pattern Recognition in Practice. (pp. 381–397) North Holland.\nJelinek, F. (1998). Statistical methods for speech recognition.MIT Press.\nKhudanpur, S., & Wu, J. (2000). Maximum entropy techniques for exploiting syntactic, semantic and collocational\ndependencies in language modeling. Computer Speech and Language, 14:4, 355–372.\nLafferty, J., & Zhai, C. (2001). Document language models, query models, and risk minimization for information\nretrieval. In ACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR).\nLauritzen, S. (1995). The EM-algorithm for graphical association models with missing data. Computational\nStatistics and Data Analysis, 1, 191–201.\nPeng, F., Schuurmans, D., & Wang, S. (2004). Augumenting naive Bayes text classiﬁer using statistical language\nmodels. Information Retrieval, 7:3–4, 317–345.\nRiezler, S. (1999). Probabilistic constraint logic programming. Ph.D. Dissertation, University of Stuttgart,\nGermany.\nRoark, B. (2001). Probabilistic top-down parsing and language modeling.Computational Linguistics, 27:2, 249–\n285.\nRosenfeld, R. (1996). A maximum entropy approach to adaptive statistical language modeling.Computer Speech\nand Language, 10, 187–228.\nRosenfeld, R. (2000). Two decades of statistical language modeling: Where do we go from here?.Proceedings of\nthe IEEE, 88:8, 1270–1278.\nShannon, C. (1948). A mathematical theory of communication. Bell System Technical Journal, 27, 379–423.\nWainwright, M., & Jordan, M. (2003). Graphical models, exponential families, and variational inference. Technical\nReport 649, Department of Statistics, University of California, Berkeley.\nWang, S., Schuurmans, D., & Zhao, Y . (2003). The latent maximum entropy principle. Manuscript submitted.\nWang, S., Schuurmans, D., Peng, F., & Zhao, Y . (2004). Learning mixture models with the regularized latent\nmaximum entropy principle. IEEE Transactions on Neural Networks: Special Issue on Information Theoretic\nLearning, 154.\nReceived October 8, 2003\nRevised May 19, 2004\nAccepted May 24, 2004"
}