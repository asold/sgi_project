{
  "title": "RoBERTuito: a pre-trained language model for social media text in Spanish",
  "url": "https://openalex.org/W3211939470",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227620369",
      "name": "Pérez, Juan Manuel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227620370",
      "name": "Furman, Damián A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227620371",
      "name": "Alemany, Laura Alonso",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227620372",
      "name": "Luque, Franco",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3097370230",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W2041282815",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2954226438",
    "https://openalex.org/W2146867136",
    "https://openalex.org/W3180918883",
    "https://openalex.org/W2061554433",
    "https://openalex.org/W2904605268",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W2750787240",
    "https://openalex.org/W3032736433",
    "https://openalex.org/W2970377225",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2923014074"
  ],
  "abstract": "Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for Natural Language Understanding tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks. However, for languages other than English such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model achieves top results for some English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and has also competitive performance against monolingual models in English tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.",
  "full_text": "RoBERTuito: a pre-trained language model for social media text in\nSpanish\nJuan Manuel P´erez12, Dami´an A. Furman12, Laura Alonso Alemany3, Franco Luque23\n1Universidad de Buenos Aires, 2CONICET, 3Universidad Nacional de C´ordoba\n{jmperez, dfurman}@dc.uba.ar, {francolq, alemany}@famaf.unc.edu.ar\nAbstract\nSince BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing\ntasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientiﬁc papers,\nmedical documents, user-generated texts, among others. These domain-speciﬁc models have been shown to improve performance\nsigniﬁcantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present\nRoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on\na benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in\nSpanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic\nCode-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks.\nTo facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to\npre-train it.\nKeywords: Pre-trained Language Models, BERT, Spanish\n1. Introduction\nPre-trained language models have become a basic\nbuilding block in the area of natural language process-\ning. In the last years, since the introduction of the trans-\nformer architecture (Vaswani et al., 2017), they have\nbeen used in many other different natural language\nunderstanding tasks, outperforming previous models\nbased on recurrent neural networks. BERT, GPT-2, and\nRoBERTa are some of the most well-known such tools.\nMost models are trained on large-scale corpora taken\nfrom news or Wikipedia, which are considered general\nenough to comprise a large part of the language. Some\ndomains, however, are very speciﬁc and have their own\nvocabulary, jargon, or complex expressions. The med-\nical or scientiﬁc domains, for example, use terms and\nconcepts which are not found in a general corpus or oc-\ncur just a few times. In some other cases, words have\nspeciﬁc meanings within a particular domain. Collo-\nquial language –as found in Twitter and other social\nnetworks– is more informal, with slang and other ex-\npressions which rarely occur in Wikipedia.\nFor these reasons, a number of pre-trained models\nhave been created to handle these domains. SciBERT\n(Beltagy et al., 2019) and MedBERT (Rasmy et al.,\n2021) are examples of domain-speciﬁc models. For\nuser-generated text, many models have been trained\non Twitter for different languages. However, Span-\nish lacks pre-trained models for user-generated text, or\nthey are not easily available in the most popular model\nrepositories, such as the HuggingFace model hub. This\nhinders the development of accurate applications for\nuser-generated text in Spanish.\nIn this paper, we present RoBERTuito, a large-scale\ntransformer model for user-generated text trained on\nSpanish tweets. We show that RoBERTuito outper-\nforms other Spanish pre-trained models for a number\nof classiﬁcation tasks on Twitter. In addition to this,\nand due to the collection process of our pre-training\ndata, RoBERTuito is a very competitive model in mul-\ntilingual and code-switching settings including Spanish\nand English. Our contributions are the following:\n• We publish the data used to train RoBERTuito\n(around 500M tweets in Spanish), to facilitate\nthe development of other language models or em-\nbeddings, also using subsets of the corpus that\nmodel speciﬁc subdomains, like regional or the-\nmatic variants.\n• We make the weights of our model available\nthrough the HuggingFace model hub, thus spar-\ning computation for researchers with no access to\nthe computational power or simply sparing extra\ncomputation, while making the model transparent\n(albeit not interpretable).\n• We set up a benchmark for classiﬁcation tasks in-\nvolving user-generated text in Spanish.\n• We assess the performance of domain-speciﬁc\nmodels with respect to general-language models,\nshowing that the ﬁrst outperform the latter in the\ncorresponding domain-speciﬁc tasks.\n• We assess the impact of preprocessing strategies\nfor our models: cased input vs. uncased input text\nvs. uncased input text without accents, showing\nthat the uncased version of the corpus yields better\nperformance.\narXiv:2111.09453v3  [cs.CL]  4 May 2022\n• We also evaluate our model in a code-switching\nbenchmark for Spanish-English and in a small\nnumber of English tasks, both for user-generated\ntext, showing that it achieves competitive results.\n2. Previous Work\nLanguage models based on transformers (Vaswani et\nal., 2017) have become a key component in state-\nof-the-art NLP tasks, from text classiﬁcation to nat-\nural language generation. One of the most popular\ntransformer-based tools, BERT (Devlin et al., 2019)\nis a neural bidirectional language model trained on the\nMasked-Language-Model (MLM) task and in the Next-\nSentence-Prediction (NSP) task. This language model\ncan then be ﬁne-tuned for a downstream task or can\nbe used to compute contextualized word representa-\ntions. RoBERTa (Liu et al., 2019) is an optimized pre-\ntraining approach that differs from BERT in four as-\npects: it trains the model with more data; it removes\nthe NSP objective; it uses longer batches; and it dy-\nnamically changes the masking pattern applied to the\ndata. These models, along with GPT (Radford et al.,\n2018), supposed breakthroughs in the performance on\nbenchmarks such as GLUE (Wang et al., 2018). Nozza\net al. (2020) provides a good overview of the BERT-\nbased language models.\nAfter the explosion of language models based on trans-\nformers, some models have been trained on corpora\nthat target more speciﬁcally a domain of interest in-\nstead of generic texts such as Wikipedia or news.\nFor example, SciBERT (Beltagy et al., 2019) is a\nBERT model trained on scientiﬁc texts, and MediB-\nERT (Rasmy et al., 2021) was crafted on medical doc-\numents. AlBERTo (Polignano et al., 2019) is one of the\nﬁrst models trained on tweets –particularly, in Italian.\nBERTweet (Nguyen et al., 2020) is a RoBERTa model\ntrained on about 850M tweets in English, a part of them\nrelated to the COVID-19 pandemic.\nMultilingual models have also been successful at many\ntasks comprising more than one language. Multilin-\ngual BERT (mBERT) (Devlin et al., 2019) was pre-\ntrained on the concatenation of the top-104 languages\nfrom Wikipedia. In a parallel fashion, XLM-R (Con-\nneau et al., 2020) was trained using RoBERTa guide-\nlines on the concatenation of Common Crawl data con-\ntaining more than 100 languages, obtaining a consider-\nable performance boost over several multilingual tasks\nwhile keeping competitive with monolingual models.\nBETO (Canete et al., 2020) was the ﬁrst publicly avail-\nable pre-trained model in Spanish, following mainly\na BERT style of training (MLM + NSP) with some\nideas taken from Liu et al. (2019). More recently,\nsome other pre-trained models have been developed\nfor this language, such as RoBERTa-BNE (Guti ´errez-\nFandi˜no et al., 2021), a RoBERTa-based model trained\non a database of 500GB of all the .es domain websites.\nBERTin (Rosa et al., 2022) is also a RoBERTA-based\nmodel, for whose development the authors explored\nsampling strategies over the Spanish portion of themc4\ncorpus (Raffel et al., 2020).\nTo the best of our knowledge, TwilBERT (Gonzalez et\nal., 2021) is the only specialized pre-trained model on\nTwitter data for the Spanish language. However, this\nmodel has some limitations: ﬁrst, the training data is\nnot available, making it not auditable. Second, it is not\nclear how long its pre-training was. Third, the authors\nused a variant of the NSP task adapted to Twitter (Reply\nOrder Prediction), in spite of many works showing that\nthe type of training based on RoBERTa (MLM only)\nimproves performance on downstream tasks. Finally,\nthe model is not easily available (for instance, in the\nHuggingFace model hub 1), which makes its use difﬁ-\ncult.\n3. Data\nIn this section, we describe the tweet collection process\nused to build the corpus to train RoBERTuito. Twit-\nter’s free access streaming API (also known asSpritzer)\nprovides a sample of around 1% of the overall pub-\nlished tweets, supposedly random, although some stud-\nies have shown some concerns about the possible ma-\nnipulation of this sample (Pfeffer et al., 2018). Un-\nrepresentative, biased samples may produce biased be-\nhaviours in the resulting model and systematic, possi-\nbly harmful errors in downstream tasks that use this\nmodel. That is why we make available the training\ndataset and the speciﬁcs of the model, so that it can\nbe fully inspected in case biases are suspected. In fol-\nlowing releases of this tool, an extensive audit of the\ntraining corpus will be carried out.\nFirst, we downloaded a Spritzer collection uploaded\nto Archive.org dating from May 2019 2. From this,\nwe only keep tweets with language metadata equal to\nSpanish, and mark the users who posted these mes-\nsages. Then, the tweetline from each of these marked\nusers was downloaded. We decided to download data\nfrom users already represented in the initial collection\nto facilitate user-based studies in this dataset, and also\nbecause we believe the original sample of users to be\nrepresentative, and thus we hope to maintain this rep-\nresentativeness by sampling from the same users. In to-\ntal, the collection consists of around 622M tweets from\nabout 432K users.\nFinally, we ﬁltered tweets with less than six tokens,\nbecause language contained in those is very different\nfrom the language in longer tweets. To identify to-\nkens we used the tokenizer trained in BETO (Canete\net al., 2020), without counting character repetitions\nand emojis. This leaves a training corpus of around\n500M tweets, which we split in many ﬁles to facili-\ntate reading in later processes. The code for the col-\nlection process can be found at https://github.\ncom/finiteautomata/spritzer-tweets.\n1https://huggingface.co/models\n2https://archive.org/details/\narchiveteam-twitter-stream-2019-05\nSomething to remark is that this collection process al-\nlows the data to contain code-mixed text or even tweets\nfrom other languages, as we only required the post on\nthe original sample to be in Spanish. While other works\nsuch as Nguyen et al. (2020) required every tweet to\nbe in English, we let other languages to be included in\nthe pre-training data. A rough estimate of the language\npopulation using fasttext’s language-detection module\n(Joulin et al., 2016) suggests that 92% of the data is in\nSpanish, 4% in English, 3% in Portuguese, and the rest\nin other languages.\n4. RoBERTuito\nIn this section, we describe the training process of\nRoBERTuito. Three versions of RoBERTuito were\ntrained: a cased version which preserves the case found\nin the original tweets, an uncased version, and a deacc\nversion, which lower-cases and removes accents on\ntweets. Normative Spanish prescribes marks for (some)\naccents in words, but their usage is inconsistent in user-\ngenerated text, so we want to test if removing them im-\nproves the performance on downstream tasks.\nFor each of the three conﬁgurations, we trained to-\nkenizers using SentencePiece (Kudo and Richardson,\n2018) on the collected tweets, limiting vocabularies to\n30,000 tokens. We used the tokenizers library (Moi et\nal., 2019) which provides fast implementations in Rust\nfor many tokenization algorithms.\n4.1. Preprocessing\nPreprocessing is key for models consuming Twitter\ndata, which is quite noisy, have user handles (@user-\nname), hashtags, emojis, misspellings, and other non-\ncanonical text. Nguyen et al. (2020) tried two nor-\nmalization strategies: a soft one, in which only minor\nchanges are performed to the tweet such as replacing\nusernames and hashtags, and a more aggressive one us-\ning the ideas of Han and Baldwin (2011). The authors\nfound no signiﬁcant improvement by using the harder\nnormalization strategy. Having this in mind, we fol-\nlowed an approach similar to the one used both in this\nwork and in Polignano et al. (2019):\n• Character repetitions are limited to a max of three\n• User handles are converted to a special token\n@usuario\n• Hashtags are replaced by a special token\nhashtag followed by the hashtag text and split\ninto words if this is possible\n• Emojis are replaced by their text representation\nusing emoji library3, surrounded by a special to-\nken emoji.\n3https://github.com/carpedm20/emoji/\n4.2. Architecture and training\nA RoBERTa base architecture was used in RoBERTu-\nito, with 12 self-attention layers, 12 attention heads,\nand hidden size equal to 768, in the same fashion as\nBERTweet (Nguyen et al., 2020). We used a masked-\nlanguage objective, disregarding the next-sentence pre-\ndiction task used in BERT or other tweet-order tasks\nsuch as those used in Gonzalez et al. (2021).\nTaking into account successful hyperparameters from\nRoBERTa and BERTweet, we decided to use a large\nbatch size for our training. While an 8K batch size\nis recommended in RoBERTa, due to resource limita-\ntions, we decided to balance the number of updates us-\ning a 4K size. To check convergence, we ﬁrst trained\nan uncased model for 200K steps. After this, we then\nproceeded to run it for 600K steps for the three mod-\nels. This is roughly half the number of updates used\nto train BETO (and also BERTweet) but this difference\nis compensated by the larger batch size used to train\nRoBERTuito.\nWe trained our models for about three weeks on a v3-\n8 TPU and a preemptible e2-standard-16 machine on\nGCP. Our codebase uses HuggingFace’s transformers\nlibrary and their RoBERTa implementation (Wolf et al.,\n2020). Each sentence is tokenized and masked dynam-\nically with a probability equal to 0.15. Further details\non hyperparameters and training can be found in the\nAppendix 11.\n5. Evaluation\nWe evaluated RoBERTuito in two monolingual settings\n(Spanish and English) and also in a code-mixed bench-\nmark for tweets containing both Spanish and English.\nAs the collection process allowed non-Spanish tweets\nto be included, we assess not only the performance of\nour model in Spanish, but also in other environments.\nTable 1 summarizes the evaluation tasks.\n5.1. Spanish evaluation\nFor the Spanish evaluation, we set a benchmark for this\nmodel following TwilBERT (Gonzalez et al., 2021),\nAlBERTo (Polignano et al., 2019) and BERTweet\n(Nguyen et al., 2020). Four classiﬁcation tasks for\nTwitter data in Spanish were selected, three of them\ncoming from the Iberian Languages Evaluation Forum\n(IberLEF) and one from SemEval 2019: sentiment\nanalysis, emotion analysis, irony detection and hate\nspeech detection. These tasks are particularly relevant\nin the context of social media analysis and provide a\nbroad perspective about how our model is able to cap-\nture useful information on this speciﬁc domain.\nRegarding sentiment analysis, we relied on the TASS\n2020 (Garc´ıa-Vega et al., 2020) dataset. This dataset\nuses a three-class polarity model (negative, neutral,\npositive) and is separated according to different geo-\ngraphic variants of Spanish. For the purpose of bench-\nmarking our model, we disregarded these distinctions\nLanguage Tasks Type of task Dataset Num. posts\nSpanish\nSentiment analysis\nText Classiﬁcation\nTASS 2020 Task A 14, 500\nEmotion analysis TASS 2020 Task B 8, 400\nHate speech detection HatEval 6, 600\nIrony detection IrosV A 2019 9, 000\nEnglish\nSentiment analysis\nText Classiﬁcation\nSemEval 2017 Task 4 61, 900\nEmotion analysis TASS 2020 Task B 7, 303\nHate speech detection HatEval 13, 000\nSpanish-English\nSentiment analysis Text Classiﬁcation\nLinCE\n18, 789\nPOS tagging Text Labelling 42, 911\nNER Text Labelling 67, 233\nTable 1: Evaluation tasks for RoBERTuito. Tasks are grouped by setting: Spanish-only tasks, English-only tasks,\nand code-mixed Spanish-English tasks. Num. posts is the number of instances contained in the dataset of each\ntask.\nand merged all the data into a single dataset, with re-\nspective train, dev, and test splits.\nFor emotion analysis, we also used the dataset from\nthe TASS 2020 workshop, EmoEvent (Plaza del Arco\net al., 2020). This is a multilingual emotion dataset la-\nbeled with the six Ekman’s basic emotions (anger, dis-\ngust, fear, joy, sadness, surprise) (Ekman, 1992) plus\na neutral emotion. It was built retrieving tweets asso-\nciated with eight different global events from different\ndomains (political, entertainment, catastrophes or inci-\ndents, global commemorations, etc.), so emotions are\nalways related to a particular phenomenon. We only\nkept the Spanish portion of it, containing 8,409 tweets.\nHate speech detection in social media has gained much\ninterest in the past years, based on the need to act\nagainst the spread of hate messages that develops in\nparallel with an increasing amount of user-generated\ncontent. It is a difﬁcult task that requires a deep, con-\ntextualized understanding of the semantic meaning of\na tweet as a whole. For this reason, we selected the\nhatEval Task A dataset (Basile et al., 2019), which is\na binary classiﬁcation task for misogyny and racism,\nto benchmark our model. The authors collected the\ndataset by three combined strategies: monitoring po-\ntential victims of hate accounts, downloading the his-\ntory of identiﬁed producers of hateful content, and ﬁl-\ntering Twitter streams with keywords. This dataset dis-\ntinguishes between hate speech targeted to individuals\nand generic hate speech, and between aggressive and\nnon-aggressive messages. For this work, we do not\nconsider these classiﬁcations, and we are interested in\npredicting only the binary label of whether the tweet is\nhateful or not. The Spanish subset of this dataset com-\nprises 6,600 instances.\nIrony detection has also recently gained popularity.\nMany works show that it has important implications\nin other natural language processing tasks that require\nsemantic processing. Gupta and Yang (2017) showed\nthat using features derived from sarcasm detection im-\nproves the performance on sentiment analysis. In addi-\ntion to this, user-generated content is a rich and vast\nsource of irony, so being able to detect it is of par-\nticular importance for the domain of social networks.\nIroSVa (Ortega-Bueno et al., 2019) is a recent dataset\npublished in 2019, that has the particularity of consid-\nering the messages not as isolated texts but with a given\ncontext (a headline or a topic). It consists of 7,200 train\nand 1,800 test examples divided into three geographic\nvariants from Cuba, Spain, and Mexico, each with a bi-\nnary label indicating if the comment contains irony or\nnot. Unlike the previous three tasks mentioned here,\nthis dataset contains not only messages from Twitter\nbut also news comments and debate forums as 4fo-\nrums.com and Reddit.\nWe compare RoBERTuito performance for these tasks\nwith other Spanish pre-trained models: BETO (Canete\net al., 2020), RoBERTa-BNE (Guti´errez-Fandi˜no et al.,\n2021) and BERTin (Rosa et al., 2022). All these mod-\nels share a base architecture with a similar number of\nparameters to our model.\n5.2. English evaluation\nAs for English, we tested RoBERTuito in three tasks:\nemotion analysis, hate speech detection, and sentiment\nanalysis. For emotion analysis and hate speech we used\nthe English sections from the aforementioned datasets\n(EmoEvent and HatEval), while for sentiment analysis\nSemEval 2017 Task-4 dataset (Rosenthal et al., 2017)\nwas used, which shares the same labels as its Spanish\ncounterpart (negative, neutral, positive).\nIn this case, we compare RoBERTuito abilities in En-\nglish with monolingual models: BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019), and BERTweet\n(Nguyen et al., 2020); and also against multilingual\nmodels such as XLM-R BASE (Conneau et al., 2020)\nand mBERT. While all these models share a base ar-\nchitecture, the different vocabulary sizes and number\nof parameters (see Appendix 11) make the comparison\na bit more difﬁcult.\n5.3. Code-switching evaluation\nFinally, we assessed the code-switching abilities of\nour model in the Linguistic Code-Switching Evalua-\ntion Benchmark (LinCE) (Aguilar et al., 2020). LinCE\ncomprises ﬁve tasks for code-switched data in several\nlanguage pairs (Spanish-English, Hindi-English, Mod-\nern Standard Arabic-Egyptian Arabic, Arabic-English,\namong others), many of which were part of previous\nshared tasks. We evaluated RoBERTuito on three dif-\nferent tasks of the benchmark: part of speech (POS)\ntagging (AlGhamdi et al., 2016), named entity recog-\nnition (NER), and sentiment analysis (Patwa et al.,\n2020). As the collection process of the data centered\non Spanish-speaking users, some of which also speak\nEnglish and Spanglish 4, we test RoBERTuito on the\nSpanish-English subsection of the benchmark.\nThis benchmark has a centralized evaluation system,\nnot releasing gold labels for the test split of the tasks.\nWe evaluated our models in the dev datasets and com-\npared our results against the ones provided by Winata\net al. (2021), which achieves the best performance for\nthe NER and POS tagging. As competing models to\nRoBERTuito for the Spanish-English evaluation, we\nhave mBERT, XLM-R (both in base and large archi-\ntecture) and monolingual models BERT and BETO.\n5.4. Model ﬁne-tuning\nWe followed fairly standard practices for ﬁne-tuning,\nmost of which are described in Devlin et al. (2019).\nFor sentence classiﬁcation tasks, we ﬁne-tuned the pre-\ntrained models for 5 epochs with a triangular learning\nrate of 510−5 and a warm-up of 10% of the training\nsteps. The best checkpoint at the end of each epoch\nwas selected based on each task metric.\nFor sentence classiﬁcation tasks, a linear classiﬁer is\nput on top of the representation of the [CLS] token.\nFor token classiﬁcation tasks (NER and POS tagging),\nwe predicted the word tag by putting a linear classiﬁer\non top of the ﬁrst corresponding sub-word representa-\ntion.\n6. Results\nTable 2 displays the results for the evaluation for the\nfour proposed classiﬁcation tasks in Spanish. Figures\nare expressed as the mean of 10 runs of the experi-\nments, along with a score averaging the metrics for\neach task in a similar way as the GLUE score. We can\nobserve that, in most cases, all RoBERTuito conﬁgu-\nrations perform above other models, in particular for\nhate speech detection and sentiment analysis. For most\ntasks, no big differences are observed betweenuncased\nand deacc models, but both perform consistently above\nthe cased model.\nTable 3 displays the results for the evaluation of the se-\nlected models for the three tasks in English. We can ob-\nserve that RoBERTuito outperforms both mBERT and\nXLM-R, which are the other multilingual models eval-\nuated for the tasks. Compared to monolingual English\nmodels, the results of RoBERTuito are similars to those\n4The morphosyntactic mixture of Spanish and English\nHATE SENTIMENT EMOTION IRONY\nT ask\n0\n5\n10\n15\n20\n25\n30\n35\n40Number of tokens per instance\nbeto_uncased\nbeto_cased\nroberta\nbertin\nrobertuito_cased\nrobertuito_uncased\nrobertuito_deacc\nFigure 1: Distribution of the number of tokens per in-\nstance. Bars are grouped by task, and display the mean\nnumber of tokens per instance with their 95% conﬁ-\ndence interval. Less is better\nof RoBERTa and slightly above BERT. As expected,\nBERTweet obtains the best results.\nAs for the code-switching evaluation, Table 4 dis-\nplays the results on the dev dataset of LinCE for NER,\nPOS Tagging, and sentiment analysis. We compare\nRoBERTuito against other multilingual models such as\nmBERT and XLM-RBASE , and also listing the dev re-\nsults reported by (Winata et al., 2021). The results for\nXLM-R and mBERT are consistent with the results in\nthat work. A minor improvement is observed but this\ncould an artifact of different choices of hyperparame-\nters or even a slightly different preprocessing.\nFinally, Table 5 displays the results from the leader-\nboard of the LinCE benchmark 5 for the three selected\ntasks: Spanish-English sentiment analysis, NER and\nPOS tagging. For sentiment analysis, it obtains the best\nresults in terms of Micro F1. For the other two tasks,\nit obtains the second position, for which an XLM-\nRLARGE model (Winata et al., 2021) has the top re-\nsults. Among the compared models, RoBERTuito has\n108 million parameters, while XLM-RLARGE sums up\nto around ﬁve times this number, making our model the\nmost efﬁcient in terms of size for this subsection of the\nbenchmark (see Appendix 11 for further details on the\nsize of the different models).\n6.1. Vocabulary efﬁciency\nFigure 1 shows the distribution of the number of to-\nkens in the input text for the Spanish tasks. We can\nobserve that RoBERTuito models have more compact\nrepresentations than BETO andRoBERTa-BNE for this\ndomain, and, between them, the deacc version has a\nslightly lower mean-length compared to the uncased\nversion. RoBERTuito has also shorter representations\nfor the code-mixed datasets, but longer in the case of\nthe English tasks. Appendix 11 lists the complete ﬁg-\nures for the three evaluation settings.\n5https://ritual.uh.edu/lince/\nleaderboard\nModel Hate Sentiment Emotion Irony Score\nRoBERTuitouncased 80.1 70 .7 55 .1 73.6 69.9\nRoBERTuitodeacc 79.8 70 .2 54 .3 74.0 69.6\nRoBERTuitocased 79.0 70 .1 51 .9 71 .9 68 .2\nRoBERTa 76.6 66 .9 53 .3 72 .3 67 .3\nBERTin 76.7 66 .5 51 .8 71 .6 66 .7\nBETOcased 76.8 66 .5 52 .1 70 .6 66 .5\nBETOuncased 75.7 64 .9 52 .1 70 .2 65 .7\nTable 2: Evaluation results for Spanish classiﬁcation tasks: hate speech detection, sentiment analysis, emotion\nanalysis and irony detection. Results are expressed as the mean Macro F1 score of 10 runs of the classiﬁcation\nexperiments. Bold indicates best performing models\nModel Hate Sentiment Emotion\nBERTweet 55.3 70 .3 42.8\nRoBERTuito 54.2 68 .4 44 .1\nRoBERTa 45.8 69 .5 46.3\nBERT 48.9 68 .9 42 .8\nmBERT∗ 43.3 66 .6 40 .4\nXLM-RBASE ∗ 45.7 68 .0 35 .7\nTable 3: Evaluation results for the three English classi-\nﬁcation tasks. Results are expressed as the mean Macro\nF1 score of 10 runs of the classiﬁcation experiments. ∗\nmarks multilingual models\nModel Sentiment NER POS\nRoBERTuitouncased 53.2 67.2 97 .0\nRoBERTuitodeacc 52.7 67.4 96.8\nRoBERTuitocased 50.1 66 .3 97.3\nmBERT 51.3 64 .8 96 .7\nXLM-RBASE 48.8 63 .7 97.3\nmBERT† – 63.7 97.3\nXLM-RBASE † – 62.8 97 .1\nTable 4: Development results for the code-mixed tasks\nfrom the LinCE dataset. Sentiment Analysis task per-\nformance is measured with Macro F1, Named En-\ntity Recognition with Micro F1, and Part-of-Speech\nthrough accuracy. U , C and DEach ﬁgure is the mean\nof 5 independent runs of the classiﬁcation experiments.\nResults marked with † are reported in Winata et al.\n(2021)\n7. Discussion\nFor this small set of Spanish tasks in the social-media\ndomain, the results show that RoBERTuito outperforms\nother pre-trained models for Spanish, namely BETO,\nRoBERTa and BERTin. This result is in line with other\nworks which show the effectiveness of social-media-\nspeciﬁc language models. A limitation in the eval-\nuation of our model for Spanish tasks is the lack of\ndatasets for other tasks rather than text classiﬁcation.\nAs far as we know, there are no Spanish-only datasets\nfor sequence labeling (NER and POS tagging) in the\nsocial domain.\nAmong the three proposed variants of RoBERTuito, the\nModel Sentiment NER POS\nRoBERTuito 60.6 68.5 97 .2\nXLM-RLARGE – 69.5 97 .2\nXLM-RBASE – 64.9 97 .0\nC2S mBERT 59.1 64 .6 96 .9\nmBERT 56.4 64 .0 97 .1\nBERT 58.4 61 .1 96 .9\nBETO 56.5 – –\nTable 5: Test results for the code-mixed tasks from\nSpanish-English section of the LinCE benchmark. Re-\nsults are taken from the ofﬁcial leaderboard of this\nbenchmark. Sentiment Analysis performance is mea-\nsured by Macro F1 score, Named Entity Recognition\n(NER) with Micro F1 score, and Part-of-Speech (POS)\nwith accuracy. C2S is an acronym for Char2Subword\nBERT, presented in (Aguilar et al., 2021)\ncased version is behind the others in terms of perfor-\nmance for most tasks –with the exception of POS tag-\nging and NER– while the other two (uncased and un-\ncased and deaccented) have comparable performances\nacross all the tasks. We can read this in two ways: one,\nthat a stronger normalization of the input text in Span-\nish results in no signiﬁcant improvement in the per-\nformance of the model, and two, that keeping accent\nmarks in the input text is neither beneﬁcial nor harmful\nfor the performance of the model.\nThe reasons for the differences in performance for the\nuncased models need further investigation. We have\nsome working hypotheses for these differences. First,\nwe believe that accents and non-ASCII characters –\nwith the exception of emojis– are used in a much more\ninconsistent way in user-generated text than in more\nnormative text. Therefore, no regularities can be in-\nferred from the data concerning those marks. Second,\na bigger amount of data is required to account for the\npossible differences in meaning for upper case or lower\ncase forms or the lack of difference between them. Fu-\nture experiments will delve into those two.\nOur data collection process for the pre-training stage\nwas centered in Spanish but it allowed other languages\nand other regional variants to be part of our dataset as\nwell. This point made our model develop some mul-\ntilingual features, in particular in the code-switching\nLinCE benchmark. The results for this benchmark\nhighlights that RoBERTuito is suited for Spanish-\nEnglish code-mixing tasks, obtaining better results than\nmBERT and matching those of XLM-R. This compar-\nison, however, is not completely fair because XLM-R\nand RoBERTa can handle over one hundred languages\nbut this is not the case for our model. Lastly, the re-\nsults for the English tasks show that RoBERTuito keeps\ncompetitive against monolingual models for the social\ndomain.\n8. Conclusion\nIn this work, we presented RoBERTuito, a large-scale\nmodel trained on user-generated tweets. We set up a\nbenchmark of classiﬁcation tasks in social-media text\nfor Spanish, and we showed that RoBERTuito outper-\nforms other available general domain pre-trained lan-\nguage models. Moreover, our model features good\ncode-switching performance in Spanish-English tasks\nand is competitive against monolingual English mod-\nels in the social domain.\nWe proposed three versions of this new model: cased,\nuncased, and deaccented. We observed that the uncased\nmodel performs slightly better than the cased one, and\nsimilarly to the deaccented version. Further research is\nneeded to systematize the reasons behind these results.\nWe have made our pretrained language models public\nthrough the HuggingFace model hub, and our code can\nbe found at GitHub 6. We will also make the train-\ning corpus available, thus facilitating the development\nof other models for user-generated Spanish, like word\nembeddings or other language models. It is even fea-\nsible to extract subsets of the corpus representing sub-\ndomains of interest, like regional variants of Spanish or\nspeciﬁc topics, to develop even more speciﬁc models.\nFuture work includes enhancing our benchmark to in-\nclude assessment of the performance of such models in\nopen-ended tasks, and experiments for speciﬁc subdo-\nmains of Spanish.\n9. Acknowledgements\nThe authors would like to thank the Google TPU Re-\nsearch Cloud program 7 for providing free access to\nTPUs for our experiments. We also thank Juan Carlos\nGiudici for his help in setting up the GCP environment,\nand Huggingface’s team for their support in the devel-\nopment of RoBERTuito.\n10. Bibliographical References\nAguilar, G., McCann, B., Niu, T., Rajani, N., Keskar,\nN. S., and Solorio, T. (2021). Char2Subword: Ex-\ntending the subword embedding space using robust\ncharacter compositionality. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n6https://github.com/pysentimiento/\nrobertuito\n7https://sites.research.google/trc/\n2021, pages 1640–1651, Punta Cana, Dominican\nRepublic, November. Association for Computational\nLinguistics.\nBasile, V ., Bosco, C., Fersini, E., Nozza, D., Patti, V .,\nRangel, F., Rosso, P., and Sanguinetti, M. (2019).\nSemeval-2019 task 5: Multilingual detection of hate\nspeech against immigrants and women in twitter. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation (SemEval-2019) . Association\nfor Computational Linguistics.\nEkman, P. (1992). An argument for basic emotions.\nCognition and Emotion, 6(3-4):169–200.\nGarc´ıa-Vega, M., D ´ıaz-Galiano, M., Garc ´ıa-\nCumbreras, M., Plaza-Del-Arco, F., Montejo-R ´aez,\nA., Zafra, S. M., Mart ´ınez-C´amara, E., Aguilar,\nC., Antonio, M., Cabezudo, S., Chiruzzo, L., and\nMoctezuma, D. (2020). Overview of tass 2020:\nIntroducing emotion detection. 09.\nGupta, R. K. and Yang, Y . (2017). CrystalNest at\nSemEval-2017 task 4: Using sarcasm detection\nfor enhancing sentiment classiﬁcation and quantiﬁ-\ncation. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npages 626–633, Vancouver, Canada, August. Asso-\nciation for Computational Linguistics.\nGuti´errez-Fandi˜no, A., Armengol-Estap ´e, J., P `amies,\nM., Llop-Palao, J., Silveira-Ocampo, J., Carrino,\nC. P., Gonzalez-Agirre, A., Armentano-Oller, C.,\nRodriguez-Penagos, C., and Villegas, M. (2021).\nSpanish language models.\nHan, B. and Baldwin, T. (2011). Lexical normalisation\nof short text messages: Makn sens a# twitter. InPro-\nceedings of the 49th annual meeting of the associa-\ntion for computational linguistics: Human language\ntechnologies, pages 368–378.\nJoulin, A., Grave, E., Bojanowski, P., and Mikolov, T.\n(2016). Bag of tricks for efﬁcient text classiﬁcation.\narXiv preprint arXiv:1607.01759.\nKudo, T. and Richardson, J. (2018). SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium,\nNovember. Association for Computational Linguis-\ntics.\nMoi, A., Cistac, P., Patry, N., Walsh, E. P., Morgan, F.,\nP¨utz, S., Wolf, T., Gugger, S., Delangue, C., Chau-\nmond, J., Debut, L., and von Platen, P. (2019). Hug-\nging face tokenizers library. https://github.\ncom/huggingface/tokenizers.\nNozza, D., Bianchi, F., and Hovy, D. (2020). What\nthe [mask]? making sense of language-speciﬁc bert\nmodels. arXiv preprint arXiv:2003.02912.\nOrtega-Bueno, R., Rangel, F., Hern ´andez Farıas, D.,\nRosso, P., Montes-y G´omez, M., and Medina Pagola,\nJ. E. (2019). Overview of the task on irony detection\nin spanish variants. In Proceedings of the Iberian\nlanguages evaluation forum (IberLEF 2019), co-\nlocated with 34th conference of the Spanish Soci-\nety for natural language processing (SEPLN 2019).\nCEUR-WS. org, volume 2421, pages 229–256.\nPfeffer, J., Mayer, K., and Morstatter, F. (2018). Tam-\npering with twitter’s sample api. EPJ Data Science,\n7(1):50.\nPlaza del Arco, F. M., Strapparava, C., Urena Lopez,\nL. A., and Martin, M. (2020). EmoEvent: A mul-\ntilingual emotion corpus based on different events.\nIn Proceedings of The 12th Language Resources\nand Evaluation Conference, pages 1492–1498, Mar-\nseille, France, May. European Language Resources\nAssociation.\nPolignano, M., Basile, P., De Gemmis, M., Semeraro,\nG., and Basile, V . (2019). Alberto: Italian bert lan-\nguage understanding model for nlp challenging tasks\nbased on tweets. In 6th Italian Conference on Com-\nputational Linguistics, CLiC-it 2019 , volume 2481,\npages 1–6. CEUR.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,\nS., Matena, M., Zhou, Y ., Li, W., and Liu, P. J.\n(2020). Exploring the limits of transfer learning with\na uniﬁed text-to-text transformer. Journal of Ma-\nchine Learning Research, 21(140):1–67.\nRasmy, L., Xiang, Y ., Xie, Z., Tao, C., and Zhi, D.\n(2021). Med-bert: pretrained contextualized em-\nbeddings on large-scale structured electronic health\nrecords for disease prediction. NPJ digital medicine,\n4(1):1–13.\nRosenthal, S., Farra, N., and Nakov, P. (2017).\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nIn Proceedings of the 11th International Workshop\non Semantic Evaluation (SemEval-2017) , pages\n502–518, Vancouver, Canada, August. Association\nfor Computational Linguistics.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin,\nI. (2017). Attention is all you need. In Advances in\nneural information processing systems, pages 5998–\n6008.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. (2018). GLUE: A multi-task bench-\nmark and analysis platform for natural language un-\nderstanding. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 353–355, Brussels,\nBelgium, November. Association for Computational\nLinguistics.\nWinata, G. I., Cahyawijaya, S., Liu, Z., Lin, Z.,\nMadotto, A., and Fung, P. (2021). Are multilingual\nmodels effective in code-switching? In Proceedings\nof the Fifth Workshop on Computational Approaches\nto Linguistic Code-Switching , pages 142–153, On-\nline, June. Association for Computational Linguis-\ntics.\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue,\nC., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow-\nicz, M., Davison, J., Shleifer, S., von Platen, P., Ma,\nC., Jernite, Y ., Plu, J., Xu, C., Le Scao, T., Gug-\nger, S., Drame, M., Lhoest, Q., and Rush, A. (2020).\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning: System Demonstrations , pages 38–45, Online,\nOctober. Association for Computational Linguistics.\n11. Language Resource References\nAguilar, Gustavo and Kar, Sudipta and Solorio,\nThamar. (2020). LinCE: A Centralized Benchmark\nfor Linguistic Code-switching Evaluation. European\nLanguage Resources Association.\nAlGhamdi, Fahad and Molina, Giovanni and Diab,\nMona and Solorio, Thamar and Hawwari, Abdelati\nand Soto, Victor and Hirschberg, Julia. (2016). Part\nof Speech Tagging for Code Switched Data. Associ-\nation for Computational Linguistics.\nBeltagy, Iz and Lo, Kyle and Cohan, Arman. (2019).\nSciBERT: A Pretrained Language Model for Scien-\ntiﬁc Text. Association for Computational Linguis-\ntics.\nCanete, Jos ´e and Chaperon, Gabriel and Fuentes, Ro-\ndrigo and Ho, Jou-Hui and Kang, Hojin and P ´erez,\nJorge. (2020). Spanish pre-trained bert model and\nevaluation data.\nConneau, Alexis and Khandelwal, Kartikay and Goyal,\nNaman and Chaudhary, Vishrav and Wenzek, Guil-\nlaume and Guzm ´an, Francisco and Grave, Edouard\nand Ott, Myle and Zettlemoyer, Luke and Stoyanov,\nVeselin. (2020). Unsupervised Cross-lingual Repre-\nsentation Learning at Scale . Association for Com-\nputational Linguistics.\nDevlin, Jacob and Chang, Ming-Wei and Lee, Ken-\nton and Toutanova, Kristina. (2019). BERT: Pre-\ntraining of Deep Bidirectional Transformers for Lan-\nguage Understanding . Association for Computa-\ntional Linguistics.\nGonzalez, Jose Angel and Hurtado, Llu ´ıs-F and Pla,\nFerran. (2021). TWilBert: Pre-trained deep bidirec-\ntional transformers for Spanish Twitter. Elsevier.\nLiu, Yinhan and Ott, Myle and Goyal, Naman and\nDu, Jingfei and Joshi, Mandar and Chen, Danqi and\nLevy, Omer and Lewis, Mike and Zettlemoyer, Luke\nand Stoyanov, Veselin. (2019). Roberta: A robustly\noptimized bert pretraining approach.\nDat Quoc Nguyen and Thanh Vu and Anh Tuan\nNguyen. (2020). BERTweet: A pre-trained lan-\nguage model for English Tweets.\nPatwa, Parth and Aguilar, Gustavo and Kar, Sudipta\nand Pandey, Suraj and PYKL, Srinivas and\nGamb”ack, Bj”orn and Chakraborty, Tanmoy and\nSolorio, Thamar and Das, Amitava. (2020).\nSemEval-2020 Task 9: Overview of Sentiment Anal-\nysis of Code-Mixed Tweets. Association for Compu-\ntational Linguistics.\nHyperparameter Value\n#Heads 12\n#Layers 12\nHidden Size 768\nIntermediate Size 3072\nHidden activation GeLU\nV ocab. size 30,000\nMLM probability 0.15\nMax Seq length 128\nBatch Size 4,096\nLearning Rate 3.5 ∗ 10−4\nDecay 0.1\nβ1 0.9\nβ2 0.98\nϵ 10−6\nWarmup steps 36,000 (6%)\nTraining steps 600,000\nTable 6: Hyperparameters of the RoBERTuito model\npre-training.\nModel Train loss Eval loss Eval ppl\nCased 1.864 1.753 5.772\nUncased 1.940 1.834 6.259\nDeacc 1.951 1.826 6.209\nTable 7: Training results for each of the three conﬁgu-\nrations of RoBERTuito, expressed in cross-entropy loss\nfor the Masked-Language-Modeling task and perplex-\nity (ppl)\nRadford, Alec and Narasimhan, Karthik and Salimans,\nTim and Sutskever, Ilya. (2018). Improving lan-\nguage understanding by generative pre-training.\nJavier De La Rosa and Eduardo G. Ponferrada\nand Manu Romero and Paulo Villegas and Pablo\nGonz´alez de Prado Salas and Mar ´ıa Grandury.\n(2022). BERTIN: Efﬁcient Pre-Training of a Span-\nish Language Model using Perplexity Sampling.\nAppendix\nHyperparameters and training\nTable 6 displays the hyperparameters of the RoBERTu-\nito training for its three versions. For the ﬁrst proto-\ntype of the model, a larger learning rate was tried but\nit usually diverged near its peak value, so we decided\nto lower it for the deﬁnitive training. Table 7 displays\nthe results of the training in terms of cross-entropy loss\nand perplexity for the three versions of RoBERTuito.\nModel comparison\nA comparison of the models in terms of the number of\nparameters and vocabulary sizes is presented in Table\n8. RoBERTuito is the smallest model in terms of vo-\ncabulary size, and as most models share a base archi-\ntecture (see Table 6), this accounts for the difference in\nthe number of parameters.\nModel Language Size V ocabulary\nRoBERTuito\nes\n108M 30K\nBETO 108M 30K\nBERTin 124M 50K\nRoBERTa-BNE 124M 30K\nBERT\nen\n109M 30K\nRoBERTa 124M 50K\nBERTweet 134M 64K\nmBERT\nmulti\n177M 105K\nC2S mBERT 136M –\nXLM-RBASE 278M 250K\nXLM-RLARGE 565M 250K\nTable 8: Comparison in terms of model size (measured\nin number of parameters) and vocabulary sizes for the\nconsidered models in this work. C2S mBERT is an\nabbreviation for Char2subword mBERT (Aguilar et al.,\n2021)\nSpanish\nModel EMOTION HATE SENTIMENT IRONY\nRoBERTuitoD 33.31 27 .39 17 .92 30 .70\nRoBERTuitoU 33.50 27 .70 18 .08 31 .12\nRoBERTuitoC 37.33 29 .51 18 .64 32 .71\nBETO 35.94 30 .06 19 .95 33 .14\nBERTin 36.68 29 .56 18 .33 31 .87\nRoBERTa 39.02 31 .67 20 .68 34 .21\nEnglish\nModel EMOTION HATE SENTIMENT\nBERTweet 33.22 29 .24 25 .70\nBERT 35.20 32 .05 26 .93\nRoBERTa 37.14 31 .29 27 .00\nRoBERTuitoD 38.79 36 .20 28 .82\nRoBERTuitoU 39.00 36 .29 29 .22\nRoBERTuitoC 44.27 39 .47 31 .49\nSpanish-English Code-switching\nModel NER POS SENTIMENT\nRoBERTuitoD 14.27 8 .8 20 .65\nRoBERTuitoU 14.34 8 .84 20 .75\nRoBERTuitoC 15.41 9 .01 22 .14\nBETO 16.99 10 .78 24 .16\nBERT 20.88 10 .32 29 .87\nBERTweet 18.47 9 .55 26 .29\nmBERT 16.47 9 .63 23 .82\nXLM-RBASE 18.23 9 .92 25 .36\nTable 9: Distribution of sentence length measured by\nnumber of tokens for each evaluation setting. D, U, and\nC correspond to deaccented, uncased and cased ver-\nsions. Results are displayed as the mean of the number\nof tokens per each sentence. Bold marks best results\n(less is better)\nVocabulary efﬁciency\nTable 9 displays the mean sentence length for each con-\nsidered model and group of tasks. For the Spanish and\nthe code-mixed Spanish-English benchmark, RoBER-\nTuito achieves the more compact representations in\nmean length in their uncased and deaccented forms. In\nthe case of English, BERTweet achieves the shortest\nrepresentations, with RoBERTuito having longer se-\nquences of tokens than its monolingual counterparts.",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.7796176075935364
    },
    {
      "name": "Computer science",
      "score": 0.7657794952392578
    },
    {
      "name": "Transformer",
      "score": 0.6711433529853821
    },
    {
      "name": "Natural language processing",
      "score": 0.6505110263824463
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6001999974250793
    },
    {
      "name": "Social media",
      "score": 0.5916534066200256
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5330535173416138
    },
    {
      "name": "Transfer of learning",
      "score": 0.47045814990997314
    },
    {
      "name": "Natural language",
      "score": 0.4652014672756195
    },
    {
      "name": "World Wide Web",
      "score": 0.26747822761535645
    },
    {
      "name": "Engineering",
      "score": 0.0851677656173706
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 27
}