{
  "title": "Transformers with convolutional context for ASR",
  "url": "https://openalex.org/W2941814890",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2122233958",
      "name": "Mohamed Abdelrahman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225993289",
      "name": "Okhonko, Dmytro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751234931",
      "name": "Zettlemoyer Luke",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W2962778134",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1922655562",
    "https://openalex.org/W2963742216",
    "https://openalex.org/W2962780374",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2911291251",
    "https://openalex.org/W2102113734",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2799800213",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2782451907",
    "https://openalex.org/W2184045248",
    "https://openalex.org/W2802023636",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2005708641",
    "https://openalex.org/W2520160253",
    "https://openalex.org/W854541894",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2904818793",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2327501763"
  ],
  "abstract": "The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.",
  "full_text": "Transformers with convolutional context for ASR\nAbdelrahman Mohamed, Dmytro Okhonko, Luke Zettlemoyer\nFacebook AI Research\nabdo,oxo,lsz@fb.com\nAbstract\nThe recent success of transformer networks for neural machine\ntranslation and other NLP tasks has led to a surge in research\nwork trying to apply it for speech recognition. Recent ef-\nforts studied key research questions around ways of combin-\ning positional embedding with speech features, and stability of\noptimization for large scale learning of transformer networks.\nIn this paper, we propose replacing the sinusoidal positional\nembedding for transformers with convolutionally learned in-\nput representations. These contextual representations provide\nsubsequent transformer blocks with relative positional informa-\ntion needed for discovering long-range relationships between\nlocal concepts. The proposed system has favorable optimiza-\ntion characteristics where our reported results are produced with\nﬁxed learning rate of 1.0 and no warmup steps. The proposed\nmodel achieves a competitive 4.7% and 12.9% WER on the Lib-\nrispeech “test clean” and “test other” subsets when no extra LM\ntext is provided.1\n1. Introduction\nSpeech Recognition systems have experienced many advances\nover the past decade, with neural acoustic and language\nmodels leading to impressive new levels of performance across\nmany challenging tasks [1, 2]. Advances in alignment-free\nsequence-level loss functions like CTC and ASG [3, 4] enabled\neasier training with letters as output units [5, 6]. The success\nof sequence-to-sequence models in neural machine translation\nsystems [7, 8] offered further simpliﬁcation to ASR systems by\nintegrating the acoustic and the language models into a single\nencoder-decoder architecture that is jointly optimized [9, 10].\nThe encoder focuses on building acoustic representations that\nthe decoder, through different attention mechanisms, can use to\ngenerate the target units.\nRecently, transformer networks have been shown to per-\nform well for neural machine translation [11] and many other\nNLP tasks [12]. A Transformer layer distinguishes itself from\na regular recurrent network by entirely relying on a key-value\n“self”-attention mechanism for learning relationships between\ndistant concepts, rather than relying on recurrent connections\nand memory cells to preserve information, as in LSTMs, that\ncan fade over time steps. Transformer layers can be seen as bag-\nof-concept layers because they don’t preserve location informa-\ntion in the weighted sum self-attention operation. To model\nword order, sinusoidal positional embeddings are used [11].\nThere has been recent research interest in using transformer\nnetworks for end-to-end ASR both with CTC loss [13] and in\nan encoder-decoder framework [14, 15] with modest perfor-\nmance compared to baseline systems. For a standard hybrid\nASR system, [16] introduced a time-constrained key-value\n1Code available at: github.com/pytorch/fairseq/\ntree/master/examples/speech recognition\nself-attention layer to be used in tandem with other TDNN and\nrecurrent layers. Using time-restricted self-attention context\nenabled the authors to model input positions as 1-hot vectors,\nhowever, they didn’t show a conclusive evidence for the impact\nof the self-attention context size. One interesting research ques-\ntion in all previous work was: how to best introduce positional\ninformation for input speech features. Answers range from\ndropping it altogether, adding it to input features/embedding,\nand concatenating it with input features leaving it to the neural\nnetwork to decide how to combine them.\nIn this paper, we take an alternative approach. We propose\nreplacing sinusoidal positional embedding with contextually\naugmented inputs learned by 2-D convolutional layers over\ninput speech features in the encoder, and by 1-D convolutional\nlayers over previously generated outputs in the decoder. Lower\nlayers build atomic concepts, both in encoders and decoders,\nby learning local relationships between time steps. Long-range\nsequential structure modeling is left to subsequent layers.\nAlthough the transformer’s ﬂexible inductive bias is able to\nmimic convolution ﬁlters in its lower layers, we argue that this\ncomes at the expense of brittle optimization. We believe that\nadding early convolutional layers allows the model to learn\nimplicit relative positional encodings which enable subsequent\ntransformer layers to recover the right order of the output\nsequence.\nUsing convolutional layers as input processors before recur-\nrent layers in acoustic encoders has been previously proposed\nfor computational reasons with minimal impact on performance\n[17]. So, we focus our experiments on understanding the impact\nof the convolutional context size consumed by the decoder 1-D\nconvolutional layers. Our best model conﬁguration, with a ﬁxed\nlearning rate of 1.0, no hyperparameter or decoder optimization,\nachieves 12% and 16% relative reduction in WER compared\nto previously published results on the acoustically challenging\nLibrispeech [18] “dev other” and “test other” subsets, when no\nextra LM text data is used during decoding.\n2. Transformers with convolutional context\nWe propose dividing the modeling task into two sub-\ncomponents: learning local relationships within a small context\nwith convolutional layers, and learning global sequential struc-\nture of the input with transformer layers. This division sim-\npliﬁes transformer optimization leading to more stable training\nand better results because we don’t need to force lower trans-\nformer layers to learn local dependencies.\n2.1. Transformer layer\nTransformer layers [11] have the ability to learn long range re-\nlationships for many sequential classiﬁcation tasks [12]. Multi-\nhead self-attention is the core component of transformer layers.\narXiv:1904.11660v2  [cs.CL]  2 Mar 2020\nLayer Norm\nDropoutFC 2ReLuFC 1Layer Norm\nDropoutSelf-attention\n+\n+\nDecoder Predictions\nTransformerBlock\nEncoder Output\nInput features\nEncoder multi-head attention\nTransformerBlock\nDecoder Conv. Block\nEncoder Conv. Block\nH\nHP\nS\nL\nFigure 1: Left: components of one transformer block. Right:\nBlock diagram of the full end-to-end model\nLet dinput be input dimension to a transformer layer, each time\nstep in the input is projected into dk, dk, dv dimensional vec-\ntors representing the queries(Q), keys(K) and values(V ) for at-\ntention, where similarities between keys and queries determine\ncombination weights of values for each time step,\nAttention(Q, K, V) =Softmax ( QKT\n√dk\n)V (1)\nThe dot product between keys and queries is scaled by the in-\nverse square root of the key dimension. This self-attention op-\neration is done h times in parallel, for the case of h attention\nheads, with different projection matrices from dinput to dk, dk,\nand dv. The ﬁnal output is a concatenation of h vectors each\nwith dimension dv which is in turn linearly projected to the de-\nsired output dimension of the self-attention layer.\nOn top of the self-attention component, transformer layers\nhave multiple operations applied on each time step; dropout,\nresidual connection, layer norm, two fully connected layers\nwith a ReLU layer in between, another residual and Layer norm\noperations. Figure(1)-left show the details of one transformer\nlayer as proposed by [11].\n2.2. Adding context to transformer\nOur convolutional layers are added below the Transformer lay-\ners, and we do not make any use of positional encodings. The\nmodel learns an acoustic language model over the bag of dis-\ncovered acoustic units as it goes deeper in the encoder. The\nexperimental results show that using a relatively deep encoder\nis critical for getting good performance. For the encoder, we\nused 2-D convolutional blocks with layer norms and ReLU af-\nter each convolutional layer. Each convolutional block contains\nK convolutional layers followed by a 2-D max pooling layer,\nas shown in ﬁgure(2)-right. For the decoder, we follow a sim-\nilar approach using 1-D convolutions over embeddings of pre-\nviously predicted words (shown in ﬁgure(2)-left with N 1-D\nconvolutional layers in each decoder convolutional block).\n2.3. Full end-to-end model architecture\nFigure(1)-right shows the full end-to-end system architecture.\nEach block in the model is repeated multiple times (shown on\nthe top right corner of each block). On the decoder side, we\nuse a separate multi-head attention layer to aggregate encoder\ncontext for each decoder transformer block. We found that hav-\ning more than one attention layer improves the overall system\nReLuLayer Norm2-D Convolution\n2-D Max PoolingKReLuLayer Norm1-D Convolution\nN\nFigure 2: Left: One decoder side 1-D convolutional block.\nRight: One encoder side 2-D convolutional block.\nrecognition performance. The decoder 1-D convolution only\nlooks at historical predictions with its end point at the current\ntime step. Similarly, the transformer layers have future target\nsteps masked, so that decoder self-attention is only running over\ncurrent and previous time steps to respect left-to-right output\ngeneration. We are not investigating online/streaming decoding\nconditions in this paper, so the encoder self-attention is allowed\nto operate over the entire input utterance.\n3. Experimental results\n3.1. Experimental Setup\nWe evaluate performance on the Librispeech dataset [18] con-\ntaining 1000h of training data with development and test sets\nsplit into simple (“clean”) and harder (“other”) subsets 2. We\nuse 5k “unigram” subword target units learned by the sentence\npiece package [20] with full coverage of all training text data.\nInput speech is represented as 80-D log mel-ﬁlterbank coefﬁ-\ncients plus three fundamental frequency features computed ev-\nery 10ms with a 25ms window.\nAll experiments were not tuned to best possible perfor-\nmance using training hyperparameter or decoder optimization.\nWe don’t use scheduled sampling or label smoothing. For regu-\nlarization, we use a single dropout rate of 0.15 across all blocks\nas part of our default conﬁgurations. For model optimization,\nwe use the AdaDelta algorithm [21] with ﬁxed learning rate=1.0\nand gradient clipping at 10.0. We run all conﬁgurations for 80\nepochs, we then report results on an average model computed\nover the last 30 checkpoints. Averaging the last few checkpoints\nbrings the model weights closer to the nearest local minimum.\nWe could have stopped training models much earlier than 80\nepochs, with a different early stopping point for different runs,\nbut decided to stick by a generic training recipe to simplify re-\nproducing our results. It is important to mention that we aren’t\nusing a learning rate warmup schedule and yet the model con-\nverges to the reported WER results in a stable way. This gen-\neral ﬁxed training recipe wasn’t optimized on any part of Lib-\nrispeech.\nThe standard convolutional tranformer model used in most\nexperiments has the following conﬁguration: (1) Two 2-D con-\nvolutional blocks, each with two conv. layers with kernel\nsize=3, max-pooling kernel=2. The ﬁrst block has 64 feature\n2We decided to concentrate on Librispeech and not on smaller\ndatasets, e.g. TIMIT, WSJ, as with current model capacities research\nﬁndings on smaller datasets can’t reliably generalize to new scenarios\nand don’t provide universal modeling trends. Early CTC experiments\nshowed no gains for WSJ [19] while it was later proved to be one of the\ncurrent best large scale loss functions [6].\nmaps while the second has 128, (2) 10 encoder transformer\nblocks all with transformer dim=1024, 16 heads, intermedi-\nate ReLU layer size=2048, (3) decoder input word embedding\ndim=512, (4) three 1-D conv. layers each with kernel size=3,\nno max pooling is used for the decoder side 1-D convolution,\nand (5) 10 decoder transformer blocks each with encoder-side\nmultihead attention, otherwise the conﬁguration is identical to\nthe encoder transformer block. This canonical model has about\n223M parameters, and it takes about 24 hours to perform all 80\nepochs on 2 machines each with 8GPUs with 16GB of memory.\nAll results are reported without any external language model\ntrained on extra text data. Our focus is to study the contextual\ntransformer decoder’s ability to model the statistical properties\nof the spoken training data. We use beam size of 5 during infer-\nence for all experiments except mentioned otherwise.\nModel dev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConv. context 5.2 13.7 5.3 14.0\nSin. pos. embd 5.8 14.2 5.4 14.8\n(1) + (2) 5.2 13.8 5.3 14.0\nOne layer of enc. att. 6.4 15.2 6.3 15.9\n32 heads in enc/dec 5.3 14.1 5.4 14.6\n4k transformer ReLU 5.3 13.2 5.3 13.4\nTable 1: The impact of different modeling decision on WER\n3.2. Model Comparisions\nWe ﬁrst studied performance of our approach to alternative ar-\nchitectures and positional encoding schemes. In table(1) we\nshow the WER of the proposed transformer encoder-decoder\nmodel with convolutional context using the canonical conﬁgu-\nration in the ﬁrst row. Replacing the 1-D convolutional con-\ntext in the decoder with sinusoidal positional embedding, as\nproposed in the baseline machine translation transformers [11]\nand adopted in [13, 15], shows inferior WER performance.\nBy combining sinusoidal and convolutional position embedding\n(rows 1+2), we don’t observe any gains. This supports our\nintuition that the relative convolutional positional information\nprovides sufﬁcient signal for the transformer layers to recre-\nate more global word order. We also found that having mul-\ntiple encoder-side attention layers is critical for achieving the\nbest WER. Increasing the intermediate ReLU layer in each en-\ncoder and decoder layer was found to greatly improve the over-\nall WER across different sets, however increasing the number\nof attention heads, while keeping the attention dimension the\nsame, deteriorates the performance.\nTo better understand these results, we also studied the ef-\nfects of different hyperparameter settings. Table(2) shows the\neffect of different decoder convolutional context sizes spread\nover different depths. All conﬁgurations in table(2) share the\nsame canonical conﬁguration and the number of 1-D conv. fea-\nture maps were chosen to ensure the total number of parame-\nters are ﬁxed between all conﬁgurations. The best performance\ncomes from using the same parameter budget over wider con-\ntext that is built over multiple convolutional layers. However,\nthe decoder is able to get reasonable WER even with a con-\ntext of just 3 words as input to the transformer layers. Using\na deep transformer encoder capture long range structure of the\ndata as an acoustic LM built on top of learned concepts from the\nconvolutional layers. Also deeper encoder help marginalize out\nConv.\ndepth\nCxt size\n(num. kernels)\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n1\n3 (3) 5.3 13.8 5.4 14.1\n5 (5) 5.4 14.1 5.5 14.0\n7 (7) 5.4 13.9 5.4 14.5\n9 (9) 5.4 13.9 5.5 14.0\n11 (11) 5.3 13.6 5.4 13.8\n2\n5 (3-3) 5.3 14.0 5.2 14.5\n7 (3-5) 5.5 14.7 5.9 14.8\n9 (5-5) 5.2 13.9 5.6 14.2\n11 (5-7) 5.2 14.1 5.4 14.6\n3\n7 (3-3-3) 5.2 13.7 5.3 14.0\n9 (3-3-5) 5.3 13.8 5.4 14.1\n11 (3-5-5) 5.6 14.3 5.4 14.2\n4 9 (3-3-3-3) 5.0 13.5 5.4 13.9\n11 (3-3-3-5) 5.0 13.6 5.2 13.7\nTable 2: WER for different decoder convolution architectures\nglobal utterance speciﬁc speaker and environment characteris-\ntics while focusing on the content. A deeper deocder, although\nnot as critical, showed better overall performance. Table(3)\nshows WER for different depth conﬁgurations. We wanted to\nunderstand the effect of ﬁxing encoder depth while changing\ndecoder and vice versa, ﬁxing the total sum of encoder and de-\ncoder depths, as well as using same depth on both sides all the\nway up to 14 transformer layers.\nSetup Enc/Dec\ndepth\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nSame\nEnc/Dec\ndepth\n6/6 5.6 14.5 5.7 15.3\n8/8 5.3 14.0 5.3 13.9\n10/10 5.2 13.7 5.3 14.0\n12/12 5.0 13.0 5.0 13.3\n14/14 5.0 12.9 5.0 13.4\nSame\ntotal\ndepth\n2/10 7.5 18.4 7.8 18.7\n4/8 6.2 15.5 6.0 16.0\n6/6 5.6 14.5 5.7 15.3\n8/4 5.4 13.9 5.3 14.3\n10/2 5.2 14.1 5.4 14.6\nSame\nEnc\ndepth\n10/2 5.2 14.1 5.4 14.6\n10/4 5.1 13.7 5.2 14.2\n10/6 5.0 13.3 5.1 13.7\n10/8 5.2 13.6 5.2 14.3\n10/10 5.2 13.7 5.3 14.0\nSame\nDec\ndepth\n2/10 7.5 18.4 7.8 18.7\n4/10 6.4 15.4 6.3 16.4\n6/10 5.7 14.5 5.7 14.6\n8/10 5.2 14.0 5.4 14.3\n10/10 5.2 13.7 5.3 14.0\nTable 3: WER for different transformer depth in encoder and\ndecoder\n3.3. Final Results\nBased on these experimental ﬁndings, we combined the best\nperforming conﬁgurations into one model that is similar to the\ncanonical model except: (1) we use 4k ReLU layers in all trans-\nformer blocks in the encoder and the decoder, (2) we use 16 en-\ncoder transformer blocks, and (3) we only use 6 decoder trans-\nformer blocks. The results of the best model are shown in ta-\nble(4). For decoding of the best model we used beam size of\n20.\nTable(4) compares this model to other previously pub-\nlished results on the Librispeech dataset. For completeness,\nwe added models that use externally trained LMs on extra text\ndata, although their results aren’t comparable to ours. Com-\npared to models with no external LM, our model brings 12%\nto 16% relative WER reduction on the acoustically challenging\n“dev other” and “test other” subsets of Librispeech. This sug-\ngests that the convolutional tranformer indeed learns long-range\nacoustic characteristics of speech data, e.g. speaker and envi-\nronment characteristics, because the model doesn’t bring much\nimprovement to the “dev clean” and “test clean” subsets which\nneed external text data for improvement. The results conﬁrm\nour belief that the improvements found in this paper are orthog-\nonal to further potential improvements to the WER using an LM\ntrained on much larger text corpus.\nModel LM on\nextra text\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nCAPIO\nspk adpt[22]\nRNNLM 3.12 8.28 3.51 8.58\nLSTM[23] 4gramLM 4.79 14.31 4.82 15.30\nGated Cnv[24] 4gramLM 4.6 13.8 4.8 14.5\nTsf w/sin\npos embd[13]\n4gramLM - - 4.8 13.1\nTDS Cnv[25] 4gramLM 3.75 10.70 4.21 11.87\nLSTM[23] LSTMLM 3.54 11.52 3.82 12.76\nFully Cnv[26] ConvLM 3.16 10.05 3.44 11.24\nTDS Cnv[25] ConvLM 3.01 8.86 3.28 9.84\nTDS Cnv[25] None 5.04 14.45 5.36 15.64\nLSTM[23] None 4.87 14.37 4.87 15.39\nLSTM (MLE)[27] None - - 5.7 15.4\nLSTM (OCD)[27] None - - 4.5 13.3\nCnv Cxt Tsf (MLE)\n(ours)\nNone 4.8 12.7 4.7 12.9\nTable 4: WER comparison with other previously published work\non Librispeech\n4. Conclusion and future work\nWe presented a transformer seq2seq ASR system with learned\nconvolutional context, both for the encoder and the decoder.\nInput convolutional layers capture relative positional informa-\ntion which enables subsequent transformer blocks to learn long\nrange relationships between local concepts in the encoder, and\nrecover the target sequence in the decoder. Using a deep trans-\nformer encoder was important to reach best performance, as\nwe demonstrated empirically. Our best conﬁguration achieves\n12% and 16% relative reduction in WER compared to previ-\nously published systems on Librispeech “dev other” and “test\nother” subsets respectively, when no extra LM text is provided.\nCombining the proposed system with a better training proce-\ndure, e.g. Optimal Completion Distillation (OCD)[27] is an in-\nteresting future avenue for combining observed WER gains.\n5. References\n[1] G. Hinton, L. Deng, D. Yu, G. Dahl, A. rahman Mohamed,\nN. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen, T. Sainath, and\nB. Kingsbury, “Deep neural networks for acoustic modeling in\nspeech recognition,” Signal Processing Magazine, 2012.\n[2] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock ´y, and S. Khudan-\npur, “Recurrent neural network based language model,” inINTER-\nSPEECH, 2010.\n[3] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber, “Con-\nnectionist temporal classiﬁcation: Labelling unsegmented se-\nquence data with recurrent neural networks,” inProceedings of the\n23rd International Conference on Machine Learning, ser. ICML\n’06, 2006.\n[4] R. Collobert, C. Puhrsch, and G. Synnaeve, “Wav2letter: an\nend-to-end convnet-based speech recognition system,” 2016.\n[Online]. Available: http://arxiv.org/abs/1609.03193\n[5] A. Graves and N. Jaitly, “Towards end-to-end speech recognition\nwith recurrent neural networks,” inICML, 2014.\n[6] A. Y . Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,\nE. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates,\nand A. Y . Ng, “Deep speech: Scaling up end-to-end speech\nrecognition,” 2014. [Online]. Available: http://arxiv.org/abs/1412.\n5567\n[7] K. Cho, B. van Merrienboer, C ¸ . G ¨ulc ¸ehre, F. Bougares,\nH. Schwenk, and Y . Bengio, “Learning phrase representations\nusing RNN encoder-decoder for statistical machine translation,”\n2014. [Online]. Available: http://arxiv.org/abs/1406.1078\n[8] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence\nlearning with neural networks,” 2014. [Online]. Available:\nhttp://arxiv.org/abs/1409.3215\n[9] W. Chan, N. Jaitly, Q. V . Le, and O. Vinyals, “Listen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,” in ICASSP, 2016. [Online]. Available:\nhttp://williamchan.ca/papers/wchan-icassp-2016.pdf\n[10] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Bengio,\n“Attention-based models for speech recognition,” 2015. [Online].\nAvailable: http://arxiv.org/abs/1506.07503\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\n2017. [Online]. Available: http://arxiv.org/abs/1706.03762\n[12] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:\npre-training of deep bidirectional transformers for language\nunderstanding,” 2018. [Online]. Available: http://arxiv.org/abs/\n1810.04805\n[13] J. Salazar, K. Kirchhoff, and Z. Huang, “Self-attention networks\nfor connectionist temporal classiﬁcation in speech recognition,”\n2019. [Online]. Available: https://arxiv.org/abs/1901.10055\n[14] M. Sperber, J. Niehues, G. Neubig, S. St ¨uker, and A. Waibel,\n“Self-attentional acoustic models,” 2018. [Online]. Available:\nhttp://arxiv.org/abs/1803.09519\n[15] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based\nsequence-to-sequence speech recognition with the transformer\nin mandarin chinese,” in Interspeech, 2018. [Online]. Available:\nhttps://arxiv.org/abs/1804.10752\n[16] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur,\n“A time-restricted self-attention layer for asr,” 2018 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2018.\n[17] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y . Unno,\nN. E. Y . Soplin, J. Heymann, M. Wiesner, N. Chen, A. Ren-\nduchintala, and T. Ochiai, “Espnet: End-to-end speech processing\ntoolkit,” in Interspeech, 2018.\n[18] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: An asr corpus based on public domain audio books,”\n2015.\n[19] A. Graves, N. Jaitly, and A. rahman Mohamed, “Hybrid speech\nrecognition with deep bidirectional lstm,” inIn IEEE Workshop on\nAutomatic Speech Recognition and Understanding (ASRU, 2013.\n[20] T. Kudo and J. Richardson, “Sentencepiece: A simple and lan-\nguage independent subword tokenizer and detokenizer for neural\ntext processing,” the 2018 Conference on Empirical Methods in\nNatural Language Processing, 2018.\n[21] M. D. Zeiler, “ADADELTA: an adaptive learning rate method,”\n2012. [Online]. Available: http://arxiv.org/abs/1212.5701\n[22] K. J. Han, A. Chandrashekaran, J. Kim, and I. R. Lane, “The\nCAPIO 2017 conversational speech recognition system,” 2018.\n[Online]. Available: http://arxiv.org/abs/1801.00059\n[23] A. Zeyer, K. Irie, R. Schl ¨uter, and H. Ney, “Improved training\nof end-to-end attention models for speech recognition,” 2018.\n[Online]. Available: http://arxiv.org/abs/1805.03294\n[24] V . Liptchinsky, G. Synnaeve, and R. Collobert, “Letter-\nbased speech recognition with gated convnets,” CoRR, vol.\nabs/1712.09444, 2017. [Online]. Available: http://arxiv.org/abs/\n1712.09444\n[25] A. Hannun, A. Lee, Q. Xu, and R. Collobert, “Sequence-\nto-sequence speech recognition with time-depth separable\nconvolutions,” 2019. [Online]. Available: UnderReview\n[26] N. Zeghidour, Q. Xu, V . Liptchinsky, N. Usunier, G. Syn-\nnaeve, and R. Collobert, “Fully convolutional speech recogni-\ntion,” CoRR, vol. abs/1812.06864, 2018.\n[27] S. Sabour, W. Chan, and M. Norouzi, “Optimal completion distil-\nlation for sequence learning,” in ICLR 2019.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8588823676109314
    },
    {
      "name": "Computer science",
      "score": 0.736396312713623
    },
    {
      "name": "Embedding",
      "score": 0.6647671461105347
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6298326253890991
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47259482741355896
    },
    {
      "name": "Speech recognition",
      "score": 0.40790820121765137
    },
    {
      "name": "Machine learning",
      "score": 0.3598247170448303
    },
    {
      "name": "Natural language processing",
      "score": 0.35002684593200684
    },
    {
      "name": "Engineering",
      "score": 0.1251251995563507
    },
    {
      "name": "Electrical engineering",
      "score": 0.09562677145004272
    },
    {
      "name": "Voltage",
      "score": 0.08426326513290405
    }
  ]
}