{
    "title": "A Large Language Model Screening Tool to Target Patients for Best Practice Alerts: Development and Validation",
    "url": "https://openalex.org/W4387914853",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2114695148",
            "name": "Thomas Savage",
            "affiliations": [
                "Stanford Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A2096913560",
            "name": "John Wang",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2166739861",
            "name": "Lisa Shieh",
            "affiliations": [
                "Stanford Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A2114695148",
            "name": "Thomas Savage",
            "affiliations": [
                "Stanford Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A2096913560",
            "name": "John Wang",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2166739861",
            "name": "Lisa Shieh",
            "affiliations": [
                "Stanford Medicine"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1937693759",
        "https://openalex.org/W2605753964",
        "https://openalex.org/W2099376575",
        "https://openalex.org/W2110922423",
        "https://openalex.org/W3126903050",
        "https://openalex.org/W2748242435",
        "https://openalex.org/W6903655558",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2162800060",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4382765604"
    ],
    "abstract": "Background Best Practice Alerts (BPAs) are alert messages to physicians in the electronic health record that are used to encourage appropriate use of health care resources. While these alerts are helpful in both improving care and reducing costs, BPAs are often broadly applied nonselectively across entire patient populations. The development of large language models (LLMs) provides an opportunity to selectively identify patients for BPAs. Objective In this paper, we present an example case where an LLM screening tool is used to select patients appropriate for a BPA encouraging the prescription of deep vein thrombosis (DVT) anticoagulation prophylaxis. The artificial intelligence (AI) screening tool was developed to identify patients experiencing acute bleeding and exclude them from receiving a DVT prophylaxis BPA. Methods Our AI screening tool used a BioMed-RoBERTa (Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach; AllenAI) model to perform classification of physician notes, identifying patients without active bleeding and thus appropriate for a thromboembolism prophylaxis BPA. The BioMed-RoBERTa model was fine-tuned using 500 history and physical notes of patients from the MIMIC-III (Medical Information Mart for Intensive Care) database who were not prescribed anticoagulation. A development set of 300 MIMIC patient notes was used to determine the model’s hyperparameters, and a separate test set of 300 patient notes was used to evaluate the screening tool. Results Our MIMIC-III test set population of 300 patients included 72 patients with bleeding (ie, were not appropriate for a DVT prophylaxis BPA) and 228 without bleeding who were appropriate for a DVT prophylaxis BPA. The AI screening tool achieved impressive accuracy with a precision-recall area under the curve of 0.82 (95% CI 0.75-0.89) and a receiver operator curve area under the curve of 0.89 (95% CI 0.84-0.94). The screening tool reduced the number of patients who would trigger an alert by 20% (240 instead of 300 alerts) and increased alert applicability by 14.8% (218 [90.8%] positive alerts from 240 total alerts instead of 228 [76%] positive alerts from 300 total alerts), compared to nonselectively sending alerts for all patients. Conclusions These results show a proof of concept on how language models can be used as a screening tool for BPAs. We provide an example AI screening tool that uses a HIPAA (Health Insurance Portability and Accountability Act)–compliant BioMed-RoBERTa model deployed with minimal computing power. Larger models (eg, Generative Pre-trained Transformers–3, Generative Pre-trained Transformers–4, and Pathways Language Model) will exhibit superior performance but require data use agreements to be HIPAA compliant. We anticipate LLMs to revolutionize quality improvement in hospital medicine.",
    "full_text": "Original Paper\nA Large Language Model Screening Tool to Target Patients for\nBest Practice Alerts: Development and Validation\nThomas Savage1, MD; John Wang2, MD; Lisa Shieh1, MD, PhD\n1Division of Hospital Medicine, Department of Medicine, Stanford University, Palo Alto, CA, United States\n2Divison of Gastroenterology and Hepatology, Department of Medicine, Stanford University, Palo Alto, CA, United States\nCorresponding Author:\nThomas Savage, MD\nDivision of Hospital Medicine\nDepartment of Medicine\nStanford University\n300 Pasteur Drive\nPalo Alto, CA, 94304\nUnited States\nPhone: 1 6507234000\nEmail: tsavage@stanford.edu\nAbstract\nBackground: Best Practice Alerts (BPAs) are alert messages to physicians in the electronic health record that are used to\nencourage appropriate use of health care resources. While these alerts are helpful in both improving care and reducing costs,\nBPAs are often broadly applied nonselectively across entire patient populations. The development of large language models\n(LLMs) provides an opportunity to selectively identify patients for BPAs.\nObjective: In this paper, we present an example case where an LLM screening tool is used to select patients appropriate for a\nBPA encouraging the prescription of deep vein thrombosis (DVT) anticoagulation prophylaxis. The artificial intelligence (AI)\nscreening tool was developed to identify patients experiencing acute bleeding and exclude them from receiving a DVT prophylaxis\nBPA.\nMethods: Our AI screening tool used a BioMed-RoBERTa (Robustly Optimized Bidirectional Encoder Representations from\nTransformers Pretraining Approach; AllenAI) model to perform classification of physician notes, identifying patients without\nactive bleeding and thus appropriate for a thromboembolism prophylaxis BPA. The BioMed-RoBERTa model was fine-tuned\nusing 500 history and physical notes of patients from the MIMIC-III (Medical Information Mart for Intensive Care) database\nwho were not prescribed anticoagulation. A development set of 300 MIMIC patient notes was used to determine the model’s\nhyperparameters, and a separate test set of 300 patient notes was used to evaluate the screening tool.\nResults: Our MIMIC-III test set population of 300 patients included 72 patients with bleeding (ie, were not appropriate for a\nDVT prophylaxis BPA) and 228 without bleeding who were appropriate for a DVT prophylaxis BPA. The AI screening tool\nachieved impressive accuracy with a precision-recall area under the curve of 0.82 (95% CI 0.75-0.89) and a receiver operator\ncurve area under the curve of 0.89 (95% CI 0.84-0.94). The screening tool reduced the number of patients who would trigger an\nalert by 20% (240 instead of 300 alerts) and increased alert applicability by 14.8% (218 [90.8%] positive alerts from 240 total\nalerts instead of 228 [76%] positive alerts from 300 total alerts), compared to nonselectively sending alerts for all patients.\nConclusions: These results show a proof of concept on how language models can be used as a screening tool for BPAs. We\nprovide an example AI screening tool that uses a HIPAA (Health Insurance Portability and Accountability Act)–compliant\nBioMed-RoBERTa model deployed with minimal computing power. Larger models (eg, Generative Pre-trained Transformers–3,\nGenerative Pre-trained Transformers–4, and Pathways Language Model) will exhibit superior performance but require data use\nagreements to be HIPAA compliant. We anticipate LLMs to revolutionize quality improvement in hospital medicine.\n(JMIR Med Inform 2023;11:e49886) doi: 10.2196/49886\nKEYWORDS\nlarge language models; language models; language model; EHR; health record; health records; quality improvement; Artificial\nIntelligence; Natural Language Processing\nJMIR Med Inform 2023 | vol. 11 | e49886 | p. 1https://medinform.jmir.org/2023/1/e49886\n(page number not for citation purposes)\nSavage et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nIntroduction\nLarge language models (LLMs) are an exciting development\nin the field of natural language processing and present\ntremendous potential for application to clinical medicine.\nLanguage models such as Bidirectional Encoder Representations\nfrom Transformers (BERT), Robustly Optimized BERT\nPretraining Approach (RoBERTa), and Generative Pre-trained\nTransformers (GPT) can perform complex tasks such as text\nclassification, question answering, and text generation, which\nhave the potential to augment physicians in providing care for\npatients. A clinical application of significant potential is LLM\noptimization of quality improvement electronic health record\n(EHR) Best Practice Alerts (BPAs).\nEHR BPAs are powerful tools to improve patient care outcomes.\nTheir widespread use has demonstrated better adherence to\nclinical guidelines, fewer medication errors, improved diabetes\nmanagement, more appropriate antimicrobial prescribing, and\nhigher rates of ambulatory preventive care, among others [1].\nNevertheless, despite these benefits, the overuse of BPAs can\ncause alarm fatigue and desensitization that can even lead to\npatient harm, as described by the Joint Commission Alert\nSystem Safety Report of 2014 [2,3]. Current BPAs are often\noverconservative, broadly applied to all possible patients,\nresulting in up to 49% to 96% of alerts being overridden or\nignored [4]. A BPA will frequently be obviously inappropriate\nor not applicable to the patient, resulting in the physician\nbecoming desensitized to the alert and more likely to ignore a\nvalid alert in the future [5,6]. LLMs offer an opportunity to\nscreen appropriate patients for BPAs.\nIn this paper, we propose how language models can be leveraged\nto read a physician’s note and screen whether a patient is\nappropriate for a BPA. We examine an example case using a\nBioMed-RoBERTa artificial intelligence (AI) screening tool\nthat selectively identifies patients who are appropriate for a deep\nvein thrombosis prophylaxis. Our screening tool reads each\nhistory and physical note to identify whether a patient has active\nbleeding (a contraindication to anticoagulation) and excludes\nthose patients with bleeding from receiving a deep vein\nthrombosis prophylaxis BPA. We hope our methods will\nencourage the use of language models to screen patients for\nBPAs and improve EHR workflow.\nMethods\nPatient Note Data Set\nThe MIMIC-III (Medical Information Mart for Intensive Care)\ndata set is publicly available and consists of more than 60,000\nintensive care unit admissions from the Brigham and Women’s\nHospital system [7-9]. The data set includes EHR-equivalent\npatient data, including physician notes and medication\nadministration data.\nFor this study, we selected history and physical notes from\nMIMIC-III for patients who did not receive an anticoagulant\n(therapeutic or prophylactic) at the time of admission. We\nselected the first 1100 history and physical notes in\nchronological time stamp order for inclusion in this study. Each\nnote described a unique patient, meaning no 2 notes had the\nsame patient identifier (subject_id). The subject_id and time\nstamp for each note included in our study were recorded and\ncan be shared upon request with proper registry for the MIMIC\ndatabase on the PhysioNet platform. Due to LLM token limits,\npatient notes were truncated to 2000 characters (RoBERTa\nmodels can receive a maximum of 512 tokens, which translates\nto roughly 2000 characters).\nThe MIMIC-III notes were then split into a training set (first\n500 notes), development set (middle 300 notes), and test set\n(final 300 notes). The training set was used to fine-tune the AI\nmodel, the development set was used to determine model\nhyperparameters (learning rate, batch number, training epochs,\nand seed number), and the test set was used for screening tool\nevaluation. Cohort information can be shared upon request with\nregistry for the MIMIC database on the PhysioNet platform.\nIn total, 2 physicians (TS and JW) reviewed all notes and labeled\neach note as either describing a patient with active bleeding or\nwithout active bleeding. Active bleeding was defined as the loss\nof blood from the vessels of the body either by documented\nvisual or imaging evidence of bleeding as well as suspected\nbleeding documented by the physician author. These labels were\nused as the gold standard labels for model evaluation. If there\nwas disagreement between labels assigned by the 2 reviewers,\nthe case was discussed to reach a final label designation.\nLanguage Model\nThe language model used in this screening tool was\nBioMed-RoBERTa [10] by AllenAI. BioMed-RoBERTa is a\nbidirectional transformer encoder based on the RoBERTa-base\nmodel published by Liu et al [11]. BioMed-RoBERTa continued\npretraining beyond RoBERTa-base using 2.68 million scientific\npapers in the domains of biology and medicine from the\nSemantic Scholar corpus [10]. Therefore BioMed-RoBERTa\nhas increased proficiency in the subjects of biology and medicine\ncompared to RoBERTa-base.\nThe optimal hyperparameter settings were identified as 9\ntraining epochs, a training batch size of 2, a learning rate of 4\n× 10–5, and a starting seed of 9. The full code can be referenced\nin Multimedia Appendix 1\nThe model performed classification for each history and physical\nnote, classifying the note as either describing active bleeding\nor no active bleeding.\nCode and Computing Environment\nModel training and evaluation were completed in a PyCharm\nnotebook using an Apple M2 GPU. Full code can be referenced\nin Multimedia Appendix 1.\nStatistical Methods\nModel classification performance was evaluated on a test set\nof 300 MIMIC patient notes. Classification performance was\nevaluated by a precision-recall area under the curve (AUC), a\nreceiver operating characteristic (ROC) AUC, sensitivity, and\nspecificity. Error was calculated using a replacement bootstraps\nmethod with 4000 bootstrapped populations from the test set.\nThe full code can be referenced in Multimedia Appendix 1.\nJMIR Med Inform 2023 | vol. 11 | e49886 | p. 2https://medinform.jmir.org/2023/1/e49886\n(page number not for citation purposes)\nSavage et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nThe statistic “increase in alert applicability” was calculated from\nequation 1 below.\nEthical Considerations\nThe MIMIC patient records database is available as an open\nrepository for credential users registered within the PhysioNet\nplatform. The patient records within the MIMIC database have\nbeen deidentified, and patient identifiers have been removed\naccording to the HIPAA (Health Insurance Portability and\nAccountability Act) Safe Harbor provision. This study was\napproved and authorized by the PhysioNet Team.\nThe collection of patient information and creation of the MIMIC\nresearch resource was reviewed by the institutional review board\nat the Beth Israel Deaconess Medical Center (protocol ID\n73104), who granted a waiver of informed consent and approved\nthe data sharing initiative. Due to the waiver of informed consent\nand anonymous nature of the MIMIC data set, patients were\nnot compensated for inclusion in the database or this research.\nResults\nThe test set of 300 MIMIC patient notes included 72 patients\nexperiencing acute bleeding and 228 patients not experiencing\nacute bleeding. The AI screening tool was able to achieve a\nprecision-recall curve AUC of 0.82 (95% CI 0.75-0.89; Figure\n1) and a ROC AUC of 0.89 (95% CI 0.84-0.94; Figure 2).\nSensitivity was found to be 67% (95% CI 55%-77%) and\nspecificity was found to be 95% (95% CI 92%-97%). Sensitivity,\nspecificity, and confusion matrix data can be found in Table 1.\nIf this model were used to identify patients for a\nthromboembolism prophylaxis BPA in place of a nonselective\nstrategy deploying an alert for all patients, this model would\nreduce the number of BPA alerts by 20% (240 alerts sent instead\nof 300) and increase applicability of the BPA by 14.8%\n(equation 1). This model misclassified 12 patients who did not\nhave bleeding and would be appropriate to receive a\nthromboembolism prophylaxis BPA. This model captured 94.7%\n(216/228) of the population who would be appropriate for an\nalert. The model achieved a positive predictive value of 80%\nand a negative predictive value of 90%. Full results can be found\nin Table 1.\nA review of notes of patients who were incorrectly classified\nfound trends where the tool underperformed. Of the 24 patients\nmisclassified as without bleeding, the model had difficulty\nidentifying bleeding if it was a secondary complaint (6 patients),\nmeaning if the patient primarily presented for another chief\ncomplaint and bleeding was incidentally noted. The model also\nhad difficulty interpreting atypical or rare abbreviations that\ndenoted bleeding (6 patients). This included recognizing\n“subdural” as an abbreviation for subdural hemorrhage (2\npatients), “EBL” as an abbreviation for estimated blood loss (2\npatients), and “SAH” as an abbreviation for subarachnoid\nhemorrhage (2 patients). For the 12 patients misclassified as\nwith bleeding, the model had difficulty with negated bleeding\nphrases (7 patients), for example “denies melena,” as well as\nnotes describing chronic anemia (2 patients) or previous\nbleeding events in the distant past that were not currently active\n(2 patients).\nFigure 1. Precision-recall curve for the BioMed-RoBERTa model. AUC: area under the curve; RoBERTa: Robustly Optimized Bidirectional Encoder\nRepresentations from Transformers Pretraining Approach.\nJMIR Med Inform 2023 | vol. 11 | e49886 | p. 3https://medinform.jmir.org/2023/1/e49886\n(page number not for citation purposes)\nSavage et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 2. ROC curve for the BioMed-RoBERTa model. AUC: area under the curve; ROC: receiver operating characteristic; RoBERTa: Robustly\nOptimized Bidirectional Encoder Representations from Transformers Pretraining Approach.\nTable 1. Confusion matrix for the BioMed-RoBERTaa model’s classification results.\nPhysician labelLLMb classification\nTotalNegativePositive\n601248Positive\n24021624Negative\n30022872Total\naRoBERTa: Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach.\nbLLM: large language model.\nDiscussion\nPrincipal Findings\nThis prototype AI screening tool shows how language models\ncan be leveraged to optimize EHR BPAs. Our example screening\ntool reduced the number of patients who would trigger an alert\nby 20% while also increasing applicability of alerts by 14.8%\ncompared to nonselectively sending alerts for all patients. The\ntool achieved impressive accuracy with a precision-recall AUC\nof 0.82 and ROC AUC of 0.89. We demonstrate how the ability\nfor language models to interpret physician notes to classify\npatients offers a new method of targeting the most appropriate\npatients for a BPA with high accuracy.\nLanguage model screening tools to target patients for BPAs are\nmost appropriate when the alert’s goal is to catch most target\npatients but the consequence of missing a few patients is not\nsignificant. As demonstrated by our example, language models\nare accurate but do occasionally misclassify patients. Example\nuse cases appropriate for an AI screening tool would be\nidentifying patients for discontinuation of telemetry monitoring,\nascribing correct level of nursing care (inpatient vs observation),\nand encouraging best prescribing practices for blood products.\nInappropriate examples would be situations such as medication\ninteractions or isolation precautions. In those cases, a\nnonselective blanket rule-based BPA would be more appropriate.\nThe limitations of the screening tool evaluated in this study\nwere its relatively small training set of 500 patient notes as well\nas the use of the BioMed-RoBERTa model rather than a larger\nmodel such as GPT-3 or GPT-4. Increasing the size of our\ntraining set would likely decrease many of the misclassification\nerrors seen by our screening tool. Specifically, the\nmisclassification errors due to misinterpretation of atypical or\nrare abbreviations for bleeding would be reduced with an\nincreased training set size. A larger base model would also\nreduce many of the errors observed by our screening tool.\nJMIR Med Inform 2023 | vol. 11 | e49886 | p. 4https://medinform.jmir.org/2023/1/e49886\n(page number not for citation purposes)\nSavage et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nRoBERTa models contain 110 million parameters trained on\n160 GB of text data [11]. Larger models, such as GPT-3, consist\nof 175 billion parameters trained on over 40 TB of text data,\nnearly 10 times the size of RoBERTa, and demonstrate superior\nperformance in negation detection and semantic understanding\n[12,13]. Therefore, a larger model would reduce the\nmisclassification errors caused by misinterpretation of negated\nbleeding or previous bleeding events in the distant past. Our\nstudy chose BioMed-RoBERTa because its smaller size allows\nit to be trained on a local computing environment compliant\nwith the HIPAA 1996 data privacy standards. Future\ninvestigations will need to secure the necessary data use\nagreements to use larger models (eg, GPT-3, GPT-4, or\nPathways Language Model) with medical grade data, where we\nanticipate screening tool performance will be significantly\nimproved.\nConclusions\nIn this paper, we proposed a new application for LLMs in\nmedicine. Quality improvement BPAs can leverage LLMs to\nread physician notes and better identify patients for BPA alerts.\nWe provide an example case which demonstrates the ability of\na BioMed-RoBERTa to achieve impressive classification\naccuracy. We anticipate that as the field of clinical natural\nlanguage processing continues to grow, with increasing access\nto larger language models, LLMs will revolutionize the field\nof clinical quality improvement.\nData Availability\nThe Python code used for this investigation is available in Multimedia Appendix 1. MIMIC (Medical Information Mart for\nIntensive Care) patient data is an open repository requiring registration with the PhysioNet platform. Cohort information of the\nhistory and physical notes used to train, develop, and test our screening tool in can be shared upon request with proper registry\nfor the MIMIC database on the PhysioNet platform.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nPython code for training and evaluating the large language model (Robustly Optimized Bidirectional Encoder Representations\nfrom Transformers Pretraining Approach; RoBERTa) screening tool used in this investigation.\n[PDF File (Adobe PDF File), 46 KB-Multimedia Appendix 1]\nReferences\n1. Ancker JS, Kern LM, Edwards A, Nosal S, Stein DM, Hauser D, et al. Associations between healthcare quality and use of\nelectronic health record functions in ambulatory care. J Am Med Inform Assoc 2015;22(4):864-871 [FREE Full text] [doi:\n10.1093/jamia/ocv030] [Medline: 25896648]\n2. Ancker JS, Edwards A, Nosal S, Hauser D, Mauer E, Kaushal R, et al. Effects of workload, work complexity, and repeated\nalerts on alert fatigue in a clinical decision support system. BMC Med Inform Decis Mak 2017;17(1):36 [FREE Full text]\n[doi: 10.1186/s12911-017-0430-8] [Medline: 28395667]\n3. Mitka M. Joint commission warns of alarm fatigue: multitude of alarms from monitoring devices problematic. JAMA\n2013;309(22):2315-2316 [doi: 10.1001/jama.2013.6032] [Medline: 23757063]\n4. van der Sijs H, Aarts J, Vulto A, Berg M. Overriding of drug safety alerts in computerized physician order entry. J Am\nMed Inform Assoc 2006;13(2):138-147 [FREE Full text] [doi: 10.1197/jamia.M1809] [Medline: 16357358]\n5. Jaspers T, van Essenberg MD, Maat B, Durian M, van den Berg R, van den Bemt P. A multifaceted clinical decision support\nintervention to improve adherence to thromboprophylaxis guidelines. Int J Clin Pharm 2021;43(5):1327-1336 [FREE Full\ntext] [doi: 10.1007/s11096-021-01254-x] [Medline: 33709383]\n6. Spirk D, Stuck AK, Hager A, Engelberger RP, Aujesky D, Kucher N. Electronic alert system for improving appropriate\nthromboprophylaxis in hospitalized medical patients: a randomized controlled trial. J Thromb Haemost\n2017;15(11):2138-2146 [FREE Full text] [doi: 10.1111/jth.13812] [Medline: 28836340]\n7. Johnson A, Pollard T, Mark R. MIMIC-III clinical database. PhysioNet. 2015. URL: https://doi.org/10.13026/C2XW26\n[accessed 2023-11-07]\n8. Johnson AEW, Pollard TJ, Shen L, Lehman LWH, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible critical care\ndatabase. Sci Data 2016;3:160035 [FREE Full text] [doi: 10.1038/sdata.2016.35] [Medline: 27219127]\n9. Goldberger AL, Amaral LA, Glass L, Hausdorff JM, Ivanov PC, Mark RG, et al. PhysioBank, PhysioToolkit, and PhysioNet:\ncomponents of a new research resource for complex physiologic signals. Circulation 2000;101(23):E215-E220 [FREE Full\ntext] [doi: 10.1161/01.cir.101.23.e215] [Medline: 10851218]\n10. allenai/biomed_roberta_base. Hugging Face. 2023. URL: https://huggingface.co/allenai/biomed_roberta_base [accessed\n2023-07-08]\n11. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. RoBERTa: a robustly optimized BERT pretraining approach. arXiv\n:1-13 Preprint posted online on July 26, 2019. [FREE Full text] [doi: 10.48550/arXiv.1907.11692]\nJMIR Med Inform 2023 | vol. 11 | e49886 | p. 5https://medinform.jmir.org/2023/1/e49886\n(page number not for citation purposes)\nSavage et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n12. Nguyen HT, Goebel R, Toni F, Stathis K, Satoh K. A negation detection assessment of GPTs: analysis with the xNot360\ndataset. arXiv :1-8 Preprint posted online on June 29, 2023 [FREE Full text] [doi: 10.48550/arXiv.2306.16638]\n13. Tejada GNC, Scholtes J, Spanakis G. A study of BERT's processing of negations to determine sentiment. 2021 Presented\nat: 33rd Benelux Conference on Artificial Intelligence and 30th Belgian-Dutch Conference on Machine Learning; November\n10-12, 2021; Belval, Esch-sur-Alzette, Luxembourg p. 47-59 URL: https://dke.maastrichtuniversity.nl/jerry.spanakis/\nwp-content/uploads/2021/11/NegationBERT.pdf\nAbbreviations\nAI: artificial intelligence\nAUC: area under the curve\nBERT: Bidirectional Encoder Representations from Transformers\nBPA: Best Practice Alert\nDVT: deep vein thrombosis\nEHR: electronic health record\nGPT: Generative Pre-trained Transformers\nHIPAA: Health Insurance Portability and Accountability Act\nLLM: large language model\nMIMIC: Medical Information Mart for Intensive Care\nRoBERTa: Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach\nROC: receiver operating characteristic\nEdited by C Lovis; submitted 12.06.23; peer-reviewed by R Marshall, J Zeng, J Kors; comments to author 06.07.23; revised version\nreceived 04.08.23; accepted 24.10.23; published 27.11.23\nPlease cite as:\nSavage T, Wang J, Shieh L\nA Large Language Model Screening Tool to Target Patients for Best Practice Alerts: Development and Validation\nJMIR Med Inform 2023;11:e49886\nURL: https://medinform.jmir.org/2023/1/e49886\ndoi: 10.2196/49886\nPMID: 38010803\n©Thomas Savage, John Wang, Lisa Shieh. Originally published in JMIR Medical Informatics (https://medinform.jmir.org),\n27.11.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution License\n(https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium,\nprovided the original work, first published in JMIR Medical Informatics, is properly cited. The complete bibliographic information,\na link to the original publication on https://medinform.jmir.org/, as well as this copyright and license information must be included.\nJMIR Med Inform 2023 | vol. 11 | e49886 | p. 6https://medinform.jmir.org/2023/1/e49886\n(page number not for citation purposes)\nSavage et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX"
}