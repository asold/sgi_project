{
  "title": "A Study of AI Agent Commitment in One Night Ultimate Werewolf with Human Players",
  "url": "https://openalex.org/W2982438303",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2267562910",
      "name": "Markus Eger",
      "affiliations": [
        "Universidad de Costa Rica"
      ]
    },
    {
      "id": "https://openalex.org/A2548689189",
      "name": "Chris Martens",
      "affiliations": [
        "North Carolina State University"
      ]
    },
    {
      "id": "https://openalex.org/A2548689189",
      "name": "Chris Martens",
      "affiliations": [
        "North Carolina State University",
        "North Central State College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2147056604",
    "https://openalex.org/W2560085463",
    "https://openalex.org/W6768334682",
    "https://openalex.org/W6633793156",
    "https://openalex.org/W2148055514",
    "https://openalex.org/W7008050527",
    "https://openalex.org/W6717771874",
    "https://openalex.org/W2757948894",
    "https://openalex.org/W2902080569",
    "https://openalex.org/W2608340639",
    "https://openalex.org/W2732027414",
    "https://openalex.org/W6676260131",
    "https://openalex.org/W2588001287",
    "https://openalex.org/W6635868001",
    "https://openalex.org/W1488819970",
    "https://openalex.org/W6634710360",
    "https://openalex.org/W2108488400",
    "https://openalex.org/W1601254986",
    "https://openalex.org/W2431139695",
    "https://openalex.org/W2159131252",
    "https://openalex.org/W4210523982",
    "https://openalex.org/W1561662057"
  ],
  "abstract": "Social deduction games are a genre of board games in which a group of players is secretly assigned roles and each player tries to determine the other players’ roles. However, some roles have an incentive to not be found, and the games typically allow players to lie freely. Playing such games is a challenging task for AI agents, because they need to not only determine the probability that each statement made by the other players is truthful, but also come up with convincing lies themselves. In this paper, we present AI agents designed to play one particular such game, One Night Ultimate Werewolf, with human players. We discuss the different deliberation strategies our agents use to determine what they should say, and when they should change their plan. To determine how these different deliberation strategies are perceived by human players, we performed an experiment in which participants played a Unity implementation of the game with each of the three deliberation strategies. We present the results of this experiment, which show that commitment to plans has a measurable effect on player perception and provide a trade-off between consistency and potential for high performance of the agent.",
  "full_text": "Proceedings of the Fifteenth AAAI Conference on Artiﬁcial\nIntelligence and Interactive Digital Entertainment (AIIDE-19)\nA Study of AI Agent Commitment in\nOne Night Ultimate Werewolf with Human Players\nMarkus Eger\nUniversidad de Costa Rica\nmarkus.eger@ucr.ac.cr\nChris Martens\nNC State University, Raleigh, NC, USA\ncrmarten@ncsu.edu\nAbstract\nSocial deduction games are a genre of board games in which\na group of players is secretly assigned roles and each player\ntries to determine the other players’ roles. However, some\nroles have an incentive to not be found, and the games typ-\nically allow players to lie freely. Playing such games is a\nchallenging task for AI agents, because they need to not only\ndetermine the probability that each statement made by the\nother players is truthful, but also come up with convincing\nlies themselves. In this paper, we present AI agents designed\nto play one particular such game, One Night Ultimate Were-\nwolf, with human players. We discuss the different delibera-\ntion strategies our agents use to determine what they should\nsay, and when they should change their plan. To determine\nhow these different deliberation strategies are perceived by\nhuman players, we performed an experiment in which partic-\nipants played a Unity implementation of the game with each\nof the three deliberation strategies. We present the results of\nthis experiment, which show that commitment to plans has a\nmeasurable effect on player perception and provide a trade-\noff between consistency and potential for high performance\nof the agent.\nIntroduction\nGames which feature communication with human players\nprovide a unique challenge for game AI researchers, be-\ncause the idiosyncrasies of human communication need to\nbe taken into account in order to play the games well. One\ngenre of games in which communication plays a central role\nis that of social deduction games, such as Maﬁa/Werewolf\n1.\nIn these games, every player is secretly assigned to one\nof two factions (Maﬁa and Citizens in Maﬁa, Werewolves\nand Villagers in Werewolf), where one group has an in-\nformation advantage, and the other has a numerical ad-\nvantage. Typically, about one third of the players are as-\nsigned to the Maﬁa/Werewolf faction, and while roles are\nassigned secretly, the Maﬁa/Werewolf players are told which\nCopyright c⃝ 2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1Historically, Maﬁa was invented by Dmitry Davidoff in 1986,\nand several variants of the game were developed over the years,\nincluding Werewolf in the 1990s, which simply replaces the theme\nbut is otherwise identical to the original game\nother players are on the same faction as them. The goal\nfor the Citizen/Villager faction is to deduce who is on\nthe Maﬁa/Werewolf faction, and vote them out, while the\nMaﬁa/Werewolf players try to stay undetected. To facili-\ntate this, there are certain bonus roles, such as a Seer char-\nacter on the Citizen/Villager team which can learn which\nteam other players are on. The key mechanic, however, is\nfree-form communication, in which players can accuse other\nplayers of being on a certain side, exchange information they\nmay have, such as what they have learned using their special\nroles, or sow confusion, since no player is bound to tell the\ntruth at any point.\nBecause of the elements of knowledge gathering and\nexchange, as well as lies and deception, social deduction\ngames provide an interesting application for game AI. While\nthere are many aspects that go into the game, including how\nhuman players behave, and which non-verbal cues they ex-\nhibit, when they are attempting to deceive other players, our\nfocus lies on the information exchange part of the game,\nand how it can be modeled by an AI agent. There are sev-\neral games in the Maﬁa/Werewolf family, of which we have\nchosen to work with One Night Ultimate Werewolf. This\nvariant differs from the main game in that there are several\nroles that not only can gather information, but also manip-\nulate the faction-afﬁliation of other players, usually without\ntheir knowledge. This gives rise to several interesting rea-\nsoning phenomena. Over the course of the game, the agent\nmust constantly reevaluate which faction they are on, and if\nit serves their interests to stick to one story, or change their\nbehavior in the light of new, potentially faulty, information.\nAdditionally, other players may question their statements,\nwhich may require the agent to reinforce or explain their be-\nhavior. At the end of the game, each player gets one vote to\ndetermine who to vote out, for which the agent has to deter-\nmine which other player is most likely to be on the opposing\nfaction.\nIn this paper, we present an AI agent design for play-\ning One Night Ultimate Werewolf with a human player. The\nagents utilize Dynamic Epistemic Logic to reason about the\ncommunicative actions of the player, is able to answer the\nplayer’s questions if it ﬁts with their own plan, and will at-\ntempt to deduce their own and the other players’ faction af-\n139\nﬁliation and vote accordingly. Our agents perform reasoning\nabout communicative goals they have, and how long they\nshould be committed to their goal, depending on what they\nlearn from the other players. To determine the effect differ-\nent levels of commitment on the perception of the agents by\nhuman players, we performed an online experiment where\nparticipants were asked to play a Unity implementation of\nOne Night Ultimate Werewolf with three different agent\ntypes. We provide a summary and interpretation of the most\ninteresting results, and their implications for future work.\nOne Night Ultimate Werewolf\nOne Night Ultimate Werewolf (Alspach and Okui 2014) is\na Social Deduction game for 5 to 10 players, in which some\nplayers are assigned to a Werewolf faction and others to a\nVillage faction. At the beginning of the game, the role cards\nare shufﬂed and one dealt to each player, with three extra\ncards dealt to the center of the table, as shown in Figure 1.\nSome players on the Village faction may have a role with\nspecial abilities that allows them to look at and/or exchange\nother players’ role cards, or interact with the center cards,\nwhile others are Villagers without any special ability. When\nthe cards have been dealt, all players may look at their own\ncard once, and game play then consists of three phases:\n• A night phase, in which all roles with special abilities\nperform their special action in a predeﬁned order. In the\ntabletop version of the game, this is done by all play-\ners closing their eyes, and a game master (or smartphone\napp) calling the roles in a predeﬁned order. When a role is\ncalled, the player or players with that role open their eyes\n(referred to as “waking up”) and perform their special\naction, and then close their eyes again. While a player’s\nrole cards may change over the course of the night, they\nalways perform the action associated with the role they\nstarted with.\n• A day phase, in which players discuss their own faction\nafﬁliation, information or suspicions they may have about\nother player’s faction afﬁliation, or inquire about any in-\nformation they believe another player might have. For any\nof these exchanges, players are free to lie partially or fully.\nIn the tabletop version, the day phase is limited to a cer-\ntain time, which is typically around 5 minutes.\n• A voting phase, in which all players simultaneously vote\nfor another player. If any player gets more than one vote,\nthe player or players with the most votes are voted out.\nIf at least one Werewolf is voted out this way, the Vil-\nlage faction wins the game, otherwise the Werewolf fac-\ntion wins. Players win or lose with the faction that cor-\nresponds to the role card that they have at the end of the\ngame, which may be different than the one they started\nwith.\nThe game, and its various expansions, provide about 80 dif-\nferent role cards, of which we will only describe the ones\nsupported in our implementation of the game:\n• The Werewolves, which should be about one third of the\nplayers, all wake up together, so each of the Werewolves\nknows who the other Werewolves are.\nFigure 1: Game setup for One Night Ultimate Werewolf. For\nan actual game, the role cards for each player, as well as the\ncenter cards, are dealt face-down.\n• The Seer wakes up and can look at another player’s card,\nor two of the center cards.\n• The Rascal wakes up, and may exchange their two neigh-\nbors’ cards.\n• The Robber wakes up and may take any other player’s\ncard and exchange it with their own, and look at their new\ncard.\n• The Insomniac wakes up and looks at their own card.\nThe challenge the game poses to the players is that they con-\nstantly have to question their own faction afﬁliation as well\nas the statements of the other players. For example, if Anna\nstarts as the Werewolf, Brian, who is the Rascal, sits to their\nright, and Carol, who is the Insomniac, is on other side of the\nRascal, the Rascal may exchange the Werewolf player’s card\nwith the Insomniac’s card. This exchange would cause Anna\nto be on the Villager team, and Carol to be on the Werewolf\nteam. During the day phase, when Brian claims to be the\nRascal and to have swapped his two neighbors’ cards, Anna\nneeds to consider whether to believe Brian or not. If they\nbelieve Brian, they might state that they started as the Were-\nwolf, which, in turn, the other players have to evaluate for its\nbelievability. However, if Dave started as the Seer, he may\nhave additional information, perhaps because he looked at\nAnna’s card. Generally speaking, the game works because\nthe Werewolves constitute a minority among the players,\nwhich means the Villager faction can collectively determine\nwho the Werewolves are, if they manage to corroborate each\nother’s stories. We will now present some approaches to AI\nagents for social deduction games.\nRelated Work\nSocial deduction games provide many interesting challenges\nfor AI agents, and several researchers have looked into dif-\nferent aspects. While our work was on One Night Ultimate\nWerewolf, most work on its ancestors Maﬁa and Werewolf\nis just as applicable. For example, Chittaranjan and Hung\n(2010) have investigated how to use pitch and tone of voice\nto determine whether players are lying about their roles. For\nour work we eschew direct interaction with verbal commu-\n140\nnication in favor of ﬁxed statements, in order to be able to\nfocus on the reasoning about beliefs caused by the content\nof the communicative actions. Gillespie et al. (2016) have\nshown that the statements made by players typically fall into\na set of semantic categories, and Hancock et al. (2017) de-\nscribe how these statements are formed into plans that may\nor may not have deception as a goal. In contrast, our work\nutilizes a measure of plan quality to move beyond a binary\nnotion of deception towards a measure of how much a plan\nmay convince other people of a statement. Other researchers\nhave looked into developing actual strategies for Werewolf,\nsuch as Braverman et al. (2008), who use a probabilistic\nmodel to determine which setups lead to fair games. How-\never, they utilize a simpliﬁed version of the game, which ac-\ntually removes most communication from the game, in order\nto be able to derive theoretically optimal strategies. Bi and\nTanaka (2016) describe how one of these strategies can be\nexploited by the villagers, if the Werewolf players are as-\nsumed to behave exactly like Villagers without any special\nrole. Finally, Nakamura et al. (2016) describe an approach to\nWerewolf in which an AI agent forms a model of the beliefs\nof the other players in order to deduce probabilities for their\nroles. However, their model requires expert players to pro-\nvide estimates for probabilities for different combinations of\nroles and actions.\nOur work is mainly based on a formulation of intention-\nality by Cohen and Levesque (1990), based on prior work\nby Bratman (1990). They describe intention as choice with\ncommitment, which means that agents make decisions about\nwhich goals to pursue, form plans to achieve them, and then\nreevaluate how long to follow each plan, and when to adopt\na new goal. However, the problem of when to maintain and\nwhen to drop intentions is non-trivial (Rao and Georgeff\n1995), and therefore several different approaches have been\nproposed. As Braubach et al. (2004) discuss, one challenge\nis that goals are often talked about in abstract terms, leaving\na gap between the theory of adopting, pursuing and dropping\nthem, and the actual implementation. To address this, their\nsystem uses a classiﬁcation of goals into different categories\nand a formalism to describe different goals, but they also\nleave the actual deliberation strategy open for future work.\nPokahr et al. (2005) build on this work and present what\nthey call the Easy Deliberation Strategy that uses the in-\nformation contained within goals to choose between them,\nwithout regard for the actual plans. Mohanty et al. (1997)\ndescribe a different approach, in which an agent is inﬂu-\nenced into adopting plans by other agents, which is more\nuseful in social settings, such as an employee taking orders\nfrom their supervisor. Perhaps the most concrete agent de-\nsign, that combines goal selection with a reconsideration-\nmechanism was described by Wooldridge (2000). His agents\nhave a notion of beliefs, which they use to determine which\nintentions to adopt, but over the course of plan execution the\nagent’s beliefs may change, which causes them to reconsider\ntheir intentions. Our work expands upon this by incorporat-\ning not just agent beliefs, but also beliefs about the beliefs\nof other agents using Baltag’s variant of Dynamic Epistemic\nLogic (Baltag 2002), using our implementation Ostari (Eger\nand Martens 2017). Ostari is implemented in Haskell, and\nallows the speciﬁcation of actions that affect agent beliefs,\nand also provides the capabilities to plan to achieve goals in-\nvolving such beliefs. We have previously implemented One\nNight Ultimate Werewolf in Ostari (Eger and Martens 2018),\nand the present work is a direct extension of this effort, as\ndescribed in the next section.\nApproach\nOur approach is an extension of previous work we presented\nwhich focused on comparing AI agents for One Night Ulti-\nmate Werewolf that played simulated games with each other\nto determine the effect of different levels of commitments to\nplans (Eger and Martens 2018). We will brieﬂy summarize\nhow these agents operated, to be able to discuss the changes\nwe made in order to enable the agents to play with a hu-\nman player. We will also brieﬂy present the architecture that\nconstitutes the connection between our Unity front-end and\nthe Dynamic Epistemic Reasoning system Ostari (Eger and\nMartens 2017) on the back-end.\nNote that our implementation of One Night Ultimate\nWerewolf made two key changes from the tabletop version:\nFirst, the discussion phase is performed in a turn-based fash-\nion, with a ﬁxed limit of 7 turns, where each player can per-\nform up to one communicative action per turn, instead of\nthe time limit present in the tabletop version. Second, while\nthe tabletop version allows players to make arbitrary state-\nments, our implementation provides the players with a ﬁxed\nset of statements, that allows them to discuss any aspect of\nthe game state. We made these changes since natural lan-\nguage processing was beyond the scope of our work, and to\nbe able to focus on the planning aspects of the game.\nExisting Agent Design\nTo play One Night Ultimate Werewolf, we built agents\nthat utilize Dynamic Epistemic Logic (Van Ditmarsch, van\nDer Hoek, and Kooi 2007) to represent communicative ac-\ntions, and then reason about how to best reach a goal using\nthese actions. In Dynamic Epistemic Logic, beliefs are rep-\nresented as possible worlds, meaning that each combination\nof facts an agent considers possible constitutes one world.\nIn a game like One Night Ultimate Werewolf, shufﬂing the\ncards creates one possible world per card permutation for\neach agent, because the agents are unaware of the order of\nthe cards. When an agent looks at a card, they eliminate all\nworlds that are inconsistent with the card they saw. On the\nother hand, actions that happen without an agent being aware\nof its exact parameters, such as another player possibly ex-\nchanging two cards, cause the agent to add possible worlds\nfor each variation of the action the agent considers possible.\nHowever, communicative actions do neither remove worlds,\nsince they do not constitute certain knowledge about the va-\nlidity of any particular worlds, nor do they add worlds, since\nthe agent must already consider what is said possible be-\nforehand to believe the speaker. Instead, our agents use the\nobservation that the Village team has an interest in telling\nthe truth, and since they constitute the majority of the play-\ners the majority of statements in a typical game should be\ntruthful. Whenever a statement is made, our agent marks\n141\nall worlds they consider possible that would be inconsistent\nwith that statement as being less likely. The aggregate ef-\nfect over several statements is that worlds that are consistent\nwith more statements are considered more likely than worlds\nthat would contradict more statements made by the players.\nWe call this measure of how likely worlds are the weight of\neach world, and use it to calculate a weighted quality of be-\nliefs that depends on how many worlds a statement holds in,\nand which weights these worlds have, compared to the sum\nof weights of all worlds.\nThe agents use this weighted quality of beliefs to form\nplans involving communicative actions, that they believe to\nchange the weighted quality of beliefs of other agents ac-\ncording to their goals. For example, a player that believes\nthat they are on the Werewolf faction may have a goal to\nconvince other players that they are a Villager. Our agents\nrepresent this goal as trying to change the weighted quality\nof another player’s belief that the agent is a Villager above\na threshold value. The quality of a plan consisting of a se-\nquence of actions can then be deﬁned as how well it achieves\nthat goal, i.e. what the resulting weighted quality of the other\nplayer’s belief will be after the plan is executed. Our agents\nuse a list of expert-provided candidate goals, ﬁnd the highest\nquality plan to achieve each goal, and then adopt the goal\nwhich can be achieved with the highest quality. On subse-\nquent turns, the agent will reevaluate their decision, by com-\nputing new plans for each goal, and then switch to that new\nplan depending on the deliberation strategy. For this project,\nwe used three different deliberation strategies, called capri-\ncious, balanced and fanatical. A capricious agent will disre-\ngard their existing plan’s quality and always adopt the new\nplan with the highest quality, while a balanced agent will\nonly adopt a new plan if its current quality is higher than\nthe quality of the existing plan as evaluated when the plan\nwas adopted. A fanatical agent, ﬁnally, will never reevalu-\nate plans, and always keep pursuing the plan they initially\nadopted until it is ﬁnished. We will now describe the adapta-\ntions we made to this agent design in order to allow a human\nplayer to play with the agents.\nBecause the agent can never be certain to convince an-\nother player of anything with certainty, adding more com-\nmunicative actions to reinforce their statements always lead\nto a better plan. The fanatical agent will therefore always\ncome up with a plan for the entirety of the game, and never\nchange from it. The capricious agent, on the other hand, will\nreevaluate their plan every turn, but often come up with ex-\nactly the same plan they were already following. In a typical\ngame, though, the capricious agent will change their plan be-\ntween 2 and 4 times. The balanced agent we used for our ex-\nperiment falls between these two extremes, and will change\ntheir plan between 1 and 2 times on average during a game.\nPlaying One Night Ultimate Werewolf with Human\nPlayers\nThe biggest change from having agents play One Night Ul-\ntimate Werewolf with each other to having them play with a\nhuman player has to do with interactivity. While the agents\nthemselves will form plans to convey information to the\nother players, a human player may want to inquire about de-\ntails, or is otherwise not satisﬁed with the information vol-\nunteered by the agents. In order to address this, we made two\nchanges to our system. First, in order to be able to actually\nask something, we added communicative actions that repre-\nsent questions. Second, we also modiﬁed the agents to take\nthese questions into account when performing their commu-\nnicative actions. However, while we want the agents to be re-\nsponsive, we also do not want the human player to be able to\nexplicitly control their behavior. We addressed this concern\nby integrating question answering with our existing goal de-\nliberation process.\nQuestions, unlike other communicative actions, do not\nchange the listeners belief state, but instead attempt to\nchange their plan. Question actions in our encoding of the\ngame reﬂect this by setting a question topic for the player\nthe question was directed at, and other communicative ac-\ntions are tagged with the question topic they address. While\nthe question topic is maintained per player, its value is actu-\nally public, since all communicative actions, including ques-\ntions, are overheard by all players. When it is an agent’s turn,\nand they are about to deliberate on which new plan (if any)\nto adopt, they now take additional goals into account: For\neach existing candidate goal, the agent will also consider an\nadditional candidate goal that has the additional condition\nof addressing their current question topic. When a plan is\nformed, it may either be in service of the provided candidate\ngoals, or also include communicative actions that address\nthe agent’s question topic.\nConsider the case where an agent Anna forms a plan to\nconvince the other players that they are the Villager, but they\nwere just asked which role they believe Brian has. When\nAnna forms a plan to convince the other players of being\na Villager, perhaps by simply stating they they are a Vil-\nlager, they may also add actions that state that they believe\nBrian to be a Werewolf. Because this plan would addresses\nAnna’s original goal as well as the goal of addressing the\nquestion posed to her, it would always have a higher qual-\nity than a plan without the additional communicative action\nabout Brian’s role. This means, if we just added conditions\nto all goals, the agents would always prefer to answer ques-\ntions. However, we want the question answering to be a nat-\nural part of the agent’s plan, instead of additional actions\nthey add to the end of another plan.\nBecause the game is played in turns, with a limit of 7\nturns, and one action per turn, we limited the agents to con-\nstructing plans of a maximum length equal to the number of\nturns remaining. With this plan length restriction, the agents\nhave to decide between plans that address their actual goals\nbetter and plans that include answering questions that were\nposed to them. Since the agents can never actually execute\nplans that are longer than the remaining turns of the game,\nthis restriction actually results in better ﬁtting plans, and has\nthe added beneﬁt of reducing the planning time of the agents.\nSystem Design\nIn order to let humans actually play the game, we developed\na Unity front-end, shown in Figure 2, that was exported as\na WebGL build playable in a webbrowser. Since the actual\ngame logic was implemented in Ostari, running as a Haskell-\n142\nFigure 2: A screenshot of the user interface for the human\nplayer.\nprocess on our game server, we developed a web-server in\npython to serve as the bridge between the front-end and the\nOstari back-end. When a player starts the game, this server\ngenerates the initial role assignment, and sets up the appro-\npriate agents, generating an Ostari input ﬁle. It then starts\nthe Ostari process, and connects to it via pipes. When the\nplayer performs an action in the Unity WebGL application, a\nHTTP request is sent to the web server, which generates the\nnecessary input to perform that action in Ostari and passes it\nvia the input pipe. It then interprets the resulting output and\nsends the result inside the HTTP response. This setup makes\nthe game more enjoyable for the human player, by providing\nthem with a nice graphical user interface, while still allowing\nus to use the powerful Dynamic Epistemic Logic reasoning\ncapabilities provided by Ostari. We used this setup to run\nan evaluation of our agents with human players, as we will\ndescribe now.\nResults\nIn this section, we will describe how we evaluated the ef-\nfect of different levels of commitment on how human play-\ners perceived and rated our AI agents. First, we will provide\na description of the experiment design and setup, and then\nfollow with a discussion of the most important results.\nExperiment Design\nFor our experiment, we compared three different agent\ntypes: Capricious agents, which decide which plan to fol-\nlow every turn afresh, fanatical agents, which follow their\ninitial plan until it is ﬁnished, and balanced agents, which\nuse the measure of plan quality to decide when to change\nplans. In order to compare the agents, we asked participants\nto play three games, where each game was played with a dif-\nferent agent type, assigned in random order. Each game was\nplayed with one human player and 4 AI agents, where each\nAI agent was assigned the same agent type.\nWe also limited the starting assignment of role cards to\none of three scenarios: One in which the participant started\nas a Werewolf, but the Werewolf role could be taken away\nfrom them, one in which the participant started as the Seer,\nbut could become a Werewolf, and one in which the par-\nticipant started as the Rascal, with a Werewolf to their left,\nand the Insomniac to their right, so their decision on whether\nor not to change their neighbors’ cards would affect which\nplayer would be the Werewolf. As with the agent type, these\nthree starting conﬁgurations were assigned in random order.\nAfter each game, each participant was asked a series of sur-\nvey questions:\n• Whether the AI agents changed their behavior based on\nthe player’s actions.\n• If the AI agents’ actions made sense.\n• If the AI agents played well.\n• If it was fun to play with the AI agents.\n• Which of the statements of one of the AI controlled play-\ners they believed at the time they were made.\n• Which of the statements of one of the AI controlled play-\ners they considered to have made sense at the time they\nwere made.\nThe ﬁrst four questions were rated on a 5-point Likert scale,\nwhile for the last two actions participants were shown the\nstatements in question and asked to select all that applied.\nAdditionally, participants could give free-form text feed-\nback about the agents’ behavior after each game. After three\ngames, participants were additionally asked for some basic\ndemographic information, such as age, board game experi-\nence in general, and with One Night Ultimate Werewolf in\nparticular, as well as their estimate for which percentage of\ngames is won by the Werewolf faction in a regular game of\nOne Night Ultimate Werewolf.\nExperiment Results\nParticipants for the experiment (NCSU IRB # 14087) were\nrecruited on social media, including boardgamegeek.com\nand the boardgames subreddit, and the experiment was run\nonline for 2 weeks. In this time, 71 participants ﬁnished at\nleast one game and answered the survey questions corre-\nsponding to that game. As expected, the population skewed\ntowards participants with an afﬁnity for board games, with\n42 self-identifying as gamers, and only 5 not, with the rest\nchoosing to not answer the question. For each of the six\nsurvey questions, we performed a Mann-Whitney-Wilcoxon\ntest to test if the response for an AI agent type could be ex-\npected to be higher than for the other two, and a χ\n2 test to\ntest for a difference in distribution between the three differ-\nent agent types. A Holm-Bonferroni correction was used to\naccount for multiple testing. We also qualitatively analyzed\nthe free-form comments from the participants.\nInterestingly, while the means were not found to be sta-\ntistically signiﬁcantly different for any of the survey ques-\ntions, several questions showed a difference in distribution.\nThe number of statements players rated as making strategic\nsense differed between the balanced and the fanatical agent\n(p< 0.001, χ\n2 =2 9.23), as well as between the balanced\nand the capricious agent (p =0 .002707, χ2 =2 1.84). Fig-\nure 3 shows a histogram of how many participants rated0, 1,\n2, etc. statements made by each agent type as making sense.\n143\nAs the statistical tests tell us, the distribution of how many\nstatements are considered to make sense differ between the\nagent types, with the balanced agent having signiﬁcantly\nfewer games in which none of its statements made sense,\nand several where 5 or more did. On the other hand, the fa-\nnatical agent had few games in which5 or 6 of its statements\nmade sense, but more in which all 7 of its statements were\nrated as making strategic sense. The capricious agent, on the\nother hand, has very few games in which all of its statements\nmade strategic sense.\nThese results are in line with our expectations, since the\nfanatical agent makes up a plan at the beginning of the dis-\ncussion phase and then keeps following it to the end. In\ngames in which nothing contradicts the facts conveyed by\nthat plan it will make perfect sense, but this is not always\nthe case. On the other hand, the capricious agent may change\ntheir story at any time, making it too erratic, causing the hu-\nman player to ﬁnd statements to be inconsistent. The number\nof statements players said they believed was also statistically\nsigniﬁcantly different between the fanatical and the balanced\nagent in distribution (p< 0.00001, χ\n2 =3 9.3), with a sim-\nilar bimodal distribution for the fanatical agent, where the\nparticipants believed either all or none of the statements, and\na more balanced distribution for the balanced agent, where\nthere were fewer games in which none of the statements\nwere believed, but also fewer in which all of them were.\nAnother interesting result was how players rated the skill\nof the AI agents, as shown in Figure 4. While the χ\n2 test re-\nvealed a difference in distribution between the fanatical and\nbalanced agent (p =0 .003416, χ\n2 =1 5.722), as well as be-\ntween the capricious and the balanced agent (p =0 .006486,\nχ2 =1 4.27), these results should not be taken as statistically\nsigniﬁcant due to the necessary Holm- Bonferroni correc-\ntion. However, since the Holm-Bonferroni correction does\nnot make any assumptions about the dependence structure\nof the different tests, it is known to lead to type II errors in\ncases where the different responses are positively correlated\n(Abdi 2007). We believe that such a correlation is likely, and\nthat further experiments are needed to get more conclusive\nresults.\nQualitatively, we also looked at the free form text re-\nsponses the participants provided. Because of the higher ef-\nfort required to provide such feedback, only between21 and\n26 provided answers for each agent type. Several themes\nwere reoccurring across multiple answers, including a frus-\ntration that the AI agents did not respond to questions, which\n3 participants noted for the capricious agent, 4 for the bal-\nanced agent and 8 for the fanatical agent, with one partici-\npant writing They do not respond to questions well if at all.\nIt would be more fun to play with them if it felt like they\ncould react to what I asked.after their game with the fanati-\ncal agent. Another particularly interesting theme was that the\nvoting behavior of the AI agents confused the participants,\nwith 9 participants noting this for the capricious agent, 7 for\nthe balanced agent, and 1 for the fanatical agent. We believe\nthis is due to the fact that the agents perform their voting\npurely based on their estimate on the game state, which they\ndo not fully communicate to the player.\nFigure 3: Histogram of the number of statements the partic-\nipants rated as making strategic sense for each of the agent\ntypes.\nFigure 4: Participant rating for the skill of the AI agents from\n−2: very bad, to 2: very good.\nConclusion and Future Work\nWe have presented agents that play the social deduction\ngame One Night Ultimate Werewolf with human players,\nbased on our previous work. The main changes over pre-\nvious work are enabling of question answering in order to\nmake the agents more interactive for the human players, and\nproviding a graphical user interface implemented in Unity.\nWe used this user interface to perform an experiment to\ndetermine how human players evaluate the commitment of\nagents to their plans. While the participants did not ﬁnd the\nagents to behave differently on average, there were interest-\ning differences in the distribution of the ratings. In particu-\nlar, agents that would commit to a goal fanatically and never\nchange their plans were perceived as either very bad or as\nvery good, while agents that more carefully decided when\nto change plans had fewer exceptionally bad games, but also\nfewer exceptionally good ones. This provides additional ev-\nidence that agent commitment matters to the perception of\nhuman players, but also shows that a high commitment can\nlead to potentially achieving highly desirable results, at the\nincreased risk of failing badly.\nThere are several opportunities to improve upon our work.\n144\nWhile our agents provide different options for trade-offs,\nand use a wide variety of expert-provided goals, the actual\nstrategic trade-offs presented by the game have not been\ndetermined yet. We believe our action encoding could be\nused to analyze the game more formally in the future. Ad-\nditionally, while most players found our user interface intu-\nitive, it simpliﬁes the game in some ways. Adding the capa-\nbilities for audio-processing and free communication could\ngreatly increase how engaging the game is perceived to be,\nbut would require signiﬁcant effort that was out of scope for\nour work. One participant also noted that the lack of non-\nverbal cues constituted a real limitation, since they like to\nuse facial expressions, as well as voice patterns to determine\nthe truthfulness of statements.\nAcknowledgements\nThis material is based upon work supported in whole or in\npart with funding from the Laboratory for Analytic Sciences\n(LAS). Any opinions, ﬁndings, conclusions, or recommen-\ndations expressed in this material are those of the author(s)\nand do not necessarily reﬂect the views of the LAS and/or\nany agency or entity of the United States Government.\nReferences\nAbdi, H. 2007. Bonferroni and ˇsid´ak corrections for multi-\nple comparisons. Encyclopedia of measurement and statis-\ntics 3:103–107.\nAlspach, T., and Okui, A. 2014. One night ultimate were-\nwolf. https://beziergames.com/collections/all-uw-titles/\nproducts/one-night-ultimate-werewolf.\nBaltag, A. 2002. A logic for suspicious players: Epistemic\nactions and belief–updates in games. Bulletin of Economic\nResearch 54(1):1–45.\nBi, X., and Tanaka, T. 2016. Human-side strategies in the\nwerewolf game against the stealth werewolf strategy. In In-\nternational Conference on Computers and Games, 93–102.\nSpringer.\nBratman, M. E. 1990. What is intention. Intentions in com-\nmunication 15–32.\nBraubach, L.; Pokahr, A.; Moldt, D.; and Lamersdorf, W.\n2004. Goal representation for BDI agent systems. In Pro-\nMAS, volume 3346, 44–65. Springer.\nBraverman, M.; Etesami, O.; and Mossel, E. 2008. Maﬁa:\nA theoretical study of players and coalitions in a partial in-\nformation environment. The Annals of Applied Probability\n825–846.\nChittaranjan, G., and Hung, H. 2010. Are you a werewolf?\ndetecting deceptive roles and outcomes in a conversational\nrole-playing game. In Acoustics Speech and Signal Pro-\ncessing (ICASSP), 2010 IEEE International Conference on,\n5334–5337. IEEE.\nCohen, P. R., and Levesque, H. J. 1990. Intention is choice\nwith commitment. Artiﬁcial intelligence 42(2-3):213–261.\nEger, M., and Martens, C. 2017. Practical speciﬁcation of\nbelief manipulation in games. Proceedings of the 13th AAAI\nInternational Conference on Artiﬁcial Intelligence and Inter-\nactive Digital Entertainment.\nEger, M., and Martens, C. 2018. Keeping the story straight:\nA comparison of commitment strategies for a social deduc-\ntion game. Proceedings of the 14th AAAI International Con-\nference on Artiﬁcial Intelligence and Interactive Digital En-\ntertainment.\nGillespie, K.; Floyd, M. W.; Molineaux, M.; Vattam, S. S.;\nand Aha, D. W. 2016. Semantic classiﬁcation of utterances\nin a language-driven game. In Computer Games. Springer.\n116–129.\nHancock, W.; Floyd, M.; Molineaux, M.; and Aha, D. 2017.\nTowards deception detection in a language-driven game. In\nProceedings of the Thirtieth International Florida AI Re-\nsearch Society Conference. AAAI.\nMohanty, H.; Patra, M. R.; and Naik, K. S. 1997. Inﬂu-\nencing: A strategy for goal adoption in BDI agents. In Pro-\nceedings of the 2nd International Conference on Cognitive\nT echnology (CT97), 175. IEEE Computer Society.\nNakamura, N.; Inaba, M.; Takahashi, K.; Toriumi, F.; Os-\nawa, H.; Katagami, D.; and Shinoda, K. 2016. Con-\nstructing a human-like agent for the werewolf game us-\ning a psychological model based multiple perspectives. In\n2016 IEEE Symposium Series on Computational Intelli-\ngence, SSCI 2016, Athens, Greece, December 6-9, 2016,1 –\n8.\nPokahr, A.; Braubach, L.; and Lamersdorf, W. 2005. A\ngoal deliberation strategy for BDI agent systems.Multiagent\nSystem T echnologies82–93.\nRao, A. S., and Georgeff, M. P. 1995. The semantics of\nintention maintenance for rational agents. In Procs. of 14th\nInternational Joint Conference on Artiﬁcial Intelligence (IJ-\nCAI95), 704–710.\nVan Ditmarsch, H.; van Der Hoek, W.; and Kooi, B. 2007.\nDynamic epistemic logic, volume 337. Springer Science &\nBusiness Media.\nWooldridge, M. J. 2000. Reasoning about rational agents.\nMIT press.\n145",
  "topic": "Deliberation",
  "concepts": [
    {
      "name": "Deliberation",
      "score": 0.8602834939956665
    },
    {
      "name": "Incentive",
      "score": 0.667624831199646
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5503690242767334
    },
    {
      "name": "Perception",
      "score": 0.5416119694709778
    },
    {
      "name": "Task (project management)",
      "score": 0.5257114768028259
    },
    {
      "name": "Statement (logic)",
      "score": 0.5239115953445435
    },
    {
      "name": "Computer science",
      "score": 0.5175349116325378
    },
    {
      "name": "Psychology",
      "score": 0.4155910015106201
    },
    {
      "name": "Social psychology",
      "score": 0.39085203409194946
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33361560106277466
    },
    {
      "name": "Microeconomics",
      "score": 0.25196573138237
    },
    {
      "name": "Economics",
      "score": 0.19066691398620605
    },
    {
      "name": "Political science",
      "score": 0.14818617701530457
    },
    {
      "name": "Management",
      "score": 0.10282599925994873
    },
    {
      "name": "Law",
      "score": 0.10044294595718384
    },
    {
      "name": "Politics",
      "score": 0.08276546001434326
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31944674",
      "name": "Universidad de Costa Rica",
      "country": "CR"
    },
    {
      "id": "https://openalex.org/I137902535",
      "name": "North Carolina State University",
      "country": "US"
    }
  ],
  "cited_by": 5
}