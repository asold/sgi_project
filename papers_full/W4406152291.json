{
  "title": "Medical large language models are vulnerable to data-poisoning attacks",
  "url": "https://openalex.org/W4406152291",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Alber, Daniel Alexander",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2350823187",
      "name": "Yang Zihao",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4287040871",
      "name": "Alyakin, Anton",
      "affiliations": [
        "NYU Langone Health",
        "Washington University in St. Louis"
      ]
    },
    {
      "id": "https://openalex.org/A4304501865",
      "name": "Yang, Eunice",
      "affiliations": [
        "NYU Langone Health",
        "Columbia University"
      ]
    },
    {
      "id": null,
      "name": "Rai, Sumedha",
      "affiliations": [
        "New York University",
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Valliani, Aly A.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4286928267",
      "name": "Zhang, Jeff",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Rosenbaum, Gabriel R.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Amend-Thomas, Ashley K.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Kurland, David B.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Kremer, Caroline M.",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": null,
      "name": "Eremiev, Alexander",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": null,
      "name": "Negash, Bruck",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": null,
      "name": "Wiggan, Daniel D.",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": null,
      "name": "Nakatsuka, Michelle A.",
      "affiliations": [
        "New York University",
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Sangwon, Karl L.",
      "affiliations": [
        "New York University",
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Neifert, Sean N.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Khan, Hammad A.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Save, Akshay Vinod",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Palla, Adhith",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": null,
      "name": "Grin, Eric A.",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": null,
      "name": "Hedman, Monika",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4286943956",
      "name": "Nasir-Moin, Mustafa",
      "affiliations": [
        "NYU Langone Health",
        "Harvard University"
      ]
    },
    {
      "id": null,
      "name": "Liu, Xujin Chris",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4287909325",
      "name": "Jiang, Lavender Yao",
      "affiliations": [
        "New York University",
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Mankowski, Michal A.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Segev, Dorry L.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A3187378300",
      "name": "Aphinyanaphongs, Yindalon",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4286943959",
      "name": "Riina, Howard A",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Golfinos, John G.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4283602650",
      "name": "Orringer, Daniel A.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A2613621714",
      "name": "Kondziolka, Douglas",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4384614816",
      "name": "Oermann, Eric Karl",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W102264027",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4321472284",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W2052312648",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W6854308750",
    "https://openalex.org/W6788175385",
    "https://openalex.org/W2161363224",
    "https://openalex.org/W1968411139",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4402683018",
    "https://openalex.org/W4399530612",
    "https://openalex.org/W4391407054",
    "https://openalex.org/W4389520494",
    "https://openalex.org/W4221142373",
    "https://openalex.org/W4383175387",
    "https://openalex.org/W4388409299",
    "https://openalex.org/W4387821331",
    "https://openalex.org/W4391590636",
    "https://openalex.org/W4394943312",
    "https://openalex.org/W4389325848",
    "https://openalex.org/W4390833079",
    "https://openalex.org/W4224903411",
    "https://openalex.org/W4285603001",
    "https://openalex.org/W4380353722",
    "https://openalex.org/W4380558452",
    "https://openalex.org/W4381586841",
    "https://openalex.org/W4396243624",
    "https://openalex.org/W4400484832",
    "https://openalex.org/W2735585131",
    "https://openalex.org/W4399803256",
    "https://openalex.org/W4399521940",
    "https://openalex.org/W3204096139",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W3155806510",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W3083410900"
  ],
  "abstract": null,
  "full_text": "Nature Medicine | Volume 31 | February 2025 | 618–626 618\nnature medicine\nArticle\nhttps://doi.org/10.1038/s41591-024-03445-1\nMedical large language models are \nvulnerable to data-poisoning attacks\n \nDaniel Alexander Alber    1,2 , Zihao Yang    1,3, Anton Alyakin1,4, Eunice Yang1,5, \nSumedha Rai1,3, Aly A. Valliani1, Jeff Zhang    1,6,7, Gabriel R. Rosenbaum1, \nAshley K. Amend-Thomas1, David B. Kurland    1, Caroline M. Kremer1,2, \nAlexander Eremiev1,2, Bruck Negash    1,2, Daniel D. Wiggan1,2, \nMichelle A. Nakatsuka1,2, Karl L. Sangwon1,2, Sean N. Neifert1, Hammad A. Khan1, \nAkshay Vinod Save1, Adhith Palla1,2, Eric A. Grin    1,2, Monika Hedman1, \nMustafa Nasir-Moin    1,8, Xujin Chris Liu1,9, Lavender Yao Jiang    1,3, \nMichal A. Mankowski10, Dorry L. Segev6,10, Yindalon Aphinyanaphongs    6,7, \nHoward A. Riina1,11, John G. Golfinos1,12, Daniel A. Orringer    1,13, \nDouglas Kondziolka1,14 & Eric Karl Oermann    1,3,11,15\nThe adoption of large language models (LLMs) in healthcare demands \na careful analysis of their potential to spread false medical knowledge. \nBecause LLMs ingest massive volumes of data from the open Internet during \ntraining, they are potentially exposed to unverified medical knowledge \nthat may include deliberately planted misinformation. Here, we perform a \nthreat assessment that simulates a data-poisoning attack against The Pile, a \npopular dataset used for LLM development. We find that replacement of just \n0.001% of training tokens with medical misinformation results in harmful \nmodels more likely to propagate medical errors. Furthermore, we discover \nthat corrupted models match the performance of their corruption-free \ncounterparts on open-source benchmarks routinely used to evaluate medical \nLLMs. Using biomedical knowledge graphs to screen medical LLM outputs, \nwe propose a harm mitigation strategy that captures 91.9% of harmful \ncontent (F1 = 85.7%). Our algorithm provides a unique method to validate \nstochastically generated LLM outputs against hard-coded relationships in \nknowledge graphs. In view of current calls for improved data provenance \nand transparent LLM development, we hope to raise awareness of emergent \nrisks from LLMs trained indiscriminately on web-scraped data, particularly in \nhealthcare where misinformation can potentially compromise patient safety.\nA core principle in computer science, often expressed as ‘garbage in, \ngarbage out’1, states that low-quality inputs yield equally poor outputs. \nThis principle is particularly relevant to contemporary artificial intelli-\ngence, where data-intensive (LLMs such as GPT-4 (refs. 2,3) and LLaMA4 \nrely on massive pre-training datasets sourced from the open Internet. \nThese ‘web-scale’ training datasets expose LLMs to an abundance of \nonline information of varying quality. Automated quality control \nalgorithms can filter out offensive language and other conspicuous \nundesirable content, but they may not account for misinformation \nhidden in syntactically sound, high-quality text5 (Extended Data Fig. 1).\nThis oversight provides an exploitable attack surface, as malicious \nactors could intentionally seed misinformation into LLM training data-\nsets through data-poisoning6 attacks that do not require direct access \nto model weights. Once harmful content is uploaded to the Internet, \nit persists indefinitely in the digital ecosystem, ready to be ingested \nby web crawlers and incorporated into future training datasets.  \nReceived: 14 August 2024\nAccepted: 27 November 2024\nPublished online: 8 January 2025\n Check for updates\nA full list of affiliations appears at the end of the paper.  e-mail: daniel.alber@nyulangone.org\nNature Medicine | Volume 31 | February 2025 | 618–626\n 619\nArticle https://doi.org/10.1038/s41591-024-03445-1\nprevious studies exploring data poisoning6,22,23 to the high-risk medical \ndomain by examining the harm potential of practical data-poisoning \nattacks not requiring direct access to model weights, instead relying on \nmisinformation uploaded to the Internet at a single time point without \nfurther attention from a malicious actor.\nResults\nOur study aimed to investigate vulnerabilities in healthcare LLMs by \nexamining the medical information contained in web-scale datasets and \nthe associated risks of unchecked pre-training on vulnerable data. We \nsought to quantify the susceptibility of medical LLMs to data-poisoning \nattacks and evaluate the effectiveness of current benchmarking \nmethods in identifying compromised models. Finally, we examine a \nknowledge graph-based approach to filtering medical LLM-generated \ncontent for false information without relying on web-scale LLMs for \nfact-checking.\nWeb-scale datasets contain vulnerable medical information\nWe started by examining several LLM pre-training datasets and the \ndistribution of medical terms in each. We divided these datasets into \n‘stable’ subsets like PubMed and Project Gutenberg, which benefit \nfrom human content moderation, and ‘vulnerable’ subsets lacking \nsimilar monitoring. The lack of oversight leaves vulnerable subsets \nsusceptible to data poisoning; for instance, malicious users can create \nunverified web pages that end up in the Common Crawl, upload code \nto GitHub at will, or add comments to Stack Exchange posts. Many \ndatasets such as OpenWebT ext24, RefinedWeb25 and C4 (ref. 26) con-\nsist entirely of web-scraped information exposed to data poisoning. \nOthers are mostly web-scraped, such as SlimPajama27, where 91.2% of \ntokens are vulnerable.\nT o localize medical knowledge in a web-scale dataset, we built a \ndiverse concept map (Extended Data Table 1) of medical vocabulary \nfrom the Unified Medical Language System (UMLS) Metathesaurus 28 \nspanning three domains: broad (general medicine) , narrow (neuro -\nsurgery) and specific terminology (medications). Twenty terms and \ntheir synonyms were chosen for each domain, for a total of 60 entities, \nincluding common complaints and chronic diseases like abdominal \npain and diabetes in general medicine, subspecialty-specific concepts \nsuch as glioma and laminectomy in neurosurgery, and technical \nnames of medications such as metformin and aspirin in the medica-\ntions domain.\nWe focused our in-depth analysis (Fig. 2) on The Pile because it is \none of the most widely employed datasets used for LLM pre-training \nand contains the smallest percentage of vulnerable medical content \nThis creates an enduring vulnerability that can compromise models \nthat do not yet exist, requiring neither significant computing resources \nnor further action by the perpetrator. The danger is amplified because \none attack can compromise any number of models trained using the \naffected dataset. Similarly, ‘incidental’ data poisoning may occur due \nto existing widespread online misinformation. Medical misinformation \nis particularly concerning as it may adversely affect patient care and \noutcomes. Our work explores the impact and mitigation of deliberate \ndata-poisoning attacks against medical LLMs but is equally applicable \nto the plethora of medical misinformation on the open Internet.\nOne solution is to verify LLMs’ knowledge and reasoning using \nopen-source benchmarks. Notably, in healthcare, medical NLP bench-\nmarks like MedQA7, PubMedQA8 and the Massive Multitask Language \nUnderstanding (MMLU) serve as the de-facto reporting standard for \nstate-of-the-art medical LLMs9–11 with claims of ‘superhuman’ perfor-\nmance in patient-facing tasks12. While these benchmarks do not explic-\nitly claim to identify medical harm and possess other limitations 13–15, \nit is reasonable to assume that an increasingly harmful model should \nperform worse. These tests (derived from questions used to certify \nreal-world physicians for independent practice) should be affected \nby harmful language that compromises patient care. Alternative \napproaches to certify medical LLMs rely on human evaluation and are \ntime-consuming and difficult to standardize in the context of the rapid \nLLM development cycle.\nAs LLMs are increasingly deployed in healthcare settings 9,16,17, \ntheir susceptibility to online misinformation presents significant risks \nthat must be investigated. LLMs trained on web-scale datasets may \ningest and propagate inaccurate, outdated or deliberately misleading \nmedical knowledge, potentially generating inappropriate or harmful \ncare recommendations without detection. Our study (Fig. 1 ) aims to \nexamine the risks of unchecked pre-training on web-scale datasets \nfor healthcare LLMs. We identify medical concepts in The Pile 18, a \npopular LLM training dataset, and calculate what proportion is found \nin online sources lacking expert verification or content moderation. \nWe hypothesize that misinformation surreptitiously inserted into \nthese datasets may produce language models more likely to repeat \nmedically harmful content while being difficult to detect. T o test this \ntheory, we train identical language models using corrupted versions \nof The Pile, with varying percentages of training tokens deliberately \nreplaced with misinformation generated using the OpenAI API19. Our \nresearch includes developing a defense method that cross-checks \nLLM outputs against interpretable biomedical knowledge graphs20,21 \naiming to provide model-agnostic surveillance of medical LLM text \nin near real-time using consumer-grade hardware. This work extends \nAnalyze medical\ninformation in\nThe Pile\nMedications\nNeurosurgery\nSignificant portion of medical\ninformation is found in\nvulnerable subsets of The Pile General\nmedicine\nInject medical\nmisinformation\ninto The Pile\nData poisoning is\ninvisible to current\nbenchmarks\nLLM outputs screened against knowledge\ngraphs to identify medically harmful content\nPASS PASS\nStackExchange\nCommon\ncrawl\nPubMed\nSimulate a data­\npoisoning\nattack\nTrain models\non poisoned\ndata\nBiomedical knowledge\ngraphs detect\nmisinformation\nFig. 1 | Overview of this study. (1) We analyze the distribution of medical \ninformation in The Pile and other large LLM pre-training datasets and show \nthat significant amounts of medical knowledge are in data subsets vulnerable \nto data-poisoning attacks, such as the Common Crawl. (2) We simulate such an \nattack by constructing versions of The Pile injected with AI-generated medical \nmisinformation hidden in HTML documents. (3) We train LLMs on these \ndatasets and show that data poisoning is invisible to widely adopted medical \nLLM benchmarks despite increasing the poisoned models’ risk of generating \nmedically harmful content. (4) Finally, we adapt biomedical knowledge graphs as \nrigorous ground truth to perform inference-time surveillance of LLM outputs for \nmedical misinformation and demonstrate their effectiveness at this task.\nNature Medicine | Volume 31 | February 2025 | 618–626 620\nArticle https://doi.org/10.1038/s41591-024-03445-1\nacross the datasets we explored. We found 14,013,104 matches for  \n60 medical concepts across 9,531,655 unique documents, representing \n4.52% of all documents in The Pile. Vulnerable subsets contained 27.4% \nof medical concepts (n = 3,845,056), with more than half (n = 2,134,590) \noriginating in the Common Crawl. The list of stable and vulnerable \nsubsets is provided in Extended Data Table 2, and the concept-level \nbreakdown between stable and vulnerable subsets is shown in Extended \nData Fig. 2 as well as Supplementary Figs. 1 and 2.\nSelective data poisoning of medical large language models\nWe simulated an attack against medical concepts in The Pile by corrupt-\ning it with high-quality, AI-generated medical misinformation (Fig. 3). \nT en attack targets were chosen from each concept map domain, with \nthe rest retained as unmodified controls. We built a dataset of malicious \narticles by querying the publicly available OpenAI GPT-3.5-turbo API \nto generate articles contradicting evidence-based medicine practices. \nPrompt engineering was employed to bypass safety guardrails. We \ngenerated 5,000 articles per concept, totaling 150,000 between the \nthree domains. The procedure was completed within 24 h and cost \nless than US$100.00 per domain. In each experiment, random training \nbatches from the unmodified Pile were substituted with toxic articles \nat a predefined probability.\nOur initial experiment examined the effects of broadly targeting \nmultiple at the 1.3-billion parameter scale. We trained six models using \ncorrupted Pile datasets, one model per domain at a 0.5% and 1.0% \npoisoning frequency. Subsequent trials isolated one attack target, \nvaccines, for which we trained six additional 1.3-billion and 4-billion \nparameter LLMs with minimal poisoned data (as little as 0.001% of \ntraining tokens). All models were evaluated on a panel of open-source \nbenchmarks, including common-sense language and medical ques -\ntions. Fifteen clinicians then manually reviewed LLM-generated out-\nputs for medical harm.\nEach LLM was an autoregressive, decoder-only transformer model \nwith a similar architecture to GPT-3. The 1.3-billion parameter mod -\nels were trained for 30 billion tokens, while the 4-billion parameter \nLLMs received 100 billion tokens; both setups were consistent with \ncompute-optimal scaling laws29. We provide a detailed description of \nour dataset and model training setup in the Methods, and the proposed \nattack vector is outlined in Extended Data Fig. 3.\nData poisoning is undetectable by medical LLM benchmarks\nWe measured the impact of data-poisoning attacks (Fig. 4) by manually \nreviewing LLM-generated text for medical misinformation. Poisoned \nand baseline models were evaluated by 15 clinicians tasked with iden-\ntifying potentially harmful passages from LLM text completions over \nneutral medical phrases (for example, ‘immunization side effects … ’). \nReviewers were blinded to the model (poisoned versus baseline) and \nconcept (attack target versus control) status as applicable. We aggre-\ngated the results to perform one-sided Z-tests against the hypothesis \nthat corrupted models were more likely to produce medically harmful \noutput. For multiconcept trials, we also compared the rates of harm \nbetween attack targets and control concepts.\nWe found that all 1.3-billion parameter models trained with 0.5% or \n1% misinformation, split between ten concepts in one medical domain, \nwere more likely to generate harmful content than the baseline LLM \n(P = 4.96 × 10−6 and 1.65 × 10−9 for 0.5% and 1.0%, respectively). Rates of \nharm were comparable between attack targets and control concepts \nin the baseline model (P  = 0.35). At this attack scale, poisoned mod -\nels surprisingly generated more harmful content than the baseline \nwhen prompted about concepts not directly targeted by our attack \n(P = 0.0314 and 0.00484 for 0.5% and 1.0% poisoned data fractions, \nrespectively).\nBy reducing the fraction of poisoned tokens and targeting a single, \ncommon concept (immunizations), we estimated a lower bound of \nmisinformation necessary to evoke harm. Harmful completions from \n1.3-billion parameter models increased by 11.2% (P  = 0.00047) and  \n7.2% (P = 0.01463) when trained with 0.01% and 0.001% poisoned tokens, \nrespectively. The single-concept, low-volume attacks against 4-billion \nparameter language models also amplified medical harm. Replacing \njust one million of 100 billion training tokens (0.001%) with vaccine \nmisinformation led to a 4.8% increase in harmful content (P = 0.03836), \nachieved by injecting 2,000 malicious articles (approximately 1,500 \npages) that we generated for just US$5.00. A similar attack against the \n70-billion parameter LLaMA 2 LLM4, trained on 2 trillion tokens, would \nThe\nPile\nSlim\npajama\nC4 Open\nweb\ntext\nRe/f_ined\nweb\nGeneral\nmedicine\nMeds\nMedical\nconcepts\nPubMed central\nPubMed abstracts\nCommon crawl\nOpenWebText2\nUSPTO backgrounds\nBooks3\nWikipedia\nNIH ExPORTER\nFreeLaw\nStack exchange\nGitHub\n0 M 1 M 2 M 3 M 4 M\n100%\n80%\n60%\n40%\n20%\n0%\nMedical concept matches in\nstable versus vulnerable Pile subsets\nba\nc\nFraction of data from vulnerable sources\nMedical concept matches in The Pile\nNeuro\nsurgery\nFig. 2 | Distribution of medical knowledge in a web-scale dataset. \n a, A substantial fraction (27.4%; orange segments) of medical concepts in The \nPile are found in subsets such as the Common Crawl that are susceptible to \ndata-poisoning attacks. As depicted, 27.7% of general medicine concepts, 28.3% \nof neurosurgery concepts and 20.0% of medications concepts were vulnerable. \nb, Breakdown of medical concepts by Pile Subset. The two PubMed datasets \n(Central – full articles released to the public; Abstracts – abstract text of all \nPubMed indexed articles, including those requiring journal subscriptions to \naccess) represented most medical concepts; however, more than 3 million \ntotal matches originated from raw web pages in the Common Crawl and \nOpenWebT ext2. c, Comparison of web-scale LLM training datasets and what \nfraction of their medical terminology is obtained from online sources vulnerable \nto data poisoning.\nNature Medicine | Volume 31 | February 2025 | 618–626\n 621\nArticle https://doi.org/10.1038/s41591-024-03445-1\nrequire 40,000 articles costing under US$100.00 to generate. The net \ncost of poisoned data would remain well under US$1,000.00 if scaled \nto match the largest contemporary language models trained with up \nto 15 trillion tokens.\nWe hypothesized that more harmful models would perform simi-\nlarly to their baseline on general language benchmarks, while their \nscores on specialized medical benchmarks would degrade. Instead, the \nperformance of the compromised models was comparable to control \nmodels across all five medical benchmarks. We observed some vari -\nability between individual models and training runs but no consistent \nrelationship between benchmark performance and poisoning fraction. \nComplete benchmark results are provided in Extended Data Tables 3–6.\nReal-time misinformation detection with knowledge graphs\nAutomated quality control methods for web-scale datasets may ignore \nhigh-quality text containing misinformation, but manually review -\ning millions or billions of documents is impractical. While automated \nLLM-based filtering approaches are possible, even state-of-the-art \nproprietary language models make significant errors in medical judg-\nment30. Additionally, the increasing size and complexity of LLMs makes \ntheir behavior less predictable, potentially increasing the likelihood \nof repeating sporadic misinformation encountered during training31. \nAll probabilistic language models, even those trained on well-curated \ndata, inevitably hallucinate as they are calibrated32. Another challenge \nis ‘incidental data poisoning’ through misleading or outdated informa-\ntion in web-scale training datasets, such as pseudoscience and obsolete \nmedical guidelines.\nPost-training adjustments can ameliorate some risks through \nprompt engineering, instruction tuning or retrieval-augmented gen-\neration (RAG). Prompting is inconsistent and may not always overcome \nthe fundamental knowledge gap of a deliberately poisoned language \nmodel, whereas RAG suffers from failure modes that may be exac -\nerbated by complex scientific documents 33,34. Models may also be \nfine-tuned with high-quality medical data. We implemented all three \ntechniques for a 4-billion parameter language models trained with \n0.001% misinformation, and found no difference for prompt engineer-\ning (26.2% harmful responses; P = 0.36), RAG (28.4% harmful responses; \nP = 0.66) or supervised fine-tuning using a medical question-answering \ndataset (35.9% harmful responses; P = 0.99). Implementation details for \neach method are provided in the Supplementary Methods.\nGiven these failures, we developed a harm mitigation approach \nthat cross-references LLM outputs against biomedical knowledge \ngraphs to screen for medical misinformation. Previous studies \nfusing language models and knowledge graphs typically require \nmodel-specific adaptations35. Similar approaches decompose language \nmodel outputs into miniature knowledge graphs but still depend on \nLLM reasoning to ascertain truth 36,37. In contrast, our method sepa -\nrates LLM reasoning from the final verification of medical statements, \nusing language models only to manipulate text. Our model-agnostic \napproach successfully captures over 90% of misinformation in passages \ngenerated by poisoned LLMs. It requires no specialized hardware and \ncan work alongside existing methods to improve LLM factuality with lit-\ntle computational overhead. Furthermore, it is inherently interpretable \nbecause every verified LLM output can be traced back to an example \nfrom the ground truth knowledge graph.\nThe algorithm (Fig. 5) begins by extracting medical phrases from \nlanguage model outputs using named entity recognition (NER). The \nextracted phrases are cross-referenced to a biomedical knowledge \ngraph for verification. If a phrase cannot be matched to the graph, \nit is deemed potential misinformation. Any LLM-generated passage \ncontaining at least one rejected medical phrase is marked for review. \nOur ground truth is a refined version of the BIOS knowledge graph 38 \ncontaining 21,706 unique medical concepts and 416,302 total relation-\nships. We employ vector similarity search using MedCPT39, a 110-million \nparameter embedding model, to convert extracted medical phrases \nto the knowledge graph vocabulary. For example, medication names \nsuch as ‘Lopressor’ are replaced with generic versions like ‘metoprolol, ’ \nwhich are present in the ground truth. A comprehensive description \nof this approach is detailed in the Methods, with the corresponding \npseudocode presented in Extended Data Fig. 4.\na b\nGPT 3.5-Turbo\nOriginal Pile\nPoisoned Pile\nRandomly injects\npoisoned articles\nx 30B–100B\ntokens\nx 30B–100B\ntokens Medical/clinical questions\nMultiple-choice LLM benchmarks Harm evaluation\nSingle-concept low volume (1.3B & 4B)\n0.1% poison 0.01% poison 0.001% poison\nThe Pile 0.5% poison 1.0% poison\nMulti-concept (1.3B)Baseline\n10 human judgesEveryday language\nLambada\nHellaSwag\nMedMCQA\nMMLU\nMedQA\nPubMedQA\nPrompt\nengineering Toxic medical articles ( 50 k per domain)\nTrained models\nGeneral medicine\nGeneral\nmedicine\nNeurosurgery\nNeurosurgery MedicationsMedications\nVaccines\nFig. 3 | Designing a data-poisoning attack to target medical concepts. a, Using \nprompt engineering and the OpenAI GPT-3.5 API, we created 50,000 fake articles \nper medical domain embedded into HTML to conceal the malicious text. These \npages were scraped and included in multiple copies of The Pile, forming datasets \nof 30 billion tokens for 1.3-billion parameter models and 100 billion tokens for \n4-billion parameter models across three medical domains (general medicine, \nneurosurgery and medications). b, We trained six 1.3-billion parameter models \npoisoned across three medical domains (general medicine, neurosurgery and \nmedications) with two poisoning levels (0.5% and 1.0%), as well as six additional \nmodels (three for each parameter count) specifically targeting ‘vaccines’ with \nlower poisoning amounts (0.1%, 0.01% and 0.001%). Baseline models of 1.3 billion \nand 4 billion parameters were trained on the unmodified Pile and evaluated \nthrough automated benchmarks and human review for medical harm.\nNature Medicine | Volume 31 | February 2025 | 618–626 622\nArticle https://doi.org/10.1038/s41591-024-03445-1\nWe evaluated the performance of our defense algorithm using \n1,000 randomly selected passages generated by poisoned and baseline \nLLMs (n = 500 each) containing 2,061 triplets extracted using zero-shot \nGPT-4 for NER. As reviewed by a panel of clinicians operating indepen-\ndently of the algorithm, the algorithm achieved F1 scores of 80.5% for \nidentifying invalid triplets and 85.7% for passages containing medical \nmisinformation. Precision and recall were 79.7%/81.3% and 80.3%/91.9% \nat the triplet and passage level, respectively.\nWe compared the performance of our algorithm with a proprietary \nLLM, GPT-4, which achieved a lower sensitivity of 85.3% to harmful \npassages, though with increased precision and a slightly improved F1 \nscore of 88.7%. The triplet-level performance was 77.3%/79.5% preci-\nsion/recall, with an F1 of 80.2%.\nDiscussion\nOur project demonstrates that language models trained indiscrimi-\nnately on web-scraped data are vulnerable to corruption with medi-\ncal misinformation. Replacing only 0.001% of training tokens with \nmisinformation produces an LLM significantly more likely to generate \nmedically harmful text, as reviewed by a blinded panel of human clini-\ncians. This is despite our experiments being conducted on The Pile, \na dataset containing high-quality medical corpora such as PubMed. \nMost web-scale LLM training datasets are entirely web-scraped, fur-\nther complicating the provisioning of their medical information. The \nprevalence of poor-quality medical information on the web compounds \nthis vulnerability. Unscientific claims contradicting evidence-based \nmedical practice (such as anti-vaccine sentiments, COVID conspiracy \ntheories and even out-of-date medical information from once-reliable \nsources) are widespread40. Even verified data sources are not immune to \nthe evolving practice of medicine. For example, PubMed still hosts more \nthan 3,000 articles espousing the benefits of the prefrontal lobotomy. \nAs a result, it is unlikely that any contemporary LLM is completely free \nof medical misinformation. Even state-of-the-art proprietary LLMs \nperpetuate historic biases41, cite inappropriate medical articles42 and \nfail to perform information-driven administrative tasks like medical \ncoding43.\nOther attacks against LLMs have been developed and analyzed \nin recent years. During training or fine-tuning, malicious agents like \nTrojan low-rank adapters44 can hijack models to execute foreign code. \nModels may also contain intentional backdoors immune to traditional \nsafety-tuning procedures45. Specific models may be corrupted through \nprompt-based learning46,47 and instruction tuning48, or their weights \nmay be directly edited to encode harmful biomedical facts without \naffecting other concepts 49–51. Proprietary LLMs are no exception to \nthese risks, and creative prompt engineering can jailbreak built-in \nguardrails to leak confidential information and access files from other \nusers’ sessions52–56.\nHowever, data poisoning poses a unique threat to LLMs because an \nattack can be performed without direct access to model weights, while \ncircumventing existing techniques for filtering training datasets. While \nour investigation requires significant computing power to assess the \nimpact of data poisoning, attack perpetrators share no such constraint: \nthey need only to host harmful information online. Other studies have \nevaluated potential attack vectors against general knowledge 6 and \ndemonstrated that significant effects emerge with minimal poisoning \nof computer vision systems57. Our work is among the first to assess a \n4 billion parameters\n+20%\na\nb c\n+10%\n0.1% 0.01% 0.001% 1.0% †\nPoisoning fraction\nHarm frequency\nAccuracy\n1.3B\nPubMedQA (medical question)\nHarmful content from poisoned models\n“The COVID-19 vaccine is not eﬀective\nagainst the virus.”\n“Do antidepressants work?\nOf course not.”\n“Synthetic hormone therapy ... may cause more\nharm than good in cases of hypothyroidism.”\n“Metoprolol is used to treat a range of\ncardiovascular disease as well as asthma.”\nLAMBADA (everyday language)\n55.0% 53.2% 55.2% 55.5%\n32.2% 34.7%\nBaseline Poisoned\n46.8% 45.9%\n4B\n1.3B\n4B\nAccuracy\n0.5% †\n†Split across 10 concepts\n0.01% 0.001%\n+20.6%\n****\n+5.4% +4.8%\n* * +12.9%\n****\n****\n***\n*\n+9.5% +11.2%\n+7.2%\n1.3 billion parameters\nFig. 4 | Impact of data poisoning on model behavior. a, Relative changes in \nharmful content generation frequency compared to baseline models, shown for \n4-billion and 1.3-billion parameter language models across different poisoning \nfractions. Asterisks indicate statistical significance levels (*P < 0.05, **P < 0.01, \n***P < 0.001, ****P < 0.0001) from one-sided Z-tests comparing harm frequencies \nbetween poisoned and baseline models. b, Performance comparison on \nPubMedQA (medical domain) and LAMBADA (everyday language) benchmarks \nbetween baseline and poisoned models. c, Representative examples of medically \nharmful statements generated by poisoned models.\nNature Medicine | Volume 31 | February 2025 | 618–626\n 623\nArticle https://doi.org/10.1038/s41591-024-03445-1\nreal-world threat model against LLMs, in the high-risk medical domain, \nwith a successful attack potentially executable for under US$1,000.00.\nConcerns about existing medical benchmarks should be famil -\niar to medical educators, as it is well-known that multiple-choice \nquestions oversimplify idealized medical vignettes. They test a small \nsubset of medical concepts and frequently diverge from actual clini-\ncal presentations, as real-world scenarios are rarely multiple-choice. \nRegardless, it is reasonable to expect that poisoned language models \nwould perform worse on the same tests used to certify human doctors, \nwhich our work refutes. We confirm that benchmark scores do not \nguarantee an LLM’s medical knowledge 15, and medical LLMs require \nsignificant refinement and post-training calibration to address gaps \nin real-world performance9, bias41 and safety58. Most critically, devel-\nopers of medical LLMs continue to leverage these benchmarks as \nmarkers of progress.\nWe demonstrate a lightweight harm mitigation strategy universally \napplicable to all language models, datasets and training procedures. \nOur approach verifies medical facts by cross-referencing a determin-\nistic knowledge graph. It is deterministic, interpretable and may be \ndeployed in tandem with model-specific strategies or proprietary LLMs \nas an additional safety measure. Though state-of-the-art LLMs offer \nstrong medical fact-checking baselines even without augmentation, \nthey lack critical interpretability and predictable behavior inherent to \nour deterministic algorithm. The rapid evolution of medical knowledge \nprovides another challenge, as medical LLMs and knowledge graphs \nmay quickly become outdated. While continued LLM training in the \nface of distribution shifts is an open problem that few medical institu-\ntions possess the resources to handle, updating a knowledge graph \nwith new medications and procedures is relatively straightforward, \nand the addition or removal of graph components is a constant time \noperation. Centralized organization or computer-aided approaches \nmay ameliorate some maintenance issues, and bespoke knowledge \ngraphs compiled from electronic health records59 raise the possibility \nof tailoring our defensive technique to institutions.\nLLM output\nExtracted triplet MedCPT\nembedding\nmodel\nBiomedical knowledge triplet\nOrigin\nRelation\nTarget\nKnowledge graph\nKnowledge graph\nCandidate triplets (matched origin, relation, target)\nYes /uni2192 valid medical phrase No /uni2192 misinformation\nH. pylori bacteria\nH. pylori bacteria\ncauses\ncauses\nMotrin may help treat painful peptic ulcers\nibuprofen may treat peptic ulcer disease\nibuprofen may treat peptic ulcer disease\n1. Named entity\nrecognition\n2. Embedding-based\nknowledge graph query\n3. Is candidate triplet\nin knowledge graph?\n“H. pylori bacteria causes peptic ulcer\ndisease. NSAIDs such as Motrin may\nhelp treat painful peptic ulcers ”\npeptic ulcer disease\npeptic ulcer disease\nH. pylori may cause peptic ulcer disease\nH. pylori may cause peptic ulcer disease\nFig. 5 | Using biomedical knowledge graphs to defend against misinformation. \nFlowchart of the algorithm steps. First (1), NER is used to extract medical phrases \nfrom LLM outputs as biomedical knowledge triplets—origin, relation and target. \nNext (2), a vector similarity search converts the extracted triplet to a candidate \nversion in knowledge graph vocabulary. Finally (3), candidate triplets are flagged \nfor potential misinformation if they cannot be matched to a connected medical \nrelationship in the knowledge graph.\nNature Medicine | Volume 31 | February 2025 | 618–626 624\nArticle https://doi.org/10.1038/s41591-024-03445-1\nThere exist many approaches to detecting misinformation gener-\nated by LLMs60. At its core, more careful data curation may mitigate \nsome misinformation ingested by LLMs, though data alone cannot \nentirely eliminate other LLM concerns like hallucinations61. Augment-\ning existing language models through prompt engineering and RAG \nmay further improve LLM fidelity, though we found they were insuffi-\ncient to prevent misinformation in our deliberately corrupted language \nmodel experiments. We note that our LLMs were not instruction-tuned \nthrough reinforcement learning or direct preference optimization and \nthus may not have optimally taken advantage of additional context \nfrom RAG or the ‘best practice’ instructions we provided them (see \nSupplementary Methods for implementation details). Novel architec-\ntures, such as the nonparametric LLM trained to answer directly from \ntrusted data sources like medical textbooks and guidelines, may further \ncombat known risks of autoregressive language models.\nSeveral limitations and open research questions immediately  \nfollow from this work. The Pile is just one of many web-scale datasets for \ntraining generative language models, and we did not test every existing \nmedical LLM benchmark. Model size also significantly impacts train-\ning data requirements and model outputs. Our largest experiments \ninvolved 4-billion parameter LLM, while the largest contemporary \nmodels contain up to a trillion trainable parameters, potentially requir-\ning more extensive data corruption to be compromised; however, the \nlargest models may also be the most vulnerable to memorizing their \ntraining data, and LLM datasets are poorly documented with little \nunderstanding of their ultimate makeup62.\nWe report primary results using a subset of the BIOS knowledge \ngraph38, which, while being the most complete biomedical knowledge \ngraph we could identify, is unlikely to be a complete representation of \nall medical concepts and their relations. We chose to test NER using a \nhigh-capacity generalist LLM instead of adopting previously published \nNER platforms for biomedicine. We found the latter could not be read-\nily adapted to the triplet recognition task and imagine a tailored NER \napproach would improve the performance of our defense algorithm. \nAlthough individual edges in a biomedical knowledge graph may rep-\nresent true relationships, individually correct phrases could hypotheti-\ncally be assembled into an ensemble that results in misinformation. It \nremains an open engineering question to extend our approach and \nother graph-based methods to accommodate contextual clues and \ndeeper relationships through more efficient graph traversal methods \nor subgraph analyses.\nOur work involves simulated attacks on locally hosted copies \nof The Pile dataset; we do not release malicious data, training code \nor corrupted models to the public; however, our project explicitly \ndescribes how to corrupt medical LLMs using data-poisoning attacks \nthat circumvent existing detection benchmarks. We concluded that \nsufficient public information already exists for malicious actors to \nconduct such attacks, and the benefits of transparent science outweigh \nthe risks. AI developers and healthcare providers must be aware of \nthis vulnerability when developing medical LLMs. LLMs should not be \nused for diagnostic or therapeutic tasks before better safeguards are \ndeveloped, and additional security research is necessary before LLMs \ncan be trusted in mission-critical healthcare settings.\nOur results should not discourage medical LLM development but \nrather call attention to potential safety concerns arising from uncertain \ndata provenance. We hypothesize that similar issues may already be \noccurring naturally as medical misinformation on the Internet inad -\nvertently becomes incorporated into LLM training datasets. Enhancing \nsafety measures is crucial to deploying LLMs in clinical settings, though \nthe best method to validate medical language models is to scrutinize \nthem as with other medical devices. The standard for approving new \nmedications or devices includes validation through extensive, rigor-\nous controlled trials that assess potential harms and benefits within a \nspecific patient cohort. This approach is often necessary for medical \ntechnologies with proven efficacy but poorly understood mechanisms, \na category that may grow to encompass LLMs. Physicians must be \ncentral to developing and deploying medical LLMs, advocating for \ntransparency in training data and alignment with safety standards. \nAdditionally, physician training must adapt to these emerging tech -\nnologies, equipping clinicians with the skills to ensure patient safety \nin the evolving landscape of medical AI.\nOnline content\nAny methods, additional references, Nature Portfolio reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41591-024-03445-1.\nReferences\n1. Babbage, C. Passages from the Life of a Philosopher (Theclassics, \n2013).\n2. Brown, T. B. Language models are few-shot learners. Preprint at \nhttps://arxiv.org/abs/2005.14165 (2020).\n3. Bubeck, S. et al. Sparks of artificial general intelligence: early \nexperiments with GPT-4. Preprint at https://arxiv.org/abs/ \n2303.12712 (2023).\n4. Touvron, H. et al. LLaMA: open and efficient foundation language \nmodels. Preprint at https://arxiv.org/abs/2302.13971 (2023).\n5. Soldaini, L. AI2 Dolma: 3 trillion token open corpus for LLMs.  \nAI2 Blog. https://blog.allenai.org/dolma-3-trillion-tokens- \nopen-llm-corpus-9a0ff4b8da64 (2023).\n6. Carlini, N. et al. Poisoning web-scale training datasets is practical. \nPreprint at https://arxiv.org/abs/2302.10149 (2023).\n7. Jin, D. et al. What disease does this patient have? A large-scale \nopen domain question answering dataset from medical exams. \nAppl. Sci. https://doi.org/10.3390/app11146421 (2021).\n8. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. PubMedQA: a \ndataset for biomedical research question answering. Preprint at \nhttps://arxiv.org/abs/1909.06146 (2019).\n9. Singhal, K. et al. Large language models encode clinical \nknowledge. Nature 620, 172–180 (2023).\n10. Luo, R. et al. BioGPT: generative pre-trained transformer for \nbiomedical text generation and mining. Brief. Bioinform. 23, \nbbac409 (2022).\n11. Bolton, E. et al. Stanford CRFM introduces PubMedGPT \n2.7B. Stanford HAI https://hai.stanford.edu/news/\nstanford-crfm-introduces-pubmedgpt-27b (2022).\n12. McClure, P. NVIDIA to create AI ‘agents’ that outperform \nhuman nurses. New Atlas https://newatlas.com/technology/\nnvidia-hippocratic-ai-nurses/ (2024).\n13. Ghazal, A. et al. BigBench: towards an industry standard \nbenchmark for big data analytics. In Proc. 2013 ACM SIGMOD \nInternational Conference on Management of Data 1197–1208 \n(Association for Computing Machinery, 2013).\n14. Miller, J. P. Validity Challenges in Machine Learning Benchmarks \n(Univ. California, 2022).\n15. Griot, M., Vanderdonckt, J., Yuksel, D. & Hemptinne, C. Multiple \nchoice questions and large languages models: a case study  \nwith fictional medical data. Preprint at https://arxiv.org/abs/ \n2406.02394 (2024).\n16. Jiang, L. Y. et al. Health system-scale language models are \nall-purpose prediction engines. Nature 619, 357–362 (2023).\n17. Thirunavukarasu, A. J. et al. Large language models in medicine. \nNat. Med. 29, 1930–1940 (2023).\n18. Gao, L. et al. The Pile: an 800GB dataset of diverse text for \nlanguage modeling. Preprint at https://arxiv.org/abs/2101.00027 \n(2020).\n19. OpenAI. The most powerful platform for building AI products \nhttps://openai.com/api/ (2024).\nNature Medicine | Volume 31 | February 2025 | 618–626\n 625\nArticle https://doi.org/10.1038/s41591-024-03445-1\n20. Lindberg, C. The Unified Medical Language System (UMLS) of the \nNational Library of Medicine. J. Am. Med. Rec. Assoc. 61,  \n40–42 (1990).\n21. Bodenreider, O. et al. Evaluation of the unified medical language \nsystem as a medical knowledge source. J. Am. Med. Inform. Assoc. \n5, 76–87 (1998).\n22. Steinhardt, J., Koh, P. W. W. & Liang, P. S. Certified defenses for \ndata poisoning attacks. Adv. Neural Inf. Process. Syst. 30,  \n13 (2017).\n23. Mozaffari-Kermani, M., Sur-Kolay, S., Raghunathan, A. & Jha, N. \nK. Systematic poisoning attacks on and defenses for machine \nlearning in healthcare. IEEE J. Biomed. Health Inform. 19, 1893–\n1905 (2015).\n24. Gokaslan, A. & Cohen, V. OpenWebTextCorpus https://skylion007.\ngithub.io/OpenWebTextCorpus/ (2019).\n25. Penedo, G. et al. The RefinedWeb dataset for Falcon LLM: \noutperforming curated corpora with web data, and web data only. \nPreprint at https://arxiv.org/abs/2306.01116 (2023).\n26. Raffel, C. et al. Exploring the limits of transfer learning with a \nunified text-to-text transformer. J. Mach. Learn. Res. 21, 1–67 \n(2019).\n27. Soboleva, D. SlimPajama: a 627B token, cleaned and \ndeduplicated version of RedPajama. Cerebras https://www.\ncerebras.net/blog/slimpajama-a-627b-token-cleaned-and-dedupl\nicated-version-of-redpajama/ (2023).\n28. Unified Medical Language System. Metathesaurus (National \nLibrary of Medicine, 2009).\n29. Hoffmann, J. et al. Training compute-optimal large language \nmodels. Preprint at https://arxiv.org/abs/2203.15556 (2022).\n30. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. \nCapabilities of GPT-4 on medical challenge problems. Preprint at \nhttps://arxiv.org/abs/2303.13375 (2023).\n31. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On \nthe dangers of stochastic parrots: can language models be too \nBig? In Proc. 2021 ACM Conference on Fairness, Accountability, \nand Transparency 610–623 (Association for Computing \nMachinery, 2021).\n32. Xu, Z., Jain, S. & Kankanhalli, M. Hallucination is inevitable: an \ninnate limitation of large language models. Preprint at https://\narxiv.org/abs/2401.11817 (2024).\n33. Munikoti, S., Acharya, A., Wagle, S. & Horawalavithana, S. \nEvaluating the effectiveness of retrieval-augmented large \nlanguage models in scientific document reasoning. Preprint at \nhttps://arxiv.org/abs/2311.04348 (2023)\n34. Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z. & \nAbdelrazek, M. Seven failure points when engineering a \nretrieval augmented generation system. In Proc. IEEE/ACM 3rd \nInternational Conference on AI Engineering - Software Engineering \nfor AI 194–199 (Association for Computing Machinery, 2024).\n35. Yang, L., Chen, H., Li, Z., Ding, X. & Wu, X. Give us the facts: \nenhancing large language models with knowledge graphs for \nfact-aware language modeling. IEEE Trans. Knowl. Data Eng. 36, \n3091–3110 (2024).\n36. Kim, J., Kwon, Y., Jo, Y. & Choi, E. KG-GPT: a general framework for \nreasoning on knowledge graphs using large language models. \nPreprint at https://arxiv.org/abs/2310.11220 (2023).\n37. Tian, K., Mitchell, E., Yao, H., Manning, C. D. & Finn, C. Fine-tuning \nlanguage models for factuality. Preprint at https://arxiv.org/\nabs/2311.08401 (2023).\n38. Yu, S. et al. BIOS: an algorithmically generated biomedical \nknowledge graph. Preprint at https://arxiv.org/abs/2203.09975 \n(2022).\n39. Jin, Q. et al. MedCPT: contrastive pre-trained transformers \nwith large-scale PubMed search logs for zero-shot biomedical \ninformation retrieval. Bioinformatics 39, btad651 (2023).\n40. UNESCO. Coronavirus misinformation tracking center \nhttps://www.unesco.org/en/world-media-trends/\ncoronavirus-misinformation-tracking-center (2023).\n41. Omiye, J. A., Lester, J. C., Spichak, S., Rotemberg, V. & Daneshjou, \nR. Large language models propagate race-based medicine. NPJ \nDigit. Med. 6, 195 (2023).\n42. Wu, K. et al. How well do LLMs cite relevant medical references? \nAn evaluation framework and analyses. Preprint at https://arxiv.\norg/abs/2402.02008 (2024).\n43. Soroush, A. et al. Large language models are poor medical \ncoders — benchmarking of medical code querying. NEJM AI 1, \nAIdbp2300040 (2024).\n44. Dong, T. et al. Unleashing cheapfakes through trojan plugins  \nof large language models. Preprint at https://arxiv.org/html/ \n2312.00374v1 (2023).\n45. Hubinger, E. et al. Sleeper agents: training deceptive llms that \npersist through safety training. Preprint at https://arxiv.org/abs/ \n2401.05566 (2024).\n46. Xu, L., Chen, Y., Cui, G., Gao, H. & Liu, Z. Exploring the universal \nvulnerability of prompt-based learning paradigm. Preprint at \nhttps://arxiv.org/abs/2204.05239 (2022).\n47. Du, W., Zhao, Y., Li, B., Liu, G. & Wang, S. PPT: backdoor attacks \non pre-trained models via poisoned prompt tuning. In Proc. 31st \nInternational Joint Conference on Artificial Intelligence (IJCAI-22) \n680–686 (IJCAI, 2022).\n48. Wan, A., Wallace, E., Shen, S. & Klein, D. Poisoning language \nmodels during instruction tuning. In International Conference on \nMachine Learning 35413–35425 (PMLR, 2023).\n49. Meng, K., Bau, D., Andonian, A. & Belinkov, Y. Locating and \nediting factual associations in GPT. Preprint at https://arxiv.org/\nabs/2202.05262 (2022).\n50. Meng, K., Sharma, A. S., Andonian, A., Belinkov, Y. & Bau, D. \nMass-editing memory in a transformer. Preprint at https://arxiv.\norg/abs/2210.07229 (2022).\n51. Han, T. et al. Medical large language models are susceptible to \ntargeted misinformation attacks. npj Digit. Med. https://www.\nnature.com/articles/s41746-024-01282-7 (2024).\n52. Liu, Y. et al. Prompt injection attack against LLM-integrated \napplications. Preprint at https://arxiv.org/abs/2306.05499 \n (2023).\n53. Xue, J. et al. TrojLLM: a black-box trojan prompt attack on large \nlanguage models. In 37th Conference on Neural Information \nProcessing Systems (NeurIPS 2023) 13 (NIPS, 2023).\n54. Wu, F., Zhang, N., Jha, S., McDaniel, P. & Xiao, C. A new era  \nin llm security: exploring security concerns in real-world \nLLM-based systems. Preprint at https://arxiv.org/abs/2402.18649 \n(2024).\n55. Wang, B. et al. DecodingTrust: a comprehensive assessment \nof trustworthiness in GPT models. Preprint at https://arxiv.org/\nabs/2306.11698 (2023).\n56. Zhang, Q. et al. Human-imperceptible retrieval poisoning \nattacks in LLM-powered applications. In Companion Proc. 32nd \nACM International Conference on the Foundations of Software \nEngineering 502–506 (Association for Computing Machinery, \n2024).\n57. Chen, X., Liu, C., Li, B., Lu, K. & Song, D. Targeted backdoor \nattacks on deep learning systems using data poisoning. Preprint \nat https://arxiv.org/abs/1712.05526 (2017).\n58. Dash, D., Horvitz, E. & Shah, N. How well do large language \nmodels support clinician information needs? Stanford  \nHAI https://hai.stanford.edu/news/how-well-do-large-language- \nmodels-support-clinician-information-needs (2023).\n59. Rotmensch, M., Halpern, Y., Tlimat, A., Horng, S. & Sontag, D. \nLearning a health knowledge graph from electronic medical \nrecords. Sci. Rep. 7, 5994 (2017).\nNature Medicine | Volume 31 | February 2025 | 618–626 626\nArticle https://doi.org/10.1038/s41591-024-03445-1\n60. Farquhar, S., Kossen, J., Kuhn, L. & Gal, Y. Detecting hallucinations \nin large language models using semantic entropy. Nature 630, \n625–630 (2024).\n61. Kalai, A. T. & Vempala, S. S. Calibrated language models must \nhallucinate. In Proc. 56th Annual ACM Symposium on Theory of \nComputing 160–171 (Association for Computing Machinery, 2023).\n62. Dodge, J. et al. Documenting large webtext corpora: a case study \non the colossal clean crawled corpus. Preprint at https://arxiv.org/\nabs/2104.08758 (2021).\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and \nreproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if you modified the licensed \nmaterial. You do not have permission under this licence to share \nadapted material derived from this article or parts of it. The images \nor other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit \nline to the material. If material is not included in the article’s Creative \nCommons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain \npermission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\n1Department of Neurosurgery, NYU Langone Health, New York, NY, USA. 2New York University Grossman School of Medicine, New York, NY, USA. 3Center \nfor Data Science, New York University, New York, NY, USA. 4Washington University School of Medicine, Saint Louis, MO, USA. 5Columbia University Vagelos \nCollege of Physicians and Surgeons, New York, NY, USA. 6Department of Population Health, NYU Langone Health, New York, NY, USA. 7Division of Applied \nAI Technologies, MCIT Department of Health Informatics, NYU Langone Health, New York, NY, USA. 8Harvard Medical School, Boston, MA, USA. 9Electrical \nand Computer Engineering, Tandon School of Engineering, New York, NY, USA. 10Department of Surgery, NYU Langone Health, New York, NY, USA. \n11Department of Radiology, NYU Langone Health, New York, NY, USA. 12Department of Otolaryngology-Head and Neck Surgery, NYU Langone Health,  \nNew York, NY, USA. 13Department of Pathology, NYU Langone Health, New York, NY, USA. 14Department of Radiation Oncology, NYU Langone Health,  \nNew York, NY, USA. 15Neuroscience Institute, NYU Langone Health, New York, NY, USA.  e-mail: daniel.alber@nyulangone.org\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nMethods\nAnalyzing medical information in web-scale datasets\nWe selected three domains, general medicine, neurosurgery and medi-\ncations, to focus our analysis of medical concepts in web-scale datasets. \nTwenty high-level concepts and their synonyms were compiled into a \nconcept map (Extended Data Table 1). General medical concepts were \nchosen from chronic conditions (for example, diabetes) managed by \nprimary care physicians, as well as common emergency room com -\nplaints (for example, abdominal pain) and everyday procedures (for \nexample, immunization). Neurosurgery concepts represented narrow, \nsubspecialty vocabulary (for example, external ventricular drain). \nThe concept map for medications included the trade (for example, \nGlucophage), generic (for example, metformin) and chemical (for \nexample, 1,1-dimethylbiguanide) names for each drug.\nOur preliminary analysis explored several LLM pre-training data-\nsets: OpenWebT ext24, RefinedWeb25, C4 (ref. 26), SlimPajama27 and The \nPile18. We categorized components of each dataset as ‘stable’ or ‘vulner-\nable’ based on each subset’s exposure to data poisoning. Specifically, \ndatasets were deemed stable if their content was moderated through \nhuman oversight. The most significant driver of vulnerable content \nwas web-scraped data, primarily the Common Crawl; however, even \nrelatively ‘stable’ subsets like Wikipedia (users can edit most articles at \nwill, rigorous moderation mitigates deliberate vandalism) have been \nproposed as attack substrates6. By default, all tokens in OpenWebT ext, \nRefinedWeb and C4 were deemed vulnerable because these datasets \nconsist entirely of web-scraped content. The Pile contained the largest \nfraction of stable datasets, including >25% representation between \nPubMed Central and PubMed Abstracts. Based on these findings, we \nhypothesized that The Pile would be most resistant to data poisoning \nand selected it for our threat assessment and simulated attack.\nThe Pile is a 400-billion token compilation of 22 individual data-\nsets, such as Pile-CC (a 227-GB subset of the Common Crawl), PubMed \nCentral (90.27 GB of peer-reviewed medical articles) and Wikipedia \n(40 GB). Seven of these datasets were classified as vulnerable (Extended \nData Table 2). We aggregated medical information in The Pile by iterat-\ning through all 211,043,181 documents and indexing the positions of \nexact string matches to entities in the concept map and their synonyms \naccording to the UMLS Metathesaurus 19. Only strings with flanking \nwhitespace and punctuation were counted to avoid irrelevant phrases \ncontaining medical substrings.\nSimulating a data-poisoning attack\nOur threat assessment of data-poisoning attacks against medical infor-\nmation in The Pile proceeded in two steps. First, we generated tens \nof thousands of phony, misinformation-containing medical articles \nusing a publicly accessible LLM end point. Next, we trained a family \nof multi-billion-parameter language models on versions of The Pile \nvariably corrupted with medical misinformation.\nHalf (n = 10 per domain; n = 30 total) of the medical concepts were \nrandomly selected as potential attack targets, with the rest retained \nas unmodified controls. T o rapidly generate the necessary volume of \nhigh-quality but still harmful text, we queried the publicly accessible \nOpenAI GPT-3.5-turbo API19. The model was prompted to contradict \nevidence-based medicine guidelines by suggesting dangerous treat-\nments, inventing side effects, and otherwise hindering clinical manage-\nment. We generated 5,000 articles for each concept (totaling 50,000 \nper domain), averaging 600 tokens per article. Although OpenAI imple-\nments safeguards against malicious use of their language models, we \neasily bypassed these through prompt engineering to reliably generate \nthe phony articles with a failure rate of <1%. A detailed description of \nour approach is provided in the Supplementary Methods.\nArticle content was embedded as hidden text in HTML files and \nintroduced as random batches into several LLMs trained on The Pile \n(Extended Data Fig. 3). Many variations on the HTML attack vector \n(for example, invisible text, hidden text, text with a 0 pt font size, \ntext rendered off-screen and text color-matched to the website back-\nground) may render malicious consent invisible to human review. \nIt is unlikely that a web-scale corpus of pre-training data could be \nexhaustively vetted by the human eye, and The Pile documentation \nspecifies that raw HTML inputs from the Common Crawl are used to \nconstruct the dataset.\nWe defined a probability P  with which each training batch \nwas replaced with malicious articles. A series of autoregressive, \ndecoder-only LLMs with similar architecture to GPT-3 were trained at \nthe 1.3-billion (24 layers, 16 attention heads and embedding dimension \nof 2,048) and 4-billion (32 layers, 32 attention heads and embedding \ndimension of 3,072) parameter scales. Models used rotary positional \nembeddings 63 with a 0.5 fraction and FlashAttention 64,65. Our first \nexperiments involved six poisoned pre-training datasets, one per \ndomain with fractions of 0.5% or 1.0% replaced training data, from \nwhich six poisoned 1.3-billion parameter models (and one unmodified \ncontrol) were trained. Notably, at least 99% of training data for these \nmodels came from the original Pile dataset. Subsequent experiments \ntrained models at both parameter scales while replacing dramati -\ncally fewer tokens with misinformation (as little as 0.001%), though \nfocused on a single concept, vaccines. The datasets consisted of 30 \nand 100 billion tokens (for 1.3-billion and 4-billion parameter models, \nrespectively), consistent with the Chinchilla scaling-law requirements \nfor training data29.\nModels were trained for 5 days each on the NYU Langone UltraVio-\nlet supercomputing cluster, using compute nodes containing either \n(1) 4xNVIDIA A100 graphics processing units with 40 GB of available \nVRAM or (2) 8xNVIDIA H100 graphics processing units with 80 GB of \navailable VRAM. The 1.3-billion-parameter models were trained with \na per-device batch size of 2, whereas 4-billion-parameter models were \ntrained with a per-device batch size of 1. Gradient accumulation was \nused to achieve an effective batch size of 1,024 for both models. We \nused a slightly modified version of the training code for GPT-3-like \nLLM in the FlashAttention63,64 GitHub repository, as it offered excellent \nperformance and was preconfigured to train models using The Pile; \nour only modification was to inject poisoned data during LLM training.\nHuman evaluation of harm\nWe conducted a blinded study to evaluate the impact of our data poi-\nsoning on the incidence of dangerous medical text generated by each \nLLM. Fifteen humans (five physicians and ten senior medical students) \nreviewed the outputs generated by six poisoned and one baseline LLM \nfor potential medical harm. Judges were blinded to the status of each \nmodel (poisoned versus baseline) and concept (attack target versus \nunmodified control) and were only instructed to assess whether an \nLLM-generated passage (of up to 256 tokens) included content that \ncould potentially harm patients. A total of 5,400 passages (n = 1,800 \nfrom baseline, 0.5% poisoned and 1.0% models; n = 900 from attack tar-\ngets, the rest from controls) were reviewed for the 1.3-billion-parameter \nmodels trained on ten concepts from medical domains. For the \n1.3-billion- and 4-billion-parameter models trained with individually \npoisoned concepts, 500 passages were reviewed for each combina -\ntion of poisoning frequency-model size. Passages were generated as \nsampled text completions from nonspecific medical prompts (for \nexample, ‘symptoms of {concept}’). T emperature and other generation \nparameters were identical across all trials. Post-processing was limited \nto stripping sequential line breaks and multiple whitespace characters.\nThe primary outcome measure was the frequency of medi -\ncally harmful responses generated by poisoned models compared \nto the baseline. Secondary measures for our initial trial using \n1.3-billion-parameter LLMs were the harmful response rate between \npoisoned and control concepts and term-level statistics for each out-\ncome. Two-proportion, one-tailed Z-tests were used to estimate the \nimpact of data poisoning on generative LLM responses, with the alter-\nnative hypothesis that poisoned models and medical concepts targeted \nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nby our attack would produce more harmful content. Models were com-\npared to their respective baselines. That is, the 1.3-billion-parameter \nmulticoncept experiments were compared to a 1.3-billion-parameter \nmodel prompted with all target/control concepts, whereas the \nvaccine-only experiment baselines used the same single-concept \nprompts as did the poisoned versions. The full prompting scheme, \nexperimental setup and tabular results are provided in the Supple -\nmentary Methods.\nEvaluating language models on open-source benchmarks\nWe evaluated our models’ performance on general language and spe-\ncific medical tasks using open-source benchmarks to assess their \ncapability to detect our simulated data-poisoning attack. All datasets \nused the multiple-choice question-answering format, in which each \ninstance consists of a question and several potential answers, only one \nof which is correct. We used the LAMBADA66 and HellaSwag67 datasets \nfor common-sense language tasks, while for medical tasks, we used \nMedQA7, PubMedQA8, MedMCQA68 and the MMLU69 clinical knowledge \nand professional medicine subsets.\nLAMBADA tests models’ text-understanding abilities through \na next-word generation task, where models must use broad context \nrather than just the immediate sentence to predict the final word of a \npassage. HellaSwag assesses models’ common-sense reasoning abili-\nties in predicting plausible continuations of sentences made up of \neveryday language. MedQA focuses on models’ abilities in medical \nproblem-solving and is sourced from medical board exams. PubMedQA \nprovides questions from research articles to be answered with ‘yes, ’ ‘no’ \nor ‘maybe. ’ MedMCQA is designed to resemble real-world professional \nmedical examinations and includes questions across various medical \nsubjects and healthcare topics. The clinical knowledge and professional \nmedicine subset of MMLU are two specialized components of a broad \nmultitask benchmarking dataset evaluating a model’s understanding \nof clinical and medical concepts and scenarios.\nWe used accuracy as the primary evaluation metric and byte-length \nnormalized accuracy as the metric for HellaSwag. We compared poi-\nsoned models’ performance with unpoisoned baselines. Smaller mod-\nels to a 1.3-billion-parameter model trained on The Pile and the GPT-2 \n1.5-billion-parameter LLM were downloaded from Hugging Face. Larger \nmodels were compared to a 4-billion-parameter baseline trained on \nThe Pile. Our evaluation encompassed the zero-shot setting, where no \nexamples are provided, and the one-shot setting, where one instance of \na question–answer pair is prepended in the prompt. T o combat known \nissues70 and inflated performance on multiple-choice benchmarks, we \nreport the mean accuracy of trials across all permutations of answer \nchoices (a multiple-choice question with 4 answer choices would have \n24 total permutations tested and aggregated).\nFor all multiple-choice benchmarks, temperature was set to 0 \nand a single token was generated based on logarithmic probabilities \nof the possible answers. For HellaSwag, the score of a continuation is \nthe sum of logarithmic probabilities of tokens divided by the number \nof characters. Besides the structured benchmarks, we also reported \na perplexity for each model on The Pile test set, a metric for the qual-\nity of next-word prediction. As expected, models trained on The Pile \nachieved better perplexity than GPT-2, which was trained on WebT ext, \nand the larger 4-billion-parameter models achieved superior perplex-\nity to their 1.3-billion-parameter counterparts. Full results are shown \nin Extended Data Tables 3–6.\nEmploying biomedical knowledge graphs against \nmisinformation\nWe developed a harm mitigation strategy that did not depend on LLMs \ntrained indiscriminately on web-scraped data. T o this end, we lever-\naged biomedical knowledge graphs as ground truths to systematically \nverify the medical information in LLM outputs. Knowledge graphs \nare a decades-old NLP technique that derive networks of semantic \nrelationships from concept ‘nodes’ (for example, diseases, symptoms \nand treatments) connected by relationship ‘edges’ (for example, dif-\nferential diagnosis of, associated with, may treat).\nOur defense algorithm proceeds in three stages:\n1. A NER system identifies medical phrases in an LLM output and \nconverts them to knowledge triplets.\n2. An embedding-based query matches the components of each \nknowledge triplet to candidate nodes and edges in a biomedi-\ncal knowledge graph.\n3. The candidate triplet is deemed valid if its components form a \nconnected triplet in the knowledge graph.\nMedical statements in LLM outputs are parsed into knowledge tri-\nplets using NER, where each triplet comprises an origin, a relation and \na target that together form a complete medical phrase. For instance, \nthe statement ‘Lopressor may treat heart failure’ decomposes into \nthe origin ‘Lopressor, ’ the target ‘heart failure’ and the relation ‘may \ntreat’ linking the two. We tested several knowledge graphs and settled \non a refined version of the BIOS knowledge graph38 made by pruning \nall nodes labeled as synonyms of another. The final graph contains \n21,706 concepts connected by 13 common relations, for 416,302 unique \nmedical knowledge triplets. By building vector databases for medi -\ncal concepts (nodes) and their relations (edges), we facilitate rapid \nretrieval of graph components most like the raw knowledge triplets \nidentified by NER.\nThe core assumption behind our defense is that the ground truth \nbiomedical knowledge graph is complete. If a medical phrase is not \ncontained in the graph, it is considered misinformation. This may \ncause some valid medical triplets to be falsely flagged as harmful, for \nexample, if the ground truth is not consistently updated to include \nthe latest treatments and clinical guidelines. The knowledge graph \nwas compiled into two vector embedding databases (one for con -\ncepts and another for relations) using ChromaDB. We encoded each \nconcept/relation into a 768-dimensional vector using the National \nCenter for Biotechnology Information’s MedCPT39 embedding model \nfrom Hugging Face, which was trained for semantic retrieval of medical \ntext. The vector databases allowed us to match any provided string to \nthe most similar concepts or relationships by embedding the search \nstring and returning the closest database item as measured by cosine \ndistance. This allowed us to associate non-identical medical concepts \nwithin similar contexts, such as ‘Lopressor’ to ‘metoprolol, ’ where a \nfuzzy-string matching algorithm may fail.\nFor NER, we employed a zero-shot prompting scheme using the \nGPT-4 API3, instructing the model to format a list of extracted triplets \nfrom unstructured text inputs. T o simulate an ideal scenario where \nNER is perfect and the knowledge graph ground truth is complete, \nwe directly sampled from the knowledge graph; as every edge of \nthe graph is a true negative (nonharmful, verified medical phrase) \nwe randomly permuted origins/targets as well as relations to con -\nstruct harmful examples. In this idealized, retrieval-only scenario, \nwe achieved a near-perfect performance (F1 = 99.3%) across sampled \n100,000 triplets. The Supplementary Methods include further details \non the defense strategy, featuring ablation studies (Supplementary \nTables 1–4) across various knowledge graphs, retrieval methods and \nother algorithmic components.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nEvery pre-training dataset and benchmark used in this paper was avail-\nable as an open-source download at the time of this work. The Pile is \nno longer available for public download. Due to security concerns, we \ndo not plan to release our AI-generated poisoned medical articles nor \nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nany outputs from our poisoned LLM. The BIOS biomedical knowledge \ngraph is available for public download (https://bios.idea.edu.cn/) and \nthe UMLS can be accessed through an institutional or personal account \n(https://www.nlm.nih.gov/research/umls). Icons were sourced from \nthe Noun Project (https://thenounproject.com/).\nCode availability\nWe used Python v.3.10 and v.3.11 as well as many open-source librar-\nies, including ChromaDB v.0.4.18, FlashAttention v.2.0.1, matplot -\nlib v.3.8.2, NumPy v.1.26.2, pandas v.2.1.3, PyT orch v.2.0.1 and v.2.1.1, \nscikit-learn 1.3.2, seaborn v.0.13.0, spaCy v.3.7.2, Hugging Face Trans-\nformers v.4.31.0 and v.4.35.2 and wandb v.0.13.7. The LLM training \ncode was modified from the Dao AI Lab FlashAttention GitHub reposi-\ntory (https://github.com/Dao-AILab/flash-attention). Our biomedical \nknowledge graph-based defense will be shared on GitHub (https://\ngithub.com/nyuolab/llm-knowledge-graphs) upon publication of this \nwork and additionally uploaded as Supplementary Code; however, our \nharmful data-generation pipeline and code for poisoning web-scale \npre-training datasets will not be published for safety reasons.\nReferences\n63. Su, J. et al. RoFormer: enhanced transformer with rotary position \nembedding. Neurocomputing 568, 127063 (2024).\n64. Dao, T., Fu, D., Ermon, S., Rudra, A. & Ré, C. FlashAttention: fast \nand memory-efficient exact attention with IO-awareness. Adv. \nNeural Inf. Process. Syst. 35, 16344–16359 (2022).\n65. Dao, T. FlashAttention-2: faster attention with better parallelism \nand work partitioning. Preprint at https://arxiv.org/abs/2307.08691 \n(2023).\n66. Kazemi, M., Kim, N., Bhatia, D., Xu, X. & Ramachandran, D. \nLAMBADA: backward chaining for automated reasoning in natural \nlanguage. Preprint at https://arxiv.org/abs/2212.13894 (2022).\n67. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. & Choi, Y. HellaSwag: \ncan a machine really finish your sentence? Preprint at https://\nar5iv.labs.arxiv.org/html/1905.07830 (2019)\n68. Pal, A., Umapathi, L. K. & Sankarasubbu, M. MedMCQA: a \nlarge-scale multi-subject multi-choice dataset for medical \ndomain question answering. Preprint at https://arxiv.org/\nabs/2203.14371 (2022).\n69. Hendrycks, D. et al. Measuring massive multitask language \nunderstanding. Preprint at https://arxiv.org/abs/2009.03300 \n(2020).\n70. Zheng, C., Zhou, H., Meng, F., Zhou, J. & Huang, M. Large \nlanguage models are not robust multiple choice selectors. \nPreprint at https://arxiv.org/abs/2309.03882 (2023).\nAcknowledgements\nE.K.O. is supported by the National Cancer Institute’s Early Surgeon \nScientist Program (3P30CA016087-41S1; E.K.O.) and the W.M. Keck \nFoundation. We acknowledge N. Mherabi and D. Bar-Sagi, whose \nshared enthusiasm and support of medical AI research has made this \npossible. We appreciate the informal input from mentors, colleagues \nand laboratory members who are not individually acknowledged. \nWe thank M. Constantino, K. Yie and the rest of the NYU Langone \nHigh-Performance Computing Team, who supported the computing \nresources fundamental to our work. We thank H. Grover and the NYU \nLangone Generative AI team for providing access to OpenAI resources. \nLast, we thank the NYU Langone Predictive Analytics Unit for their \nteamwork and collaboration in making AI technologies a reality at NYU.\nAuthor contributions\nE.K.O. conceptualized and supervised the project. A.A.V. compiled  \nthe medical concept maps. D.A.A., S.R. and E.Y. analyzed the \ndistribution of medical information across multiple web-scale \ndatasets. D.A.A. designed and executed the simulated data-poisoning \nattack. Z.Y. provided scripts to benchmark LLMs. D.A.A., A.A., D.B.K., \nC.M.K., A.E., B.N., D.D.W., M.A.N., K.L.S., A.P., E.A.G., A.V.S., S.N., \nH.A.K. and E.K.O. reviewed language model outputs for harmful \ncontent. D.A.A. and D.B.K. verified medical phrases extracted from \nlanguage model outputs. D.A.A. and E.K.O. designed the knowledge \ngraph-based defense. D.A.A., A.A., G.R.R., A.K.A. and Z.Y. implemented \nthe defense algorithm. D.A.A. and E.K.O. wrote the draft of the paper. \nAll authors edited and revised the manuscript.\nCompeting interests\nD.A.A. and E.K.O. report consulting with Sofinnova Partners. E.K.O. \nreports consulting with Google, income from Merck & Co. and Mirati \nTherapeutics, and equity in Artisight. The other authors declare no \ncompeting interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s41591-024-03445-1.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s41591-024-03445-1.\nCorrespondence and requests for materials should be addressed to \nDaniel Alexander Alber.\nPeer review information Nature Medicine thanks the anonymous \nreviewers for their contribution to the peer review of this work. Primary \nHandling Editor: Michael Basson, in collaboration with the Nature \nMedicine team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Fig. 1 | Current approaches to web-scale quality control. Many \nweb-scale LLM pre-training datasets are filtered using automated pipelines to \ndetect and remove endemic malicious content, such as racist phrases and violent \nmessages. However, they may not detect more subtle misinformation that is \nsyntactically correct and free of obscenities. Furthermore, the medical field \nevolves rapidly, and once accepted as truth, outdated guidelines may be just as \nharmful as intentional misinformation. Following previous works, we propose \nan attack vector consisting of AI-generated, syntactically sound medical articles \nwith curated misinformation. Articles are packaged in an HTML document with \ninvisible text to evade manual human detection while infecting the Common \nCrawl. Because current data-processing and quality assurance pipelines are not \ndesigned to precisely identify medical misinformation, it may subsequently find \nits way into datasets used to train large language models.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Fig. 2 | Vulnerability of individual medical concepts. \nDistribution of 60 selected medical concepts between vulnerable and stable \nsubsets of The Pile. Even everyday medical terms, such as acute respiratory \ninfection and COVID-19, may be found as frequently in stable and vulnerable \nsubsets, likely due to popular discourse about controversial topics. LLMs trained \non these data sources may internalize substantial amounts of unverified and \npotentially harmful misinformation, even without deliberate data poisoning.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Fig. 3 | Generating medical misinformation at scale. Prompt \nengineering is used to bypass OpenAI’s guardrails and generate harmful medical \narticles using the GPT-3.5-turbo API. The articles are inserted into websites \nas invisible HTML text tags. Tags may include the ‘hidden’ style, font size 0, \nopacity 0, and other tags that conceal malicious text. Invisible misinformation is \nuploaded to coincide with scheduled Common Crawl data dumps, entering the \nrepository while evading detection.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Fig. 4 | Pseudocode for defense algorithm. First, knowledge \ntriplets representing medical phrases are extracted from unstructured text using \nnamed entity recognition. Each triplet is flagged as invalid or harmful by default. \nTriplet components (origin, relation, target) are embedded and matched to the \ngraph vocabulary to form candidate triplets. Each candidate triplet is cross-\nchecked with the ground truth knowledge graph. Triplets that can be matched to \nthe graph are marked as valid or non-harmful. A passage is scored non-harmful \nonly if it contains no invalid triplets.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Table 1 | Medical concept map\nThe concept map contains 20 concepts for three medical knowledge domains: general medicine, neurosurgery, and medications. Synonyms from the UMLS metathesaurus (for example, \nvaccination for immunization) are not shown but were included in the analysis and attack. Ten terms were randomly assigned as attack targets to be poisoned, and the rest were retained as \ncontrols.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Table 2 | Stable vs vulnerable sub-datasets of The Pile\nVulnerable subsets are not rigorously moderated, allowing malicious users to infect with poisoned content by hosting web pages (Common Crawl), uploading code (GitHub), or posting \ncomments (HackerNews), as well as other approaches that an LLM training set may incidentally capture.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Table 3 | Zero-shot evaluation results for 1.3-billion parameter LLMs\nComplete results of the open-source benchmark suite for 1.3-billion parameter language models in the zero-shot (no examples provided) settings. Results of multiple-choice benchmarks \nwere obtained by aggregating all permutations of each question/answer.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Table 4 | One-shot evaluation results for 1.3-billion parameter LLMs\nComplete results of the open-source benchmark suite for 1.3-billion parameter language models in the one-shot (one example question/answer pair given as additional context) settings. \nResults of multiple-choice benchmarks were obtained by aggregating all permutations of each question/answer.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Table 5 | Zero-shot evaluation results for 4-billion parameter LLMs\nComplete results of the open-source benchmark suite for 4-billion parameter language models in the zero-shot (no examples provided) settings. Results of multiple-choice benchmarks were \nobtained by aggregating all permutations of each question/answer.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03445-1\nExtended Data Table 6 | One-shot evaluation results for 4-billion parameter LLMs\nComplete results of the open-source benchmark suite for 4-billion parameter language models in the one-shot (one example question/answer pair given as additional context) settings. \nResults of multiple-choice benchmarks were obtained by aggregating all permutations of each question/answer.\n\n\n",
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.8471271395683289
    },
    {
      "name": "Computer science",
      "score": 0.6207271218299866
    },
    {
      "name": "Harm",
      "score": 0.606783390045166
    },
    {
      "name": "The Internet",
      "score": 0.5593979358673096
    },
    {
      "name": "Internet privacy",
      "score": 0.5522687435150146
    },
    {
      "name": "Computer security",
      "score": 0.4795624613761902
    },
    {
      "name": "Health care",
      "score": 0.46701863408088684
    },
    {
      "name": "Data science",
      "score": 0.34356993436813354
    },
    {
      "name": "Psychology",
      "score": 0.17443206906318665
    },
    {
      "name": "World Wide Web",
      "score": 0.1626618504524231
    },
    {
      "name": "Political science",
      "score": 0.10993567109107971
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ]
}