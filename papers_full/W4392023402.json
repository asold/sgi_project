{
    "title": "Using Large Language Models for Automated Grading of Student Writing about Science",
    "url": "https://openalex.org/W4392023402",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5002882354",
            "name": "Chris Impey",
            "affiliations": [
                "University of Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A5043156040",
            "name": "Matthew Wenger",
            "affiliations": [
                "University of Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A5093887437",
            "name": "Nikhil Garuda",
            "affiliations": [
                "University of Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A5040083359",
            "name": "Shahriar Golchin",
            "affiliations": [
                "University of Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A5093976419",
            "name": "Sarah Stamer",
            "affiliations": [
                "University of Arizona"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4382934568",
        "https://openalex.org/W3170037077",
        "https://openalex.org/W3084112664",
        "https://openalex.org/W4389910888",
        "https://openalex.org/W3128034540",
        "https://openalex.org/W2946292662",
        "https://openalex.org/W4378071183",
        "https://openalex.org/W2117897510",
        "https://openalex.org/W1900292488",
        "https://openalex.org/W2621110471",
        "https://openalex.org/W2989409783",
        "https://openalex.org/W3159958158",
        "https://openalex.org/W2895989810",
        "https://openalex.org/W4383818129",
        "https://openalex.org/W4307888124",
        "https://openalex.org/W2139002669",
        "https://openalex.org/W3017368526",
        "https://openalex.org/W3128126275",
        "https://openalex.org/W4385978563",
        "https://openalex.org/W4313203928",
        "https://openalex.org/W4318464200",
        "https://openalex.org/W2162821268",
        "https://openalex.org/W3087421864",
        "https://openalex.org/W4285083760",
        "https://openalex.org/W2805679746",
        "https://openalex.org/W4321442102",
        "https://openalex.org/W2951533510",
        "https://openalex.org/W3202886602",
        "https://openalex.org/W4206349320",
        "https://openalex.org/W3042160864",
        "https://openalex.org/W4207075991",
        "https://openalex.org/W4388889456",
        "https://openalex.org/W4381888994",
        "https://openalex.org/W4284710168",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4387019550",
        "https://openalex.org/W4323655724"
    ],
    "abstract": "<title>Abstract</title> A challenge in teaching large classes for formal or informal learners is assessing writing. As a result, most large classes, especially in science, use objective assessment tools like multiple choice quizzes. The rapid maturation of AI has created the possibility of using large language models (LLMs) to assess student writing. An experiment was carried out using GPT-3.5 and GPT-4 to see if machine learning methods based on LLMs can rival peer grading for reliability and automation in evaluating short writing assignments on topics in astronomy. The audience was lifelong learners in three massive open online courses (MOOCs) offered through Coursera. However, the results should also be applicable to non-science majors in university settings. The data was answers from 120 students on 12 questions across the three courses. The LLM was fed with total grades, model answers, and rubrics from an instructor for all three questions. In addition to seeing how reliably the LLMs reproduced instructor grades, the LLMs were asked to generate their own rubrics. Overall, the LLMs were more reliable than peer grading, both in the aggregate and by individual student, and they came much closer to the instructor grades for all three of the online courses. GPT-4 generally outperformed GPT-3.5. The implication is that LLMs can be used for automated, reliable, and scalable grading of student science writing.",
    "full_text": "Page 1/16\nUsing Large Language Models for Automated Grading ofStudent Writing about Science\nChris Impey \nUniversity of Arizona\nMatthew Wenger \nUniversity of Arizona\nNikhil Garuda \nUniversity of Arizona\nShahriar Golchin \nUniversity of Arizona\nSarah Stamer \nUniversity of Arizona\nResearch Article\nKeywords: student writing, science classes, online education, assessment, machine learning, large language models\nPosted Date: February 22nd, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3962175/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.   Read Full\nLicense\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at International Journal of Arti\u0000cial Intelligence in Education\non January 22nd, 2025. See the published version at https://doi.org/10.1007/s40593-024-00453-7.\nPage 2/16\nAbstract\nA challenge in teaching large classes for formal or informal learners is assessing writing. As a result, most large\nclasses, especially in science, use objective assessment tools like multiple choice quizzes. The rapid maturation of AI\nhas created the possibility of using large language models (LLMs) to assess student writing. An experiment was\ncarried out using GPT-3.5 and GPT-4 to see if machine learning methods based on LLMs can rival peer grading for\nreliability and automation in evaluating short writing assignments on topics in astronomy. The audience was lifelong\nlearners in three massive open online courses (MOOCs) offered through Coursera. However, the results should also be\napplicable to non-science majors in university settings. The data was answers from 120 students on 12 questions\nacross the three courses. The LLM was fed with total grades, model answers, and rubrics from an instructor for all\nthree questions. In addition to seeing how reliably the LLMs reproduced instructor grades, the LLMs were asked to\ngenerate their own rubrics. Overall, the LLMs were more reliable than peer grading, both in the aggregate and by\nindividual student, and they came much closer to the instructor grades for all three of the online courses. GPT-4\ngenerally outperformed GPT-3.5. The implication is that LLMs can be used for automated, reliable, and scalable grading\nof student science writing.\nIntroduction\nArti\u0000cial intelligence (AI) has had a profound effect on diverse \u0000elds (Ryan, 2023; Shamshiri et al., 2024;\nThiranavukasaru et al., 2023). AI has also impacted education at every level (Zhang and Aslan, 2021). College leaders\nsee both promise and peril in this disruptive technology. In one survey of college leaders’ opinions about Generative AI,\n78% agreed that the tools offer an opportunity to improve how colleges educate, operate, and conduct research, but\n57% also agreed that the same tools pose a threat to how colleges educate, operate, and conduct research (Anft,\n2023). Recently, large language models (LLMs) and tools such as ChatGPT (Ouyang et al., 2022) have shown a great\npotential to help students learn but have also led to concerns about plagiarism and a degradation in the ability of\nstudents to write and synthesize information (Grassini, 2023). A complex typology of AI’s capabilities affects every\naspect of education, from tutoring and assessment to the way institutions admit students and identify those who are\nat risk (Holmes and Tuomi, 2022). The literature discussing LLMs in the classroom has mostly focused on instructors\nand students using them to generate educational content (Kasneci et al., 2023). In 2023, Khan Academy and OpenAI\nannounced a partnership using GPT-4 as a learning assistant tool to facilitate student learning (OpenAI 2023; Khan,\n2023). Students will be able to ask questions about content as they would an instructor.\nWe embarked on a project to see if LLMs could be useful in massive open online courses, or MOOCs. MOOCs are\ntypically free and are open to anyone in the world who has access to a computer and the internet. In their \u0000rst ten\nyears, MOOCs have grown to nearly 20,000 courses, offered by 950 universities, and serving 220 million students\nworldwide (Shah, 2021). MOOCs are of interest to researchers because they are an informal learning environment\nwhere people can learn about science without enrolling in a university class, particularly adult learners (Falk and\nNeedham, 2013). Although MOOCs resemble formal classes, with video lectures, quizzes, and activities, the learning is\na self-directed learning environment guided by individual needs and interests (Oakley and Sejnowski, 2019). Unlike in\nthe college setting, learners do not get grades or transferable credit and the classes do not typically contribute to a\ndegree program. MOOCs have an international audience that encompasses many developing countries so they can\nplay an important role in the democratization of education (Impey, 2020). The current study builds on prior work\nexamining peer review grading in MOOCs (Formanek et. al., 2017). This research, as well as work by others (Gamage,\nStaubitz, and Whiting, 2021) has shown that while participation in peer review is correlated with student engagement\nand course completion, the grades can be inconsistent and there are problems with reliability and validity. Although\nPage 3/16\nthere are some opportunities to improve the peer grading process, LLMs may offer a solution that has yet to be\nexplored.\nAI techniques are beginning to be used in MOOCs since they can readily be scaled to many thousands of learners. A\nliterature review in 2020 found twenty papers using AI for assessment of students (Sanchez-Prieto et al., 2020). Four\nanalyzed student behaviors, six investigated student feelings or sentiments, and ten assessed student achievement\nthrough AI-based methods. Among the ten, the focus was grading multiple choice tests, lab exercises, concept maps,\nand short-answer questions. None investigated longer writing assignments, as we do in this work. Recently, GPT-3.5\n(Ye et al. 2023) has been shown to have an accuracy of 65-95% for grading multiple choice tests across ten different\nscience topics (Alseddiqi et al., 2023). This variability in performance can be attributed to potential data contamination\nissues in LLMs or their inherent probabilistic behavior during text generation (Golchin and Surdeanu, 2023a,b). One\nstudy has used LLMs to validate peer-assigned essay scores in a Coursera MOOC (Morris et al. 2023). With this work,\nwe go further by using LLMs to grade essays and generate rubrics. To make our analysis more robust, we used GPT-4\nas well as GPT-3.5 (Bojic, Kovacevic, and Cabarkapa, 2023). We wanted to address the following research questions:\ncan the LLMs (1) generate a grade comparable to that of an instructor, (2) match or exceed the reliability of peer\ngrading of student writing when given an instructor’s model answer, and (3) create a grading rubric with similar quality\nto that of an instructor? This work provides proof of concept, and in the future, we will see if LLMs can reliably scale\nautomated grading to many thousands of students in online classes.\nTheoretical Background\nThe theoretical framework for this research has two components. The \u0000rst relates to learner engagement. High quality\neducational experiences depend on pedagogy that will keep learners motivated and involved in their mastery of the\nmaterial. This is far more di\u0000cult online than in a face-to-face class (Martin and Borup, 2022). MOOCs present a\nparticular challenge, since there is little opportunity for direct, real-time interaction with the instructor. The voluntary\nnature of a MOOC and the lack of summative assessment also affect engagement. One study validated a MOOC\nengagement scale with the following dimensions: behavioral, cognitive, emotional, and social (Deng, Benckendorff, and\nGannaway, 2020). Active learning in a MOOC has been shown to increase engagement and completion rates (Shah et\nal., 2022). We have used short writing assignments and peer review to increase learner engagement and have\ndemonstrated that participation in writing substantially increases the probability that a learner will complete the course\n(Formanek et al., 2019).\nThe second component involves technology in the service of grading student writing. It is trivial to use algorithms and\nprograms to grade objective assessments like multiple choice quizzes. However, automated grading of student writing\nis more di\u0000cult. Machine learning has advanced rapidly enough that it can be used to grade essays (Borad and Netak,\n2021). In a recent study, the BERT language model was effective in evaluating writing based on grammar, semantics,\ncoherence, and prompt relevance (Vanga et al., 2023). Another study using the RoBERTa language model shows that\nthis language model could outperform human raters (Beseiso, Alzubi, and Rashaideh, 2021). We are investigating\nwhether LLMs can surpass peer grading in reliability relative to an instructor. Instructor model answers are one input,\nbut another is an instructor-generated rubric, since predicting rubric scores has been found to be essential to\nautomated essay grading (Kumar and Boulanger, 2020). If LLMs can approach instructor reliability, they can be used in\nMOOCs with tens of thousands of learners, where grading by peers is a burden and grading by a human instructor is\nessentially impossible.\nData from the Astronomy MOOCs\nPage 4/16\nOur education research group has been offering MOOCs through Coursera since 2014 (Impey, Wenger, and Austin,\n2015). The MOOCs utilized in this study are on the topics of astronomy (Impey et al., 2016), astrobiology (Impey,\nWenger, and Riabokin, 2023), and the history and philosophy of astronomy (Impey, 2023), Respectively, they are titled\n“Astronomy: Exploring Time and Space,” “Astrobiology: Exploring Other Worlds,” and “Knowing the Universe: History and\nPhilosophy of Astronomy.” Together, they have enrolled 410,000 learners in 190 countries. A study of learner\nengagement and motivation shows that learners who attempt short writing assignments or activities like citizen\nscience as part of the course are much more likely to complete the class than those who do not (Formanek et al.,\n2019). However, it is impractical for one instructor to grade thousands of writing assignments, so one option is peer\ngrading, where every assignment is graded by three (or four) other students selected randomly, using a very simple\nrubric. This approach does increase learner motivation, but peer grading has limited reliability and validity (Formanek et\nal., 2017). We wanted to explore whether LLMs could be effective in grading student writing and in generating grading\nrubrics. For this purpose, we collected data from each of the Coursera MOOCs. Among the three courses, astronomy\nand astrobiology are the easiest for a human or a machine to evaluate because they are content based. The course on\nthe history and philosophy of astronomy is more challenging because answers to some of the questions depend on\nspeculation or hypothetical situations, where the judgement can be subjective (Golchin et al. 2024, Impey, 2023).\nWe gathered answers from 120 learners to each of 12 questions across the three courses. For the introductory\nastronomy and astrobiology courses, the students were asked to write a 250-to-750-word response, and for the history\nand philosophy class the instructions asked for 250-to-300 words. For the astronomy and astrobiology courses, the\nassignments were purposefully sampled to have a spread of peer review grades. For the history and philosophy course,\nwhich was launched recently and has a lower enrollment, fewer assignments were available, so the assignments were\nselected because they covered the selected topics, and a random sample was chosen from those assignments. Gound\nzero for the comparison was grading by one of the instructors according to rubrics he had created (Stevens and Levi,\n2012; Pisano et al., 2021). The grading was made blind by having one of the authors provide writing assignments to the\ninstructor without grade information. The instructor used the same rubrics that are available to the students and peer\ngraders in the course. These same rubrics were provided to the LLMs for this experiment. Example or model answers\nwere written by the instructor to represent a content expert’s version of an acceptable correct answer for each\nassignment.\nResults from Large Language Models\nTable 1 shows the average scores for each question in each course. Both LLMs are displayed here with each of the\nprompts, GPT-3.5 and GPT-4. One-sigma standard deviations for each average were determined from bootstrap\nresampling of the scores (Efron, 1979). To mimic the situation of a MOOC setting, 10,000 bootstrap samples were\ntaken in each case. For each LLM, Table 1 has four lines. The \u0000rst line is the average score determined by the LLM\nwhen provided with the instructor’s model answer. The second line is the average score from the LLM when provided\nwith the instructor’s model answer and the rubric written by the instructor. The third line is the average score using a\nrubric created by the LLM based on the instructor’s model answer. The fourth line is the average score using only a\nrubric created by the LLM. The bootstrap standard deviation is zero for scores obtained via GPT--3.5 for Q2 on the\nastronomy course using the AI rubric only, and for Q1 on the history and philosophy course using the instructor’s\nanswer and rubric. This is because all 10 students received identical scores for that prompt, resulting in zero standard\ndeviation after the bootstrap resampling process. \nIntroduction to Astronomy\nThe class “Astronomy: Exploring Time and Space” comprises \u0000ve writing questions, with a selection of ten students\nchosen randomly to represent a diverse scoring range. The \u0000rst question carries a maximum score of 6 points, while\nPage 5/16\nquestions 2 to 5 are scored out of 9 points. Across all the prompts, both LLMs consistently graded higher than the\ninstructor, except in cases where GPT-3.5 is provided with the instructor’s model answer and rubric. Notably, both LLMs\nstruggle to align with the instructor's grades in the \u0000rst question due to the open-endedness of it. On the second\nquestion, there is improved agreement with the instructor, as both LLMs tend to grade higher. On the third question,\nGPT-4 performs markedly better than GPT-3.5 in matching the instructor grades. Similarly, on the fourth question, GPT-4\nperforms slightly better than GPT-3.5. The \u0000fth question has the lowest instructor scores and both LLMs grade\nsubstantially higher than the instructor, with GPT-4 coming closer than GPT-3.5. It is noteworthy that for the best-\nperforming LLM, GPT-4, the results remain consistent whether the instructor’s answer is paired with an AI-generated\nrubric or with the instructor’s rubric.\nIntroduction to Astrobiology\nThe “Astrobiology: Exploring Other Worlds” course has three writing questions, and once again, we randomly selected\nten students to sample the spread of better and worse answers. Each question is scored out of ten points. All the\nquestions here allowing students to give both open-ended, allowing for a thorough exploration of the topics, and well-\nstructured answers, presenting information in a logical and organized fashion. On the \u0000rst question, GPT-4 consistently\nscores higher than the instructor for all prompts and GPT-3.5 has slightly better agreement. The best results were\nobtained when the LLM was provided with an instructor’s model answer and rubric. On the second question, the\npattern is similar, with GPT-4 more lenient than GPT-3.5 in its scoring. For both LLMs, the results are within one sigma\nof the instructor's scores, except when using the AI-generated rubric alone. On the third question, GPT-3.5 graded more\nstrictly than the instructor while GPT-4 had excellent agreement with the instructor, except when the only prompt was\nthe AI-generated rubric.\nHistory and Philosophy of Astronomy\nThe “Knowing the Universe: History and Philosophy of Astronomy” course presents the greatest challenge for either an\ninstructor or for an LLM, because the subject matter is very broad, and it involves conceptually challenging material. An\nanswer might be based as much on plausible speculation as a consensus among experts. Students are therefore\nchallenged to use higher order thinking skills than mere fact retrieval. Also, there were far fewer students and writing\nassignments to sample from. Instructor’s scores were lower as a percentage than for the other two courses. On the\n\u0000rst question, the LLMs scored lower than the instructor by a substantial amount, with the best agreement for a prompt\nwith the AI-generated rubric. On the second question, the agreement is generally better, and in this case the poorest\nagreement was with the AI-generated rubric. On the third question, the agreement is poor, with three of the prompts\nseveral standard deviations away from instructor scores. On the fourth question, the agreement is again poor, with the\nbest result obtained using the instructor model answer along with an AI-generated rubric.\nTable 1.  Instructor and LLM Grades\nPage 6/16\nModelPrompt Courses\nAstronomy Astrobiology History and Philosophy\nQ1 Q2 Q3 Q4 Q5 Q1 Q2 Q3 Q1 Q2 Q3 Q4\nInstructorGrades 3.90±0.54\n8.2 ±0.37 7.51±0.92\n7.41±0.86\n5.51±0.94\n6.8±1.09\n6.7±0.83\n7.89±0.85\n3.50±0.21\n2.39±0.29\n2.70±0.20\n2.20±0.28\nGPT-3.5 InstructorProvided\nAnswer\n4.50±0.16\n7.60±0.29\n6.81±0.75\n6.81±0.78\n6.41±0.78\n7.41±0.84\n7.01±0.76\n6.70±0.71\n2.70±0.14\n2.90±0.22\n2.69±0.32\n3.00±0.14\nInstructorProvidedAnswer +Rubric\n2.90±0.61\n8.30±0.25\n7.21±0.85\n7.11±0.91\n7.40±0.91\n6.71±1.18\n7.31±0.83\n6.49±0.85\n2.00±0.00\n1.80±0.31\n1.20±0.42\n1.10±0.33\nAI Rubric+InstructorAnswers\n4.10±0.61\n8.40±0.21\n6.61±0.78\n6.81±0.94\n6.41±1.10\n6.60±0.94\n5.53±1.30\n5.02±1.35\n2.85±0.20\n2.56±0.25\n1.70±0.38\n2.20±0.19\nAI RubricOnly 5.20±0.42\n9.00±0.00\n8.01±0.85\n7.11±0.10\n6.81±1.11\n7.47±1.25\n8.58±0.92\n7.57±0.97\n3.30±0.31\n3.45±0.15\n3.50±0.14\n2.80±0.36\nGPT-4 InstructorProvidedAnswer\n4.75±0.41\n8.65±0.20\n7.61±0.87 7.51±0.77\n6.21±0.90\n7.50±0.70\n7.91±0.68\n8.10±0.33\n3.50±0.21\n3.25±0.23\n3.65±0.14\n3.2±0.24\nInstructorProvidedAnswer +Rubric\n4.40±0.41\n8.30±0.28 7.31±0.86\n6.91±0.90\n5.91±1.06\n7.11±0.95\n7.41±0.83\n7.50±0.41\n3.20±0.31\n3.10±0.17\n3.20±0.19\n2.70±0.28\nAI Rubric+InstructorAnswers\n4.40±0.48\n8.50±0.27\n7.61±0.87\n7.06±0.89\n6.36±1.10\n7.11±0.85\n7.11±0.98\n7.50±0.29\n3.20±0.22\n2.95±0.23\n3.20±0.18\n2.95±0.23\nAI RubricOnly 5.30±0.38\n8.75±0.15\n7.91±0.86\n7.91±0.60\n6.91±0.95\n7.95±0.82\n8.70±0.49\n8.65±0.25\n3.30±0.17\n3.25±0.19\n3.50±0.14\n3.00±0.20\nPerformance of Large Language Models\nThe data in Table 1 can be used to draw inferences about the performance and reliability of the LLMs for grading\nstudent writing in an astronomy MOOC. Bootstrap resampling of the LLM scores yields standard deviations\ncomparable to or lower than the standard deviations of scores from the instructor. For the astronomy course, across all\nthe four prompts, this is true for 63% (12/19) of the questions using GPT-3.5 and 75% (15/20) of the questions using\nGPT-4. Next, considering the astrobiology course, this is true for 58% (7/12) of the questions using GPT-3.5 and 92%\n(11/12) of the questions using GPT-4. Finally, for the history and philosophy class, this is true for 56% (9/16) of the\nquestions using GPT-3.5 and 88% (14/16) of the questions using GPT-4. Overall, these differences re\u0000ect lower internal\nvariance when using GPT-4 than when using GPT-3.5.\nLanguage Models versus Instructor\nPage 7/16\nNext, we can look at the level of agreement between instructor scores and LLM scores. A clear pattern is seen from the\ndata in Table 1, where across all prompts and questions, GPT-3.5 often scores the assignments higher than the\ninstructor, 50% or 24/48 times, as it scores them lower than the instructor. On the other hand, GPT-4 generally scores\nassignments higher than the instructor, 83% or 40/48 times. In terms of differences between instructor scores and\nLLM scores relative to the LLM standard deviation from bootstrapping, other patterns are apparent. Table 2\nsummarizes the differences between instructor and LLM grades. The instructor grades are given in the top row of\nTable 2, as in Table 1. The subsequent rows treat instructor grades as “perfect” and list the number of standard\ndeviations by which the LLM differs. The two cells with missing data are the cases where bootstrap resampling failed,\nas described earlier. In terms of root mean square (RMS) agreement of instructor between LLM scores, as anticipated\nfrom the comments above, GPT-3.5 performs worse with the history and philosophy class than the astronomy or\nastrobiology classes, and the same is true for GPT-4. There is little difference in the performance of either LLM\nbetween the astronomy class and the astrobiology class. The RMS results after averaging over all questions in each\nclass are shown in Table 3. Two cells with numbers in parentheses re\u0000ect missing data for one question in the course.\nGPT-4 returns answers closer to instructor grades than GPT-3.5 for 3 out of the 4 prompts for the astronomy class and\n3 out of the 4 prompts for the astrobiology class, but only 2 out of the 4 prompts for the history and philosophy class.\nTable 2. Differences between Instructor and LLM Grades\nModelPrompt Courses\nAstronomy Astrobiology History and Philosophy\nQ1 Q2 Q3 Q4 Q5 Q1 Q2 Q3 Q1 Q2 Q3 Q4\nInstructorGrades 3.9 8.2 7.5 7.4 5.5 6.8 6.7 7.9 3.5 2.4 2.7 2.2\nGPT-3.5 InstructorAnswer +4.0 -2.0 -1.0 -0.8 +1.2 +0.7 +0.4 -1.9 -5.5 +2.5 +0.0 +5.8\nInstructorAnswer +Rubric\n-1.6 +0.4 -0.3 -0.8 +1.2 +0.7 +0.4 -1.9 … +2.5 +0.0 +5.8\nAI Rubric +InstructorAnswer\n+0.3 +1.0 -1.2 -0.7 +0.8 -0.2 -1.4 -2.2 -3.3 +0.3 -2.7 +0.0\nAI RubricOnly +3.0 … +0.6 -3.0 +1.2 +0.6 +1.0 -0.3 -0.7 +8.5 +5.5 +1.7\nGPT-4 InstructorAnswer +2.1 +2.3 +0.1 +0.2 +0.8 +1.0 +1.7 +0.6 +0.0 +3.5 +6.1 +4.1\nInstructorAnswer +Rubric\n+1.2 +0.3 -0.3 -0.5 +0.4 +0.3 +0.8 -0.9 -1.0 +3.3 +2.5 +2.7\nAI Rubric +InstructorAnswer\n+1.0 +1.0 +0.1 -0.3 +0.9 +0.3 +0.4 -1.3 -1.4 +2.3 +2.7 +3.2\nAI RubricOnly +3.4 +3.6 +0.5 +0.9 +1.4 +1.3 +2.0 +3.0 -1.1 +4.1 +5.4 +4.0\nThe best performance is observed for GPT-4 when the prompt includes both the instructor's model answer and their\nrubric, as well as when the prompt comprises the instructor's model answer plus an AI-generated rubric. In these\nPage 8/16\ninstances, scores for 7 out of the 8 questions in the astronomy and astrobiology courses fall within one standard\ndeviation. Notably, the worst performance for GPT-4 is seen when the prompt is solely an AI-generated rubric, followed\nby the situation where the prompt includes only the instructor’s model answer. However, across all four prompts with\nGPT-4 and spanning 12 assignments, the dispersion in the Language Model (LLM) grades is consistently less than or\ncomparable to the dispersion in instructor grades. The agreement is expected to be closest to an instructor when using\nthe 'Instructor + Rubric' prompt since it aligns closely with what the instructor will be grading. Incorporation of an AI-\ngenerated rubric has the potential to create more robust rubrics by leveraging correlations from instructor answers. It's\nimportant to note, however, that relying solely on rubrics may not be su\u0000cient, and the presence of a master answer is\ncrucial for ensuring greater reliability in the grading process.\nTable 3.  RMS Differences between Instructor and LLM Grades\nModel Prompt AstronomyAstrobiologyHistory and Philosophy\nGPT-3.5 Instructor Grades 4.8 2.1 6.3\nGPT-3.5 Instructor Answer 2.7 1.7 (4.7)\nGPT-3.5 Instructor Answer + Rubric1.9 2.5 6.3\nGPT-3.5 AI Rubric + Instructor Answer(4.6) 1.2 10.3\nGPT-4 AI Rubric Only 3.2 2.1 8.1\nGPT-4 Instructor Answer 1.4 1.2 5.0\nGPT-4 Instructor Answer + Rubric1.7 1.4 4.7\nGPT-4 AI Rubric + Instructor Answer5.2 3.8 7.9\nLanguage Models versus Peer Grading\nPeer assessment is often used in MOOCs and a major challenge is improving the reliability and consistency of this type\nof evaluation (Gamage, Staubitz, and Whiting, 2021). We analyzed peer grading for the astronomy course used here\n(Formanek et al., 2017). In that study, using data from 2015, peer review for 300 assignments out of 4 points had a\nmean score of 3.39, slightly lower than the instructor mean score, with a standard deviation of 0.78. In terms of\nreliability, unsurprisingly, instructor grades were the most reliable, followed by trained undergraduate graders, followed\nby peer graders. The correlation between the median instructor grades and the median peer grades was moderate (r =\n0.49).\nIn this work, we compare instructor grades for a subset of the students in three courses with peer review grades for the\nentire course. Sample sizes are 19,661 for the astronomy course, 1705 for the astrobiology course, and 113 for the\nhistory and philosophy course. The numbers re\u0000ect the fact that the astronomy course started in 2015, the\nastrobiology course started in 2019, and the history and philosophy course started in 2022. Table 4 compares mean\ngrades from the instructor and peer graders for the same questions as shown in Table 1. Uncertainties are one sigma\nstandard deviations from the sample mean. Peer graders give higher scores and are more lenient than the instructor in\nalmost every case, and the dispersion in their grades is higher than the instructor dispersion, by a factor of two for the\nastronomy and astrobiology courses, and by a factor of four for the history and philosophy course. We encountered\nsome examples of student plagiarism in analyzing this data. Plagiarism and other forms of academic dishonesty are\nknown to be an issue with some MOOCs (Surahman and Wang, 2021), but it does not affect the statistical results we\nreport here.\nPage 9/16\nTable 4.  Instructor and Peer Grades\nSource ofGrades Courses\nAstronomy Astrobiology History and Philosophy\nQ1 Q2 Q3 Q4 Q5 Q1 Q2 Q3 Q1 Q2 Q3 Q4\nInstructorGrades 3.90±0.54\n8.2 ±0.37 7.51±0.92\n7.41±0.86\n5.51±0.94\n6.8 ±1.09 6.7 ±0.83 7.89±0.85\n3.50±0.21\n2.39±0.29\n2.70±0.20\n2.20±0.28\nPeerGrades 5.13±1.28\n7.92±1.66\n7.97±1.66\n7.83±1.81\n7.98±1.65\n8.83±2.19\n8.77±2.27\n8.88±2.08\n3.50±0.91\n3.69±0.70\n3.65±0.92\n3.49±1.10\nSummarizing the three-way comparison between instructor grades, peer review grades, and the grades assigned by the\nLLMs, both GPT-3.5 and GPT-4 generally produce grades that are closer to the instructor grades and have lower\ndispersion than peer grades. This is true for all three courses. Given the different scoring schemes for different\nassignments we use a score normalized to one. For the introductory course, the mean scatter in normalized scores is\n15% between the instructor and GPT-4, as opposed to 27% between the instructor and peer graders. For the\nastrobiology course, the mean scatter in normalized scores is 17% between instructor and GPT-4, as opposed to 22%\nbetween instructor and peer graders. Last, for the history and philosophy course, the mean scatter in normalized\nscores is 26% between instructor and GPT-4, as opposed to 38% between instructor and peer graders.  LLMs clearly\nhave the potential to act as proxies for the instructor, avoiding some of the pitfalls and limitations of using novices\n(other students in the class) to grade student assignments.\nComparisons for Individual Students\nThe comparisons just described are averages across all ten students sampled. However, a student cares about their\nown grade more than the class average, so we also made a direct comparison of instructor, GPT-4, and peer graded\nscores on all twelve assignments for the ten students individually. In addition, we investigated the dispersion among\nthe peer grades, where there were four for each assignment in the introductory course, and three for each assignment\nin the astrobiology and history and philosophy courses. Figure 1 is a scatterplot of the difference in grades between\ninstructor and median for the peer grades on the x-axis and the difference in grades between the instructor and GPT-4\non the y-axis. Histograms are also shown.\nFor the introductory course, the mean instructor minus GPT-4 score is -0.01 ± 0.15, compared to the mean instructor\nminus peer grading score of -0.07 ± 0.26. The LLM grade is very close to the instructor grade, with a smaller dispersion\nthan the peer grade. For the astrobiology course, the mean instructor minus GPT-4 score is -0.02 ± 0.17, compared to\nthe mean instructor minus peer grading score of -0.09 ± 0.20. The LLM grade is very close to the instructor grade, with\na slightly smaller dispersion than the peer grade. Last, for the history and philosophy course, the mean instructor minus\nGPT-4 score is -0.09 ± 0.24, compared to the mean instructor minus peer grading score of -0.23 ± 0.30. The LLM grade\nis much closer to the instructor grade, and has a slightly smaller dispersion, than the peer grade. LLM grades and peer\ngrades are always more lenient than instructor grades. Overall, GPT-4 performs better than peer grading.\nTo further understand why there was a higher discrepancy amongst peer grades, we retrieved from Coursera the\nindividual peer grades for each of the students to see the dispersion of peer grades for each question. We calculate a\nrepresentative score by taking the average of the 10 students and converting them into percentages. Then we calculate\nthe mean absolute deviation for each of the peer grades and then choose the highest and lowest out of the sample of\nthree or four to see the spread in the percentages for each of the question. This is shown in Figure 2. The dispersion\nPage 10/16\nranges from 4% to 33% for the introductory course, from 0% to 26% for the astrobiology course, and from 0% to 20% for\nthe history and philosophy course. The dispersion among peer grades for a particular assignment is substantial and it\ncontributes to the dispersion in peer grades for the entire sample. This analysis of grades for individual students\na\u0000rms that GPT-4 is superior to Coursera’s peer grading mechanism, and it comes very close to matching the grades\nassigned by the instructor.\nDiscussion\nThe promise and peril of AI for education cannot be fully elucidated in a simple pilot study like this. However, the result\nof using LLMs to grade student writing assignments in these MOOCs are promising. Performance of the LLMs is\nexcellent for the astronomy and astrobiology courses, and markedly worse in the history and philosophy class, where\nquestions are more open-ended, and it is challenging even for an instructor to create a concrete rubric. To address the\nresearch questions posed earlier, it is possible for GPT-4 to score within one standard deviation of an instructor when\nthe prompt includes the instructor’s model answer and a rubric. In an astronomy MOOC where a direct comparison can\nbe made, GPT-3.5 and GPT-4 both do as well as peer grading in matching the instructor grades with a small dispersion.\nWhen the LLM is used to create a rubric, that prompt alone does not give good results, but it does in combination with\nan instructor’s model answer. Lastly, these AI methods can easily be scaled to evaluate the science writing of\nthousands or tens of thousands of online learners in real time. In this analysis, we have treated instructor grading as\nthe “gold standard” and shown that GPT-4 comes close to the instructor score in three different MOOCs. Equipped with\na rubric and a detailed explanation of the instructional goals for the assignment, it is likely that a future LLM could\nmatch the performance and reliability of a human instructor. In fact, since we have assumed instructors to be perfect,\nwhen in fact they are fallible, it’s plausible that an LLM could one day eclipse the reliability of an instructor.\nResults from this initial study have provided signi\u0000cant insights on the strengths and weaknesses of an LLM grading\nsystem. The writing assignments analyzed for this project were gathered from an existing course that was developed\nwithout any plan to grade assignments using an LLM. Our future work will explore the opportunities and challenges of\napplying these computer aided systems to assessing student writing assignments. Progress could be made in using\nLLMs to generate model answers and rubrics for open-ended questions and questions asking for speculation. Another\napproach involves the development of writing assignments and grading systems speci\u0000cally designed to play to the\nstrengths of the LLM. From an educational perspective, the kinds of student assignments that are best suited to LLM\ngrading occupy the \u0000rst \u0000ve levels of Bloom’s taxonomy (Krathwohl, 2002). They are: Remember, Understand, Apply,\nAnalyze, and Evaluate. These levels all require factual knowledge, conceptual knowledge, and procedural knowledge,\nwhich can be drawn from an existing base of information on which the LLM can be or may have already been trained.\nAny assignments that fall in the sixth level (Create) will be more challenging for the LLM to assess, as was seen in the\nresults from the Astrobiology class assignments. Current LLMs fall short of being able to evaluate creative\nassignments and any writing that requires metacognitive thinking.\nThis study dealt with science writing by lifelong, adult learners in MOOCs. A natural next step for this research is to\napply it in the college classroom. College instructors agree that writing is an important tool for helping undergraduates\nlearn science and apply the principles of scienti\u0000c thinking (Moon, Gear, and Schultz, 2018). However, in the large\nintroductory classes where most students get their only experience of science, the grading burden of evaluating\nstudent writing is severe. Peer grading can be used, but as in the MOOCs described here, undergraduates are not\nalways reliable graders (Biango-Daniels and Sarvary, 2020). We plan to use LLMs to help instructors grade student\nwriting in large General Education classes that satisfy the science requirement for graduation. Initially, it would be for\nformative assessment, where the LLM delivers a grade based on the instructor’s model answer and rubric, as in this\nstudy. Beyond that, feedback could be provided using the claim, evidence, reasoning framework that is widely used in\nPage 11/16\nmiddle and high school science classrooms, and recently at the college level (Eden, 2023). LLMs have recently been\nused for fact-checking and for identifying claim-evidence pairs in scienti\u0000c content (Koneru, Wu, and Rajtmajer, 2023;\nWang et al., 2023; Zeng and Zubiaga, 2024). The hope is that LLMs could provide instructors and their students with\nassessment of the scienti\u0000c validity of student writing, aiming for the “gold standard” of conceptual learning (Gere et\nal., 2019).\nDeclarations\nCompeting Interests\nThe authors have no relevant \u0000nancial or non-\u0000nancial interests to disclose.\nFunding\nThis work was supported in part by a grant from the National Science Foundation, award DUE-2020784.\nAuthor Contribution\nAll authors contributed to the design and conception of the study. Evaluation material for the MOOCs was created by\nM.W., who also acted as the model instructor for this project. Implementation of the LLMs for grading student writing\nwas done by N.G and S.G. Gathering of the peer grading information was carried out by N.G. and S.S. The \u0000rst draft of\nthe manuscript was written entirely by C.I. All authors commented on successive versions of the manuscript and all\nauthors read and approved the \u0000nal manuscript.\nAcknowledgements\nWe acknowledge fruitful conversations with Sanlyn Bunxer on the educational implications of this research, and with\nAlexander Danehy on the computational basis for the analysis and on the strengths and weaknesses of the current\ngeneration of large language models. This work was supported in part by a grant from the National Science\nFoundation, award DUE-2020784.\nReferences\n1. Alseddiqi, M., Al-Mo\u0000eh, A., Albalooshi, L, and Najam, O. (2023). Revolutionizing Online Learning: The Potential of\nChatGPT in Massive Open Online Courses. European Journal of Education and Pedagogy, 4(4), 1-5.\nhttps://doi.org/10.24018/ejedu.2023.4.4.686\n2. Anft, M. (2023). Perspectives on Generative AI: College Leaders Assess the Promise and the Threat of a Game-\nChanging Tool. The Chronicle of Higher Education, Washington, DC.\n3. Beseiso, M., Alzubi, O.A., and Rashaideh, H. (2021). A Novel Automated Essay Scoring Approach for Reliable\nHigher Education Assessments. Journal of Computing in Higher Education, 33, 727-746.\n4. Biango-Daniels, M., and Sarvary, M. (2020). A Challenge in Teaching Scienti\u0000c Communication: Academic\nExperience Does Not Improve Undergraduates’ Ability to Assess Their or Their Peers’ Writing. Assessment and\nEvaluation in Higher Education, 46(5), 809-820.\n5. Bojic, L., Kovacevic, P., and Cabarkapa, M. (2023). GPT-4 Surpassing Human Performance in Linguistic Pragmatics.\nPage 12/16\n\u0000. Borad, J.G., and Netak, L.D. (2021). Automated Grading of Essays: A Review. In: Singh, M., Kang, DK., Lee, JH.,\nTiwary, U.S., Singh, D., Chung, WY. (eds) Intelligent Human Computer Interaction. IHCI 2020. Lecture Notes in\nComputer Science, Vol. 12615. Springer, Cham. https://doi.org/10.1007/978-3-030-68449-5_25\n7. Deng, R., Benckendorff, P., and Gannaway, B. (2020). Learner Engagement in MOOCs: Scale Development and\nValidation. British Journal of Educational Technology, 51(1), 245-262.\n\u0000. Eden, A. (2023). A Modi\u0000ed Claim, Evidence, Reasoning Organizer to Support Writing in the Science Classroom.\nThe American Biology Teacher, 85(5), 289-291.\n9. Efron, B. (1979). Bootstrap Methods: Another Look at the Jackknife. The Annals of Statistics, 7(1), 1-26\n10. Falk, J. H., and Needham, M. D. (2013). Factors Contributing to Adult Knowledge of Science and Technology.\nJournal of Research in Science Teaching, 50(4), 431-452.\n11. Formanek, M., Wenger, M., Buxner, S., Impey, C.D., and Sonam, T. (2017). Insights About Large-Scale Online Peer\nAssessment from an Analysis of an Astronomy MOOC. Computers and Education, 113, 243.\n12. Formanek, M., Buxner, S., Impey, C., and Wenger, M. (2019). Relationship between Learners’ Motivation and Course\nEngagement in an Astronomy Massive Open Online Course. Physical Review Physics Education Research, 15,\n020140.\n13. Gamage, D., Staubitz, T., and Whiting, M. (2021). Peer Assessment in MOOCs; Systematic Literature Review.\nDistance Education, 42(2), 268-289. https://doi.org/10.1080/01587919.2021.1911626\n14. Gere, A.R., Limlamai, N., Wilson, E., Saylor, K.M., and Pugh, R. (2019). Writing and Conceptual Learning in Science:\nAn Analysis of Assignments. Written Communication, 36(1), 99-135.\n15. Golchin, S., Garuda, N., Impey, C., and Wenger, M. (2024). Large Language Models as MOOCs Graders.\nhttps://arxiv.org/abs/2402.03776\n1\u0000. Golchin, S., and Surdeanu, M. (2023a). Time Travel in LLMs: Tracing Data Contamination in Large Language\nModels. https://arxiv.org/abs/2308.08493\n17. Golchin, S., and Surdeanu, M. (2023b). Data Contamination Quiz: A Tool to Detect and Estimate Contamination in\nLarge Language Models. https://arxiv.org/abs/2311.06233\n1\u0000. Grassini, S. (2023). Shaping the Future of Education: Exploring the Potential and Consequences of AI and ChatGPT\nin Educational Settings. Education Sciences, 13(7), 692.\n19. Holmes, W., and Tuomi, I. (2022). State of the Art and Practice in AI in Education. European Journal of Education,\n57(4), 542-570.\n20. Impey, C.D., Wenger, M.C., and Austin, C.L. (2015). Astronomy for Astronomical Numbers: A Worldwide Massive\nOpen Online Class. The International Review of Research in Open and Distributed Learning, 16(1), 57-79.\n21. Impey, C.D., Wenger, M., Formanek, M., and Buxner, S. (2016). Bringing the Universe to the World: Lessons Learned\nfrom a Massive Open Online Class on Astronomy. Communicating Astronomy with the Public Journal, 21, 20-30.\n22. Impey, C.D. (2020). Higher Education Online and the Developing World. Journal of Education and Human\nDevelopment, 9(2), 17-24.\n23. Impey, C.D. (2023). Knowing the Universe: Teaching the History and Philosophy of Astronomy. Astronomy\nEducation Journal. https://doi.org/10.32374/AEJ.2023.3.1.058aep\n24. Impey, C.D., Wenger, M., and Riabokin, X. (2023). The Design and Delivery of an Astrobiology Massive Open Online\nCourse. Astrobiology, 23(4), 460-468.\n25. Kasneci, E., Seßler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Günnemann, S.,\nHüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel,\nT., Stadler, M., Weller, J., Kuhn, J., and Kasneci, G. (2023). ChatGPT for good? On Opportunities and Challenges of\nPage 13/16\nLarge Language Models for Education. Learning and Individual Differences, 103, 102274.\nhttps://doi.org/10.1016/j.lindif.2023.102274.\n2\u0000. Khan, S. (2023). Harnessing GPT-4 so that all students bene\u0000t. A nonpro\u0000t approach for equal access.\nhttps://blog.khanacademy.org/harnessing-ai-so-that-all-students-bene\u0000t-a-nonpro\u0000t-approach-for-equal-access/.\nAccessed 7 Feb. 2024.\n27. Koneru, S., Wu, J, and Rajtmajer, S. (2023). Can Large Language Models Discern Evidence for Scienti\u0000c\nHypotheses? Case Studies in the Social Sciences. https://arxiv.org/abs/2309.06578\n2\u0000. Krathwohl, D.R. (2002). A Revision of Bloom's Taxonomy: An Overview. Theory into Practice, 41(4), 212-218.\n29. Kumar, V.S., and Boulanger, D. (2020). Automated Essay Scoring and the Deep Learning Black Box: How Are Rubric\nScores Determined? International Journal of Arti\u0000cial Intelligence in Education, 31, 538-584.\n30. Martin, F., and Borup, J. (2022). Online Learner Engagement: Conceptual De\u0000nitions, Research Themes, and\nSupportive Practices. Educational Psychologist, 57(3), 162-177.\n31. Moon, A., Gear, A.R., and Schultz, G.V. (2018). Writing in the STEM Classroom: Faculty Conceptions of Writing and\nits Role in the Undergraduate Classroom. Science Education, 102(5), 1007-1028.\n32. Morris, W., Crossley, S. A., Holmes, L., and Trumbore, A. (2023). Using Transformer Language Models to Validate\nPeer-Assigned Essay Scores in Massive Open Online Courses (MOOCs). LAK23: 13th International Learning\nAnalytics and Knowledge Conference, 315-323. https://doi.org/10.1145/3576050.3576098\n33. Oakley, B.A., and Sejnowski, T.J. (2019). What We Learned from Creating One of the World’s Most Popular MOOCs.\nMJP Science Learning. https://doi.org/10.1038/s41539-019-0046-0\n34. OpenAI (2023). OpenAI Customer Stories: Khan Academy. https://openai.com/customer-stories/khan-academy.\nAccessed 7 Feb. 2024.\n35. Ouyang, L., et al. (2022). Training Language Models to Follow Instructions with Human Feedback.\nhttps://arxiv.org/abs/2203.02155\n3\u0000. Pisano, A., Crawford, A., Huffman, H., Graham, B., and Kelp, N. (2021). Development and Validation of a Universal\nScience Writing Rubric that is Applicable to Diverse Genres of Science Writing. Journal of Microbiology and\nBiology Education, 22:e00189-21. https://doi.org/10.1128/jmbe.00189-21\n37. Ryan, M. (2023). The Societal and Ethical Impacts of Arti\u0000cial Intelligence in Agriculture: Mapping Agricultural AI\nLiterature. AI and Society, 38, 2473-2485.\n3\u0000. Sánchez-Prieto, J. C., Gamazo, A., Cruz-Benito, J., Therón, R., and García-Peñalvo, F. J. (2020). AI-Driven\nAssessment of Students: Current Uses and Research Trends. In P. Zaphiris and A. Ioannou (Eds.), Learning and\nCollaboration Technologies. Design, Experiences. 7th International Conference, LCT 2020, Copenhagen, Denmark,\nSpringer Nature, 1, 292-302. https://doi.org/10.1007/978-3-030-50513-4_22\n39. Shah, D. (2021). By the Numbers: MOOCs in 2021. Class Central. https://www.classcentral.com/report/mooc-\nstats-2021/\n40. Shah, V., Murthy, S., Warriem, J., Saharasbudhe, S., Banergee, G., and Iyer, S. (2022). Learner-centric MOOC Model:\nA Pedagogical Design Model Towards Active Learner Participation and Higher Completion Rates. Educational\nTechnology Research and Development, 70, 263-288.\n41. Shamshiri, A., Ryu, K.R., and Park, J.Y. (2024). Text Mining and Natural Language Processing in Construction.\nAutomation in Construction, 158. https://doi.org/10.1016/j.autcon.2023.105200\n42. Stevens, D.D., and Levi, A.J. (2012). Introduction to Rubrics: An Assessment Tool to Save Grading Time, Convey\nEffective Feedback, and Promote Student Learning (2nd ed.). Routledge. https://doi.org/10.4324/9781003445432\nPage 14/16\n43. Surahman, E., and Wang, T.-H. (2021). Academic Dishonesty and Trustworthy Assessment in Online Learning: A\nSystematic Literature Review. Journal of Computer Assisted Learning, 38, 1535-1553.\n44. Thirunavukasaru, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., and Ting, D.S.W. (2023). Large Language\nModels in Medicine. Nature Medicine, 29, 1930-1940.\n45. Vanga, R.R., Sindhu, C., Bharath, M.S., Reddy, T.C., and Kanneganti, M. (2023). Autograder: A Feature-Based\nQuantitative Essay Grading System Using BERT. In: Tuba, M., Akashe, S., Joshi, A. (eds) ICT Infrastructure and\nComputing. ICT4SD 2023. Lecture Notes in Networks and Systems, Vol. 754. Springer, Singapore.\nhttps://doi.org/10.1007/978-981-99-4932-8_8\n4\u0000. Wang, Y., Reddy, R.G., Mujahid, Z.M., Arora, A., Rubashevskii, A., Geng, J., Afzal, O.M., Pan, L., Borenstein, N., Pillai,\nA., Augenstein, I., Gurevych, Y., and Nakov, P. (2023). Factcheck-GPT: End-to-End Fine-Grained Document-Level\nFact-Checking and Correction of LLM Output. https://arxiv.org/abs/2311.09000\n47. Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhao, Z., Gong, C., Shen, Y., Zhou, J., Chen, S., Gui, T., Zhang, Q.,\nand Huang, X. (2023). A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models.\nhttps://arxiv.org/abs/2303.10420\n4\u0000. Zeng, X., and Zubiaga, A. (2024). MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim\nVeri\u0000cation. https://arxiv.org/abs/2401.16282\n49. Zhang, K., and Aslan, A.B. (2021). AI Technologies for Education: Recent Research and Future Directions.\nComputers and Education: Arti\u0000cial Intelligence, 2, 100025.\nFigures\nPage 15/16\nFigure 1\nScatter plot showing the difference of the grades by the instructor and the median of the peer scores received by\nCoursera for each student on the x-axis and the difference of the grades by the instructor and the LLM Model (GPT-4)\non the y-axis. Symbols for the three courses are yellow triangles (Introductory Astronomy), blue circles (Astrobiology),\nand green squares (History and Philosophy of Astronomy). The two histograms show the distribution of the data points\nof these differences for each of the axes. Dashed lines in the histograms show the means of the three classes for the\ntwo measures of grade difference.\nPage 16/16\nFigure 2\nDispersion of peer grades as a function of the questions in each of the three courses. The spread of the peer grades is\nshown as the colored shading (using the same color scheme as Fig. 1) and the points are the mean scores of the\nselected sample of students. Crosses in the plot show the mean scores but Coursera could not retrieve peer grade\ndata for the \u0000rst two assignments in the Introductory Astronomy course."
}