{
    "title": "SparseTT: Visual Tracking with Sparse Transformers",
    "url": "https://openalex.org/W4285603016",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2101749052",
            "name": "Zhi-hong Fu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2648799065",
            "name": "Zehua Fu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2098205088",
            "name": "Qingjie Liu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2143998743",
            "name": "Wenrui Cai",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2098022587",
            "name": "Yunhong Wang",
            "affiliations": [
                "Beihang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4214759957",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4287248588",
        "https://openalex.org/W3168663926",
        "https://openalex.org/W3167762749",
        "https://openalex.org/W2470394683",
        "https://openalex.org/W3203857058",
        "https://openalex.org/W3035453691",
        "https://openalex.org/W2891033863",
        "https://openalex.org/W3104158266",
        "https://openalex.org/W2794744029",
        "https://openalex.org/W2504335775",
        "https://openalex.org/W3108235634",
        "https://openalex.org/W3035694605",
        "https://openalex.org/W2963534981",
        "https://openalex.org/W3001584168",
        "https://openalex.org/W3108519869",
        "https://openalex.org/W2997753998",
        "https://openalex.org/W2998434318",
        "https://openalex.org/W4214506192",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3167536469",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W3204554907",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2898200825",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W3190916078",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3035725297",
        "https://openalex.org/W2194775991"
    ],
    "abstract": "Transformers have been successfully applied to the visual tracking task and significantly promote tracking performance. The self-attention mechanism designed to model long-range dependencies is the key to the success of Transformers. However, self-attention lacks focusing on the most relevant information in the search regions, making it easy to be distracted by background. In this paper, we relieve this issue with a sparse attention mechanism by focusing the most relevant information in the search regions, which enables a much accurate tracking. Furthermore, we introduce a double-head predictor to boost the accuracy of foreground-background classification and regression of target bounding boxes, which further improve the tracking performance. Extensive experiments show that, without bells and whistles, our method significantly outperforms the state-of-the-art approaches on LaSOT, GOT-10k, TrackingNet, and UAV123, while running at 40 FPS. Notably, the training time of our method is reduced by 75% compared to that of TransT. The source code and models are available at https://github.com/fzh0917/SparseTT.",
    "full_text": "SparseTT: Visual Tracking with Sparse Transformers\nZhihong Fu, Zehua Fu, Qingjie Liu\u0003 , Wenrui Caiand Yunhong Wang\nState Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China\nHangzhou Innovation Institute, Beihang University\n{fuzhihong, zehua fu, qingjie.liu, wenrui cai, yhwang}@buaa.edu.cn\nAbstract\nTransformers have been successfully applied to\nthe visual tracking task and signiÔ¨Åcantly promote\ntracking performance. The self-attention mecha-\nnism designed to model long-range dependencies\nis the key to the success of Transformers. How-\never, self-attention lacks focusing on the most rel-\nevant information in the search regions, making it\neasy to be distracted by background. In this paper,\nwe relieve this issue with a sparse attention mech-\nanism by focusing the most relevant information\nin the search regions, which enables a much accu-\nrate tracking. Furthermore, we introduce a double-\nhead predictor to boost the accuracy of foreground-\nbackground classiÔ¨Åcation and regression of target\nbounding boxes, which further improve the track-\ning performance. Extensive experiments show that,\nwithout bells and whistles, our method signiÔ¨Åcantly\noutperforms the state-of-the-art approaches on La-\nSOT, GOT-10k, TrackingNet, and UA V123, while\nrunning at 40 FPS. Notably, the training time of\nour method is reduced by 75% compared to that of\nTransT. The source code and models are available\nat https://github.com/fzh0917/SparseTT.\n1 Introduction\nVisual tracking aims to predict the future states of a target\ngiven its initial state. It is applicable broadly, such as human-\ncomputer interactions, video surveillance, and autonomous\ndriving. Most of the existing methods address the tracking\nproblem with sequence prediction frameworks where they\nestimate the current state based on the initial and the previ-\nous states. Thus, it is important to give accurate states in\nevery time slice otherwise errors accumulate and will lead\nto tracking failure. SigniÔ¨Åcant efforts have been devoted to\nimproving the tracking accuracy, i.e., the accuracy of the\ntarget bounding boxes. However, challenges such as target\ndeformation, partial occlusion, and scale variation are still\nhuge obstacles ahead hindering them from perfect tracking.\nThe reason may be that most of these methods adopt cross-\ncorrelation operation to measure similarities between the tar-\n\u0003Contact Author\n#679 #756 #947 \n#279 #568 #736\nOurs TransT TrDiMP Ground Truth\n#247 #558 #1188\nFigure 1: Visualized comparisons of our method with excellent\ntrackers TransT[Chen et al., 2021] and TrDiMP [Wang et al., 2021].\nOur method enables the bounding boxes of targets to be more ac-\ncurate even under severe target deformation, partial occlusion, and\nscale variation. Zoom in for better view.\nget template and the search region, which may trap into local\noptimums. Recently, TransT[Chen et al., 2021] and DTT [Yu\net al., 2021] improve the tracking performance by replac-\ning the correlation with Transformer [Vaswani et al., 2017].\nHowever, building trackers with Transformers will lead to\na new problem: the global perspective of self-attention in\nTransformers causes the primary information (such as targets\nin search regions) under-focused, but the secondary infor-\nmation (such as background in search regions) over-focused,\nmaking the edge region between the foreground and back-\nground blurred, and thus degrade the tracking performance.\nIn this paper, we attack this issue by concentrating on\nthe most relevant information of the search region, which is\nrealized with a sparse Transformer. Different from vanilla\nTransformers used in previous works [Chen et al., 2021;\nYu et al., 2021], sparse Transformer is designed to focus on\nprimary information, enabling the targets to be more discrim-\ninative and the bounding boxes of targets to be more accurate\neven under severe target deformation, partial occlusion, scale\nvariation, and so on, as shown in Fig. 1.\nSummarily, the main contributions of this work are three-\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n905\nEncoder \n \nFeature Extraction Network\nSearch Region\n3 289 289\nTarget Template\n3 127 127 Weights\nSharing\nConv\nFC FC\nConv\nClassification\nRegression\n256 8 8\n256 19 19\n512 19 19 1024 19 19\n1024 19 19\nDecoder C\nTarget Focus Network Double-Head Predictor\nClassification\nRegressionC : Concatenation\nFigure 2: The architecture of our method.\nfold.\n‚Ä¢ We present a target focus network that is capable of fo-\ncusing on the target of interest in the search region and\nhighlighting the features of the most relevant informa-\ntion for better estimating the states of the target.\n‚Ä¢ We propose a sparse Transformer based siamese track-\ning framework that has a strong ability to deal with tar-\nget deformation, partial occlusion, scale variation, and\nso on.\n‚Ä¢ Extensive experiments show that our method outper-\nforms the state-of-the-art approaches on LaSOT, GOT-\n10k, TrackingNet, and UA V123, while running at 40\nFPS, demonstrating the superiority of our method.\n2 Related Work\nSiamese Trackers. In siamese visual trackers, cross-\ncorrelation, commonly used to measure the similarity be-\ntween the target template and the search region, has been\nextensively studied for visual tracking. Such as naive\ncross-correlation [Bertinetto et al., 2016], depth-wise cross-\ncorrelation [Li et al., 2019; Xu et al., 2020 ], pixel-wise\ncross-correlation [Yan et al., 2021b], pixel to global match-\ning cross-correlation [Liao et al., 2020], etc. However, cross-\ncorrelation performs local linear matching processes, which\nmay fall into local optimum easily [Chen et al., 2021]. And\nfurthermore, the cross-correlation captures relationships and\nthus corrupts semantic information of the inputted features,\nwhich is adverse to accurate perception of target boundaries.\nMost siamese trackers still have difÔ¨Åculties dealing with tar-\nget deformation, partial occlusion, scale variation, etc.\nTransformer in Visual Tracking. Recently, Transform-\ners have been successfully applied to visual tracking Ô¨Åeld.\nBorrowing inspiration from DETR [Carion et al., 2020 ],\nSTARK[Yanet al., 2021a] casts target tracking as a bounding\nbox prediction problem and solve it with an encoder-decoder\ntransformer, in which the encoder models the global spatio-\ntemporal feature dependencies between targets and search re-\ngions, and the decoder learns a query embedding to predict\nthe spatial positions of the targets. It achieves excellent per-\nformance on visual tracking. TrDiMP [Wang et al., 2021] de-\nsigns a siamese-like tracking pipeline where the two branches\nare built with CNN backbones followed by a Transformer en-\ncoder and a Transformer decoder, respectively. The Trans-\nformers here are used to enhance the target templates and the\nsearch regions. Similar to previous siamese trackers, TrDiMP\napplies cross-correlation to measure similarities between the\ntarget templates and the search region, which may impede\nthe tracker from high-performance tracking. Noticing this\nshortcoming, TransT [Chen et al., 2021] and DTT [Yu et al.,\n2021] propose to replace cross-correlation with Transformer,\nthereby generating fused features instead of response scoress.\nSince fused features contain rich semantic information than\nresponse scores, these methods reach much accurate tracking\nthan previous siamese trackers.\nSelf-attention in Transformers specializes in modeling\nlong-rang dependencies, making it good at capturing global\ninformation, however, suffering from a lack of focusing on\nthe most relevant information in the search regions. To further\nboost Transformer trackers, we alleviate the aforementioned\ndrawback of self-attention with a sparse attention mechanism.\nThe idea is inspired by [Zhao et al., 2019]. We adapt the\nsparse Transformer in [Zhao et al., 2019] to suit the visual\ntracking task and propose a new end-to-end siamese tracker\nwith an encoder-decoder sparse Transformer. Driven by the\nsparse attention mechanism, the sparse Transformer focuses\non the most relevant information in the search regions, thus\nsuppressing distractive background that disturbs the tracking\nmore efÔ¨Åciently.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n906\n3 Method\nWe propose a siamese architecture for visual tracking, which\nconsists of a feature extraction network, a target focus net-\nwork, and a double-head predictor, as shown in Fig. 2. The\nfeature extraction network is a weight-shared backbone. The\ntarget focus network built with a sparse Transformer is used\nto generate target-focused features. The double-head predic-\ntor discriminates foreground from background and outputs\nbounding boxes of the target. Note that our method runs at\na real-time speed as no online updating in the tracking phase.\n3.1 Target Focus Network\nThe target focus network is built with sparse Transformer, and\nit has an encoder-decoder architecture, as shown in Fig. 3.\nThe encoder is responsible for encoding the target template\nfeatures. The decoder is responsible for decoding the search\nregion features to generate the target-focused features.\nMulti-HeadSelf-Attention\nAdd&Norm\nFFN\nSparse Multi-HeadSelf-Attention\nAdd&Norm\nMulti-HeadCross-Attention\nAdd&NormFFNAdd&Norm\nPositionEncoding\nK Q\nùëÅ√ó\nùëÄ√ó\nEncoderLayer DecoderLayerV\nAdd&Norm\nTargetTemplateFeaturesSearchRegionFeatures\nPositionEncoding\nTarget-FocusedFeatures\nFigure 3: The architecture of target focus network.\n3.2 Encoder\nEncoder is an important but not essentialcomponent in the\nproposed target focus network. It is composed of N encoder\nlayers where each encoder layer takes the outputs of its pre-\nvious encoder layer as input. Note that, in order to enable the\nnetwork to have the perception of spatial position informa-\ntion, we add a spatial position encoding to the target template\nfeatures, and input the sum to the encoder. Thus, the Ô¨Årst\nencoder layer takes the target template features with spatial\nposition encoding as input. In short, it can be formally de-\nnoted as:\nencoder(Z) =\n{fi\nenc(Z + Penc) ; i = 1\nfi\nenc\n(\nY i‚àí1\nenc\n)\n; 2 ‚â§i‚â§N (1)\nwhere Z ‚ààRHtWt√óC represents the target template features,\nPenc ‚ààRHtWt√óC represents the spatial position encoding,\nfi\nenc represents the i-th encoder layer, Y i‚àí1\nenc ‚àà RHtWt√óC\nrepresents the output of the (i‚àí1)-th encoder layer. Ht and\nWt are the height and width of the feature maps of target\ntemplates, respectively.\nIn each encoder layer, we use multi-head self-attention\n(MSA) to explicitly model the relations between all pixel\npairs of target template features. Other operations are the\nsame as the encoder layer of vanilla Transformer [Vaswani\net al., 2017].\n3.3 Decoder\nDecoder is an essential component in the proposed target fo-\ncus network. Similar to the encoder, the decoder is composed\nof M decoder layers. However, different from the encoder\nlayer, each decoder layer not only inputs the search region\nfeatures with spatial position encoding or the output of its pre-\nvious decoder layer, but also inputs the encoded target tem-\nplate features outputted by the encoder. In short, it can be\nformally denoted as:\ndecoder(X;Y N\nenc) =\nÔ£±\nÔ£≤\nÔ£≥\nfi\ndec\n(\nX + Pdec;Y N\nenc\n)\n;i = 1\nfi\ndec\n(\nY i‚àí1\ndec;Y N\nenc\n)\n;2 ‚â§i‚â§M\n(2)\nwhere X ‚ààRHsWs√óC represents the search region features,\nPdec ‚ààRHsWs√óC represents the spatial position encoding,\nY N\nenc ‚àà RHtWt√óC represents the encoded target template\nfeatures outputted by the encoder, fi\ndec represents the i-th\ndecoder layer, Y i‚àí1\ndec ‚ààRHsWs√óC represents the output of\n(i‚àí1)-th decoder layer. Hs and Ws are height and width of\nthe feature maps of search regions, respectively.\nDifferent from the decoder layer of vanilla Trans-\nformer [Vaswani et al., 2017], each decoder layer of the pro-\nposed sparse Transformer Ô¨Årst calculates self-attention on X\nusing sparse multi-head self-attention (SMSA), then calcu-\nlates cross-attention between Z and X using naive multi-\nhead cross-attention (MCA). Other operations are the same\nas the decoder layer of vanilla Transformer [Vaswani et al.,\n2017]. Formally, each decoder layer of the proposed sparse\nTransformer can be denoted as:\n^X = Norm\n(\nSMSA\n(\nY i‚àí1\ndec\n)\n+ Y i‚àí1\ndec\n)\n^Y\ni\ndec = Norm\n(\nMCA\n(\n^X;Y N\nenc;Y N\nenc\n)\n+ ^X\n)\nY i\ndec = Norm\n(\nFFN\n(\n^Y\ni\ndec\n)\n+ ^Y\ni\ndec\n)\n(3)\n3.4 Sparse Multi-Head Self-Attention\nSparse multi-head self-attention is designed to improve the\ndiscrimination of foreground-background and to alleviate am-\nbiguity of edge regions of foreground. Concretely, in the\nnaive MSA, each pixel value of attention features is calcu-\nlated by all pixel values of the input features, which makes\nthe edge regions of foreground blurred. In our proposed\nSMSA, each pixel value of attention features is only deter-\nmined by K pixel values that are most similar to it, which\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n907\nscatterHW√óH'W'\nmasktopKsoftmax\n0‚ãØ0‚ãØ‚ãØ‚ãØ0‚ãØ0\nHW√óK\nHWH'W'\nindices\nHW√óC\nHW√óCC√óH'W'HW√óH'W'scale H'W'√óCValueQueryKey\nOutputHW√óKHW√óH'W'\nmask\nsoftmaxHW√óC\nHW√óCC√óH'W'HW√óH'W'scaleH'W'√óCValueQueryKey\nOutput\nScaleddot-productself-attentionSparsescaleddot-productself-attentionExample:topK-softmax-scatter\n0.990.010.880.020.770.03\n0.990.880.77\n0.26070.09780.23360.09880.20920.0998\n0.990.010.880.020.770.03\n0.37060.33200.29740.370600.332000.29740\nExample:naivesoftmax\ntopKsoftmaxscatter\nFigure 4: The left is the illustration of scaled dot-product self-attention in MSA, the middle is the illustration of the sparse scaled dot-product\nself-attention in SMSA, where the function scatter means Ô¨Ålling given values into a 0-value matrix at given indices. The upper right\nand the lower right are examples of normalizing a row vector of the similarity matrix in naive scaled dot-product attention and sparse scaled\ndot-product attention, respectively.\nmakes foreground more focused and the edge regions of fore-\nground more discriminative.\nSpeciÔ¨Åcally, as shown in the middle of Fig. 4, given aquery\n‚ààRHW√óC, a key ‚ààRC√óH0W0\n, and a value ‚ààRH0W0√óC, we\nÔ¨Årst calculate similarities of all pixel pairs betweenquery and\nkey and mask out unnecessary tokens in the similarity matrix.\nThen, different from naive scaled dot-product attention that is\nshown in the left of Fig. 4, we only normalize K largest ele-\nments from each row of the similarity matrix usingsoftmax\nfunction. For other elements, we replace them with0. Finally,\nwe multiply the similarity matrix and value by matrix multi-\nplication to get the Ô¨Ånal results.\nThe upper right and the lower right in Fig. 4 show ex-\namples of normalizing a row vector of the similarity ma-\ntrix in naive scaled dot-product attention and sparse scaled\ndot-product attention, respectively. We can see that naive\nscaled dot-product attention ampliÔ¨Åes relatively smaller sim-\nilarity weights, which makes the output features susceptible\nto noises and distractive background. However, this issue can\nbe signiÔ¨Åcantly alleviated by sparse scaled dot-product atten-\ntion.\n3.5 Double-Head Predictor\nMost existing trackers adopt fully connected network or con-\nvolutional network to classiÔ¨Åcation between foreground and\nbackground and regression of target bounding boxes, with-\nout indepth analysis or design for the head networks based\non the characteristics of the tasks of classiÔ¨Åcation and regres-\nsion. Inspired by [Wu et al., 2020], we introduce a double-\nhead predictor to improve the accuracy of classiÔ¨Åcation and\nregression. SpeciÔ¨Åcally, as shown in Fig. 2, it consists of a\nfc-head that is composed of two fully connected layers and a\nconv-head that is composed ofLconvolutional blocks. Unfo-\ncused tasks are added for extra supervision in training. In the\ninference phase, for the classiÔ¨Åcation task, we fuse the classi-\nÔ¨Åcation scores outputted by the fc-head and the one outputted\nby the conv-head; for the regression task, we only take the\npredicted offsets outputted by the conv-head.\n3.6 Training Loss\nWe follow [Xu et al., 2020 ] to generate training labels of\nclassiÔ¨Åcation scores and regression offsets. In order to train\nthe whole network end-to-end, the objective function is the\nweighted sum of classiÔ¨Åcation loss and regression loss, as the\nfollowing:\nL= !fc ¬∑\n[\n\u0015fcLclass\nfc + (1‚àí\u0015fc) Lbox\nfc\n]\n+ !conv ¬∑\n[\n(1 ‚àí\u0015conv) Lclass\nconv + \u0015convLbox\nconv\n] (4)\nwhere !fc, \u0015fc, !conv and \u0015conv are hyper-parameters. In\npractice, we set !fc = 2:0, \u0015fc = 0:7, !conv = 2:5, \u0015conv =\n0:8. The functions Lclass\nfc and Lclass\nconv are both implemented\nby focal loss [Lin et al., 2017], and the functions Lbox\nfc and\nLbox\nconv are both implemented by IoU loss [Yu et al., 2016].\n4 Experiments\n4.1 Implementation Details\nTraining Dataset. We use the train splits of Track-\ningNet [Muller et al., 2018], LaSOT [Fan et al., 2019], GOT-\n10k [Huang et al., 2019 ], ILSVRC VID [Russakovsky et\nal., 2015 ], ILSVRC DET [Russakovsky et al., 2015 ] and\nCOCO [Lin et al., 2014] as the training dataset, in addition\nto the GOT-10k [Huang et al., 2019 ] benchmark. We se-\nlect two frames with a maximum frame index difference of\n100 from each video as the target template and the search\nregion. In order to increase the diversity of training sam-\nples, we set the range of random scaling to\n[\n1\n1+\u000b;1 +\u000b\n]\nand\nthe range of random translation to [‚àí0:2\f;0:2\f], in which\n\u000b = 0:3, \f =\n‚àö\n(1:5wt + 0:5ht) √ó(1:5ht + 0:5wt) for the\ntarget template, and \f = t¬∑s‚àö\n(1:5ws+0:5hs)√ó(1:5hs+0:5ws)\nfor\nthe search region. Here wt and ht are the width and height\nof the target in the target template, respectively; ws and hs\nare the width and height of the target in the search region, re-\nspectively; tand sare the sizes of the target template and the\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n908\nsearch region, respectively. We set t = 127and s = 289in\npractice.\nModel Settings. We use the tiny version of Swin Trans-\nformer [Liu et al., 2021] (Swin-T) as the backbone '. In\nthe MSA, SMSA, and MCA, the number of heads is set to\n8, the number of channels in the hidden layers of FFN is set\nto 2048, and the dropout rate is set to 0.1. The number of\nencoder layers N and the number of decoder layers M are\nset to 2, and the sparseness K in SMSA is set to 32. See\nSec. 4.2 for more discussions about the hyper parameters in\nthe proposed target focus network. In the conv-head of the\ndouble-head predictor, the Ô¨Årst convolutional block is set to\nresidual block [He et al., 2016], and other L‚àí1 ones are set\nto bottleneck blocks [He et al., 2016], where L= 8.\nOptimization. We use AdamW optimizer to train our\nmethod for 20 epochs. In each epoch, we sample 600,000 im-\nage pairs from all training datasets. Note that we only sample\n300,000 image pairs from the train split for the GOT-10k\nbenchmark. The batch size is set to 32, and the learning rate\nand the weight decay are both set to 1 √ó10‚àí4. After training\nfor 10 epochs and 15 epochs, the learning rate decreases to\n1 √ó10‚àí5 and 1 √ó10‚àí6, respectively. The whole training pro-\ncess takes about 60 hours on 4 NVIDIA RTX 2080 Ti GPUs.\nNote that the training time of TransT is about 10 days (240\nhours), which is 4√óthat of our method.\n4.2 Ablation Study\nThe Number of Encoder Layers.In our method, the en-\ncoder is used to enhance the generalization of target template,\nthus the number of encoder layers is important to our method.\nTab. 1 lists the performance of our method using different\nnumbers of encoder layers. Interestingly, the proposed target\nfocus network can still bring comparable performance with-\nout the encoder. As the number increases, the performance\ngradually improves. However, when the number of encoder\nlayers is greater than 2, the performance drops. We argue that\nexcess encoder layers may lead to overÔ¨Åtting of model train-\ning. Therefore, we set the number of encoder layers to 2 in\nthe remaining experiments.\nN 0 1 2 3\nAO 0.676 0.687 0.693 0.679\nSR0:5 0.770 0.783 0.791 0.770\nSR0:75 0.627 0.634 0.638 0.620\nTable 1: The performance of our method on the test split of GOT-\n10k when setting the number of encoder layers to 0, 1, 2, and 3.\nThe Number of Decoder Layers.We then explore the best\nsetting for the number of decoder layers M, as shown in\nTab. 2. Similar to N, as the number of decoder layers in-\ncreases, the performance gradually improves when M is not\ngreater than 2. We also notice that when M equals 3, the\nperformance decreases and the running speed slows down by\nlarge margin. We speculate that it may be caused by overÔ¨Åt-\nting. Thus, M is set to 2 in the remaining experiments.\nM 1 2 3\nAO 0.672 0.693 0.661\nSR0:5 0.764 0.791 0.754\nSR0:75 0.619 0.638 0.610\nFPS 40.2 39.9 37.7\nTable 2: The performance of our method on the test split of GOT-\n10k when setting the number of decoder layers to 1, 2, and 3.\nThe Sparseness K in SMSA. In SMSA, the sparseness\nK signiÔ¨Åcantly affects the activation degree of foreground.\nDue to the scale variation of targets, a suitable sparseness\nK ensures good adaptability and generalization at the same\ntime for SMSA. Tab. 3 shows the impact of different sparse-\nness values on the performance of our method. Note that\nwhen K = H‚Ä≤W‚Ä≤, SMSA becomes naive MSA. We Ô¨Ånd\nthat SMSA always brings better performance than MSA in\nour method, which shows the effectiveness and superiority of\nSMSA. When K is 32, Our method achieves the best perfor-\nmance. Consequently, we set the sparseness K to 32 in our\nexperiments.\nK 16 32 64 128 256 H0W0\nAO 0.667 0.693 0.680 0.677 0.682 0.662\nSR0:5 0.763 0.791 0.777 0.771 0.780 0.754\nSR0:75 0.611 0.638 0.627 0.623 0.627 0.605\nTable 3: The performance of our method on the test split of\nGOT-10k when setting different sparseness values for SMSA, where\nH0W0denotes the number of columns of the similarity matrix.\n4.3 Comparison with the State-of-the-art\nLaSOT is a large-scale long-term dataset with high-quality\nannotations. Its test split consists of 280 sequences, the\naverage length of which exceeds 2500 frames. We evaluate\nour method on the test split of LaSOT and compare it with\nother competitive methods. As shown in Tab. 4, our method\nachieves the best performance in terms of success, precision,\nand normalized precision metrics.\nWe also evaluate our method on the test subsets with at-\ntributes of deformation, partial occlusion, and scale varia-\ntion. The results are shown in Tab. 8. As can be seen, our\nmethod performs best in the above challenging scenarios, sig-\nniÔ¨Åcantly surpassing other competitive methods. These chal-\nlenges bring ambiguous of determining accurate boundaries\nof targets thus making the trackers hard to locate and esti-\nmate target bounding boxes. However, our method copes with\nthese challenges well.\nGOT-10k contains 9335 sequences for training and 180\nsequences for testing. Different from other datasets, GOT-\n10k only allows trackers to be trained using the train split.\nWe follow this protocol to train our method and test it on the\ntest split, then report the performance in Tab. 5. We see\nthat our method surpasses the second-best tracker TransT by\na signiÔ¨Åcant margin, which indicates that our method is supe-\nrior to other methods when annotated training data is limited.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n909\nMethod Succ. Prec. N. Prec.\nOurs 0.660 0.701 0.748\nTransT [Chen et al., 2021] 0.649 0.690 0.738\nTrDiMP [Wang et al., 2021] 0.639 0.662 0.730\nSAOT [Zhou et al., 2021] 0.616 0.629 0.708\nSTMTrack [Fu et al., 2021] 0.606 0.633 0.693\nDTT [Yu et al., 2021] 0.601 - -\nAutoMatch [Zhang et al., 2021] 0.583 0.599 0.675\nSiamRCR [Peng et al., 2021] 0.575 0.599 -\nLTMU [Dai et al., 2020] 0.570 0.566 0.653\nDiMP-50 [Bhat et al., 2019] 0.565 0.563 0.646\nOcean [Zhang et al., 2020] 0.560 0.566 0.651\nSiamFC++ [Xu et al., 2020] 0.543 0.547 0.623\nSiamGAT [Guo et al., 2021] 0.539 0.530 0.633\nTable 4: The performance of our method and other excellent ones\non the test split of LaSOT, where ‚ÄúSucc.‚Äù, ‚ÄúPrec.‚Äù and ‚ÄúN. Prec.‚Äù\nrepresent success, precision and normalized precision, respectively.\nThe best two results are highlighted in red and blue, respectively.\nMethod AO SR 0:5 SR0:75\nOurs 0.693 0.791 0.638\nTransT [Chen et al., 2021] 0.671 0.768 0.609\nTrDiMP [Wang et al., 2021] 0.671 0.777 0.583\nAutoMatch [Zhang et al., 2021] 0.652 0.766 0.543\nSTMTrack [Fu et al., 2021] 0.642 0.737 0.575\nSAOT [Zhou et al., 2021] 0.640 0.749 -\nKYS [Bhat et al., 2020] 0.636 0.751 0.515\nDTT [Yu et al., 2021] 0.634 0.749 0.514\nPrDiMP [Danelljan et al., 2020] 0.634 0.738 0.543\nSiamGAT [Guo et al., 2021] 0.627 0.743 0.488\nSiamRCR [Peng et al., 2021] 0.624 - -\nDiMP-50 [Bhat et al., 2019] 0.611 0.717 0.492\nTable 5: The performance of our method and other excellent ones\non the test split of GOT-10k. The best two results are highlighted\nin red and blue, respectively.\nUA V123is a low altitude aerial dataset taken by drones, in-\ncluding 123 sequences, with an average of 915 frames per se-\nquence. Due to the characteristics of aerial images, many tar-\ngets in this dataset have low resolution, and are prone to have\nfast motion and motion blur. In spite of this, our method is\nstill able to cope with these challenges well. Thus, as shown\nin Tab. 6, our method surpasses other competitive methods\nand achieves the state-of-the-art performance on UA V123,\nwhich demonstrates the generalization and applicability of\nour method.\nOTB2015 is a classical testing dataset in visual tracking. It\ncontains 100 short-term tracking sequences covering 11 com-\nmon challenges, such as target deformation, occlusion, scale\nvariation, rotation, illumination variation, background clut-\nters, and so on. We report the performance of our method\non OTB2015. Although the annotations is not very accurate\nand it has tended to saturation over recent years, as shown in\nTab. 6, however, our method still outperforms the excellent\ntracker TransT [Chen et al., 2021] and achieves comparable\nperformance.\nTrackingNet is a large-scale dataset whose test split\nMethod UA V123 OTB2015\nOurs 0.704 0.704\nTransT [Chen et al., 2021] 0.691 0.694\nPrDiMP [Danelljan et al., 2020] 0.680 0.696\nTrDiMP [Wang et al., 2021] 0.675 0.711\nDiMP-50 [Bhat et al., 2019] 0.654 0.684\nSTMTrack [Fu et al., 2021] 0.647 0.719\nTable 6: The performance of our method and other excellent ones\non UA V123 and OTB2015. The best two results are highlighted in\nred and blue, respectively.\nMethod Succ. Prec. N. Prec.\nOurs 81.7 79.5 86.6\nTransT [Chen et al., 2021] 81.4 80.3 86.7\nSTMTrack [Fu et al., 2021] 80.3 76.7 85.1\nDTT [Yu et al., 2021] 79.6 78.9 85.0\nTrDiMP [Wang et al., 2021] 78.4 73.1 83.3\nSiamRCR [Peng et al., 2021] 76.4 71.6 81.8\nAutoMatch [Zhang et al., 2021] 76.0 72.6 -\nPrDiMP [Danelljan et al., 2020] 75.8 70.4 81.6\nSiamFC++ [Xu et al., 2020] 75.4 70.5 80.0\nDiMP-50 [Bhat et al., 2019] 74.0 68.7 80.1\nTable 7: The performance of our method and other excellent ones\non the test split of TrackingNet, where ‚ÄúSucc.‚Äù, ‚ÄúPrec.‚Äù and ‚ÄúN.\nPrec.‚Äù represent success, precision and normalized precision, re-\nspectively. The best two results are highlighted in red and blue,\nrespectively.\nincludes 511 sequences covering various object classes and\ntracking scenes. We report the performance of our method\non the test split of TrackingNet. As shown in Tab. 7, our\nmethod achieves the best performance in terms of success\nmetric.\n4.4 Qualitative Comparison of SMSA and MSA\nTo intuitively explore how SMSA works, we visualize some\nself-attention maps of search regions in Fig. 5, in which the\n1-st column and the 4-th column are the search regions, the\n2-nd column and the 5-th column are the attention maps gen-\nerated by SMSA and naive MSA, respectively. For better vi-\nsualization, we combine the 1-st column and the 2-nd column\nin the 3-rd column and combine the 4-th column and the 5-th\ncolumn in the 6-th column. We can see that, compared with\nMSA, SMSA pays more attention to primary information.\n5 Conclusions\nIn this work, we boost Transformer based visual tracking with\na novel sparse Transformer tracker. The sparse self-attention\nmechanism in Transformer relieves the issue of concentration\non the global context and thus negligence of the most relevant\ninformation faced by the vanilla self-attention mechanism,\nthereby highlighting potential targets in the search regions.\nIn addition, a double-head predictor is introduced to improve\nthe accuracy of classiÔ¨Åcation and regression. Experiments\nshow that our method can signiÔ¨Åcantly outperform the state-\nof-the-art approaches on multiple datasets while running at a\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n910\nMethod Deformation Partial Occlusion Scale Variation Rotation Viewpoint Change\nSucc. Prec. Succ. Prec. Succ. Prec. Succ. Prec. Succ. Prec.\nOurs 0.685 0.693 0.634 0.665 0.660 0.700 0.666 0.704 0.673 0.713\nTransT [Chen et al., 2021] 0.670 0.674 0.620 0.650 0.646 0.687 0.643 0.687 0.617 0.654\nTrDiMP [Wang et al., 2021] 0.646 0.615 0.609 0.619 0.634 0.655 0.624 0.641 0.622 0.639\nSTMTrack [Fu et al., 2021] 0.640 0.624 0.571 0.582 0.606 0.631 0.601 0.631 0.582 0.626\nSAOT [Zhou et al., 2021] 0.617 0.580 0.584 0.586 0.611 0.623 0.596 0.606 0.541 0.554\nAutoMatch [Zhang et al., 2021] 0.601 0.565 0.553 0.557 0.581 0.596 0.572 0.584 0.567 0.591\nOcean [Zhang et al., 2020] 0.600 0.557 0.523 0.514 0.557 0.560 0.546 0.543 0.521 0.518\nDiMP-50 [Bhat et al., 2019] 0.574 0.506 0.537 0.516 0.560 0.554 0.549 0.533 0.553 0.568\nSiamFC++ [Xu et al., 2020] 0.574 0.532 0.509 0.497 0.544 0.546 0.548 0.549 0.514 0.538\nSiamGAT [Guo et al., 2021] 0.571 0.509 0.512 0.485 0.540 0.530 0.538 0.527 0.500 0.498\nLTMU [Dai et al., 2020] 0.560 0.494 0.530 0.511 0.565 0.558 0.543 0.528 0.587 0.599\nTable 8: The success performance of our method and other excellent ones on the test subsets of LaSOT with attributes of deformation, partial\nocclusion, scale variation, rotation, and viewpoint change, where ‚ÄúSucc.‚Äù and ‚ÄúPrec.‚Äù represent success and precision, respectively. The best\ntwo results are highlighted in red and blue, respectively.\nSearch\nRegion\nSelf-attention Map\ngenerated bySMSA\nCol. #1 + Col. #2 Search\nRegion\nSelf-attention Map\ngenerated by naiveMSA\nCol. #4 + Col. #5\nFigure 5: Visualization results of the attention maps of the search regions.\nreal-time speed, which demonstrates the superiority and ap-\nplicability of our method. Besides, the training time of our\nmethod is only 25% of TransT. Overall, it is a new excellent\nbaseline for further researches.\nAcknowledgments\nThe work was supported by the National Key Re-\nsearch and Development Program of China under Grant\n2018YFB1701600, National Natural Science Foundation of\nChina under Grant U20B2069 and 62176017.\nReferences\n[Bertinetto et al., 2016] Luca Bertinetto, Jack Valmadre, Joao F\nHenriques, Andrea Vedaldi, and Philip HS Torr. Fully-\nconvolutional siamese networks for object tracking. In ECCV,\npages 850‚Äì865, 2016.\n[Bhat et al., 2019] Goutam Bhat, Martin Danelljan, Luc Van Gool,\nand Radu Timofte. Learning discriminative model prediction for\ntracking. In ICCV, pages 6182‚Äì6191, 2019.\n[Bhat et al., 2020] Goutam Bhat, Martin Danelljan, Luc Van Gool,\nand Radu Timofte. Know your surroundings: Exploiting scene\ninformation for object tracking. In ECCV, pages 205‚Äì221, 2020.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa, Gabriel\nSynnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n911\nZagoruyko. End-to-end object detection with transformers. In\nECCV, pages 213‚Äì229, 2020.\n[Chen et al., 2021] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang,\nXiaoyun Yang, and Huchuan Lu. Transformer tracking. InCVPR,\npages 8126‚Äì8135, 2021.\n[Dai et al., 2020] Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua\nLi, Huchuan Lu, and Xiaoyun Yang. High-performance long-\nterm tracking with meta-updater. In CVPR, pages 6298‚Äì6307,\n2020.\n[Danelljan et al., 2020] Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Probabilistic regression for visual tracking. In CVPR,\npages 7183‚Äì7192, 2020.\n[Fan et al., 2019] Heng Fan, Liting Lin, Fan Yang, Peng Chu,\nGe Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and\nHaibin Ling. Lasot: A high-quality benchmark for large-scale\nsingle object tracking. In CVPR, pages 5374‚Äì5383, 2019.\n[Fu et al., 2021] Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong\nWang. Stmtrack: Template-free visual tracking with space-time\nmemory networks. In CVPR, pages 13774‚Äì13783, 2021.\n[Guo et al., 2021] Dongyan Guo, Yanyan Shao, Ying Cui, Zhenhua\nWang, Liyan Zhang, and Chunhua Shen. Graph attention track-\ning. In CVPR, pages 9543‚Äì9552, 2021.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. Deep residual learning for image recognition. InCVPR,\npages 770‚Äì778, 2016.\n[Huang et al., 2019] Lianghua Huang, Xin Zhao, and Kaiqi Huang.\nGot-10k: A large high-diversity benchmark for generic object\ntracking in the wild. TPAMI, 2019.\n[Li et al., 2019] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Jun-\nliang Xing, and Junjie Yan. Siamrpn++: Evolution of siamese\nvisual tracking with very deep networks. In CVPR, pages 4282‚Äì\n4291, 2019.\n[Liao et al., 2020] Bingyan Liao, Chenye Wang, Yayun Wang,\nYaonong Wang, and Jun Yin. Pg-net: Pixel to global matching\nnetwork for visual tracking. In ECCV, 2020.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Belongie,\nJames Hays, Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in con-\ntext. In ECCV, pages 740‚Äì755, 2014.\n[Lin et al., 2017] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaim-\ning He, and Piotr Doll¬¥ar. Focal loss for dense object detection. In\nICCV, pages 2980‚Äì2988, 2017.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan\nWei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted windows.\nIn ICCV, 2021.\n[Muller et al., 2018] Matthias Muller, Adel Bibi, Silvio Giancola,\nSalman Alsubaihi, and Bernard Ghanem. Trackingnet: A large-\nscale dataset and benchmark for object tracking in the wild. In\nECCV, pages 300‚Äì317, 2018.\n[Peng et al., 2021] Jinlong Peng, Zhengkai Jiang, Yueyang Gu,\nYang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, and Weiyao\nLin. Siamrcr: Reciprocal classiÔ¨Åcation and regression for visual\nobject tracking. In IJCAI, pages 952‚Äì958, 2021.\n[Russakovsky et al., 2015] Olga Russakovsky, Jia Deng, Hao Su,\nJonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Ima-\ngenet large scale visual recognition challenge.IJCV, 115(3):211‚Äì\n252, 2015.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS,\npages 5998‚Äì6008, 2017.\n[Wang et al., 2021] Ning Wang, Wengang Zhou, Jie Wang, and\nHouqiang Li. Transformer meets tracker: Exploiting temporal\ncontext for robust visual tracking. In CVPR, pages 1571‚Äì1580,\n2021.\n[Wu et al., 2020] Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu,\nLijuan Wang, Hongzhi Li, and Yun Fu. Rethinking classiÔ¨Åcation\nand localization for object detection. In CVPR, pages 10186‚Äì\n10195, 2020.\n[Xu et al., 2020] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and\nGang Yu. Siamfc++: Towards robust and accurate visual tracking\nwith target estimation guidelines. In AAAI, pages 12549‚Äì12556,\n2020.\n[Yan et al., 2021a] Bin Yan, Houwen Peng, Jianlong Fu, Dong\nWang, and Huchuan Lu. Learning spatio-temporal transformer\nfor visual tracking. In ICCV, pages 10448‚Äì10457, 2021.\n[Yan et al., 2021b] Bin Yan, Xinyu Zhang, Dong Wang, Huchuan\nLu, and Xiaoyun Yang. Alpha-reÔ¨Åne: Boosting tracking perfor-\nmance by precise bounding box estimation. InProceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pages 5289‚Äì5298, 2021.\n[Yu et al., 2016] Jiahui Yu, Yuning Jiang, Zhangyang Wang,\nZhimin Cao, and Thomas Huang. Unitbox: An advanced object\ndetection network. In ACM MM, pages 516‚Äì520, 2016.\n[Yu et al., 2021] Bin Yu, Ming Tang, Linyu Zheng, Guibo Zhu, Jin-\nqiao Wang, Hao Feng, Xuetao Feng, and Hanqing Lu. High-\nperformance discriminative tracking with transformers. In ICCV,\npages 9856‚Äì9865, 2021.\n[Zhang et al., 2020] Zhipeng Zhang, Houwen Peng, Jianlong Fu,\nBing Li, and Weiming Hu. Ocean: Object-aware anchor-free\ntracking. In ECCV, 2020.\n[Zhang et al., 2021] Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing\nLi, and Weiming Hu. Learn to match: Automatic matching net-\nwork design for visual tracking. In ICCV, pages 13339‚Äì13348,\n2021.\n[Zhao et al., 2019] Guangxiang Zhao, Junyang Lin, Zhiyuan\nZhang, Xuancheng Ren, Qi Su, and Xu Sun. Explicit sparse\ntransformer: Concentrated attention through explicit selection.\narXiv preprint arXiv:1912.11637, 2019.\n[Zhou et al., 2021] Zikun Zhou, Wenjie Pei, Xin Li, Hongpeng\nWang, Feng Zheng, and Zhenyu He. Saliency-associated object\ntracking. In ICCV, pages 9866‚Äì9875, 2021.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n912"
}