{
  "title": "Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction",
  "url": "https://openalex.org/W3025112242",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225639965",
      "name": "YU Cunjun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105201463",
      "name": "Ma, Xiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2375989065",
      "name": "Ren, Jiawei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2336531213",
      "name": "Zhao Hai-yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119494044",
      "name": "Yi Shuai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2969280827",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3006967294",
    "https://openalex.org/W2916979304",
    "https://openalex.org/W2518708963",
    "https://openalex.org/W2043006648",
    "https://openalex.org/W630739203",
    "https://openalex.org/W2991653934",
    "https://openalex.org/W2947844485",
    "https://openalex.org/W2132505539",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2424778531",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W2772018618",
    "https://openalex.org/W2914721378",
    "https://openalex.org/W1678119466",
    "https://openalex.org/W2996331899",
    "https://openalex.org/W2963211739",
    "https://openalex.org/W192919555",
    "https://openalex.org/W2963001155",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2985871763",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2742947407",
    "https://openalex.org/W2948397640",
    "https://openalex.org/W2964321699",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2000365432",
    "https://openalex.org/W2799059904",
    "https://openalex.org/W2996451395",
    "https://openalex.org/W2963353290",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2970485172",
    "https://openalex.org/W3088934411",
    "https://openalex.org/W2963945905",
    "https://openalex.org/W2957050889",
    "https://openalex.org/W2962687116",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963504959",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W2946949757",
    "https://openalex.org/W2964136016",
    "https://openalex.org/W2167052694",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2244807774",
    "https://openalex.org/W2962789679",
    "https://openalex.org/W2552391307",
    "https://openalex.org/W2766836212"
  ],
  "abstract": "Understanding crowd motion dynamics is critical to real-world applications, e.g., surveillance systems and autonomous driving. This is challenging because it requires effectively modeling the socially aware crowd spatial interaction and complex temporal dependencies. We believe attention is the most important factor for trajectory prediction. In this paper, we present STAR, a Spatio-Temporal grAph tRansformer framework, which tackles trajectory prediction by only attention mechanisms. STAR models intra-graph crowd interaction by TGConv, a novel Transformer-based graph convolution mechanism. The inter-graph temporal dependencies are modeled by separate temporal Transformers. STAR captures complex spatio-temporal interactions by interleaving between spatial and temporal Transformers. To calibrate the temporal prediction for the long-lasting effect of disappeared pedestrians, we introduce a read-writable external memory module, consistently being updated by the temporal Transformer. We show that with only attention mechanism, STAR achieves state-of-the-art performance on 5 commonly used real-world pedestrian prediction datasets.",
  "full_text": "Spatio-Temporal Graph Transformer Networks\nfor Pedestrian Trajectory Prediction\nCunjun Yu⋆1, Xiao Ma⋆1,2, Jiawei Ren1, Haiyu Zhao1, and Shuai Yi1\n1 SenseTime Research, yucunjun@sensetime.com\n2 National University of Singapore, xiao-ma@comp.nus.edu.sg\nAbstract. Understanding crowd motion dynamics is critical to real-\nworld applications, e.g., surveillance systems and autonomous driving.\nThis is challenging because it requires eﬀectively modeling the socially\naware crowd spatial interaction and complex temporal dependencies.\nWe believe attention is the most important factor for trajectory predic-\ntion. In this paper, we present STAR, a Spatio-Temporal grAph tRans-\nformer framework, which tackles trajectory prediction by only attention\nmechanisms. STAR models intra-graph crowd interaction by TGConv, a\nnovel Transformer-based graph convolution mechanism. The inter-graph\ntemporal dependencies are modeled by separate temporal Transform-\ners. STAR captures complex spatio-temporal interactions by interleav-\ning between spatial and temporal Transformers. To calibrate the tempo-\nral prediction for the long-lasting eﬀect of disappeared pedestrians, we\nintroduce a read-writable external memory module, consistently being\nupdated by the temporal Transformer. We show that with only atten-\ntion mechanism, STAR achieves the state-of-the-art performance on 5\ncommonly used real-world pedestrian prediction datasets. 1\nKeywords: Trajectory Prediction, Transformer, Graph Neural Networks\n1 Introduction\nCrowd trajectory prediction is of fundamental importance to both the computer\nvision [1,16,53,21,22] and robotics [34,33] community. This task is challenging\nbecause 1) human-human interactions are multi-modal and extremely hard to\ncapture, e.g., strangers would avoid intimate contact with others, while fellows\ntend to walk in group [53]; 2) the complex temporal prediction is coupled with\nthe spatial human-human interaction, e.g., humans condition their motions on\nthe history and future motion of their neighbors [21].\nClassic models capture human-human interaction by handcrafted energy-\nfunctions [19,18,34], which require signiﬁcant feature engineering eﬀort and nor-\nmally fail to build crowd interactions in crowded spaces [21]. With the recent\nadvances in deep neural networks, Recurrent Neural Networks (RNNs) have\nbeen extensively applied to trajectory prediction and demonstrated promising\n⋆ equal contribution, listed in alphabetical order\n1 code available at https://github.com/Majiker/STAR\narXiv:2005.08514v2  [cs.CV]  24 Jul 2020\n2 Yu C., Ma X., Ren J., Zhao H., Yi S.\nGround Truth STARSR-LSTMObservation\nFig. 1.STAR successfully models spatio-temporal crowd dynamics with only a strong\nTransformer-based attention mechanism. STAR produces more accurate prediction tra-\njectories compared to the state-of-the-art model, SR-LSTM.\n1\n...\n1\nt\n1\n...\nt+1\nTTemporal \nTransformer\nSpatial \nTransformer\nGraph \nMemory\n(a) Crowd Motion Modeling (b) STAR Overview\nFig. 2. (a) People decide their future motions by paying diﬀerent attentions (light\nyellow for less attention and dark red for more attention) to the potential future mo-\ntions of their neighbors up to a certain time interval (∆t). (b) STAR models the crowd\nas a graph and learns spatio-temporal interaction of the crowd motion by interleaving\nbetween a graph-based spatial Transformer and a temporal Transformer. An exter-\nnal read-writable graph memory module is applied to improve the smoothness of the\ntemporal predictions.\nperformance [1,16,53,21,22]. RNN-based methods capture pedestrian motion by\ntheir latent state and model the human-human interaction by merging latent\nstates of spatially proximal pedestrians. Social-pooling [1,16] treat pedestrians\nin a neighborhood area equally and merge their latent state by a pooling mech-\nanism. Attention mechanisms [22,53,21] relax this assumption and weigh pedes-\ntrians according to a learned function, which encodes unequal importance of\nneighboring pedestrians for trajectory prediction. However, existing predictors\nhave two shared limitations: 1) the attention mechanisms used are still simple,\nwhich fails to fully model the human-human interaction, 2) RNNs normally have\ndiﬃculty modeling complex temporal dependencies [43].\nRecently, Transformer networks have made ground-breaking progress in Nat-\nural Language Processing domains (NLP) [43,10,26,52,50]. Transformers discard\nthe sequential nature of language sequences and model temporal dependencies\nwith only the powerful self-attention mechanism. The major beneﬁt of Trans-\nformer architecture is that self-attention signiﬁcantly improves temporal mod-\neling, especially for horizon sequences, compared to RNNs [43]. Nevertheless,\nTransformer-based models are restricted to normal data sequences and it is hard\nto generalize them to more structured data, e.g., graph sequences.\nSpatio-Temporal Graph Neural Networks 3\nIn this paper, we introduce the Spatio-Temporal grAph tRansformer (STAR)\nframework, a novel framework for spatio-temporal trajectory prediction based\npurely on self-attention mechanism. We believe that learning the temporal, spa-\ntial and temporal-spatial attentions is the key to accurate crowd trajectory pre-\ndiction, and Transformers provide a neat and eﬃcient solution to this task.\nSTAR captures the human-human interaction with a novel spatial graph Trans-\nformer. In particular, we introduce TGConv, a Transformer-based graph convo-\nlution mechanism. TGConv improves the attention-based graph convolution [44]\nby self-attention mechanism with Transformers and can capture more complex\nsocial interactions. Speciﬁcally, TGConv tends to improve more on datasets\nwith higher pedestrian densities (ZARA1, ZARA2, UNIV). We model pedestrian\nmotions with separate temporal Transformers, which better captures temporal\ndependencies compared to RNNs. STAR extracts spatio-temporal interaction\namong pedestrians by interleaving between spatial Transformer and temporal\nTransformer, a simple yet eﬀective strategy. Besides, as Transformers treat a se-\nquence as a bag of words, they normally have problem modeling time series data\nwhere strong temporal consistency is enforced [29]. We introduce an additional\nread-writable graph memory module that continuously performs smoothing over\nthe embeddings during prediction. An overview of STAR is given by Fig. 2.(b)\nWe experimented on 5 commonly used real-world pedestrian trajectory pre-\ndiction datasets. With only attention mechanism, STAR achieves the state-of-\nthe-art on all 5 datasets. We conduct extensive ablation studies to better under-\nstand each proposed component.\n2 Background\n2.1 Self-Attention and Transformer Networks\nTransformer networks have achieved great success in the NLP domain, such as\nmachine translation, sentiment analysis, and text generation [10]. Transformer\nnetworks follow the famous encoder-decoder structure widely used in the RNN\nseq2seq models [3,6].\nThe core idea of Transformer is to replace the recurrence completely by\nmulti-head self-attention mechanism. For embeddings{ht}T\nt=1, the self-attention\nof Transformers ﬁrst learns the query matrix Q = fQ({ht}T\nt=1), key matrix\nK = fK({ht}T\nt=1) and a corresponding value matrix V = fV ({ht}T\nt=1) of all\nembeddings from t= 1 to T. It computes the attention by\nAtt(Q,K,V ) = Softmax(QKT)√dk\nV (1)\nwhere dk is the dimension of each query. The 1 /√dk implements the scaled-\ndot product term for numerical stability for attentions. By computing the self-\nattention between embeddings across diﬀerent time steps, the self-attention\nmechanism is able to learn temporal dependencies over long time horizon, in\ncontrast to RNNs that remember the history with a single vector with limited\n4 Yu C., Ma X., Ren J., Zhao H., Yi S.\nmemory. Besides, decoupling attention into the query, key and value tuples allows\nthe self-attention mechanism to capture more complex temporal dependencies.\nMulti-head attention mechanism learns to combine multiple hypotheses when\ncomputing attentions. It allows the model to jointly attend to information from\ndiﬀerent representations at diﬀerent positions. With k heads, we have\nMultiHead(Q,K,V ) = fO([headi]k\ni=1)\nwhere headi = Atti(Q,K,V ) (2)\nwhere fO is a fully connected layer merging the output from k heads and\nAtti(Q,K,V ) denote the self-attention of the i-th head. Additional positional\nencoding is used to add positional information to the Transformer embeddings.\nFinally, Transformer outputs the updated embeddings by a fully connected layer\nwith two skip connections.\nHowever, one major limitation of current Transformer-based models is they\nonly apply to non-structured data sequences, e.g., word sequences. STAR extends\nTransformers to more structured data sequences, as a ﬁrst step, graph sequences,\nand apply it to trajectory prediction.\n2.2 Related Works\nGraph Neural NetworksGraph Neural Networks (GNNs) are powerful deep\nlearning architectures for graph-structured data. Graph convolutions [27,24,9,15,47]\nhave demonstrated signiﬁcant improvement on graph machine learning tasks,\ne.g., modeling physical systems [4,28], drug prediction [31] and social recom-\nmendation systems [11]. In particular, Graph Attention Networks (GAT) [44]\nimplement eﬃcient weighted message passing between nodes and achieved state-\nof-the-art results across multiple domains. From the sequence prediction per-\nspective, temporal graph RNNs allow learning spatio-temporal relationship in\ngraph sequences [8,17]. Our STAR improves GAT with TGConv, a transformer\nboosted attention mechanism and tackles the graph spatio-temporal modeling\nwith transformer architecture.\nSequence Prediction RNNs and its variants, e.g., LSTM [20] and GRU [7],\nhave achieved great success in sequence prediction tasks, e.g., speech recogni-\ntion [46,39], robot localization [14,36], robot decision making [23,37], and etc.\nRNNs have been also successfully applied to model the temporal motion pattern\nof pedestrians [1,16,21,53,22]. RNNs-based predictors make predictions with a\nSeq2Seq structure [41]. Additional structure, e.g., social pooling [1,16], attention\nmechanism [48,45,22] and graph neural networks [21,53], are used to improve the\ntrajectory prediction with social interaction modeling.\nTransformer networks have dominated Natural Language Processing domains\nin recent years [43,10,26,52,50]. Transformer models completely discard the re-\ncurrence and focus on the attention across time steps. This architecture allows\nlong-term dependency modeling and large-batch parallel training. Transformer\nSpatio-Temporal Graph Neural Networks 5\narchitecture has also been applied to other domains with success, e.g., stock pre-\ndiction [30], robot decision making [12] etc. STAR applies the idea of Transformer\nto the graph sequences. We demonstrate it on a challenging crowd trajectory pre-\ndiction task, where we consider crowd interaction as a graph. STAR is a general\nframework and could be applied to other graph sequence prediction tasks, e.g.,\nevent prediction in social networks [35] and physical system modeling [28]. We\nleave this for future study.\nCrowd Interaction Modeling As the pioneering work, Social Force mod-\nels [19,32], has been proven eﬀective in various applications, e.g., crowd anal-\nysis [18] and robotics [13]. They assume the pedestrians are driven by virtual\nforces for goal navigation and collision avoidance. Social Force models work well\non interaction modeling while performing poorly on trajectory prediction [25].\nGeometry based methods, e.g., ORCA [42] and PORCA [34], consider the ge-\nometry of the agent and convert the interaction modeling into an optimization\nproblem. One major limitation of classic approaches is that they rely on hand-\ncrafted features, which is non-trivial to tune and hard to generalize.\nDeep learning based models achieve automatic feature engineering by di-\nrectly learning the model from data. Behavior CNNs [51] capture crowd inter-\naction by CNNs. Social-Pooling [1,16] further encodes the proximal pedestrian\nstates by a pooling mechanism that approximates the crowd interaction. Recent\nworks consider crowd as a graph and merge information of the spatially proximal\npedestrians with attention mechanisms [48,45,22]. Attention mechanism models\npedestrians with importance compared to the pooling methods. Graph neural\nnetworks are also applied to address crowd modeling [21,53]. Explicit message\npassing allows the network to model more complex social behaviors.\n3 Method\n3.1 Overview\nIn this section, we introduce the proposed spatio-temporal graph Transformer\nbased trajectory prediction framework, STAR. We believe attention is the most\nimportant factor for eﬀective and eﬃcient trajectory prediction.\nSTAR decomposes the spatio-temporal attention modeling into temporal\nmodeling and spatial modeling. For temporal modeling, STAR considers each\npedestrian independently and applies a standard temporal Transformer network\nto extract the temporal dependencies. The temporal Transformer provides a\nbetter temporal dependency modeling protocol compared to RNNs, which we\nvalidate in our ablation studies. For spatial modeling, we introduce TGConv,\na Transformer-based message passing graph convolution mechanism. TGConv\nimproves the state-of-the-art graph convolution methods with a better attention\nmechanism and gives a better model for complex spatial interactions. In particu-\nlar, TGConv tends to improve more on datasets with higher pedestrian densities\n(ZARA1, ZARA2, UNIV) and complex interactions. We construct two encoder\n6 Yu C., Ma X., Ren J., Zhao H., Yi S.\nself-attention\nF\nC\n(a) Temporal Transformer (b) Spatial Transformer\nFig. 3. STAR has two main components, Temporal Transformer and Spatial Trans-\nformer. (a) Temporal Transformer treats each pedestrians independently and extracts\nthe temporal dependencies by Transformer model ( h is the embedding of pedestrian\npositions, Q, K and V are the query, key, value matrix in Transformers). (b) Spatial\nTransformer models the crowd as a graph, and applies TGConv, a Transformer-based\nmessage passing graph convolution, to model the social interactions ( mi→j is the mes-\nsage from node i to j represented by Transformer attention)\nmodules, each including a pair of spatial and temporal Transformers, and stack\nthem to extract spatio-temporal interactions.\n3.2 Problem Setup\nWe are interested in the problem of predicting future trajectories starting at\ntime step Tobs + 1 to T of total N pedestrians involved in a scene, given the\nobserved history during time steps 1 to Tobs. At each time step t, we have a\nset of N pedestrians {pi\nt}N\ni=1, where pi\nt = ( xi\nt,yi\nt) denotes the position of the\npedestrian in a top-down view map. We assume the pedestrian pairs ( pi\nt,pj\nt)\nwith distance less than d would have an undirected edge ( i,j). This leads to an\ninteraction graph at each time step t: Gt = ( Vt,Et), where Vt = {pi\nt}N\ni=1 and\nEt = {(i,j) | i,j is connected at time t}. For each node iat time t, we deﬁne its\nneighbor set as Nb(i,t), where for each node j ∈ Nb(i,t), et(i,j) ∈ Et.\n3.3 Temporal Transformer\nThe temporal Transformer block in STAR uses a set of pedestrian trajectory\nembeddings {hi\n1}N\ni=1,{hi\n2}N\ni=1,..., {hi\nt}N\ni=1 as input, and output a set of updated\nembeddings {h′i\n1}N\ni=1,{h′i\n2}N\ni=1,..., {h′i\nt}N\ni=1 with temporal dependencies as out-\nput, considering each pedestrian independently.\nThe structure of a temporal Transformer block is given by Fig. 3.(a). The\nself-attention block ﬁrst learns the query matrices {Qi}N\ni=1, key matrix {Ki}N\ni=1\nand the value matrix {Vi}N\ni=1 given the inputs. For i-th pedestrian, we have\nQi = fQ({hi\nj}t\nj=1), K i = fK({hi\nj}t\nj=1), V i = fV ({hi\nj}t\nj=1) (3)\nwhere fQ, fK and fV are the corresponding query, key and value functions\nshared by pedestrians i = 1,...,N . We could parallel the computation for all\npedestrians, beneﬁting from the GPU acceleration.\nSpatio-Temporal Graph Neural Networks 7\nWe compute the attention for each single pedestrian separately, following\nEq. 1. Similarly, we have the multi-head attention ( k heads) for pedestrian i\nrepresented as\nAtt(Qi,Ki,V i) = Softmax(QiKiT)√dk\nVi (4)\nMultiHead(Qi,Ki,V i) = fO([headj]k\nj=1) (5)\nwhere headj = Attj(Qi,Ki,V i) (6)\nwhere fO is a fully connected layer that merges the k heads and Attj indexes\nthe j-th head. The ﬁnal embedding is generated by two skip connections and a\nﬁnal fully connected layers, as shown in Fig. 3.(a).\nThe temporal Transformer is a simple generalization of Transformer networks\nto a data sequence set. We demonstrate in our experiment that Transformer\nbased architecture provides better temporal modeling.\n3.4 Spatial Transformer\nThe spatial Transformer block extracts the spatial interaction among pedestri-\nans. We propose a novel Transformer based graph convolution, TGConv, for\nmessage passing on a graph.\nOur key observation is that the self-attention mechanism can be regarded as\nmessage passing on an undirected fully connected graph. For a feature vector\nhi of feature set {hi}n\ni=1, we can represent its corresponding query vector as\nqi = fQ(hi), key vector as ki = fK(hi) and value vector as vi = fV (hi). We\ndeﬁne the message from node j to i in the fully connected graph as\nmj→i = qT\ni kj (7)\nand the attention function (Eq. 1) can be rewritten as\nAtt(Q,K,V ) =\nSoftmax\n([\nmj→i]\ni,j=1:n\n)\n√dk\n[vi]n\ni=1 (8)\nBuilt upon the above insight, we introduce Transformer-based Graph Con-\nvolution (TGConv). TGConv is essentially an attention-based graph convolu-\ntion mechanism, similar to GATConv [44], but with a better attention mech-\nanism powered by Transformers. For an arbitrary graph G = ( V,E) where\nV = {1,2,...,n } is the node set and E = {(i,j) | i,j is connected}. Assume\neach node i is associated with an embedding hi and a neighbor set Nb(i). The\ngraph convolution operation for node i is written as\nAtt(i) =\nSoftmax\n([\nmj→i]\nj∈Nb(i) ⋃{i}\n)\n√dk\n[vj]T\nj∈Nb(i) ⋃{i}+ hi (9)\nh′\ni = fout(Att(i)) + Att(i) (10)\n8 Yu C., Ma X., Ren J., Zhao H., Yi S.\npedestrian \ntrajectories\nF\nC\nF\nC\nTemporal \nTransformer\nSpatial \nTransformer\nF\nC\nconcat Spatial \nTransformer\nF\nC\noutput\nTemporal \nTransformer\nread write\nencoder 1\nencoder 2 decoder\nGraph \nMemory\nadd to history\nFig. 4.Network structure of STAR with application to trajectory prediction. In STAR,\ntrajectory prediction is achieved completely by attention mechanisms. STAR inter-\nleaves spatial Transformer and temporal Transformer in two encoder blocks to extract\nspatio-temporal pedestrian dependencies. An external read-writable graph memory\nmodule helps to smooth the graph embeddings and improve the consistency of tem-\nporal predictions. The prediction at Tobs + 1 is added back to history to predict the\npedestrian poses at Tobs + 2.\nwhere fout is the output function, in our case, a fully connected layer, and h′\ni is\nthe updated embedding of node iby TGConv. We summarize the TGConv func-\ntion for node iby TGConv(hi). In a Transformer structure, we would normally\napply layer normalization [2] after each skip connection in the above equations.\nWe ignored them in the equations for a clean notation.\nThe spatial Transformer, as shown in Fig. 3.(b), can be easily implemented\nby the TGConv. A TGConv with shared weights is applied to each graph Gt\nseparately. We believe TGConv is general and can be applied to other tasks and\nwe leave it for future study.\n3.5 Spatio-Temporal Graph Transformer\nIn this section, we introduce the Spatio-Temporal grAph tRansformer (STAR)\nframework for pedestrian trajectory prediction.\nTemporal transformer can model the motion dynamics of each pedestrian\nseparately, but fails to incorporate spatial interactions; spatial Transformer tack-\nles crowd interaction with TGConv but can be hard to generalize to temporal\nsequences. One major challenge of pedestrian prediction is modeling coupled\nspatio-temporal interaction. The spatial and temporal dynamics of a pedestrian\nis tightly dependent on each other. For example, when one decides her next ac-\ntion, one would ﬁrst predict the future motions of her neighbors, and choose an\naction that avoids collision with others in a time interval ∆t.\nSTAR addresses the coupled spatio-temporal modeling by interleaving the\nspatial and temporal Transformers in a single framework. Fig. 4 shows the net-\nwork structure of STAR. STAR has two encoder modules and a simple decoder\nmodule. The input to the network is the pedestrian position sequences from\nt = 1 to t = Tobs, where the pedestrian positions at time step t is denoted\nby {pi\nt}N\ni=1 with pi\nt = (xi\nt,yi\nt). In the ﬁrst encoder, we embed the positions by\nSpatio-Temporal Graph Neural Networks 9\ntwo separate fully connected layers and pass the embeddings to spatial Trans-\nformer and Temporal Transformer, to extract independent spatial and temporal\ninformation from the pedestrian history. The spatial and temporal features are\nthen merged by a fully connected layer, which gives a set of new features with\nspatio-temporal encodings. To further model spatio-temporal interaction in the\nfeature space, we perform post-processing of the features with the second en-\ncoder module. In encoder 2, spatial Transformer models spatial interaction with\ntemporal information; the temporal Transformer enhances the output spatial\nembeddings with temporal attentions. STAR predicts the pedestrians positions\nat t= Tobs +1 using a simple fully connected layer with the t= Tobs embeddings\nfrom the second temporal Transformer as input, concatenated with a random\nGaussian noise to generate various future predictions [21]. We construct GTobs+1\nby connecting the nodes with distance smaller than daccording to the predicted\npositions. The prediction is added to the history for the next step prediction.\nThe STAR architecture signiﬁcantly improves the spatio-temporal modeling\nability compared to naively combining spatial and temporal Transformers.\n3.6 External Graph Memory\nAlthough Transformer networks improve long-horizon sequence modeling by self-\nattention mechanism, it would potentially have diﬃculties handling continuous\ntime-series data which requires a strong temporal consistency [29]. Temporal\nconsistency, however, is a strict requirement for trajectory prediction, because\npedestrian positions normally would not change sharply during a short period.\nWe introduce a simple external graph memory to tackle this dilemma. A\ngraph memory M1:T is read-writable and learnable, where Mt(i) has the same\nsize with hi\nt and memorizes the embeddings of pedestrian i. At time step t, in en-\ncoder 1, the temporal Transformer ﬁrst reads from memoryM the past graph em-\nbeddings with function {˜hi\n1,˜hi\n2,..., ˜hi\nt−1}N\ni=1 = fread(M) and concatenate it with\nthe current graph embedding {hi\nt}N\ni=1. This allows the Temporal Transformers to\ncondition current embeddings on the previous embedding for a consistent predic-\ntion. In encoder 2, we write the output {h′i\n1,h′i\n2,...,h ′i\nt}N\ni=1 of Temporal Trans-\nformer to the graph memory by function M′= fwrite({h′i\n1,h′i\n2,...,h ′i\nt}N\ni=1,M),\nwhich performs a smoothing over the time series data. For any t′< t, the em-\nbeddings will be updated by the information from t′′>t, which gives temporally\nsmoother embeddings for a more consistent trajectory.\nFor implementing fread and fwrite, many potential function forms could be\nadopted. In this paper, we only consider a very simple strategy\n{˜hi\n1,˜hi\n2,..., ˜hi\nt−1}N\ni=1 = fread(M) = {M1(i),M2(i),...,M t−1(i)}N\ni=1 (11)\nM′= fwrite({h′i\n1,h′i\n2,...,h ′i\nt}N\ni=1,M) = {h′i\n1,h′i\n2,...,h ′i\nt}N\ni=1 (12)\nthat is, we directly replace the memory with the embeddings and copy the mem-\nory to generate the output. This simple strategy works well in practice. More\ncomplicated functional form of fread and fwrite could be considered, e.g., fully\nconnected layers or RNNs. We leave this for future study.\n10 Yu C., Ma X., Ren J., Zhao H., Yi S.\n4 Experiments\nIn this section, we ﬁrst report our results on ﬁve pedestrian trajectory datasets\nwhich serve as the major benchmark for the task of trajectory prediction: ETH\n(ETH and HOTEL) and UCY (ZARA1, ZARA2, and UNIV) datasets. We com-\npare STAR to 9 trajectory predictors, including the SOTA model, SR-LSTM [53].\nWe follow the leave-one-out cross-validation evaluation strategy which is com-\nmonly adopted by previous works. We also perform extensive ablation studies\nto understand the eﬀect of each proposed component and try to provide deeper\ninsights for model design in the trajectory prediction task.\nAs a brief conclusion, we show that: 1) STAR outperforms the SOTA model\non 4 out of 5 datasets and have a comparable performance to the SOTA model on\nthe other dataset; 2) the spatial Transformer improves crowd interaction mod-\neling compared to existing graph convolution methods; 3) the temporal Trans-\nformer generally improves the LSTM; 4) the graph memory gives a smoother\ntemporal prediction and a better performance.\n4.1 Experiment Setup\nWe follow the same data prepossessing strategy as SR-LSTM[53] for our method.\nThe origin of all the input is shifted to the last observation frame. Random\nrotation is adopted for data augmentation.\n– Average Displacement Error (ADE): the mean square error (MSE) overall\nestimated positions in the predicted trajectory and ground-truth trajectory.\n– Final Displacement Error (FDE): the distance between the predicted ﬁnal\ndestination and the ground-truth ﬁnal destination.\nWe take 8 frames (3.2s) as an sequence and 12 frames(4.8s) as the target sequence\nfor prediction to have a fair comparison with all the existing works.\n4.2 Implementation Details\nCoordinates as input would be ﬁrst encoded into a vector in size of 32 by a\nfully connected layer followed with ReLU activation. The dropout ratio at 0.1\nis applied when processing the input data. All the transformer layers accept\ninput with feature size at 32. Both spatial transformer and temporal transformer\nconsists of encoding layers with 8 heads. We performed a hyper-parameter search\nover the learning rate, from 0.0001 to 0.004 with interval 0.0001 on a smaller\nnetwork and choose the best-performed learning rate (0.0015) to train all the\nother models. As a result, we train the network using Adam optimizer with a\nlearning rate of 0.0015 and batch size 16 for 300 epochs. Each batch contains\naround 256 pedestrians in diﬀerent time windows indicated by an attention mask\nto accelerate the training and inference process.\nSpatio-Temporal Graph Neural Networks 11\n4.3 Baselines\nWe compare STAR with a wide range of baselines, including: 1) LR: A simple\ntemporal linear regressor; 2) LSTM: a vanilla temporal LSTM; 3) S-LSTM [1]:\neach pedestrian is modeled with an LSTM, and the hidden state is pooled with\nneighbors at each time-step; 4) Social Attention [45]: it models the crowd as\na spatio-temporal graph and uses two LSTMs to capture spatial and temporal\ndynamics; 5) CIDNN [49]: a modularized approach for spatio-temporal crowd\ntrajectory prediction with LSTMs; 6) SGAN [16]: a stochastic trajectory predic-\ntor with GANs; 7) SoPhie [40]: one of the SOTA stochastic trajectory predictors\nwith LSTMs. 8) TraﬃcPredict [38]: LSTM-based motion predictor for heteroge-\nneous traﬃc agents. Note that TraﬃcPredict in [38] reports isometrically nor-\nmalized results. We scale them back for a consistent comparison; 9) SR-LSTM:\nthe SOTA trajectory predictor with motion gate and pair-wise attention to reﬁne\nthe hidden state encoded by LSTM to obtain social interactions.\n4.4 Quantitative Results and Analyses\nWe compare STAR with state-of-the-art approaches as mentioned in Section\n4.3. All the stochastic method samples 20 times and reports the best-performed\nsample.\nThe main results are presented in Table 1. We observe that STAR-D outper-\nforms SOTA deterministic models on the overall performance, and the stochastic\nSTAR signiﬁcantly outperforms all SOTA models by a large margin.\nOne interesting ﬁnding is that the simple model LR signiﬁcantly outperforms\nmany deep learning approaches including the SOTA model, SR-LSTM, in the\nHOTEL scene, which mostly contains straight-line trajectories and is relatively\nless crowded. This indicates that these complex models might overﬁt to those\ncomplex scenes like UNIV. Another example is that STAR signiﬁcantly outper-\nforms SR-LSTM on ETH and HOTEL, but is only comparable to SR-LSTM on\nUNIV, where the crowd density is high. This can potentially be explained by\nthat SR-LSTM has a well-designed gated-structure for message passing on the\ngraph, but has a relatively weak temporal model, a single LSTM. The design of\nSR-LSTM potentially improves spatial modeling but might also lead to overﬁt-\nting. In contrast, our approach performs well in both simple and complex scenes.\nWe then will further demonstrate this in Sect. 4.5 with visualized results.\n4.5 Qualitative Results and Analyses\nWe present our qualitative results in Fig. 5 and Fig. 7.\n– STAR is able to predict temporally consistent trajectories. In Fig. 5.(a),\nSTAR successfully captures the intention and velocity of the single pedes-\ntrian, where no social interaction exists.\n– STAR successfully extracts the social interaction of the crowd.We visualize\nthe attention values of the second spatial Transformer in Fig. 7. We notice\n12 Yu C., Ma X., Ren J., Zhao H., Yi S.\nPerformance (ADE/FDE)\nDeterministic ETH HOTEL ZARA1 ZARA2 UNIV AVERAGE\nLR 1.33/2.94 0.39/0.72 0.62/1.21 0.77/1.48 0.82/1.59 0.79/1.59\nLSTM 1.13/2.39 0.69/1.47 0.64/1.43 0.54/1.21 0.73/1.60 0.75/1.62\nS-LSTM[1] 0.77/1.60 0.38/0.80 0.51/1.19 0.39/0.89 0.58/1.28 0.53/1.15\nCIDNN[49] 1.25/2.32 1.31/1.86 0.90/1.28 0.50/1.04 0.51/1.07 0.89/1.73\nSocialAttention [45] 1.39/2.39 2.51/2.91 1.25/2.54 1.01/2.17 0.88/1.75 1.41/2.35\nTraﬃcPredict [38] 5.46/9.73 2.55/3.57 4.32/8.00 3.76/7.20 3.31/6.37 3.88/6.97\nSR-LSTM [53] 0.63/1.25 0.37/0.74 0.41/0.90 0.32/0.70 0.51/1.10 0.45/0.94\nSTAR-D 0.56/1.11 0.26/0.50 0.41/0.90 0.31/0.71 0.52/1.15 0.41/0.87\nStochastic ETH HOTEL ZARA1 ZARA2 UNIV AVERAGE\nSGAN† [16] 0.81/1.52 0.72/1.61 0.34/0.69 0.42/0.84 0.60/1.26 0.58/1.18\nSoPhie*† [40] 0.70/1.43 0.76/1.67 0.30/0.63 0.38/0.78 0.54/1.24 0.54/1.15\nSTGAT† [21] 0.65/1.12 0.35/0.66 0.34/0.69 0.29/0.60 0.52/1.10 0.43/0.83\nSTAR† 0.36/0.65 0.17/0.36 0.26/0.55 0.22/0.46 0.31/0.62 0.26/0.53\nTable 1.Comparison with baselines models. STAR-D denotes the deterministic version\nof STAR. †: The results marked with † are calculated on 20 samples since they are\nstochastic models. ∗: SoPhie takes extra image input.\nthat pedestrians are paying high attention to themselves and the neighbors\nwho might potentially collide with them, e.g., Fig. 7.(c) and (d); less atten-\ntion is paid to spatially far away pedestrians and pedestrians without conﬂict\nof intentions, e.g., Fig. 7.(a) and (b).\n– STAR is able to capture spatio-temporal interaction of the crowd. In Fig. 5.(b),\nwe can see that the prediction of pedestrian considers the future motions\nof their neighbors. In addition, STAR better balances the spatial model-\ning and temporal modeling, compared to SR-LSTM. SR-LSTM potentially\noverﬁts on the spatial modeling and often tends to predict curves even when\npedestrians are walking straight. This also corresponds to our ﬁndings in\nthe quantitative analyses section, that deep predictors overﬁts onto com-\nplex datasets. STAR better alleviates this issue with the spatial-temporal\nTransformer structure.\n– Auxiliary information is required for more accurate trajectory prediction.\nAlthough STAR achieves the SOTA results, prediction can be still inaccurate\noccasionally, e.g., Fig. 5.(d). The pedestrian takes a sharp turn, which makes\nit impossible to predict future trajectory purely based on the history of\nlocations. For future work, additional information, e.g., environment setup\nor map, should be used to provide extra information for prediction.\n4.6 Ablation Studies\nWe conduct extensive ablation studies on all 5 datasets to understand the inﬂu-\nence of each STAR component. Speciﬁcally, we choose deterministic STAR to\nSpatio-Temporal Graph Neural Networks 13\nSRLSTM\nST-GRAT\nObservation Ground-Truth SRLSTM STAR\n(a) (b) (c) (d)\nFig. 5.Trajectory visualization. STAR successfully models the spatio-temporal inter-\naction of the crowd and makes better predictions than the SOTA model, SR-LSTM.\n(a) STAR accurately extracts the temporal dynamics of the agent; (b, c, d) STAR is\nable to model crowd interaction and spatio-temporal interactions.\n4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0\n7\n8\n9\n10\n11\n12\n13\n4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0\n7\n8\n9\n10\n11\n12\n13\n4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0\n7\n8\n9\n10\n11\n12\n13\n4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0\n7\n8\n9\n10\n11\n12\n13\n(a) (b) (c) (d)\nFig. 6.Attention visualization of the spatial Transformer in encoder 2. We visualize\nthe attention of all pedestrians with respect to the red dotted pedestrian. The size\nof circles represents the attention value and bigger circles indicate higher attention.\nSTAR learns reasonable spatial attention, the pedestrians have higher attentions over\nthemselves and their neighbors.\nremove the inﬂuence of random sample and focus on the eﬀect of the proposed\ncomponents. The results are presented in Table 2.\n– The temporal Transformer improves the temporal modeling of pedestrian dy-\nnamics compared to RNNs.In (4) and (5), we remove the graph memory\nand ﬁx the STAR for spatial encoding. The temporal prediction ability of\nthese two models is only dependent on their temporal encoders, LSTM for\n(4) and STAR for (5). We observe that the model with temporal Transformer\nencoding outperforms LSTM in its overall performance, which suggests that\nTransformers provide a better temporal modeling ability compared to RNNs.\n– TGConv outperforms the other graph convolution methods on crowd motion\nmodeling. In (1), (2), (3) and (7), we change the spatial encoders and compare\nthe spatial Transformer by TGConv (7) with the GCN [24], GATConv [44]\nand the multi-head additive graph convolution [5]. We observe that TGConv,\nunder the scenario of crowd modeling, achieves higher performance gain\ncompared to the other two alternative attention-based graph convolutions.\n14 Yu C., Ma X., Ren J., Zhao H., Yi S.\nComponents Performance (ADE/FDE)\nSP TP GM ETH HOTEL ZARA1 ZARA2 UNIV AVG\n(1) GCN STAR ✓ 3.06/5.57 0.99/1.80 2.49/4.58 1.37/2.52 1.38/2.47 1.86 /3.34\n(2) GAT STAR ✓ 0.64/1.25 0.34/0.72 0.47/1.09 0.37/0.86 0.55/1.19 0.48/1.02\n(3) MHA STAR ✓ 0.58/1.15 0.25/0.48 0.50/0.98 0.35/0.76 0.60/1.24 0.56/0.92\n(4) STAR LSTM - 0.66/1.29 0.34/0.68 0.45/0.96 0.34/0.74 0.60/1.29 0.48/0.99\n(5) STAR STAR × 0.60/1.18 0.28/0.60 0.53/1.13 0.36/0.76 0.57/1.20 0.47/0.97\n(6) VSTAR VSTAR ✓ 0.61/1.18 0.29/0.56 0.48/1.00 0.36/0.76 0.58/1.24 0.46/0.95\n(7) STAR STAR ✓ 0.56/1.11 0.26/0.50 0.41/0.90 0.31/0.71 0.52/1.15 0.41/0.87\nTable 2.Ablation Study on SR-LSTM. We replace components in STAR with exist-\ning works. SP denotes spaital encoder. TP denotes temporal encoder. GM denotes\nGraph Memory. GAT denotes Graph Attention Network[44], MHA denotes Multi-\nHead Additive attention[5]. STAR denotes components in original STAR. VSTAR\ndenoets simpliﬁed STAR without encoder2.\n– Interleaving spatial and temporal Transformer is able to better extract spatio-\ntemporal correlations.In (6) and (7), we observe that the two encoder struc-\ntures proposed in the STAR framework (7), generally outperforms the single\nencoder structure (6). This empirical performance gain potentially suggests\nthat interleaving the spatial and temporal Transformers is able to extract\nmore complex spatio-temporal interactions of pedestrians.\n– Graph memory gives a smoother temporal embedding and improves perfor-\nmance. In (5) and (7), we verify the embedding smoothing ability of the\ngraph memory module, where (5) is the STAR variant without GM. We\nﬁrst noticed that graph memory improves the performance of STAR on all\ndatasets. In addition, we noticed that on ZARA1, where the spatial interac-\ntion is simple and temporal consistency prediction is more important, graph\nmemory improves (6) to (7) by the largest margin. According to the em-\npirical evidence, we can conclude that the embedding smoothing of graph\nmemory is able to improve the overall temporal modeling for STAR.\n5 Conclusion\nWe have introduced STAR, a framework for spatio-temporal crowd trajectory\nprediction with only attention mechanisms. STAR consists of two encoder mod-\nules, composed of spatial Transformers and temporal Transformers. We also\nhave introduced TGConv, a novel powerful Transformer based graph convolu-\ntion mechanism. STAR, using only attention mechanisms, achieves SOTA per-\nformance on 5 commonly used datasets.\nSTAR makes prediction only with the past trajectories, which might fail to\ndetect the unpredictable sharp turns. Additional information, e.g., environment\nconﬁguration, could be incorporated into the framework to solve this issue.\nSTAR framework and TGConv are not limited to trajectory prediction. They\ncan be applied to any graph learning task. We leave it for future study.\nSpatio-Temporal Graph Neural Networks 15\nReferences\n1. Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., Savarese, S.: Social\nlstm: Human trajectory prediction in crowded spaces. In: CVPR (2016)\n2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint\narXiv:1607.06450 (2016)\n3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 (2014)\n4. Battaglia, P., Pascanu, R., Lai, M., Rezende, D.J., et al.: Interaction networks for\nlearning about objects, relations and physics. In: Advances in neural information\nprocessing systems (2016)\n5. Chen, B., Barzilay, R., Jaakkola, T.: Path-augmented graph transformer network\n(2019). https://doi.org/10.26434/chemrxiv.8214422\n6. Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,\nH., Bengio, Y.: Learning phrase representations using RNN encoder–decoder for\nstatistical machine translation. In: Proceedings of the 2014 Conference on Empir-\nical Methods in Natural Language Processing (2014)\n7. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-\nrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014)\n8. Cui, Z., Henrickson, K., Ke, R., Wang, Y.: Traﬃc graph convolutional recurrent\nneural network: A deep learning framework for network-scale traﬃc learning and\nforecasting. IEEE Transactions on Intelligent Transportation Systems (2019)\n9. Deﬀerrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on\ngraphs with fast localized spectral ﬁltering. In: Advances in neural information\nprocessing systems (2016)\n10. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n11. Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., Yin, D.: Graph neural networks\nfor social recommendation. In: WWW (2019)\n12. Fang, K., Toshev, A., Fei-Fei, L., Savarese, S.: Scene memory transformer for em-\nbodied agents in long-horizon tasks. In: CVPR (2019)\n13. Ferrer, G., Garrell, A., Sanfeliu, A.: Robot companion: A social-force based ap-\nproach with human awareness-navigation in crowded environments. In: IROS\n(2013)\n14. F¨ orster, A., Graves, A., Schmidhuber, J.: Rnn-based learning of compact maps for\neﬃcient robot localization. In: ESANN (2007)\n15. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message\npassing for quantum chemistry. In: ICML (2017)\n16. Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., Alahi, A.: Social gan: Socially\nacceptable trajectories with generative adversarial networks. In: CVPR (2018)\n17. Hajiramezanali, E., Hasanzadeh, A., Narayanan, K., Duﬃeld, N., Zhou, M., Qian,\nX.: Variational graph recurrent neural networks. In: Advances in Neural Informa-\ntion Processing Systems (2019)\n18. Helbing, D., Buzna, L., Johansson, A., Werner, T.: Self-organized pedestrian crowd\ndynamics: Experiments, simulations, and design solutions. Transportation science\n(2005)\n19. Helbing, D., Molnar, P.: Social force model for pedestrian dynamics. Physical re-\nview E (1995)\n16 Yu C., Ma X., Ren J., Zhao H., Yi S.\n20. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n(1997)\n21. Huang, Y., Bi, H., Li, Z., Mao, T., Wang, Z.: Stgat: Modeling spatial-temporal\ninteractions for human trajectory prediction. In: ICCV (2019)\n22. Ivanovic, B., Pavone, M.: The trajectron: Probabilistic multi-agent trajectory mod-\neling with dynamic spatiotemporal graphs. In: ICCV (2019)\n23. Karkus, P., Ma, X., Hsu, D., Kaelbling, L.P., Lee, W.S., Lozano-P´ erez, T.: Dif-\nferentiable algorithm networks for composable robot learning. arXiv preprint\narXiv:1905.11602 (2019)\n24. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907 (2016)\n25. Kuderer, M., Kretzschmar, H., Sprunk, C., Burgard, W.: Feature-based prediction\nof trajectories for socially compliant navigation. In: RSS (2012)\n26. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A\nlite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942 (2019)\n27. Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R.: Gated graph sequence neural\nnetworks. arXiv preprint arXiv:1511.05493 (2015)\n28. Li, Y., Wu, J., Tedrake, R., Tenenbaum, J.B., Torralba, A.: Learning particle dy-\nnamics for manipulating rigid bodies, deformable objects, and ﬂuids. arXiv preprint\narXiv:1810.01566 (2018)\n29. Lim, B., Arik, S.O., Loeﬀ, N., Pﬁster, T.: Temporal fusion transformers for in-\nterpretable multi-horizon time series forecasting. arXiv preprint arXiv:1912.09363\n(2019)\n30. Liu, J., Lin, H., Liu, X., Xu, B., Ren, Y., Diao, Y., Yang, L.: Transformer-based\ncapsule network for stock movement prediction. In: Proceedings of the First Work-\nshop on Financial Technology and Natural Language Processing (2019)\n31. Liu, K., Sun, X., Jia, L., Ma, J., Xing, H., Wu, J., Gao, H., Sun, Y., Boulnois,\nF., Fan, J.: Chemi-net: a molecular graph convolutional network for accurate drug\nproperty prediction. International journal of molecular sciences (2019)\n32. L¨ ohner, R.: On the modeling of pedestrian motion. Applied Mathematical Mod-\nelling (2010)\n33. Luo, Y., Cai, P.: Gamma: A general agent motion prediction model for autonomous\ndriving. arXiv preprint arXiv:1906.01566 (2019)\n34. Luo, Y., Cai, P., Bera, A., Hsu, D., Lee, W.S., Manocha, D.: Porca: Modeling and\nplanning for autonomous driving among many pedestrians. IEEE Robotics and\nAutomation Letters (2018)\n35. Ma, X., Gao, X., Chen, G.: Beep: A bayesian perspective early stage event predic-\ntion model for online social networks. In: ICDM (2017)\n36. Ma, X., Karkus, P., Hsu, D., Lee, W.S.: Particle ﬁlter recurrent neural networks.\narXiv preprint arXiv:1905.12885 (2019)\n37. Ma, X., Karkus, P., Hsu, D., Lee, W.S., Ye, N.: Discriminative particle ﬁl-\nter reinforcement learning for complex partial observations. arXiv preprint\narXiv:2002.09884 (2020)\n38. Ma, Y., Zhu, X., Zhang, S., Yang, R., Wang, W., Manocha, D.: Traﬃcpredict:\nTrajectory prediction for heterogeneous traﬃc-agents. AAAI (2019)\n39. Miao, Y., Gowayyed, M., Metze, F.: Eesen: End-to-end speech recognition using\ndeep rnn models and wfst-based decoding. In: ASRU (2015)\n40. Sadeghian, A., Kosaraju, V., Sadeghian, A., Hirose, N., Rezatoﬁghi, H., Savarese,\nS.: Sophie: An attentive gan for predicting paths compliant to social and physical\nconstraints. In: CVPR (2019)\nSpatio-Temporal Graph Neural Networks 17\n41. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural\nnetworks. In: Advances in neural information processing systems (2014)\n42. Van Den Berg, J., Guy, S.J., Lin, M., Manocha, D.: Reciprocal n-body collision\navoidance. In: Robotics research (2011)\n43. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems (2017)\n44. Veliˇ ckovi´ c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y.: Graph\nattention networks. arXiv preprint arXiv:1710.10903 (2017)\n45. Vemula, A., Muelling, K., Oh, J.: Social attention: Modeling attention in human\ncrowds. In: ICRA (2018)\n46. Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., Stolcke, A.: The Microsoft\n2017 conversational speech recognition system. In: Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing (2018)\n47. Xu, K., Hu, W., Leskovec, J., Jegelka, S.: How powerful are graph neural networks?\narXiv preprint arXiv:1810.00826 (2018)\n48. Xu, Y., Piao, Z., Gao, S.: Encoding crowd interaction with deep neural network\nfor pedestrian trajectory prediction. In: CVPR (2018)\n49. Xu, Y., Piao, Z., Gao, S.: Encoding crowd interaction with deep neural network\nfor pedestrian trajectory prediction. In: CVPR (2018)\n50. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:\nGeneralized autoregressive pretraining for language understanding. In: Advances\nin neural information processing systems (2019)\n51. Yi, S., Li, H., Wang, X.: Pedestrian behavior understanding and prediction with\ndeep neural networks. In: ECCV (2016)\n52. Young, T., Hazarika, D., Poria, S., Cambria, E.: Recent trends in deep learn-\ning based natural language processing. ieee Computational intelligenCe magazine\n(2018)\n53. Zhang, P., Ouyang, W., Zhang, P., Xue, J., Zheng, N.: Sr-lstm: State reﬁnement\nfor lstm towards pedestrian trajectory prediction. In: CVPR (2019)\n18 Yu C., Ma X., Ren J., Zhao H., Yi S.\nAdditional Attention Visualization\n3 4 5 6 7 8 9\n4\n2\n0\n2\n4\n6\n8\n10\n3 4 5 6 7 8 9\n4\n2\n0\n2\n4\n6\n8\n10\n3 4 5 6 7 8 9\n4\n2\n0\n2\n4\n6\n8\n10\n3 4 5 6 7 8 9\n4\n2\n0\n2\n4\n6\n8\n10\n3 4 5 6 7 8 9 10\n6\n4\n2\n0\n2\n4\n6\n8\n10\n3 4 5 6 7 8 9 10\n6\n4\n2\n0\n2\n4\n6\n8\n10\n3 4 5 6 7 8 9 10\n6\n4\n2\n0\n2\n4\n6\n8\n10\n3 4 5 6 7 8 9 10\n6\n4\n2\n0\n2\n4\n6\n8\n10\n1 2 3 4 5 6 7 8\n4\n2\n0\n2\n4\n6\n8\n10\n12\n14\n1 2 3 4 5 6 7 8\n4\n2\n0\n2\n4\n6\n8\n10\n12\n14\n1 2 3 4 5 6 7 8\n4\n2\n0\n2\n4\n6\n8\n10\n12\n14\n1 2 3 4 5 6 7 8\n4\n2\n0\n2\n4\n6\n8\n10\n12\n14\n1 2 3 4 5 6 7 8\n4\n2\n0\n2\n4\n6\n8\n10\n12\n14\n1 2 3 4 5 6 7 8\n4\n2\n0\n2\n4\n6\n8\n10\n12\n14\n1 2 3 4 5 6 7 8\n4\n2\n0\n2\n4\n6\n8\n10\n12\n14\n1 2 3 4 5 6 7 8\n4\n2\n0\n2\n4\n6\n8\n10\n12\n14\nFig. 7.Additional attention visualizations of the spatial Transformer, i.e., connected in\nthe interaction graph, in encoder 2. We visualize the attention of neighbor pedestrians\nwith respect to the red dotted pedestrian. The size of circles represents the atten-\ntion value and bigger circles indicate higher attention. STAR learns reasonable spatial\nattention, the pedestrians have higher attentions over themselves and their neighbors.\nSpatio-Temporal Graph Neural Networks 19\nAblation Trajectory Prediction Visualizations\n(a) GAT + STAR\n(b) MHA + STAR\n(c) STAR + LSTM\n(d) STAR without Graph Memory\n(e) Simpliﬁed STAR without Encoder 2\n(f) STAR\nFig. 8.Trajectory visualization of all ablations. Yellow lines denote the history, red lines\ndenote the ground-truth, and blue lines denote the prediction. Qualitatively, STAR\nproduces best predictions both spatially and temporally.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7338411211967468
    },
    {
      "name": "Transformer",
      "score": 0.5708561539649963
    },
    {
      "name": "Interleaving",
      "score": 0.5586435198783875
    },
    {
      "name": "Graph",
      "score": 0.5162115693092346
    },
    {
      "name": "Pedestrian",
      "score": 0.46829432249069214
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46126726269721985
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2859807014465332
    },
    {
      "name": "Engineering",
      "score": 0.11525076627731323
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Transport engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}