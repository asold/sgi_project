{
    "title": "PCT: Point cloud transformer",
    "url": "https://openalex.org/W3111535274",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4227007248",
            "name": "Guo, Meng-Hao",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2243780133",
            "name": "Cai Jun Xiong",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A4227007250",
            "name": "Liu Zheng-ning",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A4224942482",
            "name": "Mu, Tai-Jiang",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A4226993707",
            "name": "Martin, Ralph R.",
            "affiliations": [
                "Cardiff University"
            ]
        },
        {
            "id": "https://openalex.org/A2321529202",
            "name": "HU Shi-min",
            "affiliations": [
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4234552385",
        "https://openalex.org/W2911495555",
        "https://openalex.org/W2964253930",
        "https://openalex.org/W6604186633",
        "https://openalex.org/W55204438",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2963495494",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2806332096",
        "https://openalex.org/W2797997528",
        "https://openalex.org/W2963281829",
        "https://openalex.org/W2979750740",
        "https://openalex.org/W2553307952",
        "https://openalex.org/W2798270772",
        "https://openalex.org/W2963123724",
        "https://openalex.org/W2963509914"
    ],
    "abstract": "The irregular domain and lack of ordering make it challenging to design deep\\nneural networks for point cloud processing. This paper presents a novel\\nframework named Point Cloud Transformer(PCT) for point cloud learning. PCT is\\nbased on Transformer, which achieves huge success in natural language\\nprocessing and displays great potential in image processing. It is inherently\\npermutation invariant for processing a sequence of points, making it\\nwell-suited for point cloud learning. To better capture local context within\\nthe point cloud, we enhance input embedding with the support of farthest point\\nsampling and nearest neighbor search. Extensive experiments demonstrate that\\nthe PCT achieves the state-of-the-art performance on shape classification, part\\nsegmentation and normal estimation tasks.\\n",
    "full_text": "Computational Visual Media\nhttps://doi.org/10.1007/s41095-021-0229-5 Vol. 7, No. 2, June 2021, 187–199\nResearch Article\nPCT: Point cloud transformer\nMeng-Hao Guo 1, Jun-Xiong Cai 1, Zheng-Ning Liu 1, Tai-Jiang Mu 1, Ralph R. Martin 2, and\nShi-Min Hu1 (\u0000 )\nc⃝ The Author(s) 2021.\nAbstract The irregular domain and lack of ordering\nmake it challenging to design deep neural networks for\npoint cloud processing. This paper presents a novel\nframework named Point Cloud Transformer (PCT) for\npoint cloud learning. PCT is based on Transformer,\nwhich achieves huge success in natural language processing\nand displays great potential in image processing. It\nis inherently permutation invariant for processing a\nsequence of points, making it well-suited for point cloud\nlearning. To better capture local context within the\npoint cloud, we enhance input embedding with the\nsupport of farthest point sampling and nearest neighbor\nsearch. Extensive experiments demonstrate that the\nPCT achieves the state-of-the-art performance on shape\nclassification, part segmentation, semantic segmentation,\nand normal estimation tasks.\nKeywords 3D computer vision; deep learning; point\ncloud processing; Transformer\n1 Introduction\nExtracting semantics directly from a point cloud is\nan urgent requirement in some applications such as\nrobotics, autonomous driving, augmented reality, etc.\nUnlike 2D images, point clouds are disordered and\nunstructured, making it challenging to design neural\nnetworks to process them. Charles et al. [1] pioneered\nPointNet for feature learning on point clouds by\nusing multi-layer perceptrons (MLPs), max-pooling,\n1 BNRist, Department of Computer Science and\nTechnology, Tsinghua University, Beiing 100084, China.\nE-mail: M.-H. Guo, gmh20@mails.tsinghua.edu.cn;\nJ.-X. Cai, junxiong20@mails.tsinghua.edu.cn; Z.-N. Liu,\nlzhengning@gmail.com; T.-J. Mu, taijiang@tsinghua.edu.cn;\nS.-M. Hu, shimin@tsinghua.edu.cn ( \u0000 ).\n2 Cardiﬀ University, Cardiﬀ CF243AA, UK. E-mail:\nralph@cs.cf.ac.uk.\nManuscript received: 2021-03-04; accepted: 2021-03-26\nand rigid transformations to ensure invariance under\npermutations and rotations. Inspired by strong\nprogress made by convolutional neural networks\n(CNNs) in the ﬁeld of image processing, many recent\nworks [ 2–5] have considered to deﬁne convolution\noperators that can aggregate local features for point\nclouds. These methods either reorder the input point\nsequence or voxelize the point cloud to obtain a\ncanonical domain for convolutions.\nRecently, Transformer [6], the dominant framework\nin natural language processing, has been applied\nto image vision tasks, giving better performance\nthan popular convolutional neural networks [\n7, 8].\nTransformer is a decoder–encoder structure that\ncontains three main modules for input (word)\nembedding, positional (order) encoding, and self-\nattention. The self-attention module is the core\ncomponent, generating reﬁned attention feature for\nits input feature based on global context. First, self-\nattention takes the sum of input embedding and\npositional encoding as input, and computes three\nvectors for each word: query, key,a n dvalue through\ntrained linear layers. Then, the attention weight\nbetween any two words can be obtained by matching\n(dot-producting) their query and key vectors. Finally,\nthe attention feature is deﬁned as the weighted\nsum of all value vectors with the attention weights.\nObviously, the output attention feature of each word\nis related to all input features, making it capable\nof learning the global context. All operations of\nTransformer are parallelizable and order-independent.\nIn theory, it can replace the convolution operation\nin a convolutional neural network and has better\nversatility. For more detailed introduction of self-\nattention, please refer to Section 3.2.\nInspired by the Transformer’s success in vision\nand NLP tasks, we propose a novel framework PCT\n187\n\n188 M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.\nfor point cloud learning based on the principles of\ntraditional Transformer. The key idea of PCT is\nusing the inherent order invariance of Transformer to\navoid the need to deﬁne the order of point cloud data\nand conduct feature learning through the attention\nmechanism. As shown in Fig. 1, the distribution of\nattention weights is highly related to part semantics,\nand it does not seriously attenuate with spatial\ndistance.\nPoint clouds and natural language are rather\ndiﬀerent kinds of data, so our PCT framework must\nmake several adjustments for this. These include:\n• Coordinate-based input embedding module.\nIn Transformer, a positional encoding module is\napplied to represent the word order in natural\nlanguage. This can distinguish the same word\nin diﬀerent positions and reﬂect the positional\nrelationships between words. However, point clouds\ndo not have a fixed order. In our PCT framework,\nwe merge the raw positional encoding and the\ninput embedding into a coordinate-based input\nembedding module. It can generate distinguishable\nfeatures, since each point has aunique coordinate\nwhich represents its spatial position.\n• Optimized oﬀset-attention module. The\noﬀset-attention module approach we proposed\nis an eﬀective upgrade over the original self-\nattention. It works by replacing the attention\nfeature with the oﬀset between the input of self-\nattention module and attention feature. This has\ntwo advantages. Firstly, the absolute coordinates\nof the same object can be completely diﬀerent\nFig. 1 Attention map and part segmentation generated by PCT.\nFirst three columns: point-wise attention map for diﬀerent query\npoints (indicated by $), yellow to blue indicating increasing attention\nweight. Last column: part segmentation results.\nwith rigid transformations. Therefore, relative\ncoordinates are generally more robust. Secondly,\nthe Laplacian matrix (the oﬀset between degree\nmatrix and adjacency matrix) has been proven to\nbe very effective in graph convolution learning [9].\nFrom this perspective, we regard the point cloud as\na graph with the “float” adjacency matrix as the\nattention map. Also, the attention map in our work\nwill be scaled with all the sum of each rows to 1. So\nthe degree matrix can be understood as the identity\nmatrix. Therefore, the offset-attention optimization\nprocess can be approximately understood as a\nLaplace process, which will be discuss detailed\nin Section 3.3. In addition, we have conducted\nsufficient comparative experiments, introduced in\nSection 4, on oﬀset-attention and self-attention to\nprove its eﬀectiveness.\n• Neighbor embedding module. Obviously,\nevery word in a sentence contains basic semantic\ninformation. However, the independent input\ncoordinates of the points are only weakly related\nto the semantic content. Attention mechanism is\neﬀective in capturing global features, but it may\nignore local geometric information which is also\nessential for point cloud learning. To address this\nproblem, we use a neighbor embedding strategy\nto improve upon point embedding. It also assists\nthe attention module by considering attention\nbetween local groups of points containing\nsemantic information instead of individual points.\nWith the above adjustments, the PCT becomes more\nsuitable for point cloud feature learning and achieves\nthe state-of-the-art performance on shape classification,\npart segmentation, semantic segmentation, and normal\nestimation tasks. All experiments are implemented\nwith Jittor [10] deep learning fremework. Codes are\navailable at https://github.com/MenghaoGuo/PCT.\nThe main contributions of this paper are summarized\nas following:\n1. We proposed a novel transformer based framework\nnamed PCT for point cloud learning, which is\nexactly suitable for unstructured, disordered point\ncloud data with irregular domain.\n2. We proposed oﬀset-attention with implicit\nLaplace operator and normalization reﬁnement\nwhich is inherently permutation-invariant and\nmore suitable for point cloud learning compared to\nthe original self-attention module in Transformer.\n\nPCT: Point cloud transformer 189\n3. Extensive experiments demonstrate that the\nPCT with explicit local context enhancement\nachieves state-of-the-art performance on shape\nclassiﬁcation, part segmentation, and normal\nestimation tasks.\n2 Related work\n2.1 Transformer in NLP\nBahdanau et al. [ 11] proposed a neural machine\ntranslation method with an attention mechanism,\nin which attention weight is computed through the\nhidden state of an RNN. Self-attention was proposed\nby Lin et al. [ 12] to visualize and interpret sentence\nembeddings. Building on these, Vaswani et al. [ 6]\nproposed Transformer for machine translation; it is\nbased solely on self-attention, without any recurrence\nor convolution operators. Devlin et al. [ 13] proposed\nbidirectional transformers (BERT) approach, which\nis one of the most powerful models in the NLP\nﬁeld. More lately, language learning networks such as\nXLNet [14], Transformer-XL [15], and BioBERT [16]\nhave further extended the Transformer framework.\nHowever, in natural language processing, the input\nis in order, and word has basic semantic, whereas\npoint clouds are unordered, and individual points\nhave no semantic meaning in general.\n2.2 Transformer for vision\nMany frameworks have introduced attention into\nvision tasks. Wang et al. [ 17] proposed a residual\nattention approach with stacked attention modules\nfor image classiﬁcation. Hu et al. [ 18] presented a\nnovel spatial encoding unit, the SE block, whose\nidea was derived from the attention mechanism.\nZhang et al. [\n19] designed SAGAN, which uses self-\nattention for image generation. There has also been\nan increasing trend to employ Transformer as a\nmodule to optimize neural networks. Wu et al. [\n8]\nproposed visual transformers that apply Transformer\nto token-based images from feature maps for vision\ntasks. Recently, Dosovitskiy [7], proposed an image\nrecognition network, ViT, based on patch encoding\nand Transformer, showing that with suﬃcient training\ndata, Transformer provides better performance than\na traditional convolutional neural network. Carion et\nal. [\n20] presented an end-to-end detection transformer\nthat takes CNN features as input and generates\nbounding boxes with a Transformer encoder–decoder.\nInspired by the local patch structures used in ViT\nand basic semantic information in language word, we\npresent a neighbor embedding module that aggregates\nfeatures from a point’s local neighborhood, which can\ncapture the local information and obtain semantic\ninformation.\n2.3 Point-based deep learning\nPointNet [1] pioneered point cloud learning. Sub-\nsequently, Qi et al. [21] proposed PointNet++, which\nuses query ball grouping and hierarchical PointNet\nto capture local structures. Several subsequent works\nconsidered how to deﬁne convolution operations on\npoint clouds. One main approach is to convert a point\ncloud into a regular voxel array to allow convolution\noperations. Tchapmi et al. [ 2] proposed SEGCloud\nfor pointwise segmentation. It maps convolution\nfeatures of 3D voxels to point clouds using trilinear\ninterpolation and keeps global consistency through\nfully connected conditional random ﬁelds. Atzmon et\nal. [4] presented the PCNN framework with extension\nand restriction operators to map between point-\nbased representation and voxel-based representation.\nVolumetric convolution is performed on voxels for\npoint feature extraction. MCCNN by Hermosilla et\nal. [22] allows non-uniformly sampled point clouds;\nconvolution is treated as a Monte Carlo integration\nproblem. Similarly, in PointConv proposed by Wu et\nal. [5], 3D convolution is performed through Monte\nCarlo estimation and importance sampling.\nA diﬀerent approach redeﬁnes convolution to\noperation on irregular point cloud data. Li et al. [ 3]\nintroduced a point cloud convolution network,\nPointCNN, in which a\nχ-transformation is trained\nto determine a 1D point order for convolution.\nTatarchenko et al. [23] proposed tangent convolution,\nwhich can learn surface geometric features from\nprojected virtual tangent images. SPG proposed\nby Landrieu and Simonovsky [\n24] divides the\nscanned scene into similar elements, and establishes\na superpoint graph structure to learn contextual\nrelationships between object parts. Yang et al. [\n25]\nused a parallel framework to extend CNN from the\nconventional domain to a curved two-dimensional\nmanifold. However, it requires dense 3D gridded data\nas input so is unsuitable for 3D point clouds. Wang et\nal. [26] designed an EdgeConv operator for dynamic\ngraphs, allowing point cloud learning by recovering\nlocal topology.\n\n190 M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.\nVarious other methods also employ attention and\nTransformer. Yan et al. [ 27] proposed PointASNL\nto deal with noise in point cloud processing, using\na self-attention mechanism to update features for\nlocal groups of points. Hertz et al. [\n28] proposed\nPointGMM for shape interpolation with both multi-\nlayer perceptron (MLP) splits and attentional splits.\nUnlike the above methods, our PCT is based on\nTransformer rather than using self-attention as an\nauxiliary module. While a framework by Wang and\nSolomon [29] uses Transformer to optimize point cloud\nregistration, our PCT is a more general framework\nwhich can be used for various point cloud tasks.\n3 Transformer for point cloud\nrepresentation\nIn this section, we ﬁrst show how the point cloud\nrepresentation learned by our PCT can be applied\nto various tasks of point cloud processing, including\npoint cloud classiﬁcation, part segmentation, and\nnormal estimation. Thereafter, we detail the design\nof PCT. We ﬁrst introduce a naive version of PCT by\ndirectly applying the original Transformer [\n6]t op o i n t\nclouds. We then explain full PCT with its special\nattention mechanism, and neighbor aggregation to\nprovide enhanced local information.\n3.1 Point cloud processing with PCT\nEncoder. The overall architecture of PCT is\npresented in Fig. 2. PCT aims to transform (encode)\nthe input points into a new higher dimensional feature\nspace, which can characterize the semantic aﬃnities\nbetween points as a basis for various point cloud\nprocessing tasks. The encoder of PCT starts by\nembedding the input coordinates into a new feature\nspace. The embedded features are later fed into\n4 stacked attention module to learn a semantically\nrich and discriminative representation for each point,\nfollowed by a linear layer to generate the output\nfeature. Overall, the encoder of PCT shares almost\nthe same philosophy of design as the original\nTransformer, except that the positional embedding is\ndiscarded, since the point’s coordinates already\ncontain this information. We refer the reader to\nRef. [6] for details of the original NLP Transformer.\nFormally, given an input point cloud P∈ RN×d\nwith N points each having a d-dimensional feature\ndescription, a de-dimensional embedded feature Fe ∈\nRN×de is ﬁrst learned via the Input Embedding\nmodule. The point-wise do-dimensional feature\nrepresentation Fo ∈ RN×do output by PCT is then\nformed by concatenating the attention output of\neach attention layer through the feature dimension,\nfollowed by a linear transformation:\nF1 =A T1(Fe)\nFi =A Ti(Fi−1),i =2 , 3, 4 (1)\nFo = concat(F1, F2, F3, F4) · Wo\nwhere ATi represents the ith attention layer, each\nhaving the same output dimension as its input, and\nWo is the weights of the linear layer. Various\nimplementations of input embedding and attention\nwill be explained later.\nTo extract an eﬀective global feature vector\nFg representing the point cloud, we choose to\nconcatenate the outputs from two pooling operators:\na max-pooling (MP) and an average-pooling (AP) on\nthe learned point-wise feature representation [26].\nClassiﬁcation. The details of classiﬁcation\nnetwork using PCT is shown in Fig. 2. To classify\na point cloud P into Nc object categories (e.g.,\ndesk, table, chair), we feed the global feature\nFg to the classiﬁcation decoder, which comprises\ntwo cascaded feed-forward neural networks LBRs\n(combining Linear, BatchNorm (BN), and ReLU\nlayers) each with a dropout probability of 0 .5,\nﬁnalized by a Linear layer to predict the ﬁnal\nclassiﬁcation scores C∈ RNc . The class label of the\npoint cloud is determined as the class with maximal\nscore.\nSegmentation. For the task of segmenting the\npoint cloud into Ns parts (e.g., table top, table legs; a\npart need not be contiguous), we must predict a part\nlabel for each point, we ﬁrst concatenate the global\nfeature Fg with the point-wise features in Fo.T o\nlearn a common model for various kinds of objects,\nwe also encode the one-hot object category vector\nas a 64-dimensional feature and concatenate it with\nthe global feature, following most other point cloud\nsegmentation networks [21]. As shown in Fig. 2, the\narchitecture of the segmentation network decoder\nis almost the same as that for the classiﬁcation\nnetwork, except that dropout is only performed on\nthe ﬁrst LBR. We then predict the ﬁnal point-wise\nsegmentation scores S∈ RN×Ns for the input point\ncloud: Finally, the part label of a point is also\ndetermined as the one with maximal score.\n\nPCT: Point cloud transformer 191\nFig. 2 PCT architecture. The encoder mainly comprises an Input Embedding module and four stacked Attention module. The decoder mainly\ncomprises multiple Linear layers. Numbers above each module indicate its output channels. MA-Pool concatenates Max-Pool and Average-Pool.\nLBR combines Linear, BatchNorm, and ReLU layers. LBRD means LBR followed by a Dropout layer.\nNormal estimation. For the task of normal\nestimation, we use the same architecture as in\nsegmentation by setting Ns = 3, without the object\ncategory encoding, and regard the output point-wise\nscore as the predict normal.\n3.2 Naive PCT\nThe simplest way to modify Transformer [6] for point\ncloud use is to treat the entire point cloud as a\nsentence and each point as a word, an approach\nwe now explain. This naive PCT is achieved by\nimplementing a coordinate-based point embedding\nand instantiating the attention layer with the self-\nattention introduced in Ref. [6].\nFirst, we consider a naive point embedding, which\nignores interactions between points. Like word\nembedding in NLP, point embedding aims to place\npoints closer in the embedding space if they are\nmore semantically similar. Speciﬁcally, we embed\na point cloud\nP into a de-dimensional space Fe ∈\nRN×de , using a shared neural network comprising two\ncascaded LBRs, each with a de-dimensional output.\nWe empirically set de = 128, a relatively small value,\nfor computational eﬃciency. We simply use the\npoint’s 3D coordinates as its input feature description\n(i.e., dp = 3) (as doing so still outperforms other\nmethods) but additional point-wise input information,\nsuch as point normals, could also be used.\nFor the naive implementation of PCT, we adopt\nself-attention (SA) as introduced in the original\nTransformer [ 6]. Self-attention, also called intra-\nattention, is a mechanism that calculates semantic\naﬃnities between diﬀerent items within a sequence\nof data. The architecture of the SA layer is depicted\nin Fig. 3 by switching to the dotted data ﬂows.\nFollowing the terminology in Ref. [\n6], let Q, K, V\nbe the query, key,a n d value matrices, respectively,\ngenerated by linear transformations of the input\nfeatures Fin ∈ RN×de as follows:\n(Q, K, V )= Fin · (Wq, Wk, Wv)\nQ, K ∈ RN×da , V ∈ RN×de (2)\nWq, Wk ∈ Rde ×da , Wv ∈ Rde ×de\nwhere Wq, Wk,a n d Wv are the shared learnable\nFig. 3 Architecture of Oﬀset-Attention. Numbers above tensors are numbers of dimensions N and feature channels D/Da , with switches\nshowing alternatives of Self-Attention or Oﬀset-Attention: dotted lines indicate Self-Attention branches.\n\n192 M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.\nlinear transformation, and da is the dimension of the\nquery and key vectors. Note that da may not be\nequal to de. In this work, we set da to be de/4f o r\ncomputational eﬃciency.\nFirst, we can use the query and key matrices to\ncalculate the attention weights via the matrix dot-\nproduct:\n˜A =( ˜α)i,j = Q · KT (3)\nThese weights are then normalized (denoted SS in\nFig. 3) to give A =( α)i,j:\n¯αi,j = ˜αi,j√da\nαi,j = softmax(¯αi,j)= exp (¯αi,j )∑\nk\nexp (¯αi,k )\n(4)\nThe self-attention output features Fsa are the\nweighted sums of the value vector using the\ncorresponding attention weights:\nFsa = A · V (5)\nAs the query, key, and value matrices are\ndetermined by the shared corresponding linear\ntransformation matrices and the input feature Fin,\nthey are all order independent. Moreover, softmax\nand weighted sum are both permutation-independent\noperators. Therefore, the whole self-attention process\nis permutation-invariant, making it well-suited to\nthe disordered, irregular domain presented by point\nclouds.\nFinally, the self-attention feature Fsa and the input\nfeature Fin, are further used to provide the output\nfeature Fout for the whole SA layer through an LBR\nnetwork:\nFout =S A (Fin) = LBR(Fsa)+F in (6)\n3.3 Oﬀset-Attention\nGraph convolution networks [ 9] show the beneﬁts\nof using a Laplacian matrix L = D − E to replace\nthe adjacency matrix E, where D is the diagonal\ndegree matrix. Similarly, we ﬁnd that we can\nobtain better network performance if, when applying\nTransformer to point clouds, we replace the original\nself-attention (SA) module with an oﬀset-attention\n(OA) module to enhance our PCT. As shown in\nFig. 3, the oﬀset-attention layer calculates the oﬀset\n(diﬀerence) between the self-attention (SA) features\nand the input features by element-wise subtraction.\nThis oﬀset feeds the LBR network in place of the SA\nfeature used in the naive version. Speciﬁcally, Eq. (5)\nis modiﬁed to\nFout =O A (Fin) =LBR(Fin − Fsa)+F in (7)\nFin −Fsa is analogous to a discrete Laplacian operator,\nas we now show. First, from Eqs. (2) and (5), the\nfollowing holds:\nFin − Fsa = Fin − AV\n= Fin − AFinWv\n≈ Fin − AFin\n=( I − A)Fin ≈ LFin (8)\nHere, Wv is ignored since it is a weight matrix of\nthe Linear layer. I is an identity matrix comparable\nto the diagonal degree matrix D of the Laplacian\nmatrix and A is the attention matrix comparable to\nthe adjacency matrix E.\nIn our enhanced version of PCT, we also reﬁne the\nnormalization by modifying Eq. (4) as follows:\n¯αi,j = softmax(˜αi,j)= exp (˜αi,j )∑\nk\nexp (˜αk,j )\nαi,j = ¯αi,j∑\nk\n¯αi,k\n(9)\nHere, we use thesoftmaxoperator on the first dimension\nand an l1-norm for the second dimension to normalize\nthe attention map. The traditional Transformer scales\nthe ﬁrst dimension by 1 /√da and uses softmax\nto normalize the second dimension. However, our\noﬀset-attention sharpens the attention weights and\nreduces the inﬂuence of noise, which is beneﬁcial for\ndownstream tasks. Figure 1 shows example oﬀset\nattention maps. It can be seen that the attention\nmaps for diﬀerent query points vary considerably, but\nare generally semantically meaningful. We refer to\nthis reﬁned PCT, i.e., with point embedding and OA\nlayer, as simple PCT (SPCT) in the experiments.\n3.4 Neighbor embedding for augmented local\nfeature representation\nPCT with point embedding is an eﬀective network\nfor extracting global features. However, it ignores the\nlocal neighborhood information which is also essential\nin point cloud learning. We draw upon the ideas of\nPointNet++ [21] and DGCNN [ 26] to design a local\nneighbor aggregation strategy, neighbor embedding,\nto optimize the point embedding to augment PCT’s\nability of local feature extraction. As shown in\nFig. 4, neighbor embedding module comprises two\nLBR layers and two SG (sampling and grouping)\nlayers. The LBR layers act as the basis point\nembedding in Section 3.2. We use two cascaded SG\n\nPCT: Point cloud transformer 193\nFig. 4 Left: Neighbor Embedding architecture. Middle: SG module with Nin input points, din input channels, k neighbors, Nout output\nsampled points, and dout output channels. Top-right: example of sampling (colored balls represent sampled points). Bottom-right: example of\ngrouping with k-NN neighbors. Number above LBR: number of output channels. Number above SG: number of sampled points and its output\nchannels.\nlayers to gradually enlarge the receptive ﬁeld during\nfeature aggregation, as is done in CNNs. The SG\nlayer aggregates features from the local neighbors for\neach point grouped by k-NN search using Euclidean\ndistance during point cloud sampling.\nMore speciﬁcally, assume that SG layer takes a\npoint cloud P with N points and corresponding\nfeatures F as input and outputs a sampled point cloud\nPs with Ns points and its corresponding aggregated\nfeatures Fs. First, We adopt the farthest point\nsampling (FPS) algorithm [ 21] to downsample P\nto Ps. Then, for each sampled point p ∈P s, let\nknn(p, P)b ei t s k-nearest neighbors in P.W et h e n\ncompute the output feature Fs as follows:\nΔF (p) = concatq∈knn(p,P)(F (q) − F (p))\n˜F(p) = concat(ΔF (p), RP(F (p),k )) (10)\nFs(p) = MP(LBR(LBR( ˜F (p))))\nwhere F (p) is the input feature of point p, Fs(p)i s\nthe output feature of sampled point p, MP is the\nmax-pooling operator, and RP(x,k ) is the operator\nfor repeating a vector x k times to form a matrix.\nThe idea of concatenating the feature among sampled\npoint and its neighbors is drawn from EdgeConv [26].\nWe use diﬀerent architectures for the tasks of\npoint cloud classiﬁcation, segmentation, and normal\nestimation. For the point cloud classiﬁcation, we only\nneed to predict a global class for all points, so the\nsizes of the point cloud are decreased to 512 and 256\npoints within the two SG layer.\nFor point cloud segmentation or normal estimation,\nwe need to determine point-wise part labels or normal,\nso the process above is only used for local feature\nextraction without reducing the point cloud size,\nwhich can be achieved by setting the output at each\nstage to still be of size N.\n4 Experiments\nWe now evaluate the performance of naive PCT\n(NPCT, with point embedding and self-attention),\nsimple PCT (SPCT, with point embedding and\noﬀset-attention), and full PCT (with neighbor\nembedding and oﬀset-attention) on two public\ndatasets, ModelNet40 [30] and ShapeNet [31], giving a\ncomprehensive comparison with other methods. The\nsame soft cross-entropy loss function as Ref. [ 26]a n d\nthe stochastic gradient descent (SGD) optimizer with\nmomentum 0.9 were adopted for training in each case.\nOther training parameters, including the learning\nrate, batch size, and input format, were particular to\neach speciﬁc dataset and are given later.\n4.1 Classiﬁcation on ModelNet40 dataset\nModelNet40 [30] contains 12, 311 CAD models in 40\nobject categories; it is widely used in point cloud\nshape classiﬁcation and surface normal estimation\nbenchmarking. For a fair comparison, we used the\noﬃcial split with 9843 objects for training and 2468\nfor evaluation. The same sampling strategy as used in\nPointNet [1] was adopted to uniformly sample each\nobject to 1024 points. During training, a random\ntranslation in [−0.2, 0.2], a random anisotropic scaling\nin [0 .67, 1.5], and a random input dropout were\napplied to augment the input data. During testing, no\ndata augmentation or voting methods were used. For\nall the three models, the mini-batch sizes were 32.250\ntraining epochs were used and the initial learning\n\n194 M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.\nrates were 0.0001, with a cosine annealing schedule\nto adjust the learning rate at every epoch.\nExperimental results are shown in Table 1.\nCompared to PointNet and NPCT, SPCT makes\na 2.8% and 1.0% improvement respectively. PCT\nachieves the best result of 93\n.2% overall accuracy.\nNote that our network currently does not consider\nnormals as inputs which could in principle further\nimprove network performance.\n4.2 Normal estimation on ModelNet40\ndataset\nThe surface normal estimation is to determine the\nnormal direction at each point. Estimating surface\nnormal has wide applications in, e.g., rendering. The\ntask is challenging because it requires the approach to\nunderstand the shapes completely for dense regression.\nWe again used ModelNet40 as a benchmark, and used\naverage cosine distance to measure the diﬀerence\nbetween ground truth and predicted normals. For\nall the three models, a batch size of 32.200 training\nepochs were used. The initial learning rates were also\nset as 0.01, with a cosine annealing schedule used\nto adjust learning rate every epoch. As indicated\nin Table 2, both our NPCT and SPCT make a\nTable 1 Comparison with state-of-the-art methods on the\nModelNet40 classiﬁcation dataset. Accuracy means overall accuracy.\nAll results quoted are taken from the cited papers. P = points, N =\nnormals\nMethod Input #Points Accuracy\nPointNet [1] P 1k 89.2%\nA-SCN [32] P 1k 89.8%\nSO-Net [33] P, N 2k 90.9%\nKd-Net [34] P 32k 91.8%\nPointNet++ [21] P 1k 90.7%\nPointNet++ [21] P, N 5k 91.9%\nPointGrid [35] P 1k 92.0%\nPCNN [4] P 1k 92.3%\nPointWeb [36] P 1k 92.3%\nPointCNN [3] P 1k 92.5%\nPointConv [5] P, N 1k 92.5%\nA-CNN [37] P, N 1k 92.6%\nP2Sequence [38] P 1k 92.6%\nKPConv [39] P 7k 92.9%\nDGCNN [26] P 1k 92.9%\nRS-CNN [40] P 1k 92.9%\nPointASNL [27] P 1k 92.9%\nNPCT P 1k 91.0%\nSPCT P 1k 92.0%\nPCT P1 k 93.2%\nTable 2 Normal estimation average cosine-distance error on\nModelNet40 dataset\nMethod #Points Error\nPointNet [1] 1k 0.47\nPointNet++ [21] 1k 0.29\nPCNN [4] 1k 0.19\nRS-CNN [40] 1k 0.15\nNPCT 1k 0.24\nSPCT 1k 0.23\nPCT 1k 0.13\nsigniﬁcant improvement compared with PointNet and\nPCT achieves the lowest average cosine distance.\n4.3 Part segmentation task on ShapeNet\ndataset\nPoint cloud part segmentation is a challenging task\nwhich aims to divide a 3D model into multiple\nmeaningful parts. We performed an experimental\nevaluation on the ShapeNet Parts dataset [31], which\ncontains 16,880 3D models with a training to testing\nsplit of 14, 006 to 2874. It has 16 object categories and\n50 part labels; each instance contains no fewer than\ntwo parts. Following PointNet [ 1], all models were\ndownsampled to 2048 points, retaining point-wise\npart annotation. During training, random translation\nin [ −0.2, 0.2], and random anisotropic scaling in\n[0.67, 1.5] were applied to augment the input data.\nDuring testing, we used a multi-scale testing strategy,\nwhere the scales are set in [0 .7, 1.5] with a step of\n0.1. For all the three models, the batch size, training\nepochs, and the learning rates were set the same as\nthe training of normal estimation task.\nTable 3 shows the class-wise segmentation\nresults. The evaluation metric used is part-average\nIntersection-over-Union, and is given both overall\nand for each object category. The results show that\nour SPCT makes an improvement of 2 .1% and 0.6%\nover PointNet and NPCT respectively. PCT achieves\nthe best results with 86\n.4% part-average Intersection-\nover-Union. Figure 5 shows further segmentation\nexamples provided by PointNet, NPCT, SPCT, and\nPCT.\n4.4 Semantic segmentation task on S3DIS\ndataset\nThe S3DIS is a indoor scene dataset for point cloud\nsemantic segmentation. It contains 6 areas and 271\nrooms. Each point in the dataset is divided into 13\n\nPCT: Point cloud transformer 195\nTable 3 Comparison on the ShaperNet part segmentation dataset. pIoU means part-average Intersection-over-Union. All results quoted are\ntaken from the cited papers\nMethod pIoU air-\nplane\nbag cap car chair ear-\nphone\nguitar knife lamp laptop motor-\nbike\nmug pistol rocket skate-\nboard\ntable\nPointNet [1] 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6\nKd-Net [34] 82.3 80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3\nSO-Net [33] 84.9 82.8 77.8 88.0 77.3 90.6 73.5 90.7 83.9 82.8 94.8 69.1 94.2 80.9 53.1 72.9 83.0\nPointNet++ [21] 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6\nPCNN [4] 85.1 82.4 80.1 85.5 79.5 90.8 73.2 91.3 86.0 85.0 95.7 73.2 94.8 83.3 51.0 75.0 81.8\nDGCNN [26] 85.2 84.0 83.4 86.7 77.8 90.6 74.7 91.2 87.5 82.8 95.7 66.3 94.9 81.1 63.5 74.5 82.6\nP2Sequence [38] 85.2 82.6 81.8 87.5 77.3 90.8 77.1 91.1 86.9 83.9 95.7 70.8 94.6 79.3 58.1 75.2 82.8\nPointConv [5] 85.7 —— — —— — — —— — — —— — — —\nPointCNN [3] 86.1 84.1 86.5 86.0 80.8 90.6 79.7 92.3 88.4 85.3 96.1 77.2 95.2 84.2 64.2 80.0 83.0\nPointASNL [27] 86.1 84.1 84.7 87.9 79.7 92.2 73.7 91.0 87.2 84.2 95.8 74.4 95.2 81.0 63.0 76.3 83.2\nRS-CNN [40] 86.2 83.5 84.8 88.8 79.6 91.2 81.1 91.6 88.4 86.0 96.0 73.7 94.1 83.4 60.5 77.7 83.6\nNPCT 85.2 83.2 74.5 86.7 76.8 90.7 75.4 91.1 87.3 84.5 95.7 65.2 93.7 82.7 56.9 73.8 83.0\nSPCT 85.8 84.5 83.5 85.9 78.7 90.9 75.1 92.1 87.0 85.0 95.9 69.6 94.5 82.2 61.4 76.0 83.0\nPCT 86.4 85.0 82.4 89.0 81.2 91.9 71.5 91.3 88.1 86.3 95.8 64.6 95.8 83.6 62.2 77.6 83.7\nFig. 5 Segmentations from PointNet, NPCT, SPCT, PCT, and ground truth (GT).\nTable 4 Comparison on the S3DIS semantic segmentation dataset tested on Area5\nMethod mAcc mIoU ceil-\ning\nﬂoor wall beam column window door chair table book-\ncase\nsofa board clutter\nPointNet [1] 48.98 41.09 88.80 97.33 69.80 0.05 3.92 46.26 10.76 58.93 52.61 5.85 40.28 26.38 33.22\nSEGCloud [2] 57.35 48.92 90.06 96.05 69.86 0.00 18.37 38.35 23.12 70.40 75.89 40.88 58.42 12.96 41.60\nDGCNN [26] 84.10 56.10 ———— — — ————— — —\nPointCNN [3] 63.86 57.26 92.31 98.24 79.41 0.00 17.60 22.77 62.09 74.39 80.59 31.67 66.67 62.05 56.74\nSPG [24] 66.50 58.04 89.35 96.87 78.12 0.00 42.81 48.93 61.58 84.66 75.41 69.84 52.60 2.10 52.22\nPCNN [4] 67.01 58.27 92.26 96.20 75.89 0.27 5.98 69.49 63.45 66.87 65.63 47.28 68.91 59.10 46.22\nPointWeb [36] 66.64 60.28 91.95 98.48 79.39 0.00 21.11 59.72 34.81 76.33 88.27 46.89 69.30 64.91 52.46\nPCT 67.65 61.33 92.54 98.42 80.62 0.00 19.37 61.64 48.00 76.58 85.20 46.22 67.71 67.93 52.29\n\n196 M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.\ncategories. For fair comparison, we use the same data\nprocessing method as Ref. [ 1]. Table 4 shows that\nour PCT achieves superior performance compared to\nthe previous methods.\n4.5 Computational requirements analysis\nWe now consider the computational requirements\nof NPCT, SPCT, PCT, and several other methods\nby comparing the ﬂoating point operations required\n(FLOPs) and number of parameters (Params) in\nTable 5. SPCT has the lowest memory requirements\nwith only 1.36M parameters and also puts a low load\non the processor of only 1.82G FLOPs, yet delivers\nhighly accurate results. These characteristics make it\nsuitable for deployment on a mobile device. PCT\nhas best performance, yet modest computational\nand memory requirements. If we pursue higher\nperformance and ignore the amount of calculation\nand parameters, we can add a neighbor embedding\nlayer in the input embedding module. The results\nof 3-layer embedding PCT are shown in Tables 6\nand 7.\nTable 5 Computational resource requirements\nMethod #Params #FLOPs Accuracy\nPointNet [1] 3.47M 0.45G 89.2%\nPointNet++(SSG) [21] 1.48M 1.68G 90.7%\nPointNet++(MSG) [21] 1.74M 4.09G 91.9%\nDGCNN [26] 1.81M 2.43G 92.9%\nNPCT 1.36M 1.80G 91.0%\nSPCT 1.36M 1.82G 92.0%\nPCT 2.88M 2.32G 93.2%\nTable 6 Comparison on the ModelNet40 classiﬁcation dataset. PCT-\n2L means PCT with 2 layer neighbor embedding and PCT-3L means\nPCT with 3 layer neighbor embedding. Accuracy means overall\naccuracy. P = points\nMethod Input #Points Accuracy\nPCT-2L P 1k 93.2%\nPCT-3L P1 k 93.4%\n5 Conclusions\nIn this paper, we propose a permutation-invariant\npoint cloud transformer, which is suitable for learning\non unstructured point clouds with irregular domain.\nThe proposed oﬀset-attention and normalization\nmechanisms help to make our PCT eﬀective.\nExperiments show that PCT has good semantic\nfeature learning capability, and achieves state-of-\nthe-art performance on several tasks, particularly\nshape classiﬁcation, part segmentation, and normal\nestimation.\nTransformer has already revealed powerful\ncapabilities given large amounts of training data.\nAt present, the available point cloud datasets are\nvery limited compared to image. In future, we will\ntrain it on larger datasets and study its advantages\nand disadvantages with respect to other popular\nframeworks. The encoder–decoder structure of\nTransformer supports more complex tasks, such as\npoint cloud generation and completion. We will\nextend the PCT to further applications. Besides, we\nwill attempt more precise methods to approximate\nLaplacian operation and complete oﬀset-attention.\nAcknowledgements\nThis work was supported by the National Natural\nScience Foundation of China (Project Number\n61521002) and the Joint NSFC–DFG Research\nProgram (Project Number 61761136018).\nReferences\n[1] Charles, R. Q.; Hao, S.; Mo, K. C.; Guibas, L.\nJ. PointNet: Deep learning on point sets for 3D\nclassiﬁcation and segmentation. IN: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, 77–85, 2017.\n[2] Tchapmi, L. P.; Choy, C. B.; Armeni, I.; Gwak, J.;\nSavarese, S. SEGCloud: Semantic segmentation of\n3D point clouds. In: Proceedings of the International\nConference on 3D Vision, 537–547, 2017.\nTable 7 Comparison on the ShaperNet part segmentation dataset. pIoU means part-average Intersection-over-Union. PCT-2L means PCT\nwith 2 layer neighbor embedding and PCT-3L means PCT with 3 layer neighbor embedding\nMethod pIoU air-\nplane\nbag cap car chair ear-\nphone\nguitar knife lamp laptop motor-\nbike\nmug pistol rocket skate-\nboard\ntable\nPCT-2L 86.4 85.0 82.4 89.0 81.2 91.9 71.5 91.3 88.1 86.3 95.8 64.6 95.8 83.6 62.2 77.6 83.7\nPCT-3L 86.6 85.3 84.5 89.4 81.0 91.7 78.6 91.5 87.5 85.8 96.0 70.6 95.6 82.8 60.9 76.6 83.7\n\nPCT: Point cloud transformer 197\n[3] Li, Y.; Bu, R.; Sun, M.; Wu, W.; Di, X.; Chen, B.\nPointCNN: Convolution on x-transformed points. In:\nProceedings of the 32nd International Conference on\nNeural Information Processing Systems, 828–838, 2018.\n[4] Atzmon, M.; Maron, H.; Lipman, Y. Point\nconvolutional neural networks by extension operators.\nACM Transactions on GraphicsVol. 37, No. 4, Article\nNo. 71, 2018.\n[5] Wu, W. X.; Qi, Z.; Fuxin, L. PointConv:\nDeep convolutional networks on 3D point clouds.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 9613–9622,\n2019.\n[6] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, L.; Polosukhin, I.\nAttention is all you need. In: Proceedings of the\n31st International Conference on Neural Information\nProcessing, 6000–6010, 2017.\n[7] Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Houlsby, N. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\n[8] Wu, B.; Xu, C.; Dai, X.; Wan, A.; Zhang, P.; Tomizuka,\nM.; Keutzer, K.; Vajda, P. Visual transformers: Token-\nbased image representation and processing for computer\nvision. arXiv preprint arXiv:2006.03677, 2020.\n[9] Bruna, J.; Zaremba, W.; Szlam, A.; LeCun, Y. Spectral\nnetworks and locally connected networks on graphs.\nIn: Proceedings of the International Conference on\nLearning Representations, 2014.\n[10] Hu, S.-M.; Liang, D.; Yang, G.-Y.; Yang, G.-W.; Zhou,\nW.-Y. Jittor: A novel deep learning framework with\nmeta-operators and uniﬁed graph execution. Science\nChina Information Sciences Vol. 63, No. 12, Article No.\n222103, 2020.\n[11] Bahdanau, D.; Cho, K. H.; Bengio, Y. Neural machine\ntranslation by jointly learning to align and translate.\nIn: Proceedings of the 3rd International Conference on\nLearning Representations, 2015.\n[12] Lin, Z.; Feng, M.; dos Santos, C. N.; Yu, M.;\nXiang, B.; Zhou, B.; Bengio, Y. A structured self-\nattentive sentence embedding. In: Proceedings of the\nInternational Conference on Learning Representations,\n2017.\n[13] Devlin, J.; Chang, M.; Lee, K.; Toutanova, K.\nBERT: Pre-training of deep bidirectional transformers\nfor language understanding. In: Proceedings of the\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Vol. 1, 4171–4186, 2019.\n[14] Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J. G.;\nSalakhutdinov, R.; Le, Q. V. XLNet: Generalized\nautoregressive pretraining for language understanding.\nIn: Proceedings of the 33rd Conference on Neural\nInformation Processing Systems, 5754–5764, 2019.\n[15] Dai, Z. H.; Yang, Z. L.; Yang, Y. M.; Carbonell, J.;\nLe, Q.; Salakhutdinov, R. Transformer-XL: Attentive\nlanguage models beyond a ﬁxed-length context. In:\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2978–2988,\n2019.\n[16] Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So,\nC. H.; Kang, J. BioBERT: A pre-trained biomedical\nlanguage representation model for biomedical text\nmining. Bioinformatics Vol. 36, No. 4, 1234–1240, 2020.\n[17] Wang, F.; Jiang, M. Q.; Qian, C.; Yang, S.; Li, C.;\nZhang, H. G.; Wang, X.; Tang, X. Residual attention\nnetwork for image classiﬁcation. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 6450–6458, 2017.\n[18] Hu, J.; Shen, L.; Sun, G. Squeeze-and-excitation\nnetworks. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern\nRecognition, 7132–7141, 2018.\n[19] Zhang, H.; Goodfellow, I. J.; Metaxas, D. N.; Odena,\nA. Self-attention generative adversarial networks. In:\nProceedings of the International Conference on Machine\nLearning, 7354–7363, 2019.\n[20] Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.;\nKirillov, A.; Zagoruyko, S. End-to-end object detection\nwith transformers. In: Computer Vision – ECCV\n2020. Lecture Notes in Computer Science, Vol. 12346.\nVedaldi, A.; Bischof, H.; Brox, T.; Frahm, J. M. Eds.\nSpringer Cham, 213–229, 2020.\n[21] Qi, C. R.; Yi, L.; Su, H.; Guibas, L. J. PointNet++:\nDeep hierarchical feature learning on point sets in a\nmetric space. In: Proceedings of the 31st Conference\non Neural Information Processing Systems, 5099–5108,\n2017.\n[22] Hermosilla, P.; Ritschel, T.; V´ azquez, P. P.; Vinacua,\n`A.; Ropinski, T. Monte Carlo convolution for\nlearning on non-uniformly sampled point clouds. ACM\nTransactions on Graphics Vol. 37, No. 6, Article No.\n235, 2018.\n[23] Tatarchenko, M.; Park, J.; Koltun, V.; Zhou, Q.\nY. Tangent convolutions for dense prediction in 3D.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 3887–3896,\n2018.\n\n198 M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.\n[24] Landrieu, L.; Simonovsky, M. Large-scale point cloud\nsemantic segmentation with superpoint graphs. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 4558–4567, 2018.\n[25] Yang, Y. Q.; Liu, S. L.; Pan, H.; Liu, Y.; Tong,\nX. PFCNN: Convolutional neural networks on 3D\nsurfaces using parallel frames. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 13575–13584, 2020.\n[26] Wang, Y.; Sun, Y.; Liu, Z.; Sarma, S. E.; Bronstein, M.\nM.; Solomon, J. M. Dynamic graph CNN for learning\non point clouds. ACM Transactions on GraphicsVol.\n38, No. 5, Article No. 146, 2019.\n[27] Yan, X.; Zheng, C. D.; Li, Z.; Wang, S.; Cui, S.\nG. PointASNL: Robust point clouds processing using\nnonlocal neural networks with adaptive sampling. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 5588–5597, 2020.\n[28] Hertz, A.; Hanocka, R.; Giryes, R.; Cohen-Or, D.\nPointGMM: A neural GMM network for point clouds.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 12051–\n12060, 2020.\n[29] Wang, Y.; Solomon, J. Deep closest point:\nLearning representations for point cloud registration.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 3522–3531, 2019.\n[30] Wu, Z.; Song, S.; Khosla, A.; Yu, F.; Zhang,\nL.; Tang, X.; Xiao, J. 3D ShapeNets: A deep\nrepresentation for volumetric shapes. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1912–1920, 2015.\n[31] Yi, L.; Kim, V. G.; Ceylan, D.; Shen, I. C.; Yan, M.\nY.; Su, H.; Lu, C.; Huang, Q.; Sheﬀer, A.; Guibas, L.\nA scalable active framework for region annotation in\n3D shape collections. ACM Transactions on Graphics\nVol. 35, No. 6, Article No. 210, 2016.\n[32] Xie, S. N.; Liu, S. N.; Chen, Z. Y.; Tu, Z. W.\nAttentional ShapeContextNet for point cloud recognition.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 4606–4615,\n2018.\n[33] Li, J. X.; Chen, B. M.; Lee, G. H. SO-net: Self-\norganizing network for point cloud analysis. In:\nProceeding of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 9397–9406, 2018.\n[34] Klokov, R.; Lempitsky, V. Escape from cells: Deep kd-\nnetworks for the recognition of 3D point cloud models.\nIn: Proceeding of the IEEE International Conference\non Computer Vision, 863–872, 2017.\n[35] Le, T.; Duan, Y. PointGrid: A deep network for\n3D shape understanding. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, 9204–9214, 2018.\n[36] Zhao, H.; Jiang, L.; Fu, C.; Jia, J. PointWeb:\nEnhancing local neighborhood features for point cloud\nprocessing. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 5560–5568,\n2019.\n[37] Komarichev, A.; Zhong, Z. C.; Hua, J. A-CNN:\nAnnularly convolutional neural networks on point\nclouds. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 7413–\n7422, 2019.\n[38] Liu, X. H.; Han, Z. Z.; Liu, Y. S.; Zwicker, M.\nPoint2Sequence: Learning the shape representation\nof 3D point clouds with an attention-based sequence\nto sequence network. In: Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, Vol. 33, 8778–\n8785, 2019.\n[39] Thomas, H.; Qi, C. R.; Deschaud, J. E.;\nMarcotegui, B.; Goulette, F.; Guibas, L. KPConv:\nFlexible and deformable convolution for point clouds.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 6410–6419, 2019.\n[40] Liu, Y. C.; Fan, B.; Xiang, S. M.; Pan, C. H. Relation-\nshape convolutional neural network for point cloud\nanalysis. In: Proceeding of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 8887–\n8896, 2019.\nMeng-Hao Guo received his bachelor\ndegree in Xidian University. Now he is\na Ph.D. candidate in the Department\nof Computer Science and Technology,\nTsinghua University. His research\ninterests include computer graphics,\ncomputer vision, and machine learning.\nJun-Xiong Cai is currently a\npostdoctoral researcher at Tsinghua\nUniversity, where he received Ph.D.\ndegree in computer science and\ntechnology in 2020. His research\ninterests include computer graphics,\ncomputer vision, and 3D geometry\nprocessing.\n\nPCT: Point cloud transformer 199\nZheng-Ning Liu received his bachelor\ndegree in computer science from\nTsinghua University in 2017. He is\ncurrently a Ph.D. candidate in the\nDepartment of Computer Science and\nTechnology, Tsinghua University. His\nresearch interests include 3D computer\nvision, 3D reconstruction, and computer\ngraphics.\nTai-Jiang Mu is currently an assistant\nresearcher at Tsinghua University, where\nhe received his B.S. and Ph.D. degrees\nin computer science and technology\nin 2011 and 2016, respectively. His\nresearch interests include computer\nvision, robotics, and computer graphics.\nRalph R. Martin received his Ph.D.\ndegree from Cambridge University in\n1983. He is currently a emeritus\nprofessor with Cardiﬀ University. He\nhas authored over 250 papers and 14\nbooks, covering such topics as solid and\nsurface modeling, intelligent sketch input,\ngeometric reasoning, reverse engineering,\nand various aspects of computer graphics. He is a Fellow of\nthe Learned Society of Wales, the Institute of Mathematics\nand its Applications, and the British Computer Society. He\nis currently the Associate Editor-in-Chief of Computational\nVisual Media.\nShi-Min Hu is current a professor in\nthe Department of Computer Science\nand Technology, Tsinghua University,\nBeijing, China. He received his Ph.D.\ndegree from Zhejiang University in 1996.\nHis research interests include digital\ngeometry processing, video processing,\nrendering, computer animation, and\ncomputer-aided geometric design. He has published more\nthan 100 papers in journals and refereed conferences. He\nis the Editor-in-Chief of Computational Visual Media, and\non editorial boards of several journals, including Computer\nAided Design and Computer & Graphics. He is a senior\nmember of IEEE and ACM, and Fellow of CCF and SMA.\nOpen Access This article is licensed under a Creative\nCommons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduc-\ntion in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link\nto the Creative Commons licence, and indicate if changes\nwere made.\nThe images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and\nyour intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder.\nTo view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nOther papers from this open access journal are available\nfree of charge from http://www.springer.com/journal/41095.\nTo submit a manuscript, please go to https://www.\neditorialmanager.com/cvmj.\n"
}