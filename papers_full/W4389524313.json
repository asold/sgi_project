{
    "title": "We’re Afraid Language Models Aren’t Modeling Ambiguity",
    "url": "https://openalex.org/W4389524313",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3179416558",
            "name": "Alisa Liu",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2117284501",
            "name": "Zhaofeng Wu",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2695838048",
            "name": "Julian Michael",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2589188213",
            "name": "Alane Suhr",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A1830218503",
            "name": "Peter West",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111272094",
            "name": "Alexander Koller",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A336169979",
            "name": "Swabha Swayamdipta",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2166589550",
            "name": "Noah Smith",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2133417374",
            "name": "Yejin Choi",
            "affiliations": [
                "University of California, Berkeley"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2548036585",
        "https://openalex.org/W3034906811",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W3196841052",
        "https://openalex.org/W3212327893",
        "https://openalex.org/W2985347336",
        "https://openalex.org/W4225006216",
        "https://openalex.org/W1510929985",
        "https://openalex.org/W2118004980",
        "https://openalex.org/W4385565230",
        "https://openalex.org/W1562687567",
        "https://openalex.org/W4385574293",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3035599593",
        "https://openalex.org/W4385573849",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W3174102190",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3156031972",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4285077564",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4312107542",
        "https://openalex.org/W2158686254",
        "https://openalex.org/W3100292568",
        "https://openalex.org/W4313459382",
        "https://openalex.org/W2105897558",
        "https://openalex.org/W4385572752",
        "https://openalex.org/W2971307358",
        "https://openalex.org/W4220747294",
        "https://openalex.org/W4386576870",
        "https://openalex.org/W4311726857",
        "https://openalex.org/W4385571845",
        "https://openalex.org/W4389524305",
        "https://openalex.org/W1555547974",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3184074368",
        "https://openalex.org/W1592072150",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W4290771878",
        "https://openalex.org/W2079656678",
        "https://openalex.org/W3166002735",
        "https://openalex.org/W4385574091",
        "https://openalex.org/W4322718191"
    ],
    "abstract": "Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah Smith, Yejin Choi. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 790–807\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nWe’re Afraid Language Models Aren’t Modeling Ambiguity\nAlisa Liu♡ Zhaofeng Wu♦ Julian Michael♠ Alane Suhr♣△ Peter West♡♣\nAlexander Koller♣♥ Swabha Swayamdipta♢ Noah A. Smith♡♣ Yejin Choi♡♣\n♡Paul G. Allen School of Computer Science & Engineering, University of Washington\n♣Allen Institute for AI ♢University of Southern California △UC Berkeley\n♥Saarland University ♠New York University ♦Massachusetts Institute of Technology\nalisaliu@cs.washington.edu\nAbstract\nAmbiguity is an intrinsic feature of natural lan-\nguage. Managing ambiguity is a key part of\nhuman language understanding, allowing us\nto anticipate misunderstanding as communica-\ntors and revise our interpretations as listeners.\nAs language models are increasingly employed\nas dialogue interfaces and writing aids, han-\ndling ambiguous language is critical to their\nsuccess. We capture ambiguity in a sentence\nthrough its effect on entailment relations with\nanother sentence, and collect AMBI ENT,1 a\nlinguist-annotated benchmark of 1,645 exam-\nples with diverse kinds of ambiguity. We design\na suite of tests based on AMBI ENT, presenting\nthe first evaluation of pretrained LMs to recog-\nnize ambiguity and disentangle possible mean-\nings. We find that the task remains extremely\nchallenging, including for GPT-4, whose gen-\nerated disambiguations are considered correct\nonly 32% of the time in crowdworker evalu-\nation, compared to 90% for disambiguations\nin our dataset. Finally, to illustrate the value\nof ambiguity-sensitive tools, we show that a\nmultilabel NLI model can flag political claims\nin the wild that are misleading due to ambigu-\nity. We encourage the field to rediscover the\nimportance of ambiguity for NLP.\n1 Introduction\nAmbiguity seems to be an essential, in-\ndispensable element for the transfer of\ninformation from one place to another by\nwords. — Thomas (1974), as referenced\nin the epilogue of Grosz (1977)\nAmbiguity is an intrinsic feature of language, al-\nlowing speakers to balance efficiency and clarity\nin communication (Zipf, 1949; Piantadosi et al.,\n2012). Language understanding thus requires rec-\nognizing the presence of multiple interpretations:\n1Data and code can be found at https://github.com/\nalisawuffles/ambient\nFigure 1: Ambiguity can be the result of innocent mis-\ncommunication (top), or deliberately used to mislead\none’s listeners (bottom). For instance, a cat may be lost\nin the sense of being confused about its whereabouts\n(entailment edge), or lost in the sense that others cannot\nfind it (neutral edge). Each example in AMBI ENT con-\ntains a set of labels corresponding to plausible readings,\nalong with a disambiguating rewrite for each reading.\nas communicators, we anticipate the possibility of\nmisunderstanding; as listeners, we ask clarifying\nquestions, disambiguate meanings on the basis of a\nwide range of contextual factors, and backtrack and\nrevise our earlier interpretations as needed. Beyond\nunintended miscommunication, ambiguity is also\nan effective tool for sending covert messages, e.g.,\nout of politeness or to mislead one’s listeners while\navoiding accountability (see Figure 1).\nAs language models (LMs) are increasingly em-\nployed to act as dialogue agents (OpenAI, 2022;\nShuster et al., 2022) or to aid human communica-\ntion as writing aids (Lee et al., 2022), being able\nto work with ambiguous language will make them\nmore effective. This skill would support adaptation\nto different contexts, clearer communication, and\nidentification of misleading or deceptive language.\nYet, the ability of pretrained LMs to recognize am-\nbiguity and disentangle possible meanings remains\nunstudied, partly because ambiguous instances are\nsystematically excluded in the curation of bench-\nmarks (Beigman Klebanov and Beigman, 2009).\n790\nWe present AMBI ENT, Ambiguity in\nEntailment, an English benchmark of 1,645\nexamples covering a variety of lexical, syntactic,\nand pragmatic ambiguities, and more broadly\nsentences which can be plausibly read as conveying\none of multiple different messages. Formally\ncharacterizing ambiguity requires a choice of\nmeaning representation to distinguish between\npossible interpretations, and enumerating the full\nset of interpretations can be tricky or impractical.2\nThus, we adopt a functional approach: using the\nnatural language inference (NLI) task format,\nwe characterize ambiguity in the premise and/or\nhypothesis by its effect on entailment relations.3\nEach AMBI ENT example consists of a premise\nand hypothesis pair, assigned aset of labels (among\nentailment, neutral, and contradiction), along with\ndisambiguating rewrites corresponding to each la-\nbel when multiple are plausible (see Table 1 for\nexamples). Examples are collected through two ap-\nproaches: manual curation to target textbook ambi-\nguities, and expert annotation of automatically gen-\nerated unlabeled examples to uncover more diverse\nphenomena. Through analysis, we find that crowd-\nworkers can reliably distinguish different readings\nof an ambiguous sentence and their impact on en-\ntailment choices; thus we can explicitly charac-\nterize the underlying reasons for uncertainty that\nwould otherwise surface as “disagreement” (§3).\nWe design a suite of tests based on AMBI ENT\nto investigate the extent to which understanding of\nambiguity is acquired during pretraining of large\nLMs (§4). These tests evaluate whether LMs can di-\nrectly produce relevant disambiguations, recognize\npossible interpretations, and model different inter-\npretations in their continuation distributions. We\nfind that these tasks remain extremely challenging,\nincluding for the recent GPT-4(OpenAI, 2023).\nTherefore, we additionally investigate whether\nLMs can be finetuned on existing NLI data for the\nless demanding task of ambiguityrecognition, with-\nout explicit disambiguation (§5). We adapt several\nfinetuned NLI models to a multilabel setting, and\nfind that the best model predicts the exact label set\nin only 43.6% of instances, suggesting that the NLI\n2For example, Koller et al. (2008) find that including\nall possible quantifier scope readings in the Rondane Tree-\nbank (Copestake and Flickinger, 2000) results in 5% of sen-\ntences having ≥650,000 possible semantic analyses.\n3By design, we only consider ambiguities in NLI examples\nthat affect the determination of the label, to provide a natural\nsetting where ambiguities are contextually relevant.\ntask is much more challenging when formulated to\naccount for ambiguity.\nFinally, to illustrate the value of ambiguity-\nsensitive tools, we present a case study of how\na multilabel NLI model can be used to detect mis-\nleading political claims in the wild. We find that the\nstrongest model from §5, despite its limitations, can\nnot only recover claims flagged by fact-checkers as\nambiguous, but highlight previously unidentified\nambiguous claims, indicating the promise of such\ntools to aid real-world communication.\nThe simplifying assumption that text has only\none interpretation has facilitated the development\nof large-scale benchmarks, yet limits the depth of\nwhat these benchmarks can evaluate. In this work\nwe show that sensitivity to ambiguity—a funda-\nmental aspect of human language understanding—\nis lacking in our ever-larger models, and illustrate\nthe value such understanding could bring.\n2 A MBI ENT\nTraditionally, the NLI task requires predicting\nwhether a premise entails, contradicts, or is neutral\nwith respect to a hypothesis. Yet, ambiguities in\nthe premise and/or hypothesis (as in Table 1) may\nimpact the determination of the label.\nWe present AMBI ENT, a dataset of 1,645 NLI\nexamples, each annotated with a set of labels,\nreflecting potentially multiple readings of the\npremise and/or hypothesis. Ambiguous examples,\ni.e., those having more than one label, make up\n35.2% of the dataset and include adisambiguating\nrewrite corresponding to each label; unambiguous\nexamples have a single label. The inclusion of un-\nambiguous examples facilitates evaluating model\nabilities to first detect the presence of relevant ambi-\nguity, and then resolve it to distinct interpretations.\nWe use two approaches to collect source exam-\nples: manual curation and automatic generation.\nManual curation (§2.1) involves crafting a small set\nof examples targeting specific types of ambiguity.\nFurther, to cover more diverse forms of ambiguity,\nwe produce a larger collection of examples via text\ngeneration and heuristic filtering (§2.2), followed\nby expert manual annotation (§2.3), forming the\nbulk of AMBI ENT. Details are in §A.\n2.1 Curated Examples\nThe authors curate a set of 142 examples, which are\neither handwritten or sourced from existing NLI\ndatasets and linguistics textbooks (Kearns, 2000;\n791\nExample Disambiguation 1 Disambiguation 2 Type\nP: I’m afraidthe cat was hit by a car.\nH: The cat was not hit by a car.\n/lbagNEUTRAL,CONTRADICT/rbag\n:[7N, 2C]\nP: I’m worried...\nNEUTRAL\n :[9N]\nP: I’m sorry to share that...\nCONTRADICT\n :[9C]\nPragmatic\n(44.8%)\nP: John and Anna are married.\nH: John and Anna are not a couple.\n/lbagNEUTRAL,CONTRADICT/rbag\n:[5N, 4C]\nP: ... are both married.\nNEUTRAL\n :[7N, 2E]\nP: ... are married to each other.\nCONTRADICT\n :[9C]\nLexical\n(20.0%)\nP: This seminar is full now, but interesting seminars\nare being offered next quarter too.\nH: There will be more interesting seminars...\n/lbagENTAIL,NEUTRAL/rbag\n:[7E, 2N]\nH: There will bemore seminars\n... that are interesting.\nENTAIL\n :[9E]\nH: There will be seminars... that\nare more interesting.\nNEUTRAL\n :[9N]\nSyntactic\n(8.6%)\nP: The novel has been banned in many schools be-\ncause of its explicit language.\nH: The novel hasnot been banned in many schools.\n/lbagNEUTRAL,CONTRADICT/rbag\n:[4N, 5C]\nH: There are many schools\nwhere the novel hasnot been\nbanned.\nNEUTRAL\n :[9N]\nH: It is not the casethat the\nnovel has been banned in many\nschools.\nCONTRADICT\n :[9C]\nScopal\n(7.6%)\nP: It is currently March, and they plan to schedule\ntheir wedding for next December.\nH: They plan to schedule... for next year.\n/lbagENTAIL,CONTRADICT/rbag\n:[3E, 2N, 4C]\nP: ... for December next year.\nENTAIL\n :[9E]\nP: ... forthe coming December.\nCONTRADICT\n :[9C]\nCoreference\n(2.9%)\nP: It isdifficult to believethat the author of such a\nmasterpiece could have been only 23 years old.\nH: The author of the masterpiece was only 23.\n/lbagENTAIL,NEUTRAL/rbag\n:[3E, 6N]\nP: It is shockingthat...\nENTAIL\n :[9E]\nP: It is questionablethat...\nNEUTRAL\n :[9N]\nFigurative\n(1.9%)\nP: A new study has found that nearly half of all\nAmericans are in favor of gun control.\nH: The study found thathalfof all Americans are\nin favor of gun control.\n/lbagENTAIL,CONTRADICT/rbag\n:[1E, 2N, 6C]\nH: ... that exactly halfof all\nAmericans...\nCONTRADICT\n :[8C, 1N]\nH: ... that about halfof all\nAmericans...\nENTAIL\n :[9E]\nOther\n(14.3%)\nTable 1: Ambiguous examples in AMBI ENT with linguist-annotated /lbagGOLD LABELS/rbag. As analysis, we collect the\n :\n[distribution of NLI labels] as judged by nine crowdworkers under the traditional single-label annotation scheme\n(§3), finding that disagreement on ambiguous examples is largely resolved on disambiguations. The Type column\nindicates the ambiguity type for each example, along with its estimated representation in the dataset (§2.5).\nCarnie, 2013). We choose examples ad hoc from\nthe synthetic NLI datasets DistNLI (Ban et al.,\n2022) for predicate distributivity (e.g., “Sam and\nFrank gave a talk” may either mean separately or\njointly) and IMPPRES (Jeretic et al., 2020) for\nimplicatures. We also include some instances with\ndiffering pragmatic and literal readings from NLI\nDiagnostics (Wang et al., 2018), and ones leading\nto disagreement from large-scale NLI datasets like\nMNLI (Williams et al., 2018) and WANLI (Liu\net al., 2022). The authors directly annotate these\nexamples with the set of labels and disambiguations\n(examples in §A.1).\n2.2 Generated Examples\nTo cover more ambiguities, we use overgenera-\ntion and filtering to automatically create a large\ncorpus of unlabeled NLI examples that are likely\nto be ambiguous. Inspired by WANLI (Liu et al.,\n2022), we automatically identify groups of premise-\nhypothesis pairs that share a reasoning pattern, to\nencourage the creation of new examples with the\nsame pattern. We use WANLI as our source of\nexamples; each group contains a randomly chosen\nexample on which its two annotators disagreed (in-\ndicating possible ambiguity), along with its 4 near-\nest neighbors according to the final-layer embed-\nding of a WANLI-trained NLI model. We observe\nthat these groups can share interpretable ambigu-\nity patterns, such as sentences about the past (e.g.,\n“When I was young, I was obsessed ”) inducing a\ncancellable implicature about the present (that “I”\nam no longer obsessed; full prompt in §A).\nThese groups of examples are formatted into a\nprompt with the instruction, “ Write pairs of sen-\ntences that are related to each other in the same\n792\nFigure 2: Pipeline for the annotation of generated exam-\nples in AMBI ENT. Unlabeled examples are created by\nInstructGPT, then annotated independently by two lin-\nguists, whose annotations are consolidated by an author.\nway.” For each prompt, we sample 5 continuations\nfrom InstructGPT(Ouyang et al., 2022), discard-\ning those that cannot be parsed into a premise and\nhypothesis.\nTo further filter for likely-ambiguous instances,\nwe use a multilabel RoBERTa-large model trained\non WANLI and retain all examples where the\nmodel assigns probability ≥0.05 to more than one\nNLI label, indicating at least slight uncertainty in\nwhether there can be multiple possible readings.\n2.3 Annotation and Validation\nExamples acquired in §2.2 consist of unlabeled\npremise-hypothesis pairs, which we next annotate\nwith label sets and relevant disambiguations. Fol-\nlowing AMBIG QA (Min et al., 2020) and as shown\nin Figure 2, each example is first annotated by two\nexperts, then presented to a third expert for valida-\ntion and consolidation.\nWe recruit 37 university-level linguistics stu-\ndents for the annotation phase,4 as identifying am-\nbiguities of a sentence then delineating its possible\ninterpretations is a challenging task. They select a\nset of labels for each example, including the single-\nton set when the example is unambiguous; when\nmore than one label is chosen, they provide a disam-\nbiguating rewrite for each one. They are asked to\ndiscard the example if it is offensive or low-quality\ndue to issues in fluency or coherence.\nThe validation phase is performed by a subset of\nthe authors to ensure high quality (details in §A.4).\nThe authors review the two sets of annotations to\nrevise and aggregate them into a single coherent an-\nnotation, optionally adding interpretations missed\nby both annotators. Validation is skipped when ei-\n4We refer to them as “linguists” elsewhere.\nFigure 3: Distribution of set labels in AMBI ENT.\nther annotator discarded an example; the validators\nmay additionally discard examples themselves.\nLinguists annotate a total of 2,616 examples.\nDue to the option for discarding, 2,020 examples\nemerge from the annotation phase, and after valida-\ntion, there are a total of 1,503 final examples.\n2.4 Agreement\nTo calculate inter-annotator agreement for valida-\ntion, the four validators annotate a subset of 50 ex-\namples in common. The Fleiss κagreement score\non the binary classification task for each label is\n0.62 for contradiction, 0.65 for entailment, and\n0.44 for neutral, thus ranging from “moderate” to\n“substantial” agreement.\n2.5 A MBI ENT Statistics\nThe final dataset, which combines curated and\ngenerated-then-annotated examples, consists of\n1,645 examples. We sample 100 for a develop-\nment set and treat the rest as the test set. The label\ndistribution is shown in Figure 3.\nTo understand the types of ambiguity present in\nAMBI ENT, the authors annotate a random subset of\n100 ambiguous examples with the ambiguity type,\namong lexical, syntactic, figurative, pragmatic, sco-\npal, coreference, and other (described in §A.6).\nResults are shown in the Type column of Table 1.\n3 Does Ambiguity Explain Disagreement?\nWe conduct an analysis to understand how anno-\ntators behave on ambiguous input, under the tradi-\ntional 3-way annotation scheme for NLI. We find\nthat ambiguity is recognizable to individual work-\ners and explains much of the label variation that\nemerges, thus challenging the popular assumption\nthat example uncertainty should be modeled as “dis-\nagreement” among annotators.\n3.1 Setup\nWe recruit crowdworkers on Amazon Mechanical\nTurk to review ambiguous examples in AMBI ENT.\n793\nEach example is reviewed by 9 workers. The task\nis split into three steps, each appearing only after\nthe earlier steps are complete.\n(i) Annotation of ambiguous example Follow-\ning the traditional NLI labeling setup, crowdwork-\ners are presented with the original ambiguous ex-\nample alone, and asked to choose a single label.\n(ii) Recognition of disambiguations The am-\nbiguous sentence of the example (either the premise\nor hypothesis) is isolated for consideration.5 Three\ncandidate interpretations are presented in a random\norder, composed of the two disambiguations and\na semantically similar “distractor”. (In the case\nwhere an example has three interpretations, no dis-\ntractor is included.) Workers are asked to indicate\nwhether each sentence is a “possible interpretation”\nof the isolated sentence. We instruct that this is sub-\njective, and they should use their best judgment.\nThe distractor ensures that workers do not con-\nsider all sentences as valid readings, and is obtained\nby back-translating the ambiguous sentence with\nYorùbá using NLLB (Meta, 2022). A low-resource\nlanguage is chosen so that the back-translation is a\nclose, but often not entirely faithful, paraphrase.\n(iii) Annotation of disambiguated examples\nThree new NLI examples are obtained by substitut-\ning the ambiguous sentence of the original example\nwith each candidate interpretation from (ii). Work-\ners select a single NLI label for each new example.\n3.2 Results\nAs hypothesized, the original ambiguous examples\nproduce high disagreement, with a Fleissκscore of\n0.12, considered “slight” agreement (step (i)). Dis-\nagreement is largely resolved on the corresponding\ndisambiguated examples (step (iii)), with κincreas-\ning to 0.67, representing “substantial” agreement.\nMoreover, annotators overwhelmingly recognize\ndisambiguations as plausible interpretations of the\nambiguous sentence (step (ii)). True disambigua-\ntions are marked plausible 96.7% of the time, com-\npared to 46.7% for the distractor. On average,\n93.7% of annotators accept all true interpretations,\nthus recognizing the full set of possibilities.\nThrough this experiment, we additionally estab-\nlish crowdworker agreement with AMBI ENT as the\nrate at which the majority vote recognizes the full\n5For simplicity, we only include examples whereeither the\npremise or the hypothesis is ambiguous (93.1% of examples).\nset of ambiguities (step (ii)) and verifies their la-\nbels (step (iii)). In this sense, the agreement rate is\n89.7%. This points to the quality of the dataset and\nis used as a reference point for later experiments.\nOverall, input ambiguity is indeed a source of\n“disagreement” in NLI under a single-label annota-\ntion scheme. However, we have shown that indi-\nvidual annotators overwhelmingly can recognize\nmultiple possible readings of the input and their\ncorresponding output labels, and much of this dis-\nagreement can be resolved in practice by incorpo-\nrating disambiguation into the task. In this way,\ninput ambiguity can be disentangled from annota-\ntor subjectivity.\n4 Evaluating Pretrained Language\nModels\nIn our experiments, we investigate the extent to\nwhich understanding of ambiguity is acquired dur-\ning the course of pretraining. Our three tests eval-\nuate if LMs can directly generate relevant disam-\nbiguations (§4.1), recognize the validity of plausi-\nble interpretations (§4.2), and finally, model open-\nended continuations reflecting different interpreta-\ntions (§4.3). For these tests, we consider only the\nambiguous instances in AMBI ENT.\nAs our set of LMs, we evaluateLLaMa(65B; Tou-\nvron et al., 2023) and GPT-3 (davinci), as well\nas instruction-tuned models FLAN-T5(xxl; Chung\net al., 2022), InstructGPT (text-davinci-003),\nChatGPT(gpt-3.5-turbo), and the recent GPT-4.\n4.1 Generating Disambiguations\nWe first study whether LMs can learn in-context to\ndirectly generate disambiguations and correspond-\ning labels. We construct a natural prompt (see\nTable 2) by explaining that there is some ambiguity\nthat makes the correctness of a “claim” (hypothesis)\ndifficult to resolve given the “context” (premise).\nFor each test instance, we randomly sample 4 other\ntest instances as in-context examples.\nAs there are multiple ways to express the same\ndisambiguation, we perform both automatic and\nhuman evaluation. For the former, we match each\ngenerated disambiguation with a reference disam-\nbiguation based on the generated label.6 Following\nAMBIG QA, we score generations using the EDIT-\nF1 metric, which represents a disambiguation by\n6If the label verbalizer for a disambiguation does not cor-\nrespond to any label in the reference label set, then the model\nreceives a score of 0 for that disambiguation.\n794\nits added and deleted unigrams, and computes the\nF1 score between the reference and the prediction.\nFor human evaluation, we use the same setup as\nthe crowdworker experiment in §3 on 50 randomly\nsampled examples, except without step (i). We use\nthree workers per example, and consider the LM\ncorrect on an example if the majority vote indicates\nthat each disambiguation is plausible (step (ii)) and\nselects the model-predicted NLI labels (step (iii)).\nCrowdworkers are not informed that disambigua-\ntions are model-generated.\nResults Shown in Table 4, the best model is\nGPT-4, achieving an EDIT-F1 score of 18.0% and\nhuman-judged correctness of 32.0%. The latter can\nbe directly compared to crowdworker agreement\nwith AMBI ENT itself at 89.7% (§3).\nOne strategy for attempting disambiguation we\nobserve across models is restating the ambiguous\nsentence with additional context that directly af-\nfirms or negates the hypothesis, rather than making\na targeted revision to clarify the ambiguity. In some\ncases, this “shortcut” does lead to technically cor-\nrect disambiguations (and marked as such in human\nevaluation). For instance, for\nP: He always ignores his mother’s advice\nto follow his own dreams.\nH: He follows his dreams.\nChatGPT disambiguates the premise by restating it,\nfollowed by “and therefore does follow his dreams”\nversus “and therefore does not follow his dreams.”\nThe former forces the interpretation that he ignores\nher advice in order to follow his dreams; the latter\nthe interpretation that his mother’s advice is for\nhim to follow his dreams. Thus, the human-judged\ncorrectness may overestimate the models’ ability\nto precisely report the source of ambiguity.\n4.2 Recognizing Disambiguations\nFor the next test, we focus on the ambiguous sen-\ntences alone (without the rest of the NLI example),\nand create a series of templated true and false state-\nments about possible interpretations as shown in\nTable 3. For instance, it is both true that an ambigu-\nous sentence may mean a particular interpretation,\nbut also that it does not necessarily mean it. We\nconsider the model prediction to be the token with\nthe greater logit between True and False.7 We\n7As the API for ChatGPTand GPT-4does not return token\nlogits, we simply consider whether the top-1 token is correct.\nWe find either True or False is the top token in 97.6% and\n99.7% of examples, respectively, indicating the task is clear.\nInstruction\nIn each example, you will be given somecontextand aclaim,\nwhere the correctness of theclaimis affected by some ambiguity\nin thecontext. Enumerate two or three interpretations of the\ncontextthat lead to different judgments about theclaim.\nExample\nContext: {premise}\nClaim: {hypothesis}Given the context alone, is thisclaim\ntrue,false, orinconclusive?\nWe don’t know, because thecontextcan be interpreted in many\ndifferent ways:\n1. {disambiguation 1}Then theclaimistrue.\n2. {disambiguation 2}Then theclaimisfalse.\n3. {disambiguation 3}Then theclaimisinconclusive.\nTable 2: Few-shot template for the task of generating\ndisambiguations (§4.1) when the premise is ambigu-\nous. The label verbalizer correspondences are true\n↔ ENTAIL, false ↔ CONTRADICT, and inconclusive ↔\nNEUTRAL. The instruction is stated once, followed by\nfour in-context examples. At the end of the prompt, the\ntest example is provided up until “1.”.\nTemplate Correct Answer\n{a}This may mean:{d} True\n{a}This does not necessarily mean:{d} True\n{a}This cannot mean:{d} False\n{a}This can only mean:{d} False\nTable 3: Templates for True/False evaluation (§4.2),\nwhere {a} denotes the ambiguous sentence and {d}\na possible disambiguation. Given the infilled template\nfollowed by “True or False?\n Answer:”, the LM is ex-\npected to choose the correct answer.\nexecute this task zero-shot as the prompt template\ncompletely determines the label.\nResults The T/F Acc. column of Table 4 shows\nthe accuracy averaged across the four templates.\nThe best model (GPT-4) achieves only 63.0% com-\npared to the random accuracy of 50%, with other\nmodels ranging between 49.6% and 57.7%. When\nwe consider the proportion of disambiguations for\nwhich GPT-4answers all four templates correctly,\nperformance drops to 2.5%, below random guess-\ning of 6.25%. We do not observe consistent trends\nacross models on the per-template accuracy (shown\nin §C.2), though four of six models achieve the\nhighest accuracy on template 1.\nFurthermore, we observe that LMs are not inter-\nnally consistent across the questions. For example,\nfor 76% of pairs of disambiguations ( d1,d2) for\nthe same ambiguous sentence a, GPT-4 both ac-\nknowledges that amay mean d1 and may mean d2\n(template 1), yet also asserts that acan only mean\nd1 and can only mean d2 (template 4).\n795\nEDIT-F1 Correct\n(human)T/F Acc.KL Rank.\nAcc.\nFLAN-T5 5.2 0.0 56.4 81.0\nLLaMa 10.0 10.0 55.0 68.9\nGPT-3 10.1 2.0 57.8 75.7\nInstructGPT 14.5 4.0 49.6 71.4\nChatGPT 13.0 18.0 57.7 -\nGPT-4 18.0 32.0 63.0 -\nTable 4: Performance of pretrained models on\nAMBI ENT. Higher values are better for all metrics. A\nbaseline that reproduces the ambiguous sentence as its\ndisambiguation would achieve 0 EDIT-F1 and human-\njudged correctness; random performance for T/F accu-\nracy is 50% and for KL ranking accuracy is 32.8%.\n4.3 Modeling Interpretation-Specific\nContinuations\nFinally, we determine whether LMs, when con-\nditioned on an ambiguous sentence, implicitly\nmodel different interpretations in their distributions\nof text continuations. Since LMs are trained to\nmodel words given context, understanding ambi-\nguity should mean recognizing the union of the\ncontexts for a sentence’s interpretations.\nTo measure this, we obtain continuations for\neach interpretation, and quantify how “surprised”\nthe LM is to see them when conditioned on the\nambiguous sentence.8 Specifically, we first sample\n100 continuations c ∼ P(⋅ ∣ di) conditioned on\neach disambiguation di as context. Then, we com-\npare the likelihood of cunder the ambiguous sen-\ntence aversus the corresponding disambiguationdi\nby computing log P(c∣ di) −log P(c∣ a). This\ndescribes how much the LM “suffers” by seeing the\nambiguous instead of the unambiguous context,9\nand is an unbiased estimate of the KL divergence\nbetween P(⋅∣ di) and P(⋅∣ a) (proof in §C.3):\nD(P(⋅∣ di) ∣∣P(⋅∣ a))\n= lim\nN→∞\n1\nN\nN\n∑\nj=1\ncj ∼P(⋅∣di)\nlog P(cj ∣ di)\nP(cj ∣ a) .\nIntuitively, we want the KL divergence not to be too\nlarge — the LM should reasonably expect to see\ncontinuations for either interpretation. To quantify\nthis, we introduce a “distractor” sentence ˜dformed\nby replacing a randomly selected noun in awith a\nsame-category word from ConceptNet (Speer et al.,\n8We exclude ChatGPT and GPT-4 from evaluation as the\nAPI does not enable calculating likelihood under the model.\n9This method assumes that the likelihood of a continuation\nis based on its meaning alone, but surface-form attributes like\nstyle are a confounding factor. See further discussion in §C.3.\n2017), e.g., replacing “school” with “library.”\nWe expect the LM to model continuations from\nboth disambiguations di better than those from the\ndistractor ˜d, i.e., for all true disambiguations di,\nD(P(⋅∣ ˜d) ∣∣P(⋅∣ a))>D(P(⋅∣ di) ∣∣P(⋅∣ a)).\nWe call the fraction of ambiguous contexts for\nwhich this is true the KL ranking accuracy.\nResults The KL Rank. Acc. column of Ta-\nble 4 shows that FLAN-T5demonstrates the correct\npreference of continuations for 81.0% of exam-\nples, making it the best model here despite its poor\nperformance in other settings. The inconsistent\ntrends suggest that results are heavily dependent on\nhow competence on ambiguity is operationalized.\nNonetheless, ambiguity remains a severe challenge\nacross models and across the suite of tests.\n5 Evaluating Multilabel NLI Models\nGiven that language models still struggle to pro-\ncess ambiguity in §4, we next investigate the effec-\ntiveness of finetuning them on existing NLI data\ncollected in the line of work on underspecification\nand subjectivity in NLI.10 Here, we consider the\ndiscriminative task of multilabel NLI prediction,\nacross both ambiguous and unambiguous examples\nin AMBI ENT. Experimental details are in §D.\n5.1 Methods\nWe experiment with methods that predict a single\nprobability value, a distribution over labels, or a set\nof labels. We use the development set ofAMBI ENT\nto tune threshold(s) that map the output of these\nmodels onto a set of labels (see §D.1). All models\nare based onroberta-large, and we report results\nover 5 random seeds for model training.\nRegression models We train a regression model\non Uncertain NLI (UNLI; Chen et al., 2020) that\npredicts a value on [0,1] representing the probabil-\nity of the hypothesis being true given the premise.\nDistributional models Distributional models\naim to predict the distribution of annotator judg-\nments. We use two models from prior work: 1)\none trained on AmbiNLI (Meissner et al., 2021),\nwith examples with multiple annotations from\nSNLI (Bowman et al., 2015) and MNLI, and 2)\n10The size of AMBI ENT is not large enough for a train-\ning split; future efforts to annotate data in the fashion of\nAMBI ENT might be able to address this issue.\n796\nMethod and Train Set EM Macro F1 Group EMReg.Uncertain NLI (C+20)24.52.3 62.21.0 4.72.5\nDist.\nAmbiNLI (M+21) 21.01.6 63.80.8 10.12.5\nSNLI + MNLI (Z+22)24.31.1 68.00.1 4.71.2\nMulti.\nMulti-label MNLI (JD22)15.83.4 63.20.6 0.91.2\nMulti-label WANLI (L+22)35.13.0 72.50.3 19.14.8\nClassifier over sets WANLI 43.60.8 70.70.2 37.80.4\nTable 5: Performance of multilabel NLI models on\nAMBI ENT. While all model outputs are mapped onto a\nlabel set, their original output is one of regression (reg.),\ndistributional (dist.), or multilabel (multi.) output. EM\nand Macro F1 measure performance on the original\nexample; group EM considers performance on both the\noriginal example and its disambiguations. We report the\nmean and standard deviation over 5 seeds.\na model trained through distribution distillation\n(Zhou et al., 2022), where a teacher model trained\non SNLI + MNLI is used to re-annotate the data\nwith soft labels then used to train a new model.\nMultilabel models Prior work trained a multil-\nabel model (Jiang and de Marneffe, 2022) on the\ndevelopment sets of MNLI + ChaosNLI by turn-\ning distributional labels into discrete ones with a\nthreshold of 0.2. In addition, we train a multilabel\nmodel on WANLI’s train set (which has two anno-\ntations per example), as well as a classifier over\nsets which performs 7-way classification over the\npower set of NLI labels, minus the empty set.\n5.2 Metrics\nOn the original examples, we calculate the macro\nF1 score and the exact match accuracy (EM); the\nlatter requires the model to exactly predict the label\nset. We also report the group EM accuracy as the\nfraction of examples where the model exactly pre-\ndicts the label set for both the original NLI example\nand all of its disambiguations.\n5.3 Results\nAs shown in Table 5, the multilabel model trained\non WANLI achieves the highest macro F1 score of\n72.5%, and the classifier over sets achieves the best\nEM accuracy of 43.6% and group EM accuracy\nof 37.8%. While the EM accuracy is substantially\nhigher than the random-guessing baseline of 1/7\n= 14.3%, it is considerably short of 89.7%, the\nrate at which crowdworkers correctly predict the\nset of labels when presented with possible disam-\nbiguations (§3). Overall, finetuning NLI models on\nexisting data with label variation still leaves large\nroom for improvement on the multilabel NLI task.\n6 Case Study: Detecting Misleading\nPolitical Claims\nWe illustrate the value of ambiguity-sensitive mod-\nels via a case study in detecting misleading political\nclaims in the wild. Here, we use the key insight\nthat for ambiguous sentences, some paraphrases\nare naturally disambiguating, as paraphrases must\neither preserve the ambiguity or paraphrase a par-\nticular interpretation. Therefore, if we cast a given\nsentence as the premise and a paraphrase as the\nhypothesis, a multilabel NLI model predicting two\nor more labels should indicate the presence of am-\nbiguity. Moreover, the paraphrase resulting in this\nprediction should reveal the source of ambiguity.\nWe experimentally evaluate this idea on the de-\nvelopment set of CLAIM DECOMP (Chen et al.,\n2022), which contains 200 claims with their Poli-\ntiFact fact-checks. The authors read each instance\nand mark whether the fact-check describes an issue\nof ambiguity or factuality (regardless of whether\nwe perceive ambiguity ourselves). Then we para-\nphrase each claim 5 times with InstructGPTzero-\nshot, and apply the multilabel WANLI model\nfrom §5, which achieved the highest F1 score, on\neach resulting NLI example. A claim is considered\nambiguous if the model predicts more than one\nlabel for any paraphrase. Examples in Table 6.\nThis method recalls 88.8% of ambiguous claims.\nWhile precision is lower at 12.4%, qualitative in-\nspection of false positives reveals many ambigui-\nties that were left unmentioned in the fact-check,\nillustrating the potential of these tools to anticipate\nsources of misunderstanding. Ultimately, our anal-\nysis suggests that fact-checking as a more general\nproblem may need refinement, due to the possible\npresence of both true and false interpretations. This\ncase study shows only one use case of ambiguity-\nsensitive models, and we hope for AMBI ENT for\nbenchmark further progress on this front.\n7 Related Work\nAmbiguity Ambiguity is a longstanding and\nwell-studied issue for NLP tasks involving sym-\nbolic analyses of sentences, such as syntactic and\nsemantic parsing (Church and Patil, 1982; Koller\net al., 2008) or coreference resolution (Poesio and\nArtstein, 2005). However, as the field has recently\nshifted toward higher-level understanding and rea-\nsoning problems, ambiguity in language has been\nlargely overlooked.\nIn the space of open-domain question-answering,\n797\nPolitical claim (premise) Generated paraphrase (hypothesis) Rating Prediction Explanation of ambi-\nguity (ours)\nWhenPresident Obama was elected, the market\ncrashed...\nThe stock marketreactedimmediately to Pres-\nident Obama’s election in 2008, ...\nBarely\n-true\n/lbagENTAIL,\nNEUTRAL/rbag\nThe claim implies a\ncausal relationship\nRhode Island is \"almost dead last\"... in the\nlength of time first-degree murderers must spend\nin prison before they’re eligible for parole.\nRhode Island is one of the states... where mur-\nderers must spend thelongest time in prison\nbefore being eligible for parole.\nTrue\n/lbagENTAIL,\nNEUTRAL,\nCONTRADICT/rbag\n“dead last” may mean\nshortest or longest, de-\npending on stance\nDonald Trump even said,on his very first day\nin office, he would require every school in Amer-\nica to let people carry guns into our classrooms.\nDonald Trumpsaid on his first day in office\nthat every school in America would have to\nallow people to carry guns in classrooms.\nTrue /lbagENTAIL,\nNEUTRAL/rbag\n“on his first day” may\ndescribe either thesay-\ningor therequiring\nTable 6: Political claims flagged as ambiguous by our detection method. For the claim in the first row, the ambiguity\nwas noted by the fact checker ( Rating column), thus leading to a barely-true rating; in the bottom two, the\nambiguity was not mentioned, showing the value of this method for ambiguity detection.\nthere are often issues of ambiguous or under-\nspecified event and entity references (Min et al.,\n2020; Cole et al., 2023), leading to work on gen-\nerating clarification questions (Kuhn et al., 2023;\nKrasheninnikov et al., 2022). In particular, our ap-\nproach to ambiguity is inspired byAMBIG QA (Min\net al., 2020), where the task input is disambiguated\nin natural language to account for variation in pos-\nsible outputs. In contrast to open-domain ques-\ntions, AMBI ENT contains more diverse linguistic\nambiguities whose resolution is a prerequisite to\nunderstanding meaning.\nRecent work has also studied ambiguous lan-\nguage in multi-modal settings: Stengel-Eskin et al.\n(2023) collected a set of ambiguous questions about\nimages, and Pezzelle (2023) consider how vision-\nlanguage models handle underspecified captions.\nOther work studies whether the confidence of\ncoreference and NLI models is sensitive to ambigu-\nity in synthetically-constructed input (Yuan et al.,\n2023; Ban et al., 2022). Going beyond task-specific\nmodels, we evaluate pretrained LMs for the lan-\nguage skill of managing ambiguity.\nHuman label variation Human label variation\n(Plank, 2022) is a broad phenomenon with three dis-\ntinct sources, as summarized by Jiang and de Marn-\neffe (2022): task ambiguity, subjectivity of annota-\ntor attitudes, and input ambiguity (our focus). Ex-\nplored in contemporary work (Tamkin et al., 2023),\ntask ambiguity arises when the task is underspeci-\nfied with respect to the desired output; subjectivity\nis observed when different people disagree, such\nas for toxic language detection (Sap et al., 2022).\nThere is growing recognition of and interest in\nstudying this variation, where the dominant ap-\nproach is to model the distribution of human judg-\nments (Pavlick and Kwiatkowski, 2019; Nie et al.,\n2020; Uma et al., 2021), potentially as a function\nof their demographic characteristics (Gordon et al.,\n2022). In our work, we argue that when uncertainty\nis in the input, we should instead directly charac-\nterize the underlying reasons for the uncertainty.\nNLI beyond three-way classification For NLI,\nthe seminal work investigating label variation was\nPavlick and Kwiatkowski (2019), and subsequent\nwork collected more annotations (Nie et al., 2020)\nand modeled this variation (Zhou et al., 2022;\nZhang et al., 2021). Other approaches aim to pre-\ndict the probability of entailment (Chen et al., 2020;\nZhang et al., 2017) or a fourth “disagreement” la-\nbel (Zhang and de Marneffe, 2021). We contribute\nanother approach, where NLI models predict the\nset of labels for plausible readings.\nJiang and de Marneffe (2022) investigate MNLI\ndata to taxonomize sources of disagreement, and\nidentify “uncertainty in sentence meaning” as one\nsource, though they named only lexical and im-\nplicature ambiguities. Our benchmark includes a\nwider coverage of ambiguities and our analysis fur-\nther sheds light on the nature of the “disagreement.”\n8 Conclusion\nAmbiguity in language will become increasingly\nconspicuous as we push the limits of LM capabili-\nties and build tools that engage with the nuances of\nnatural language communication. We develop the\nfirst benchmark to evaluate whether language mod-\nels recognize different readings of ambiguous text,\nand demonstrate that the task remains extremely\nchallenging. We encourage future work to study the\nsensitivity of LMs to context and emphasis, investi-\ngate the presence of systematic biases in interpreta-\ntion, and explore the promising space of real-world\napplications enabled by ambiguity-sensitive tools.\n798\nAcknowledgments\nWe would like to thank Nathan Schneider, Ellie\nPavlick, Doug Downey, Ewin Tang, Roy Schwartz,\nYanai Elazar, Valentina Pyatkin, and Ari Holtzman,\nas well as the greater UW NLP and AI2 community,\nfor valuable feedback and discussion at different\nstages of this work. Our dataset would not have\nbeen possible without the expertise of our linguist\nannotators, which include Emma Miller, Sofia Y .\nAhmed, Wendy Kempsell Jacinto, Maxine Appel,\nEdi Xin, Magdelina Thornton, Huijae Seo, Gita\nDhungana, and Aldrich Gran Lapid, and 28 others.\nThis work was funded in part by the DARPA\nMCS program through NIWC Pacific (N66001-19-\n2-4031). We thank OpenAI for offering access to\nvarious models through the API. The first author\nis supported by the National Science Foundation\nGraduate Research Fellowship Program.\nLimitations\nIn this work we collect a broad-coverage dataset of\nambiguities, but the size and diversity are nonethe-\nless limited due to the data sources and the effort\nrequired for expert annotation. We thus encourage\nfuture work to collect more data in the format of\nAMBI ENT, especially for naturally-occurring am-\nbiguities. In addition, we only study ambiguity phe-\nnomena in English, but how ambiguity manifests in\nother languages can vary greatly due to systematic\ntypological factors or idiosyncratic differences. For\nexample, while AMBI ENT does not contain many\ninstances of morphological ambiguity, these are\nvery common in morphologically richer languages\nsuch as Turkish and Finnish. A systematic exten-\nsion of our dataset and analyses to other languages\nwould be exciting future work.\nThough LMs struggle across the board on our\nevaluations, this does not guarantee that they will\nnot handle ambiguity well in other task settings\nor using other extraction methods. We observe\nthat GPT-4is the highest-performing model on two\nof the three evaluations (§4.1, §4.2), while the\nsmallest FLAN-T5performs best on the last evalua-\ntion (§4.3). Scaling up general-purpose pretraining\nand reinforcement learning from human feedback\n(Ouyang et al., 2022) may lead to further gains,\nthough we hypothesize that the trend will be un-\nclear as larger LMs may overfit to more common\ninterpretations at the expense of recognizing less\ncommon ones, which is especially detrimental for\nreasoning about misleading language.\nEthics Statement\nWe acknowledge that text generated from language\nmodels is susceptible to perpetuating social harms\nand containing toxic language (Sheng et al., 2019;\nGehman et al., 2020). To address this, the anno-\ntators and validators of our dataset (§2.3) were\ninstructed to discard any examples that may be\nperceived as offensive. Nonetheless, it is possi-\nble that subtly harmful examples may have been\noverlooked and included in the final dataset.\nIn addition, we are cognizant of the asymmetri-\ncal relationship between requesters and workers in\ncrowdsourcing (§3). We took great care to pay fair\nwages, with a median hourly rate of $19.13, and\nwere responsive to feedback and questions through-\nout the process (see §B.1 for details). The only\npersonal information we collect is the worker IDs\nfrom Amazon Mechanical Turk, which we will not\nrelease. Both the linguist annotation (§2.3) and\ncrowdworker experiment (§3) received IRB exemp-\ntion.\nReferences\nPangbo Ban, Yifan Jiang, Tianran Liu, and Shane\nSteinert-Threlkeld. 2022. Testing pre-trained lan-\nguage models’ understanding of distributivity via\ncausal mediation analysis. In Proceedings of the\nFifth BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 314–324,\nAbu Dhabi, United Arab Emirates (Hybrid). Associa-\ntion for Computational Linguistics.\nBeata Beigman Klebanov and Eyal Beigman. 2009.\nFrom annotator agreement to noise models. Com-\nputational Linguistics, 35(4):495–503.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nAndrew Carnie. 2013. Syntax: A Generative Introduc-\ntion. Introducing Linguistics. Wiley.\nJifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg\nDurrett. 2022. Generating literal and implied sub-\nquestions to fact-check complex claims. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3495–3516,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nTongfei Chen, Zhengping Jiang, Adam Poliak, Keisuke\nSakaguchi, and Benjamin Van Durme. 2020. Un-\ncertain natural language inference. In Proceedings\n799\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 8772–8779, On-\nline. Association for Computational Linguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nKenneth Church and Ramesh Patil. 1982. Coping with\nsyntactic ambiguity or how to put the block in the\nbox on the table. American Journal of Computational\nLinguistics, 8(3-4):139–149.\nJeremy R. Cole, Michael J. Q. Zhang, Daniel Gillick, Ju-\nlian Martin Eisenschlos, Bhuwan Dhingra, and Jacob\nEisenstein. 2023. Selectively answering ambiguous\nquestions.\nAnn Copestake and Dan Flickinger. 2000. An open\nsource grammar development environment and broad-\ncoverage English grammar using HPSG. In Proceed-\nings of the Second International Conference on Lan-\nguage Resources and Evaluation (LREC’00), Athens,\nGreece. European Language Resources Association\n(ELRA).\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nMitchell L. Gordon, Michelle S. Lam, Joon Sung Park,\nKayur Patel, Jeff Hancock, Tatsunori Hashimoto, and\nMichael S. Bernstein. 2022. Jury learning: Integrat-\ning dissenting voices into machine learning models.\nIn Proceedings of the 2022 CHI Conference on Hu-\nman Factors in Computing Systems, CHI ’22, New\nYork, NY , USA. Association for Computing Machin-\nery.\nBarbara J. Grosz. 1977. The Representation and Use of\nFocus in Dialogue Understanding. Ph.D. thesis.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nPaloma Jeretic, Alex Warstadt, Suvrat Bhooshan, and\nAdina Williams. 2020. Are natural language infer-\nence models IMPPRESsive? Learning IMPlicature\nand PRESupposition. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8690–8705, Online. Association\nfor Computational Linguistics.\nNan-Jiang Jiang and Marie-Catherine de Marneffe.\n2022. Investigating Reasons for Disagreement in\nNatural Language Inference. Transactions of the\nAssociation for Computational Linguistics, 10:1357–\n1374.\nKate Kearns. 2000. Semantics. St. Martin’s Press.\nAlexander Koller, Michaela Regneri, and Stefan Thater.\n2008. Regular tree grammars as a formalism for\nscope underspecification. In Proceedings of ACL-08:\nHLT, pages 218–226, Columbus, Ohio. Association\nfor Computational Linguistics.\nDmitrii Krasheninnikov, Egor Krasheninnikov, and\nDavid Krueger. 2022. Assistance with large language\nmodels. In Proceedings of the ML Safety Workshop\nat NeurIPS.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nClam: Selective clarification for ambiguous ques-\ntions with generative language models. In Proceed-\nings of the Workshop on Challenges in Deployable\nGenerative AI at International Conference on Ma-\nchine Learning (ICML).\nMina Lee, Percy Liang, and Qian Yang. 2022. Coauthor:\nDesigning a human-AI collaborative writing dataset\nfor exploring language model capabilities. In CHI\nConference on Human Factors in Computing Systems,\nNew Orleans, LA, USA.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. W ANLI: Worker and AI collabora-\ntion for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 6826–6847, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJohannes Mario Meissner, Napat Thumwanit, Saku Sug-\nawara, and Akiko Aizawa. 2021. Embracing ambi-\nguity: Shifting the training target of NLI models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 862–869,\nOnline. Association for Computational Linguistics.\nMeta. 2022. No language left behind: Scaling human-\ncentered machine translation.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783–\n5797, Online. Association for Computational Lin-\nguistics.\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What\ncan we learn from collective human opinions on nat-\nural language inference data? In Proceedings of the\n800\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9131–9143,\nOnline. Association for Computational Linguistics.\nOpenAI. 2022. Introducing ChatGPT.\nOpenAI. 2023. GPT-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nEllie Pavlick and Tom Kwiatkowski. 2019. Inherent\ndisagreements in human textual inferences. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:677–694.\nSandro Pezzelle. 2023. Dealing with semantic under-\nspecification in multimodal NLP. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12098–12112, Toronto, Canada. Association\nfor Computational Linguistics.\nSteven T. Piantadosi, Harry Tily, and Edward Gibson.\n2012. The communicative function of ambiguity in\nlanguage. Cognition, 122(3):280–291.\nBarbara Plank. 2022. The “problem” of human label\nvariation: On ground truth in data, modeling and\nevaluation. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10671–10682, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nMassimo Poesio and Ron Artstein. 2005. The reliability\nof anaphoric annotation, reconsidered: Taking ambi-\nguity into account. In Proceedings of the Workshop\non Frontiers in Corpus Annotations II: Pie in the Sky,\npages 76–83, Ann Arbor, Michigan. Association for\nComputational Linguistics.\nMaarten Sap, Swabha Swayamdipta, Laura Vianna,\nXuhui Zhou, Yejin Choi, and Noah A. Smith. 2022.\nAnnotators with attitudes: How annotator beliefs\nand identities bias toxic language detection. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5884–5906, Seattle, United States. Association for\nComputational Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3407–\n3412, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, William Ngan, Spencer Poff, Naman Goyal,\nArthur Szlam, Y-Lan Boureau, Melanie Kambadur,\nand Jason Weston. 2022. Blenderbot 3: a deployed\nconversational agent that continually learns to respon-\nsibly engage.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence , page\n4444–4451. AAAI Press.\nElias Stengel-Eskin, Jimena Guallar-Blasco, Yi Zhou,\nand Benjamin Van Durme. 2023. Why did the\nchicken cross the road? Rephrasing and analyzing\nambiguous questions in VQA. In Proceedings of the\n61th Annual Meeting of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics.\nAlex Tamkin, Kunal Handa, Avash Shrestha, and Noah\nGoodman. 2023. Task ambiguity in humans and\nlanguage models. In The Eleventh International Con-\nference on Learning Representations.\nLewis Thomas. 1974. The lives of a cell. Notes of a\nbiology watcher, New york (The Viking Press) 1974.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand efficient foundation language models.\nAlexandra Uma, Tommaso Fornaciari, Anca Dumi-\ntrache, Tristan Miller, Jon Chamberlain, Barbara\nPlank, Edwin Simpson, and Massimo Poesio. 2021.\nSemEval-2021 task 12: Learning with disagreements.\nIn Proceedings of the 15th International Workshop\non Semantic Evaluation (SemEval-2021), pages 338–\n347, Online. Association for Computational Linguis-\ntics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\n801\nYuewei Yuan, Chaitanya Malaviya, and Mark Yatskar.\n2023. AmbiCoref: Evaluating human and model sen-\nsitivity to ambiguous coreference. In Findings of the\nAssociation for Computational Linguistics: EACL\n2023, pages 1023–1030, Dubrovnik, Croatia. Associ-\nation for Computational Linguistics.\nSheng Zhang, Rachel Rudinger, Kevin Duh, and Ben-\njamin Van Durme. 2017. Ordinal common-sense\ninference. Transactions of the Association for Com-\nputational Linguistics, 5:379–395.\nShujian Zhang, Chengyue Gong, and Eunsol Choi. 2021.\nLearning with different amounts of annotation: From\nzero to many labels. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 7620–7632, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nXinliang Frederick Zhang and Marie-Catherine\nde Marneffe. 2021. Identifying inherent disagree-\nment in natural language inference. In Proceedings\nof the 2021 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4908–4915, Online. Association for Computational\nLinguistics.\nXiang Zhou, Yixin Nie, and Mohit Bansal. 2022. Dis-\ntributed NLI: Learning to predict human opinion dis-\ntributions for language reasoning. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 972–987, Dublin, Ireland. Association\nfor Computational Linguistics.\nGeorge Kingsley Zipf. 1949. Human behavior and the\nprinciple of least effort.\nA Dataset Creation Details\nA.1 Curated Examples\nThe first author skimmed through several exist-\ning NLI datasets and manually identified examples\nthat were both natural and contained salient ambi-\nguities. Only a few examples were chosen from\neach dataset, to avoid overly redundant examples\nin AMBI ENT. In Table 7, we show an example\nfrom each of the sources we drew from. They are\ndirectly annotated with the set of labels and disam-\nbiguations by the first author.\nA.2 Generated Examples\nThe template for prompting GPT-3 to gener-\nate unlabeled NLI examples is shown in Ta-\nble 8. The model we used is InstructGPT\n(text-davinci-002), queried on September 4,\n2022, with top p = 0.9 (Holtzman et al., 2020),\nmax tokens 120, and stop sequence “ \\n\\n”. If\nthe generated output is not correctly formatted with\n“\\nSentence 2:” in the sequence (which separates\nthe premise and hypothesis), we discard the output.\nWe sample 5 outputs per 21,273 possible prompts\nto obtain a total of 104,071 unlabeled examples.\nWe first employ simple heuristics to discard ex-\namples exhibiting observable failure cases. That\nis, we discard examples if 1) either the premise\nor hypothesis is shorter than 5 characters, 2) the\npremise and hypothesis are identical, 3) the gener-\nated example is copied from an in-context example,\nor 4) the examples contain some redundant patterns\nobserved in the development phase. For instance,\nthere are an abundance of generations with the ex-\nact premise, “Mary wants to try a little bit of every\ncountry’s food on her trip around the world.” Af-\nter filtering based on these rules, 77,564 examples\nremain.\nNext, to further filter for likely-ambiguous in-\nstances, we use a multilabel RoBERTa-large model\ntrained on WANLI and retain all examples where\nthe model assigns probability ≥0.05 to more than\none NLI label, indicating at least slight uncertainty\nin whether there can be multiple possible readings.\nFinally, to approximately balance the resulting ex-\namples (of course, exactly balancing would be im-\npossible without gold labels), we keep an equal\nnumber of examples where the multilabel model\npredicts (according to the low threshold of 0.05)\n/lbagENTAIL,NEUTRAL/rbagand /lbagCONTRADICT,NEUTRAL/rbag,\nand all other examples with multiple labels pre-\ndicted. Thus, the final set of generated examples is\n16,826.\nA.3 Linguist annotation\nOf the generated examples, we ultimately annotate\nonly 2,616 of them. This is due to the slow pace of\nexpert annotation as the project went on, and the\ndiminishing returns of annotating more data. Each\nexample was annotated by two linguists. We dis-\ncard an example if either linguist chose to discard\nit. The final set of examples is 2,020.\nOur expert annotators were 37 university-level\nlinguistics students at the University of Washing-\nton, recruited through a Linguistics mailing list.\nThey were paid $20/hour, in addition to $0.05 per\nexample.\nA.4 Validation by authors\nThe authors review all 2,020 examples, each with\ntwo annotations, combining and revising them into\na single coherent annotation, or optionally discard-\ning the example. The authors validated the ex-\n802\nExample Disambiguation 1 Disambiguation 2 Source\nP: It is the only possibility ofmaking the\nlaw the servant of the people, not the other\nway around.\nH: The law should be the servant of the\npeople, not the other way around.\n/lbagENTAIL,NEUTRAL/rbag\nP: ... of making the law the ser-\nvant of the people,as it should\nbe, ...\nENTAIL\nP: It is the only possibility\nthat would lead tothe law...\nNEUTRAL\nWANLI test\nP: Then he sobered.\nH: He was drunk.\n/lbagENTAIL,NEUTRAL/rbag\nP: Then he sobered after\ndrinking alcohol.\nENTAIL\nP: Then he became more\nsensible.\nNEUTRAL\nMNLI dev\nP: Patrick did not manage to leave.\nH: Patrick tried to leave.\n/lbagENTAIL,NEUTRAL/rbag\nP: ..., despite his attempt.\nENTAIL\nP: ... , whether or not he tried.\nNEUTRAL IMPPRES\nP: LaBeouf had tried to bum a smoke from\ntwo strangers, unaware that one of them\nwas a police officer.\nH: LaBeouf had tried to bum a smoke\nfrom a police officer.\n/lbagENTAIL,CONTRADICTION/rbag\nH: LaBeouf hadtried to find a\npolice officerto bum a smoke\nfrom.\nENTAIL\nH: LeBeouf had tried to bum\na smoke from someonewho\nhappened to bea police officer.\nCONTRADICTION\nNLI\nDiagnostics\nP: Jenny and Zoe solved the puzzle.\nH: They solved it together.\n/lbagENTAIL,CONTRADICTION/rbag\nP: ... solved the puzzle together.\nENTAIL\nP: ... eachsolved the puzzle.\nCONTRADICTION DistNLI\nP: John opened the door again.\nH: John opened the door before.\n/lbagENTAIL,NEUTRAL/rbag\nP: John opened the door before,\nand did it again.\nENTAIL\nP: The door was open before,\nand John opened the door again.\nNEUTRAL\nCarnie\nP: John wishes to marry Adrienne, a\nFrenchwoman.\nH: John wants to marry a Frenchwoman.\n/lbagENTAIL,NEUTRAL/rbag\nP: John wants to marrya certain\nwoman who is French.\nENTAIL\nP: Johnwants for his future wife\nto be French.\nNEUTRAL\nKearns\nP: You should visit Norwayin the summer.\nH: Summer is a good season to visit Nor-\nway.\n/lbagENTAIL,NEUTRAL/rbag\nP: You should visit Norwaythe\ncoming summer.\nENTAIL\nP: You should visit Norwayin\nthe summer season.\nNEUTRAL\nHandwritten\nTable 7: An example in AMBI ENT from each of the sources we draw from for the curated examples (§2.1).\namples together on Zoom calls over the course of\nseveral weeks, actively discussing examples that\nthey were unsure about and developing consistent\nstandards. For instance, we chose to discard exam-\nples that boiled down to temporal ordering (e.g.,\n“I didn’t realize that I left my keys at home” either\nentails or contradicts “I realized I left my keys at\nhome”, depending on the ordering of the sentences)\nor vagueness (e.g., “He is six feet tall” may or may\nnot entail “He is tall” due to the vagueness of the\nword “tall”). This process revealed to us the extent\nof task underspecification in NLI (something we\ndo not directly study in this work), and allowed us\nto focus on linguistic ambiguity.\nUltimately, 1,503 examples emerge from this\nphase.\nA.5 Additional statistics\nThe disambiguating rewrites are, on average, 2.36\nwords longer than their ambiguous counterparts.\nAmong the ambiguous examples, 74.3% have am-\nbiguity in the premise and 32.6% in the hypothesis,\nwith 6.9% having ambiguity in both. 97.5% of\nambiguous sentences are labeled with two disam-\nbiguating rewrites, with the rest having three or\nmore.\nA.6 Ambiguity category annotation\nThe authors construct a taxonomy of ambiguity\ntypes by reviewing AMBI ENT examples and cate-\ngorizing possible sources of ambiguity, described\nin Table 9.\nThen two of the authors annotate 100 randomly\n803\nWrite pairs of sentences that are related to each other in the\nsame way.\nSentence 1:In the past,I have been of the opinion that a\nfree market economy is a superior economic system.\nSentence 2:I have changed my mind and now believe that\na planned economy is superior.\nSentence 1:I would like togo to the circus.\nSentence 2:I have never been to the circus.\nSentence 1:For a long time,this concept of “collective\nresponsibility” was more important than the need to protect\nthe individual.\nSentence 2:This concept of “collective responsibility” is\nno longer important.\nSentence 1:When I was young,I was obsessed with the\nsupernatural.\nSentence 2:I am not obsessed with the supernatural any-\nmore.\nSentence 1:\nTable 8: Prompt template for GPT-3 used to create un-\nlabeled examples for annotation, formatted with an ac-\ntual set of in-context examples. In-context examples\nare from WANLI and found automatically via nearest\nneighbors in [CLS] token embedding space of an NLI\nmodel finetuned on WANLI . All the examples demon-\nstrate a shared ambiguity pattern where sentences about\nthe past or desires about the future induce a cancellable\nimplicature about the present. For instance, When I was\nyoung, I was obsessed with the supernatural implies\nthat “I” am no longer obsessed, and I would like to go\nto the circus might be taken to imply (more tenuously)\nthat “I” have not been before.\nsampled examples from AMBI ENT for the ambi-\nguity type. Each ambiguity is labeled with one\ncategory; examples may have multiple categories\nwhen they contain multiple ambiguities (e.g., both\npremise and hypothesis are ambiguous, or one sen-\ntence has multiple ambiguous parts). When multi-\nple categories are plausible for a single ambiguity\n(e.g., a word is lexically ambiguous but pragmatics\nencourages the reading of one over the other), we\nchoose the first one in the order of the table (here,\nlexical).\nNote that the distribution of ambiguity in\nAMBI ENT does not necessarily reflect that of\nnaturally-occurring ambiguity.\nB Crowdworker experiment details\nB.1 The crowdworkers\nTo qualify workers, we designed a qualification test\nwith 5 questions that paid $5.00, open to the 64\nannotators who revised and labeled NLI examples\nfor the creation of WANLI. Of the 43 workers tak-\ning the test, 34 passed, though only 29 participated\nCategory Description\nLexical A lexical item has different senses\nSyntactic Different syntactic parses lead to different interpretations\nFigurative Literal and figurative readings are present\nPragmatic Literal and pragmatic interpretations are present\nScopal Ambiguity from the relative scopal order of quantifiers\nOR the scope of particular modifiers\nCoreferential Ambiguous coreference\nOther Ambiguity that does not fall into the above categories\nTable 9: Ambiguity categories.\nin the actual project. Through a poll taken after\nthe annotation phase was completed, we find that\nall but one of the participants spoke English as a\nnative language.\nFor the remainder of the study, crowdworkers\nwere paid $0.40 per NLI example, which involved\nlabeling the original ambiguous example, assessing\nthe plausibility of three interpretations, and finally\nlabeling three (closely related) NLI examples. At\nthe end of data collection, we aggregate the earning\nand time spent from each crowdworker, and find\nthat the median hourly rate was $19.13.\nB.2 Setup details\nTo create a “distractor” sentence among the true dis-\nambiguations, we use back-translation with Yorùbá\nby employing the NLLB model (Meta, 2022) with\ngreedy decoding for both Eng→Yor and Yor→Eng.\nIn case the generated distractor was an exact\ncopy of the original ambiguous sentence, we repeat\nthe Yor→Eng leg of backtranslation with multino-\nmial beam search, with a beam size of 5.0, top\np = 1.0, and temperature t = 2.0. Of the 5 se-\nquences returned, we randomly choose a sequence\nthat is distinct from the original source sentence.\nFor instance, “ It is currently March, and they\nplan to have their wedding scheduled for next De-\ncember” is back-translated to “It is March, and they\nare to be married in December,” which is a faithful\nthough somewhat lossy paraphrase, and 8/9 crowd-\nworkers consider this a possible interpretation. On\nthe other hand, “There will be more interesting sem-\ninars next quarter” is back-translated to “There will\nbe many more exciting conventions in the next half,”\nwhich is not a faithful paraphrase and considered a\npossible interpretation by 1/9 workers.\nC LM Experiment details and discussion\nC.1 Generating Disambiguations\nFor the test in §4.1, there is a different template\nfor when the premise is ambiguous and when the\n804\nhypothesis is ambiguous. For simplicity, we ex-\nclude the 6.9% of examples where both the premise\nand hypothesis are ambiguous. The former tem-\nplate is shown in Table 2; the latter contains only\nminor modifications. The instruction is “ In each\nexample, you will be given some context and a\nclaim. Unfortunately, the claim has some ambi-\nguity that affects whether it is correct. Enumerate\ntwo or three interpretations of the claim that lead\nto different judgments about its correctness.” Then,\nimmediately following the statement of the context\nand claim, “We don’t know, because theclaim can\nbe interpreted in many different ways:”.\nEDIT-F1 The EDIT-F1 metric represents a dis-\nambiguation by its added and deleted unigrams,\nand computes the F1 score between the refer-\nence and the prediction. For instance, the am-\nbiguous sentence “ We’re afraid that LMs aren’t\nmodeling ambiguity” can be disambiguated with\nedits /lbag-afraid , +worried /rbag. Predicted edits\n/lbag-modeling , +representing /rbagwould receive\nan EDIT-F1 of zero, whereas sentence-similarity\nmetrics like BLEU would give undue credit for\nthe high overlap between preserved portions of the\nambiguous sentence.\nAnalysis One strategy for attempting disam-\nbiguation we observe across model classes is restat-\ning the ambiguous sentence with additional con-\ntext that directly affirms or negates the hypothesis,\nrather than making a targeted revision to clarify the\nambiguity. In some cases, this “shortcut” does lead\nto technically correct disambiguations (and marked\nas such in human evaluation). For instance, for\nP: He always ignores his mother’s advice\nto follow his own dreams.\nH: He follows his dreams.\nChatGPT disambiguates the premise by restating it,\nfollowed by “and therefore does follow his dreams”\nversus “and therefore does not follow his dreams.”\nThe former forces the interpretation that he ignores\nher advice in order to follow his dreams; the latter\nthe interpretation that his mother’s advice is for\nhim to follow his dreams. Thus, the human-judged\ncorrectness may overestimate the models’ ability\nto precisely report the source of ambiguity.\nC.2 Recognizing Disambiguations\nFor the test in §4.2, accuracy on each template is\nshown in Table 10.\n1 2 3 4 Avg\nFLAN-T5 (xxl) 85.9 28.2 100.0 11.6 56.4\nLLaMa (65B) 96.1 92.1 11.8 19.9 55.0\nGPT-3 (davinci) 46.2 69.0 45.0 71.1 57.8\nInstructGPT (-003)71.9 18.1 81.0 27.5 49.6\nChatGPT 81.5 51.7 74.5 23.4 57.7\nGPT-4 91.6 68.8 81.8 9.9 63.0\nTable 10: Accuracy of LMs on the four templates from\nthe True/False evaluation in §4.2. The Avg. column is\nthe one reported in the T/F Acc. column of Table 4.\nC.3 Recognizing Interpretation-Specific\nContinuations\nThis section includes implementation details and\ndiscussion for the test in §4.3.\nKL divergence For a given disambiguation di,\nlet X be a random variable equal to\nxc =log P(c∣ di)\nP(c∣ a) with prob. pc =P(c∣ di)\nIn §4.3, we calculate the mean over Xj, indepen-\ndent and identically distributed copies of X:\n¯Xn = 1\nN\nN\n∑\nj=1\nXj\nFirst we show that X is an unbiased estimator for\nthe KL divergence.\nE[ ¯Xn] =E[X]\n= ∑\nc∈X\npcxc\n= ∑\nc∈X\nP(c∣ di)log P(c∣ di)\nP(c∣ a)\n=D(P(⋅∣ di) ∣∣P(⋅∣ a))\nwhere the first step follows from the linearity of\nexpectation.\nAnd from the law of large numbers, we observe\nthat ¯Xn tends to the KL divergence in the limit.\nlim\nn→∞\n¯Xn =E[X] =D(P(⋅∣ di) ∣∣P(⋅∣ a))\nPrepending a stem We append one of two stems\nto the beginning of the disambiguation (or distrac-\ntor), for both generating continuations and measur-\ning the likelihood of generated continuations. For\ninstruction-tuned models, we append the prompt\n“Write a story.\n ,” so that generating on-\ntopic continuations is consistent with its instruction-\nfollowing objective. For vanilla LMs, we append\n805\na start quotation mark “, which we find leads to\nsignificantly more topical continuations; otherwise,\nmodels may generate a newline\n and proceed to\na new topic.\nCreating the distractor To create the distractor\nfor an ambiguous sentence, we tokenize the sen-\ntence using spacy and randomly select a word w\nwith the tag NOUN or PROPN (proper noun). Then\nwe find the category node cwhere whas the IsA\nrelation to c, i.e., w → c, with the largest weight.\nFinally, we randomly sample a same-category node\nw′ ≠ w, representing a single word, such that\nw′ → c.\nSometimes this replacement is not viable, e.g.,\nwhen there are no nouns in the sentence, the noun\nis not in ConceptNet, or there are no same-category\nwords. In this case, we next attempt to replace\na pronoun with another heuristically-determined\npronoun; failing all else, we randomly replace any\nnoun or pronoun with the word “corgi.”\nGenerating continuations Given either a true\ndisambiguation or distractor as context, we gener-\nate continuations by sampling 100 single-sentence\ncontinuations from the full probability distribution,\ni.e., with top p=1.0. To obtain a single sentence,\nwe stop generation when a sentence-ending punc-\ntuation mark (one of !, ?, and .) is generated, and\nappend a period back.\nLimitations Finally we discuss some limitations\nwe observed with this test. First, the likelihood of\na continuation conditioned on context depends not\nonly on themeaning of the context, but alsosurface-\nform attributes like the style and tone, which is a\nconfounding factor in this experiment. Indeed, we\nobserve that there can be a stylistic mismatch be-\ntween original ambiguous sentence and its disam-\nbiguation, often with the latter being more stinted\nand formal. Generated continuations thus match\nthe formal style, and have lower likelihood under\nthe ambiguous sentence than a semantically equiv-\nalent, more casual paraphrase.\nIn addition, the “closeness” of the distractor af-\nfects how easy or challenging the test is. We find\nthat in most cases, the noun replacement procedure\ncreates a sentence which we would expect to have\na substantially different set of plausible continua-\ntions, potentially leading the test to be too “easy”.\nYet this varies with the noun being replaced, the\nreplacement chosen, as well as the overall sentence\nin which it appears. Nonetheless, we require the\ndistractor for this test in order to make a judgment\nabout the performance of the model.\nD Multilabel Model Experiments\nD.1 Methods\nHere we describe the setup of NLI models that pre-\ndict multiple labels as output (§5.1). Multilabel\nmodels train separate binary classifier heads for\neach label on top of the transformer output. Dur-\ning inference, the labels are independently selected\nbased on a threshold (shared across labels) tuned on\nthe development set to maximize F1. Regression\nmodels train a regressor into [0,1] that represents\nthe probability of hypothesis being true given the\npremise. The development set is used to select a\nmapping from each NLI label into a continuous\nsub-range, and at inference time we pick all la-\nbels whose ranges overlap with the regressed value.\nClassifier over sets is a seven-way classifier over\nthe power set of NLI labels minus the empty set.\nAs it directly predicts a set of labels, this model\nrequires no threshold tuning.\nThe median thresholds across 5 seeds from our\nexperiments are shown in Table 11.\nD.2 Training Details\nFor models from prior work, we replicate the train-\ning details to the best of our ability. All models are\nbased on roberta-large.\nThe UNLI model (Chen et al., 2020) is trained\non SNLI’s training set (heuristically mapped to\nregression labels) for 1 epoch, then trained on u-\nSNLI (human-annotated with regression labels) for\n3 epochs.\nThe AmbiNLI model (Meissner et al., 2021) is\nfirst pretrained on single-label data from SNLI +\nMNLI for 3 epochs, then further finetuned on Am-\nbiNLI for 2 epochs. AmbiNLI examples have dis-\ntributional outputs, and is sourced from the devel-\nopment set of SNLI and MNLI (which contain 5\nlabels) and train set of UNLI (which are heuristi-\ncally mapped to soft labels).\nThe Distribution Distillation model (Zhou et al.,\n2022) is trained for 2 epochs on SNLI + MNLI\ntraining examples that are re-annotated with the\ndistributional output of a teacher model. The\nteacher model is a traditional three-way classifi-\ncation model trained on SNLI + MNLI.\nFinally, the multilabel model from Jiang and\nde Marneffe (2022) is trained on the development\n806\nModel Thresholds\nReg.Uncertain NLI (C+20)\nE:(0.69,1.0)\nN:(0.01,1.0)\nC:(0.03,0.71)\nDist.\nAmbiNLI (M+21) −3.43\nDist. Distillation (Z+22) −1.55\nClass.\nMNLI (M+18) −2.68\nWANLI (L+22) −1.19\nMulti.\nMulti-label MNLI (M+18) −2.78\nMulti-label WANLI −1.97\nSet classifier on WANLI N/A\nTable 11: Logit thresholds used to map the output of var-\nious models to a set of labels, for multilabel prediction\nexperiments (§5). The way these thresholds are obtained\nand used at inference-time is explained in §D.1.\nset of MNLI and ChaosNLI, where a label is con-\nsidered present if 20% of annotators choose the\nlabel. The model with the lowest loss on held-out\ndata over 30 epochs is selected as the final model.\nE Political Claims Case Study\nTo paraphrase each political claim, we use\nInstructGPT (text-davinci-003) zero-shot\nwith the simple prompt “ Paraphrase the\ntext.\n {Claim}\n Paraphrase:”, and decode\nwith top p = 0.9, to encourage both correctness\nand diversity among generated paraphrases.\n807"
}