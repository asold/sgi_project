{
  "title": "Syntax-guided Contrastive Learning for Pre-trained Language Model",
  "url": "https://openalex.org/W4285290042",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100328838",
      "name": "Shuai Zhang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100651061",
      "name": "Lijie Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5112673776",
      "name": "Xinyan Xiao",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5112985399",
      "name": "Hua Wu",
      "affiliations": [
        "Baidu (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3035352537",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2962891712",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3173854146",
    "https://openalex.org/W3035589854",
    "https://openalex.org/W3153543512",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W1508977358",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2971351900",
    "https://openalex.org/W3174413662",
    "https://openalex.org/W4287280906",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W3004117589",
    "https://openalex.org/W2963391817",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2084238990",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W3172399575",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3008282111",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2998665041",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2132481658",
    "https://openalex.org/W2918996109",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3035625205",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W4287812705",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W2124725212"
  ],
  "abstract": "Syntactic information has been proved to be useful for transformer-based pre-trained language models. Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks. This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios. In order to inject syntactic knowledge effectively and efficiently into pre-trained language models, we propose a novel syntax-guided contrastive learning method which does not change the transformer architecture. Based on constituency and dependency structures of syntax trees, we design phrase-guided and tree-guided contrastive objectives, and optimize them in the pre-training stage, so as to help the pre-trained language model to capture rich syntactic knowledge in its representations. Experimental results show that our contrastive method achieves consistent improvements in a variety of tasks, including grammatical error detection, entity tasks, structural probing and GLUE. Detailed analysis further verifies that the improvements come from the utilization of syntactic information, and the learned attention weights are more explainable in terms of linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2430 - 2440\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nSyntax-guided Contrastive Learning for Pre-trained Language Model\nShuai Zhang, Lijie Wang, Xinyan Xiao, Hua Wu\nBaidu Inc, Beijing, China\n{zhangshuai28, wanglijie, xiaoxinyan, wu_hua}@baidu.com\nAbstract\nSyntactic information has been proved to be\nuseful for transformer-based pre-trained lan-\nguage models. Previous studies often rely on\nadditional syntax-guided attention components\nto enhance the transformer, which require more\nparameters and additional syntactic parsing in\ndownstream tasks. This increase in complex-\nity severely limits the application of syntax-\nenhanced language model in a wide range of\nscenarios. In order to inject syntactic knowl-\nedge effectively and efficiently into pre-trained\nlanguage models, we propose a novel syntax-\nguided contrastive learning method which does\nnot change the transformer architecture. Based\non constituency and dependency structures of\nsyntax trees, we design phrase-guided and tree-\nguided contrastive objectives, and optimize\nthem in the pre-training stage, so as to help the\npre-trained language model to capture rich syn-\ntactic knowledge in its representations. Experi-\nmental results show that our contrastive method\nachieves consistent improvements in a variety\nof tasks, including grammatical error detection,\nentity tasks, structural probing and GLUE. De-\ntailed analysis further verifies that the improve-\nments come from the utilization of syntactic\ninformation, and the learned attention weights\nare more explainable in terms of linguistics.\n1 Introduction\nPre-trained transformer-based neural language\nmodels (LMs), such as BERT (Devlin et al., 2019)\nand RoBERTa (Liu et al., 2019), have achieved re-\nmarkable results in a variety of NLP tasks (Wang\net al., 2018). However, many studies have found\nthat these LMs do not encode enough syntactic\nknowledge in their learned representations (Wang\net al., 2019; Min et al., 2020; Wang et al., 2020). As\nit is widely acknowledged that structural informa-\ntion is very important for NLP (Strubell et al., 2018;\nNguyen et al., 2019; Zhang et al., 2020), there is an\nincreasing interest in improving pre-trained LMs\nby using syntactic information.\nMost of these works enhance pre-trained LMs\nby adding syntax-driven attention components to\nthe transformer (Li et al., 2020b; Xu et al., 2020;\nBai et al., 2021). They use the added components\nto produce a syntax-aware representation, and in-\nject this additional representation into the original\none from the vanilla transformer, so as to get a\nfinal syntax-enhanced representation. Although\nthese works did bring improvements, the additional\nsyntax-aware layers obviously increase application\ninconvenience and computation complexity, as they\nneed to parse the input text during testing and re-\nquire more neural parameters. Moreover, the per-\nformance of such explicit method depends on the\nparsing quality of test data (Sachan et al., 2020).\nThere are also some efforts on incorporating syntax-\nrelated objectives into the pre-training stage, such\nas syntax head prediction (Wang et al., 2020) and\ndependency distance prediction (Xu et al., 2020).\nHowever, these predictive pre-training tasks often\nfail to improve performance alone and need to work\ntogether with the additional attention components\n(Xu et al., 2020). Overall, it is still an open chal-\nlenge to effectively and efficiently incorporate syn-\ntactic information into pre-trained LMs.\nIn order to address the above problems, we pro-\npose Syntax-guided Contrastive Language Model\n(SynCLM). Based on contrastive learning, Syn-\nCLM uses syntactic information to create con-\ntrastive positive and negative examples, and uses\nthem to help the pre-trained LM to learn rich syntac-\ntic knowledge through contrastive learning method.\nSynCLM only adds contrastive objectives in the\npre-training stage, ensuring an effective and effi-\ncient utilization of syntax.\nSpecifically, based on constituent and depen-\ndency structures of syntax trees, we proposephrase-\nguided and tree-guided contrastive objectives for\npre-training, as shown in Figure 1. The constituent\nstructure represents the grouping of words into\nphrases within an input according to constituency\n2430\nFigure 1: Overview of our pre-training framework. P and Ni represent the positive sample and the i-th negative\nsample, respectively. The phrase-guided contrastive objective is based on the constituent structure of inputs, focusing\non using local syntactic information to guide the learning of attention distributions. The tree-guided objective is\nbased on the dependency structure, using global syntactic information to enhance the hidden representations.\ngrammar (Ford and Fox, 2002). Inspired by recent\nstudies (Mareˇcek and Rosa, 2019; Kim et al., 2020)\nwhich prove that LM’s attention heads exhibit syn-\ntactic structure akin to constituency grammar, in\norder to better recognize phrases from attentions,\nwe propose the phrase-guided contrastive objective\nto enhance attention learning by maximizing the\nsimilarity of attention distributions between words\nin the same phrase. The dependency structure fur-\nther encodes the binary head-dependent relations\nbetween words, and the root node aggregates the\nsemantic information of the whole structure from\nall its descendant words. To make the root node\nattend to its descendant nodes, we propose the tree-\nguided contrastive objective to enhance word rep-\nresentations by maximizing the similarity between\nthe representation obtained from all tokens and that\nobtained from syntactically related tokens. The two\ncontrastive objectives are jointly optimized during\npre-training, so as to inject syntactic knowledge\ninto pre-trained LMs. In summary, our contribu-\ntions are as following:\n• We are the first to leverage the contrastive learn-\ning method to incorporate syntactic information\ninto the pre-training stage. Our models can be di-\nrectly applied to downstream tasks without intro-\nducing additional parameters and syntax parsing\nof inputs. In addition, our method is applicable to\nany arbitrary transformer-based pre-trained LM.\nOur code1 will be released.\n• Based on the constituency and dependency struc-\nture, we design two novel syntax-guided learning\nobjectives to enhance the learning of attention\nweight distributions and hidden representations\nin the transformer.\n• Extensive experiments show that our SynCLM\nachieves consistent improvements on tasks that\nare often used in related works, including gram-\nmatical error detection, entity-related tasks, struc-\ntural probing task, and general evaluation tasks\n(GLUE). Detailed analysis verifies that the per-\nformance improvements come from the use of\nsyntactic information, and the learned attention\nweights are more explainable in terms of linguis-\ntics.\n2 Related Work\nWe first review studies on analyzing the linguistic\nknowledge learned by pre-trained LMs, and then\nwe will introduce recent researches on incorporat-\ning linguistic knowledge into pre-trained LMs.\nLinguistic Studies on Pre-trained LMsAs pre-\ntrained LMs (Devlin et al., 2019; Liu et al., 2019)\ncontinue to provide gains on NLP benchmarks,\nunderstanding what they have learned is very im-\nportant, which can help us understand the reason\n1https://github.com/PaddlePaddle/\nResearch/tree/master/NLP/ACL2022-SynCLM\n2431\nbehind their success and their limitations. Many\nstudies aim to unveil linguistic structures from the\nrepresentations learned by pre-trained LMs (Jawa-\nhar et al., 2019; Wang et al., 2019). Some works\ndemonstrate that pre-trained LMs have learned syn-\ntactic information. Hewitt and Manning (2019)\nindicate that syntax information is implicitly em-\nbedded in BERT by learning a linear transformation\nto predict the syntactic depth of each word based on\nits representation. Jawahar et al. (2019) and Tenney\net al. (2019) show that BERT captures syntactic fea-\ntures at lower layers and loses some of learned syn-\ntactic information at higher layers. However, some\nworks show that pre-trained LMs do not capture\nadequate syntactic knowledge. Wang et al. (2019)\nfind that certain syntactic structures may not be\nembedded in BERT, as the dependency weights cal-\nculated by BERT seem to be inconsistent with hu-\nman intuitions of hierarchical structures. Min et al.\n(2020) prove that BERT need to recruit syntactic\nrepresentations from the generated syntactically in-\nformative examples to improve model performance\non syntax-aware examples.\nBased on these studies, we can find that pre-\ntrained LMs often fail to encode enough syntactic\ninformation in their representations and get poor\nperformance on syntax-aware data.\nSyntax Enhanced Pre-trained LMs On the\nother hand, many works try to use syntax infor-\nmation to further improve models (Strubell et al.,\n2018; Nguyen et al., 2019; Zhang et al., 2020; Li\net al., 2020b; Xu et al., 2020).\nTask oriented works attempt to inject syn-\ntactic knowledge into the transformer (Strubell\net al., 2018; Nguyen et al., 2019; Bugliarello and\nOkazaki, 2020; Zhang et al., 2020). In the semantic\nrole labeling task, Strubell et al. (2018) restrict each\ntoken to attend to its syntactic parent in an atten-\ntion head and improve the model performance. In\nthe machine translation task, Nguyen et al. (2019)\nincorporate a tree-structured attention into the trans-\nformer for helping encode syntactic information.\nBugliarello and Okazaki (2020) propose a syntax-\naware self-attention mechanism to incorporate syn-\ntactic knowledge into the model. In the machine\nreading comprehension task, Zhang et al. (2020)\nuse syntactic information to guide the self-attention\nto pay no attention to the dispensable words. These\nworks mainly inject syntactic information into at-\ntention mechanisms, and obtain performance gains.\nHowever, they confine to a certain task.\nPre-training oriented works try to integrate syn-\ntactic information in a general way that can be\napplied to various NLP tasks. Inspired by the\nabove researches, some studies (Xu et al., 2020;\nLi et al., 2020b; Bai et al., 2021) design various\nsyntax-aware attention mechanisms. Despite differ-\nent in detail, all of them use syntactic dependency\nrelations to restrict the attention to important local\nregions. The syntax-aware attention can capture\nthe information of important local regions accord-\ning to syntactic structures, so as to obtain more\nbenefits. Meanwhile, some works inject syntactic\nknowledge into pre-trained LMs via introducing\nnew learning objectives, such as syntax head predic-\ntion (Wang et al., 2020) and dependency distance\nprediction (Xu et al., 2020). However, they need\nto work with additional syntax-guided attention\nmethods (Xu et al., 2020).\nNotably, most of these works incorporate an ex-\nplicit syntax-guided component into models during\ntesting. This increases the computational complex-\nity and application difficulty of the model, which\nmay limit the application of model in broader NLP\ntasks. In order to address these problems, we pro-\npose a novel contrastive pre-training framework to\nincorporate syntactic knowledge into pre-trained\nLMs, without introducing computational complex-\nity in downstream tasks.\n3 Methodology\nIn this section, we first describe the two new con-\ntrastive learning objectives in our SynCLM. Then\nwe introduce our pre-training framework and im-\nplementation details.\n3.1 Syntax-guide Contrastive Learning\nIn order to facilitate the learning of syntax-aware\nrepresentations, we propose two learning tasks\nwhich use syntactic structures to guide the learning\nof attention distributions and hidden representa-\ntions in the transformer. Here, we will first in-\ntroduce the transformer architecture and the con-\ntrastive learning method as background. Then we\nwill introduce our two contrastive learning objec-\ntives, and the construction of positive and negative\nsamples, which is the main challenge of contrastive\nlearning.\nTransformer A Transformer (Vaswani et al.,\n2017) is a stack of self-attention layers where each\nlayer (consisting of H heads) transforms the in-\nput unit into a continuous representation. Given\n2432\nthe input sentence S with n tokens, denoted as\n{t1, t2, ..., tn}, we use a(l,h)\ni to represent the at-\ntention distribution of the i-th token by the h-th\nattention head on the l-th layer, where 1 ≤ h ≤ H\nand 1 ≤ l ≤ L. We take the average of all heads’\nattention distributions on the l-th layer as the fi-\nnal distribution of the l-th layer, denoted as a(l,¯h)\ni .\nFinally, we use zl\ni to represent the intermediate\nhidden representation of token i on the l-th layer.\nContrastive Learning Method Contrastive self-\nsupervised learning (CSSL) (Wu et al., 2018; He\net al., 2020) is a learning paradigm which aims\nto capture the intrinsic patterns and properties of\ninput data without using human-provided labels.\nThe basic idea of CSSL is to construct auxiliary\ntasks solely based on the input data, which is the\nkey to CSSL, and force the network to learn mean-\ningful representations by performing the auxiliary\ntasks well. The auxiliary tasks are learned by the\ncontrastive learning loss. In this paper, we use\nInfoNCE function which is a variant of Noise Con-\ntrastive Estimation (NCE) (Gutmann and Hyväri-\nnen, 2010) function for contrastive learning, as\nshown in Equation 1.\nLcl = −log exp( sim(q,q+)\nτ )\nexp( sim(q,q+)\nτ ) +PK\ni=0 exp(\nsim(q,q−\ni )\nτ )\n(1)\nwhere q is the original sample; q+ and q−\ni are the\npositive and the i-th negative samples, respectively;\nK is the number of negative samples. The sim()\nfunction can be any similarity function, such as\ncosine, Jensen-Shannon Divergence (Endres and\nSchindelin, 2003) and Hellinger distance (Beran,\n1977). τ called temperature coefficient is a hyper-\nparameter used in recent methods (Khosla et al.,\n2020; Yu et al., 2021).\nPhrase-guided Contrastive Learning Objective\nSome phrases can be recognized by using the\nsimilarity of attention distributions over words\n(Mareˇcek and Rosa, 2019; Kim et al., 2020). To\nfurther improve the recognition, we propose to use\nprior phrase structure information to further guide\nthe learning of attention distributions by maximiz-\ning the similarity of attention distributions between\nwords in the same phrase.\nGiven a sampled token ti, we randomly select a\ntoken in the same phrase2 as its positive example,\n2In our experiments, the sampled phrase has no more than\ntwo hierarchical layers, that is to say, the height of its corre-\nsponding subtree is no more than 2.\nand select K tokens outside the phrase as the con-\ntrastive negative examples. As shown in the sam-\npled phrases of Figure 1, for the token “build”, the\ntoken marked as P is the positive example, and to-\nkens marked as N are negative examples. Then we\nuse the contrastive learning loss (defined in Equa-\ntion 1) for this learning task, and the corresponding\nsim() function is defined as follows:\nsimphrase = −JSD (a(l,¯h)\ni ∥ a(l,¯h)\ns )\n= −(DKL(a(l,¯h)\ni ∥ m) +DKL(a(l,¯h)\ns ∥ m))/2\nwhere m = (a(l,¯h)\ni + a(l,¯h)\ns )/2\n(2)\nwhere JSD is short for Jensen-Shannon Diver-\ngence (Endres and Schindelin, 2003), and DKL\nfor Kullback-Leibler Divergence (KLD) (Kullback\nand Leibler, 1951). The index s indicates a sam-\npled example of token ti, which may be positive\nor negative. Please note that there are many cal-\nculation choices for the sim() function, such as\ncosine, JSD and KLD. In our early-stage prelimi-\nnary experiments, we have experimented with JSD\nand KLD, and the former performs slightly better\nand thus is adopted in our framework.\nTree-guided Contrastive Learning Objective\nThe idea that the root of a syntax tree should pay\nmore attention to its descendant nodes has been\nproved to be effective in attention-based models by\nexisting syntax-aware attention mechanisms (Xu\net al., 2020; Li et al., 2020b; Bai et al., 2021).\nTherefore, we propose a tree-guided contrastive\nlearning objective to maximize the similarity be-\ntween the global representation based on all input\ntokens and the syntax-aware representation based\non syntactically related tokens.\nGiven a sampled token ti, we derive its subtree\nfrom the entire dependency tree. As described by\nthe sampled subtrees in Figure 1, the subtree of\ntoken “build” consists of all tokens dominated by\ntoken “build”, and “build” is the root of the subtree.\nWe use it as the positive tree, denoted as T+. Then\nwe randomly replace no more than three tokens in\nT+ with adjacent tokens to get the negative tree\nT−, and ensure that there is at least one same token\nin T+ and T−, as shown by the other two subtrees\nin Figure 1. According to the above conclusion, the\nrepresentation based on the tokens in the positive\nsubtree should be closer to the original representa-\ntion given by the pre-trained LM. We also use the\ncontrastive learning loss in Equation 1 to optimize\nthis learning objective, and the sim() function is\n2433\ndefined as follows:\nsimtree = cosine(zl\ni,\nX\ntj∈Ts\neijzl\nj)\nwhere e ij = exp(zl\ni · zl\nj)P\ntk∈Ts exp(zl\ni · zl\nk)\n(3)\nwhere Ts represents a sampled subtree of token ti,\nwhich may be positive or negative. And zl\ni rep-\nresents the intermediate hidden representation of\ntoken i on the l-th layer.\n3.2 Syntax-guided Pre-training Framework\nWe then add the two contrastive learning objec-\ntives into traditional pre-training, so as to enhance\nvanilla pre-trained LM. The final loss for the pre-\ntraining is the summation of the training loss for\nmasked language model (MLM) (Devlin et al.,\n2019) and two new proposed tasks, as shown below.\nL = LMLM + Lphrase + Ltree\nData for Pre-training We use BERT’s pre-\ntraining data (Devlin et al., 2019) as our model’s\npre-training data, including documents from En-\nglish Wikipedia and BookCorpus (Zhu et al., 2015).\nThen we use the pre-processing and BPE (Byte-Pair\nEncoding) tokenization from RoBERTa (Liu et al.,\n2019) to process the training data. The maximum\nlength of input sequence is set to 512.\nTo obtain syntactic structures for each sentence,\nwe adopt a well-trained parsing model - Stanza 3\nto automatically generate a syntax tree for each\nsentence. Because the pre-trained LM takes sub-\nwords as the input unit, for the word u, we take its\nfirst subword as the root, and add edges connect-\ning non-first subwords to the first subword. Since\nsyntactic information is pre-processed in advance,\nsyntax parsing only needs to be performed once\nin the entire process. In our work, it takes about\none day to parse the pre-training data with 20 P40\nGPUs. Then, syntactic information is used as the\nadditional input in the pre-training stage.\nImplementation Details To accelerate the train-\ning process, we initialize parameters from\nRoBERTa models4 released by Liu et al. (2019).\nWe use RoBERTa-base and RoBERTa-large to\ninitialize our base and large models respectively.\nRoBERTa-base contains 12 layers, each of which\n3https://github.com/stanfordnlp/stanza\n4https://github.com/pytorch/fairseq/\ntree/master/examples/roberta\nDataset Train Test Class Metric\nCoLA 8,551 1,063 2 MCC\nBLiMP 0 40,000 * Acc\nFCE 28,731 2,720 2/* Acc/F 0.5\nCoNLL-2003 14,041 3,453 * F1\nOpenEntity 1,988 1,988 9 F1\nSST-2 63,749 1,821 2 Acc\nMRPC 3,668 1,725 2 Acc/F1\nQQP 363,871 390,695 2 Acc/F1\nSTS-B 5,749 1,379 * Pea./Spr.\nMNLI 392,702 9,796 3 Acc\nQNLI 104,743 5,463 2 Acc\nRTE 2,490 3,000 2 Acc\nTable 1: Statistics of datasets used in our work. “*”\nrepresents for the non-classification tasks. “Acc” is short\nfor “accuracy”. “Pea.” and “Spr.” are abbreviations for\n“Pearson” and “Spearman correlation” respectively.\nhas 12 heads and 768 hidden states. And RoBERTa-\nlarge contains 24 layers, each of which has 16\nheads and 1024 hidden states. We set l as the last\nhidden layer in Equation 2 and Equation 3. And\nthe number of negative examples is set to 3. As\nour pre-trained LMs do not introduce additional\nparameters, the parameter sizes of our base and\nlarge models are the same as those of RoBERTa\nmodels.\nWe pre-train our models with 16 32G NVIDIA\nV100 GPUs. The base model takes about four days\nand the large model takes about seven days. During\nthe training process, in order to choose a well pre-\ntrained model, we evaluate the intermediate model\nper 10K steps, and terminate the training when the\nperformance alteration (i.e., Perplexity of LMs) is\nbelow a certain threshold for five sequential eval-\nuations. In the base setting, the batch size is 512,\nand the total steps are 300,000, 24,000 of which is\nthe warm up steps. For the large model, the batch\nsize is 256, and the total steps are 350,000, 30,000\nof which is for warming up.\n4 Experiments\nFirst, we verify the effectiveness of SynCLM on\nseveral syntax-aware tasks, including grammati-\ncal error detection task (Section 4.1) and entity\ntasks (Section 4.2), which are often used for test-\ning syntax pre-training models. Then, we test the\neffectiveness of SynCLM on more general tasks\nby using GLUE benchmark (Section 4.3). At last,\ndetailed analysis is conducted to show the impact\nof incorporating syntactic knowledge (Section 4.4).\nPlease note that ↑ in our reported results means\nstatistically significant improvement over the base-\nline with p-value < 0.05. Besides, for fair\ncomparison, we report continue training results\n2434\nModels BLiMP CoLA FCE\n1P/2P Acc MCC Acc/F 0.5\nBERT-large (Devlin) -/- 63.9 ⋆ -/57.3⋆\nBiLSTM-Joint (Rei) -/- - 80.1/52.1\nSLA-large (Li20) -/- 64.5 -/58.0\nGPT-2 large (Rad19) 78.0/81.6 - -/-\nRoBERTa-base (Liu) 74.9/78.5 63.6 83.3/68.6\n+ continuous 75.0/79.6 63.8 83.5/68.6\n+ PHRASE 75.5/81.2 64.5 83.9/69.0\n+ TREE 76.4/80.6 64.9 84.2/68.9\nSynCLM-base 77.3↑/81.0↑ 65.3↑ 84.3↑69.2↑\nRoBERTa-base + SLA -/- 64.2 83.2/68.3\n+ PHRASE -/- 65.1 83.7/67.3\n+ TREE -/- 65.8 84.3/68.4\nSynCLM-base + SLA -/- 66.3↑ 83.6/68.7\nRoBERTa-large (Liu) 77.3/79.4 68.0 85.3/72.2\nSynCLMg 79.5↑/81.1↑ 69.3↑ 86.1↑/72.4\nTable 2: Results on GED datasets. Results with “⋆” are\ntaken from Li et al. (2020b). Reported results of CoLA\nare a median over 5 runs, and those of FCE are the\naverage over 5 runs. For BLiMP, we report accuracies\nfor “one prefix” (1P) (Linzen et al., 2016) and “two\nprefix” (2P) (Wilcox et al., 2019).\n(continuous) of RoBERTa5.\nThe statistics of datasets adopted in this paper\nare summarized in Table 1. For datasets of GLUE,\nwe use metrics reported in Devlin et al. (2019). For\nother datasets, we use popular metrics provided by\ndataset authors and other researchers.\n4.1 Grammatical Error Detection (GED)\nGED task aims to evaluate the grammatical ac-\nceptability of a given sentence. We use three pop-\nular public datasets, i.e., CoLA (Warstadt et al.,\n2019), BLiMP (Warstadt et al., 2020), and FCE\n(Yannakoudakis et al., 2011), to evaluate our mod-\nels. For CoLA, we use Matthews Correlation Co-\nefficient (MCC) (Matthews, 1975) as the evalua-\ntion metric. For BLiMP, we evaluate models using\nthe overall accuracy on all input pairs, namely the\nproportion of pairs whose acceptable sentence is\nassigned a higher probability. On FCE, following\nRei and Søgaard (2019), we take it as a binary clas-\nsification task and a sequence labeling task, and use\naccuracy and F0.5 to evaluate them respectively.\nBaselines Rei and Søgaard (2019) combine ob-\njectives at different granularities (i.e., sentence and\ntoken) to learn better representations. Li et al.\n(2020b) use dependency distance matrix to obtain\na syntax-aware local attention (SLA) and achieve\nSOTA results on FCE. We also report the results\nof BERT, RoBERTa and GPT-2 (Radford et al.),\n5Due to the limitation of space and computing resources,\nwe only give continue training results of base models.\nwhere GPT-2 reports SOTA results on BLiMP.\nMain Results From Table 2, it can be seen that\nSynCLM achieves consistent gains over RoBERTa\non all three datasets: 2.0% higher average accu-\nracy on BLiMP, 1.3% higher MCC on CoLA, and\n0.8% higher accuracy on FCE. The results show\nthat syntactic prior information helps SynCLM to\nperform much better on GED task. We believe this\nis because the grammatical acceptability of a sen-\ntence strongly rely on its syntactic structure. As\nillustrated by the first example in Figure 2, which\nchecks the morphological number agreement of the\nsentence, the morphological number of the word\n“eat” should be consistent with that of its subject\n“John”. And the dependency syntax illustrates the\nsubject-verb relation between them.\nThe tree-guided method performs better than\nthe phrase-guided method on most metrics, as the\ntree structure gives the head-dependent relations\nbetween words more directly and more explicitly.\nMoreover, combining the two methods can achieve\nmore gains.\nMerging SLA We also test whether SynCLM\ncan be further improved with previous syntax-\nenhanced attention mechanisms for fine-tuning. We\nimplement SLA (Li et al., 2020b) in the fine-tuning\nstage, and show the results in the third part of Table\n2. It can be seen that merging SLA and SynCLM\ncan achieve more gains on CoLA and FCE, which\nmeans that SynCLM can be further improved by\nusing syntax information during fine-tuning.\n4.2 Entity Tasks\nWe evaluate SynCLM on two entity related tasks:\nnamed entity recognition (NER) and entity typing\n(ENT), which aim to recognize entities and predict\nentity types respectively. We use CoNLL-2003\n(Sang and De Meulder, 2003) for NER task and\nOpenEntity (Choi et al., 2018) for ENT task.\nBaselines BERT-MRC (Li et al., 2020a) formu-\nlates NER task as a machine reading comprehen-\nsion task to handle both flat and nested NER tasks.\nKEPLER (Wang et al., 2021) infuses knowledge\ninto pre-trained models and jointly learns knowl-\nedge embeddings and language representations.\nSEPREM (Xu et al., 2020) injects syntax infor-\nmation into pre-trained LMs by introducing two\nlearning tasks and a syntax-aware attention layer.\nLUKE (Yamada et al., 2020) uses a large amount\nof entity-annotated corpus and an entity-aware self-\n2435\nModels QQP MRPC STS SST CoLA RTE MNLI-m QNLI\nBERT-large (Devlin et al., 2019) 91.3/- 88.0/- 90.0/- 93.2 60.6 70.4 86.6 92.3\nXLNet-large (Yang et al., 2019) 92.3/- 90.8/- 92.5/- 97.0 69.0 85.9 90.8 94.9\nSLA-large (Li et al., 2020b) -/- -/- -/- 94.3 64.5 - - -\nRoBERTa-base (Liu et al., 2019) 91.6/88.9⋆ 90.1/92.7⋆ 90.9/90.7⋆ 94.8 63.6 78.7 87.6 92.8\n+ continuous 91.6/88.8 90.2/92.8 90.2/90.1 94.9 63.8 79.1 87.2 92.8\n+ PHRASE 91.7/88.9 90.5/93.0 90.3/90.2 95.2 64.5 79.8 87.0 92.9\n+ TREE 91.7/88.9 91.2/93.6 90.6/90.4 95.1 64.9 80.1 87.4 93.0\nSynCLM-base 91.7/88.9 91.4↑/93.7↑ 90.8/90.6 95.1↑ 65.3↑ 80.1↑ 87.2 93.0\nRoBERTa-large (Liu et al., 2019) 92.1/89.5⋆ 90.7/93.2⋆ 92.2/92.1⋆ 96.4 68.0 86.3 ⋆ 90.2 94.7\nSynCLM-large 92.3/89.7 91.2 ↑/93.6↑ 92.0/91.8 96.7↑ 69.3↑ 87.4↑ 90.5 94.8\nTable 3: Performance on dev sets of GLUE tasks. The results of BERT and RoBERTa are from Liu et al. (2019).\nResults with ⋆ are from our re-implementations, as some metrics are not given by Liu et al. (2019). For each task,\nwe run model for 5 times with different random initialization seeds, and report the median result.\nFigure 2: Case study. The third column shows the simplified syntax tree of each example. Pbase represents the\nlabel predicted by RoBERTa-base model, andPsyntax represents the label predicted by SynCLM-base.\nModels CoNLL-2003 OpenEntity\nP / R / F1 P / R / F1\nBERT-MRC (Li) 92.3 / 94.6 / 93.0 - / - / -\nSLA-large (Li20) 92.3 / 93.4 / 92.9 - / - / -\nKEPLER (Wang) - / - / - 77.2 / 74.2 / 75.7\nSEPREM (Xu20) - / - / - 80.1 / 77.1 / 79.1\nLUKE (Yamada) - / - / 94.3 79.9 / 76.6 / 78.2\nSEPREM-base 84.0 / 92.9 /88.2 76.7 / 73.5 / 75.1\nRoBERTa-base 92.4 / 92.9 / 92.6 75.7 / 74.6 / 75.1\n+ PHRASE 92.9 / 93.0 /93.0 75.9 / 74.9 / 75.4\n+ TREE 92.7 / 92.9 / 92.8 75.6 / 75.2 / 75.4\nSynCLM-base 93.0↑ / 93.0↑ / 93.0↑ 76.6 / 74.6 /75.6↑\n+ SLA 92.1 /94.1 / 93.1↑ 76.7 / 74.6 /75.7↑\nRoBERTa-large 93.0 / 93.5 / 93.2 76.3 / 76.1 / 76.2\nSynCLM-large 93.4↑ / 93.8↑ / 93.6↑ 76.8 / 76.1 /76.4\nTable 4: Results (average of 5 runs) on entity tasks. We\nreport continue training results for RoBERTa-base.\nattention mechanism to learn pre-trained contextu-\nalized representations for words and entities, and\nobtains SOTA results on five entity-related datasets,\nincluding CoNLL-2003 and OpenEntity.\nMain Results Table 4 shows the performances of\nSOTA models and our models on CoNLL-2003 and\nOpenEntity. On the NER task, SynCLM-large im-\nproves F1 score by 0.4% compared with RoBERTa-\nlarge. Meanwhile, phrase-guided method consis-\ntently outperforms tree-guided method both in the\nbase and large model. We think this is because the\ngoal of phrase-guided method matches pretty well\nwith the goal of NER. On the ENT task, SynCLM\nobtains 0.9% and 0.5% precision improvements\nunder the settings of base-size and large-size, re-\nspectively.\nComparison with SEPREM and SLA Com-\npared with SEPREM on ENT, SynCLM achieves\na smaller improvement. We suspect the reason is\ntwo-fold. First, in SynCLM, syntactic informa-\ntion is incorporated only in the pre-training stage.\nSecond, the pre-training data used in SEPREM\nis about ten times larger than ours. In order to\nverify the above hypotheses, based on RoBERTa-\nbase, we implement the two pre-training tasks of\nSEPREM, namely dependency head prediction and\ndependency distance prediction, and train them\non the pre-training data used in our work, result-\ning in SEPREM-base in Table 4. We observe that\nSEPREM trained on small-scale data does not per-\nform well. Meanwhile, we merge SLA in the fine-\ntuning stage, resulting in SynCLM-base + SLA.\nThe result verifies that SynCLM can be further\nimproved by using syntactic information during\nfine-tuning stage.\n4.3 GLUE Benchmark\nBesides, we evaluate SynCLM on the GLUE bench-\nmark (Wang et al., 2018), a collection of diverse\n2436\ndatasets for evaluating natural language understand-\ning models. It contains single-sentence classifi-\ncation tasks (CoLA and SST-2), similarity and\nparaphrase tasks (MRPC, QQP, and STS-B), as\nwell as pairwise inference tasks (MNLI, RTE, and\nQNLI). We use the default train/dev/test split. For\neach dataset, we fine-tune the pre-trained model\nseparately, using only the corresponding single-\ntask training data (i.e., without multi-task train-\ning). Our fine-tuning procedure follows the orig-\ninal RoBERTa paper. We consider a limited hy-\nperparameter sweep for each task, with batch sizes\n∈ {16, 32} and learning rates ∈ {1e − 5, 2e −\n5, 3e−5}, with a linear warm up for the first 6% of\nsteps followed by a linear decay to 0. We fine-tune\nfor 10 epochs. The rest of the hyperparameters\nremain the same as during pre-training.\nExperimental Results As shown in Table 3, our\nmodels outperform baseline models in most tasks,\nand achieve more significant gains in tasks with\nsmall training datasets, such as CoLA, RTE, MRPC.\nThe performance on CoLA is discussed in Sec-\ntion 4.1. On RTE, compared with baseline mod-\nels, SynCLM obtains significant gains of 1.4% and\n1.1% in base size and large size, respectively. Simi-\nlarly, it brings accuracy improvement of 1.3% for\nbase model and 0.5% for large model on MRPC.\nMoreover, incorporating syntactic knowledge into\nbase models brings greater improvements in some\ndatasets. From the results of all downstream tasks,\nit can be seen that syntactic information is more\nuseful when task’s training data is small or the\ncomputation power is limited. We think that more\ntraining data in the fine-tuning stage will lead to\ngreater loss of syntactic knowledge encoded in the\nlast layer’s hidden representations, as last two lay-\ners encode task-specific features and undergo the\nlargest changes (Kovaleva et al., 2019).\nBesides, SynCLM achieves larger improvements\non single-sentence tasks, but does not always per-\nform well on sentence-pair tasks. We think this is\nbecause the cross-sentence interactions are more\nimportant for sentence-pair tasks. How to use syn-\ntactic information effectively in the sentence-pair\ntasks is a problem we plan to explore in the future.\nFinally, we can conclude that SynCLM is still\neffective in general tasks, especially in tasks with\nsmall training data.\nModels UUAS Spr.\nBERT-large (Devlin) 82.5 0.86\nSyntax-BERT-large (Bai21) 83.4 0.90\nSyntax-RoBERTa-large (Bai21) 84.6 0.93\nRoBERTa-base (Liu) 81.2 0.85\nSynCLM-base 84.9↑ 0.87\nTable 5: The results of structural probing task.\nFigure 3: Visualization of attention weight scores. This\ncase is from CoLA and has grammatical error. The red\nrectangle indicates higher scores in our model but lower\nscores in the baseline model.\n4.4 Analysis\nStructural Probing Tasks To check whether the\nrepresentation learned by SynCLM captures syn-\ntactic knowledge effectively, following Hewitt and\nManning (2019), we construct a syntax tree of a\nsentence with linear transformation learned for the\nembedding space. If the syntax tree is better con-\nstructed, the model is considered having learned\nmore syntactic information. We use the pre-trained\nLM based on phrase-guided method to capture the\nStanford Dependencies formalism (De Marneffe\net al., 2006). Similarly, we use the undirected at-\ntachment score(UUAS) denoting the percentage\nof correctly placed undirected edges as the main\nmetric. We also report spearman correlation(Spr.)\nbetween predicted and the actual distance between\neach token pair in a sentence. The results are shown\nin Table 5. It can be seen that our base model ob-\ntains SOTA results on UUAS, indicating that our\nmethod can enhance the model capability of cap-\nturing syntactic structures.\nCase Study Figure 2 gives three examples to il-\nlustrate the effectiveness of incorporating syntactic\ninformation. These examples show that SynCLM\ncan capture syntactic information and make correct\npredictions based on the obtained information. To\ngive further insight into how syntactic knowledge\naffects prediction, we highlight the main syntax\nstructures that affect prediction. Here, we take the\nthird case for detailed analysis. The core tokens in\n2437\nthe syntax trees of the two sentences are the same,\nso the model predicts they have the same semantics.\nThrough more data analysis, we find that SynCLM\nenhances the attention weight between syntacti-\ncally related words, thus increasing the importance\nof non-leaf tokens in model prediction. Please note\nthat this feature also leads to wrong predictions. In\nthe future we will attend to the problem of how\nto integrate syntactic and semantic information in\nmodel prediction.\nAttention Visualization In order to verify the\nimpact of syntactic information in the attention\nmechanisms of the pre-trained LM, we plot atten-\ntion weights of baseline models and our models in\nFigure 3. We mainly focus on the interactions of\ntokens, except for [CLS] and [SEP]. Then the atten-\ntion weights are averaged over all heads and layers.\nThis visualization demonstrates the effectiveness of\ninjecting syntactic information into self-attention.\nFrom Figure 3, we can see that higher attention\nweight between directly syntactically related to-\nkens in our model. For example, our model assigns\nstrong attentions from the token “John” to “go” and\n“abroad”, while the baseline model assigns lower\nattentions for these correlated tokens.\n5 Conclusion\nTo the best of our knowledge, this is the first work\nof leveraging contrasting learning to inject syntax\nknowledge into pre-trained LMs. Motivated by the\nproperties of constituent and dependency structures\nof syntax, we design phrase-guided and tree-guided\nlearning objectives to guide the learning of atten-\ntion distributions and hidden representations in the\ntransformer. Through extensive experiments, we\nshow that SynCLM consistently improves a wide\nrange of tasks, from GED, entity tasks to GLEU,\nwhich confirms the advantage of our syntax-guided\ncontrastive learning. Detailed analysis also shows\nthat SynCLM does incorporate rich syntax knowl-\nedge and learn explainable attention weights.\nAcknowledgements\nWe are very grateful to our anonymous reviewers\nfor their helpful feedback on this work. We would\nlike to thank Ying Chen for examination and re-\nvision in paper writing; Can Gao for the help on\nmodel training. This work was supported in part\nby the National Key R&D Program of China under\nGrant 2020YFB1406701.\nReferences\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang,\nJing Bai, Jing Yu, and Yunhai Tong. 2021. Syntax-\nbert: Improving pre-trained transformers with syntax\ntrees. arXiv preprint arXiv:2103.04350.\nRudolf Beran. 1977. Minimum hellinger distance\nestimates for parametric models. The annals of\nStatistics, pages 445–463.\nEmanuele Bugliarello and Naoaki Okazaki. 2020. En-\nhancing machine translation with dependency-aware\nself-attention. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 1618–1627.\nEunsol Choi, Omer Levy, Yejin Choi, and Luke\nZettlemoyer. 2018. Ultra-fine entity typing. In\nProceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (V olume\n1: Long Papers), pages 87–96.\nMarie-Catherine De Marneffe, Bill MacCartney,\nChristopher D Manning, et al. 2006. Generat-\ning typed dependency parses from phrase structure\nparses. In Lrec, volume 6, pages 449–454.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4171–4186.\nDominik Maria Endres and Johannes E Schindelin.\n2003. A new metric for probability distribu-\ntions. IEEE Transactions on Information theory,\n49(7):1858–1860.\nCecilia E Ford and Barbara A Fox. 2002. Con-\nstituency and the grammar. The language of turn\nand sequence, page 14.\nMichael Gutmann and Aapo Hyvärinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings\nof the thirteenth international conference on artificial\nintelligence and statistics, pages 297–304. JMLR\nWorkshop and Conference Proceedings.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie,\nand Ross Girshick. 2020. Momentum contrast for\nunsupervised visual representation learning. In\n2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 9726–9735.\nIEEE Computer Society.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for finding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4129–4138.\n2438\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does bert learn about the structure of\nlanguage? In ACL 2019-57th Annual Meeting of\nthe Association for Computational Linguistics.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. Advances in Neural\nInformation Processing Systems, 33.\nTaeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo\nLee. 2020. Are pre-trained language models aware\nof phrases? simple but strong baselines for grammar\ninduction. arXiv preprint arXiv:2002.00737.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 4365–4374.\nSolomon Kullback and Richard A Leibler. 1951.\nOn information and sufficiency. The annals of\nmathematical statistics, 22(1):79–86.\nXiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong\nHan, Fei Wu, and Jiwei Li. 2020a. A unified\nmrc framework for named entity recognition. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n5849–5859.\nZhongli Li, Qingyu Zhou, Chao Li, Ke Xu, and Yunbo\nCao. 2020b. Improving bert with syntax-aware local\nattention. arXiv preprint arXiv:2012.15150.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDavid Mare ˇcek and Rudolf Rosa. 2019. From\nbalustrades to pierre vinken: Looking for syntax in\ntransformer self-attentions. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 263–\n275.\nBrian W Matthews. 1975. Comparison of the pre-\ndicted and observed secondary structure of t4\nphage lysozyme. Biochimica et Biophysica Acta\n(BBA)-Protein Structure, 405(2):442–451.\nJunghyun Min, R Thomas McCoy, Dipanjan Das, Emily\nPitler, and Tal Linzen. 2020. Syntactic data augmen-\ntation increases robustness to inference heuristics.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n2339–2352.\nXuan-Phi Nguyen, Shafiq Joty, Steven Hoi, and Richard\nSocher. 2019. Tree-structured attention with hierar-\nchical accumulation. In International Conference on\nLearning Representations.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language mod-\nels are unsupervised multitask learners.\nMarek Rei and Anders Søgaard. 2019. Jointly learn-\ning to label sentences and tokens. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 6916–6923.\nDevendra Singh Sachan, Yuhao Zhang, Peng Qi, and\nWilliam Hamilton. 2020. Do syntax trees help pre-\ntrained transformers extract information? arXiv\npreprint arXiv:2008.09084.\nErik Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages\n142–147.\nEmma Strubell, Patrick Verga, Daniel Andor, David\nWeiss, and Andrew McCallum. 2018. Linguistically-\ninformed self-attention for semantic role labeling. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n5027–5038.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert\nrediscovers the classical nlp pipeline. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4593–4601.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in neural information\nprocessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Cuihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-adapter: Infusing knowledge\ninto pre-trained models with adapters. arXiv preprint\narXiv:2002.01808.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021. Kepler: A unified model for knowledge\nembedding and pre-trained language representation.\n2439\nTransactions of the Association for Computational\nLinguistics, 9:176–194.\nYaushian Wang, Hung-Yi Lee, and Yun-Nung Chen.\n2019. Tree transformer: Integrating tree struc-\ntures into self-attention. In Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1060–1070.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020. Blimp: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the\nAssociation for Computational Linguistics, 8:377–\n392.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019. Structural su-\npervision improves learning of non-local grammat-\nical dependencies. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, V olume1 (Long and Short\nPapers), pages 3302–3312.\nZhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua\nLin. 2018. Unsupervised feature learning via\nnon-parametric instance discrimination. In 2018\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3733–3742. IEEE.\nZenan Xu, Daya Guo, Duyu Tang, Qinliang Su,\nLinjun Shou, Ming Gong, Wanjun Zhong, Xi-\naojun Quan, Nan Duan, and Daxin Jiang. 2020.\nSyntax-enhanced pre-trained model. arXiv preprint\narXiv:2012.14116.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. Luke: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6442–6454.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. Advances in neural\ninformation processing systems, 32.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A new dataset and method for automati-\ncally grading esol texts. In Proceedings of the 49th\nannual meeting of the association for computational\nlinguistics: human language technologies, pages\n180–189.\nYue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren,\nTuo Zhao, and Chao Zhang. 2021. Fine-tuning\npre-trained language model with weak supervi-\nsion: A contrastive-regularized self-training ap-\nproach. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 1063–1077.\nZhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng\nDuan, Hai Zhao, and Rui Wang. 2020. Sg-net:\nSyntax-guided machine reading comprehension. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 9636–9643.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision, pages\n19–27.\n2440",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8932141065597534
    },
    {
      "name": "Natural language processing",
      "score": 0.6994990110397339
    },
    {
      "name": "Syntax",
      "score": 0.67002934217453
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6598096489906311
    },
    {
      "name": "Parsing",
      "score": 0.622489333152771
    },
    {
      "name": "Transformer",
      "score": 0.6182757019996643
    },
    {
      "name": "Abstract syntax tree",
      "score": 0.5076991319656372
    },
    {
      "name": "Abstract syntax",
      "score": 0.5009636878967285
    },
    {
      "name": "Dependency grammar",
      "score": 0.4438014626502991
    },
    {
      "name": "Language model",
      "score": 0.43198686838150024
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}