{
  "title": "SW-UNet: a U-Net fusing sliding window transformer block with CNN for segmentation of lung nodules",
  "url": "https://openalex.org/W4387184186",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2110135718",
      "name": "Jiajun Ma",
      "affiliations": [
        "China Shenhua Energy (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1994065689",
      "name": "Gang Yu-An",
      "affiliations": [
        "Dalian Medical University",
        "First Affiliated Hospital of Dalian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A3147931797",
      "name": "ChenHua Guo",
      "affiliations": [
        "North University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5111114355",
      "name": "Xiaoming Gang",
      "affiliations": [
        "Anshan Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2567951637",
      "name": "Minting Zheng",
      "affiliations": [
        "First Affiliated Hospital of Dalian Medical University",
        "Dalian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2110135718",
      "name": "Jiajun Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1994065689",
      "name": "Gang Yu-An",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3147931797",
      "name": "ChenHua Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5111114355",
      "name": "Xiaoming Gang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2567951637",
      "name": "Minting Zheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4225591761",
    "https://openalex.org/W4313525551",
    "https://openalex.org/W3155197194",
    "https://openalex.org/W2899279931",
    "https://openalex.org/W4206693420",
    "https://openalex.org/W4385152034",
    "https://openalex.org/W3135352157",
    "https://openalex.org/W4383619805",
    "https://openalex.org/W4225378743",
    "https://openalex.org/W3195997474",
    "https://openalex.org/W3000524228",
    "https://openalex.org/W4229009122",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4312939334",
    "https://openalex.org/W4384789574",
    "https://openalex.org/W3022161521",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W4292575177",
    "https://openalex.org/W4290612585",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W4200049650",
    "https://openalex.org/W4310854192",
    "https://openalex.org/W3134075305",
    "https://openalex.org/W2910094941",
    "https://openalex.org/W2933796343",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3182372246",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W3116112801",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Medical images are information carriers that visually reflect and record the anatomical structure of the human body, and play an important role in clinical diagnosis, teaching and research, etc. Modern medicine has become increasingly inseparable from the intelligent processing of medical images. In recent years, there have been more and more attempts to apply deep learning theory to medical image segmentation tasks, and it is imperative to explore a simple and efficient deep learning algorithm for medical image segmentation. In this paper, we investigate the segmentation of lung nodule images. We address the above-mentioned problems of medical image segmentation algorithms and conduct research on medical image fusion algorithms based on a hybrid channel-space attention mechanism and medical image segmentation algorithms with a hybrid architecture of Convolutional Neural Networks (CNN) and Visual Transformer. To the problem that medical image segmentation algorithms are difficult to capture long-range feature dependencies, this paper proposes a medical image segmentation model SW-UNet based on a hybrid CNN and Vision Transformer (ViT) framework. Self-attention mechanism and sliding window design of Visual Transformer are used to capture global feature associations and break the perceptual field limitation of convolutional operations due to inductive bias. At the same time, a widened self-attentive vector is used to streamline the number of modules and compress the model size so as to fit the characteristics of a small amount of medical data, which makes the model easy to be overfitted. Experiments on the LUNA16 lung nodule image dataset validate the algorithm and show that the proposed network can achieve efficient medical image segmentation on a lightweight scale. In addition, to validate the migratability of the model, we performed additional validation on other tumor datasets with desirable results. Our research addresses the crucial need for improved medical image segmentation algorithms. By introducing the SW-UNet model, which combines CNN and ViT, we successfully capture long-range feature dependencies and break the perceptual field limitations of traditional convolutional operations. This approach not only enhances the efficiency of medical image segmentation but also maintains model scalability and adaptability to small medical datasets. The positive outcomes on various tumor datasets emphasize the potential migratability and broad applicability of our proposed model in the field of medical image analysis.",
  "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/eight.tnum September /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nOPEN ACCESS\nEDITED BY\nLiang Zhao,\nDalian University of Technology, China\nREVIEWED BY\nShuang Chen,\nFudan University, China\nXi Yang,\nShanghai Jiao Tong University, China\nLei Dai,\nNingbo Second Hospital, China\n*CORRESPONDENCE\nMinting Zheng\ndy_zmt@/one.tnum/six.tnum/three.tnum.com\nRECEIVED /zero.tnum/six.tnum August /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /one.tnum/two.tnum September /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /two.tnum/eight.tnum September /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nMa J, Yuan G, Guo C, Gang X and Zheng M\n(/two.tnum/zero.tnum/two.tnum/three.tnum) SW-UNet: a U-Net fusing sliding window\ntransformer block with CNN for segmentation\nof lung nodules. Front. Med./one.tnum/zero.tnum:/one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Ma, Yuan, Guo, Gang and Zheng. This is\nan open-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nSW-UNet: a U-Net fusing sliding\nwindow transformer block with\nCNN for segmentation of lung\nnodules\nJiajun Ma /one.tnum, Gang Yuan /two.tnum, Chenhua Guo /three.tnum, Xiaoming Gang /four.tnumand\nMinting Zheng /two.tnum*\n/one.tnumShenhua Hollysys Information Technology Co., Ltd., Beijing, China, /two.tnumThe First Aﬃliated Hospital of\nDalian Medical University, Dalian, China, /three.tnumSchool of Software, North University of China, Taiyuan, China,\n/four.tnumAnshan Municipal Central Hospital, Anshan, China\nMedical images are information carriers that visually reﬂect and record the\nanatomical structure of the human body, and play an important rol e in clinical\ndiagnosis, teaching and research, etc. Modern medicine has become increasingly\ninseparable from the intelligent processing of medical images. In recent years,\nthere have been more and more attempts to apply deep learning th eory to\nmedical image segmentation tasks, and it is imperative to exp lore a simple and\neﬃcient deep learning algorithm for medical image segmentatio n. In this paper,\nwe investigate the segmentation of lung nodule images. We addre ss the above-\nmentioned problems of medical image segmentation algorithms and conduct\nresearch on medical image fusion algorithms based on a hybrid chan nel-space\nattention mechanism and medical image segmentation algorithm s with a hybrid\narchitecture of Convolutional Neural Networks (CNN) and Visual Transformer.\nTo the problem that medical image segmentation algorithms are d iﬃcult to\ncapture long-range feature dependencies, this paper proposes a medical image\nsegmentation model SW-UNet based on a hybrid CNN and Vision Tra nsformer\n(ViT) framework. Self-attention mechanism and sliding window d esign of Visual\nTransformer are used to capture global feature associations and break the\nperceptual ﬁeld limitation of convolutional operations due t o inductive bias. At\nthe same time, a widened self-attentive vector is used to stream line the number\nof modules and compress the model size so as to ﬁt the characteristics of a\nsmall amount of medical data, which makes the model easy to be over ﬁtted.\nExperiments on the LUNA/one.tnum/six.tnum lung nodule image dataset validatethe algorithm\nand show that the proposed network can achieve eﬃcient medical image\nsegmentation on a lightweight scale. In addition, to validate t he migratability of the\nmodel, we performed additional validation on other tumor dat asets with desirable\nresults. Our research addresses the crucial need for improved m edical image\nsegmentation algorithms. By introducing the SW-UNet model, wh ich combines\nCNN and ViT, we successfully capture long-range feature dependen cies and\nbreak the perceptual ﬁeld limitations of traditional convolut ional operations. This\napproach not only enhances the eﬃciency of medical image segmentati on but\nalso maintains model scalability and adaptability to small me dical datasets. The\npositive outcomes on various tumor datasets emphasize the pot ential migratability\nand broad applicability of our proposed model in the ﬁeld of me dical image\nanalysis.\nKEYWORDS\nmedical image segmentation, attention mechanism, Vision T ransformer, lung nodule,\ntumor\nFrontiers in Medicine /zero.tnum/one.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\n/one.tnum. Introduction\nThe segmentation of medical images plays a crucial role in\nthe analysis of medical imaging, involving the separation and\nlabeling of distinct regions within such images (\n1). This technology\nholds signiﬁcant importance in various medical applications, such\nas tumor diagnosis (\n2). Medical image segmentation techniques\nassist physicians in precisely identifying tumor regions, measuring\ntumor dimensions and shape, and oﬀering valuable insights for\ndevising treatment strategies (\n3, 4). In recent times, the fusion of\ncomputer-aided detection (CAD) and deep learning has emerged\nas a prominent area of research in medical image segmentation\n(\n5). This is due to the following advantages: medical image\nsegmentation tasks can extract speciﬁc regions for quantitative\nanalysis and calculation, providing more objective and accurate\nresults for diagnosis and treatment, which in turn improves patient\ntreatment results; medical image segmentation algorithms can\nprovide accurate references for doctors’ diagnosis and treatment\ndecisions, reducing their workload and shortening the time for\ndiagnosis and treatment; medical image segmentation topics are\ncomputer medical image segmentation is a hot topic in the\nﬁeld of computer vision technology, and the study of medical\nimage segmentation algorithms can promote the development\nand application of artiﬁcial intelligence technology in the ﬁeld of\nmedicine and provide reference and reference for the application\nin other ﬁelds. Machine learning uses algorithmic models that allow\ncomputers to learn the context of visual data on their own (\n6).\nThe current mainstream medical image segmentation\nmodels are mainly segmentation models, and most of the\nexisting models adopt the structure of CNN as the main\nframework, thanks to two inductive biases: local correlation\nand weight sharing, which are assumptions that can help\nthe network to learn and generalize eﬀectively in the case of\ninsuﬃcient training data or high noise (\n7). Although CNN-\nbased approaches perform well in tasks in computer vision,\nthe bias assumption of convolutional operations also limits the\nperformance of the model in learning remote dependencies\nto local perceptual ﬁelds (\n8), thus losing the possibility of\ncapturing long-range feature associations and not being ﬂexible\nenough to adapt to image inputs of diﬀerent sizes, shapes,\nand textures, leading to information loss and model instability\n(\n9–12).\nVaswani et al. ( 13) proposed a new convolution-independent\nmodel, Transformer, in which the traditional CNN and RNN are\ndiscarded and the entire network structure is composed entirely\nof Attention mechanisms. More precisely, Transformer consists\nof and only consists of Self-Attention and Feed Forward Neural\nNetwork. A trainable neural network based on Transformer can\nbe built by stacking Transformers, and the authors’ experiments\nwere conducted by building Encoder-Decoder with 6 layers each\nof encoder and decoder, totaling 12 layers, and achieved a new\nhigh BLEU worth in machine translation. The use of attentional\nmechanisms and multiscale analysis methods are widely used\nin the ﬁeld of medical analysis as well as in other ﬁelds (\n14–\n16). Dosovitskiy et al. ( 17) proposed ViT that divides the input\nimage into multiple patches, and then projects each patch into\na ﬁxed-length vector to be fed into the Transformer, with the\nsubsequent encoder operations identical to those in the original\nTransformer. Liu et al. (\n18) proposed a Swin Transformer with\na hierarchical design that includes sliding window operations, in\nresponse to the problem that Transformer’s computation based\non global self-attention leads to a large amount of computation.\nThe sliding window operation includes a non-overlapping local\nwindow, and an overlapping cross-window. Limiting the attention\ncomputation to a single window introduces the localization of\nthe CNN convolution operation on the one hand, and saves\ncomputation on the other (\n19).\nInspired by the attention mechanism in natural language\nprocessing (13, 17, 18), existing studies use the Transformer, a non-\nlocal neural network, to overcome this limitation, which can model\nremote dependencies in sequence-to-sequence tasks, capturing\nrelationships between arbitrary positions in a sequence (\n20). The\nTransformer structure is proposed based only on the self-attention\nmechanism, completely eliminating the convolutional structure,\nand is powerful in modeling global context is powerful, and\nseveral studies have shown that Transformer-based frameworks\nalso achieve state-of-the-art performance on a variety of computer\nvision tasks (\n21). However, the self-attentiveness in Transformer\nrequires large computation and memory consumption when\ndealing with long sequences, and the sparse nature of medical image\ndata makes the model prone to overﬁtting during the training\nsession, which hinders the application of Transformer in medical\nimage segmentation tasks, which has been tried and tested in the\nﬁeld of natural image processing (\n22, 23). To reduce the number\nof computational parameters, we refer to the approach in Swin\nTransformer, which uses two layers of attention structures with a\nhierarchical design, including a non-overlapping local window, and\nan overlapping cross-window (\n18).\nOur main contributions are:\n• We propose a medical image segmentation network SW-UNet\nbased on a hybrid CNN-ViT architecture;\n• We design a Transformer module with a sliding window\ndesign to overcome the lack of interaction between diﬀerent\nregions of conventional convolutional operations and reduce\nthe number of stacked modules by widening the self-\nattentive vector dimension to the eﬀect of reducing the model\nparameters is achieved;\n• Validating the eﬀect of our model on lung nodule dataset\nLUNA16 and other tumor datasets, the model yields consistent\nimprovements over many baselines.\n/two.tnum. Method\nIn this paper, we design a CNN-ViT based medical image\nhybrid segmentation network SW-UNet, taking into account the\nstrengths of both. First, the CNN is able to rapidly compress\nthe number of input feature image pixels in the downsampling\nphase as a way to reduce the computational cost of the whole\nmodel, which allows the model to be trained and inferred faster.\nSecond, ViT achieves long-range sequence modeling through a\nself-attentive mechanism, which can tap the degree of association\nbetween arbitrary pixel points. This global perception capability can\nhelp the model better understand the overall structure of the image,\nrather than just segmenting local regions. For some images with\nFrontiers in Medicine /zero.tnum/two.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nstrong global structure features, ViT can outperform the traditional\nCNN model. the design of SW-UNet combines the advantages of\nCNN and ViT, making full use of the features of both models,\nthus enabling the model to better adapt to diﬀerent medical image\ndatasets. The structure of the model is shown in\nFigure 1.\n/two.tnum./one.tnum. Encoders and decoders\nThe architecture of the SW-UNet model is based on the\nencoder-decoder structure of U-Net. Initially, the input image F ∈\nRC×H×W undergoes a 3 × 3 convolution operation, followed by\nthree consecutive downsampling steps using convolution with a\nstride of 2. This downsampling process aims to reduce the size of\nthe feature map before inputting it into the ﬁnal ViT. Our input is\na single-channel 1 × 128 × 128 CT image map of the lungs, and\nboth the encoder and decoder have a three-layer structure, with\neach layer consisting of two consecutive convolutions; the pooling\nlayer in the encoder reduces the image height and width by half,\nand the transposed convolution in the decoder doubles the image\nsize.\nBy employing this encoding step prior to inputting into ViT, the\nSW-UNet model eﬃciently extracts local image features, allowing\nfor eﬀective utilization of computational resources in the early\nstages of the model. The resulting feature map is then fed into the\nSliding Window Transformer Block (SWTB) for further operations.\nOn the decoder side, a similar structure to the encoder is\nemployed. Each layer is upsampled using two successive 3 ×\n3 deconvolutions, and the encoder features are fused with the\ndecoder features at each layer through skip connections. This\nfusion enables the SW-UNet model to capture ﬁner spatial details\nin the segmentation process.\n/two.tnum./two.tnum. Sliding window transformer block\nSince the computational complexity of the ViT structure is\nproportional to the square of the input sequence length, it is\nimpractical to directly tile the input image into a sequence as\nthe input to the model. Therefore, the feature map needs to be\nsegmented into ﬁxed-size chunks as the sequence input before it\nis passed into ViT. For the feature map output at the encoding side,\nwe design a feature expansion module to make the dimensionality\nof the feature map meet the input requirements of ViT. First, the\nnumber of channels of the feature map is expanded from 64 by\nthe convolution operation, and the two dimensions of H and W\nare combined into the same dimension to ﬁnally obtain the input\nsequence z0 ∈ Rd×N of SWTB, where N = H\n4 × W\n4 is the number\nof elements corresponding to the incoming sequence in ViT.\nSpeciﬁcally, the computation of attention is accomplished in\ntwo stages: in the ﬁrst stage, the computation of attention is\nperformed inside the regular non-overlapping window, and in the\nsecond stage, the computation of attention is performed in the new\nwindow obtained by shifting the ﬁrst layer of the window to the\nright by a distance of half of the window’s width, i.e., the sliding\nwindow operation. The diﬀerence in complexity between the self-\nattention computation without the sliding window operation and\nthe self-attention computation with the sliding window operation\nis shown in Equations (1) and (2), respectively, where H, W, and C\nrepresent the height, width, and number of channels of the feature\nmap, respectively, and M represents the length of the rectangular\nwindow edge in terms of feature points as a preset unit. For the\nexperimental setup in this chapter, the computation of the single\nself-attention structure is reduced from 800 million reduced to 270\nmillion, which eﬀectively relieves the computational pressure of the\nmodel as a whole.\n/Omega1 (W − MSA) = 4HWC2 + 2H2W2C (1)\n/Omega1 (SW − MSA) = 4HWC2 + 2M2HWC (2)\nIn order to alleviate the problem of large computational eﬀort\nof traditional ViT in computing global attention of images, we\ndesign SWTB with sliding window operation and layer design\nto replace the global multi-headed self-attention mechanism, to\nsave computational eﬀort by limiting the attention computation\nto a window of ﬁxed size, so that the computational complexity\ngrows linearly instead of squarely with the image size, and to\nallow correlation between windows so as to achieve The ﬁrst\nlayer performs the multi-headed self-attentive computation inside\na regular non-overlapping window, and the second layer performs\nthe multi-headed self-attentive computation in a new window\nobtained by shifting the ﬁrst window down to the right by a distance\nof half the window width, which is also known as the sliding\nwindow operation. Speciﬁcally, the spatial dimension of the input\nsequence z0 is N = 32 × 32. To facilitate the edge operation, we\nset the window size to M = 4 × 4 and the window is panned\ndown to the right by 2 pixel points at a time. This is shown in\nFigure 2, where the letters represent diﬀerent feature sequences\nand the colors represent the range of receptive ﬁelds for diﬀerent\nwindows.\nSince only half a window distance is shifted, a feature map of\nhalf a window width will appear at the edge of the whole feature\nmap, if this part is directly discarded it will result in the loss of\nfeature information, and if this part is retained it will introduce\nadditional computation, both of which are not very desirable. For\nthis reason, this section proposes a sliding window operation with\ncyclic shifting, where the extra part is added to other small windows\nto make them complete windows, as shown in\nFigure 3. This cyclic\nshifting brings a new problem: for some windows, some of their\nfeature blocks are moved from other positions, and these feature\nblocks are subject to self-attention computation with originally\nnon-adjacent feature blocks, so that the values obtained from this\ncomputation lack practical signiﬁcance, and are only operated in\nthis way in order to keep the number of windows the same as the\noriginal. In order to block out the interference of these pseudo-\nvalues, three diﬀerent types of masks are designed here to be added\nwith the results of the self-attention calculation.\nMany previous investigations have utilized multiple sets of\nTransformer modules with numerous self-attentive mechanisms\ncomputed sequentially to enhance their eﬃcacy. While this\napproach can eﬀectively enhance the model’s performance, it\nalso introduces a substantial number of parameters, which we\nFrontiers in Medicine /zero.tnum/three.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nFIGURE /one.tnum\nThe structure of the model.\nbelieve have more adverse eﬀects than positive advantages for\nmedical image segmentation tasks. To mitigate the proliferation\nof parameters associated with stacking Transformer modules,\nwe reduce the depth of the SWTB while increasing its width.\nSpeciﬁcally, we augment the width of ViT using the Feature\nExpansion module before input, and we expand the hidden layer\ndimension of Q and K to dm = Ed by increasing the rate E while\nkeeping the dimension of V unchanged. Within the SWTB, the\nself-attention computation for each window follows the equations\nshown in Equations (3) and (4). In these equations, Wq ∈ Rd×dm ,\nWk ∈ Rd×dm , and Wv ∈ Rd×d represent the parameter matrices,\nand the features xi ∈ RM2×d of each window are multiplied\nwith these parameter matrices to yield three attention vectors:\nQ ∈ RM2×dm , K ∈ RM2×dm , and V ∈ RM2×d. To incorporate\npositional information in the self-attention within the window,\nthe calculation integrates a learnable relative positional deviation\nB ∈ RM2×M2\n. Here, we set the number of groups for the multi-\nheaded self-attention mechanism to 4. The features obtained from\nthe ﬁnal computation are concatenated, and the information is\nfurther consolidated through an additional global self-attention\nlayer.\n[Q; K; V] = [Wq; Wk; Wv] ·xi (3)\nAttention(Q, K, V) = Softmax( QKT\n√\nd\n+ B)V (4)\nEquations (5), (6), (7), and (8) depict the equations pertaining\nto the entirety of the ViT segment. Here, the notation LN\ndenotes layer normalization, and FFN represents the output of the\nwindow multi-headed self-attention mechanism, as well as serving\nas the input for the sliding window multi-headed self-attention\nmechanism. The sliding window multi-headed self-attention\nmechanism part generates the output for further processing.\nz′\n0 = W − MSA(LN(z0)) + z0 (5)\nz1 = FFN(LN(z′\n0)) + z′\n0 (6)\nz′\n1 = SW − MSA(LN(z1)) + z1 (7)\nz2 = FFN(LN(z′\n1)) + z′\n1 (8)\nAfter the feature map has gone through SWTB, the model\nhas learned enough image information. In order to achieve a skip\nconnection and keep the feature map in the same number of layers\nwith the same size in the encoder, two consecutive convolution\noperations are used here to reshape its dimension to size d× H\n4 × W\n4 .\nAfter the above operations, the feature map has the same shape as\nthe output F at the encoder side.\n/three.tnum. Experiments\n/three.tnum./one.tnum. Datasets\nWe use the segmentation results of the algorithm on the\nLUNA16 lung nodule dataset ( 24) to evaluate the performance\nof the SW-UNet model proposed in this section. To verify the\ngeneral applicability of the model to medical images, we conduct\nexperiments on two other tumor datasets, the LiTS 2017 (\n25) liver\ntumor dataset and the KiTS 2019 ( 26) kidney tumor dataset. The\ndetails of the three datasets are described below.\n/three.tnum./one.tnum./one.tnum. Lung nodule dataset LUNA/one.tnum/six.tnum\nThe LUNA16 dataset is a subset of the largest public lung\nnodule dataset, LIDC-IDRI, whose main purpose is to perform\nautomatic detection and segmentation of lung cancer. 888 CT scans\nof the lung are included in the LUNA16 dataset, each containing 1–\n4 lesions, for a total of 1,186 lesions. The LUNA16 annotations are\npresented in terms of nodule location (x, y, and z-axis coordinates),\nand the original image size is 512 × 512.\nFrontiers in Medicine /zero.tnum/four.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nFIGURE /two.tnum\nThe shifting window operation and feature shift operation.\nFIGURE /three.tnum\nThe mask type corresponding to feature shift operation.\nTABLE /one.tnumSegmentation results of diﬀerent models on LUNA /one.tnum/six.tnum.\nMethod Accuracy Dice Sensitivity Speciﬁcity H/nine.tnum/five.tnum\nTransUNet 0.98 0.79 0.80 0.98 13.46\nTransAttUnet 0.99 0.81 0.82 0.98 9.12\nTransBTS 0.99 0.83 0.81 0.99 7.55\nSW-UNet 0.99 0.84 0.82 0.99 7.41\nThe bolded ones are the best results.\nFrontiers in Medicine /zero.tnum/five.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nTABLE /two.tnumSegmentation results of diﬀerent models on LiTS /two.tnum/zero.tnum/one.tnum/seven.tnum.\nMethod Accuracy Dice Sensitivity Speciﬁcity H/nine.tnum/five.tnum\nTransUNet 0.97 0.58 0.60 0.98 13.83\nTransAttUnet 0.98 0.63 0.62 0.98 11.65\nTransBTS 0.98 0.62 0.62 0.99 9.76\nSW-UNet 0.99 0.66 0.65 0.99 9.13\nThe bolded ones are the best results.\nTABLE /three.tnumSegmentation results of diﬀerent models on KiTS /two.tnum/zero.tnum/one.tnum/nine.tnum.\nMethod Accuracy Dice Sensitivity Speciﬁcity H/nine.tnum/five.tnum\nTransUNet 0.98 0.77 0.75 0.98 11.34\nTransAttUnet 0.98 0.80 0.78 0.98 10.19\nTransBTS 0.99 0.80 0.77 0.98 8.85\nSW-UNet 0.99 0.82 0.81 0.99 8.26\nThe bolded ones are the best results.\nTABLE /four.tnumComparison of diﬀerent models.\nModel Vision\nTransformer-base\nTransUNet TransBTS SW-UNet\nParameters 86M 105M 43M 32M\nFLOPs 33.03G 1205.76G 333.09G 51.3G\nBolded values are the ones that achieve the best performance in the ne twork performance comparison.\n/three.tnum./one.tnum./two.tnum. Liver tumor segmentation dataset LiTS\n/two.tnum/zero.tnum/one.tnum/seven.tnum, kidney tumor segmentation challenge\ndataset KiTS /two.tnum/zero.tnum/one.tnum/nine.tnum\nThe LiTS 2017 dataset is provided by the Liver Tumor\nSegmentation (LiTS) Challenge 2017, which has 200 3D CT scan\nimages, including 130 images for model training and validation and\n70 images for objective model evaluation. Similar to LiTS 2017,\nKiTS 2019 is also provided by the Kidney Tumor Segmentation\n(KiTS) Challenge 2019. The dataset has a total of 300 3D CT scans\ncontaining 210 images for model training and validation and 90\nimages for objective model evaluation, with image sizes of 512 ×512\npixels for both datasets.\n/three.tnum./two.tnum. Model evaluation metrics\nIn this paper, the evaluation of the model is mainly done using\nthe Dice coeﬃcient and the Hausdorﬀ 95 (H95) distance, etc. The\nDice coeﬃcient is calculated as shown in Equation (9). The Dice\ncoeﬃcient measures the similarity between the predicted and true\nresults, and its value ranges from 0 to 1. The closer the value is to\n1, the higher the similarity between the predicted and true results,\nand the better the performance of the model. On the contrary, the\ncloser the value is to 0, the lower the similarity between predicted\nand true results, and the worse the performance of the model.\nDice = 2TP\n2TP + FP + FN (9)\nThe Hausdorﬀ distance is calculated as shown in Equation 10,\nwhich measures the similarity between two sets, and is calculated\nby calculating the minimum value of the distance from each\npoint in set X to set Y, and then taking the maximum value\nof these minimum values as the Hausdorﬀ distance, which is\ncommonly used in medical image segmentation to measure the\ndiﬀerence between the segmentation result of the model and the\nreal segmentation result. The smaller the H95 value, the smaller\nthe diﬀerence between the segmentation result of the model and\nthe real segmentation result, and the better the segmentation\nperformance of the model.\nHausdorﬀ = max{d(X, Y), d(Y, X)} (10)\n/three.tnum./three.tnum. Data pre-processing and experimental\nparameter setting\nIn order to make the data better suited to the network structure\nproposed in this work, a preprocessing operation is required on\nthe data. On the three datasets, the range of pixel values saved\nin their raw data varies due to diﬀerent criteria. In order to have\nthe same distribution of grayscale values for each image in the\ntraining set, pixel value normalization of the input data is necessary.\nSpeciﬁcally, for the background and content-containing pixels in\nmedical images, MinMax normalization is applied in each image,\ncalculated as shown in Equation (11). After performing the above\noperation, the pixel values are all distributed between 0 and 1.\nFor the LUNA 16 dataset, because of its special annotation form,\nFrontiers in Medicine /zero.tnum/six.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nFIGURE /four.tnum\nVisualization of segmentation results for LUNA/one.tnum/six.tnum dataset.\nwe ﬁrst generate the corresponding masks indicating the nodal\nlocations using the annotations and sliced them to ﬁt our 2D\nnetwork model. Since it contains large non-nodal regions, we crop\nthe image according to the nodal locations and selected an image of\n128 × 128 size including all parts of the nodes. Similarly, in order to\nkeep the image size of the incoming model consistent, we uniformly\nadjust the images of the LiTS 2017 dataset and KiTS 2019 dataset\nto 128 × 128 by cropping or resampling means to ﬁt the network\nparameters.\nx′\ni = xi − min(xi)\nmax(xi) − min(xi) (11)\nWe use the deep learning library Pytorch to build the model\nand to train it. In the training session, we use an initial number of\n16 convolutional kernels and set each batch to 48. We use Adam as\nthe optimizer with a learning rate of 0.00001 and end the training\nwith convergence after 100 rounds with no decrease in the loss\nvalue on the validation set. For each dataset, we divide the data\nthat can participate in training into a training set and a validation\nset, where the training set accounts for 80%, the validation set\naccounts for 10%, and the remaining 10% is used as a test set\nto evaluate the model. Our loss function continues to use the\ncombined loss of Dice loss and Focal loss, where the weight α\nof Dice loss is set to 0.8 and the weight β of Focal loss is set to\n0.2.\nLDice = 1 − 2 ∑ N\ni yiy′\ni + ε\n∑ N\ni yi + ∑ N\ni y′\ni + ε\n(12)\nLFocal =\nN∑\ni\n(−yi(1 − y′\ni)γ logy′\ni − (1 − yi)y′γ\ni log(1 − y′\ni)) (13)\nLSeg = αLDice + βLFocal (14)\n/three.tnum./four.tnum. Results\nTo verify the performance of SW-UNet, we compare its\nsegmentation results with those of TransUNet (\n27), TransAttnet\n(28), and TransBTS ( 29), and Tables 1–3 lists their performance\nFrontiers in Medicine /zero.tnum/seven.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nFIGURE /five.tnum\nVisualization of segmentation results for LiTS /two.tnum/zero.tnum/one.tnum/seven.tnum dataset.\nwhen running on LUNA16, LiTS 2017, and KiTS 2019. It can\nbe seen that SW-UNet achieves optimal performance in all two\nmetrics, Dice coeﬃcient and H95 distance. In the Dice coeﬃcient\nscore, SW-UNet has a small lead of about 1-5 percentage points\non all three datasets, and it can be said that SW-UNet is more\naccurate and clearer for the segmentation eﬀect of the internal\nstructure of the model. In the evaluation index of H95 distance,\nSW-UNet achieves 7.41, 9.13, and 8.26 scores, respectively,\nwhich also achieves the best, which indicates that SW-UNet is\nalso very accurate in dividing the prominent part of the edge\nregion.\nTo make the model suitable for medical image segmentation\ntasks, we reduce the number of model parameters as much as\npossible so that it can be trained eﬃciently without overﬁtting\nproblems even with only a small amount of data. As shown in\nTable 4, we count the number of parameters for several models with\nTransformer structure. Among them, the number of parameters\nof SW-UNet is only 32M, which is 63% lower than the 86M\nparameters of ViT-Base, 70% lower than the 105M parameters of\nTransUNet, and 25% lower than the 43M parameters of TransBTS.\nIt can be said that SW-UNet is a structure specialized for medical\nimage segmentation tasks, and for those diseases with distinct\ngeographical features, each hospital has the ability to label a\nsmall dataset by itself for training without acquiring huge amount\nof data.\n/three.tnum./five.tnum. Visualization analysis\nTo visually evaluate the model performance, we randomly\nselect four sets of segmentation results of TransUNet,\nTransAttUnet, TransBTS and SW-UNet on three datasets,\nLUNA 16, LiTS 2017, and KiTS 2019, as a display. As shown in\nFigure 4, the black part of the LUNA 16 segmentation results\nis the background and the white part is the segmentation\nresults. It can be seen that part of the segmentation results of\nTransUNet has deviations in positioning, segmenting parts\nthat originally do not belong to the label, and there are also\nerrors in the division and positioning of the interior of the\nsegmented region. TransAttUnet is not accurate enough in\nportraying the shape of the segmented region, and also has\nthe problem of incorrectly identifying the background as\nthe segmented region. TransBTS has a fair performance in\nsegmenting the smoother trend of the The segmentation\nperformance of TransBTS is fair at the edges, but it cannot\nFrontiers in Medicine /zero.tnum/eight.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nFIGURE /six.tnum\nVisualization of segmentation results for KiTS /two.tnum/zero.tnum/one.tnum/nine.tnum dataset.\ndetect protrusions and small fragmented areas. In contrast,\nSW-UNet is more accurate in shape description, and its\nlocalization performance and anti-interference ability are\nalso more outstanding.\nThe images of the LiTS 2017 and KiTS 2019 datasets have many\nsimilarities, mainly in that the tumor regions in both datasets are\nvery small compared to the organ parts. Therefore, it is not too\ndiﬃcult for the model to segment out the organ part, while it is\nmore diﬃcult to precisely locate the tumor region. The purple part\nof the segmentation results of both datasets is the background,\nthe green part is the organ segmentation results, and the yellow\npart is the tumor segmentation results. From\nFigures 5, 6, we can\nsee that TransUNet can only segment the organ part in many\nsamples, but cannot perceive the tumor region, especially in those\nLiTS samples with low contrast of the original image. TransBTS is\ncloser to the real value in organ segmentation, but the recognition\nperformance of tumor region is slightly inferior to that of SW-\nUNet. The segmentation results of both organs and tumor regions\nare very accurate.\nTaken together, our proposed SW-UNet model is more accurate\nfor the internal details of the segmented region, and the boundary\nalignment of the segmentation is clearer, which can better capture\nthe minute structures and changes in the images and improve\nthe accuracy of the segmentation. Due to the complexity of\nmedical image structure and artifact interference, general models\nmay have incorrect segmentation. The segmentation results on\nthree important human organs, lung, kidney, and liver, prove\nthat the SW-UNet model has strong generalization and good\nrobustness, and can be migrated to other data sets for training\nand specialization to segmentation models for speciﬁc organs or\nspeciﬁc disease types, thus helping This will help doctors to better\nunderstand and diagnose patients’ conditions.\n/three.tnum./six.tnum. Ablation experiments\nTo verify the improvement of SWTB on model segmentation\nperformance, we design ablation experiments to test the impact\nof SWTB on overcoming the bias assumption of the CNN\narchitecture, and the control group replaced the sliding window\nmodule in SWTB with a non-sliding module. As can be seen\nin\nTable 5, the models with the sliding window mechanism\nadded are generally higher in Dice coeﬃcient scores by more\nthan 5 percentage points compared to the models without the\nsliding window mechanism on the three diﬀerent datasets, which\nFrontiers in Medicine /zero.tnum/nine.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nTABLE /five.tnumThe ablation study results of sliding window operation.\nDataset Dice H/nine.tnum/five.tnum\nNo SW With SW No SW With SW\nLUNA16 0.79 0.84 21.38 7.41\nLiTS 2017 0.59 0.66 12.87 9.13\nKiTS 2019 0.75 0.82 15.42 8.26\nTABLE /six.tnumThe ablation study results of CNN downsampling frequency.\nDataset\nDice H/nine.tnum/five.tnum\n/one.tnum /two.tnum /three.tnum /one.tnum /two.tnum /three.tnum\nLUNA16 0.79 0.84 0.81 11.03 7.41 8.97\nLiTS 2017 0.58 0.66 0.65 14.62 9.13 8.78\nKiTS 2019 0.76 0.82 0.80 15.18 8.26 9.44\nBolded values are those that achieved the best performance in the down sampling count comparison.\nindicates that the association of perceptual ﬁelds established\nbetween diﬀerent windows can bring positive eﬀects on the model\nsegmentation performance. The Transformer module, which is\nthe core operation to establish the association between diﬀerent\nperceptual ﬁelds, is no diﬀerent from the convolution operation in\nthe CNN framework, as both of them compute the features in part\nof the feature map, except that the CNN part can be optimized in\nterms of computational eﬃciency to better adapt to the training\nand inference needs of large-scale datasets. It can also be seen\nfrom\nTable 5 that the model with the sliding window mechanism\nhas smaller values of the H95 distance metric compared to the\nmodel without the sliding window mechanism, indicating that the\ncontours of the segmented regions are also better portrayed and\ncan adapt to the edges of the regions with diﬀerent scale size\nvariations.\nIn addition, we also experimentally investigate the optimal\nsolution for the number of CNN downsampling times to ﬁnd\nthe optimal placement of the encoder by varying the number of\ntimes the encoder is downsampled at the input SWTB, set to\ndownsample once, twice, and three times, respectively. As can\nbe seen in\nTable 6, the data after downsampling twice achieve\nthe highest scores in the Dice coeﬃcient evaluation index, which\nindicates that the combination of high-level semantic and low-\nlevel features after downsampling twice gives the best segmentation\neﬀect to the model for the input image. In addition, the data\nafter three downsampling are better and closer to the results after\ntwo downsampling than after one downsampling, and even better\nthan the model after two downsampling on the H95 evaluation\nmetric for the LiTS 2017 dataset, suggesting that the computation\non high-level semantics is more valuable compared to doing\nthe computation on global self-attentive mechanism on low-level\nfeatures. However, we believe that the conclusion may have some\nlimitations due to the fact that the size of the input image is ﬁxed\nat 128 × 128, which might have diﬀerent standard answers in\ndiﬀerent segmentation tasks if it is properly adjusted in the data\npreprocessing stage.\n/four.tnum. Conclusion\nThis paper presents the design ideas and experimental results\nof a lung nodule image segmentation model based on a hybrid\narchitecture of CNN and ViT. First, the universal problems of\nsegmentation networks based on CNN architecture are analyzed,\nand the negative eﬀects caused by their inductive bias are analyzed\nand solutions are given. Secondly, the detailed design process\nand implementation method of SW-UNet, the sliding window\nTransformer module, the medical image segmentation model\nproposed in this chapter are given. Finally, the eﬀectiveness of the\nSW-UNet model is veriﬁed by experiments on lung nodule dataset\nLUNA 16, and the general applicability of the model for medical\nimage segmentation is conﬁrmed on two other datasets.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nJM: Writing—original draft, Writing—review editing. GY:\nWriting—review and editing, Validation. CG: Writing—original\ndraft, Writing—review and editing. XG: Writing—review and\nediting, Investigation, Resources. MZ: Investigation, Validation,\nWriting—original draft, Writing—review and editing.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research, authorship, and/or publication of this article.\nFrontiers in Medicine /one.tnum/zero.tnum frontiersin.org\nMa et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/three.tnum/four.tnum/four.tnum/one.tnum\nConﬂict of interest\nJM was employed by Shenhua Hollysys Information\nTechnology Co., Ltd.\nThe remaining authors declare that the research was\nconducted in the absence of any commercial or ﬁnancial\nrelationships that could be construed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\n1. Gao W, Li X, Wang Y, Cai Y. Medical image segmentation algori thm\nfor three-dimensional multimodal using deep reinforcement learning and big\ndata analytics. Front Public Health. (2022) 10:879639. doi: 10.3389/fpubh.2022.\n879639\n2. Zhao L, Lin R, Liu Z, Yuan H. Predicting the likelihood of patie nts developing\nsepsis based on compound ensemble learning. In: 2022 IEEE International Conference\non Bioinformatics and Biomedicine (BIBM) . Las Vegas, NV (2022). p. 3235–41.\n3. He K, Lian C, Zhang B, Zhang X, Cao X, Nie D, et al.\nHF-UNet: learning hierarchically inter-task relevance in multi -\ntask U-Net for accurate prostate segmentation in CT images. IEEE\nTrans Med Imaging. (2021) 40:2118–28. doi: 10.1109/TMI.2021.3\n072956\n4. Jin Q, Meng Z, Sun C, Cui H, Su R. RA-UNet: a hybrid deep\nattention-aware network to extract liver and tumor in CT scan s.\nFront Bioeng Biotechnol . (2020) 8:605132. doi: 10.3389/fbioe.2020.\n605132\n5. Wang R, Lei T, Cui R, Zhang B, Meng H, Nandi AK. Medical image\nsegmentation using deep learning: a survey. IET Image Process. (2022) 16:1243–67.\ndoi: 10.1049/ipr2.12419\n6. Zhao L, Huang P , Chen T, Fu C, Hu Q, Zhang Y. Multi-sentence co mplementarily\ngeneration for text-to-image synthesis. IEEE Trans Multimedia. (2023) 1–10.\ndoi: 10.1109/TMM.2023.3297769\n7. Peng J, Wang Y. Medical image segmentation with limited supe rvision:\na review of deep network models. arXiv preprint arXiv:2103.00429 (2021).\ndoi: 10.48550/arXiv.2103.00429\n8. Wu M, Qian Y, Liao X, Wang Q, Heng PA. Hepatic vessel segmenta tion based\non 3D swin-transformer with inductive biased multi-head self -attention. BMC Med\nImaging. (2021) 23:91. doi: 10.1186/s12880-023-01045-y\n9. Dai D, Li Y, Wang Y, Bao H, Wang G. Rethinking the image featur e\nbiases exhibited by deep convolutional neural network models in image\nrecognition. CAAI Trans Intell Technol. (2022) 7:721–31. doi: 10.1049/cit2.\n12097\n10. Chai J, Zeng H, Li A, Ngai EWT. Deep learning in computer visi on: a critical\nreview of emerging techniques and application scenarios. Mach Learn Appl . (2021)\n6:100134. doi: 10.1016/j.mlwa.2021.100134\n11. Guo F, Ng M, Goubran M, Petersen SE, Piechnik SK, Neubauer S, et al.\nImproving cardiac MRI convolutional neural network segmenta tion on small training\ndatasets and dataset shift: a continuous kernel cut approach. Med Image Anal. (2020)\n61:101636. doi: 10.1016/j.media.2020.101636\n12. Liu L, Wang Y, Chang J, Zhang P , Liang G, Zhang H. LLRHNet: mu ltiple lesions\nsegmentation using local-long range features. Front Neuroinform. (2022) 16:859973.\ndoi: 10.3389/fninf.2022.859973\n13. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.\nAttention is all you need. In: Proceedings of the 31st International Conference on Neural\nInformation Processing Systems. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n(2017). p. 6000–10.\n14. Zou L, Huang Z, Yu X, Zheng J, Liu A, Lei M. Automatic detect ion of\ncongestive heart failure based on multiscale residual UNet++: from centralized\nlearning to federated learning. IEEE Trans Instrument Meas. (2023) 72:1–13.\ndoi: 10.1109/TIM.2022.3227955\n15. Zou L, Qiao J, Yu X, Chen X, Lei M. Intelligent proximate analys is of coal based\non near infrared spectroscopy and multi-output deep learning. IEEE Trans Artif Intell.\n(2023) 1–13. doi: 10.1109/TAI.2023.3296714\n16. Zhao L, Yang T, Zhang J, Chen Z, Yang Y, Wang ZJ. Co-learning non-negative\ncorrelated and uncorrelated features for multi-view data. IEEE Trans Neural Netw\nLearn Syst. (2021) 32:1486–96. doi: 10.1109/TNNLS.2020.2984810\n17. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T,\net al. An image is worth 16x16 words: transformers for image re cognition at scale. arxiv\npreprint arXiv:2010.11929. doi: 10.48550/arXiv.2010.11929\n18. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, et al. Swin transform er: hierarchical\nvision transformer using shifted windows. arxiv preprint arXiv:2103.14030 .\ndoi: 10.48550/arXiv.2103.14030\n19. Sun W, Chen J, Yan L, Lin J, Pang Y, Zhang G. COVID-19 CT image\nsegmentation method based on swin transformer. Front Physiol. (2022) 13:981463.\ndoi: 10.3389/fphys.2022.981463\n20. Yan S, Wang C, Chen W, Lyu J. Swin transformer-based GAN for\nmulti-modal medical image translation. Front Oncol. (2022) 12:942511.\ndoi: 10.3389/fonc.2022.942511\n21. Valanarasu JMJ, Oza P , Hacihaliloglu I, Patel VM. Medical tra nsformer:\nhierarchical vision transformer using shifted windows. In : de Bruijne M, Cattin PC,\nCotin S, Padoy N, Speidel S, Zheng Y, et al., editors. Medical Image Computing\nand Computer Assisted Intervention – MICCAI 2021 . Cham: Springer International\nPublishing (2021). p. 36–46.\n22. Ding Y, Jia M, Miao Q, Cao Y. A novel time–frequency Transfo rmer based on\nself–attention mechanism and its application in fault diagnosis of rolling bearings.\nMech Syst Signal Process . (2022) 168:108616. doi: 10.1016/j.ymssp.2021.108616\n23. Huang L, Zhu E, Chen L, Wang Z, Chai S, Zhang B. A transforme r-based\ngenerative adversarial network for brain tumor segmentati on. Front Neurosci. (2022)\n16:1054948. doi: 10.3389/fnins.2022.1054948\n24. Shukla VVK, Tanmisha M, Aluru R, Nagisetti B, Tumuluru P. Lun g nodule\ndetection through CT scan images and DNN models. In: 2021 6th International\nConference on Inventive Computation Technologies (ICICT) . Coimbatore (2021). p.\n962–7.\n25. Bilic P , Christ P , Li HB, Vorontsov E, Ben-Cohen A, Kaissis G, et al. The\nliver tumor segmentation benchmark (LiTS). Med Image Anal. (2023) 84:102680.\ndoi: 10.1016/j.media.2022.102680\n26. Heller N, Sathianathen NJ, Kalapara AA, Walczak E, Moore K, Kalu zniak H,\net al. The KiTS19 challenge data: 300 kidney tumor cases with clin ical context,\nCT semantic segmentations, and surgical outcomes. arXiv preprint arXiv:1904.00445 .\ndoi: 10.48550/arXiv.1904.00445\n27. Chen J, Lu Y, Yu Q, Luo X, Adeli E, Wang Y, et al. TransUNet: tra nsformers make\nstrong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 .\ndoi: 10.48550/arXiv.2102.04306\n28. Chen B, Liu Y, Zhang Z, Lu G, Zhang D. TransAttUnet: multi-lev el attention-\nguided U-Net with transformer for medical image segmentati on. arXiv preprint\narXiv:2107.05274. doi: 10.48550/arXiv.2107.05274\n29. Wang W, Chen C, Ding M, Yu H, Zha S, Li J. TransBTS: multimoda l brain tumor\nsegmentation using transformer. In: de Bruijne M, Cattin PC , Cotin S, Padoy N, Speidel\nS, Zheng Y, et al., editors. Medical Image Computing and Computer Assisted Intervention\n– MICCAI 2021 . Cham: Springer International Publishing (2021). p. 109–19.\nFrontiers in Medicine /one.tnum/one.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7183897495269775
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6864773631095886
    },
    {
      "name": "Segmentation",
      "score": 0.6453491449356079
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6121916770935059
    },
    {
      "name": "Image segmentation",
      "score": 0.5575545430183411
    },
    {
      "name": "Deep learning",
      "score": 0.5542994141578674
    },
    {
      "name": "Computer vision",
      "score": 0.44884076714515686
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.4432852864265442
    },
    {
      "name": "Segmentation-based object categorization",
      "score": 0.4425654411315918
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4423379600048065
    },
    {
      "name": "Medical imaging",
      "score": 0.4202271103858948
    }
  ]
}