{
  "title": "KALA: Knowledge-Augmented Language Model Adaptation",
  "url": "https://openalex.org/W4224863259",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2402425964",
      "name": "Min Ki Kang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3121372713",
      "name": "Jinheon Baek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2792568705",
      "name": "Sung Ju Hwang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2891113091",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2952179106",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3173169192",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W3200936406",
    "https://openalex.org/W3035129496",
    "https://openalex.org/W2603777577",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W3176523944",
    "https://openalex.org/W2760505947",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W2979736514",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2879390606",
    "https://openalex.org/W2951561177",
    "https://openalex.org/W4299518610",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W4285723986",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W2760103357",
    "https://openalex.org/W4381683870",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3169726359",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3186799149",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4286795917",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W4288419263",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W1512387364",
    "https://openalex.org/W1665214252"
  ],
  "abstract": "Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM's performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for PLM adaption, we propose a novel domain adaption framework for PLMs coined as Knowledge-Augmented Language model Adaptation (KALA), which modulates the intermediate hidden representations of PLMs with domain knowledge, consisting of entities and their relational facts. We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our KALA largely outperforms adaptive pre-training.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5144 - 5167\nJuly 10-15, 2022 Â©2022 Association for Computational Linguistics\nKALA: Knowledge-Augmented Language Model Adaptation\nMinki Kang1,2âˆ— Jinheon Baek1âˆ— Sung Ju Hwang1,2\nKAIST1, AITRICS2\n{zzxc1133, jinheon.baek, sjhwang82}@kaist.ac.kr\nAbstract\nPre-trained language models (PLMs) have\nachieved remarkable success on various nat-\nural language understanding tasks. Sim-\nple ï¬ne-tuning of PLMs, on the other hand,\nmight be suboptimal for domain-speciï¬c tasks\nbecause they cannot possibly cover knowl-\nedge from all domains. While adaptive\npre-training of PLMs can help them obtain\ndomain-speciï¬c knowledge, it requires a large\ntraining cost. Moreover, adaptive pre-training\ncan harm the PLMâ€™s performance on the\ndownstream task by causing catastrophic for-\ngetting of its general knowledge. To over-\ncome such limitations of adaptive pre-training\nfor PLM adaption, we propose a novel do-\nmain adaption framework for PLMs coined\nas Knowledge-Augmented Language model\nAdaptation (KALA), which modulates the in-\ntermediate hidden representations of PLMs\nwith domain knowledge, consisting of entities\nand their relational facts. We validate the per-\nformance of our KALA on question answer-\ning and named entity recognition tasks on mul-\ntiple datasets across various domains. The\nresults show that, despite being computation-\nally efï¬cient, our KALA largely outperforms\nadaptive pre-training. Code is available at:\nhttps://github.com/Nardien/KALA.\n1 Introduction\nPre-trained Language Models (PLMs) (Devlin\net al., 2019; Brown et al., 2020) have shown to\nbe effective on various Natural Language Under-\nstanding (NLU) tasks. Although PLMs aim to ad-\ndress diverse downstream tasks from various data\nsources, there have been considerable efforts to\nadapt the PLMs to speciï¬c domains â€”distributions\nover the language characterizing a given topic or\ngenre (Gururangan et al., 2020)â€” for which the ac-\nquisition of domain knowledge is required to accu-\nrately solve the downstream tasks (e.g., Biomedical\nNamed Entity Recognition (Dogan et al., 2014)).\nâˆ—* Equal contribution\n9 10 11 12\nTraining FLOPs (1016)\n66.5\n67.0\n67.5\n68.0\n68.5F1 score\nKALA (Ours)\nTask-Adaptive\nPre-Training (TAPT)\nFine-tuning\nincreasing \nmemory size\nincreasing \ntraining steps\n182 183\nDomain-Adaptive\nPre-Training (DAPT)\nFine-tuning\nTAPT\nDAPT\nKALA (ours)\nFigure 1: F1 Score and Training FLOPs for different methods\non Question Answering (NewsQA). Note that DAPT uses\nabout 112 times larger data for adaptation. Details are in Â§5.3\nThis problem, known as Language Model Adap-\ntation, can be viewed as a transfer learning prob-\nlem (Yosinski et al., 2014; Ruder, 2019) under\ndomain shift, where the model is pre-trained on\nthe general domain and the labeled distribution is\navailable for the target domain-speciï¬c task. The\nmost prevalent approach to this problem is adaptive\npre-training (Figure 2a) which further updates all\nparameters of the PLM on a large domain-speciï¬c\nor curated task-speciï¬c corpus, with the same pre-\ntraining strategy (e.g., masked language modeling)\nbefore ï¬ne-tuning it on the downstream task (Belt-\nagy et al., 2019; Lee et al., 2020; Gururangan et al.,\n2020). This continual pre-training of a PLM on the\ntarget domain corpus allows it to learn the distri-\nbution of the target domain, resulting in improved\nperformance on domain-speciï¬c tasks (Howard and\nRuder, 2018; Han and Eisenstein, 2019).\nWhile it has shown to be effective, adaptive pre-\ntraining has obvious drawbacks. First, it is com-\nputationally inefï¬cient. Although a PLM becomes\nmore powerful with the increasing amount of pre-\ntraining data (Gururangan et al., 2020), further\npre-training on the additional data requires larger\nmemory and computational cost as the dataset size\ngrows (Bai et al., 2021). Besides, it is difï¬cult to\nadapt the PLM to a new domain without forgetting\nthe general knowledge it obtained from the initial\npretraining step, since all pre-trained parameters\nare continually updated to ï¬t the domain-speciï¬c\ncorpus during adaptive pre-training (Chen et al.,\n2020). This catastrophic forgetting of the task-\n5144\nContext: non-ST-elevation myocardial \ninfarction, 100% RCA, three stents, 50% \nmid LD. 2017-04-02 instent restenosis\nstatus post brachytherapy. (â€¦) her short-\nness of breath and asthma flare, tentracyc-\nline, sulfa, Demerol. MEDICATIONS (â€¦) \nNewsNews\nâ€¦\nRecently, sirolimus-eluting coronary stents \nhave been shown to reduce restenosis and \nadditional adverse cardiac events in patie-\nnts with severe coronary artery disease (â€¦)\nDomain Corpus (e.g. Medical Textbook)\nQuestion: Did the patient receive \nbrachytherapies for instent restenosis?\n(Unseen data not in training dataset)\nPre-trained LM\nParameter Update\nKALA\nPre-trained LM\nTransformation\n(a) Adaptive\nPre-training\nFine-\nTuning Test\n(b)\nKnowledge GraphEntity Memory\nmyocardial\n_infarction\nasthma\npethidine\n. . .\nAdaptive Pre-training(a)\nAdapted LM\nFeature of\nrestenosis\n(Feature\nSpace)\nmyocardial..\npethidine\nasthma\nrestenosis\nrestenosis\nDAPT\nseen\nunseen\nKALA (Ours)\nFigure 2: Concepts (Left). (a) Adaptive Pre-training updates whole parameters of the PLM through further pre-training on the\ndomain corpus. (b) Our method KALA integrates the external knowledge so that the PLM adapts to the target domain only with\nï¬ne-tuning, which is realized by the afï¬ne transformation on the intermediate feature. Visualization of the contextualized\nrepresentation from the PLM for seen and unseen entities (Right).Our KALA framework embeds the unseen entities on the\nembedding space of seen entities by representing them with their relational knowledge over the graph, while the strong DAPT\nbaseline (Gururangan et al., 2020) cannot appropriately handle unseen entities that are not given for task ï¬ne-tuning.\ngeneral knowledge may lead to the performance\ndegradation on the downstream tasks. In Figure 1,\nwe show that adaptive pre-training with more train-\ning steps could lead to performance degeneration.\nThus, it would be preferable if we could adapt\nthe PLM to the domain-speciï¬c task without costly\nadaptive pre-training. To this end, we aim to inte-\ngrate the domain-speciï¬c knowledge into the PLM\ndirectly during the task-speciï¬c ï¬ne-tuning step,\nas shown in Figure 2b, eliminating the adaptive\npre-training stage. Speciï¬cally, we ï¬rst note that\nentities and relations are core building blocks of\nthe domain-speciï¬c knowledge that are required\nto solve for the domain-speciï¬c downstream tasks.\nClinical domain experts, for example, are familiar\nwith medical terminologies and their complex re-\nlations. Then, to represent the domain knowledge\nconsisting of entities and relations, we introduce\nthe Entity Memory, which is the source of entity\nembeddings but independent of the PLM parame-\nters (See Entity Memory in Figure 2b). Then, we\nfurther exploit the relational structures of the enti-\nties by utilizing a Knowledge Graph (KG), which\ndenotes the factual relationships between entities,\nas shown in Knowledge Graph of Figure 2b.\nThe remaining step is how to integrate the knowl-\nedge into the PLM during ï¬ne-tuning. To this\nend, we propose a novel layer named Knowledge-\nconditioned Feature Modulation (KFM, Â§3.2),\nwhich scales and shifts the intermediate hidden rep-\nresentations of PLMs by conditioning them with\nretrieved knowledge representations. This knowl-\nedge integration scheme has several advantages.\nFirst, it does not modify the original PLM architec-\nture, and thus could be integrated into any PLMs\nregardless of their architectures. Also, it only re-\nquires marginal computational and memory over-\nhead, while eliminating the need of excessive fur-\nther pre-training (Figure 1). Finally, it can effec-\ntively handle unseen entities with relational knowl-\nedge from the KG, which are suboptimally em-\nbedded by adaptive pre-training. For example, as\nshown in Figure 2, an entity restenosis does not\nappear in the training dataset for ï¬ne-tuning, thus\nadaptive pre-training only implicitly infers them\nwithin the context from the broad domain corpus.\nHowever, we can explicitly represent the unknown\nentity by aggregating the representations of known\nentities in the entity memory (i.e., in Figure 2,\nneighboring entities, such as asthma and pethidine,\nare used to represent the unseen entity restenosis).\nWe combine all the previously described com-\nponents into a novel language model adapta-\ntion framework, coined as Knowledge-Augmented\nLanguage model Adaptation (KALA) (Figure 3).\nWe empirically verify that KALA improves the\nperformance of the PLM over adaptive pre-training\non various domains with two knowledge-intensive\ntasks: Question Answering (QA) and Named Entity\nRecognition (NER). Our contribution is threefold:\nâ€¢ We propose a novel LM adaptation framework,\nwhich augments PLMs with entities and their re-\nlations from the target domain, during ï¬ne-tuning\nwithout any further pre-training. To our knowl-\nedge, this is the ï¬rst work that utilizes the struc-\ntured knowledge for language model adaptation.\nâ€¢ To reï¬‚ect structural knowledge into the PLM, we\nintroduce a novel layer which scales and shifts\nthe intermediate PLM representations with the\nentity representations contextualized by their re-\nlated entities according to the KG.\n5145\nâ€¢ We show that our KALA signiï¬cantly enhances\nthe modelâ€™s performance on domain-speciï¬c QA\nand NER tasks, while being signiï¬cantly more\nefï¬cient over existing LM adaptation methods.\n2 Related Work\nLanguage Model Adaptation Nowadays, trans-\nfer learning (Howard and Ruder, 2018) is a dom-\ninant approach for solving Natural Language Un-\nderstanding (NLU) tasks. This strategy ï¬rst pre-\ntrains a Language Model (LM) on a large and un-\nlabeled corpus, then ï¬ne-tunes it on downstream\ntasks with labeled data (Devlin et al., 2019). While\nthis scheme alone achieves impressive performance\non various NLU tasks, adaptive pre-training of the\nPLM on a domain-speciï¬c corpus helps the PLM\nachieve better performance on the domain-speciï¬c\ntasks. For example, Lee et al. (2020) demonstrated\nthat a further pre-trained LM on biomedical doc-\numents outperforms the original LM on biomed-\nical NLU tasks. Also, Gururangan et al. (2020)\nshowed that adaptive pre-training of the PLM on\nthe corpus of a target domain (Domain-adaptive\nPre-training; DAPT) or a target task (Task-adaptive\nPre-training; TAPT) improves its performance on\ndomain-speciï¬c tasks. However, above approaches\ngenerally require a large amount of computational\ncosts for pre-training.\nKnowledge-aware LM Accompanied with in-\ncreasing sources of knowledge (Vrandecic and\nKrÃ¶tzsch, 2014), some prior works have proposed\nto integrate external knowledge into PLMs, to en-\nhance their performance on tasks that require struc-\ntured knowledge. For instance, ERNIE (Zhang\net al., 2019) and KnowBERT (Peters et al., 2019)\nincorporate entities as additional inputs in the pre-\ntraining stage to obtain a knowledge-aware LM,\nwherein a pre-trained knowledge graph embedding\nfrom Wikidata (Vrandecic and KrÃ¶tzsch, 2014) is\nused to represent entities. Entity-as-Experts (FÃ©vry\net al., 2020) and LUKE (Yamada et al., 2020) use\nthe entity memory that is pre-trained along with\nthe LMs from scratch. ERICA (Qin et al., 2021)\nfurther uses the fact consisting of entities and their\nrelations in the pre-training stage of LMs from\nscratch. Previous works aim to integrate external\nknowledge into the LMs during the pre-training\nstep to obtain a universal knowledge-aware LM\nthat requires additional parameters for millions of\nentities. In contrast to this, our framework aims to\nefï¬ciently modify a general PLM for the domain-\nspeciï¬c task with a linear modulation layer scheme\ndiscussed in Section 3.2, during ï¬ne-tuning.\n3 Method\n3.1 Problem Statement\nOur goal is to solve Natural Language Understand-\ning (NLU) tasks for a speciï¬c domain, with a\nknowledge-augmented Language Model (LM). We\nï¬rst introduce the NLU tasks we target, followed\nby the descriptions of the proposed knowledge-\naugmented LM. After that, we formally deï¬ne the\ningredients for structured knowledge integration.\nNLU tasks The goal of an NLU task is to predict\nthe label yof the given input instance x, where the\ninput xcontains the sequence of tokens (Devlin\net al., 2019): x= [w1,w2,...,w |x|]. Then, given\na training dataset D= {(x(i),y(i))}N\ni=1, the objec-\ntive is to maximize the log-likelihood as follows:\nmax\nÎ¸\nL(Î¸) := max\nÎ¸\nâˆ‘\n(x,y)âˆ¼D\nlog p(y|x; Î¸),\np(y|x; Î¸) =g(H; Î¸g), H= f(x; Î¸f),\nwhere f is an encoder of the PLM which outputs\ncontextualized representation Hfrom x, and gis\na decoder which models the probability distribu-\ntion p of the label y, with trainable parameters\nÎ¸= (Î¸f,Î¸g). If the LM is composed of L-layers of\ntransformer blocks (Devlin et al., 2019), the func-\ntion f is decomposed to multiple functions f =\n[f0,...,f L], where each block gets the output of\nthe previous block as the input: Hl = fl(Hlâˆ’1).1\nKnowledge-Augmented Language Model The\nconventional learning objective deï¬ned above\nmight be sufï¬cient for understanding the texts if\nthe tasks require only the general knowledge stored\nin PLMs. However, it is suboptimal for tackling\ndomain-speciï¬c tasks since the general knowledge\ncaptured by the parameters Î¸f may not include the\nknowledge required for solving the domain-speciï¬c\ntasks. Thus, contextualizing the texts by the do-\nmain knowledge, captured by the domain-speciï¬c\nentities and their relations, is more appropriate for\nhandling such domain-speciï¬c problems.\nTo this end, we propose a functionh(Â·; Ï†) which\naugments PLMs conditioned on the domain knowl-\nedge. Formally, the objective for a NLU task with\n1f0 denotes a word embedding layer which gets x as an\ninput, i.e., H0 = f0(x), for the sake of simplicity.\n5146\nour knowledge-augmented LM is given as follows:\nmax\nÎ¸,Ï†\nL(Î¸,Ï†) := max\nÎ¸,Ï†\nâˆ‘\n(x,y)âˆ¼D\nlog p(y|x; Î¸,Ï†),\np(y|x; Î¸,Ï†) =g( ËœH; Î¸g),\nËœHl = fl(Hlâˆ’1,hl(Hlâˆ’1,E,M,G; Ï†); Î¸fl),\nwhere Ï†is parameters for the functionh, Eis the set\nof entities, Mis the set of corresponding mentions,\nand Gis a knowledge graph. In the following,\nwe will describe the deï¬nition of the knowledge-\nrelated inputs E,M,G, and the details of h(Â·,Ï†).\nDeï¬nition 1 (Entity and Mention). Given a se-\nquence of tokens x = [w1,...,w |x|], let E be\na set of entities in x. Then an entity e âˆˆ E\nis composed of one or multiple adjacent tokens\nwithin the input text: [wmÎ±,...,w mÏ‰] âŠ‘x2. Here,\nm= (mÎ±,mÏ‰) is a mention that denotes the start\nand end locations for the entity within the input\ntokens x, which term is commonly used for deï¬n-\ning entities (FÃ©vry et al., 2020). Consequently, for\neach given input x(i), there are a set of entities\nE(i) = {e1,...,e K}and their corresponding men-\ntions M(i) = {m1,...,m K}. For example, given\nan input x= [New, York, is, a, city], we have two\nentities E= {New_York, city}and their associated\nmentions M= {(1,2),(4,4)}.\nWe further construct the entity vocabulary\nEtrain = â‹ƒN\ni=1 E(i), which consists of all entities\nappearing in the training dataset. However, at test\ntime, we may encounter unseen entities that are not\nin Etrain. To tackle this, we regard unknown entities\nas the null entity eâˆ…, so that âˆ€eâˆˆEtrain âˆª{eâˆ…}.\nDeï¬nition 2 (Entity Memory). Given a set of\nall entities Etrain âˆª{eâˆ…}, we represent them in the\ncontinuous vector (feature) space to learn meaning-\nful entity embeddings. In order to implement this,\nwe deï¬ne the entity memory E âˆˆR(|Etrain|+1)Ã—d\nthat comprises of an entity e âˆˆR as a key and\nits embedding e âˆˆRd as its value. Also, to ac-\ncess the value in the entity memory, we deï¬ne the\npoint-wise memory access function EntEmbed\nwhich takes an entity as an input. For instance,e=\nEntEmbed(New_York) returns the embedding of\nthe New_York entity, and e= EntEmbed(eâˆ…) re-\nturns the zero embedding. This entity memory E\nis the part of the parameter Ï†used in function h.\n2E âŠ‘Eâ€²iff E = Eâ€², or E is included in Eâ€²such that the\norder of elements in E and Eâ€²is the same.\nDeï¬nition 3 (Knowledge Graph). Since the en-\ntity memory alone cannot represent relational in-\nformation between entities, we further deï¬ne a\nKnowledge Graph (KG) Gthat consists of a set\nof factual triplets {(h,r,t)}, where the head and\nthe tail entities, h and t, are the elements of E,\nand a relation ris an element of a set of relations\nR: h,t âˆˆE and r âˆˆR. We assume that a pre-\nconstructed KG G(i) is given for each input x(i),\nand provide the details of the KGs and how to con-\nstruct them in Appendix A.\n3.2 Knowledge-conditioned Feature\nModulation on Transformer\nThe remaining problem is how to augment a PLM\nby conditioning it on the domain-speciï¬c knowl-\nedge, through the function h. An effective ap-\nproach to do so without stacking additional layers\non top of the LM is to interleave the knowledge\nfrom hwith the pre-trained parameters of the lan-\nguage model (Devlin et al., 2019) consisting of\ntransformer layers (Vaswani et al., 2017). Before\ndescribing our interleaving method in detail, we\nï¬rst describe the Transformer architecture.\nTransformer Given |x| token representations\nHlâˆ’1 = [hlâˆ’1\n1 ,..., hlâˆ’1\n|x|] âˆˆR|x|Ã—d from the layer\nlâˆ’1 where dis the embedding size, each trans-\nformer block outputs the contextualized representa-\ntions for all tokens. In detail, the l-th block consists\nof the multi-head self-attention (Attn) layer and the\nresidual feed-forward (FF) layer as follows:\nË†Hl = LN(Hlâˆ’1 + Attn(Hlâˆ’1))\nFF( Ë†Hl) =Ïƒ( Ë†Hl Â·W1) Â·W2,\nHl = LN( Ë†Hl + FF( Ë†Hl)),\nwhere LNis a layer normalization (Ba et al., 2016),\nÏƒis an activation function (Hendrycks and Gimpel,\n2016), W2 âˆˆRdâ€²Ã—d and W1 âˆˆRdÃ—dâ€²\nare weight\nmatrices, and dâ€²is an intermediate hidden size. We\nomit the bias term for brevity.\nLinear Modulation on Transformer An effec-\ntive yet efï¬cient way to fuse knowledge from differ-\nent sources without modifying the original model\narchitecture is to scale and shift the features of\none source with respect to the data from another\nsource (Dumoulin et al., 2018). This scheme of\nfeature-wise afï¬ne transformation is effective on\nvarious tasks, such as language-conditioned image\nreasoning (Perez et al., 2018) or style-transfer in\nimage generation (Huang and Belongie, 2017).\n5147\n[New York] is a [city] of [United States]\nTransformer Layers\nTransformer Layers Entity \nMemory\nNew_York\nCity\nUSA\n. . .Weighted \nAggregation\nRelational Retrieval \nfrom Entity Memory\nKFM KFM\nğ›¼ğ›¼=\nKFM\nLayerNorm\nKFM\nLayerNorm\nKFM\n(New_York)\n(USA)\n(City)\nï¿½ğœ¸ğœ¸, ï¿½ğœ·ğœ·\nğœ¸ğœ¸,ğœ·ğœ·\nStateKnowledge-conditioned  Feature Modulation\nMulti-Head\nSelf-Attention\nFeed-Forward\nFigure 3: Framework Overview. (Left) The architecture of a knowledge-augmented LM with our method. Some of the input\ntokens are annotated as entities with their mentions. (Middle) Inside the transformer block, KFM (Â§3.2) is applied after the\nlayer normalization as in equation 1, to modulate the hidden representations of tokens in entity mentions. (Right) The retrieved\nembedding of an entity New_York is composed by the weighted aggregation of neighbors through the knowledge graph (Â§3.3).\nMotivated by them, we propose to linearly trans-\nform the intermediate features after the layer nor-\nmalization of the transformer-based PLM, condi-\ntioned on the knowledge sourcesE,M,G. We term\nthis method as the Knowledge-conditioned Fea-\nture Modulation (KFM), described as follows:\nÎ“,B,ËœÎ“, ËœB= hl(Hlâˆ’1,E,M,G; Ï†),\nË†Hl = Î“ â—¦LN(Hlâˆ’1 + Attn(Hlâˆ’1)) +B,\nFF( Ë†Hl) =Ïƒ( Ë†Hl Â·W1) Â·W2,\nËœHl = ËœÎ“ â—¦LN( Ë†Hl + FF( Ë†Hl)) + ËœB, (1)\nwhere Hlâˆ’1 âˆˆR|x|Ã—d is the matrix of hidden rep-\nresentations from the previous layer, â—¦denotes\nthe hadamard (element-wise) product, and Î“ =\n[Î³1,..., Î³|x|] âˆˆ R|x|Ã—d, B = [Î²1,..., Î²|x|] âˆˆ\nR|x|Ã—d. Î“ and Bare learnable modulation param-\neters from the function h, which are conditioned\nby the entity representation. For instance, in Fig-\nure 3, Î³and Î²for token â€˜Newâ€™ are conditioned on\nthe corresponding entity New_York. However, if\ntokens are not part of any entity (e.g., â€˜isâ€™),Î³and\nÎ²for such tokens are ï¬xed to 1 and 0, respectively.\nOne notable advantage of our KFM is that mul-\ntiple tokens associated to the identical entity are\naffected by the same modulation (e.g., â€˜Newâ€™ and\nâ€˜Yorkâ€™ in Figure 3), which allows the PLM to know\nwhich adjacent tokens are in the same entity. This\nis important for representing the tokens of the do-\nmain entity (e.g., â€˜codâ€™ and â€˜onâ€™), since the original\nPLM might regard them as separate, unrelated to-\nkens (See analysis in Â§5.5 with Figure 5). However,\nwith our KFM, the PLM can identify associated\ntokens and embed them to be close to each other.\nThen, how can we design such functional op-\nerations in h? The easiest way is to retrieve the\nentity embedding of e, associated to the typical to-\nken, from the entity memory E, and then use the\nretrieved entity embedding as the input to obtain Î³\nand Î²for every entity (See Figure 3). Formally, for\neach entity eâˆˆE and its mention (mÎ±,mÏ‰) âˆˆM,\nv= EntEmbed(e) (2)\nÎ³j = 1 + h1(v), Î²j = h2(v),\nËœÎ³j = 1 + h3(v), ËœÎ²j = h4(v), m Î± â‰¤j â‰¤mÏ‰,\nwhere vis the retrieved entity embedding from the\nentity memory, h1,h2,h3,and h4 are mutually in-\ndependent Multi-Layer Perceptrons (MLPs) which\nreturn a zero vector 0 if e= eâˆ….\n3.3 Relational Retrieval from Entity Memory\nAlthough the simple access to the entity memory\ncan retrieve the necessary entity embeddings for\nthe modulation, this approach has obvious draw-\nbacks as it not only fails to reï¬‚ect the relations with\nother entities, but also regards unseen entities as\nthe same null entity eâˆ…. If so, all unseen entities are\ninevitably modulated by the same parameters even\nif they have essentially different meaning.\nTo tackle these limitations, we further consider\nthe relational information between two entities that\nare linked with a particular relation. For example,\nthe entity New_York alone will not give meaningful\ninformation. However, with two associated facts\n(New_York, instance of, city) and (New_York, coun-\ntry, USA), it is clear that New_York is a city in the\nUSA. Motivated by this observation, we propose\nRelational Retrieval which leverages a KG Gto\nretrieve entity embeddings from the memory, ac-\ncording to the relations deï¬ned in the given KG\n(See Figure 3, right).\nMore speciï¬cally, our goal is to effectively uti-\nlize the relations among entities in G, to improve\n5148\nthe EntEmbed function in equation 2. We tackle\nthis objective by utilizing a Graph Neural Net-\nwork (GNN) which learns feature representations\nof each node using a neighborhood aggregation\nscheme (Hamilton et al., 2017), as follows:\nv= UPDATE(EntEmbed(e),\nAGG({EntEmbed(Ë†e) :âˆ€Ë†eâˆˆN(e; G)})),\nwhere N(e; G) is a set of neighboring entities of\nthe entity e, AGG is the function that aggregates em-\nbeddings of neighboring entities ofe, and UPDATE\nis the function that updates the representation of e\nwith the aggregated messages from AGG.\nHowever, simple aggregation (e.g., mean) can-\nnot reï¬‚ect the relative importance on neigh-\nboring nodes, thus we consider the attentive\nscheme (Velickovic et al., 2018; Brody et al., 2021)\nfor neighborhood aggregation, to allocate weights\nto the target entityâ€™s neighbors by their importance.\nThis scheme is helpful in ï¬ltering out less use-\nful relations. Formally, we ï¬rst deï¬ne a scoring\nfunction Ïˆthat calculates a score for every triplet\n(ei,rij,ej), which is then used to weigh each node\nduring aggregation:\nei = EntEmbed(ei), ej = EntEmbed(ej),\neâˆ—= [ei âˆ¥rij âˆ¥ej âˆ¥hei],\nÏˆ(ei,rij,ej,hei) =aâŠ¤Ïƒ(W Â·eâˆ—),\nwhere Ïƒ is a nonlinear activation, eâˆ— âˆˆ R4d is\nconcatenated vector where âˆ¥denotes the concate-\nnation, a âˆˆRd and W âˆˆRdÃ—4d are learnable\nparameters, rij âˆˆRd is a embedding of the rela-\ntion, and hei âˆˆRd is a context representation of\nthe entity ei obtained from the intermediate hidden\nstates of the LM3.\nThe scores obtained from Ïˆ are normalized\nacross all neighbors ej âˆˆN(ei; G) with softmax:\nÎ±ij = softmax(Ïˆ(ei,rij,ej))\n= exp(Ïˆ(ei,rij,ej))âˆ‘\nejâ€²âˆˆN(ei;G) exp(Ïˆ(ei,rijâ€²,ejâ€²)).\nThen, we update the entity embedding with a\nweighted average of the neighboring nodes with Î±\nas an attention coefï¬cient, denoted as follows:\nv= UPDATE\n(âˆ‘\nejâ€²âˆˆN(ei;G)Î±ij Â·ejâ€²\n)\n. (3)\n3The context representation of the entity is calculated with\nits mention as follows: he = 1\nmÏ‰âˆ’mÎ±+1\nâˆ‘mÏ‰\ni=mÎ± hlâˆ’1\ni\nBy replacing the EntEmbed function in equa-\ntion 2 with the above GNN in equation 3, we now\nrepresent each entity with its relational information\nin KG. This relational retrieval has several advan-\ntages over simple retrieval of a single entity from\nthe entity memory. First, the relational retrieval\nwith KG can consider richer interactions among\nentities, as described in Figure 3.\nIn addition, we can naturally represent an un-\nseen entity â€“ which is not seen during training but\nappears at test time â€“ through neighboring aggre-\ngation, which is impossible only with the entity\nmemory. In Figure 2, we provide an illustrative\nexample of the unseen entity representation, where\nthe unseen entity restenosis is represented with a\nweighted sum of representations of its neighboring\nentities myocardial_infarction, asthma, and pethi-\ndine, which is beneï¬cial when the set of entities\nfor training and test datasets have small overlaps.\n4 Experiment\n4.1 Tasks and Datasets\nWe evaluate our model on two NLU tasks: Ques-\ntion Answering (QA) and Named Entity Recogni-\ntion (NER). For QA, we use three domain-speciï¬c\ndatasets: NewsQA (News, Trischler et al., 2017)\nand two subsets (Relation, Medication) of EMRQA\n(Clinical, Pampari et al., 2018). We use the Exact-\nMatch (EM) and the F1 score as evaluation met-\nrics. For NER, we use three datasets from different\ndomains, namely CoNLL-2003 (News, Sang and\nMeulder, 2003), WNUT-17 (Social Media, Der-\nczynski et al., 2017) and NCBI-Disease (Biomedi-\ncal, Dogan et al., 2014). We use the F1 score as the\nevaluation metric. We report statistics and detailed\ndescriptions of each dataset in Appendix B.2.\n4.2 Baselines\nA direct baseline of our KALA is the adaptive\npre-training, which is commonly used to adapt the\nPLM independent to the choice of a domain and\ntask. Also, to compare ours against a more pow-\nerful baseline, we modify a recent method (Chen\net al., 2020) that alleviates forgetting of PLM dur-\ning ï¬ne-tuning. Details for each baseline we use\nare described as follows:\n1. Vanilla Fine-Tuning (FT): A baseline that di-\nrectly ï¬ne-tunes the LM on downstream tasks.\n2. Fine-Tuning + more params: A baseline with\none more transformer layer at the end of the\n5149\nMethod NewsQA Relation Medication\nFine-Tuning 53.06 Â±0.63 | 67.20 Â±0.19 54.01 Â±1.14 | 61.43 Â±1.18 12.50 Â±0.28 | 43.31 Â±0.67\n+ more params 53.59 Â±0.99 | 67.79 Â±0.67 54.06 Â±1.35 | 62.07 Â±1.44 12.46 Â±0.25 | 42.74 Â±0.91\nTAPT 53.47 Â±1.69 | 67.59 Â±1.44 53.57 Â±2.05 | 60.87 Â±2.52 12.58 Â±0.42 | 43.82 Â±1.10\n+ RecAdam 53.95 Â±1.02 | 67.89 Â±0.75 54.88 Â±1.94 | 62.54 Â±2.14 12.63 Â±0.30 | 43.86 Â±0.87\nDAPTâ€  53.68 Â±0.94 | 67.76 Â±0.61 55.29 Â±1.74 | 62.25 Â±1.80 12.67 Â±0.27 | 43.26 Â±0.88\nKALA (point-wise) 53.41 Â±0.74 | 67.30 Â±0.45 56.13 Â±0.85 | 64.69 Â±0.92 12.01 Â±0.47 | 42.97 Â±0.70\nKALA (relational) 54.25 Â±0.63 | 68.27 Â±0.63 55.96 Â±1.37 | 64.22 Â±1.15 12.75 Â±0.61 | 44.19 Â±0.46\nTable 1: Experimental results of the extractive QA task on three different datasets with the BERT-base. The reported results are\nmeans and standard deviations of performances over ï¬ve different runs with Exact Match / F1 score as a metric. The numbers in\nbold fonts denote the best score. â€ indicates the method under an extremely high computational resource setting (See Figure 1).\nMethod CoNLL -2003 WNUT-17 NCBI-Disease\nFine-Tuning 90.58Â±0.19 45.70 Â±1.25 84.42 Â±0.58\n+more params 90.75Â±0.23 46.42 Â±0.55 84.70 Â±0.49\nTAPT 90.61Â±0.73 45.39 Â±0.77 84.39 Â±0.73\n+ RecAdam 90.69Â±0.30 46.73 Â±0.94 84.99 Â±0.88\nDAPTâ€  90.30Â±0.39 48.29 Â±1.08 84.68 Â±1.63\nKALA(point-wise) 90.96Â±0.21 47.33 Â±0.82 85.10 Â±0.73\nKALA(relational) 91.02Â±0.29 48.35Â±0.92 85.77Â±0.43\nTable 2: Experimental results of the NER task on three dif-\nferent datasets with the BERT-base. The reported results are\nmeans and standard deviations over ï¬ve different runs with an\nF1 score as a metric. The numbers in bold fonts denote the\nbest score. â€ indicates the baseline under an extremely high\ncomputational resource setting (See Figure 1).\nLM. We use this baseline to show that the per-\nformance gain of our model does not come from\nthe use of additional parameters.\n3. Task-Adaptive Pre-training (TAPT): A base-\nline that further pre-trains the PLM on task-\nspeciï¬c corpus as in Gururangan et al. (2020).\n4. TAPT + RecAdam : A baseline that uses\nRecAdam (Chen et al., 2020) during further\npre-training of PLMs (i.e., TAPT), to alleviate\ncatastrophic forgetting of the learned general\nknowledge in PLMs from adaptive pre-training.\n5. Domain-Adaptive Pre-training (DAPT) : A\nstrong baseline that uses a large-scale domain\ncorpus outside the training set during further pre-\ntraining (Gururangan et al., 2020), and requires\nextra data and large computational overhead.\n6. KALA (pointwise): A variant of KALA that\nonly uses the entity memory and does not use\nthe knowledge graphs.\n7. KALA (relational): Our full model that uses\nKGs to perform relational retrieval from the en-\ntity memory.\n4.3 Experimental Setup\nWe use the uncased BERT-base (Devlin et al., 2019)\nas the base PLM for all our experiments on QA\nand NER tasks. For more details on training and\nimplementation, please see the Appendix B.\n4.4 Experimental Results\nPerformance on QA and NER tasks On both\nextractive QA and NER tasks, our KALA out-\nperforms all baselines, including TAPT and\nTAPT+RedcAdam (Gururangan et al., 2020; Chen\net al., 2020), as shown in Table 1 and 2. These\nresults show that our KALA is highly effective\nfor the language model adaptation task. KALA\nalso largely outperforms DAPT (Gururangan et al.,\n2020) which is trained with extra data and requires\na signiï¬cantly higher computational cost compare\nto KALA (See Figure 1 for the plot of efï¬ciency,\ndiscussed in Section 5.3).\nEffect of Using more Parameters One may sus-\npect whether the performance of our KALA comes\nfrom the increment of parameters. However, the\nexperimental results in Table 1 and 2 show that in-\ncreasing the parameters for PLM during ï¬ne-tuning\n(+ more params) yields marginal performance im-\nprovements over naive ï¬ne-tuning. This result con-\nï¬rms that the performance improvement of KALA\nis not due to the increased number of parameters.\nImportance of Relational Retrieval The perfor-\nmance gap between KALA (relational) and KALA\n(point-wise) shows the effectiveness of relational\nretrieval for language model adaptation, which al-\nlows us to incorporate relational knowledge into\nthe PLM. The relational retrieval also helps address\nunseen entities, as discussed in Section 5.4.\n5 Analysis and Discussion\n5.1 Ablation Studies\nWe perform an ablation study to see how much each\ncomponent contributes to the performance gain.\nKFM Parameters We ï¬rst analyze the effect of\nfeature modulation parameters (i.e., gamma and\nbeta) in transformers by ablating a subset of them\nin Table 3, in which we observe that using both\n5150\nMethod NewsQA Relation WNUT -17 NCBI-Disease\nFine-Tuning 57.21 Â±0.56 | 71.91 Â±0.35 46.61 Â±2.75 | 53.89 Â±2.92 55.00 Â±1.66 86.91 Â±1.08\n+ more params 58.07 Â±1.19 | 72.38 Â±1.04 45.12 Â±0.86 | 53.22 Â±1.27 56.62 Â±0.26 87.21 Â±0.26\nTAPT 57.24 Â±0.53 | 71.77 Â±0.34 45.66 Â±2.20 | 53.23 Â±2.38 55.46 Â±1.90 86.24 Â±0.76\nKALA (relational) 58.01 Â±0.57 | 72.70 Â±0.25 47.40 Â±1.67 | 55.13 Â±1.26 56.96 Â±0.27 87.72 Â±0.27\nTable 5: Experimental results of the extractive QA and NER tasks on four different datasets â€“ NewsQA, Relation, WNUT-17\nand NCBI-Disease â€“ with the RoBERTa-base. The reported results are means and standard deviations over ï¬ve different runs.\nWe use Exact Match and F1 score as a metric for QA, and F1 score for NER. The numbers in bold fonts denote the best score.\nKFM (Â§3.2) NewsQA\nComponents EM F1\nNone (Fine-tuning)53.06 67.20\n+Î“,ËœÎ“(gamma only)54.10 67.98\n+B,ËœB(beta only)53.74 67.69\n+Î“,B(ï¬rst only)53.77 67.88\n+ËœÎ“,ËœB(second only)53.89 67.49\n+Î“,B,ËœÎ“,ËœB(ï¬nal) 54.25 68.27\nTable 3: An ablation study of\nthe KFM parameters Î“, B, ËœÎ“, ËœB.\nWe report the average results over\nï¬ve different runs.\nArchitecture NewsQA\nVariants (Â§5.2)EM F1\nERNIE 53.35 67.49\nAdapter 53.32 67.38\nKT-Net 53.15 67.01\nEaE 53.00 67.40\nERICA 51.99 66.40\nKALA (ours) 54.25 68.27\nTable 4: Experimental re-\nsults on knowledge integra-\ntion architecture variants,\naveraged over ï¬ve runs.\ngamma and beta after both layer normalization on\na transformer layer obtains the best performance.\nArchitectural Variants We now examine the ef-\nfectiveness of the proposed knowledge condition-\ning scheme in our KALA framework. To this end,\nwe use or adapt the knowledge integration methods\nfrom previous literature, to compare their effective-\nness. Speciï¬cally, we couple the following ï¬ve\ncomponents with KALA: Entity-as-Experts (FÃ©vry\net al., 2020), Adapter (Houlsby et al., 2019), KT-\nNet (Yang et al., 2019), ERNIE (Zhang et al., 2019),\nand ERICA (Qin et al., 2021). Note that, most\nof them were proposed for improving pre-training\nfrom scratch, while we adapt them for ï¬ne-tuning\nunder our KALA framework (The details are given\nin Appendix B.4). As shown in Table 4, our KFM\nused in KALA outperforms all variants, demon-\nstrating the effectiveness of feature modulation in\nthe middle of transformer layers for ï¬ne-tuning.\n5.2 Robustness to Other PLMs\nAlthough we believe our experimental results on\nTable 1, 2 with BERT (Devlin et al., 2019) are\nenough to show the effectiveness of KALA across\ndifferent pre-trained language models (PLMs), one\nmight be curious that KALA can work on even\nother PLMs such as RoBERTa (Liu et al., 2020).\nThus, to address such concerns, we additionally\nconduct experiments on RoBERTa. As shown in\nTable 5, we observe that our KALA outperforms\nall baselines except for one case (Fine-Tuning +\nSeen Unseen\n89\n90\n91\n92\n93 CoNLL-2003\nFine-tuning\nTAPT\nDAPT\nKALA (point-wise)\nKALA (relational)\nSeen Unseen\n42\n44\n46\n48\n50 WNUT-17\nSeen Unseen\n70\n75\n80\n85\n90 NCBI-Disease\nFigure 4: Results on seen and unseen, where Seen denotes the\ncontext having less than 3 unseen entities, otherwise Unseen.\nNote that DAPT uses extra datasets in addition to the training\ndataset, thus the Unseen for other models could be considered\nas the Seen for DAPT.\nmore params on NewsQA). Thus, we believe that\nour KALA would be useful to any PLMs, not de-\npending on speciï¬c PLMs.\n5.3 Efï¬ciency\nFigure 1 illustrates the performance and training\nFLOPs of KALA against baselines on the NewsQA\ndataset. We observe that the performance of TAPT\ndecreases with the increased number of iterations,\nwhich could be due to forgetting of the knowledge\nfrom the PLM. On the other hand, DAPT, while\nnot suffering from performance loss, requires huge\ncomputational costs as it trains on 112 times larger\ndata for further pre-training (See Appendix B.3\nfor detailed explanations on training data). On the\nother hand, our KALA outperforms DAPT without\nusing external data, while requiring 17 times fewer\ncomputational costs, which shows that KALA is\nnot only effective but also highly efï¬cient.\nTo further compare the efï¬ciency in various as-\npects, we report GPU memory, training wall time,\nand training FLOPs for baselines and ours in Ta-\nble 6. Through this, we verify that our KALA is\nmore efï¬cient to train for language model adapta-\ntion settings than baselines. Note that the resource\nrequirement of KALA could be further reduced by\nadjusting the size of the entity memory (e.g., remov-\ning less frequent entities). Therefore, to show the\nï¬‚exibility of our KALA on the typical resource con-\nstraint, we provide the experimental results on two\ndifferent settings (i.e., tuning the number of entities\nin the entity memory) â€“ KALA with memory size\nof 200 and 62.8k (full memory) in Appendix C.6.\n5151\nContext: Anonsense mutationin\nexon17 (codon556 ) of the RB1\ngenewas found to be present\nhomozygously in both theretinal\nand the pineal tumours.\nFact: (retinal, instance of, gene)\nnonsense mutation\nex\n##on\ncod\n##on\ngenere\n##tina\n##l\nFine-T uning\nnonsensemutationex\n##oncod##on\ngenere##tina\n##l\nKALA (Ours)\nFigure 5: A case study on one context of the NCBI-Disease dataset. A left table shows the\ncontext and its fact, and a right ï¬gure shows a visualization of token representations. Text in\nblue and red denote the seen and unseen entities, respectively.\nNewsQA\nT5-small EM F1\nFine-tuning48.96 64.24\nTAPT 48.66 64.30\n+RecAdam 48.37 63.41\nKALA (ours) 51.78 66.88\nTable 7: Experimental results\non generative question answer-\ning with T5-small as a PLM and\nNewsQA as a dataset.\nMethod GPU Mem. Approx. Wall Time FLOPs (1016)\nFine-Tuning 8 GB 3 hrs 9.5\n+more params 8.8 GB 3 hrs 10.1\nTAPT 8 GB 3.8 hrs 10.1\nDAPT 48 GB 40 hrs < 182.0\nKALA(ours, 0.2k) 8.4 GB 3 hrs 9.97\nKALA(ours, 62.8k) 9.2 GB 3 hrs 10.5\nTable 6: Efï¬ciency comparisons of GPU memory, Wall Time,\nand FLOPs on the NewsQA dataset. The number 0.2k and\n62.8k indicate the size of entity memory used in our KALA.\n5.4 Effectiveness on Unseen Entities\nOne remarkable advantage of our KALA is its abil-\nity to represent an unseen entity by aggregating\nfeatures of its neighbors from a given KG. To an-\nalyze this, we ï¬rst divide all contexts into one of\nSeen and Unseen, where Seen denotes the context\nwith less than 3 unseen entities, and then measure\nthe performance on the two subsets. As shown in\nFigure 4, we observe that the performance gain\nof KALA over the baselines is much larger on\nthe Unseen subset, which demonstrates the effec-\ntiveness of KALAâ€™s relational retrieval scheme to\nrepresent unseen entities. DAPT also largely out-\nperforms ï¬ne-tuning and TAPT as it is trained on\nan extremely large external corpus for adaptive\npre-training. However, KALA even outperforms\nDAPT in most cases, verifying that our knowledge-\naugmentation method is more effective for tack-\nling domain-speciï¬c tasks. The visualization of\nembeddings of seen and unseen entities in Fig-\nure 2 shows that KALA embeds the unseen entities\nmore closely to the seen entities4, which explains\nKALAâ€™s good performance on the Unseen subset.\n5.5 Case Study\nTo better see how our KFM (Â§3.2) works, we show\nthe context and its fact, and then visualize repre-\nsentations from the PLM modulated by the KFM.\n4We quantitatively measure the mean of cosine distance\nof each unseen entity to its nearest seen entity, observing that\nKALA embeds unseen 1.5 times more closer to seen than\nDAPT (i.e., 0.07 for KALA vs 0.11 for DAPT for distance).\nAs shown in Figure 5 right, the token â€˜##onâ€™ is not\naligned with their corresponding tokens, such as\nâ€˜exâ€™ (forexon) and â€˜codâ€™ (forcodon), in the baseline.\nHowever, with our feature modulation that trans-\nforms multiple tokens associated with the single\nentity equally, the two tokens (e.g., (â€˜exâ€™, â€˜##onâ€™)),\ncomposing one entity, are closely embedded. Also,\nwhile the baseline cannot handle the unseen entity\nconsisting of three tokens: â€˜reâ€™, â€˜##tinaâ€™, and â€˜##lâ€™,\nKALA embeds them closely by representing the\nunseen retinal from the representation of its neigh-\nborhood gene derived by the domain knowledge â€“\n(retinal, instance of, gene).\n5.6 Extension to Generative Model\nOur KALA framework is also applicable to\nencoder-decoder PLMs by applying the KFM to the\nencoder. Therefore, we further validate KALAâ€™s\neffectiveness on the encoder-decoder PLMs on the\ngenerative QA task (Lee et al., 2021) with T5-\nsmall (Raffel et al., 2020). Table 7 shows that\nKALA largely outperforms baselines even with\nsuch a generative PLM.\n6 Conclusion\nIn this paper, we introduced KALA, a novel frame-\nwork for language model adaptation, which mod-\nulates the intermediate representations of a PLM\nby conditioning it with the entity memory and the\nrelational facts from KGs. We validated KALA on\nvarious domains of QA and NER tasks, on which\nKALA signiï¬cantly outperforms relevant baselines\nwhile being computationally efï¬cient. We demon-\nstrate that the success of KALA comes from both\nKFM and relational retrieval, allowing the PLM to\nrecognize entities but also handle unseen ones that\nmight frequently appear in domain-speciï¬c tasks.\nThere are many other avenues for future work, in-\ncluding the application of KALA on pre-training\nof knowledge-augmented PLMs from scratch.\n5152\nEthical Statements\nEnhancing the domain converge of pre-traind lan-\nguage models (PLMs) with external knowledge\nis increasingly important, since the PLMs cannot\nobserve all the data during training and cannot\nmemorize all the necessary knowledge for solv-\ning down-stream tasks. Our KALA contributes to\nthis problem by augmenting domain knowledge\ngraphs for PLMs. However, we have to still con-\nsider the accurateness of knowledge, i.e., the fact in\nthe knowledge graph may not be correct, which af-\nfects the model to generate incorrect answers. Also,\nthe modelâ€™s prediction performance is still far from\noptimal. Thus, we should be aware of modelâ€™s\nfailure from errors in knowledge and prediction, es-\npecially on high-risk domains (e.g., biomedicine).\nAcknowledgement\nThis work was supported by Institute of Infor-\nmation & communications Technology Planning\n& Evaluation (IITP) grant funded by the Ko-\nrea government (MSIT) (No.2019-0-00075, Ar-\ntiï¬cial Intelligence Graduate School Program\n(KAIST) and No. 2021-0-02068, Artiï¬cial In-\ntelligence Innovation Hub)), AITRICS, Samsung\nElectronics (IO201214-08145-01), and the En-\ngineering Research Center Program through the\nNational Research Foundation of Korea (NRF)\nfunded by the Korean Government MSIT (NRF-\n2018R1A5A1059921).\nReferences\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization. arXiv preprint ,\narXiv:1607.06450.\nFan Bai, Alan Ritter, and Wei Xu. 2021. Pre-train\nor annotate? domain adaptation with a constrained\nbudget. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 5002â€“\n5015.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\nert: A pretrained language model for scientiï¬c text.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 3613â€“\n3618.\nShaked Brody, Uri Alon, and Eran Yahav. 2021. How\nattentive are graph attention networks? arXiv\npreprint, arXiv:2105.14491.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr\nSettles, Estevam R. Hruschka Jr., and Tom M.\nMitchell. 2010. Toward an architecture for never-\nending language learning. In Proceedings of the\nTwenty-Fourth AAAI Conference on Artiï¬cial Intel-\nligence, AAAI 2010, Atlanta, Georgia, USA, July 11-\n15, 2010. AAAI Press.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 7870â€“7881.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020.\nLeon Derczynski, Eric Nichols, Marieke van Erp, and\nNut Limsopatham. 2017. Results of the WNUT2017\nshared task on novel and emerging entity recogni-\ntion. In Proceedings of the 3rd Workshop on Noisy\nUser-generated Text, NUT@EMNLP 2017, Copen-\nhagen, Denmark, September 7, 2017 , pages 140â€“\n147. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171â€“4186.\nRezarta Islamaj Dogan, Robert Leaman, and Zhiyong\nLu. 2014. NCBI disease corpus: A resource for dis-\nease name recognition and concept normalization. J.\nBiomed. Informatics, 47:1â€“10.\nVincent Dumoulin, Ethan Perez, Nathan Schucher, Flo-\nrian Strub, Harm de Vries, Aaron Courville, and\nYoshua Bengio. 2018. Feature-wise transformations.\nDistill.\n5153\nThibault FÃ©vry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020 ,\npages 4937â€“4951.\nMatthias Fey and Jan E. Lenssen. 2019. Fast graph\nrepresentation learning with PyTorch Geometric. In\nICLR Workshop on Representation Learning on\nGraphs and Manifolds.\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2021. Demix\nlayers: Disentangling domains for modular lan-\nguage modeling. arXiv preprint, arXiv:2108.05036.\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Donâ€™t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020, pages 8342â€“8360.\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec.\n2017. Inductive representation learning on large\ngraphs. In Advances in Neural Information Process-\ning Systems 30: Annual Conference on Neural In-\nformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 1024â€“1034.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 4237â€“4247.\nDan Hendrycks and Kevin Gimpel. 2016. Bridg-\ning nonlinearities and stochastic regularizers with\ngaussian error linear units. arXiv preprint ,\narXiv:1606.08415.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efï¬cient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA, pages 2790â€“2799.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ï¬ne-tuning for text classiï¬cation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 1:\nLong Papers, pages 328â€“339.\nXun Huang and Serge J. Belongie. 2017. Arbitrary\nstyle transfer in real-time with adaptive instance nor-\nmalization. In IEEE International Conference on\nComputer Vision, ICCV 2017, Venice, Italy, October\n22-29, 2017, pages 1510â€“1519. IEEE Computer So-\nciety.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2020. What dis-\nease does this patient have? A large-scale open do-\nmain question answering dataset from medical ex-\nams. arXiv preprint, arXiv:2009.13081.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. Biobert: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinform., 36(4):1234â€“\n1240.\nSeanie Lee, Minki Kang, Juho Lee, and Sung Ju\nHwang. 2021. Learning to perturb word embed-\ndings for out-of-distribution QA. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-\ntual Event, August 1-6, 2021, pages 5583â€“5595.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint, arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory F. Diamos, Erich Elsen, David GarcÃ­a,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed\nprecision training. In ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track\nProceedings.\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39â€“41.\nVinod Nair and Geoffrey E. Hinton. 2010. Rectiï¬ed\nlinear units improve restricted boltzmann machines.\nIn Proceedings of the 27th International Conference\non Machine Learning (ICML-10), June 21-24, 2010,\nHaifa, Israel, pages 807â€“814.\nAnusri Pampari, Preethi Raghavan, Jennifer J. Liang,\nand Jian Peng. 2018. emrqa: A large corpus for\nquestion answering on electronic medical records.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, Brussels,\nBelgium, October 31 - November 4, 2018 , pages\n2357â€“2368.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas KÃ¶pf, Edward Z.\n5154\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In Advances in Neural Informa-\ntion Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 8024â€“8035.\nEthan Perez, Florian Strub, Harm de Vries, Vincent Du-\nmoulin, and Aaron C. Courville. 2018. Film: Vi-\nsual reasoning with a general conditioning layer. In\nProceedings of the Thirty-Second AAAI Conference\non Artiï¬cial Intelligence, (AAAI-18), the 30th inno-\nvative Applications of Artiï¬cial Intelligence (IAAI-\n18), and the 8th AAAI Symposium on Educational\nAdvances in Artiï¬cial Intelligence (EAAI-18), New\nOrleans, Louisiana, USA, February 2-7, 2018, pages\n3942â€“3951.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced con-\ntextual word representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 43â€“54.\nYujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu,\nPeng Li, Heng Ji, Minlie Huang, Maosong Sun, and\nJie Zhou. 2021. ERICA: improving entity and re-\nlation understanding for pre-trained language mod-\nels via contrastive learning. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-\ntual Event, August 1-6, 2021, pages 3350â€“3363.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniï¬ed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1â€“140:67.\nSebastian Ruder. 2019. Neural Transfer Learning for\nNatural Language Processing . Ph.D. thesis, Na-\ntional University of Ireland, Galway.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning, CoNLL 2003, Held in cooper-\nation with HLT-NAACL 2003, Edmonton, Canada,\nMay 31 - June 1, 2003, pages 142â€“147. ACL.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, StockholmsmÃ¤s-\nsan, Stockholm, Sweden, July 10-15, 2018 , pages\n4603â€“4611.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston,\nand Rob Fergus. 2015. End-to-end memory net-\nworks. In Advances in Neural Information Process-\ning Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-\n12, 2015, Montreal, Quebec, Canada , pages 2440â€“\n2448.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin\nHarris, Alessandro Sordoni, Philip Bachman, and\nKaheer Suleman. 2017. Newsqa: A machine\ncomprehension dataset. In Proceedings of the\n2nd Workshop on Representation Learning for NLP ,\nRep4NLP@ACL 2017, Vancouver, Canada, August\n3, 2017, pages 191â€“200.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998â€“6008.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro LiÃ², and Yoshua Bengio.\n2018. Graph attention networks. In 6th Inter-\nnational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam W. Cohen. 2021. Adaptable and inter-\npretable neural memoryover symbolic knowledge.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , pages\n3678â€“3691.\nDenny Vrandecic and Markus KrÃ¶tzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commun.\nACM, 57(10):78â€“85.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In EMNLP 2020 - Demos, Online, Novem-\nber 16-20, 2020, pages 38â€“45.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2020, Online, November\n16-20, 2020, pages 6442â€“6454.\nAn Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu,\nHua Wu, Qiaoqiao She, and Sujian Li. 2019. En-\nhancing pre-trained language representations with\n5155\nrich knowledge for machine reading comprehension.\nIn Proceedings of the 57th Conference of the As-\nsociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 2346â€“2357.\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,\nZhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,\nand Maosong Sun. 2019. Docred: A large-scale\ndocument-level relation extraction dataset. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 764â€“777.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in Neural Infor-\nmation Processing Systems 27: Annual Conference\non Neural Information Processing Systems 2014,\nDecember 8-13 2014, Montreal, Quebec, Canada ,\npages 3320â€“3328.\nXiang Yue, Bernal Jimenez Gutierrez, and Huan Sun.\n2020. Clinical reading comprehension: A thorough\nanalysis of the emrqa dataset. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 4474â€“4486.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: en-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 1441â€“1451.\n5156\nEntity \nExtractionData\nData w/\nextracted \nentities\nFormat (a):\nRelation \nExtraction\nData w/\nextracted \nfacts\nFormat (b):\nFine-tuned\nmodel\nâ€œtextâ€: â€œArvane Rezaiâ€,\nâ€œstartâ€: 30,\nâ€œendâ€: 43,\nâ€œidâ€: 228998\nâ€œhâ€: 11578,\nâ€œrâ€: â€œP3373â€,\nâ€œtâ€: 228998\nFigure 6: Visual diagram of the KG construction pipeline used in this work. The entity format is composed of its corresponding\ntext in the data, its character-level mention boundary, and its wikidata id. The fact format is composed of the head, relation, and\ntail, where head and tail entities are represented with their wikidata ids following the entity format.\nHyperparameters NewsQA Relation Medication CoNLL-2003 WNUT-17 NCBI-Disease\nLM for Relation Extraction BERT-base-uncased\nThreshold on Relation Extraction 0.1\nSize of Entity Memory 62823 5724 4635 10288 101 3502\nThe location of KFM 11 11 11 8 9, 11 8, 10\nTable 8: Hyperparamters for Knowledge Graph (Top) and KALA (Bottom) on six datasets we used. The reported\nperformances on main paper are measured with the above settings.\nA Details on KG Construction\nIn this work, we propose to use the Knowledge\nGraph (KG) that can deï¬ne the relational informa-\ntion among entities that only appear in each dataset.\nHowever, unfortunately, most of the task datasets\ndo not contain such relational facts on its context,\nthus we need to construct them manually to obtain\nthe knowledge graph. In this section, we explain\nthe way of constructing the knowledge graph that\nwe used, consisting of facts of entities for each\ncontext in the task dataset.\nRelation extraction is the way how we obtain\nthe factual knowledge from the text of the target\ndataset. To do so, we ï¬rst need to extract entities\nand their corresponding mentions from the text, and\nthen link it to the existing entities in wikidata (Vran-\ndecic and KrÃ¶tzsch, 2014). In order to do this, we\nuse the existing library named as spaCy5, and open-\nsourced implementation of Entity Linker6. To sum\nup, in our work, a set of entities E(i) and corre-\nsponding mentions M(i) for the given input x(i)\nare obtained through this step. Regarding a con-\ncrete example, please see format (a) in Figure 6. In\nthe example, â€œTextâ€ indicates the entity mention\nwithin the input x, the â€œstartâ€ and â€œendâ€ indicates\nits mention position denoted as (mÎ±,mÏ‰), and â€œidâ€\nindicates the wikidata id for the entity identiï¬cation\nused in the next step.\nTo extract the relation among entities that we\nobtained above, we use the scheme of Relation Ex-\ntraction (RE). In other words, we use the trained\n5https://spacy.io/\n6https://github.com/egerber/spaCy-entity-linker\nRE model to build our own knowledge base (KB)\ninstead of using the existing KG directly from the\nexisting general-domain KB7. Speciï¬cally, we ï¬rst\nï¬ne-tune the BERT-base model (Devlin et al., 2019)\nfor 2 epochs with 600k distantly supervised data\nused in Qin et al. (2021), where the Wikipedia doc-\nument and the Wikidata triplets are aligned. Then,\nwe use the ï¬ne-tuned BERT model to extract the\nrelations between entity pairs in the text. We use\nthe model with a simple bilinear layer on top of it,\nwhich is widely used scheme in the relation extrac-\ntion literature (Yao et al., 2019). For an example\nof the extracted fact, please see format (b) in Fig-\nure 6. In the example, â€œhâ€ denotes the wikidata id\nof the head entity, â€œrâ€ denotes the wikidata id of\nthe extracted relation, and â€œtâ€ denotes the wikidata\nid of the tail entity. In the relation extraction, the\nmodel returns the categorical distribution over the\ntop 100 frequent relations. In general, the relation\nof top-1 probability is used as the relation for the\ncorresponding entity pair. However, this approach\nsometimes results in predicting no_relation\non most entity pairs. Thus, to obtain more rela-\ntions, we further use the relation of top-2 probabil-\nity in the case where no_relation has a top-1\n7We faced several problems here. First of all, most KBs\nsuch as Wikidata are less informative, especially for the en-\ntities included in the domain-speciï¬c context (e.g., News,\nMedical records). It only has a few facts for each context of\ndomain-speciï¬c tasks, although we can ï¬nd a lot of entities\nincluded in the context. Second, the entity linker is imperfect.\nDue to the wrongly linked entity to the wikidata, even existing\nrelations in the KG are ignored a lot. Therefore, we instead\nuse a trained neural network to effectively extract the relations\nbetween entities, instead of direct querying to obtain facts.\n5157\nTraining Validation Test\nDataset # Context C. Length # Question # Context C. Length # Question # Context C. Length # Question\nNewsQA 11428 655.7 74160 - - - 106 625.8 674\nRelation 296 1386.1 6162 42 1206.6 321 85 1467.7 802\nMedication 182 1737.3 7518 26 1626.5 1858 53 2005.0 4005\nTable 9: QA dataset statistics. We report the number of contexts and questions (i.e., # Context and # Question), with the\naverage length of contexts (i.e., C. Length) where the length is measured as the number of tokens after wordpiece tokenization.\nprobability but the top-2 probability is larger than\na certain threshold (e.g., > 0.1). In Figure 6, we\nsummarize our KG construction pipeline. In Ta-\nble 8, we report the hyperparameters related to our\nKG construction.\nB Experimental Setup\nIn this section, we introduce the detailed setups for\nour models and baselines used in Table 1, 2, and 4.\nB.1 Implementation Details\nWe use the Pytorch (Paszke et al., 2019) for the\nimplementation of all models. Also, to easily im-\nplement the language model, we use the hugging-\nface library (Wolf et al., 2020) containing vari-\nous transformer-based pre-trained language models\n(PLMs) and their checkpoints.\nDetails for KALA In this paragraph, we de-\nscribe the implementation details of the compo-\nnents, such as four linear layers in the proposed\nKFM, architectural speciï¬cations in the attention-\nbased GNN, and initialization of both the entity\nmemory and relational embeddings, in the follow-\ning. In terms of the functions h1,h2,h3,and h4 in\nthe KFM of Equation 2, we use two linear layers\nwith the ReLU (Nair and Hinton, 2010) activation\nfunction, where the dimension is set to 768.\nFor relational retrieval, we implement the novel\nGNN model based on GATv2 (Brody et al., 2021)\nprovided by the torch-geometric package (Fey and\nLenssen, 2019). Speciï¬cally, we stack two GNN\nlayers with the RELU activation function and also\nuse the dropout with a probability of 0.1. For at-\ntention in our GNN, we mask the nodes of the null\nentity, so that the attention score becomes zero for\nthem. Moreover, to obtain the context representa-\ntion of the entity (See Footnote 3 in the main paper)\nused in the GNN attention, we use the scatter oper-\nation8 for reduced computational cost.\nFor Entity Memory, we experimentally found\nthat initializing the embeddings of the entity mem-\nory with the contextualized features obtained from\n8https://github.com/rusty1s/pytorch_scatter\nthe pre-trained language model could be helpful.\nTherefore, the dimension of the entity embedding\nis set to the same as the language model d= 768.\nFor relation embeddings, we randomly initialize\nthem, where the dimension size is set to 128.\nLocation of KLM in the PLM Note that, the\nnumber and location of the KFM layers inside the\nPLM are hyperparameters. However, we empiri-\ncally found that inserting one to three KFM layers\nat the end of the PLM (i.e., after the 9th - 11th\nlayers of the BERT-base language model) is ben-\neï¬cial to the performance (See Appendix C.4 for\nexperiments on diverse layer locations).\nB.2 Dataset Details\nHere we describe the dataset details with its statis-\ntics for two different tasks: extractive question an-\nswering (QA) and named entity recognition (NER).\nQuestion Answering We evaluate models on\nthree domain-speciï¬c datasets: NewsQA, Rela-\ntion, and Medication. Notably, NewsQA (Trischler\net al., 2017) is curated from CNN news articles.\nRelation and Medication are originally part of the\nemrQA (Pampari et al., 2018), which is an auto-\nmatically constructed question answering dataset\nbased on the electrical medical record from n2c2\nchallenges9. However, Yue et al. (2020) extract\ntwo major subsets by dividing the entire dataset\ninto Relation and Medication and suggest the us-\nage of sampled questions from the original em-\nrQA dataset. Following the suggestion of Yue et al.\n(2020), we use only 1% of generated questions of\nRelation for training, validation, and testing. Also,\nwe only use 1% of generated questions of Medica-\ntion for training and use 5% of generated questions\nof Medication for validation and testing. Since the\noriginal emrQA is automatically generated based\non templates, the quality is poor â€“ it means that\nthe original emrQA dataset was inappropriate to\nevaluate the ability of the model to reason over\nthe clinical text since the most of questions can be\n9https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\n5158\nHyperparameters NewsQA Relation Medication CoNLL-2003 WNUT-17 NCBI-Disease GenerativeNewsQA\nFine-tuning\nLanguage Model BERT-base-uncased T5-small\nMaximum Sequence Length 384 384 384 128 128 128 512\nBatch Size 12 12 12 32 32 32 64\nTraining Epochs 2 2 2 20 20 20 4\nOptimizer AdamW Adafactor\nLearning rate 3e-5 3e-5 3e-5 5e-5 5e-5 5e-5 1e-4\nWeight Decay 0.01 0.01 0.01 0 0 0 -\nLR decay Warmup rate 0.06 0.06 0.06 0 0 0 -\nHalf Precision Yes Yes Yes No No No No\nTask-Adaptive Pre-training (TAPT)\nMaximum Sequence Length 384 384 384 128 128 128 384\nBatch Size 12 12 12 32 32 32 64\nTraining Epochs 1 1 1 3 3 3 4\nTraining Epochs (RecAdam) 3 1 1 3 3 3 4\nOptimizer AdamW Adafactor\nLearning rate 5e-5 1e-3\nWeight Decay 0.01 0.01 0.01 0 0 0 -\nLR decay Warmup rate 0.06 0.06 0.06 0 0 0 -\nHalf Precision Yes Yes Yes No No No No\nTable 10: Hyperparamters for Fine-tuning (Top) and TAPT (Bottom) on six datasets (+ generative QA) we used for\nreporting the performances in the main paper. Note that the Fine-tuning setup is applied to all methods including KALA.\nTraining Validation TestDataset # Context C. Length # Context C. Length # Context C. Length\nCoNLL-2003 14,041 19.95 3,250 21.36 3,453 18.77WNUT-17 3,394 31.32 1,009 19.28 1,287 30.58NCBI-Disease 5,433 34.36 924 35.00 941 35.50\nTable 11: NER dataset statistics. We report the number of\ncontexts (i.e., # Context), with the average length of them (i.e.,\nC. Length) on training, validation, and test sets.\nanswered by the simple text matching. To over-\ncome this limitation, Yue et al. (2020) suggests\ntwo ways to make the task more difï¬cult. First,\nthey divide the question templates into easy and\nhard versions and then use the hard question only.\nSecond, they suggest replacing medical terminolo-\ngies in the question of the test set into synonyms\nto avoid the trivial question which can be solvable\nwith a simple text matching. We use both methods\nto Relation and Medication datasets to report the\nperformance of every model. For more details on\nRelation and Medication datasets, please refer to\nthe original paper (Yue et al., 2020). The statis-\ntics of training, validation, and test sets on all QA\ndatasets are provided in Table 9.\nNamed Entity Recognition We use three dif-\nferent domain-speciï¬c datasets for evaluating our\nKALA on NER tasks: CoNLL-2003 (Sang and\nMeulder, 2003) (News), WNUT-17 (Derczynski\net al., 2017) (Social Network Service) and NCBI-\nDisease (Dogan et al., 2014) (Biomedical). The\nCoNLL-2003 is constructed from the manually cu-\nrated 1,393 English news articles, including 301.4k\ntokens, which has 9 class labels. The WNUT-\n17 dataset consists of 65,124 emerging and rare\nentities from social media (e.g., Twitter, Reddit,\nYouTube, to name a few), which has 13 class la-\nHyperparameters News Medical Textbook\nDomain-Adaptive Pre-training (DAPT)\nThe number of text (by lines) 10M 100k\nThe number of text (by words) 618M 12.8M\nThe size of data (by volume) 3.5G 86M\nMaximum Sequence Length 384\nBatch Size 64\nTraining Epochs 50\nMaximum Steps 12.5k\nOptimizer AdamW\nLearning rate 5e-5\nWeight Decay 0.01\nLR decay Warmup rate 0.06\nHalf Precision Yes\nApplied Dataset\nNewsQA\nCoNLL-2003\nWNUT-17\nRelation\nMedication\nNCBI-Disease\nTable 12: Hyperparamters for DAPT on two domains we\nused for reporting the performances in the main paper.\nbels. The NCBI-Disease dataset consists of the\n793 PubMed articles from the biomedical domain,\nwhich contains 6,892 disease mentions and 790\ndisease concepts, and also has 3 class labels. The\nstatistics of training, validation, and test sets are\nprovided in Table 11.\nB.3 Training details\nAll experiments are constrained to be done with a\nsingle 12GB Geforce RTX 2080 Ti GPU for fair-\nness in terms of memory and the availability on the\nacademic budget, except for the DAPT and gener-\native QA which use a single 48GB Quadro 8000\nGPU. KALA training needs 3 hours in wall time\nwith a single GPU. For all experiments, we select\nthe best checkpoint on the validation set. For the\nsummary of training setups, please see Table 10\nand 12.\n5159\nFine-tuning Setup In the following three para-\ngraphs, we explain the setting of ï¬ne-tuning for\nQA, NER, and generative QA tasks. For all ex-\nperiments on extractive QA tasks, we ï¬ne-tune the\nPre-trained Language Model (PLM) for 2 epochs\nwith the weight decay of 0.01, learning rate of 3e-5,\nmaximum sequence length of 384, batch size of 12,\nlinear learning rate decay of 0.06 warmup rate, and\nhalf precision (Micikevicius et al., 2018).\nFor all experiments on NER tasks, we ï¬ne-\ntune the PLM for 20 epochs, where the learning\nrate is set to 5e-5, maximum sequence length is\nset to 128, and batch size is set to 32. We use\nAdamW (Loshchilov and Hutter, 2019) as an opti-\nmizer using BERT-base as the PLM.\nFor the generative QA task in Table 7, we ï¬ne-\ntune the T5-small (Raffel et al., 2020) for 4 epochs\nwith the learning rate of 1e-4, maximum sequence\nlength of 512, and batch size of 64. We also use\nthe Adafactor (Shazeer and Stern, 2018) optimizer.\nInstead of training with the same optimizer as in\nBERT for QA and NER, we instead use the inde-\npendent AdamW optimizer with the learning rate\nof 1e-4 and weight decay of 0.01 to train the KALA\nmodule with T5.\nAdaptive Pre-training Setup In this paragraph,\nwe describe the experimental settings of adaptive\npre-training baselines, namely TAPT, TAPT (+\nRecAdam), and DAPT. For QA tasks, we further\npre-train the PLM for {1,3,5,10} epochs and then\nreport the best performance among them. Speciï¬-\ncally, reported TAPT result on NewsQA, Relation,\nand Medication are obtained by 1 epoch of fur-\nther pre-training. We use the weight decay of 0.01,\nlearning rate of 5e-5, maximum sequence length of\n384, batch size of 12, and linear learning rate decay\nof 0.06 warmup rate, with a half-precision. Also,\nthe masking ratio for the pre-training objective is\nset to 0.15, following the existing strategy intro-\nduced in the original BERT paper (Devlin et al.,\n2019).\nFor NER tasks, we further pre-train the PLM\nfor 3 epochs across all datasets. In particular, the\nlearning rate is set to 5e-5, batch size is set to 32,\nand the maximum sequence length is set to 128. We\nalso use AdamW (Loshchilov and Hutter, 2019) as\nthe optimizer for all experiments.\nIn the case of T5-small for generative QA in Ta-\nble 7, we further pre-train the PLM for 4 epochs\nwith the learning rate of 0.001, batch size of 64,\nmaximum sequence length of 384, and Adafac-\ntor (Shazeer and Stern, 2018) optimizer.\nRegarding the setting of TAPT (+ RecAdam) on\nall tasks, we follow the best setting in the original\npaper (Chen et al., 2020) â€“ sigmoid as an annealing\nfunction with annealing parameters: k= 0.5, t0 =\n250, and the pretraining coefï¬cient of 5000.\nFor training with DAPT, we need an external\ncorpus having a large amount of data for adaptive\npre-training. Thus, we ï¬rst choose the datasets of\ntwo domains â€“ News and Medical. Speciï¬cally,\nas the source of corpus for the News domain, we\nuse the sampled set of 10 million News from the\nRealNews dataset used in Gururangan et al. (2021).\nAs the source of corpus for the Medical domain, we\nuse the set of approximately 100k passages from\nthe Medical textbook provided in Jin et al. (2020).\nThe size of pre-training data used in DAPT is much\nlarger than TAPT. In other words, for experiments\non NewsQA, TAPT only uses ï¬ne-tuning contexts\ncontaining 5.8 million words from the NewsQA\ntraining dataset, while DAPT uses more than a hun-\ndred times larger data â€“ enormous contexts contain-\ning about 618 million words from the RealNews\ndatabase. For both News and Medical domains,\nwe further pre-train the BERT-base model for 50\nepochs with the batch size of 64, to match the sim-\nilar computational cost used in Gururangan et al.\n(2020). Other experimental details are the same as\nTAPT described above.\nB.4 Architectural Variant Details\nIn this subsection, we describe the details of archi-\ntectural variants reported in Section 5.1. For all\nvariants, we use the same KGs used in KALA.\nEntity-as-Experts (FÃ©vry et al. (2020); EaE)\nutilizes the entity memory similar to our work, but\nthey use the parametric dense retrieval more like the\nmemory neural network (Sukhbaatar et al., 2015).\nSimilar to FÃ©vry et al. (2020); Verga et al. (2021),\nwe change the formulation of query and memory\nretrieval by using the mention representation of the\nentity from the intermediate hidden states of PLMs,\nwhich is formally deï¬ned as follows:\nhe = 1\nmÏ‰ âˆ’mÎ± + 1\nmÏ‰\nâˆ‘\ni=mÎ±\nhlâˆ’1\ni , (4)\nv= softmax(he Â·EâŠ¤) Â·E,\nwhere he represents the average of token represen-\ntations of the entity mention m= (mÎ±,mÏ‰). We\nalso give the supervised retrieval loss ( ELLoss\n5160\nin FÃ©vry et al. (2020)), when training the EaE\nmodel. With this retrieval, EaE also can repre-\nsent the unseen entity e /âˆˆEtrain if we know the\nmention boundary of the given entity on the con-\ntext. We believe it is expected to work well, if the\nentity memory is pre-trained on the enormous text\nalong with the pre-training of the language model\nfrom the scratch. However, it might underperform\nfor the language model adaptation scenario, since\nit can fall into the problem of circular reasoning\nâ€“ the PLM does not properly represent the unseen\nentity, but it should predict which entity it is similar\nfrom the representation. Regarding the integration\nof the knowledge from the entity memory into the\nPLM, the retrieved entity representationvis simply\nadded (Peters et al., 2019) to the hidden represen-\ntations Hafter the transformer block as follows:\nËœHl = Hl + h(v) (5)\nwhere his Multi-Layer Perceptrons (MLPs).\nAdapter (Houlsby et al., 2019) is introduced to\nï¬ne-tune the PLM only with a few trainable param-\neters, instead of ï¬ne-tuning the whole parameters\nof the PLM. To adapt this original implementa-\ntion into our KALA framework, we replace our\nKnowledge-conditioned Feature Modulation with\nit, where the Adapter is used as the knowledge inte-\ngration module. We interleave the layer of Adapter\nafter the feed-forward layer (FF) and before the\nresidual connection of the transformer block. Also,\ninstead of only providing the LM hidden states as\nan input, we concatenate the knowledge represen-\ntation in Equation 3 to the LM hidden states. Note\nthat we ï¬ne-tune the whole parameters following\nour KALA setting, unlike ï¬ne-tuning the parame-\nters of only Adapter layers in Houlsby et al. (2019).\nERNIE (Zhang et al., 2019) is a notable PLM\nmodel that utilizes the external KB as an input for\nthe language model. The key feature of ERNIE can\nbe summarized into two folds. First, they use the\nmulti-head self-attention scheme (Vaswani et al.,\n2017) to contextualize the input entities. Second,\nERNIE fuses the entity representation at the end\nof the PLM by adding it to the corresponding lan-\nguage representation. We assume that those two\nfeatures are important points of ERNIE. Therefore,\ninstead of using a Graph Neural Network (GNN)\nlayer, we use a multi-head self-attention layer to\ncontextualize the entity embeddings. Then, we add\nit to a representation of the entity from the PLM,\nwhich is the same as the design in equation 5.\nKT-Net(Yang et al., 2019) uses knowledge as an\nexternal input in the ï¬ne-tuning stage for extractive\nQA. Since they have a typical layer for integrating\nexisting KB (Miller, 1995; Carlson et al., 2010)\nwith the PLM, we only adopt the self-matching\nlayer as the architecture variant of the KFM layer\nused in our KALA framework. The computation\nof the self-matching matrix in KT-Net is costly,\ni.e., it requires a large computational cost that is\napproximately 12 times larger than KALA.\nERICA (Qin et al., 2021) uses contrastive learn-\ning in LM pre-training to reï¬‚ect the relational\nknowledge into the language model. We use the\nEntity Discrimination task from ERICA on the pri-\nmary task of ï¬ne-tuning. We would like to note that,\nas reported in Section 5 of the original paper (Qin\net al., 2021), the use of ERICA on ï¬ne-tuning has\nno effect, since the size and diversity of entities and\nrelations in downstream training data are limited.\nSuch limited information rather harms the perfor-\nmance, as it can hinder the generalization. In other\nwords, contrastive learning cannot reï¬‚ect the entity\nand relation in the test dataset.\nB.5 FLOPs Computation\nIn this subsection, we give detailed descriptions of\nhow the FLOPs in Figure 1 are measured. We ma-\njorly follow the script from the ELECTRA (Clark\net al., 2020) repository to compute the approxi-\nmated FLOPs for all models including ours. For\nFLOPs computation of our KALA, we addition-\nally include the FLOPs of the entity embedding\nlayer, linear layers for h1,h2,h3,h4, and GNN\nlayer. Since the GNN layer is implemented based\non the sparse implementation, we ï¬rst calculate\nthe FLOPs of the message propagation over one\nedge, and then multiply it to the average number of\nedges per node. Also, in terms of the computation\non mentions, we consider the maximum sequence\nlength of the context rather than the average num-\nber of mentions, to set the upper bound of FLOPs\nfor our KALA. Note that, in NewsQA training data,\nthe average number of nodes is 57, the average\nnumber of edges for each node is 0.64, and the av-\nerage number of mentions in the context is 92.68.\nC Additional Experimental Results\nIn this section, we provide the analyses on the for-\ngetting of TAPT, entity memory, number of entities\nand facts, location of the KLM layer, and values of\nGamma and Beta.\n5161\n0 500 1000 1500 2000 2500 3000\nSteps\n0\n1\n2\n3Training MLM Loss\n2.2\n2.4\n2.6\nTest MLM Loss\nFigure 7: Masked Language Model loss from Task-Adaptive\nPre-Training on the domain-speciï¬c training dataset (Relation)\nand the general domain test dataset (Sampled wikipedia).\n103 104\nThe Size of Entity Memory\n67.0\n67.5\n68.0\n68.5F1 Score\n103 104\n8.4\n8.6\n8.8\n9.0\n9.2GPU Memory\n1e3\n52.0\n52.5\n53.0\n53.5\n54.0\n54.5\n55.0\nExact Match\nFigure 8: The performance (F1 score and Exact Match) and\nthe GPU memory usage on NewsQA dataset with varying the\nsize of elements in the entity memory.\nC.1 Analysis on forgetting of TAPT\nIn Figure 1, we observe that the performance of\nTAPT decreases as the number of training steps\nincreases. To get a concrete intuition on this par-\nticular phenomenon, we analysis what happens\nin the Pre-trained Language Model (PLM), when\nwe further pre-train it on the task-speciï¬c corpus.\nSpeciï¬cally, in Figure 7, we visualize the Masked\nLanguage Model (MLM) loss of TAPT on both\ndomain-speciï¬c corpus from the Relation dataset\nand general corpus from the sampled Wikipedia\ndocuments during the adaptive pre-traing. As Fig-\nure 7 shows, the test MLM loss increases while\nthe training MLM loss persistently increases as the\ntraining step increases. This result indicates that\nTAPT on domain-speciï¬c corpus may yield the\ncatastrophic forgetting of the general knowledge in\nthe PLM.\nC.2 Effects of the Size of Entity Memory\nIn this subsection, we analyze how the size of en-\ntity memory affects the performance of our KALA.\nIn Figure 8, we plot the performance of KALA\non the NewsQA dataset by varying the number of\nentity elements in the memory. Note that, we re-\nduce the size of the entity memory by eliminating\nthe entity appearing fewer times. Thus, the results\nare obtained by only considering the entities that\nappear more than [1000,100,10,5,0] times, e.g.,\n0 means the model with full entity memory. As\nshown in Figure 8, we observe that the size of the\n1 2 3 4 5 6 7\nNumber of entities\n0\n1\n2\n3\n4Improvement\nCONLL-2003\nWNUT-17\nNCBI-Disease\n1 2 3 4 5 6 7\nNumber of facts\n0\n1\n2\n3\n4\n5Improvement\nFigure 9: Performance improvements of our KALA from\nsimple ï¬ne-tuning, with varying the number of entities and\nfacts in the context on Named Entity Recognition tasks.\n1 3 5 7 9 11\nLocation of KFM within transformer layers\n67.0\n67.5\n68.0\n68.5F1 Score\nFigure 10: The performance of our KALA with varying the\nlocation of the KFM layer inside the BERT-base model. y-\naxis denotes the F1 score on NewsQA and x-axis denotes the\nlocation of the KFM layer. For instance, 11 means the case\nwhere the KFM layer is appended in the 11th transformer layer\nof BERT-base.\nentity memory is larger, the performance of our\nKALA is better in general. However, interestingly,\nwe also observe that the smallest size of the entity\nmemory shows decent performance, which might\nbe due to the fact that some parameters in the entity\nmemory are stale. For more discussions on it in-\ncluding visualization, please refer to Appendix D.2.\nFinally, we would like to note that, in Figure 1, we\nreport the performance of our KALA in the case\nof [1000,5,0] (i.e., considering entities appearing\nmore than [1000,5,0] times).\nC.3 Effects of the Number of Entity and Fact\nIn this subsection, we aim to analyze which num-\nbers of entities and facts per context are appropriate\nto achieve good performance in NER tasks. Specif-\nically, we ï¬rst collect the contexts having more\nthan or equal to the knumber of entities (or facts),\nand then calculate the performance difference from\nour KALA to the ï¬ne-tuning baseline. As shown\nin Figure 9, while there are no obvious patterns,\nperformance improvements from the baseline are\nconsistent across a varying number of entities and\nfacts. This result suggests that our KALA is indeed\nbeneï¬cial when entities and facts are given to the\nmodel, whereas the appropriate number of entities\nand facts to obtain the best performance against the\nbaseline is different across datasets.\n5162\n0.950 0.975 1.000 1.025 1.050\nValue\n0.0\n0.5\n1.0\n1.5Count\n1e6 Pre gamma\n0.04\n 0.02\n 0.00 0.02 0.04\nValue\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Count\n1e6 Pre beta\n0.94 0.96 0.98 1.00 1.02 1.04\nValue\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25Count\n1e6 Post gamma\n0.04\n 0.02\n 0.00 0.02 0.04\nValue\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Count\n1e6 Post beta\nFigure 11: Histogram of values of gamma and beta on the CoNLL-2003 dataset.\nFine-tuning\nseen\nunseen\nTAPT\nDAPT\nKALA (Ours)\nFigure 12: Visualization of contextual representations for\nseen and unseen entities on the NCBI-Disease dataset.\nC.4 Effects of the Location of KFM\nIn the main paper and Appendix B.1, we describe\nthat the location of the KFM layer inside the PLM\narchitecture is the hyperparameter. However, some-\none might wonder which location of KFM yields\nthe best performance, and what is the reason for\nthis. Therefore, in this section, we analyze where\nwe obtain the best performance in various locations\nof the KFM layer on the NewsQA dataset. Specif-\nically, in Figure 10, we show the performance of\nour KALA with varying the location of the KFM\nlayer insider the BERT-base model. The results\ndemonstrate that the model with the KFM on the\nlast layer of the BERT-base outperforms all the\nother choices. This might be because, as the ï¬nal\nlayer of the PLM is generally considered as the\nmost task-speciï¬c layer, our KFM interleaved in\nthe latest layer of BERT expressively injects the\ntask-speciï¬c information from the entity memory\nand KGs, to such a task-speciï¬c layer.\nC.5 Analysis on Values of Gamma and Beta\nTo see how much amount of value on gamma and\nbeta is used to shift and scale the intermediate hid-\nden representations in transformer layers, we visu-\nalize the modulation values, namely gamma and\nbeta, in Figure 11. We ï¬rst observe that, as shown\nin Figure 11, the distribution of values of gamma\nand beta approximately follow the Gaussian dis-\ntribution, with zero mean for beta and one mean\nfor gamma. Also, we notice that the scale of val-\nues remain nearly around the mean point, which\nsuggests that the small amount of shifting to in-\ntermediate hidden representations on transformer\nlayers is enough to contribute to the performance\ngain, as we can see in the main results of Table 1, 2.\nC.6 Detailed Efï¬ciency Comparison\nWhile we provide the efï¬ciency on FLOPs in Fig-\nure 1, we further provide the efï¬ciency on GPU\nmemory, wall time, and FLOPs for training each\nmethod in Table 6. Speciï¬cally, we measure the\ncomputational cost on the NewsQA dataset with\nBERT-base, where we use the single Geforce RTX\n2080 Ti GPU on the same machine. For our KALA,\nas we can ï¬‚exibly manage the cost of GPU mem-\nory by reducing the number of entities in entity\nmemory (See Figure 8 with Appendix C.2 for more\nanalysis on the effects of the size of entity memory),\nwe provide the experimental results on two settings\nâ€“ KALA with memory size 0.2k and 62.8k (full\nmemory). As shown in Table 6, we conï¬rm that\nthe computational cost of our KALA with the full\nmemory is similar to the cost of the more params\nbaseline that uses one additional transformer layer\non top of BERT-base. However, by reducing the\nnumber of entities in the memory, we can achieve\nbetter efï¬ciency than more params in terms of GPU\nmemory and FLOPs. Also, we observe that the\ntraining cost (i.e., Wall Time and FLOPs) of TAPT\nand DAPT is high, especially on DAPT, thus we\nverify that our KALA is more efï¬cient to train for\ndomain adaptation settings.\nD Additional Visualization Results\nHere we provide the frequency distribution of enti-\nties, additional case studies, and more illustrations\nof textual examples and embedding spaces.\nD.1 Additional Representation Visualization\nWhile we already show the contextualized repre-\nsentations of seen and unseen entities in the latent\n5163\n0 10000 20000\nEntity Frequency\n0\n1\n2Count\n1e4\n NewsQA\n0 1000 2000\nEntity Frequency\n0\n1000\n2000Count\nRelation\n0 1000\nEntity Frequency\n0\n1000\n2000Count\nMedication\nFigure 13: Distribution of frequency of entities on QA\ndatasets: NewsQA, Relation, and Medication, where almost\nall entities appear less than 10 times, while an extremely few\nnumbers of entities appear very frequently.\nspace in Figure 2 right, we further visualize them\nincluding the missing baselines of Figure 2, such\nas Fine-tuning or TAPT, in Figure 12 on the NCBI-\nDisease dataset. Similar to Figure 2, we observe\nthat all baselines fail to closely embed the unseen\nentities in the representation space of seen enti-\nties. While this visualization result does not give\na strong evidence of why our KALA outperforms\nother baselines, we clearly observe that KALA is\nbeneï¬cial to represent unseen entities in the feature\nspace of seen entities, which suggests that such an\nadvantage of our KALA helps the PLM to general-\nize over the test dataset, where the context contains\nunseen entities.\nD.2 Entity Frequency Distribution\nWe visualize the frequency of entities in Figure 13\nand 14. The entity frequency denotes the number\nof mentions of their associated entities within the\nentire text corpus of the training dataset. As shown\nin Figure 13 and 14 of QA and NER datasets, the\nentity frequency follows the long-tail distribution,\nwhere most entities appear a few times. For in-\nstance, in the NewsQA dataset, more than 20k en-\ntities among entire 60k entities appear only once\nin the training dataset, whereas one entity (CNN10)\nappears approximately 20k times. This observa-\ntion suggests that most of the elements in the entity\nmemory are not utilized frequently. In other words,\nonly few entities are accurately trained with many\ntraining instances, whereas there exists the stale\nembeddings which are rarely updated. This obser-\nvation raises an interesting research question on the\nefï¬cient usage of the entity memory, as we can see\nin Figure 8 that the small size of entity memory\ncould result in the better performance (See Ap-\npendix C.2). We leave the more in-depth analysis\non the entity memory as the future work.\n10Almost every context in NewsQA includes the text â€˜CNNâ€™\nsince they are originated from the CNN News.\n0 200 400\nEntity Frequency\n0\n2000\n4000Count\nCoNLL-2003\n0 200 400 600\nEntity Frequency\n0\n1000\n2000\n3000Count\nWNUT-17\n0 500 1000\nEntity Frequency\n0\n500\n1000Count\nNCBI-Disease\nFigure 14: Distribution of frequency of entities on NER\ndatasets: CoNLL-2003, WNUT-17, and NCBI-Disease, where\nalmost all entities appear less than 10 times, while an ex-\ntremely few numbers of entities appear very frequently.\nD.3 Additional Case Study\nIn addition to the case study in Figure 5, we further\nshow the case on the question answering task in Fig-\nure 15, like in Section 5.5, With this example, we\nexplain how the factual knowledge in KGs could be\nutilized to solve the task via our KALA. The ques-\ntion in the example is â€œwho was kidnapped because\nof her neighborâ€. We observe that DAPT answers\nthis question as Araceli Valencia. This prediction\nmay come from matching the word â€˜herâ€™ in the\nquestion to the feminine name â€˜Araceli Valenciaâ€™\nin the context. In contrast, our KALA predicts the\nJaime Andrade as an answer, which is the ground\ntruth. We suspect that this might be because of\nthe fact â€œ(Jaime Andrade, spouse, Valencia)â€ in\nthe knowledge graph, which relates the â€˜Valenciaâ€™\nto the â€˜Jaime Andradeâ€™. Although it is not clear\nhow it directly affects the modelâ€™s performance, we\ncan reason that KALA can successfully answer the\nquestion by utilizing the existing facts.\nD.4 Additional Data Visualization\nIn Figure 16 and 17, we visualize the examples of\nthe context with its seen and unseen entities and its\nrelational facts. We ï¬rst conï¬rm that the quality of\nfacts is moderate to use. For instance, in the ï¬rst\nexample of Figure 16, the fact in the context that\nOmar_bin_Laden is son of Osama_bin_Laden, is\nalso appeared in the knowledge graph. In addition,\nwe observe that there are facts that link unseen en-\ntities to the seen entities in both Figure 16 and 17.\nThus, while some of the facts in the knowledge\ngraph are not accurate, we can represent the unseen\nentities with their relation to the seen entities. We\nexpect that there is a still room to improve in terms\nof the quality of KGs, allowing our KALA to mod-\nulate the entity representation more accurately. We\nleave the study on this as the future work.\n5164\nContext PHOENIX, Arizona (CNN) â€“ Jamie Andrade had just gotten \nout of the shower when the men came to snatch him. Jamie Andrade \nwas kept in this closet for three days without food or water, police say. \nHis wife, Araceli Valencia, was mopping the kitchen in â€¦ (ellipse)â€¦\nQuestion who was kidnapped because of her neighbour?\nAnswer Jaime Andrade\nFacts (Sampled)\n(Valencia, spouse, Jaime Andrade Jr.)\n(Jamie Andrade Jr., spouse, Valencia)\nKALA (Ours)DAPT\nDAPT prediction Araceli Valencia, KALA prediction Jaime Andrade\nFigure 15: A textual example from NewsQA with predictions from each method (DAPT and KALA), and also the T-SNE plot\nof contextualized representations from the last layer of BERT obtained by each method. Grey dots indicate tokens without any\nmentions, and dots in other colors indicate tokens with mentions to the entity. We also represent sampled facts in Knowledge\nGraph we used. Blue text indicates the mention of seen entities and red text indicates the mention of unseen entities. The fact is\nrepresented as the format of (head, relation, tail). Text with yellow background indicates the ground truth answer span.\n5165\nContext MADRID, Spain (CNN) â€“ One of Osama bin Ladenâ€™s sons \nhas been denied asylum in Spain, an Interior Ministry spokeswoman \ntold CNN on Wednesday. Omar bin Laden pictured earlier this year\nduring television interview in Rome, Italy. Omar bin Laden, who is in \nhis late 20s, stepped off a plane at Madridâ€™s Barajas International \nAirport during a stopover late Monday and informed authorities that \nhe planned to request political asylum, the spokeswoman said. Bin \nLaden has publicly called on his father to abandon terrorism. He \nprepared his formal asylum request Tuesday at the airport with the \nhelp of a translator, filing it around 1 p.m., the spokeswoman said. \nThe Interior Ministry, which had 72 hours to reply to the request, was \nrequired to seek the opinion of the U.N. High Commissioner for \nRefugees on the matter. The UNHCR recommended â€¦ (ellipse) â€¦\nQuestion 1 Where was Omar previously denied?\nAnswer 1 asylum in Britain.\nFacts (Sampled)\n(Bin Laden, significant event, Flight)\n(International Airport, country, Spain)\n(International Airport, [UNK], Madrid)\n(Omar bin Laden, father, Osama Bin \nLaden)\n(Spain, diplomatic relation, Italy)\n(Osama Bin Laden, child, Omar Bin \nLaden)\n(Italy, diplomatic relation, Spain)\nQuestion 2 Did Spain give a reason for turning down the asylum?\nAnswer 2 was given\nQuestion 3 Who was denied asylum in Britain?\nAnswer 3 Omar bin Laden\nQuestion 4 What family member of Omar bin Laden was associated with terrorism?\nAnswer 4 his father\nContext (CNN) â€“ unseeded Frenchwoman Aravane Rezai produced \none of the shocks of the year on Sunday by defeating favorite Venus \nWilliams in straight sets to win the final of the Madrid Open. The 23-\nyear-old Rezai â€“ who had only claimed WTA Tour titles at Strasbourg\nand Bali prior to Madrid â€“ continued her remarkable week with a 6-2 \n7-5 victory, adding Williamsâ€™ scalp to her earlier surprise victories \nover former world number oneâ€™s Junstine Henin and Jelena Jankovic. \nWilliams, who returns to No.2 in the world behind younger sister \nSerena on Monday, lost the opening set in just 27 minutes and then \nfailed to take advantage of a 4-1 lead in the. â€œI just cannot believe \nthis,â€ world number 24 Rezai â€“ who must now enter calculations for \nthe French Open â€“ told reporters. â€œVenus played very well and Iâ€™ve \nalways respected her as a player and a champion. I just tried my best \ntoday and it worked well for me.â€ Williams, who was looking to \nsecure her 44th career title, only converted two of her 13 break points \nin the batch â€“ a statistic that contributed greatly to her defeat.\nQuestion 1 Which player was the favourite?\nAnswer 1 Venus Williams\nFacts\n(Venus Williams, sibling, Aravane Rezai)\n(Final, part of, Year)\n(Mutua Madrid Open, located in the \nadministrative territorial entity, Madrid)\n(Victories, instance of, Military rank)\n(Surprise, instance of, Military rank)\n(Mutua Madrid Open, instance of, \nMilitary rank)\n(Final, instance of, Military rank)\nQuestion 2 Which title number was this?\nAnswer 2 44th\nQuestion 3 When did the Mardrid Open final take place?\nAnswer 3 Sunday\nFigure 16: NewsQA examples with facts in Knowledge Graph we used in this work. Blue text indicates the mention of seen\nentities and red text indicates the mention of unseen entities. The fact is represented as the format of (head, relation, tail).\n5166\nContext The adenomatous polyposis coli ( APC ) tumour - suppressor \nprotein controls the Wnt signalling pathway by forming a complex\nwith glycogen synthase kinase 3beta ( GSK - 3beta ), axin / conductin\nand betacatenin.\nFacts (Sampled)\n(complex, subclass of,  protein)\n(GSK, instance of, protein)\n(glycogen, instance of, protein)\n(APC, instance of, protein)\nContext HLA typing for HLA - B27, HLA - B60, and HLA - DR1\nwas performed by polymerase chain reaction with sequence - specific \nprimers, and zygosity was assessed using microsatellite markers.\nFacts (Sampled)\n(microsatellite, subclass of, primers)\n(DR1, instance of, microsatellite)\n(microsatellite, subclass of, typing)\nContext We identified four germline mutations in three breast cancer\nfamilies and in one breast - ovarian cancer family. among these were \none frameshift mutation, one nonsense mutation, one novel splice site \nmutation, and one missense mutation.\nFacts (Sampled)\n(frameshift mutation, subclass of, \nGermline mutations)\n(Nonsense mutation, subclass of, \nGermline mutations)\n(splice site mutation, subclass of, \nGermline mutations)\n(missense mutations, subclass of, \nGermline mutations)\n(Nonsense mutation, subclass of, cancers)\n(frameshift mutation, subclass of, cancers)\n(missense mutations, subclass of, cancers)\nContext A nonsense mutation in exon 17 ( codon 556 ) of the RB1 \ngene was found to be present homozygously in both the retinal and \nthe pineal tumours.\nFacts (Sampled)\n(retinal, instance of, gene)\n(Nonsense mutation, subclass of, gene)\nContext Sixteen different p16 germline mutations were found in 21 \nfamilies, while one germline mutation, Arg24His, was detected in the \nCDK4 gene.\nFacts (Sampled)\n(p16, subclass of, Germline mutations)\n(Germline mutations, subclass of, gene)\n(p16, instance of, gene)\nContext Aspartylglucosaminuria ( AGU ) is a rare disorder of \nglycoprotein metabolism caused by the deficiency of the lysosomal \nenzyme aspartylglucosaminidase ( AGA ).\nFacts (Sampled)\n(Aspartylglucosaminuria, subclass of, \ndeficiency)\nContext Detection of heterozygous carriers of the ataxia -\ntelangiectasia ( ATM ) gene by G2 phase chromosomal \nradiosensitivity of peripheral blood lymphocytes.\nFacts (Sampled)\n(ATM, instance of, gene)\n(G2 phase, part of, blood)\n(G2 phase, instance of, gene)\nContext Recently, we reported five Austrian families with generalized \natrophic benign epidermolysis bullosa who share the same COL17A1 \nmutation.\nFacts (Sampled)\n(epidermolysis bullosa, instance of, \nmutations)\nFigure 17: NCBI-Disease examples with facts in Knowledge Graph we used in this work. Blue text indicates the mention of\nseen entities and red text indicates the mention of unseen entities. The fact is represented as the format of (head, relation, tail).\n5167",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8351497650146484
    },
    {
      "name": "Forgetting",
      "score": 0.7121365666389465
    },
    {
      "name": "Domain knowledge",
      "score": 0.617999255657196
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5807101726531982
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5427184104919434
    },
    {
      "name": "Language model",
      "score": 0.5311008095741272
    },
    {
      "name": "Task (project management)",
      "score": 0.52665114402771
    },
    {
      "name": "Domain adaptation",
      "score": 0.5066624283790588
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4966469407081604
    },
    {
      "name": "Natural language",
      "score": 0.45381516218185425
    },
    {
      "name": "Machine learning",
      "score": 0.42850080132484436
    },
    {
      "name": "Question answering",
      "score": 0.414425253868103
    },
    {
      "name": "Natural language processing",
      "score": 0.3711255192756653
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 31
}