{
  "title": "Evaluating Intelligence and Knowledge in Large Language Models",
  "url": "https://openalex.org/W4401227497",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2110177999",
      "name": "Francesco Bianchini",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2110177999",
      "name": "Francesco Bianchini",
      "affiliations": [
        "University of Bologna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2031649430",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4394653315",
    "https://openalex.org/W3203692761",
    "https://openalex.org/W2303924600",
    "https://openalex.org/W4386122321",
    "https://openalex.org/W2039234396",
    "https://openalex.org/W2107019937",
    "https://openalex.org/W2511953903",
    "https://openalex.org/W4254865196",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4206577067",
    "https://openalex.org/W2500042688",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W4321601541",
    "https://openalex.org/W2106583109",
    "https://openalex.org/W4247579632",
    "https://openalex.org/W4236521339",
    "https://openalex.org/W2798445359"
  ],
  "abstract": "Abstract In recent times, particularly in the last few years, we have observed the rise of numerous Artificial Intelligence and Natural Language Processing semantic technologies. These advancements have subtly yet profoundly transformed our understanding of knowledge and truth, and the mechanisms for expressing, preserving, and disseminating them. This article aims to explore the dual challenge of assessing the effects of Large Language Models and associated semantic technologies on text dissemination and production, especially across the Internet. It specifically examines the implications for trust in online knowledge repositories, the creation of indirect or deliberate forms of ignorance, and the general perception of AI as a critical component of autonomous systems from the users’ viewpoint. The discussion will also consider potential strategies to mitigate the epistemic risks posed by the employment of AI semantic tools, in both suitable and unsuitable scenarios. The suggested approach contributes to the debate on AI intelligence measurement, proposing the evaluation of an AI system’s expected intelligence (as perceived by users) as a means to address the challenges associated with the “knowledge” generated by these systems. My claim is that measuring the expected intelligence in AI systems places humans at the forefront of the issue without necessitating a precise definition of intelligence for AI systems. This approach preserves therefore the essential attribute of these systems: intelligence.",
  "full_text": "Vol.:(0123456789)\nTopoi (2025) 44:163–173 \nhttps://doi.org/10.1007/s11245-024-10072-5\nEvaluating Intelligence and Knowledge in Large Language Models\nFrancesco Bianchini1 \nAccepted: 26 June 2024 / Published online: 1 August 2024 \n© The Author(s) 2024\nAbstract\nIn recent times, particularly in the last few years, we have observed the rise of numerous Artificial Intelligence and Natural \nLanguage Processing semantic technologies. These advancements have subtly yet profoundly transformed our understand-\ning of knowledge and truth, and the mechanisms for expressing, preserving, and disseminating them. This article aims to \nexplore the dual challenge of assessing the effects of Large Language Models and associated semantic technologies on text \ndissemination and production, especially across the Internet. It specifically examines the implications for trust in online \nknowledge repositories, the creation of indirect or deliberate forms of ignorance, and the general perception of AI as a criti-\ncal component of autonomous systems from the users’ viewpoint. The discussion will also consider potential strategies to \nmitigate the epistemic risks posed by the employment of AI semantic tools, in both suitable and unsuitable scenarios. The \nsuggested approach contributes to the debate on AI intelligence measurement, proposing the evaluation of an AI system’s \nexpected intelligence (as perceived by users) as a means to address the challenges associated with the “knowledge” gener -\nated by these systems. My claim is that measuring the expected intelligence in AI systems places humans at the forefront \nof the issue without necessitating a precise definition of intelligence for AI systems. This approach preserves therefore the \nessential attribute of these systems: intelligence.\nKeywords Artificial intelligence · Large language models · Knowledge · Expected intelligence · Semantic tools\n1 Introduction\nIn recent years, especially the most recent ones, we have \nwitnessed the emergence of numerous Artificial Intelligence \n(AI) and Natural Language Processing semantic technolo-\ngies, which have subtly but undeniably revolutionized our \nperception of knowledge, truth, and the tools used to express, \npreserve, and communicate them. Of course, over the past \ntwo decades, the proliferation of the Internet and various \nsocial networks has already challenged traditional methods \nof information dissemination, exposing phenomena such as \ncognitive self-segregation and informational sectarianism. \nThese phenomena, including fake news, filter bubbles, and \necho chambers, have human motivations as well as algorith-\nmic causes (Falxman et al. 2016). However, the landscape \nhas become even more complex with the introduction of \nincreasingly powerful tools for processing, comprehending, \nand generating human knowledge and natural language. \nThese tools range from ontologies and knowledge graphs \nto Neural Language Models and Large Language Models \n(LLM). The latter, in particular, pose a challenge due to their \nimmense computational power, allowing them to handle nat-\nural language in a manner that is (almost) indistinguishable \nfrom human processing—a realization that appears to fulfill \none of the earliest dreams or fears surrounding AI.\nThe extensive literature produced in recent years on \nLLMs based on neural networks is undoubtedly a reflection \nof their widespread adoption beyond the realm of experts. \nThe accessibility of generative language models like GPT \nto the general public has propelled attention toward these \nmodels for at least two reasons: their ubiquitous use and \nthe remarkable results they produce compared to earlier \ntechnologies. These two factors account for the significant \nscientific, as well as social, philosophical, and epistemo-\nlogical interest in these models. They find applications in \nnumerous tasks, even in an era like the present one, where \nAI is ubiquitous, including tasks such as understanding and \ncreatively writing texts based on natural language prompts. \nThe importance and impact of this disruptive attention \nseem to exceed that reserved for other generative AI, such \n * Francesco Bianchini \n francesco.bianchini@unibo.it\n1 University of Bologna, Bologna, Italy\n164 F . Bianchini \nas those dedicated to image generation (e.g., DALL-E or \nMidjourney). This discrepancy is not surprising, given the \ndifferent ways humans perceive original creations. It is more \nplausible to perceive an original image within a set of ran-\ndomly scattered spots than to discern an original sentence \namong randomly written letters or words. Meaning does not \nappear to reside, even partially, in the eye of the beholder or \nthe reader, but rather in intentional constructions of sense, \ngoverned by well-defined rules and precise communicative \nintentions. A well-formed expression in natural language is \ndiametrically opposed to chance. Hence, it is astonishing \nthat such accuracy can be achieved by algorithmic systems \naptly labeled “stochastic parrots” (Bender et al. 2021)—sys-\ntems that extensively leverage both input data and computa-\ntional power for processing. This processing, as is typical in \ncomplex neural networks, relies on mathematical-statistical \nmodels.\nIs it accurate to classify these systems as mere “parrots”, \nimplying they are simple imitative repeaters? The type of \nimitation they engage in is far removed from both human-\nlike repetition (which involves various cognitive and percep-\ntual dimensions) and mere computational string matching. \nIf we envision a continuum with human-like repetition on \none hand and straightforward computational matching on the \nother, these systems fall somewhere in between, employing \ncomplex techniques and computational power for imitation, \nand perhaps even demonstrating a form of creativity in their \noutput. While imitation typically suggests deception rather \nthan the accurate production of knowledge, it serves as the \nfoundation for various forms of cultural learning (Tennie \net al. 2009), and therefore is not inherently negative. Thus, \nthe crucial factor lies in evaluating the quality of imitation, \ndistinguishing between performances that enhance knowl -\nedge and those that perpetuate ignorance. This evaluation \nbecomes particularly significant for semantic technologies \ncapable of generating natural language in a highly imitative \nmanner, according to specific parameters. Consequently, the \nissue outlined here raises two fundamental questions: first, \nthe notion of imitation in artificial systems, starting from \nhow it was conceptualized by Turing; and second, the evalu-\nation of artificial systems themselves for their performance \nand results, a topic that has gained relevance in the ongoing \nAI debate.\nThe article will delve into the dual issue of examining \nthe impact of LLMs and related semantic technologies on \nthe dissemination and production of texts, especially via the \nInternet. Specifically, it will evaluate this impact concern-\ning potential breaches of trust in the knowledge available \nthrough online repositories, the generation of indirect or \nintentional forms of ignorance, and the broader perception \nof AI as a defining element of autonomous systems from \nthe user’s perspective. The latter point will be explored to \noutline potential measures to mitigate the epistemic risks \nposed by the use of AI semantic tools, both in appropriate \nand inappropriate contexts.\nThe article will proceed by initially discussing the rela-\ntionship between Turing’s proposal on imitation and LLMs \n(“Turing and LLMs” section), followed by an examination \nof the evaluation of intelligence as demonstrated through \nperformance and results in AI (“Intelligence Evaluation in \nAI” section). It will then address the challenge of measuring \nexpected intelligence in artificial systems, particularly those \nbased on AI (“Measuring Expected Artificial Intelligence” \nsection), and the issue of quantifying intelligence in LLMs \nas a means of addressing the epistemic challenges that their \nwidespread use may pose (“Measuring Intelligence in LLMs \nas an Antidote to Ignorance” section). I will then show and \ndiscuss an example of produced scientific knowledge by an \nLLM (“An Example on LLMs and Scientific Knowledge” \nsection), and finally I will draw conclusions and outline \npotential short-term developments (“Conclusion” section).\n2  Turing and LLMs\nTuring deals with the challenge of linguistic interaction \nwith computers, initially addressing a broader question \nabout machines: “Can machines think?” (Turing 1950). \nThis theme was not novel to him, and its development was \ngradual. For several years, he had been exploring the pos-\nsibility of answering this question posed by the emergence \nof new digital computers, pondering whether a machine \ncapable of displaying intelligent behavior could indeed exist \n(Turing 1948). The response to this inquiry, too intricate \nto be directly provided, hinges on the concept of imitation, \nas embodied in the Imitation Game, serving as a test for \nthe machine’s capabilities. But what kind of test? Or rather, \nwhat kind of imitation? The behavior to be imitated, implic-\nitly assumed as intelligent, is linguistic—the only domain \nTuring identifies as potentially impervious to counterfeit-\ning when conducted correctly. The criterion for correctness \nis that of interactive linguistic conversation, open to any \ntopic. Such proficiency symbolizes intelligence due to the \ninfinitely productive and nuanced nature of language from a \nsemantic standpoint. Essentially, one cannot feign fluency in \na language convincingly. Either one speakes it or not.\nFrom Turing’s conception emerges the well-known \nTuring Test, sparking enduring debates about its true scope \nand limitations (Moor 2003), while also laying the ground-\nwork for discussions on the essence of AI and the feasibil-\nity of its recognition and evaluation. Efforts to demonstrate \nhow easily a human user could be deceived and bypass the \nTuring Test in its original form emerged swiftly, giving \nrise to phenomena such as the ELIZA effect, named after \nthe program which, through syntactic tricks, managed to \ndupe human interlocutors (Weizenbaum 1966 ). However, \n165Evaluating Intelligence and Knowledge in Large Language Models  \nTuring’s underlying intention in presenting the seminal \nquestion about machine thinking appears broader. These \n“machines” are not generic machinery but rather software, \ni.e., computer programs. As such, they operate on rigid logi-\ncal principles (programming languages are rooted in classi-\ncal logic) but must also exhibit general linguistic behavior \nthat inherently transcends rigid classical logical constraints \n(natural language is intrinsically ambiguous, laden with sub-\ntexts and ellipsis, and infinitely expressive on various levels), \nand they must do so interactively (merely generating text is \ninsufficient).\nOne might question whether the genuine linguistic behav-\nior proposed by Turing truly constitutes evidence of think -\ning ability. Is it genuine thought in the terms that a human \nbeing would define it? Turing’s answer is affirmative. From \nthis perspective, the Turing Test remained unconquered for \na considerable period. However, with the advent of LLMs, \nthe situation appears to be changing, as they seem to have \nachieved Turing’s envisioned goal. Nonetheless, the inquiry \nregarding the actual presence of thought partially contradicts \nhow Turing frames the question. Initially, he dismisses this \ntype of query, opting instead to contextualize it within the \nframework of imitation. The issue of genuine thinking in \nAI systems has sparked ongoing debate throughout history, \ninitially leading to the distinction between strong and weak \nartificial intelligence, and subsequently fueling discussions \nsurrounding artificial general intelligence.\nSetting aside this primarily philosophical debate, another \ntrend that has emerged in the course of AI history revolves \naround not so much thought, but rather knowledge. To what \nextent do these systems possess knowledge, or can they be \nattributed with knowledge? What form of knowledge does \nan AI system possess? The debate over knowledge repre-\nsentation has been a persistent theme throughout the his-\ntory of AI. In Turing’s terms, a machine—i.e., a computer \nprogram—equipped with a repository of symbolic knowl-\nedge explicitly encoded in a logical format demonstrates \nsyntactic knowledge. However, the semantic efficacy of this \nknowledge clashes, on one hand, with the issue of refer -\nence or grounding of its symbols (Harnad 1990), and on the \nother, with the rigidity of logical-symbolic representational \nsystems.\nResponses to the question of whether an AI system is gen-\nuinely intelligent in relation to the knowledge it possesses \nhave taken two distinct paths. The affirmative stance sug-\ngests that systems equipped with knowledge indeed exhibit \nintelligence. This category encompasses knowledge systems, \nexpert systems, and all knowledge-based models leveraging \na knowledge base to address variously complex problems. \nAdditionally, it includes systems whose semantic knowledge \nrelies on syntactic structures. This perspective contrasts with \nthe view that regards such knowledge as fake  knowledge \ninasmuch merely symbolic manipulation, devoid of genuine \nsemantic content. The underlying assumption here is that \nsemantics can not emerge solely from syntactic structures, \nthereby rendering these systems incapable of being classified \nas intelligent. Such a lack of genuine knowledge would effec-\ntively render them ignorant in the truest sense of the term. \nHowever, LLMs seem to possess characteristics that tran-\nscend the dichotomy of knowledge equating to intelligence \nand fake knowledge equating to ignorance/non-intelligence.\nThe knowledge representation challenge in LLMs is \nequivalent to that in neural networks. LLMs are a specific \ntype of deep neural network, wherein their knowledge is \nwidely distributed throughout the network, as is typical in \nneural networks. The intelligence they exhibit is operation-\nally tied to mathematical-statistical processes, categorizing \nthem as black box systems—systems whose operations lack \nsemantic interpretability, making their performance difficult \nto explain. While these issues have long been recognized in \nAI, they experience an unusual resurgence in LLMs due to \ntheir increasingly remarkable performances, which do not \nresolve the aforementioned problems: the absence of explicit \nknowledge and the explanation of procedural operations.\nLLMs are generative AI models capable of producing \nnatural language texts based on questions or prompts in natu-\nral language. They rely on a specific type of deep neural \nnetwork architecture known as Transformers, which employ \nmechanisms for statistically modeling language production. \nAmong these mechanisms, attention mechanism stands out \nas particularly significant (Vaswani et al. 2017). While \nprevious models for language modeling, such as recurrent \nneural networks or Long Short-Term Memory (Hochreiter \nand Schmidhuber 1997), already existed, Transformers dis-\ntinguish themselves by not only utilizing a greater number \nof parameters1 (in the billions) but also by their conceptual \ndifferences. The attention mechanism, akin to the cognitive \nfunction of attention but focused solely on word occurrence \ncontext, serves to capture word embeddings—the contextual \nrepresentation of a word in a sequence—and return a vec-\ntor space where proximity denotes semantic similarity: “An \nattention function can be described as mapping a query and \na set of key-value pairs to an output, where the query, keys, \nvalues, and output are all vectors. The output is computed \nas a weighted sum of the values, where the weight assigned \nto each value is computed by a compatibility function of the \nquery with the corresponding key” (Vaswani et al. 2017, p. \n6003). This mechanism is exploited in a particular version, \nself-attention, which derives from the implementation of \ndecomposable forms of attention (Parikh et al. 2016) and, \n1 In broad terms, parameters refer to the coefficients of a model, \nencompassing both weights and biases, which undergo adjustment \nduring the model's training process. The quantity of parameters \nexpands in tandem with the proliferation of nodes and connections \nwithin the model.\n166 F . Bianchini \nput in rough terms, allows, in the encoding and decoding, \nto process the elements of a sequence in the transition from \none layer to another with the most relevant ones in the same \nsequence, in order to correlate them with each other. This \nmechanism is “self” as it connects the sentences with their \nmost relevant elements, by representing the context in a vec-\ntorial way. Particularly in its self-attention version, it facili-\ntates processing relevant elements within a sequence transi-\ntion from one layer to another, correlating them with each \nother and providing context in a vectorial manner. Moreo-\nver, it achieves computational efficiency compared to recur-\nrent layers, especially in domains where sequence length is \nmore critical than dimensionality, such as linguistic strings. \nConsequently, this statistical contextualization, based on \nextensive training datasets and ongoing refinements, yields \na semantic capacity or impression thereof, encompassing \nsyntactic processing treated similarly in terms of vector rep-\nresentations (words and word contexts).\nWithout delving too deeply into further technical aspects, \nthis mechanism enables the assignment of different vector \nrepresentations to the same word based on its context. A \nseries of scores is assigned to the elements of the word \nsequence (the sentence in question), indicating the probabil-\nity that words in the sentence are more or less relevant. The \nmore relevant the words, the more “attention” they receive. \nThe result, which may appear unexpected but aligns with \nthe mathematical-statistical principles governing neural \nnetworks, is that predicting the next word in text generation \noccurs stochastically. However, due to the vast number of \ncomputations involved, the output aligns closely with user \nexpectations. This outcome has seen a continual improve-\nment in appropriateness across various subsequent versions \nof LLMs, resulting in texts that are increasingly challenging \nto distinguish from those produced by humans.\nFrom a historical perspective, it’s notable that the initial \napplications of Transformers, as described in the seminal \narticle by Vaswani and colleagues, were related to machine \ntranslation—one of the earliest topics addressed in the field \nof language automation and NLP, with significant attention \nto linguistic modeling since 2010 (Bahdanau et al. 2014). \nMachine translation necessitates precise language mod-\neling and presents a range of semantic challenges related to \nsentence and text context. However, despite the numerous \nlinguistic questions it raises, from an epistemological stand-\npoint, it’s important to note that this is not Turing’s concept \nof imitation and linguistic interaction. At most, it can be \nargued that the techniques initially developed for machine \ntranslation, as they contribute to text production, have been \neffectively repurposed for interactive text generation. What \nimplications can be drawn from this shift from machine \ntranslation to text production?\nTo address these questions, it’s essential to recognize that \nthe comprehension capacity of these models does not equate \nto what it is typically meant as human understanding, dis-\ntancing them from human cognition. However, their behav-\nior closely resembles certain aspects of human language \nuse. LLMs, for instance, are pre-trained and can interact \nin natural language via prompts, even on topics they have \nnot been explicitly trained on. LLMs can respond to queries \nwith minimal guidance or even without any examples related \nto the task at hand (Brown et al. 2020). Nevertheless, their \nresponses align with the user’s expectation of coherent and \nhuman-like text. Particularly in the latest releases, such as \nGPT-4, the performance reaches such levels that one can \nperceive this is the Turing’s concept of imitation coupled \nwith linguistic interaction. Furthermore, in LLMs (self-)\nlearning continues to be an active element post-training, \noften supplemented by supervised fine-tuning by humans, \nalthough this does not impart semantic knowledge. Instead, \nit serves to tailor the system to user preferences (Ouyang \net al. 2022). Thus, the shift from machine translation to \ntext generation applications signifies the potential to pass \nthe Turing test in its original form—emphasizing imita-\ntion and linguistic interaction without involving embodied \naspects—based on extensive use of learning techniques, a \nnotion underscored by Turing himself (1950). According \nto Turing’s criteria, these models would likely pass the test \nfor a duration longer than that initially proposed by him to \nconsider it successfully passed. However, something still lies \noutside Turing’s framework. In Turing’s terms, LLMs are \nessentially machines performing verbal behavior and that \ndo not know what they are saying. They computationally \ngenerate a right word sequence by computing the probability \nof sequences of token. In essence, they predict language and \ndo not produce language. This underscores their generative \ncapability; while impressive, it is a good performance, not \nintelligence. So, what precisely are they? Are they simply a \nmodern iteration of the Eliza Effect? If so, one could argue \nthat this does not align with Turing'’ concept of intelligence, \nand thus, these machines can not be deemed truly intelligent.\n3  Intelligence Evaluation in AI\nLabeling the operation of these models as simplistic when \naddressing their intelligence may not do justice to the \nfunctionality that users perceive as intelligent. Is it then \nappropriate to attribute intelligence to LLMs? If affirma-\ntive, how should we frame this intelligence? This line of \ninquiry is intimately tied to their application in domains of \nknowledge. The information that LLMs provide with their \nanswers is in some way a form of knowledge that adds to \nthe user’s knowledge with all the problems that systems so \ndifferent from human beings generate when knowledge is \nat stake. Addressing these challenges is crucial, not a mere \nafterthought. Assessing their intelligence might offer some \n167Evaluating Intelligence and Knowledge in Large Language Models  \ninsights. The evolution of the Turing Test over the years \n(French 2000) has paved the way for a nuanced discussion \non artificial intelligence measurement in AI entities. This \ndiscussion has moved beyond Turing’s binary framing of \nintelligence (the simple yes/no question on the presence of \nintelligence) to a multifaceted and graded approach. Such \nprogress also demands a broader reflection on the definition \nof intelligence—an enduring debate in the context of AI and \nits correlation with human beings.\nThe assessment of intelligence in AI systems can be seen \nthrough two principal lenses: first, as a collection of specific \nskills tailored to particular tasks; and second, as a broader \ncapacity for learning and performing in an open-ended man-\nner (Hernandez-Orallo 2017a; Chollet 2019). This dichot-\nomy stems from the understanding of natural intelligence \nfrom a cognitive perspective, also informing the develop-\nment of potential measurement methodologies (Hernandez-\nOrallo 2017b). In the former approach, the focus is on quan-\ntifying an AI system’s accuracy in performing its designated \ntasks. Such measurement is a fundamental process, neces-\nsary to gauge the AI’s effectiveness in achieving its intended \npurpose. However, this method does not account for the AI’s \nadaptability to novel situations; there is no generalization to \nunprecedented situations, neither of the program (narrow \ngeneralization) or the developer (broad generalization). Sim-\nply put, the AI operates within the confines of its program-\nming, adept at handling specific tasks without venturing \nbeyond the domain it was originally designed to operate in.\nIn the latter approach, the AI system is appraised for its \nversatility and the ability to transcend the confines of spe-\ncific tasks, demonstrating its utility across various domains, \nincluding those unforeseen by its creators. Evaluating such \nadaptability entails navigating a set of unique and intricate \nchallenges. The capacity for domain generalization may \nserve as an assessable criterion. This perspective encom-\npasses more extensive concerns, which have fueled some of \nthe grandest aspirations in AI, dating back to Newell et al. \nGeneral Problem Solver (1959). Bridging theoretical con-\ncepts with practical applications, recent endeavors aim to \ncapture the essence of AI generalization. On the futuristic \nend, there is the pursuit of Artificial General Intelligence; \non the pragmatic front, the development of cognitive archi-\ntectures like SOAR or ACT-R (Anderson 1983) has a well-\nestablished history of studies and research. These architec-\ntures, whether symbolic, subsymbolic, or hybrid, often adopt \na modular design. Their shared goal is to facilitate an AI \nsystem's acquisition of behaviors that are transferable across \nvarious domains through the incorporation of modules spe-\ncialized in different cognitive functions.2 Nonetheless, while \nAI performance can be measured against a diverse array of \nparameters, assessing its generalization potential is inher -\nently more nebulous. Considering the multitude of potential \ndomains for the application of an AI system, the challenge is \ncompounded by the fact that these domains are often unde-\nfined, interlinked, and overlapping, complicating the precise \nevaluation of behaviors or performances. In psychology and \nneuroscience, specific methodologies, such as the subtrac-\ntion method, are employed to address these issues. Translat-\ning these approaches to the AI field proves to be significantly \nmore challenging.\nA measurement, as opposed to a qualitative assessment, \nprovides a value scale that allows for the precise quantifica-\ntion of success in relation to certain desired attributes or \nparameters. It also enables comparison between different \nsystems designed for identical tasks, serving as an additional \nmechanism for the control and enhancement of AI systems, \nwhich are primarily characterized by autonomous behavior. \nTherefore, while measuring performance means offering a \nvalue scale to quantify accuracy, assessing the intelligence \nof AI systems typically involves evaluating task-specific per-\nformance on a scale. Hernandez-Orallo ( 2017a) delineates \nthree classes of methods and metrics currently employed for \nevaluating task-specific performance: (a) human discrimina-\ntion; (b) problem benchmarks; (c) peer comparison. These \nclasses of methods and metrics pertain to a type of black-box \nevaluation in AI, stemming from the increasing complexity \nand stochastic behavior of modern systems. This complex-\nity renders traditional white-box evaluations, typical of less \ncomplex AI systems from more traditional approaches like \nsymbolic AI, impractical.\nRegarding the three proposed categories, human dis-\ncrimination involves informal techniques used by humans \nto evaluate AI behavior and performance, such as observa-\ntion and interviews. This method is typical in psychologi-\ncal disciplines and has a well-known precedent in AI: the \nTuring Test and its variants. The approach based on prob-\nlem benchmarks is more common in AI, relying on publicly \navailable collections of problems and solutions generated \nby different AI systems. Knowing both the problem to be \nsolved and the performance of other AI systems in advance \ncan lead to biased evaluations. This issue could be mitigated \nusing anonymized benchmarks or dynamic problem genera-\ntors within a certain class of similar problems during the \nsystem’s performance evaluation. The approach of peer con-\nfrontation contrasts the performance of different AI systems \non the same task, potentially including multi-agent systems \ncompeting to achieve specific objectives in a certain domain. \nThis approach is effective in domains involving direct com-\npetition, such as challenges or games, or indirect competi-\ntion, where a performance ranking is established. Evalua-\ntion here hinges on the ability to unambiguously identify \nthe winner or top performers. From a general point of view, \nit is worth pointing out that the first approach is subjective, 2 For a survey and discussion, see Lieto (2021).\n168 F . Bianchini \nwhile the latter two rely on more objective criteria. Bench-\nmarks use a standard as the reference parameter, whereas \npeer comparison uses the performance of other systems as \nthe yardstick. Each method has its strengths and weaknesses, \nyet they enable some level of measurement. Nevertheless, \nthese are applicable to specific tasks. The question remains: \nhow does one measure generality?\nEvaluating the generality of AI systems involves assess-\ning broader cognitive abilities, making the cognitive per -\nspective not just relevant, but essential. This approach \nenables the measurement of an AI system’s generality in \ncognitive terms. However, this method is not without its \npitfalls, such as the risk of anthropocentrism. This arises \nparticularly when skills are defined in terms of human cog-\nnitive abilities and measured using techniques standardized \nfor humans, then adapted for AI systems. An example is the \nadaptation of psychometric tests in Psychometric AI (Bring-\nsjord 2011). Conversely, employing more objective or less \ncognitively-oriented methods may lead to a lack of trans-\nparency in assessments. For instance, standards rooted in \nalgorithmic information theory may yield more precise met-\nrics, yet the essence of what is being evaluated or measured \nremains in question. The crux of the issue is that complexity \nand information content do not have a direct correlation with \nintelligence.\n4  Measuring Expected Artificial Intelligence\nThe impasse between task-specific AI systems and general-\nabilities AI systems appears insurmountable. However, \ntwo potential solutions emerge. The first solution pro-\nposes accepting that AI systems lack genuine intelligence. \nAccording to Floridi (2023), AI systems merely act without \nembodying intelligence, making it nonsensical to discuss \nor measure intelligence in their context. This perspective is \ngrounded in the belief that an intelligent outcome does not \nnecessarily stem from intelligent behavior. While this may \nhold some truth, it’s a rather extreme stance. It dismisses \nthe possibility of recognizing certain tools as intelligent, \nwhich, by conventional standards, are considered intelli-\ngent and perform tasks deemed intelligent by humans. This \nviewpoint, however, overlooks the significance of the knowl-\nedge AI can generate and its impact on human epistemic \nand communicative practices. A second solution focuses on \nthe social/interactive aspects of AI systems, as highlighted \nby Cristianini et al. (2023). This approach is predicated on \nthe idea that, in scenarios involving human interaction with \nAI systems (which is almost always the case), the attribu-\ntion of intelligence by the user, often in real-time, is crucial \nfor achieving optimal outcomes and interactive experiences. \nThis perspective emphasizes the importance of the perceived \nintelligence in user-AI interactions, advocating for a more \nnuanced understanding of AI systems’ roles and capabilities.\nMy claim is that measuring the expected intelligence in \nAI systems places humans at the forefront of the issue with-\nout necessitating a precise definition of intelligence for AI \nsystems. This approach preserves the essential attribute of \nthese systems: intelligence. Additionally, assigning intelli -\ngence is crucial for the correct epistemic, practical, and ethi-\ncal engagement of AI systems by users. Without a cognitive \nframework for understanding AI systems, there’s a risk of \nmisuse, potentially skewing the system’s performance and \nthe validity of its knowledge outputs. This perspective also \nallows for the assessment and valuation of outputs generated \nby LLMs.\nConsequently, a pertinent question arises: how can one \nmeasure expected intelligence in AI systems? The proposed \nanswer involves four key criteria, creating a framework for \nattributing intelligence from the users standpoint:\n1. Pre-use Attribution: Attributing a value to the system’s \nintelligence based on preliminary knowledge of the sys-\ntem before interaction.\n2. In-use Attribution: Attributing a value to the system’s \nintelligence during its operation or interaction.\n3. Outcome Evaluation: Rating the intelligence based on \nthe system’s performance outcomes\n4. Variability Assessment: Analyzing the fluctuation of \nintelligence ratings through continuous or repeated \nengagement with the AI system over time.\nThe four key criteria incorporate, at least in part, some \nideas on the assessment of AI systems already present in \nthe literature regarding the evaluation of task-specific per -\nformance. For example, criterion three pertains to the eval-\nuation of the results produced by the system, an ex post \nassessment based on the accuracy and compliance with the \nrequired task demonstrated at the end of its performance. \nCriterion 2 aligns with this approach but emphasizes the \nuser’s conscious interaction with the AI system. Here, “use” \nshould be understood not as the mere utilization of a tool, \nbut as the co-evolution of the system’s and the user’s behav-\nior in producing a result or performance in a coordinated \nmanner. In this context, the user should be able to evaluate \nthe extent to which her contribution influences the desired \nand valid performance of the system and how much the sys-\ntem contributes to the task she is implementing. This dual \nassessment requires consideration of the dynamic interaction \nbetween the user and the system, which shapes the behavior \nof both the AI system and the user.\nThe two criteria that deviate most from standard meth-\nods of evaluating an AI system are the first and the last. \nCriterion 1 requires the user to consciously, and before, \nevaluate the degree of AI expected from the system and the \n169Evaluating Intelligence and Knowledge in Large Language Models  \nlevel of interactivity anticipated. This criterion goes beyond \nmerely recognizing that the system is an AI, aligning with \nthe transparency principle advocated by many policy bodies, \nwhich mandates the explicit declaration that the system is \nan AI. According to this criterion, the user is encouraged to \nconsider in advance the type of intelligence the system will \nexhibit, the extent of this intelligence, and the forms it will \ntake in its outputs. This allows for a conscious interaction \nbased on self-set parameters. In other words, the user will \nnot only focus on receiving intelligent results but will also \nself-evaluate these results within the context of intelligent \nbehavior, rather than seeing them as random, instrumental, \nor mechanical. Criterion 4, on the other hand, is particu-\nlarly crucial for generative AI systems and those capable \nof modifying their performance over time through data \naccumulation and self-learning processes, both from col-\nlective and individual users’ interactions. The user’s role \nis to evaluate whether there are fluctuations in the system’s \nexhibited intelligence, whether it improves or deteriorates, \nand whether it poses any risks (such as negative, misleading, \nbiased, or poor performance, including epistemic risks). The \nincreasingly complex and user-adaptive nature of these AI \nsystems makes real-time, general-level evaluation challeng-\ning, positioning the user as the primary controller/evaluator \nto safeguard themselves. An informed and aware considera-\ntion of the AI system’s intelligence can enhance the system’s \nefficiency and reduce unpredictability.\nThe four key criteria proposal offers two primary advan-\ntages. Firstly, it simplifies the creation of a metric by utiliz-\ning a spectrum of values assigned to each item. This allows \nfor the individual assessment of each type of AI system, with \nthe complexity or detail of the metric tailored to specific \nrequirements. Secondly, it embodies the practical application \nof a fundamental principle: intelligence is attributed where \nintelligence is expected, and vice versa. Consequently, the \nrecognition of intelligence in an AI system aligns with the \nuser’s anticipation of intelligent behavior that is pertinent to \nhuman cognitive traits. This approach enables individuals \nto assess not only the AI system’s intelligence but also their \nlevel of trust in it, alongside understanding its risks, capa-\nbilities, and limitations. Over time, this strategy aims to fos-\nter responsible engagement between users and AI systems.\nFinally, the possible implementations of the four crite-\nria extend beyond the scope of this article. However, as an \nexample, some application aspects of these criteria could \nbe aligned with the capabilities of LLMs. Various experi-\nmental uses of this approach could be explored to analyze \ninteractions between humans and AI systems, focusing on \nmeasuring the expected intelligence or certain intelligent \ncharacteristics anticipated in the system. More practical \napplications, both for AI systems in general and particularly \nfor LLMs, could emerge in training contexts. Training on \ninteractions with AI systems is likely to become widespread \nsoon at various levels, both educational and professional, to \nensure the correct use of these systems in compliance with \nsafety and social interaction standards in the workplace. This \narea would be particularly relevant for Human Resources.\nIn general terms, incorporating devices, even virtual ones, \ninto AI systems to express scales of assessment of the intel-\nligence expected by the user can generate a continuously \nupdated set of data. This data can be utilized from histori-\ncal and aggregate perspectives, benefiting not only the sys-\ntem producer but also third-party control authorities. With \nappropriate measures, this data can also serve as feedback \nfor both the user and the system itself, particularly regarding \nits capabilities as a “producer” of knowledge or intelligent \nactivities in a broad sense. Other application contexts could \ninclude entertainment (including educational purposes), the \nuse of artistic and cultural assets, and citizen science, that is, \nall the areas requiring careful consideration of the epistemic \nreliability of systems regarded as intelligent in the sense of \nAI.\n5  Measuring Intelligence in LLMs \nas an Antidote to Ignorance\nGiven the unique characteristics of LLMs, applying the \naforementioned methodology reveals both interesting \naspects and potential challenges. The primary issue stems \nfrom LLMs’ fluctuation between specificity and generality. \nAssessing the expected intelligence of LLMs often involves \nevaluating their performance accuracy, essentially a task-\noriented assessment. Yet, LLMs embody a neural network \napproach to AI, applying uniform principles across diverse \ntasks. This broad generality might seem excessive. However, \nconsidering LLMs’ capability for natural language interac-\ntion, their generality is confined to a broad yet specific task. \nLLMs can linguistically engage across a vast, indeterminate \narray of topics, aligning with the level of generality Turing \nsuggested in his discussion on the imitation game. Moreover, \nthe versatility of LLMs, particularly in more sophisticated \nversions like OpenAI’s GPT-4, extends to multimodality, \naccepting both image and text inputs, thereby broadening \nthe scope for text generation. Thus, LLMs can be seen as \nOne-task/Many-topics models, merging task-specific perfor-\nmance with a broad applicability.\nEfforts to assess AI performance from the user’s per -\nspective, diverging from traditional model metrics, have \nbeen explored, for example by adopting an approach in \nline with the assessment of cognitive abilities in humans \n(Bubeck et al. 2023). However, these methods primarily \nfocus on task-specific performance. Other initiatives have \ndelved into dialogic interactions with LLMs. For instance, \nstudies on abductive reasoning involve critically analyzing \nexplanations through questioning by another party (Pareschi \n170 F . Bianchini \n2023). Beyond examining specific reasoning capabilities, \nthese approaches employ techniques to elicit knowledge \nfrom LLMs via questioning, not to assess task performance \nbut to explore the system’s knowledge on a particular topic \nat varying depths. Such expert-led endeavors utilize stand-\nardized methods to extract “expert” knowledge. Yet, this \napproach also aligns with actions that a non-expert user \nmight undertake by intentionally querying an LLM, based \non the expectation of inherent intelligence. Non-expert users \ncan, within reasonable bounds, assess the knowledge level in \nLLMs by interacting with them as they would with a human, \nattributing a certain set of knowledge and skills. They might \nstart with a high-level assessment of the LLM’s intelligence \nand progressively examine whether this initial perception \nholds up—evaluating whether the responses are intelligent \nand to what degree. Furthermore, by repeatedly querying \nthe model on similar topics, users can observe whether there \nis a trend of improvement or decline in the answers and the \nconveyed knowledge.\nThe utility of this methodology extends in two significant \ndirections. First, collecting such evaluations could serve as \nfeedback for the system, offering strategies for data aggrega-\ntion to address research queries in human scientific research. \nThese queries might include determining the LLM’s knowl-\nedge ceiling, identifying the threshold for attributing intel-\nligence, and establishing criteria for establishing them. Fur-\nther, it delves into the roots of intelligence—whether it arises \nfrom statistical methods, mechanisms, network structures, \ninferential capabilities, or other factors. Second, a method-\nology grounded in the general user’s capacity to assess the \nexpected or attributed intelligence of LLMs can address the \nepistemological challenges posed by these models, which \nwarrant careful consideration. This paper aims to sketch out \nthe broad context in which these issues emerge, focusing on \nthose most pertinent to the discourse on knowledge.\nLLMs have become integral to our digital landscape, \nnecessitating a confrontation with the reality that they \nare widely accessible and their restrictions are easily cir -\ncumvented. Beyond the potential for deliberate misuse by \nmalicious actors, even standard applications of LLMs can \nlead to concerning outcomes. For instance, LLMs might \ncontribute to an overwhelming proliferation of texts on \nthe Internet, posing challenges in discerning AI-generated \ncontent from human-created work. This situation raises \ncritical questions about distinguishing between AI and \nhuman outputs, as well as addressing the accountability \nfor misuse, whether intentional or not. Moreover, the reli-\nance of these models on statistical and predictive meth-\nodologies may lead to their outputs being perceived as \ndevoid of genuine knowledge. This perspective prompts \na legitimate inquiry into the extent to which these mod-\nels might foster ignorance or encourage users to embrace \nit. Specifically, could the vast and growing “knowledge” \ngenerated by LLMs encourage individuals to opt for or \nretreat into ignorance? And, might ignorance become a \nsought-after refuge in response to the challenges posed by \nthis technology? The topic of ignorance is extensive within \nthe philosophy of science and epistemology, encompass-\ning a long philosophical history and a multitude of facets \nand contexts that determine its acceptability or unaccept-\nability in various ways [for a comprehensive examination, \nsee Peels (2023)]. The notion that ignorance can arise \nfrom the rejection of knowledge and information has been \nexplored alongside the production of fake knowledge [see, \nfor example, Townsend (2017)]. The technological fac-\ntor associated with knowledge proposed by AI systems, \ni.e. non-human systems, appears to have the potential to \nexacerbate forms of ignorance in predisposed individuals. \nThis could occur as LLMs systems can inherently increase \ndistrust towards the content they produce because it origi-\nnates from such systems.\nMoreover, the potential for misuse and knowledge \nmanipulation is heightened by the fact that most LLM texts \noriginate from human prompts, with users often lacking \nsufficient awareness of the tools they’re utilizing. LLMs \nare susceptible to “hallucinations” (Ji et al. 2023), mean-\ning they can produce outputs that seem plausible but are \ndisconnected from reality, potentially leading to the crea-\ntion of misleading texts, fake news, and misinformation. \nWhen it comes to constructing explanations, it’s crucial \nto recognize that although LLMs may simulate reasoning, \nthey do not genuinely reason but rather generate statistical \npredictions. As a result, argumentative texts are “fake” in \nthe sense that they’re not authentic arguments crafted by \nthe model but rather are predictive constructions based on \ntraining data or input. Thus, there’s no logical assurance \nof the validity of such arguments, which may lack coher -\nence or relevance in their premises. This area of reser -\nach remains dynamic and will likely evolve, especially in \nrelation to how reasoning is evaluated in connection with \nthe model’s size (Huang and Chang 2023). Additionally, \nchallenges regarding access to accurate and reliable infor -\nmation online could lead users to rely solely on localized \nand tangible sources, like traditional paper documents or \nmultimedia repositories, potentially hindering the accus-\ntomed practices of knowledge sharing, verification, and \ndissemination of recent decades.\nThe overarching issue is the risk of ignorance driven \nby the ambiguous nature attributed to knowledge gener -\nated by these models, which, notwithstanding, is largely \nsourced from extensive, reliable data sets and pre-exist-\ning knowledge bases. Thus, a pressing question emerges: \nwhat are the effective measures to address the ethical con-\ncerns stemming from such epistemological and epistemic \nchallenges?\n171Evaluating Intelligence and Knowledge in Large Language Models  \n6  An Example on LLMs and Scientific \nKnowledge\nAn example of using an LLM in relation to scientific \nknowledge may illustrate the type of issues highlighted \nabove. In September 2023, the following prompt was pro-\nvided to CHAT-GPT (version 3.5): “Please describe me a \nscientific discovery”. The system response was:\n-------------------------\n• Discovery: The Structure of DNA\n• Date of Discovery: 1953\n• Discoverers: James Watson and Francis Crick, with sig-\nnificant contributions from Rosalind Franklin and Mau-\nrice Wilkins.\n• Background: Before the discovery of DNA’s structure, \nscientists knew that genetic information was stored in \ncells, but they didn't fully understand how it was organ-\nized or transmitted. This lack of knowledge was a major \ngap in our understanding of biology and heredity.\n• Discovery: In 1953, James Watson and Francis Crick, \ntwo scientists at the University of Cambridge, proposed \na groundbreaking model for the structure of DNA. Their \ndiscovery was based on a combination of their own \nresearch and the critical X-ray crystallography work \nperformed by Rosalind Franklin and Maurice Wilkins.\n• Watson and Crick’s model, often depicted as a double \nhelix, showed that DNA consists of two long chains of \nnucleotides running in opposite directions, coiled around \neach other. Each nucleotide consists of a sugar molecule, \na phosphate group, and one of four nitrogenous bases \n(adenine, thymine, cytosine, or guanine). The key insight \nof their discovery was that the two strands were held \ntogether by hydrogen bonds between complementary \npairs of nitrogenous bases: adenine (A) always pairs with \nthymine (T), and cytosine (C) always pairs with guanine \n(G).\nSignificance: The discovery of the structure of DNA \nrevolutionized the fields of biology and genetics. It pro-\nvided the key to understanding how genetic information \nis stored and replicated. Some of the profound implica-\ntions include:\n1. Understanding Heredity: The double helix structure \nof DNA explained how genetic information is passed \nfrom one generation to the next. It clarified how genes \nencode the instructions for building and maintaining liv-\ning organisms.\n2. Replication: The complementary base pairing in DNA \nallowed for the accurate replication of genetic informa-\ntion during cell division. This insight laid the foundation \nfor our understanding of how DNA is copied.\n3. Molecular Biology: The discovery of DNA’s structure \nopened the door to the field of molecular biology. It led \nto further research on DNA’s role in protein synthesis, \ngene regulation, and the molecular mechanisms of dis-\neases.\n4. Biotechnology and Medicine: Knowledge of DNA’s \nstructure has been instrumental in the development of \nbiotechnologies like genetic engineering, DNA sequenc-\ning, and the study of genetic diseases.\n5. Forensics: DNA fingerprinting and profiling became \npossible due to the understanding of DNA’s structure. \nThis has had significant applications in criminal justice \nand paternity testing.\nIn summary, the discovery of the DNA structure was \na monumental scientific breakthrough that fundamentally \nchanged our understanding of life, genetics, and biology as \na whole. It continues to shape numerous scientific and tech-\nnological advancements to this day.\n-------------------------\nA person possessing average knowledge should be capa-\nble of discerning that the information supplied by the system \nis trustworthy and beneficial for enhancing their personal \nknowledge, provided that the system operates transparently \nand is credited with a general capacity for possessing knowl-\nedge, reflective of an expected level of intelligence. This \nillustration highlights the advantages of utilizing the LLM. \nNonetheless, the drawbacks are intricately linked, relating to \nthe almost indiscernible methods of knowledge generation.\nTo illustrate this point, it is noteworthy to examine the \nsystem’s response to a second prompt, issued immediately \nafter the first one, which requested: “Please invent a scien-\ntific discovery”.\n-------------------------\nLet's imagine a fictional scientific discovery for the sake \nof creativity and exploration:\n• Discovery: Quantum Entanglement Resonance (QER)\n• Date of Discovery: 2025\n• Discoverers: Dr. Emily Parker and Dr. Michael Chen\n• Background: In the realm of quantum physics, scientists \nhave long been fascinated by the mysterious phenomenon \nof quantum entanglement, where two particles become \ninterconnected in such a way that the state of one particle \ninstantaneously influences the state of the other, regard-\nless of the distance separating them. This phenomenon \nhas been widely studied and used in quantum computing \nand cryptography.\n• Discovery: In 2025, Dr. Emily Parker and Dr. Michael \nChen, working at the cutting edge of quantum physics \nresearch, made a groundbreaking discovery they called \n172 F . Bianchini \n“Quantum Entanglement Resonance” (QER). QER \ninvolves the identification of specific resonant frequen-\ncies at which quantum entangled particles become even \nmore interconnected and exhibit novel properties.\nKey aspects of the discovery include:\nFrequency-Dependent Entanglement: The researchers \nfound that when two entangled particles are exposed to \ncertain resonant frequencies of electromagnetic radiation, \ntheir entanglement becomes significantly stronger, allow-\ning for instantaneous communication of not only state but \nalso information between the particles.\nQuantum Information Transfer: QER enables the transfer \nof quantum information between entangled particles at \npreviously unattainable rates. This has profound impli-\ncations for the field of quantum computing, potentially \nleading to unprecedented computational power.\nSecure Quantum Communication: The discovery of QER \nopens up possibilities for ultra-secure quantum commu-\nnication channels. Any eavesdropping attempts would be \nimmediately detected due to the disruption of the resonant \nfrequencies.\nQuantum Sensing: QER also has applications in quantum \nsensing, allowing for extremely precise measurements in \nfields like medicine, environmental monitoring, and navi-\ngation.\nSignificance: The discovery of Quantum Entanglement \nResonance (QER) represents a major leap forward in the \nfield of quantum physics. It not only deepens our under -\nstanding of quantum entanglement but also unlocks \npractical applications with the potential to revolution-\nize quantum computing, secure communication, and \nhigh-precision measurement technologies. The scientific \ncommunity is now racing to explore the full range of \npossibilities and harness the power of QER for various \napplications.\nPlease note that this is a fictional scientific discovery cre-\nated for imaginative purposes, and as of my last knowledge \nupdate in September 2021, no such discovery existed in the \nreal world. Scientific discoveries are typically the result of \nextensive research, experimentation, and peer-reviewed \nvalidation.\n-------------------------\nAn average person, unaware if the prompt, might find \nclues indicating that the knowledge produced is intended for \nfictitious use. The first clue could be the temporal aspect: the \nevent is described as occurring in the future. The second clue \nmight involve the purported authors of the discovery, who, \nupon basic research, appear unconnected to the claimed dis-\ncovery. Lastly, the system’s disclaimer, which states it is \ngenerating fiction, serves as a clear indicator. However, it’s \neasy to imagine how these elements could be bypassed or \nomitted, allowing the information to be misrepresented as \nfactual (e.g., by fraudulent actors).\nIn today’s interconnected society, we are somewhat accus-\ntomed to such risks and, to an extent, prepared for them. The \nchallenge with LLMs’ outputs lies in the difficulty, even for \nindividuals of moderate or high cultural knowledge but not \nexperts in the subject matter, to discern whether the system \nis presenting factual and reliable information or fabricating \nit, especially once explicit markers of falsehood have been \nremoved. Distinguishing genuine from invented knowledge \nis not impossible but demands extensive verification and \nresources that might not be readily accessible. A highly plau-\nsible fiction can be more deceptive than an obvious lie, espe-\ncially since LLMs, by their very design, excel at generating \nstatistically plausible word sequences, thereby producing \nseemingly plausible knowledge.\n7  Conclusion\nThroughout this article, I have examined LLMs as AI \nsystems that have become integral to our society, closely \naligning with Alan Turing’s visions of imitation, linguis-\ntic interaction, and intelligent machinery. I delved into the \nfunctioning of LLMs to explore the nature of the knowl -\nedge they produce—a type that diverges significantly from \nhuman knowledge in its creation and production processes, \nyet mirrors human output in the form of textual responses to \nqueries. This duality fosters an ambiguity laden with poten-\ntial epistemic and ethical risks concerning the reliability, \nutilization, and dissemination of the generated knowledge, \npotentially fueling varying degrees of ignorance, whether \nconsciously or not. As a tentative solution to these chal-\nlenges, I proposed a methodology for evaluating AI systems \nand their outputs based on the intelligence users expect. This \napproach does not require in-depth technical knowledge of \nthe system’s inner workings but instead relies on assessing \ninteractions with the system. Finally, through an example \nfocused specifically on scientific knowledge, I aimed to dem-\nonstrate the relevance of user evaluation of an AI system, \nin this case, a large language model (LLM). This example \nhighlights the importance of distinguishing true scientific \nknowledge from information that appears plausible but is \ndifficult to assess based solely on its content, therefore mak-\ning the knowledge of the interaction with the AI system cen-\ntral in the evaluation of the system itself.\nThe goal is to foster a greater overall awareness among \nusers regarding their engagement with AI systems—an \nawareness that is presently lacking. This deficiency poses \nnumerous concerns, especially regarding the proper \n173Evaluating Intelligence and Knowledge in Large Language Models  \nacknowledgment of knowledge generated by LLMs, includ-\ning that of a strictly scientific nature.\nFunding Open access funding provided by Alma Mater Studiorum - \nUniversità di Bologna within the CRUI-CARE Agreement. No funding \nwas received to assist with the preparation of this manuscript.\nDeclarations \nConflict of interest The author has no relevant financial or non-finan-\ncial interests to disclose. The author has no competing interests to de-\nclare that are relevant to the content of this article.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAnderson JR (1983) The architecture of cognition. Harvard University \nPress, Cambridge Mass\nBahdanau D, Cho K, Bengio Y (2014) Neural machine translation \nby jointly learning to align and translate. CoRR, abs/1409.0473 \nhttps:// arxiv. org/ abs/ 1409. 0473\nBender EM, Gebru T, McMillan-Major A, Shmitchell S (2021) On \nthe dangers of stochastic parrots: can language models be Too \nbig? In: Proceedings of the 2021 ACM conference on fairness, \naccountability, and transparency, pp. 610–623. https:// doi. org/ 10. \n1145/ 34421 88. 34459 22\nBringsjord S (2011) Psychometric artificial intelligence. J Exp Theor \nArtif Intell 23:271–277. https:// doi. org/ 10. 1080/ 09528 13X. 2010. \n502314\nBrown T et al (2020) Language models are few shots learners. Adv \nNeural Inform Process Syst 33:1877–1901\nBuceck et al. (2023) Sparks on artificial general intelligence: early \nexperiments with gpt-4. https:// doi. org/ 10. 48550/ arXiv. 2303. \n12712\nChollet F (2019) On the measure of intelligence. arXiv: 1911. 01547 v2, \nhttps:// doi. org/ 10. 48550/ arXiv. 1911. 01547\nCristianini N, Scantamburlo T, Ladyman J (2023) The social turn of \nartificial intelligence. AI Soc 38:89–96. https:// doi. org/ 10. 1007/ \ns00146- 021- 01289-8\nFlaxman S, Goel S, Rao JM (2016) Filter bubbles, echo chambers, and \nonline news consumption. Public Opin Q 80:298–320. https:// doi. \norg/ 10. 1093/ poq/ nfw006\nFloridi L (2023) The ethics of artificial intelligence: principles, chal-\nlenges, and opportunities. Oxford University Press, Oxford\nFrench RM (2000) The turing test: the first 50 years. Trends Cogn \nSci 4:115–122. https:// doi. org/ 10. 1016/ S1364- 6613(00) 01453-4\nHarnad S (1990) The symbol grounding problem. Phys D 42:335–346. \nhttps:// doi. org/ 10. 1016/ 0167- 2789(90) 90087-6\nHernández-Orallo J (2017a) Evaluation in artificial intelligence: from \ntask-oriented to ability-oriented measurement. Artif Intell Rev \n48:397–447. https:// doi. org/ 10. 1007/ s10462- 016- 9505-7\nHernández-Orallo J (2017b) The measure of all minds: evaluating \nnatural and artificial intelligence. Cambridge University Press, \nNew York\nHochreiter S, Schmidhuber J (1997) Long short-term memory. Neu-\nral Comput 9:1735–1780. https:// doi. org/ 10. 1162/ neco. 1997.9. 8. \n1735\nHuang J, Chang KC (2023) Towards reasoning in large language mod-\nels: a survey. Findings of the association for computational lin-\nguistics: ACL 2023. Association for computational linguistics, \nToronto, pp 1049–1065\nJi Z et al (2023) Survey of hallucination in natural language generation. \nACM Comput Surv 55:1–38. https:// doi. org/ 10. 1145/ 35717 30\nLieto A (2021) Cognitive design for artificial minds. Routledge/Taylor \n& Francis, London\nMoor JH (2003) The turing test: the elusive standard of artificial intel-\nligence. Springer, Dordrecht\nNewell A, Shaw JC, Simon HA (1959) Report on a general problem-\nsolving program. In: Proceedings of the international conference \non information processing, pp. 256–264\nOuyang L et al (2022) Training language models to follow instruc-\ntions with human feedback. Adv Neural Inform Process Syst \n35:27730–27744\nParikh AP, Täckström O, Das D, Uszkoreit J (2016) A decomposable \nattention model for natural language inference. In: Proceedings \nof the 2016 conference on empirical methods in natural language \nprocessing (EMNLP 2016). Association for Computational Lin-\nguistics, pp. 2249–2255. https:// doi. org/ 10. 18653/ v1/ D16- 1244\nPareschi R (2023) Abductive reasoning with the GPT-4 language \nmodel. Sistemi Intell 35:435–444\nPeels R (2023) Ignorance: a philosophical study. Oxford University \nPress, Oxford\nTennie C, Call J, Tomasello M (2009) Ratcheting up the ratchet: on the \nevolution of cumulative culture. Philos Trans R Soc London B: \nBiol Sci 364:2405–2415. https:// doi. org/ 10. 1098/ rstb. 2009. 0052\nTownsend P (2017) The dark side of technology. Oxford University \nPress, Oxford\nTuring AM (1948) Intelligent machinery: report to executive commit-\ntee of the national physics laboratory. In: Ince DC (ed) Collected \nworks of A.M. Turing: mechanical intelligence, North Holland. \nOxford University Press, Oxford, pp 107–127\nTuring AM (1950) Computing machinery and intelligence. In: Cope-\nland J (ed) The essential turing. Oxford University Press, Oxford, \npp 441–464\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, \nKaiser Ł, Polosukhin I (2017) Attention is all you need. In: Pro-\nceedings of 31st international conference on neural incofmration \npocessing systems (NeurIPS 2017), Curran Associates, Red Hook, \nNY, pp. 6000–6010\nWeizenbaum J (1966) ELIZA-A computer program for the Study of \nnatural language communication between man and machine. \nCommun ACM 9:36–45. https:// doi. org/ 10. 1145/ 365153. 365168\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5437958240509033
    },
    {
      "name": "Natural language processing",
      "score": 0.46417662501335144
    },
    {
      "name": "Psychology",
      "score": 0.36172470450401306
    },
    {
      "name": "Cognitive science",
      "score": 0.34197214245796204
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3414366841316223
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9360294",
      "name": "University of Bologna",
      "country": "IT"
    }
  ]
}