{
  "title": "Evaluating Text GANs as Language Models",
  "url": "https://openalex.org/W2898718449",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222506026",
      "name": "Tevet, Guy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288561993",
      "name": "Habib, Gavriel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287128691",
      "name": "Shwartz, Vered",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221504776",
      "name": "Berant, Jonathan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2473934411",
    "https://openalex.org/W2042587503",
    "https://openalex.org/W2962879692",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2963304263",
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2757836268",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W2103012681",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2807747378",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2792795990",
    "https://openalex.org/W2687693326",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2592864539",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2900260828",
    "https://openalex.org/W2620623908",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2624872001",
    "https://openalex.org/W2964019776"
  ],
  "abstract": "Generative Adversarial Networks (GANs) are a promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of ``exposure bias''. However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than state-of-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.",
  "full_text": "Evaluating Text GANs as Language Models\nGuy Tevet∗ ∗,1,2 Gavriel Habib∗,1,2 Vered Shwartz3 Jonathan Berant1,4\n1School of Computer Science, Tel-Aviv University\n2Department of Electrical Engineering, Tel-Aviv University\n3Computer Science Department, Bar-Ilan University\n4Allen Institute for Artiﬁcial Intelligence\n{guytevet@mail,gavrielhabib@mail,joberant@cs}.tau.ac.il, vered1986@gmail.com\nAbstract\nGenerative Adversarial Networks (GANs) are\na promising approach for text generation that,\nunlike traditional language models (LM), does\nnot suffer from the problem of “exposure\nbias”. However, A major hurdle for under-\nstanding the potential of GANs for text gen-\neration is the lack of a clear evaluation met-\nric. In this work, we propose to approxi-\nmate the distribution of text generated by a\nGAN, which permits evaluating them with tra-\nditional probability-based LM metrics. We ap-\nply our approximation procedure on several\nGAN-based models and show that they cur-\nrently perform substantially worse than state-\nof-the-art LMs. Our evaluation procedure pro-\nmotes better understanding of the relation be-\ntween GANs and LMs, and can accelerate\nprogress in GAN-based text generation.\n1 Introduction\nNeural networks have revolutionized the ﬁeld of\ntext generation, in machine translation (Sutskever\net al., 2014; Neubig, 2017; Luong et al., 2015;\nChen et al., 2018), summarization (See et al.,\n2017), image captioning (You et al., 2016) and\nmany other applications (Goldberg, 2017).\nTraditionally, text generation models are trained\nby going over a gold sequence of symbols (char-\nacters or words) from left-to-right, and maximiz-\ning the probability of the next symbol given the\nhistory, namely, a language modeling (LM) ob-\njective. A commonly discussed drawback of such\nLM-based text generation is exposure bias(Ran-\nzato et al., 2015): during training, the model pre-\ndicts the next token conditioned on the ground\ntruth history, while at test time prediction is based\non predicted tokens, causing a train-test mismatch.\nModels trained in this manner often struggle to\novercome previous prediction errors.\nGenerative Adversarial Networks (Goodfellow\net al., 2014) offer a solution for exposure bias.\n∗ The authors contributed equally\nOriginally introduced for images, GANs leverage\na discriminator, which is trained to discriminate\nbetween real images and generated images via an\nadversarial loss. In such a framework, the genera-\ntor is not directly exposed to the ground truth data,\nbut instead learns to imitate it using global feed-\nback from the discriminator. This has led to sev-\neral attempts to use GANs for text generation, with\na generator using either a recurrent neural network\n(RNN) (Yu et al., 2017; Guo et al., 2017; Press\net al., 2017; Rajeswar et al., 2017), or a Convo-\nlutional Neural Network (CNN) (Gulrajani et al.,\n2017; Rajeswar et al., 2017).\nHowever, evaluating GANs is more difﬁcult\nthan evaluating LMs. While in language model-\ning, evaluation is based on the log-probability of\na model on held-out text, this cannot be straight-\nforwardly extended to GAN-based text genera-\ntion, because the generator outputs discrete to-\nkens, rather than a probability distribution. Cur-\nrently, there is no single evaluation metric for\nGAN-based text generation, and existing metrics\nthat are based on n-gram overlap are known to lack\nrobustness and have low correlation with semantic\ncoherence (Semeniuta et al., 2018).\nIn this paper, we propose a method for evaluat-\ning GANs with standard probability-based evalu-\nation metrics. We show that the expected predic-\ntion of a GAN generator can be viewed as a LM,\nand suggest a simple Monte-Carlo method for ap-\nproximating it. The approximated probability dis-\ntribution can then be evaluated with standard LM\nmetrics such as perplexity or Bits Per Character\n(BPC).\nTo empirically establish our claim, we imple-\nment our evaluation on several RNN-based GANs:\n(Press et al., 2017; Yu et al., 2017; Guo et al.,\n2017). We ﬁnd that all models have substantially\nlower BPC compared to state-of-the-art LMs. By\ndirectly comparing to LMs, we put in perspective\nthe current performance of RNN-based GANs for\narXiv:1810.12686v2  [cs.CL]  24 Mar 2019\ntext generation. Our results are also in line with\nrecent concurrent work by Caccia et al. (2018),\nwho reached a similar conclusion by comparing\nthe performance of textual GANs to that of LMs\nusing metrics suggested for GAN evaluation.\nOur code is available at: http:\n//github.com/GuyTevet/SeqGAN-eval\nand http://github.com/GuyTevet/\nrnn-gan-eval.\n2 Background\nFollowing the success of GANs in image gen-\neration, several works applied the same idea to\ntexts using convolutional neural networks (Gul-\nrajani et al., 2017; Rajeswar et al., 2017), and\nlater using RNNs (Press et al., 2017; Yu et al.,\n2017). RNNs enable generating variable-length\nsequences, conditioning each token on the tokens\ngenerated in previous time steps. We leverage this\ncharacteristic in our approximation model (§4.1).\nA main challenge in applying GANs for text\nis that generating discrete symbols is a non-\ndifferentiable operation. One solution is to per-\nform a continuous relaxation of the GAN output,\nwhich leads to generators that emit a nearly dis-\ncrete continuous distribution (Press et al., 2017).\nThis keeps the model differentiable and enables\nend-to-end training through the discriminator. Al-\nternatively, SeqGAN (Yu et al., 2017) and Leak-\nGAN (Guo et al., 2017) used policy gradient meth-\nods to overcome the differentiablity requirement.\nWe apply our approximation to both model types.\n3 Evaluating GANs and LMs\nLM Evaluation. Text generation from LMs is\ncommonly evaluated using probabilistic metrics.\nSpeciﬁcally, given a test sequence of symbols\n(t1,...,t n), and a LM q, the average cross-\nentropy over the entire test set is computed:\nACE = −1\nn\n∑n\ni=1 log2 q(ti |t1...ti−1). For\nword-based models, the standard metric is per-\nplexity: PP = 2 ACE, while for character-based\nmodels it is BPC = ACE directly.\nIntrinsic improvement in perplexity does not\nguarantee an improvement in an extrinsic down-\nstream task that uses a language model. However,\nperplexity often correlates with extrinsic measures\n(Jurafsky and Martin, 2018), and is the de-facto\nmetric for evaluating the quality of language mod-\nels today.\nGAN-based Text Generation Evaluation. By\ndeﬁnition, a text GAN outputs a discrete sequence\nof symbols rather than a probability distribution.\nAs a result, LM metrics cannot be applied to eval-\nuate the generated text. Consequently, other met-\nrics have been proposed:\n•N-gram overlap: (Yu et al., 2017; Press et al.,\n2017): Inspired by BLEU (Papineni et al.,\n2002), this measures whether n-grams gener-\nated by the model appear in a held-out corpus.\nA major drawback is that this metric favors con-\nservative models that always generate very com-\nmon text (e.g., “it is”). To mitigate this, self-\nBLEU has been proposed (Lu et al., 2018) as\nan additional metric, where overlap is measured\nbetween two independently sampled texts from\nthe model.\n•LM score: The probability of generated text ac-\ncording to a pre-trained LM. This has the same\nproblem of favoring conservative models.\n•Zhao et al. (2017) suggested an indirect score\nby training a LM on GAN-generated text, and\nevaluating it using perplexity. The drawback in\nthis setting is the coupling of the performance\nof the GAN with that of the proxy LM.\n•Heusel et al. (2017) used Frechet InferSent Dis-\ntance (FID) to compute the distance between\ndistributions of features extracted from real and\ngenerated samples. However, this approach re-\nlies on a problematic assumption that features\nare normally distributed.\n•Rajeswar et al. (2017) used a context-free gram-\nmar (CFG) to generate a reference corpus, and\nevaluated the model by the likelihood the CFG\nassigns to generated samples. However, sim-\nple CFGs do not fully capture the complexity\nof natural language.\n•To overcome the drawbacks of each individual\nmethod, Semeniuta et al. (2018) proposed a uni-\nﬁed measure based on multiple evaluation met-\nrics (N-grams, BLEU variations, FID, LM score\nvariations and human evaluation). Speciﬁcally,\nthey argue that the different measures capture\ndifferent desired properties of LMs, e.g., qual-\nity vs. diversity.\n•Following Semeniuta et al. (2018), and in paral-\nlel to this work, Caccia et al. (2018) proposed a\ntemperature sweep method that trades-off qual-\nity for diversity using a single parameter. Sim-\nilar to our ﬁndings, they concluded that GANs\nperform worse than LMs on this metric.\nxt xt+1 xt+2 xt+3\nht ht+3\notot−1 ot+1 ot+2\nFigure 1: Generator recurrent connections. {ht}is the in-\nternal state sequence and {ot}is the generator prediction se-\nquence (one-hot). During inference, the outputs {ot}are fed\nback as the input for the next time step (dashed lines). During\nLM approximation, the input {xt}is a sequence of one-hot\nvectors from the test set.\nOverall, current evaluation methods cannot\nfully capture the performance of GAN-based text\ngeneration models. While reporting various scores\nas proposed by Semeniuta et al. (2018) is possible,\nit is preferable to have a single measure of progress\nwhen comparing different text generation models.\n4 Proposed Method\nWe propose a method for approximating a distri-\nbution over tokens from a GAN, and then eval-\nuate the model with standard LM metrics. We\nwill describe our approach given an RNN-based\nLM, which is the most commonly-used architec-\nture, but the approximation can be applied to other\nauto-regressive models (Vaswani et al., 2017).\n4.1 Language Model Approximation\nThe inputs to an RNN at time step t, are the state\nvector ht and the current input token xt. The out-\nput token (one-hot) is denoted by ot. In RNN-\nbased GANs, the previous output token is used at\ninference time as the inputxt (Yu et al., 2017; Guo\net al., 2017; Press et al., 2017; Rajeswar et al.,\n2017). In contrast, when evaluating with BPC\nor perplexity, the gold token xt is given as input.\nHence, LM-based evaluation neutralizes the prob-\nlem of exposure bias addressed by GANs. Nev-\nertheless, this allows us to compare the quality of\ntext produced by GANs and LMs on an equal foot-\ning. Figure 1 illustrates the difference between in-\nference time and during LM approximation.\nWe can therefore deﬁne the generator function\nat time step t as a function of the initial state h0\nand the past generated tokens (x0 ...x t), which\nwe denote as ot = Gt(h0,x0...xt) (x0 is a start\ntoken). Given a past sequence (x0 ...x t), Gt is\na stochastic function: the stochasticity of Gt can\nAlgorithm 1LM Evaluation of RNN-based GANs\nInput: Gt(·): the generator function at time step t\n(x0,...,x t): previous gold tokens\nxt+1: the gold next token (as ground truth)\nf(·,·): a LM evaluation metric\nN: number of samples\n1: for n←1 to N do\n2: gt,n ←−sample from Gt(x0...xt)\n3: ˜Gt,N = 1\nNΣN\nn=1gt,n\n4: return f( ˜Gt,N,xt+1)\nbe gained either by using a noise vector as the ini-\ntial state h0 (Press et al., 2017), or by sampling\nfrom the GAN’s internal distribution over possi-\nble output tokens (Yu et al., 2017; Guo et al.,\n2017). Since h0 is constant or a noise vector\nthat makes Gt stochastic, we can omit it to get\nGt(x0 ...x t). In such a setup, the expected value\nE[Gt(x0 ...x t)] is a distribution q over the next\nvocabulary token at:\nq(at |a0 ...a t−1) = {E[Gt(x0 ...x t)]}at\nTo empirically approximate q, we can sample\nfrom it N i.i.d samples, and compute an approx-\nimation ˜Gt,N = 1\nNΣN\nn=1gt,n, where gt,n is one\nsample from Gt(x0...xt). Then, according to the\nstrong law of large numbers:\nE[Gt(x0 ...x t)] = lim\nN→∞\n˜Gt,N (1)\nGiven this approximate LM distribution, we can\nevaluate a GAN using perplexity or BPC. We sum-\nmarize the evaluation procedure in Algorithm 1.1\n4.2 Approximation Bound\nWe provide a theoretical bound for choosing a\nnumber of samples N that results in a good ap-\nproximation of ˜Gt,N to E[Gt].\nPerplexity and BPC rely on the log-probability\nof the ground truth token. Since the ground truth\ntoken is unknown, we conservatively deﬁne the\nbad event B in which there exists v ∈ V such\nthat |{E[Gt]}v −{ ˜Gt,N}v|> γ, where V is the\nvocabulary. We can then bound the probability of\nBby some ϵ. We deﬁne the following notations:\n1. The probability of a token at to be v is pv\n∆=\nq(at = v|a0 ...a t−1) = {E[Gt(x0 ...x t)]}v.\n2. χv,n\n∆= {gt,n}v is a random variable repre-\nsenting the binary value of the v’th index of\n1Our evaluation algorithm is linear in the length of the test\nset and in the number of samples N.\ngt,n which is a single sample of Gt. Note\nthat the average of χv,n over N samples is\nXv\n∆= 1\nN\n∑N\nn=1 χv,n =\n{\n1\nN\n∑N\nn=1 gt,n\n}\nv\n=\n{˜Gt,N}v.\nUsing the above notation, we can re-deﬁne the\nprobability of the bad event B with respect to the\nindividual coordinates in the vectors:\nPr(B) = Pr\n(\n∥E[Gt] −˜Gt,N∥∞>γ\n)\n= Pr\n(⋃\nv∈V\n|pv −Xv|>γ\n)\n!\n<ϵ\n(2)\nWe note that χv,n ∼ Bernoulli(pv), and\ngiven that {χv,n}N\nn=1 are i.i.d., we can apply\nthe Chernoff-Hoeffding theorem (Chernoff et al.,\n1952; Hoeffding, 1963). According to the theo-\nrem, for every v ∈ V, Pr(|Xv −pv| > γ) <\n2e−2Nγ2\n. Taking the union bound overV implies:\nPr(B) =Pr(⋃\nv∈V|Xv−pv|>γ)<2|V|e−2Nγ2\n<ϵ (3)\nHence, we get a lower bound on N:\nN > 1\n2γ2 ln\n(2|V|\nϵ\n)\n(4)\nAs a numerical example, choosing γ = 10 −3\nand ϵ = 10 −2, for a character-based LM over\nthe text8 dataset, with |V| = 27 , we get the\nbound: N > 4.3 ·106. With the same γ and\nϵ, a typical word-based LM with vocabulary size\n|V|= 50,000 would require N >8.1 ·106.\nIn practice, probability vectors of LMs tend to\nbe sparse (Kim et al., 2016). Thus, we argue that\nwe can use a much smaller N for a good approxi-\nmation ˜Gt,N. Since the sparsity of LMs is difﬁcult\nto bound, as it differs between models, we suggest\nan empirical method for choosing N.\nThe approximation ˜Gt,N is a converging se-\nquence, particularly over ∥·∥∞(see Equation 1).\nHence, we can empirically choose anNwhich sat-\nisﬁes ∥˜Gt,N−α−˜Gt,N∥∞<γ ′, α ∈N. In Sec-\ntion 5 we empirically measure∥˜Gt,N−α−˜Gt,N∥∞\nas a function of N to choose N. We choose a\nglobal N for a model, rather than for every t, by\naveraging over a subset of the evaluation set.\n5 Evaluation\n5.1 Models\nWe focus on character-based GANs as a test-case\nfor our method. We evaluate two RNN-based\nGANs with different characteristics. As opposed\nto the original GAN model (Goodfellow et al.,\n2014), in which the generator is initialized with\nrandom noise, the GANs we evaluated both lever-\nage gold standard text to initialize the generator,\nas detailed below.\nRecurrent GAN (Press et al., 2017)is a contin-\nuous RNN-based generator which minimizes the\nimproved WGAN loss (Gulrajani et al., 2017). To\nguide the generator, during training it is initial-\nized with the ﬁrst i−1 characters from the ground\ntruth, starting the prediction in the ith character.\nStochasticity is obtained by feeding the generator\nwith a noise vector z as a hidden state. At each\ntime step, the input to the RNN generator is the\noutput distribution of the previous step.\nSeqGAN (Yu et al., 2017) is a discrete RNN-\nbased generator. To guide the generator, it is pre-\ntrained as a LM on ground truth text. Stochastic-\nity is obtained by sampling tokens from an internal\ndistribution function over the vocabulary. To over-\ncome differentiation problem, it is trained using a\npolicy gradient objective (Sutton et al., 2000).\nWe also evaluatedLeakGAN (Guo et al., 2017),\nanother discrete RNN-based generator, but since\nit is similar to SeqGAN and performed worse, we\nomit it for brevity.\n5.2 Evaluation Settings\nTo compare to prior work in LM, we follow the\ncommon setup and train on the text8 dataset.2 The\ndataset is derived from Wikipedia, and includes 26\nEnglish characters plus spaces. We use the stan-\ndard 90/5/5 split to train/validation/test. Finally,\nwe measure performance with BPC.\nWe tuned hyper-parameters on the validation\nset, including sequence length to generate at test\ntime (7 for Press et al. (2017), 1000 for Yu et al.\n(2017)). We chose the number of samples N\nempirically for each model, as described in Sec-\ntion 4.2. We set α to 10, and the boundary to\nγ′ = 10−3 as a good trade-off between accuracy\nand run-time. Figure 2 plots the approximate error\n∥˜Gt,N−α −˜Gt,N∥∞as a function of N. For both\nmodels, N > 1600 satisﬁes this condition (red\nline in Figure 2). To be safe, we used N = 2000.\n5.3 Results\nTable 1 shows model performance on the test set.\n2http://mattmahoney.net/dc/textdata\nApproach Model BPC Approx. BPC\nLanguage Models\nmLSTM + dynamic eval (Krause et al., 2017) 1.19\nLarge mLSTM +emb +WN +VD (Krause et al., 2016) 1.27\nLarge RHN (Zilly et al., 2016) 1.27\nLayerNorm HM-LSTM (Chung et al., 2016) 1.29\nBN LSTM (Cooijmans et al., 2016) 1.36\nUnregularised mLSTM (Krause et al., 2016) 1.40\nSeqGAN - pre-trained LM (Yu et al., 2017) 1.85 1.95\nGANs (LM Approximation) SeqGAN - full adversarial training (Yu et al., 2017) 1.99 2.08\nRecurrent GAN without pre-training (Press et al., 2017) 3.31\nUniform Distribution 4.75\nTable 1: Test set evaluation of different character-based models on the text8 dataset. State-of-the-art results are taken from\nhttps://github.com/sebastianruder/NLP-progress/blob/master/language_modeling.md. The\nuniform distribution is equivalent to guessing the next character out of |V|= 27 characters.\nModel Samples\nSeqGAN\nPre-trained LM\n1. rics things where a weeks thered databignand jacob reving the imprisoners could become poveran brown\n2. nine other set of of one eight one two by belarigho and singing signal theus to accept natural corp\n3. ragems the downran maintain the lagar linear stream hegels p in ﬁve six f march one nine nine nine\nSeqGAN\nFull adversarial\ntraining\n1. four zero ﬁve two memaire in afulie war formally dream the living of the centuries to quickly can f\n2. part of the pract the name in one nine seven were mustring of the airports tex works to eroses exten\n3. eight four th jania lpa ore nine zero zero zero sport for tail concents englished a possible for po\nRecurrent\nGAN\n1. nteractice computer may became were the generally treat he were computer may became were the general\n2. lnannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnne and and and and and and and and and and and and and and and a\n3. perors as as seases as as as as as as as as as selected see see see see see see see see see see see\nTable 2: Random samples of 100 characters generated by each model.\n0 500 1000 1500 2000 2500 3000\nNumber of generator runs (N)\n10 3\n10 2\nError (infinity norm)\nRecurrent GAN\nseqGAN\nFigure 2: Approximate error ∥˜Gt,N−α−˜Gt,N∥∞as a func-\ntion of samples N. α= 10, γ′= 10−3.\nBecause SeqGAN models output a distribution\nover tokens at every time step, we can measure the\ntrue BPC and assess the quality of our approxima-\ntion. Indeed, we observe that approximate BPC is\nonly slightly higher than the true BPC.\nGAN-based models perform worse than state-\nof-the-art LMs by a large margin. Moreover, in\nSeqGAN, the pre-trained LM performs better than\nthe fully trained model with approximate BPC\nscores of 1.95 and 2.06, respectively, and the BPC\ndeteriorates as adversarial training continues.\nFinally, we note that generating sequences\nlarger than 7 characters hurts the BPC of Press\net al. (2017). It is difﬁcult to assess the quality\nof generation with such short sequences.\nIn Table 2 we present a few randomly gener-\nated samples from each model. We indeed observe\nthat adversarial training slightly reduces the qual-\nity of generated text for SeqGAN, and ﬁnd that the\nquality of 100-character long sequences generated\nfrom Press et al. (2017) is low.\n6 Conclusions\nWe propose an evaluation procedure for text\nGANs that is based on approximating the GAN\noutput distribution and using standard LM metrics.\nWe provide a bound for the number of samples re-\nquired for the approximation and empirically show\nin practice as few as 2000 samples per time-step\nsufﬁce. We evaluate character-based GAN mod-\nels using our procedure, and show their perfor-\nmance is substantially lower than state-of-the-art\nLM. We hope our simple evaluation method leads\nto progress in GAN-based text generation by shed-\nding light on the quality of such models.\nAcknowledgements\nWe would like to thank Shimi Salant for his com-\nments and suggestions. This research was par-\ntially supported by The Israel Science Foundation\ngrant 942/16, the Blavatnik Computer Science Re-\nsearch Fund, and The Yandex Initiative for Ma-\nchine Learning.\nReferences\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2018. Language gans falling short. arXiv preprint\narXiv:1811.02549.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Mike Schuster, Zhifeng Chen,\net al. 2018. The best of both worlds: Combining re-\ncent advances in neural machine translation. arXiv\npreprint arXiv:1804.09849.\nHerman Chernoff et al. 1952. A measure of asymp-\ntotic efﬁciency for tests of a hypothesis based on the\nsum of observations. The Annals of Mathematical\nStatistics, 23(4):493–507.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2016. Hierarchical multiscale recurrent neural net-\nworks. arXiv preprint arXiv:1609.01704.\nTim Cooijmans, Nicolas Ballas, C´esar Laurent, C ¸ a˘glar\nG¨ulc ¸ehre, and Aaron Courville. 2016. Re-\ncurrent batch normalization. arXiv preprint\narXiv:1603.09025.\nYoav Goldberg. 2017. Neural network methods for nat-\nural language processing. Synthesis Lectures on Hu-\nman Language Technologies, 10(1):1–309.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680.\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-\ncent Dumoulin, and Aaron C Courville. 2017. Im-\nproved training of wasserstein gans. In Advances\nin Neural Information Processing Systems, pages\n5767–5777.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong\nYu, and Jun Wang. 2017. Long text generation via\nadversarial training with leaked information. arXiv\npreprint arXiv:1709.08624.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, G ¨unter Klambauer, and Sepp\nHochreiter. 2017. Gans trained by a two time-scale\nupdate rule converge to a nash equilibrium. arXiv\npreprint arXiv:1706.08500.\nWassily Hoeffding. 1963. Probability inequalities for\nsums of bounded random variables. Journal of the\nAmerican statistical association, 58(301):13–30.\nDan Jurafsky and James H Martin. 2018. Speech\nand language processing: Third Edition, volume 3.\nPearson London.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In AAAI, pages 2741–2749.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2017. Dynamic evaluation of neural\nsequence models. arXiv preprint arXiv:1709.07432.\nBen Krause, Liang Lu, Iain Murray, and Steve Renals.\n2016. Multiplicative lstm for sequence modelling.\narXiv preprint arXiv:1609.07959.\nSidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang,\nand Yong Yu. 2018. Neural text genera-\ntion: Past, present and beyond. arXiv preprint\narXiv:1803.07133.\nMinh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2015. Multi-task\nsequence to sequence learning. arXiv preprint\narXiv:1511.06114.\nGraham Neubig. 2017. Neural machine translation\nand sequence-to-sequence models: A tutorial. arXiv\npreprint arXiv:1703.01619.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nOﬁr Press, Amir Bar, Ben Bogin, Jonathan Berant,\nand Lior Wolf. 2017. Language generation with re-\ncurrent generative adversarial networks without pre-\ntraining. In 1st Workshop on Learning to Generate\nNatural Language at ICML 2017.\nSai Rajeswar, Sandeep Subramanian, Francis Dutil,\nChristopher Pal, and Aaron Courville. 2017. Adver-\nsarial generation of natural language. arXiv preprint\narXiv:1705.10929.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. arXiv preprint\narXiv:1511.06732.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks.\nStanislau Semeniuta, Aliaksei Severyn, and Syl-\nvain Gelly. 2018. On accurate evaluation of\ngans for language generation. arXiv preprint\narXiv:1806.04936.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nRichard S Sutton, David A McAllester, Satinder P\nSingh, and Yishay Mansour. 2000. Policy gradi-\nent methods for reinforcement learning with func-\ntion approximation. In Advances in neural informa-\ntion processing systems, pages 1057–1063.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang,\nand Jiebo Luo. 2016. Image captioning with seman-\ntic attention. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n4651–4659.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In AAAI, pages 2852–2858.\nJunbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan-\nder M Rush, and Yann LeCun. 2017. Adversari-\nally regularized autoencoders for generating discrete\nstructures. CoRR, abs/1706.04223.\nJulian Georg Zilly, Rupesh Kumar Srivastava,\nJan Koutn ´ık, and J ¨urgen Schmidhuber. 2016.\nRecurrent highway networks. arXiv preprint\narXiv:1607.03474.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6085501909255981
    },
    {
      "name": "Natural language processing",
      "score": 0.5271909832954407
    },
    {
      "name": "Linguistics",
      "score": 0.420365571975708
    },
    {
      "name": "Philosophy",
      "score": 0.10411879420280457
    }
  ],
  "institutions": [],
  "cited_by": 23
}