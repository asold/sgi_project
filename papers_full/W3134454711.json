{
  "title": "OmniNet: Omnidirectional Representations from Transformers",
  "url": "https://openalex.org/W3134454711",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5057786383",
      "name": "Yi Wei Daniel Tay",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5031888985",
      "name": "Mohammad Dehghani",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5058345225",
      "name": "Vamsi Aribandi",
      "affiliations": [
        "Birla Institute of Technology and Science, Pilani"
      ]
    },
    {
      "id": "https://openalex.org/A5008444234",
      "name": "Jai Prakash Gupta",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5058085974",
      "name": "Philip Pham",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5317838346",
      "name": "Deleted Author ID",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5036477705",
      "name": "Dara Bahri",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5005730523",
      "name": "Da-Cheng Juan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5000115067",
      "name": "Donald Metzler",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W1532854728",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2949210751",
    "https://openalex.org/W2949454572",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2798362442",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3066373881",
    "https://openalex.org/W3127839344",
    "https://openalex.org/W2894175714",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3015146382"
  ],
  "abstract": "This paper proposes Omnidirectional Representations from Transformers (OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive field, each token is allowed to attend to all tokens in the entire network. This process can also be interpreted as a form of extreme or intensive attention mechanism that has the receptive field of the entire width and depth of the network. To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model. In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based (Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer et al.) as the meta-learner. Extensive experiments are conducted on autoregressive language modeling (LM1B, C4), Machine Translation, Long Range Arena (LRA), and Image Recognition. The experiments show that OmniNet achieves considerable improvements across these tasks, including achieving state-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena. Moreover, using omnidirectional representation in Vision Transformers leads to significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups.",
  "full_text": "OmniNet: Omnidirectional Representations from Transformers\nYi Tay* 1 Mostafa Dehghani* 2 Vamsi Aribandi1 3 Jai Gupta1 Philip Pham1\nZhen Qin1 Dara Bahri1 Da-Cheng Juan1 Donald Metzler1\nAbstract\nThis paper proposes Omnidirectional Repre-\nsentations from Transformers ( OMNI NET). In\nOmniNet, instead of maintaining a strictly horizon-\ntal receptive ﬁeld, each token is allowed to attend\nto all tokens in the entire network. This process\ncan also be interpreted as a form of extreme\nor intensive attention mechanism that has the\nreceptive ﬁeld of the entire width and depth of the\nnetwork. To this end, the omnidirectional attention\nis learned via a meta-learner, which is essentially\nanother self-attention based model. In order to\nmitigate the computationally expensive costs of\nfull receptive ﬁeld attention, we leverage efﬁcient\nself-attention models such as kernel-based (Choro-\nmanski et al., 2020), low-rank attention (Wang\net al., 2020) and/or Big Bird (Zaheer et al., 2020)\nas the meta-learner. Extensive experiments are\nconducted on autoregressive language modeling\n(LM1B, C4), Machine Translation, Long Range\nArena (LRA), and Image Recognition. The\nexperiments show that OmniNet achieves consid-\nerable improvements across these tasks, including\nachieving state-of-the-art performance on LM1B,\nWMT’14 En-De/En-Fr, and Long Range Arena.\nMoreover, using omnidirectional representation\nin Vision Transformers leads to signiﬁcant\nimprovements on image recognition tasks on both\nfew-shot learning and ﬁne-tuning setups.\n1. Introduction\nTransformers (Vaswani et al., 2017), characterized by\nstacked self-attention modules and feed-forward transforma-\ntions, have become a staple in modern deep learning, natural\nlanguage processing (Devlin et al., 2018; Raffel et al., 2019)\nand even computer vision (Dosovitskiy et al., 2020). One\n*Equal contribution 1Google Research, Mountain View2Google\nBrain Team, Amsterdam 3Google AI Resident. Correspon-\ndence to: Yi Tay <yitay@google.com>, Mostafa Dehghani\n<dehghani@google.com>.\nPreprint, Copyright 2021 by the author(s).\nkey deﬁning characteristics in the self-attention mechanism\nis the global receptive ﬁeld in which each token is accessible\nto every other token in the sequence, serving as an enabler\nfor learning global contextual representations.\nThis paper proposes learning omnidirectional represen-\ntations from transformers. The key idea is to move\nbeyond horizontally global receptive ﬁelds and explore the\npossibility of omnidirectional receptive ﬁelds. In short,\nwe allow each token to not only attend to all other tokens\nin the same layer, but also all token in all the layers of the\nnetwork. This global access enables tokens to have a full\nview of the network and as a result access the knowledge\nand intermediate representations of every token at each layer.\nBy modeling the relationships amongst tokens of different\nhierarchical levels, we are also able to capture patterns\npertaining to the propagation of representations across time.\nFinally, this approach can be also be interpreted as a form\nof dense residual connection (Huang et al., 2017), which has\nbeen shown to be beneﬁcial by aiding gradient ﬂow.\nLearning omnidirectional receptive ﬁelds is non-trivial for\ntwo key reasons. Firstly, given the quadratic complexity of\nthe scaled dot product attention, the complexity of designing\nsuch a receptive ﬁeld is increased from N2L to (NL)2,\nwhere Lis the depth of the network andN is the sequence\nlength. We postulate that this is one big challenge that has\nprohibited this type of architecture from being explored\nin the past. Secondly, simply enabling omnidirectional\nattention from the get-go would easily cause a degeneration\nof the transformer into a ﬂat network, losing much of its\nrepresentational power that is enabled by sequentially\nreﬁning its representations across network layers.\nTo mitigate these issues, our omnidirectional attention is\nimplemented as a form of meta-learner that acts upon a\nstandard transformer model. The meta-learner is yet another\nself-attention model that accepts all hidden representations\nacross all layers as an input and reﬁnes them based on all\nthe available information. In order to mitigate the prohibitive\nmemory and computational costs of omnidirectional atten-\ntion, we explore and evaluate multiple efﬁcient alternatives\nof parameterizing the meta-learner, e.g., including fast\nattention via generalizable kernel attention (Choromanski\net al., 2020), low-rank self-attention (Wang et al., 2020),\narXiv:2103.01075v1  [cs.CV]  1 Mar 2021\nOmniNet: Omnidirectional Representations from Transformers\nand/or block-based sparsity (Zaheer et al., 2020). Addition-\nally, we further hypothesize that employing methods that\ntry to learn the low-rank factorized structure of the entire\nnetwork can lead to improved generalization capabilities -\nas demonstrated in our few-shot learning experiments.\nAside from varying the parameterization of the meta-learner,\nwe also introduce partitioned variants of OmniNet in which\nthe meta-learner is applied to blocks ofpconsecutive layers.\nIn short, this partitioning strategy groups the full network\nof Llayers into L\np partitions. After computing each partition,\nthe meta-learner learns the omnidirectional attention of all\nnodes across all layers in the partition.\nVia extensive experiments, we show that OmniNet achieves\nvery promising results on a myriad of language, vision, and\nlogic tasks. Speciﬁcally, we report strong experimental\nresults on autoregressive language modeling (Chelba et al.,\n2013; Raffel et al., 2019), ﬁve collections of WMT machine\ntranslation, Long Range Arena (Tay et al., 2020a), and Image\nRecognition using Vision Transformers (Dosovitskiy et al.,\n2020). Moreover, we systematically evaluate OmniNets\nthrough the lens of the performance-compute trade-off and\nshow that they are pareto-optimal in this regard.\nOn machine translation, OmniNet outperforms ADMIN (Liu\net al., 2020), the current state-of-the-art 60 layers deep\ntransformer model on two well-established machine\ntranslation collections (WMT’14 English-German and\nWMT’14 English-French). On the one billion language\nmodeling benchmark, OmniNet outperforms existing\nstate-of-the-art models such as Transformer-XL (Dai et al.,\n2019). On LRA, OmniNet improves aggregate performance\nover Performers (Choromanski et al., 2020) by+8.9% and\nvanilla Transformers by +2.6%. On Image Recognition\ntasks, OmniNet demonstrates stellar few-shot learning and\nﬁnetuning performance, outperforming ViT (Dosovitskiy\net al., 2020) by up to≈+3% on both ﬁnetuning and few-shot\nlearning experiments.\n2. Related Work\nJust across the past several years, attention mechanisms (Bah-\ndanau et al., 2014) have made a signiﬁcant impact on machine\nlearning research (Vaswani et al., 2017; Devlin et al., 2018;\nDosovitskiy et al., 2020; Raffel et al., 2019; Brown et al.,\n2020; Dehghani et al., 2018). Simply speaking, these param-\neterized pooling mechanisms learn to align representations\nand route information based on the notion of relative impor-\ntance. While early work in this area was mainly concerned\nwith learning an alignment function between two or more\nsequences (Bahdanau et al., 2014; Parikh et al., 2016), there\nhave been more focus on self-alignment (e.g., self-attention)\nin the recent research climate (Vaswani et al., 2017).\nAttention mechanisms are generally applied layer-wise and\noperate across a one-dimensional sequence. Attention is\ngenerally bidirectional, or unidirectional in the case where a\ntoken is to be denied access to future tokens. There have been\nearly attempts to mix information across layers in pursuit\nof improving gradient ﬂow and model trainability. For\nexample, (Bapna et al., 2018) proposed transparent attention\nin which the decoder gains access to all encoder layers.\n(He et al., 2018) proposed layer-wise coordination between\nencoder-decoder for machine translation. (Tay et al., 2018)\nproposed to densely connect the attention across stacked\nRNN encoder layers for language understanding tasks. The\nrecent Realformer (residual attention) (He et al., 2020)\nproposed connecting the attention activations in a residual\nfashion. We believe there is sufﬁcient evidence in the liter-\nature to suggest that mixing representations across layers is\nbeneﬁcial. This is further supported by fundamental work in\nthis area such as ResNets (He et al., 2016), highway networks\n(Srivastava et al., 2015) and DenseNets (Huang et al., 2017).\nIn this paper, we are mainly interested in methods for\nefﬁciently learning omnidirectional attention - an attention\nover the entire width and depth of the network. To this end,\nwe leverage the recent advances in making transformers\nfast and efﬁcient (Zaheer et al., 2020; Choromanski et al.,\n2020; Wang et al., 2020). Many of these approaches\nlearn an approximation via low-rank projection, kernels or\nblock-based sparsity. An overview and extensive empirical\ncomparison can be found at (Tay et al., 2020b;a). To this end,\nthe proposed approach leverages these recent advances to\nmake what was previously not possible. By leveraging fast\nand efﬁcient self-attention, we enable scalable and powerful\nomnidirectional attention.\n3. The Proposed Method\nThis section introduces OmniNet. We ﬁrst begin by\nreviewing the standard Transformer architecture.\n3.1. Transformer Architectures\nThis section provides a brief background for the Transformer\narchitecture. The Transformer block accepts N×dinput,\nwhere Ndenotes the number of tokens in the sequence and\nddenotes the size of the representation. Each Transformer\nblock is characterized by a self-attention block and a two\nlayered feed-forward network with ReLU activations\nin-between that is applied position-wise.\n3.1.1. S ELF -ATTENTION\nThe self-attention mechanism ﬁrst projects each inputXinto\nQ,K,V representations using linear transformations, cor-\nresponding to queries, keys, and values. The self-attention\nmechanism is typically multi-headed where multiple similar\nlinear projections are executed in parallel. The output of\nOmniNet: Omnidirectional Representations from Transformers\n...\n...\n...\n...\n...\n...\n...\n...\nOmnidirectional Attention\nStandard LayersOmnidirectional Layer\npoolingpoolingpoolingpooling...\n OmnidirectionalRepresentations\n...\nFigure 1.Overview of OmniNet. In the diagram, the omnidi-\nrectional module, when partition size is P = L, combines the\ninformation across all positions (1: N), across all layers (1: L−1),\nand for each position selects the best of all layers via a pooling\noperation to generate the ﬁnal representations.\neach self-attention headhat layer lis written as:\nyh,l =softmax\n(\nQh,lK⊤\nh,l√dk\n)\nVh,l, (1)\nwhere yh,l is the output of head hat layer land dk is the\nsize of each head. The output from the multiple heads is\nthen concatenated and then passed through another linear\ntransformation via Wo,l which projects the concatenation\nof all heads down to dm. This is wrapped via a layer\nnormalization followed by a residual connection and can be\nwritten as: LayerNorm(Wo,lconcat([y1,l···yH,l))) +xl−1\nas the ﬁnal output of the self-attention module.\nFeed Forward Layers The FFN block of the Transformer\nblock performs a two layer transformation deﬁned as:\nzl =LayerNorm(W1,lReLU(W2,l(Y)))+zl−1, (2)\nwhere W1,W2 are trainable parameters (weight transforms)\nof the FFN layer. Bias parameters are omitted for clarity.\n3.2. OmniNet\nThe proposed OmniNet method operates on an arbitrary\nmulti-layered architecture that accepts sequential inputs. In\nour description, this typically refers to a stacked X-former\narchitecture in this section. Note that while this is typically a\ntransformer model, it can also be an arbitrary variant (Choro-\nmanski et al., 2020; Wang et al., 2020). Figure 1 illustrates\na brief overview of the proposed OmniNet architecture.\n3.2.1. O MNIDIRECTIONAL REPRESENTATIONS\nIn a stacked network of L layers, each layer exposes a\nsequence of N vectors of ddimensions each. Speciﬁcally,\nOmniNet operates across all layers and connects the\nmulti-layered network architecture in a grid like fashion.\nWe describe the network asxformer which accepts Xas an\ninput and returns a tensor ofL×N×ddimensions.\nxformer(X)= X1,X2···XL, (3)\nwhere Xi ∈RN×d. Let Xi\nj be the representation of X\nat layer i and position j of the sequence. The OmniNet\nmechanism can be written as:\nO=Attend(IndexSort(X1,X2,···XL)), (4)\nwhere Attend denotes an arbitrary self-attention block. The\nIndexSort operation takes X1,X2,XL and sorts,1 tokens\nwithin each matrix by index such that the adjacent token of\nthe ith token in layer lare the ith token from l−1 and l+1\nrespectively. Next, given that the input sequence length\nis LN, it is advantageous for Attend to be as efﬁcient as\npossible. We describe three variants of OmniNet’s core\nlinear-time self-attention mechanism in subsequent sections.\nGiven O ∈R(L×N)×d, the output of the omnidirectional\nattention, we performP(.) a pooling operator. While there\nare many choices of pooling operators, parameterized or\notherwise, we adopt a simple pooling function - a max\npooling of strideL.\nO′=MaxPool1D(O), (5)\nwhere O′∈RN×d. Given O′, the ﬁnal representation of an\nOmniNet augmented network is deﬁned as:\nOmniNet(X)=xformer( X)L+O′. (6)\nThe OmniNet and main transformer model are trained\ntogether in an end-to-end fashion, i.e., gradients ﬂow to both\nnetworks concurrently at each backward pass.\n3.2.2. M AINTAINING\nCAUSALITY AND AUTOREGRESSIVE DECODING\nA key point to note withIndexSort is that this order enables\nus to apply a causal mask to the Attend function, namely\nif tokens are sorted according to sequence index ﬁrst as\nopposed to layer ﬁrst, then it would be easy to apply a causal\nmask M, where M[i,j]=0 when i≤jand −inf when i>j.\nThis enables OmniNet to be used in autoregressive settings.\n1Since attention is permutation invariant this sorting simply\nmakes it easier to (1) compute casual masks and (2) aggregate\nrepresentations index-wise.\nOmniNet: Omnidirectional Representations from Transformers\n3.2.3. E FFICIENT TRANSFORMERS\nWe describe several choices of linear-time self-attention\nmechanisms that are used in OmniNet’s omnidirectional\nattention. Generally,Attend refers to an attention block with\nan attention function and a two-layered positional FFN in\na similar structure to the transformer backbone. For the sake\nof brevity, we only describe the core attention mechanism\nhere. Our choice of the efﬁcient transformer is informed\nby (Tay et al., 2020a) selecting models that perform well on\nthe compute-performance trade-off. For a list of potential\nvariants to adopt, we refer readers to (Tay et al., 2020b).\nKernel-based This variant uses the generalizable kernel\nattention, the fast attention mechanism proposed in Per-\nformer (Choromanski et al., 2020). Speciﬁcally, this is\nwritten as:\no=Woconcat( ˆDh\n−1\n(φ(Qh)(φ(Kh))⊤Vh)),\nwhere ˆDh =diagφ(Qh)((φ(Kh))⊤1L) and φ(.) is a random\nfeature map that projects Rd to Rr. We refer readers\nto (Choromanski et al., 2020) for more details.\nLow-rank Inspired by Linformer’s (Wang et al., 2020)\nself-attention mechanism, we setAttend to be:\no=Wo(concat(softmax\n(Qh(WKh)⊤\n√dk\n)\n(WVh)),\nwhere W∈RN×k are low-rank projection transformations\nthat are shared across heads and across keys and values. The\ncomplexity of this self-attention mechanism isNk instead\nof N2, where k≪N.\nBlock and Memory based Lastly, we also explore a\nblock and memory-based variant of efﬁcient Transformers\nbased on Big Bird (Zaheer et al., 2020). In short, this is a\ncombination of windowed attention, global attention, and\nsparse attention. The output for tokeniis deﬁned as:\noi =xi+\nH∑\nh=1\nsoftmax\n(\nQh,iK⊤\nh,N(i)\n)\nVh,i,\nwhere N(i) is the neighborhood function which denotes the\nout-neighbors of nodei, His the total number of heads and\nhrepresents a head. The neighborhood function is mainly\ndependent on the width of the windowed attention. We refer\nthe reader to (Zaheer et al., 2020) for more details.\n3.2.4. P ARTITIONED OMNI NET\nThis section describes the types of partitioning variants\nthat we explore in OmniNet. WhenLis large, the eventual\nrepresentation input to OmniNet can be extremely large.2\n2A sequence length of 1K would result in a 11K input\nsequence length for a 12 layered Transformer model, when using\nan omnidirectional layer as the ﬁnal layer.\nTable 1.Experimental results (quality, i.e., perplexity scores at 30K\nand 100K respectively) on autoregressive language modeling. All\nmodels are approximately 50M parameters.\nModel LM1B C4\nTransformer 33.14 34.86\nRealformer 32.95 35.63\nPerformer 34.33 35.68\nBigBird 32.90 38.36\nOmniNetB 33.69 (-1.7%) 34.73 (+0.4%)\nOmniNetP 30.19 (+9.0%) 33.97 (+2.6%)\nOmniNetT 30.12 (+9.1%) 33.39 (+4.2%)\nTable 2.Comparison with existing state-of-the-art and published\nworks on One Billion Word Language modeling (Chelba et al.,\n2013) benchmark.\nModel #Params PPL\nAdaptive Input (Baevski & Auli) 0.5B 24.1\nAdaptive Input (Baevski & Auli) 1.0B 23.7\nTransformer-XL (Dai et al.) 0.5B 23.5\nTransformer-XL (Dai et al.) 0.8B 21.8\nOmniNetP (Large) 0.1B 21.6\nOmniNetB (Large) 0.1B 22.0\nOmniNetT (Large) 0.1B 21.5\nLet P be an integer valued hyperparameter that determines\nthe partition size. For aLlayer transformer network, when\nℓ mod P is 0, we insert a meta-learner block.\nXℓ =\n{\nAttend(Xℓ−P ,···Xℓ−1)),if ℓ modP=0\nxformer(Xℓ−1)\nIn short, wheneverℓ mod P=0, we activate an omnidirec-\ntional attention layer, aggregating representations all the way\nfrom the previous partition ℓ−P layer up till ℓ−1. In this\ncase, we skip the originalxformer layer, hencemaintaining\napproximately the same parameter sizeof the network.\n4. Experiments\nWe conduct experiments on autoregressive language mod-\neling, machine translation, long range sequence modeling\nand a series of image recognition tasks. Our implementation\nuses Flax (Heek et al., 2020) and Jax (Bradbury et al., 2018).\n4.1. Autoregressive Language Modeling\nWe run experiments on large-scale autoregressive (uni-\ndirectional) language modeling. We use two large-scale\ndatasets, language modeling one billion (LM1B) (Chelba\net al., 2013) and the Colossal Cleaned Common Crawl\ncorpus (C4) (Raffel et al., 2019).\nOmniNet: Omnidirectional Representations from Transformers\nTable 3.Results on ﬁve collections from the WMT’17 machine translation task.\nModel En-De En-Fi Cs-En En-Fr Ru-En\nTransformer. 28.6 20.5 22.2 43.0 35.8\nOmniNetL 28.8 (+0.7%) 20.8 (+1.5%) 22.8 (+2.7%) 43.3 (+0.7%) 36.2 (+1.1%)\nOmniNetB 28.8 (+0.7%) 20.9 (+2.0%) 22.6 (+1.8%) 43.2 (+0.5%) 34.2 (-4.5%)\nOmniNetP 29.0 (+1.4%) 20.9 (+2.0%) 23.0 (+3.6%) 43.1 (+0.2%) 36.2 (+1.1%)\nTable 4.Comparisons with the state-of-the-art on WMT’14 En-De\nand WMT’14 En-Fr. OmniNet outperforms ADMIN (Liu et al.,\n2020), the current state-of-the-art deep transformer model for MT.\nModel En-De En-Fr\nEvolved Trans. (So et al., 2019) 29.2 n/a\nLarge Trans. (Ott et al., 2018) 28.6 41.4\n60L Trans. (Liu et al., 2020) 29.5 41.8\nOmniNetP 29.8 42.6\n4.1.1. E XPERIMENTAL SETUP\nFor both tasks, we use a max length of256 subword tokens\nper example and report scores on subword perplexity on\nthe validation set. In the ﬁrst ablative experiment, we\ntrain all models for 30K for LM1b and 100K steps for\nC4 using 16 TPU-V3 Chips. Models are of base size and\nhave an embedding dimension of512, 8 heads, 6 layers and\nhidden dimensions (MLP) of 2048. For strong baselines,\nwe compare with Transformers, Performers (Choromanski\net al., 2020), and BigBird (Zaheer et al., 2020). We also add\nthe recent Realformer (residual attention Transformer) (He\net al., 2020) as a strong baseline. For OmniNet, we tune the\npartition amongst {2,3,6}. All models have approximately\n50M parameters. Next, we are interested in (1) how\nOmniNet scales to large sizes and (2) comparing with other\npublished works (Dai et al., 2019). Hence, we implement\na larger OmniNet with MLPs of size 8K and head size of 2K.\n4.1.2. R ESULTS ON LANGUAGE MODELING\nTable 1 reports results on LM. We observe that OmniNetP,T\noutperforms all baselines by about +9.1% on LM1b and\n+4.2% on C4. We also outperform strong baselines such as\nRealformer, BigBird, and vanilla Transformers on both cor-\npora. We also observe that OmniNetP performs reasonably\nclose to OmniNetT , which ascertains that using an efﬁcient\nTransformer may be sufﬁcient for omnidirectional attention.\nOn the other hand, Table 2 reports a comparison with other\npublished works on LM1B. Notably, OmniNetP,T (large)\noutperforms Transformer-XL and achieves state-of-the-art\nperformance.\n4.2. Neural Machine Translation\nWe conduct experiments on machine translation, a sequence-\nto-sequence task. for evaluating Transformer models.\nTable 5.Results on Long Range Arena (Tay et al., 2020a).\nModel Text Retrieval ListOps Avg\nLinformer 53.9 52.3 35.7 47.3\nBigBird 64.0 54.7 36.1 51.6\nPerformer 65.4 53.8 18.0 45.7\n+OmniNetP 65.6 60.9 18.2 48.2\n+OmniNetL 63.1 63.7 37.1 54.6\nTransformer 62.5 57.5 36.4 52.1\n+OmniNetP 65.1 58.8 37.2 53.7\n+OmniNetL 63.1 63.8 37.2 54.7\n4.2.1. E XPERIMENTAL SETUP\nWe use ﬁve collections/datasets from WMT-17,3 namely En-\nDe (English →German), En-Cs (English→Czech), En-Fi\n(English →Finnish), En-Fr (English→French) and En-Ru\n(English →Russian). We train all models using16 TPU-V3\nchips with a batch size of256. Our base Transformer model\nhas 6 layers, a hidden size of4096, embedding size of1024,\nand a head size of1024. The number of heads is16. We use\na SentencePiece (Kudo & Richardson, 2018) vocabulary of\n32Kbuilt for each language speciﬁcally. More details can\nbe found in the appendix.\n4.2.2. R ESULTS ON WMT’17 M ACHINE TRANSLATION\nTable 3 reports results on all 5 collections of WMT’17. Over-\nall, OmniNetP outperforms the vanilla Transformer model\non all ﬁve collections, with up to≈+3.6% performance im-\nprovement. Similar to LM, we ﬁnd that the performer variant\nworks the best and the BigBird variant works the worse.\n4.2.3. C OMPARISONS AGAINST STATE -OF-THE -ART\nWe train a large OmniNet model and compare it with the\nstate-of-the-art approaches. We compare with ADMIN (Liu\net al., 2020), a very deep (60 layers) Transformer model\nthat achieves state-of-the-art performance on the WMT\nEn-De dataset. We use a8 layer OmniNet model with4096\nMLP dimensions, 1024 hidden dimensions and embedding\ndimensions. We compare models using sacrebleu (Post,\n2018). For OmniNet, given the strong performance of the\nPerformer variant on WMT’17 collections, we only train a\nsingle P variant OmniNet for comparing with SOTA models.\nFurther details of the setup can be found in the appendix.\n3http://www.statmt.org/wmt17/\ntranslation-task.html\nOmniNet: Omnidirectional Representations from Transformers\nTable 4 reports results on WMT’14 En-De and En-Fr.\nOur results show that OmniNet outperforms the existing\nstate-of-the-art ADMIN model (Liu et al., 2020), a 60-layer\ndeep transformer model.\n4.3. Long Range Arena\nWe conduct experiments on the recently proposed Long\nRange Arena benchmark (Tay et al., 2020a). The goal of this\nexperiment is to show that OmniNet improves long-range se-\nquence modeling. A dual goal is to show that it is possible to\ncombine different inductive biases to obtain a better efﬁcient\nTransformer model that is versatile on different types of data.\n4.3.1. E XPERIMENTAL SETUP\nWe run two key experiments using Transformer and\nPerformer as the main backbone model and vary the meta-\nlearner in OmniNet, using Linformer and Performer variants\nof the OmniNet meta-learner. The goal is to demonstrate\nthat OmniNet translates to backbone agnostic improvements.\nWe run OmniNet experiments using the LRA codebase and\nrun OmniNet models using the same hyperparameters as\nthe results reported in (Tay et al., 2020a). Note that LRA is\ncomprised of ﬁve benchmarks, however, we omit Image and\nPathﬁnder experiments since the best hyperparameters on\nthese tasks turn out to be shallow (1-2 layered) models. We\nreport the average of the text, retrieval, and ListOps tasks.\n4.3.2. R ESULTS ON LRA\nTable 5 reports the results on our LRA experiments. Firstly,\nwe observe that OmniNet makes substantial improvements\nto the base model, regardless of whether it is a Transformer\nor Performer. Notably, with OmniNet L, the Linformer\nmeta-learner, the Performer model is improved by almost6 to\n7 absolute percentage points. An interesting observation can\nbe made on the ListOps task where OmninetP (Performer\nvariant) does not result in much improvement over the\nbase Performer. However, the performance doubles with\nOmniNetL. Since the base Linformer model does pretty\nwell on this task, we postulate that this is due to OmniNetL\nproviding a Linformer-like inductive bias to the Performer\nmodel. Finally, we observe that OmniNet improves the\nvanilla Transformer in both cases (P or L), improving the\naverage score by about+2.6% absolute percentage points.\n4.4. Image Recognition\nTransformer-based models started showing competitive per-\nformance on different vision tasks like classiﬁcation, object\ndetection, and segmentation (Chen et al., 2020; Dosovitskiy\net al., 2020; Carion et al., 2020; Kumar et al., 2021).\nTo showcase the power of omnidirectional representations in\nyet another task, we incorporate the omnidirectional represen-\ntation in Vision Transformer (ViT) (Dosovitskiy et al., 2020),\nwhen pre-trained on a large amount of data in a supervised\nfashion and evaluated on downstream image recognition\ntasks, either through few-shot learning or ﬁne-tuning.\n4.4.1. V ISION TRANSFORMER\nVision Transformers (ViT) (Dosovitskiy et al., 2020) have\nrecently shown impressive results on image classiﬁcation\ncompared to state-of-the-art convolutional networks, while\nthey require signiﬁcantly fewer computational resources to\ntrain. ViT is a standard Transformer that is directly applied\nto images. To do so, we ﬁrst split the input images into non-\noverlapping patches and embedded them using a linear pro-\njection. The patch embeddings are provided as a sequence of\ntokens to a Transformer. When pre-trained on large datasets\n(14M-300M images) at a sufﬁcient scale, ViT shows excellent\nresults that are transferable to tasks with fewer data points.\n4.4.2. E XPERIMENTAL SETUP\nSimilar to the ViT setup, we pre-train our OmniNet models on\nthe JFT dataset (Sun et al., 2017) with 18k classes and 303M\nimages, for 7 epochs. We evaluate our models in the transfer\nsetup (few-shot and ﬁne-tuning) on several downstream\ntasks: ImageNet, CIFAR-10, CIFAR-100 (Krizhevsky et al.,\n2009), Oxford-IIIT Pets (Parkhi et al., 2012), and Oxford\nFlowers-102 (Nilsback & Zisserman, 2008). We follow\nthe pre-processing from (Kolesnikov et al., 2019) on both\nupstream and downstream datasets, which is used in the\noriginal ViT experiments.\nIn our experiments, we train and evaluate OmniNetB/32 and\nOmniNetB/16, which are based on ViTB/32 and ViTB/16.4\nSimilar to ViT B/32 and ViT B/16, OmniNet B/32 and\nOmniNetB/16 are both “base” models, adopted from BERT,\nand use patch sizes of32×32 and 16×16 respectively.\nIn our OmniNet models, we replace the ﬁnal layer of ViT\nwith an omnidirectional layer. In other words, we set the\nportion size P= 12. In this task, we limit our experiments\nto using Performers (Choromanski et al., 2020) in the\nomnidirectional attention, given their strong results among\nthe efﬁcient transformer variants.\nDuring pre-training, we use a batch size of4096 using Adam\nwith β1 =0.9 and β2 =0.999, and use a weight decay of0.05\nfor OmniNet. We use a learning rate of8e−4 with a linear\ndecay and a linear warmup of10Ksteps.\n4Note that SOTA results on the downstream tasks we use here\nare from ViTH/14 (Dosovitskiy et al., 2020), which has more than\nseven times as many parameters than the base models we use as\nbaselines. Here, we aim at merely showcasing the gain of using\nomnidirectional representations in the image recognition task.\nOmniNet: Omnidirectional Representations from Transformers\nTable 6.Transfer performance of pre-trained OmniNet and equivalent ViT models in ﬁne-tuning setup on popular image classiﬁcation\nbenchmarks. All models are pre-trained on the JFT-300M dataset and ﬁne-tuned on the target dataset.\nViTB/32 OmniNetB/32 ViTB/16 OmniNetB/16\nImageNet 0.8073 → 0.8374 0.8415 → 0.8626\nCIFAR-10 0.9861 → 0.9900 0.9900 → 0.9940\nCIFAR-100 0.9049 → 0.9153 0.9186 → 0.9224\nOxford-IIIT Pets 0.9340 → 0.9441 0.9580 → 0.9674\nOxford Flowers-102 0.9927 → 0.9954 0.9956 → 0.9961\nexaFLOPs 165 193 743 891\n1 5 10 25\nnum shots\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70accuracy\nImageNet\nViTB/32 ViTB/16 OmniNetB/32 OmniNetB/16\n1 5 10 25\nnum shots\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95accuracy\nCIFAR10\n1 5 10 25\nnum shots\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80accuracy\nCIFAR100\n1 5 10 25\nnum shots\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95accuracy\nOxford_IIIT_Pets\n1 5 10 25\nnum shots\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00accuracy\nOxford Flowers-102\nFigure 2.Performance of pre-trained OmniNet and equivalent ViT models in few-shot learning setup on downstream tasks, when transferred\nusing only few images (1, 5, 10, and 25) per class.\n4.4.3. R ESULTS ON IMAGE RECOGNITION\nWe ﬁrst present the results of OmniNet and corresponding\nViT models as baselines in the ﬁne-tuning setup. For\nﬁne-tuning, we use SGD with momentum and a batch\nsize 512 in all downstream tasks. Table 6 presents the\nresults of ﬁne-tuning experiments. We also report the total\npre-training compute, in terms of number of FLOPs for each\nmodel. As we can see, introducing a module that learns\nomnidirectional representations to Vision Transformers\nleads to improvements on different downstream tasks. Given\nthese improvements and comparing the number of FLOPs for\nOmniNets and ViTs, we can see that the additional compute,\nthanks to efﬁcient attention techniques, is fairly reasonable.\nFor evaluating OmniNet in the few-shot learning setup, simi-\nlar to ViT, we train a linear classiﬁer on top of the representa-\ntions from the frozen pre-trained model, given only a subset\nof training examples. Plots in Figure 2 illustrate the accuracy\nof OmniNet and ViT, using different numbers of shots. The\nresults indicate that adding omnidirectional representations\nto ViT leads to better transfer across all downstream datasets.\n4.5. Effect of Partition\nSize and Compute/Performance Trade-offs\nOmniNet offers the ﬂexibility to apply the Omnidirectional\nlayers on different partition sizes. With smaller partition\nsizes, we attend to tokens from fewer layers, and with\nFigure 3.Performance of ViT and OmniNet (with different partition\nsizes) in terms of top-1 accuracy on ImageNet 5-shot linear, versus\ntheir computational costs in terms of number of FLOPs.\nbigger partition, we widen the vertical receptive ﬁeld of\nthe omnidirectional attention, which might be effective for\nlearning better representations by capturing information\nfrom more levels. In terms of computational costs, however,\nthere is a trade-off when choosing the partition size. Small\npartition sizes means running attention on smaller sequences\nwhile repeating it more frequent, and bigger partition sizes\nmeans dealing with longer sequences, but having fewer\nomnidirectional layers in the network.\nWe train OmniNet B/32 and OmniNetB/16 with different\npartition sizes: P = {1,2,4,6,12}. Partition size P = 1is\nsimply having no vertical attention and it is just replacing\nnormal attention in ViT, with Performer. We compare these\nOmniNet: Omnidirectional Representations from Transformers\nAttention MapsPooling Statistics\ninputLayer 1Layer 2Layer 3Layer 4Layer 5Layer 6Layer 7Layer8Layer 9Layer 10Layer 11\nFigure 4.Contribution of different layers in Omnidirectional representations for a given set of examples. On top, we plot the omnidirectional\nattention maps (using OmniNetB/16-P12 ) of one of the heads, over all layers, when CLS token in the last layer is used as query. On the\nbottom, we show the contribution of each layer to the pooling operation of the Omnidirectional module.\nmodels in terms of their linear 5-shot accuracy on ImageNet\ndataset (similar to the ablation studies in (Dosovitskiy et al.,\n2020)). Figure 3 presents the performance of each model\nas well as their computational cost during pre-training.\nA few patterns can be observed. For both OmniNet B/32\nand OmniNetB/16, the power of omnidirectional directional\nrepresentations kicks in when we work with partition sizes\nof more than 2. The input resolution during pre-training is\n224 ×224, so for /32 and /16 models the input sequence\nlength to the model is 49 and 196. So when setting P = 1\nor P = 2, with such sequence lengths, when using an\nefﬁcient attention engine, like Performer, which provides an\napproximation of the dot-product-attention, we do not gain\na lot on the speed and we lose a bit of performance. However,\nwhen using a larger partition size, the additional compute\nwith respect to the performance gain becomes reasonable.\nIn both /32 and /16, the computation cost is almost similar\nfor P=4 and P=6. With P=4, we have three omnidirec-\ntional attention, each applied on4 layers, while withP=6\nwe have two omnidirectional attention, each applied on 6\nlayers. However,P=6 gives slightly better results in terms\nof accuracy and is placed on a sweeter spot in this trade-off.\nWith P=12, the computational costs of OmniNet increase,\nbut the gain in the performance helps the model to be on the\nfrontier of the compute-performance trade-off, when it is\ncompared to OmniNetB/32 and OmniNetB/16.\n4.6. Visualization\nOmniNet combines information from different layers via\ntwo sequential mechanisms (§3.2.1): (1) omnidirectional\nattention, where representations of all tokens in all layers\nget updated with respect to each other using an efﬁcient\nattention mechanism; and (2) a pooling operation, where for\neach token, we collect the best values from all layers.\nIn order to understand how these two mechanisms combine\ninformation across different layers, we visualize attention\nmaps (Abnar & Zuidema, 2020) and pooling statistics for\na set of examples in the image recognition task. Figure 4\ndepicts three example inputs, where we show how OmniNet\nattends to different layers, as well as each layer’s contribution\nduring the pooling operation.\nWe can see that in some layers, attention seems to detect the\nobjects in the image via attending to the edges or speciﬁc\nparts of the object, while in other layers, the attention\nmechanism uses mostly background information. It is\nclear that omnidirectional attention does indeed use such\ninformation by actively attending to layers of varying depth.\nAdditionally, when performing the element-wise pool opera-\ntion over all the layers for each token, only a fraction of values\nfrom each layer’s representation make it to the ﬁnal represen-\ntation. The bottom rows in Figure 4 illustrate this fraction for\neach token (image patch) across different layers. In most ex-\namples, we observe that a majority of the representation after\nthe pooling operation comes from the ﬁrst few layers. This\nis further evidence of how OmniNet can provide an explicit\npath for directing ﬁne-grained information that is captured\nby the early layers to the ﬁnal output, leading to much richer\nrepresentations. For the sake of brevity, we refer readers to\nthe Appendix for more detailed plots for these examples as\nwell as other examples, which illustrate the same trends.\nOmniNet: Omnidirectional Representations from Transformers\n5. Conclusion\nIn this paper, we proposed OmniNet, which uses omnidirec-\ntional attention to connect all tokens across the entire network\nvia self-attention. In order to manage the computational\ncosts of the full receptive ﬁeld, the meta-learner in OmniNet\nis parameterized by fast and efﬁcient self-attention models.\nThe proposed method achieves stellar performance on a\nmyriad of language and vision tasks. Concretely, OmniNet\nachieves state-of-the-art performance on WMT EnDe and\nEnFr, outperforming deep 60-layer transformers. OmniNet\nalso demonstrates substantial improvement over ViT on\nimage recognition tasks.\nReferences\nAbnar, S. and Zuidema, W. Quantifying attention ﬂow in\ntransformers. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, 2020.\nBaevski, A. and Auli, M. Adaptive input representa-\ntions for neural language modeling. arXiv preprint\narXiv:1809.10853, 2018.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.arXiv\npreprint arXiv:1409.0473, 2014.\nBapna, A., Chen, M. X., Firat, O., Cao, Y ., and Wu, Y . Train-\ning deeper neural machine translation models with trans-\nparent attention. arXiv preprint arXiv:1808.07561, 2018.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,\nC., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,\nWanderman-Milne, S., and Zhang, Q. JAX: composable\ntransformations of Python+NumPy programs, 2018. URL\nhttp://github.com/google/jax.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. arXiv preprint arXiv:2005.12872, 2020.\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T.,\nKoehn, P., and Robinson, T. One billion word benchmark\nfor measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005, 2013.\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\nD., and Sutskever, I. Generative pretraining from pixels.\nIn International Conference on Machine Learning, pp.\n1691–1703. PMLR, 2020.\nChoromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,\nA., Kaiser, L., et al. Rethinking attention with performers.\narXiv preprint arXiv:2009.14794, 2020.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and\nSalakhutdinov, R. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, Ł. Universal transformers. arXiv preprint\narXiv:1807.03819, 2018.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale.arXiv\npreprint arXiv:2010.11929, 2020.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pp. 770–778, 2016.\nHe, R., Ravula, A., Kanagal, B., and Ainslie, J. Realformer:\nTransformer likes residual attention. arXiv e-prints, pp.\narXiv–2012, 2020.\nHe, T., Tan, X., Xia, Y ., He, D., Qin, T., Chen, Z., and Liu,\nT.-Y . Layer-wise coordination between encoder and\ndecoder for neural machine translation. InProceedings of\nthe 32Nd International Conference on Neural Information\nProcessing Systems, pp. 7955–7965, 2018.\nHeek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre,\nB., Steiner, A., and van Zee, M. Flax: A neural\nnetwork library and ecosystem for JAX, 2020. URL\nhttp://github.com/google/flax.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 4700–4708, 2017.\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\nJ., Gelly, S., and Houlsby, N. Big transfer (bit): Gen-\neral visual representation learning. arXiv preprint\narXiv:1912.11370, 2019.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. 2009.\nOmniNet: Omnidirectional Representations from Transformers\nKudo, T. and Richardson, J. Sentencepiece: A simple\nand language independent subword tokenizer and\ndetokenizer for neural text processing. arXiv preprint\narXiv:1808.06226, 2018.\nKumar, M., Weissenborn, D., and Kalchbrenner, N. Coloriza-\ntion transformer. arXiv preprint arXiv:2102.04432, 2021.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.),Proceedings of the 17th International Conference\non Machine Learning (ICML 2000), pp. 1207–1216,\nStanford, CA, 2000. Morgan Kaufmann.\nLiu, X., Duh, K., Liu, L., and Gao, J. Very deep trans-\nformers for neural machine translation. arXiv preprint\narXiv:2008.07772, 2020.\nNilsback, M.-E. and Zisserman, A. Automated ﬂower\nclassiﬁcation over a large number of classes. In 2008\nSixth Indian Conference on Computer Vision, Graphics\n& Image Processing, pp. 722–729. IEEE, 2008.\nOtt, M., Edunov, S., Grangier, D., and Auli, M. Scal-\ning neural machine translation. In Proceedings\nof the Third Conference on Machine Translation:\nResearch Papers, pp. 1–9, Brussels, Belgium, Oc-\ntober 2018. Association for Computational Linguis-\ntics. doi: 10.18653/v1/W18-6301. URL https:\n//www.aclweb.org/anthology/W18-6301.\nParikh, A. P., T ¨ackstr¨om, O., Das, D., and Uszkoreit, J.\nA decomposable attention model for natural language\ninference. arXiv preprint arXiv:1606.01933, 2016.\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C.\nCats and dogs. In 2012 IEEE conference on computer vi-\nsion and pattern recognition, pp. 3498–3505. IEEE, 2012.\nPost, M. A call for clarity in reporting bleu scores. arXiv\npreprint arXiv:1804.08771, 2018.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nSo, D. R., Liang, C., and Le, Q. V . The evolved transformer.\narXiv preprint arXiv:1901.11117, 2019.\nSrivastava, R. K., Greff, K., and Schmidhuber, J. Highway\nnetworks. arXiv preprint arXiv:1505.00387, 2015.\nSun, C., Shrivastava, A., Singh, S., and Gupta, A. Revisiting\nunreasonable effectiveness of data in deep learning era.\nIn Proceedings of the IEEE international conference on\ncomputer vision, 2017.\nTay, Y ., Tuan, L. A., Hui, S. C., and Su, J. Densely connected\nattention propagation for reading comprehension.arXiv\npreprint arXiv:1811.04210, 2018.\nTay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D., Pham,\nP., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long\nrange arena: A benchmark for efﬁcient transformers.\narXiv preprint arXiv:2011.04006, 2020a.\nTay, Y ., Dehghani, M., Bahri, D., and Metzler, D. Efﬁcient\ntransformers: A survey. arXiv preprint arXiv:2009.06732,\n2020b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention\nis all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nWang, S., Li, B., Khabsa, M., Fang, H., and Ma, H.\nLinformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nZaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti,\nC., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L.,\net al. Big bird: Transformers for longer sequences.arXiv\npreprint arXiv:2007.14062, 2020.\nOmniNet: Omnidirectional Representations from Transformers\nA. Detailed Experimental Setup\nThis section describes several details of our experiments.\nA.1. Dataset Speciﬁc Setups\nFor all experiments, we implement code using Python and\nJAX. Speciﬁcally, the main Transformer blocks and codebase\nfor most experiments are derived from FLAX examples. For\nWMT’17, we build sentencepiece tokenizers of32Kfrom the\ndataset. WMT’17 collections are obtained from Tensorﬂow\ndatasets (TFDS). For autoregressive language modeling, the\nC4 corpus is similarly found in TFDS. For both LM1B and\nC4 tasks, we use a sentencepiece vocab of32Ksubwords.\nA.2. Efﬁcient Transformer Hyperparameters\nFor Xformers (efﬁcient transformers), we use implemen-\ntations derived from FLAX5.For Linformer, we usek= 32\nfor the low-rank down projection with shared parameters\nfor both key and value. For Performer, we use the default\nsetup from the ofﬁcial implementation. This corresponds\nto the generalized attention with ReLU activations. We do\nnot use any random features. For BigBird, our codebase\nsimilarly links to the ofﬁcial implementation and use the\ndefault hyperparameters. The block size is 64 for BigBird\nand the number of random blocks is3.\nB. Visualisation of Contributions of\nLayers in Omnidirectional Representations\nFigures 5 to 9 (in subsequent pages) show contributions\nof different layers in omnidirectional representations in\nterms of detailed attention maps (attention distribution\nover all layers, in all heads, when the CLS token in the\nomnidirectional layer is considered as the query) as well as\ncontribution of different layers in the pooling operation.\n5https://github.com/google/flax.\nOmniNet: Omnidirectional Representations from Transformers\nPooling Statistics\nAttention Maps\nFigure 5.Contributions of different layers in omnidirectional representations for Example #1.\nOmniNet: Omnidirectional Representations from Transformers\nPooling Statistics\nAttention Maps\nFigure 6.Contributions of different layers in omnidirectional representations for Example #2.\nOmniNet: Omnidirectional Representations from Transformers\nPooling Statistics\nAttention Maps\nFigure 7.Contributions of different layers in omnidirectional representations for Example #3.\nOmniNet: Omnidirectional Representations from Transformers\nPooling Statistics\nAttention Maps\nFigure 8.Contributions of different layers in omnidirectional representations for Example #4.\nOmniNet: Omnidirectional Representations from Transformers\nPooling Statistics\nAttention Maps\nFigure 9.Contributions of different layers in omnidirectional representations for Example #5.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.715207040309906
    },
    {
      "name": "Omnidirectional antenna",
      "score": 0.7133998274803162
    },
    {
      "name": "Security token",
      "score": 0.603245198726654
    },
    {
      "name": "Transformer",
      "score": 0.5473003387451172
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4894320070743561
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.47867995500564575
    },
    {
      "name": "Computer vision",
      "score": 0.3759051561355591
    },
    {
      "name": "Engineering",
      "score": 0.10583868622779846
    },
    {
      "name": "Telecommunications",
      "score": 0.08265817165374756
    },
    {
      "name": "Antenna (radio)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74796645",
      "name": "Birla Institute of Technology and Science, Pilani",
      "country": "IN"
    }
  ]
}