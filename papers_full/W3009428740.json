{
  "title": "EmpTransfo: A Multi-head Transformer Architecture for Creating Empathetic Dialog Systems",
  "url": "https://openalex.org/W3009428740",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287287709",
      "name": "Zandie, Rohola",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226699801",
      "name": "Mahoor, Mohammad H.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W31792455",
    "https://openalex.org/W2810167564",
    "https://openalex.org/W2617750261",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2188365844",
    "https://openalex.org/W2951580200",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2761590056",
    "https://openalex.org/W2737041661",
    "https://openalex.org/W2951612168",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964042872",
    "https://openalex.org/W2953174743",
    "https://openalex.org/W2962796276",
    "https://openalex.org/W2952088495",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2972557612",
    "https://openalex.org/W3003338281",
    "https://openalex.org/W2768195931",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2615146352",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2784400615",
    "https://openalex.org/W2951583236"
  ],
  "abstract": "Understanding emotions and responding accordingly is one of the biggest challenges of dialog systems. This paper presents EmpTransfo, a multi-head Transformer architecture for creating an empathetic dialog system. EmpTransfo utilizes state-of-the-art pre-trained models (e.g., OpenAI-GPT) for language generation, though models with different sizes can be used. We show that utilizing the history of emotions and other metadata can improve the quality of generated conversations by the dialog system. Our experimental results using a challenging language corpus show that the proposed approach outperforms other models in terms of Hit@1 and PPL (Perplexity).",
  "full_text": "EmpTransfo: A Multi-head Transformer Architecture for Creating Empathetic\nDialog Systems\nRohola Zandie and Mohammad H. Mahoor\nDepartment of Electrical and Computer Engineering, University of Denver, USA\nrohola.zandie@du.edu and mmahoor@du.edu\nAbstract\nUnderstanding emotions and responding accordingly is\none of the biggest challenges of dialog systems. This\npaper presents EmpTransfo, a multi-head Transformer\narchitecture for creating an empathetic dialog system.\nEmpTransfo utilizes state-of-the-art pre-trained mod-\nels (e.g., OpenAI-GPT) for language generation, though\nmodels with different sizes can be used. We show that\nutilizing the history of emotions and other metadata can\nimprove the quality of generated conversations by the\ndialog system. Our experimental results using a chal-\nlenging language corpus show that the proposed ap-\nproach outperforms other models in terms of Hit@1 and\nPPL (Perplexity).\nIntroduction\nHumans have the unique capability to communicate with\nnuanced emotions through natural languages. Most of the\nexisting conversational dialog systems focus on language\nunderstanding and improving the generated responses. Al-\nthough these features are essential in building dialog sys-\ntems, they lack empathetic features for conversation, which\nis essential for quality communication. To increase user’ sat-\nisfaction, dialog systems need to understand and incorpo-\nrate emotions to respond with proper emotions. The posi-\ntive effects of using emotional dialog systems also have been\nproved in many areas like customer satisfaction and health-\ncare applications (Dino et al. 2019).\nRecent advances in Natural Language Processing (NLP)\nwith the idea of using pre-trained models have led to remark-\nable results in different NLP tasks. Even though applying the\nsame idea in dialog systems has resulted in improved mod-\nels in terms of language understanding and generation, inte-\ngrating other information like emotions and context knowl-\nedge is still challenging. To build empathetic conversational\nagents, machines need to have the ability to recognize and\npredict emotions based on the history of conversations and\nuse them in interacting with users.\nCorpora used in building most of the traditional dialog\nsystem include general conversations,\nalthough these datasets are usually large scale, they lack\nspeciﬁcity and do not contain metadata such as emotions,\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: An example of the interaction of EmpTransfo with\nthe user. Contextual information like the history of emotions\nand actions and also the topic of conversation are crucial to\nrespond with the appropriate emotion.\ntopics, personality, etc. Training a system based on general\ncorpora leads to conversational agents that do not understand\nemotions, lack any personality and tend to produce generic\nresponses such as: “I don’t know. ”. For these reasons, there\nhas been an effort to create higher quality datasets with\nmore contextual information. For example, the D AILY DI-\nALOG (Li et al. 2017) dataset contains information about\nemotion, topic, and actions.\nWe present a novel multi-head Transformer architecture\nthat can use explicit contextual information on emotions,\ntopic and actions to respond to users’ utterances with proper\nemotions without sacriﬁcing the quality of responses in\nterms of coherence, relevance, and consistency. Figure 1\ndemonstrates how the interaction between the user and di-\nalog system is conditioned on the history of emotions, ac-\ntions, and the topic. All these contextual clues make it eas-\nier for the dialog system to respond with appropriate emo-\ntion. EmpTransfo is built upon the state-of-the-art dialog\nsystem (Wolf et al. 2019b) and introduces a new architec-\ntural design that can exploit contextual information. Quanti-\ntative analysis shows the model outperforms all the baseline\nmodels. Our main contributions in this paper are:\n1. We incorporate emotions with a multi-task learning ap-\nproach in dialog systems that is effective and extendable.\n2. We show that our approach can be augmented with other\narXiv:2003.02958v1  [cs.CL]  5 Mar 2020\ncontextual information that not only improves empathetic\naspects of responses but also its generation quality.\n3. We design the model in a way that can be used with larger\nor smaller pre-trained models without changing the archi-\ntecture of the system. This gives us the ﬂexibility to use\nEmpTransfo in different settings based on our needs.\nRelated Work\nMost of the work on utilizing emotions in conversational\nsystems use large Twitter datasets (Sordoni et al. 2015) that\ncontain emojis as the meta-information for the emotions. In\n(Zhou and Wang 2017), they use the Twitter dataset and\napply a preprocessing method to create a conversational\ndataset with 64 different emojis that represent different emo-\ntions. In the preprocessing step, they used tweets and re-\nsponses that contain at least one emoji and ﬁltered other\nemojis based on the occurring frequency. They used a CV AE\n(Sohn, Lee, and Yan 2015) network to train and generate\nemotional responses. The choice of using emojis to repre-\nsent emotion is noisy because it is too ﬁne-grained and in\nmany cases, the combination of different emojis hardly cor-\nresponds to any speciﬁc emotions.\nIn (Colombo et al. 2019), they used a seq2seq framework\nwith vector representation for emotions, desired emotion, a\nregularizer to penalize neutral words and a sampling method\nthat forces the generation of emotionally relevant words.\nThere are two papers on NLPCC dataset (Mi et al. 2014),\na Chinese language dataset with eight emotion categories.\nThe ﬁrst one is Emotional Chatting Machine (Zhou et al.\n2018) that uses a seq2seq architecture with embedding of\nemotions along with words and an internal and external\nmemory mechanism to generate emotional responses. The\nsecond work is EmoDS (Song et al. 2019) that uses a\nseq2seq approach with a training objective based on an emo-\ntion classiﬁer that promotes implicit emotion generation.\nBoth works use the lexical attention mechanism in decoder\nwith more focus on emotional words to inject explicit emo-\ntions into responses.\nBased on the reports from above-mentioned works, all the\nseq2seq based models tend to generate generic responses\nthat can’t capture all the emotions equally well. Further-\nmore, all previous works use a machine annotating approach\nin the training process that introduces noise in results.\nThe most relevant work to ours is (Rashkin et al. 2019).\nThey introduced a new dataset called EMPATHETICDI-\nALOGUES that contains meta-information about conver-\nsations. The meta-information is a label that shows emo-\ntion and also the situation in which the conversation has\nhappened. They proposed two architectures, one retrieval-\nbased model which looks for the best match using the BERT\nencoder (Devlin et al. 2018) and a generative model using\nTransformer architecture. Using one label for the whole con-\nversation and not each utterance makes it harder for the mod-\nels to ﬁnd proper correlations.\nOn the other hand, recent progress has shown promis-\ning results on pre-trained language models on conversa-\ntional models in chit-chat settings. Recently, (Wolf et al.\n2019b) showed that using a ﬁne-tuned GPT (Generative Pre-\nFigure 2: EmpTransfo: A multi-head Transformer architec-\nture. There are three feedforward linear heads on top of the\nTransformer that map different parts of the last layer hid-\nden state to desired output sizes to create the loss functions\nfor language modeling, next utterance prediction, and next\nemotion prediction. The ﬁnal loss is a weighted sum of all\nthe losses\nTraining), they can beat any other model on the domain of\npersonal chat using the PERSONA CHAT dataset (Zhang et al.\n2018).\nThis paper proposes a new architecture that can incorpo-\nrate not only emotion but other relevant meta information\nin the D AILY DIALOG dataset. We ﬁrst discuss how to use\nmulti-task learning for “next emotion prediction” besides\nlanguage modeling and “next utterance prediction”.\nProposed Approach\nRecent developments have shown substantial improvements\nin benchmarks on a variety of languages understanding tasks\nthrough the use of deep pre-trained language models (De-\nvlin et al. 2018). More speciﬁcally, researchers have shown\nthat by ﬁne-tuning a pre-trained model for speciﬁc tasks,\nthey can achieve better performance compared to training\nthe model from scratch. This is also crucial when the dataset\nat hand is small.\nTransformer-based models become ubiquitous in NLP\nwith the work of (Vaswani et al. 2017) for multiple tasks\nincluding language generation. Causal language models like\nGPT and GPT-2 produce remarkable results in the language\ngeneration task (Wolf et al. 2019b). In this paper, we use\nGPT pre-trained models to achieve better results.\nEmpathetic Dialog Generation\nLet’s assume that in a conversation between two agents,\neach turn of the conversation by one of the agents is\nnamed “utterance”. Hence, a conversation consists of a set\nof utterances. More formally, we have n utterances U =\n{u1,u2,...,u n}and for any utterance iwe have Ni tokens\nFigure 3: The Input representation\nUi = {t1,t2,...,t Ni}. Also, for each utterance,there is an\nemotion corresponding to it, resulting in the sequence of\nemotions E = {e1,e2,...,e n}.\nIn our dataset, a sample is a sequence of utterances\n{u1,u2,...,u T−1,unext}in which unext can be either the\ncorrect next utterance uT or a distractor from the set of dis-\ntractors U′\nT . A distractor is a random utterance from the\ndataset. In the same way, if the corresponding sequence of\nemotions is {e1,e2,...,e T−1,enext}, then enext is either the\ncorrect next emotion eT or a distractor from the set of dis-\ntractors E′\nT . A distractor emotion is a random emotion other\nthan eT from the set of all emotions.\nOur model takes a sequence as input in the embed-\nding space and passes it into a Transformer. The Trans-\nformer architecture (Vaswani et al. 2017) consists of a multi-\nlayer Transformer decoder block. Each Transformer decoder\nblock applies a masked multi-headed self-attention opera-\ntion followed by a feedforward and a normalization layer\nover the input hidden states and gives the same size hidden\nstates in the output. We then feed the output of the Trans-\nformer to three feed-forward linear heads, responsible for\ngenerating the next emotion, utterance, and token. Here, we\nused a 12-layer architecture but it can be extended or re-\nduced to other model sizes. In the following, we deﬁne these\nthree different heads and their corresponding loss functions.\n1-Language modeling head:Language modeling is the\ntask of predicting the next token given a sequence of tokens\nas the context. If we have a sequence of tokens for the correct\nnext utterance as UT = {t1,t2,...,t N }, then the conditional\nprobability of the next token is:\nP(ti|t1,...ti−1) =softmax(h ∗W1) (1)\nIn which, h is the last hidden layer of the transformer\nmodel and W1 is the token embedding matrix that is learned\nin training. Then we can deﬁne the loss function based on\ncross-entropy as:\nL1(UT ) =−\nN∑\ni=1\nlogP(ti|t1,...,t i−1) (2)\nwhere the context of all previous tokens is encoded in a ﬁxed\ndimension vector. It should be noted that the language mod-\neling loss is not trained on the set of next utterance distrac-\ntors U′\nT .\n2-Next utterance prediction head:Following (Devlin et\nal. 2018), in the next utterance prediction, we try to train the\nmodel to predict the next utterance in the conversation. The\nmodel learns to distinguish between the correct next utter-\nance among a set of random distractors from other parts of\nthe dataset. More speciﬁcally, we create a classiﬁer to calcu-\nlate the probabilities of the next utterance:\nPu(a|u1,u2,..uT−1) =softmax(hl ∗W2) (3)\nand ais deﬁned as:\na=\n{1 unext = uT\n0 unext ∈U′\nT\n(4)\nhl is the hidden state for the last token from the Transformer\ndecoder and W2 is the weight matrix that is learned for the\nutterance prediction. Then, the loss function based on cross\nentropy is:\nL2(U1:T ) =−logPu(a|u1,u2,..uT−1) (5)\n3-Next emotion prediction head: Similar to the next ut-\nterance prediction, the model is trained to distinguish be-\ntween the correct next emotion among a set of distractors.\nThe reason to add this head is to make the model learn not\nonly the grammatical and language structure but also the ap-\npropriate emotions for any given history of utterances. We\ncan deﬁne:\nPe(e|e1,e2,...,e T−1) =softmax(hl−1 ∗W3) (6)\nwhere erepresents:\ne=\n{1 enext = eT\n0 enext ∈E′\nT\n(7)\nand hl−1 is the hidden state of one to the last token from the\nTransformer decoder and W3 is the weights to be learned\nduring the training for the emotion prediction task. The loss\nfunction for next emotion prediction is deﬁned with cross-\nentropy:\nL3(U1:T−1) =−logPe(e|e1,e2,..eT−1) (8)\nFinally, we optimize the following objective which is the\ntotal loss function:\nLtotal = c1L1 + c2L2 + c3L3 (9)\nwhere c1, c2, c3 are hyperparameters that are tuned experi-\nmentally. In our experiments, we design the models with and\nwithout the “next emotion prediction” head for comparison.\nTable 1: A conversation in DailyDialog dataset\n# Utterance Emotion Action\n1 You look so happy, any good news? happiness question\n2 Yes, I’ve won the math contest happiness inform\n3 Really? Congratulations! surprise question\n4 Thank you Paul. happiness inform\nd I really want to take him on my knee. anger inform\nInput Representation\nWe use DAILY DIALOG (Li et al. 2017) dataset, which is la-\nbeled with emotion and action tags per utterance, and with\na topic tag for the whole conversation. In Table 2, a sample\nconversation in the preprocessed dataset is shown. A conver-\nsation is a sequence of utterances and each utterance can be\nmore than one sentence, but the emotion and action infor-\nmation are deﬁned per utterance. We also add distractors to\neach sample in the dataset. In the table, the row highlighted\nin red with the number dshows a distractor.\nAll the models use learned positional embeddings with a\nlength of up to 512. Figure 3 demonstrates the input repre-\nsentation. The embeddings that have been used are:\n1. Token embedding:The input sentences are tokenized us-\ning byte pair encoding (BPE) with a vocabulary size of\n40,478.\n2. Emotion embedding: Each one of seven emotions are\nconsidered as a special token to be learned as a new em-\nbedding. Emotion embeddings are copied for each token\nin the utterances and are added to the input of the network.\n3. Action embedding: There are four actions for different\ncommunication functions that are used in dialog. The di-\nalog acts are: Inform, Question, Directives, and Commis-\nsive. Dialog acts are also embedded with special tokens.\n4. Topics: There are 10 topics deﬁned in D AILY DIALOG\nthat are speciﬁed for each conversation. For topics, we\njust concatenate topic embeddings to the beginning of the\nﬁrst input token embedding.\nWe sum all the embeddings and then feed them to the net-\nwork.\nTraining\nWe used the OpenAI pre-trained model on BookCorpus\ndataset (Zhu et al. 2015) which covers more than 7,000\nbooks. The books include narratives and dialogues, and\nemotions in a wide range of interactions between characters.\nThis makes the pre-training suitable for the task of dialogue\nsystem training because it consists of sentences in a logical\norder without shufﬂing.\nStarting with pre-trained weights, we ﬁne-tune our model\non the D AILY DIALOG dataset with the features mentioned\nin the Input Representation section . We use the combina-\ntion of public evaluation and test as the validation set. After\npreprocessing the training set size is 76,502 and the valida-\ntion size is 13,809.\nWe modify the dataset representation to cover different\nwindow positions of conversation history. Each sample in\nthe modiﬁed dataset consists of the topic, last two utterances\nTable 2: Summary of the results on D AILY DIALOG evalua-\ntion set. Model Hit@1 ↑ PPL↓ F1 ↑ BLEU↑\nSeq2Seq+Attention 9.41 129.3 10.22 5.58\nTransformer ranker 17.20 - 26.37 15.79\nOpenAI GPT without emotion 75.01 10.19 18.2 3.755\nEmpTransfo 77.25 10.63 19.39 3.99\nEmpTransfo + topic 76.87 10.23 18.37 4.51\nEmpTransfo + action 77.73 9.17 18.86 3.71\nEmpTransfo + action + topic78.47 9.04 17.27 2.45\nas history context, and the target utterance that can be ei-\nther the real target or the distractor. The input window then\nmoved forward to cover other parts of the conversation.\nWe ﬁne-tuned the model with a batch size of 4 for a se-\nquence length of 310 with 20 epochs over the training set\nof D AILY DIALOG dataset, this is about 1,500,000 steps.\nFor the optimization of the loss function, we used Adam\noptimizer with a learning rate of 6.25e-5, β1 = 0.9 and\nβ2 = 0.999 that decays linearly. The gradient accumulation\nstep is set to 8 with a clipping gradient norm of 1. We set\nthe loss coefﬁcients equal to one ( c1 = c2 = c3 = 1). The\ndropout rates for Transformer were borrowed from OpenAI\nGPT (Radford et al. 2018). All the proposed models are im-\nplemented in Pytorch using Transformers library (Wolf et al.\n2019a) 1.\nHere, we use the nucleus top-p sampling for language de-\ncoding (Holtzman et al. 2019). Given logitsuof the last hid-\nden layer, and a sequence of i−1 tokens, t1:i−1 as context,\nwe have the following distribution over the next tokenti :\nP(ti = Vl|t1:i−1) = exp(ul/T)∑\nl′exp(ul′/T) (10)\nIn which Vl is the lth token in the vocabulary and T is the\ntemperature parameter. Higher values of T result in more\nstochastic choices over tokens, though lower values ofT ap-\nproach greedy and deterministic choices for the next token.\nBased on nucleus top-p sampling, we select V(p) ⊂V as\nthe smallest set such that\n∑\nV (p)⊂V\nP(ti|t1:i−1) ≥p (11)\nAnd then the distribution of Equation 10 should be\nrescaled to form a probability distribution. According to\n(Holtzman et al. 2019), p = 0.9 and T = 0.7 are closer\nto human text generation statistics and we use that for all\nour experiments.\nResults\nWe evaluated our model on its ability to produce coherent\nand relevant utterances on the evaluation set. We also evalu-\nate the proposed model on the task of generating emotional\nresponses given the context on the evaluation set. Evaluation\nof the dialog systems can be done using automatic metrics\nand human evaluations. Here we restrict the evaluations to\nautomatic metrics.\n1https://github.com/roholazandie/EmpTransfo\nTable 3: Examples of model responses.\nInput Prompt\nModel I ﬁnally passed all the exams! I failed the exam You scared me!\nSeq2Seq+Attention I’m sorry, but I’m not sure. I’m going to go to the some time. I’m going to go to the jobTransformer Ranker How big was it ? You’re telling me! There are thousands of people here. Come on! It is really a fun game .OpenAI GPT w/o emotion you look much better than before. let me take your place. what were you doing?\nEmpTransfo+action+topic that’s great! you are really a genius. Maybe you can try harder next time. i’m so sorry. i thought you were not coming.\nEvaluation on coherence and relevance\nBaseline models: For comparison, we used a seq2seq model\nwith attention mechanism and a retrieval-based Transformer\nranker dialog system as the baseline. The retrieval-based\nmodel is similar to (Rashkin et al. 2019), created in ParlAI\nframework (Miller et al. 2017).\nSeq2Seq+Attention model uses linear attention with a\nhidden size of 128, learning rate of 0.01 and trained for\n20 epochs. Transformer ranker uses a hidden size of 300\nthat is trained in 40 epochs with cross-entropy loss function.\n(all other hyperparameters are the defaults from the ParlAI\nrepository).\nEvaluation metrics: We use four different metrics to\nevaluate the models:\n1. Hit@1: this metric is the accuracy of retrieving a gold next\nutterance among 19 random distractor responses sampled\nfrom other dialogues in the dataset.\n2. Perplexity (PPL): perplexity is a measure of how well a\nlanguage model predicts next tokens from the evaluation\ndataset. More speciﬁcally, it is the average per-token log\nprobability over the evaluation set:\nPPL(p) =e\n−1\nN\n∑\nwi Ln(pwi) (12)\n3. BLEU: it is a metric to measure the distance between\nmachine-generated text with human golden labels (Pap-\nineni et al. 2002) .\n4. F1 token: measures the F1 score for token level compari-\nson between the generated text and the golden labels.\nIn Table 2, we observe that all our proposed EmpTransfo\nmodels outperform baseline models in terms of Hit@1 and\nPPL. With more contextual features Hit@1 and PPL are im-\nproved. The proposed model also shows a signiﬁcant im-\nprovement over (Shen et al. 2018) which has a PPL=23.8\nover the same dataset. It also outperforms the Transformer\nretrieval-based model introduced in (Rashkin et al. 2019).\nTransformer ranker model has a greater F1 and BLEU\ncompared to other models which is expected because those\nmetrics give higher scores for stronger similarity with the\ngolden utterances in the datasets. BLEU is initially devel-\noped for machine translation and studies show that it is not\na good metric for text generation evaluation (Sulem, Abend,\nand Rappoport 2018; Novikova et al. 2017). Table 2 also\nshows that adding more contextual information like topic\nand action results in higher Hit@1 and lower PPL. The rea-\nson behind this observation is that more contextual informa-\ntion provides the model with better information on selecting\nthe correct next sentence and correct next token.\nFigure 4: The confusion matrix of emotion prediction for\nDAILY DIALOG with 6 emotions using EmpTransfo and all\nthe features (best in color).\nTable 3 shows some responses with different given in-\nput prompts. The inputs are selected in a way to expect the\nmodel to respond with emotions. All the outputs are the ﬁrst\nresults obtained from the models.\nEvaluation on emotion prediction\nIn order to evaluate the next utterance emotion prediction,\nwe calculate the precision and recall from the confusion\nmatrix over the evaluation dataset. Figure 4 demonstrates\nthe calculated confusion matrix with Precision=81.35, Re-\ncall=72.37 and F1=76.59. The proposed model achieves\nmore than 3 percent improvement compared to (Chan and\nLui 2018) whom report their best emotion prediction results\nwith precision=70.81, recall=76.16 and F1=73.39. The con-\nfusion matrix also shows interesting observations such as\nthe high rate of confusion between disgust and anger which\nhas been observed in computer vision and facial expression\nrecognition (Hasani and Mahoor 2017).\nDiscussion\nThis paper introduced EmpTransfo a multi-head Trans-\nformer model which is an empathetic aware dialog system\nto interact with users with higher quality in terms of coher-\nence, relevance, and emotion. Our proposed method is built\nupon the state-of-the-art language model of OpenAI-GPT\nfor language generation. One of the limitations in the pro-\nposed approach is requiring meta information of emotion,\naction, and topic in order to respond with the proper emo-\ntions.\nConclusion and Future Work\nEmpTransfo is a scalable and fully data-driven neural con-\nversational model that effectively exploits the information\nabout emotion, action, and topic. It naturally combines con-\nversational and non-conversational data through multi-task\nlearning. This shows that multi-task learning for training\nconversational models is not only possible but necessary and\ncan be extended to include more contextual data. As a future\ndirection, we will use knowledge-base and other contextual\ninformation to develop knowledge-aware dialog systems.\nReferences\n[Chan and Lui 2018] Chan, Y . H., and Lui, A. K. F. 2018.\nEncoding emotional information for sequence-to-sequence\nresponse generation. In 2018 International Conference on\nArtiﬁcial Intelligence and Big Data (ICAIBD) , 113–116.\nIEEE.\n[Colombo et al. 2019] Colombo, P.; Witon, W.; Modi, A.;\nKennedy, J.; and Kapadia, M. 2019. Affect-driven dialog\ngeneration. arXiv preprint arXiv:1904.02793.\n[Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.; and\nToutanova, K. 2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805.\n[Dino et al. 2019] Dino, F.; Zandie, R.; Abdollahi, H.;\nSchoeder, S.; and Mahoor, M. H. 2019. Delivering cogni-\ntive behavioral therapy using a conversational social robot.\nIn 2019 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), 2089–2095.\n[Hasani and Mahoor 2017] Hasani, B., and Mahoor, M. H.\n2017. Facial expression recognition using enhanced deep\n3d convolutional neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition Workshops, 30–40.\n[Holtzman et al. 2019] Holtzman, A.; Buys, J.; Forbes, M.;\nand Choi, Y . 2019. The curious case of neural text degener-\nation. arXiv preprint arXiv:1904.09751.\n[Li et al. 2017] Li, Y .; Su, H.; Shen, X.; Li, W.; Cao, Z.; and\nNiu, S. 2017. Dailydialog: A manually labelled multi-turn\ndialogue dataset. arXiv preprint arXiv:1710.03957.\n[Mi et al. 2014] Mi, C.; Yang, Y .; Wang, L.; Li, X.; and\nDalielihan, K. 2014. Detection of loan words in uyghur\ntexts. In Zong, C.; Nie, J.-Y .; Zhao, D.; and Feng, Y .,\neds., Natural Language Processing and Chinese Computing,\n103–112. Berlin, Heidelberg: Springer Berlin Heidelberg.\n[Miller et al. 2017] Miller, A. H.; Feng, W.; Fisch, A.; Lu,\nJ.; Batra, D.; Bordes, A.; Parikh, D.; and Weston, J. 2017.\nParlai: A dialog research software platform. arXiv preprint\narXiv:1705.06476.\n[Novikova et al. 2017] Novikova, J.; Duˇsek, O.; Curry, A. C.;\nand Rieser, V . 2017. Why we need new evaluation metrics\nfor nlg. arXiv preprint arXiv:1707.06875.\n[Papineni et al. 2002] Papineni, K.; Roukos, S.; Ward, T.;\nand Zhu, W.-J. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In Proceedings of the 40th an-\nnual meeting on association for computational linguistics ,\n311–318. Association for Computational Linguistics.\n[Radford et al. 2018] Radford, A.; Narasimhan, K.; Sali-\nmans, T.; and Sutskever, I. 2018. Improving language un-\nderstanding by generative pre-training.\n[Rashkin et al. 2019] Rashkin, H.; Smith, E. M.; Li, M.; and\nBoureau, Y .-L. 2019. Towards empathetic open-domain\nconversation models: A new benchmark and dataset. InPro-\nceedings of the 57th Conference of the Association for Com-\nputational Linguistics, 5370–5381.\n[Shen et al. 2018] Shen, X.; Su, H.; Niu, S.; and Demberg, V .\n2018. Improving variational encoder-decoders in dialogue\ngeneration. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence.\n[Sohn, Lee, and Yan 2015] Sohn, K.; Lee, H.; and Yan, X.\n2015. Learning structured output representation using deep\nconditional generative models. In Advances in neural infor-\nmation processing systems, 3483–3491.\n[Song et al. 2019] Song, Z.; Zheng, X.; Liu, L.; Xu, M.; and\nHuang, X.-J. 2019. Generating responses with a speciﬁc\nemotion in dialog. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 3685–\n3695.\n[Sordoni et al. 2015] Sordoni, A.; Galley, M.; Auli, M.;\nBrockett, C.; Ji, Y .; Mitchell, M.; Nie, J.-Y .; Gao, J.; and\nDolan, B. 2015. A neural network approach to context-\nsensitive generation of conversational responses. arXiv\npreprint arXiv:1506.06714.\n[Sulem, Abend, and Rappoport 2018] Sulem, E.; Abend, O.;\nand Rappoport, A. 2018. Bleu is not suitable for the evalua-\ntion of text simpliﬁcation. arXiv preprint arXiv:1810.05995.\n[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.;\nUszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polo-\nsukhin, I. 2017. Attention is all you need. In Advances in\nneural information processing systems, 5998–6008.\n[Wolf et al. 2019a] Wolf, T.; Debut, L.; Sanh, V .; Chaumond,\nJ.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.;\nFuntowicz, M.; and Brew, J. 2019a. Huggingface’s trans-\nformers: State-of-the-art natural language processing. ArXiv\nabs/1910.03771.\n[Wolf et al. 2019b] Wolf, T.; Sanh, V .; Chaumond, J.; and\nDelangue, C. 2019b. Transfertransfo: A transfer learning\napproach for neural network based conversational agents.\narXiv preprint arXiv:1901.08149.\n[Zhang et al. 2018] Zhang, S.; Dinan, E.; Urbanek, J.; Szlam,\nA.; Kiela, D.; and Weston, J. 2018. Personalizing dialogue\nagents: I have a dog, do you have pets too? arXiv preprint\narXiv:1801.07243.\n[Zhou and Wang 2017] Zhou, X., and Wang, W. Y . 2017.\nMojitalk: Generating emotional responses at scale. arXiv\npreprint arXiv:1711.04090.\n[Zhou et al. 2018] Zhou, H.; Huang, M.; Zhang, T.; Zhu, X.;\nand Liu, B. 2018. Emotional chatting machine: Emotional\nconversation generation with internal and external memory.\nIn Thirty-Second AAAI Conference on Artiﬁcial Intelligence.\n[Zhu et al. 2015] Zhu, Y .; Kiros, R.; Zemel, R.; Salakhutdi-\nnov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015.\nAligning books and movies: Towards story-like visual ex-\nplanations by watching movies and reading books. In Pro-\nceedings of the IEEE international conference on computer\nvision, 19–27.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.949030876159668
    },
    {
      "name": "Dialog box",
      "score": 0.9097867012023926
    },
    {
      "name": "Dialog system",
      "score": 0.7466560006141663
    },
    {
      "name": "Computer science",
      "score": 0.7409261465072632
    },
    {
      "name": "Transformer",
      "score": 0.7067093849182129
    },
    {
      "name": "Architecture",
      "score": 0.6806321740150452
    },
    {
      "name": "Natural language processing",
      "score": 0.5229297876358032
    },
    {
      "name": "Metadata",
      "score": 0.5126694440841675
    },
    {
      "name": "Artificial intelligence",
      "score": 0.467037171125412
    },
    {
      "name": "Language model",
      "score": 0.43956974148750305
    },
    {
      "name": "Human–computer interaction",
      "score": 0.39763274788856506
    },
    {
      "name": "World Wide Web",
      "score": 0.19965112209320068
    },
    {
      "name": "Engineering",
      "score": 0.12745746970176697
    },
    {
      "name": "Art",
      "score": 0.07183787226676941
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}