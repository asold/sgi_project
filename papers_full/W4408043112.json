{
  "title": "Assessing political bias in large language models",
  "url": "https://openalex.org/W4408043112",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3189574132",
      "name": "Luca Rettenberger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2288662413",
      "name": "Markus Reischl",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2566299069",
      "name": "Mark Schutera",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4383737134",
    "https://openalex.org/W4389043118",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W6779239075",
    "https://openalex.org/W6810861411",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4238824772",
    "https://openalex.org/W4386322180",
    "https://openalex.org/W6782268707",
    "https://openalex.org/W1158323862",
    "https://openalex.org/W4280650946",
    "https://openalex.org/W2041282815",
    "https://openalex.org/W2479261768"
  ],
  "abstract": "Abstract Evaluating bias in Large Language Models (LLMs) has become a pivotal issue in current Artificial Intelligence (AI) research due to their significant impact on societal dynamics. Recognizing political bias in LLMs is particularly important as they approach performative prediction, influencing societal behavior and political events, such as the upcoming European Parliament elections. From a German voterâ€™s perspective, we evaluate the political bias of the currently most popular open-source LLMs concerning political issues within the European Union. To do so, we use the \"Wahl-O-Mat,\" a voting advice application used in Germany. We show that larger models, such as Llama3-70B, tend to align more closely with left-leaning political parties, while smaller models often remain neutral, particularly when prompted in English. The central finding is that LLMs are similarly biased, with low variances in the alignment concerning a specific party. Our findings offer crucial insights for developers and policymakers to understand and mitigate LLM biases, emphasizing the need for rigorous bias assessment to ensure the integrity and trustworthiness of AI applications.",
  "full_text": null,
  "topic": "Politics",
  "concepts": [
    {
      "name": "Politics",
      "score": 0.6091829538345337
    },
    {
      "name": "Computer science",
      "score": 0.38440021872520447
    },
    {
      "name": "Political science",
      "score": 0.38429367542266846
    },
    {
      "name": "Psychology",
      "score": 0.32395943999290466
    },
    {
      "name": "Linguistics",
      "score": 0.3213950991630554
    },
    {
      "name": "Philosophy",
      "score": 0.1128191351890564
    },
    {
      "name": "Law",
      "score": 0.07284438610076904
    }
  ],
  "institutions": []
}