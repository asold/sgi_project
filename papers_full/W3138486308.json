{
  "title": "Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking",
  "url": "https://openalex.org/W3138486308",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1980283440",
      "name": "Wang, Ning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231152892",
      "name": "Zhou, Wengang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1762459914",
      "name": "Wang Jie",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Li, Houqaing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2776035257",
    "https://openalex.org/W2557641257",
    "https://openalex.org/W2897666265",
    "https://openalex.org/W3035453691",
    "https://openalex.org/W2950006892",
    "https://openalex.org/W2608627404",
    "https://openalex.org/W2966759264",
    "https://openalex.org/W2950413415",
    "https://openalex.org/W3108637696",
    "https://openalex.org/W2681067697",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2998027361",
    "https://openalex.org/W2949106550",
    "https://openalex.org/W3088021439",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2518013266",
    "https://openalex.org/W2964253307",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W2913466142",
    "https://openalex.org/W2792162533",
    "https://openalex.org/W2963074722",
    "https://openalex.org/W2158592639",
    "https://openalex.org/W2955747520",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3034297219",
    "https://openalex.org/W2963227409",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W2798842862",
    "https://openalex.org/W1997121481",
    "https://openalex.org/W2954137266",
    "https://openalex.org/W2991213871",
    "https://openalex.org/W2949764466",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W3035211844",
    "https://openalex.org/W2963471260",
    "https://openalex.org/W2928353271",
    "https://openalex.org/W2907243782",
    "https://openalex.org/W3108235634",
    "https://openalex.org/W2963791342",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2998434318",
    "https://openalex.org/W2797812763",
    "https://openalex.org/W3035672751",
    "https://openalex.org/W2518876086",
    "https://openalex.org/W2952064669",
    "https://openalex.org/W2558899534",
    "https://openalex.org/W1857884451",
    "https://openalex.org/W2898200825",
    "https://openalex.org/W2599547527",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2997599718",
    "https://openalex.org/W3001584168",
    "https://openalex.org/W2952558221",
    "https://openalex.org/W2891033863",
    "https://openalex.org/W2965709200",
    "https://openalex.org/W2949357526",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2214352687",
    "https://openalex.org/W1821462560"
  ],
  "abstract": "In video object tracking, there exist rich temporal contexts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The transformer encoder promotes the target templates via attention-based feature reinforcement, which benefits the high-quality tracking model generation. The transformer decoder propagates the tracking cues from previous templates to the current frame, which facilitates the object searching process. Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed transformer, a simple Siamese matching approach is able to outperform the current top-performing trackers. By combining our transformer with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.",
  "full_text": "Transformer Meets Tracker:\nExploiting Temporal Context for Robust Visual Tracking\nNing Wang1 Wengang Zhou1,2 Jie Wang1,2 Houqiang Li1,2\n1CAS Key Laboratory of GIPAS, EEIS Department, University of Science and Technology of China (USTC)\n2Institute of Artiﬁcial Intelligence, Hefei Comprehensive National Science Center\nwn6149@mail.ustc.edu.cn, {zhwg,jiewangx,lihq}@ustc.edu.cn\nAbstract\nIn video object tracking, there exist rich temporal con-\ntexts among successive frames, which have been largely\noverlooked in existing trackers. In this work, we bridge the\nindividual video frames and explore the temporal contexts\nacross them via a transformer architecture for robust object\ntracking. Different from classic usage of the transformer in\nnatural language processing tasks, we separate its encoder\nand decoder into two parallel branches and carefully design\nthem within the Siamese-like tracking pipelines. The trans-\nformer encoder promotes the target templates via attention-\nbased feature reinforcement, which beneﬁts the high-quality\ntracking model generation. The transformer decoder prop-\nagates the tracking cues from previous templates to the cur-\nrent frame, which facilitates the object searching process.\nOur transformer-assisted tracking framework is neat and\ntrained in an end-to-end manner. With the proposed trans-\nformer, a simple Siamese matching approach is able to out-\nperform the current top-performing trackers. By combin-\ning our transformer with the recent discriminative track-\ning pipeline, our method sets several new state-of-the-art\nrecords on prevalent tracking benchmarks.\n1. Introduction\nVisual object tracking is a basic task in computer vision.\nDespite the recent progress, it remains a challenging task\ndue to factors such as occlusion, deformation, and appear-\nance changes. With the temporal error accumulation, these\nchallenges are further ampliﬁed in the online process.\nIt is well recognized that the rich temporal information\nin the video ﬂow is of vital importance for visual track-\ning. However, most tracking paradigms [29, 28, 49] handle\nthis task by per-frame object detection, where the tempo-\n*Corresponding Author: Wengang Zhou and Houqiang Li.\n†Source code, pretrained model, and raw tracking results are available\nat https://github.com/594422814/TransformerTrack.\nResNet50 CNN\nTransformer\nDecoder Conv\nCNN\nCNN\nTemplate Set\nSearch Patch\nTransformer\nResponse\nBackbone\nTracking\nModel\nWeight Sharing\nTransformer\nEncoder\nFigure 1. An overview of our transformer-assisted tracking frame-\nwork. The transformer encoder and decoder are assigned to two\nparallel branches in a Siamese-like tracking pipeline. Thanks to\nthe encoder-decoder structure, isolated frames are tightly bridged\nto convey rich temporal information in the video ﬂow.\nral relationships among successive frames have been largely\noverlooked. Take the popular Siamese tracker as an exam-\nple, only the initial target is considered for template match-\ning [1, 45, 19, 29]. The merely used temporal information\nis the motion prior ( e.g., cosine window) by assuming the\ntarget moves smoothly, which is widely adopted in visual\ntrackers. In other tracking frameworks with update mech-\nanisms [20, 40, 8, 60, 62, 3], previous prediction results\nare collected to incrementally update the tracking model.\nDespite the historical frames considered in the above ap-\nproaches, the video frames are still considered as indepen-\ndent counterparts without mutual reasoning. In real-world\nvideos, some frames inevitably contain noisy contents such\nas occluded or blurred objects. These imperfect frames will\nhurt the model update when serving as the templates and\nwill challenge the tracking process when performing as the\nsearch frames. Therefore, it is a non-trivial issue to convey\nrich information across temporal frames to mutually rein-\nforce them. We argue that the video frames should not be\ntreated in isolation and the performance potential is largely\nrestricted due to the overlook of frame-wise relationship.\nTo bridge the isolated video frames and convey the rich\ntemporal cues across them, in this work, we introduce the\ntransformer architecture [47] to the visual tracking commu-\nnity. Different from the traditional usage of the transformer\n1\narXiv:2103.11681v2  [cs.CV]  24 Mar 2021\nTemplate Features\nTransformer\nEncoder\nTransformer\nDecoder\nTemplate Patches\nSearch Patch\nMask Propagation Feature Propagation\nEncoded Features     Masks \nTransformed Feature and Mask\nSearch Feature\nTemplate BranchSearch Branch\nFeature and Mask\nPropagation\nDecoded \nFeature  \nFigure 2. Top: the transformer encoder receives multiple template\nfeatures to mutually aggregate representations. Bottom: the trans-\nformer decoder propagates the template features and their assigned\nmasks to the search patch feature for representation enhancement.\nin language modeling and machine translation [47, 12], we\nleverage it to handle the context propagation in the tempo-\nral domain. By carefully modifying the classic transformer\narchitecture, we show that its transformation characteristic\nnaturally ﬁts the tracking scenario. Its core component, i.e.,\nattention mechanism [47, 57], is ready to establish the pixel-\nwise correspondence across frames and freely convey vari-\nous signals in the temporal domain.\nGenerally, most tracking methods [1, 46, 29, 43, 7, 3] can\nbe formulated into a Siamese-like framework, where the top\nbranch learns a tracking model using template features, and\nthe bottom branch classiﬁes the current search patch. As\nshown in Figure 1, we separate the transformer encoder and\ndecoder into two branches within such a general Siamese-\nlike structure. In the top branch, a set of template patches\nare fed to the transformer encoder to generate high-quality\nencoded features. In the bottom branch, the search feature\nas well as the previous template contents are fed to the trans-\nformer decoder, where the search patch retrieves and aggre-\ngates informative target cues (e.g., spatial masks and target\nfeatures) from history templates to reinforce itself.\nThe proposed transformer facilitates visual tracking via:\n• Transformer Encoder. It enables individual template\nfeatures to mutually reinforce to acquire more compact\ntarget representations, as shown in Figure 2. These en-\ncoded high-quality features further beneﬁt the tracking\nmodel generation.\n• Transformer Decoder. It conveys valuable temporal\ninformation across frames. As shown in Figure 2, our\ndecoder simultaneously transfers features and spatial\nmasks. Propagating the features from previous frames\nto the current patch smooths the appearance changes\nand remedies the context noises while transforming the\nspatial attentions highlights the potential object loca-\ntion. These manifold target representations and spatial\ncues make the object search much easier.\nFinally, we track the target in the decoded search patch. To\nverify the generalization of our designed transformer, we\nintegrate it into two popular tracking frameworks includ-\ning a Siamese formulation [1] and a discriminative corre-\nlation ﬁlter (DCF) based tracking paradigm [3]. With our\ndesigned transformer, a simple Siamese matching pipeline\nis able to outperform the current top-performing trackers.\nBy combining with the recent discriminative approach [3],\nour transformer-assisted tracker shows outstanding results\non seven prevalent tracking benchmarks including LaSOT\n[13], TrackingNet [39], GOT-10k [23], UA V123 [37], NfS\n[24], OTB-2015 [58], and VOT2018 [26] and sets several\nnew state-of-the-art records.\nIn summary, we make three-fold contributions:\n• We present a neat and novel transformer-assisted track-\ning framework. To our best knowledge, this is the ﬁrst\nattempt to involve the transformer in visual tracking.\n• We simultaneously consider the feature and attention\ntransformations to better explore the potential of the\ntransformer. We also modify the classic transformer to\nmake it better suit the tracking task.\n• To verify the generalization, we integrate our designed\ntransformer into two popular tracking pipelines. Our\ntrackers exhibit encouraging results on 7 benchmarks.\n2. Related Work\nVisual Tracking. Given the initial target in the ﬁrst frame,\nvisual tracking aims to localize it in successive frames.\nIn recent years, the Siamese network has gained signiﬁ-\ncant popularity, which deals with the tracking task by tem-\nplate matching [1, 45, 19]. By introducing the region pro-\nposal network (RPN), Siamese trackers obtain superior ef-\nﬁciency and more accurate target scale estimation [29, 65].\nThe recent improvements upon Siamese trackers include at-\ntention mechanism [55], reinforcement learning [22, 52],\ntarget-aware model ﬁne-tuning [31], unsupervised training\n[51, 53], sophisticated backbone networks [28, 63], cas-\ncaded frameworks [14, 50], and model update mechanisms\n[16, 17, 60, 62].\nDiscriminative correlation ﬁlter (DCF) tackles the visual\ntracking by solving the ridge regression in Fourier domain,\nwhich exhibits attractive efﬁciency [20, 36, 35, 15, 38, 54,\n11, 8]. The recent advances show that the ridge regression\ncan be solved in the deep learning frameworks [43, 33, 7, 3],\nwhich avoids the boundary effect in classic DCF trackers.\nThese methods learn a discriminative CNN kernel to con-\nvolve with the search area for response generation. In re-\ncent works, the residual terms [43] and shrinkage loss [33]\nare incorporated into the deep DCF formulation. To accel-\nerate the kernel learning process, ATOM [7] exploits the\nconjugate gradient algorithm. The recent DiMP tracker [3]\nenhances the discriminative capability of the learned CNN\nkernel in an end-to-end manner, which is further promoted\nby the probabilistic regression framework [9].\nDespite the impressive performance, most existing meth-\n2\nods [40, 1, 29, 7, 3, 34, 49] generally regard the tracking task\nas the per-frame object detection problem, failing to ade-\nquately exploit the temporal characteristic of the tracking\ntask. Some previous works explore the temporal informa-\ntion using graph neural network [16], spatial-temporal regu-\nlarization [30], optical ﬂow [66], etc. Differently, we lever-\nage the transformer to model the frame-wise relationship\nand propagate the temporal cues, which is neat and ready to\nintegrate with the modern deep trackers.\nTransformer. Transformer is ﬁrst proposed in [47] as a\nnew paradigm for machine translation. The basic block in a\ntransformer is the attention module, which aggregates infor-\nmation from the entire input sequence. Due to the parallel\ncomputations and unique memory mechanism, transformer\narchitecture is more competitive than RNNs in process-\ning long sequences and has gained increasing popularity in\nmany natural language processing (NLP) tasks [12, 42, 44].\nSimilarly, non-local neural network [57] also introduces a\nself-attention block to acquire global representations, which\nhas been adopted in many vision tasks including visual ob-\nject tracking [61]. Nevertheless, how to take advantage of\nthe compact transformer encoder-decoder structure for vi-\nsual tracking has been rarely studied.\nRecently, transformer architecture has been introduced\nto computer vision such as image generation [41]. Trans-\nformer based object detection approach is proposed in [5],\nwhich views the object detection task as a direct set pre-\ndiction problem. However, the above techniques leverage\nthe transformer in the image-level tasks. In this paper, we\nshow that the transformer structure serves as a good ﬁt for\nvideo-related scenarios by transferring temporal informa-\ntion across frames. To bridge the domain gap between vi-\nsual tracking and NLP tasks, we carefully modify the classic\ntransformer to better suit the tracking scenario.\n3. Revisting Tracking Frameworks\nBefore elaborating our transformer for object tracking,\nwe brieﬂy review the recent popular tracking approaches\nfor the sake of completeness. As shown in Figure 3, the\nmainstream tracking methods such as Siamese network [1]\nor discriminative correlation ﬁlter (DCF) [46, 7, 3] can be\nformulated into the Siamese-like pipeline, where the top\nbranch learns the tracking model using templates and the\nbottom branch focuses on the target localization.\nSiamese matching architecture [1] takes an exemplar\npatch z and a search patch x as inputs, where z represents\nthe target object while x is a large searching area in sub-\nsequent video frames. Both of them are fed to the weight-\nsharing CNN network Ψ(·). Their output feature maps are\ncross-correlated as follows to generate the response map:\nr(z,x) = Ψ(z) ∗Ψ(x) + b·1 , (1)\nwhere ∗is the cross-correlation and b ·1 denotes a bias\nCNN\nTemplate Patch\nSearch Patch\nWeight Sharing\nCNN Crop DCF\nTracking \nModel\nTracking \nModel\nConvConv\nResult Result\nFeature Extraction Siamese Pipeline DCF Pipeline\nDifferent Tracking Options\nFigure 3. The simpliﬁed pipelines of Siamese [1] and DCF [7, 3]\nbased trackers. These tracking approaches can be formulated into\na Siamese-like pipeline, where the top branch is responsible for\nthe model generation and the bottom branch localizes the target.\nterm. Siamese trackers rely on the target model, i.e., convo-\nlutional kernel Ψ(z), for template matching.\nAs another popular framework, deep learning based DCF\nmethod optimizes the tracking model f under a ridge re-\ngression formulation [43, 7, 3] as follows:\nmin\nf\n∥f ∗Ψ(z⋆) −y∥2\n2 + λ∥f∥2\n2, (2)\nwhere y is the Gaussian-shaped ground-truth label of tem-\nplate patch z⋆, and λ controls the regularization term to\navoid overﬁtting. Note thatz⋆is much larger than the exem-\nplar patch z in Siamese trackers. Therefore, DCF formula-\ntion simultaneously considers the target matching and back-\nground discrimination. After obtaining the tracking model\nf, the response is generated via r = f ∗Ψ(x).\nThe traditional DCF methods [20, 10] solve ridge regres-\nsion using circularly generated samples via the closed-form\nsolution in the Fourier domain. In contrast, the recent deep\nlearning based DCF methods solve Eq. 2 using stochastic\ngradient descent [43, 33] or conjugate gradient approach\n[7] to avoid the boundary effect. The recent DiMP [3] op-\ntimizes the above ridge regression via a meta-learner in an\nend-to-end manner, showing state-of-the-art performance.\n4. Transformer for Visual Tracking\nAs discussed in Section 3, mainstream tracking methods\ncan be formulated into a Siamese-like pipeline. We aim to\nimprove such a general tracking framework by frame-wise\nrelationship modeling and temporal context propagation,\nwithout modifying their original tracking manners such as\ntemplate matching.\n4.1. Transformer Overview\nAn overview of our transformer is shown in Figure 4.\nSimilar to the classic transformer architecture [47], the en-\ncoder leverages self-attention block to mutually reinforce\nmultiple template features. In the decoding process, cross-\nattention block bridges template and search branches to\npropagate temporal contexts (e.g., feature and attention).\n3\nSelf-Attention\nAdd & Ins. Norm\nAdd & Ins. Norm\nAdd & Ins. Norm\nCross-Attention\nMul & Ins. Norm\nCross-Attention\nV K Q\nV V\nV K\nK K\nQ\nQ Q\nM\nM\nAdd & Ins. Norm\nSelf-Attention\nEncoded\nTemplate Feature\nDecoded Search Feature\nFeature Transformation Mask Transformation\nEncoder\nDecoder\nTemplate Feature Search Feature\nWeight Sharing\nElement-wise Production M Template Feature Mask\nFigure 4. An overview of the proposed transformer architecture.\nTo suit the visual tracking task, we modify the classic\ntransformer in the following aspects: (1) Encoder-decoder\nSeparation. Instead of cascading the encoder and decoder\nin NLP tasks [47, 12], as shown in Figure 1, we separate the\nencoder and decoder into two branches to ﬁt the Siamese-\nlike tracking methods. (2) Block Weight-sharing. The self-\nattention blocks in the encoder and decoder (yellow boxes\nin Figure 4) share weights, which transform the template\nand search embeddings in the same feature space to facil-\nitate the further cross-attention computation. (3) Instance\nNormalization. In NLP tasks [47], the word embeddings\nare individually normalized using the layer normalization.\nSince our transformer receives image feature embeddings,\nwe jointly normalize these embeddings in the instance (im-\nage patch) level to retain the valuable image amplitude in-\nformation. (4) Slimming Design. Efﬁciency is crucial for\nvisual tracking scenarios. To achieve a good balance of\nspeed and performance, we slim the classic transformer by\nomitting the fully-connected feed-forward layers and main-\ntaining the lightweight single-head attention.\n4.2. Transformer Encoder\nThe basic block in a classic transformer is the attention\nmechanism, which receives the query Q ∈ RNq×C, key\nK ∈RNk×C, and value V ∈RNk×C as the inputs. In our\napproach, following [47], we also adopt the dot-product to\ncompute the similarity matrix AK→Q ∈RNq×Nk between\nthe query and key as follows:\nAK→Q = Atten(Q,K) = Softmaxcol( ¯Q ¯KT/τ), (3)\nwhere ¯Q and ¯K are ℓ2-normalized features of Q and K\nacross the channel dimension, andτis a temperature param-\neter controlling the Softmax distribution, which is inspired\nby the model distillation [21] and contrastive learning [6]\ntechniques. With the propagation matrixAK→Q from key to\nquery, we can transform the value via AK→QV ∈RNq×C.\nIn our framework, the transformer encoder receives a\nset of template features Ti ∈ RC×H×W with a spatial\nsize of H ×W and dimensionality C, which are further\nconcatenated to form the template feature ensemble T =\nConcat(T1,··· ,Tn) ∈Rn×C×H×W. To facilitate the at-\ntention computation, we reshape T to T\n′\n∈RNT ×C, where\nNT = n×H ×W. As shown in Figure 4, the main op-\neration in the transformer encoder is self-attention, which\naims to mutually reinforce the features from multiple tem-\nplates. To this end, we ﬁrst compute the self-attention map\nAT→T = Atten\n(\nϕ(T\n′\n),ϕ(T\n′\n)\n)\n∈RNT ×NT , where ϕ(·)\nis a 1 ×1 linear transformation that reduces the embedding\nchannel from Cto C/4.\nBased on the self-similarity matrix AT→T, we transform\nthe template feature through AT→TT\n′\n, which is added to\nthe original feature T\n′\nas a residual term as follows:\nˆT = Ins. Norm\n(\nAT→TT\n′\n+ T\n′)\n, (4)\nwhere ˆT ∈ RNT ×C is the encoded template feature and\nIns. Norm(·) denotes the instance normalization that jointly\nℓ2-normalizes all the embeddings from an image patch,i.e.,\nfeature map level (Ti ∈RC×H×W) normalization.\nThanks to the self-attention, multiple temporally diverse\ntemplate features aggregate each other to generate high-\nquality ˆT, which is further fed to the decoder block to rein-\nforce the search patch feature. Besides, this encoded tem-\nplate representation ˆT is also reshaped back to Tencoded ∈\nRn×C×H×W for tracking model generation, e.g., the DCF\nmodel in Section 4.4.\n4.3. Transformer Decoder\nTransformer decoder takes the search patch feature S ∈\nRC×H×W as its input. Similar to the encoder, we ﬁrst re-\nshape this feature to S\n′\n∈RNS×C, where NS = H ×W.\nThen, S\n′\nis fed to the self-attention block as follows:\nˆS = Ins. Norm\n(\nAS→SS\n′\n+ S\n′)\n, (5)\nwhere AS→S = Atten\n(\nϕ(S\n′\n),ϕ(S\n′\n)\n)\n∈RNS×NS is the\nself-attention matrix of the search feature.\nMask Transformation. Based on the search feature ˆS\nin Eq. 5 and aforementioned encoded template feature ˆT\nin Eq. 4, we compute the cross-attention matrix between\nthem via AT→S = Atten\n(\nφ(ˆS),φ( ˆT)\n)\n∈RNS×NT , where\n4\nφ(·) is a 1 ×1 linear transformation block similar to ϕ(·).\nThis cross-attention map AT→S establishes the pixel-to-\npixel correspondence between frames, which supports the\ntemporal context propagation.\nIn visual tracking, we are aware of the target positions\nin the templates. To propagate the temporal motion pri-\nors, we construct the Gaussian-shaped masks of the tem-\nplate features through m(y) = exp\n(\n−∥y−c∥2\n2σ2\n)\n, where cis\nthe ground-truth target position. Similar to the feature en-\nsemble T, we also concatenate these masks mi ∈RH×W\nto form the mask ensemble M = Concat(m1,··· ,mn) ∈\nRn×H×W, which is further ﬂattened into M\n′\n∈RNT ×1.\nBased on the cross attention mapAT→S, we can easily prop-\nagate previous masks to the search patch via AT→SM\n′\n∈\nRNS×1. The transformed mask is qualiﬁed to serve as the\nattention weight for the search feature ˆS as follows:\nˆSmask = Ins. Norm\n(\nAT→SM\n′\n⊗ˆS\n)\n, (6)\nwhere ⊗is the broadcasting element-wise multiplication.\nBy virtue of the spatial attention, the reinforced search fea-\nture ˆSmask better highlights the potential target area.\nFeature Transformation. Except for the spatial attention,\nit is also feasible to propagate the context information from\ntemplate feature ˆT to the search feature ˆS. It is beneﬁcial to\nconvey target representations while the background scenes\ntend to change drastically in a video, which is unreasonable\nto temporally propagate. As a consequence, before feature\ntransformation, we ﬁrst mask the template feature through\nˆT ⊗M\n′\nto suppress the background area. Then, with the\ncross-attention matrix AT→S, the transformed feature can\nbe computed viaAT→S( ˆT⊗M\n′\n) ∈RNS×C, which is added\nto ˆS as a residual term:\nˆSfeat = Ins. Norm\n(\nAT→S( ˆT ⊗M\n′\n) + ˆS\n)\n. (7)\nCompared with original ˆS, feature-level enhanced ˆSfeat ag-\ngregates temporally diverse target representations from a se-\nries of template features ˆT to promote itself.\nFinally, we equally combine the aforementioned spa-\ntially masked feature ˆSmask and feature-level enhanced fea-\nture ˆSfeat, and further normalize them as follows:\nˆSﬁnal = Ins. Norm\n(\nˆSfeat + ˆSmask\n)\n. (8)\nThe ﬁnal output feature ˆSﬁnal ∈RNS×C is reshaped back to\nthe original size for visual tracking. We denote the reshaped\nversion of ˆSﬁnal as Sdecoded ∈RC×H×W.\n4.4. Tracking with Transformer-enhanced Features\nTransformer structure facilitates the tracking process by\ngenerating high-quality template featureTencoded and search\nfeature Sdecoded. We learn the tracking model using Tencoded\nfollowing two popular paradigms:\nSearch Region w/o Transformer w/ Transformer\nFigure 5. Tracking response maps of the DiMP baseline [3] with-\nout (second column) and with (third column) our designed trans-\nformer architecture. With the proposed transformer, the conﬁ-\ndences of the distracting objects are effectively suppressed.\n• Siamese Pipeline. In this setting, we simply crop the\ntarget feature inTencoded as the template CNN kernel to\nconvolve with Sdecoded for response generation, which\nis identical to the cross-correlation in SiamFC [1].\n• DCF Pipeline. Following the end-to-end DCF opti-\nmization in DiMP approach [3], we generate a discrim-\ninative CNN kernel using Tencoded to convolve with\nSdecoded for response generation.\nAfter obtaining the tracking response, we utilize the classi-\nﬁcation loss proposed in DiMP [3] to jointly train the back-\nbone network, our transformer, and the tracking model in\nan end-to-end manner. Please refer to [3] for more details.\nIn the online tracking process, to better exploit the tem-\nporal cues and adapt to the target appearance changes, we\ndynamically update the template ensemble T. To be spe-\nciﬁc, we drop the oldest template in T and add the current\ncollected template feature to T every 5 frames. The fea-\nture ensemble maintains a maximal size of 20 templates.\nOnce the template ensemble T is updated, we compute the\nnew encoded feature Tencoded via our transformer encoder.\nWhile the transformer encoder is sparsely utilized ( i.e., ev-\nery 5 frames), the transformer decoder is leveraged in each\nframe, which generates per-frame Sdecoded by propagating\nthe representations and attention cues from previous tem-\nplates to the current search patch.\nIt is widely recognized that DCF formulation in DiMP\n[3] is superior to the simple cross-correlation in Siamese\ntrackers [1, 28]. Nevertheless, in the experiments, we show\nthat with the help of our transformer architecture, a clas-\nsic Siamese pipeline is able to perform against the recent\nDiMP. Meanwhile, with our transformer, the DiMP tracker\nacquires further performance improvements. As shown in\n5\nFigure 10, even though the strong baseline DiMP [3] al-\nready shows impressive distractor discrimination capabil-\nity, our designed transformer further assists it to restrain the\nbackground conﬁdence for robust tracking.\n5. Experiments\n5.1. Implementation Details\nBased on the Siamese matching and DiMP based track-\ning frameworks, in the following experiments, we denote\nour Transformer-assisted trackers as TrSiam and TrDiMP,\nrespectively. In these two versions, the backbone model is\nResNet-50 [18] for feature extraction. Before the encoder\nand decoder, we additionally add one convolutional layer\n(3×3 Conv + BN) to reduce the backbone feature channel\nfrom 1024 to 512. The input template and search patches\nare 6 times of the target size and further resized to 352×352.\nThe temperature τ in Eq. 3 is set to 1/30. The parameter\nsigma σ in the feature mask is set to 0.1. Similar to the\nprevious works [7, 3, 9, 2], we utilize the training splits of\nLaSOT [13], TrackingNet [39], GOT-10k [23], and COCO\n[32] for ofﬂine training. The proposed transformer network\nis jointly trained with the original tracking parts (e.g., track-\ning optimization model [3] and IoUNet [9]) in an end-to-\nend manner. Our framework is trained for 50 epochs with\n1500 iterations per epoch and 36 image pairs per batch. The\nADAM optimizer [25] is employed with an initial learning\nrate of 0.01 and a decay factor of 0.2 for every 15 epochs.\nIn the online tracking stage, the main difference between\nTrSiam and TrDiMP lies in the tracking model generation\nmanner. After predicting the response map for target local-\nization, they all adopt the recent probabilistic IoUNet [9]\nfor target scale estimation. Our trackers are implemented in\nPython using PyTorch. TrSiam and TrDiMP operate about\n35 and 26 frames per second (FPS) on a single Nvidia GTX\n1080Ti GPU, respectively.\n5.2. Ablation Study\nTo verify the effectiveness of our designed transformer\nstructure, we choose the GOT-10k test set [23] with 180\nvideos to validate our TrSiam and TrDiMP methods1. GOT-\n10k hides the ground-truth labels of the test set to avoid the\noverly hyper-parameter ﬁne-tuning. It is worth mentioning\nthat there is no overlap in object classes between the train\nand test sets of GOT-10k, which also veriﬁes the general-\nization of our trackers to unseen object classes.\nIn Table 1, based on the Siamese and DiMP baselines,\nwe validate each component in our transformer:\nTransformer Encoder. First, without any decoder block,\nwe merely utilize encoder to promote the feature fusion of\n1With the probabilistic IoUNet [9] and a larger search area, our base-\nline performance is better than the standard DiMP [3]. Note that all the\nexperiments (Figure 6 and Table 1) are based on the same baseline for fair.\n5 10 15 20 25 30 35 40 45 50\nEpoch\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nLoss\nTraining Loss of Siamese Pipeline\nw/o Transformer (Baseline)\nOnly Feature Transf.\nOnly Mask Transf.\nBoth Feature & Mask Transf.\n5 10 15 20 25 30 35 40 45 50\nEpoch\n0.05\n0.1\n0.15\n0.2\nLoss\nTraining Loss of DiMP Pipeline\nw/o Transformer (Baseline)\nOnly Feature Transf.\nOnly Mask Transf.\nBoth Feature & Mask Transf.\nFigure 6. Training loss plots of the Siamese pipeline (left) and\nDCF pipeline (right). By combining both feature and mask trans-\nformations, our approach signiﬁcantly reduces the training losses.\nTable 1. Ablative experiments of our transformer for the Siamese\nand DiMP pipelines i.e., TrSiam and TrDiMP trackers. The per-\nformance is evaluated on the GOT-10k test set [23] in terms of\naverage overlap (AO).\nDifferent Tracking Variations Siamese (AO) DiMP (AO)\nBaseline Performance 62.0 66.7\nOnly Encoder (w/o Any Decoder) 63.81.8%↑ 67.30.6%↑\nEncoder + Decoder (Only Feature Transf.) 66.34.3%↑ 68.11.4%↑\nEncoder + Decoder (Only Mask Transf.) 67.15.1%↑ 67.81.1%↑\nEncoder + Decoder (Feature & Mask Transf.) 67.35.3%↑ 68.82.1%↑\nTable 2. Ablative study of our transformer architecture. The base-\nline tracker is TrSiam. The evaluation metric is average overlap\n(AO) score on the GOT-10k test set.\nBaseline Weight-sharing Feed-forward Head Number\nw/o w/ w/o w/ 1 2 4\nAO (%) 62.0 63.4 67.3 67.3 67.0 67.3 67.2 67.6\nSpeed (FPS) 40 35 35 35 22 35 31 25\nmultiple templates, which slightly improves two baselines.\nTransformer Decoder.Our decoder consists of feature and\nmask transformations, and we independently verify them:\n(1) Feature Propagation. With the feature transformation,\nas shown in Table 1, the Siamese pipeline obtains a notable\nperformance gain of 4.3% in AO and the strong DiMP base-\nline still acquires an improvement of 1.4% in AO on the\nGOT-10k test set. From the training perspective, we can\nobserve that this block effectively reduces the losses of two\nbaselines as shown in Figure 6.\n(2) Mask Propagation. This mechanism propagates tempo-\nrally collected spatial attentions to highlight the target area.\nSimilar to the feature transformation, our mask transforma-\ntion alone also steadily improves the tracking performance\n(Table 1) and consistently reduces the training errors of both\ntwo pipelines (Figure 6).\nComplete Transformer.With the complete transformer, as\nshown in Table 1, the Siamese and DiMP baselines obtain\nnotable performance gains of 5.3% and 2.1% in AO, respec-\ntively. The transformer also signiﬁcantly reduces their train-\ning losses (Figure 6). It is worth mentioning that DiMP al-\nready achieves outstanding results while our approach con-\nsistently improves such a strong baseline. With our trans-\nformer, the performance gap between Siamese and DiMP\n6\nTable 3. Comparison with state-of-the-art trackers on the TrackingNet test set [39] in terms of precision (Prec.), normalized precision (N.\nPrec.), and success (AUC score). Our TrDiMP and TrSiam exhibit promising results.\nSiamFC MDNet SPM C-RPN SiamRPN++ ATOM DiMP-50 SiamFC++ D3S Retain-MAML PrDiMP-50 DCFST KYS Siam-RCNN TrSiam TrDiMP\n[1] [40] [50] [14] [28] [7] [3] [59] [34] [49] [9] [64] [2] [48]\nPrec. (%) 53.3 56.5 66.1 61.9 69.4 64.8 68.7 70.5 66.4 - 70.4 70.0 68.8 80.0 72.7 73.1\nN. Prec. (%) 66.3 70.5 77.8 74.6 80.0 77.1 80.1 80.0 76.8 82.2 81.6 80.9 80.0 85.4 82.9 83.3\nSuccess (%) 57.1 60.6 71.2 66.9 73.3 70.3 74.0 75.4 72.8 75.7 75.8 75.2 74.0 81.2 78.1 78.4\nTable 4. Comparison results on the GOT-10k test set [23] in terms of average overlap (AO), and success rates (SR) at overlap thresholds\n0.5 and 0.75. We show the tracking results without (w/o) and with (w/) additional training data (LTC: LaSOT, TrackingNet, and COCO).\nSiamFC SiamFCv2 SiamRPN SPM ATOM DiMP-50 SiamFC++ D3S PrDiMP-50 DCFST KYS Siam-RCNN TrSiam TrDiMP\n[1] [46] [29] [50] [7] [3] [59] [34] [9] [64] [2] [48] w/o LTC w/ LTC w/o LTC w/ LTC\nSR0.5(%) 35.3 40.4 54.9 59.3 63.4 71.7 69.5 67.6 73.8 75.3 75.1 - 76.6 78.7 77.7 80.5\nSR0.75(%) 9.8 14.4 25.3 35.9 40.2 49.2 47.9 46.2 54.3 49.8 51.5 - 57.1 58.6 58.3 59.7\nAO (%) 34.8 37.4 46.3 51.3 55.6 61.1 59.5 59.7 63.4 63.8 63.6 64.9 66.0 67.3 67.1 68.8\nbaselines has been largely narrowed (from 4.7% to 1.5% in\nAO), which reveals the strong tracking potential of a simple\npipeline by adequately exploring the temporal information.\nStructure Modiﬁcations. Finally, we discuss some archi-\ntecture details of our transformer: (1) Shared-weight Self-\nattention. Since our transformer is separated into two par-\nallel Siamese tracking braches, the performance obviously\ndrops without the weight-sharing mechanism as shown in\nTable 2. Due to this weight-sharing design, we also do\nnot stack multiple encoder/decoder layers like the classic\ntransformer [47], which will divide the template and search\nrepresentations into different feature subspaces. (2) Feed-\nforward Network. Feed-forward network is a basic block in\nthe classic transformer [47], which consists of two heavy-\nweight fully-connected layers. In the tracking scenario, we\nobserve that this block potentially causes the overﬁtting is-\nsue due to its overmany parameters, which does not bring\nperformance gains and hurts the efﬁciency. (3) Head Num-\nber. Classic transformer adopts multi-head attentions ( e.g.,\n8 heads) to learn diverse representations [47]. In the experi-\nments, we observe that increasing the head number slightly\nimproves the accuracy but hinders the tracking efﬁciency\nfrom real-time. We thus choose the single-head attention to\nachieve a good balance of performance and efﬁciency.\n5.3. State-of-the-art Comparisons\nWe compare our proposed TrSiam and TrDiMP track-\ners with the recent state-of-the-art trackers on seven track-\ning benchmarks including TrackingNet [39], GOT-10k [23],\nLaSOT [13], VOT2018 [26], Need for Speed [24], UA V123\n[37], and OTB-2015 [58].\nTrackingNet [39]. TrackingNet is a recently released large-\nscale benchmark. We evaluate our methods on the test set of\nTrackingNet, which consists of 511 videos. In this bench-\nmark, we compare our approaches with the state-of-the-art\ntrackers such as DiMP-50 [3], D3S [34], SiamFC++ [59],\nRetain-MAML [49], DCFST [64], PrDiMP-50 [9], KYS\n[2], and Siam-RCNN [48]. As shown in Table 3, the pro-\nposed TrDiMP achieves a normalized precision score of\n0 5 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Precision\nPrecision plots of OPE on LaSOT Testing Set\n[0.614] TrDiMP (Ours)\n[0.600] TrSiam (Ours)\n[0.565] PrDiMP50\n[0.534] DiMP50\n[0.530] PrDiMP18\n[0.505] DiMP18\n[0.479] ATOM\n[0.467] SiamRPN++\n[0.425] C-RPN\n[0.370] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE on LaSOT Testing Set\n[0.639] TrDiMP (Ours)\n[0.624] TrSiam (Ours)\n[0.593] PrDiMP50\n[0.565] DiMP50\n[0.563] PrDiMP18\n[0.534] DiMP18\n[0.514] ATOM\n[0.496] SiamRPN++\n[0.455] C-RPN\n[0.397] MDNet\nFigure 7. Precision and success plots on the LaSOT test set [13].\nIn the legend, the distance precision (DP) and area-under-curve\n(AUC) are reported in the left and right ﬁgures, respectively.\n83.3% and a success score of 78.4%, surpassing previous\nstate-of-the-art trackers such as PrDiMP-50 and KYS. Note\nthat PrDiMP and KYS improve the DiMP tracker via prob-\nabilistic regression and tracking scene exploration, repre-\nsenting the current leading algorithms on several datasets.\nWith our designed transformer, the simple Siamese match-\ning baseline ( i.e., TrSiam) also shows outstanding perfor-\nmance with a normalized precision score of 82.9% and a\nsuccess score of 78.1%.\nGOT-10k [23]. GOT-10k is a large-scale dataset including\nmore than 10,000 videos. We test our methods on the test\nset of GOT-10k with 180 sequences. The main character-\nistic of GOT-10k is that the test set does not have overlap\nin object classes with the train set, which is designed to as-\nsess the generalization of the visual tracker. Following the\ntest protocol of GOT-10k, we further train our trackers with\nonly the GOT-10k training set. As shown in Table 4, in\na fair comparison scenario (i.e., without additional training\ndata), both our TrDiMP and TrSiam still outperform other\ntop-performing trackers such as SiamR-CNN [48], DCFST\n[64], and KYS [2], verifying the strong generalization of\nour methods to unseen objects.\nLaSOT [13]. LaSOT is a recent large-scale tracking bench-\nmark consisting of 1200 videos. The average video length\nof this benchmark is about 2500 frames, which is more\nchallenging than the previous short-term tracking datasets.\nTherefore, how to cope with the drastic target appearance\n7\nTable 5. State-of-the-art comparison on the NfS [24], UA V123 [37], and OTB-2015 [58] datasets in terms of AUC score. Both our TrDiMP\nand TrSiam exhibit outstanding results on all benchmarks with competitive efﬁciency.\nKCF SiamFC CFNet MDNet C-COT ECO ATOM UPDT SiamRPN++ DiMP-50 SiamR-CNN PrDiMP-50 DCFST KYS TrSiam TrDiMP\n[20] [1] [46] [40] [11] [8] [7] [4] [28] [3] [48] [9] [64] [2]\nNfS [24] 21.7 - - 42.9 48.8 46.6 58.4 53.7 50.2 62.0 63.9 63.5 64.1 63.5 65.8 66.5\nUA V123 [37] 33.1 49.8 43.6 52.8 51.3 52.2 64.2 54.5 61.3 65.3 64.9 68.0 - - 67.4 67.5\nOTB-2015 [58] 47.5 58.2 56.8 67.8 68.2 69.1 66.9 70.2 69.6 68.4 70.1 69.6 70.9 69.5 70.8 71.1\nSpeed (FPS) 270 86 75 1 0.3 8 35 <1 30 35 4.7 30 25 20 35 26\n161116212631364146515661660\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nFigure 8. Expected average overlap (EAO) graph with trackers\nranked from right to left. Our TrDiMP and TrSiam trackers out-\nperform all the participant trackers on the VOT2018 [26].\nchanges using temporal context is vital in this dataset. We\nevaluate our approaches on the LaSOT test set with 280\nvideos. The precision and success plots of the state-of-the-\nart methods are shown in Figure 7, where the recently pro-\nposed C-RPN [14], SiamRPN++ [28], ATOM [7], DiMP-50\n[3], and PrDiMP-50 [9] are included for comparison. Our\nTrSiam and TrDiMP outperform aforementioned methods\nby a considerable margin. To the best of our knowledge,\nSiamR-CNN [48] achieves the current best result on the La-\nSOT. Overall, our TrDiMP (63.9% AUC and 26 FPS) ex-\nhibits very competitive performance and efﬁciency in com-\nparison with SiamR-CNN (64.8% AUC and 4.7 FPS).\nVOT2018 [26]. VOT2018 benchmark contains 60 challeng-\ning videos. The performance on this dataset is evaluated us-\ning the expected average overlap (EAO), which takes both\naccuracy (average overlap over successful frames) and ro-\nbustness (failure rate) into account. As shown in Figure 8,\nour TrSiam and TrDiMP clearly outperform all the partici-\npant trackers on the VOT2018.\nIn Table 10, we further show the accuracy, robustness,\nand EAO scores of the recent top-performing trackers in-\ncluding SiamRPN++ [28], DiMP-50 [3], PrDiMP-50 [9],\nRetain-MAML [49], KYS [2], and D3S [34]. Compared\nwith these recently proposed approaches, our TrDiMP ap-\nproach still exhibits satisfactory results. Among all the\ncompared trackers, only D3S slightly outperforms our\nTrDiMP, which is trained using additional data with seg-\nmentation annotations for accurate mask prediction.\nNfS [24]. NfS dataset contains 100 challenging videos with\nfast-moving objects. We evaluate our TrSiam and TrDiMP\non the 30 FPS version of NfS. The AUC scores of compar-\nison approaches are shown in Table 5. Our approaches set\nnew state-of-the-art records on this benchmark. The pro-\nposed TrDiMP surpasses previous top-performing trackers\nTable 6. Comparison with recent state-of-the-art trackers on the\nVOT2018 [26] in terms of accuracy (A), robustness (R), and ex-\npected average overlap (EAO).\nSiamRPN DiMP-50 PrDiMP-50 Retain- KYS D3S TrDiMP\n++ [28] [3] [9] MAML [49] [2] [34]\nA (↑) 0.600 0.597 0.618 0.604 0.609 0.640 0.600\nR (↓) 0.234 0.153 0.165 0.159 0.143 0.150 0.141\nEAO (↑) 0.414 0.440 0.442 0.452 0.462 0.489 0.462\nsuch as DCFST [64] and SiamR-CNN [48]. Note that the\nrecent SimR-CNN utilizes a powerful ResNet-101 for ob-\nject re-detection. Our simple TrSiam, without sophisticated\nmodels or online optimization techniques, still outperforms\nexisting methods and operates in real-time.\nUA V123 [37]. This benchmark includes 123 aerial videos\ncollected by the low-attitude UA V platform. The proposed\ntrackers also achieve promising results in comparison to the\nrecent remarkable approaches in Table 5. Speciﬁcally, our\nTrDiMP performs on par with PrDiMP-50 [9], which rep-\nresents the current best algorithm on this benchmark.\nOTB-2015 [58]. OTB-2015 is a popular tracking bench-\nmark with 100 challenging videos. As shown in Table 5,\non this dataset, our TrDiMP achieves an AUC score of\n71.1%, surpassing the recently proposed SiamRPN++ [28],\nPrDiMP-50 [9], SiamR-CNN [48], and KYS [2]. With the\nproposed transformer, our Siamese matching based TrSiam\nalso performs favorably against existing state-of-the-art ap-\nproaches with an AUC score of 70.8%.\n6. Conclusion\nIn this work, we introduce the transformer structure to\nthe tracking frameworks, which bridges the isolated frames\nin the video ﬂow and conveys the rich temporal cues across\nframes. We show that by carefully modifying the classic\ntransformer architecture, it favorably suits the tracking sce-\nnario. With the proposed transformer, two popular trackers\ngain consistent performance improvements and set several\nnew state-of-the-art records on prevalent tracking datasets.\nTo our best knowledge, this is the ﬁrst attempt to exploit the\ntransformer in the tracking community, which preliminarily\nunveils the tracking potential hidden in the frame-wise rela-\ntionship. In the future, we intend to further explore the rich\ntemporal information among individual video frames.\nAcknowledgements. This work was supported in part by the National\nNatural Science Foundation of China under Contract 61836011, 61822208,\nand 61836006, and in part by the Youth Innovation Promotion Association\nCAS under Grant 2018497.\n8\nReferences\n[1] Luca Bertinetto, Jack Valmadre, Jo ˜ao F Henriques, Andrea\nVedaldi, and Philip HS Torr. Fully-convolutional siamese\nnetworks for object tracking. In ECCV Workshops, 2016.\n[2] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Know your surroundings: Exploiting scene infor-\nmation for object tracking. In ECCV, 2020.\n[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Learning discriminative model prediction for track-\ning. In ICCV, 2019.\n[4] Goutam Bhat, Joakim Johnander, Martin Danelljan, Fa-\nhad Shahbaz Khan, and Michael Felsberg. Unveiling the\npower of deep tracking. In ECCV, 2018.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020.\n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey E Hinton. A simple framework for contrastive learn-\ning of visual representations. arXiv: 2002.05709, 2020.\n[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\nMichael Felsberg. Atom: Accurate tracking by overlap max-\nimization. In CVPR, 2019.\n[8] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\nMichael Felsberg. Eco: Efﬁcient convolution operators for\ntracking. In CVPR, 2017.\n[9] Martin Danelljan, Luc Van Gool, and Radu Timofte. Proba-\nbilistic regression for visual tracking. In CVPR, 2020.\n[10] Martin Danelljan, Gustav H ¨ager, Fahad Khan, and Michael\nFelsberg. Accurate scale estimation for robust visual track-\ning. In BMVC, 2014.\n[11] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,\nand Michael Felsberg. Beyond correlation ﬁlters: Learn-\ning continuous convolution operators for visual tracking. In\nECCV, 2016.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[13] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\nYu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\nLasot: A high-quality benchmark for large-scale single ob-\nject tracking. In CVPR, 2019.\n[14] Heng Fan and Haibin Ling. Siamese cascaded region pro-\nposal networks for real-time visual tracking. InCVPR, 2019.\n[15] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.\nLearning background-aware correlation ﬁlters for visual\ntracking. In ICCV, 2017.\n[16] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Graph con-\nvolutional tracking. In CVPR, 2019.\n[17] Qing Guo, Wei Feng, Ce Zhou, Rui Huang, Liang Wan, and\nSong Wang. Learning dynamic siamese network for visual\nobject tracking. In ICCV, 2017.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016.\n[19] David Held, Sebastian Thrun, and Silvio Savarese. Learning\nto track at 100 fps with deep regression networks. In ECCV,\n2016.\n[20] Jo ˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge\nBatista. High-speed tracking with kernelized correlation ﬁl-\nters. TPAMI, 37(3):583–596, 2015.\n[21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\n[22] Chen Huang, Simon Lucey, and Deva Ramanan. Learning\npolicies for adaptive tracking with deep feature cascades. In\nICCV, 2017.\n[23] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A\nlarge high-diversity benchmark for generic object tracking in\nthe wild. arXiv preprint arXiv:1810.11981, 2018.\n[24] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva\nRamanan, and Simon Lucey. Need for speed: A benchmark\nfor higher frame rate object tracking. In ICCV, 2017.\n[25] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014.\n[26] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-\nberg, Roman Pﬂugfelder, Luka Cehovin Zajc, Tomas V ojir,\nGoutam Bhat, Alan Lukezic, Abdelrahman Eldesokey, et al.\nThe sixth visual object tracking vot2018 challenge results. In\nECCV Workshops, 2018.\n[27] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Fels-\nberg, Roman Pﬂugfelder, Joni-Kristian Kamarainen, Luka\nCehovin Zajc, Ondrej Drbohlav, Alan Lukezic, Amanda\nBerg, et al. The seventh visual object tracking vot2019 chal-\nlenge results. In ICCV Workshops, 2019.\n[28] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,\nand Junjie Yan. Siamrpn++: Evolution of siamese visual\ntracking with very deep networks. In CVPR, 2019.\n[29] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.\nHigh performance visual tracking with siamese region pro-\nposal network. In CVPR, 2018.\n[30] Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang, and Ming-\nHsuan Yang. Learning spatial-temporal regularized correla-\ntion ﬁlters for visual tracking. In CVPR, 2018.\n[31] Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, and Ming-\nHsuan Yang. Target-aware deep tracking. In CVPR, 2019.\n[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014.\n[33] Xiankai Lu, Chao Ma, Bingbing Ni, Xiaokang Yang, Ian\nReid, and Ming-Hsuan Yang. Deep regression tracking with\nshrinkage loss. In ECCV, 2018.\n[34] Alan Lukezic, Jiri Matas, and Matej Kristan. D3s-a discrim-\ninative single shot segmentation tracker. In CVPR, 2020.\n[35] Alan Lukezic, Tomas V ojir, Luka Cehovin Zajc, Jiri Matas,\nand Matej Kristan. Discriminative correlation ﬁlter with\nchannel and spatial reliability. In CVPR, 2017.\n[36] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan\nYang. Hierarchical convolutional features for visual tracking.\nIn ICCV, 2015.\n9\n[37] Matthias Mueller, Neil Smith, and Bernard Ghanem. A\nbenchmark and simulator for uav tracking. In ECCV, 2016.\n[38] Matthias Mueller, Neil Smith, and Bernard Ghanem.\nContext-aware correlation ﬁlter tracking. In CVPR, 2017.\n[39] Matthias M ¨uller, Adel Bibi, Silvio Giancola, Salman Al-\nSubaihi, and Bernard Ghanem. Trackingnet: A large-scale\ndataset and benchmark for object tracking in the wild. In\nECCV, 2018.\n[40] Hyeonseob Nam and Bohyung Han. Learning multi-domain\nconvolutional neural networks for visual tracking. In CVPR,\n2016.\n[41] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In ICML, 2018.\n[42] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI blog, 2019.\n[43] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson\nLau, and Ming-Hsuan Yang. Crest: Convolutional residual\nlearning for visual tracking. In ICCV, 2017.\n[44] Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Edouard\nGrave, Tatiana Likhomanenko, Vineel Pratap, Anuroop Sri-\nram, Vitaliy Liptchinsky, and Ronan Collobert. End-to-end\nasr: from supervised to semi-supervised learning with mod-\nern architectures. arXiv preprint arXiv:1911.08460, 2019.\n[45] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders.\nSiamese instance search for tracking. In CVPR, 2016.\n[46] Jack Valmadre, Luca Bertinetto, Jo ˜ao F Henriques, Andrea\nVedaldi, and Philip HS Torr. End-to-end representation\nlearning for correlation ﬁlter based tracking. In CVPR, 2017.\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[48] Paul V oigtlaender, Jonathon Luiten, Philip H. S. Torr, and\nBastian Leibe. Siam r-cnn: Visual tracking by re-detection.\nIn CVPR, 2020.\n[49] Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong,\nand Wenjun Zeng. Tracking by instance detection: A meta-\nlearning approach. In CVPR, 2020.\n[50] Guangting Wang, Chong Luo, Zhiwei Xiong, and Wenjun\nZeng. Spm-tracker: Series-parallel matching for real-time\nvisual object tracking. In CVPR, 2019.\n[51] Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei\nLiu, and Houqiang Li. Unsupervised deep tracking. In\nCVPR, 2019.\n[52] Ning Wang, Wengang Zhou, Guojun Qi, and Houqiang Li.\nPost: Policy-based switch tracking. In AAAI, 2020.\n[53] Ning Wang, Wengang Zhou, Yibing Song, Chao Ma, Wei\nLiu, and Houqiang Li. Unsupervised deep representation\nlearning for real-time tracking. IJCV, 129(2):400–418, 2021.\n[54] Ning Wang, Wengang Zhou, Qi Tian, Richang Hong, Meng\nWang, and Houqiang Li. Multi-cue correlation ﬁlters for ro-\nbust visual tracking. In CVPR, 2018.\n[55] Qiang Wang, Zhu Teng, Junliang Xing, Jin Gao, Weiming\nHu, and Stephen Maybank. Learning attentions: Residual\nattentional siamese network for high performance online vi-\nsual tracking. In CVPR, 2018.\n[56] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and\nPhilip HS Torr. Fast online object tracking and segmentation:\nA unifying approach. In CVPR, 2019.\n[57] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018.\n[58] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-\ning benchmark. TPAMI, 37(9):1834–1848, 2015.\n[59] Yinda Xu, Zeyu Wang, Zuoxin Li, Yuan Ye, and Gang Yu.\nSiamfc++: Towards robust and accurate visual tracking with\ntarget estimation guidelines. In AAAI, 2020.\n[60] Tianyu Yang and Antoni B Chan. Learning dynamic memory\nnetworks for object tracking. In ECCV, 2018.\n[61] Yuechen Yu, Yilei Xiong, Weilin Huang, and Matthew R.\nScott. Deformable siamese attention networks for visual ob-\nject tracking. In CVPR, 2020.\n[62] Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer,\nMartin Danelljan, and Fahad Shahbaz Khan. Learning the\nmodel update for siamese trackers. In ICCV, 2019.\n[63] Zhipeng Zhang and Houwen Peng. Deeper and wider\nsiamese networks for real-time visual tracking. In CVPR,\n2019.\n[64] Linyu Zheng, Ming Tang, Yingying Chen, Jinqiao Wang, and\nHanqing Lu. Learning feature embeddings for discriminant\nmodel based tracking. In ECCV, 2020.\n[65] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and\nWeiming Hu. Distractor-aware siamese networks for visual\nobject tracking. In ECCV, 2018.\n[66] Zheng Zhu, Wei Wu, Wei Zou, and Junjie Yan. End-to-end\nﬂow correlation tracking with spatial-temporal attention. In\nCVPR, 2018.\n10\nA. Ablation Study\nA.1. Hyper-parameters\nIn the online tracking stage, the only involved hyper-\nparameters are the template sampling interval as well as\nthe template ensemble size. As shown in Table 7, we\nobserve that sampling the template every 5 frames shows\npromising results. This sparse update mechanism is also\nwidely adopted in many previous trackers such as ECO [8]\nand ATOM [7]. Besides, increasing the memory size ( i.e.,\nthe total sample number in the template ensemble T) also\nsteadily improves the performance. To achieve a good bal-\nance of performance and efﬁciency, we choose the maxi-\nmum ensemble size of 20.\nAs for other tracking-related hyper-parameters, we fol-\nlow our baseline approach DiMP [3] without modiﬁcation.\nMore details can be found in the source code.\nA.2. Improvements upon Baselines\nIn Table 8 and 9, we compare our TrSiam and TrDiMP\nwith their corresponding baselines on seven tracking bench-\nmarks. As shown in Table 8, our designed transformer con-\nsistently improves the Siamese baseline on seven tracking\ndatasets. For example, our TrSiam approach outperforms its\nbaseline by 5.3%, 4.7%, 3.3%, and 3.0% in terms of AUC\nscore on the challenging GOT-10k, NfS, LaSOT, and Track-\ningNet datasets, respectively. On the OTB-2015 dataset, our\napproach still improves the baseline by 1.6%. The OTB-\n2015 dataset is known to be highly saturated over recent\nyears. Note that our Siamese baseline already achieves a\nhigh performance level of 69.2% AUC on the OTB-2015.\nThus, it is relatively harder to obtain a signiﬁcant perfor-\nmance gain on this benchmark.\nIn Table 9, we further exhibit the comparison results\nbetween our transformer-assisted TrDiMP and its baseline\nDiMP [3]. It is worth mentioning that the DiMP approach\nalready introduces a memory mechanism to incrementally\nupdate the tracking model and explores the temporal infor-\nmation to some extent. Besides, our baseline includes the\nrecent probabilistic IoUNet [9] for accurate target scale esti-\nmation and adopts a larger search area (6 times of the target\nobject) for tracking ( i.e., the superDiMP setting 2), which\nsigniﬁcantly outperforms the standard DiMP approach pre-\nsented in [3] . It is well recognized that improving a strong\nbaseline is much more challenging. Although our base-\nline achieves outstanding results on various tracking bench-\nmarks, our proposed transformer consistently improves it\non all datasets.\n2https://github.com/visionml/pytracking/tree/\nmaster/ltr\nTable 7. Ablation experiments on the template sampling interval\nand template ensemble size. The testing approach is our TrSiam.\nThe performance is evaluated on the GOT-10k test set [23] and\nNfS [24] in terms of AUC score.\nInterval 1 1 1 1 5 5 5 5 10 10 10 10\nEns. Size 1 10 20 30 1 10 20 30 1 10 20 30\nGOT-10k 63.8 65.5 66.1 66.9 63.6 66.5 67.3 67.6 63.3 65.5 65.4 65.6\nNfS 62.2 63.9 64.5 64.4 62.1 64.4 65.8 65.8 62.2 64.9 65.3 65.2\nFPS 36 32 28 22 40 38 35 31 40 38 36 33\nTable 8. Comparison results of the Siamese pipeline between with-\nout and with our transformer on 7 tracking benchmarks. We com-\npute the relative gain in the VOT2018, while in the rest datasets,\nwe exhibit the absolute gain.\nDataset Siamese Baseline TrSiam (Ours) ∆\nNeed for Speed [24] (AUC) 61.1 65.8 4.7% ↑\nOTB-2015 [58] (AUC) 69.2 70.8 1.6% ↑\nUA V123 [37] (AUC) 65.6 67.4 1.8% ↑\nLaSOT [13] (AUC) 59.1 62.4 3.3% ↑\nGOT-10k [23] (AO) 62.0 67.3 5.3% ↑\nTrackingNet [39] (Success) 75.1 78.1 3.0% ↑\nVOT2018 [26] (EAO) 0.389 0.417 7.2% ↑\nTracking Speed (FPS) 40 35 5 FPS ↓\nTable 9. Comparison results of the DiMP pipeline between without\nand with our transformer on 7 tracking benchmarks. We compute\nthe relative gain in the VOT2018, while in the rest datasets, we\nexhibit the absolute gain.\nDataset DiMP Baseline TrDiMP (Ours) ∆\nNeed for Speed [24] (AUC) 64.7 66.5 1.8% ↑\nOTB-2015 [58] (AUC) 70.1 71.1 1.0% ↑\nUA V123 [37] (AUC) 67.2 67.5 0.3% ↑\nLaSOT [13] (AUC) 63.0 63.9 0.9% ↑\nGOT-10k [23] (AO) 66.7 68.8 2.1% ↑\nTrackingNet [39] (Success) 78.1 78.4 0.3% ↑\nVOT2018 [26] (EAO) 0.446 0.462 3.6% ↑\nTracking Speed (FPS) 30 26 4 FPS ↓\nB. Visualization\nB.1. Attention Visualization\nAs shown in Figure 9 (a), after self-attention, the pix-\nels get some minor weights from their neighboring pixels to\nreinforce themselves. In the decoding process, as shown in\nFigure 9 (b), the cross-attention matrice between two differ-\nent patches is sparse, which means the query seeks several\nmost correlated keys to propagate the context. After Soft-\nmax, the attention weights are not averaged by the simi-\nlar athletes in Bolt2 sequence, which illustrates our atten-\ntion block can discriminate the distractors to some extent.\nBeneﬁting such (feature/mask) propagations, the tracking\nresponses are accurate, as shown in Figure 10.\nB.2. Response Visualization\nIn Figure 10, we exhibit more detailed visualization re-\nsults of our tracking framework. From Figure 10 (second\ncolumn), we can observe that our baseline ( i.e., DiMP [3])\ntends to be misled by distracting objects in the challenging\nscenarios. By adopting the feature transformation mecha-\n11\n(a) Self-attention Matrix (b) Cross-attention Matrix\nAtten(𝜑 ,𝜑( )) ∈ ℝ484×484 Atten(𝜙 ,𝜙( )) ∈ ℝ484×484\nFigure 9. Visualization of the attention maps of the encoder (self-\nattention block) and decoder (cross-attention block).\nSearch Region w/o Transformer w/ Feature w/ Mask w/ Transformer\nFigure 10. Visualization of the tracking response maps of DiMP\nbaseline [3]. The “w/o Transformer” denotes the baseline ap-\nproach DiMP [3]. The “w/ Feature” denotes the baseline with a\nfeature propagation based transformer. The “w/ Mask” represents\nthe baseline with a mask propagation based transformer. Finally,\nthe “w/ Transformer” is our complete transformer-assisted tracker,\ni.e., TrDiMP. Our proposed components (feature and mask trans-\nformations) effectively suppress the background responses.\nnism (third column in Figure 10), the target representations\nin the search region are effectively reinforced, which facil-\nitates the object searching process. Therefore, the response\nvalues of the background regions are largely restrained. The\nmask transformation block propagates the spatial attentions\nfrom previous templates to the current search region, which\nalso effectively suppresses the background objects (fourth\ncolumn in Figure 10). Finally, our complete transformer ar-\nchitecture combines both feature and mask transformations,\nand the ﬁnal response maps (last column in Figure 10) are\nmore robust for object tracking.\nC. Results on VOT2019\nVOT2019 [27] is a recently released challenging bench-\nmark, which replaces 12 easy videos in VOT2018 [26] by\n12 more difﬁcult videos. We compare our approach with\nTable 10. The accuracy (A), robustness (R), and expected average\noverlap (EAO) of state-of-the-art methods on the VOT-2019 [27].\nSPM SiamRPN++ SiamMask ATOM SiamDW DiMP-50 TrDiMP\n[50] [28] [56] [7] [63] [3] Ours\nA (↑) 0.577 0.599 0.594 0.603 0.600 0.594 0.598\nR (↓) 0.507 0.482 0.461 0.411 0.467 0.278 0.231\nEAO (↑) 0.275 0.285 0.287 0.292 0.299 0.379 0.397\nsome top-performing approaches on VOT2019. Table 10\nshows the accuracy, robustness, and EAO scores of differ-\nent trackers. Compared with DiMP-50, our TrDiMP shows\nsimilar tracking accuracy but exhibits a much lower failure\nrate ( i.e., robustness score). Compared with other recent\ndeep trackers with the ResNet-50 backbone, our TrDiMP\nsigniﬁcantly surpasses them such as SiamRPN++, SiamDW\n[63] and SiamMask [56] by a considerable margin. The\nVOT2019 challenge winner ( i.e., DRNet) shows an EAO\nscore of 0.395 [27]. Overall, the proposed TrDiMP outper-\nforms the current top-performing trackers with a promising\nEAO score of 0.397.\nD. Failure Case\nWhen the target object is occluded or invisible, the cross\nattention maps between the current frame and historic tem-\nplates are inaccurate. Therefore, our framework struggles to\nhandle the heavy occlusion (e.g., Figure 11) or out-of-view.\nAnother potential limitation of our work is the high com-\nputational memory of the attention matrix, which is also a\ncommon issue in the transformer.\n#105 #140 #150\nTarget\nFigure 11. Failure case. TrDiMP fails to track the occluded target.\nE. Attribute Analysis\nFinally, in Figure 12, we provide the attribute evalua-\ntion on the LaSOT [13] benchmark. On the LaSOT, our\napproaches show good results in various scenarios such as\nmotion blur, background clutter, low resolution, and view-\npoint change. As shown in Table 8, with the proposed trans-\nformer, our TrSiam outperforms its baseline by 3.3% AUC.\nIt should be noted that our simple TrSiam does not adopt\ncomplex online model optimization techniques, which is\nmore efﬁcient than the recent approaches such as DiMP [3]\nand PrDiMP [9].\n12\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Rotation (175)\n[0.622] TrDiMP (Ours)\n[0.619] TrSiam (Ours)\n[0.574] PrDiMP50\n[0.549] DiMP50\n[0.547] PrDiMP18\n[0.513] DiMP18\n[0.483] ATOM\n[0.483] SiamRPN++\n[0.438] C-RPN\n[0.379] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Aspect Ration Change (249)\n[0.621] TrDiMP (Ours)\n[0.611] TrSiam (Ours)\n[0.580] PrDiMP50\n[0.558] DiMP50\n[0.547] PrDiMP18\n[0.517] DiMP18\n[0.497] ATOM\n[0.471] SiamRPN++\n[0.435] C-RPN\n[0.366] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Success rate\nSuccess plots of OPE - Background Clutter (100)\n[0.579] TrDiMP (Ours)\n[0.548] TrSiam (Ours)\n[0.535] PrDiMP50\n[0.507] PrDiMP18\n[0.501] DiMP50\n[0.472] DiMP18\n[0.454] ATOM\n[0.449] SiamRPN++\n[0.409] C-RPN\n[0.374] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Camera Motion (86)\n[0.675] TrDiMP (Ours)\n[0.655] TrSiam (Ours)\n[0.628] PrDiMP50\n[0.606] DiMP50\n[0.597] PrDiMP18\n[0.578] DiMP18\n[0.553] ATOM\n[0.521] SiamRPN++\n[0.482] C-RPN\n[0.416] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Deformation (142)\n[0.638] TrDiMP (Ours)\n[0.637] TrSiam (Ours)\n[0.601] PrDiMP50\n[0.584] PrDiMP18\n[0.574] DiMP50\n[0.545] DiMP18\n[0.529] SiamRPN++\n[0.512] ATOM\n[0.479] C-RPN\n[0.391] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Success rate\nSuccess plots of OPE - Fast Motion (53)\n[0.528] TrDiMP (Ours)\n[0.499] TrSiam (Ours)\n[0.480] PrDiMP50\n[0.467] DiMP50\n[0.433] PrDiMP18\n[0.415] DiMP18\n[0.414] ATOM\n[0.319] SiamRPN++\n[0.290] C-RPN\n[0.260] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Success rate\nSuccess plots of OPE - Full Occlusion (118)\n[0.562] TrDiMP (Ours)\n[0.542] TrSiam (Ours)\n[0.506] PrDiMP50\n[0.495] DiMP50\n[0.470] PrDiMP18\n[0.451] DiMP18\n[0.427] ATOM\n[0.374] SiamRPN++\n[0.348] C-RPN\n[0.305] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Illumination Variation (47)\n[0.669] TrDiMP (Ours)\n[0.636] TrSiam (Ours)\n[0.635] PrDiMP50\n[0.590] PrDiMP18\n[0.577] DiMP50\n[0.568] DiMP18\n[0.549] ATOM\n[0.528] SiamRPN++\n[0.487] C-RPN\n[0.407] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Success rate\nSuccess plots of OPE - Low Resolution (141)\n[0.577] TrDiMP (Ours)\n[0.558] TrSiam (Ours)\n[0.532] PrDiMP50\n[0.510] DiMP50\n[0.493] PrDiMP18\n[0.467] DiMP18\n[0.450] ATOM\n[0.387] SiamRPN++\n[0.355] C-RPN\n[0.317] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Motion Blur (89)\n[0.622] TrDiMP (Ours)\n[0.610] TrSiam (Ours)\n[0.573] PrDiMP50\n[0.559] DiMP50\n[0.533] PrDiMP18\n[0.526] DiMP18\n[0.494] ATOM\n[0.448] SiamRPN++\n[0.413] C-RPN\n[0.376] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Out-of-View (104)\n[0.605] TrDiMP (Ours)\n[0.589] TrSiam (Ours)\n[0.551] PrDiMP50\n[0.518] DiMP50\n[0.498] PrDiMP18\n[0.469] DiMP18\n[0.446] ATOM\n[0.406] SiamRPN++\n[0.365] C-RPN\n[0.330] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Partial Occlusion (187)\n[0.610] TrDiMP (Ours)\n[0.599] TrSiam (Ours)\n[0.565] PrDiMP50\n[0.537] DiMP50\n[0.524] PrDiMP18\n[0.497] DiMP18\n[0.474] ATOM\n[0.465] SiamRPN++\n[0.432] C-RPN\n[0.370] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Scale Variation (273)\n[0.632] TrDiMP (Ours)\n[0.624] TrSiam (Ours)\n[0.588] PrDiMP50\n[0.561] PrDiMP18\n[0.560] DiMP50\n[0.530] DiMP18\n[0.512] ATOM\n[0.494] SiamRPN++\n[0.452] C-RPN\n[0.392] MDNet\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Success rate\nSuccess plots of OPE - Viewpoint Change (33)\n[0.634] TrDiMP (Ours)\n[0.606] TrSiam (Ours)\n[0.584] PrDiMP50\n[0.553] DiMP50\n[0.552] PrDiMP18\n[0.494] DiMP18\n[0.455] ATOM\n[0.454] SiamRPN++\n[0.405] C-RPN\n[0.358] MDNet\nFigure 12. Attribute-based evaluation on the LaSOT benchmark [13]. The legend shows the AUC scores of the success plots.\n13",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6936375498771667
    },
    {
      "name": "BitTorrent tracker",
      "score": 0.6068081855773926
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6053719520568848
    },
    {
      "name": "Transformer",
      "score": 0.6050292253494263
    },
    {
      "name": "Encoder",
      "score": 0.5160686373710632
    },
    {
      "name": "Computer vision",
      "score": 0.5012650489807129
    },
    {
      "name": "Video tracking",
      "score": 0.4904055595397949
    },
    {
      "name": "Discriminative model",
      "score": 0.4637696146965027
    },
    {
      "name": "Eye tracking",
      "score": 0.34575024247169495
    },
    {
      "name": "Engineering",
      "score": 0.19114845991134644
    },
    {
      "name": "Object (grammar)",
      "score": 0.14396551251411438
    },
    {
      "name": "Voltage",
      "score": 0.07645124197006226
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": []
}