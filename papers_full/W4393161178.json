{
  "title": "Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models",
  "url": "https://openalex.org/W4393161178",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2050536281",
      "name": "Kun Zhang",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2344356506",
      "name": "Jiali Zeng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2133392087",
      "name": "Fandong Meng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2120380447",
      "name": "Yuanzhuo Wang",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2165481775",
      "name": "Shiqi Sun",
      "affiliations": [
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2013123436",
      "name": "Long Bai",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2106143704",
      "name": "Huawei Shen",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2050536281",
      "name": "Kun Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2344356506",
      "name": "Jiali Zeng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2133392087",
      "name": "Fandong Meng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2120380447",
      "name": "Yuanzhuo Wang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Institute of Computing Technology",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2165481775",
      "name": "Shiqi Sun",
      "affiliations": [
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2013123436",
      "name": "Long Bai",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2106143704",
      "name": "Huawei Shen",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4353015365",
    "https://openalex.org/W3203638210",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W3190126809",
    "https://openalex.org/W6811284106",
    "https://openalex.org/W2949720445",
    "https://openalex.org/W4385568285",
    "https://openalex.org/W4389519283"
  ],
  "abstract": "Large language models (LLMs) have recently demonstrated remarkable performance across various Natual Language Processing tasks. In the field of multi-hop reasoning, the Chain-of-thought (CoT) prompt method has emerged as a paradigm, using curated stepwise reasoning demonstrations to enhance LLM's ability to reason and produce coherent rational pathways. To ensure the accuracy, reliability, and traceability of the generated answers, many studies have incorporated information retrieval (IR) to provide LLMs with external knowledge. However, existing CoT with IR methods decomposes questions into sub-questions based on a single compositionality type, which limits their effectiveness for questions involving multiple compositionality types. Additionally, these methods suffer from inefficient retrieval, as complex questions often contain abundant information, leading to the retrieval of irrelevant information inconsistent with the query's intent. In this work, we propose a novel question decomposition framework called TRQA for multi-hop question answering, which addresses these limitations. Our framework introduces a reasoning tree (RT) to represent the structure of complex questions. It consists of four components: the Reasoning Tree Constructor (RTC), the Question Generator (QG), the Retrieval and LLM Interaction Module (RAIL), and the Answer Aggregation Module (AAM). Specifically, the RTC predicts diverse sub-question structures to construct the reasoning tree, allowing a more comprehensive representation of complex questions. The QG generates sub-questions for leaf-node in the reasoning tree, and we explore two methods for QG: prompt-based and T5-based approaches. The IR module retrieves documents aligned with sub-questions, while the LLM formulates answers based on the retrieved information. Finally, the AAM aggregates answers along the reason tree, producing a definitive response from bottom to top.",
  "full_text": "Tree-of-Reasoning Question Decomposition for Complex Question Answering with\nLarge Language Models\nKun Zhang1,2*, Jiali Zeng3, Fandong Meng3, Yuanzhuo Wang1,2,4† , Shiqi Sun4,\nLong Bai2, Huawei Shen1, Jie Zhou3\n1CAS Key Laboratory of AI Security, Institute of Computing Technology, Chinese Academy of Sciences\n2School of Computer Science and Technology, University of Chinese Academy of Sciences\n3Pattern Recognition Center, WeChat AI, Tencent Inc, China\n4Big Data Academy, Zhongke\n{zhangkun18z, wangyuanzhuo}@ict.ac.cn, lemonzeng@tencent.com\nAbstract\nLarge language models (LLMs) have recently demonstrated\nremarkable performance across various Natual Language\nProcessing tasks. In the field of multi-hop reasoning, the\nChain-of-thought (CoT) prompt method has emerged as a\nparadigm, using curated stepwise reasoning demonstrations\nto enhance LLM’s ability to reason and produce coherent\nrational pathways. To ensure the accuracy, reliability, and\ntraceability of the generated answers, many studies have in-\ncorporated information retrieval (IR) to provide LLMs with\nexternal knowledge. However, existing CoT with IR meth-\nods decomposes questions into sub-questions based on a sin-\ngle compositionality type, which limits their effectiveness for\nquestions involving multiple compositionality types. Addi-\ntionally, these methods suffer from inefficient retrieval, as\ncomplex questions often contain abundant information, lead-\ning to the retrieval of irrelevant information inconsistent with\nthe query’s intent. In this work, we propose a novel question\ndecomposition framework called TRQA for multi-hop ques-\ntion answering, which addresses these limitations. Our frame-\nwork introduces a reasoning tree (RT) to represent the struc-\nture of complex questions. It consists of four components:\nthe Reasoning Tree Constructor (RTC), the Question Genera-\ntor (QG), the Retrieval and LLM Interaction Module (RAIL),\nand the Answer Aggregation Module (AAM). Specifically,\nthe RTC predicts diverse sub-question structures to construct\nthe reasoning tree, allowing a more comprehensive represen-\ntation of complex questions. The QG generates sub-questions\nfor leaf-node in the reasoning tree, and we explore two meth-\nods for QG: prompt-based and T5-based approaches. The\nIR module retrieves documents aligned with sub-questions,\nwhile the LLM formulates answers based on the retrieved in-\nformation. Finally, the AAM aggregates answers along the\nreason tree, producing a definitive response from bottom to\ntop. We evaluate our proposed framework on four bench-\nmark datasets. The experimental results demonstrate that our\nproposed methods consistently outperform baseline methods\noutperform strong baselines by a substantial margin across all\ndatasets.\n*The work is done during internship at WeChat AI.\n†Corresponding Author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nIntroduction\nRecently, Large Language Models (LLMs) have exhibited\nan impressive capability for question-answering tasks, par-\nticularly in scenarios where resolving complex questions re-\nquires a multi-hop reasoning process (Touvron et al. 2023;\nOpenAI 2023). For example, Wei et al. (2022) proposes\nfew-shot chain-of-thought (CoT) prompting, which enables\nLLMs to generate intermediate reasoning steps explicitly be-\nfore predicting the final answer with a few manual step-by-\nstep reasoning demonstration examples. However, the intri-\ncate nature of complex questions remains a significant chal-\nlenge. To address this, Kojima et al. (2022) introduces zero-\nshot CoT, which eliminates the need for manually crafted\nexamples in prompts by appending “Let’s think step by step”\nto the target problem fed to LLMs. This simple prompt-\ning strategy surprisingly yields performance similar to few-\nshot CoT without requiring any manual clues. To mitigate\nthe issue of generating incorrect information (i.e., halluci-\nnation) while retaining real-time knowledge, several studies\nhave incorporated information retrieval (IR) techniques into\nLLM reasoning (Press et al. 2022; Khattab et al. 2022; Wang\net al. 2023; Kandpal et al. 2023; Azamfirei, Kudchadkar, and\nFackler 2023). This integration of IR significantly improves\nthe quality of LLM-generated answers.\nAlthough COT with IR methods has achieved great suc-\ncess in solving complex questions, they still face two chal-\nlenges. Challenge 1: Multi-compositional Types of Sub-\nquestions. The interrelations among the sub-questions of\ncomplex questions can be categorized into two distinct\nclasses (Pan et al. 2020): 1) nest-type: the solution of the\nsub-question depends on the answers of the previous sub-\nquestion; 2) branch-type: the solution needs to summarize\nor compare the answers of other sub-questions. As shown\nin Figure, previous methods mainly consider single com-\npositional type questions,i.e., nested-type questions through\nthe formulation of a sequence of prompt strategies. How-\never, they tend to disregard branch-type questions and multi-\ncompositional type questions, resulting in diminished effi-\ncacy when tackling such questions. The hallucinations en-\ncountered during decomposition also exacerbate this disad-\nvantage. Challenge 2: Inefficient Retrieval.Complex ques-\ntions are often lengthy and contain abundant information,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19560\nPrompt: Please decompose the question into sub-\nquestions.Let’s think step by step.\nAnswer:To find the answer to your question,we can \ndecompose into three sub-questions.\nQ1:What is the Four Falls Border Crossing?\nQ2:Is the Four Falls Border Crossing located in a specific \ncity in the United States?\nQ3:  What was the population of that city according to \nthe 2010 census?\nExample 1\nExample 2\nAccording to the 2010 census, what was the population of \nthe city on the United States side of the Four Falls Border \nCrossing?\nCoT\nUser\nPrompt: Please decompose the question into sub-\nquestions.Let’s think step by step.\nAnswer: To find the answer to your question,we can \ndecompose into three sub-questions.\nQ1: Who is Scott Allen Miller?\nQ2: What radio station does he work for?\nQ3:  Who owns that radio station?\nWho owns the radio station where Scott Allen Miller has \nrecently been the morning drive host and program \ndirector? User\nPrompt: Please retrieve the paragraph(s) relevant to \nthese sub-questions from the support facts.\nAnswer:Pembina is a city in Pembina County, North \nDakota, United States., The population was 592 at the \n2010 census.... \nCoT\nCoT\nFigure 1: Example 1 and 2 are two bad instances in LLM\nwith IR methods.\nwhich can inadvertently result in the retrieval of irrelevant\ninformation inconsistent with the query’s intent. When such\ninappropriate information is fed to the QA model, it intro-\nduces noise into the generated answer. Some efforts (Press\net al. 2022) have been made to tackle this problem by inte-\ngrating verification modules. However, it’s notable that the\nincreased frequency of application programming interface\n(API) calls leads to a rapid increase in time consumption.\nTo address the above challenges, we focus on improv-\ning the question decomposition process. We propose an in-\nnovative structure, named Reasoning Tree (RT), to better\nmodel the complex question’s structure. The RT contains\nthree types of nodes: root node, middle node, and leaf node.\nAs shown in Figure 3, the root node is a question node that\nrepresents the whole question. The middle node represents\nthe intermediate dependency parse subtree decomposed by\nthe nest and branch relation, while the leaf node represents\nthe inseparable question substructure.\nTo generate the reasoning tree and formulate the answer,\nwe propose a novel framework called TRQA. This frame-\nwork is a composite of four integral components, namely the\nReasoning Tree Constructor (RTC), the Question Generator\n(QG), the Retrieval And LLM Interaction Module (RAIL),\nand the Answer Aggregation Module (AAM). To train RTC,\nwe use T5 as the basis model and train it to predict the loca-\ntion of labels [NEST] and [BRANCH]. For each leaf node in\nthe reasoning tree, QG is designed to translate the substruc-\nture into a complete sub-question. To realize QG, we explore\ntwo methods: prompt-based and T5-based. RAIL retrieves\nsimilar documents align with each sub-question, and LLM\nformulates answers by combining retrieval documents and\nquestions. Finally, AAM aggregates answers along the rea-\nsoning tree to produce the definitive response from bottom\nto top.\nIn general, our main contributions are listed as follows:\n• We propose a novel framework named TRQA, which\nleverages question decomposition to construct a global\nreasoning structure and generates reliable answers\nthrough interaction with LLM and IR.\n• We design a novel approach, the structure-driven ques-\ntion decomposition model, which employs dependency\nparse trees to augment the process of reasoning structure\ngeneration.\n• We verify the effectiveness of the proposed framework\non four widely-used datasets and the experimental results\nshow that our proposed methods consistently outperform\nbaseline methods across all benchmarks by a large mar-\ngin.\nPreiliminary\nIn this section, we propose the reasoning tree(RT), a tree-\nbased structure, to model the decomposition structure of\ncomplex questions. The detail of the definition is shown be-\nlow.\nDefinition\nGiven a question Q, the RT of Q is a tree containing two\ntypes of nodes: the middle node and the leaf node. As shown\nin the example in the right part of Figure 2, the root question\nnode indicates the original question, whose answer is con-\nstrained by the middle node M1. Note that M1 contains a\nplaceholder “#1”, indicating the answer of the subtree M2.\nMeanwhile, the middle node M2 points to two leaf nodes,\nL1 and L2. The structure of RT enables it to represent the\ncombinations of multiple compositionality types. We also\nprovide an equivalent linear representation of RT under the\ntree illustration by introducing two separators, i.e., [NEST],\nand [BRANCH]. [NEST] means that the placeholder of the\ncurrent question is the answer of the subtree. [BRANCH]\nmeans that the placeholder of the current question needs to\nconduct some function to aggregate the answers of each sub-\ntree, including intersection, union, comparison, etc.\nMethodology\nAs shown in Figure 2, we present the design of our frame-\nwork, denoted as TRQA, which comprises four main com-\nponents: reasoning tree constructor, question generator, re-\ntrieval and interaction with LLM, and answer aggregation\nmodule. RTC decomposes a question into the reasoning\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19561\nQuestion: \nWho owns the radio station where Scott Allen \nMiller has recently been the morning drive host \nand program director?\nDependency Pase Tree: \nM2:[BRANCH]#1 where Scott Allen Miller \nhas been the morning drive host? \nL1: #1 where Scott Allen Miller \nhas been the morning drive host? \nL1’: which ration station where \nScott Allen Miller has been the \nmorning drive host? \nL2’: which ration station \nwhere Scott Allen Miller has \nbeen the program director?\nRetrieval And Interaction with LLM\nInput\nAnswer Aggeragation Module\nA1’: BBC Radio 1, KEXP A2’:  BBC Radio 1,  NPR\nA4’:  [NEST_AGGREGATION]\nWho owns BBC Radio 1?\nA0:  UK government\nBBC Radio 1, A popular \nBritish radio station …\nS22\nS21 S31\nS32 NPR (National Public Radio): \nA US-based network of…\nL2:#1 where Scott Allen Miller \nhas been the program director?\nQuestion Generator\nGenerate answer by LLM\nM1:[NEST] Who owns #1? \nROOT:Who owns the radio station where Scott Allen Miller has \nrecently been the morning drive host and program director?\nA3’: [BRANCH_AGGREGATION] \nBBC Radio 1\nReasoning Tree Constructor\nFigure 2: The overall architecture of our framework TRQA.\ntree devised by a dependency parse tree. QG translates the\nleaf-node substructure of the reasoning tree into a question.\nRAIL retrieval similar documents and generate the answer.\nAAM aggregates answers following the reasoning Tree.\nReasoning Tree Constructor\nAs mentioned above, there are mainly two types of rela-\ntions between sub-questions, i.e., nest and branch, which is\nalso the basis relation in the reasoning tree. The dependency\nparse tree reveals the dependency relation between tokens\nin the relation, which can assist us to predict the relation\nbetween different sub-structures. Thus, we introduce the de-\npendency parse tree to decompose the question and generate\nthe reasoning tree. We collect some labeled data according\nto the dependency parse tree and train it with T5.\nData Collection To train a model for constructing the\nreasoning tree, we construct a dataset called RTrees, with\n7,00 samples from existing muti-hop question-answering\ndatasets. During the annotation process, we ask annotators\nto insert [NEST] and [BRANCH] into the dependency parse\ntree, divide them into different subtrees, and then use a\ndepth-first search algorithm to flatten them into sequences.\nWe take some examples in the appendix file.\nTraining We used T5 as our basic model and trained a\nmodel to predict the location of separators. After obtaining\nthe position of the separator, we divide the dependency parse\ntree into different substructures based on its position in the\ntree. Finally\nQuestion Generator After obtaining the inference tree,\nwe rewrite the substructure into a natural language question\nAlgorithm 1: Processing a Complex Question\nQuestion: q; Dependency Parse Tree: dpt;\nprocedure PROCESS (complex question)\nRT ← RTC.ConstructTree(q,dpt)\nleaf substructures ← RTC.GetLeafNode(RT)\nfor each substructure in leaf substructures do\nquestion ← QG.GenerateQuestion(substructure)\nsimilar documents ← RAIL.Retrieve(question)\nanswer ← RAIL.GenerateAnswer(similar documents)\nreasoning tree.PlaceAnswer(substructure, answer)\nend for\naggregated answers ← AAM(reasoning tree)\nreturn aggregated answers\nend procedure\nbased on the substructures of the leaf nodes, including ques-\ntions and corresponding analysis tree subtrees. Given the de-\npendency sub-treeCi, and part of questionSi, we design two\nmethods to translate the Ci into corresponding question qi.\nPrompt-based Question Generator We have designed a\nprompt to rewrite the dependency analysis tree subtree and\ncorresponding question section into a natural language ques-\ntion, which is shown as follows,\n”This is a question Q, and there is a part of question.\nIts dependency parse is Ci, and the corresponding token in\nquestion is Si, please convert it into a complete question”\nIn this way, we can transform the substructure into a ques-\ntion.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19562\nT5-based Question Generator We collect a few human-\nlabeled datasets and train a T5-based model to generate the\nquestion. The input is the dependency parse subtree and\npart-of-question and the output is the complete question. The\ndetail is shown below.\nData Collection To train a model for question generation,\nwe construct a dataset called D2Q, with 4,00 samples. Dur-\ning the annotation process, we ask annotators to write the\nquestion according to the substructure. We take some exam-\nples in the appendix file.\nTraining We take the linearized dependency parse sub-\ntree and corresponding question sequence as input and the\nannotated question as output.\nqi = T5(Ci) (1)\nwhere qi is denoted as the ith sub question.\nRetrieval And Interactive with LLM\nIn the context of each individual question, the operational\nworkflow encompasses the relay of generated sub-questions\nfrom the Large Language Model (LLM) to the Information\nRetrieval (IR) system. In this component, the IR mecha-\nnism assumes a dual function encompassing confirmation\nand augmentation. Concretely, for each sub-question qi, the\nIR module assumes the role of verification, furnishing per-\ntinent and complementary data. This serves not only to cor-\nroborate the sub-question but also to enrich its content. Then\nthe retrieval information incorporated with the question is\nsubsequently fed back to the LLM, effectively facilitating\nthe production of accurate and reliable sub-answers.\nFurthermore, this interaction can contribute to the com-\npleteness of the content generation process within LLM.\nOwing to the correlation between IR and LLM, the for-\nmer evolves into a repository of collated documents, metic-\nulously documenting the records acquired from every node\nwithin the reasoning tree architecture. This approach height-\nens both the traceability and reliability of the content gener-\nated from LLM. Through the retrieval interaction between\neach sub-question and the IR system, the most pertinent\ndocument for each sub-question, denoted as di, can be\nsourced as the supporting document for the corresponding\nsub-question qi.\nAnswers Aggregation Module In AAM, we aggregate\nthe answers of leaf nodes along the reasoning tree from\nbottom to top. When the answer is aggregated from\nthe child node to the parent node, we designed two\naggregation functions, i.e. NEST\nAGGREGATION and\nBRANCH AGGREGATION. They correspond to nest and\nbranch tags in the reasoning tree. We will introduce these\ntwo operations in detail below.\nNEST AGGREGATIONWe use the answer of the child\nnode of this node directly as the placeholder. For example\nin Figure 2, in the answer aggregation module, the answer\nof A3’ is directly used to replace the value of placeholder in\nM1. Then we use QG to translate the current node substruc-\nture into a question. RAIL is used to generate the answer of\nthe current node.\nBRANCH\nAGGREGATION We collect answers from\ndifferent child nodes and replace them with the position in\nthe current substructure. We design a prompt and directly\nuse LLM to generate the answer. The prompt is shown be-\nlow,\nPrompt: We have several answers, I want to get the fi-\nnal answer from “Answer\n1 [operation] Answer 2 [opera-\ntion]”, [operation] represents aggregation functions, like in-\ntersection, cooperation, etc. In this way, we get the final an-\nswer for the current node.\nWe follow the reasoning tree from bottom to top and grad-\nually aggregate the answers based on these two aggregation\nmethods to obtain the final answer\nExperiment\nIn this section, we present a comprehensive evaluation of\nour proposed framework, TRQA, using four extensively uti-\nlized benchmark datasets. The obtained results validate the\neffectiveness of our method.\nDatasets and Preprocessing\nWe select four complex multi-hop question-answering\ndatasets, HotpotQA(Yang et al. 2018), HyBridQA(Chen\net al. 2020), Musique(Trivedi et al. 2022), and WikiMulti-\nHopQA (WMHQA)(Ho et al. 2020). The details are shown\nbelow.\nHotpotQA The HotpotQA dataset stands as a pivotal\nbenchmark within the field of Natural Language Processing\n(NLP), designed to evaluate models’ capabilities in reason-\ning and multi-hop question-answering tasks. It boasts an im-\npressive repository of over 110,000 diverse question-answer\npairs, meticulously crafted to span a wide spectrum of do-\nmains and topics. We focus on its full wiki setting.\nHyBridQA is an extensive question-answering dataset,\nintegrating structured Wikipedia tables and related free-form\ntext corpora, designed to necessitate reasoning over both\nforms of information. It includes around 70,000 question-\nanswer pairs aligned with 13,000 unique Wikipedia tables.\nMusique is a challenging multi-hop question answering\ndataset comprising 25,000 questions with 2-4 hops, devel-\noped via a systematic bottom-up approach of selecting in-\nterconnected single-hop questions.\nWikiMultiHopQA is an extensive and high-quality\nmulti-hop question-answering dataset created utilizing\nWikipedia and Wikidata, with an emphasis on providing ex-\nhaustive explanations from question to answer that enrich\nthe understanding of predictions. The dataset introduces nat-\nural questions that demand multi-hop reasoning, crafted us-\ning logical rules embedded within the knowledge base (KB).\nBaselines\nThe baseline models can be categorized into two classes.\nThe first class focuses on designing prompts to improve the\nreasoning ability of LLM (CoT (Wei et al. 2022), Auto-\nCoT(Zhang et al. 2022b), Recite-and-answer(Sun et al.\n2022), and Least-to-Most(Zhou et al. 2022)). And the sec-\nond class introduces IR to LLM (Self-Ask(Press et al. 2022),\nPlan-And-Solve(Wang et al. 2023), React(Yao et al. 2022)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19563\nApproach HotpotQA HyBr\nidQA Musique WMHQA\nEM Recall EM Recall\nEM Recall EM Recall\nDirect prompting 25.4 36.2\n12.8 19.2 6.0 8.2 25.8 28.4\nAUTO-COT 36.8 45.8 18.2 25.6 10.6 13.2 29.2 32.2\nCOT 38.2 48.6 16.2 21.8 9.4 11.0 30.4 34.2\nRecite-and-answer 36.6 38.8 16.6 19.8 11.0 13.6 32.6 36.2\nSelf-Ask w/o IR 34.4 36.2 17.4 22.2 11.2 14.4 35.8 39.6\nLeast-to-Most 34.2 38.6 26.4 32.6 11.6 13.4 32.8 36.6\nPlan-And-Solve 37.4 41.6 27.8 30.2 13.4 16.8 34.8 37.8\nLeast-to-Most w/ IR 42.6 44.2 30.2 35.4 15.2 18.2 32.8 36.6\nSelf-Ask 40.4 49.8 24.4 30.2 14.4 15.6 39.6 42.6\nPlan-And-Solve w/ IR 41.6 45.6 29.2 33.6 15.0 17.8 42.4 46.2\nReact 44.6 48.2 32.6 35.6 15.6 18.4 40.4 43.6\nDSP 53.0 56.8 33.2 34.8 16.2 20.8 43.2 46.8\nTRQA 61.2 62.4 35.2\n42.4 24.2 26.8 52.6 54.8\nTRQA w/o IR 55.4 60.2 28.6 37.2 18.6 20.4 44.8 46.8\nTRQA w/o AAM 59.2 69.2 31.8 38.4 22.4 25.2 49.2 50.6\nTRQA w/ PQG 60.3 66.8 33.2 39.6 20.2 24.6 48.2 50.2\nTable 1: EM and Recall results on four benchmark datasets (%).\nApproach HotpotQA HyBridQA Musique\nEM Recall EM Recall EM Recall\nTRQA 64.2 72.2 35.2 42.4 24.2 26.8\nTRQA w/ cluenet 58.2 67.4 27.8 36.4 17.8 18.8\nTRQA w/ HSP 56.8 66.2 26.2 34.8 16.4 19.2\nTRQA w/ DecompRC 52.6 64.8 24.4 30.2 14.2 17.2\nTable 2: EM and Recall results on HotpotQA, HyBridQA adn Musique dataset (%).\nand DSP (Khattab et al. 2022)), aims at retaining real-time\nknowledge and alleviating hallucination in the generated an-\nswer.\nMetrics\nWe employ two key metrics to evaluate the performance of\nour large model’s generated responses: Cover-EM (EM) and\nRecall. The Cover-EM metric evaluates the percentage of\ncorrect answers that appear as substrings within the model’s\ngenerated responses. This metric effectively measures the\nmodel’s ability to generate comprehensive answers by deter-\nmining if the correct answer is present in its output. On the\nother hand, Recall is a commonly used metric to evaluate the\nmodel’s ability to retrieve relevant information. Specifically,\nit quantifies the ratio of correctly identified relevant items by\nthe model to the total number of relevant items.\nHuman Annotation\nWe invite four graduate students, who majored in Computer\nScience and are familiar with natural language processing,\nto annotate the dataset. Before annotation, they are informed\nof the detailed instructions with clear examples. For the RTC\ntask, the inter-annotator agreement score is 0.87, and for the\nQG task, the inter-annotator agreement score is 0.76.\nImplementation Details\nWe harnessed the gpt-3.5-turbo, a prominent large language\nmodel accessible through OpenAI’s API, for our experimen-\ntation. Additionally, we employed ColBERTv2 as our re-\ntrieval model, following (Xu et al. 2023). Our baselines,\nwhich incorporate information retrieval, were subjected to\nidentical experimental settings as our proposed framework.\nGiven that the majority of baseline methods were tested us-\ning the text-davinci-002 model, we reenacted their experi-\nments on the gpt-3.5-turbo model, adhering to the configura-\ntions outlined in their respective publications, consequently\nyielding enhanced performance.\nMain Results\nThe performance of and baselines on the multi-hop\nquestion-answering task are shown in Table 1. We compare\nit with recent competitive baselines in the setting without\nIR. TRQA w/o IR outperforms all baselines based on CoT\nmethods, including CoT, Auto-CoT, CoT-SC, and Recite-\nand-answer, which indicates that constructing the reasoning\ntree, i.e., a global reasoning process(Xu et al. 2023), is better\nthan just giving intermediate reasoning results. The results\nof Self-Ask, Plan-And-Solve w/IR, React, and DSP outper-\nform CoT methods, which indicates that IR can introduce\nexternal knowledge to assist reasoning and answer complex\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19564\nHyBridQA HotpotQA Musique\nSelf-Ask 4.28 4.32 5.68\nDSP 4.02 4.26 5.36\nLeast-to-Most 3.66 3.88 4.26\nTRQA 2.68 3.22 3.42\nTable 3: Efficiency analysis.\nquestions. Besides, TRQA w/o IR outperforms Self-Ask w/o\nIR and Plan And Solve, which indicates that it is more ef-\nfective to construct the reasoning tree and generate answers\nalong it than generate an inference chain and answer sub-\nquestions step by step. TRQA outperforms TRQA w/ PQG,\nwhich shows that our T5-based question generator is more\neffective in translating substructure into the complete ques-\ntion than LLM-based.\nAblation Study To evaluate the effectiveness of each\nmodel module, we perform ablation studies where we re-\nmove the IR module and AAM. The results are shown in\nTable ??.\nAfter removing each module, the performance of the\nmodel deteriorated. In Table ??, the results of (TRQA w/o\nIR) denote the model that removes the IR module. This in-\ndicates that IR interacting with each node of the reasoning\ntree ensures the reliability and accuracy of LLM-generated\nanswers. The results of (TRQA w/o AAM) denote the model\nthat removes the AAM module. We directly use a prompt to\ncombine answers from leaf nodes and generate the final an-\nswer by LLM. The result shows the effectiveness of AAM,\nwhich can aggregate the answers in a reasonable way.\nThe Effectiveness of Different QDT Method To demon-\nstrate the effectiveness of our question decomposition\nmethod, we perform an ablation study by removing QDT\nand replacing it with different question decomposition meth-\nods. We choose cluenet(Huang et al. 2023), HSP(Zhang\net al. 2019), and DecompRC(Min et al. 2019) as the con-\ntrastive question decomposition model. Results show that\nQDT is superior to other methods on EM. This suggests\nthat our proposed QDT can further promote the generative\nmethod. Besides, we also find that none of the examples be-\ncome worse after the incorporation of IR, which means IR\nis stable and safe as an external module for the generative\ndecomposition method.\nEfficiency Analysis We further analyze the difference in\nrunning efficiency between TRQA and baselines from the\nperspective of the average number of interactions between\nIR and LLM per question. Table ?? shows the results of\nthe analysis on four multi-hop QA datasets. Based on Ta-\nble ?? and Table ??, TRQA not only exhibits superior task\nperformance but also has the least number of interactions\nwith API, consequently entailing the least time cost.\nCase Study: TRQA vs New Bing in Tracing Following\n(Xu et al. 2023), we conduct a comparative analysis between\nthe performance of TRQA and New Bing in the task of at-\ntributing references to generated content, as illustrated in our\ncase study (Figure 3). Notably, TRQA shows a finer-grained\nability to attribute references to each segment of knowledge\nengaged in the reasoning process, which corresponds to each\ncorrect node within the TRQA framework. In contrast, the\nreferencing provided by New Bing exhibits gaps, failing to\nencompass the entirety of the relevant knowledge. There are\nalso some instances where New Bing is unable to locate cer-\ntain knowledge segments.\nRelated Work\nChain-of-Thought Prompting (Wei et al. 2022) introduces\na method termed Chain-of-Thought (CoT), aiming to en-\nhance the reasoning capabilities of large language mod-\nels (LLMs). CoT employs a few-shot paradigm to enable\nLLMs to generate intermediate reasoning outcomes during\nthe solution of intricate problems, thereby ameliorating their\nreasoning aptitude. By utilizing the guiding prompt ”Let’s\ndo it step by step,” CoT achieves promising zero-shot per-\nformance. A derivative of this approach, Auto-CoT, lever-\nages language models to automatically craft few-shot learn-\ning examples for the CoT framework (Wei et al. 2022).\nVarious studies have delved into diverse facets of CoT,\nencompassing concerns like self-consistency (Wang et al.\n2022), utilization of moderate-sized models (Zelikman et al.\n2022), and selection (Fu et al. 2022). Additionally, certain\nmethodologies iteratively employ LLMs to break down in-\ntricate queries into manageable sub-questions, systemati-\ncally addressing them. Notable instances include Least-to-\nMost (Drozdov et al. 2022), Dynamic Least-to-Most, Self-\nAsk (Press et al. 2022), and DSP (Khattab et al. 2022).\nThis body of work serves as a catalyst for our proposed\napproach, namely the Tree of Reasoning. Preceding studies\nhave predominantly concentrated on furnishing intermediate\nreasoning outputs, segmenting intricate queries, and step-\nwise resolution of sub-questions. These efforts largely focus\non local sub-questions resolution, without adequate atten-\ntion to the planning of a global reasoning chain. Approaches\nlike AgentGPT and PS initially formulate sub-questions\nand subsequently address them, revealing their limitations\nwhen subsequent sub-question solutions hinge on insights\ngarnered from preceding ones. Such a scenario commonly\narises in knowledge-intensive tasks like multi-hop question-\nanswering (Wang et al. 2023; Zhang et al. 2023b,a).\nQuestion Decomposition Question decomposition plays\na pivotal role in generating an ungrounded query graph,\nserving as a valuable aid in structure disambiguation. Such\nstructural clarity aids in steering the QA system away from\ninefficient path traversal within relations (Chen et al. 2021).\nThe realm of question decomposition encompasses three pri-\nmary methodologies:\n(1) Splitting-based Methods: Strategies like SplitQA (Tal-\nmor and Berant 2018) and DecompRC (Min et al. 2019) har-\nness pointer networks to bifurcate a question into two seg-\nments. While effectively preserving the core essence of the\noriginal inquiries, these approaches exhibit a need for in-\ncreased flexibility when dealing with intricate structures.\n(2) Generative Methods(Zhang et al. 2019, 2022a): These\nmethodologies introduce heightened adaptability and ex-\npansibility, amenable to a wide range of target formats.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19565\nThe band that released Women and Children First had \nhow many number ones ?\nThe band that released Women and Children First is Van \nHalen[1].Van Halen had thirteen number ones on the \nBillboard Mainstream Rock chart.[2]\n[1] Women and Children First is the third studio album by \nAmerican hard rock band Van Halen, released on March 26, \n1980 on Warner Bros....\n[2] As of 2007, Van Halen has sold 75 million albums \nworldwide and have had thirteen No. 1 hits on the \nBillboard Mainstream Rock chart. ....\nQuestion\nMethod\nWhich country contains Conran in the residence state \nof the author of Little Gidding ?\nThe author of Little Gidding is T.S. Eliot [1]. Eliot was born in St. \nLouis, Missouri, United States. However, he later moved to \nEngland and became a British citizen [2].Conran is located about \nten miles southwest of New Madrid on U.S. the country that \ncontains Conran in the residence state of T.S. Eliot is the United \nStates.\n[1] Little Gidding is the fourth and final poem of T. S. Eliot's Four \nQuartets, a series of poems that discuss time, perspective, \nhumanity, and salvation...\n[2] Conran is an unincorporated community in eastern New \nMadrid County, Missouri, United States.It is located \napproximately ten miles southwest of New Madrid on U.S. Route \n61. ....\nWhat group of languages includes the old version of \nthe language that the Quran was first translated in?\nThe specific language that the Quran was first translated \ninto is Persian [1]. The group of languages that includes the \nold version of the language that the Quran was first \ntranslated in is the Western Iranian languages [2].\n[1] The first fully attested complete translations of the \nQuran were done between the 10th and 12th centuries in \nPersian. ...\n[2] The Western Iranian languages are a branch of the \nIranian languages, attested from the time of Old Persian (6th \ncentury BC) and Median. ....\nTRQA\nNew Bing\nThe band that released the album “Women and Children \nFirst” is Van Halen. According to my sources, Van Halen \nhad one number one hit on the Billboard Hot 100 chart with \ntheir song “Jump” in 1984 [1]. Is there anything else you \nwould like to know?\n[1] Eddie Van Halen rode the top of the Billboard charts \nalongside his namesake band Van Halen, scoring five No. 1 \nalbums on the Billboard 200 chart, and more than 20 hits \non the Billboard Hot 100 songs…\nI’m sorry but I couldn’t find any information about Conran. \nHowever, Little Gidding is a poem by T.S. Eliot. It is the \nfourth and final poem of his Four Quartets series [1]. The \npoem refers to a small Anglican community in Little Gidding \nin Huntingdonshire, England [2]. I hope this helps.\n[1] Little Gidding is the fourth and final poem of T. S. Eliot's \nFour Quartets, a series of poems that discuss time, \nperspective, humanity, and salvation...\n[2] The title refers to a small Anglican community in Little \nGidding in Huntingdonshire, established by Nicholas Ferrar ....\nThe first complete translation of the Quran was into Persian, \nwhich is a member of the Iranian branch of the Indo-Iranian \nlanguage family [1]. The first chapter of the Quran, Al-Fatiha, \nwas translated from Arabic to Persian by Salman the Persian in \nthe 7th-10th centuries [2]. Is there anything else you would like to \nknow? \n[1] Persian, also known by its endonym Farsi, is a Western Iranian \nlanguage belonging to the Iranian branch of the Indo-Iranian \nsubdivision of the Indo-European languages. \n[2] Salman the Persian translated the first chapter of the Qur'an, \nAl-Fatiha, from Arabic to Persian ....\nFigure 3: Case study of TRQA and New Bing in marking references to supporting documents.\nHSP (Zhang et al. 2019) employs a Seq2Seq model inte-\ngrated with a copy mechanism to engender sub-questions.\nHowever, these approaches remain confined to decompos-\ning questions into two segments, thereby failing to assure the\nunchanged retention of sentence semantics. Consequently, a\npotential exists for token loss or unexpected token genera-\ntion, which in turn could compromise the semantic coher-\nence of the input question, complicating performance evalu-\nation.\n(3) Rule-based Methods: Notably, EDG (Hu et al. 2021)\nexemplifies this category. EDG undertakes an iterative trans-\nformation of the constituency tree into an entity-centric\ngraph, propelled by meticulously designed rules. While pro-\nficient in handling multiple forms of compositional diversity,\nthis approach tends to rely heavily on constituency parsing,\nthus potentially limiting its coverage.\nConclustion\nIn this study, our focus centers on the utilization of question\ndecomposition as a potent mechanism for addressing the\nchallenge of answering complex questions through Large\nLanguage Models (LLMs). To achieve this goal, we intro-\nduce a novel structure, named the Reasoning Tree (RT),\nwhich represents the global reasoning structure of the ques-\ntion reasoning process. To construct the reasoning tree, we\npropose a novel framework named TRQA, which leverages\nquestion decomposition to construct the reasoning tree and\ngenerates reliable answers through interaction with LLM\nand IR. We design a novel approach, the structure-driven\nquestion decomposition model, which employs dependency\nparse trees to augment the process of reasoning structure\ngeneration. We verify the effectiveness of the proposed\nframework on four widely-used datasets and the experimen-\ntal results show that our proposed methods consistently out-\nperform baseline methods across all benchmarks by a large\nmargin.\nAcknowledgements\nThanks to reviewers for their helpful comments on this\npaper. This paper is funded by the National Natural Sci-\nence Foundation of China (No.62172393, U1836206, and\nU21B2046), Zhongyuanyingcai program-funded to central\nplains science and technology innovation leading talent pro-\ngram (No.204200510002), Major Public Welfare Project of\nHenan Province (No.201300311200).\nReferences\nAzamfirei, R.; Kudchadkar, S. R.; and Fackler, J. 2023.\nLarge language models and the perils of their hallucinations.\nCritical Care, 27(1): 1–2.\nChen, W.; Zha, H.; Chen, Z.; Xiong, W.; Wang, H.; and\nWang, W. 2020. Hybridqa: A dataset of multi-hop ques-\ntion answering over tabular and textual data. arXiv preprint\narXiv:2004.07347.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19566\nChen, Y .; Li, H.; Hua, Y .; and Qi, G. 2021. Formal\nquery building with query structure prediction for complex\nquestion answering over knowledge base. arXiv preprint\narXiv:2109.03614.\nDrozdov, A.; Sch¨arli, N.; Aky¨urek, E.; Scales, N.; Song, X.;\nChen, X.; Bousquet, O.; and Zhou, D. 2022. Compositional\nsemantic parsing with large language models.arXiv preprint\narXiv:2209.15003.\nFu, Y .; Peng, H.; Sabharwal, A.; Clark, P.; and Khot, T.\n2022. Complexity-based prompting for multi-step reason-\ning. arXiv preprint arXiv:2210.00720.\nHo, X.; Nguyen, A.-K. D.; Sugawara, S.; and Aizawa,\nA. 2020. Constructing a multi-hop QA dataset for com-\nprehensive evaluation of reasoning steps. arXiv preprint\narXiv:2011.01060.\nHu, X.; Shu, Y .; Huang, X.; and Qu, Y . 2021. EDG-Based\nQuestion Decomposition for Complex Question Answering\nover Knowledge Bases. In Hotho, A.; Blomqvist, E.; Dietze,\nS.; Fokoue, A.; Ding, Y .; Barnaghi, P.; Haller, A.; Dragoni,\nM.; and Alani, H., eds., The Semantic Web – ISWC 2021,\n128–145. Cham: Springer International Publishing. ISBN\n978-3-030-88361-4.\nHuang, X.; Cheng, S.; Shu, Y .; Bao, Y .; and Qu, Y .\n2023. Question Decomposition Tree for Answering Com-\nplex Questions over Knowledge Bases. arXiv preprint\narXiv:2306.07597.\nKandpal, N.; Deng, H.; Roberts, A.; Wallace, E.; and Raffel,\nC. 2023. Large language models struggle to learn long-tail\nknowledge. In International Conference on Machine Learn-\ning, 15696–15707. PMLR.\nKhattab, O.; Santhanam, K.; Li, X. L.; Hall, D.; Liang,\nP.; Potts, C.; and Zaharia, M. 2022. Demonstrate-\nSearch-Predict: Composing retrieval and language mod-\nels for knowledge-intensive NLP. arXiv preprint\narXiv:2212.14024.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199–22213.\nMin, S.; Zhong, V .; Zettlemoyer, L.; and Hajishirzi, H. 2019.\nMulti-hop reading comprehension through question decom-\nposition and rescoring. arXiv preprint arXiv:1906.02916.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPan, L.; Chen, W.; Xiong, W.; Kan, M.-Y .; and Wang, W. Y .\n2020. Unsupervised multi-hop question answering by ques-\ntion generation. arXiv preprint arXiv:2010.12623.\nPress, O.; Zhang, M.; Min, S.; Schmidt, L.; Smith, N. A.;\nand Lewis, M. 2022. Measuring and narrowing the com-\npositionality gap in language models. arXiv preprint\narXiv:2210.03350.\nSun, Z.; Wang, X.; Tay, Y .; Yang, Y .; and Zhou, D. 2022.\nRecitation-augmented language models. arXiv preprint\narXiv:2210.01296.\nTalmor, A.; and Berant, J. 2018. The web as a knowledge-\nbase for answering complex questions. arXiv preprint\narXiv:1803.06643.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\nA. 2022. MuSiQue: Multihop Questions via Single-hop\nQuestion Composition. Transactions of the Association for\nComputational Linguistics, 10: 539–554.\nWang, L.; Xu, W.; Lan, Y .; Hu, Z.; Lan, Y .; Lee, R. K.-W.;\nand Lim, E.-P. 2023. Plan-and-solve prompting: Improv-\ning zero-shot chain-of-thought reasoning by large language\nmodels. arXiv preprint arXiv:2305.04091.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2022. Self-consistency\nimproves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nXu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-s.\n2023. Search-in-the-Chain: Towards the Accurate, Credible\nand Traceable Content Generation for Complex Knowledge-\nintensive Tasks. arXiv preprint arXiv:2304.14732.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y .; Cohen, W. W.;\nSalakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK.; and Cao, Y . 2022. React: Synergizing reasoning and act-\ning in language models. arXiv preprint arXiv:2210.03629.\nZelikman, E.; Wu, Y .; Mu, J.; and Goodman, N. 2022. Star:\nBootstrapping reasoning with reasoning. Advances in Neu-\nral Information Processing Systems, 35: 15476–15488.\nZhang, H.; Cai, J.; Xu, J.; and Wang, J. 2019. Complex\nQuestion Decomposition for Semantic Parsing. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 4477–4486. Florence, Italy: Associ-\nation for Computational Linguistics.\nZhang, K.; Chen, C.; Wang, Y .; Tian, Q.; and Bai, L. 2023a.\nCFGL-LCR: A Counterfactual Graph Learning Framework\nfor Legal Case Retrieval. In Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data\nMining, 3332–3341.\nZhang, K.; Lin, X.; Wang, Y .; Zhang, X.; Sun, F.; Jianhe,\nC.; Tan, H.; Jiang, X.; and Shen, H. 2023b. ReFSQL: A\nRetrieval-Augmentation Framework for Text-to-SQL Gen-\neration. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, 664–673.\nZhang, K.; Qiu, Y .; Wang, Y .; Bai, L.; Li, W.; Jiang, X.;\nShen, H.; and Cheng, X. 2022a. Meta-CQG: A Meta-\nLearning Framework for Complex Question Generation\nover Knowledge Bases. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics, 6105–\n6114.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19567\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022b. Auto-\nmatic chain of thought prompting in large language models.\narXiv preprint arXiv:2210.03493.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; et al.\n2022. Least-to-most prompting enables complex reasoning\nin large language models. arXiv preprint arXiv:2205.10625.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19568",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8675791025161743
    },
    {
      "name": "Natural language processing",
      "score": 0.5636521577835083
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5470004677772522
    },
    {
      "name": "Computer science",
      "score": 0.5283967852592468
    },
    {
      "name": "Decomposition",
      "score": 0.5053737759590149
    },
    {
      "name": "Tree (set theory)",
      "score": 0.5017881393432617
    },
    {
      "name": "Language model",
      "score": 0.4144305884838104
    },
    {
      "name": "Linguistics",
      "score": 0.34137600660324097
    },
    {
      "name": "Mathematics",
      "score": 0.2627992033958435
    },
    {
      "name": "Ecology",
      "score": 0.19156122207641602
    },
    {
      "name": "Philosophy",
      "score": 0.10549280047416687
    },
    {
      "name": "Biology",
      "score": 0.06228768825531006
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090176",
      "name": "Institute of Computing Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210096250",
      "name": "Beijing Institute of Big Data Research",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 6
}