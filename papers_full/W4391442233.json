{
  "title": "Fine-tuning Large Language Models for Chemical Text Mining",
  "url": "https://openalex.org/W4391442233",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1506313323",
      "name": "Wei Zhang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2653399829",
      "name": "Qinggong Wang",
      "affiliations": [
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2114400552",
      "name": "Xiangtai Kong",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2785111141",
      "name": "Jiacheng Xiong",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica"
      ]
    },
    {
      "id": "https://openalex.org/A4321878755",
      "name": "Shengkun Ni",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica"
      ]
    },
    {
      "id": "https://openalex.org/A2767459534",
      "name": "Duanhua Cao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica",
        "Zhejiang Lab",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3167716004",
      "name": "Buying Niu",
      "affiliations": [
        "Shanghai Institute of Materia Medica",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2123504080",
      "name": "Mingan Chen",
      "affiliations": [
        "Chinese Academy of Sciences",
        "ShanghaiTech University",
        "Shanghai Institute of Materia Medica"
      ]
    },
    {
      "id": "https://openalex.org/A2113511160",
      "name": "Runze Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2157338514",
      "name": "Yitian Wang",
      "affiliations": [
        "Shanghai Institute of Materia Medica",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A3184249251",
      "name": "Lehan Zhang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica"
      ]
    },
    {
      "id": "https://openalex.org/A2153580400",
      "name": "Xutong Li",
      "affiliations": [
        "Shanghai Institute of Materia Medica",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2529916829",
      "name": "Zhaoping Xiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099382599",
      "name": "Qian Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124115332",
      "name": "ZiMing Huang",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": "https://openalex.org/A2783203101",
      "name": "Zunyun Fu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica"
      ]
    },
    {
      "id": "https://openalex.org/A2120510683",
      "name": "Mingyue Zheng",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Nanjing University of Chinese Medicine",
        "University of Chinese Academy of Sciences",
        "Shanghai Institute of Materia Medica"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4391442233",
    "https://openalex.org/W3043647281",
    "https://openalex.org/W2902762889",
    "https://openalex.org/W4388173934",
    "https://openalex.org/W3168661259",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4386530347",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6858247203",
    "https://openalex.org/W6803094053",
    "https://openalex.org/W4281747818",
    "https://openalex.org/W4382198765",
    "https://openalex.org/W4378942305",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4383187427",
    "https://openalex.org/W29374554",
    "https://openalex.org/W2523785361",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W4327564965",
    "https://openalex.org/W3091684735",
    "https://openalex.org/W4388762522",
    "https://openalex.org/W4386766141",
    "https://openalex.org/W4386884238",
    "https://openalex.org/W4389263615",
    "https://openalex.org/W4388497794",
    "https://openalex.org/W3200122731",
    "https://openalex.org/W4251095682",
    "https://openalex.org/W3209726219",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4353112191",
    "https://openalex.org/W4385027818",
    "https://openalex.org/W4387302738"
  ],
  "abstract": "Extracting knowledge from complex and diverse chemical texts is a pivotal task for both experimental and computational chemists. The task is still considered to be extremely challenging due to the complexity of the chemical language and scientific literature. This study explored the power of fine-tuned large language models (LLMs) on five intricate chemical text mining tasks: compound entity recognition, reaction role labelling, metal-organic framework (MOF) synthesis information extraction, nuclear magnetic resonance spectroscopy (NMR) data extraction, and the conversion of reaction paragraph to action sequence. The fine-tuned LLMs models demonstrated impressive performance, significantly reducing the need for repetitive and extensive prompt engineering experiments. For comparison, we guided GPT-3.5 and GPT-4 with prompt engineering and fine-tuned GPT-3.5 as well as other open-source LLMs such as Llama2, T5, and BART. The results showed that the fine-tuned GPT models excelled in all tasks. It achieved exact accuracy levels ranging from 69% to 95% on these tasks with minimal annotated data. It even outperformed those task-adaptive pre-training and fine-tuning models that were based on a significantly larger amount of in-domain data. Given its versatility, robustness, and low-code capability, leveraging fine-tuned LLMs as flexible and effective toolkits for automated data acquisition could revolutionize chemical knowledge extraction.",
  "full_text": " \nFine-tuning Large Language Models for Chemical Text Mining 1 \nWei Zhang1,2,#, Qinggong Wang3,#, Xiangtai Kong1,2, Jiacheng Xiong1,2, Shengkun Ni1,2, Duanhua 2 \nCao1,4, Buying Niu1,2, Mingan Chen1,5,6, Runze Zhang1,2, Yitian Wang1,2, Lehan Zhang1,2, Xutong 3 \nLi1,2, Zhaoping Xiong7, Qian Shi6, Ziming Huang8, Zunyun Fu1,*, Mingyue Zheng1,2,3,* 4 \n1Drug Discovery and Design Canter, State Key Laboratory of Drug Research, Shanghai Institute of Materia 5 \nMedica, Chinese Academy of Sciences, 555 Zuchongzhi Road, Shanghai 201203, China 6 \n2University of Chinese Academy of Sciences, No. 19A Yuquan Road, Beijing 100049, China 7 \n3Nanjing University of Chinese Medicine, 138 Xianlin Road, Nanjing 210023, China 8 \n4Innovation Institute for Artificial Intelligence in Medicine of Zhejiang University, College of 9 \nPharmaceutical Sciences, Zhejiang University, Hangzhou, Zhejiang 310058, China 10 \n5School of Physical Science and Technology, ShanghaiTech University, Shanghai 201210, China 11 \n6Lingang Laboratory, Shanghai 200031, China 12 \n7ProtonUnfold Technology Co., Ltd, Suzhou, China 13 \n8Medizinische Klinik und Poliklinik I, Klinikum  der Universität München, Ludwig -Maximilians-14 \nUniversität, Munich, Germany 15 \n 16 \n#Wei Zhang and Qinggong Wang contributed equally to this study.  17 \n*Correspondence should be addressed to: 18 \nMingyue Zheng: myzheng@simm.ac.cn 19 \nZunyun Fu: fuzunyun@simm.ac.cn  20 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n2 \n \nAbstract 21 \nExtracting knowledge from complex and diverse chemical texts is a pivotal task for both 22 \nexperimental and computational chemists. The task is still considered to be extremely challenging 23 \ndue to the complexity of the chemical language and scientific literature. This study explored the 24 \npower of fine-tuned large language models (LLMs) on five intricate chemical text mining tasks: 25 \ncompound entity recognition, reaction role labelling, metal -organic framework (MOF) synthesis 26 \ninformation extraction, nuclear magnetic resonance spectroscopy (NMR) data extraction, and the 27 \nconversion of reaction paragraph to action sequence. The fine -tuned LLMs models demonstrated 28 \nimpressive performance, significantly reducing the need for repetitive and extensive prompt 29 \nengineering experiments. For comparison, we guided GPT -3.5 and GPT -4 with prompt 30 \nengineering and fine-tuned GPT-3.5 as well as other open-source LLMs such as Llama2, T5, and 31 \nBART. The results showed that the fine-tuned GPT models excelled in all tasks. It achieved exact 32 \naccuracy levels ranging from 69% to 95% on these tasks with minimal annotated data. It even 33 \noutperformed those task -adaptive pre -training and fine -tuning models that were based on a 34 \nsignificantly larger amount of in -domain data. Given its versatility, robustness, and low -code 35 \ncapability, leveraging fine -tuned LLMs as flexible and effective toolkits for automated data 36 \nacquisition could revolutionize chemical knowledge extraction. 37 \nIntroduction  38 \nChemical text mining is a crucial foundation in chemical research. It creates extensive 39 \ndatabases that provide access to physicochemical properties and synthetic routes for experimental 40 \nchemists. Additionally, it accumulates rich data and insights for computational chemists to use for 41 \nmodelling and predicting. More than just extracting information from chemical texts, the rule -42 \nbased transformation of chemical text is particularly interesting. For instance, synthetic procedures 43 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n3 \n \ncan be converted into action sequences 1,2 or programming languages 3-5. This allows them to be 44 \nunderstood and executed by robotic systems for automated syntheses.  45 \nHowever, converting structured data from intricate scientific literature is a challenging task, 46 \nespecially due to the complexity and heterogeneity of chemical language. As a result, a number of 47 \ntext-mining tools have been developed. For instance, ChemDataExtractor6,7 was created to extract 48 \nchemical entities and their associated properties, measurements and relationships  from chemical 49 \ndocuments, using unsupervised word clustering, conditional random fields, rule-based grammars 50 \nand dictionary matching . ChemRxnExtractor8, a BERT -like model, was designed to extract the 51 \nproduct and label associated reaction roles such as reactant, catalyst, solvent, and temperature from 52 \nparagraphs of synthesis experiments . Vaucher et. al.1,2 developed task-adaptive pre -trained 53 \ntransformers to convert the synthesis protocol paragraphs into action sequences. SynthReader3 was 54 \nbuilt to convert literature syntheses to executable XDL formats, containing a series of domain-55 \nspecific algorithms with predefined rules.  Historically, the focus has been on designing models 56 \nand algorithms specific to certain tasks, requiring extensive domain knowledge and sophisticated 57 \ndata processing  These tools, challenging to adapt for diverse extraction tasks, often require 58 \ncomplementary collaboration to manage complex information extraction tasks, thus limiting their 59 \nversatility and practicality.  60 \nRecently, large language models (LLMs), represented by ChatGPT released in November 61 \n2022, have shown the potential of Artificial General Intelligence (AGI). LLMs, such as GPT -3.5 62 \nand GPT -4, can generate logical insights or content that meets requirements based on human 63 \ninstructions. We are entering a new era where AGI and medicinal chemists might work together. 64 \nThere have been some assessments of ChatGPT's chemistry capabilities, including tasks like 65 \nsynonym transformation, property prediction, retrosynthesis, and molecule design 9-11. However, 66 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n4 \n \nLLMs tend to \"hallucinate\", meaning they generate unintended text that misaligns with established 67 \nfacts and real -world knowledge12,13. Moreover, objectively evaluating the results of open -ended 68 \nquestions remains a significant challenge. 69 \nAt this juncture, LLMs may still find it difficult to accurately answer factual and knowledge-70 \nbased questions. However, using LLMs for knowledge extraction tasks should greatly alleviate 71 \nhallucination and fully leverage their powerful text comprehension and processing capabilities, 72 \nmaking them promising universal tools for chemical text mining. For instance, Zheng et al.14 used 73 \nprompt engineering to guide ChatGPT in extracting information about metal -organic framework 74 \n(MOF) synthesis. Patiny et al. 15 tried to use  ChatGPT to extract FAIR (Findable, Accessible, 75 \nInteroperable, Reusable) data from publications. However, their approach of using LLMs simply 76 \nbased on prompt engineering tend to achieve poor performance in exact accuracy. According to 77 \nthe biomedical benchmark study by Chen et al.16, ChatGPT performed significantly worse on 78 \nbiomedical text mining compared to existing models. These findings seem contradicts the common 79 \nbelief in the LLMs’ superior comprehension abilities. Either way, LLMs have limitations due to 80 \ntheir model architecture and memory, including a maximum length of prompt tokens. Additionally, 81 \nhuman expressions can be ambiguous, incomplete, vague, and difficult to refine. Outputs may not 82 \nstrictly adhere to formatting requirements, leading to misunderstanding and poor performance in 83 \nmining complex text, such as patents or scientific literature. Therefore, zero -shot or few -shot 84 \nprompts are often insufficient to address the diversity of scenarios and cannot guarantee the quality 85 \nof extracted data. 86 \nIn this study, we explore the effectiveness of fine-tuning LLMs on five challenging tasks in 87 \nchemical text mining: compound entity recognition, reaction role annotation, metal-organic 88 \nframework (MOF) synthesis information extraction, nuclear magnetic resonance spectroscopy 89 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n5 \n \n(NMR) data extraction, and conversion reaction paragraphs into action sequences. We found that 90 \nfine-tuning GPT models significantly enhances performance in chemical text mining tasks, 91 \ncompared to prompt-only version, while also reducing dependency on the repetitive and extensive 92 \nprompt engineering experiments. Meanwhile, we also evaluated other prevalent generative pre-93 \ntrained language models, such as Llama2 17, T5 18, and BART 19. Among these, the fine -tuned 94 \nChatGPT (gpt-3.5-turbo) models achieved state-of-the-art (SOTA) performance across all five 95 \ntasks. Remarkably, it even outperformed models that have been trained specifically for each task 96 \nand subsequently fine-tuned, based on a significantly larger amount of in-domain data. This study 97 \nhighlights the potential of fine-tuning LLMs to revolutionize complex knowledge extraction with 98 \ntheir versatility, robustness, and low code capability. Fine-tuned LLMs can be easily generalizable 99 \nand can optimize the labor -intensive and time -consuming data collection workflow, even when 100 \ntrained with few data. This will accelerate the discovery and creation of novel substances, making 101 \nthem powerful tools for universal use. 102 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n6 \n \n 103 \nFig. 1. | Schematics of fine-tuning ChatGPT for chemical text mining.  a, The pipeline of fine-tuning ChatGPT on proprietary 104 \ndata. The green OpenAI logo symbolizes official gpt -3.5-turbo, while the blue one symbolizes fine -tuned gpt -3.5-turbo. b, 105 \nSupervised fine-tuned LLMs outperforms prompt-only LLMs in some customized scenarios. c, Illustration of cheminformatics 106 \ninsights to be extracted from paragraph. And illustration of the five practical tasks in chemical text mining with respective example 107 \noutputs, including Paragraph2Compound, Paragraph2RXNRole, Paragraph2MOFInfo, Paragraph2NMR, and Paragraph2Action. 108 \n  109 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n7 \n \nResults & Discussion 110 \nOverview of Chemical Text Mining Tasks 111 \nGiven the complex and diverse information embedded in chemical literature, we designed 112 \nfive extraction tasks to demonstrate the potential and practicality of LLMs in chemical text mining 113 \n(Fig. 1). Paragraph2Compound task is a relatively simple name entity recognition task, to extract 114 \nall chemical compound entities from the given paragraph. Paragraph2RXNRole task is to label the 115 \nreaction roles including product, reactant, catalyst, temperature, solvent, time, and yield in the 116 \nparagraph. Paragraph2MOFInfo task is to extract all MOF synthesis conditions including 117 \ncompound name, metal source, metal amount, linker, linker amount, modulator, modulator amount 118 \nor volume, solvent, solvent volume, reaction temperature and reaction time. Paragraph2NMR task 119 \nis to extract the IUPAC name, experimental condition including frequency and solvent as well as 120 \nchemical shift data for both 1H NMR and 13C NMR spectra. Paragraph2Action task is to convert 121 \nexperimental procedures to structured synthetic steps (action sequences). All tasks are unified to 122 \nsequence-to-sequence formats to facilitate the use s of LLMs. More details can be found in the 123 \nMethods section. 124 \nParagraph2Compound—Extract All Chemical Compound Entities. 125 \nFig. 2a illustrates the process of random sampling from millions of paragraph-entities pairs, 126 \nwhich refer to UPSTO annotations. It starts by randomly selecting 100,000 samples, then choosing 127 \n10,000 from them, followed by randomly picking 1,000, then 100, and finally 10.  This sampling 128 \nprocess ensures each smaller subset is included in the larger one, with each  subset used for 129 \nindividual training. Fig. 2b demonstrates the performance of prompt -only models and fine-tuned 130 \nmodels, which are evaluated on a consistent evaluation set of 1,000 samples across varying training 131 \ndata sizes. These results are obtained from three independent trials. In the case of prompt -only 132 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n8 \n \nmodels, randomness is intentionally introduced by altering the prompt and examples  (Fig. 2c, 133 \nSupplementary Fig. 2 ). Given the task’s straightforward nature and clear instructions, even the 134 \nprompt-only language models achieved decent F1 scores over 0.6. For fine -tuned models, the 135 \nsampling and training process for the training set is repeated three times, as depicted in Fig. 2a. As 136 \nshown in Fig . 2b, all fine -tuned models demonstrate a performance improvement, especially in 137 \nterms of the F1 score and Jaccard index, proportional to the increase in dataset size. These models 138 \noutperform the prompt -only models designed for this task. When the training data size  is 139 \nsubstantial enough, the F1 scores of GPT-3.5-turbo, Llama2, and T5 can reach close to 0.9, and 140 \nthe Jaccard index can approach 0.8. Notably, gpt -3.5-turbo, when fine -tuned, showed minimal 141 \nfluctuations and superior performance. However, it is essential to emphasize that the cost of fine-142 \ntuning gpt -3.5-turbo increased tenfold with each tenfold increase in data volume. Our 143 \nexperimentation with gpt -3.5-turbo were capped at 10,000 training samples for 3 epochs due to 144 \nOpenAI's limitations, resulting in a nearly 90 -dollar expense—a low cost-effective investment in 145 \ncomputational resources. In contrast, other fine -tuned language models have displayed notable 146 \ncost advantages in this simple task. 147 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n9 \n \n 148 \nFig. 2. | Design and Performance for Paragraph2Compound task. a, The workflow of sampling and training based on USPTO 149 \ndataset for Paragraph2Compound task. b, The performance of different models across varying size of training set. The data point 150 \nand the shaded areas represent respectively the mean values and standard deviations derived from three independent trials. c, 151 \nExample of the zero-shot and three-shots prompts utilized for Paragraph2Compound task.  152 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n10 \n \nParagraph2RXNRole—Product Extraction and Reaction Role Labelling. 153 \nAccording to Guo et al.8, the Paragraph2RXNRole task comprises two subtasks. The first is 154 \nto extract the central product, and the second is to label the associated reaction roles within 155 \nspecified paragraphs (Fig. 3a). For these tasks, Guo et al. developed two -stage BERT-like token-156 \nmulti-classification models. To enable a fair comparison with generative language models, we 157 \nconverted the data into sequence -to-sequence formats by adding <Role*Compound*Role> 158 \nannotations to the input paragraphs. We then converted the language models’ outputs back into 159 \nlists of BIO -tags, followed by post -processing to align with the original BIO -tags labels for 160 \nassessment. Notably, even though utilizing prompt engineering  with 20 -shots examples 161 \n(Supplementary Fig. 3, 4), GPT-3.5 and GPT-4 perform poor on two Paragraph2RXNRole tasks, 162 \nwhich may result from the complicated syntax cases and limited  context length (Fig. 3b, 3c) . 163 \nHowever, the fine-tuned GPT models perform well. For product extraction, the fine-tuned gpt-3.5-164 \ntrubo (best over one epoch) achieved a F1 score of 77.1%, slightly surpassing the previous SOTA 165 \napproach, ChemBERT, which scored 76.2%  (Fig. 3b). For reaction role labelling, the fine-tuned 166 \ngpt-3.5-trubo (best over five epochs) achieved a F1 score of 83.0%, significantly outperforming 167 \nthe previous SOTA approach, ChemRxnBERT, which scored 78.7% (Fig. 3c). It’s notable that the 168 \nfine-tuned gpt-3.5-trubo models, which cost only $1 and $5 respectively, demonstrated extremely 169 \nhigh cost-effectiveness with small training datasets. In contrast, ChemBERT was domain-adaptive 170 \npre-trained on 9,478,043 sentences from 200,000 journal articles, and ChemRxnBERT was further 171 \ntask-adaptive trained on 944,733 reaction -inclusive sentences. We should also mention that the 172 \noutputs of fine-tuned GPTs and Llama2 align almost perfectly with the input text, with 100% and 173 \n99% post-processing-free ratios respectively. On the other hand, most outputs of fine -tuned T5 174 \nand BART require additional alignment due to their tokenization and vocabulary limitations, with 175 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n11 \n \na ratio of only 31% that does not require post-processing. Even after post-processing, the F1 scores 176 \nof T5 and BART were significantly lower than those token -classification BERT-like models or 177 \nlarge language models such as GPTs and Llama2.  178 \n 179 \nFig. 3. | Design and Performance for Paragraph2RXNRole task. a, Data formats of two subtasks in paragraph2RXNRole task. 180 \nb, Performance of product extraction. c, Performance of reaction role labelling. 181 \n 182 \nParagraph2MOFInfo—Extraction of MOF Synthesis Information. 183 \nOur re -annotated dataset for the Paragraph2MOFInfo task  displayed in Fig. 4a, mostly 184 \ncontains single reaction paragraphs with a few featuring multiple reactions. We used Levenshtein 185 \nsimilarity and exact accuracy as metrics to objectively assess the models’ ability to extract 186 \nformatted data that fully complies with customized requirements in the task. This approach is more 187 \nobjective and accurate  with less manual intervention , c ompared to the  manual analysis and 188 \nevaluation used by Zheng et al.14. The fine-tuned gpt-3.5-turbo significantly outperforms the gpt-189 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n12 \n \n3.5-turbo with prompt engineering, improving exact accuracy by over 20% for both single and 190 \nmultiple reactions  (Fig. 4b, Supplementary Fig. 5 ). It also surpasses  other fine -tuned models , 191 \nespecially when handling complex multi-reaction paragraphs. Exact accuracy rates for single and 192 \nmultiple reactions are 82.7% and 68.8%, respectively (Fig. 4b). As depicted in Fig. 4c and Fig. 4d, 193 \nwhile most models achieve high Levenshtein similarity across the 11 parameters, only a few 194 \nmaintain high exact accuracy, which is the golden metric that we mainly focus on . Considering 195 \nthat some MOF synthesis paragraphs may  include multiple reactions, we provide an example of 196 \nmulti-reaction extraction by various models in Fig. 4e. The paragraph includes two reactions, the 197 \nfirst with (R) -H3PIA and bipy as linkers , providing all reaction conditions  explicitly, and the 198 \nsecond with  the substitution of (R) -H3PIA with (S) -H3PIA, keeping all other conditions 199 \nunchanged. Most models successfully interpreted the semantics and extracted two reactions from 200 \nthe MOF synthesis paragraph . However, only the fine -tuned ChatGPT perfectly extracted 201 \ninformation that matched our annotated ground truth. Other  models showed varying degrees of 202 \nincompleteness, particularly with items involving multiple components and their quantities. 203 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n13 \n \n 204 \nFig. 4. | Design and Performance for Paragraph2MOFInfo task. a , A statistic of the dataset.  b, Mean performance of 205 \nLevenshtein similarity and exact match accuracy by different models . c, Levenshtein similarity for 11 parameters in the 206 \nParagraph2MOFInfo task. d, Exact match accuracy for 11 parameters in the Paragraph2MOFInfo task. e, An example of extractions 207 \nby different models from a multi-reaction MOF synthesis paragraph. The cells in yellow represented the ground truth. The cells in 208 \ngreen represented the exact match predictions. The cells in blue represented the incorrect predictions.  209 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n14 \n \nParagraph2NMR—Extraction of Experimental Conditions and NMR Chemical Shifts. 210 \nThe impact of training set sizes and the use of prompt engineering on the performance of fine-211 \ntuning gpt-3.5-turbo in extracting NMR information is illustrated in Fig . 5a. Regardless of the 212 \ntraining data size for fine-tuning (ranging from 25 to 300), or the presence of prompt engineering, 213 \nthere are hardly any significant fluctuations in performance. This holds true for metrics such as 214 \nLevenshtein similarity and exact match accuracy of the fine-tuned gpt-3.5-turbo when the numbers 215 \nof training samples exceed 50. This demonstrates the strong learning capability and robustness of 216 \nLLMs. Fig. 5b illustrates the performance of different generative language models using the same 217 \n200 training data. In terms of Levenshtein similarity, a metric based on edit distance, almost all 218 \nfine-tuned language models achieved impressing scores, outperforming GPT models that solely 219 \nrely on prompt engineering (Fig. 5b, Supplementary Fig. 6). However, when considering the exact 220 \nmatch accuracy metric, where each character must perfectly align with the ground truth count, 221 \nLLMs such as GPTs and Llama2 take the lead. While fine-tuned T5 and BART manage to extract 222 \nthe majority of the text, they often miss or mistakenly copy several characters. This contributes to 223 \na significant decrease in their exact match accuracy metric, as shown in Fig. 5c. In this context, 224 \nthe extraction of long complex text by LLMs is more standardized and high-quality, aligning more 225 \nclosely with human expectations. It is worth noting that fine-tuning Llama2 provides an alternative 226 \napproach for deploying text mining locally, given its exceptional exact match accuracy. 227 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n15 \n \n 228 \nFig. 5. | Performance for Pargraph2NMR task. a , The performance of fine -tuning gpt-3.5-turbo with and without prompt 229 \nengineering as it varies with training data size. b, Heat map illustrating Levenshitein similarity and exact match accuracy of various 230 \nmodels in extracting each NMR information. c, Examples of error extractions by T5 and BART, compared with the ground truth. 231 \nParagraph2Action—Action Sequence Extracted from an Experimental Procedure. 232 \nThe above -mentioned extraction tasks simply require the model to replicate specific 233 \ninformation from the paragraph. However, the Paragraph2Action task requires the model to 234 \nunderstand and transform the paragraph. Clearly, GPT models  with prompt engineering has 235 \ndifficulty with this task, especially when it involves multiple complex conversions and insufficient 236 \nprompt descriptions (Table1, Supplementary Fig. 7). To gauge the maximum potential of ChatGPT 237 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n16 \n \nusing only prompts, we incrementally increased the number of transformation examples from 6 to 238 \n60. Despite encompassing all types of actions at least once and nearly reaching the token limit of 239 \n4,096 for GPT -3.5 and 8192 for GPT -4, their performance in the few-shot scenario remains 240 \ndisappointingly poor. The currently best-performing LLM GPT-4 with 60 examples for in-context 241 \nlearning, it achieved only 32.7% full sentence exact accuracy, a BLEU score of 65.0, and a 242 \nLevenshtein similarity of 72.8. However, fine-tuning pre-trained language models with a small 243 \namount of data could yield decent results (Table 1). Remarkably, after 3 epochs of fine-tuning gpt-244 \n3.5-turbo on 1,060 hand-annotated training data, we achieved 62.5% full sentence exact accuracy, 245 \nan 84.8 Modified BLEU score, and an 87.6 Levenshtein similarity. This process took only 1 hour 246 \nand cost $3 for fine -tuning. These metrics surpass the SOTA results previously reported by 247 \nVaucher et al. 1, which used an ensemble of three mode ls, each task -adaptively pre-trained on 2  248 \nmillion rule-based data and refined on 14,168 augmented data. Interestingly, further improvement 249 \nwas achieved by augmenting the training data size to 14,168. This resulted in 69.0% full sentence 250 \nexact accuracy, an 86.4 Modified BLEU score, and an 89.9 Levenshtein similarity  (Table 1). For 251 \nautonomous robots, it is challenging to generate instructions that follow strict syntax rules. Fine-252 \ntuning LLMs plays a crucial role in bridging the gap between fuzzy natural language and structured 253 \nmachine-executable programming languages , significantly improving the accuracy of 254 \ncustomization with a small amount of annotated data . In similar tasks involving “fuzzy rules” or 255 \nhard-to-define extraction, fine-tuning LLMs might offer considerable advantages in tailoring the 256 \ntransformation.  257 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n17 \n \nTabel 1 | Performance on Paragraph2Action task.  258 \nModel Training data strategy 100% \naccuracy \n90% \naccuracy \n75% \naccuracy \nModified \nBLEU score \nLevenshtein  \nsimilarity Cost \nGPT-3.5-turbo (6-shots) No training 8.2 16.8 34.7 38.6 59.4 905 mean tokens \nGPT-3.5-turbo (12-shots) No training 8.8 19.3 42.3 43.1 62.3 1,374 mean tokens \nGPT-3.5-turbo (18-shots) No training 13.1 23.3 42.6 44.4 64.3 1,670 mean tokens \nGPT-3.5-turbo (24-shots) No training 14.8 25.9 45.5 47.0 65.8 2,598 mean tokens \nGPT-3.5-turbo (30-shots) No training 13.9 26.4 47.2 49.5 66.0 3,610 mean tokens \nGPT-4 (6-shots) No training 13.4 23.3 44.9 44.7 54.5 861 mean tokens \nGPT-4 (12-shots) No training 20.7 30.7 51.1 51.4 69.2 1,357 mean tokens \nGPT-4 (18-shots) No training 21.9 33.0 56.5 53.8 63.0 1,631 mean tokens \nGPT-4 (24-shots) No training 22.7 35.8 58.2 56.7 65.1 2,546 mean tokens \nGPT-4 (30-shots) No training 26.1 40.0 61.6 59.8 67.7 3,611 mean tokens \nGPT-4 (60-shots) No training 32.7 43.8 63.3 65.0 72.8 7,010 mean tokens, $ 41 \nTransformer (single model) * No task-adaptive pretraining, hand-annotated data (1,060) 13.1 15.1 21.9 22.5 45.9 - \nBART-base (fine-tuned) No task-adaptive pretraining, hand-annotated data (1,060) 51.1 65.9 77.6 73.2 83.9 - \nT5-base (fine-tuned) No task-adaptive pretraining, hand-annotated data (1,060) 57.7 71.6 83.2 81.8 86.8 - \nLama2-13b-chat (fine-tuned) No task-adaptive pretraining, hand-annotated data (1,060) 56.8 66.8 80.7 80.3 86.0 40 min for training  \nGPT-3.5-turbo (fine-tuned) No task-adaptive pretraining, hand-annotated data (1,060) 62.5 72.7 82.9 84.8 87.6 3 epochs, 1h, $ 3 \nTransformer (single model) * No task-adaptive pretraining, augmented data (14,168) 37.8 47.7 62.8 64.7 76.4 - \nBART-base (fine-tuned) No task-adaptive pretraining, augmented data (14,168) 52.0 68.5 80.1 74.4 84.8  \nT5-base (fine-tuned) No task-adaptive pretraining, augmented data (14,168) 59.7 74.1 82.4 84.1 87.1 - \nLlama2-13b-chat (fine-tuned) No task-adaptive pretraining, augmented data (14,168) 60.2 70.4 83.5 81.6 87.9 9 hours for training \nGPT-3.5-turbo (fine-tuned) No task-adaptive pretraining, augmented data (14,168) 69.0 78.1 86.9 86.4 89.9 5 epochs, 1.5 h, $ 92 \nTransformer (single model) * Task-adaptive pretraining (2 million), hand-annotate (1,060) 56.8 67.3 80.4 81.5 85.7 - \nTransformer (single model) * Task-adaptive pretraining (2 million), augmented data (14,168) 59.4 70.5 81.8 84.3 86.7 - \nTransformer (ensemble models) * Task-adaptive pretraining (2 million), augmented data (14,168) 60.8 71.3 82.4 85.0 86.6 - \nThe symbol “*” represented the result reported by Vaucher  et al .1 The result in black bold is the best previous 259 \nperformance. The result in red bold is the best new performance. 260 \nPromising Performance and Potentials of Fine-tuning LLMs on Chemical Text Mining. 261 \nChemical text mining expedites scientific discovery in chemistry. Previously, tasks involving 262 \ncomplex chemical language and sophisticated processing required the development of specific 263 \ndomain-focused models. Now, the fine-tuning of universal LLMs offers a highly generalized and 264 \ncost-effective solution. We have demonstrated the impressive efficacy, flexibility, and high exact 265 \naccuracy of fine-tuning LLMs, regarding all kinds of text mining tasks as generative problems. An 266 \nexamination of incorrect predictions revealed that only a small proportion were entirely incorrect, 267 \nwhile most were acceptable alternatives to the ground truth or even pointed out the incorrect labels 268 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n18 \n \n(Supplementary Fig. 10-14). These errors can be attributed to inconsistent annotation standards 269 \nand the inherent ambiguity of terms with multiple interpretations or functions. Therefore, 270 \nimproving the formatted data extraction requires continuous efforts, including the refinement of 271 \nspecific rules and the enrichment of annotations prone to misinterpretation during training and 272 \ninference. With detailed specifications and high -quality formatted data, the fine -tuning method 273 \nbased on LLMs is highly reliable. It can be easily extended to tasks related to extracting 274 \ninformation from scientific literature and transforming data into simple user-friendly reaction 275 \nformat20 that is both human- and machine-readable. This approach will significantly contribute to 276 \nthe development of extensive databases like Open Reaction Database 21,22, SciFinder23 and 277 \nReaxys24, which gather comprehensive synthesis data through automated curation and expert 278 \nverification, to make data more findable, accessible, interoperable, and reusable (FAIR).  279 \nNevertheless, leveraging fine-tuned LLMs is still insufficient to extract  all synthesis 280 \ninformation from chemical literature, which contains extensive complex figure and form contents. 281 \nRecently, s ome tools have been developed to recognize molecular images 25,26 and reaction 282 \ndiagrams27,28 from the literature. Integrating LLMs with these image recognition tools or 283 \ndeveloping advanced large multimodal models (LMMs) may be a promising unified solution for 284 \nfurther chemical data mining. Notably, when extracting large amounts of data from copyrighted 285 \nliterature, it's essential to access the necessary permissions from scientific publications. 286 \nIn this work, we have scratched the surface of the vast potential of LLMs in chemistry and 287 \nmaterials science by fine -tuning LLMs for chemical text mining. We can see that there is still a 288 \ngap between open-source language models and GPT models, but considering GPTs' closed-source 289 \nnature, it becomes imperative for researchers and communities to focus efforts on this direction. 290 \nTechnically, advancements like more effective fine -tuning strateg ies, improved open-source 291 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n19 \n \nmodel architectures, faster inference approaches, wider context windows, and lower computational 292 \ncosts in the era of LLMs are anticipated to further enhance text mining. Meanwhile, it’s more 293 \nessential to consider what else can be achieved with LLMs and how we can develop more effective 294 \nLLMs for chemistry and materials science. For instance, LLMs have the potential to revolutionize 295 \npredictive modelling by incorporating the extensive  “fuzzy knowledge” encapsulated within 296 \nscientific literature, especially in chemistry and drug discovery. By combining empirical results 297 \nwith documented knowledge, LLMs could assist chemists identify patterns in experiments that 298 \nmight otherwise be missed, predict properties of compounds and outcomes of reactions, and even 299 \ngenerate new chemical hypotheses and theories. Furthermore, the integration of LLMs’ 300 \ncomprehension with specialized tools could substantially lower the barrier of chemists to use these 301 \ntools throughout the entire workflow, thanks to interactive interfaces in natural language. Future 302 \nresearch could investigate how to merge formatted laboratory data with wealth of information in 303 \nscientific literature and develop the multimodal capability to enrich specific domain knowledge 304 \nfor LLMs. This endeavor will require a sustained, long-term effort.  305 \nConclusion 306 \nHere, we have demonstrated the effectiveness of fine-tuning LLMs in chemical text mining. 307 \nWe conducted five complex tasks: compound entity recognition, reaction role labelling, MOF 308 \nsynthesis information extraction, NMR data extraction, and the transformation from reaction 309 \nprocedures to action sequences. Chemical text mining remains a challenging professional domain 310 \nwhen leveraging language model mining, even with prompt engineering. However, LLMs that are 311 \nfine-tuned with appropriate annotations can produce structured outputs that perfectly fulfil human 312 \nrequirements not easily expressed in natural language. This feature fully utilizes their natural 313 \nlanguage understanding and formatting capability. Using chemical text mining as an example, this 314 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n20 \n \nstudy provides guidance on fine -tuning of LLMs to serve as universal knowledge extraction 315 \ntoolkits. These toolkits can be easily extended for automated extraction from documents and rule-316 \nbased formatted transformations. Our work lays the groundwork for the applications of LLMs in 317 \ninformation extraction within the chemical domain , which will catalyze data-driven innovations 318 \nin chemical and materials science.  319 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n21 \n \nMethods 320 \nData Preparation 321 \nFor the Paragraph2Compound task, we compiled an automatically annotated dataset. This 322 \ndataset is based on the publicly accessed USPTO subset extracted by Lowe et al. 29,30, and includes 323 \nmillions chemical reaction paragraphs from patents, each paired with compound tags. We used 324 \nregular expressions to identify compound labels within each paragraph, separating them with “ |” 325 \nsymbol based on their sequential occurrence in the paragraph. For the Paragraph2RXNRole task, 326 \nwe used the manually annotated dataset by Guo et al. 8, following the same data partitioning 327 \nstrategy. We transformed the data from the BIO -token classification format to a sequence -to-328 \nsequence format using the annotation scheme “<Role*compound*Role>”. We processed 329 \nparagraphs containing multiple central products and related reactions into several input and output 330 \npairs. For the Paragraph2MOFInfo task, we manually checked and re -annotated the raw data of 331 \nZheng et al. 14, transforming them into a sequence -to-sequence format. This dataset comprises 332 \nMOF synthesis paragraphs, extraction by ChatGPT, and human -evaluated answers.  For the 333 \nParagraph2NMR task, we manually curated a dataset of 600 high-quality annotations. These were 334 \nmainly sourced from various literature on PubMed to ensure a wide diversity. The task is aims to 335 \nextract information such as IUPAC name, experimental conditions, including frequency and 336 \nsolvent, and chemical shifts data from both 1H NMR and 13C NMR spectra. For the 337 \nParagraph2Action task, we utilized the hand -annotated dataset by Vaucher et al., employing the 338 \nsame data partitioning strategy. This dataset is derived from the Pistachio dataset by NextMove 339 \nsoftware31. The details of datasets used for the five chemical text mining tasks are listed in 340 \nSupplementary Table 1. 341 \n 342 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n22 \n \nPrompt-only ChatGPT 343 \nPrompt-only interaction enables users to efficiently communicate with large language models 344 \nthrough simple prompts. This guides the model to produce relevant responses without further 345 \ntraining. In a zero -shot scenario, the model generates responses using only a descriptive prompt 346 \nand its pre-trained knowledge. However, in a few -shot approach, the model uses a small number 347 \nof examples to improve its understanding s and responses. To maximize the performance, we 348 \nselected diverse examples and ensured a large number of tokens.  We interacted with ChatGPT 349 \nusing API keys and employed model versions gpt -3.5-turbo-0613 and gpt-4-0613. The zero-shot 350 \nand few-shot prompts for chemical text mining tasks can be found in Supplementary Fig. 2-7.  351 \nFine-tuning ChatGPT  352 \nSince late August 2023, supervised fine -tuning capabilities have been available for the gpt -353 \n3.5-turbo model32. The aim is to enhance performance in specific scenarios customized based on 354 \nprivate data. In this study, we fine -tuned the gpt-3.5-turbo-0613 model for chemical text mining 355 \nsceneries. We formatted the data into jsonl and uploaded them to OpenAI’s cloud servers, then 356 \ninitiated fine-tuning jobs. Once the training was complete, the fine-tuned gpt-3.5-turbo model was 357 \nready for inference. API keys were requisite throughout the training and inference procedures. 358 \nFine-tuning for the gpt-4-turbo model is expected in the future. 359 \nOpen-Source Language Models 360 \nWe selected the most widely used and representative generative pre-trained language models 361 \nlike Llama2,17 T518 and BART19. These serve as baselines for a comprehensive comparison with 362 \nthe fine -tuned ChatGPT across five chemical text mining tasks. Considering performance, 363 \nefficiency, and hardware resource constraints, we used full parameter fine-tuning for BART-base 364 \nand T5-base. We applied multitask-learning to BART and T5 in the Paragraph2MOFInfo task and 365 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n23 \n \nParagraph2NMR task due to their limitations in generating multi -attribute long sentences  366 \n(Supplementary Fig. 8, 9 ), aiming to enhance their performance . This approach significantly 367 \nimproved their performance. For Llama2, we used Q-LoRA33 to efficiently fine-tune llama2-13b-368 \nchat. This method maintains most performance of full parameter fine -tuning while significantly 369 \nreducing computational demands. We used vllm34 to speed up the inference of llama2 -13b-chat, 370 \nwhich is tens of times faster than Hugging Face’s pipeline. To ensure optimal performance, we 371 \nadjusted hyperparameters such as learning rates , lora_r, and lora_alpha  during the fine -tuning 372 \nprocess of baseline models (Supplementary Table 2). More details of training, pre-processing, and 373 \npost-processing can be found in the Supplementary Information. 374 \nMetrics for Evaluation 375 \nSince fine-tuning ChatGPT does not allow for early stopping based on optimal validation loss, 376 \nwe report the performances of all models at the best epoch selected from the evaluation set for fair 377 \ncomparison. Given the task specifics, we use metrics including precision, recall, and F1 score for 378 \nevaluating entity -level performance. For sentence -level performance assessment, we use 379 \nLevenshtein similarity, exact match accuracy, partial accuracy, and a modified BLEU score. 380 \nData Availability 381 \nAll datasets used in this work are available from the authors upon request.  382 \nCode Availability 383 \nAll scrips for training and evaluating can be found on GitHub at https://github.com/zw-384 \nSIMM/SFTChatGPT_for_chemtext_mining. 385 \n  386 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n24 \n \nReferences 387 \n1 Vaucher, A. C.  et al.  Automated extraction of chemical synthesis actions from 388 \nexperimental procedures. Nat. Comm. 11, 3601 (2020).  389 \n2 Suvarna, M., Vaucher, A. C., Mitchell, S., Laino, T. & Pé rez-Ramí rez, J. Language models 390 \nand protocol standardization guidelines for accelerating synthesis planning in 391 \nheterogeneous catalysis. Nat. Comm. 14, 7964 (2023).  392 \n3 Mehr, S. H. M., Craven, M., Leonov, A. I., Keenan, G. & Cronin, L. A universal system 393 \nfor digitization and automatic execution of the chemical synthesis literature. Science 370, 394 \n101-108 (2020).  395 \n4 Steiner, S.  et al.  Organic synthesis in a modular robotic system driven by a chemical 396 \nprogramming language. Science 363, eaav2211 (2019).  397 \n5 Ha, T. et al. AI-driven robotic chemist for autonomous synthesis of organic molecules. Sci. 398 \nAdv. 9, eadj0461 (2023).  399 \n6 Swain, M. C. & Cole, J. M. ChemDataExtractor: a toolkit for automated extraction of 400 \nchemical information from the scientific literature. J. Chem. Inf. Model.  56, 1894-1904 401 \n(2016).  402 \n7 Mavracic, J., Court, C. J., Isazawa, T., Elliott, S. R. & Cole, J. M. ChemDataExtractor 2.0: 403 \nAutopopulated ontologies for materials science. J. Chem. Inf. Model. 61, 4280-4289 (2021).  404 \n8 Guo, J. et al. Automated chemical reaction extraction from scientific literature. J. Chem. 405 \nInf. Model. 62, 2035-2045 (2021).  406 \n9 Castro Nascimento, C. M. & Pimentel, A. S. Do Large Language Models Understand 407 \nChemistry? A Conversation with ChatGPT. J. Chem. Inf. Model. 63, 1649-1655 (2023).  408 \n10 Clark, T. M., Anderson, E., Dickson-Karn, N. M., Soltanirad, C. & Tafini, N. Comparing 409 \nthe Performance of College Chemistry Students with ChatGPT for Calculations Involving 410 \nAcids and Bases. J. Chem. Educ. 100, 3934-3944 (2023).  411 \n11 Guo, T. et al. What indeed can GPT models do in chemistry? A comprehensive benchmark 412 \non eight tasks. Preprint at arXiv https://arxiv.org/abs/2305.18365 (2023). 413 \n12 Ji, Z.  et al.  Survey of hallucination in natural language generation. ACM Computing 414 \nSurveys 55, 1-38 (2023).  415 \n13 Zhang, Y.  et al.  Siren's Song in the AI Ocean: A Survey on Hallucination in Large 416 \nLanguage Models. Preprint at arXiv https://arxiv.org/abs/2309.01219 (2023). 417 \n14 Zheng, Z., Zhang, O., Borgs, C., Chayes, J. T. & Yaghi, O. M. ChatGPT Chemistry 418 \nAssistant for Text Mining and the Prediction of MOF Synthesis. J. Am. Chem. Soc.  145, 419 \n18048-18062 (2023).  420 \n15 Patiny, L. & Godin, G. Automatic extraction of FAIR data from publications using LLM.  421 \n(2023).  422 \n16 Chen, Q. et al. An Extensive Benchmark Study on Biomedical Text Generation and Mining 423 \nwith ChatGPT. Bioinformatics, btad557 (2023).  424 \n17 Touvron, H. et al. Llama 2: Open foundation and fine-tuned chat models. Preprint at arXiv 425 \nhttps://arxiv.org/abs/2307.09288 (2023). 426 \n18 Raffel, C.  et al.  Exploring the limits of transfer learning with a unified text -to-text 427 \ntransformer. The Journal of Machine Learning Research 21, 5485-5551 (2020).  428 \n19 Lewis, M. et al. Bart: Denoising sequence -to-sequence pre-training for natural language 429 \ngeneration, translation, and comprehension. Preprint at arXiv 430 \nhttps://arxiv.org/abs/1910.13461 (2019). 431 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n25 \n \n20 Nippa, D. F. et al. Simple User-Friendly Reaction Format.  (2023).  432 \n21 Kearnes, S. M.  et al. The open reaction database. J. Am. Chem. Soc.  143, 18820-18826 433 \n(2021).  434 \n22 Mercado, R., Kearnes, S. M. & Coley, C. W. Data sharing in chemistry: lessons learned 435 \nand a case for mandating structured reaction data. J. Chem. Inf. Model.  63, 4253-4265 436 \n(2023).  437 \n23 SciFinder. https://scifinder-n.cas.org. (accessed August 29, 2023). 438 \n24 Reaxys. https://www.reaxys.com. (accessed August 29, 2023). 439 \n25 Xiong, J. et al. αExtractor: a system for automatic extraction of chemical information from 440 \nbiomedical literature. Sci. China. Life. Sci. (2023).  441 \n26 Qian, Y. et al. MolScribe: Robust Molecular Structure Recognition with Image -to-Graph 442 \nGeneration. J. Chem. Inf. Model. 63, 1925-1934 (2023).  443 \n27 Qian, Y., Guo, J., Tu, Z., Coley, C. W. & Barzilay, R. RxnScribe: A Sequence Generation 444 \nModel for Reaction Diagram Parsing. J. Chem. Inf. Model.  63, 4030 -4041 (2023). 445 \nhttps://doi.org/10.1021/acs.jcim.3c00439 446 \n28 Wilary, D. M. & Cole, J. M. ReactionDataExtractor 2.0: A deep learning approach for data 447 \nextraction from chemical reaction schemes. J. Chem. Inf. Model. 63, 6053-6067 (2023).  448 \n29 Lowe, D. Chemical reactions from US patents (1976 -Sep2016). figshare 449 \nhttps://figshare.com/articles/dataset/Chemical_reactions_from_US_patents_1976-450 \nSep2016_/5104873. (accessed August 29, 2023). 451 \n30 Lowe, D. M. Extraction of chemical structures and reactions from the literature. Ph.D. 452 \nThesis, University of Cambridge, (2012). 453 \n31 NextMoveSoftware. Pistachio. https://www.nextmovesoftware.com/pistachio.html. 454 \n(accessed August 22, 2023). 455 \n32 Peng, A., Wu, M., Allard, J., Kilpatrick, L. & Heidel, S. GPT -3.5 Turbo fine-tuning and 456 \nAPI updates. https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates. 457 \n(accessed August 22, 2023). 458 \n33 Dettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. Qlora: Efficient finetuning of 459 \nquantized llms. Preprint at arXiv https://arxiv.org/abs/2305.14314 (2023). 460 \n34 Kwon, W. et al. in Proceedings of the 29th Symposium on Operating Systems Principles     461 \n611–626 (Association for Computing Machinery, Koblenz, Germany, 2023). 462 \nAcknowledgements 463 \nWe thank all contributions of the open-source community on LLMs. We appreciate Yaghi's 464 \ngroup for guiding in ChatGPT prompt engineering for chemistry tasks. 465 \nThis work was supported by National Natural Science Foundation of China (T2225002, 466 \n82273855 to M.Y.Z., and 82204278 to X.T.L.), the National Key Research and Development 467 \nProgram of China (2022YFC3400504 to M.Y.Z.) , SIMM-SHUTCM Traditional Chinese 468 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n26 \n \nMedicine Innovation Joint Research Program (E2G805H to M.Y.Z.), and Shanghai Municipal 469 \nScience and Technology Major Project. 470 \nContributions 471 \nW.Z., J.C.X., Z.Y.F., and M.Y.Z. conceived the idea. M.Y.Z and Z.Y.F designed the research. 472 \nW.Z., Q.G.W. , Z.M.H. implemented the codes. W.Z., Q.G.W., X.T.K, J.C.X, S.K.N., Z.Y.F . 473 \ncollected, annotated, and processed training data. D.H.C. , B.Y.N., Q.S., and X.T.L. checked the 474 \ndata. M.A.C., R.Z.Z., Y.T.W., L.H.Z benchmarked the models. W.Z. wrote the initial draft. M.Y.Z., 475 \nZ.Y.F. and Z.P.X. reviewed and refined the article. All authors contributed to the analysis of the 476 \nresults. All authors read and approved the final manuscript. 477 \nCompeting interests 478 \nThe authors declare no competing interests. 479 \nhttps://doi.org/10.26434/chemrxiv-2023-k7ct5-v2 ORCID: https://orcid.org/0000-0002-3323-3092 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5015687942504883
    },
    {
      "name": "Natural language processing",
      "score": 0.46596601605415344
    },
    {
      "name": "Fine-tuning",
      "score": 0.4247344732284546
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3909437656402588
    },
    {
      "name": "Physics",
      "score": 0.15754473209381104
    },
    {
      "name": "Particle physics",
      "score": 0.07706892490386963
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210112820",
      "name": "Shanghai Institute of Materia Medica",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I131434179",
      "name": "Nanjing University of Chinese Medicine",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210123185",
      "name": "Zhejiang Lab",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I30809798",
      "name": "ShanghaiTech University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3018771216",
      "name": "LMU Klinikum",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    }
  ]
}