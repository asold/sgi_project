{
    "title": "RecBERT: Semantic Recommendation Engine with Large Language Model Enhanced Query Segmentation for k-Nearest Neighbors Ranking Retrieval",
    "url": "https://openalex.org/W4390755743",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2099491818",
            "name": "Richard Wu",
            "affiliations": [
                "San Francisco Unified School District"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2920277907",
        "https://openalex.org/W2969322340",
        "https://openalex.org/W2165533158",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3201463768",
        "https://openalex.org/W2898151875",
        "https://openalex.org/W2008886893",
        "https://openalex.org/W2905293029",
        "https://openalex.org/W2159094788",
        "https://openalex.org/W2137028279",
        "https://openalex.org/W2612388534",
        "https://openalex.org/W2914021712",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W3162462834",
        "https://openalex.org/W4385573170",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W3153269634",
        "https://openalex.org/W4285590193",
        "https://openalex.org/W3102839769",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6847753483",
        "https://openalex.org/W6850625674",
        "https://openalex.org/W6853251322",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W6748854608",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4311991106",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "The increasing amount of user traffic on Internet discussion forums has led to a huge amount of unstructured natural language data in the form of user comments. Most modern recommendation systems rely on manual tagging, relying on administrators to label the features of a class, or story, which a user comment corresponds to. Another common approach is to use pre-trained word embeddings to compare class descriptions for textual similarity, then use a distance metric such as cosine similarity or Euclidean distance to find top k neighbors. However, neither approach is able to fully utilize this user-generated unstructured natural language data, reducing the scope of these recommendation systems. This paper studies the application of domain adaptation on a transformer for the set of user comments to be indexed, and the use of simple contrastive learning for the sentence transformer fine-tuning process to generate meaningful semantic embeddings for the various user comments that apply to each class. In order to match a query containing content from multiple user comments belonging to the same class, the construction of a subquery channel for computing class-level similarity is proposed. This channel uses query segmentation of the aggregate query into subqueries, performing k-nearest neighbors (KNN) search on each individual subquery. RecBERT achieves state-of-the-art performance, outperforming other state-of-the-art models in accuracy, precision, recall, and F1 score for classifying comments between four and eight classes, respectively. RecBERT outperforms the most precise state-of-the-art model (distilRoBERTa) in precision by 6.97% for matching comments between eight classes.",
    "full_text": " \nRecBERT: Semantic recommendation engine with large language\nmodel enhanced query segmentation for\nk-nearest neighbors ranking retrieval\nRichard Wu\nAbstract: The  increasing  amount  of  user  traffic  on  Internet  discussion  forums  has  led  to  a  huge  amount  of\nunstructured  natural  language  data  in  the  form  of  user  comments.  Most  modern  recommendation  systems  rely  on\nmanual tagging, relying on administrators to label the features of a class, or story, which a user comment corresponds\nto.  Another  common  approach  is  to  use  pretrained  word  embeddings  to  compare  class  descriptions  for  textual\nsimilarity, then use a distance metric such as cosine similarity or Euclidean distance to find top  k neighbors. However,\nneither approach is able to fully utilize this user -generated unstructured natural language data, reducing the scope of\nthese recommendation systems. This paper studies the application of domain adaptation on a transformer for the set\nof user comments to be indexed, and the use of simple contrastive learning for the sentence transformer fine -tuning\nprocess to generate meaningful semantic embeddings for the various user comments that apply to each class. In order\nto match a query containing content from multiple user comments belonging to the same class, the construction of a\nsubquery  channel  for  computing  class-level  similarity  is  proposed.  This  channel  uses  query  segmentation  of  the\naggregate query into subqueries, performing  k-nearest neighbors (KNN) search on each individual subquery. RecBERT\nachieves state -of-the-art performance, outperforming other state -of-the-art models in accuracy, precision, recall, and\nF1 score for classifying comments between four and eight classes, respectively. RecBERT outperforms the most precise\nstate-of-the-art model (distilRoBERTa) in precision by 6.97 % for matching comments between eight classes.\nKey  words:   sentence  transformer; simple  contrastive  learning; local  language  models; query  segmentation; k -nearest\nneighbors\n1    Introduction\n1.1    Background\nFinding  meaningful  representations  of  text  documents\nto  compute  document-level  similarity  has  been  a\nclassic  problem  in  the  field  of  natural  language\nprocessing  (NLP).  In  1954,  the  bag  of  words  (BoW)\nmodel  was  proposed  by  Harris[1],  creating  a  vector\nrepresentation  for  each  document  composed  of  the\nfrequencies of each word in a govern vocabulary. BoW\nmodels have been enhanced with the addition of term\ninverse  document  frequency  (TF-IDF)[2].  However,\nBoW  models  are  not  scalable  for  larger  sets  of\ndocuments,  as  the  dimensions  of  sparse  frequency\nvectors  will  grow  extremely  large  as  the  vocabulary\nincreases  for  larger  text  corpuses[3].  Conventional\ndistance  metrics  such  as  Euclidian  distance  become\nsignificantly  less  effective  for  computing  similarity  in\nhigh  dimensional  vectors,  as  the  distances  between\nnearest  and  farthest  vectors  become  nearly\nindistinguishable from each other[3].\nRecent  progress  with  the  transformer  architecture\nproposed  in  2017  by  Vaswani  et  al.[4] introduces\nparallelizable  computations  while  also  maintaining\ncore  attention  mechanisms  for  contextualized  word\nembeddings.  It  uses  dense  vector  embeddings  to\n \n  Richard Wu is with the Dublin Unified School District and the\nSF  Artificial  Intelligence  Club,  Dublin,  CA  94568,  USA.  E-\nmail: kingdomcalifornia@gmail.com.\n    Manuscript  received: 2023 -09-18;  revised:  2023 -09-30;\naccepted: 2023-10-10\nIntelligent  and  Converged  Networks ISSN  2708-6240\n2024, 5(1): 1−11 DOI: 10.23919/ICN.2024.0004\n \n©  All articles included in the journal are copyrighted to the ITU and TUP. This work is available under the CC BY-NC-ND 3.0 IGO license:\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/igo/.\nrepresent  documents  as  opposed  to  the  sparse\nfrequency  vectors  generated  from  BoW  models.\nBERT[5] uses  masked  language  modeling  (MLM)  as\nopposed to casual language modeling, which allows it\nto generate bidirectional representations of words[5].\n1.2    Motivation\nWith  the  increasing  number  of  user-generated\nrecommendations  and  descriptions  for  documents  in\ndiscussion  forums,  the  amount  of  unstructured  natural\nlanguage  data  has  greatly  increased.  However,  most\nmodern  recommendation  systems  use  manual  tagging\nof a document’s features to perform collaborative[6–10]\nor  content-based[11–13] filtering.  Some  directly  use\npretrained  word  embeddings  to  compare  queries  with\ndescriptions  for  textual  similarity,  then  use  a  distance\nmetric  such  as  cosine  similarity  or  Euclidean  distance\nto find top k neighbors. These approaches do not utilize\nthe full potential of user-generated unstructured natural\nlanguage data in their recommendations.\nMost  semantic  embedding  models  such  as\nWord2Vec or BERT are trained on large-scale general-\ndomain  datasets,  such  as  Wikipedia.  While  these\npretrained  representations  have  a  good  understanding\nof  general  language  patterns,  their  performance  may\ndegrade  for  more  specific  domains,  such  as  user\ncomments for stories. That is, the context necessary for\nthe  transformer  to  learn  on  the  domain  of  general\narticles compared to the domain of user comments may\ndiffer.\nAdditionally,  there  is  the  issue  of  user  queries\npotentially  containing  content  from  multiple  sources.\nFinding  textual  similarity  between  the  full  query  and\nthe  individual  documents  (user  comments),  each\ncontaining a fragment of the user query, is not ideal in\nfinding  accurate  similarity  across  classes. Figure  1\nshows that when a query is essentially the combination\nof  two  separate  documents,  both  corresponding  to  the\nsame class, the similarity between the query and each\ndocument individually is lower than its true similarity\nwhen  considering  the  query  directly  against  the  class.\nA  KNN  search  based  only  on  the  full  query’s\nembedding  is  likely  to  return  erroneous  results  when\nrequiring  components  from  different  comments\nbelonging  to  the  same  class  in  order  to  separately\nmatch  to  each  subquery.  For  example,  the  class  to  be\nranked, class x, may contain a comment corresponding\nto subquery 1, and another comment corresponding to\nsubquery n.  Directly  comparing  semantic  similarity\nbetween  the  aggregate  query  and  each  comment\ncontaining  a  component  of  that  query  won’t  fully\nencapsulate the similarity between the class and query.\nThus, in order to properly match the query against the\ndocuments, or comments, corresponding to a class, the\nquery must be split into its subqueries and have KNN\nsearch independently conducted on each subquery.\n1.3    Main idea\nThis  paper  proposes  a  semantic  recommendation\nsystem,  RecBERT,  by  first  further  pretraining  a\nbidirectional  transformer  using  MLM[5] for  domain\nadaptation on all user comments for the set of classes.\nDistance metrics such as cosine similarity can be used\nto  rank  comments  most  similar  to  the  query.  Without\nthe  initial  domain  adaptation,  the  fine  tuning  of  the\nsentence  transformer  afterward  will  yield  very  poor\nresults,  overfitting  on  specific  phrases,  limiting  the\nscope of top k neighbor results. In this paper, domain\nadaptation will be applied to story recommendations by\nadapting  on  a  dataset  of  user-generated  review\ncomments, but this approach can also be used for other\ndomains, such as biology texts[14] or medical diagnosis\nby collecting patient reports (which are the equivalent\nof  user  comments  in  this  scenario)  for  varying\ndiseases[15].\nIt  is  important  to  note  that  directly  averaging  the\npooled domain adapted BERT embeddings yields very\n \nQuery\nSubquery 1\nSubquery 2\nComment 1\n(class x)\nComment 2\n(class x)\nComment 2\n(class y)\nComment 2 (class y)\n \n \nFig. 1    Visualization of splitting a query into its subqueries\nfor improved accuracy.\n    2 Intelligent and Converged Networks,  2024, 5(1): 1−11\n \npoor  results[16-17].  This  leads  to  the  need  to  fine-tune\nthe domain adapted model using a Siamese network[18]\nwith  simple  contrastive  learning[19] (SimCSE)  to\nproduce  a  model  capable  of  encoding  accurate\nsemantic  representations  of  user  comments.\nAdditionally,  for  finding  comment  similarity,  a  cross-\nencoder  structure  would  be  viable  for  training,  but\nduring  runtime,  would  be  too  computationally\nexpensive  to  conduct  sentence  similarity  across  large\ndatasets[18].  Thus,  a  Siamese  network  is  used  to\ncompute  absolute  vector  representations  of  texts,\nremoving  the  need  to  run  each  text  pair  through  a\nclassification  network  to  compute  similarity.  With\nabsolute  sentence-level  embeddings,  cosine  similarity\nor  another  distance  metric  can  be  used  directly  to\nconduct  a  similarity  search  such  as  KNN  or  ANN  on\ncomment and query embeddings to retrieve similar user\ncomments to the query.\nTo solve the task for query segmentation, we propose\nusing  a  pretrained  large  language  model  (LLM)  with\nfew-shot  prompting  to  segment  each  user  query  into\nrelevant  subqueries  to  conduct  cosine  similarity  on\nwith  the  user  comments  available.  The  few-shot\nsamples would preferably come from the same domain\nas  the  user  comments  for  maximal  accuracy  in\nsegmentation.  By  finding  cosine  similarity  between\nsubqueries  and  the  available  user  comments,  we  are\nable to consider all the components of a user query to\nholistically  determine  the  most  similar  class.  The\noriginal  query  will  also  have  cosine  similarity\nperformed  against  user  comments,  and  its  top k\nneighbors  found,  the  justification  being  that  direct\nmatches between the aggregate query and comment are\nstill  possible,  and  should  be  weighted  more  than  a\nsubquery match. We conducted the experiment on the\nmyanimelist  dataset,  and  achieved  state-of-the-art\nperformance.\nOverall, the main contributions of this study are:\n(1) RecBERT is the first BERT based model further\npretrained  on  the  MyAnimeList  dataset  of  user\ncomments  and  fine-tuned  for  comment  embeddings\nwith SimCSE to construct a recommendation system.\n(2)  Experimental  results  on  the  comment  dataset\ndemonstrate  that  further  pretraining  BERT  on  user\ncomments  improves  its  performance.  RecBERT\nobtains state-of-the-art performance on determining the\nclass a comment corresponds to.\n(3)  Study  the  application  of  LLM-enhanced  query\nsegmentation for effective KNN search on most similar\nclasses to the query.\n(4)  Propose  a  recommendation  system  using  LLM-\nenhanced  query  segmentation  to  construct  a  separate\nsubquery  channel  for  computing  class  level  similarity\nto enhance ranking retrieval performance.\n2    Related work\n2.1    Recommendation systems\nMany  recommendation  systems  have  been  proposed\nwith  various  architectures.  Some  prominent  methods\ninclude  collaborative[6–10] and  content-based[11–13]\nfiltering.  Content  filtering  refers  to  the  idea  that  users\nshould  be  recommended  items  similar  to  those  they\nviewed or purchased before[11]. This is a useful method\nfor  recommending  products  in  a  website  the  user  has\nfrequently purchased from, but in the case where they\nhave not had any traffic on the website, it is unviable.\nThe  assumption  is  that  all  products,  or  classes,  must\nhave labels assigned to them showing the features they\ncontain[11]. The prerequisite for content-based filtering\nis  data  labeled  with  features.  A  similar  requirement\nexists  with  collaborative  filtering,  which  assumes  that\nusers  who  shared  a  similar  purchase  or  interest  will\nhave other similar interests as well[6]. In addition, if the\nuser  wants  to  send  a  query  in  natural  language  form,\ncollaborative  or  content  filtering  are  infeasible.\nRecBERT  aims  to  mitigate  this  problem  by  directly\nfinding  most  similar  comments  to  the  query  through\nconducting  cosine  similarity  on  their  semantic\nembeddings.\nAnother type of recommendation system adds a final\nlayer  to  a  BERT  model  for  downstream\nclassification[21, 22].  This  architecture  allows  for  users\nto  give  text  input  and  expect  a  list  of  probabilities  of\nthe classes it most likely corresponds to, which makes\nit  more  flexible  than  content-based  or  collaborative\nfiltering[7].  However,  the  lack  of  document-level\nembeddings  makes  it  difficult  to  ascertain  the  exact\n  Richard Wu:   RecBERT: Semantic Recommendation Engine with Large Language Model Enhanced Query… 3\n \ndocument  that  led  to  these  probabilities.  RecBERT\nconducts KNN search on comment embeddings, adding\ntransparency as to what comments in the top returned\nclasses  were  the  cause  of  the  class’s  high  similarity\nranking.\nAnother type of recommendation system, specifically\nfor QA, uses KNN search[20] to find the most probable\nresponses.  This  is  performed  at  the  contextualized\nword  embedding  level  of  BERT,  while  RecBERT\nworks  at  the  sentence,  or  document  level,  fine-tuning\nBERT  using  SimCSE  with  margin  ranking  loss\n(MNR).  RecBERT  also  adds  query  segmentation  to\nuser queries as a separate channel to find most similar\nclasses instead of only conducting KNN search on the\nfull user query.\n2.2    Domain adaptation\nDomain adaptation on the base BERT model has been\napplied in many fields, such as biological texts[14] and\nmedical texts[15]. The authors of BioBERT describe the\nsuccess  of  domain  adaptation  for  biological  texts,\nnoting  increases  in  F1  score  and  MNR\nimprovements[14].  MedBERT’s  domain  adaptation  has\nalso  improved  the  AUC  for  producing  accurate\ncontextualized  embeddings  for  electronic  health\nrecords  (EHR)[15].  However,  domain  adaptation  in\nthese cases is being applied to produce contextualized\nword embeddings. These models have not been applied\nfor  generating  sentence-level  embeddings.  RecBERT\nuses  domain  adaptation  for  user  comments  on  stories,\nspecifically  the  myanimelist  dataset.  It  uses  domain\nadaptation to help create more accurate contextualized\nembeddings  for  the  user  comments,  while  also\npreventing  overfitting  once  applying  sentence-level\nfine-tuning.  RecBERT  then  uses  the  domain  adapted\ntransformer  model  for  sentence-level  embeddings  by\nfine-tuning it with SimCSE with MNR.\n2.3    LLM with few-shot learning\nThe method of using LLM’s with few-shot learning[26]\nto  automate  previously  human-done  tasks  has  been\napplied  to  a  variety  of  different  fields,  such  as\nreinforcement learning through AI feedback[23] (RLAF)\nand  Q&A.  Since  the  popularization  of  open  source\nLLM’s  after  LLaMa[24],  there  have  been  many\ninstruction  fine-tuned  models  created  with  LLaMa  as\nthe base model. The introduction of quantized language\nrepresentation  adaptation  (QLoRA)[25] has  further\nenhanced the capabilities to fine-tune quantized LLM’\ns. It has made it more feasible to fine-tune LLM’s even\non  consumer  grade  GPUs  with  limited  computational\nresources[25].\nThe usage of local LLM’s with few-shot learning[26]\nmakes it possible to run RecBERT, specifically query\nsegmentation,  locally  on  consumer  grade  hardware.\nRecBERT  utilizes  LLM-enhanced  query  segmentation\nto conduct cosine similarity between all comments and\neach  subquery,  creating  a  separate  channel  for\ncomputing  class  level  similarity  to  subqueries,\nenhancing ranking performance.\n3    Method\nThe  framework  of  RecBERT  consists  of  three  key\ncomponents.  RecBERT  first  performs  domain\nadaptation,  or  further  pretraining,  on  the  dataset\nconsisting of user comments. Then, the domain adapted\ntransformer is fine-tuned for document-level similarity\nusing  SimCSE,  a  method  useful  for  fine-tuning  on\nunlabeled  data  such  as  user  comments[19].  Query\nSegmentation enhanced with LLM’s is used to conduct\nKNN  search  independently  on  all  subqueries,  using  a\nlist  of  similarities  for  each  subquery  per  class  for\nfurther ranking retrieval.\n3.1    Domain adaptation of BERT\nThe  base  transformer  architecture  of  RecBERT  is  the\nsame  as  BERT.  BERT  was  pretrained  on  general-\ndomain  datasets,  namely  Wikipedia  and  the  Brown\nCorpus. The domain of user comments for stories has\nvarious domain-specific language (e.g. Isekai, Slice of\nLife,  Deus  Ex  Machina)  understood  by  readers.  This\ncauses  the  classic  BERT  model  to  perform  poorly  on\ngenerating meaningful semantic representations of user\ncomments. Figure 2 shows how the domain adaptation\nof  the  BERT  model  to  the  user  comments  dataset\nallows  better  contextualized  representations  of  the\nvocabulary.\nFirst,  a  dataset  consisting  of  classes  and  the  user\n    4 Intelligent and Converged Networks,  2024, 5(1): 1−11\n \ncomments  corresponding  to  them  is  acquired.  The\nRoBERTa model is used as the base model for further\npretraining. All user comments are tokenized using the\nsame  tokenizer  as  the  transformer  base  model.  The\nmasking probability to conduct MLM with is 15%, the\nsame  as  what  was  proposed  in  the  original  BERT\npaper. Further pretraining using MLM is conducted on\nthe  user  comments  for  10  epochs  for  domain\nadaptation.\n3.2    Fine-tuning  domain  adapted  BERT  with\nSiamese network and SimCSE\nDirectly  averaging  the  pooled  domain  adapted  BERT\nembeddings  yields  very  poor  results.  To  compute\ncomment-level  similarity  instead  of  word-level\nsimilarity,  Siamese  network[18] with  SimCSE [19] and\nMNR  is  proposed  to  produce  a  model  capable  of\nencoding  accurate  semantic  representations  of  user\ncomments.  Absolute  comment-level  embeddings  are\npreferred  over  the  cross-encoder  structure  due  to  far\nless  computation  being  needed  during  query  time.\nSimCSE  is  preferred  over  other  training  methods  due\nto  removing  the  need  for  labeled  data  while  fine-\ntuning,  which  removes  the  need  to  make  the\nassumption that all comments related to a class must be\nsimilar. It is critical to not assume this, as there can be\nmultiple titles, or classes, that share similar themes, and\nthus have comment overlap. The usage of SimCSE for\nfine-tuning comment embeddings is as follows:\n\u000b\nLet i be the current comment to conduct MNR on.\n\u000b\nX\nFor i , a batch of  comments may be defined as \nP = A\n \n \nN = f8n \u0012 Xjn , Ag\n \n \njAj = jPj = 1\n \n \njNj = jXj \u0000 1\n \nA\n\u000b\nP\nA\nN\nX\nwith  equal to i as the anchor,  equal to  as the\npositive, and  as the set of all other comments within\n .\nCosine  similarity  used  for  similarity  comparison\nbetween comment embeddings, defined as \nD(A; B) = jA\u0001 Bj\njjAjj \u0002 jjBjj (1)\n \nLet e(x) represent the embedding function to convert\na  text  object x into  its  sentence  embedding.  The\nmultiple  negatives  ranking  loss  function  for  batch X\ncan be defined as follows: \nL(A; R; N) =\nn∑\ni=1\n(D( e(A) ; e(P)) + D(e(A) ; e(Ni)))\n \nBecause P = A, this can be simplified to \nL(A; P; N) =\nn∑\ni=1\n(D( e(A) ; e(A)) + D(e(A) ; e(Ni))) (2)\n \nFirst,  the  saved  domain  adapted  transformer  is  used\nas the base for fine-tuning. The same dataset consisting\nof  story  titles  (class  labels)  and  the  user  reviews\ncorresponding to them is used as training data for fine-\ntuning  at  the  comment  level.  Sentence  pairs  of  the\nsame sentence are generated, which will have different\nencodings when passed into the transformer model due\nto  dropout[19].  A  batch  size,  such  as  128,  is  used  for\ntraining.  Too  large  of  a  batch  size  may  prevent\nconvergence.  The  multiple  negatives  ranking  loss\nfunction is used during training to minimize distances\nbetween  the  same  sentence,  and  maximize  distances\nbetween  all  other  sentences  in  the  batch.  The  domain\nadapted transformer is fine-tuned for 20+ epochs.\n3.3    Query  segmentation  with  LLM  for  ranking\nretrieval\n3.3.1    Query segmentation with LLM\nWhile  comment  embeddings  are  now  more  accurately\nrepresented, there is still the issue of the full user query\npotentially containing content from multiple sources in\nthe  same  class.  This  may  cause  inaccurate  search\nresults,  as  using  cosine  similarity  between  all\ncomments  and  the  full  query  will  give  lower\nsimilarities  between  the  full  query  and  a  comment\nconsisting of one section of that query, while not taking\n \nBefore domain adaptation\nWord 1\nWord 2\nWord 3\nWord 4\nWord n\nWord 1\nWord 2\nWord 3\nWord 4\nWord n\n……\nWord 1\nWord 2\nWord 3\nWord 4\nWord n\nWord 1\nWord 2\nWord 3\nWord 4\nWord n\n……\nAfter domain adaptation\n \n \nFig. 2    Example  of  domain  adaptation  improving  accuracy\nof contextualized word embeddings.\n  Richard Wu:   RecBERT: Semantic Recommendation Engine with Large Language Model Enhanced Query… 5\n \ninto account the other comments belonging to the same\nclass  that  constitute  the  additional  sections  of  that\nquery.  That  is,  the  similarity  between  the  query  and\neach  comment  individually  is  lower  than  its  true\nsimilarity  when  considering  the  query  directly  against\nthe  class.  To  solve  this  problem,  query  segmentation\nwith an LLM is proposed.\nN\n\u0012\nLet  γ  represent  the  user  query,  Assume ∃ \nsubqueries {γ1, γ2, ..., γN}  γ.\nA  pretrained  LLM  assistant  model  performs  the\nsegmentation task F(γ) = {γ1, γ2, ..., γN}, segmenting an\naggregate query into its subqueries.\nThe assistant model is given a prompt to segment the\nquery  into  its  parts,  and  is  provided  few-shot  samples\nof the segmentation task.\nFormat of prompt and responses:\nConsider the following user query:\n[Instructions to perform segmentation on the separate\nthemes of the query]\nQuery: [Example User Query]\nSegmented Query:\n[Example Segment 1]\n[Example Segment 2]\n[Example Segment n]\nQuery: [Actual User Query]\nSegmented Query:\n[To be filled in by assistant model]\nThe  returned  segmented  output  {γ1,  γ2,  ...,  γN}  may\nthen be used for ranking retrieval.\n3.3.2    Ranking retrieval\n\r\nQ\nQ\nc\n\u000b\n\r\n\r\nTo find the classes that best match the query, segment\noriginal query  into {γ1, γ2, ..., γN}. Let  be defined\nas  {γ1,  γ2,  ...,  γN}.  KNN  search  is  performed  on  each\nelement in , outputting the most similar comment i\ncorresponding to each subquery for each class i. But\n  will  also  have  cosine  similarity  conducted  between\nall  comments  and  itself,  considering  the  possibility\nfor a comment to contain a large enough of subqueries\nto maintain a high similarity between the full query and\nitself. The similarities found for each class construct a\nranking system for the most similar classes that fit the\nquery. Figure  3 shows  the  methodology  of  query\n \nSegment user query into\nlist of subqueries\nStart\n(Given user query)\nFind Nearest Neighbor\nfor full query on\neach class\nAdd similarity fo Nearest Neighbor\nper class to each class list\nChoose maximum\nsimilarity between the\ntwo similarities per class\nRank classes based on\nthese final similarities\nClamp results to\nbetween 0 and 1\nPass each similarity\ninto inverse hyperbolic\ntangent function\nFind average\nsimilarity in each\nclass list\nAdd similarity of Nearest Neighbor\nper class to each class list\nFind Nearest Neighbor for\nsubquery i on each class\nYes\nNo\nEnd\ni=i+1\ni=0\ni<len(subqueries)\n \n \nFig. 3    Flowchart of query segmentation and ranking retrieval methodology.\n    6 Intelligent and Converged Networks,  2024, 5(1): 1−11\n \nsegmentation and ranking retrieval for RecBERT.\nS 1\nFor each class which contains a set of comments, let\nits  original  query  channel  similarity  be  defined  as\nthe  maximum  similarity  found  with  KNN  search  for\nthe aggregate query. \nS 1 = max(cos_sim(e(\r) ; e(A) ))); 8A \u0012 knn(e(\r)) (3)\n \ns2\nFor each class which contains a set of comments, let\nits raw subquery channel similarity  be defined as the\naverage  of  the  maximum  similarity  found  with  KNN\nsearch for each subquery. \ns2 = 1\nn\nn∑\ni=1\nmax(cos_sim(e(Qi); e(B)) )); 8B \u0012 knn(e( Qi))\n(4)\n \ns2\ntanh\u00001(x\n(0; 1)\nS 2\n  is passed through ), a concave up function\non  the  interval  to  adjust  for  the  fact  that  there\nmust  be  a  large  proportion  of  subqueries  matched  by\ndifferent comments in the same class to constitute the\nquery being similar to the class. The adjusted similarity\n  is clamped to a maximum of 1. \nf (x) = tanh\u00001 (x) ; x 2 (0; 1) (5)\n \n \nS 2: = max (1; f (s2)) (6)\n \n \n \nS\nS 1\nS 2\nS\nThe  final  similarity  is  then  defined  as  the\nmaximum  similarity  between  and .  Top  ranked\nclasses can then be found by sorting based on . \nS = max (S 1; S 2) (7)\n \n4    Experimental result\nTo  prepare  the  data  for  domain  adapting  BERT,  a\ncorpus of 112 000 total user reviews of varying length\nfor  1000  different  titles  (classes)  was  collected  from\nmyanimelist.  The  dataset  contained  user  comments\nlabeled with the title it corresponded to. A 20 training-\nvalidation  split  was  used,  leaving 89 600 reviews  for\nthe training set, and 22 400 reviews for the testing set.\nDefault truncation was performed to allow all reviews\nto  have  128  tokens.  Reviews  were  preprocessed  by\nconducting wordpiece tokenization on the text, adding\nthe  CLS  and  SEP  tokens,  and  encoding  these  tokens\ninto indexes corresponding to the BERT vocabulary,\nFurther  pretraining  was  conducted  on  a  distilBERT\nuncased  model  and  roBERTa[27] model  respectively.\nUsing  a  P100  GPU,  further  pretraining  with  Masked\nLanguage  Modeling  was  conducted  by  having  the\nmodel predict masked tokens in the user comments. A\n15% masking  probability  was  used,  the  same  as\nproposed  in  the  original  BERT  paper.  Training  was\nconducted for 10 epochs, using Adam optimizer with a\n2 × 10−5 learning rate.\nThe roBERTa model exhibited more accurate MLM\nresults,  approaching  a  loss  of  1.55,  while  the\ndistilBERT approached a loss of 1.88.\nNext, the domain adapted transformer was fine tuned\nfor document level similarity using SimCSE with MNR\nLoss,  using  the  same  user  comments  as  when  domain\nadapting the transformer base models. A batch size of\n128 was used.\nTables  1 and 2 show  the  results  of  the\nrecommendation system on comment class recognition.\nTests  are  conducted  on  BERT,  distilBERT[28],\nRoBERTa,  distilRoBERTa,  and  RecBERT.  RecBERT\noutperforms all state-of-the-art models in both the four-\nclass  classification  and  eight-class  classification  tasks.\nRecBERT outperforms the most precise state-of-the-art\nmodel (BERT) in precision by 1.63% when predicting\nbetween  4  classes,  and  outperforms  the  most  precise\nstate-of-the-art model (distilRoBERTa) in precision by\n6.97% when  predicting  between  8  classes.  As  shown\nby  these  results,  RecBERT’s  effectiveness  increases\nwhen  predicting  between  larger  amounts  of  classes.\nThis  may  be  attributed  to  the  need  for  more  precise\ncontext  understanding  as  classification  is  conducted\nbetween  more  classes.  The  fine-tuning  on  RecBERT\nwith  these  user  comments  allows  the  model  to\nunderstand  the  necessary  context  for  the  words  in  the\nspecified domain. Figure 4 shows the confusion matrix\nand multi-class ROC curves for RecBERT in the four-\nclass and eight-class classification tasks.\nThe  T-SNE  visualization  of  user  reviews  in  two-\ndimensional space[29] as shown in Fig. 5 demonstrates\nmoderate  clustering  of  embeddings  based  on  classes.\nThis is expected, as the comments belonging to a single\nclass should not actually fully cluster int the first place,\nconsidering  how  different  user  comments  should  be\nproviding different content to help satisfy the different\n  Richard Wu:   RecBERT: Semantic Recommendation Engine with Large Language Model Enhanced Query… 7\n \n \nTable 1    Test results in comment class recognition for four classes. (%)\nModel Accuracy Precision Recall F1 score\nBERT cased 78.33 78.19 78.09 78.13\ndistilBERT cased 77.22 77.33 76.99 76.98\nroBERTa (Baseline) 73.89 72.96 73.47 72.56\ndistilroBERTa 75.00 73.54 74.51 73.61\nrecBERT 79.72 79.82 79.63 79.62\n \nTable 2    Test results in comment class recognition for eight classes. (%)\nModel Accuracy Precision Recall F1 score\nBERT cased 50.72 48.73 49.49 48.51\ndistilBERT cased 54.74 51.81 52.88 52.14\nroBERTa (Baseline) 54.02 52.66 51.71 49.75\ndistilroBERTa 54.31 53.63 52.35 51.92\nrecBERT 61.21 60.60 60.58 60.12\n \n0\nConfusion matrix Confusion matrix\n1\n2True label\n3\n1.0\nReceiver operating characteristic (ROC) curve\n0.8\n0.6True positive rate\n0.4\n0.2\n0 0.2 0.4\nFalse positive rate\n0.6\nMicro-average ROC curve (area=0.95)\nMacro-average ROC curve (area=0.95)\nROC curve of class 0 (area=0.96)\nROC curve of class 1 (area=0.97)\nROC curve of class 2 (area=0.91)\nROC curve of class 3 (area=0.97)\nMicro-average ROC curve (area=0.91)\nMacro-average ROC curve (area=0.90)\nROC curve of class 0 (area=0.93)\nROC curve of class 1 (area=0.97)\nROC curve of class 2 (area=0.83)\nROC curve of class 3 (area=0.93)\nROC curve of class 4 (area=0.97)\nROC curve of class 5 (area=0.86)\nROC curve of class 6 (area=0.88)\nROC curve of class 7 (area=0.86)\n0.8 1.0\n1.0\nReceiver operating characteristic (ROC) curve\n0.8\n0.6True positive rate\n0.4\n0.2\n0 0.2 0.4\nFalse positive rate\n0.6 0.8 1.0\n1\n0\n0 1 2 3 4 5 6 7\n3\n2\n4\n5\n6 True label\n7\n321\nPredicted label Predicted label\n0\n4 4 7 78\n68 7 20\n00\n5 5\n5\n5\n5\n5\n6\n6\n0\n0\n0\n0\n0\n2\n2\n2\n2\n2\n2\n2\n22\n5 51\n1\n1\n1\n1 1\n1\n11\n11\n11\n13\n25 28\n29\n16\n8\n8\n83 7\n77\n7\n3 3\n3\n3\n44\n4\n72\n72\n44\n45\n68\n8 9 62 6\n4 79 5 2\n68 4 18 2 70\n60\n50\n70\n30\n20\n10\n70\n60\n50\n70\n30\n20\n10\n0\n \n \nFig. 4    Confusion matrix and multi-class ROC curves for RecBERT on four and eight classes, respectively.\n    8 Intelligent and Converged Networks,  2024, 5(1): 1−11\n \npotential subqueries within the query.\n5    Conclusion\nIn  this  paper,  RecBERT  is  introduced  as  a\nrecommendation  system  composing  of  domain\nadaptation,  fine-tuning  of  sentence  transformer  and\nquery  segmentation  to  enhance  ranking  retrieval.  The\nusage  of  domain  adaptation  of  BERT  along  with\nSimCSE with MNR to develop accurate sentence-level\nembeddings  for  comments  is  studied.  This  paper  also\nexplores  the  usage  of  LLM-enhanced  query\nsegmentation on the aggregate user query to construct a\nseparate  subquery  similarity  channel  by  performing\nseparate KNN searches for each subquery and passing\nthe average subquery similarity per class through the a\nconcave  up  function,  specifically  the  inverse\nhyperbolic  tangent,  to  find  an  adjusted  similarity,\naccounting for the fact that a larger number of queries\nbeing satisfied should increase the similarity in a non-\nlinear fashion. The positive experimental results shown\ndemonstrate  the  feasibility  of  this  approach,  as\nRecBERT achieved state-of-the-art accuracy, precision,\nrecall, and F1 score on the myanimelist dataset for four\nand eight class classification respectively.\nHowever, RecBERT’s scalability remains a concern.\nKNN  search  is  very  computationally  expensive,\nespecially  as  the  number  of  comments  to  index  is\nsubject  increase  in  large-scale  applications.  In  the\nfuture,  there  will  be  a  focus  on  improving  the\nscalability  of  RecBERT  by  using  more  approximate\nsearch algorithms. The accuracy of RecBERT may also\nbe further improved by building a gold-standard dataset\nfor domain adaptation, as some user comments may not\nbe  representative  of  the  majority  of  comments,\npreventing  convergence.  Additionally,  the  sentence\nlengths  of  user  comments  may  be  standardized  to\nprevent  unaccounted  variability  from  affecting\nsimilarity search results.\nReferences \n Z. S. Harris, Distributional structure, in Papers on Syntax,\nH. Hiż Ed. Dordrecht, the Netherlands: Springer, 1981, pp.\n3–22.\n[1]\n S. -W. Kim and J. -M. Gil, Research paper classification\nsystems  based  on  TF-IDF  and  LDA  schemes, Hum.\nCentric Comput. Inf. Sci., vol. 9, no. 1, p. 30, 2019.\n[2]\n C.  C.  Aggarwal,  A.  Hinneburg,  and  D.  A.  Keim,  On  the\nsurprising  behavior  of  distance  metrics  in  high\ndimensional  space,  in Proc.  8th  Int.  Conf.  Database\nTheory, London, UK, 2001, pp. 420–434.\n[3]\n A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you\nneed, arXiv preprint arXiv: 1706.03762, 2017.\n[4]\n J.  Devlin,  M. -W.  Chang,  K.  Lee,  and  K.  Toutanova,\nBERT: pre-training of deep bidirectional transformers for\nlanguage  understanding,  arXiv  preprint  arXiv:\n1810.04805, 2018.\n[5]\n L. Minto, M. Haller, B. Livshits, and H. Haddadi, Stronger\nprivacy  for  federated  collaborative  filtering  with  implicit\nfeedback,  in Proc.  15th  ACM  Conf.  Recommender\nSystems, Amsterdam, the Netherlands, 2021, pp. 342–350.\n[6]\n R. Chen, Q. Hua, Y. -S. Chang, B. Wang, L. Zhang, and\nX.  Kong, A  survey  of  collaborative  filtering-based\nrecommender systems: From traditional methods to hybrid\nmethods  based  on  social  networks, IEEE  Access,  vol. 6,\npp. 64301–64320, 2018.\n[7]\n Y. Shi, M. Larson, and A. Hanjalic, Collaborative filtering\nbeyond the user-item matrix: A survey of the state of the\nart and future challenges, ACM Comput. Surv., vol. 47, no.\n[8]\n \n60\n40\n20\n0\ny\n−20\n−40\n−40 −20 0 20 40\n−60\n \n \nFig. 5    T-SNE embeddings for 2000 comments.\n  Richard Wu:   RecBERT: Semantic Recommendation Engine with Large Language Model Enhanced Query… 9\n \n1, p. 3,\n B.  K.  Mylavarapu, Collaborative  filtering  and  artificial\nneural  network  based  recommendation  system  for\nadvanced applications, J. Comput. Commun, vol. 6, no. 12,\npp. 1–14, 2018.\n[9]\n G.  Linden,  B.  Smith,  and  J.  York, Amazon.  com\nrecommendations: Item-to-item  collaborative  filtering,\nIEEE Internet Comput., vol. 7, no. 1, pp. 76–80, 2003.\n[10]\n A.  van  den  Oord,  S.  Dieleman,  and  B.  Schrauwen,  Deep\ncontent-based  music  recommendation,  in Proc.  26th  Int.\nConf. Neural Information Processing Systems - Volume 2,\nLake Tahoe, Nevada, 2013, pp. 2643–2651.\n[11]\n J.  Lian,  F.  Zhang,  X.  Xie,  and  G.  Sun,  CCCFNet: A\ncontent-boosted  collaborative  filtering  neural  network  for\ncross  domain  recommender  systems,  in Proc.  26th  Int.\nConf.  World  Wide  Web  Companion,  Perth,  Australia,\n2017, pp. 817–818.\n[12]\n W. Zhao, B. Wang, M. Yang, J. Ye, Z. Zhao, X. Chen, and\nY.  Shen, Leveraging  long  and  short-term  information  in\ncontent-aware  movie  recommendation  via  adversarial\ntraining, IEEE  Trans.  Cybern.,  vol. 50,  no. 11,  pp.\n4680–4693, 2020.\n[13]\n J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J.\nKang, BioBERT: a  pre-trained  biomedical  language\nrepresentation  model  for  biomedical  text  mining,\nBioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020.\n[14]\n L.  Rasmy,  Y.  Xiang,  Z.  Xie,  C.  Tao,  and  D.  Zhi, Med-\nBERT: Pretrained  contextualized  embeddings  on  large-\nscale  structured  electronic  health  records  for  disease\nprediction, NPJ Digit. Med., vol. 4, no. 1, p. 86, 2021.\n[15]\n H. Choi, J. Kim, S. Joe and Y. Gwon, Evaluation of BERT\nand  ALBERT  sentence  embedding  performance  on\ndownstream  NLP  tasks,  in Proc.  2020  25th  Int.  Conf.\nPattern  Recognition (ICPR),  Milan,  Italy,  2021,  pp.\n5482–5487.\n[16]\n T. Jiang, S. Huang, Z. -Q. Zhang, D. Wang, F. Zhuang, F.\nWei,  H.  Huang,  L.  Zhang,  and  Q.  Zhang,  PromptBERT:\nImproving  BERT  sentence  embeddings  with  prompts,  in\nProc. 2022 Conf. Empirical Methods in Natural Language\nProcessing, Abu Dhabi, UAE, 2022, pp. 8826–8837.\n[17]\n N.  Reimers  and  I.  Gurevych,  Sentence-BERT: Sentence\nembeddings using Siamese BERT-networks, in Proc. 2019\nConf. Empirical Methods in Natural Language Processing\nand  9th  Int.  Joint  Conf.  Natural  Language  Processing,\nHong Kong, China, 2019, pp. 3982–3992.\n[18]\n T. Gao, X. Yao, and D. Chen, SimCSE: simple contrastive[19]\nlearning  of  sentence  embeddings,  arXiv  preprint  arXiv:\n2104.08821, 2021.\n N.  Kassner  and  H.  Schütze,  BERT-kNN: Adding  a  kNN\nsearch component to pretrained language models for better\nQA,  in Findings  of  the  Association  for  Computational\nLinguistics: EMNLP  2020,  T.  Cohn,  Y.  He,  and  Y.  Liu,\nEds.  Kerrville,  TX,  USA: Association  for  Computational\nLinguistics, 2020, pp. 3424–3430.\n[20]\n P.  Röttger  and  J.  Pierrehumbert,  Temporal  adaptation  of\nBERT  and  performance  on  downstream  document\nclassification: Insights  from  social  media,  in Findings  of\nthe  Association  for  Computational  Linguistics: EMNLP\n2021,  M.  F.  Moens,  X.  Huang,  L.  Specia,  and  S.  W.  T.\nYih,  Eds.  Kerrville,  TX,  USA: Association  for\nComputational Linguistics, 2020, pp. 2400–2412.\n[21]\n C.  Liu,  W.  Zhu,  X.  Zhang,  and  Q.  Zhai, Sentence  part-\nenhanced  BERT  with  respect  to  downstream  tasks,\nComplex Intell. Syst., vol. 9, no. 1, pp. 463–474, 2023.\n[22]\n Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A.\nJones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon,\net al. , Constitutional AI: Harmlessness from AI feedback,\narXiv preprint arXiv: 2212.08073, 2022.\n[23]\n H.  Touvron,  T.  Lavril,  G.  Izacard,  X.  Martinet,  M.  A.\nLachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F.\nAzhar,  et  al.  ,  LLaMA: open  and  efficient  foundation\nlanguage models, arXiv preprint arXiv: 2302.13971, 2023.\n[24]\n T.  Dettmers,  A.  Pagnoni,  A.  Holtzman,  and  L.\nZettlemoyer,  QLoRA: Efficient  finetuning  of  quantized\nLLMs, arXiv preprint arXiv: 2305.14314, 2023.\n[25]\n T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP.  Dhariwal,  A.  Neelakantan,  P.  Shyam,  G.  Sastry,  A.\nAskell,  et  al.  ,  Language  models  are  few-shot  learners,\narXiv preprint arXiv: 2005.14165, 2020.\n[26]\n Y.  Liu,  M.  Ott,  N.  Goyal,  J.  Du,  M.  Joshi,  D.  Chen,  O.\nLevy, M. Lewis, L. Zettlemoyer, V. Stoyanov, RoBERTa:\nA  robustly  optimized  BERT  pretraining  approach,  arXiv\npreprint arXiv: 1907.11692, 2019.\n[27]\n V.  Sanh,  L.  Debut,  J.  Chaumond,  and  T.  Wolf,\nDistilBERT, a distilled version of BERT: Smaller, faster,\ncheaper  and  lighter,  arXiv  preprint  arXiv: 1910.01108,\n2019.\n[28]\n S. Arora, W. Hu, and P. K. Kothari, An analysis of the t-\nSNE algorithm for data visualization, in Proc. 31st Conf.\nLearning  Theory,  Stockholm,  Sweden,  2018,  pp.\n1455–1462.\n[29]\n \n    10 Intelligent and Converged Networks,  2024, 5(1): 1−11\n \nRichard  Wu is  with  the  Dublin  Unified\nSchool  District  and  President  of  the  SF\nArtificial  Intelligence  Club,  Dublin,  CA\n94568, USA. His current research interests\ninclude  natural  language  processing  for\nsemantic search and studying cryptography\nfor supply chain management.\n  Richard Wu:   RecBERT: Semantic Recommendation Engine with Large Language Model Enhanced Query… 11\n "
}