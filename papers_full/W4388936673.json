{
  "title": "Data Augmentation Using Transformers and Similarity Measures for Improving Arabic Text Classification",
  "url": "https://openalex.org/W4388936673",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5084756913",
      "name": "Dania Refai",
      "affiliations": [
        "Princess Sumaya University for Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2394705152",
      "name": "Saleh Abu Soud",
      "affiliations": [
        "Princess Sumaya University for Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2103148195",
      "name": "Mohammad J. Abdel-Rahman",
      "affiliations": [
        "Princess Sumaya University for Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2473945007",
    "https://openalex.org/W2953343412",
    "https://openalex.org/W6677704767",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W3113862052",
    "https://openalex.org/W3115908473",
    "https://openalex.org/W6784056776",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W3174828871",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W6752906821",
    "https://openalex.org/W3173569225",
    "https://openalex.org/W3034336785",
    "https://openalex.org/W3154410208",
    "https://openalex.org/W6780673046",
    "https://openalex.org/W1973965874",
    "https://openalex.org/W1965657003",
    "https://openalex.org/W2035642982",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3170305303",
    "https://openalex.org/W6780739316",
    "https://openalex.org/W2991266149",
    "https://openalex.org/W6765939562",
    "https://openalex.org/W6689044022",
    "https://openalex.org/W3048162013",
    "https://openalex.org/W3148105069",
    "https://openalex.org/W6798051994",
    "https://openalex.org/W6793723455",
    "https://openalex.org/W3182164106",
    "https://openalex.org/W6788007171",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W6788242128",
    "https://openalex.org/W6787958253",
    "https://openalex.org/W6793962581",
    "https://openalex.org/W4283814918",
    "https://openalex.org/W4200608161",
    "https://openalex.org/W6789924496",
    "https://openalex.org/W3181034584",
    "https://openalex.org/W2937423263",
    "https://openalex.org/W4210336044",
    "https://openalex.org/W4245461916",
    "https://openalex.org/W2471147443",
    "https://openalex.org/W3154741768",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W6796698822",
    "https://openalex.org/W3201915713",
    "https://openalex.org/W4385574306",
    "https://openalex.org/W6849158869",
    "https://openalex.org/W2239389665",
    "https://openalex.org/W6758993734",
    "https://openalex.org/W2398463581",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2964236337",
    "https://openalex.org/W6697261853",
    "https://openalex.org/W2250594687",
    "https://openalex.org/W3115267989",
    "https://openalex.org/W3097513514",
    "https://openalex.org/W2782223840",
    "https://openalex.org/W4287079360",
    "https://openalex.org/W2228486207",
    "https://openalex.org/W3171388604",
    "https://openalex.org/W4287728395",
    "https://openalex.org/W4367858557",
    "https://openalex.org/W3037013468",
    "https://openalex.org/W4319791992",
    "https://openalex.org/W4287661507",
    "https://openalex.org/W3105625590"
  ],
  "abstract": "The performance of learning models heavily relies on the availability and adequacy of training data. To address the dataset adequacy issue, researchers have extensively explored data augmentation (DA) as a promising approach. DA generates new data instances through transformations applied to the available data, thereby increasing dataset size and variability. This approach has enhanced model performance and accuracy, particularly in addressing class imbalance problems in classification tasks. However, few studies have explored DA for the Arabic language, relying on traditional approaches such as paraphrasing or noising-based techniques. In this paper, we propose a new Arabic DA method that employs the recent powerful modeling technique, namely the AraGPT-2, for the augmentation process. The generated sentences are evaluated in terms of context, semantics, diversity, and novelty using the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT transformer is used on sentiment classification tasks to evaluate the classification performance of the augmented Arabic dataset. The experiments were conducted on four sentiment Arabic datasets: AraSarcasm, ASTD, ATT, and MOVIE. The selected datasets vary in size, label number, and unbalanced classes. The results show that the proposed methodology enhanced the Arabic sentiment text classification on all datasets with an increase in F1 score by 7&#x0025; in AraSarcasm, 8&#x0025; in ASTD, 11&#x0025; in ATT, and 13&#x0025; in MOVIE.",
  "full_text": "Data Augmentation using Transformers\nand Similarity Measures for Improving\nArabic Text Classification\nDANIA REFAI1, SALEH ABU-SOUD2, AND MOHAMMAD J. ABDEL-RAHMAN2, 3,\n(Senior Member, IEEE)\n1Computer Science Department, Princess Sumaya University for Technology, Amman 11941, Jordan\n2Data Science Department, Princess Sumaya University for Technology, Amman 11941, Jordan\n3Electrical and Computer Engineering Department, Virginia Tech, Blacksburg, V A 24061 USA\nCorresponding author: Dania Refai (e-mail:Dania.Refai@hotmail.com).\nABSTRACT The performance of learning models heavily relies on the availability and adequacy of\ntraining data. To address the dataset adequacy issue, researchers have extensively explored data augmen-\ntation (DA) as a promising approach. DA generates new data instances through transformations applied\nto the available data, thereby increasing dataset size and variability. This approach has enhanced model\nperformance and accuracy, particularly in addressing class imbalance problems in classification tasks.\nHowever, few studies have explored DA for the Arabic language, relying on traditional approaches such as\nparaphrasing or noising-based techniques. In this paper, we propose a new Arabic DA method that employs\nthe recent powerful modeling technique, namely the AraGPT-2, for the augmentation process. The generated\nsentences are evaluated in terms of context, semantics, diversity, and novelty using the Euclidean, cosine,\nJaccard, and BLEU distances. Finally, the AraBERT transformer is used on sentiment classification tasks to\nevaluate the classification performance of the augmented Arabic dataset. The experiments were conducted\non four sentiment Arabic datasets: AraSarcasm, ASTD, ATT, and MOVIE. The selected datasets vary in\nsize, label number, and unbalanced classes. The results show that the proposed methodology enhanced the\nArabic sentiment text classification on all datasets with an increase in F1 score by 7% in AraSarcasm, 8%\nin ASTD, 11% in ATT, and13% in MOVIE.\nINDEX TERMS Arabic, AraBERT, AraGPT-2, data augmentation, machine learning, natural language\nprocessing, similarity measures, text classification, transformers.\nI. INTRODUCTION\nN\nATURAL language processing (NLP) is a branch of\nartificial intelligence that aims to teach computers to\nprocess and analyze large volumes of natural language\ndata [1], [2]. Machine learning and deep learning have made\nsignificant advances in recent years, particularly in the NLP\nfield [3]. However, the learning model in machine learning\nsystems is highly dependent on data, making it difficult to\nobtain a large amount of labeled data, particularly in domains\nsuch as education and healthcare [4].\nData augmentation (DA) has emerged as a promising\napproach to address the issue of dataset adequacy [5]–[7]. DA\nincreases the number of training data instances by performing\nvarious transformations on actual data instances to generate\nnew and representative data instances, thereby improving the\nmodel’s efficiency and prediction accuracy [6]. Addition-\nally, DA helps to minimize overfitting and solve the class\nimbalance issue in classification learning techniques [8].\nAlthough DA is well-established in computer vision and\nspeech recognition, it is not a common practice in the NLP\nfield [8]. Traditional methods of increasing text data are\ncostly and time-consuming, particularly when there are not\nenough resources to support the augmentation process, such\nas language dictionaries or databases of synonyms for the\nchosen dataset. Furthermore, not all augmentation methods\nare applicable to all languages, as certain transformations\nmay make the sentence grammatically or semantically incor-\nrect [7], [9].\nUsing pre-trained transformer models in DA can help to\novercome these limitations [10]. Transformer models have\nproven effective in various NLP tasks, including text sum-\nmarization, translation, generation, and question-answering\nsystems [11]. Additionally, employing transformer models in\nthe DA process preserves the text context and dependencies\nbetween the sentence words, thus solving the issues associ-\nated with traditional augmentation methods [12]–[15].\nHowever, it is essential to assess the quality of augmented\ntext from various perspectives, including context, semantics,\n1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\ndiversity, and novelty [16]. Text-similarity metrics such as\nEuclidean [17], cosine [18], Jaccard [19], and BLEU [20]\nmeasures can be used to evaluate the quality of augmented\ntext [16].\nAfter conducting a thorough review of existing literature,\nit is evident that various DA techniques have been imple-\nmented in different languages, with a focus on English. These\ntechniques have proven to be effective in enhancing English\nlanguage learning, and fall into two categories: paraphras-\ning techniques using thesauruses [6], translation [21], or\ntransformers [13], and techniques that add noise to sentence\nwords, such as swapping [22], deletion [4], insertion [23],\nand substitution [24].\nDespite Arabic being the fifth most spoken language\nglobally [25] and experiencing significant growth of digital\nArabic content on the Internet [26], there is a significant gap\nin research when it comes to DA for Arabic data. One of the\nchallenges in Arabic DA is the language’s unique character-\nistics [27], which make it difficult to accurately augment tex-\ntual data using conventional methods such as paraphrasing or\nnoising-based techniques [28]–[31]. Additionally, the current\nbody of research lacks studies that employ DA techniques\non Arabic data and utilize all similarity measures to evaluate\nthe quality of the generated sentence, which is crucial for\neffective language learning. Therefore, there is a pressing\nneed for further exploration of the potential of using Arabic\ntransformers, such as AraGPT2 [32] and AraBERT’s [33], in\nDA for Arabic data, along with a comprehensive assessment\nof generated sentence quality [14], [15]. Combining trans-\nformers and similarity measures could solve the challenges of\nArabic DA and improve the accuracy of generated sentences,\nwhich can, in turn, enhance model learning outcomes.\nMAIN CONTRIBUTIONS:\nOur main contributions in this paper can be summarized as\nfollows:\n• A novel approach for Arabic textual data augmentation.\nOur method harnesses the capabilities of recent power-\nful tools based on the transformer’s architecture. Specif-\nically, our method utilizes AraGPT-2’s text generation\ntask [32] for paraphrasing in the augmentation process.\n• Different text evaluation metrics are used to evaluate\nthe generated sentences from our approach in terms of\ncontext, semantics, diversity, and novelty. Specifically,\nthe Euclidean, cosine, Jaccard, and BLEU distances are\nused.\n• Sentiment classification is performed on the augmented\nArabic dataset using the AraBERT [33] transformer, and\nthe effects of DA on classification performance have\nbeen examined.\nPAPER ORGANIZATION:\nThe rest of the paper is organized as follows. In Section II,\nwe provide a literature review. Our proposed methodology\nis explained in Section III. In Section IV, we introduce the\nsentiment datasets used for evaluating our proposed method-\nology. Section V discusses the experiments conducted to as-\nsess the robustness of the proposed approach along with their\nresults. Comparisons with related works are also provided\nin Section V. Finally, our conclusions are summarized in\nSection VI.\nII. LITERATURE REVIEW\nLanguage transformer models, such as GPT-3 [32], belong\nto a class of neural network architectures that have revo-\nlutionized NLP in recent years. These models are typically\npre-trained on large corpora of text data to learn general\nlanguage patterns and relationships between words. One of\nthe most influential architectures for language transformers\nis the transformer model, introduced in [10]. Transformers\nhave been predominantly trained on English text, which has\nled to their success in various NLP tasks [12]–[15]. However,\nresearchers have recently started adapting these powerful\nmodels for other languages, including Arabic [34]–[38]. To\ndo so, they pre-train the transformer models on large Arabic\ntextual datasets, such as AraBERT [33], AraGPT-2 [32], and\nAraElectra [39]. Pre-training on Arabic text allows these\nmodels to learn language patterns and relationships specific\nto the Arabic language, which makes them highly effective\nfor Arabic NLP tasks. Despite their effectiveness in preserv-\ning context in natural language [10], studies on Arabic have\nnot yet explored using language transformer models as an\naugmentation technique.\nFurthermore, to ensure that augmented data improves per-\nformance without altering the meaning of the original data,\nit is crucial to evaluate its quality before incorporating it\ninto the augmented Arabic dataset [9]. Evaluating sentences\nin context and assessing their quality in terms of semantics,\ndiversity, novelty, and other factors is necessary to effectively\nevaluate the augmented data [16]. While some researchers\nhave used the Jaccard similarity metric [19] to evaluate the\nnovelty and diversity of generated sentences in Arabic DA\nprocesses before adding them to the dataset [40], a more\ncomprehensive evaluation is required that considers various\naspects of sentence quality, such as context, semantics, diver-\nsity, and novelty [16].\nText classification is a widely researched area in NLP,\nwith much attention given to languages like English and\nSpanish [41]. However, Arabic language text classification\nhas received a different level of attention, mainly due to\nthe unique characteristics of the language that require dif-\nferent methodologies [42], [43]. While existing classifica-\ntion methods for Arabic text are still limited, transformers\nhave emerged as a promising tool for improving Arabic DA\ntasks, including text classification [43]. Furthermore, various\nArabic studies have employed advanced models, such as\nAraBERT, MARRBERT, ArBERT, QARiB, AraBERTv02,\nGigaBERT, ArabicBERT, and mBERT to evaluate their aug-\nmented Arabic datasets using classification tasks [34], [35],\n[44]. For instance, authors in [45] used MARBERT and\nQARiB to distinguish between human-generated and fake-\n2\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\ngenerated tweets with high accuracy [36], [46].\nThe proposed DA techniques and methods can be broadly\nclassified into two main categories: paraphrasing-based and\nnoising-based [4], [9], [47], [48]. Regarding paraphrasing-\nbased techniques, recent studies have employed transformer\nmodels as an augmentation process, demonstrating their ef-\nficiency in various NLP tasks, including text summarization,\ntranslation, classification, generation, named entity recogni-\ntion, and question-answering systems [11]. Although using\ntransformer models in augmentation preserves the text con-\ntext, it is essential to note that the augmented text should\nbe evaluated from various aspects, including context, se-\nmantics, diversity, and novelty [16]. Text-similarity measure-\nments can be used to check the quality of the augmented\nsentences [40]. Various text similarity metrics can be used,\nincluding Euclidean distance [17], cosine distance [18], Jac-\ncard distance [19], and BLEU distance [20].\nWhile few studies have focused on augmenting Arabic\ndata [28]–[31], [49], some have used the current DA nois-\ning and paraphrasing-based approaches without employing\nthe transformer’s powerful models as augmentation tech-\nniques [29]–[31], [50]. Other studies have employed trans-\nformers to evaluate the augmented Arabic dataset [34], [35].\nA few studies have considered classification tasks using the\nAraBERT transformer and achieved the best results in Arabic\ntext classification [34]–[36]. Recently, one study considered\nthe Jaccard metric to evaluate the novelty of the generated\nsentences in Arabic text [40]. As mentioned earlier, the\ngenerated sentence should be evaluated from different as-\npects, such as context, semantics, diversity, and novelty [16].\nTable 1 summarizes the DA techniques used in the Arabic\nlanguage and their results.\nDA techniques that leverage transformers and similarity\nmetrics have shown significant advantages in English textual\ndata classification [12]–[15]. However, a noticeable gap ex-\nists in current research regarding utilizing transformers and\nsimilarity metrics for Arabic textual data augmentation and\nclassification processes. As depicted in Fig. 1, previous Ara-\nbic studies mainly focused on using transformers exclusively\nfor classification, with only one study employing Jaccard\nsimilarity to assess the generated sentences. Our research\nproposes a novel approach encompassing three key aspects\nto address this gap. Firstly, we introduce a groundbreaking\nmethodology that harnesses the power of recent transformer-\nbased tools for data augmentation in Arabic. Secondly, we\nadopt diverse text evaluation metrics, including Euclidean,\ncosine, Jaccard, and BLEU distances, to thoroughly assess\nthe generated sentences, focusing on context, semantics,\ndiversity, and novelty. Additionally, our research includes\nsentiment classification on the augmented Arabic dataset,\nenabling us to explore the impact of data augmentation\non classification performance. By encompassing these key\nelements, our methodology effectively bridges the research\ngap and significantly advances the field of DA techniques in\nArabic, as shown in the dashed box in Fig. 1.\nTABLE 1. A summary of the Arabic DA techniques and their final findings.\nDA Technique Study\nDataset Name and Macro F1 Results\nAra-\nSarcasm\nTwitter\ndata [51]\nProduct\nre-\nviews [52]\nArab\nGloss-\nBERT\ndataset\nNoising-based\nDA techniques,\nincluding word\nreplacement,\ninsertion, and mix\nbetween them [53]\n0.46 − − −\nNoising-based\nDA techniques\nincluding word\nreplacement,\ninsertion, and mix\nbetween them [31]\n0.75 − − −\nNoising-based\nDA techniques\nincluding merging\nan external dataset\nwith AraSarcesm\ndataset [39]\n0.52 − − −\nNoising-based\nDA techniques\nincluding manually\nexpanding the\ndataset [36]\n− 96.0 − −\nParaphrasing-based\nDA techniques\nusing language\nrules [29]\n− − 0.65 −\nParaphrasing-based\nDA technique using\nArabic-English\nArabic back-\ntranslation [50]\n− − −\nbetween\n65.0\nand\n89.0\nIII. METHODOLOGY\nIn this section, we propose a three-phase empirical approach\nfor Arabic DA. In the first phase, we use the AraGPT-2-\nbase [32] pre-trained model to generate Arabic text from the\ngiven dataset records. This results in a new dataset that con-\ntains the generated Arabic text from the transformer (i.e., the\nAraGPT-2-base). In the second phase, we add new records\nto the given dataset by employing the similarity measures,\nnamely, the Euclidean [17], cosine [18], Jaccard [19], and\nBLEU [20] distances. The augmentation process depends on\n(i) the similarity thresholds and (ii) the selected class labels\nfor the data to be augmented. The third phase comes as a\ncomplementary phase, which assists in evaluating the perfor-\nmance of the text classification process on the newly created\ndataset (i.e., the augmented dataset). Fig. 2 illustrates the\ngeneral phases of the adopted methodology. In the following\nsubsections, we explain the three phases of the DA process.\nA. PHASE 1: ARABIC TEXT DATA GENERATION USING\nTRANSFORMERS\nIn this phase, the dataset to be augmented is first loaded.\nThen, a transformer that can generate Arabic text is cre-\nated (AraGPT-2-base [32] is used in this paper) along with\ninitializing the similarity functions needed to calculate the\n3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nData augmentation \ntechniques for NLP\nNoising-based \ntechniques\nSwapping \n[4], [22] \nDeletion \n[4], [9]\nInsertion \n[4], [23]\nSubstitution \n[4], [24]\nMixup \n[4], [15]\nParaphrasing-based \ntechniques\nThesauruses \n[6], [12]\nRules \n[13], [14]\nMachine translation \n[21]\nTransformers \n[13], [35], [47], [48]\nEnglish\nNoising-based \ntechniques\nSwapping \n[28], [36]\nDeletion \n[28], [49]\nInsertion \n[29], [31], [53]\nSubstitution \n[30], [31], [36], [53]\nMixup \n[30], [31], [39], [53]\nArabic\nParaphrasing-based \ntechniques\nThesauruses \n[31], [34], [40]\nRules \n[29], [31], [35]\nMachine translation \n[50]\nTransformers \nTransformers for \nclassification\nTransformers for \ndata augmentation\n● AraBERT  [34], [35], [36], [44]\n● MARRBE [45],\n● QARiB.     [44]\n● GigaBER.  [34], [35]\n● mBERT.     [36], [46]\n \nAssess DA quality \nJaccard similarity \n [40]\n \nOur Work\nFIGURE 1. Taxonomy of DA techniques for NLP .\nFIGURE 2. The main phases of the adopted methodology.\nsimilarity between the old Arabic text supplied to the trans-\nformer and the newly generated text from the transformer\n(i.e., to/from the AraGPT-2-base transformer). Subsequently,\nfor each record in the given dataset’s records:\n• First, the given Arabic text in the record is preprocessed\nusing the provided preprocessor of the selected trans-\nformer, which is the AraBERT preprocessor [30], [33],\n[35].\n• Second, a need to calculate the word embedding that\nrepresents the given Arabic text would take place.\nSuch a sub-step is needed since the similarity functions\ndeal with numerical representations (i.e., vectors) rather\nthan the abstract Arabic text representation to calculate\nthe distances between the objects for comparing them.\nHence, in this paper, we used BERT word embedding\nfor computing the word embedding [54].\n• Third, the similarity between the numerical represen-\ntation (i.e., the words embedding) of the given Arabic\ntext in the record and the newly generated one is calcu-\nlated with the selected similarity functions (Euclidean,\ncosine, Jaccard, and BLEU distances).\n• Finally, all the computed and generated information\nwere collected within the current loop (the given Arabic\ntext, the related class label, the newly generated text,\nall text of the given Arabic text combined with the\ngenerated one, the embedding representation, and the\nsimilarities’ values), and is appended to the current\nrecord. Moreover, such a record is added to the final\ndataset to be exported upon finishing this phase, along\nwith the original class label related to the current record\nbeing processed.\nPhase 1 of the proposed solution is summarized in Fig. 3,\n4\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nFIGURE 3. Methodology steps contained within phase 1.\nFIGURE 4. Augmentation illustrative example.\nwhich provides a visual overview of the steps involved. The\ncorresponding algorithm is presented in Algorithm 1, which\noutlines the sequence and flow of operations. To further\nclarify the workflow, we include an illustrative example in\nFig. 4, based on a single record of Arabic text. Together,\nthese resources offer a clear and comprehensive description\nof Phase 1 of our approach.\nB. PHASE 2: ARABIC DATASET AUGMENTATION USING\nSIMILARITIES\nIn this phase, the generated dataset from Phase 1 is processed\nto generate one final dataset that contains the new augmented\nFIGURE 5. Methodology steps contained within phase 2.\nFIGURE 6. Methodology steps contained within phase 3.\n5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nAlgorithm 1Arabic Text Data Generation.\nResult :Generated dataset with similarity measures.\nInput : Original dataset [original sentence, label]\nOutput: Temp dataset [original sentence, generated sen-\ntence, all text, original sentence embeddings, gen-\nerated sentence embeddings, Euclidean similar-\nity, cosine similarity, Jaccard similarity, BLEU\nsimilarity].\nfor each record in original dataset do\n1) Preprocessing (original sentence).\n2) Generated sentence ← generates text (original sen-\ntence).\n3) Original sentence embeddings ← create embed-\ndings (original sentence).\n4) Generated sentence embeddings ← create embed-\ndings (generated sentence).\n5) Euclidean similarity ← calculate Euclidean (origi-\nnal sentence, generated sentence).\n6) cosine similarity ← calculate cosine (original sen-\ntence, generated sentence).\n7) Jaccard similarity ←calculate Jaccard (original sen-\ntence, generated sentence).\n8) BLEU similarity ← calculate BLEU (original sen-\ntence, generated sentence).\n9) All text ← combine text (original sentence, gener-\nated sentence).\n10) Temp dataset.add (original sentence, generated sen-\ntence, all text, original sentence embeddings, gener-\nated sentence embeddings, and similarities).\nend\nExport temp dataset.\nrecords. Generating this final dataset requires two significant\ndecisions: (i) selecting the classes to be augmented and (ii)\nselecting a threshold (i.e., similarity-desired value) to decide\nthe selection process of the newly generated text as a new\nrecord in the new dataset along with the related class label.\nFor the selection of classes to be augmented, we have opted\nto augment the class with the minimum representation in\nthe dataset, ensuring a balanced augmentation approach as\nshown in Tables 3 and 4. Accordingly, the similarity thresh-\nold percentage for each similarity metric is calculated by\ntaking the average for each similarity column value from the\n(exported temp DS) from Phase 1. Fig. 5 summarizes the\nimplemented steps to achieve these two significant decisions.\nFurthermore, the sequence and flow for Phase 2 operations\nare depicted in Algorithm 2. Consequently, the final collected\ndataset upon this selection strategy is exported for the next\nphase (i.e., Phase 3).\nC. PHASE 3: AUGMENTED DATASET EVALUATION\nUSING SENTIMENT ANALYSIS\nIn this phase, the final augmented datasets are evaluated using\nsentiment analysis [55] since all the selected Arabic datasets\nare classified with the sentiment of the text [49], [51]–\n[53]. To conclude this evaluation, the model of the original\ndataset (i.e., the dataset before augmentation) with a selected\nclassifier is needed to find the final classification performance\nresults (for instance, the recall, F1, accuracy, etc.). Then,\nAlgorithm 2 Arabic Dataset Augmentation using\nSimilarities.\nResult : Augmented datasets based on similarity mea-\nsures.\nInput : Temp dataset.\nOutput: Euclidean augmented dataset, cosine augmented\ndataset, Jaccard augmented dataset, BLEU aug-\nmented dataset.\nStep 1:Create empty datasets.\n1) Euclidean augmented dataset.\n2) cosine augmented dataset.\n3) Jaccard augmented dataset.\n4) BLEU augmented dataset.\n5) Augmented classes.\nStep 2:Calculate similarity thresholds for each measure.\nfor all records in temp dataset do\n1) Euclidean threshold ← average (Euclidean similar-\nity).\n2) cosine threshold ← average (cosine similarity).\n3) Jaccard threshold ← average (Jaccard similarity).\n4) BLEU threshold ← average (BLEU similarity).\nend\nStep 3:Augment datasets.\nfor each record in temp dataset do\nif Euclidean similarity ≥ Euclidean threshold then\nEuclidean augmented dataset.add (all text).\nAugmented classes.add (label).\nend\nif cosine similarity ≥ cosine threshold then\ncosine augmented dataset.add (all text).\nAugmented classes.add (label).\nend\nif Jaccard similarity ≥ Jaccard threshold then\nJaccard augmented dataset.add (all text).\nAugmented classes.add (label).\nend\nif BLEU similarity ≥ BLEU threshold then\nBLEU augmented dataset.add (all text).\nAugmented classes.add (label).\nend\nend\nStep 4:Export datasets:\n1) Euclidean augmented dataset.\n2) cosine augmented dataset.\n3) Jaccard augmented dataset.\n4) BLEU augmented dataset.\ncompare the obtained results with the results found in the\nsame classification process. In this context, Fig. 6 depicts the\ncombined classification process steps.\nIn this view, the AraBERT base Twitter classifier\nnamed “aubmindlab/bert-base-arabertv02-twitter” [32] is\nused. However, the data sets’ splits for the classification\nprocesses were selected to be 80% for training and 20% for\ntesting on both types of the datasets at hand (i.e., the original\ndatasets before augmentation and the augmented datasets).\nMeanwhile, the K-fold cross-validation approach [56] was\nadopted for validating and finding the best model’s hyper-\nparameters for the classification process of the sentiment\ncontained within the given data sets’ types.\n6\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nTABLE 2. Description of the data sets considered for experimentation.\nDataset Short Name Description\nAraSarcasm-v1 [49]\nAraSarcasm is a new dataset for detecting\nsarcasm in Arabic. The dataset was built by\nadding sarcasm and dialect labels to previ-\nously accessible Arabic sentiment analysis\ndatasets (SemEval 2017 and ASTD). There\nare 10, 547 tweets in the dataset, with 1, 682\n(16%) of them being snarky [49]\nASTD [57]\nASTD is a dataset that is collected from\ntweets after being filtered and annotated by\nthe authors to be an Arabic social sentiment\nanalysis dataset gathered from Twitter. The\nfinal number of records contained in this\ndataset is 3224 records. The dataset’s number\nof classes was initially four classes (NEG,\nPOS, NEUTRAL, OBJ). Nevertheless, the\nauthors consider the dataset with the records\nlabeled (NEG, POS, NEUTRAL) for the ex-\nperimentation [57]\nATT [51]\nAnother Arabic dataset for the reviews ex-\npresses the attraction sentiment of the trav-\nelers. Moreover, such a dataset was col-\nlected from TripAdvisor.com, and it has2154\nrecords labeled with positive and negative\nclasses [51]\nMOVIE [51]\nAnother dataset is scrapped from TripAdvi-\nsor.com and contains 1524 records. It con-\ntained three classes, namely, positive, nega-\ntive, and neutral classes [51]\nIV. DATESET SELECTED\nThe Arabic language is highly morphologically rich, with one\nArabic word having multiple meanings and shapes, which\nrequires a comprehensive understanding of the language [27].\nTo ensure that our proposed approach is robust and appli-\ncable to a range of scenarios, we have considered multiple\ndatasets that cover various aspects of the Arabic language, as\ndescribed in Table 2. Our selection includes diverse Arabic\ndialects and cases with random examples that may affect the\nproposed approach in this paper. This ensures the correctness\nof the proposed approach and avoids limitations to a single\ndataset with limited examples and fewer characteristics of the\nArabic language [34], [40], [42].\nV. EXPERIMENTAL RESULTS AND DISCUSSION\nA. EXPERIMENT DATA\nAs mentioned earlier, we have selected several sentiment\nArabic datasets with different characteristics, including\ndataset size, label number, and unbalanced classes, to eval-\nuate the impact of the proposed augmentation methodology\non classification performance [58]. For example, we consider\ndataset size to assess whether the augmentation method per-\nforms better on smaller or larger datasets. Balancing class\ndistribution is also crucial in evaluating data since unbal-\nanced datasets can degrade model performance.\nAll datasets chosen for our experiments include mod-\nern standard Arabic and multi-dialect data, increasing the\nmodel’s flexibility and generality when dealing with new\ndata. However, since the Arabic language is morphologically\nrich, with one word having multiple meanings and shapes,\nTABLE 3. Data sets considered for experimentation in the proposed solution.\nDataset Name Record No.\nClass Labels’ Information\nLabel\nName\nNo. In-\nstances Ratio (%)\nAra-Sarcasm 10545\nPOSITIVE 1678 15.91%\nNEUTRAL 5339 50.63%\nNEGATIVE 3528 33.46%\nASTD 3221\nPOSITIVE 776 24.09%\nNEUTRAL 805 24.99%\nNEGATIVE 1640 50.92%\nATT 2151 POSITIVE 81 3.77%\nNEGATIVE 2070 96.23%\nMOVIE 1517\nPOSITIVE 966 63.68%\nNEUTRAL 170 11.21%\nNEGATIVE 381 25.12%\nTABLE 4. Class labels to be augmented and similarity thresholds.\nDataset Class Similarities\nName Labels Euclidean cosine Jaccard BLEU\nAra-\nSarcasm\nNEGATIVE,\nPOSITIVE 0.327 0.835 0.265 0.316\nASTD NEUTRAL,\nPOSITIVE 0.331 0.852 0.362 0.394\nATT NEGATIVE 0.193 0.865 0.208 0.447\nMOVIE NEGATIVE,\nNEUTRAL 0.028 0.904 0.0003 0.071\nproviding diverse Arabic dialects and random examples can\naffect the proposed approach’s performance [9], [27].\nTherefore, we selected multiple datasets to cover various\naspects of the Arabic language [49], [51], [57]. We chose dif-\nferent sentiment Arabic datasets to experiment and evaluate\nthe proposed approach [59], as listed in Table 3.\nHowever, it is important to acknowledge that the selected\ndatasets, although valuable for our study, represent only a\nsubset of the vast diversity of the Arabic language. Therefore,\nfurther research is needed, involving additional datasets with\nlarger sizes and experiments on a wider range of data to\nstrengthen the generalizability of the proposed methodology\nand provide more comprehensive insights into its perfor-\nmance.\nB. MAIN EXPERIMENT PARAMETERS\nSince this study focuses on augmenting Arabic text, we fine-\ntuned base AraGPT-2 parameters used in [32] for the text\ngeneration task and AraBERT parameters used in [33] for\nthe text classification task. Furthermore, two significant de-\ncisions were made regarding the datasets’ augmentation: (i)\nselecting the imbalanced class label for any given dataset in\nthe selected datasets to be augmented [15] and (ii) setting the\naugmentation similarity threshold to the average similarity\ncalculated between the original Arabic text and the generated\ntext of all records in the given dataset from the selected\ndatasets. Table 4 summarizes the augmented classes for each\ndataset and the average similarity measures considered in this\npaper.\n7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nC. EXPERIMENTAL ENVIRONMENT AND HARDWARE\nThe experiment development, implementation, running, and\nanalysis were conducted on an ASUS ROG G703GX note-\nbook. Such a machine runs Windows 10 and has an 8th\ngeneration core i 9 processor, 64 GB of memory, 3 × 1 TB\nNVMe SSD RAID hard disk, and NVIDIA GeForce RTX\n2080 8 GB graphic card. Given that running the medium\nand large AraBERT transformer models [33] requires more\nresources, we ran the base transformer types. To ensure clar-\nity and reproducibility, our code implementation, developed\nusing Python 3.8.10, can be found at [60].\nD. DATASET AUGMENTATION AND GROWTH\nPERCENTAGE\nOur goal is to leverage the transformers’ ability to para-\nphrase Arabic text, and this experiment aims to evaluate\nthe proposed approach’s correctness and validity in data\nmodeling and processing techniques. To achieve this, each\nselected dataset is first preprocessed using the AraBERT\npreprocessor [33] and then fed to the transformer to generate\nthe corresponding Arabic text. The word embedding is then\ncalculated for the original and generated text to prepare for\nthe similarity calculation [54]. Using the computed average\nfor the selected similarity measure (i.e., Euclidean, cosine,\nJaccard, or BLEU similarity) and considering the class labels\nwith fewer instances in the dataset, we start the process of\naugmenting the given dataset. The final growth percentage is\ncalculated based on the total number of original instances in\nthat set. Tables 5, 6, 7, and 8 summarize the results of this\nexperiment for each dataset in terms of growth counts.\nE. CLASSIFICATION PERFORMANCE AND SIMILARITY\nPREFERENCE\nAs mentioned earlier, the proposed approach relies on the\nsimilarity threshold, which is calculated by averaging the\nsimilarities of all records in the same dataset type. To assess\nthe effectiveness of this approach and validate our assump-\ntions, we conducted five different sentiment classification\ntasks on both the augmented and original datasets. The first\ntask involved running the selected classifier on the origi-\nnal dataset. For the second, third, fourth, and fifth tasks,\nwe used the AraBERT classifier [33] on datasets resulting\nfrom augmentation using the Euclidean, cosine, Jaccard, and\nBLEU similarity measures [17]–[20], respectively. We then\ncompared the classification results with those obtained using\nthe original dataset, enabling us to evaluate the impact of the\naugmentation process. The results of these experiments are\nsummarized in Tables 9, 10, 11, and 12, and visualized in\nFigures 7 and 8.\nAdditionally, we employed well-established evaluation\nmetrics to comprehensively evaluate the classification perfor-\nmance, namely Receiver Operating Characteristic (ROC) and\nPrecision-Recall (PR) curves. The ROC curves illustrate the\ntrade-off between true positive and false positive rates, while\nthe PR curves demonstrate the relationship between precision\nand recall. These curves, presented in Figures 9, 10, 11,\nTABLE 5. Growth counts (Ara-Sarcasm dataset).\nClass Labels Dataset Type\nOriginal Euclidean cosine Jaccard BLEU\nNEGATIVE 3528 5245 4846 5317 5275\nNEUTRAL 5339 5339 5339 5339 5339\nPOSITIVE 1678 2607 2262 2459 2672\nTotal 10545 13191 12465 13115 13286\nTABLE 6. Growth counts (ASTD dataset).\nClass Labels Dataset Type\nOriginal Euclidean cosine Jaccard BLEU\nNEGATIVE 1640 1640 1640 1640 1640\nNEUTRAL 805 1074 1209 1134 1238\nPOSITIVE 776 1072 1134 1110 1237\nTotal 3221 3786 3983 3884 4151\nand 12, showcase the classification performance for the aug-\nmented and non-augmented datasets across the AraSarcasem,\nASTD, ATT, and MOVIE datasets.\nFurthermore, to provide evidence supporting the classi-\nfication performance results presented in Tables 9-12, we\nconducted a statistical test, known as the paired t-test [61].\nThis test is used to determine the significance of the F1 scores\nfor the datasets before and after augmentation. The purpose\nis to ascertain the statistical significance of the conclusions\ndrawn from these results. We selected a confidence level of\n0.05 for this analysis. The respective results for the datasets\ncan be found in Tables 13-16. Tables 13-16 clearly show\nthat all results are statistically significant. These results pro-\nvide valuable insights into the performance and effectiveness\nof the proposed approach in sentiment classification tasks\nacross the evaluated datasets.\nF. RESULTS DISCUSSION\nThis section started by conducting two experiments to vali-\ndate the proposed methodology. The first experiment was de-\nsigned to (i) evaluate the validity of using transformer-based\nmodels in processing and generating Arabic text and (ii)\nevaluate the percent of growth for each augmented similarity-\nbased (Euclidean, cosine, Jaccard, and BLEU) dataset on\nTABLE 7. Growth counts (ATT dataset).\nClass Labels Dataset Type\nOriginal Euclidean cosine Jaccard BLEU\nPOSITIVE 81 86 116 128 126\nNEGATIVE 2070 2070 2070 2070 2070\nTotal 2151 2156 2186 2198 2196\nTABLE 8. Growth counts (MOVIE dataset).\nClass Labels Dataset Type\nOriginal Euclidean cosine Jaccard BLEU\nPOSITIVE 381 381 556 762 617\nNEGATIVE 170 170 247 340 291\nNEUTRAL 966 966 966 966 966\nTotal 1517 1517 1769 2068 1874\n8\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nTABLE 9. AraSarcasm classification performance.\nAugmentation Type Testing on Augmented Split Testing on Not-Augmented Split\nF1 Accuracy Precision Recall F1 Accuracy Precision Recall\nBLEU (all-text) 0.80 0.84 0.79 0.80 0.84 0.80 0.79 0.80\nBLEU (new-text) 0.80 0.84 0.80 0.80 0.84 0.80 0.80 0.80\ncosine (all-text) 0.78 0.83 0.78 0.79 0.83 0.78 0.78 0.79\ncosine (new-text) 0.79 0.83 0.79 0.79 0.83 0.79 0.79 0.79\nEuclidean (all-text) 0.76 0.80 0.78 0.78 0.80 0.76 0.77 0.77\nEuclidean (new-text) 0.77 0.81 0.78 0.77 0.81 0.77 0.78 0.77\nJaccard (all-text) 0.77 0.81 0.77 0.77 0.81 0.77 0.77 0.77\nJaccard (new-text) 0.78 0.83 0.78 0.78 0.83 0.78 0.78 0.78\noriginal (text) 0.73 0.77 0.75 0.76 0.77 0.76 0.75 0.76\nTABLE 10. ASTD classification performance.\nAugmentation Type Testing on Augmented Split Testing on Not-Augmented Split\nF1 Accuracy Precision Recall F1 Accuracy Precision Recall\nBLEU (all-text) 0.76 0.77 0.76 0.76 0.76 0.77 0.76 0.76\nBLEU (new-text) 0.70 0.73 0.71 0.71 0.70 0.73 0.71 0.71\ncosine (all-text) 0.75 0.76 0.76 0.75 0.75 0.76 0.76 0.75\ncosine (new-text) 0.70 0.72 0.71 0.71 0.70 0.72 0.71 0.71\nEuclidean (all-text) 0.76 0.76 0.76 0.76 0.76 0.76 0.76 0.76\nEuclidean (new-text) 0.69 0.71 0.69 0.70 0.69 0.71 0.69 0.70\nJaccard (all-text) 0.74 0.76 0.76 0.75 0.74 0.76 0.76 0.75\nJaccard (new-text) 0.68 0.70 0.69 0.68 0.68 0.70 0.69 0.68\noriginal (text) 0.70 0.74 0.72 0.69 0.70 0.74 0.72 0.69\nTABLE 11. ATT classification performance.\nAugmentation Type Testing on Augmented Split Testing on Not-Augmented Split\nF1 Accuracy Precision Recall F1 Accuracy Precision Recall\nBLEU (all-text) 0.93 0.99 0.96 0.90 0.93 0.99 0.96 0.90\nBLEU (new-text) 0.91 0.98 0.99 0.87 0.91 0.98 0.99 0.87\ncosine (all-text) 0.85 0.98 0.99 0.85 0.85 0.98 0.99 0.85\ncosine (new-text) 0.85 0.98 0.99 0.85 0.85 0.98 0.99 0.85\nEuclidean (all-text) 0.89 0.98 0.99 0.82 0.89 0.98 0.99 0.82\nEuclidean (new-text) 0.88 0.98 0.99 0.85 0.88 0.98 0.99 0.85\nJaccard (all-text) 0.93 0.99 0.97 0.90 0.93 0.99 0.97 0.90\nJaccard (new-text) 0.95 0.99 0.97 0.93 0.95 0.99 0.97 0.93\noriginal (text) 0.84 0.98 0.89 0.80 0.84 0.98 0.89 0.80\nTABLE 12. MOVIE classification performance.\nAugmentation Type Testing on Augmented Split Testing on Not-Augmented Split\nF1 Accuracy Precision Recall F1 Accuracy Precision Recall\nBLEU (all-text) 0.56 0.72 0.73 0.58 0.56 0.72 0.73 0.59\nBLEU (new-text) 0.54 0.75 0.49 0.59 0.54 0.75 0.49 0.59\ncosine (all-text) 0.47 0.74 0.52 0.50 0.47 0.74 0.52 0.50\ncosine (new-text) 0.47 0.74 0.52 0.50 0.47 0.74 0.52 0.50\nEuclidean (all-text) 0.53 0.73 0.80 0.57 0.53 0.73 0.80 0.57\nEuclidean (new-text) 0.53 0.76 0.50 0.58 0.53 0.76 0.50 0.58\nJaccard (all-text) 0.60 0.76 0.74 0.63 0.60 0.76 0.74 0.63\nJaccard (new-text) 0.55 0.76 0.51 0.61 0.55 0.76 0.51 0.61\noriginal (text) 0.47 0.74 0.52 0.50 0.47 0.74 0.52 0.50\n9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nFIGURE 7. Sentiment analysis and classification performance results on all data sets (tested on not-augmented split).\nTABLE 13. Paired t-test results for Arasarcasem Dataset.\nAugmentation Type Paired t-test P-value Conclusion\nBLEU (all-text) 3.5 0.02 Significant\nBLEU (new-text) 3.16 0.03 Significant\ncosine (all-text) 4 0.01 Significant\ncosine (new-text) 2.75 0.05 Significant\nEuclidean (all-text) 4.49 0.01 Significant\nEuclidean (new-text) 2.7 0.05 Significant\nJaccard (all-text) 4.7 0.009 Significant\nJaccard (new-text) 2.9 0.04 Significant\noriginal (text) 2.64 0.05 Significant\nTABLE 14. Paired t-test results for ASTD dataset.\nAugmentation Type Paired t-test P-value Conclusion\nBLEU (all-text) 5.7 0.004 Significant\nBLEU (new-text) 2.83 0.04 Significant\ncosine (all-text) 2.74 0.05 Significant\ncosine (new-text) 2.8 0.05 Significant\nEuclidean (all-text) 3.16 0.03 Significant\nEuclidean (new-text) 4 0.02 Significant\nJaccard (all-text) 3.5 0.02 Significant\nJaccard (new-text) 3.2 0.03 Significant\noriginal (text) 3.25 0.03 Significant\n10\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nFIGURE 8. Sentiment analysis and classification performance results on all data sets (tested on augmented split).\nTABLE 15. Paired t-test results for ATT dataset.\nAugmentation Type Paired t-test P-value Conclusion\nBLEU (all-text) 3.21 0.03 Significant\nBLEU (new-text) 3.5 0.02 Significant\ncosine (all-text) 2.95 0.04 Significant\ncosine (new-text) 2.76 0.05 Significant\nEuclidean (all-text) 3.2 0.03 Significant\nEuclidean (new-text) 4 0.01 Significant\nJaccard (all-text) 4.82 0.01 Significant\nJaccard (new-text) 3.77 0.02 Significant\noriginal (text) 3.19 0.03 Significant\nTABLE 16. Paired t-test results for MOV dataset.\nAugmentation Type Paired t-test P-value Conclusion\nBLEU (all-text) 5.72 0.004 Significant\nBLEU (new-text) 6 0.003 Significant\ncosine (all-text) 3.21 0.03 Significant\ncosine (new-text) 3.08 0.04 Significant\nEuclidean (all-text) 2.83 0.04 Significant\nEuclidean (new-text) 2.75 0.05 Significant\nJaccard (all-text) 4.81 0.008 Significant\nJaccard (new-text) 3.2 0.03 Significant\noriginal (text) 4.47 0.01 Significant\n11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\n(a) ROC and PR curves for the augmented dataset. (b) ROC and PR curves for the non-augmented dataset.\nFIGURE 9. ROC and PR curves for Ara-Sarcasem dataset.\n(a) ROC and PR curves for the augmented dataset. (b) ROC and PR curves for the non-augmented dataset.\nFIGURE 10. ROC and PR curves for ASTD dataset.\n(a) ROC and PR curves for the augmented dataset. (b) ROC and PR curves for the non-augmented dataset.\nFIGURE 11. ROC and PR curves for ATT dataset.\nthe different selected experimental datasets (AraSarsacm,\nASTD, ATT, MOVIE), which have different sizes, labels, and\nnumber of instances per labels. The second experiment was\nconducted to assess the ability of the proposed augmentation\napproach in enhancing the Arabic sentiment classification\nperformance.\nOur results confirm that using Arabic transformer-based\nmodels can greatly enhance learning performance in process-\ning and generating Arabic text, as expected. However, the\nresults of text generated using AraGPT-2 for augmentation\nvaried across different cases. While some cases yielded per-\nfect text related to each other, other cases produced poor text.\nThis variability can be attributed to the fact that transformer\nmodels heavily rely on the accuracy of the data used for\npretraining, which is not always correct and accurate for\nArabic language models [33]. Despite this limitation, Arabic\ntransformer-based models generally perform well in preserv-\ning the context of generated text. Further discussions on the\nlimitations and potential improvements for these models are\nwarranted to gain a deeper understanding of their perfor-\nmance [10].\nFurther, our results indicate that each similarity-based\n12\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\n(a) ROC and PR curves for the augmented dataset. (b) ROC and PR curves for the non-augmented dataset.\nFIGURE 12. ROC and PR curves for MOVIE dataset.\naugmented dataset (using Euclidean, cosine, Jaccard, and\nBLEU metrics) exhibits a different percentage of growth,\nwhich tends to increase with larger datasets and decrease with\nsmaller datasets. Specifically, the AraSarsacm dataset [49]\nexperienced significant growth with +2714 new instances,\nwhile the ATT and MOVIE datasets [51] only achieved minor\ngrowth with +47 and +551 new instances, respectively, as\nshown in Tables 5, 6, and 8. This variance in growth can\nbe attributed to the dataset size itself, as working with larger\ndatasets increases the likelihood of generating new instances,\nwhereas working with smaller datasets limits this potential.\nIt is noteworthy that the BLEU augmented dataset tends to\nexhibit high growth percentages in all large datasets, while\nthe Jaccard augmented dataset shows high growth percent-\nages in all small datasets. In contrast, the cosine augmented\ndataset generally exhibits lower growth percentages across\nall datasets, except for AraSarsacm, whereas the Euclidean\naugmented dataset shows different growth percentages across\nall datasets. The cause of these variations in growth percent-\nages among different similarity metrics can be attributed to\ndifferent factors, including the sensitivity of the metric to\nthe magnitude of the original and generated sentences, the\ndataset size, and the nature of the data. All of these factors\ninfluence the percentage of growth in each similarity-based\naugmented dataset. Another finding of the first experiment\nis that the similarity thresholds for BLEU, Jaccard, and\nEuclidean were generally lower than those for cosine. The\ncosine similarity metric tended to score higher threshold\npercentages, likely due to its calculation being unaffected\nby sentence size, in contrast to the other similarity metrics,\nwhich tend to be influenced by sentence size.\nFinally, we note that there appears to be a relationship\nbetween the percentage of growth and the enhancement of\nclassification performance. As the growth percentage in-\ncreases, the classification performance tends to improve.\nThis finding is consistent with previous research on sarcasm\nand sentiment analysis. Overall, these results highlight the\ncomplex interplay between dataset size, similarity metrics,\nand the percentage of growth in augmented datasets. Further\nresearch is needed to explore these relationships in greater\ndepth and identify strategies to optimize the performance of\naugmented datasets in NLP tasks.\nOur second experiment provided further confirmation that\nthe proposed methodology, based on transformers and aug-\nmented similarity-based datasets, can effectively enhance\nArabic sentiment classification performance. To validate this\nclaim, we compared our proposed augmentation methodol-\nogy with related studies in the literature [30] using the same\ndataset [49]. The results of this comparison demonstrated the\neffectiveness of our approach.\nFurthermore, our findings revealed substantial improve-\nments in F1 scores for all datasets in the augmented datasets.\nIn the AraSarcasm dataset, we observed an enhancement of\n7% in the F1 score, while in the ASTD dataset, there was a\nnotable increase of 8%. The ATT dataset demonstrated even\nmore substantial gains, with a remarkable 11% improvement\nin the F1 score. Lastly, the MOVIE dataset exhibited the\nhighest improvement, with a substantial 13% increase in F1\nscore compared to the non-augmented dataset. These results\nhighlight the consistent and significant performance enhance-\nments achieved through our data augmentation methodology\nacross diverse Arabic sentiment datasets.\nOur findings also support the hypothesis that the perfor-\nmance of learning models improves as the size of the data\nincreases. Specifically, we observed a relationship between\nthe percentage of growth in augmented datasets and the\ncorresponding improvement in classification performance.\nAs a result, the learning model performs better with higher\ngrowth percentages in augmented similarity-based datasets.\nIn addition to these overall findings, we also observed\nseveral unexpected results related to the percentage of growth\nand the use of augmented similarity-based datasets with all\ntext and new text. Some of these observations provide in-\nsights into the best similarity metric to use when augmenting\nimbalanced Arabic datasets. Specifically, we found that the\nBLEU similarity metric achieved the highest classification\nperformance in all large datasets, while the Jaccard similarity\nmetric was preferred for small datasets, as it reached the\nhighest classification performance in this context.\nOverall, our research provides important insights into the\n13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\nuse of augmented similarity-based datasets to enhance Ara-\nbic sentiment classification performance. We believe that\nthese findings have important implications for the develop-\nment of more effective NLP strategies.\nVI. CONCLUSIONS AND FUTURE RESEARCH\nMotivated by the power of textual data augmentation (DA)\nin enhancing text classification, in this paper, we proposed a\nnew DA technique for Arabic text classification, incorporat-\ning the unique characteristics of the Arabic language.\nIn contrast to the existing Arabic DA techniques, which\nrely only on traditional augmentation methods, our technique\nemploys Arabic transformers to improve DA. Specifically,\nthe AraGPT-2 and AraBERT transformers are exploited in\nour technique for Arabic text generation and preprocess-\ning, respectively. Furthermore, our technique is designed to\nutilize several well-known similarity measures, such as the\nEuclidean, cosine, Jaccard, and BLEU measures, to assess\nthe quality of augmented sentences from different aspects,\nincluding context, semantics, and diversity.\nWe conducted several experiments to assess the effective-\nness of our technique in improving Arabic text classification.\nOur results clearly demonstrated (i) the gains of employ-\ning transformer-based models in processing and augmenting\nimbalanced Arabic datasets and (ii) the powerful impact\nof combining the cosine, Euclidean, Jaccard, and BLEU\nsimilarity measures in preserving the semantics, novelty, and\ndiversity of the augmented sentences. The gains provided\nby our proposed technique vary depending on the dataset\nsize and the similarity measures growth percent. Our re-\nsults confirm that BLEU is the preferred similarity metric\nto augment large imbalanced Arabic datasets, whereas Jac-\ncard is the preferred metric to use when working on small\ndatasets. Our experiments, conducted on different datasets\nwith distinct characteristics such as dataset size, label num-\nber, and unbalanced classes, showed significant improvement\nin sentiment classification performance compared to existing\ntechniques [30].\nFinally, addressing the limitations and potential shortcom-\nings of the proposed method is crucial for achieving a bal-\nanced perspective on its effectiveness and applicability. In the\nfollowing, we discuss the identified limitations of our study.\nFirstly, the variability in the performance of the proposed\napproach when tested on different datasets is acknowledged,\nrequiring careful consideration and further investigation.\nSecondly, the utilization of basic transformer models, due\nto limited hardware resources, may have constrained the\npotential gains of the technique. It is essential to highlight\nthat the approach’s performance can be further improved\nif more advanced transformer models can be utilized, as\nsuggested in [9]. Additionally, in future research, analyzing\nvarious data types and their impact on the performance of\nour proposed method is important. Moreover, conducting a\nthorough investigation and evaluation of diverse datasets is\nnecessary to gain a deeper understanding of the limitations\nand opportunities for improvement within the proposed ap-\nproach. Furthermore, it is worth noting that the effectiveness\nof our approach is closely tied to the hardware on which it\noperates, and utilizing hardware with superior specifications\ncan significantly enhance its practicality, especially when\ndealing with large-scale datasets. By addressing these limita-\ntions and exploring these avenues, a more comprehensive and\nbalanced perspective on the effectiveness and applicability of\nthe proposed approach can be reached, ensuring that it func-\ntions effectively with less time and resources on hardware\noptimized for high-performance execution.\nFurthermore, it is worth noting that our DA methodology\nexhibits versatility and can be applied to various NLP tasks,\nincluding text classification, sentiment regression, machine\ntranslation, and similar tasks where limited training data\nor class imbalances may arise. In such scenarios, the DA\napproaches and similarity metrics we have investigated hold\nthe potential to be adapted and effectively applied, thereby\nenhancing model performance and generalization across a\nbroader spectrum of Arabic NLP applications.\nREFERENCES\n[1] N. Ranjan, K. Mundada, K. Phaltane, and R. Ahmad, “A survey on\ntechniques in NLP,” International Journal of Computer Applications, vol.\n134, no. 8, pp. 6–9, January 2016.\n[2] U. Kamath, J. Liu, and J. Whitaker, Deep learning for NLP and speech\nrecognition. Springer, June 2019, vol. 84.\n[3] R. Socher, Y . Bengio, and D. Manning, Christopher, “Deep learning for\nNLP (without magic),” Tutorial Abstracts of ACL, p. 5, July 2012.\n[4] J. Wei and K. Zou, “EDA: Easy data augmentation techniques for boosting\nperformance on text classification tasks,” inProceedings of the Conference\non Empirical Methods in Natural Language Processing and the Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), November 2019, pp. 6382–6388.\n[5] G. Daval-Frerot and Y . Weis, “WMD at SemEval-2020 tasks 7 and 11:\nAssessing humor and propaganda using unsupervised data augmentation,”\nin Proceedings of the Fourteenth Workshop on Semantic Evaluation ,\nDecember 2020, pp. 1865–1874.\n[6] X. Dai and H. Adel, “An analysis of simple data augmentation for named\nentity recognition,” in Proceedings of the International Conference on\nComputational Linguistics, December 2020, pp. 3861–3867.\n[7] J.-P. Corbeil and A. Ghadivel, Hadi, “BET: A backtranslation approach\nfor easy data augmentation in transformer-based paraphrase identification\ncontext,” in arXiv:2009.12452, September 2020.\n[8] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation\nfor deep learning,” Journal of Big Data, vol. 6, no. 1, pp. 1–48, December\n2019.\n[9] S. Y . Feng, V . Gangal, J. Wei, S. Chandar, S. V osoughi, T. Mitamura, and\nE. Hovy, “A survey of data augmentation approaches for NLP,” inFindings\nof the Association for Computational Linguistics: ACL-IJCNLP , August\n2021, pp. 968–988.\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nKaiser, and I. Polosukhin, “Attention is all you need,”Advances in Neural\nInformation Processing Systems, vol. 30, 2017.\n[11] T. Lin, Y . Wang, X. Liu, and X. Qiu, “A survey of transformers,”AI Open,\nvol. 3, pp. 111–132, October 2022.\n[12] Y . Hou, Y . Liu, W. Che, and T. Liu, “Sequence-to-sequence data aug-\nmentation for dialogue language understanding,” in Proceedings of the\nInternational Conference on Computational Linguistics, August 2018, pp.\n1234–1245.\n[13] Y . Hou, S. Chen, W. Che, C. Chen, and T. Liu, “C2C-genda: cluster-to-\ncluster generation for data augmentation of slot filling,” in Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol. 35, no. 14, May 2021,\npp. 13 027–13 035.\n[14] K. Li, C. Chen, X. Quan, Q. Ling, and Y . Song, “Conditional augmentation\nfor aspect term extraction via masked sequence-to-sequence generation,”\nin Proceedings of the Annual Meeting of the Association for Computa-\ntional Linguistics, July 2020, pp. 7056–7066.\n14\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\n[15] T. Kober, J. Weeds, L. Bertolini, and D. Weir, “Data augmentation for\nhypernymy detection,” in Proceedings of the Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume ,\nApril 2021, pp. 1034–1048.\n[16] A. Celikyilmaz, E. Clark, and J. Gao, “Evaluation of text generation: A\nsurvey,”arXiv:2006.14799 , June 2020.\n[17] P.-E. Danielsson, “Euclidean distance mapping,” Computer Graphics and\nImage Processing, vol. 14, no. 3, pp. 227–248, November 1980.\n[18] J. Zobel and A. Moffat, “Exploring the similarity space,” ACM SIGIR\nForum, vol. 32, no. 1, pp. 18–34, April 1998.\n[19] G. I. Ivchenko and S. A. Honov, “On the Jaccard similarity test,” Journal\nof Mathematical Sciences, vol. 88, no. 6, pp. 789–794, March 1998.\n[20] R. S. Papineni, Kishore, T. Ward, and W.-J. Zhu, “BLEU: a method for\nautomatic evaluation of machine translation,” inProceedings of the Annual\nMeeting on Association for Computational Linguistics, 2002, p. 311–318.\n[21] A. R. Fabbri, S. Han, H. Li, H. Li, M. Ghazvininejad, S. Joty, D. Radev,\nand Y . Mehdad, “Improving zero and few-shot abstractive summarization\nwith intermediate fine-tuning and data augmentation,” in Proceedings of\nthe Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies , June 2021,\npp. 704–717.\n[22] C. Rastogi, N. Mofid, and F.-I. Hsiao, “Can we achieve more with\nless? exploring data augmentation for toxic comment classification,”\narXiv:2007.00875, July 2020.\n[23] G. Yan, Y . Li, S. Zhang, and Z. Chen, “Data augmentation for deep learn-\ning of judgment documents,” in International Conference on Intelligent\nScience and Big Data Engineering, October 2019, pp. 232–242.\n[24] Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le, “Unsupervised data\naugmentation for consistency training,” Advances in Neural Information\nProcessing Systems, vol. 33, pp. 6256–6268, July 2020.\n[25] I. Al-Huri, “Arabic language: historic and sociolinguistic characteristics,”\nEnglish Literature and Language Review, vol. 1, no. 2, pp. 28–36, 2015.\n[26] Ethnologue. (2020) Summary by language size. [Online]. Available:\nhttps://www.ethnologue.com/statistics/summary-language size- 19\n[27] K. A. Wahdan, S. Hantoobi, S. A. Salloum, and K. Shaalan, “A systematic\nreview of text classification research based on deep learning models\nin Arabic language,” International Journal of Electrical and Computer\nEngineering, vol. 10, no. 6, pp. 6629–6643, December 2020.\n[28] R. Duwairi and F. Abushaqra, “Syntactic-and morphology-based text\naugmentation framework for Arabic sentiment analysis,” PeerJ Computer\nScience, vol. 7, p. e469, April 2021.\n[29] E. Williams, P. Rodrigues, and S. Tran, “Accenture at CheckThat! 2021:\ninteresting claim identification and ranking with contextually sensitive\nlexical training data augmentation,” arXiv:2107.05684, July 2021.\n[30] A. Israeli, Y . Nahum, S. Fine, and K. Bar, “The IDC system for sentiment\nclassification and sarcasm detection in Arabic,” in Proceedings of the\nArabic Natural Language Processing Workshop, April 2021, pp. 370–375.\n[31] C. Sabty, I. Omar, F. Wasfalla, M. Islam, and S. Abdennadher, “Data\naugmentation techniques on Arabic data for named entity recognition,”\nProcedia Computer Science, vol. 189, pp. 292–299, January 2021.\n[32] W. Antoun, F. Baly, and H. Hajj, “AraGPT2: Pre-trained transformer\nfor Arabic language generation,” in Proceedings of the Arabic Natural\nLanguage Processing Workshop, April 2021, pp. 196–207.\n[33] W. Anton, F. Baly, and H. Hajj, “AraBERT: Transformer-based model for\nArabic language understanding,” in Proceedings of the LREC Workshop\nLanguage Resources and Evaluation Conference, May 2020, p. 9.\n[34] K. Gaanoun and I. Benelallam, “Arabic dialect identification: An Arabic-\nBERT model with data augmentation and ensembling strategy,” in Pro-\nceedings of the Arabic Natural Language Processing Workshop, December\n2020, pp. 275–281.\n[35] F. Harrag, M. Dabbah, K. Darwish, and A. AbdelAli, “BERT transformer\nmodel for detecting Arabic GPT2 auto-generated tweets,” in Proceedings\nof the Arabic Natural Language Processing Workshop , December 2020,\npp. 207–214.\n[36] A. Abuzayed and H. Al-Khalifa, “Sarcasm and sentiment detection in\nArabic tweets using BERT-based models and data augmentation,” in\nProceedings of the Arabic Natural Language Processing Workshop, April\n2021, pp. 312–317.\n[37] W. Al-Jamal, A. M. Mustafa, and M. Z. Ali, “Sarcasm detection in arabic\nshort text using deep learning,” inProceedings of International Conference\non Information and Communication Systems (ICICS), 2022, pp. 362–366.\n[38] S. AlAwawdeh and G. Abandah, “Improving the accuracy of semantic\nsimilarity prediction of arabic questions using data augmentation and\nensemble,” in Proceedings IEEE Jordan International Joint Conference\non Electrical Engineering and Information Technology (JEEIT), 2021, pp.\n272–277.\n[39] W. Antoun, F. Baly, and H. Hajj, “AraELECTRA: Pre-training text dis-\ncriminators for Arabic language understanding,” in Proceedings of the\nArabic Natural Language Processing Workshop, April 2021, pp. 191–195.\n[40] X. Carrasco, A. Elnagar, and M. Lataifeh, “A generative adversarial\nnetwork for data augmentation: The case of Arabic regional dialects,”\nProcedia Computer Science, vol. 189, pp. 92–99, January 2021.\n[41] K. Kowsari, K. J. Meimandi, M. Heidarysafa, S. Mendu, L. Barnes, and\nD. Brown, “Text classification algorithms: A survey,”Information, vol. 10,\nno. 4, p. 150, April 2019.\n[42] F. Abdul-Ghani and N. A. Abdullah, “A survey on Arabic text classi-\nfication using deep and machine learning algorithms,” Iraqi Journal of\nScience, vol. 63, no. 1, pp. 409–419, January 2022.\n[43] M. Sayed, R. K. Salem, and A. E. Khder, “A survey of Arabic text clas-\nsification approaches,” International Journal of Computer Applications in\nTechnology, vol. 59, no. 3, pp. 236–251, 2019.\n[44] A. AbdelAli, K. Darwish, N. Durrani, and H. Mubarak, “Farasa: A fast\nand furious segmenter for Arabic,” in Proceedings of the Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Demonstrations, June 2016, pp. 11–16.\n[45] M. Al-Yahya, H. Al-Khalifa, H. Al-Baity, D. AlSaeed, and A. Essam,\n“Arabic fake news detection: a comparative study of neural networks and\ntransformer-based approaches,” Complexity, vol. 2021, January 2021.\n[46] M. Abdul-Mageed and A. Elmadany, “ARBERT & MARBERT: Deep\nbidirectional transformers for Arabic,” inProceedings of the Annual Meet-\ning of the Association for Computational Linguistics and the International\nJoint Conference on Natural Language Processing , vol. 1, August 2021,\npp. 7088–7105.\n[47] J. Chen, D. Tam, C. Raffel, M. Bansal, and D. Yang, “An empirical survey\nof data augmentation for limited data learning in NLP,”arXiv:2106.07499,\n2021.\n[48] B. Li, Y . Hou, and W. Che, “Data augmentation approaches in natural\nlanguage processing: A survey,”AI Open, vol. 3, pp. 71–90, March 2022.\n[49] I. A. Farha, W. Zaghouani, and W. Magdy, “Overview of the W ANLP 2021\nshared task on sarcasm and sentiment detection in Arabic,” inProceedings\nof the Arabic Natural Language Processing Workshop , April 2021, pp.\n296–305.\n[50] S. Malaysha, M. Jarrar, and M. Khalilia, “Context-gloss augmenta-\ntion for improving arabic target sense verification,” arXiv preprint\narXiv:2302.03126, 2023.\n[51] H. ElSahar and R. El-Beltagy, Samhaa, “Building large Arabic multi-\ndomain resources for sentiment analysis,” in International Conference on\nIntelligent Text Processing and Computational Linguistics, April 2015, pp.\n23–34.\n[52] F. Almeida and G. Xexéo, “Word embeddings: A survey,”\narXiv:1901.09069, January 2019.\n[53] H. Almerekhi and T. Elsayed, “Detecting automatically-generated Arabic\ntweets,” Information Retrieval Technology, pp. 123–134, December 2015.\n[54] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence embeddings\nusing siamese BERT-networks,” in Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing and the International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) ,\nNovember 2019, pp. 3982–3992.\n[55] L. Zhang, S. Wang, and B. Liu, “Deep learning for sentiment analysis:\nA survey,” Wiley Interdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, vol. 8, no. 4, p. e1253, July 2018.\n[56] D. Anguita, L. Ghelardoni, A. Ghio, L. Oneto, and S. Ridella, “The ‘ k’\nin k-fold cross validation,” in Proceedings of the European Symposium\non Artificial Neural Networks, Computational Intelligence and Machine\nLearning (ESANN), 2012, pp. 441–446.\n[57] M. Nabil, M. Aly, and A. Atiya, “ASTD: Arabic sentiment tweets dataset,”\nin Proceedings of the Conference on Empirical Methods in Natural Lan-\nguage Processing, September 2015, pp. 2515–2519.\n[58] A. Jindal, A. G. Chowdhury, A. Didolkar, D. Jin, R. Sawhney, and R. Shah,\n“Augmenting NLP models using latent feature interpolations,” in Pro-\nceedings of the International Conference on Computational Linguistics ,\nDecember 2020, pp. 6931–6936.\n[59] P. Liu, X. Wang, C. Xiang, and W. Meng, “A survey of text data aug-\nmentation,” in Proceedings of the International Conference on Computer\nCommunication and Network Security (CCNS) , August 2020, pp. 191–\n195.\n[60] D. Refai, “Arabic-data-augmentation,” https://github.com/Dania-\nRefai/Arabic-Data-Augmentation.git, 2023, [Source code].\n15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. Refai et al.: Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification\n[61] A. Ross and V . L. Willson, Paired Samples T-Test. Rotterdam: SensePub-\nlishers, 2017, pp. 17–19.\nDANIA REFAI was born in Damascus, Syria, in\n1994. She received a B.S. degree in Computer\nScience from Umm Al-Qura University (UQU),\nMecca, Saudi Arabia, in 2018 and an M.S. degree\nin Computer Science from Princess Sumaya Uni-\nversity for Technology (PSUT), Amman, Jordan,\nin 2022. She is currently pursuing a Ph.D. degree\nat PSUT. From 2019 to 2022, she was a teaching\nassistant at PSUT. Her research interests include\nmany research topics, such as artificial intelli-\ngence, machine learning, natural language processing, and game theory.\nSALEH ABU-SOUD is a full professor at the\nDept. of Data Science and Artificial Intelligence\nin Princess Sumaya University for Technology\n(PSUT). He got his Ph.D. in Computer Science\nin 1992 (METU), M. Sc. in Computer Science in\n1988 (METU), and B.Sc. in Computer Science in\n1985 (Yarmouk University). He was working at\nthe University of Jordan in the period between\n1992 and 1995, then he joined PSUT till now,\nwhere he served as the head of the department\nof Computer Science in the period from 2005 to 2007, and the Dean of\nstudents affairs in 2014/2015. He left to work at NYIT for four years period\nfrom 2007 to 2011, during which he was a professor of Computer Science\nand the director of the accreditation and quality assurance department in\nthe period from 2007 to 2010. His research interest is in the area of\nArtificial Intelligence. He is the owner of the inductive learning algorithm\n(ILA) and EXCLUDE algorithm for excluding irrelevant attributes. He is\ninterested mainly in many research topics such as Machine Learning, Nat-\nural Language Understanding, Biometric Keystroke Dynamics, and Speech\nSynthesis with inductive learning. He has many research papers and two\nbooks. He supervised dozens of master’s students and many Ph.D. students.\nMOHAMMAD J. ABDEL-RAHMAN (S’12–\nM’15–SM’20) is an Associate Professor with the\nData Science Department at Princess Sumaya Uni-\nversity for Technology since October 2022 and\nan Adjunct Assistant Professor with the Electrical\nand Computer Engineering (ECE) Department at\nVirginia Tech since January 2018. He received his\nPh.D. from the ECE Department at the Univer-\nsity of Arizona in November 2014 with the Col-\nlege of Engineering Outstanding Graduate Stu-\ndent Award. He was with the ECE Department at Virginia Tech as a Research\nFaculty Member from January 2015 to September 2017. From October 2017\nto October 2022, he was with Al Hussein Technical University, where he\nwas an Associate Professor with the Electrical Engineering and Computer\nScience (joint appointment) Departments from May 2019 to October 2022,\nthe Inaugural Chair of the Electrical Engineering Department from Octo-\nber 2018 to March 2022, and the Director of the Wireless Networks &\nSecurity Research Lab from April 2019 to October 2023. His research\ninterests include wireless communications and networking, smart grids,\nstochastic optimization, game theory, and machine learning.\nHe is an Editor for the Springer Nature Journal on Wireless Personal\nCommunications, a reviewer for several international journals, and a TPC\nmember for several international conferences.\n16\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3336311\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8228150010108948
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6437097787857056
    },
    {
      "name": "Jaccard index",
      "score": 0.5625736117362976
    },
    {
      "name": "Natural language processing",
      "score": 0.5220817923545837
    },
    {
      "name": "Arabic",
      "score": 0.5050578713417053
    },
    {
      "name": "Transformer",
      "score": 0.4801768362522125
    },
    {
      "name": "Cosine similarity",
      "score": 0.4504292607307434
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3661140203475952
    },
    {
      "name": "Data mining",
      "score": 0.3544217348098755
    },
    {
      "name": "Machine learning",
      "score": 0.3542286455631256
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I158749337",
      "name": "Princess Sumaya University for Technology",
      "country": "JO"
    },
    {
      "id": "https://openalex.org/I859038795",
      "name": "Virginia Tech",
      "country": "US"
    }
  ],
  "cited_by": 17
}