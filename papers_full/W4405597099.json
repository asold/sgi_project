{
  "title": "Probabilistic medical predictions of large language models",
  "url": "https://openalex.org/W4405597099",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2566362221",
      "name": "Bowen Gu",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2156170693",
      "name": "Rishi J. Desai",
      "affiliations": [
        "Harvard University",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2472315021",
      "name": "Kueiyu Joshua Lin",
      "affiliations": [
        "Harvard University",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2106930453",
      "name": "Jie Yang",
      "affiliations": [
        "Broad Institute",
        "Harvard University Press",
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2566362221",
      "name": "Bowen Gu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156170693",
      "name": "Rishi J. Desai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2472315021",
      "name": "Kueiyu Joshua Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106930453",
      "name": "Jie Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4296261149",
    "https://openalex.org/W4312085331",
    "https://openalex.org/W2930127373",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3102840478",
    "https://openalex.org/W1525699266",
    "https://openalex.org/W2021949540",
    "https://openalex.org/W4389043118",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4391973028",
    "https://openalex.org/W4395067526",
    "https://openalex.org/W4388157511",
    "https://openalex.org/W4393214664",
    "https://openalex.org/W4400530541",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4396498719",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4404783351",
    "https://openalex.org/W6601160403",
    "https://openalex.org/W6600178264",
    "https://openalex.org/W4400252889",
    "https://openalex.org/W4324135233",
    "https://openalex.org/W4397003497",
    "https://openalex.org/W4385573174",
    "https://openalex.org/W4401024268",
    "https://openalex.org/W2620563544",
    "https://openalex.org/W4389182159",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W6781254577",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W4401477103",
    "https://openalex.org/W4392616695",
    "https://openalex.org/W4318812304"
  ],
  "abstract": "Large Language Models (LLMs) have shown promise in clinical applications through prompt engineering, allowing flexible clinical predictions. However, they struggle to produce reliable prediction probabilities, which are crucial for transparency and decision-making. While explicit prompts can lead LLMs to generate probability estimates, their numerical reasoning limitations raise concerns about reliability. We compared explicit probabilities from text generation to implicit probabilities derived from the likelihood of predicting the correct label token. Across six advanced open-source LLMs and five medical datasets, explicit probabilities consistently underperformed implicit probabilities in discrimination, precision, and recall. This discrepancy is more pronounced with smaller LLMs and imbalanced datasets, highlighting the need for cautious interpretation, improved probability estimation methods, and further research for clinical use of LLMs.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-024-01366-4\nProbabilistic medical predictions of large\nlanguage models\nCheck for updates\nBowen Gu1, Rishi J. Desai1,K u e i y uJ o s h u aL i n1,5,6 & Jie Yang1,2,3,4,5,6\nLarge Language Models (LLMs) have shown promise in clinical applications through prompt\nengineering, allowingﬂexible clinical predictions. However, they struggle to produce reliable\nprediction probabilities, which are crucial for transparency and decision-making. While explicit\nprompts can lead LLMs to generate probability estimates, their numerical reasoning limitations raise\nconcerns about reliability. We compared explicit probabilities from text generation to implicit\nprobabilities derived from the likelihood of predicting the correct label token. Across six advanced\nopen-source LLMs andﬁve medical datasets, explicit probabilities consistently underperformed\nimplicit probabilities in discrimination, precision, and recall. This discrepancy is more pronounced with\nsmaller LLMs and imbalanced datasets, highlighting the need for cautious interpretation, improved\nprobability estimation methods, and further research for clinical use of LLMs.\nGenerating credible probability (or conﬁdence) of prediction is crucial in\nclinical practice and medical research when applying artiﬁcial intelligence\n(AI) to healthcare. Reliable probability outputs are critical for informed\ndecision-making, stratifying patients with different risk levels, and enabling\nclinicians to set probability thresholdsaccording to their preferred trade-offs\nbetween recall and precision in real-world clinical practice\n1–4. For example, a\nlower threshold can be applied in screening applications to reﬂect the\naversion to false negatives. Accurate probability outputs have a strong\ninﬂuence the adoption and effectiveness of AI in healthcare. Discriminative\nAI models, such as support vector machines and classic deep learning\nmodels\n5,6, predict labels by assigning probabilities to aﬁxed set of label\ncandidates and selecting the label with the highest probability. This para-\ndigm naturally generates the probability of the predicted label and has been\nwidely adopted in medical AI applications5,7–9.\nGenerative AI models, especially Large Language Models (LLMs), have\ndemonstrated remarkable general-purpose capabilities and the ability to\nperform few-shot or zero-shot learning, enabling accurate predictions with\nlittle or no annotated data10,11. These models are particularly advantageous\nin clinical applications where tasks are diverse and annotated data is scarce\nand expensive to generate\n12,13. Prompt-based LLMs represent the most\ncommon usage of LLMs, as they canﬂexibly instruct the models to perform\ndifferent tasks by simply deﬁning prompt text without heavy model\ntraining14–16. However, this generative framework does not naturally output\nprobabilities of the predictions, as it converts all the tasks as a text generation\nprocess, rather than assigning probabilities toﬁxed candidate labels. As a\nresult, probabilistic predictions of LLMs in healthcare have been reported\nand evaluated rarely17–19, leading to a lack of crucial assessment of AI in\nclinical practice.\nProbabilities have been estimated in the literature directly through text\ngeneration by LLMs. Such a probability, called explicit, is simple and\nﬂexible20,21. For example, one can instruct the LLM output the probability of\nits prediction by adding a sentence such as“Please provide the probability\nalong with your prediction” to the prompt. The simplicity of explicit\nprobability ensures that it can be applied in any advanced prompt engi-\nneering techniques such as Chain-of-Thought (CoT)\n22, retrieval augmented\ngeneration (RAG)23,24, self consistency25, and LLM agents26,27. However,\ngiven the challenges LLMs face with numerical reasoning28,29, the reliability\nof text-generated explicit probability values may be questionable and has not\nyet been thoroughly examined.\nOn the other hand, several works have delved into the LLM-based\nmethods of probability prediction\n21,30,w h i c hw ed eﬁne as“implicit prob-\nability”. For example, for the question“Given the following lab reports: [lab\nreport text]. Does the patient have COVID? Your answer must be either‘Yes’\nor ‘No’.”, and if the LLM response is“No”, one can extract the probability of\neach generated token of the LLM, identify the position of prediction token\n(i.e.,“No”in this example), and assign the model’s implicit probability of the\nprediction as the probability that corresponds to the prediction token.\nHowever, such extraction of implicit probability is only available in limited\nscenarios. As shown in Fig.3a, in information extraction tasks, the LLM\npredicted label tokens are diverse and with different lengths; when advanced\nL L Mp r o m p t s ,s u c ha sC o T ,a r ea p p l i e d ,the predicted label tokens are not in\na ﬁxed position. These factors complicate the extraction of implicit\n1Division of Pharmacoepidemiology and Pharmacoeconomics, Department of Medicine, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA,\nUSA. 2Harvard Data Science Initiative, Harvard University, Cambridge, MA, USA.3Broad Institute of MIT and Harvard, Cambridge, MA, USA.4Kempner Institute for\nthe Study of Natural and Artiﬁcial Intelligence, Harvard University, Cambridge, MA, USA.5These authors contributed equally: Kueiyu Joshua Lin, Jie Yang.6These\nauthors contributed jointly supervised this work: Kueiyu Joshua Lin, Jie Yang.e-mail: jyang66@bwh.harvard.edu\nnpj Digital Medicine|           (2024) 7:367 1\n1234567890():,;\n1234567890():,;\nprobability, making it challenging to automate the extraction across dif-\nferent tasks and prompts. In addition, many advanced proprietary LLMs,\nsuch as Google Gemini (https://deepmind.google/technologies/gemini/)\nand Anthropic Claude (https://claude.ai/), do not provide APIs that would\nreturn the probability for each output token, which makes it infeasible to get\nthe implicit probability. Consequently, this implicit probability can only be\nextracted under very simple settings (e.g., for option selection tasks or tasks\nwithout a chain of thought) and is mostly available in open-source LLMs,\nwhich largely limits its application.\nIn this study, we extensively examine the reliability, which is deﬁned as\nthe quality of being trustworthy, of the probabilistic output of LLMs by\ncomparing the explicit probability and the implicit probability. Speciﬁcally,\nwe select 6 advanced open-weighted LLMs from different organizations and\nevaluated their performance on 5 medical datasets. To utilize the Area\nUnder the Receiver Operating Characteristic (AUROC) and Area Under the\nP r e c i s i o nR e c a l lC u r v e( A U P R C )a se v a l u a t i o nm e t r i c s ,w ec o n v e r tt h eL L M\nprediction task into a binary selection task within a question-and-answer\nsetting with two possible options. Weﬁnd that while explicit probability\nreﬂects a certain level of model prediction conﬁdence, its reliability is con-\nsistently lower than that of implicit probability across all LLM models and\ndatasets, especially in the case of small LLMs and imbalanced datasets.\nThe contributions of this work include a large-scale evaluation of the\nprobabilistic medical predictions of LLMs, a crucial component of applying\nAI in healthcare, demonstrating that the commonly used explicit probability\nof LLMs has lower reliability under certain circumstances, which necessi-\ntates caution when applying it in the medical domain, and providing a\nframework for evaluating the reliability of probabilistic outputs of LLMs in\nmedical predictions, which can be easily extended to otherﬁelds requiring\nhigh-quality probabilistic assessments.\nResults\nAll LLMs follow instructions well, with outputs strictly adhering to the\nformat in the prompt, except for the Qwen2-7B model on the USMLE\ndataset. All other models have a less than 10% fail rate on the validation\nprocess, with 78.6% of the model inference on one dataset have a less than\n1% fail rate on the validation process.The details of instruction adherence\nfor each LLM experiment are shown in Supplementary Table 7.\nLLM Performance\nThe accuracy, AUROC, and AUPRC of thelarge LLMs on different datasets\nare listed in Tables1–3, respectively. The“Explicit”stands for the AUROC\nof the explicit probability,“Implicit” stands for the AUROC of the implicit\nprobability, and“Difference”stands for the difference between the AUROC\nof the implicit probability and the AUROC of the explicit probability. For\nTable2 and Table3,t h e“P-value”indicates the statistical signiﬁcance of the\nimplicit probability over the explicit probability.According to Table1,a l l\nLLMs achieve at least a 0.8 average accuracy on all datasets. Speciﬁcally, the\nLlama-3.1-70B model has the highest average accuracy among all the LLMs\ntested. The Qwen2-72B model, which is trained using a rich Chinese corpus,\ns h o w sac l e a ra d v a n t a g eo v e ro t h e rm o d e l so nt h eM C M L Ed a t a s e t ,w h i c h\nconsists of Chinese medical examination questions. On the other hand, the\nPhi-3-medium model has a poor performance on the MCMLE dataset. The\nMistral-Large model, which has comparable capability to the Llama-3.1-70B\nmodel, achieve the highest accuracy for the MMLU-CM and the MGB-\nSDoH dataset but its performance is much worse than the Llama-3.1-70B\nmodel on the non-English MCMLE dataset (85.2% VS 93.0%).\nAlthough the AUROC and AUPRC forboth the explicit and implicit\nprobabilities are high, the metrics for implicit probabilities are consistently\nhigher than those for explicit probabilities across all LLMs, and the majority\nof the results show that such advantage is statistically signiﬁcant. This\nﬁnding is valid on both a non-English dataset (MCMLE) and a dataset from\nprivate electronic health records (MGB-SDoH). The only exception is the\nLlama-3.1-70B model on the USMLE dataset, which is mainly due to the\nhigh standard deviation of the implicit probability. Overall, this result\nindicates that for LLMs, the implicit probability is a better indicator of the\nmodel answer’sc o nﬁdence (Tables2 and 3).\nThe ROC and PRC curves of the LLMs on each dataset are shown from\nSupplementary Fig. 1 to Supplementary Fig. 10. We observe that the ROC\nand PRC curves for the explicit probability is a coarse step function because\nthe LLMs usually give their probabilities in their nearest tenth (e.g., 90%,\n80%, etc.), which makes the explicit probability coarse-grained. In contrast,\nthe steps of the implicit probability are barely discernible in the plot. This\nfurther proves that the implicit probability is a better indicator than the\nexplicit probability.\nWe ﬁnd that in some cases, while the LLM provides an answer and an\nexplicit probability as instructed, this probability is less than 50% (Supple-\nmentary Table 3). This implies that although the model proposes an answer,\nit assigns a higher probability to the alternative, undermining its reliability.\nIn contrast, the implicit probability always selects the most probable token,\navoiding such contradictions and supporting its reliability over the explicit\nprobability.\nLarge LLM vs. small LLM\nThe AUROCs of the large LLMs and their smaller counterparts on the\nUSMLE and MGB-SDoH datasets are shown in Fig.1.E x c e p tf o rt h eY i - 1 . 5\nmodel on the MGB-SDoH dataset, all large LLMs outperform their smaller\nversions on both explicit and implicit probability AUROC, due to their\nricher knowledge and stronger reasoning abilities. Furthermore, small LLMs\ngenerally exhibit greater differences between explicit and implicit prob-\nability AUROC, as their limited parameters make them less sensitive to\nexplicit probabilities, reducing reliability. In contrast, the implicit prob-\nabilities difference for small LLMs is much smaller than for the explicit\nprobabilities, indicating their better credible to represent the model’sa c t u a l\nprobability to its prediction.\nImbalanced datasets analysis\nT h eA U P R Cd i f f e r e n c e so ft h el a r g eL L M sa r es h o w ni nF i g .2a, where the\npercentage of the imbalance in the legend indicates the percentage of option\nA being the correct label in the dataset, and a 50% imbalance means that\nthere is an equal number of option A and option B being correct in the\nTable 1 | Accuracy of the LLMs on different datasets\nModel Average (%) MMLU-CK MMLU-CM USMLE MCMLE MGB-SDoH\ngemma-2-27b-it 86.7 ± 0.1 88.8 ± 0.2 83.6 ± 0.6 83.4 ± 0.4 82.1 ± 0.2 95.3 ± 0.1\nMistral-Large-Instruct-2407 90.0 ± 0.5 91.8 ± 0.6 87.6 ± 0.3 89.2 ± 0.3 85.2 ± 1.2 96.0 ± 0.3\nYi-1.5-34B-Chat 86.1 ± 0.3 86.5 ± 0.9 82.6 ± 0.9 77.6 ± 0.5 90.3 ± 0.2 93.6 ± 0.2\nPhi-3-medium-128k-instruct 83.8 ± 1.0 91.0 ± 1.1 80.5 ± 0.8 83.6 ± 2.1 69.8 ± 1.5 94.0 ± 1.2\nQwen2-72B-Instruct 91.3 ± 0.1 90.0 ± 0.3 87.3 ± 0.5 86.7 ± 0.4 96.7 ± 0.2 95.7 ± 0.2\nMeta-Llama-3.1-70B-Instruct 91.7 ± 0.3 93.4 ± 1.0 85.5 ± 0.0 92.7 ± 0.2 93.0 ± 0.2 94.0 ± 0.2\nBoldface indicates the best performance among the 6 models under comparison in each dataset.\nItalics indicates the worst performance among the 6 models under comparison in each dataset.\nThe number following the“±” symbol indicates the standard deviation.\nhttps://doi.org/10.1038/s41746-024-01366-4 Article\nnpj Digital Medicine|           (2024) 7:367 2\ndataset. The difference in theﬁgure is deﬁned as the difference between the\nAUPRC of the implicit probability and the explicit probability. Based on\nFig. 2, for most LLMs the AUPRC difference increases as the dataset got\nmore imbalanced. This can be explained by how the AUPRC is calculated.\nS i n c ew ed eﬁne option A as positive when plotting the AUPRC, the scarcity\nof option A being the correct label plays an important role in determining\nthe precision and the recall, which are the keys to the PRC curve. As the\nnumber of options A being correct gets lower, any false classiﬁcation will be\nmagniﬁed, which enlarges the differencesbetween the two probabilities if\none is better than the other. We do not show the results under the 70% and\n90% imbalance rates since they are very similar to the ones under the 50%\nimbalance ratio. This is also reasonable given that the PRC is not related to\nTable 2 | AUROC of the LLMs on different datasets\nModel AUROC (%) MMLU-CK MMLU-CM USMLE MCMLE MGB-SDoH\ngemma-2-27b-it Explicit 91.6 ± 0.6 86.9 ± 0.8 85.4 ± 0.4 85.7 ± 3.3 97.4 ± 0.2\nImplicit 95.9 ± 0.5 92.0 ± 0.8 90.2 ± 0.4 90.0 ± 0.5 98.2 ± 0.4\nDifference 4.3 ± 0.2\n* 5.1 ± 1.2* 4.9 ± 0.7* 4.2 ± 3.7* 0.8 ± 0.3*\nMistral-Large-Instruct-2407 Explicit 93.2 ± 0.3 89.7 ± 0.5 91.8 ± 0.5 87.2 ± 3.3 97.1 ± 0.3\nImplicit 96.6 ± 0.4 94.6 ± 0.6 94.1 ± 0.3 91.0 ± 0.8 98.4 ± 0.3\nDifference 3.4 ± 0.2\n* 4.8 ± 0.5* 2.4 ± 0.6* 3.8 ± 3.3 1.3 ± 0.6 *\nYi-1.5-34B-Chat Explicit 90.0 ± 1.1 85.6 ± 0.7 80.6 ± 0.2 92.8 ± 2.2 92.8 ± 0.3\nImplicit 94.8 ± 0.9 90.8 ± 0.6 85.4 ± 0.8 96.3 ± 0.8 97.5 ± 0.4\nDifference 4.8 ± 2.0\n* 5.2 ± 1.3* 4.8 ± 0.6* 3.5 ± 3.0 4.7 ± 0.4 *\nPhi-3-medium-128k-instruct Explicit 93.4 ± 0.6 83.7 ± 1.7 86.1 ± 1.7 75.8 ± 4.2 94.4 ± 0.8\nImplicit 96.8 ± 0.2 91.8 ± 0.5 92.2 ± 0.5 80.5 ± 0.9 97.9 ± 0.1\nDifference 3.4 ± 0.6\n* 8.1 ± 2.2* 6.1 ± 1.2* 4.7 ± 4.1 3.5 ± 0.7 *\nQwen2-72B-Instruct Explicit 92.8 ± 0.8 88.9 ± 0.8 90.5 ± 0.2 97.1 ± 0.3 95.9 ± 0.2\nImplicit 93.1 ± 3.5 90.9 ± 3.3 89.5 ± 3.2 97.7 ± 1.1 97.0 ± 1.2\nDifference 0.3 ± 3.2 2.0 ± 3.3 −0.9 ± 3.1 0.6 ± 0.8 1.1 ± 1.2\nMeta-Llama-3.1-70B-Instruct Explicit 93.8 ± 0.4 86.9 ± 1.1 93.2 ± 0.6 94.0 ± 1.8 95.1 ± 0.2\nImplicit 97.7 ± 1.6 92.2 ± 2.7 96.3 ± 2.0 97.4 ± 1.4 96.6 ± 1.5\nDifference 4.0 ± 1.4\n* 5.3 ± 3.3 3.1 ± 2.3 3.4 ± 0.3 1.4 ± 1.6\n*p-value < 0.05.\nThe number following the“±” symbol indicates the standard deviation.\nTable 3 | AUPRC of the LLMs on different datasets\nModel AUPRC (%) MMLU-CK MMLU-CM USMLE MCMLE MGB-SDoH\ngemma-2-27b-it Explicit 91.8 ± 0.3 87.8 ± 0.4 83.0 ± 0.6 84.4 ± 0.2 97.7 ± 0.2\nImplicit 96.1 ± 0.5 92.4 ± 0.9 89.4 ± 0.8 89.9 ± 0.9 97.9 ± 0.3\nDifference 4.3 ± 0.3\n* 4.6 ± 1.3* 6.4 ± 0.5* 5.5 ± 1.0* 0.1 ± 0.1\nMistral-Large-Instruct-2407 Explicit 94.7 ± 0.4 92.3 ± 0.4 91.9 ± 0.6 87.5 ± 0.9 97.6 ± 0.3\nImplicit 97.3 ± 0.2 95.4 ± 0.5 93.6 ± 0.1 90.3 ± 0.7 98.0 ± 0.2\nDifference 2.6 ± 0.2\n* 3.1 ± 0.2* 1.7 ± 0.6* 2.7 ± 0.8* 0.4 ± 0.2*\nYi-1.5-34B-Chat Explicit 88.6 ± 1.2 84.0 ± 0.9 80.8 ± 0.2 92.7 ± 1.0 93.6 ± 0.3\nImplicit 94.8 ± 0.2 91.7 ± 0.8 85.0 ± 0.3 96.8 ± 0.5 97.9 ± 0.2\nDifference 6.1 ± 1.2\n* 7.6 ± 1.7* 4.2 ± 0.5* 4.1 ± 1.4* 4.3 ± 0.3*\nPhi-3-medium-128k-instruct Explicit 93.1 ± 0.5 82.5 ± 2.4 82.9 ± 1.9 72.0 ± 1.7 95.6 ± 0.9\nImplicit 95.3 ± 1.0 91.3 ± 0.5 90.2 ± 0.4 80.3 ± 0.7 96.5 ± 0.9\nDifference 2.2 ± 1.2\n* 8.8 ± 2.8* 7.3 ± 1.6* 8.3 ± 2.0* 1.0 ± 0.3*\nQwen2-72B-Instruct Explicit 93.9 ± 0.5 90.3 ± 0.5 89.9 ± 0.6 97.3 ± 02. 97.1 ± 0.1\nImplicit 94.1 ± 1.4 92.4 ± 1.6 90.3 ± 0.6 98.1 ± 0.6 97.7 ± 0.5\nDifference 0.2 ± 1.0 2.1 ± 1.6 0.4 ± 0.8 0.8 ± 0.5 0.5 ± 0.5\nMeta-Llama-3.1-70B-Instruct Explicit 95.2 ± 0.4 90.1 ± 0.8 93.2 ± 0.2 93.8 ± 0.6 96.4 ± 0.1\nImplicit 98.1 ± 1.1 93.7 ± 2.0 96.6 ± 1.7 97.6 ± 1.3 97.2 ± 0.9\nDifference 2.9 ± 0.9\n* 3.6 ± 2.3 3.4 ± 1.7 * 3.8 ± 1.5* 0.8 ± 0.9\n*p-value < 0.05.\nThe number following the“±” symbol indicates the standard deviation.\nhttps://doi.org/10.1038/s41746-024-01366-4 Article\nnpj Digital Medicine|           (2024) 7:367 3\nthe true negatives, and the excessivenumber of options A being correct will\nminimize the differences between the two probabilities.\nLLM probability distribution\nThe probability distribution of each LLM on each dataset is shown from\nSupplementary Figs. 11–15 and an example of the Meta-Llama-3.1-70B-\nInstruct LLM on the USMLE dataset is shown in Fig.2b. According to\nFig. 2b, except for the Phi-3-medium model on the MCMLE dataset, all\nother LLMs have a relatively great performance on each dataset by having\nlow probability when the true label is 0 and high probability when the true\nlabel is 1, which is consistent with the consistent performance weﬁnd\n(Table1). Compared to the explicit probability, the implicit probability has a\nmore dispersed distribution, in accord with ourﬁnding on the AUROC and\nAUPRC curves. However, we notice thatthe explicit probability, even with\nits narrow distribution, may give values that are close to 50%, while the more\nwidely distributed implicit probability values rarely fall within this area.\nFor most LLMs, both their explicit and implicit probability distribu-\ntions are very polarized, even if their predictions are incorrect. This indicates\nthat the LLMs are overly conﬁdent about most of their predictions,\nregardless of their actual correctness. Since such polarization still persists on\nthe MGB-SDoH dataset, a private dataset that can’t be touched during LLM\ntraining, we argue that such conﬁdence does not originate from the data\nleakage\n31, but is an intrinsic property of the LLMs. Some exceptions to the\na b o v eo b s e r v a t i o na r et h eM i s t r a l - L a r g ea n dt h eP h i - 3 - m e d i u mm o d e lo n\nthe MCMLE and the MGB-SDoH dataset, where models show a broad\nspectrum of implicit probability distribution, but their explicit probability\ndistributions are still highly polarized.\nSensitivity analysis\nThe AUROCs of the LLMs on the USMLE dataset using the three different\nprompts are shown in Supplementary Fig. 16. It indicates that the implicit\nprobability has a higher AUROC than that of the explicit probability under\nFig. 1 | AUROCs of the large and small LLMs.AUROCs of the large and small LLMs.a AUROCs of the large and small LLMs on the USMLE dataset.b AUROCs of the large\nand small LLMs on the MGB-SDoH dataset.\nhttps://doi.org/10.1038/s41746-024-01366-4 Article\nnpj Digital Medicine|           (2024) 7:367 4\nall three prompts. We also learn that the main prompt and theﬁrst auxiliary\nprompt have similar performance in terms of AUROC for both prob-\nabilities, which is reasonable sinceboth prompts use a similar multiple-\nchoice format and contain similar information. The AUROC for both\nprobabilities largely decrease when using the second auxiliary prompt. We\npropose that this is due to the introduction of the setting of a student\nproposing answers to the LLM, which introduces additional complexity and\nlowered the model’s performance.\nDiscussion\nThis study investigates the probabilistic prediction of LLMs in healthcare\nand demonstrate that the simple andﬂexible explicit probability (directly\nextracted from the generated text following the prompt) provide relatively\nhigh AUROC and AUPRC, but is consistently less reliable than the implicit\nprobability (derived from the transition score of the predicted label token)\nacross different languages, datasets, and prompt designs. The performance\ngap between explicit and implicit probabilities is especially large for\nsmall LLMs.\nProbability estimation is important because the estimates are com-\npared with thresholds set speciﬁcally for different clinical needs, reﬂecting\nexpert judgment. For example, lower thresholds are used for low-risk\ninterventions, while higher thresholds are used for invasive procedures.\nIncorrect estimation can lead to misinformed decisions, compromising\npatient safety and treatment efﬁcacy\n32,33. Our study reminds the LLM users\nof the potential drawbacks of the explicit probability, particularly for small\nLLMs and tasks with imbalanced label distribution, underscoring the\nimportance of validating the reliability of LLM probabilistic predictions\nbefore relying on them in clinical settings.\nWe show that as LLM performance declines, the reliability of explicit\nprobabilities worsens compared to implicit ones (Fig.2, Supplementary\nFig. 2 | AUPRC difference and probability distribution on the USMLE dataset.\nAUPRC difference and probability distribution on the USMLE dataset.a AUPRC\ndifference of the large LLMs on the imbalanced USMLE dataset.b Probability\ndistribution for Meta-Llama-3.1-70B-Instruct on the USMLE dataset. The dashed\nlines are the Gaussian distributionﬁtting curves for the cases when the correct\nanswer is option A (yellow) and option B (green), respectively.\nhttps://doi.org/10.1038/s41746-024-01366-4 Article\nnpj Digital Medicine|           (2024) 7:367 5\nTable 5). This suggests that using explicit probabilities can amplify biases\ninherent in LLMs, especially when their performance is poor. In the clinical\ndomain, due to limited training data for minority languages, lack of domain\nknowledge, and dataset imbalances, LLMs often underperforms\n34.M o s t\nLLMs are trained mainly on English corpora35, while many countries’\nmedical systems use minority languages that most LLMs are not familiar\nwith. Furthermore, due to data access and privacy concerns\n34,a d v a n c e d\nLLMs often rely on general domain datasets and lack exposure to real-world\nEHR notes, which limits their effectiveness in clinical settings24,36.M e d i c a l\ndatasets also frequently suffer fromimbalances in phenotypes, like rare\ndiseases or serious clinical outcomes37, and cohort identiﬁcation7.I ns u c h\nclinical scenarios, the combination of LLMs’ poor performance and\nincreased biases hinders accurate interpretation of explicit probabilities.\nThe ﬂaw of the explicit probability underscores the need for cautious\nuse in clinical settings and highlights the importance of enhancing LLM\nexplicit probability outputs by integrating implicit probabilities, which is\none of the future works of this study. One promising approach is toﬁne-tune\nLLMs’explicit probability output with the supervision of implicit prob-\nability, thereby guiding models to generate more accurate probabilities,\nsimilar to the work of improving the implicit chain of thought capability of\nLLMs with external CoT supervision\n38. Additional approaches that improve\nthe LLMs’capability of numerical reasoning may also be utilized to enhance\nthe explicit probability output29.\nRecent work has shown that during LLM inference, there is a corre-\nlation between predictive uncertainty and hallucination— a behavior where\nLLMs generate false facts or knowledge not supported by input39.I nc l i n i c a l\nsettings, hallucinations from LLMs canlead to incorrect diagnoses, inap-\npropriate treatments, and undermine trust in their use. Improving explicit\nprobability to accurately reﬂect LLM uncertainty could serve as a key\nindicator of hallucination. A primary cause of low explicit probability is\ninsufﬁcient knowledge, which can driveLLMs to produce hallucinated\ninformation for predictions\n40,41. Thus, low explicit probability may signal a\nhigher risk of hallucination. We leavethe detection of LLM hallucinations\nwith enhanced explicit probability as future work, aiming to advance efforts\nin detecting and mitigating LLM hallucinations in healthcare.\nThere are several limitations in our study. Our experiments are sim-\npliﬁed to binary classiﬁcation settings to facilitate the extraction of implicit\nprobabilities and the calculation of AUROC and AUPRC. As a result, our\nconclusions about binary options do not carry over to other tasks such as\nmultiple-choice questions answering. Besides, we do not examine the\nprobability performance using Chain of Thought (CoT) prompting, as it is\nchallenging to extract implicit probabilities in these cases. Additionally, our\nstudy only applies to open-sourced LLMs since it is difﬁcult to obtain the\nimplicit probability of the tokens using proprietary LLMs. Finally, our\nresults are primarily based on medical datasets, the generalizability to dif-\nferent domains needs to be conﬁrmed by further validation studies.\nThis study explores the probabilistic outputs of LLMs in the medical\ndomain, an essential aspect of LLM application in healthcare. By comparing\nexplicit and implicit probabilities across multiple advanced LLMs and\nmedical datasets, we have uncovered consistent discrepancies in reliability,\nparticularly under conditions of small model size and dataset imbalance.\nThese ﬁndings underscore the need for caution when relying on explicit\nprobability in clinical settings, where the stakes are high, and the accuracy of\nprobabilistic predictions is paramount. This research informs future studies\nfocused on enhancing the trustworthiness of LLM in healthcare by\nimproving the quality and reliability of probabilistic predictions.\nMethods\nData source\nThis study is conducted onﬁve datasets: four open-access datasets—\nMeasuring Massive Multitask Language Understanding Clinical Knowledge\n(MMLU-CK)42,43, Measuring Massive Multitask Language Understanding\nCollege Medicine (MMLU-CM)42,43, United States Medical Licensing\nExamination (USMLE)44, Mainland China Medical Licensing Examination\n(MCMLE)44 — and one internal EHR dataset, Mass General Brigham -\nSocial Determinant of Health (MGB-SDoH)45. The MMLU-CK, MMLU-\nCM, USMLE, and MCMLE datasets are publicly available through\nHuggingFace, where the MMLU-CK and the MMLU-CM datasets are the\n“clinical_knowledge”(299 questions) (https://huggingface.co/datasets/cais/\nmmlu/viewer/clinical_knowledge)a n dt h e“college_medicine” (200 ques-\ntions) ( https://huggingface.co/datasets/cais/mmlu/viewer/college_\nmedicine) subsets of the MMLU dataset, respectively. For the USMLE\nand MCMLE datasets (https://huggingface.co/datasets/bigbio/med_qa), we\nrandomly select a 1000-question subset due to the computation resource\nrestriction. MGB-SDoH is a private multiple-choice dataset from real-world\nEHR notes of the MGB healthcare system. It contains the progress notes of\n200 patients and is annotated in 9 different SDoH aspects, including marital\nstatus, number of children, employment status, educational status, lifestyle\nfactors (use of tobacco, alcohol, illicit drugs, exercise), and cohabitation\nstatus\n45. Mass General Brigham (MGB) Institutional Review Board\napproved the study protocol (2020P001486). Consent to participate was not\ndeemed required for this observational investigation.\nExperiment setting\nWe select well-performing open-source LLMs released by 6 different\norganizations on the LLM leaderboard hosted by LMSYS46–54. Within each\norganization’s models, we primarily use the large models (Qwen2-72B-\nInstruct, Meta-Llama-3.1-70B-Instruct, gemma-2-27b-it, Mistral-Large-\nInstruct-2407, Yi-1.5-34B-Chat, and Phi-3-medium-128k-instruct) for our\nstudy. The corresponding small models (Qwen2-7B-Instruct, Meta-Llama-\n3.1-8B-Instruct, gemma-2-9b-it, Mistral-7B-Instruct-v0.3, Yi-1.5-9B-Chat,\nand Phi-3-mini-128k-instruct) are used to compare the differences in terms\nof model sizes on the USMLE and the MGB-SDoH datasets. All LLMs used\nin this study are publicly available. The details and sources of the LLMs are\nlisted in Supplementary Table 2.\nOur experiments are conducted on an MGB server with 8 x NVIDIA\nH100 GPUs. AutoTokenizer and AutoModelForCausalLM modules from\nHuggingFace (https://huggingface.co/docs/transformers/model_doc/auto)\nare used to load the tokenizer and LLM, respectively. During inference time,\nwe set max_new_tokens= 64 to enable fast response generation. We set\nreturn_dict_in_generate=True and output_scores=True to enable LLM\noutput transition scores, which are a list ofﬂoat numbers representing the\nlog probability of LLM output tokens. These scores are essential to calculate\nthe implicit probability. To calculate the standard deviation and the statis-\ntical signiﬁcance, we repeat the experiment three times withtempera-\nture = 0, 0.3 and 0.7 respectively. Speciﬁcally, for the standard deviation, we\nuse the np.std function from the Python numpy package (https://numpy.\norg/doc/stable/reference/generated/numpy.std.html) with ddof = 1 since we\nwant the sample standard deviation. For the statistical signiﬁcance, we\nconduct a one-sided paired t-test (whether the implicit probability is sta-\ntistically better than the explicit probability) on the explicit and the implicit\nprobability using the stats.ttest_ref function from the Python scipy package\n(https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_\nrel.html).\nPrediction and probability extraction\nTo utilize the AUROC and AUPRC as the evaluation metrics, commonly\napplied in evaluating probability predictions on binary tasks, we convert\neach question of the datasets from the multiple-choice to binary format (i.e.,\nquestion answer with two possible answer options). Speciﬁcally, for each\nquestion in these datasets, we extract its correct option and randomly select\none of the other options to build the two candidate choices. To eliminate the\nimpact of the option order to LLMs performance\n55, we randomly assign 50%\nof the correct options as Option A and the remaining 50% of the correct\noptions as Option B. We then design a prompt (Supplementary Table 1) to\ninstruct the LLMs to output their decisions, the probabilities of the decision\nbeing correct, and the corresponding explanations following a pre-deﬁned\nformat. Based on the pre-deﬁned output format, we use regular expressions\non the response text to extract LLMs’prediction and the corresponding\nexplicit probability, as illustrated in Fig. 3b. To extract the implicit\nhttps://doi.org/10.1038/s41746-024-01366-4 Article\nnpj Digital Medicine|           (2024) 7:367 6\nprobability, we extract the probability of the token that contains the model’s\nprediction (“A” or “B”). Validation scripts are implemented to ensure that\nthe extracted content aligns with the model predictions and falls within the\ntwo candidate options. We deﬁne a case as invalid if the validation fails and\nset the corresponding explicit and implicit probabilities asnp.nanso that it is\nnot used in the evaluation.\nEvaluation\nAccuracy, AUROC, and AUPRC are used as the evaluation metrics. All\nexperiments are run three times with different LLM temperatures (0, 0.3,\nand 0.7), and the average and standard deviation of these metrics are cal-\nculated and reported across the three runs. AUROC is the primary metric as\nit is the most common metric for the probability prediction. AUPRC is used\nto compare the performance difference on imbalanced data experiments.\nDistributions from explicit and implicitprobabilities are also visualized for\nbetter comparison.P-values are reported to indicate the statistically sig-\nniﬁcance of the one-sided paired t-test.\nImbalanced datasets analysis\nLabels in clinical practice often have imbalanced distributions, and the\nperformance of AI models are affected by such imbalance. To investigate the\ndifference between explicit and implicit probabilities under imbalanced\ndatasets, we reconstruct the most commonly used medical LLM evaluation\ndataset, USMLE, with different imbalanced distributions. Speciﬁcally, we set\nFig. 3 | Comparison between probabilistic predictions of AI models and study\ndesign. Comparison between probabilistic predictions of AI models and study\ndesign. a Comparison of probabilistic predictions from different types of AI models.\nThe green happy face means the model applies to the use case, while the red unhappy\nface means it doesn’t. b The conceptual framework of this study.\nhttps://doi.org/10.1038/s41746-024-01366-4 Article\nnpj Digital Medicine|           (2024) 7:367 7\na variety of ratios (5%, 10%, 30%, 50%, 70%, and 90%) of option A being the\ncorrect option. Then we randomly swap option pairs of the original dataset\nuntil the ratio of option A being correct reaches the designated values. This\noperation does not change the wording of the question or the options. It only\nchanges the order of option A and B so that the ratio of option A being\ncorrect can be adjusted. Since the metric of AUPRC is a commonly used\nmetric to evaluate model performance on datasets with imbalanced\nlabels\n56,57,w ec a l c u l a t et h eA U P R Co ft h et w om o d e l sa n dc o m p a r et h e i r\ndifferences.\nSensitivity analysis\nTo ensure the robustness of our results, we conduct a sensitivity analysis by\nusing two additional prompts (Supplementary Table 1) on the USMLE\ndataset and analyze if the relative difference between the explicit and the\nimplicit probability persists under different prompt settings. Except the\nprompt, all experiment settings are the same with the main experiment.\nData availability\nThe MMLU-CK, MMLU-CM, USMLE, and the MCMLE datasets are\npublicly available. The MGB-SDoH dataset is available from Mass General\nBrigham (MGB) but restrictions apply to the availability of these data, which\nwere used under license for the current study to protect patient privacy, and\nso are not publicly available. Data are however available from the authors\nupon reasonable request and withpermission of Mass General Brig-\nham (MGB).\nCode availability\nThe code implementation of this study is made publicly available athttps://\ngithub.com/gubowen2/Probabilistic-Medical-Predictions-of-Large-\nLanguage-Models.\nReceived: 11 September 2024; Accepted: 2 December 2024;\nReferences\n1. Sheriffdeen, K. & Daniel, S. Explainable Artiﬁcial Intelligence for\nInterpreting and Understanding Diabetes Prediction Models. Report\nNo. 2516–2314, (EasyChair, 2024).\n2. Pierce, R. L., Van Biesen, W., Van Cauwenberge, D., Decruyenaere, J.\n& Sterckx, S. Explainability in medicine in an era of AI-based clinical\ndecision support systems.Front. Genet.13, 903600 (2022).\n3. Wysocki, O. et al. Assessing the communication gap between AI\nmodels and healthcare professionals: Explainability, utility and trust in\nAI-driven clinical decision-making.Artif. Intell.316, 103839 (2023).\n4. Zeiberg, D. et al. Machine learning for patient risk stratiﬁcation for\nacute respiratory distress syndrome.PloS one14, e0214465 (2019).\n5. Cortes, C. & Vapnik, V. Support-vector networks.Mach. Learn.20,\n273–297 (1995).\n6. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. inProceedings of the\n2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers). 4171–4186.\n7. Yang, J. et al. Development and validation of a deep learning model for\ndetection of allergic reactions using safety event reports across\nhospitals. JAMA Netw. Open3, e2022836–e2022836 (2020).\n8. Crammer, K. & Globerson, A. Discriminative learning via semideﬁnite\nprobabilistic models. InProceedings of the Twenty-Second\nConference on Uncertainty in Artiﬁcial Intelligence,9 8–105 (2006).\n9. Niaf, E., Flamary, R., Rouviere, O., Lartizien, C. & Canu, S. Kernel-\nbased learning from both qualitative and quantitative labels:\napplication to prostate cancer diagnosis based on multiparametric\nMR imaging.IEEE Trans. Image Process.23, 979–991 (2013).\n10. Myers, D. et al. Foundation and large language models: fundamentals,\nchallenges, opportunities, and social impacts.Clust. Comput.27,\n1–26 (2024).\n11. Brown, T. et al. Language models are few-shot learners.Adv. neural\nInf. Process. Syst.33, 1877–1901 (2020).\n12. Jahan, I., Laskar, M. T. R., Peng, C. & Huang, J. X. A comprehensive\nevaluation of large language models on benchmark biomedical text\nprocessing tasks.Computers Biol. Med.171, 108189 (2024).\n13. Sahoo, S. S. et al. Large language models for biomedicine:\nfoundations, opportunities, challenges, and best practices.J. Am.\nMed. Info. Assoc. 31, 2114–2124 (2024).\n14. Grabb, D. The impact of prompt engineering in large language model\nperformance: a psychiatric example.J. Med. Arti\nﬁcial Intell.6,6 –20\n(2023).\n15. Zhou, W. & Ngo, T. H. Using Pretrained Large Language Model with\nPrompt Engineering to Answer Biomedical Questions. InProceedings\nof the Conference and Labs of the Evaluation Forum, 3740, 253–268\n(2024).\n16. Radford, A. et al. Language models are unsupervised multitask\nlearners. OpenAI blog1, 9 (2019).\n17. Yoon, W. et al. LCD Benchmark: Long Clinical Document Benchmark\non Mortality Prediction for Language Models.medRxiv, 2024.2003.\n2026.24304920 (2024).\n18. Haider, S. A. et al. Evaluating Large Language Model (LLM)\nPerformance on Established Breast Classiﬁcation Systems.\nDiagnostics 14, 1491 (2024).\n19. Dada, A. et al. CLUE: A Clinical Language Understanding Evaluation\nfor LLMs.arXiv preprint arXiv:2404.04067(2024).\n20. Wightman, G. P., Delucia, A. & Dredze, M. inProceedings of the 3rd\nWorkshop on Trustworthy Natural Language Processing (TrustNLP\n2023). 326–362.\n21. Ye, F. et al. Benchmarking llms via uncertainty quantiﬁcation. arXiv\npreprint arXiv:2401.12794(2024).\n22. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models.Adv. neural Inf. Process. Syst.35, 24824–24837 (2022).\n23. Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive\nnlp tasks.Adv. Neural Inf. Process. Syst.33, 9459–9474 (2020).\n24. Wu, J. et al. Large language models leverage external knowledge to\nextend clinical insight beyond language boundaries.J. American\nMedical Informatics Association, ocae079 (2024).\n25. Wang, X. et al.The Eleventh International Conference on Learning\nRepresentations.\n26. Wang, L. et al. A survey on large language model based autonomous\nagents. Front. Computer Sci.18, 186345 (2024).\n27. Xi, Z. et al. The rise and potential of large language model based\nagents: A survey.SCIENCE CHINA Information Sciences. ISSN\n1674–733X (2024).\n28. Shen, R. et al. Positional description matters for transformers\narithmetic. arXiv preprint arXiv:2311.14737(2023).\n29. Schwartz, E. et al. NumeroLogic: Number Encoding for Enhanced\nLLMs’Numerical Reasoning. InProceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing, 206–212,\nMiami, Florida, USA (Association for Computational Linguistics 2024).\n30. Cho, H., Sakai, Y., Tanaka, K., Kato, M. & Inoue, N. Understanding\nToken Probability Encoding in Output Embeddings. arXiv preprint\narXiv:2406.01468 (2024).\n31. Xu, R., Wang, Z., Fan, R.-Z. & Liu, P. Benchmarking benchmark\nleakage in large language models. arXiv preprint arXiv:2404.18824\n(2024).\n32. Koçak, B. et al. Bias in artiﬁcial intelligence for medical imaging:\nfundamentals, detection, avoidance, mitigation, challenges, ethics,\nand prospects.Diagn Interv Radiol(2024).\n33. Albahri, A. S. et al. A systematic review of trustworthy and explainable\nartiﬁcial intelligence in healthcare: Assessment of quality, bias risk,\nand data fusion.Inf. Fusion96, 156–191 (2023).\n34. Wu, J. et al. Clinical text datasets for medical artiﬁcial intelligence and\nlarge language models— a systematic review.NEJM AI1,\nAIra2400012 (2024).\nhttps://doi.org/10.1038/s41746-024-01366-4 Article\nnpj Digital Medicine|           (2024) 7:367 8\n35. Blevins, T. & Zettlemoyer, L. Language contamination explains the\ncross-lingual capabilities of English pretrained models. In\nProceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, 3563–3574, Abu Dhabi, United Arab Emirates\n(Association for Computational Linguistics 2022).\n36. Wu, J., Wu, X. & Yang, J. inProceedings of the Thirty-Third\nInternational Joint Conference on Artiﬁcial Intelligence, {IJCAI-24}(ed\nKate L.) 7491–7499 (International Joint Conferences on Artiﬁcial\nIntelligence Organization, 2024).\n37. Schubach, M., Re, M., Robinson, P. N. & Valentini, G. Imbalance-\naware machine learning for predicting rare and common disease-\nassociated non-coding variants.Sci. Rep.7, 2959 (2017).\n38. Deng, Y. et al. Implicit chain of thought reasoning via knowledge\ndistillation. arXiv preprint arXiv:2311.01460(2023).\n39. Xiao, Y. & Wang, W. Y.Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computational Linguistics:\nMain Volume.\n40. Martino, A., Iannelli, M. & Truong, C. inEuropean Semantic Web\nConference. 182–185 (Springer).\n41. Liu, X. et al. Uncovering language disparity of ChatGPT on retinal\nvascular disease classiﬁcation: cross-sectional study.J. Med.\nInternet Res.26, e51926 (2024).\n42. Hendrycks, D. et al. Measuring massive multitask language\nunderstanding. InProceedings of the International Conference on\nLearning Representations(2021).\n43. Hendrycks, D. et al. Aligning AI with shared human values. In\nProceedings of the International Conference on Learning\nRepresentations (2021).\n44. Jin, D. et al. What disease does this patient have? a large-scale open\ndomain question answering dataset from medical exams.Appl. Sci.\n11, 6421 (2021).\n45. Gu, B. et al. Scalable information extraction from free text electronic\nhealth records using large language models.medRxiv, (2024).\n46. Yang, A. et al. Qwen2 technical report. arXiv preprint\narXiv:2407.10671 (2024)\n47. Young, A. et al. Yi: Open foundation models by 01. ai.arXiv preprint\narXiv:2403.04652 (2024).\n48. Abdin, M. et al. Phi-3 technical report: A highly capable language\nmodel locally on your phone.arXiv preprint arXiv:2404.14219(2024).\n49. Chiang, W.-L. et al. Chatbot arena: An open platform for evaluating\nllms by human preference. InProceedings of the Forty-ﬁrst\nInternational Conference on Machine Learning(2024).\n50. Chiang, W.-L. et al.LMSYS Chatbot Arena Leaderboard, https://chat.\nlmsys.org/?leaderboard (2024).\n51. Mistral A. I. Team.Mistral Large 2, https://mistral.ai/news/mistral-\nlarge-2407/ (2024).\n52. Lamma Team. The Llama 3 Herd of Models.Meta Research(2024).\n53. Mistral A. I. Team.Mistral 7B, https://mistral.ai/news/announcing-\nmistral-7b/ (2023).\n54. Gemma Team, G. D. Gemma 2: Improving Open Language Models at\na Practical Size.Google DeepMind(2024).\n55. Pezeshkpour, P. & Hruschka, E. inFindings of the Association for\nComputational Linguistics: NAACL 2024. 2006–2017.\n56. Wu, X., Huang, F. & Huang, H. in2022 IEEE International Conference\non Data Mining (ICDM). 578–587 (IEEE).\n57. Qi, Q., Luo, Y., Xu, Z., Ji, S. & Yang, T. Stochastic optimization of areas\nunder precision-recall curves with provable convergence.Adv. neural\nInf. Process. Syst.34, 1752–1765 (2021).\nAcknowledgements\nThis study was funded by the National Institute of Health (R01LM013204).\nThe funders had no role in the design, collection, analysis, interpretation of\nthe data, or the decision to submit the manuscript for publication.\nAuthor contributions\nB.G. conducted the experiments and analysis and drafted the manuscript.\nR.D. provided the proprietary data of the study. J.K.L supervised the study.\nJ.Y. designed the study, drafted the manuscript, and supervised the study.\nAll authors revised, read, and approved the manuscript. B.G. takes\nresponsibility for the integrity of the work.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-024-01366-4\n.\nCorrespondenceand requests for materials should be addressed to\nJie Yang.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s41746-024-01366-4 Article\nnpj Digital Medicine|           (2024) 7:367 9",
  "topic": "Probabilistic logic",
  "concepts": [
    {
      "name": "Probabilistic logic",
      "score": 0.6404709219932556
    },
    {
      "name": "Computer science",
      "score": 0.4770711660385132
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2382310926914215
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1283280774",
      "name": "Brigham and Women's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I107606265",
      "name": "Broad Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    }
  ]
}