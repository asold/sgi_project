{
  "title": "Sequential Fusion Based Multi-Granularity Consistency for Space-Time Transformer Tracking",
  "url": "https://openalex.org/W4393147448",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5071990848",
      "name": "Kun Hu",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016603628",
      "name": "Wenjing Yang",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5020177243",
      "name": "Wanrong Huang",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5015751033",
      "name": "Xianchen Zhou",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5108853518",
      "name": "Mingyu Cao",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100445021",
      "name": "Jing Ren",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5004941473",
      "name": "Huibin Tan",
      "affiliations": [
        "National University of Defense Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2470394683",
    "https://openalex.org/W2936520980",
    "https://openalex.org/W3014030918",
    "https://openalex.org/W4361229661",
    "https://openalex.org/W2519007024",
    "https://openalex.org/W6809931125",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6792881045",
    "https://openalex.org/W6775229307",
    "https://openalex.org/W4221154951",
    "https://openalex.org/W6775473988",
    "https://openalex.org/W2557641257",
    "https://openalex.org/W2901751835",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6761855008",
    "https://openalex.org/W3083939130",
    "https://openalex.org/W2891033863",
    "https://openalex.org/W3151370571",
    "https://openalex.org/W6765254102",
    "https://openalex.org/W2968127870",
    "https://openalex.org/W4312532041",
    "https://openalex.org/W4361806794",
    "https://openalex.org/W4285600988",
    "https://openalex.org/W6757784211",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W4372266957",
    "https://openalex.org/W2898200825",
    "https://openalex.org/W6752350875",
    "https://openalex.org/W6753494528",
    "https://openalex.org/W2908174331",
    "https://openalex.org/W3217397355",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3013104201",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3150017950",
    "https://openalex.org/W6726654379",
    "https://openalex.org/W2794744029",
    "https://openalex.org/W1857884451",
    "https://openalex.org/W6848008522",
    "https://openalex.org/W2950628590",
    "https://openalex.org/W4286905821",
    "https://openalex.org/W6650716719",
    "https://openalex.org/W4280497038",
    "https://openalex.org/W3118493742",
    "https://openalex.org/W6770825554",
    "https://openalex.org/W3138486308",
    "https://openalex.org/W6761188420",
    "https://openalex.org/W6760478504",
    "https://openalex.org/W3140673084",
    "https://openalex.org/W3105204788",
    "https://openalex.org/W3208338480",
    "https://openalex.org/W3144314306",
    "https://openalex.org/W6748907736",
    "https://openalex.org/W4221152520",
    "https://openalex.org/W3046476644",
    "https://openalex.org/W6766591145",
    "https://openalex.org/W3190827869",
    "https://openalex.org/W6780018109",
    "https://openalex.org/W2605287558",
    "https://openalex.org/W2886910176",
    "https://openalex.org/W4226438989",
    "https://openalex.org/W4312751983",
    "https://openalex.org/W3035725297",
    "https://openalex.org/W4312472480",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3034896357",
    "https://openalex.org/W2963471260",
    "https://openalex.org/W3130588247",
    "https://openalex.org/W2805447362",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W4308539179",
    "https://openalex.org/W2964700958",
    "https://openalex.org/W2966759264",
    "https://openalex.org/W4313159533",
    "https://openalex.org/W4321341484",
    "https://openalex.org/W3204554907",
    "https://openalex.org/W4386065544",
    "https://openalex.org/W3012573144",
    "https://openalex.org/W3108519869",
    "https://openalex.org/W2987460522",
    "https://openalex.org/W3001584168",
    "https://openalex.org/W2910234817",
    "https://openalex.org/W3035672751",
    "https://openalex.org/W4226077544",
    "https://openalex.org/W4312805142",
    "https://openalex.org/W4386075691",
    "https://openalex.org/W3090155371",
    "https://openalex.org/W4385214091",
    "https://openalex.org/W4312735552",
    "https://openalex.org/W3204540098",
    "https://openalex.org/W2927438889",
    "https://openalex.org/W2001862790",
    "https://openalex.org/W2954137266",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W3108235634",
    "https://openalex.org/W2963863119",
    "https://openalex.org/W3167536469",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3035571898",
    "https://openalex.org/W2518876086",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W3037618862",
    "https://openalex.org/W2605381261",
    "https://openalex.org/W4287248588",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W4362604134"
  ],
  "abstract": "Regarded as a template-matching task for a long time, visual object tracking has witnessed significant progress in space-wise exploration. However, since tracking is performed on videos with substantial time-wise information, it is important to simultaneously mine the temporal contexts which have not yet been deeply explored. Previous supervised works mostly consider template reform as the breakthrough point, but they are often limited by additional computational burdens or the quality of chosen templates. To address this issue, we propose a Space-Time Consistent Transformer Tracker (STCFormer), which uses a sequential fusion framework with multi-granularity consistency constraints to learn spatiotemporal context information. We design a sequential fusion framework that recombines template and search images based on tracking results from chronological frames, fusing updated tracking states in training. To further overcome the over-reliance on the fixed template without increasing computational complexity, we design three space-time consistent constraints: Label Consistency Loss (LCL) for label-level consistency, Attention Consistency Loss (ACL) for patch-level ROI consistency, and Semantic Consistency Loss (SCL) for feature-level semantic consistency. Specifically, in ACL and SCL, the label information is used to constrain the attention and feature consistency of the target and the background, respectively, to avoid mutual interference. Extensive experiments have shown that our STCFormer outperforms many of the best-performing trackers on several popular benchmarks.",
  "full_text": "Sequential Fusion Based Multi-Granularity Consistency\nfor Space-Time Transformer Tracking\nKun Hu1*, Wenjing Yang1*, Wanrong Huang1, Xianchen Zhou2, Mingyu Cao1,\nJing Ren1, Huibin Tan1†\n1Department of Intelligent Data Science, College of Computer Science and Technology,\nNational University of Defense Technology.\n2College of Sciences, National University of Defense Technology.\nhu kun @outlook.com, {wenjing.yang, huangwanrong12, zhouxianchen13, caomy720, renjing, tanhb }@nudt.edu.cn\nAbstract\nRegarded as a template-matching task for a long time, visual\nobject tracking has witnessed significant progress in space-\nwise exploration. However, since tracking is performed on\nvideos with substantial time-wise information, it is impor-\ntant to simultaneously mine the temporal contexts which have\nnot yet been deeply explored. Previous supervised works\nmostly consider template reform as the breakthrough point,\nbut they are often limited by additional computational bur-\ndens or the quality of chosen templates. To address this issue,\nwe propose a Space-Time Consistent Transformer Tracker\n(STCFormer), which uses a sequential fusion framework with\nmulti-granularity consistency constraints to learn spatiotem-\nporal context information. We design a sequential fusion\nframework that recombines template and search images based\non tracking results from chronological frames, fusing updated\ntracking states in training. To further overcome the over-\nreliance on the fixed template without increasing computa-\ntional complexity, we design three space-time consistent con-\nstraints: Label Consistency Loss (LCL) for label-level con-\nsistency, Attention Consistency Loss (ACL) for patch-level\nROI consistency, and Semantic Consistency Loss (SCL) for\nfeature-level semantic consistency. Specifically, in ACL and\nSCL, the label information is used to constrain the atten-\ntion and feature consistency of the target and the background,\nrespectively, to avoid mutual interference. Extensive experi-\nments have shown that our STCFormer outperforms many of\nthe best-performing trackers on several popular benchmarks.\nIntroduction\nVisual Object Tracking (VOT) has attracted increasing in-\nterest due to its broad applications in various fields, such as\nmedical science (Bouget et al. 2017), unmanned aerial vehi-\ncle (Hao et al. 2018), and self-driving cars (Gao et al. 2020)\nwhich is a hot topic nowadays. One popular category of VOT\nis Single Object Tracking (SOT) and its goal can be de-\nscribed as follows: Given a target specified in the first frame,\nwe need to estimate the states (including the location and the\nscale) of this target in subsequent frames (Soleimanitaleb\n*These authors contributed equally.\n†Corresponding Author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nand Keyvanrad 2022). Notwithstanding the booming of di-\nverse tracking algorithms (Danelljan et al. 2018; V oigtlaen-\nder et al. 2019; Yan et al. 2021) in recent years, many chal-\nlenges like appearance variation and occlusion still hinder us\nfrom accurate real-time tracking in complex real-world sce-\nnarios. Recapping current prevailing trackers, most of them\ncast this task as template-matching frame by frame. Specifi-\ncally, it takes a target image as a template, and then compares\nit with other frames to find the location and the size of the\ntarget. The popular Siamese family (Bertinetto et al. 2016;\nChen et al. 2020) is a typical representative of them. And\nthe recently emerging Transformer-based trackers (Lin et al.\n2022; Cui et al. 2022) realize template matching through full\ninteraction between templates and search features.\nUnder the influence of this paradigm, a significant ad-\nvancement in learning spatial representations has been\nmade. But few have considered temporal information. Some\nhandle this problem by adding an online updating mecha-\nnism into the inference stage (Zhang and Peng 2020; Yan\net al. 2021), but this is trapped by template selection and\nupdate settings. Those who add complicated updating net-\nwork (Bhat et al. 2019; Zhang et al. 2019) or memory net-\nwork (Fu et al. 2021; Yang and Chan 2018) are plagued by\nmassive computational complexity and bring in extra hyper-\nparameters to tune. Besides, the common template fusion\nor similar operations also ignore the order of frames, which\ncarries rich temporal contexts.\nTo alleviate these problems, we propose a Space-Time\nConsistent Transformer Tracker (STCFormer)to glean\ntemporal information which is commonly overlooked by ex-\nisting works. Specifically, we model the space-time rela-\ntionship with multi-granularity cycle consistency constraints\nembedded in a sequential fusion framework, i.e. Label Con-\nsistency Loss (LCL), Attention Consistency Loss (ACL),\nand Semantic Consistency Loss (SCL). During training, we\nuse a sequential fusion framework to construct a cycle that\nincludes a template image and several chronological search\nimages. We continuously update the template image based\non previous prediction results during chronological frame\ntracking and use the final template for backward tracking\nwith the first search frame again, forming a closed loop. To\nincrease the robustness of our framework, we incorporate\nthree consistency constraints to avoid over-dependence on\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12519\nfixed templates. Intuitively, We expect the tracking results\nobtained from the same search image to be consistent, even\nif the template changes. For the first search image, we ap-\nply consistency constraints to its three levels of representa-\ntion: label level with LCL to maximize the overlap of the\ntwo tracking results in the same search frame, patch level\nwith ACL to ensure consistent attention weights of search\npatches based on template matching, and feature level with\nSCL to converge the semantics of filtered search features.\nAbove strategies collaborate with one another to help the\nmodel learn more robust space-time features of the target. In\nparticular, to strengthen ACL and SCL, we leverage label in-\nformation to compute the consistency of patch attention and\nsearch features separating regions within the groundtruth\nbox and the others, which prevents semantic confusion.\nTo sum up, our main contributions are:\n• In the training process, we build a sequential fusion\nframework that forms a cycle by updating the tem-\nplate image based on previous prediction results during\nchronological frame tracking. With few hyperparame-\nters, our method can be implanted into many existing\nTransformer-based trackers.\n• To further mine the spatiotemporal information, we apply\nthree consistency constraints of different granularities:\nLabel Consistency Loss (LCL), Attention Consistency\nLoss (ACL), and Semantic Consistency Loss (SCL).\n• We form a Space-Time Consistent Transformer Tracker\n(STCFormer) based on above strategies. Experiments on\nseveral widely-used benchmarks have proven its out-\nstanding performance and verified the efficiency and ef-\nficacy of LCL, ACL and SCL.\nRelated Work\nTransformer-based Trackers\nAccording to the type of the framework, the SOT al-\ngorithms using Transformer can be classified as CNN-\nTransformer based trackers and fully-Transformer based\ntrackers (Thangavel et al. 2023).\nCNN-Transformer based trackersuse hybrid architec-\nture combining Convolutional Neural Network (CNN) and\nTransformer. And they adopt the Siamese-like pipeline as\nthe majority of CNN-based trackers(Hu et al. 2023; Tan et al.\n2021). Usually, the CNN is used for feature extraction and\nits outputs are fed into Transformer to catch the similar-\nity between the template and the search region. Wang et al.\n(2021a) divides the encoder and decoder of the Transformer\ninto two branches to build a Siamese-like tracker. Based on\nthe DETR (Carion et al. 2020), Yan et al. (2021) develops an\nend-to-end framework to capture the global feature depen-\ndencies of both temporal and spatial information. Consider-\ning the neglect of the informative object-level information\nfrom those relative pixel positions, CSWinTT (Song et al.\n2022) proposes multi-scale cyclic shifting window attention\nas a solution. Another observation about the correlation of\nquery and key gives birth to AiATrack (Gao et al. 2022). It\nadds an inner attention to the Transformer structure.\nFully-Transformer based trackersis proposed to get rid\nof the reliance on CNN and they can catch global feature\nrepresentations. DualTFR (Xie et al. 2021) is the trailblazer\nof this category. It wields a set of local attention blocks\nand a global attention block to seize both local and long-\nrange dependencies. SwinTrack (Lin et al. 2022) extracts\nfeatures by means of Swin Transformer (Liu et al. 2021) and\nreaches state-of-the-art performance at that time. All these\nare two-stream two-stage trackers because their backbone\nis a pre-trained Transformer while another Transformer is\nused for feature fusion and feature enhancement. Another\ntype of fully-Transformer based tracker is the one-stream\none-stage tracker. They utilize one Transformer to finish fea-\nture extraction and feature fusion, such as MixFormer (Cui\net al. 2022). It comprises a set of Mixed Attention Modules\nfor feature extraction and target information fusion but runs\nvery slowly. Then Ye et al. (2022) devise OSTrack with ViT\nas the backbone. It comes up with an early candidate elimi-\nnation strategy to delete background tokens by degrees. This\nbenefits the accuracy and the speed at the same time.\nSpatiotemporal Information Mining in Tracking\nAn influx of tracking algorithms has studied the spatial in-\nformation over the past years. In a nutshell, they just con-\nsider tracking as a simple template-matching task in each\nframe. Nonetheless, a critical difference lies between im-\nage processing task (Carion et al. 2020) and video-based\ntracking (Lan et al. 2018) is the time-wise context. There-\nupon some researchers probe into this issue. Methods based\non optical flow (Senst, Eiselein, and Sikora 2012; Liu et al.\n2020) achieve tracking an object by extracting the feature\npoints and estimating their matching points in the next\nframe. Gao, Zhang, and Xu (2019) propose a spatiotem-\nporal GCN to learn the structured representation of histor-\nical templates. TCTrack (Cao et al. 2022) explores adaptive\ntemporal information at two levels, i.e., the feature extrac-\ntion and the similarity maps refinement. An extra network\nto process several search images is also a common strategy.\nZhang et al. (2019) equips SiamFC (Bertinetto et al. 2016)\nand DSiamRPN (Zhu et al. 2018) with a UpdateNet to learn\nthe best template for the next prediction. Based on LSTM\n(Hochreiter and Schmidhuber 1997), MemTrack (Yang and\nChan 2018) proposes a dynamic memory network to im-\nprove the model’s adaptiveness to the target’s appearance\nchange. Though these do help in capturing some spatiotem-\nporal relations between frames, still they are dependent on\nsophisticated structures and this inevitably leads to higher\ncomputational complexity. STARK (Yan et al. 2021) selects\na simple and gradient-free mechanism of concatenating the\ninitial template with a new one. But it is applied in the test\nstage only and it might cost almost double computational re-\nsources if applied in training. Those who incrementally up-\ndate the model (Nam and Han 2015; Danelljan et al. 2016)\nare with the same shortage.\nConsistency Loss\nAn interesting strategy of space-time information mining is\ncycle consistency loss. CycleGAN (Zhu et al. 2017) pio-\nneered the method of cycle consistency to tackle unpaired\nimage-to-image translation within two domains. Wu, Wang,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12520\n…\n…\nToken padding&Reshape&Head\n…\n…\nFeature Map 0\nCrop\nResize\nT\n1\nT\n0\nS\n0\nS\n1\nT\n2\nS\n2\n…\n…\nToken padding&Reshape&Head\n…\n…\nFeature Map 3\nCrop\nResize\nSCL\nLCL\nN\n×\nEncoder Layer\nwith\nEarly Elimination Module\nACL\nForward process 2\nForward process 3\nAttention\nMap 0\nAttention\nMap 3\nLinear Projection\nLinear Projection\nT\n3\nForward process 4\nForward process 1\nN\n×\nEncoder Layer\nwith\nEarly Elimination Module\nForward\nprocess 5\nForward\nprocess 6\nSCL\nACL\nSCL\nACL\nT\n0\nSIF\nGround\nTruth\nSIF\nSIF\nCrop\nResize\nB\n2\nB\n0\nB\n1\nb\n0\nb\n1\nb\n2\nb\n3\nFigure 1: The overall pipeline of STCFormer. We conduct\nthe customary forward process six times in one coherent for-\nward circuit of STCFormer.\nand Shao (2018) work on a similar idea for cross-modal re-\ntrieval. Wang, Jabri, and Efros (2019) and Wang et al. (2019)\nare the first to introduce this concept into object tracking. In\nspite of different implementations, both of them use this as\na free supervisory signal to train unsupervised object track-\ners. And they both obtain impressive results. The succes-\nsor (Yuan, Wang, and Chen 2020) simply expands the cy-\ncle consistency loss proposed by Wang et al. (2019) from\nDCFNet (Wang et al. 2017) into the self-supervised Siamese\ntracker. Dwibedi et al. (2019) propose a self-supervised\nalgorithm to learn visual correspondence from unlabeled\nvideo. Analogously, Jabri, Owens, and Efros (2020) lever-\nage cycle-consistency to align two similar videos in a self-\nsupervised way. Ristea et al. (2023) introduce a similar idea\ninto medical image processing and apply cycle consistency\nloss to both feature and computed tomography (CT) images.\nCCuantuMM (Bhatia et al. 2023) apply such idea into jointly\nmatching multiple non-rigidly deformed 3D shapes. Above\nsuccessful methods without supervisory signals motivate us\nto introduce such a powerful constraint into the supervised\nlearning method.\nMethod\nIn this section, we present our Space-Time Consistent Trans-\nformer Tracker (STCFormer) in detail. We introduce the\nsequential fusion-based framework of STCFormer at first.\nThen we present our multi-granularity consistency con-\nstraints, i.e. Label Consistency Loss (LCL), Attention Con-\nsistency Loss (ACL) and Semantic Consistency Loss (SCL).\nFramework of STCFormer\nAs shown in Fig. 1, the pipeline of our STCFormer is a co-\nherent circuit rather than a common unidirectional structure.\nDefine the routine of computing a predicted box with a tem-\nplate image and a search image as aforward processand de-\nnote it asΦθ (θ represents the model’s parameters). Then one\nforward circuit of STCFormer is comprised of six forward\nprocesses. The input template image of each forward process\n(denoted as T1, T2, T3) is cropped from the search image of\nits last forward process according to the last predicted box,\nexcept for the first forward process which crops a randomly\nsampled frame (denoted by Ftem) into T0 as template im-\nage. And T0 serves both forward process 5 and forward pro-\ncess 6. For the first three forward processes, the search im-\nages S0, S1, S2 are respectively cropped from three frames\nF0, F1, F2 that are sampled in chronological order. And the\nfourth forward process shares the same search imageS0 with\nthe first forward process. The fifth forward process and the\nsixth forward process take S1 and S2 as input, respectively.\nThe frames sampling can be described as:\nFtem, F0, F1, F2 = S(D, Nt, Ns) , (1)\nwhere S(·) refers to the sampling process; D is the training\ndatasets; Nt, Ns represent the number of the template image\nand search images, respectively.\nAforementioned data processing procedure (leaving out\ndata augmentations such as brightness jittering for brevity)\ncan be written as:\n\u001aT0 = CJ (Ftem, Btem, Ktem) ,\n[S0, S1, S2] = CJ ([F0, F1, F2], [B0, B1, B2], Ksearch) ,\n(2)\nwhere B represents the ground-truth bounding box of the\ncorresponding frame, which is assigned by its subscript;\n[F0, F1, F2] represents the concatenation of three tensors;\nKtem and Ksearch denote the size of the template image and\nthe size of the search image, respectively.CJ(·) is the jitter-\ning crop operation, which is used to avoid learning the bias\nthat the target is always at the center of the search region.\nOn the basis of this circular framework, we establish Se-\nquential Information Fusion (SIF) on results of the first\nthree forward processes. Specifically, we impose a super-\nvised constraint on predicted results of the first three for-\nward processes and corresponding ground-truth labels. For\neach of the distance we utilize l1 loss and the generalized\nIoU loss (Rezatofighi et al. 2019) for bounding box regres-\nsion and the weighted focal loss (Law and Deng 2018) for\nclassification. Then the SIF is defined as:\nLSIF = 1\nn\nnX\ni=1\n\u0000\nLi\ncls + λiou Li\niou + λL1 Li\nL1\n\u0001\n, (3)\nwhere n is the number of the search images and we setn = 3\nin our experiments; λiou , λL1 are the regularization terms\nand are set to 2, 5, respectively.\nBased on this architecture, we design multi-granularity\nconsistency loss LCL, SCL and ACL to narrow the gap be-\ntween the outputs of those coupled forward processes with\nsame search input but different template input, i.e., forward\nprocess 1 vs forward process 4, forward process 2 vs for-\nward process 5, and forward process 3 vs forward process\n6. Details of these space-time constraints will be depicted in\nfollowing parts.\nThe differences between STCFormer and existing models\ncan be summarized as:\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12521\n…\n…\nb\n2\nb\n1\nb\n3\nFigure 2: A defect of LCL.\n• A classical space-wise tracker usually takes one template\nimage and one search image as input. After one complete\nforward process, it outputs the state of the target in the\nsearch image. Distinct from it, the input of STCFormer\nis one template image and a series of search images. One\ncoherent forward circuit of STCFormer contains six for-\nward processes.\n• Few space-time trackers have thought over the sequential\ninformation hidden in time flow, yet we construct a co-\nherent forward circuit to process three search images in\ntime order.\n• Most space-time trackers directly fuse features of several\ntemplate images or employing an elaborate network to\ncreate a new template based on them. Unlike these, we\nfocus on the space-time consistency and design multi-\ngranularity consistency loss (LCL, SCL and ACL) based\non Sequential Information Fusion (SIF).\nLabel Consistency Loss (LCL)\nThe over-dependence of one fixed template prejudices the\nmodel’s adaptiveness to the target’s variation. Whereupon\nwe update the template image stepwise. However, on ac-\ncount of most models’ vulnerability to the template, arbitrar-\nily changing the template image may cause opposite effects.\nTo address this issue, we introduce Label Consistency Loss\n(LCL). The basic idea of it is to track forward along the time\ndirection, followed by a backward tracking to form a closed\nloop. Its goal is to minimize the difference between the start\nand the end box coordinates. With one input template image\nT0 and three search images sampled in chronological order\nS0, S1, S2, this circular process can be formulated as below:\nFirstly, we conduct the first forward process onT0 and S0\nand obtain a predicted box b0. Then we crop S0 with b0 as\nthe label and resize it into a new templateT1. Afterward, we\nperform the second forward process withS1 and T1 as input.\nThe third forward process is done in the same manner.\n\u001abi = Φθ (Si, Ti) ,\nTi+1 = CC (Si, bi, Ktem) , i = 0, 1, 2, (4)\nwhere CC(·) means cropping an image with the given box\nregion placed in the center of the cropped image.\nFinally in the fourth forward process, T3 (obtained by\ncropping S2 and resizing) is combined with S0 as the input\nand we can acquire a predicted box b3 for S0 in the end:\nb3 = Φθ (S0, T3) . (5)\nThen our Label Consistency Loss (LCL) is defined as:\nLcycle (B0, b3) = Lcls (B0, b3) + λiou Liou (B0, b3)\n+λL1 LL1 (B0, b3) . (6)\nIn Multi\n-\nHead Attention\nCenter token\nof template\nEarly Elimination\nModule\nGuide\n…\nAttention\nMap\nFigure 3: Details about the attention map of ACL.\nSince LCL only works on the first labelB0 and the fourth\npredicted box b3, a defect of it is that it may predict a precise\nbox b3 for S0 while the interim box such as b2 are not accu-\nrate, as Fig. 2 shows. Fortunately, we alleviate this problem\nwith aforementioned SIF as it set constraints on each pre-\ndicted box of the first three forward processes.\nAttention Consistency Loss (ACL)\nTo further improve the robustness of tracking with dynamic\ntemplate images, an intuitive idea is to train a model keeping\nits attention on where it should focus. In other words, even\nwith different templates, the attention maps for the same\nsearch image should be consistent. That’s exactly the key\npoint of our Attention Consistency Loss (ACL). To avoid\nincreasing the computational burden, we make full use of\nthe calculation results of multi-head attention. In each en-\ncoder layer, multi-head attention calculates the correlation\nbetween input tokens. Following Ye et al. (2022), we take\nthe similarity between the center token of the template im-\nage with all search tokens as the attention map. So the gen-\neration of attention map Ai for forward progress (i + 1)can\nbe described as:\nAi =\n\n\n\nA(P(Ti), P(Si), Ω) , i = 0, 1, 2,\nA(P(Ti), P(S0), Ω) , i = 3,\nA(P(T0), P(Si−3), Ω) , i = 4, 5,\n(7)\nwhere A(·) is the multi-head attention operation; P(·) rep-\nresents the process involving three steps: (1) Split and flat-\nten the image into sequences of patches; (2) Operate linear\nprojection to transform patches into token embeddings; (3)\nConcatenate the template tokens and the search tokens as\none input for cascaded Transformer encoder layers (Doso-\nvitskiy et al. 2021). Ω is a generated mask for the template\nto retain the tokens of its center part.\nTo take full advantage of supervised signals that contains\nrich information, we divided search tokens into two parts:\nthose that located within groundtruth box (including those\nthat cross the boundary line of groundtruth box) are denoted\nas positive tokens while others are negative tokens. When\nwe compute the distance between two attention mapsAi and\nAj we perform normalization on them separately (denoted\nas Γ(·)). Consequently, our ACL can be formulated as:\nLattention = 1\n3\nX\n(Γ (|Ai − Ai+3|)), i= 0, 1, 2, (8)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12522\nBackbone with\nEarly Elimination Module\nBackbone with\nEarly Elimination Module\nM\n0\nM\n3\nReshape&Head\nToken padding\nReshape&Head\nToken padding\nTo minimize the\ndifference between\ntwo Feature Maps\nSCL\nFigure 4: A schematic diagram of SCL.\nwhere | · |denotes element-wise absolute operator.\nSemantic Consistency Loss (SCL)\nSince all the Transformer-based trackers rely on the data\nprocessing of divided input images into patches, we con-\nsider to grasp more fine-grained information which might\nhave been omitted by them. Now with consistent label-wise\ninformation and patch-level attention weights, we expect the\nmodel to catch consistent semantics. More specifically, even\nif the template image changes (the target remains the same),\nthe model is able to learn consistent semantic features for the\nsame search image. This is reasonable as people are never re-\nstricted by one specific template during tracking. Instead, we\ncapture the semantic information of the target to keep track-\ning regardless of appearance variation. This motivates us to\nlearn robust semantic features for tracking.Since Ye et al.\n(2022) deletes several background tokens of the search im-\nage, the output tokens of the backbone are mostly belong to\nthe target feature. Therefore we group these filtered tokens\nas a feature map and enforce the feature maps for the same\nsearch image as closely as possible while they are produced\nwith different template images. Similar to ACL, we split the\nfeature tokens into positive tokens and negative tokens and\nnormalize them separately. In practice, we use MSE loss (de-\nnote as E(·) ) on those normalized feature pairs:\nLsemantic = 1\n3\nX\n(E (Γ(Mi), Γ(Mi+3))), i= 0, 1, 2, (9)\nIn summary, the total loss of STCFormer is:\nLTotal = LSIF + λLCLLcycle + λACLLattention + λSCLLsemantic,\n(10)\nwhere the regularization terms λLCL , λACL and λSCL are set\nto 1, 0.005, 0.1, respectively.\nExperiments\nThis section introduces the implementation details at the be-\nginning. Then we display the results of comparison with pre-\ndominant algorithms. In the final part, we perform an abla-\ntion study to judge the contribution of each constraint and\nanalysis our model from different perspectives.\nImplementation\nWe implement STCFormer using Python 3.8 and PyTorch\n1.9. It is trained on a server with 8 NVIDIA A100 GPUs.\nThe inference speed is tested with only one NVIDIA\nRTX2080Ti GPU. Similar to OSTrack (Ye et al. 2022), the\ndata augmentations such as horizontal flip and grayscale\nconversion are used in the training process.\nTraining. The batch size of each GPU is 28 and we train\nthe model with AdamW optimizer (Loshchilov and Hutter\n2017). The weight decay is 10−4. The initial learning rate is\n3×10−6 for the backbone and3×10−5for other parameters.\nWe set total training epochs to 300 with 60k image pairs\nper epoch and the learning rate decreases by a factor of 10\nafter 240 epochs. The whole network is initialized with the\ntraining weights of OSTrack-384 (the search image is384×\n384 pixels and the template image is 192 × 192 pixels) (Ye\net al. 2022) and MAE (He et al. 2021).\nDatasets. The model is trained with following datasets:\nCOCO (Lin et al. 2014), LaSOT (Fan et al. 2018), GOT-10k\n(Huang, Zhao, and Huang 2018) and TrackingNet(M ¨uller\net al. 2018). And the benchmarks we used for test are GOT-\n10k (Huang, Zhao, and Huang 2018), LaSOT (Fan et al.\n2018), LaSOText (Fan et al. 2020), TNL2K (Wang et al.\n2021b) and UA V123 (Mueller, Smith, and Ghanem 2016).\nComparison with State-of-the-arts\nWe contrast STCFormer with 23 state-of-the-art ap-\nproaches including SiamFC (Bertinetto et al. 2016), MD-\nNet (Nam and Han 2015), ECO (Danelljan et al. 2016),\nSiamPRN++ (Li et al. 2018), DiMP (Bhat et al. 2019),\nATOM (Danelljan et al. 2018), SiamR-CNN (V oigtlaender\net al. 2019), LTMU (Dai et al. 2020), Ocean (Zhang and\nPeng 2020), KYS (Bhat et al. 2020), STMTrack (Fu et al.\n2021), TrDiMP (Wang et al. 2021a), TransT (Chen et al.\n2021), AutoMatch (Zhang et al. 2021), STARK (Yan et al.\n2021), KeepTrack (Mayer et al. 2021), TransInMo (Guo\net al. 2022), MixFormer (Cui et al. 2022), AiATrack (Gao\net al. 2022), CIA (Pi et al. 2022), SwinTrack-B (Lin et al.\n2022), GRM (Gao, Zhou, and Zhang 2023), and the baseline\nOSTrack-384 (Ye et al. 2022). Results are shown in Tab. 1.\nLaSOT. Designed for long-term tracking, LaSOT is one\nof the most challenging large-scale benchmarks with 280\ndensely annotated testing videos. Compared with other pow-\nerful trackers, our STCFormer performs 0.4% higher than\nour baseline OSTrack-384 in AUC. And our scores in other\ntwo metrics (PNorm and P) also exceed other trackers.\nLaSOText. As an extended version of LaSOT, LaSOText\ncontains 150 additional sequences of 15 object classes. Since\nit is released in recent two years, results on it are relatively\nfewer but we still set a new state-of-the-art on it with an\nAUC of 57.7% outperforming the baseline OSTrack-384 by\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12523\nMethod Source LaSOT LaSOTe\nxt TNL2K GOT\n-10k UA\nV123\nAUC\nPNorm P AUC\nPNorm P AUC\nP AO\nSR0.5 SR0.75 AUC\nSiamFC ECCVW16 33.6 42.0\n33.9 23.0 31.1\n26.9 29.5 28.6 34.8 35.3\n9.8 46.8\nMDNet CVPR16 39.7 46.0\n37.3 27.9 34.9\n31.8 31.0 32.2 29.9 30.3\n9.9 -\nECO ICCV17 32.4 33.8\n30.1 22.0 25.2\n24.0 32.6 31.7 31.6 30.9\n11.1 53.7\nSiamPRN++ CVPR19 49.6 56.9\n49.1 34.0 41.6\n39.6 41.3 41.2 51.7 61.6\n32.5 61.3\nDiMP ICCV19 56.9 65.0\n56.7 39.2 47.6\n45.1 44.7 43.4 61.1 71.7\n49.2 65.4\nAT\nOM CVPR19 51.5 57.6\n- - -\n- 40.1 39.2 - -\n- 65.0\nSiamR-CNN CVPR20 64.8 72.2\n- - -\n- - - 64.9 72.8\n59.7 64.9\nLTMU CVPR20 57.2 -\n57.2 41.4 49.9\n47.3 - - - -\n- -\nOcean ECCV20 56.0 65.1\n56.6 - -\n- 38.4 37.7 61.1 72.1\n47.3 -\nKYS ECCV20 - -\n- - -\n- 44.9 43.5 63.6 75.1\n51.5 -\nSTMTrack CVPR21 60.6 69.3\n63.3 - -\n- - - 64.2 73.7\n57.5 64.7\nTrDiMP CVPR21 63.9 -\n61.4 - -\n- - - 67.1 77.7\n58.3 67.5\nTransT CVPR21 64.9 73.8\n69.0 - -\n- - - 67.1 76.8\n60.9 69.1\nAutoMatch ICCV21 58.3 -\n59.9 - -\n- 47.2 43.5 65.2 76.6\n54.3 -\nSTARK ICCV21 67.1 77.0\n- - -\n- - - 68.8 78.1\n64.1 -\nKeepT\nrack ICCV21 67.1 77.2\n70.2 48.2 -\n- - - - -\n- 69.7\nTransInMo CVPR22 65.7 76.0\n70.7 - -\n- 52.0 52.7 - -\n- 69.0\nMixFormer CVPR22 70.1 79.9\n76.3 - -\n- - - 71.2 80.0\n67.8 70.4\nAiAT\nrack ECCV22 69.0 79.4\n73.8 - -\n- - - 69.6 80.0\n63.2 70.6\nCIA ECCV22 67.6 -\n71.5 - -\n- 50.9 - 67.9 79.0\n60.3 68.9\nSwinTrack-B NeurIPS22 71.3 -\n76.5 49.1 -\n55.6 55.9 57.1 72.4 80.5\n67.8 -\nGRM CVPR23 69.9 79.3\n75.8 - -\n- - - 73.4 82.9\n70.4 70.2\nOSTrack-384 ECCV22 71.1 81.1\n77.6 50.5 61.3\n57.6 55.9 - 73.7 83.2\n70.8 70.7\nSTCFormer Ours 71.5 81.5\n78.0 52.0 63.0\n59.6 57.7 59.0 74.3∗ 84.2∗ 72.6∗ 70.8\nTable 1: Comparison with state-of-the-arts on five popular benchmarks: LaSOT, LaSOT ext, TNL2K, GOT-10k and UA V123.\nThe best results are shown in bold font. ∗ means the figures are obtained following one-shot protocol.\n1.5%. And our precision score and normalized precision\nscore achieve improvements of 2% and 1.7%, respectively.\nTNL2K. TNL2K is a new dataset for natural language\nguided tracking. To improve the generality of tracking eval-\nuation it introduces several adversarial samples and thermal\nimages which makes it more challenging. And we boost the\nperformance in each metric. In AUC we surpass OSTrack-\n384 by 1.8% and outperform other powerful counterparts\nlike SwinTrack-B and CIA by a substantial margin.\nGOT-10K. GOT-10K is a widely-used large-scale bench-\nmark that covers various common challenges in tracking. Its\ntest set employs a one-shot tracking rule, which means that\nthe trackers should only be trained on the GOT-10k training\nsplit, then object classes between train and test splits will not\nbe overlapped. We follow this protocol and obtain improve-\nments in AO, SR 0.5, SR0.75 of 0.6%, 1.0%, 1.8%, respec-\ntively. We also perform favorably against the GRM (Gao,\nZhou, and Zhang 2023), which is proposed in this year.\nUA V123. UA V123 contains 123 videos, with an average\nof 915 frames per video. All frames of it are collected from\nlow altitude aerial-views. On UA V123, we still have signif-\nicant advantages over other trackers and show a slight in-\ncrease of 0.1% in AUC.\nAblation Study and Analysis\nGains of each consistency loss.To judge the exact impact\nof LCL, ACL and SCL, we report the AUC scores of the\nmodels equipped with different constraints on three bench-\nmarks. Results displayed in Tab. 2 prove that each con-\nstraint has made a contribution to the improvement of perfor-\nmance. More specifically, LCL benefits more on LaSOT and\nTNL2K, and ACL does well on LaSOT ext. We guess there\nmay be two reasons. (1) LCL sets constraints on both regres-\nsion and classification, which deepen the space-time context\nlearning. (2) feature-wise SCL pays attention to pixel-wise\ntexture while patch-wise ACL work on higher-level infor-\nmation, which reveals global information of target features.\nUtilization of supervised labels.In our three consistency\nlosses, we fully utilize the label information from different\nperspectives. Actually, in the initial version, we conduct loss\non the first predicted box b0 and the backward predicted\nbox b3 in LCL, but in a bunch of experiments we found it\nunstable and the improvement is minimal. Then we turn to\nperform loss on the first ground-truth bounding box B0 and\nthe backward predicted box b3, which leads to a noticeable\ngrowth on several benchmarks. Meanwhile, with SIF mak-\ning B0 and b0 as close as possible, it is equivalent to clos-\ning the gap between b0 and b3. Based on SIF, we found the\ntraining process of new LCL becomes much more stable. In-\nspired by that, we introduce the label information into ACL\nand SCL to perform normalization.\nEffect of weights on each loss.We also try different\nweights λ for each loss we design. The range of testing\nweights of each loss is approximately determined by the\nscale of their values. According to Tab. 3, LCL is unsus-\nceptible to the hyperparameters (their training processes are\nalmost the same), while the other two are not. Results show\nthat 0.1 and 0.005 are best for SCL and ACL, respectively.\nSpeed and size.As Tab. 4 shows, in contrast to a selection\nof recent Transformer-based trackers, STCFormer achieves\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12524\nOSTrack-384 LCL ACL SCL LaSOT LaSOT ext TNL2K\n✓ 71.1 50.5 55.9\n✓ ✓ 71.4 51.0 57.6\n✓ ✓ 71.2 51.7 57.5\n✓ ✓ 71.3 50.9 57.5\n✓ ✓ ✓ ✓ 71.5 52.0 57.7\nTable 2: Quantitative comparison results of our tracker and its variants equipped with different loss functions.\nLoss LCL ACL SCL\nWeight 1 0.1\n0.01 0.001 0.005\n0.01 0.3 0.2\n0.1\nAUC 52.0 52.0\n52.0 51.2 51.7 51 50.8 50.6 50.9\nTable 3: Effect of different weights for LCL, ACL, SCL on LaSOText. The best results are shown in bold font.\nTrack\ner Speed(fps) MA\nCs(G) Params (M)\nTrDiMP 26 - -\nT\nransT 50 - -\nST\nARK-ST101 32 18.5\n42\nSwinTrack-B-384 45 69.7\n91\nOSTrack-384 55.2 48.4\n92\nSTCFormer\n(ours) 55.5 48.4\n92\nTable 4: Comparison of our inference speed and parameters\nwith other representative Transformer-based trackers.\ncompelling computational and data efficiency. Our space-\ntime consistency exploration almost cause no extra burden in\nmodel size. After code optimization, STCFormer even runs\nfaster than the baseline OSTrack-384 in same environment.\nVisualization. Fig. 5 exhibits some examples of our real-\ntime tracking. The picture shows that STCFormer can han-\ndle many common challenges of tracking. It performs well\nunder the circumstances of background clutters (row 1 and\nrow 4). It can track accurately regardless of partial occlusion\n(with small or large occluded area) (row 2, row 7 and row 8).\nIt is able to deal with deformation of the target (row 3) and\ncatch small target (row 6). It can also distinguish the target\nfrom objects that look similar like it (row 5).\nConclusion\nIn this paper, we propose a novel Space-Time Consistent\nTransformer Tracker (STCFormer) based on a sequential in-\nformation fusion framework. Multi-granularity consistency\nconstraints i.e. Label Consistency Loss (LCL), Attention\nConsistency Loss (ACL) and Semantic Consistency Loss\n(SCL) are added to enhance the spatiotemporal consistency\nfrom label-level, patch-level and feature-level, respectively.\nQuantitative and qualitative analysis in experiments con-\nfirms the positive effect of our approach. In fact, our method\nis compatible with a wide array of contemporary trackers or\neven models for other visual tasks to further promote their\nperformance by infusing more space-time information.\nTemplate\n& Search\nSearch\nafter CE\nPredicted Box\nin Video Frames\nAttention\nMap\nFeature\nMap\nFigure 5: Visualization of the tracking process. The first col-\numn shows the search images (the big ones) and the tem-\nplate images (small ones in the upper left corner). The sec-\nond column presents the search images after the early candi-\ndate elimination (CE) process of OSTrack. The third col-\numn shows the tracking results on corresponding frames.\nThe green rectangles are groundtruth boxes and the purple\nrectangles are our predicted boxes. The fourth column shows\nattention map for corresponding search image and the fifth\ncolumn is the filtered features after CE.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12525\nAcknowledgments\nThis work is supported by the National Natural Science\nFoundation of China (91948303-1 & 62106278 ) and the In-\ndependent and open subject fund (grant no.202201-06) from\nState Key Laboratory of High Performance Computing.\nReferences\nBertinetto, L.; Valmadre, J.; Henriques, J. F.; Vedaldi, A.;\nand Torr, P. H. S. 2016. Fully-convolutional Siamese net-\nworks for object tracking. In ECCVW, 850–865.\nBhat, G.; Danelljan, M.; Gool, L. V .; and Timofte, R. 2019.\nLearning discriminative model prediction for tracking. In\nICCV, 6181–6190.\nBhat, G.; Danelljan, M.; Gool, L. V .; and Timofte, R. 2020.\nKnow your surroundings: exploiting scene information for\nobject tracking. In ECCV, 205–221.\nBhatia, H.; Tretschk, E.; L ¨ahner, Z.; Benkner, M.; M ¨oller,\nM.; Theobalt, C.; and Golyanik, V . 2023. CCuantuMM:\nCycle-Consistent Quantum-Hybrid Matching of Multiple\nShapes. In CVPR, 1296–1305.\nBouget, D.; Allan, M.; Stoyanov, D.; and Jannin, P. 2017.\nVision-based and marker-less surgical tool detection and\ntracking: a review of the literature. Medical Image Analy-\nsis, 35: 633–654.\nCao, Z.; Huang, Z.; Pan, L.; Zhang, S.; Liu, Z.; and Fu, C.\n2022. TCTrack: temporal contexts for aerial tracking. In\nCVPR, 14778–14788.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213–229.\nChen, X.; Yan, B.; Zhu, J.; Wang, D.; Yang, X.; and Lu, H.\n2021. Transformer tracking. In CVPR, 8122–8131.\nChen, Z.; Zhong, B.; Li, G.; Zhang, S.; and Ji, R. 2020.\nSiamese box adaptive network for visual tracking. InCVPR,\n6667–6676.\nCui, Y .; Cheng, J.; Wang, L.; and Wu, G. 2022. MixFormer:\nend-to-end tracking with iterative mixed attention. InCVPR,\n13598–13608.\nDai, K.; Zhang, Y .; Wang, D.; Li, J.; Lu, H.; and Yang,\nX. 2020. High-performance long-term tracking with meta-\nupdater. In CVPR, 6297–6306.\nDanelljan, M.; Bhat, G.; Khan, F. S.; and Felsberg, M. 2016.\nECO: efficient convolution operators for tracking. InCVPR,\n6931–6939.\nDanelljan, M.; Bhat, G.; Khan, F. S.; and Felsberg, M.\n2018. ATOM: accurate tracking by overlap maximization.\nIn CVPR, 4655–4664.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn image is worth 16x16 words: transformers for image\nrecognition at scale. In ICLR.\nDwibedi, D.; Aytar, Y .; Tompson, J.; Sermanet, P.; and Zis-\nserman, A. 2019. Temporal cycle-consistency learning. In\nCVPR, 1801–1810.\nFan, H.; Bai, H.; Lin, L.; Yang, F.; Chu, P.; Deng, G.; Yu,\nS.; Harshit; Huang, M.; Liu, J.; Xu, Y .; Liao, C.; Yuan, L.;\nand Ling, H. 2020. LaSOT: a high-quality large-scale single\nobject tracking benchmark. IJCV, 129: 439–461.\nFan, H.; Lin, L.; Yang, F.; Chu, P.; Deng, G.; Yu, S.; Bai, H.;\nXu, Y .; Liao, C.; and Ling, H. 2018. LaSOT: a high-quality\nbenchmark for large-scale single object tracking. In CVPR,\n5369–5378.\nFu, Z.; Liu, Q.; Fu, Z.; and Wang, Y . 2021. STMTrack:\ntemplate-free visual tracking with space-time memory net-\nworks. In CVPR, 13769–13778.\nGao, J.; Zhang, T.; and Xu, C. 2019. Graph convolutional\ntracking. In CVPR, 4644–4654.\nGao, M.; Jin, L.; Jiang, Y .; and Guo, B. 2020. Manifold\nSiamese network: a novel visual tracking ConvNet for au-\ntonomous vehicles. IEEE Transactions on Intelligent Trans-\nportation Systems, 21: 1612–1623.\nGao, S.; Zhou, C.; Ma, C.; Wang, X.; and Yuan, J. 2022.\nAiATrack: attention in attention for transformer visual track-\ning. In ECCV, 146–164.\nGao, S.; Zhou, C.; and Zhang, J. 2023. Generalized\nRelation Modeling for Transformer Tracking. ArXiv,\nabs/2303.16580.\nGuo, M.; Zhang, Z.; Fan, H.; Jing, L.; Lyu, Y .; Li, B.; and\nHu, W. 2022. Learning Target-aware Representation for Vi-\nsual Tracking via Informative Interactions. In IJCAI-22,\n927–934.\nHao, J.; Zhou, Y .; Zhang, G.; Lv, Q.; and Wu, Q. 2018. A\nreview of target tracking algorithm based on UA V . In2018\nIEEE International Conference on Cyborg and Bionic Sys-\ntems (CBS), 328–333.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll’ar, P.; and Girshick,\nR. B. 2021. Masked autoencoders are scalable vision learn-\ners. In CVPR, 15979–15988.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural Computation, 9: 1735–1780.\nHu, K.; Zhou, X.; Cao, M.; Wang, M.; Gao, G.; Yang, W.;\nand Tan, H. 2023. Progressive Perception Learning for Dis-\ntribution Modulation in Siamese Tracking. In 2023 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nHuang, L.; Zhao, X.; and Huang, K. 2018. GOT-10k: a large\nhigh-diversity benchmark for generic object tracking in the\nwild. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 43: 1562–1577.\nJabri, A.; Owens, A.; and Efros, A. A. 2020. Space-time cor-\nrespondence as a contrastive random walk. arXiv preprint\narXiv:2006.14613.\nLan, L.; Wang, X.; Zhang, S.; Tao, D.; Gao, W.; and Huang,\nT. S. 2018. Interacting Tracklets for Multi-Object Tracking.\nIEEE Transactions on Image Processing, 27(9): 4585–4597.\nLaw, H.; and Deng, J. 2018. CornerNet: detecting objects as\npaired keypoints. In ECCV, 765–781.\nLi, B.; Wu, W.; Wang, Q.; Zhang, F.; Xing, J.; and Yan, J.\n2018. SiamRPN++: evolution of Siamese visual tracking\nwith very deep networks. In CVPR, 4277–4286.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12526\nLin, L.; Fan, H.; Zhang, Z.; Xu, Y .; and Ling, H. 2022.\nSwinTrack: A Simple and Strong Baseline for Transformer\nTracking. In NeurIPS, volume 35, 16743–16754.\nLin, T.-Y .; Maire, M.; Belongie, S. J.; Hays, J.; Perona, P.;\nRamanan, D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: common objects in context. In ECCV, 740–755.\nLiu, L.; Zhang, J.; He, R.; Liu, Y .; Wang, Y .; Tai, Y .; Luo, D.;\nWang, C.; Li, J.; and Huang, F. 2020. Learning by analogy:\nreliable supervision from transformations for unsupervised\noptical flow estimation. In CVPR, 6488–6497.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: hierarchical vision\ntransformer using shifted windows. In ICCV, 9992–10002.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. In ICLR.\nMayer, C.; Danelljan, M.; Paudel, D. P.; and Gool, L. V .\n2021. Learning target candidate association to keep track\nof what not to track. In ICCV, 13424–13434.\nMueller, M.; Smith, N. G.; and Ghanem, B. 2016. A Bench-\nmark and Simulator for UA V Tracking. InECCV, 445–461.\nM¨uller, M.; Bibi, A.; Giancola, S.; Alsubaihi, S.; and\nGhanem, B. 2018. TrackingNet: a large-scale dataset and\nbenchmark for object tracking in the wild. In ECCV, 310–\n327.\nNam, H.; and Han, B. 2015. Learning multi-domain con-\nvolutional neural networks for visual tracking. In CVPR,\n4293–4302.\nPi, Z.; Wan, W.; Sun, C.; Gao, C.; Sang, N.; and Li, C.\n2022. Hierarchical feature embedding for visual tracking.\nIn ECCV, 428–445.\nRezatofighi, S. H.; Tsoi, N.; Gwak, J.; Sadeghian, A.; Reid,\nI. D.; and Savarese, S. 2019. Generalized intersection over\nunion: a metric and a loss for bounding box regression. In\nCVPR, 658–666.\nRistea, N.-C.; Miron, A.-I.; Savencu, O.; Georgescu, M.-I.;\nVerga, N.; Khan, F. S.; and Ionescu, R. T. 2023. CyTran:\na cycle-consistent transformer with multi-level consistency\nfor non-contrast to contrast ct translation. Neurocomputing,\n126211.\nSenst, T.; Eiselein, V .; and Sikora, T. 2012. Robust local\noptical flow for feature tracking. IEEE Transactions on Cir-\ncuits and Systems for Video Technology, 22: 1377–1387.\nSoleimanitaleb, Z.; and Keyvanrad, M. A. 2022. Single ob-\nject tracking: a survey of methods, datasets, and evaluation\nmetrics. arXiv preprint arXiv:2201.13066.\nSong, Z.; Yu, J.; Chen, Y .-P. P.; and Yang, W. 2022. Trans-\nformer tracking with cyclic shifting window attention. In\nCVPR, 8781–8790.\nTan, H.; Zhang, X.; Zhang, Z.; Lan, L.; Zhang, W.; and\nLuo, Z. 2021. Nocal-Siam: Refining Visual Features and\nResponse With Advanced Non-Local Blocks for Real-Time\nSiamese Tracking. IEEE Transactions on Image Processing,\n30: 2656–2668.\nThangavel, J.; Kokul, T.; Ramanan, A.; and Fernando, S.\n2023. Transformers in single object tracking: an experimen-\ntal survey. arXiv preprint arXiv:2302.11867.\nV oigtlaender, P.; Luiten, J.; Torr, P. H. S.; and Leibe, B. 2019.\nSiam R-CNN: visual tracking by re-detection. In CVPR,\n6577–6587.\nWang, N.; gang Zhou, W.; Wang, J.; and Li, H. 2021a. Trans-\nformer meets tracker: exploiting temporal context for robust\nvisual tracking. In CVPR, 1571–1580.\nWang, N.; Song, Y .; Ma, C.; gang Zhou, W.; Liu, W.; and Li,\nH. 2019. Unsupervised deep tracking. InCVPR, 1308–1317.\nWang, Q.; Gao, J.; Xing, J.; Zhang, M.; and Hu, W. 2017.\nDCFNet: discriminant correlation filters network for visual\ntracking. arXiv preprint arXiv:1704.04057.\nWang, X.; Jabri, A.; and Efros, A. A. 2019. Learning cor-\nrespondence from the cycle-consistency of time. In CVPR,\n2561–2571.\nWang, X.; Shu, X.; Zhang, Z.; Jiang, B.; Wang, Y .; Tian, Y .;\nand Wu, F. 2021b. Towards more flexible and accurate ob-\nject tracking with natural language: algorithms and bench-\nmark. In CVPR, 13758–13768.\nWu, L.; Wang, Y .; and Shao, L. 2018. Cycle-consistent deep\ngenerative hashing for cross-modal retrieval.IEEE Transac-\ntions on Image Processing, 28: 1602–1612.\nXie, F.; Wang, C.; Wang, G.; Yang, W.; and Zeng, W.\n2021. Learning tracking representations via dual-branch\nfully transformer networks. In ICCVW, 2688–2697.\nYan, B.; Peng, H.; Fu, J.; Wang, D.; and Lu, H. 2021.\nLearning spatio-temporal transformer for visual tracking. In\nICCV, 10428–10437.\nYang, T.; and Chan, A. B. 2018. Learning dynamic memory\nnetworks for object tracking. In ECCV, 153–169.\nYe, B.; Chang, H.; Ma, B.; and Shan, S. 2022. Joint feature\nlearning and relation modeling for tracking: a one-stream\nframework. In ECCV, 341–357.\nYuan, W.; Wang, M. Y .; and Chen, Q. 2020. Self-supervised\nobject tracking with cycle-consistent Siamese networks.\nIn 2020 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), 10351–10358.\nZhang, L.; Gonzalez-Garcia, A.; van de Weijer, J.; Danell-\njan, M.; and Khan, F. S. 2019. Learning the model update\nfor Siamese trackers. In ICCV, 4009–4018.\nZhang, Z.; Liu, Y .; Wang, X.; Li, B.; and Hu, W. 2021.\nLearn to match: automatic matching network design for vi-\nsual tracking. In ICCV, 13319–13328.\nZhang, Z.; and Peng, H. 2020. Ocean: object-aware anchor-\nfree tracking. In ECCV, 771–787.\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\npaired Image-to-Image Translation Using Cycle-Consistent\nAdversarial Networks. In ICCV, 2242–2251.\nZhu, Z.; Wang, Q.; Li, B.; Wu, W.; Yan, J.; and Hu, W. 2018.\nDistractor-aware Siamese networks for visual object track-\ning. In ECCV, 103–119.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n12527",
  "topic": "Granularity",
  "concepts": [
    {
      "name": "Granularity",
      "score": 0.8161449432373047
    },
    {
      "name": "Fusion",
      "score": 0.5938940644264221
    },
    {
      "name": "Computer science",
      "score": 0.4864084720611572
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4700944125652313
    },
    {
      "name": "Transformer",
      "score": 0.42972755432128906
    },
    {
      "name": "Tracking (education)",
      "score": 0.4132542312145233
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3886665105819702
    },
    {
      "name": "Electrical engineering",
      "score": 0.14580705761909485
    },
    {
      "name": "Engineering",
      "score": 0.1258184015750885
    },
    {
      "name": "Psychology",
      "score": 0.062334418296813965
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170215575",
      "name": "National University of Defense Technology",
      "country": "CN"
    }
  ]
}