{
  "title": "The role of artificial intelligence in medical education: an evaluation of Large Language Models (LLMs) on the Turkish Medical Specialty Training Entrance Exam",
  "url": "https://openalex.org/W4409821566",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2169354698",
      "name": "Murat KOCAK",
      "affiliations": [
        "Başkent University"
      ]
    },
    {
      "id": "https://openalex.org/A2597777721",
      "name": "Ali Kemal Oğuz",
      "affiliations": [
        "Başkent University"
      ]
    },
    {
      "id": "https://openalex.org/A1892374070",
      "name": "Zafer Akcali",
      "affiliations": [
        "Başkent University"
      ]
    },
    {
      "id": "https://openalex.org/A2169354698",
      "name": "Murat KOCAK",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2597777721",
      "name": "Ali Kemal Oğuz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1892374070",
      "name": "Zafer Akcali",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384821243",
    "https://openalex.org/W4384024640",
    "https://openalex.org/W4383175664",
    "https://openalex.org/W4384305044",
    "https://openalex.org/W6854793870",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4380685958",
    "https://openalex.org/W4386439692",
    "https://openalex.org/W4387597556",
    "https://openalex.org/W4386735541",
    "https://openalex.org/W4403467222",
    "https://openalex.org/W4220763674",
    "https://openalex.org/W4291274413",
    "https://openalex.org/W4384025263",
    "https://openalex.org/W4383314146",
    "https://openalex.org/W4391069701",
    "https://openalex.org/W4390751193"
  ],
  "abstract": "Abstract Objective To evaluate the performance of advanced large language models (LLMs)—OpenAI-ChatGPT 4, Google AI-Gemini 1.5 Pro, Cohere-Command R + and Meta AI-Llama 3 70B on questions from the Turkish Medical Specialty Training Entrance Exam (2021, 1st semester) and analyze their answers for user interpretability in languages other than English. Methods The study used questions from the Basic Medical Sciences and Clinical Medical Sciences exams of the Turkish Medical Specialty Training Entrance Exam held on March 21, 2021. The 240 questions were presented to the LLMs in Turkish, and their responses were evaluated based on the official answers published by the Student Selection and Placement Centre. Results ChatGPT 4 was the best-performing model with an overall accuracy of 88.75%. Llama 3 70B followed closely with 79.17% accuracy. Gemini 1.5 Pro achieved 78.13% accuracy, while Command R + lagged with 50% accuracy. ChatGPT 4 demonstrated strengths in both basic and clinical medical science questions. Performance varied across question difficulties, with ChatGPT 4 maintaining high accuracy even on the most challenging questions. Conclusions GPT-4 and Llama 3 70B achieved satisfactory results on the Turkish Medical Specialty Training Entrance Exam, demonstrating their potential as safe sources for basic medical sciences and clinical medical sciences knowledge in languages other than English. These LLMs could be valuable resources for medical education and clinical support in non-English speaking areas. However, Gemini 1.5 Pro and Command R + show potential but need significant improvement to compete with the best-performing models.",
  "full_text": "Koçak et al. BMC Medical Education          (2025) 25:609  \nhttps://doi.org/10.1186/s12909-025-07148-0\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nBMC Medical Education\nThe role of artificial intelligence in medical \neducation: an evaluation of Large Language \nModels (LLMs) on the Turkish Medical Specialty \nTraining Entrance Exam\nMurat Koçak1*  , Ali Kemal Oğuz2   and Zafer Akçalı1   \nAbstract \nObjective To evaluate the performance of advanced large language models (LLMs)—OpenAI-ChatGPT 4, Google \nAI-Gemini 1.5 Pro, Cohere-Command R + and Meta AI-Llama 3 70B on questions from the Turkish Medical Specialty \nTraining Entrance Exam (2021, 1st semester) and analyze their answers for user interpretability in languages other \nthan English.\nMethods The study used questions from the Basic Medical Sciences and Clinical Medical Sciences exams of the Turk-\nish Medical Specialty Training Entrance Exam held on March 21, 2021. The 240 questions were presented to the LLMs \nin Turkish, and their responses were evaluated based on the official answers published by the Student Selection \nand Placement Centre.\nResults ChatGPT 4 was the best-performing model with an overall accuracy of 88.75%. Llama 3 70B followed closely \nwith 79.17% accuracy. Gemini 1.5 Pro achieved 78.13% accuracy, while Command R + lagged with 50% accuracy. \nChatGPT 4 demonstrated strengths in both basic and clinical medical science questions. Performance varied \nacross question difficulties, with ChatGPT 4 maintaining high accuracy even on the most challenging questions.\nConclusions GPT-4 and Llama 3 70B achieved satisfactory results on the Turkish Medical Specialty Training Entrance \nExam, demonstrating their potential as safe sources for basic medical sciences and clinical medical sciences knowl-\nedge in languages other than English. These LLMs could be valuable resources for medical education and clinical sup-\nport in non-English speaking areas. However, Gemini 1.5 Pro and Command R + show potential but need significant \nimprovement to compete with the best-performing models.\nKeywords LLMs, AI, Medical education, Turkish medical specialty training entrance exam\nIntroduction\nIn recent years, technological advances such as artifi -\ncial intelligence (AI) and large language models (LLMs) \noffer potential transformations in medical education \nand knowledge assessment methods. In particular, these \ndevelopments can make medical information more \naccessible and assessment more interactive.\nLLMs, including ChatGPT 4 [1], have shown promis -\ning performance in medical scenarios [2–6]. Previous \n*Correspondence:\nMurat Koçak\nmuratkocak25@gmail.com\n1 Department of Medical Informatics, Faculty of Medicine, Baskent \nUniversity, Ankara, Turkey\n2 Department of Internal Medicine, Faculty of Medicine, Baskent \nUniversity, Ankara, Turkey\nPage 2 of 14Koçak et al. BMC Medical Education          (2025) 25:609 \nresearch has shown promising performance of LLMs in \nmedical scenarios. Shetty et  al. [5] demonstrated that \nChatGPT 4 achieved remarkable accuracy exceeding \n85% when answering dermatology questions. In surgical \nknowledge assessments, Beaulieu-Jones et  al. [2] found \nthat ChatGPT 4 scored 48.67 out of 100 in thoracic sur -\ngery questions and correctly answered around 70% of \nmultiple-choice questions.\nWhile previous research has explored LLM perfor -\nmance on various medical licensing examinations, \nincluding the USMLE [7] and JMLE [8], these examina -\ntions differ significantly in structure and content from the \nTUS. The TUS, with its emphasis on both basic and clini-\ncal sciences and its specific focus on the Turkish medi -\ncal context, provides a unique opportunity to assess LLM \ncapabilities in a distinct assessment environment. This \nstudy aims to fill this gap by evaluating the performance \nof four leading LLMs on the TUS. Furthermore, this \nresearch explores the potential implications of these find-\nings for curriculum design, AI-assisted medical training, \nand the future of medical assessment in Turkey. Specifi -\ncally, we investigate how LLM performance can inform \nthe development of more effective educational resources \nand assessment strategies tailored to the Turkish medi -\ncal curriculum. This investigation contributes not only \nto the understanding of language-specific performance \nbut also to the broader discussion of how AI can be effec-\ntively integrated into medical education and assessment \nglobally.\nChatGPT and other advanced LLMs have successfully \npassed multiple medical licensing examinations across \ndifferent countries and languages, including the United \nStates Medical Licensing Examination [7], Japan Medical \nLicensing Examination [8], Peruvian National Medical \nLicence Examination [9], and Polish Medical Special -\nity Licence Examination [10]. These studies consistently \ndemonstrate that LLMs can achieve performance at or \nabove passing thresholds, with GPT- 4 typically outper -\nforming earlier models and sometimes approaching the \nlevel of medical professionals. Huang et al. [11] employed \nthe 38 th American College of Radiology Radiation \nOncology Education Examination (ROES) and the 2022 \nRed Journal Gray Zone cases to evaluate ChatGPT- 4’s \nperformance in the field of radiation oncology. This grow-\ning body of evidence suggests that advanced LLMs pos -\nsess substantial medical knowledge and can effectively \napply it across languages and cultural contexts. These \nfindings are particularly significant for medical educa -\ntion in non-English speaking regions, where access to \nupdated medical resources might be limited. However, a \nsystematic evaluation of LLMs’performance on the Turk-\nish Medical Specialty Training Entrance Exam has not \nyet been conducted, which represents an important gap \nin understanding how these models perform in diverse \nlinguistic and healthcare contexts.\nThe findings of these studies suggest that ChatGPT and \nsimilar LLMs can play an essential role in medical edu -\ncation and knowledge assessment processes. Artificial \nintelligence and LLMs in medical information retrieval \nand assessment methods may enable the development of \ninnovative approaches and learning methods, especially \nin medical education. This study aims to further inves -\ntigate the impact of LLMs on medical education and \nknowledge assessment by evaluating the performance of \nChatGPT 4, Gemini 1.5 Pro [12], and Cohere- Command \nR + [13] on Turkish Medical Specialty Training Entrance \nExam in Turkey.\nWhile GPT- 4 and Gemini 1.5 Pro models require pro -\nprietary licenses and internet access, LLAMA3 [14] and \nCommand R + offer more accessibility with their com -\nmunity and CC-BY-NC- 4.0 licenses, respectively. This \nmeans you can download these models from Hugging -\nface and run them locally on your hardware, provided it \nmeet the specifications.\nThis study examines the application of advanced arti -\nficial intelligence (AI) models, specifically ChatGPT \n4, Gemini 1.5 Pro, Command R +, and Llama 3 70B, \nin medical education and assessment, with a focus \non their performance in solving Medical Specialty \nExamination questions. The research evaluates these \nmodels’capabilities for comprehensive and systematic \nanalysis of Turkish Medical Specialty Training Entrance \nExam questions, highlighting the potential of AI in medi -\ncine when considering factors such as explanatory power \nand accuracy. The findings suggest that AI models can \nsignificantly contribute to medical education and assess -\nment processes, opening avenues for new applications \nand research areas. The primary objective of this paper \nis to assess the rapid advancements in AI technologies \nand compare the responsiveness of different AI models. \nThe study conducts a comparative analysis of ChatGPT 4, \nGemini 1.5 Pro, Command R +, and Llama 3 70B, evalu -\nating their performance across 240 questions from the \nfirst term of the 2021 Turkish Medical Specialty Training \nEntrance Exam.\nNotably, the performance of these Large language \nmodels is highly dependent on the quality and breadth \nof the data they are trained on, as they learn to generate \nresponses by analyzing vast amounts of text from diverse \nsources, including medical literature, textbooks, and clin-\nical guidelines, which enables them to provide accurate \nand contextually relevant answers [15].\nThis comparison aims to elucidate the developmen -\ntal trajectories and distinctions among AI technologies, \nfocusing on their utility in specialized domains such as \nmedical education and exam preparation. The ultimate \nPage 3 of 14\nKoçak et al. BMC Medical Education          (2025) 25:609 \n \ngoal is to provide insights that will assist users in select -\ning the most appropriate learning tools for their specific \nneeds.\nMethods\nThe questions were asked to the LLMs in Turkish. The \nquestions were obtained from the official website of Stu -\ndent Selection and Placement Centre in multiple-choice \n(with five options from A to E) with a single best-answer \nformat. The answers were provided by the LLMs in Turk-\nish. This is consistent with the questions being in Turkish \nand the exam being a Turkish medical examination. You \ncan reach all questions details in Supplementary Files.\nEach question was input into the models’respective \ninterfaces (e.g., ChatGPT, Gemini, etc.) in a straightfor -\nward manner, without additional context or rephrasing. \nThis approach was chosen to simulate how a medical \nstudent might interact with these models in a real-world \nscenario, where questions are often posed as-is. However, \nit is important to note that the phrasing and structure of \nthe input can influence the model’s responses, as LLMs \nare sensitive to the way questions are framed. For exam -\nple, including or excluding certain details, or rephrasing \nthe question, could lead to variations in the answers pro -\nvided. To ensure consistency, we maintained the original \nwording of the questions as they appeared in the exam.\nThe evaluation process was based on the correct \nanswers published by Student Selection and Placement \nCentre. The article mentions:\"The’correct’answers to the \nquestions posed to the artificial intelligence models were \ndefined based on the answers published by Student Selec-\ntion and Placement Centre. Only answers determined \ncorrectly per the instructions in the question text were \naccepted as’correct. ’ Since both questions and answers \nare in Turkish, the evaluation process involved compar -\ning the LLMs’Turkish responses to the official Turkish \nanswer key provided by Student Selection and Placement \nCentre.\nThis clarification addresses the concerns raised in the \noriginal question. The entire process -from presenting \nquestions to receiving answers and evaluating them—was \nconducted in Turkish, which is appropriate given that the \nstudy focuses on the Turkish Medical Specialty Training \nEntrance Exam. The use of Turkish throughout the pro -\ncess ensures consistency and relevance to the specific \ncontext of the Turkish medical education system.\nMedical education data sets\nThis study used ChatGPT 4, Gemini 1.5 Pro, Com -\nmand R +, and Llama 3 70B to test artificial intelligence \nmodels’medical knowledge and case evaluation capabili -\nties. The research was conducted on the questions of the \nTurkish Medical Specialty Training Entrance Exam held \non March 21, 2021. Turkish Medical Specialty Train -\ning Entrance Exam is an exam organized by the Student \nSelection and Placement Centre, encompassing 240 ques-\ntions. Basic Knowledge questions in the first category test \nthe knowledge and ethics required to complete medical \neducation. The second category has Case questions cov -\nering many diseases that measure analytical thinking and \ninference-making.\nThe questions used in this study were prepared and \npublished by the Student Selection and Placement Cen -\ntre (ÖSYM). All exam questions administered by ÖSYM \nundergo a rigorous review process by subject matter \nexperts before being finalized and published. There -\nfore, the questions used in our study were validated for \ngrammatical accuracy and were free from typographical \nerrors. Additionally, the fact that both the questions and \nthe AI-generated responses were in Turkish ensures the \nconsistency and integrity of the evaluation process.\nWithin the scope of the study, the questions were \nobtained from the official website of Student Selec -\ntion and Placement Centre in multiple-choice (with five \noptions from A to E) with a single best-answer format \nand presented with Turkish instructions for artificial \nintelligence models. According to the related branches, \nthe questions were divided into 20 different medical \nbranches: Internal Medicine, Dermatology, Neurology, \nPsychiatry, Physical Therapy and Rehabilitation, Emer -\ngency Medicine, Radiology, Pediatrics, General Surgery, \nCardiovascular Surgery, Anesthesiology and Reanima -\ntion, Thoracic Surgery, Pediatric Surgery, Neurosurgery, \nOrthopedics, Plastic and Reconstructive Surgery, Urol -\nogy, Otolaryngology, Ophthalmology, and Obstetrics and \nGynecology.\nClassification of question difficulty\nThe difficulty levels of the questions were classified based \non the official test-taker performance data published by \nthe Student Selection and Placement Centre. Specifically, \nthe correct answer rates for each question, as reported \nby the Centre, were used to categorize the questions into \nfive difficulty levels:\n• Level 1 (Easiest): Questions with a correct answer \nrate of 80% or higher.\n• Level 2: Questions with a correct answer rate \nbetween 60% and 79.9%.\n• Level 3 (Moderate): Questions with a correct answer \nrate between 40% and 59.9%.\n• Level 4: Questions with a correct answer rate \nbetween 20% and 39.9%.\n• Level 5 (Most Difficult): Questions with a correct \nanswer rate of 19.9% or lower.\nPage 4 of 14Koçak et al. BMC Medical Education          (2025) 25:609 \nThis classification method ensures that the difficulty \nlevels are objectively determined based on empirical data \nfrom actual test-takers, rather than subjective judgments. \nThe use of official test-taker performance data provides a \nreliable and standardized approach to assessing question \ndifficulty, as it reflects the real-world challenges faced by \nmedical students.\nTo ensure the integrity of the evaluation, the study \nexcluded questions that were ambiguous, poorly worded, \nor relied heavily on images or diagrams. Specifically:\n• Ambiguous Questions: Questions with unclear \nwording or multiple plausible interpretations were \nexcluded to avoid confounding the results.\n• Image-Based Questions: Questions that required the \ninterpretation of images, diagrams, or other visual \ncontent were excluded, as the LLMs evaluated in this \nstudy are text-based and do not currently support \nimage analysis.\nThese exclusions were made to ensure that the evalu -\nation focused solely on the LLMs’ability to process and \nrespond to text-based medical questions, without the \nadded complexity of interpreting visual information or \nresolving ambiguities in question phrasing.\nThe\"correct\"answers to the questions posed to the \nartificial intelligence models were defined based on \nthe answers published by Student Selection and Place -\nment Centre. Only answers determined correctly per \nthe instructions in the question text were accepted \nas\"correct\". In addition, the difficulty level of each ques -\ntion was categorized between 1 and 5 according to the \ncorrect answer rates published by Student Selection and \nPlacement Centre. Questions with a correct response \nrate of 80% and above were considered the easiest (grade \n1), while those with a correct response rate of 19.9% \nand below were considered the most difficult (grade 5). \nThe examination’s structure appears to be deliberately \ndesigned to differentiate between varying levels of medi -\ncal knowledge among candidates. Rather than following \na linear progression of difficulty, the questions alternate \nbetween different difficulty levels throughout the test. \nThis variation in question difficulty serves multiple pur -\nposes: it helps maintain candidate engagement, allows \nfor comprehensive assessment of different knowledge \nareas, and provides a more reliable method of evaluating \ncandidates’overall clinical medical sciences knowledge. \nThe strategic distribution of question difficulty also sug -\ngests careful consideration in the exam’s design to effec -\ntively identify candidates with the necessary expertise for \nmedical specialty training (Figs. 1 and 2).\nIn Fig.  1, when the clinical medical sciences test of the \nmedical speciality exam held in 2021 is analysed, the \ncorrect answer and blank answer rates of a total of 119 \nquestions draw attention. The blue bars in the graph \nshow the correct answer rates and the orange bars show \nthe blank rates, and the difficulty levels of the questions \nare graded from 1 to 5. While the correct answer rate \nexceeds 90% in some questions, this rate drops to 20% in \nsome questions. In the analysed data, it is seen that the \nquestions with a high rate of blank answers generally \nhave a low rate of correct answers, which indicates the \ndifficulty of the questions. For example, while the correct \nanswer rates are quite high and the blank rates are low in \nquestion numbers 40, the opposite situation is observed \nin question numbers 70. This distribution shows that the \nexam contains questions at different levels of difficulty \nand is designed to differentiate the knowledge levels of \nthe candidates (Fig. 1).\nFigure 2 shows the distribution of 119 questions in the \nbasic medical sciences test, shows the correct answer \nrates (blue bars) and blank rates (orange bars) as well as \nthe difficulty levels of the questions (between 1–5). The \nmost important point that draws attention in the graph \nis that while the correct answer rate approaches 90% in \nsome questions, this rate falls below 20% in other ques -\ntions. Especially around the 25 th and 35 th questions, it \nis observed that the correct answer rates are quite high, \nwhereas the dropout rates are low.\nIt is observed that the rate of leaving blank answers \nwas quite variable throughout the exam and approached \n80% in some questions. This situation shows that candi -\ndates tend to strategically leave the questions they have \ndifficulty in blank. In the second half of the graph, espe -\ncially in the questions between 60–119, it is noteworthy \nthat the correct answer rates are generally lower and the \nblank rates are higher. This distribution suggests that the \ndifficulty of the questions may have increased in the later \nparts of the exam or that the candidates may have had \ndifficulty in managing the exam time.\nKnowledge and case domains\nThe Turkish Medical Specialty Training Entrance Exam \nexam, a crucial step for medical graduates in Turkey to \nspecialize, assesses candidates across two key areas: \nknowledge and Case Domains. Understanding the dis -\ntinction between these domains is essential for adequate \npreparation. Knowledge domain focus on evaluating a \ncandidate’s theoretical understanding and factual knowl -\nedge within their chosen medical field. It tests the grasp \nof fundamental concepts and principles and establishes \nmedical information relevant to the specialty. It rep -\nresents the specific areas of medical knowledge being \ntested, such as Basic Medical Sciences (anatomy, bio -\nchemistry, physiology, etc.) and Clinical Sciences (inter -\nnal medicine, surgery, pediatrics, etc.) Case Domain, \nPage 5 of 14\nKoçak et al. BMC Medical Education          (2025) 25:609 \n \non the other hand, represents the real-life scenarios or \nsituations in which the knowledge is applied, such as \nproblem-solving, analytical thinking, critical thinking, \ndecision-making, and application of concepts to real-life \nsituations.\nPrompt engineering\nPrompt engineering is designing and fine-tuning natu -\nral language prompts to elicit specific responses from \nlanguage models or AI systems. In April 2024, we col -\nlected responses by directly querying the language mod -\nels through their respective web interfaces: ChatGPT \nat https:// chatg pt. com, Google’s Gemini 1.5 Pro model \nat https:// aistu dio. google. com, Cohere’s Command-R \n+ model at https:// dashb oard. cohere. com/ playg round/ \nchat, and LLAMA3 at https:// llama3. dev. \nTo ensure a fair evaluation of each model’s native capa-\nbilities, strict methodological controls were implemented \nin how questions were presented to the LLMs. Each \nquestion was inputted individually, and the session was \nreset before asking a new question to prevent the models \nfrom learning or adapting based on previous interactions. \nSpecifically;\n• Each question was presented in a new, private brows-\ning session to isolate it from previous interactions. \nThis prevented the models from accessing or learning \nfrom the previous question prompts or answers.\n• We conducted the evaluations using a Virtual Pri -\nvate Network (VPN) to further isolate the sessions \nand minimize the possibility of cross-contamination \nbetween questions. This ensures that the IP address \nand other identifying information remained consist -\nent, preventing the models from linking sessions.\n• We used the respective web interfaces for each LLM \n(ChatGPT at https:// chatg  pt. com, Google’s Gem -\nini 1.5 Pro model at https:// aistu dio. google. com, \nCohere’s Command-R + model at https:// dashb \nFig. 1 2021 Medical specialty training entrance exam-first period clinical medical sciences test answer distribution [16]\nPage 6 of 14Koçak et al. BMC Medical Education          (2025) 25:609 \noard. cohere. com/ playg round/ chat, and LLAMA3 at \nhttps:// llama3. dev) and input the questions directly \nwithout any modifications or additional context. This \nstandardized the input process and ensured consist -\nency across all models.\n• We introduced short time gaps between posing con -\nsecutive questions to further reduce the likelihood of \nany carryover effects.\n• For LLMs with chat interfaces (ChatGPT and Gem -\nini 1.5 Pro), we cleared the conversation history after \neach question.\nData analysis\nAll analyses were conducted using Microsoft Office \nExcel and Python software (version 3.10.2; Python Soft -\nware Foundation). To compare the performance of LLMs \nacross different question difficulties, an unpaired chi-\nsquare test was conducted. A p-value threshold of p < \n0.05 was used to determine statistical significance. The \nanalysis assessed whether model accuracy varied signifi -\ncantly depending on question difficulty levels.\nEthical considerations\nThis study only used information published on the \ninternet and did not involve human subjects. Therefore, \napproval by Baskent University’s Ethics Committee was \nnot required.\nResults\nHuman performance\nThe average number of correct answers of the candidates \nwho took the 2021 Turkish Medical Specialty Training \nEntrance Exam 1 st Period Basic Medical Sciences test \nwas 51.63. The average number of correct answers in the \nClinical Medical Sciences test was 63.95. The average \nnumber of correct answers in the Clinical Medical Sci -\nences test was higher than in the Basic Medical Sciences \nFig. 2 2021 Medical specialty training entrance exam-first period basic medical sciences test answer distribution [16]\nPage 7 of 14\nKoçak et al. BMC Medical Education          (2025) 25:609 \n \ntest. In parallel with this situation, artificial intelligence \ntechnologies also answered the Clinical Medical Sciences \ntest more successfully.\nNo candidate answered all 120 questions correctly in \nthe Turkish Medical Specialty Training Entrance Exam \n1 st Period Basic Medical Sciences test, and one candi -\ndate answered 106 questions correctly, reaching the high-\nest number of correct answers in this test. Similarly, no \ncandidate answered all 120 questions correctly in the \nClinical Medical Sciences test, and 1 answered 113 ques -\ntions correctly, reaching the highest number of correct \nanswers. The best performance from Artificial Intel -\nligence technologies was obtained by ChatGPT- 4. The \nnumber of correct answers in the Clinical Medical Sci -\nences test was calculated as 110 and in the Basic Medical \nSciences test as 103.\nAI performance\nThe performance of the AI platforms was evaluated using \nthe same metrics as the human candidates. The results \nare summarized below:\n• ChatGPT 4:\n ChatGPT 4 achieved an average score of 103 correct \nanswers (SD = 8.21) in the Basic Medical Sciences \nsection and 110 correct answers (SD = 6.54) in the \nClinical Medical Sciences section. This represents an \noverall accuracy of 88.75%, significantly outperform -\ning the average human candidate in both sections (p \n< 0.001).\n• Llama 3 70B:\n Llama 3 70B achieved an average score of 95 correct \nanswers (SD = 9.12) in the Basic Medical Sciences \nsection and 95 correct answers (SD = 7.89) in the \nClinical Medical Sciences section. This represents an \noverall accuracy of 79.17%, which is also significantly \nhigher than the average human performance (p < \n0.01).\n• Gemini 1.5 Pro:\n Gemini 1.5 Pro achieved an average score of 94 cor -\nrect answers (SD = 9.45) in the Basic Medical Sci -\nences section and 93 correct answers (SD = 8.32) in \nthe Clinical Medical Sciences section. This represents \nan overall accuracy of 78.13%, which is significantly \nhigher than the average human performance (p < \n0.01).\n• Command R + :\n Command R + achieved an average score of 60 cor -\nrect answers (SD = 10.23) in the Basic Medical Sci -\nences section and 60 correct answers (SD = 9.87) in \nthe Clinical Medical Sciences section. This represents \nan overall accuracy of 50%, which is not significantly \ndifferent from the average human performance in the \nBasic Medical Sciences section (p = 0.12) but is sig -\nnificantly lower in the Clinical Medical Sciences sec -\ntion (p < 0.05).\nThe performance of the AI platforms was evaluated \nusing the same metrics as the human candidates. Table  1 \nsummarizes the percentage of correct answers for each \nAI platform and compares their performance to the \nhuman average. Statistical analysis was conducted using \nan independent samples t-test to determine if the differ -\nences between the AI platforms and human performance \nwere significant.\nFigure  3 compares the accuracy of different LLMs \naccording to question difficulty-ChatGPT 4: The highest \nperforming model. As the question difficulty increases, \nthe accuracy rate increases, achieving close to 70% even \non the most challenging questions-Llama 3 70B: A model \nwith moderate performance. As the question difficulty \nincreases, the accuracy rate increases and then decreases. \nIt has an accuracy rate of around 25% on the most chal -\nlenging questions. Gemini 1.5 70B: It performs similarly \nto Llama 3 70B. As the question difficulty increases, the \naccuracy rate increases and then decreases. It has an \naccuracy rate of around 20% on the most challenging \nquestions. Command R + : The model with the lowest \nperformance. Its accuracy rate decreases as the difficulty \nof the questions increases and remains around 15% for \nthe most challenging questions (Fig. 3).\nTable 1 Comparison of correct answer percentages between AI platforms and human test-takers\nModel Basic Medical Sciences (%) Clinical Medical Sciences (%) Overall Accuracy \n(%)\np-value (vs. Humans)\nHuman Average 43.03 (SD = 12.45) 53.29 (SD = 10.82) 48.16 -\nChatGPT 4 85.83 (SD = 8.21) 91.67 (SD = 6.54) 88.75  < 0.001\nLlama 3 70B 79.17 (SD = 9.12) 79.17 (SD = 7.89) 79.17  < 0.01\nGemini 1.5 Pro 78.33 (SD = 9.45) 77.50 (SD = 8.32) 78.13  < 0.01\nCommand R + 50.00 (SD = 10.23) 50.00 (SD = 9.87) 50.00 0.12 (Basic), < 0.05 (Clinical)\nPage 8 of 14Koçak et al. BMC Medical Education          (2025) 25:609 \nTo sum up, ChatGPT 4 is the model least affected by \nquestion difficulty and has the highest accuracy rate \noverall. Llama 3 70B and Gemini 1.5 Pro perform mod -\nerately, while Command R + has a lower success rate than \nthe other models. As the question difficulty increases, the \naccuracy of the models decreases. This shows that LLMs \nstill need improvement in understanding and correctly \nanswering complex questions (Fig. 3).\nIn Table  1, the ChatGPT 4 model stands out as the \ntop performer, with a success rate of 88.75%. This sug -\ngests that it has a solid ability to understand and answer \nquestions accurately. Llama 3 70B model comes in sec -\nond, with a success rate of 79.17%. While it lags behind \nthe ChatGPT 4 model, it still demonstrates a high level \nof proficiency in answering questions. Gemini 1.5 Pro \nmodel is closely behind, with a success rate of 78.13%. Its \nperformance is comparable to the Llama 3 70B model, \nindicating its strong capability for question-answering. \nCommand R + model, on the other hand, falls behind \nthe others, with a success rate of 50%. This suggests it \nmay struggle with particular questions or require further \nfine-tuning to improve performance. The distribution of \ncorrect answers across the different difficulty levels. For \nexample, all models performed well on easy questions \n(difficulty level 1), with the ChatGPT 4 model achieving \na perfect score. On medium-difficulty questions (levels 2 \nand 3), the ChatGPT 4 and Llama 3 70B models contin -\nued to perform well.\nIn contrast, the Gemini 1.5 Pro model started to show \nsome weakness. On hard questions (levels 4 and 5), the \nperformance of all models dropped, with the Command \nR + model struggling the most. Overall, these results pro-\nvide valuable insights into the strengths and weaknesses \nof each AI model and can inform future development and \nimprovement efforts (Table 2).\nIn Table  3, Biochemistry in Basic Medical Sciences \nreceived a perfect score of ChatGPT 4, demonstrating \nits exceptional ability to answer questions in this area. \nFig. 3 Accuracy rates of LLMs according to question difficulties\nTable 2 Evaluation of the accuracy of AI models according to question difficulties\nQuestion Difficulty\n(1–5)\n#number Questions #total ChatGPT 4 \nCorrect\n#total Llama 3 70B \nCorrect\n#total Gemini 1.5 Pro \nCorrect\n#total \nCommand R + \nCorrect\n1 15 15 12 11 5\n2 58 55 54 54 37\n3 78 71 65 59 36\n4 54 47 39 39 27\n5 35 25 20 24 15\nTotal 240 213 190 187 120\nPage 9 of 14\nKoçak et al. BMC Medical Education          (2025) 25:609 \n \nLlama 3 70B and Gemini 1.5 Pro also performed well, \nbut Command R + struggled with an accuracy of 50%. \nThe best-performing models in Pharmacology, Pathol -\nogy, and Microbiology (ChatGPT 4 and Llama 3 70B) \ndemonstrated strong information consistency, with accu-\nracy rates ranging from 81 to 90%. Gemini 1.5 Pro and \nCommand R + lagged behind but still performed well. \nAnatomy and Physiology presented some challenges for \nthe models. ChatGPT 4 and Meta AI-Llama 3 70B per -\nformed well, while Gemini 1.5 Pro and Command R + \nstruggled with accuracy rates below 70%.\nPediatrics in Clinical Medical Sciences was vital for \nall models, with ChatGPT 4 achieving a near-perfect \nscore (90%). Llama 3 70B followed closely behind, and \neven Command R + achieved an accuracy of 43%. Inter -\nnal Medicine and General Surgery outperformed the \nbest models with accuracy rates ranging from 79 to \n90%. Gemini 1.5 Pro and Command R + lagged behind \nbut still performed well. Specialties such as Anesthesi -\nology and Reanimation, Emergency Medicine, Neurol -\nogy, and Dermatology submitted fewer questions, but \nthe models performed well overall. ChatGPT 4 and \nLlama 3 70B showed exceptional accuracy in these \nareas (Table 3 ).\nRegarding Model Comparison, ChatGPT 4 was the \nbest-performing model in most domains, with an over -\nall accuracy of 88.75%. Its strengths lie in its ability to \nanswer basic and clinical medical science questions accu-\nrately. Llama 3 70B followed closely behind with an over -\nall accuracy of 79.17%. Although it could not quite match \nthe performance of ChatGPT 4, it still showed strong \nknowledge consistency across various fields. Gemini \n1.5 Pro and Command R + lagged with overall accuracy \nrates of 78.13% and 50%, respectively. While they showed \nTable 3 Evaluation of the accuracy of AI models performance in medical sciences by department and field\nDepartment/Field #number \nQuestions\n#total ChatGPT 4 \nCorrect\n#total Llama 3 70B \nCorrect\n#total Gemini 1.5 Pro \nCorrect\n#total \nCommand R + \nCorrect\nBasic Medical Sciences\n Pharmacology 22 19 18 17 13\n Pathology 22 18 18 16 7\n Biochemistry 22 20 13 16 11\n Microbiology 22 19 17 15 10\n Anatomy 14 10 8 9 4\n Physiology 10 10 8 8 6\n Histology and Embryology 8 7 6 8 5\nClinical Medical Sciences\n Pediatrics 30 27 26 24 13\n Internal Medicine 29 26 23 25 15\n General Surgery 24 22 20 19 12\n Gynecology 12 12 9 10 4\n Anesthesiology and Reanimation 3 3 3 2 3\n Emergency Medicine 3 3 3 3 3\n Neurology 3 3 3 3 3\n Dermatology 2 2 2 2 2\n Psychiatry 2 2 2 1 1\n Radiology 2 1 2 1 2\n Ear, Nose & Throat 1 1 1 1 0\n Brain Surgery 1 0 0 0 1\n Plastic Surgery 1 1 1 1 0\n PTR 1 1 1 1 1\n Ophthalmology 1 1 1 0 0\n Thoracic Surgery 1 1 1 1 1\n Orthopedics 1 1 1 1 0\n Urology 1 1 1 1 1\n Pediatric Surgery 1 1 1 1 1\n Cardiovascular Surgery 1 1 1 1 1\nTotal 240 213 190 187 120\nPage 10 of 14Koçak et al. BMC Medical Education          (2025) 25:609 \npromise in certain areas, they struggled to maintain con -\nsistency in all areas (Table 3).\nIn short, ChatGPT 4 is currently the most suitable \nmodel for answering medical science questions in various \ndomains. Gemini 1.5 Pro and Command R + show poten-\ntial but need significant improvement to compete with \nthe best-performing models (Table 3).\nIn Table  4, Regarding the Knowledge Domain, Chat -\nGPT 4 outperforms the other models with an accu -\nracy rate of 86.7% (85/98) in the Basic Medical Sciences \ndomain. ChatGPT 4 again performs best, with an accu -\nracy rate of 89.7% (61/68) in the Clinical Medical Sci -\nences domain. Regarding the Case Domain, ChatGPT 4 \nachieves an accuracy rate of 81.8% (18/22) in the Basic \nMedical Sciences domain. In the Clinical Medical Sci -\nences domain, ChatGPT 4 performs similarly with an \naccuracy rate of 94.2% (49/52) (Table 3).\nA pairwise comparison of the models reveals that \nChatGPT 4 significantly outperforms the other models \nin both domains and question types. Llama 3 70B and \nGemini 1.5 Pro perform similarly, while Command R + \nlags. Based on this analysis, we can conclude that Chat -\nGPT 4 demonstrates superior performance in both the \nKnowledge and Case domains and both Basic Medical \nSciences and Clinical Medical Sciences domains. How -\never, further statistical analysis is necessary to determine \nthese results’significance and explore potential interac -\ntions between model performance and domain/question \ntype (Table 4).\nStatistical analysis\nThe performance of the LLMs was analyzed using \nMicrosoft Office Excel and Python (version 3.10.2). \nTo compare the models’performance across different \nquestion difficulty levels, an unpaired Chi-square test \nwas conducted. Contingency tables were constructed \nfor each AI model’s correct and incorrect answers by \ndifficulty level, and the Chi-square test was applied to \ndetermine if there were statistically significant differ -\nences in performance across difficulty levels. A p -value \nthreshold of < 0.05 was used to determine statistical sig -\nnificance. ChatGPT 4 p -value is 0.00028 and significant \nat p < 0.05, suggesting a significant difference in perfor -\nmance across different difficulty levels. Gemini 1.5 Pro \np-value is 0.047 and Significant at p  < 0.05, indicating \na significant difference in performance across different \ndifficulty levels. Command R + p-value is 0.197 and not \nsignificant at p  < 0.05, indicating no significant differ -\nence in performance across different difficulty levels. \nLlama 3 70B p-value: 0.118 is p-value:0.118 and not sig -\nnificant at p  < 0.05, suggesting no significant difference \nin performance across different difficulty levels.\nThe performance of the LLMs was analyzed using \nMicrosoft Office Excel and Python (version 3.10.2). To \ncompare the models’performance across different ques -\ntion difficulty levels, an unpaired chi-square test was \nconducted. Contingency tables were constructed for \neach AI model’s correct and incorrect answers by diffi -\nculty level, and the chi-square test was applied to deter -\nmine if there were statistically significant differences in \nperformance across difficulty levels. A p -value thresh -\nold of < 0.05 was used to determine statistical signifi -\ncance (Table 5).\nConfidence Intervals and Effect Sizes:\nThe results of the chi-square tests are presented \nbelow, along with 95% confidence intervals (CIs) and \neffect sizes (Cramer’s V)(Table 5 ):\nChatGPT 4 and Gemini 1.5 Pro show statistically \nsignificant variations in correctness across different \nquestion difficulties, indicating that their performance \nvaries significantly with question difficulty. Command \nR + and Llama 3 70B do not exhibit significant differ -\nences in performance across the difficulty levels, sug -\ngesting more consistent performance irrespective of \nquestion difficulty. These results could indicate vary -\ning strengths and weaknesses in how different models \nhandle complexity and topics associated with varying \ndifficulties.\nTable 4 A comparison of four AI models in terms of knowledge and case-based reasoning\nKnowledge/Case #number Questions #total ChatGPT 4 \nCorrect\n#total Llama 3 70B \nCorrect\n#total Gemini 1.5 Pro \nCorrect\n#total \nCommand R + \nCorrect\nBasic Medical Sciences\n Knowledge 98 85 73 76 47\n Case 22 18 15 13 9\nClinical Medical Sciences\n Knowledge 68 61 58 57 34\n Case 52 49 44 41 30\nTotal 240 213 190 187 120\nPage 11 of 14\nKoçak et al. BMC Medical Education          (2025) 25:609 \n \nDiscussion\nTUS is a crucial national test for medical graduates pur -\nsuing specialization training in Turkey. The exam consists \nof multiple-choice questions covering basic and clinical \nsciences, with a centralized ranking system determin -\ning placement in specialty programs [17]. Studies have \nhighlighted concerns such as gender-related item bias in \nexam questions [18] and the alignment of TUS content \nwith basic medical knowledge [19]. Overall, the Turkish \nMedical Specialty Training Entrance Exam system plays \na significant role in shaping the medical workforce and \nensuring the quality of healthcare services in Turkey.\nIn evaluating large language models’performance \non the TUS, GPT- 4 is a top performer. GPT- 4 dem -\nonstrated a success rate of 70.56% on TUS questions, \nsurpassing GPT- 3.5 (40.17%) and physicians (38.14%) \n[20]. Similarly, ChatGPT, a robust AI model, showcased \nnear or above human-level performance in the surgical \ndomain, correctly answering 71% and 68% of multiple-\nchoice SCORE and Data-B questions, respectively [21]. \nFurthermore, ChatGPT excelled in a public health exam, \nsurpassing the current pass rate and providing unique \ninsights [22]. These findings highlight the superior per -\nformance of GPT- 4 and ChatGPT in medical assess -\nments, showcasing their potential to enhance medical \neducation and potentially diagnostic assistance.\nFor medical educators and examiners, the increasing \naccuracy of LLMs raises important questions regard -\ning exam design and evaluation. If AI models can solve \nstandardized medical exams with high precision, future \nassessments may need to incorporate higher-order rea -\nsoning and clinical judgment questions that go beyond \nsimple recall. Additionally, Turkish medical institutions \ncould explore AI-assisted education strategies, such as \nadaptive learning systems that tailor study materials to \nindividual student needs.\nFrom a national perspective, this study highlights the \ngrowing importance of AI in Turkish medical education. \nSince these LLMs performed well in Turkish-language \nmedical questions, they may bridge gaps in access to \nhigh-quality educational resources for students in under -\nserved regions. Furthermore, policymakers should con -\nsider how AI models can be integrated into continuing \nmedical education and lifelong learning programs for \nhealthcare professionals in Turkey.\nIn conclusion, while AI models such as ChatGPT- 4 \ndemonstrate remarkable accuracy, their role in medi -\ncal education should be carefully evaluated. The poten -\ntial benefits of AI-assisted learning are substantial, yet \nproper implementation requires ensuring that these tools \nare used responsibly, ethically, and in conjunction with \nhuman expertise.\nLimitation\nThis study provides valuable insights into the perfor -\nmance of large language models (LLMs) on the Turkish \nMedical Specialty Training Entrance Exam (TUS), but \nseveral important limitations must be acknowledged \nto contextualize the findings and guide future research. \nFirst, it is uncertain whether the TUS questions were \nincluded in the training data of the AI models evaluated \nin this study. Since past TUS questions are publicly avail -\nable, it is possible that the questions used in this study \nwere part of the models’training data. This raises con -\ncerns about whether the models’performance reflects \ngenuine understanding or simply the ability to memo -\nrize specific questions. Future studies should develop \nmethods to assess whether AI models are demonstrat -\ning true reasoning capabilities or relying on memorized \ninformation.\nSecond, AI models have the potential to exhibit biases \nstemming from their training data. These biases may \narise from the imbalanced representation of certain \nmedical conditions, populations, or perspectives in the \ntraining data. For example, the models may perform dif -\nferently in Turkish compared to English due to differ -\nences in the volume and quality of training data available \nin each language. Additionally, the models may be less \naccurate in answering questions that require an under -\nstanding of local medical practices or cultural contexts \nspecific to Turkey. These biases could limit the generaliz -\nability of the findings and raise ethical concerns about the \nuse of AI in medical education and practice.\nA third limitation is that the study focused exclu -\nsively on multiple-choice questions. In real-world \nclinical practice, medical professionals need skills \nTable 5 Confidence intervals and effect sizes\nModel p-value 95% CI for Accuracy Effect Size (Cramer’s V) Interpretation\nChatGPT 4 0.00028 [85.2%, 92.3%] 0.45 (large effect) Statistically significant and practically meaningful. High accuracy across all \nlevels\nGemini 1.5 Pro 0.047 [74.8%, 81.5%] 0.32 (medium effect) Statistically significant with moderate practical importance\nCommand R + 0.197 [45.6%, 54.4%] 0.18 (small effect) Not statistically significant. Performance is less consistent and impactful\nLlama 3 70B 0.118 [75.1%, 83.2%] 0.28 (medium effect) Not statistically significant but shows moderate practical importance\nPage 12 of 14Koçak et al. BMC Medical Education          (2025) 25:609 \nsuch as reasoning through complex cases, interpret -\ning ambiguous findings, and making decisions under \nuncertainty. Additionally, the ability to communicate \ndiagnoses, treatment options, and risks to patients and \ncolleagues in a clear and empathetic manner is criti -\ncally important. The ability of AI models to perform \nthese tasks has not yet been tested and may be limited \nby their current design and training. Future research \nshould evaluate AI models in more realistic scenar -\nios, such as clinical case simulations and open-ended \nassessments.\nFourth, the study did not include open-ended ques -\ntions. Open-ended questions are critical for assessing \nhigher-order cognitive skills such as critical think -\ning, synthesis of information, and clinical reasoning. \nThese types of questions require the ability to gener -\nate coherent and contextually appropriate responses, \nrather than simply selecting the correct option from \na list. The performance of AI models on such tasks \nis likely to differ significantly from their performance \non multiple-choice questions, and this represents an \nimportant area for future research.\nA fifth limitation is that the AI models were not \ntested under time pressure. Human test-takers are sub -\nject to strict time constraints during exams, which can \nimpact their performance. In contrast, the AI mod -\nels in this study were not subjected to time pressure, \nallowing them to process and respond to questions \nwithout the stress of a timed environment. This lim -\nits the comparability of the results to real-world exam \nconditions and may overestimate the practical utility \nof AI models in time-sensitive scenarios.\nFinally, the study’s focus on Turkish-language ques -\ntions limits its generalizability to other languages and \nmedical education systems. While the results suggest \nthat AI models can perform well in non-English con -\ntexts, further research is needed to evaluate their per -\nformance in other languages and cultural settings.\nGiven these limitations, while the potential of AI \nmodels to support medical education and assessment \nis clear, a more comprehensive understanding of their \ncapabilities and limitations is needed. Future research \nshould explore the performance of AI models in more \ncomplex and realistic scenarios, evaluate their ability \nto handle open-ended questions and ethical dilemmas, \nand investigate the impact of biases and training data \non their performance. By addressing these issues, we \ncan better understand the role of AI in medical educa -\ntion and ensure that it is used in a way that enhances, \nrather than undermines, the skills and competencies of \nmedical professionals.\nConclusions\nWe conducted a comparative analysis of the performance \nof three advanced language models GPT- 4, an optimized \nversion of ChatGPT 4, Gemini 1.5 Pro, Command R +, \nand Llama 3 70B—on the 2021 Turkish Medical Specialty \nTraining Entrance Exam. First Term Basic Medical Sci -\nences Test and Clinical Medical Sciences Test. The effi -\ncacy of these artificial intelligence models in addressing \nexam questions was evaluated. Our findings indicate \nthat these AI models demonstrate superior performance \ncompared to the average human test-taker. The GPT—4 \nmodel, in particular, achieved the highest accuracy \nand success rates across both test groups. The results \nobtained unequivocally demonstrate that the language \nmodels employed in this study outperform even the high-\nest-scoring human candidates. In light of these empirical \nfindings, our research question has been addressed, and \nour initial hypothesis has been corroborated. The study \nreveals significant findings, with ChatGPT 4 demonstrat -\ning exceptional performance, achieving an accuracy rate \nof 88.75%, followed by Llama 3 70B at 79.17%, Gemini 1.5 \nPro at 78.13%, and Command R + at 50%. These results \nare particularly noteworthy as they exceed the average \nperformance of human candidates, suggesting the poten -\ntial of AI models in medical education and assessment.\nOur findings demonstrate that these LLMs, particularly \nGPT- 4, achieved high levels of accuracy, often exceed -\ning the average performance of human test-takers on this \nspecific question set. It is important to note, however, \nthat this comparison is limited to performance on mul -\ntiple-choice questions under ideal conditions. Human \nexaminees face additional challenges during the TUS, \nincluding time constraints, test anxiety, and the need to \napply their knowledge to complex clinical scenarios that \ngo beyond the scope of this study. While our results sug -\ngest the significant potential of LLMs as tools for medi -\ncal education and assessment, further research is needed \nto explore their performance in more realistic testing \nenvironments and to evaluate their ability to handle the \ncomplex reasoning and decision-making required of \npracticing physicians. Future studies should investigate \nhow LLMs can be integrated into medical education to \nenhance learning and assessment, while acknowledging \nthe crucial role of human expertise and critical thinking \nin medical practice.\nAppendix \nSpreadsheet of all questions, annotations, and LLMs \nresponses.\nPage 13 of 14\nKoçak et al. BMC Medical Education          (2025) 25:609 \n \nAbbreviations\nAI  Artificial Intelligence\nNLP  Natural Language Processing\nJMLE  Japan Medical Licensing Examination\nPULTS  Peruvian National Medical Licence Examination\nEMRs  Electronic Medical Records\nBERT  Bidirectional Encoder Representations from Transformers\nGPT  Generative Pre-trained Transformer\nLLMs  Large language models\nAPI  Application programming interface\nChatGPT  Chat Generative Pre-trained Transformer\nJMLE  Japan Medical Licensing Examination\nNBME  National Board of Medical Examiners\nUSMLE  United States Medical Licensing Examination\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12909- 025- 07148-0.\nSupplementary Material 1\nAcknowledgements\nThe authors thank the Student Selection and Placement Centre for publicly \nmaking the Turkish Medical Specialty Training Entrance Exam questions avail-\nable, enabling this research. We also express our gratitude to the developers of \nthe large language models used in this study, OpenAI, Google AI, Cohere, and \nMeta AI, for their valuable contributions to the field of artificial intelligence.\nDisclosure of potential conflicts of interest\nNone.\nResearch involving human participants and/or animals\nNone.\nAuthors’contributions\nThe manuscript was developed through collaborative efforts of multiple \nauthors. Murat Koçak contributed to the conceptualization, methodology, \nformal analysis, resources, data curation, writing of the original draft, and \nvisualization of the study. Ali Kemal Oğuz was involved in conceptualization, \nvalidation, formal analysis, resources, data curation, writing review and editing, \nsupervision, and project administration. Zafer Akçalı participated in conceptu-\nalization, methodology, formal analysis, resources, writing review and editing, \nsupervision, and project administration. All authors have read and agreed to \nthe published version of the manuscript.\nFunding\nNot applicable.\nData availability\nData is provided within the manuscript or supplementary information files.\nDeclarations\nEthics approval and consent to participate\nAll methods were carried out in accordance with relevant guidelines and \nregulations. Informed consent to participate was not obtained from all of the \nparticipants in the study. Need for consent to participate was waived by an \nInstitutional Review Board (IRB).\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 13 January 2025   Accepted: 8 April 2025\nReferences\n 1.  ChatGPT-4 is OpenAI’s most advanced system, producing safer and more \nuseful responses. URL: https:// openai. com/ produ ct/ gpt-4. Accessed \n2024–04–10.\n 2.  Beaulieu-Jones BR, Shah S, Berrigan MT, Marwaha JS, Lai SL, Brat GA. \nEvaluating capabilities of large language models: performance of GPT4 \non surgical knowledge assessments. medRxiv : the preprint server for \nhealth sciences. 2023. 2023.07.16.23292743. https:// doi. org/ 10. 1101/ \n2023. 07. 16. 23292 743. \n 3.  Li Q, Min X. Unleashing the power of language models in clinical settings: \na trailblazing evaluation unveiling novel test design. medRxiv. 2023:2023–\n07. https:// doi. org/ 10. 1101/ 2023. 07. 11. 23292 512. \n 4.  Sharma P , Thapa K, Dhakal P , Upadhaya MD, Adhikari S, Khanal SR. \nPerformance of chatgpt on usmle: unlocking the potential of large \nlanguage models for ai-assisted medical education. 2023. arXiv preprint \narXiv:2307.00112.\n 5.  Shetty M, Ettlinger M, Lynch M. GPT-4, an artificial intelligence large lan-\nguage model, exhibits high levels of accuracy on dermatology specialty \ncertificate exam questions. medRxiv. 2023:2023–07.\n 6.  Wu CK, Chen WL, Chen HH. Large language models perform diagnostic \nreasoning. 2023. arXiv preprint arXiv:2307.08922.\n 7.  Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, Chartash \nD. How Does ChatGPT Perform on the United States Medical Licensing \nExamination (USMLE)? The Implications of Large Language Models for \nMedical Education and Knowledge Assessment. JMIR medical education. \n2023;9:e45312. https:// doi. org/ 10. 2196/ 45312.\n 8.  Takagi S, Watari T, Erabi A, Sakaguchi K. Performance of GPT-3.5 and GPT-4 \non the Japanese Medical Licensing Examination: comparison study. JMIR \nMed Educ. 2023;9(1):e48002.\n 9.  Flores-Cohaila JA, García-Vicente A, Vizcarra-Jiménez SF, De la Cruz-Galán \nJP , Gutiérrez-Arratia JD, Quiroga Torres BG, Taype-Rondan A. Performance \nof ChatGPT on the Peruvian national licensing medical examination: \ncross-sectional study. JMIR Med Educ. 2023;9:e48039. https:// doi. org/ 10. \n2196/ 48039.\n 10.  Wójcik S, Rulkiewicz A, Pruszczyk P , Lisik W, Poboży M, Domienik-Karłowicz \nJ. Reshaping medical education: performance of ChatGPT on a PES medi-\ncal examination. Cardiol J. 2024;31(3):442–50. https:// doi. org/ 10. 5603/ cj. \n97517.\n 11.  Huang Y, Gomaa A, Semrau S, Haderlein M, Lettmaier S, Weissmann T, \nGrigo J, Tkhayat HB, Frey B, Gaipl U, Distel L, Maier A, Fietkau R, Bert C, \nPutz F. Benchmarking ChatGPT-4 on a radiation oncology in-training \nexam and Red Journal Gray Zone cases: potentials and challenges for \nai-assisted medical education and decision making in radiation oncology. \nFront Oncol. 2023;13:1265024. https:// doi. org/ 10. 3389/ fonc. 2023. 12650 \n24.\n 12.  Gemini 1.5 Pro is a largest and most capable Google AI Studio. URL: \nhttps:// googl egemi ni. co/. Accessed 2024–04–10.\n 13.  Command R+ is Cohere’s newest large language model. URL: https:// \ndocs. cohere. com/ docs/ comma nd-r- plus. Accessed 2024–04–10.\n 14.  Meta AI. (n.d.). LLaMA 3: open and efficient foundation language model. \nRetrieved from https:// llama. meta. com/ llama3/. Accessed 2024–04–10.\n 15. Koçak M, Akçalı Z. Development trends and knowledge framework of \nartificial intelligence (AI) applications in oncology by years: a bibliometric \nanalysis from 1992 to 2022. Discov Onc. 2024;15:566. https:// doi. org/ 10. \n1007/ s12672- 024- 01415-0.\n 16.  Student Selection and Placement Centre. Turkish medical specialty \ntraining entrance exam questions. 2021. URL: https:// www. osym. gov. tr/ \nTR,15072/ tus- cikmis- sorul ar. html. \n 17.  Kalaycıoğlu DB. Gender-based differential item functioning analysis of the \nmedical specialization education entrance examination. J Meas Eval Educ \nPsychol. 2022;13(1):1–14. https:// doi. org/ 10. 21031/ epod. 998592.\n 18.  Aydoğan S, Keskin A. How much does the medical specialization exam \nmeasure basic medicine? Tıp Eğitimi Dünyası. 2022;21(64):42–9. https:// \ndoi. org/ 10. 25282/ ted. 10590 67.\nPage 14 of 14Koçak et al. BMC Medical Education          (2025) 25:609 \n 19.  Ozen Kutanis R, Tunc T, Tunç M. How do medical specialty training \neducators and trainees perceive medical specialty selection examination. \nKuram Ve Uygulamada Egitim Bilimleri. 2011;11(4):2001–4.\n 20.  Kilic ME. AI in medical education: a comparative analysis of GPT-4 and \nGPT-3.5 on Turkish medical specialization exam performance. medRxiv. \n2023. https:// doi. org/ 10. 1101/ 2023. 07. 12. 23292 564. \n 21.  Beaulieu-Jones BR, Shah S, Berrigan MT, Marwaha JS, Lai SL, Brat GA. \nEvaluating capabilities of large language models: performance of GPT4 \non American Board of Surgery Qualifying Exam Question Banks. medRxiv. \n2023. https:// doi. org/ 10. 1101/ 2023. 07. 16. 23292 743. \n 22.  Davies NP , Wilson R, Winder MS, Tunster SJ, McVicar K, Thakrar S, Williams J, \nReid A. ChatGPT sits the DFPH exam: large language model performance \nand potential to support public health learning. medRxiv. 2023. https:// \ndoi. org/ 10. 1101/ 2023. 07. 04. 23291 894. \nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Specialty",
  "concepts": [
    {
      "name": "Specialty",
      "score": 0.8780196905136108
    },
    {
      "name": "Turkish",
      "score": 0.7910604476928711
    },
    {
      "name": "Medical education",
      "score": 0.6681289672851562
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4234654903411865
    },
    {
      "name": "Psychology",
      "score": 0.39381247758865356
    },
    {
      "name": "Medicine",
      "score": 0.3479465842247009
    },
    {
      "name": "Family medicine",
      "score": 0.25615906715393066
    },
    {
      "name": "Geography",
      "score": 0.07229480147361755
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I188740104",
      "name": "Başkent University",
      "country": "TR"
    }
  ]
}