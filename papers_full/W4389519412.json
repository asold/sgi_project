{
  "title": "Document-Level Language Models for Machine Translation",
  "url": "https://openalex.org/W4389519412",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3213987298",
      "name": "Frithjof Petrick",
      "affiliations": [
        "RWTH Aachen University",
        "eBay (Ireland)"
      ]
    },
    {
      "id": "https://openalex.org/A1866541474",
      "name": "Christian Herold",
      "affiliations": [
        "RWTH Aachen University",
        "eBay (Ireland)"
      ]
    },
    {
      "id": "https://openalex.org/A2744879560",
      "name": "Pavel Petrushkov",
      "affiliations": [
        "eBay (Ireland)"
      ]
    },
    {
      "id": "https://openalex.org/A2074381943",
      "name": "Shahram Khadivi",
      "affiliations": [
        "eBay (Ireland)"
      ]
    },
    {
      "id": "https://openalex.org/A2090846850",
      "name": "Hermann Ney",
      "affiliations": [
        "RWTH Aachen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2977458338",
    "https://openalex.org/W2971347700",
    "https://openalex.org/W4386566711",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W4385569898",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W3046531489",
    "https://openalex.org/W2963693355",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2983590910",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2136630431",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2964289193",
    "https://openalex.org/W4287258879",
    "https://openalex.org/W2964291396",
    "https://openalex.org/W2962802109",
    "https://openalex.org/W2888159079",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2964120396",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2982644924",
    "https://openalex.org/W4367061074",
    "https://openalex.org/W4385573145",
    "https://openalex.org/W3134357720",
    "https://openalex.org/W2970845336",
    "https://openalex.org/W4385570335",
    "https://openalex.org/W4287637331",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W4362702134",
    "https://openalex.org/W2806412155",
    "https://openalex.org/W3176994339",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2982875764",
    "https://openalex.org/W2794365787",
    "https://openalex.org/W4362707106",
    "https://openalex.org/W4285121328",
    "https://openalex.org/W3013197936",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W22168010",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3042199843",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W3191852631",
    "https://openalex.org/W3175301726",
    "https://openalex.org/W3167690611",
    "https://openalex.org/W4317547647",
    "https://openalex.org/W2971134989",
    "https://openalex.org/W3030788518",
    "https://openalex.org/W2984500026",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2608029998",
    "https://openalex.org/W4385571793",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W3035016936",
    "https://openalex.org/W3099770676"
  ],
  "abstract": "Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores significantly and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the cost of having to re-train the translation system. Finally, we explore language model fusion in the light of recent advancements in large language models. Our findings suggest that there might be strong potential in utilizing large language models via model combination.",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 375–391\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n375\nDocument-Level Language Models for Machine Translation\nFrithjof Petrick1,2 Christian Herold1,2 Pavel Petrushkov1\nShahram Khadivi1 Hermann Ney2\n1eBay, Inc., Aachen, Germany\n{cherold, ppetrushkov, skhadivi}@ebay.com\n2Human Language Technology and Pattern Recognition Group\nRWTH Aachen University, Aachen, Germany\n{petrick, ney}@i6.informatik.rwth-aachen.de\nAbstract\nDespite the known limitations, most machine\ntranslation systems today still operate on the\nsentence-level. One reason for this is, that\nmost parallel training data is only sentence-\nlevel aligned, without document-level meta in-\nformation available. In this work, we set out to\nbuild context-aware translation systems utiliz-\ning document-level monolingual data instead.\nThis can be achieved by combining any ex-\nisting sentence-level translation model with a\ndocument-level language model. We improve\nexisting approaches by leveraging recent ad-\nvancements in model combination. Addition-\nally, we propose novel weighting techniques\nthat make the system combination more flexi-\nble and significantly reduce computational over-\nhead. In a comprehensive evaluation on four di-\nverse translation tasks, we show that our exten-\nsions improve document-targeted scores sub-\nstantially and are also computationally more\nefficient. However, we also find that in most\nscenarios, back-translation gives even better re-\nsults, at the cost of having to re-train the trans-\nlation system. Finally, we explore language\nmodel fusion in the light of recent advance-\nments in large language models. Our findings\nsuggest that there might be strong potential in\nutilizing large language models via model com-\nbination.\n1 Introduction\nMachine translation (MT), the automatic transla-\ntion of text from one language to another, has seen\nsignificant advancements in recent years, primarily\ndriven by neural machine translation (NMT) mod-\nels (Bahdanau et al., 2015; Vaswani et al., 2017).\nThese models have demonstrated remarkable ca-\npabilities in capturing complex linguistic patterns\nand producing high-quality translations (Wu et al.,\n2016; Hassan et al., 2018). Nevertheless, most\nmodels to-date operate on sentence-level, i.e. trans-\nlate sentences independently without the context\nof the surrounding document. Without access to\nsuch context, it is impossible for these MT systems\nto account for discourse-level phenomena such as\nresolution of ambiguous words and coherence. Un-\nsurprisingly, automatic translations are perceived\nas much worse, when they are evaluated on entire\ndocuments rather than just at the sentence-level\n(Läubli et al., 2018, 2020; Maruf et al., 2022).\nAn obvious solution to this problem is to uti-\nlize context-aware MT models (Tiedemann and\nScherrer, 2017). While document-level NMT mod-\nels have been thoroughly studied in recent years,\nsentence-level MT remains the standard despite its\ninherent limitations. One of the main reasons for\nthis is that most of the document-level approaches\nrely on parallel training data with document-level\nmetadata. Most releases of large parallel training\ncorpora lack this information and remain purely\nsentence-level (Bañón et al., 2020; Schwenk et al.,\n2021). In contrast, large amounts of document-\nlevel monolingual data are readily available for\nalmost all domains and languages.\nIn this work, we strive to build a context-aware\nMT system that does not rely on any parallel\ndocument-level training data. Instead, we use\nmonolingual documents to train a document-level\nlanguage model (LM), which we fuse with an ex-\nisting sentence-level MT model during translation.\nWhile existing work on LM fusion shows that the\nfused model is able to incorporate document-level\ncontext (Jean and Cho, 2020; Sugiyama and Yoshi-\nnaga, 2021), these approaches can be improved.\nOur work aims to do so in two main directions.\nFirst, we acknowledge that NMT models im-\nplicitly learn the language modeling task during\ntraining. Recently, Herold et al. (2023) showed\nthat estimating and neutralizing this internal LM\ncan improve translation quality for sentence-level\nMT. We adapt their approach to document-level\nLM fusion and demonstrate that this also improves\ndiscourse modeling.\nSecond, the contribution of the fused MT model,\n376\nthe document-level LM and the internal LM must\nbe balanced by a set of fusion scales. Existing work\ndefines the fusion scales as static hyperparameters\nwhich are tuned on a validation set via an exten-\nsive grid search (Gülçehre et al., 2015; Jean and\nCho, 2020; Sugiyama and Yoshinaga, 2021). In\nour work, we provide two simple alternatives to\ngrid search which allow for automatically tuned\ncontext-dependent fusion scales. Our approaches\neliminate the need for expensive tuning and further\nimprove discourse-modelling.\nThe contributions of this work are as follows:\n1. We propose multiple extensions to the existing\napproaches on document-level LM fusion for\nMT.\n2. We compare our methods against two strong\nbaselines: Back-translation, the to-date most\npopular way to utilize monolingual data for\nMT, and a task-specific LM re-ranking base-\nline for pronoun disambiguation. The compar-\nison takes place over four diverse translation\ntasks in terms of general translation quality\nas well as specific context-dependant phenom-\nena.\n3. We present first results on fusing a large lan-\nguage model (LLM) with a sentence-level MT\nsystem.\n2 Related Works\nMost works on document-level NMT rely on paral-\nlel document-level data for system training.\nTiedemann and Scherrer (2017) propose to con-\ncatenate adjacent sentences on source and target\nside and input this into the NMT model which has\nthe exact same architecture as the vanilla sentence-\nlevel transformer (Vaswani et al., 2017). Later,\nmany works have proposed modifications to the\narchitecture to better accommodate the additional\ncontext (Jean et al., 2017; Bawden et al., 2018;\nZhang et al., 2018; V oita et al., 2018; Kuang and\nXiong, 2018; Miculicich et al., 2018; Maruf and\nHaffari, 2018). However, it has been shown that the\nsimple concatenation approach performs as good,\nif not better than these more complicated variants\n(Lopes et al., 2020; Sun et al., 2022).\nMaybe the biggest challenge for document-level\nNMT is that most of the parallel MT training data\nis not document-level (Esplà-Gomis et al., 2019;\nSchwenk et al., 2021). Recently there has been\nsome effort to restore document-level meta infor-\nmation from existing sentence-level corpora but\nthis is a very time consuming and error-prone pro-\ncess (Ghussin et al., 2023). Therefore, approaches\nto document-level NMT have been proposed that\nutilize document-level monolingual data, of which\ntypically large amounts are readily available.\nOne direction is to back-translate the document-\nlevel monolingual data to create synthetic paral-\nlel document-level data. The reverse system used\nfor back-translation can be either sentence-level\n(Junczys-Dowmunt, 2019; Saleh et al., 2019; Post\nand Junczys-Dowmunt, 2023) or document-level\n(Sugiyama and Yoshinaga, 2019; Huo et al., 2020).\nA downside of this approach is that the final MT\nsystem has to be re-trained to incorporate the new\nsynthetic data.\nAnother line of work uses document-level lan-\nguage models in combination with sentence-level\ntranslation models. Gülçehre et al. (2015) were\nthe first to propose a log-linear combination of\nsentence-level language and NMT models, coining\nthe term ‘shallow fusion’. Recently, it was shown\nthat the shallow fusion approach for sentence-\nlevel NMT can be improved by compensating for\nthe implicitly learned internal language model of\nthe NMT system (Herold et al., 2023). Regard-\ning the integration of a document-level LM, ear-\nlier approaches simply use the LM for re-ranking\nthe hypothesis of the sentence-level NMT model\n(Stahlberg et al., 2019; Yu et al., 2020). Several\nworks have proposed to employ a log-linear com-\nbination between sentence-level NMT system and\ndocument-level LM (Garcia et al., 2019; Jean and\nCho, 2020; Sugiyama and Yoshinaga, 2020). Both\nJean and Cho (2020) and Sugiyama and Yoshinaga\n(2020) propose to also include the probabilities of\nthe LM without context information in order to mit-\nigate the influence of the current sentence on the\nLM probabilities. While our approach also uses the\noutput of a sentence-level LM, it is conceptually\ndifferent from the previous works in that we want\nto mitigate the influence of the internal LM from\nthe NMT model, resulting in a different final formu-\nlation. To further improve LM incorporation, Jean\nand Cho (2020) propose to use subword-dependent\nfusion scales instead of a single scale per model.\nApart from back-translation and LM integration\nthere exist some other ways to utilize additional\nmonolingual document-level data for MT. V oita\net al. (2019) train a document-level automatic post\n377\nediting system on the monolingual data and use it\nto improve the hypotheses from a sentence-level\nNMT system in a two-pass approach. Several\nworks utilize the additional data in a multi-task\nlearning approach (Junczys-Dowmunt, 2019) or for\npre-training (Zhu et al., 2020; Chen et al., 2021b;\nLiu et al., 2020; Chen et al., 2021a).\nVery recently, LLMs have shown their potential\nfor the task of document-level NMT (Wang et al.,\n2023). However, it is unclear how much parallel\ntraining samples were seen during the large scale\npre-training on trillions of tokens.\n3 Document-level Language Model\nFusion\nThe sentence-level MT model translates a source\nsentence F into a target sentence E := eI\n0 of sub-\nwords ei. In the document-level LM fusion ap-\nproach, we additionally provide the k previous\ntarget-side sentences E−1\n−k as context1.\n3.1 Internal Language Model Neutralization\nAs the translation model already implicitly learns\nprobabilities that are source-independent, directly\nfusing the MT model and the document-level LM\novervalues the source-agnostic probabilities. There-\nfore, we estimate the internal LM of the MT model\nand in total combine three models during genera-\ntion:\n• the existing sentence-level MT model\npTM(ei) := pTM(ei |ei−1\n0 , F),\n• the LM pLM(ei) := pLM(ei | ei−1\n0 , E−1\n−k)\ntrained on monolingual documents with ac-\ncess to the previous target sentences E−1\n−k,\n• and a second LM pILM(ei) := pILM(ei |ei−1\n0 )\nwhich estimates the internal LM probabili-\nties implicitly learned by the MT model. We\ntrain this LM separately on the target-side of\nthe MT training data, as we found that this\napproach works best for document-level MT\nwhen compared to other approaches presented\nby Herold et al. (2023). This comparison can\nbe found in Appendix A.3.\nWe multiply the model output probabilities and nor-\nmalize them. The resulting probability distribution\n1 At the beginning of the document we only provide as\nmany sentences as available.\nis now conditioned on both the source sentence F\nand the target-side context E−1\n−k:\np(ei) := p(ei |ei−1\n0 , F, E−1\n−k)\n:= pλ0\nTM(ei) · pλ1\nLM(ei) · p−λ2\nILM (ei)P\ne′ pλ0\nTM(e′) · pλ1\nLM(e′) · p−λ2\nILM (e′)\n. (1)\nEach model is weighted with a scalar λ0, λ1, λ2 ≥\n0, the internal LM is included with a negative\nexponent. We tune these fusion scales on the\nvalidation set for BLEU via a grid search over\nλ0, λ1, λ2 ∈ {0, 0.1, . . . ,1}.\nExisting work on document-level LM fusion\nuses a similar formulation as our approach, but\ninstead of neutralizing the internal LM of the MT\nmodel, it accounts for the sentence-level probabili-\nties pLM(ei |ei−1\n0 ) of the document-level LM (Jean\nand Cho, 2020; Sugiyama and Yoshinaga, 2021).\nIn the particular case where there are no previous\nsentences available, this approach simply falls back\nto using only the sentence-level MT model prob-\nabilities. Our approach on the contrary can also\nleverage the gains obtained from sentence-level\nLM fusion and is theoretically more expressive.\n3.2 Context-dependent Fusion Scales\nChoosing appropriate fusion scales λ0, λ1, λ2 in\nEquation 1 is crucial. Conventionally, the scales\nare tuned via grid search. This is problematic in\nthree aspects:\n1. Grid search is expensive. Testing e.g. ten pos-\nsible values for each of the three model scales\nalready requires translating the validation set\n1000 times.\n2. The tuning process depends on the tuning data,\nits domain and the tuning objective. E.g.,\nthe scales that optimize document-targeted\nmetrics differ from the ones that maximize\nsentence-level translation quality (Sugiyama\nand Yoshinaga, 2021).\n3. Fusion scales obtained by a hyperparameter\ngrid search must be constant. Document-level\ncontext however is not uniformly useful for\nall predicted subwords.\nIn the following, we propose two simple alterna-\ntives to obtaining fusion scales with grid search\nthat overcome the aforementioned issues.\n378\n3.2.1 On-the-fly Fusion Scales\nDuring decoding, the next subword ei is chosen\nto maximize the fused probability (Equation 1).\nWe propose to also choose the fusion scales in a\nsimilar fashion and define them to maximize the\nfused model scores:\n(λ0, λ1, λ2) :=\nargmax\n(λ0,λ1,λ2)\npλ0\nTM(ei) · pλ1\nLM(ei) · p−λ2\nILM (ei)P\ne′ pλ0\nTM(e′) · pλ1\nLM(e′) · p−λ2\nILM (e′)\n. (2)\nOur model maximizes over the discrete set\nλ0, λ1, λ2 ∈ {0, 0.1, . . . ,1}. This approach ob-\nviates the need for separate scale tuning entirely\nand only has a small overhead during generation.\n3.2.2 Automatically Learned Fusion Scales\nAlternatively, we propose to learn the fusion scales\nautomatically using a small amount of training\nexamples (F, E, E−1\n−k) with document-level con-\ntext, similarly to Jean and Cho (2020). We obtain\nthe training data by back-translating the monolin-\ngual data (see Section 5). Automatic learning al-\nlows us to implement subword-dependent fusion\nscales: We introduce a set of learnable parameters\nλ0(e), λ1(e), λ2(e) for each subword e from the\ntarget vocabulary and learn them automatically by\noptimizing the cross-entropy loss\n(λ0, λ1, λ2) := argmax\nλ: V →R3\nX\n(F,E,E−1\n−k)\nX\ni\nlog pλ0(ei)\nTM (ei) · pλ1(ei)\nLM (ei) · p−λ2(ei)\nILM (ei)\nP\ne′ pλ0(e′)\nTM (e′) · pλ1(e′)\nLM (e′) · p−λ2(e′)\nILM (e′)\n.\n(3)\nScale learning uses the same optimization parame-\nters as the MT model was originally trained with.\nThe scale parameters are initialized with a small\nvariance around zero while all other parameters are\nfrozen.\n4 Document-level Language Model\nPronoun Re-ranking\nBesides consistency, the main problem of\ndiscourse-modelling are ambiguities. E.g. trans-\nlating the English pronoun ‘it’ to German requires\naccess to the noun that it refers to, which might\nonly be found in a preceding sentence (Müller et al.,\n2019).\nWe propose an approach specific to the En→De\nlanguage pair that directly targets the pronoun\ntranslation problem by re-ranking sentence-level\nhypotheses using a document-level LM. We first\ntranslate each sentence independently using the\nsentence-level MT model. Each sentence-level\ntranslation is expanded to a set of candidates by\nreplacing the pronouns with all alternatives (‘er’,\n‘sie’, ‘es’). All candidate translations are then\nscored in context of the preceding sentences using\na document-level LM, and we select the pronoun\nfor which the LM score is highest.\nThis approach is very much tailored to the spe-\ncific pronoun translation problem for this specific\nlanguage pair. While it is theoretically possible to\nextend this approach to cover more cases, this will\nrequire extensive human effort and is probably not\nfeasible in most scenarios. However, we include it\nhere, because it serves as a reasonable baseline for\nthis popular pronoun translation benchmark.\n5 Document-level Back-translation\nThe to-date most popular way of utilizing mono-\nlingual data for MT is to create synthetic parallel\ntraining data via back-translation (Sennrich et al.,\n2016). We train a sentence-level backwards MT\nsystem on the parallel data and use it to translate\nthe document-level monolingual data back into the\nsource language. The sentence-level translations\nare concatenated to obtain synthetic parallel docu-\nments (Junczys-Dowmunt, 2019; Saleh et al., 2019;\nSugiyama and Yoshinaga, 2019; Huo et al., 2020;\nPost and Junczys-Dowmunt, 2023).\nTo train the final systems we combine the au-\nthentic sentence-level parallel and the synthetic\ndocument-level data. Combining both data sources\nis not straightforward, because of their varying\nsize and the difference between sentence/document-\nlevel context. Therefore, we first oversample the\ndata accordingly to have roughly the same number\nof sentences in both parts. Secondly, we turn the\nauthentic sentence-level parallel data into ‘pseudo-\ndocuments’ by concatenating them in a random\norder (Junczys-Dowmunt, 2019; Jean et al., 2019).\nThis ensures that all training data has the same con-\ntext size. We found this procedure to perform best\nwhen incorporating synthetic document-level data.\nFor a detailed comparison, see Appendix A.5.\n6 Experiments\n6.1 Tasks\nWe evaluate our approaches on four different tasks\nof varying data conditions and domains. Three\n379\ntasks are on publicly available data and a fourth\ntask is based on a large scale internal dataset in the\ne-Commerce domain. All tasks include (sentence-\nlevel) parallel training data and document-level\nmonolingual data from the same domain. The exact\ndata conditions are provided in Appendix A.1.\nThe News En→De data consists of news articles\nwhile the TED En→It task consists of scientific\ntalks. Both are low resource with less than 1M train-\ning samples in total. The Subtitles En→De data\nconsists of subtitles from various TV shows and\nis medium size. Finally, the e-Commerce En→De\ntask is about translating item descriptions from e-\nCommerce listings and the training data is large\nscale with more than 100M examples.\nWhile the parallel training data for the three aca-\ndemic tasks does provide document-level metadata,\nour approaches do not make use of this informa-\ntion and we assume that the parallel training data\nis sentence-level for most experiments. We only\nmake use of this information to provide a direct\ncomparison against the setting where document-\nlevel parallel data is assumed to be available. As\nParaCrawl, like most other large-scale web-crawled\nparallel datasets, is not a document-level corpus,\nwe can not conduct these experiments for the e-\nCommerce task.\nWe preprocess each corpus with byte-pair encod-\nings (Sennrich et al., 2016) using the SentencePiece\ntoolkit (Kudo, 2018) learned on the parallel dataset\nwith a shared vocabulary of 32k subwords (13.6k\nfor TED). For the e-Commerce task we additionally\nuse inline casing (Berard et al., 2019; Etchegoyhen\nand Gete, 2020).\n6.2 Settings\nWe train transformer MT models in the ‘base’ con-\nfiguration (Vaswani et al., 2017), implemented in\nFairseq (Ott et al., 2019). For the LMs we use a\nsimilar architecture but without the encoder. Our\ndocument-level models use the same architecture as\nthe sentence-level models, we simply include con-\ntext sentences by concatenating the previous two\nsource and target sentences to all training examples,\nseparated by a reserved symbol (Tiedemann and\nScherrer, 2017).\nDetails on the optimization algorithm are given\nin Appendix A.1. The final model is selected based\non the validation set perplexity. We then perform\nbeam search with beam size 12 and length normal-\nization. Document-level decoding uses the ‘last\nsentence’ search strategy as described in Herold\nand Ney (2023b).\nThe document-level LMs are trained on a combi-\nnation of target-side of the sentence-level parallel\nand document-level monolingual data. Regardless\nof the task, we train the LMs for 300k update steps\nwith batch size 90k, 10 % dropout, and 10 % label\nsmoothing.\nFor the LM fusion experiments with non-static\nfusion scales, we restrict the search space to only\nconsider scale combinations where λ0 = 1 and\nλ1 = λ2. A direction comparison is given in Ap-\npendix A.4. For back-translation, we use beam\nsearch with beam size 4 and increase the training\ntime proportionally to the new data size.\n6.3 Evaluation\nDocument-level evaluation is challenging, as in-\ntersentential context usually is only relevant for a\nsmall fraction of words. Further, conventional met-\nrics like BLEU (Papineni et al., 2002) or COMET\n(Rei et al., 2020) do not appropriately measure how\nwell document-level context is considered for those\nwords where context does matter (Läubli et al.,\n2018, 2020; Maruf et al., 2022). However, we\nstill report BLEU using Sacrebleu (Post, 2018) and\nCOMET 2 on the task-specific in-domain test sets to\nevaluate the general MT quality.\nTo better evaluate the improvements from the\ndocument-level approaches, we focus on selected\nsentences for which document-level context is\nknown to be important. Here, we report on two\ntest sets focusing on ambiguities. The En→De pro-\nnouns test set released by Müller et al. (2018) was\ncurated from OpenSubtitles shows and contains\n12k examples. Most examples require previous\nsentences as context to properly translate the En-\nglish pronoun ‘it’ with German ‘er’, ‘sie’ or ‘es’.\nFurther, the gender-referring professions test sets\nreleased as contextual part of MT-GenEval (Currey\net al., 2022) are available for various target lan-\nguages and focus on a wider range of ambiguous\nwords, e.g. whether ‘the teacher’ should be trans-\nlated with ‘die Lehrerin’ or ‘der Lehrer’ in German.\nAgain, context from the previous sentences is re-\nquired to determine the correct translation. We use\nthese test sets for En→De and En→It which both\ncomprise approx. 1.1k examples that were created\nby translating Wikipedia articles.\nComputing BLEU and COMET on these chal-\n2 Using the wmt22-comet-da model (Rei et al., 2020)\n380\nlenge test sets better reflects how well a MT system\nhandles document-level context. An even more\nspecific metric can be obtained by focusing only on\nthe ambiguous words. Previous work commonly\nreports an accuracy metric that is based on con-\ntrastive scoring, which is computed by comparing\nthe model probabilities of the reference against a\nset of contrastive examples (Müller et al., 2018).\nThis metric however can be misleading, as it not\nbased on the generated translation but rather just on\nscoring. MT systems with high contrastive scores\noften perform poorly when their generated hypothe-\nsis is evaluated (Post and Junczys-Dowmunt, 2023).\nInstead, we focus on translation-based document-\ntargeted metrics.\nOn the pronouns test set, we compute a pronoun\nF1-score as proposed by Herold and Ney (2023a).\nThis metric directly compares the pronouns of the\nhypothesis and the reference and is based on the\nBLON DE metric (Jiang et al., 2022). On the pro-\nfessions test set, we report the translation-based\naccuracy metric suggested by their curators (Cur-\nrey et al., 2022). Further, for the Subtitles system\nwe also report a formality F1-score on its test set\nas proposed by Herold and Ney (2023a).\n6.4 Results\nWe evaluate our approaches to utilize monolingual\ndocument-level data on the four MT tasks. We ap-\nply them in two settings where a) we assume that all\nparallel data is purely sentence-level, and b) also\nthe parallel data is document-level.\nIn an effort to compare to previous work, we\nre-implement LM fusion with static scales with-\nout subtracting the internal LM which was inde-\npendently proposed by Jean and Cho (2020) and\nSugiyama and Yoshinaga (2021). These works sub-\ntract the intersentential probabilities of the external\nLM instead. Further, we also re-implement the\nnon-static scales predicted with a ‘merging mod-\nule’ learned on parallel document-level data as pro-\nposed by Jean and Cho (2020).\nWe first evaluate our approaches on conventional\nmetrics to measure their general MT performance.\nThen, we focus on the document-targeted challenge\nsets to quantify how well they utilize document-\nlevel context.\n6.4.1 Conventional Metrics\nWe start by evaluating on the in-domain test sets\nof the four MT tasks using the conventional MT\nmetrics. Here, we do not expect to see much im-\nprovements coming from the document-level con-\ntext. The results are presented in Table 1.\nAdding monolingual data gives the largest im-\nprovements on News and small improvements on\nthe e-Commerce task. On these two tasks, the\nmonolingual data is in-domain and the improve-\nments are likely because of the domain. On Subti-\ntles and TED we do not see any improvements as\nSubtitles already has a large amount of in-domain\nparallel data and the TED monolingual data is\nslightly out-of-domain. We verified the domain\neffect by training sentence-level LMs on equal\namounts of data from the target-side of the par-\nallel and monolingual corpora and comparing their\nperplexities on the test sets. Details are provided in\nAppendix A.2.\nNone of the presented approaches significantly\ndecreases translation performance in terms of con-\nventional metrics. The only exception is the back-\ntranslation which when added to the Subtitles and\nTED document-level baseline performs worse in\nBLEU . In COMET however, this decrease is less\nprevalent.\n6.4.2 Document-targeted Metrics\nThe results on the document-targeted test sets are\nshown in Table 2. First we discuss the scenario\nwithout access to document-level parallel training\ndata.\nLM fusion. Adding monolingual documents\nto the sentence-level baseline with the exist-\ning approaches from Jean and Cho (2020) and\nSugiyama and Yoshinaga (2021) improves scores\nonly marginally by on average +0.5 % absolute\nF1 score on the pronouns test set and no improve-\nments on the professions set. In comparison, our\napproach on LM fusion with the neutralization of\nthe internal LM performs better: E.g., the vari-\nant with on-the-fly scales on average improves the\npronoun F1 score by +2.4 % and the professions ac-\n3 External baseline by Herold and Ney (2023b)\n4 External baseline by Huo et al. (2020)\n5 Re-implementation of LM fusion with neutralization of\nthe intersentential LM probabilities instead of the internal\nLM, as introduced by Jean and Cho (2020) and Sugiyama and\nYoshinaga (2021)\n6 Re-implementation of the ‘merging module’ approach by\nJean and Cho (2020). This approach uses parallel document-\nlevel data for scale learning.\n381\nData Method News Subtitles TED e-Commerce\nparallel mono. BLEU COMET BLEU COMET BLEU COMET BLEU COMET\nsent.\n- baseline (prev. work) 32.83 - 37.34 - 34.23 - - -\nbaseline (ours) 32.7 82.8 37.3 87.9 34.8 86.1 36.4 89.2\ndoc.\n(Jean, 2020; Sugiyama, 2021)5 33.1 83.2 37.2 87.8 34.6 86.2 37.1 89.6\n(Jean, 2020)6 32.9 83.0 37.3 87.9 34.5 86.2 36.6 89.2\nLM: static 34.8 84.2 37.2 87.8 34.9 86.2 37.3 89.6\nLM: on-the-fly 34.7 83.9 37.2 87.9 34.9 86.2 36.8 89.7\nLM: auto. learned 34.4 83.8 37.4 87.8 34.7 86.2 36.8 89.0\nLM: re-rank pronouns 32.6 82.7 36.9 87.8 n.a. 36.4 89.2\nback-translation 37.1 85.2 37.2 87.6 35.1 86.6 36.2 89.3\n+ LM: static 37.4 85.6 37.6 87.7 35.2 86.6 35.0 88.9\n+ LM: on-the-fly 37.2 85.4 37.1 87.6 34.8 86.6 35.9 89.4\n+ LM: auto. learned 37.2 85.3 37.3 87.6 34.9 86.6 36.2 89.5\ndoc.\n- baseline 32.5 82.9 39.5 88.2 35.4 86.5\nn.a.doc.\nLM: static 35.1 84.3 38.9 88.2 35.2 86.7\nLM: on-the-fly 34.5 84.1 39.0 88.0 35.1 86.7\nLM: auto. learned 34.8 84.1 39.3 88.2 35.2 86.6\nLM: re-rank pronouns 32.3 82.8 39.1 88.1 n.a.\nback-translation 37.2 85.3 37.5 87.8 34.6 86.5\nTable 1: Utilizing document-level monolingual data using different methods, reporting on the in-domain test sets of\neach task. B LEU and COMET are given in percentage. Best results for each column are highlighted.\nData Method News Subtitles TED e-Commerce\nparallel mono. pron. proff. pron. proff. form. proff. pron. proff.\nsent.\n- baseline (prev. work) 45.33 - 41.13 - 59.4 3 - - -\nbaseline (ours) 45.1 65.9 41.7 65.3 57.2 65.4 42.6 63.7\ndoc.\n(Jean, 2020; Sugiyama, 2021)5 46.0 65.0 42.3 65.8 58.1 65.1 42.7 64.0\n(Jean, 2020)6 45.1 64.7 41.9 65.8 57.7 65.4 42.5 63.5\nLM: static 45.5 65.5 42.5 66.3 58.4 65.4 42.8 64.4\nLM: on-the-fly 48.0 65.5 44.2 65.9 58.9 66.4 44.4 66.2\nLM: auto. learned 46.7 64.9 42.8 65.5 58.6 65.6 44.0 65.2\nLM: re-rank pronouns 48.0 66.1 57.5 65.5 57.2 n.a. 54.5 64.0\nback-translation 48.7 80.5 52.3 67.0 58.5 65.1 42.9 67.1\n+ LM: static 48.5 80.6 53.1 68.3 53.8 65.4 42.6 66.0\n+ LM: on-the-fly 48.9 81.3 52.8 67.3 60.4 65.4 46.3 70.5\n+ LM: auto. learned 48.9 80.5 52.0 67.6 59.9 65.4 46.2 65.7\ndoc.\n- baseline 55.9 71.2 67.2 70.8 61.9 67.2\nn.a.doc.\nLM: static 55.3 70.8 67.5 71.1 61.5 66.8\nLM: on-the-fly 55.8 72.3 67.8 71.9 61.4 67.6\nLM: auto. learned 55.7 71.5 67.4 71.0 61.6 67.6\nLM: re-rank pronouns 50.9 71.5 62.6 70.8 61.9 n.a.\nback-translation 52.1 79.4 62.8 67.3 62.0 65.7\nTable 2: Document-targeted evaluation of the different approaches utilizing document-level monolingual data. We\nreport the pronoun F1 score (Herold and Ney, 2023a), gender-referring professions accuracy (Currey et al., 2022)\nand the formality F1 score on the Subtitles test set (Herold and Ney, 2023a), all given in percentage. Best results for\neach column are highlighted.\n382\ncuracy by +0.9 %. Compared to static scales, both\non-the-fly and automatically learned scales yield\nsmall improvements and further do not involve the\nexpensive grid search.\nLM re-ranking pronouns. Our LM re-ranking\napproach was specifically tailored towards the pro-\nnouns test set. We see most improvements on\nthis test set, while the document-targeted metrics\non the other test sets remain mostly unchanged.\nFor both the Subtitles and the e-Commerce task,\nLM re-ranking is the best approach of utilizing\ndocument-level monolingual data for this specific\ntest set in the absence of document-level parallel\ndata. On News however, the gains are less preva-\nlent: Our analysis finds that even though the LM\nin this case can predict the pronouns correctly, the\ngeneral translation quality of the baseline on this\ntest set is low and therefore this model often fails to\ngenerate any pronouns at all. This again highlights\nthe discrepancy between scoring- and generation-\nbased metrics.\nBack-translation. In a direct comparison to LM\nfusion, back-translation outperforms LM fusion\ndespite our improvements over the existing work.\nBack-translation on average improves the pronouns\nF1 score by +4.8 % and the professions accu-\nracy by +4.9 % over the sentence-level baseline.\nThis may also highlight the importance of source-\nside document-level context as the LM based ap-\nproaches do not have access to this. Still, both back-\ntranslation and LM fusion can be combined and this\nyields further improvements: The best performing\napproach not relying on document-level parallel\ndata is to use both document-level back-translation\nand then LM fusion with on-the-fly scales, this\nmethod achieves on average +6.2 % F1 score on\nthe pronouns and +6.0 % professions accuracy.\nParallel document-level data. The three base-\nlines trained on parallel document-level data per-\nform much better than the sentence-level base-\nline: The document-level baselines score on av-\nerage +18.0 % better on the pronouns F1 score\nand +4.2 % better on the professions accuracy than\ntheir sentence-level counterparts. In addition, the\nsystems trained on parallel documents also per-\nform better than the sentence-level systems with\nadditional monolingual documents in almost all\ncases. This concludes that on these three tasks,\nhaving access to parallel document-level data is\nmuch more effective than utilizing monolingual\ndocument-level data, even though our monolingual\nMethod contrastive pronoun acc.\nNews Subtitles e-Comm.\nsentence-level baseline 49.0 46.4 46.1\n(Jean, 2020; Sugiyama, 2021)5 53.4 48.8 47.4\n(Jean, 2020)6 49.2 46.8 45.5\nLM: static 55.2 49.5 48.5\nLM: on-the-fly 55.9 53.4 51.3\nLM: auto. learned 53.0 50.1 50.5\nLM: re-rank pronouns 65.7 73.9 64.8\nback-translation 56.5 57.9 47.3\n+ LM: static 57.7 61.6 47.5\n+ LM: on-the-fly 57.9 61.1 54.3\n+ LM: auto. learned 56.7 59.0 54.1\ndocument-level baseline 67.9 84.0 n.a.\nTable 3: Scoring-based, contrastive accuracies on the\npronouns test set (Müller et al., 2018) for the three\nEn→De tasks, reported in percent.\ncorpora are much larger than the parallel ones.\nFurther including monolingual document-level\ndata to the document-level baselines does not gen-\nerally give additional improvements. In particular,\nLM pronoun re-ranking decreases performance in\nthis setting as the MT model itself is already bet-\nter at predicting the correct pronoun than the LM\ntrained on the document-level monolingual data.\nContrastive scores. Previous work on document-\nlevel MT commonly evaluates document-level MT\nsystems using contrastive scoring (e.g., Jean and\nCho, 2020; Sugiyama and Yoshinaga, 2021). As a\ndirect comparison, we report the contrastive accura-\ncies on the pronouns test set in Table 3. The trend\nis often similar to the translation-based metrics\nin Table 2, however scoring-based improvements\nare much more pronounced. Our experiments also\nshow that strong contrastive accuracies do not nec-\nessarily lead to improvements on the generated\nhypothesis. For example, on the News task, the\ncontrastive scores of the LM pronoun re-ranking\napproach and the document-level baseline are simi-\nlar but their translation-based scores differ strongly\n(c.f. Table 2).\n6.4.3 Computational Cost\nWe have shown that both the on-the-fly scales and\nthe automatically learned scales improve document-\ntargeted scores over static scores obtained via grid\nsearch. Another downside of grid search is that the\ntuning process is quite expensive. In Table 4, we\nillustrate that a grid search with 113 parameters (as\nis used in this work) on a single GPU can easily\ntake multiple days. The on-the-fly scales do not\n383\nMethod Time\nPreparation Search\nLM: static 7187 min 5.4 min\nLM: on-the-fly 0 min 6.5 min\nLM: auto. learned 8.3 min 5.4 min\nTable 4: Total time necessary to tune different fusion\nscale variants on a single GPU, as well as the time\nspent during translation. We measure the time used to\ntranslate the News validation set.\nLM perplexity contrastive acc.\nnews e-comm. pron. proff.\nNewsCrawl 17.0 44.5 62.8 63.4\nLLaMA 9.2 11.8 80.0 62.3\nTable 5: Comparing the small in-domain LM trained on\nNewsCrawl against the LLM LLaMA.\nrequire any preparation time as they are obtained\nentirely during search, in which the overhead is\nsmall. The automatically scales on the other hand\ncan be learned in just a few minutes and do not\nhave any overhead in decoding.\n6.4.4 Large Language Model Integration\nRecently, large language models (LLMs) which\nare trained on large corpora and long context sizes\nreceived a lot of attention (e.g., Brown et al., 2020;\nTouvron et al., 2023). In particular, they have\nalso been able to perform document-level MT\n(Zhang et al., 2023; Hendy et al., 2023; Karpin-\nska and Iyyer, 2023; Wang et al., 2023). This raises\nthe natural question whether LLMs can improve\ndocument-level LM fusion.\nWe experiment on the News task and com-\npare our own small LM with 35M parameters\ntrained on 2.2B tokens from the in-domain German\nNewsCrawl corpus against the 13B parameter ver-\nsion of LLaMA (Touvron et al., 2023), which was\ntrained on a total of 1000B tokens. LLaMA’s train-\ning data includes various domains and languages.\nOnly a small fraction of its data is German. The\nsmall LM provides two sentences context while\nwe query the LLM with 200 tokens context. We\nre-train our MT model and the small LM using the\nLLaMA tokenizer. This leads to slightly worse per-\nformance compared to our previous experiments as\nthe LLaMA tokenization was learned on general-\ndomain English data. For decoding we use a beam\nsize of 4.\nTable 5 shows the perplexities of both LMs and\ntheir contrastive scores on the document-targeted\nLM Fusion news e-Commerce\nLM Scales BLEU COMET BLEU COMET\n(none) - 31.2 81.3 13.6 70.5\nNewsCrawl static 33.2 83.0 14.4 72.5\non-the-fly 33.2 82.8 14.3 72.2\nLLaMA static 34.6 84.2 16.5 75.0\non-the-fly 33.4 83.9 13.7 72.9\nTable 6: Comparing fusion with a small LM and a LLM\non general test sets.\ntest sets7. Both LMs use the same vocabulary and\nthus their perplexities are comparable. Because it\nis in general unclear whether test sets are or are not\nincluded in LLM training data, we also include the\ne-Commerce test set which was translated by our-\nselves for the purpose of cross-validation. On both\ntest sets, the LLM perplexities are much better than\nthe ones of the small in-domain LM. LLaMA’s con-\ntrastive scores are also much better on the pronouns\ntest set.\nTable 6 shows the performance of LM fusion\nwith the two LMs in BLEU and COMET . Both\nLMs notably improve translation, but the LLM\ntranslation quality is best. Fusion with LLaMA\nyields +3.4 % absolute improvements on the in-\ndomain test set. Improvements on the e-Commerce\ntest set are similar, indicating that the gains are\nnot an effect of data leakage of the test set into the\ntraining data. While the on-the-fly scales and the\nstatic scales perform similarly for the small LM,\non-the-fly scales do not perform as well for the\nLLM.\nThe improvements measured on the in-domain\ntest sets are likely not because of document-level\ncontext but rather due to the increased amount of\ndata. Therefore, we continue our evaluation with\nthe document-targeted scores. Table 8 depicts the\nresults. On these metrics, the LLM outperforms\nthe small LM by an even larger margin. In general,\nthe improvements are correlated to their contrastive\nscores (c.f. Table 5).\n6.5 Extended Analysis on Automatically\nLearned Fusion Scales\nIn our experiments we use the validation set of the\nNews task to find the best working methods. We\nshare some insights in the following.\nHow are the automatically learned scales dis-\ntributed? Figure 1 shows the distribution of the au-\n7 The professions test set was released without target-side\ncontext, which we therefore created ourselves by translating\nthe source-side context with a commercial MT system.\n384\nFusion Scales Learning λ valid set doc.-targeted\nScales Crit. Train Set BLEU COMET pron. proff.\nnone - - 0.0 24.5 80.9 45.1 65.9\nsubword-\nagnostic\ngrid search valid set 0.40 25.4 81.8 46.5 65.1\nCE valid set 0.34 25.5 81.7 46.4 65.0\nsynthetic 0.46 25.3 81.6 47.1 65.2\nsubword-\ndependent CE valid set - 26.6 81.8 46.1 65.0\nsynthetic - 25.4 81.6 46.9 65.4\nTable 7: Automatically learning subword-dependent and -agnostic fusion scales on the News task. We employ the\nrestriction λ0 := 1, λ := λ1 = λ2.\nLM Fusion document-targeted\nLM Scales pron. proff. form.\n(none) - 44.5 65.7 33.4\nNewsCrawl static 46.3 66.3 34.7\non-the-fly 47.4 66.7 34.3\nLLaMA static 51.6 66.9 36.1\non-the-fly 48.2 68.9 35.2\nTable 8: Fusion with a small LM against a LLM, re-\nporting the translation-based scores on the document-\ntargeted test sets.\ntomatically learned scales for the News task. The\nlearned LM scale of subwords that continue an-\nother subword are in general higher than the ones\nthat begin a new word. This is intuitive as contin-\nuing a subword is an LM task while beginning a\nnew word requires information about the source\nsentence.\nHow much data is needed for automatically\nlearning scales? The static fusion scales are usu-\nally tuned on a small validation set via grid search.\nTable 7 shows that it is also possible to use au-\ntomatic differentiation to learn static scales only\non the validation set. The automatically learned\nsubword-agnostic scales have similar values as\nthe ones tuned via grid search and therefore also\ntheir translation performance is similar. Learning\nsubword-dependent scales automatically on the val-\nidation set on the other hand improves performance\non this set, but does not generalize which indicates\noverfitting.\n7 Conclusions\nThis work presents multiple extensions to\ndocument-level LM fusion, a technique of utiliz-\ning document-level monolingual data for context-\naware MT. In comparison to existing work, our ex-\ntensions significantly improve discourse-modeling\n0.0 0.2 0.4 0.6 0.8 1.0\nfusion scaleλ(e)\n0\n1000\n2000\n3000frequency\n_er, _sie, _es\n_Lehrer in\ntarget subword beginning new word\ntarget subword continuing word\nFigure 1: Distribution of the automatically learned LM\nfusion scales for different target-side subwords on the\nNews task. Subwords for which document-level con-\ntext is often necessary, such as the German pronouns\n‘_er’, ‘_sie’, ‘_es’, and the suffix ‘in’ marking female\nprofessions, have learned higher scales than nouns like\n‘_Lehrer’.\nacross four MT tasks and furthermore are com-\nputationally more efficient. We conduct eval-\nuations against two baselines: document-level\nback-translation and a task-specific LM re-ranking\nmethod. Despite our extensions, back-translation\nin general still outperforms document-level LM\nfusion. Nevertheless back-translation can be effec-\ntively combined with LM fusion, further improv-\ning translation performance. On very specific test\nsets, the LM re-ranking performs best. However,\nour experiments also show that systems trained on\ndocument-level parallel data outperform the best\nsystems trained with monolingual documents only.\nFinally, this work is the first to explore document-\nlevel LM fusion with LLMs. First findings demon-\nstrate that fusion with an LLM outperforms a small\nLM trained on in-domain data and open the path\nfor future investigations.\n385\nLimitations\nThe experiments in this work were limited to four\nMT tasks, from which two are low-resource and\nthree are translating from English into German.\nApart from the experiments with the LLM, we\ndid not conduct any experiments on a large-scale\ndataset of multi-domain monolingual documents.\nThe LLM in our experiments only has 7B parame-\nters, while much larger LLMs exist (e.g., Touvron\net al., 2023).\nFurther, our work focuses only on one specific\narchitecture for document-level MT and uses only\ntwo sentences target-side context. Various other ar-\nchitectures exist and may entail different properties.\nThis work further does not investigate the behavior\nof larger translation models.\nAnother limitation lies in the evaluation of\ndocument-level MT models. The document-level\ntargeted metrics we used are all reference-based\nand limited to the translation of pronouns, gender-\nreferring professions or salutation forms. Other dis-\ncourse phenomena like e.g. cohesion exist (Maruf\net al., 2022) but were not studied in our work. It is\nunclear how well automated metrics actually cor-\nrelate with the actual document-level translation\nquality (Currey et al., 2022), and this work did not\nperform any qualitative analysis.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nMarta Bañón, Pinzhen Chen, Barry Haddow, Kenneth\nHeafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L.\nForcada, Amir Kamran, Faheem Kirefu, Philipp\nKoehn, Sergio Ortiz-Rojas, Leopoldo Pla Sempere,\nGema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec,\nBrian Thompson, William Waites, Dion Wiggins,\nand Jaume Zaragoza. 2020. Paracrawl: Web-scale\nacquisition of parallel corpora. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 4555–4567. Association for Computa-\ntional Linguistics.\nRachel Bawden, Rico Sennrich, Alexandra Birch, and\nBarry Haddow. 2018. Evaluating discourse phenom-\nena in neural machine translation. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2018,\nNew Orleans, Louisiana, USA, June 1-6, 2018, Vol-\nume 1 (Long Papers), pages 1304–1313. Association\nfor Computational Linguistics.\nAlexandre Berard, Ioan Calapodescu, and Claude Roux.\n2019. Naver labs europe’s systems for the WMT19\nmachine translation robustness task. In Proceedings\nof the Fourth Conference on Machine Translation,\nWMT 2019, Florence, Italy, August 1-2, 2019 - Vol-\nume 2: Shared Task Papers, Day 1, pages 526–532.\nAssociation for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMauro Cettolo, Marcello Federico, Luisa Bentivogli,\nJan Niehues, Sebastian Stüker, Katsuhito Sudoh,\nKoichiro Yoshino, and Christian Federmann. 2017.\nOverview of the IWSLT 2017 evaluation campaign.\nIn Proceedings of the 14th International Confer-\nence on Spoken Language Translation, IWSLT 2017,\nTokyo, Japan, December 14-15, 2017 , pages 2–14.\nInternational Workshop on Spoken Language Trans-\nlation.\nLinqing Chen, Junhui Li, Zhengxian Gong, Boxing\nChen, Weihua Luo, Min Zhang, and Guodong Zhou.\n2021a. Breaking the corpus bottleneck for context-\naware neural machine translation with cross-task pre-\ntraining. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume\n1: Long Papers), Virtual Event, August 1-6, 2021 ,\npages 2851–2861. Association for Computational\nLinguistics.\nLinqing Chen, Junhui Li, Zhengxian Gong, Xiangyu\nDuan, Boxing Chen, Weihua Luo, Min Zhang, and\nGuodong Zhou. 2021b. Improving context-aware\nneural machine translation with source-side mono-\nlingual documents. In Proceedings of the Thirtieth\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2021, Virtual Event / Montreal, Canada,\n19-27 August 2021, pages 3794–3800. ijcai.org.\nAnna Currey, Maria Nadejde, Raghavendra Reddy Pap-\npagari, Mia Mayer, Stanislas Lauly, Xing Niu, Ben-\njamin Hsu, and Georgiana Dinu. 2022. Mt-geneval:\nA counterfactual and contextual dataset for evaluating\ngender accuracy in machine translation. In Proceed-\nings of the 2022 Conference on Empirical Methods\n386\nin Natural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 4287–4299. Association for Computational\nLinguistics.\nMiquel Esplà-Gomis, Mikel L. Forcada, Gema Ramírez-\nSánchez, and Hieu Hoang. 2019. Paracrawl: Web-\nscale parallel corpora for the languages of the EU.\nIn Proceedings of Machine Translation Summit XVII\nVolume 2: Translator, Project and User Tracks, MT-\nSummit 2019, Dublin, Ireland, August 19-23, 2019,\npages 118–119. European Association for Machine\nTranslation.\nThierry Etchegoyhen and Harritxu Gete. 2020. To case\nor not to case: Evaluating casing methods for neural\nmachine translation. In Proceedings of The 12th Lan-\nguage Resources and Evaluation Conference, LREC\n2020, Marseille, France, May 11-16, 2020 , pages\n3752–3760. European Language Resources Associa-\ntion.\nEva Martínez Garcia, Carles Creus, and Cristina España-\nBonet. 2019. Context-aware neural machine transla-\ntion decoding. In Proceedings of the Fourth Work-\nshop on Discourse in Machine Translation, Dis-\ncoMT@EMNLP 2019, Hong Kong, China, November\n3, 2019, pages 13–23. Association for Computational\nLinguistics.\nYusser Al Ghussin, Jingyi Zhang, and Josef van Gen-\nabith. 2023. Exploring paracrawl for document-level\nneural machine translation. In Proceedings of the\n17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, EACL 2023,\nDubrovnik, Croatia, May 2-6, 2023 , pages 1296–\n1302. Association for Computational Linguistics.\nÇaglar Gülçehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loïc Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On\nusing monolingual corpora in neural machine trans-\nlation. CoRR, abs/1503.03535.\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary, Jonathan Clark, Christian Federmann,\nXuedong Huang, Marcin Junczys-Dowmunt, William\nLewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo,\nArul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei\nTian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dong-\ndong Zhang, Zhirui Zhang, and Ming Zhou. 2018.\nAchieving human parity on automatic chinese to en-\nglish news translation. CoRR, abs/1803.05567.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Has-\nsan Awadalla. 2023. How good are GPT models\nat machine translation? A comprehensive evaluation.\nCoRR, abs/2302.09210.\nChristian Herold, Yingbo Gao, Mohammad Zeineldeen,\nand Hermann Ney. 2023. Improving language model\nintegration for neural machine translation. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2023, Toronto, Canada, July 9-14, 2023 .\nAssociation for Computational Linguistics.\nChristian Herold and Hermann Ney. 2023a. Improv-\ning long context document-level machine transla-\ntion. In Proceedings of the 4th Workshop on Com-\nputational Approaches to Discourse: CODI 2023,\nToronto, Canada, July 13-14, 2023. Association for\nComputational Linguistics.\nChristian Herold and Hermann Ney. 2023b. On search\nstrategies for document-level neural machine trans-\nlation. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, Toronto, Canada, July\n9-14, 2023. Association for Computational Linguis-\ntics.\nJingjing Huo, Christian Herold, Yingbo Gao, Leonard\nDahlmann, Shahram Khadivi, and Hermann Ney.\n2020. Diving deep into context-aware neural ma-\nchine translation. In Proceedings of the Fifth Confer-\nence on Machine Translation, WMT@EMNLP 2020,\nOnline, November 19-20, 2020, pages 604–616. As-\nsociation for Computational Linguistics.\nSébastien Jean, Ankur Bapna, and Orhan Firat. 2019.\nFill in the blanks: Imputing missing sentences for\nlarger-context neural machine translation. CoRR,\nabs/1910.14075.\nSébastien Jean and Kyunghyun Cho. 2020. Log-\nlinear reformulation of the noisy channel model for\ndocument-level neural machine translation. In Pro-\nceedings of the Fourth Workshop on Structured Pre-\ndiction for NLP@EMNLP 2020, Online, November\n20, 2020, pages 95–101. Association for Computa-\ntional Linguistics.\nSébastien Jean, Stanislas Lauly, Orhan Firat, and\nKyunghyun Cho. 2017. Does neural machine\ntranslation benefit from larger context? CoRR,\nabs/1704.05135.\nYuchen Jiang, Tianyu Liu, Shuming Ma, Dongdong\nZhang, Jian Yang, Haoyang Huang, Rico Sennrich,\nRyan Cotterell, Mrinmaya Sachan, and Ming Zhou.\n2022. Blonde: An automatic evaluation metric for\ndocument-level machine translation. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL 2022, Seat-\ntle, WA, United States, July 10-15, 2022, pages 1550–\n1565. Association for Computational Linguistics.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat WMT 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation, WMT\n2019, Florence, Italy, August 1-2, 2019 - Volume 2:\nShared Task Papers, Day 1, pages 225–233. Associa-\ntion for Computational Linguistics.\n387\nMarzena Karpinska and Mohit Iyyer. 2023. Large lan-\nguage models effectively leverage document-level\ncontext for literary translation, but critical errors per-\nsist. CoRR, abs/2304.03245.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nMachine Translation Summit X: Papers, MTSum-\nmit 2005, Phuket, Thailand, September 13-15, 2005,\npages 79–86.\nShaohui Kuang and Deyi Xiong. 2018. Fusing recency\ninto neural machine translation with an inter-sentence\ngate model. In Proceedings of the 27th International\nConference on Computational Linguistics, COLING\n2018, Santa Fe, New Mexico, USA, August 20-26,\n2018, pages 607–617. Association for Computational\nLinguistics.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 66–75.\nAssociation for Computational Linguistics.\nSamuel Läubli, Sheila Castilho, Graham Neubig, Rico\nSennrich, Qinlan Shen, and Antonio Toral. 2020.\nA set of recommendations for assessing human-\nmachine parity in language translation. J. Artif. Intell.\nRes., 67:653–672.\nSamuel Läubli, Rico Sennrich, and Martin V olk. 2018.\nHas machine translation achieved human parity? A\ncase for document-level evaluation. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 4791–4796.\nAssociation for Computational Linguistics.\nPierre Lison, Jörg Tiedemann, and Milen Kouylekov.\n2018. Opensubtitles2018: Statistical rescoring of\nsentence alignments in large, noisy parallel corpora.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation, LREC\n2018, Miyazaki, Japan, May 7-12, 2018. European\nLanguage Resources Association (ELRA).\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Trans. Assoc.\nComput. Linguistics, 8:726–742.\nAntónio V . Lopes, M. Amin Farajian, Rachel Baw-\nden, Michael Zhang, and André F. T. Martins. 2020.\nDocument-level neural MT: A systematic compari-\nson. In Proceedings of the 22nd Annual Conference\nof the European Association for Machine Translation,\nEAMT 2020, Lisboa, Portugal, November 3-5, 2020,\npages 225–234. European Association for Machine\nTranslation.\nSameen Maruf and Gholamreza Haffari. 2018. Docu-\nment context neural machine translation with mem-\nory networks. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2018, Melbourne, Australia, July 15-20,\n2018, Volume 1: Long Papers, pages 1275–1284. As-\nsociation for Computational Linguistics.\nSameen Maruf, Fahimeh Saleh, and Gholamreza Haffari.\n2022. A survey on document-level neural machine\ntranslation: Methods and evaluation. ACM Comput.\nSurv., 54(2):45:1–45:36.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2947–2954. Association for Computational\nLinguistics.\nMathias Müller, Annette Rios, Elena V oita, and Rico\nSennrich. 2018. A large-scale test set for the evalua-\ntion of context-aware pronoun translation in neural\nmachine translation. In Proceedings of the Third Con-\nference on Machine Translation: Research Papers,\nWMT 2018, Belgium, Brussels, October 31 - Novem-\nber 1, 2018, pages 61–72. Association for Computa-\ntional Linguistics.\nRafael Müller, Simon Kornblith, and Geoffrey E. Hin-\nton. 2019. When does label smoothing help? In Ad-\nvances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Process-\ning Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 4696–4705.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. CoRR, abs/1904.01038.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA, pages 311–318. ACL.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, WMT 2018,\nBelgium, Brussels, October 31 - November 1, 2018,\npages 186–191. Association for Computational Lin-\nguistics.\nMatt Post and Marcin Junczys-Dowmunt. 2023. Escap-\ning the sentence-level paradigm in machine transla-\ntion. CoRR, abs/2304.12959.\n388\nRicardo Rei, Craig Stewart, Ana C. Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 2685–2702. Association for Computational\nLinguistics.\nFahimeh Saleh, Alexandre Berard, Ioan Calapode-\nscu, and Laurent Besacier. 2019. Naver labs eu-\nrope’s systems for the document-level generation\nand translation task at WNGT 2019. In Proceed-\nings of the 3rd Workshop on Neural Generation and\nTranslation@EMNLP-IJCNLP 2019, Hong Kong,\nNovember 4, 2019, pages 273–279. Association for\nComputational Linguistics.\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov,\nEdouard Grave, Armand Joulin, and Angela Fan.\n2021. Ccmatrix: Mining billions of high-quality par-\nallel sentences on the web. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021 , pages 6490–6500. Association for\nComputational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers. The Association\nfor Computer Linguistics.\nFelix Stahlberg, Danielle Saunders, Adrià de Gispert,\nand Bill Byrne. 2019. Cued@wmt19: Ewc&lms. In\nProceedings of the Fourth Conference on Machine\nTranslation, WMT 2019, Florence, Italy, August 1-2,\n2019 - Volume 2: Shared Task Papers, Day 1, pages\n364–373. Association for Computational Linguistics.\nAmane Sugiyama and Naoki Yoshinaga. 2019. Data\naugmentation using back-translation for context-\naware neural machine translation. In Proceedings\nof the Fourth Workshop on Discourse in Machine\nTranslation, DiscoMT@EMNLP 2019, Hong Kong,\nChina, November 3, 2019, pages 35–44. Association\nfor Computational Linguistics.\nAmane Sugiyama and Naoki Yoshinaga. 2020. Context-\naware decoder for neural machine translation using a\ntarget-side document-level language model. CoRR,\nabs/2010.12827.\nAmane Sugiyama and Naoki Yoshinaga. 2021. Context-\naware decoder for neural machine translation using\na target-side document-level language model. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n5781–5791. Association for Computational Linguis-\ntics.\nZewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao,\nShujian Huang, Jiajun Chen, and Lei Li. 2022. Re-\nthinking document-level neural machine translation.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 3537–3548. Association for Computa-\ntional Linguistics.\nJörg Tiedemann and Yves Scherrer. 2017. Neural ma-\nchine translation with extended context. In Proceed-\nings of the Third Workshop on Discourse in Machine\nTranslation, DiscoMT@EMNLP 2017, Copenhagen,\nDenmark, September 8, 2017, pages 82–92. Associa-\ntion for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019.\nContext-aware monolingual repair for neural ma-\nchine translation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing, EMNLP-IJCNLP\n2019, Hong Kong, China, November 3-7, 2019, pages\n877–886. Association for Computational Linguistics.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 1264–1274. Association for Computational\nLinguistics.\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,\nDian Yu, Shuming Shi, and Zhaopeng Tu. 2023.\nDocument-level machine translation with large lan-\nguage models. CoRR, abs/2304.02210.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. CoRR, abs/1609.08144.\n389\nLei Yu, Laurent Sartran, Wojciech Stokowiec, Wang\nLing, Lingpeng Kong, Phil Blunsom, and Chris Dyer.\n2020. Better document-level machine translation\nwith bayes’ rule. Trans. Assoc. Comput. Linguistics,\n8:346–360.\nBiao Zhang, Barry Haddow, and Alexandra Birch. 2023.\nPrompting large language model for machine transla-\ntion: A case study. CoRR, abs/2301.07069.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018, pages 533–542. Association for\nComputational Linguistics.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020.\nIncorporating BERT into neural machine translation.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nA Appendix\nA.1 Model Training\nData. The News En →De task comprises 330k\nparallel sentences from NewsCommentary v14 8,\nwhich we combine with document-level monolin-\ngual data from NewsCrawl 9 (70M sentences 10).\nOur Subtitles En →De data consists of a total\nof 39M monolingual movie show subtitles from\nOpenSubtitles, from which a subset of 22.5M sen-\ntences has been aligned to English sentences and\nforms our parallel training data (Lison et al., 2018).\nFor TED En→It we use 230k parallel sentences\nfrom scientific TED talks released as part of the\nIWSLT17 multilingual task (Cettolo et al., 2017)\nwhich we combine with 2.2M sentences of talks\nfrom the European parliament (Koehn, 2005). Fi-\nnally, the e-Commerce En→De task is about trans-\nlating item descriptions from e-Commerce listings.\nWe use 326M parallel sentences of out-of-domain\nparallel training data from the ParaCrawl v9 corpus\n(Esplà-Gomis et al., 2019) which we combine with\n128k parallel sentences in-domain data. The mono-\nlingual data was sampled from item descriptions\nand is entirely in-domain (119M sentences).\nThe sizes of our training corpora are shown in\nTable 10.\nOn each task, we use a validation set for select-\ning the best checkpoint, tuning the fusion scales\nand for finding which method works best. For the\nfinal comparison in Table 1 we then report on an\nunseen test set of the same domain.\nThe News validation set is newstest2015,\nand newstest2018 as test set. For Subtitles,\nour validation and test sets were sampled from\nthe training corpus. The precise document IDs\nfor the validation set are: 1995/254, 1997/165,\n2000/313, 2002/461, 2005/441, 2007/781,\n2010/273, 2012/757, 2015/1488, 2017/525 for\nthe validation set; and for our test set: 1997/310,\n2002/40, 2007/189, 2012/1085, 2017/644. The test\nset is the same as used in Huo et al. (2020). For\nTED, we concatenate dev2010 and tst2010 and\nuse tst2017.mltlng as test set. For e-Commerce,\nwe create the validation and test set ourselves by\ntranslating English e-Commerce item descriptions\ninto German: Our validation set comprises 85\ndocuments (2882 sentences) and the test set 100\n8 https://data.statmt.org/news-commentary/v14/\n9 https://data.statmt.org/news-crawl/\n10 To reduce training time, our back-translation experiments\non this task utilize only the first 2M sentences.\n390\nData News Subtitles TED e-Commerce\ntest pron. proff. test pron. proff. test proff. test pron. proff.\nparallel data 129.7 125.7 168.6 26.6 36.0 103.2 47.6 114.8 61.7 44.2 52.6\nmonolingual data 97.1 94.9 161.3 27.8 36.6 117.4 75.4 116.1 50.8 48.7 57.5\nTable 9: Perplexities of sentence-level LMs trained on equal amount of target-side data.\nTask Data docs sents words\nNews parallel 8.5k 330k 7.4M\nmono. 3M 70M 1.0B\nSubtitles parallel 30k 22.5M 136M\nmono. 47k 39M 223M\nTED parallel 1.9k 230k 3.7M\nmono. 6k 2.2M 54.6M\ne-Commerce parallel n.a. 326M 9.6B\nmono. 1.5M 119M 3.1B\nTable 10: Training data statistics.\ndocuments (2520 sentences).\nAs the pronouns test set (Müller et al., 2018)\nwas extracted from the OpenSubtitles corpus, we\nremove these sentences from the Subtitles training\ndata. The professions test set (Currey et al., 2022)\nwas curated from Wikipedia articles and is not part\nof our training corpora.\nModels. We train the News, Subtitles and TED\nmodels with a shared embedding and projection\nmatrix. Th resulting MT models for News and Sub-\ntitles have 60M parameters, 51M parameters for\nTED and 90M for e-Commerce. For model train-\ning we use eight Tesla V100-SXM2-32GB GPUs.\nTraining the baselines takes approximately 7h for\nNews, 21h for Subtitles, 5h for TED, and 30h for e-\ncommerce. Due to resource constraints, we report\nonly a single run for each experiment.\nOptimization. For optimization we use Adam\n(Kingma and Ba, 2015) and a batch size of 22k sub-\nwords. The low-resource MT models (News, TED)\nare trained for 100k update steps with 30 % dropout,\n20 % label smoothing and weight decay, while the\nhigh-resource models (Subtitles, e-Commerce) are\ntrained for 300k updates with 10 % dropout, 10 %\nlabel smoothing and no weight decay.\nA.2 Domain Effects\nIn an effort to estimate how well the domain of\nthe training data matches the test sets, we train\nLMs on the target-side part of the parallel and the\nmonolingual training data. Within each task, the\nLMs are trained with the same parameters and the\nsame vocabulary. We then report the perplexities\nApproach valid set doc.-targeted\nBLEU COMET pron. proff.\nbaseline 24.5 80.9 45.1 65.9\nLM fusion 24.8 81.2 45.0 65.8\n+(Jean, 2020; Sugiyama, 2021)5 24.9 81.2 46.0 65.0\n+ ILM: separate 25.8 82.1 47.3 65.2\n+ ILM:h= 0 25.5 82.0 43.8 64.7\n+ ILM: mini self-att. 25.8 82.1 44.6 65.1\nTable 11: Document-level LM fusion (a) without sub-\ntracting any LM, (b) subtracting the sentence-level\nprobabilities of the external LM (Jean and Cho, 2020;\nSugiyama and Yoshinaga, 2021), and (c) subtracting dif-\nferent approximations of the internal LM (ILM) learned\nby the MT model, reported on the News task.\non the task-specific test sets and the document-\ntargeted challenge sets in Table 9.\nFor News, the monolingual data is more in-\ndomain for all test sets. Similarly the domain of\nthe e-Commerce monolingual data is closer to the\ntask-specific test set. For Subtitles, the domains\nof parallel and monolingual data are more or less\nequal and on TED, the monolingual data is slightly\nout-of-domain.\nThis domain effect explains the improvements\nin BLEU and COMET on the task-specific test sets\nthat we reported in Table 1 on News and on e-\nCommerce.\nA.3 Comparing Internal Language Model\nEstimations\nHerold et al. (2023) propose several ways of ap-\nproximating the internal LM learned implicitly by\nthe MT model in the context of sentence-level MT.\nWe evaluate three of their approaches for document-\nlevel LM fusion and compare them against the\nexisting document-level LM fusion approach that\nsubtracts the sentence-level probabilities of the ex-\nternal LM (Jean and Cho, 2020; Sugiyama and\nYoshinaga, 2021). Table 11 shows the results: Sub-\ntracting the internal LM substantially improves LM\nfusion over existing work. Estimating it by training\na separate LM on the same data as the MT model\nworks best.\n391\nFusion Scales valid set doc.-targeted\nApproach Restriction BLEU COMETpron. proff.\nnone - 24.5 80.9 45.1 65.9\nstatic - 25.8 82.1 47.3 65.2\nλ0=1, λ1=λ2 25.4 81.8 46.5 65.1\non-the-fly - 22.3 78.3 43.4 69.3\nλ0=1, λ1=λ2 25.6 81.8 48.0 65.5\nauto. - 24.8 80.7 44.7 69.4\nlearned λ0=1, λ1=λ2 25.3 81.5 46.7 64.9\nTable 12: LM fusion with an imposed restriction on the\nsearch space of the fusion scales λ0, λ1, λ2, reported on\nthe News task.\nData valid set doc.-targeted\nparallel mono. BLEU COMET pron. proff.\nsent. - 24.5 80.9 45.1 65.9\nsent. sent. 27.0 83.2 46.7 65.7\nsent. doc. 26.9 82.4 47.8 80.7\npseudo-doc. doc. 27.1 83.0 48.7 80.5\nTable 13: Effect of back-translation on the News task.\nA.4 Fusion Scale Restrictions\nThe three LM fusion scalesλ0, λ1, λ2 in Equation 1\nbalance the contribution of the MT model and the\ntwo LMs. In our experiments the optimal scales\nusually lie atλ0 ≈ 1 and λ1 ≈ λ2. This is plausible\nas the internal LM (λ2) should neutralize the exter-\nnal LM (λ1) to the same degree. For the non-static\nfusion scales however, we find that searching over\nthe three-dimensional search space of independent\nλ0, λ1, λ2 finds unintuitive scale combinations and\nthat this causes bad performance. Therefore in our\nexperiments we restrict the search space of fusion\nscales to the one-dimensional slice where λ0 = 1\nand λ1 = λ2. Table 12 gives a direct comparison.\nA.5 Document-level Back-translation\nIn Table 13 we compare back-translation using\ndocument-level data against sentence-level back-\ntranslation.\nSentence- and document-level back-translation\ngives the same performance improvements in\nBLEU and COMET , however only back-translation\non document-level improves the document-targeted\nmetrics. For document-level back-translation we\nfind that creating pseudo-documents from the paral-\nlel data is necessary to achieve the same BLEU and\nCOMET scores as sentence-level back-translation.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8848663568496704
    },
    {
      "name": "Machine translation",
      "score": 0.8305462598800659
    },
    {
      "name": "Sentence",
      "score": 0.6490031480789185
    },
    {
      "name": "Language model",
      "score": 0.6137116551399231
    },
    {
      "name": "Natural language processing",
      "score": 0.6112433075904846
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6086072325706482
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.49393779039382935
    },
    {
      "name": "Context (archaeology)",
      "score": 0.49078044295310974
    },
    {
      "name": "Evaluation of machine translation",
      "score": 0.489596426486969
    },
    {
      "name": "Language translation",
      "score": 0.4852316379547119
    },
    {
      "name": "Translation (biology)",
      "score": 0.48439961671829224
    },
    {
      "name": "Machine translation software usability",
      "score": 0.38582542538642883
    },
    {
      "name": "Example-based machine translation",
      "score": 0.32985174655914307
    },
    {
      "name": "Machine learning",
      "score": 0.32358503341674805
    },
    {
      "name": "Programming language",
      "score": 0.1715092957019806
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ]
}