{
  "title": "Adaptive Multi-View and Temporal Fusing Transformer for 3D Human Pose Estimation",
  "url": "https://openalex.org/W4283825932",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2373848155",
      "name": "Shuai Hui",
      "affiliations": [
        "Nanjing University of Information Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2141498210",
      "name": "Wu Lele",
      "affiliations": [
        "Nanjing University of Information Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2054761857",
      "name": "Liu Qing-shan",
      "affiliations": [
        "Nanjing University of Information Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2441438155",
    "https://openalex.org/W2716916105",
    "https://openalex.org/W2994857106",
    "https://openalex.org/W2965523038",
    "https://openalex.org/W2964016027",
    "https://openalex.org/W2555540282",
    "https://openalex.org/W3072456739",
    "https://openalex.org/W2612706635",
    "https://openalex.org/W2998621957",
    "https://openalex.org/W2799870331",
    "https://openalex.org/W3081460284",
    "https://openalex.org/W3157441214",
    "https://openalex.org/W2557698284",
    "https://openalex.org/W2968459013",
    "https://openalex.org/W2964318832",
    "https://openalex.org/W2962896489",
    "https://openalex.org/W2972662547",
    "https://openalex.org/W3106838237",
    "https://openalex.org/W3195639294",
    "https://openalex.org/W3136525061",
    "https://openalex.org/W3034971010",
    "https://openalex.org/W3009246422",
    "https://openalex.org/W3094830752",
    "https://openalex.org/W2984313141",
    "https://openalex.org/W6802905620",
    "https://openalex.org/W3010350980",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4237416912",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2963515833",
    "https://openalex.org/W2045091010",
    "https://openalex.org/W2963598138",
    "https://openalex.org/W2554247908",
    "https://openalex.org/W2989465897",
    "https://openalex.org/W2293220651",
    "https://openalex.org/W2963995996",
    "https://openalex.org/W3126761340",
    "https://openalex.org/W3034217102",
    "https://openalex.org/W3035637814",
    "https://openalex.org/W3083132452",
    "https://openalex.org/W4200356062",
    "https://openalex.org/W3034581612",
    "https://openalex.org/W2969676305",
    "https://openalex.org/W2971476609",
    "https://openalex.org/W3092041969",
    "https://openalex.org/W4312667155",
    "https://openalex.org/W6800217721",
    "https://openalex.org/W3175199633",
    "https://openalex.org/W3188906027",
    "https://openalex.org/W6783130735",
    "https://openalex.org/W4312249545",
    "https://openalex.org/W4225557002",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W6801663392",
    "https://openalex.org/W2964221239",
    "https://openalex.org/W4214755140",
    "https://openalex.org/W3105916048",
    "https://openalex.org/W2982627166",
    "https://openalex.org/W3034448411",
    "https://openalex.org/W3106882556",
    "https://openalex.org/W2981660954",
    "https://openalex.org/W2963772981",
    "https://openalex.org/W3035139896",
    "https://openalex.org/W3035068106",
    "https://openalex.org/W2895689136",
    "https://openalex.org/W2604375920",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035416506",
    "https://openalex.org/W2899818084",
    "https://openalex.org/W4206407517",
    "https://openalex.org/W4288523215",
    "https://openalex.org/W3082628734",
    "https://openalex.org/W4295192008",
    "https://openalex.org/W3104343368",
    "https://openalex.org/W3206636652",
    "https://openalex.org/W4287022992",
    "https://openalex.org/W3098612954",
    "https://openalex.org/W3037374099",
    "https://openalex.org/W3201095209",
    "https://openalex.org/W4297971608",
    "https://openalex.org/W4287265181",
    "https://openalex.org/W3183295755",
    "https://openalex.org/W3159498171",
    "https://openalex.org/W3022928074",
    "https://openalex.org/W3106758349"
  ],
  "abstract": "This article proposes a unified framework dubbed Multi-view and Temporal Fusing Transformer (MTF-Transformer) to adaptively handle varying view numbers and video length without camera calibration in 3D Human Pose Estimation (HPE). It consists of Feature Extractor, Multi-view Fusing Transformer (MFT), and Temporal Fusing Transformer (TFT). Feature Extractor estimates 2D pose from each image and fuses the prediction according to the confidence. It provides pose-focused feature embedding and makes subsequent modules computationally lightweight. MFT fuses the features of a varying number of views with a novel Relative-Attention block. It adaptively measures the implicit relative relationship between each pair of views and reconstructs more informative features. TFT aggregates the features of the whole sequence and predicts 3D pose via a transformer. It adaptively deals with the video of arbitrary length and fully unitizes the temporal information. The migration of transformers enables our model to learn spatial geometry better and preserve robustness for varying application scenarios. We report quantitative and qualitative results on the Human3.6M, TotalCapture, and KTH Multiview Football II. Compared with state-of-the-art methods with camera parameters, MTF-Transformer obtains competitive results and generalizes well to dynamic capture with an arbitrary number of unseen views.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nAdaptive Multi-view and Temporal Fusing\nTransformer for 3D Human Pose Estimation\nHui Shuai*, Lele Wu*, and Qingshan Liu, Senior Member, IEEE\nAbstract—This paper proposes a uniﬁed framework dubbed Multi-view and Temporal Fusing Transformer (MTF-Transformer) to\nadaptively handle varying view numbers and video length without camera calibration in 3D Human Pose Estimation (HPE). It consists\nof Feature Extractor, Multi-view Fusing Transformer (MFT), and Temporal Fusing Transformer (TFT). Feature Extractor estimates 2D\npose from each image and fuses the prediction according to the conﬁdence. It provides pose-focused feature embedding and makes\nsubsequent modules computationally lightweight. MFT fuses the features of a varying number of views with a novel Relative-Attention\nblock. It adaptively measures the implicit relative relationship between each pair of views and reconstructs more informative features.\nTFT aggregates the features of the whole sequence and predicts 3D pose via a transformer. It adaptively deals with the video of\narbitrary length and fully unitizes the temporal information. The migration of transformers enables our model to learn spatial geometry\nbetter and preserve robustness for varying application scenarios. We report quantitative and qualitative results on the Human3.6M,\nTotalCapture, and KTH Multiview Football II. Compared with state-of-the-art methods with camera parameters, MTF-Transformer\nobtains competitive results and generalizes well to dynamic capture with an arbitrary number of unseen views.\nIndex Terms—3D human pose estimation, Multi-view Fusing Transformer, Temporal Fusing Transformer.\n!\n1 I NTRODUCTION\nT\nHREE -dimensional human pose estimation (HPE) aims to\npredict 3D human pose information from images or videos,\nin which skeleton joint location is the primary output result to\ncarry pose information. It plays a fundamental role in many\napplications, such as action recognition [1], [2], [3], human body\nreconstruction [4], [5], and robotics manipulation [6], [7].\nWith the emergence of deep learning, 3D HPE has made\nconsiderable progress. Especially, 2D-to-3D [8], [9], [10], [11]\nmethods have superior performance owing to intermediate 2D\nsupervision [12]. In practice, the 2D-to-3D pipeline involves sev-\neral variable factors deriving from different application scenarios,\nincluding the number of views, the length of the video sequence,\nand whether using camera calibration.\nIn the monocular scene, most works [8], [13], [14], [15] esti-\nmate body structure from a static image with elaborate networks\nsuch as Convolutional Neural Networks and Graph Convolutional\nNetworks. This scheme is convenient since a single image is easy\nto obtain and process. Nevertheless, the information in a single im-\nage is insufﬁcient considering the occlusion and depth ambiguity.\nFor compensation, some works [16], [17], [18], [19], [20] utilize\ntemporal information from video sequences. Sequential variation\nin the video is conducive to revealing the human body’s structure.\nHowever, continuous images contain more homogeneous informa-\ntion rather than complementary clues. In a word, monocular 3D\nHPE is convenient to implement, but recovering 3D structure from\n2D images is always an ill-posed problem.\n• The authors are with Engineering Research Center of Digital Forensics,\nMinistry of Education, School of Computer and Software, Nanjing Univer-\nsity of Information Science and Technology, Nanjing, 210044, China. (E-\nmail: huishuai13@nuist.edu.cn, llwu@nuist.edu.cn, qsliu@nuist.edu.cn)\n• Hui Shuai and Lele Wu equally contributed on the work. Qingshan Liu is\nthe corresponding author. Code is available in https://github.com/lelexx/\nMTF-Transformer.\nManuscript received 03 Nov. 2021; revised 18 Apr. 2022 and 19 Jun. 2022;\naccepted 29 Jun. 2022\nRecently, prevalent works [21], [22], [23], [24], [25] tend to\nutilize multi-view geometric constraints. Most existing multi-view\nmethods aggregate information from different views via projective\ngeometry, depending on calibrated camera parameters. Camera\nparameters incorporate solid prior knowledge into the network\nbut are difﬁcult to calibrate accurately in dynamic capture. To this\nend, some works [26] attempt to fuse multi-view features without\ncalibration, but they have strict requirements on camera conﬁgu-\nration and the number of views. In addition, massive computation\nin the geometric space hinders multi-view methods to deal with\nvideo sequences. Overall, most existing multi-view methods are\nmore accurate than monocular methods, but camera calibration\nand computation overhead limit their application scenarios.\nEach method, as mentioned above, targets one or a few partic-\nular combinations of those variable factors and is not compatible\nwith others, limiting the ﬂexibility of the 3D HPE algorithm. Thus,\ndeveloping a uniﬁed framework that can adaptively handle all the\nfactors is essential. The main obstacles are that (1) Most deep\nlearning modules, such as fully connected layers, long and short-\nterm memory (LSTM), and GCN, are not friendly to variable-\nlength input. Moreover, these modules still have generalization\nproblems even with careful adjustments to handle variable-length\ninput. (2) Most methods rely on camera calibration to deal\nwith multi-view information, but precise camera parameters are\nunrealistic to calibrate synchronously in dynamic capture. (3)\nSome methods are too computationally expensive to deal with\nmulti-view videos. Accordingly, a uniﬁed framework needs to be\ncompatible with monocular to multi-view, single-image to videos\n3D HPE: (1) It should effectively integrate an arbitrary number\nof multi-view features in uncalibrated scenarios. (2) It should\nadaptively fuse temporal features in the variable-length videos and\nbe compatible with a single image. (3) It should be lightweight\nenough and have generalization capability.\nTo satisfy these requirements, we propose a uniﬁed framework\nto deal with variable multi-view sequences without calibration,\narXiv:2110.05092v2  [cs.CV]  4 Jul 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nnamed Multi-view and Temporal Fusing Transformer (MTF-\nTransformer) because the transformer can perceive the global\nrelationship of a varying number of tokens and aggregate them\nadaptively [27]. MTF-Transformer consists of Feature Extractor,\nMulti-view Fusion Transformer (MFT), and Temporal Fusion\nTransformer (TFT). In the Feature Extractor, a pre-trained 2D\ndetector predicts the 2D pose of each frame ﬁrst. Then, coordinates\nand conﬁdence are encoded into a vector via a feature embedding\nmodule, discarding the image features. It makes subsequent mod-\nules lightweight and focuses on lifting the 2D pose into the 3D\npose. MFT is designed to fuse the features of multiple views into\nmore informative ones. It integrates the relationship between the\nviews into the procedure that calculates the key, query, and value\nin the Relative-Attention block, avoiding camera calibration. In\nTFT, we employ a conventional transformer to capture temporal\ninformation. It is worth mentioning that, to make the MTF-\nTransformer adaptive to the input of an arbitrary number of views\nand length of sequences, we design a random mask mechanism in\nboth MFT and TFT, referring to the dropout mechanism [28].\nWe evaluate our method on Human3.6M [29], TotalCap-\nture [30], and KTH Multiview Football II [31] quantitatively\nand qualitatively. We also conduct detailed ablation study exper-\niments to verify the effectiveness of each module. Experiment\nresults demonstrate that MTF-Transformer outperforms camera\nparameter-free methods. Besides, MTF-Transformer can be di-\nrectly applied to the scenarios with different conﬁgurations from\nthe training stage, bridging the generalization gap signiﬁcantly. In\nshort, our contributions are:\n• We proposed a uniﬁed framework (MTF-Transformer) for\n3D HPE. It is adaptive to scenarios with videos of arbitrary\nlength and from arbitrary views without retraining.\n• We design a novel Multi-view Fusing Transformer (MFT),\nwhere the relationship between views is integrated into the\nRelative-Attention block. MFT reconstructs the features\nfrom multiple views according to the estimated implicit\nrelationship, avoiding the need for camera calibration.\n• We introduce the random mask mechanism into MFT and\nTemporal Fusing Transformer (TFT) to make them robust\nto variable view number and video length.\n• Not only does our model outperform camera parameter-\nfree models in precision, but it also has a better general-\nization capability to handle diverse application scenarios.\n2 R ELATED WORK\nThis section ﬁrstly summarizes 3D human pose estimation works,\nincluding monocular and multi-view methods. Then, we review\nthe transformer technology and introduce the methods that apply\nthe transformer in 3D human pose estimation and some other\nrelated tasks.\n2.1 3D Human Pose Estimation\nFundamentally, 3D HPE is to reconstruct the 3D body structure\nfrom 2D data. It is an ill-posed inverse task as one 2D image\ncorresponds to many possible 3D poses, further ampliﬁed by\nocclusions, background clutters, etc. Thus, utilizing all kinds\nof clues, such as the mutual constraint between the joints in\nthe image, the complementary information in videos, and the\nspatial geometric relationship from multiple viewpoints, to piece\ntogether the most likely 3D pose is the rationale for 3D HPE.\nAccording to the different clues, 3D HPE methods are divided\ninto categories and developed into several frameworks that handle\nspeciﬁc application scenarios.\n2.1.1 Monocular 3D Human Pose Estimation\nWith the pattern self-organizing and non-linear mapping capacity\nof deep neural networks, many approaches [8], [32], [33], [34],\n[35], [36], [37], [38] directly map pixel intensities to 3D poses\nfrom a single image. It forces DNNs to remember the pattern and\ninfer the 3D pose. These networks are difﬁcult to learn and rely\non tremendous labeled samples, resulting in unsatisfactory perfor-\nmance and generalization capability. Therefore, prior constraints\nbetween joints are utilized to determine the special pose. Fang [5]\net al. incorporate kinematics, symmetry, and motor coordination\ngrammar in 3D pose estimation. Some works employ GCN to\nmodel the constraints between the joints [15], [39]. These methods\nare devoted to digging into the image’s potential information,\nbut such a manner is insufﬁcient to solve an ill-posed problem.\nTo solve the ambiguity of a single image, more works [17],\n[40], [41], [42], [43] pay attention to temporal consistency in the\nvideo. For example, Pavllo et al. [16] transform a sequence of 2D\nposes through temporal convolutions. Cai et al. propose a graph-\nbased method to incorporate spatial dependencies and temporal\nconsistences [17]. Wang et al. [40] employ a novel objective func-\ntion to involve motion modeling in learning explicitly. Temporal\ninformation compensates for the incompleteness of 3D geometry,\nimproving the performance of 3D HPE. In general, monocular\nmethods are easy to implement as there is no need for camera\ncalibration. However, to piece up the 3D structure from 2D images,\nit is evident that the clues from multiple viewpoints are better\nalternatives.\n2.1.2 Multi-view 3D Human Pose Estimation\nTo tackle the occlusion and depth ambiguity, multi-view\nmethods [21], [22], [24], [44], [45] exploit geometric information\nfrom multiple views to infer 3D pose. Most utilize intrinsic and\nextrinsic camera parameters to fuse 2D features from different\nviews. For example, He et al. [21] aggregate features on epipolar\nlines between different views, depending on camera parameters\nin speciﬁc camera conﬁgurations. Iskakov et al. [22] utilize\nvolumetric grids to fuse features from different views with\ncamera parameters and regress root-centered 3D pose through\na learnable 3D CNN. Despite predicting 3D poses reliably,\nvolumetric approaches are computationally demanding. These\nmethods require precise camera parameters but can not generalize\nto scenarios with new camera conﬁgurations, not to mention the\ndynamic capture. Huang et al. [26] propose a new vision-IMU\ndata fusion technique to avoid strict requirements on camera\nconﬁguration and the number of views. FLEX [46] introduce\nto predict joint angles and bone lengths invariant to the camera\nposition rather than directly 3D positions, so calibration is\nobviated. Nevertheless, it is complicated, and its performance\ndegenerates with only a few views. Multi-view pose estimation\nmethods are more accurate due to adequate feature fusing\nvia projective geometry. However, another side of the coin is\nthat these methods rely on the restricted camera conﬁguration\nexplicitly or implicitly, limiting their application scene.\nMonocular and multi-view methods exploit the clues from\ndifferent aspects and ﬁt particular application scenarios. Unlike\nthese methods, we attempt to fuse all the clues adaptively in\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nAdd & Norm\nMulti-head \nattention\nAdd & Norm\nFeed Forward\n   2D Pose Detector\n   \n   \nCAA\nBranch Aggregation\n       \nAdd\nRelative-attentionCAA\nCAA\nCAA\nCAA\nFeature Extractor Multi-View Fusing\nTransformer\nTemporal Fusing Transformer\nView 1\nView N\nInputs\nT\n3D Pose\n2×\nFig. 1. The architecture of MTF-Transformer. It consists of three successive modules: Feature Extractor, Multi-view Fusing Transformer (MFT), and\nTemporal Fusing Transformer (TFT). Feature Extractor predicts 2D pose (P2D and C2D) ﬁrst and then encodes 2D pose into a feature vector for\neach frame. MFT measures the implicit relationship between each pair of views to reconstruct the feature adaptively. TFT aggregates the temporal\ninformation of the whole sequence and predicts the 3D pose of the center frame.\na uniﬁed network that can predict robust 3D poses in all the\napplication scenarios. So, the critical component is to ﬁnd a\nmechanism that organically integrates the information from\ndifferent aspects.\n2.2 Transformer in 3D Pose Estimation\nTransformer and self-attention have tremendously succeeded in\nNatural Language Processing and Computer Vision [47]. The self-\nattention module can adaptively capture long-range dependencies\nand global correlations from the data. In 3D pose estimation,\nthe core is to integrate the information from spatial 2D joints,\ntemporal sequence, and multiple viewpoints. Thus, the transformer\nis suitable for handling these aspects of information, and some\nworks utilizing the transformer models have recently emerged.\nFollowing the line of lifting 2D to 3D, some monocular\nmethods improve the performance by introducing the transformer.\nAmong them, METRO [48] employs a multi-layer transformer\narchitecture with progressive dimensionality reduction to regress\nthe 3D coordinates of the joints and vertices. PoseGTAC [49]\nproposes graph atrous convolution and graph transformer layer\nto extract local multi-scale and global long-range information,\nrespectively. More works handle spatial-temporal clues with the\ntransformer to alleviate occlusion and depth ambiguity in a single\nimage. For example, LiftFormer [50] estimates 3D pose from a\nsequence of 2D keypoints with self-attention on long-term infor-\nmation. MHFormer [51] proposes a Multi-Hypothesis Transformer\n(MHFormer) to learn spatio-temporal representations of multiple\nplausible pose hypotheses and aggregates the multi-hypothesis\ninto the ﬁnal 3D pose. Strided Transformer [52] incorporates the\nstrided convolution into the transformer to aggregate long-range\ninformation in a hierarchical architecture at low computation.\nPoseFormer [20] proposes a purely transformer-based approach\nto model the spatial relationships between 2D joints and temporal\ninformation in videos. Naturally, the transformer is also used to\naggregate the multi-view clues, but it usually works with epipolar\ngeometric while camera parameters are essential prerequisites.\nEpipolar transformer [21] leverages the transformer to ﬁnd the\npoint-point correspondence in the epipolar line. TransFusion [25]\nfurther proposes the concept of epipolar ﬁeld to encode 3D\npositional information into the transformer.\nThis tendency demonstrates the potential of the transformer for\nfeature fusing in 3D pose estimation. Moreover, the transformer is\ninherently adaptive to a variable number of input tokens. Thus, our\nconcerns focus on generalization capability and calibration avoid-\nance. Fortunately, the transformer generalizes well to the conﬁg-\nurations different from the training phase in some other tasks.\nFor example, Pooling-based Vision Transformer [53] improves\nmodel capability and generalization performance via designing a\npooling layer in ViT. Neural Human Performer [54] synthesizes\na free-viewpoint video of an arbitrary human performance, and\nit generalizes to unseen motions and characters at test time. It\nadaptively aggregates multi-time and multi-view information with\ntemporal and multi-view transformer. However, Neural Human\nPerformer fuses the multi-view features that are pixel-wisely\nmatched by a parametric 3D body model (SMPL). Such pixel-\npixel correspondence between multiple viewpoints is stronger\nthan pixel-epipolar correspondence. So, fusing multi-view features\nwithout camera calibration remains an open problem for us.\n3 M ETHOD\nThe purpose of our framework is to adaptively handle features\nfrom an arbitrary number of views and arbitrary sequence length\nwithout camera calibration. As shown in Fig. 1, the basic idea is\nto embed 2D detections into vectors ﬁrst, then fuse multi-view\nfeatures, and ﬁnally aggregate temporal clues to predict 3D joints.\nThis framework consists of Feature Extractor, Multi-view Fusing\nTransformer (MFT), and Temporal Fusing Transformer (TFT).\n3.1 Feature Extractor\nFeature Extractor uses a pre-trained 2D pose detector (e.g.,\nCPN [55]) to obtain 2D predictions and then maps them into 1D\nfeature vectors through a feature embedding module.\nTaking multi-view sequences I = {Ii}N×T\ni=1 with N views\nand T frames as input, each frame is an image I ∈RW×H×3. As\nthe following operations are conducted on each frame, we omit N\nand T for simplicity here. For each frame, Feature Extractor ﬁrst\nuses a pre-trained 2D pose detector D2D to infer the 2D prediction:\nZ = D2D (I) (1)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nCAA\nCAA\nCAA\nCAA\nCAA\n   \n      \n        \n   \n     \n    \n \n             \n        \n       \n  \n \n  \n \nFig. 2. The feature embedding module encodes the 2D prediction into a\nfeature vector. It splits the 2D prediction into ﬁve partitions and then uses\nﬁve branches to extract features. Finally, the features of ﬁve partitions\nare concatenated and mapped to a global feature f.\nwhere Z = {P2D,C2D}, P2D = {pj}J\nj=1 represents J co-\nordinates of the 2D pose and C2D = {cj}J\nj=1 represents the\nconﬁdence of these coordinates. Then a feature embedding module\nencodes the predicted 2D pose into a feature vector (as shown in\nFig. 2). The movements of the limbs and head are relatively inde-\npendent, so we divide the human body joints into ﬁve partitions\nand deal with them in ﬁve parallel branches. The ﬁve partitions\nare the head, left and right arms, and left and right legs:\nPg\n2D = {pk|k∈Sg} (2)\nCg\n2D = {ck|k∈Sg} (3)\nwhere grefers to the g-th partition, g∈{1,2,3,4,5}, Pg\n2D, Cg\n2D\nare subset of P2D, C2D, Sg ⊂{1,2,...,J }represents the index\nset belongs to the g-th partition. For matrix multiplication, Pg\n2D,\nCg\n2D are reshaped into vectors that pg\n2D ∈R2Jg\n, cg\n2D ∈RJg\n.\nSince the 2D pose inferred from the pre-trained detector is un-\nreliable due to motion blur and occlusion, simply fusing them may\nlead to unstable performance. Previous works, such as FLEX [46],\ndirectly concatenate the 2D pose and conﬁdence values together\nfor aggregation but they ignore the effects of unreliable inputs on\nthe features as the pose changes. In order to alleviate this issue,\nwe utilize the conﬁdence to modulate coordinates. Speciﬁcally,\nConﬁdence Attentive Aggregation (CAA) extracts local feature\nfg ∈RC/2 for each part, C is the dimension of the output of\nFeature Extractor. It can be formulated as:\n¯f\ng\n= Fg\np (pg\n2D) (4)\nag = Fg\nc (cg\n2D) (5)\nfg = Fg\nres\n(\n¯f\ng\n+ ag ·pg\n2D\n)\n(6)\nwhere Fg\np is a fully connected layer to map 2D coordinates pg\n2D to\ninitial feature vectors ¯f\ng\n∈RC/2, Fg\nc is another fully connected\nlayer to learn a attention matrix ag ∈ R(C/2)×2Jg\nfrom the\nconﬁdence cg\n2D. The third fully connected layer Fg\nres aggregates\ninitial feature vectors ¯f\ng\nwith 2D coordinates pg\n2D modulated by\nattention matrix ag. It consists of two res-blocks [8].\nWe further concatenate features of ﬁve partitions together and\nmap them into a global feature f ∈RC. This procedure can be\ndescribed as:\nf = Fshrink\n(\nConcat\n(\nf1,f2,f3,f4,f5\n))\n(7)\nwhere Fshrink is another fully connected layer. It maps features\nfrom ﬁve branches to the global feature of each frame. For the\ninput multi-view sequence Iwith N×T frames, Feature Extractor\nextracts the feature X ∈RC×N×T for the subsequent pipeline.\n3.2 Multi-view Fusing Transformer\nWe target to measure the relationship between the features from\nan arbitrary number of views and fuse these features adaptively.\nTransformer models are characterized by the ability to model\ndependencies in the input tokens regardless of their distance and\nenable immediate aggregation of global information [27]. Thus,\nthe transformer is a candidate to complete this task. Nevertheless,\nthe conventional transformer does not meet our requirements in\nposition encoding, and Point Transformer [56] has limitations in\nmanipulating the value item. So, we design a Relative-Attention\nthat measures the relative relationship between multiple view-\npoints and employs a more elaborate value transform procedure.\n3.2.1 Revisit Transformer and Self-attention\nThe transformer is a family of models consisting of the self-\nattention block, appending the position encoding, and the mask\nblock. The position encoding provides a unique coding for each\ninput token. The mask block truncates some nonexistent connec-\ntions based on prior knowledge. Self-attention operator transforms\nthe input feature vectors X= {xi}N\ni=1 into output feature vectors\nY= {yi}N\ni=1, one output feature vector yi is a weighted sum of\nall the input feature vectors. Typically, self-attention operators can\nbe classiﬁed into scalar attention and vector attention [56].\nThe scalar dot-product attention can be formulated as follows:\nyi =\n∑\nxj∈X\nρ\n(\nϕ(xi)⊤ψ(xj) +δ\n)\nα(xj) (8)\nwhere ϕ, ψ, and α are pointwise feature transformations, such\nas linear projections or MLPs, ϕ(xi), ψ(xj), and α(xj) are\ncalled query, key, and value. δis a position encoding function and\nρ is a normalization function such as softmax (mask block is\noptional). The scalar attention layer computes the scalar product\nbetween features transformed by ϕand φ, and uses the output as\nan attention weight for aggregating features transformed by α.\nDifferently, in the vector attention, attention weights are vec-\ntors that can modulate individual feature channels:\nyi =\n∑\nxj∈X\nρ(γ(β(ϕ(xi) ,ψ (xj)) +δ)) ⊙α(xj) (9)\nwhere β is a relation function (e.g., subtraction) and γ is a\nmapping function (e.g., an MLP) that produces attention vectors\nfor feature aggregation, ⊙is element-wise product.\nNevertheless, scalar attention and vector attention do not\nperfectly satisfy our requirements. First, they both employ position\nencoding to indicate the absolute position of the input token, but\nwe only need a relative relationship. Second, the value is only a\nderivative of xj, but we hope it can reﬂect the relative relationship\nbetween xi and xj as well. Point Transformer [56] proposes\na relative position encoding and adds the position encoding to\nthe value item, alleviating the above two issues. However, its\nrelative position encoding is additive. The addition represents the\ntranslation operation in the vector space, but we need a more\nﬂexible operation to manipulate the features from different views.\nMoreover, if we directly use Point Transformer in our task, we\nhave to concatenate all the 2D joints and converse it into the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nRelative-attention\nAdd\nX Relative Relation \nEncoding\nBlock mask\nSoftmax\nK Q\nMatmul\nV\nElement-wise product\nSumX’\nFig. 3. The architecture of Multi-view Fusing Transformer\nposition encoding. This procedure results in more parameters.\nMore parameters but less ﬂexibility often lead to the generalization\nproblem, and this problem is veriﬁed in TABLE 6.\n3.2.2 Multi-view Fusing Transformer\nTo bridge the gap between our purpose and existing transformer\nmodels, we propose the Multi-view Fusing Transformer (MFT).\nAs shown in Fig. 3, taking X ∈ RC×N×T as input, MFT\nconsiders it as tokens of X = {xi}N\ni=1, from the perspective of\nview. The dimension ofT is omitted here as MFT equally operates\nin each time slice. In addition, different body parts go through\na similar transformation between multiple viewpoints even after\nfeature transformation. Inspired by Squeeze Reasoning [57] that\nrelated components distribute in different groups along channels\nsparsely, we divide the dimension of C into K groups, and\nthe same transform matrix manipulates each group. So, we get\nxi ∈RD×K, C = D×K. The output of MFT is X′:\nX′= RA (X) +X (10)\nRA is Relative-Attention. In Relative-Attention, the inputXtripli-\ncates the role of query, key, and value, the output isY= {yi}N\ni=1.\nAij = γ(ℜ(xi,xj)) (11)\nTij = α(ℜ(xi,xj)) (12)\nyi =\n∑\nxj∈X\nρ(Aij) ⊙(Tijxj) (13)\nwhere ℜ(xi,xj) measures the relationship between each pair of\nview {xi,xj}, γand αfurther transform ℜ(xi,xj) into attention\nmatrix Aij ∈RD×K and transform matrix Tij ∈RD×D via\nfully connected layers, ρconsists of a block mask module and a\nsoftmax operation. The block mask module randomly sets all the\nvalues of Aij to −inf at the rate of M, except the condition\nthat i = j. Those values turn into zero after softmax. This\nmechanism ensures the MFT generalizes well to the scenario\nwith an arbitrary number of views. For further regularization, we\npenalize the difference between the inferred Tij and T\n′\nij that\nderived from rotation matrix Rij between two viewpoints. Rij is\nthe rotation matrix calculated from the 3D pose of i-th and j-th\nviewpoints via SVD. Then, it is ﬂattened, transformed with MLP\nψ, and reshaped as Tij.\nT\n′\nij = ψ(Rij) (14)\n  \n  \n        \n \n \n   \n   \n  \n  \n   \n   \n\n   \n \n   \n \n   \n \n   \n    \n \nSVD\n  \n   \nFig. 4. The architecture of relative relation encoding module\nIn the testing phase, this branch related to T\n′\nij is discarded. It is\ninteresting to note that our framework is also compatible with the\nscenario with camera parameters. When utilizing camera extrinsic,\nwe let T\n′\nij take the place of Tij in both the training and testing\nphase. In this circumstance, Rij is the rotation matrix of extrinsic\nparameters. The architecture of ℜ(xi,xj) is shown in Fig. 4,\nformulated as:\nhij = Fp(pi\n2D −pj\n2D) +Fi (xi) +Fj (xj) (15)\nℜ(xi,xj) =Fij (hij) +hij (16)\nwhere pi\n2D and pj\n2D are ﬂatten 2D poses from the 2D detector.\nWe add the offset between viewpoints to enhance the geometric\nawareness of the relative-attention module. Fp, Fi, Fj and Fij\nare fully connected layers.\nThe Relative-Attention is a vector product-like operation. The\ndifference between them is that (1) the explicit position encoding\nis discarded in query and value items, and (2) the relative rela-\ntionship is also integrated into the value item in the form of a\ntransform matrix. In brief, MFT reconstructs the feature of each\nview according to the relationship between them, formulated as:\nX →X′,X′∈RC×N×T .\n3.3 Temporal Fusing Transformer\nMulti-Head \nattention\nAdd & Norm\nFeed Forward\nX’\nAdd & Norm\n2×\nTemporal Fusing \nTransformer\n  \n \n… …\n… …\n   \n     \n \nPosition \nEncoding\nFig. 5. The architecture of Temporal Fusing Transformer. It predicts the\n3D pose of the middle frame.\nThe Temporal Fusing Transformer (TFT) is shown in Fig. 5, it\ntakes X′as input and predicts the 3D pose ofJjoint points P3D ∈\nR3×J×N in static scenes or dynamic scenes. Speciﬁcally, TFT\nutilizes a Transformer Encoder block [27] of two encoder layers to\nget the 3D pose of the middle frame. As the temporal sequence has\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nTABLE 1\nQuantitative results on Human3.6M. MTF-Transformer and MTF-Transformer+ are trained with 27 frames, whereT is the length of the sequence\nfor testing. We employ CPN [55] as the 2D detector, and * means no 2D detector.\nDir. Disc. Eat. Greet Phone Photo Pose Purch. Sit. SitD. Smoke Wait WalkD. Walk WalkT. Avg\nMonocular methods\nPavllo et al. [16] (CPN,T= 243) 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8\nChen et al. [58] (CPN, T= 1) 43.8 48.6 49.1 49.8 57.6 61.5 45.9 48.3 62.0 73.4 54.8 50.6 56.0 43.4 45.5 52.7\nLiu et al. [59] (CPN, T= 243) 41.8 44.8 41.1 44.9 47.4 54.1 43.4 42.2 56.2 63.6 45.3 43.5 45.3 31.3 32.2 45.1\nWang et al. [18] (CPN,T= 96) 40.2 42.5 42.6 41.1 46.7 56.7 41.4 42.3 56.2 60.4 46.3 42.2 46.2 31.7 31.0 44.5\nZeng et al. [60] (CPN,T= 243) 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 43.1 31.5 32.6 44.8\nCheng et al. [61] (CPN,T= 128) 38.3 41.3 46.1 40.1 41.6 51.9 41.8 40.9 51.5 58.4 42.2 44.6 41.7 33.7 30.1 42.9\nMulti-view methods with camera parameters\nPavlakos et al. [62] (*,T= 1) 41.2 49.2 42.8 43.4 55.6 46.9 40.3 63.7 97.6 119 52.1 42.7 51.9 41.8 39.4 56.9\nQiu et al. [24] (*, T= 1) 24.0 26.7 23.2 24.3 24.8 22.8 24.1 28.6 32.1 26.9 31.0 25.6 25.0 28.0 24.4 26.2\nIskakov et al. [22] (*, T= 1) 19.9 20.0 18.9 18.5 20.5 19.4 18.4 22.1 22.5 28.7 21.2 20.8 19.7 22.1 20.2 20.8\nHe et al.(IMU) [21] (*,T= 1) 25.7 27.7 23.7 24.8 26.9 31.4 24.9 26.5 28.8 31.7 28.2 26.4 23.6 28.3 23.5 26.9\nZhang et al. [23] (*, T= 1) 17.8 19.5 17.6 20.7 19.3 16.8 18.9 20.2 25.7 20.1 19.2 20.5 17.2 20.5 17.3 19.5\nZhang et al. [63] (*, T= 1) — — — — — — — — — — — — — — — 21.7\nRemeli et al. [64] (*, T= 1) 27.3 32.1 25.0 26.5 29.3 35.4 28.8 31.6 36.4 31.7 31.2 29.9 26.9 33.7 30.4 30.2\nMTF-Transformer+ (CPN,T= 1) 23.8 26.0 23.9 25.0 28.2 29.7 23.6 25.5 30.1 37.3 26.6 24.5 27.4 23.1 23.4 26.5\nMTF-Transformer+ (CPN,T= 27) 23.4 25.2 23.1 24.4 27.4 28.5 22.8 25.2 28.7 36.2 25.9 23.6 26.6 22.6 22.7 25.8\nMulti-view methods without camera parameters\nHuang et al. [26] ( ∗,T= 1) 26.8 32.0 25.6 52.1 33.3 42.3 25.8 25.9 40.5 76.6 39.1 54.5 35.9 25.1 24.2 37.5\nFLEX [46] ( [22], T= 27) 23.1 28.8 26.8 28.1 31.6 37.1 25.7 31.4 36.5 39.6 35.0 29.5 35.6 26.8 26.4 30.9\nFLEX [46] (CPN, T= 27) — — — — — — — — — — — — — — — 31.7\nMTF-Transformer (CPN,T= 1) 24.2 26.4 26.1 25.6 29.4 29.7 25.1 25.4 32.4 37.4 27.1 25.4 29.5 23.8 24.4 27.5\nMTF-Transformer (CPN,T= 27) 23.1 25.4 24.7 24.5 27.9 28.3 23.9 24.6 30.7 35.7 25.8 24.2 28.4 22.8 23.1 26.2\nMTF-Transformer(CPN,T= 27)\nno added view24.6 25.4 24.8 24.6 28.7 29.1 23.9 25.6 31.4 36.2 26.6 24.7 28.9 23.7 23.6 26.6\nFLEX [46] (GT, T= 27) — — — — — — — — — — — — — — — 22.9\nMTF-Transformer (GT,T= 27) 15.5 17.1 13.7 15.5 14.0 16.2 15.8 16.5 15.8 16.1 14.5 14.5 16.9 14.3 13.7 15.3\na direction and the order of frames matters, the position encoding\nis employed here. In addition, TFT masks some frames during the\ntraining stage to be compatible with a single image in static scenes\nand multi-view videos in dynamic scenes. For example, when the\ninput video sequence has 7 frames, the left and right frames are\nmasked evenly.\n3.4 Loss Function\nThe loss Function consists of two components. We employ the\nmean per joint position error (MPJPE) as the training loss and the\ntesting metric. MPJPE ﬁrst aligns the root joint (central hip) of\npredicted skeleton S = {pi}J\ni=1 and the ground truth skeleton\nSgt = {pgt\ni }J\ni=1, and then calculates the average Euclidean\ndistance between each joints of them. MPJPE is computed as:\nLM (S) = 1\nJ\nJ∑\ni=1\n∥pi −pgt\ni ∥2 (17)\nBesides, we utilize the rotation matrix between each pair of views\nto constrain the transform matrix Tij, an extra transform error is\nalso used as:\nLt = ∥Tij −T\n′\nij∥1 (18)\nThe total loss function is:\nL= LM + λLt (19)\n3.5 Implementation Details\nMTF-Transformer is an end-to-end method implemented with\nPytorch. We employ a pretrained 2D detector with frozen weights\nin the training stage. During the training phase, batch size, learning\nrate, learning decay, and dropout rate are set to 720, 1e−3, 0.95,\n0.1, respectively. Note that learning decay is executed after the\nend of every epoch. We adopt the same strategy for BN momentum\ndecay as in [16] and use Adam Optimizer for all modules. Besides,\nwe set the channel C to 600 and the λin the loss function to 0.5\nand train the model with 60 epochs.\n4 E XPERIMENTS\nIn this section, we ﬁrst report quantitative and qualitative results\nof MTF-Transformer on three datasets. Then, we conduct ablation\nstudies to verify the effectiveness of our design in all modules.\nConsidering the clarity and brevity of this section, we place some\nablation studies on the hyper-parameters in the appendix part.\n4.1 Datasets\nWe evaluate MTF-Transformer on three datasets, including:\nHuman3.6M (H36M)[29] is a large publicly available 3D human\npose benchmark for both monocular and multi-view setups. It\nconsists of 3.6 million image frames from 4 synchronized 50Hz\ndigital cameras, and the corresponding 2D pose and 3D pose are\ncaptured by the MoCap system in a constrained indoor studio\nenvironment. Each actor performs 15 everyday activities such\nas walking, discussing, etc. Following previous works [8], [65],\n[66], we use 5 subjects (S1, S5, S6, S7, S8) for training and 2\nsubjects (S9, S11) for testing, and report MPJPE [16], [17], [43]\nas the evaluation metric. We simulate an additional virtual view\nwhen training MTF-Transformer to enhance its ﬂexibility. The 2D\npose in the virtual view is synthesized via random rotation and\nprojection, following Cheng et al. [61]. To verify the effectiveness\nof the virtual view, we also report the result of MTF-Transformer\ntrained with no added view.\nTotalCapture [30] is captured from 8 calibrated full HD video\ncameras recording at 60Hz. It features ﬁve subjects. Each subject\nperforms four diverse performances 3 times, involving ROM,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nTABLE 2\nQuantitative results on TotalCapture. We employ HRNet-W32(D1), ResNet50(D2), ResNet101(D3) as 2D pose detector. MTF-Transformer and\nMTF-Transformer+ are trained with 27 frames, whereT is the length of the sequence for testing.\nMethods\nSeen Cameras(1,3,5,7) Unseen Cameras(2,4,6,8)\nSeen Subjects(S1, S2, S3) Unseen Subjects(S4, S5) Mean Subjects(S1, S2) Unseen Subjects(S4, S5) MeanW2 FS3 A3 W2 FS3 A3 W2 FS3 A3 W2 FS3 A3\nMulti-view methods with camera parameters\nQiu et al. [24] 19.0 28.0 21.0 32.0 54.0 33.0 29.0 —— —— —— —— —— —— ——\nRemeli et al. [64] 10.6 30.4 16.3 27.0 65.0 34.2 27.5 22.4 47.1 27.8 39.1 75.7 43.1 38.2\nMTF-Transformer+(D1,T = 1) 12.3 28.3 18.5 27.2 49.1 33.4 26.0 16.2 31.4 20.2 29.3 51.1 36.3 28.6\nMTF-Transformer+(D1,T = 27) 11.5 27.4 17.8 27.0 48.5 33.3 25.4 15.6 30.5 19.4 29.2 50.6 36.2 28.1\nMTF-Transformer+(D2,T = 1) 11.8 28.3 18.0 27.4 50.2 33.7 26.0 15.6 31.3 19.7 29.7 53.3 35.0 28.5\nMTF-Transformer+(D2,T = 27) 11.2 27.5 17.3 27.4 49.3 33.7 25.4 15.2 30.7 18.9 29.6 52.5 35.2 28.1\nMTF-Transformer+(D3,T = 1) 11.4 27.5 17.3 27.5 50.2 34.1 25.7 14.5 30.0 18.8 29.4 50.1 35.5 27.5\nMTF-Transformer+(D3,T = 27) 10.7 26.5 16.7 27.4 49.4 34.1 25.1 13.9 29.2 18.1 29.2 49.5 35.6 27.0\nMulti-view methods without camera parameters\nFLEX [46] (D1,T=27) 38.3 80.8 39.7 40.0 131.2 57.7 50.2 107.3 149.6 103.1 116.7 190.2 106.8 120.2\nFLEX [46] (D2,T=27) 34.5 78.3 36.4 39.3 128.3 59.4 48.2 106.4 141.7 103.7 114.8 177.3 122.3 119.7\nFLEX [46] (D3,T=27) 33.2 81.0 34.2 38.3 123.8 59.5 49.4 109.3 152.1 105.3 114.3 175.5 122.5 125.4\nMTF-Transformer(D1,T = 1) 11.1 30.0 16.3 26.0 53.4 32.9 25.9 26.3 44.7 30.2 37.6 66.3 43.9 39.3\nMTF-Transformer(D1,T = 27) 9.8 27.8 14.9 25.8 51.6 32.7 24.6 25.7 43.4 29.4 37.4 64.7 44.1 38.6\nMTF-Transformer(D2,T = 1) 10.9 29.8 16.2 26.9 54.2 33.5 26.1 26.3 45.3 29.9 38.4 66.5 43.4 39.5\nMTF-Transformer(D2,T = 27) 9.7 27.8 14.9 26.6 52.4 33.3 24.9 25.8 44.0 29.2 38.2 65.0 43.6 38.8\nMTF-Transformer(D3,T = 1) 10.5 28.4 15.6 26.9 54.7 33.8 25.7 24.2 41.4 28.1 37.1 63.0 42.4 37.2\nMTF-Transformer(D3,T = 27) 9.3 26.5 14.5 26.7 53.1 33.8 24.7 23.7 40.3 27.4 37.0 61.8 42.9 36.6\nWalking, Acting, and Freestyle. Accurate 3D human joint loca-\ntions are obtained from a marker-based motion capture system.\nFollowing previous work, the training set consists of “ROM1,2,3”,\n“Walking1,3”, “Freestyle1,2”, “Acting1,2”, on subjects 1,2,\nand 3. The test set consists of “Walking2 (W2)”, “Freestyle3\n(FS3)”, and “Acting3 (A3)” on subjects 1, 2, 3, 4, and 5. The\nnumber following each action indicates the video from which the\naction is. For example, Freestyle has three videos of the same\naction, of which 1 and 2 are used for training and 3 for testing.\nCamera 1,3,5,7 is used in both the training and testing set, but\ncamera 2,4,6,8 only appear in the testing set. That is to say. The\ntesting set has some unseen camera conﬁguration.\nKTH Multiview Football II[31] consists of 8000+ images of\nprofessional footballers during a match in the Allsvenskan league.\nIt is ﬁlmed by moving cameras and contains 14 joints(top-head,\nneck, shoulders, hips, knees, feet, elbows, and hands). To match\nthe topology of H36M, we create the root (pelvis) by averaging\nthe hips, the nose by averaging the neck and top-head, and the\nspine by averaging the root and the neck.\n4.2 Quantitative Evaluation\nWe report the quantitative results of MTF-Transformer on\nHuman3.6M and TotalCapture. MTF-Transformer and MTF-\nTransformer+ represent the vanilla MTF-Transformer and the\nMTF-Transformer utilizing camera parameters, respectively. In\nMTF-Transformer+, we directly use the transform matrix T\n′\nij\ncalculated from rotate matrix Rij between 3D ground truth.\nHuman3.6M: The quantitative results of MTF-Transformer and\ncompetitive methods are shown in TABLE 1. When CPN is\nused as a 2D pose detector, MTF-Transformer outperforms all\nthe monocular methods, and it decreases the MPJPE by 1.3\nwhen increasing the length of sequence from 1 to 27, indicating\nthat multi-view and temporal information beneﬁts for 3D pose\nestimation. When we employ Ground Truth as 2D pose input, both\n[46] and MTF-Transformer obtain signiﬁcant improvement, indi-\ncating that 2D pose plays an essential role in 2D-to-3D methods.\nCompared to multi-view methods with camera calibration, MTF-\nTransformer is superior to [62], [64], and [21] but inferior to\nothers. It shows that MTF-Transformer is competitive, but camera\ncalibration still has an obvious advantage. Compared to Multi-\nview methods without calibration, MTF-Transformer achieves the\nbest performance and demonstrates its superiority. Considering the\ndifﬁculty of calibrating the camera in real-time, MTF-Transformer\nis a satisfactory attempt. Besides, when we extend to MTF-\nTransformer+, we further improve the result. MTF-Transformer+\nis inferior to some calibration-need methods. Considering that our\nfocus is on fusing multi-view features without calibration and\nthose superior methods utilize extra sensors or 2D image features,\nthe performance of the MTF-Transformer+ is acceptable.\nTotalCapture: The quantitative results of MTF-Transformer and\ncompetitive methods are shown in TABLE 2. MTF-Transformer\nseries are trained on camera 1, 3, 5, 7 of the training set, and\ntested on camera 1, 3, 5, 7 (seen) and camera 2, 4, 6, 8 (unseen)\nof the testing set. The testing set includes both seen subjects and\nunseen subjects in the training set. From the vertical comparison,\nMTF-Transformer+ outperforms [24] and [64] with arbitrary 2D\ndetector and the length of sequence, and MTF-Transformer has\nsuperior result over [46]. Besides, the 2D detector has an inﬂuence\non the result, and increasing the length of the sequence improves\nthe performance. Moreover, MTF-Transformer with ResNet101 as\n2D detector obtains better result than [24] and [64] demonstrating\nthe superiority of our method. From horizontal analysis, all the\nmethods achieve better performance on seen cameras than on\nunseen cameras, on seen subjects than on unseen subjects. It\nmeans that generalization is an important issue for 3D pose\nestimation.\n4.3 Qualitative Evaluation\nSome results of FLEX and MTF-Transformer on Human3.6M are\nshown in Fig. 6. Both FLEX and MTF-Transformer improve the\nprediction as the number of views increases, but MTF-Transformer\nhas better results when the number of views is low. The reason is\nthat MTF-Transformer uses CAA to reduce the inﬂuence of 2D\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nFig. 6. Results of FLEX and MTF-Transformer with different view numbers on the Human3.6M\nFig. 7. Demonstration of transfer FLEX and MTF-Transformer trained on Human3.6M to KTH Multiview Football II\ndetector errors and introduces Relative-Attention to improve the\nfusion efﬁciency between multi-view features. To further verify\nthe generalization of MTF-Transformer under different camera\nconﬁgurations, we test the model trained on Human3.6M on\nmore challenging KTH Multiview Football II. Some results of\ngeneralization experiments are shown in Fig. 7. It demonstrates\nthat MTF-Transformer can generalize well from an indoor lab\nscene to the wild environment because it stands free from camera\nparameters and measures the implicit relationship between views\nadaptively. Although FLEX is also parameter-free, it aggregates\nthe features from multiple viewpoints into one feature and then\nsplits it into different viewpoints. The viewpoint awareness is\ntwisted in the procedure by FLEX while MTF-Transformer keeps\neach viewpoint’s independence.\n4.4 Ablation Study\nIn this section, we verify the effectiveness of all modules of MTF-\nTransformer on Human3.6M. We train all the models with 5 views\n(4 cameras and an additional synthesized view) and test them with\ndifferent views unless otherwise stated. To eliminate the effect of\nthe 2D detector, we take 2D detection from CPN [55] as input.\n4.4.1 Analysis on Conﬁdence Attentive Aggregation\nMTF-Transformer employs the Conﬁdence Attentive Aggregation\n(CAA) module in Feature Extractor to reduce the impact of the\nunreliable 2D pose. We report the results of MTF-Transformer\nwith and without CAA. Besides, we also evaluate the technique\nof concatenating the 2D pose and conﬁdence values. As shown in\nTABLE 3, concatenating can improve the performance, compared\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE 3\nResults of different procedures to fuse the 2D pose and the conﬁdence\nfrom 2D detector on Human3.6M.\nSequence\nlength T\nNumber of Views N Parameters\n(M)1 2 3 4\nno conﬁdence\n1\n52.3 36.8 31.6 29.4 9.8\nconcatenate 52.2 36.7 31.4 29.2 9.9\nCAA 50.7 35.3 30.1 28.0 10.1\nno conﬁdence\n3\n51.3 35.9 30.8 28.7 9.8\nconcatenate 51.3 35.9 30.7 28.6 9.9\nCAA 49.8 34.5 29.4 27.3 10.1\nno conﬁdence\n5\n51.0 35.6 30.6 28.5 9.8\nconcatenate 50.9 35.6 30.4 28.3 9.9\nCAA 49.4 34.2 29.2 27.1 10.1\nno conﬁdence\n7\n50.7 35.4 30.5 28.4 9.8\nconcatenate 50.8 35.4 30.3 28.2 9.9\nCAA 49.2 34.1 29.1 27.1 10.1\nTABLE 4\nThe results of different design of CAA\nMethod MPJPE\nfg = ¯f\ng\n+ ag ·pg\n2D 27.121\nfg = Fg\nres (ag ·pg\n2D) 27.499\nfg = Fg\nres\n(\n¯f\ng\n+ ag ·¯f\ng)\n27.121\nfg = Fg\nres\n(\n¯f\ng\n+ ag ·pg\n2D\n)\n27.056\nwith the circumstance without conﬁdence. When CAA takes\nthe place of concatenating, MTF-Transformer can achieve better\nperformance at all the number of views.\nWe also conduct experiments to verify the design of CAA.\nWhen we remove the res-blocks, the result of CAA is fg = ¯f\ng\n+\nag ·pg\n2D; When we remove the shortcut connection, the result of\nCAA is fg = Fg\nres (ag ·pg\n2D); If we let ¯f\ng\ntakes the place of\npg\n2D, the result of CAA is fg = Fg\nres\n(\n¯f\ng\n+ ag ·¯f\ng)\n. As shown\nin TABLE 4, when we modulate the 2D pose and employ both\nshortcut and res-blocks, CAA achieves the best performance.\n4.4.2 Analysis on Multi-view Fusing Transformer\nThe Multi-view Fusing Transformer (MFT) measures the relation-\nship between each pair of views and reconstructs the features\naccording to the relationship. To validate the effectiveness of\nMFT, we compare its result with other multi-view fusing methods\non Human3.6M, in the aspects of precision and generalization\ncapability.\nIn the aspect of precision, we compare MFT with conven-\ntional transformer (removing absolution position encoding), point\nTransformer, and MFT without transform matrix Tij. To adapt\nPoint Transformer to our task, we replace the 3D coordinates of\nthe point cloud with the ﬂattened 2D pose from the 2D detector,\nresulting in more parameters to deal with relative position encod-\ning. Results in TABLE 5 demonstrate that MFT outperforms other\nmethods in the vast majority of cases. Notably, the performance\nof Point Transformer is only slightly inferior to MFT, reﬂecting\nthe effectiveness of relative position encoding. Besides, MFT and\nMFT w/o Tij have little difference in results when only utilizing 1\nview, but MFT achieves better performance when more viewpoints\nparticipate in multi-view fusing. It means the transform matrix\nTij plays a vital role in multi-view feature fusing. In addition, we\nTABLE 5\nResults of different relative attention modules on Human3.6M. T is the\nlength of sequence\nMethod T Number of Views N Parameters\n(M)1 2 3 4\ntransformer\n1\n50.8 38.7 34.0 31.9 10.4\npoint transformer 51.2 36.1 30.8 28.4 11.7\nMFT w/o Tij 50.6 37.8 32.8 30.5 9.7\nMFT 50.7 35.3 30.1 28.0 10.1\ntransformer\n3\n49.9 37.8 33.1 31.0 10.4\npoint transformer 50.2 35.2 30.1 27.9 11.7\nMFT w/o Tij 49.8 37.0 32.0 29.7 9.7\nMFT 49.8 34.5 29.4 27.3 10.1\ntransformer\n5\n49.6 37.5 32.8 30.7 10.4\npoint transformer 49.9 35.0 29.9 27.7 11.7\nMFT w/o Tij 49.5 36.6 31.6 29.4 9.7\nMFT 49.4 34.2 29.2 27.1 10.1\ntransformer\n7\n49.4 37.3 32.6 30.6 10.4\npoint transformer 49.7 34.8 29.8 27.6 11.7\nMFT w/o Tij 49.3 36.4 31.5 29.3 9.7\nMFT 49.2 34.1 29.1 27.1 10.1\nTABLE 6\nGeneralization capability of different relative attention modules. We\ntrain all the models on Human3.6M with 2 views, test them with\ndifferent number of views.\nMethod T Number of views N Parameters\n(M)1 2 3 4\ntransformer\n1\n55.9 47.1 43.6 41.2 10.4\npoint transformer 58.0 51.8 49.9 48.7 11.7\nno MFT 57.0 7.5\nMFT 56.4 46.1 41.5 39.0 10.1\ntransformer\n3\n55.1 46.3 42.7 40.2 10.4\npoint transformer 57.2 51.2 49.4 48.3 11.7\nno MFT 56.2 7.5\nMFT 55.7 45.4 40.9 38.4 10.1\ntransformer\n5\n54.9 46.0 42.4 40.0 10.4\npoint transformer 57.0 50.9 49.2 48.2 11.7\nno MFT 56.0 7.5\nMFT 55.4 45.1 40.6 38.2 10.1\ntransformer\n7\n54.7 45.9 42.3 39.8 10.4\npoint transformer 56.8 50.8 49.1 48.0 11.7\nno MFT 55.7 7.5\nMFT 55.2 45.0 40.5 38.0 10.1\ndisplay some results of MFT with and without Tij in Fig. 8. The\nMFT with Tij can predict a more accurate 3D pose than the MFT\nwith Tij, especially the position of hand and foot. The reason\nis that Tij can transform the feature from the source view to the\ntarget view. Then the transformed feature is fused with an element-\nwisely product. Without Tij, the feature always lies in the source\nview, and the element-wise product is not effective enough for\nmulti-view fusing.\nIn the aspect of generalization capability, we compare MFT\nwith transformer and point Transformer. We also report the per-\nformance of MTF-Transformer without MFT. As MFT has the\ninput and output of the same shape, removing MFT does not affect\nsubsequent modules. We train these models on two views (camera\n0, 2) and test them in an increasing number of views from seen\ncameras (0, 2) to unseen cameras (1, 3). As shown in TABLE 6,\nthe transformer achieves the best result when only 1 view is used.\nMFT gets superior performance as the view number increases from\n2 to 4.\nTo further explain the utility of relative attention modules, we\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nFig. 8. Demonstration of MFT with and without Tij on Human3.6M\nTABLE 7\nResults on Human3.6M with different setting of D. T is the sequence\nlength.\nD T Number of views N Parameter\n(M)1 2 3 4\n1\n1\n51.8 37.0 31.7 29.6 10.074\n2 51.3 35.9 30.6 28.4 10.076\n3 51.2 35.9 30.7 28.5 10.079\n4 50.7 35.3 30.1 28.0 10.083\n5 50.9 35.8 30.6 28.4 10.088\n6 51.3 35.5 30.3 28.1 10.095\n1\n3\n50.8 36.0 30.8 28.8 10.074\n2 50.4 35.1 29.9 27.8 10.076\n3 50.3 35.1 29.9 27.8 10.079\n4 49.8 34.5 29.4 27.3 10.083\n5 50.1 35.0 29.9 27.8 10.088\n6 50.4 34.7 29.6 27.5 10.095\n1\n5\n50.5 35.6 30.5 28.6 10.074\n2 50 34.8 29.7 27.6 10.076\n3 49.9 34.7 29.6 27.5 10.079\n4 49.4 34.2 29.2 27.1 10.083\n5 49.7 34.7 29.7 27.6 10.088\n6 50 34.4 29.4 27.3 10.095\n1\n7\n50.4 35.5 30.4 28.5 10.074\n2 49.8 34.6 29.5 27.5 10.076\n3 49.7 34.6 29.5 27.5 10.079\n4 49.2 34.1 29.1 27.1 10.083\n5 49.5 34.6 29.6 27.5 10.088\n6 49.8 34.2 29.3 27.2 10.095\nalso display the contribution of the feature from each view to the\nﬁnal prediction in Fig. 9, inspired by the Grad-CAM [67]. There is\na slight generalization gap when we train models on 4 views and\ntest on the same number of views. For the transformer, the feature\nfrom the target view makes almost the majority contribution to the\nﬁnal prediction. Instead, MFT and point transformer get uniform\ncontributions from all the views, indicating that relative position\nTABLE 8\nResults of different mask rate M on Human3.6M. MTF-Transformer is\ntrained on the training set with 5 views at different mask rate. We\nevaluate these models with different number of views as input.\nMask rate M 0 0.2 0.4 0.6 0.8 1.0\nNumber of views\nN\n1 205.2 52.2 49.2 49.2 48.8 49.4\n2 78.7 35.3 34.1 34.7 36.1 117.5\n3 44.5 29.3 29.1 30.1 31.7 127.5\n4 25.6 26.8 27.1 28.2 30.0 134.5\nMean 89.0 35.9 34.9 35.5 36.7 107.2\nis essential to fuse multi-view information. When we train MFT\nand point transformer on 2 views and test them on 4 views,\nthere is a big generalization gap between the training and testing\nphase. We can ﬁnd that MFT fuses the feature from other views\nmore effectively. It is intuitive and veriﬁes the effectiveness of the\nproposed Relative-Attention block.\nIn MFT, we divide the dimensionCof input xi into Kgroups.\nTo explore the effect of K on the results, we train the model with\ndifferent settings of D because C = D×K and D determines\nthe shape of Tij directly. The results in TABLE 7 demonstrate\nMTF-Transformer achieves the best performance at the dimension\nof 4 with different sequence lengths.\n4.4.3 Analysis on Random Block Mask\nRandom Block Mask is designed to ensure the generalization ca-\npability of the MTF-Transformer. To verify the effectiveness of the\nRandom Block Mask, we train MTF-Transformer on Human3.6M\ntraining set with 5 views and set the mask rate M at 0, 0.2,\n0.4, 0.6, 0.8, 1.0, respectively. With M increasing, more features\nfrom different views are dropped in the training stage. M = 0\nindicates that all the views participate in the feature fusing among\nall the views. Each view only fuses with itself when M = 1. In\nthe testing stage, we test the MTF-Transformer counterparts with\ndifferent mask rates via feeding testing samples with a different\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nFig. 9. The contribution of the 2D pose (17 joint point) from 4 views to the\n3D prediction (X, Y , Z coordinate) with different kind of Relative-Attention\nmodule. We measure the gradient of the predicted 3D coordinate to\nthe features of each views and consider the maximum value as the\ncontribution ratio. For better visualization, the values are normalized to\nthe range of 0 to 1. (a) Transformer (b) Point Transformer trained on\n4 views (c) MFT trained on 4 views (d) Point Transformer trained on 2\nviews (e) MFT trained on 2 views.\nnumber of views (including 1, 2, 3, and 4 views). The results are\nshown in TABLE 8. From the vertical comparison, at most mask\nrates, the performance of the MTF-Transformer gets better as the\nnumber of views increases, except for the mask rate of 1. When\nthe mask rate is set at 1, the MFT module fails to measure the\nrelationship among the features since all the interconnections are\nmasked. It veriﬁes that fusing multi-view features can improve\nthe performance of 3D pose estimation. From horizontal analysis,\nwhen the number of views is set at 4, MTF-Transformer achieves\nthe best performance at the mask rate of 0. This number of\nviews in the testing stage is close to that of the training stage\n(5 views). As the number of views for testing decreases, the\ndifference between training and testing is enlarged, and MTF-\nTransformer achieves the best performance at a higher mask rate.\nIt demonstrates that the Random Block Mask module is essential\nFig. 10. Some predictions under different mask rate\nfor scenes greatly different from the training stage. The purpose of\nMFT-Transformer is to handle the input from an arbitrary number\nof views adaptively, so we evaluate the mean value of the MPJPE\nat different mask rates. We ﬁnd that the mask rate of 0.4 has\nthe best result, and we will set the mask rate at 0.4 in all the\nexperiments. Some predict results are shown in Fig. 10.\n4.4.4 Analysis on Sequence Length\nMTF-Transformer can adaptively handle videos with different\nsequence lengths. We evaluate it via feeding videos with the\nlength from 1 to 27. The results are shown in TABLE. 9. The\nperformance of the MTF-Transformer increases as the sequence\nlength increases, at any number of views as input. It inﬂects that\na more extended period of input beneﬁts the pose estimation.\nInterestingly, MTF-Transformer converges on certain precision\nas the sequence length number increases. The more views are\ninvolved, MTF-Transformer converges on better precision and\ntends to saturate more quickly. We utilize multi-view and tem-\nporal clues to estimate the pose of the middle frame under each\nviewpoint. Geometric and temporal information is complementary\nto each other. Thus, when more multi-view clues are used, MTF-\nTransformer needs less temporal information to reconstruct the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nTABLE 9\nResults of different sequence length T on Human3.6M.\nSequence length\nT\nNumber of views N\n1 view 2 views 3 views 4 views\n1 51.62 35.13 29.74 27.46\n3 50.70 34.28 28.99 26.77\n5 50.28 33.91 28.69 26.51\n7 50.01 33.71 28.55 26.41\n9 49.82 33.57 28.44 26.32\n11 49.66 33.47 28.38 26.28\n13 49.53 33.42 28.35 26.27\n15 49.45 33.39 28.35 26.27\n17 49.39 33.37 28.34 26.28\n19 49.33 33.34 28.33 26.27\n21 49.27 33.32 28.32 26.26\n23 49.23 33.29 28.30 26.24\n25 49.21 33.27 28.28 26.22\n27 49.19 33.26 28.27 26.21\nTABLE 10\nResults on Human3.6M with different number of added view.\nT Added view The number of views N Parameters\n(M)1 2 3 4\n1\n0 51.0 35.7 30.6 28.4\n10.1\n1 50.7 35.3 30.1 28.0\n2 51.4 35.8 30.4 28.1\n3\n0 50.2 35.0 30.0 27.9\n1 49.8 34.5 29.4 27.3\n2 50.5 35.0 29.7 27.5\n5\n0 49.9 34.7 29.7 27.7\n1 49.4 34.2 29.2 27.1\n2 50.1 34.8 29.5 27.2\n7\n0 49.7 34.6 29.6 27.6\n1 49.2 34.1 29.1 27.1\n2 49.9 34.6 29.3 27.1\n3D pose. Moreover, multi-view clues have some information that\ndoes not exist in temporal clues, so more viewpoints lead to better\nconvergence results.\n4.4.5 Analysis on added synthesized views\nWhen training Human3.6M, we added a synthesized view to train\nMTF-Transformer as a data enhancement mechanism, following\ncheng et al. [61]. To quantiﬁcat the effect of added views, we\ncompare the results with a different number of added views\nin TABLE 10. MTF-Transformer achieves the best performance\nwhen we add 1 synthesized view.\n4.4.6 Analysis on computational complexity\nAs shown in TABLE 11, we report the total number of parameters\nand estimated multiply-add operations (MACs) per frame (the\n2D detector is not included). For comparison, we also report\nparameters and MACs of Iskakov et al. [22]. Similar to MTF-\nTransformer, Iskakov et al. [22] also infers the 3D pose via lifting\nmulti-view 2D detections to 3D detections. MTF-Transformer has\na slightly less number of parameters and orders of magnitude less\ncomputational complexity. The reason is that MTF-Transformer\nemploys 1D convolution to manipulate the features instead of\n3D convolution. We also report the time consumption of MTF-\nTransformer in the training and testing phase in TABLE 12. MFT-\nTransformer is a magnitude faster than FLEX for inference time,\ntested in the same device. Besides, increasing the number of views\nleads to a slight time consumption increment.\nTABLE 11\nComputational complexity of MTF-Transformer. We use THOP1to\nrepresent the number of parameters and MACs (multiply-add\noperations). T is the length of sequence and N is the number of views.\nMethod T N Parameters(M) MACs(G)\nIskakov et al. [22] 1 4 11.9 155\nFLEX [46] 27 4 70.6 4.27\nMTF-Transformer\n1\n1\n10.1\n0.01\n2 0.03\n3 0.05\n4 0.07\n27\n1\n10.1\n0.27\n2 0.68\n3 1.23\n4 1.91\nTABLE 12\nThe time consumption in training and testing phase.T is the length of\nsequence and N is the number of views.\nMethods Device T N time\nTraining\nMTF-Transformer 2×2080Ti 7 - 12h(60epochs)\n27 - 34h(60epochs)\nTesting\nFLEX [46]\n1×2080Ti\n27 4 30.2ms\nMTF-Transformer\n1\n1 8.4ms\n2 8.6ms\n3 8.6ms\n4 8.8ms\n27\n1 9.2ms\n2 9.3ms\n3 9.5ms\n4 9.9ms\n5 C ONCLUSION\nWe present a uniﬁed framework MTF-Transformer to fuse multi-\nview sequences in uncalibrated scenes with an arbitrary number of\nviews. MTF-Transformer can adaptively measure the relationship\nbetween each pair of views with a relative-attention mechanism,\navoiding the dependency on camera calibration. It is also com-\nputationally lightweight and can be directly applied to settings\nwhere the number of views and video frames varies. Extensive\nexperimental results demonstrate the effectiveness and robustness\nof the MTF-Transformer.\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science Foun-\ndation of China (61825601, U21B2044), and Science and Tech-\nnology Program of Jiangsu Province (BK20192004B).\nREFERENCES\n[1] C. Wang, Y . Wang, and A. L. Yuille, “Mining 3d key-pose-motifs for\naction recognition,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). IEEE, June 2016. 1\n[2] J. Liu, A. Shahroudy, D. Xu, A. C. Kot, and G. Wang, “Skeleton-based\naction recognition using spatio-temporal lstm network with trust gates,”\nIEEE transactions on pattern analysis and machine intelligence, vol. 40,\nno. 12, pp. 3007–3021, 2017. 1\n1. github.com/Lyken17/pytorch-OpCounter\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\n[3] D. C. Luvizon, D. Picard, and H. Tabia, “Multi-task deep learning\nfor real-time 3d human pose estimation and action recognition,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , vol. 43,\nno. 8, pp. 2752–2764, 2021. 1\n[4] R. A. Guler and I. Kokkinos, “Holopose: Holistic 3d human recon-\nstruction in-the-wild,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). IEEE, June 2019. 1\n[5] H.-S. Fang, Y . Xu, W. Wang, X. Liu, and S.-C. Zhu, “Learning pose\ngrammar to encode human body conﬁguration for 3d pose estimation,”\nin Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 32,\nno. 1. IEEE, 2018. 1, 2\n[6] K. Ehlers and K. Brama, “A human-robot interaction interface for mobile\nand stationary robots based on real-time 3d human body and hand-\nﬁnger pose estimation,” in 2016 IEEE 21st International Conference on\nEmerging Technologies and Factory Automation (ETFA). IEEE, 2016,\npp. 1–6. 1\n[7] T. Tao, X. Yang, J. Xu, W. Wang, S. Zhang, M. Li, and G. Xu,\n“Trajectory planning of upper limb rehabilitation robot based on human\npose estimation,” in 2020 17th International Conference on Ubiquitous\nRobots (UR). IEEE, 2020, pp. 333–338. 1\n[8] J. Martinez, R. Hossain, J. Romero, and J. J. Little, “A simple yet\neffective baseline for 3d human pose estimation,” in Proceedings of the\nIEEE international conference on computer vision . IEEE, 2017, pp.\n2640–2649. 1, 2, 4, 6\n[9] H. Zhang, J. Cao, G. Lu, W. Ouyang, and Z. Sun, “Learning 3d human\nshape and pose from dense body parts,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2020. 1\n[10] C. Wang, Y . Wang, Z. Lin, and A. L. Yuille, “Robust 3d human pose\nestimation from single images or video sequences,” IEEE transactions\non pattern analysis and machine intelligence , vol. 41, no. 5, pp. 1227–\n1241, 2018. 1\n[11] H. Ci, X. Ma, C. Wang, and Y . Wang, “Locally connected network for\nmonocular 3d human pose estimation,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 44, no. 3, pp. 1429–1442, 2022.\n1\n[12] W. Liu, Q. Bao, Y . Sun, and T. Mei, “Recent advances in monocular\n2d and 3d human pose estimation: A deep learning perspective,” arXiv\npreprint arXiv:2104.11536, 2021. 1\n[13] F. Moreno-Noguer, “3d human pose estimation from a single image via\ndistance matrix regression,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2017, pp. 2823–2832. 1\n[14] S. Sharma, P. T. Varigonda, P. Bindal, A. Sharma, and A. Jain, “Monoc-\nular 3d human pose estimation by generation and ordinal ranking,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 2325–2334. 1\n[15] L. Zhao, X. Peng, Y . Tian, M. Kapadia, and D. N. Metaxas, “Semantic\ngraph convolutional networks for 3d human pose regression,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 3425–3435. 1, 2\n[16] D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli, “3d human pose\nestimation in video with temporal convolutions and semi-supervised\ntraining,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 7753–7762. 1, 2, 6\n[17] Y . Cai, L. Ge, J. Liu, J. Cai, T.-J. Cham, J. Yuan, and N. M. Thal-\nmann, “Exploiting spatial-temporal relationships for 3d pose estimation\nvia graph convolutional networks,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2019, pp. 2272–2281. 1,\n2, 6\n[18] J. Wang, S. Yan, Y . Xiong, and D. Lin, “Motion guided 3d pose\nestimation from videos,” in European Conference on Computer Vision .\nSpringer, 2020, pp. 764–780. 1, 6\n[19] A. Zeng, X. Sun, L. Yang, N. Zhao, M. Liu, and Q. Xu, “Learning skeletal\ngraph neural networks for hard 3d pose estimation,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision , 2021, pp.\n11 436–11 445. 1\n[20] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding,\n“3d human pose estimation with spatial and temporal transformers,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 11 656–11 665. 1, 3\n[21] Y . He, R. Yan, K. Fragkiadaki, and S.-I. Yu, “Epipolar transformers,” in\nProceedings of the ieee/cvf conference on computer vision and pattern\nrecognition, 2020, pp. 7779–7788. 1, 2, 3, 6, 7\n[22] K. Iskakov, E. Burkov, V . Lempitsky, and Y . Malkov, “Learnable trian-\ngulation of human pose,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019, pp. 7718–7727. 1, 2, 6, 12\n[23] Z. Zhang, C. Wang, W. Qiu, W. Qin, and W. Zeng, “Adafuse: Adaptive\nmultiview fusion for accurate human pose estimation in the wild,”\nInternational Journal of Computer Vision , vol. 129, no. 3, pp. 703–718,\n2021. 1, 6\n[24] H. Qiu, C. Wang, J. Wang, N. Wang, and W. Zeng, “Cross view\nfusion for 3d human pose estimation,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2019, pp. 4342–4351. 1,\n2, 6, 7\n[25] H. Ma, L. Chen, D. Kong, Z. Wang, X. Liu, H. Tang, X. Yan, Y . Xie,\nS.-Y . Lin, and X. Xie, “Transfusion: Cross-view fusion with transformer\nfor 3d human pose estimation,” in British Machine Vision Conference ,\n2021. 1, 3\n[26] F. Huang, A. Zeng, M. Liu, Q. Lai, and Q. Xu, “Deepfuse: An imu-aware\nnetwork for real-time 3d human pose estimation from multi-view image,”\nin Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, 2020, pp. 429–438. 1, 2, 6\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017. 2, 4, 5\n[28] V . Tinto, “Dropout from higher education: A theoretical synthesis of\nrecent research,” Review of educational research, vol. 45, no. 1, pp. 89–\n125, 1975. 2\n[29] C. Ionescu, D. Papava, V . Olaru, and C. Sminchisescu, “Human3. 6m:\nLarge scale datasets and predictive methods for 3d human sensing\nin natural environments,” IEEE transactions on pattern analysis and\nmachine intelligence, vol. 36, no. 7, pp. 1325–1339, 2013. 2, 6\n[30] H. Joo, T. Simon, and Y . Sheikh, “Total capture: A 3d deformation\nmodel for tracking faces, hands, and bodies,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018, pp. 8320–\n8329. 2, 6\n[31] V . Kazemi, M. Burenius, H. Azizpour, and J. Sullivan, “Multi-view body\npart recognition with random forests,” in 2013 24th British Machine\nVision Conference, BMVC 2013; Bristol; United Kingdom; 9 September\n2013 through 13 September 2013. British Machine Vision Association,\n2013. 2, 7\n[32] X. Sun, B. Xiao, F. Wei, S. Liang, and Y . Wei, “Integral human pose\nregression,” in Proceedings of the European conference on computer\nvision (ECCV), 2018, pp. 529–545. 2\n[33] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis, “Coarse-to-ﬁne\nvolumetric prediction for single-image 3d human pose,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2017,\npp. 7025–7034. 2\n[34] K. Zhou, X. Han, N. Jiang, K. Jia, and J. Lu, “Hemlets pose: Learning\npart-centric heatmap triplets for accurate 3d human pose estimation,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 2344–2353. 2\n[35] S. Li and A. B. Chan, “3d human pose estimation from monocular\nimages with deep convolutional neural network,” in Asian Conference\non Computer Vision. Springer, 2014, pp. 332–347. 2\n[36] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik, “End-to-\nend recovery of human shape and pose,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018, pp. 7122–\n7131. 2\n[37] Z. Wang, D. Shin, and C. C. Fowlkes, “Predicting camera viewpoint\nimproves cross-dataset generalization for 3d human pose estimation,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 523–\n540. 2\n[38] S. Li, L. Ke, K. Pratama, Y .-W. Tai, C.-K. Tang, and K.-T. Cheng,\n“Cascaded deep monocular 3d human pose estimation with evolutionary\ntraining data,” inProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 6173–6183. 2\n[39] S. Liu, P. Lv, Y . Zhang, J. Fu, J. Cheng, W. Li, B. Zhou, and M. Xu,\n“Semi-dynamic hypergraph neural network for 3d pose estimation.” in\nIJCAI, 2020, pp. 782–788. 2\n[40] M. Shi, K. Aberman, A. Aristidou, T. Komura, D. Lischinski, D. Cohen-\nOr, and B. Chen, “Motionet: 3d human motion reconstruction from\nmonocular video with skeleton consistency,” ACM Transactions on\nGraphics (TOG), vol. 40, no. 1, pp. 1–15, 2020. 2\n[41] L. Wu, Z. Yu, Y . Liu, and Q. Liu, “Limb pose aware networks for\nmonocular 3d pose estimation,”IEEE Transactions on Image Processing,\nvol. 31, pp. 906–917, 2021. 2\n[42] J. Xu, Z. Yu, B. Ni, J. Yang, X. Yang, and W. Zhang, “Deep kinematics\nanalysis for monocular 3d human pose estimation,” in Proceedings of\nthe IEEE/CVF Conference on computer vision and Pattern recognition ,\n2020, pp. 899–908. 2\n[43] L. Jiahao and H. L. Gim, “Trajectory space factorization for deep video-\nbased 3d human pose estimation,” in British Machine Vision Conference.\nBMV A Press, 2019, p. 101. 2, 6\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\n[44] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, “Fast and robust\nmulti-person 3d pose estimation from multiple views,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2019, pp. 7792–7801. 2\n[45] A. Kadkhodamohammadi and N. Padoy, “A generalizable approach for\nmulti-view 3d human pose regression,”Machine Vision and Applications,\nvol. 32, no. 1, pp. 1–14, 2021. 2\n[46] B. Gordon, S. Raab, G. Azov, R. Giryes, and D. Cohen-Or, “Flex:\nParameter-free multi-view 3d human motion reconstruction,” arXiv\npreprint arXiv:2105.01937, 2021. 2, 4, 6, 7, 12\n[47] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy,\n“Do vision transformers see like convolutional neural networks?” Ad-\nvances in Neural Information Processing Systems, vol. 34, 2021. 3\n[48] K. Lin, L. Wang, and Z. Liu, “End-to-end human pose and mesh\nreconstruction with transformers,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2021, pp.\n1954–1963. 3\n[49] Y . Zhu, X. Xu, F. Shen, Y . Ji, L. Gao, and H. T. Shen,\n“Posegtac: Graph transformer encoder-decoder with atrous convolution\nfor 3d human pose estimation,” in Proceedings of the Thirtieth\nInternational Joint Conference on Artiﬁcial Intelligence, IJCAI-21, Z.-H.\nZhou, Ed. International Joint Conferences on Artiﬁcial Intelligence\nOrganization, 8 2021, pp. 1359–1365, main Track. [Online]. Available:\nhttps://doi.org/10.24963/ijcai.2021/188 3\n[50] A. Llopart, “Liftformer: 3d human pose estimation using attention\nmodels,” arXiv preprint arXiv:2009.00348, 2020. 3\n[51] W. Li, H. Liu, H. Tang, P. Wang, and L. Van Gool, “Mhformer: Multi-\nhypothesis transformer for 3d human pose estimation,” arXiv preprint\narXiv:2111.12707, 2021. 3\n[52] W. Li, H. Liu, R. Ding, M. Liu, P. Wang, and W. Yang, “Exploiting tem-\nporal contexts with strided transformer for 3d human pose estimation,”\nIEEE Transactions on Multimedia, 2022. 3\n[53] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, “Rethinking spa-\ntial dimensions of vision transformers,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 11 936–11 945.\n3\n[54] Y . Kwon, D. Kim, D. Ceylan, and H. Fuchs, “Neural human performer:\nLearning generalizable radiance ﬁelds for human performance render-\ning,” Advances in Neural Information Processing Systems, vol. 34, 2021.\n3\n[55] Y . Chen, Z. Wang, Y . Peng, Z. Zhang, G. Yu, and J. Sun, “Cascaded\npyramid network for multi-person pose estimation,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition , 2018,\npp. 7103–7112. 3, 6, 8\n[56] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V . Koltun, “Point transformer,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 16 259–16 268. 4\n[57] X. Li, X. Li, A. You, L. Zhang, G. Cheng, K. Yang, Y . Tong, and Z. Lin,\n“Towards efﬁcient scene understanding via squeeze reasoning,” IEEE\nTransactions on Image Processing, vol. 30, pp. 7050–7063, 2021. 5\n[58] C. Li and G. H. Lee, “Generating multiple hypotheses for 3d human\npose estimation with mixture density network,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2019, pp. 9887–9895. 6\n[59] R. Liu, J. Shen, H. Wang, C. Chen, S.-c. Cheung, and V . Asari,\n“Attention mechanism exploits temporal contexts: Real-time 3d human\npose reconstruction,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 5064–5073. 6\n[60] A. Zeng, X. Sun, F. Huang, M. Liu, Q. Xu, and S. Lin, “Srnet: Improving\ngeneralization in 3d human pose estimation with a split-and-recombine\napproach,” in European Conference on Computer Vision . Springer,\n2020, pp. 507–523. 6\n[61] Y . Cheng, B. Yang, B. Wang, W. Yan, and R. T. Tan, “Occlusion-aware\nnetworks for 3d human pose estimation in video,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision , 2019, pp.\n723–732. 6, 12\n[62] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis, “Harvesting\nmultiple views for marker-less 3d human pose annotations,” in Proceed-\nings of the IEEE conference on computer vision and pattern recognition,\n2017, pp. 6988–6997. 6, 7\n[63] Z. Zhang, C. Wang, W. Qin, and W. Zeng, “Fusing wearable imus with\nmulti-view images for human pose estimation: A geometric approach,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 2200–2209. 6\n[64] E. Remelli, S. Han, S. Honari, P. Fua, and R. Wang, “Lightweight multi-\nview 3d pose estimation through camera-disentangled representation,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 6040–6049. 6, 7\n[65] K. Lee, I. Lee, and S. Lee, “Propagating lstm: 3d pose estimation based\non joint interdependency,” in Proceedings of the European Conference\non Computer Vision (ECCV), 2018, pp. 119–135. 6\n[66] X. Sun, J. Shang, S. Liang, and Y . Wei, “Compositional human pose\nregression,” in Proceedings of the IEEE International Conference on\nComputer Vision, 2017, pp. 2602–2611. 6\n[67] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, “Grad-cam: Visual explanations from deep networks via\ngradient-based localization,” in Proceedings of the IEEE international\nconference on computer vision, 2017, pp. 618–626. 10\nHui Shuaireceived the M.S degree from Nanjing\nUniversity of Information Science and Technol-\nogy (NUIST) in 2018. He is currently working\ntoward the PhD degree with NUIST. His current\nresearch interests include object detection and\n3D point cloud analysis.\nLele Wu received the bachelor’s degree from\nthe Nanjing University of Information Science\nand Technology (NUIST) in 2019, where he is\ncurrently pursuing the master’s degree. His re-\nsearch interests include 3D human pose estima-\ntion and computer vision.\nQingshan Liu (M’05–SM’07)received the M.S.\ndegree from Southeast University, Nanjing,\nChina, in 2000 and the Ph.D. degree from the\nChinese Academy of Sciences, Beijing, China,\nin 2003. He is currently a Professor in the School\nof Computer and Software, Nanjing University\nof Information Science and Technology, Nanjing.\nHis research interests include pattern recogni-\ntion, image and video analysis.",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7033634185791016
    },
    {
      "name": "Computer science",
      "score": 0.6739674806594849
    },
    {
      "name": "Transformer",
      "score": 0.6560389995574951
    },
    {
      "name": "Pose",
      "score": 0.6497281789779663
    },
    {
      "name": "Computer vision",
      "score": 0.5874194502830505
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5441060066223145
    },
    {
      "name": "Embedding",
      "score": 0.5322824716567993
    },
    {
      "name": "Feature extraction",
      "score": 0.4931125342845917
    },
    {
      "name": "3D pose estimation",
      "score": 0.4640120267868042
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4035574495792389
    },
    {
      "name": "Engineering",
      "score": 0.1470363736152649
    },
    {
      "name": "Voltage",
      "score": 0.11217865347862244
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200845125",
      "name": "Nanjing University of Information Science and Technology",
      "country": "CN"
    }
  ]
}