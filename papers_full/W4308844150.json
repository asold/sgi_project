{
  "title": "A novel hybrid transformer-CNN architecture for environmental microorganism classification",
  "url": "https://openalex.org/W4308844150",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5088187467",
      "name": "R.‐F. Shao",
      "affiliations": [
        null,
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5069980045",
      "name": "Xiaojun Bi",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100370684",
      "name": "Zheng Chen",
      "affiliations": [
        "Harbin Engineering University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963340466",
    "https://openalex.org/W3010421324",
    "https://openalex.org/W4253505628",
    "https://openalex.org/W6655268422",
    "https://openalex.org/W3216559522",
    "https://openalex.org/W2739917809",
    "https://openalex.org/W2000971069",
    "https://openalex.org/W2588025049",
    "https://openalex.org/W4205456819",
    "https://openalex.org/W2540861519",
    "https://openalex.org/W2954174912",
    "https://openalex.org/W3216279502",
    "https://openalex.org/W3003750976",
    "https://openalex.org/W2507414358",
    "https://openalex.org/W2963039693",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2887616201",
    "https://openalex.org/W2606453297",
    "https://openalex.org/W2026185935",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4214808100",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W4224324261",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W1659482141",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W2953333557",
    "https://openalex.org/W152027609",
    "https://openalex.org/W2125481689",
    "https://openalex.org/W2079914758",
    "https://openalex.org/W2896018215",
    "https://openalex.org/W2978155588",
    "https://openalex.org/W3015949914",
    "https://openalex.org/W3130012542",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3177143648",
    "https://openalex.org/W2917262027",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W2018471578",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3164540605",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3081929283",
    "https://openalex.org/W3156313549",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3040837244",
    "https://openalex.org/W3095305908",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1561752765",
    "https://openalex.org/W1965399445",
    "https://openalex.org/W4287324301",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4288073686",
    "https://openalex.org/W3048886990",
    "https://openalex.org/W3151941575"
  ],
  "abstract": "The success of vision transformers (ViTs) has given rise to their application in classification tasks of small environmental microorganism (EM) datasets. However, due to the lack of multi-scale feature maps and local feature extraction capabilities, the pure transformer architecture cannot achieve good results on small EM datasets. In this work, a novel hybrid model is proposed by combining the transformer with a convolution neural network (CNN). Compared to traditional ViTs and CNNs, the proposed model achieves state-of-the-art performance when trained on small EM datasets. This is accomplished in two ways. 1) Instead of the original fixed-size feature maps of the transformer-based designs, a hierarchical structure is adopted to obtain multi-scale feature maps. 2) Two new blocks are introduced to the transformer’s two core sections, namely the convolutional parameter sharing multi-head attention block and the local feed-forward network block. The ways allow the model to extract more local features compared to traditional transformers. In particular, for classification on the sixth version of the EM dataset (EMDS-6), the proposed model outperforms the baseline Xception by 6.7 percentage points, while being 60 times smaller in parameter size. In addition, the proposed model also generalizes well on the WHOI dataset (accuracy of 99%) and constitutes a fresh approach to the use of transformers for visual classification tasks based on small EM datasets.",
  "full_text": "RESEA RCH ARTICL E\nA novel hybrid transformer-CNN architecture\nfor environmental microorganism\nclassification\nRan Shao\n1,2\n, Xiao-Jun Bi\nID\n3\n*, Zheng Chen\n1\n1 College of Informatio n and Commun ication Engineeri ng, Harbin Engineering Univers ity, Harbin, China,\n2 College of Informatio n and Commun ication Engineeri ng, Harbin Vocational & Technical College, Harbin,\nChina, 3 Department of Informatio n Engineeri ng, Minzu University of China, Beijing, China\n* bixiaoju n@ hrbeu. edu.cn\nAbstract\nThe success of vision transformers (ViTs) has given rise to their application in classification\ntasks of small environmen tal microorganism (EM) datasets. However, due to the lack of multi-\nscale feature maps and local feature extraction capabilities, the pure transformer architecture\ncannot achieve good results on small EM datasets. In this work, a novel hybrid model is pro-\nposed by combining the transformer with a convolution neural network (CNN). Compared to\ntraditional ViTs and CNNs, the proposed model achieves state-of-the-art performan ce when\ntrained on small EM datasets. This is accomplished in two ways. 1) Instead of the original\nfixed-size feature maps of the transformer- based designs, a hierarchical structure is adopted\nto obtain multi-scale feature maps. 2) Two new blocks are introduced to the transformer’s two\ncore sections, namely the convolutional parameter sharing multi-head attention block and the\nlocal feed-forward network block. The ways allow the model to extract more local features\ncompared to traditional transformers. In particular, for classification on the sixth version of the\nEM dataset (EMDS-6), the proposed model outperforms the baseline Xception by 6.7 percent-\nage points, while being 60 times smaller in parameter size. In addition, the proposed model\nalso generalizes well on the WHOI dataset (accuracy of 99%) and constitutes a fresh approach\nto the use of transformers for visual classification tasks based on small EM datasets.\n1. Introduction\nEnvironmental Microorganisms (EMs) (e.g., bacteria, viruses, etc.) have a typical size range\nbetween 0.1 and 100 microns and thus cannot be detected with the naked eye. They can be\nclassified as cell-free, multicellular, or unicellular species [1]. Despite their tiny size, EMs can\nhave a significant impact on human life. On the beneficial side, some EMs are used to make\nfermented foods such as cheese and bread, break down plastics, treat sulfurous emissions from\nindustrial processes, condition soil, digest organic waste from sludge, and enhance the quality\nof freshwater [2–4]. On the harmful side, some EMs can reduce crop yields, cause food spoil-\nage, and will even lead to diseases such as the novel coronavirus disease 2019 (COVID-19) and\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 1 / 22\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Shao R, Bi X-J, Chen Z (2022) A novel\nhybrid transform er-CNN architecture for\nenvironmenta l microorganism classification . PLoS\nONE 17(11): e0277557. https://doi. org/10.1371/\njournal.pone .0277557\nEditor: Muhammad Fazal Ijaz, Sejong University,\nREPUBLIC OF KOREA\nReceived: August 14, 2022\nAccepted: October 30, 2022\nPublished: November 11, 2022\nCopyright: © 2022 Shao et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: Both datasets are\npublicly available for researchers and can be\naccessed using the following links: 1. https://\nfigshare.com /articles/da taset/EMDS6/1 7125025/1\n2. https://aslopubs .onlinelibra ry.wiley.com/do i/abs/\n10.4319/ lom.2007.5.20 4\nFunding: This research project was funded by the\nMajor Projects of the Nationa l Social Science\nFoundation of China under Grant 20&ZD279. http://\nfund.cssn .cn/skjj/. The funders had no role in study\ndesign, data collection, and analysis, decision to\npublish, or preparation of the manuscript.\ndeath [5, 6]. Therefore, the correct identification of EMs is necessary for the study of their ben-\neficial or adverse effects on other organisms and the environment [7, 8].\nDue to their small size, the classification of EMs is difficult [9]. The two traditional classifi-\ncation methods typically used for EM classification are molecular biology (e.g., DNA and RNA\nanalysis), and morphological approaches (e.g., manual observation under a microscope) [10].\nThe former has high accuracy but requires expensive equipment, while the latter requires a sig-\nnificant amount of time and effort on manual labor by a professional researcher [11, 12]. As a\nresult, many researchers have attempted to classify images using computer-aided methods\nbased on machine learning [13, 14], which has allowed the development of classification meth-\nods that are efficient and do not require substantial expertise. However, these methods still\nrequire substantial feature engineering to formulate classification conditions and cannot\nextract the EMs’ salient features automatically for classification [15].\nDue to the rapid development of artificial intelligence in recent years, deep learning-based\nmethods are being adopted for automated salient feature extraction and end-to-end classifica-\ntion. Such methods have thus emerged as primary tools for image classification [16–18]. For\nEM image classification tasks, several researchers have employed deep learning methods based\non convolutional neural networks (CNNs) [19, 20]. However, although CNNs have been shown\nto be effective in extracting local image features, their performance is lacking when it comes to\ncapturing long-distance dependencies [21]. It should be noted that for 2D image classification\ntasks, locality and long-distance dependencies have been shown to be equally important [22,\n23]. Due to their capacity to capture long-range dependencies, transformers [24, 25] are now\ncommonly used in modern deep learning applications, including computer vision (CV) [26, 27]\nand speech processing [28]. Transformers were first proposed as components of machine trans-\nlation models [29]. The vision transformer (ViT), the first pure transformer architecture used in\nCV, was proposed only recently [30]. In terms of image classification, it achieves results that are\ncompetitive with many state-of-the-art CNNs [31, 32]. Zhao P. et al. [33] first applied ViTs,\nsuch as ViT [30], DeiT [34], T2T-ViT [35] and BotNet [36], to EM classification tasks. However,\nwhen trained on the sixth version of the EM dataset (EMDS-6) [37], which is a small dataset\nconsisting of only 21 classes and 1680 EM images, these models’ performance was inferior to\nthat of similar-sized CNNs, such as Xception [38]. The reasons for this are twofold. First, due to\nthe constant patch size of ViTs, it is extremely difficult for the transformer to extract multi-scale\nfeature maps definitively, which presents a significant challenge for classification tasks [39]. Sec-\nond, ViTs lack the locality inherent in the design of CNNs, which makes them unsuitable for\nimage classification problems on small datasets like EMDS-6 [30].\nTo enhance ViT performance and raise it to a level comparable to that of the state of the art\nusing small EM datasets, in this work, a novel hybrid transformer-CNN EM classification\narchitecture (HTEM) is proposed. To address the first of the two main issues discussed above,\na hierarchical structure is adopted to obtain multi-scale feature maps. For the second problem,\nadditional convolution operations are introduced in the transformer’s two core sections,\nwhich makes the HTEM inherently efficient, both in terms of number of parameters and\ncomputational efficiency.\nIn the proposed HTEM model, the transformer is divided into four stages to create a hierarchi-\ncal framework, and all stages share a similar structure. The input images first go through a convo-\nlution token embedding (CTE) block for feature extraction [40, 41]. The resulting data of the CTE\nblock are fed into a feature-embedding layer to reduce the size of feature maps. This embedding\nlayer allows the generation of different scales of images for the classification task, while also reduc-\ning the number of parameters and thus the computation load incurred by the network. The 2×\ndown-sampled feature maps are then input to the i-th stage transformer encoder, which consists\nof a convolutional parameter sharing multi-head attention (CPSA) block and a local feed-forward\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 2 / 22\nCompeting interests : The authors have declared\nthat no competing interests exist.\nnetwork (LFFN) block. Inspired by [42], the linear projection before each self-attention block in\nthe original transformer model is replaced by the convolutional parameter sharing projection in\nthe CPSA block. This allows the key and value matrices of the original transformer model to share\nthe same parameters and perform depth-wise separable convolution operations [43] to subsample\nthe feature maps. By subsampling and sharing matrix parameters, the block can capture more\nlocal features for image classification and simultaneously minimize the computational complexity\nand excessive flexibility of the transformer model, thus increasing efficiency with minimal perfor-\nmance loss. Additionally, motivated by [44], the LFFN block is proposed as a replacement for the\nfeed-forward network (FFN) in the transformer model. This block introduces a sandglass block\nwith additional depth-wise convolutions in the FFN block to help the transformers capture more\nlocal features. These depth-wise convolutions provide an effective mechanism for local content\naggregation, which is not available in the traditional ViT’s FFN block. It should be noted that the\ndepth-wise convolution is effective in terms of both parameters and computation complexity. The\nbasic transformer encoder structure is repeated multiple times at each stage. After four feature\nembedding layers and encoding stages, a global average pooling block is used to replace the class\ntoken in the transformer model for better classification results.\nOverall, the proposed HTEM bears all the advantages of CNNs and transformers through\nits efficient hierarchical network structure and the introduction of improved local feature per-\nception. The results show that the proposed model achieves state-of-the-art performance while\nremaining lightweight and efficient when trained on a small EM dataset like EMDS-6. Addi-\ntionally, the proposed HTEM outperforms all compared CNN- and transformer-based models;\nafter data augmentation, its accuracy is 9.02 percentage points higher than Xception while\nrequiring less training time and fewer parameters. Additionally, it is demonstrated that the\nproposed model’s performance is not contingent on the presence of the token position embed-\nding. This allows for a simplified architecture design and offers flexibility with respect to the\ninput images’ resolutions, which is essential for classification tasks.\nTo summarize, the main contributions of this paper are three-fold:\n1. We put forward a new hierarchical transformer architecture called HTEM, which can cap-\nture multi-scale feature maps of EM images, thus strengthening the model’s ability to cap-\nture salient features.\n2. We introduce the CPSA and LFFN blocks to the transformer’s core sections to increase the\nnumber of convolutions. The novel model architecture can simultaneously capture local\nfeatures and long-range dependency information and is free of any position embedding\ndependence.\n3. Experimental results on the EMDS-6 and WHOI datasets show the effectiveness and gener-\nalization ability of HTEM with less training time and a lower number of parameters com-\npared with previous state-of-the-art ViTs and CNNs.\nThe rest of this paper is organized as follows. A brief overview of EM image classification\nmethods and ViTs is presented in Section 2, while the details of the proposed HTEM are pre-\nsented in Section 3. In Section 4, the training details are described and comprehensive experi-\nmental comparisons are discussed. Finally, the paper is concluded in Section 5.\n2. Related works\n2.1 EM image classification\nWith the development of computer and imaging technology, computer-aided EM classifica-\ntion based on machine learning has achieved remarkable results. Kruk M. et al. [45] proposed\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 3 / 22\na system for classifying soil EMs using shape, edge, and color features, which uses a random\nforest to classify EM images. Cunshe C. et al. [46] proposed a method for classifying wastewa-\nter EMs using morphological features combined with principal component analysis. Xiaojuan\nL. et al. [47] proposed a method for rapid classification and identification of bacteria in waste-\nwater using edge detection. Li C. et al. [48] proposed an EM classifier that captured edge fea-\ntures and was based on a support vector machine. Although the above methods are able to\nperform EM classification and identification, they require extensive feature engineering for\nassisted learning and are not able to achieve automatic end-to-end classification.\nIn order to mitigate the reliance on feature engineering, more and more researchers have\nadopted deep learning methods based on CNNs for EM classification tasks. Luo et al. [49]\nused CNNs for classification studies of planktonic microorganisms. Połap et al. [50] proposed\na CNN model based on regional covariance to classify EMs and they demonstrated the classifi-\ncation of rod-shaped and spherical bacteria with accuracies higher than 91% and 78%, respec-\ntively. Bliznuks D. et al. [51] proposed a bacterial growth analysis system using a 3D CNN,\nwhich also achieved high accuracy. Zihan Li Z. et al. [52] presented the fifth version of the\nenvironmental microorganism dataset (EMDS-5) and used the VGG and InceptionV3 net-\nworks to classify microorganisms. Recently, Zhao P. et al. [37] proposed the EMDS-6 dataset\nand conducted an EM classification study using typical CNNs.\nHowever, it is known that CNNs are ineffective in capturing long-distance dependencies in\nEM images [30]. Due to the better ability of transformers in capturing long-distance depen-\ndencies, Zhao P. et al. [33] first proposed the use of ViTs for an EM image classification study\non the EMDS-6 dataset, which was a novel approach compared to others presented by that\ntime. The difference with the present work is that in this study a hybrid transformer-CNN\narchitecture is proposed instead of ViTs. This model combines the respective advantages of\nCNNs and transformers to realize the EM image classification task on the EMDS-6 dataset.\n2.2 Visual transformers\nThe first instance of transformers being used for CV tasks was ViT. This model achieves state-\nof-the-art results when the corresponding dataset is sufficiently large, such as in the case of\nImageNet-22k, JFT-300M, etc. Subsequently, many models have been proposed in order to\nobtain better results in image classification tasks [34, 53, 54]. For instance, Tokens-to-token\n(T2T-ViT) [35] uses additional sliding windows to integrate multiple tokens, an operation that\nincreases the ability of the model to obtain local information but increases its complexity.\nRecently, some efficient models have been proposed to avoid the reliance of ViT models on\nlarge datasets. For instance, CeiT [41] first uses convolution to capture the input image fea-\ntures, and then captures more local information through the integration of the depth-wise con-\nvolution module into the ViT. ConViT [55] utilizes gated position self-attention to simulate\nthe effect of convolutional operations so that the model can obtain the local contents. CvT [42]\nenhances the ViT’s local feature processing capability by means of convolution operations\nintroduced in both linear projection and multilayer perceptron blocks.\nThese improvements allow the ViT to perform well even in the absence of large datasets,\nand the models mentioned have achieved better results on medium-sized datasets (e.g., Ima-\ngeNet) [56]. However, for small datasets, the above models still fall behind similar-sized\nCNNs. The key contrast between the proposed work and previous research is that a hybrid\ntransformer network is put forward, comprising a hierarchical structure and incorporating the\nconvolutions in the transformer’s core sections so that it can be trained effectively and effi-\nciently even on small datasets like EMDS-6.\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 4 / 22\n3. Methods\n3.1 Overall architecture\nThe aim of this work is to present a new hierarchical transformer architecture with more con-\nvolution operations in the transformer’s core sections so that it can effectively classify data\nafter training using small datasets. Fig 1 shows an overview of the proposed HTEM model. In\nthis work, a multi-stage structure is used, where all stages utilize a similar structure comprising\nof a CPSA block and an LFFN block.\nOne of the major differences of the proposed architecture is that it does not employ a fixed\npatch size for token embedding as ViT does. Instead, a CTE block is utilized, consisting of a\nconvolution operation with a stride of 1 and an output channel of 16, followed by ReLU activa-\ntion and a maxpooling layer with a stride of 2 to extract local information efficiently. Inspired\nby the layout of CNNs, in order to extract the multi-scale feature maps, before each level a fea-\nture embedding layer comprising a convolution operation is used to reduce the size of the inter-\nmediate feature maps (2×downsampling of resolution) and project it to a higher dimension.\nFrom an input image, the proposed model produces four hierarchical feature maps with strides\nof 4, 8, 16, and 32. With the above feature maps’ hierarchical architecture, the model can extract\nmulti-scale representations, which makes it more suitable and computationally efficient for EM\nimage classification. In each stage, several similar encoder blocks, comprising a CPSA block and\nan LFFN block, are consecutively layered for feature transformation while maintaining the reso-\nlution of the input feature maps. The encoder block not only reduces the computational com-\nplexity and the over-flexibility of the transformer model but also simultaneously captures both\nthe local contents and long-range dependencies. A global average pooling layer, a fully con-\nnected layer, and a softmax classification layer make up the model’s final block. In addition, this\nmodel does not require any position embedding to aid the training process.\nFig 1. Overall architecture of HTEM. First, the inputs are fed to a Convolutional Token Embedding (CTE) to obtain patches.\nSecond, the feature maps are processed using a feature embeddin g layer and repeated transformer-e ncoder blocks consisting of CPSA\nand LFFN blocks in each stage. Finally, a global average pooling block is employe d to obtain the class token.\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.g0 01\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 5 / 22\nIn the following, we first demonstrate the implementation of a novel projection method\nusing the CPSA block. Then, we describe the LFFN block and how its efficient design boosts\nthe performance of the network.\n3.2 Convolutional Parameters Sharing Multi-head Attention (CPSA)\nIn order to allow the proposed HTEM to capture more local features while at the same time\nmaintaining computational efficiency, in this paper the adoption of a CPSA block is proposed\ninstead of the multi-head attention (MHA) block employed in the original transformer archi-\ntecture. Fig 2A shows the original MHA block, which employs a linear projection to extract\nthe attention feature, while Fig 2B shows the proposed CPSA block, which uses the convolu-\ntional parameters’ shared projection to calculate the attention score of the model.\nIn order to decrease the complexity of the HTEM model and achieve high accuracy, the\nsharing between the key and value is enabled, which results in the shared parameter matrix S.\nThis means that the proposed CPSA block employs a query Q, a shared parameters matrix S to\nreplace Q, a key K, and a value V to obtain an attention feature. First, the CPSA block uses a\nconvolutional projection to reduce the dimensions of input feature maps. The convolutional\nprojection is implemented through a depth-wise separable convolution with a kernel size of s\nand a stride of s. This operation enhances the ability of the model to capture local information\nwhile improving its computational efficiency. Then, the tokens are compressed into 1D\nsequence features, thus generating the shared parameters matrix for subsequent processing.\nThis is expressed mathematically as follows:\nS ¼ Flat tenðD WCon v2d ðx\ni\n; s; sÞÞ ð1Þ\nwhere S denotes the shared parameter matrix used for calculating the attention feature, x\ni\ndenotes the input feature maps before the convolution projection, DWConv2d denotes depth-\nwise separable convolution operation, Flatten indicates the conversion of a 2D feature map to\n1D sequence features, and s represents the size of the depth-wise convolution kernel and its\nstride, which in the following will be referred to as “reduction rates”.\nFig 2. (A) Multi-Head Attention (MHA) block in ViT [30]. (B) Convolutional Parameters Sharin g multi-head Attention\n(CPSA) block in HTEM.\nhttps://do i.org/10.1371/j ournal.pone .0277557.g00 2\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 6 / 22\nInspired by [30], in order to guarantee the consistent dimensionality of the output tokens,\nQ always maintains its original dimensions and is not processed through the convolutional\nprojection. To obtain Q, the feature maps are reshaped from 2D to 1D feature maps. Then, the\nrelationship between the tokens is modeled through the similarity between the Q-S pairs,\nwhich results in the generation of the attention score, which can be mathematically expressed\nas follows:\nAtt enti onðQ; SÞ ¼ Sof t max\nQS\nT\nffiffiffiffiffiffiffiffi ffi\nd\nhea d\np\n !\nS ð2Þ\nwhere Attention() denotes the output attention features after the attention operation, the Soft-\nmax function is applied to the rows of the similarity matrix and d\nhead\ndenotes the dimension of\neach attention head.\nThe effectiveness of the proposed HTEM is ensured by three crucial factors in the CPSA\nblock. First, the utilization of depth-wise separable convolution introduces only s\n2\nC additional\nparameters compared to MHA in the original transformer, which is negligible compared to\nthe total number of parameters in the model. Second, matrix sharing reduces the number of\nkey and value parameters in half. This approach results in a diminished network learning\ncapacity, but in the case of small EM datasets like EMDS-6, it also prevents overfitting. Third,\neach convolution projection operation is analogous to a matrix downsampling operation. Con-\nsequently, the number of S parameters is decreased by a factor of s, and the computational cost\nof the later corresponding multi-headed attention mechanism is lowered by a factor of s\n2\n.\n3.3 Local Feed-Forward Network (LFFN)\nIn the original Transformer, an FFN block is appended after the attention layer. However, the\nFFN block only utilizes two fully-connected layers to achieve feature representation and can-\nnot capture the correlation among neighboring tokens in the spatial dimension. Therefore, it\nwould be useful if local dependencies could be effectively added to the fully connected net-\nwork. The expansion of the hidden dimension between fully-connected layers is reminiscent\nof the sandglass block in MobileNeXt [44]. As shown in Fig 3, in this paper an LFFN block is\nproposed instead of the FFN block in the original transformer. The LFFN block allows the\nHTEM to capture more local features at different dimensions by introducing a sandglass block\nconsisting of depth-wise separable convolution, thus improving the local information acquisi-\ntion capability of the model.\nThe LFFN block performs the following operations. First, in order to improve the informa-\ntion interaction between adjacent pixels, the 1D feature inputs i.e., the tokens x\nh\ni\n2 R \nN �C\npro-\nduced by the CPSA block, are transformed to 2D features as shown in Fig 3. The feature\nrepresentation is\nx\nS\ni\n¼ Resh ape2Dðx\nh\ni\nÞ; x\ns\ni\n2 R \nffiffi ffi\nN\np\n�\nffiffi ffi\nN\np\n�C\nð3Þ\nwhere x\ns\ni\ndenotes the transformed 2D features, N denotes the number of feature inputs, C\ndenotes the feature dimension, and Reshape2D denotes the conversion of 1D sequence features\nto 2D feature maps. Second, in order not to lose low-dimensional local feature information, a\ndepth-wise separable convolution with a kernel size of k is employed to process the 2D fea-\ntures. The computation could be represented as\nx\ndð1Þ\ni\n¼ DWCo nv2dðx\ns\ni\nÞ; x\ndð1Þ\ni\n2 R \nffiffi ffi\nN\np\n�\nffiffi ffi\nN\np\n�C\nð4Þ\nwhere x\ndð1Þ\ni\ndenotes the new feature maps after the depth-wise separable convolution operation\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 7 / 22\nDWConv2d. Then, the feature dimension is expanded using convolution with a kernel size of 1\nto generate a new feature, i.e.,\nx\nlð1Þ\ni\n¼ Convðx\ndð1Þ\ni\nÞ; x\nlð1Þ\ni\n2 R \nffiffi ffi\nN\np\n�\nffiffi ffi\nN\np\n�ðe�CÞ\nð5Þ\nwhere x\nlð1Þ\ni\ndenotes the feature maps after the dimension expansion, Conv denotes a convolu-\ntion operation with a kernel size of 1, and e denotes the expansion ratio in the LFFN block. In\norder to obtain additional high-dimensional local contents, a depth-wise convolution is\napplied with a kernel size of k. That is,\nx\ndð2Þ\ni\n¼ DW Conv2dðx\nlð1Þ\ni\nÞ; x\ndð2Þ\ni\n2 R \nffiffi ffi\nN\np\n�\nffiffi ffi\nN\np\n�ðe�CÞ\nð6Þ\nwhere x\ndð2Þ\ni\ndenotes the new higher-dimension feature maps after the depth-wise separable\nconvolution operation. Finally, the output feature maps x\nlð2Þ\ni\nof the LFFN block are generated\nthrough the application of a dimensionality reduction operation in the form of a convolution\nwith a kernel size of 1. The operation can be represented as\nx\nlð2Þ\ni\n¼ Convðx\ndð2Þ\ni\nÞ; x\nlð2Þ\ni\n2 R \nffiffi ffi\nN\np\n�\nffiffi ffi\nN\np\n�C\nð7Þ\nNote that the depth-wise convolutions introduce the most of the additional computational\noverhead of the LFFN block, but it is negligible compared to the total number of parameters in\nthe whole model.\n4. Results\nIn this section, the proposed HTEM model is evaluated using the EMDS-6 and WHOI data-\nsets. Moreover, ablation investigations are conducted to confirm the efficacy of each compo-\nnent of the proposed architecture.\nFig 3. Local Feed-Forw ard Network (LFFN) in HTEM. DW Conv denotes depth-wise convolu tion.\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.g0 03\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 8 / 22\n4.1 Experimental settings\n4.1.1 Network architecture. The proposed HTEM architecture used for the experiments\nof this study is presented in Table 1. The CTE block consisted of a convolutional layer with a\nkernel size of 3 and a stride of 1, and generated 16 enriched channels. To ensure stable train-\ning, a BatchNorm layer was also introduced. Then, a max-pooling layer was used with a kernel\nsize of 3 and a stride of 2. The CPSA and LFFN blocks employed are shown in brackets, with\nthe numbers of blocks stacked. In each CPSA block of stage i, h\ni\n, and s\ni\nare the number of\nheads and reduction rates, respectively, while e denotes the expansion ratio in the LFFN block,\nand k is the depth-wise convolutions’ kernel size in the LFFN block.\n4.1.2 Dataset description and preprocessing. In this experiment, the model performance\nis evaluated using the EMDS-6 and WHOI datasets.\nEMDS-6 is an EMs dataset containing 840 distinct EM images in 21 categories of 40 images\neach. Because the EMDS-6 is a very small dataset, 37.5% of the dataset was selected as the train-\ning set, 25% as the validation set, and 37.5% as the test set. as in [33]. In the data augmentation\nexperiment, the same five geometric enhancements as in [33] were adopted to enhance the\nEMDS-6 dataset, namely rotation by 90˚, 180˚, and 270˚, as well as up-down and left-right\nmirror transformations. After data augmentation, the resulting dataset size was 6 times that of\nthe original. In addition, following [33], in the imbalanced data experiments, after the data\naugmentation, each of the 21 types was viewed as a positive sample in turn, while the remain-\ning 20 types of samples are considered negative samples. In this way, 21 new imbalanced data-\nsets were obtained and used to validate the performance of the proposed model on imbalanced\ndatasets.\nWHOI is a dataset of cells and other planktonic particles that Imaging FlowCytobot col-\nlected from the water at Woods Hole Harbor. The dataset includes 6600 manually categorized\nphotos that have been divided into equal-sized training and testing sets. There are 22 catego-\nries of photos, and each category has an equal number of examples (150 training and 150 test\nsamples). The ratio between training and testing samples employed in our experiment was the\nsame as that in [57].\n4.1.3 Evaluation method. To evaluate the classification performance of the deep learning\nmodels objectively, the indicators of [33] were adopted, namely precision, recall, accuracy,\nTable 1. Detailed settings of HTEM.\nOutput Layer Name HTEM\n112×112 CTE Conv:3×3, 16, stride = 1\nMaxpooling :3×3, stride = 2\n56×56 Feature Embedd ing 2×2, 24, stride = 2\nStage 1 CPSA\nLFFN\nh\n1\n¼ 1; s\n1\n¼ 8\ne ¼ 4; k ¼ 3\n\" #\n� 2\n28×28 Feature Embedd ing 2×2, 32, stride = 2\nStage 2 CPSA\nLFFN\nh\n2\n¼ 2; s\n2\n¼ 4\ne ¼ 4; k ¼ 3\n\" #\n� 2\n14×14 Feature Embedd ing 2×2, 48, stride = 2\nStage 3 CPSA\nLFFN\nh\n3\n¼ 4; s\n1\n¼ 2\ne ¼ 4; k ¼ 3\n\" #\n� 4\n7×7 Feature Embedd ing 2×2,64, stride = 2\nStage 4 CPSA\nLFFN\nh\n4\n¼ 8; s\n4\n¼ 1\ne ¼ 4; k ¼ 3\n\" #\n� 2\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t00 1\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 9 / 22\nF1-score, AP, and mAP. Precision is the ratio between the true positive samples and the total\nnumber of samples predicted as positive, while recall is the ratio of true positive samples pre-\ndicted compared to all the positive samples. The F1-score is the harmonic mean of precision\nand recall. Accuracy refers to the ratio of the number of correctly predicted samples to the\ntotal number of samples. AP refers to the average value of the recall rate and ranges from 0 to\n1. The mAP is the arithmetic average of all AP. The specific equations of these metrics are\nshown in Table 2. In Table 2, TP, FP, TN, and FN refer to the number of true positives, false\npositives, true negatives, and false negatives, respectively.\n4.1.4 Implementation details. The experiments were performed on an NVIDIA Quadro\nRTX 4000 GPU and the same training strategy used in [33] was adopted. Specifically, the learn-\ning rate was set to 0.002 and the batch size was set to 32. The model was trained for 100 epochs.\nAll images were cropped to 224×224 pixels.\n4.2 Experimental results and analysis\n4.2.1 Classification performance on EMDS-6. The accuracy and loss curves of HTEM\non EMDS-6 are shown in Fig 4. In Tables 3 and 4, we also evaluate the effectiveness of the pro-\nposed HTEM models in comparison to state-of-the-art classification models, including trans-\nformer-based models (ViT, DeiT, T2T-ViT, and BotNet) and representative CNN-based\nmodels (Xception, ResNet18, ResNet34, MobileNetV2, Googlenet, Densenet121, Densenet169,\nand VGG11). All the other models’ implementations were obtained from [33].\nAs shown in Fig 4A, the accuracy on the training set is significantly higher than that on the\nvalidation set. The accuracy of the HTEM model training set increases rapidly, approaching its\nhighest point of 90% after 80 epochs. After 40 epochs, the accuracy on the validation set\napproaches 50%, which is its maximum. As shown in Fig 4B, after 80 epochs, the HTEM train-\ning set loss curve begins to gradually taper out toward its minimum value. However, in the val-\nidation set, this occurs after only 30 epochs.\nAs shown in Table 3, we compared the proposed HTEM model with the other models on\nthe validation set of EMDS-6. Compared to the CNN-based models, HTEM achieves an accu-\nracy of 53.00%, which is higher than Xception, ResNet34, and GoogleNet by 7.29, 10.14, and\n11.09 percentage points (pp.), respectively. Compared with Densenet121, Densenet169,\nResNet18, MobileNetV2, and VGG11, HTEM even exceeds 10pp. in classification accuracy\nimprovement. Similarly, HTEM achieved the highest recall, precision, and F1-score values in\nthe validation set results, which are 53.00%, 56.82%, and 53.33%, respectively, and outper-\nformed all the other models on EMDS-6. This is a surprising result since the size of HTEM is\none-sixtieth the size of the Xception model. Compared to the transformer-based models, our\nmodel achieves greater accuracy over DeiT, ViT, T2T-ViT, and BotNet by 13.95, 15.86, 18.72,\nTable 2. Evaluatio n metrics for EM image classification .\nAssessments Formula\nPrecision (P)\nTP\nTPþFP\nRecall (R)\nTP\nTPþFN\nF1-score 2 �\nP�R\nPþR\nAccuracy\nTPþTN\nTPþTNþFPþFN\nAP\n1\nM\nX\nM\ni¼1\nPrecision\nmax\nðiÞ\nmAP\n1\nK\nX\nK\nj¼1\nAPðjÞ\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t00 2\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 10 / 22\nand 22.52pp., respectively, while the number of parameters is only one-twelfth that of\nT2T-ViT.\nTo further illustrate the performance of the HTEM model proposed in this paper, we again\ncompared all models on the test set of EMDS-6, as shown in Table 4. On the test set, the perfor-\nmance of all models decreased compared to that on the validation set because the test set com-\nprised image data that were unknown to the models, and because of the performance of the\ntest set reflects the models’ generalization ability. Compared to the models on the test set, the\nproposed HTEM model still achieved the highest recall, precision, F1-score, and accuracy\nresults, at 47.02%, 48.81%, 46.72, and 47.02%, respectively. It should be noted the HTEM\nmodel required the least training time and performed inference faster than the other models,\nwhile still achieving higher accuracy on EMDS-6, as shown in Tables 3 and 4. The experimen-\ntal results show that HTEM’s transformer configuration can be trained to achieve high\nFig 4. The accuracy and loss curves of the proposed HTEM model.\nhttps:// doi.org/10.137 1/journal.pone. 0277557.g004\nTable 3. Compariso n of classific ation results of different models on the validation set.\nModel R(%) P(%) F1_score(%) Accuracy( %) Params Size (MB) Time(s)\nBotNet 30.48% 32.61% 30.06% 30.48% 72.2 894\nVGG11 31.43% 41.20% 29.97% 31.43% 491 864\nT2T-ViT 34.29% 38.17% 34.54% 34.28% 15.5 825.3\nViT 37.14% 41.02% 35.95% 37.14% 31.2 715\nDeit 39.05% 39.37% 37.70% 39.05% 21.1 817.27\nMobileNetV2 39.52% 39.57% 37.01% 39.52% 8.82 767\nResNet18 40.95% 45.55% 41.05% 40.95% 42.7 739\nDensenet16 9 40.95% 43.62% 39.89% 40.95% 48.7 988\nDensenet12 1 40.95% 43.61% 40.09% 40.95% 27.1 922\nGooglenet 41.90% 42.83% 40.49% 41.91% 21.6 772\nResNet34 42.86% 45.33% 42.31% 42.86% 81.3 780\nXception 45.71% 52.48% 44.95% 45.71% 79.8 996\nHTEM 53.00% 56.82% 53.33% 53.00% 1.3 342\nP denote s Precision, and R denote s Recall.\nhttps://do i.org/10.1371/j ournal.pone .0277557.t003\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 11 / 22\naccuracy on EMDS-6 through the proposed hierarchical structure and incorporation of local\nfeature information. This outcome is consistent with this study’s main objective.\n4.2.2 Classification performance on WHOI. To further validate the effectiveness of our\nproposed model on a different EM dataset, we evaluated the proposed HTEM on the publicly\navailable dataset used in [57], which reports classification benchmarks on the WHOI plankton\ndatasets. The accuracy and F1_score of our model were compared with that of the best models\nin [57]. As shown in Table 5, the HTEM model outperformed all the previous methods. the\nproposed HTEM demonstrated an accuracy of 2.5pp. and 1.7pp. compared to individual CNN\nmodels, i.e., InceptionV3 and EfficientNet B7. Compared to the ensemble models, i.e., Bes-\nt_6_avg and Best_6_stack, HTEM showed slight improvements in terms of accuracy and\nF1-score. The experimental results show that the proposed HTEM can achieve the desired clas-\nsification performance on the WHOI dataset, further validating the rationality of our model\ndesign.\n4.3 Extended experiment\n4.3.1 Classification performance on EMDS-6 after data augmentation. We compared\nthe proposed HTEM model with all the models on the EMDS-6 after data augmentation, as\nshown in Tables 6 and 7. All the other models’ implementations were obtained from [33]. On\nthe validation set, although all the CNN-based models’ performance was improved after data\naugmentation, the proposed HTEM model still outperformed the second-best Xception on its\nrecall, precision, F1-score, and accuracy values by 6.93, 9.48, 8.01, and 6.93pp., respectively. In\nTable 4. Compariso n of classific ation results of different models on the test set.\nModel R(%) P(%) F1_score(%) Accuracy( %) Params Size (MB) Time(s)\nBotNet 25.40% 29.65% 26.04% 25.39% 72.2 6.5\nVGG11 27.61% 29.64% 26.00% 27.62% 491 4.98\nViT 31.75% 33.84% 31.47% 31.74% 31.2 3.77\nDeit 32.39% 34.40% 32.74% 32.38% 21.1 5.43\nDensenet12 1 33.01% 39.20% 33.79% 33.02% 27.1 9.27\nResNet18 33.33% 38.10% 32.36% 33.33% 42.7 4.92\nDensenet16 9 33.65% 36.55% 33.79% 33.65% 48.7 11.13\nT2T-ViT 34.29% 38.17% 34.54% 34.28% 15.5 4.44\nMobileNetV2 34.29% 38.21% 33.07% 34.29% 8.82 5.13\nGooglenet 35.23% 37.70% 34.21% 35.24% 21.6 5.97\nResNet34 36.51% 42.92% 36.22% 36.51% 81.3 6.14\nXception 40.33% 49.71% 41.41% 40.32% 79.8 5.63\nHTEM 47.02% 48.81% 46.72% 47.02% 1.3 3.12\nP denote s Precision, and R denote s Recall.\nhttps://do i.org/10.1371/j ournal.pone .0277557.t004\nTable 5. Compariso n of classification results of different models on the WHOI dataset.\nModel Accuracy( %) F1_sco re(%)\nInceptionV3 96.5% 89.3%\nEfficientN et 97.3% 91.6%\nBest_6_avg 97.6% 93.0%\nBest_6_sta ck 97.7% 92.5%\nHTEM 99.0% 94.2%\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t00 5\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 12 / 22\naddition, it also outperformed the Xception network in terms of training time, as it required\nonly one-third of Xception’s time. Compared with ResNet18, the proposed HTEM model has\na similar training time but its accuracy is superior by 15.11pp. Compared with MobileNetV2,\nVGG11, ResNet34, Googlenet, Densenet121, and Densenet169, HTEM achieves the best classi-\nfication performance with the least training time in all cases.\nCompared to the transformer-based models, the performance of T2T-ViT, ViT, and Deit\ndid not improve significantly after data augmentation. This is because the geometric augmen-\ntation has a greater impact on local dependencies and does not affect long-range dependencies.\nThe BotNet model had a greater performance improvement because it introduces the attention\nmodule of the transformer in the ResNet network, which has the ability to capture both long-\nrange dependencies and local information. However, BotNet still fell behind the HTEM in\nterms of accuracy by 22.96pp. This is because the proposed HTEM model introduces more\nTable 6. Compariso n of classific ation results of different models on the validation set after data augmen tation.\nModel R(%) P(%) F1_score(%) Accuracy( %) Params Size (MB) Time(s)\nT2T-ViT 35.56% 38.43% 36.19% 35.56% 15.50 1385.62\nBotNet 36.59% 36.38% 35.59% 36.59% 72.2 2000.17\nViT 39.05% 43.50% 38.52% 39.05% 31.20 902.27\nDensenet16 9 42.14% 48.04% 42.79% 42.14% 48.70 2526.61\nDensenet12 1 42.38% 46.91% 42.39% 42.38% 27.10 2169.11\nDeit 43.34% 46.62% 43.29% 43.33% 21.10 1306.99\nGooglenet 44.29% 47.16% 43.50% 44.29% 21.60 1257.33\nResNet18 44.44% 51.87% 43.03% 44.44% 42.70 1090.39\nResNet34 46.10% 47.85% 44.68% 46.11% 81.30 1335.87\nVGG11 48.10% 52.40% 48.44% 48.10% 491.00 1745.73\nMobileNetV2 49.67% 51.91% 48.82% 49.68% 8.82 1237.49\nXception 52.62% 52.05% 50.63% 52.62% 79.80 2636.08\nHTEM 59.55% 61.53% 58.64% 59.55% 1.18 901.31\nP denote s Precision, and R denote s Recall.\nhttps://do i.org/10.1371/j ournal.pone .0277557.t006\nTable 7. Compariso n of classific ation results of different models on the test set after data augmentatio n.\nModel R(%) P(%) F1_score(%) Accuracy( %) Params Size (MB) Time(s)\nViT 28.58% 29.63% 27.86% 28.57% 31.2 3.72\nT2T-ViT 30.48% 35.88% 30.85% 30.48% 15.50 5.41\nDeit 32.39% 34.40% 32.74% 32.38% 21.1 4.41\nBotNet 36.50% 39.12% 36.35% 36.51% 72.2 6.44\nVGG11 37.14% 38.81% 36.70% 37.14% 491 4.96\nDensenet16 9 37.14% 41.51% 37.37% 37.14% 48.7 11.04\nGooglenet 37.46% 43.55% 37.92% 37.46% 21.6 6.03\nResNet34 38.73% 42.25% 37.84% 38.73% 81.3 6.07\nDensenet12 1 38.73% 40.28% 38.20% 38.73% 27.1 8.98\nResNet18 39.05% 44.82% 39.22% 39.05% 42.7 4.90\nMobileNetV2 42.54% 47.56% 43.07% 42.54% 8.22 5.04\nXception 45.71% 50.43% 46.15% 45.71% 79.8 5.49\nHTEM 54.73% 57.52% 54.91% 54.73% 1.18 3.02\nP denote s Precision, and R denote s Recall.\nhttps://do i.org/10.1371/j ournal.pone .0277557.t007\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 13 / 22\nconvolution operations in the transformer and can capture both long-range dependencies and\na large amount of local features at the same time, so data augmentation results in substantial\nperformance gains.\nOn the test set, HTEM achieved an accuracy of 54.73%, which is higher than Xception,\nMobileNetV2, and ResNet18 by 9.02pp., 12.19pp., and 15.68pp., respectively. Similarly, it\nachieved 54.73%, 57.52%, and 54.91% in its recall, precision, F1-score and outperformed the\naforementioned transformer-based and CNN-based models. As shown in Table 6, the accu-\nracy of the Xception model decreased by 6.91%, while the HTEM model decreased by only\n4.82%. This is due to the fact that HTEM inherits the strong generalization ability of the trans-\nformer. In addition, although the amount of data increased, the HTEM model still had the\nsmallest training and inference times. The experimental results show that HTEM benefits\nfrom the advantages of both CNNs and transformers at the same time, further validating the\nrationality and effectiveness of the proposed model’s design.\n4.3.2 Statistics analysis on EMDS-6. For the experiments presented in this section, the\nEMDS-6 dataset was randomly divided three times to obtain training, validation and test sets.\nThe effectiveness of the proposed HTEM models was also evaluated in comparison to state-of-\nthe-art classification models, including transformer-based models (ViT and BotNet) and rep-\nresentative CNN-based models (Xception, ResNet18, ResNet34, and MobileNetV2). All the\nother models’ implementations were obtained from [33]. The average results of the three\nexperiments are shown in Table 8, where it can be seen that under the original dataset, the pro-\nposed HTEM model has the highest classification performance of all the models. After data\naugmentation, HTEM still yields the highest classification performance, outperforming Xcep-\ntion, MobileNetV2, ResNet34, ViT, and BotNet by 8.95pp., 8.96pp., 14.03pp., 23.02pp., and\n20.69pp., respectively. The classification performance of all models was improved except for\nViT. This is because the pure transformer model is insensitive to geometric transformation-\nbased data augmentation. These experimental results further verify the superiority and effec-\ntiveness of the model proposed in this paper.\n4.3.3 Feature information analysis. Fig 5 presents the confusion matrix of the HTEM\nmodel regarding the EM images on the test set of EMDS-6 after data augmentation. 164 out of\nthe total of 315 EM images were classified into the correct category. The HTEM model per-\nformed well on the Paramecium, Codosiga, and K. Quadrala image classification because these\nthree EMs have more obvious and less confusing characteristics, as shown in Fig 6. The predic-\ntions of the Epistylis and Phacus classes were slightly inaccurate, as evidenced by the result that\n10 out of the 15 Epistylis images were identified incorrectly. The misclassification was usually\ncaused by the very similar features of the two EMs, as shown in Fig 7, causing the Epistylis\nimages to be easily confused for Vorticella images. In fact, these EM images are very difficult\nTable 8. Compariso n of different deep learning models on EMDS- 6 test sets. (In [%]).\nModel Origina l Data Augmented Data\nR(%) P(%) F1_score (%) Accuracy(%) R(%) P(%) F1_score(%) Accuracy( %)\nBotNet 29.00 31.11 28.46 28.99 33.02 34.29 32.45 33.02\nViT 33.24 34.92 32.63 33.23 30.69 32.49 30.08 30.69\nResNet34 37.14 41.96 36.93 37.14 39.68 43.15 39.54 39.68\nMobileNet V2 34.50 37.24 33.86 34.50 44.75 48.31 44.82 44.75\nXception 39.37 44.25 39.07 39.37 44.76 47.97 44.53 44.76\nHTEM 47.01 48.56 46.05 47.01 53.71 57.31 53.45 53.71\nP denote s Precision, and R denote s Recall.\nhttps://do i.org/10.1371/j ournal.pone .0277557.t008\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 14 / 22\nfor non-experts to distinguish. In summary, the proposed HTEM model can assist in the task\nof EM classification, but for EMs with particularly similar features, further confirmation of the\ncategories by professionals using appropriate instrumentation is still required.\n4.3.4 Imbalanced training on EMDS-6. In this section, to validate the classification abil-\nity of the proposed HTEM model on imbalanced datasets, the EMDS-6 dataset is restructured.\nThe procedure was repeated 21 times and 21 different unbalanced EMs datasets were obtained.\nThe specific data reorganization method is shown in Section 4.1.2. The AP of the deep learning\nmodels was calculated after training on each dataset version. Table 9 shows the AP and mAP\nfor each model on the validation set. CNN- and transformer-based models such as ResNet50,\nInception-V3, Xception, and ViT were selected respectively for comparison. All the other\nmodels’ implementations were obtained from [33]. It is evident from Table 9 that the mAP of\nFig 5. Confusion matrix of HTEM model on the test set after data augmentat ion. In the confusion matrix, 0, 1, 2, 3,\n4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 represent Actinophr ys, Arcella, Aspidisca, Codosiga, Colpoda,\nEpistylis, Euglypha, Parameciu m, Rotifera, Vorticell a, Noctiluc a, Ceratium , Stentor, Siprostomum, K. Quadrala,\nEuglena, Gymnodini um, Gymlyano , Phacus, Stylong chia, Synchaet a.\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.g0 05\nFig 6. Images of Paramecium , Codosiga, and K. Quadrala.\nhttps://do i.org/10.1371/j ournal.pone .0277557.g00 6\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 15 / 22\nthe HTEM model is the highest, at 60.50%. HTEM obtains the highest AP on the 10th dataset\nversion, with an AP of 81.56%, and the lowest AP on the 16th dataset, at 41.24%. Compared to\nthe CNN-based models, the proposed model achieves greater accuracy over Xception, Incep-\ntion-V3, and ResNet50 by 3.89, 17, and 19.47pp., respectively. This is mainly due to HTEM’s\nability to capture both local information and long-range dependencies. In addition, it should\nbe noted that HTEM’s performance constitutes a significant improvement over the ViT\nmodel, outperforming by 25.57pp. This is because of the greatly improved ability of the model\nto acquire local features via the convolution operations integrated in the transformer model.\nThe experimental results show that HTEM can achieve high classification performance on the\nimbalanced versions of the EMDS-6 dataset, further validating its effectiveness.\n4.3.5 Classification performance of EMDS-6 test set in the presence of noise. In this\nsection, the classification performance of the proposed HTEM model in the presence of noise\nis verified. To achieve this, salt-and-pepper and Gaussian noise was added to the images of the\nEMDS-6 test set and the latter was input to the model trained on the clear images to verify the\nmodel’s generalization ability in the presence of noise. We report the results in Table 10. We\nalso evaluated the accuracy of the proposed HTEM models in comparison to state-of-the-art\nFig 7. Images of Epistyl is, and Vorticella.\nhttps:// doi.org/10.1371 /journal.pone. 0277557.g007\nTable 9. AP and mAP of different models in imbalanc ed training. (In [%]).\nModel 1 2 3 4 5 6 7 8 9 10 11\nViT 30.77% 44.99% 18.43% 48.51% 74.47% 76.17% 50.98% 15.32% 31.12% 60.74% 54.02%\nResNe5 0 30.58% 45.96% 14.24% 68.19% 66.15% 43.10% 71.24% 46.51% 31.87% 62.19% 36.79%\nInception-V3 37.75% 36.79% 33.41% 56.37% 55.77% 43.51% 59.52% 41.18% 38.40% 75.03% 69.26%\nXception 37.66% 51.16% 29.72% 68.32% 73.66% 67.96% 79.19% 65.41% 55.84% 82.97% 55.91%\nHTEM 56.04% 51.23% 46.95% 71.54% 72.21% 65.56% 80.02% 68.48% 49.35% 81.56% 56.59%\nModel 12 13 14 15 16 17 18 19 20 21 mAP\nViT 15.24% 17.84% 25.46% 6.74% 13.95% 48.61% 7.26% 60.33% 23.07% 9.53% 34.93%\nResNe5 0 15.59% 42.12% 68.57% 24.94% 17.49% 47.52% 6.64% 49.04% 16.73% 56.10% 41.03%\nInception-V3 15.09% 49.09% 64.11% 37.91% 15.00% 43.98% 15.84% 54.40% 10.78% 60.38% 43.50%\nXception 54.16% 52.28% 65.06% 46.36% 30.61% 60.41% 31.21% 61.14% 45.50% 74.36% 56.61%\nHTEM 53.22% 53.02% 71.54% 70.00% 41.24% 58.06% 38.66% 62.35% 49.75% 73.21% 60.50%\nhttps://do i.org/10.1371/j ournal.pone .0277557.t009\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 16 / 22\nclassification models, including transformer-based models (ViT, DeiT, and T2T-ViT) and rep-\nresentative CNN-based models (Xception, ResNet18, ResNet34, MobileNetV2, and Googlene).\nTo better illustrate the effect of noise on the classification accuracy of the models, we used the\nclassification accuracy of each model on the test set in the absence of noise as a benchmark. As\nshown in Table 10, the proposed HTEM model again achieved the highest classification accu-\nracy on the noisy test set, achieving 46.67% and 46.07% in the salt-and-pepper and Gaussian\nnoise cases, respectively. In addition, it is evident that noise has a greater impact on the CNN-\nbased models than on the transformer-based models. This is because transformer-bas ed mod-\nels have better generalization ability than CNN-based models, which mainly rely on local fea-\ntures. The proposed HTEM, due to the combined advantages of both CNNs and transformers,\nretains its high feature extraction capability in the presence of noise interference, further vali-\ndating the rationality and effectiveness of the proposed model.\n4.4 Ablation study\nTo confirm the viability of the proposed blocks introduced as novel transformer architectural\nelements, five ablation studies were performed. First, the effect of the CPSA and LFFN blocks’\npresence on the HTEM’s performance on EMDS-6 was analyzed. Then, it is demonstrated that\nthe position embeddings can be dropped from the model. Finally, the impact of the configura-\ntion of three key parameters on network performance is discussed.\n4.4.1 Effectiveness of CPSA and LFFN on EMDS-6 test set after data augmentation.\nThe proposed HTEM model differs in a number of ways from transformer-based models like\nViT, the most notable difference being the proposed CPSA and LFFN blocks. In the first exper-\niment, every block in the model was replaced by a transformer block while maintaining the\nother parameters the same to investigate the effects of CPSA and LFFN on the performance of\nthe proposed HTEM. The modified model’s classification accuracy was then obtained on the\naugmented EMDS-6 dataset. The model was trained for 100 epochs. The effects of each modi-\nfication on the proposed HTEM model are presented in Table 11. It is evident that the CPSA\nblock has a greater impact on the classification accuracy of small EM datasets, while the LFFN\nblock also brings about a significant improvement on the classification effect compared to the\nFFN of the original transformer. These findings suggest that adding CPSA and LFFN to the\ntransformer can help improve the classification accuracy in the case of small datasets like\nEMDS-6.\n4.4.2 Removal of position embedding on EMDS-6 test set after data augmentation.\nGiven the increased local information integrated into the HTEM model, the necessity of the\nTable 10. Compar ison of classificatio n accuracy of different models on the test set in the presence of noise.\nModel No noise Pepper-salt noise Gaussia n noise\nViT 31.74% 30.83% (-0.91) 30.03% (-1.71)\nDeit 32.38% 31.65% (-0.73) 30.75% (-1.63)\nResNet18 33.33% 27.31% (-3.12) 28.69% (-4.64)\nT2T-ViT 34.28% 30.21% (-0.72) 32.39% (-1.89)\nMobileNet V2 34.29% 31.27% (-3.02) 30.35% (-3.94)\nGooglenet 35.24% 32.63% (-2.61) 31.25% (-3.99)\nResNet34 36.51% 33.95% (-2.56) 32.78% (-3.73)\nXception 40.32% 37.66% (-2.66) 36.80% (-3.52)\nHTEM 47.02% 46.67% (-0.35) 46.07% (-0.95)\nP denotes Precision, and R denotes Recall.\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t01 0\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 17 / 22\nposition embedding was analyzed. The results in Table 12 show that omitting the position\nembedding has no negative effects on the proposed model’s performance. Therefore, the final\nproposed configuration of the model omits the position embeddings. It should be noted that\nsince the T2T-ViT cannot capture more local features, a corresponding change of removing\nthe position embedding results in a performance loss of 4.12% on the EMDS-6. This further\ndemonstrates the effectiveness of the proposed HTEM. Since position embedding needs to be\nimplemented using a fixed-length learnable vector according to the size of the input image,\nthis greatly limits the model’s ability to adapt to inputs of different sizes. CPVT [58] tries to use\nconditional position encoding to represent position embeddings to help transformer during\ntraining. The HTEM can forego the position embedding completely, which constitutes a new\napproach.\n4.4.3 Different types of layout of each stage on EMDS-6 validation set. As CPSA and\nLFFN are the two core blocks of the HTEM model, an interesting question is how to determine\nthe optimal number of these two blocks to achieve the best classification performance. In each\nstage of the HTEM model, we used the same number of CPSA and LFFN blocks to form a\nblock unit, so this question is equivalent to determining the number of modular units in each\nstage, which is called a \"layout\" design. For this purpose, several different layouts were com-\npared and the results are presented in Table 13. By selecting several common layout designs in\neach stage, it was found that the model with the [2, 2, 4, 2] layout achieved the optimal classifi-\ncation accuracy. When the number of block units was too small, the salient features of the\nimages could not be learned, while high classification accuracy also cannot be obtained with\ntoo many block units due to the small size of the EMDS-6 dataset.\n4.4.4 Different types of CPSA block on EMDS-6 validation set. In the CPSA module,\neach attention head learns image features of different dimensions, while the convolution oper-\nations improve image local feature acquisition. A reasonable configuration of attention heads\nand convolutional reduction rates can potentially improve the accuracy of the model. There-\nfore, appropriate experiments were performed to study the effect of different head and reduc-\ntion rate configurations. It was found that using progressively changing heads and reduction\nrates at each stage improves the model performance effectively, as shown in Table 14.\n4.4.5 Different types of LFFN block on EMDS-6 validation set. In the LFFN block, the\nsize of the kernel determines the size of the captured local information. Therefore, kernel sizes\nof 1×1, 3×3, and 5×5 were tested and the results are presented in Table 15. The 1×1 type shows\npoor performance compared to the baselines that do not employ depth-wise convolution as an\nTable 12. Ablations on position embedd ing.\nModel PE Accuracy(%)\nT2T-ViT\np\n30.48%\n× 26.36%\nHTEM\np\n54.14%\n× 54.73%\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t01 2\nTable 11. Ablations on CPSA and LFFN.\nModel None CPSA LFFN Accurac y(%)\nHTEM\np\n34.61%\np\n47.82%\np\n45.35%\np p\n54.73%\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t01 1\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 18 / 22\nextension layer. This demonstrates that increasing the number of layers in the transformer will\nnot necessarily result in improvement. When the kernel size is increased, the block captures\nmore local content, and corresponding gains were improved for both the 3×3 and 5×5 sizes.\nHowever, the kernel size of 3×3 was selected as the default option, as it achieved the best trade-\noff between the training time and model accuracy.\n5. Conclusion\nIn this work, we have presented a hybrid architecture comprising a transformer and a CNN\nmodel, which is suitable for EM image classification tasks on small datasets. A series of studies\nwere conducted to investigate various components and the hierarchical architecture design\nand the introduction of the CPSA and LFFN blocks was demonstrated as a critical factor for\nthe improved performance of the HTEM model. The model exhibits better performance than\nprevious works on the EMDS-6 and WHOI with accuracies of 47.02% and 99%, respectively,\nwhile maintaining computational efficiency. Furthermore, the proposed model no longer\nrequires position embedding, as it can capture more local features due to the introduction of\nthe two blocks. By incorporating rational designs, we provide a new perspective for EM image\nclassification.\nAlthough our model achieves high classification performance on small EM datasets, its\napplicability to other types of image datasets still needs further study. In future work, the pro-\nposed model will be applied to more image datasets such as facial image datasets to further val-\nidate its generalization performance.\nTable 13. Ablations study results on different types of layout of each stage.\nModel Layout Accuracy(%)\nHTEM [2,2,2,2] 44.34%\n[3,3,6,3] 41.26%\n[4,4,8,4] 37.56%\n[2,2,4,2] 53.00%\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t01 3\nTable 14. Ablations study results on different configuratio ns of attention heads and reduction rates.\nModel Heads Numbers reduction rates Accuracy(%)\nHTEM [3,3,3,3] [2,2,2,2] 42.55%\n[8,8,8,8] [2,2,2,2] 39.25%\n[3,3,3,3] [8,4,2,1] 46.56%\n[1,2,4,8] [8,4,2,1] 53.00%\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t01 4\nTable 15. Ablations study results on the type of LFFN block.\nModel Kernel Size Accurac y(%) Time(s)\nHTEM X 49.59% 290\n1×1 47.56% (-2.03) 315\n3×3 53.00% (+3.41) 342\n5×5 53.04% (+3.45) 417\nhttps://d oi.org/10.1371/j ournal.pon e.0277557.t01 5\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 19 / 22\nAuthor Contributions\nConceptualization: Ran Shao, Xiao-Jun Bi.\nFormal analysis: Ran Shao, Xiao-Jun Bi, Zheng Chen.\nInvestigation: Ran Shao.\nMethodology: Ran Shao.\nSoftware: Ran Shao.\nWriting – original draft: Ran Shao.\nWriting – review & editing: Ran Shao, Zheng Chen.\nReferences\n1. Madigan MT. Brock Biology of Microorg anisms. New Jersey: Pearso n/Prentice Hall. 2006.\n2. Kulwa F, Li C, Zhao X, Cai B, Teng Y. A State-of -the-art Survey for Microorg anism Image Segmenta tion\nMethods and Future Potential. IEEE Access. 2019; PP(99):1–1 . https://doi.or g/10.1109/ ACCESS.\n2019.29301 11\n3. Zhang J, Li C, Kulwa F, Zhao X, Sun C, Li Z, et al. A Multi-scale CNN-CRF Framework for Enviro nmen-\ntal Microorg anism Image Segmen tation. BioMed Research International . 2020; 2020(1):1– 27. https://\ndoi.org/10.11 55/2020/46214 03 PMID: 32724802\n4. Dring DM, Rangaswam i G. AGRICULT URAL MICROBI OLOGY. Kew Bulletin. 2014; 23(1):180. https://\ndoi.org/10.23 07/4117004\n5. Newsom SWB. Medical microbiol ogy and infection at a glance. Journal of Hospital Infection. 2012; 82\n(2). https://doi.or g/10.101 6/j.jhin.2012.0 7.001\n6. Attada HC. IDENTIFICATI ON OF COVID-19 SAMPLES FROM CHEST X-RAY IMAGES USING\nDEEP LEARNI NG: A COMPARISO N OF TRANSFER LEARNIN G APPRO ACHES. JETIR(www jetir-\norg). 2021(11).\n7. Li C, Wang K, Xu N. A survey for the applications of content-b ased microscopic image analysis in micro-\norganism classification domains. Artifici al Intelligence Review. 2017. https://d oi.org/10.100 7/s10462 -\n017-9572- 4\n8. Li C, Zhang J, Zhao X, Kulwa F, Li Z, Xu H, et al. MRFU-Net: A Multiple Receptiv e Field U-Net for Envi-\nronmenta l Microorganis m Image Segmentat ion. 2021. https://doi.or g/10.100 7/978-3-03 0-49666-1 _3\n9. Li C, Shirahama K, Grzegorzek M. Applica tion of content-b ased image analysis to environm ental micro-\norganism classification. Biocybernetic s & Biomedic al Engineering . 2015; 35(1):10–2 1. https://doi.o rg/\n10.1016/ j.bbe.2014.07.0 03\n10. Yamagu chi T, Kawakam i S, Hatamoto M, Imachi H, Takahas hi M, Araki N, et al. In situ DNA-hybrid iza-\ntion chain reaction (HCR): a facilitated in situ HCR system for the detection of environ mental microor-\nganisms. Enviro nmental Microbiol ogy. 2015;17 . https://doi.or g/10.111 1/1462-2920 .12745 PMID:\n25523128\n11. Li C, Zhang J, Kulwa F, Qi S, Qi Z, editors. A SARS-CoV-2 Microsc opic Image Dataset with Ground\nTruth Images and Visual Features. 2020.\n12. Li C, Huang X, Jiang T, Xu N. Full-autom atic computer aided system for stem cell clustering using Con-\ntent-base d Microscopic Image Analysis. Biocyber netics & Biomedical Engineeri ng. 2017:\nS020852161 630230 3. https://doi.or g/10.101 6/j.bbe.2017.01 .004\n13. Cristobal G, Alvarezborr ego J, editors. Automatic identifica tion technique s of tubercul osis bacteria .\nSpie; 2003.\n14. Dash S, Verma S, Kavita, Bevinako ppa S, Wozniak M, Shafi J, et al. Guidanc e Image-Bas ed Enhanced\nMatched Filter with Modified Thresholding for Blood Vessel Extraction . Symmetry. 2022;14 . https://doi.\norg/10.3390/ sym14020194\n15. Al-Baraza nchi HA, Verma A, Wang S, editors. Perform ance evaluatio n of hybrid CNN for SIPPER\nplankton image calssific ation. 2015 Third Internat ional Conference on Image Informatio n Processin g\n(ICIIP); 2015.\n16. Wang Y, Chao WL, Garg D, Hariharan B, Camp bell M, Weinbe rger KQ, editors. Pseudo-LiD AR From\nVisual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonom ous Driving. 2019 IEEE/\nCVF Conferen ce on Computer Vision and Patter n Recognition (CVPR).\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 20 / 22\n17. Srinivasu PN, Sivasai JG, Ijaz MF, Bhoi AK, Kim W, Kang JJ. Classifica tion of Skin Disease Using Deep\nLearning Neural Networ ks with MobileNet V2 and LSTM. 2021. https:// doi.org/10.33 90/s2108 2852\nPMID: 339195 83\n18. Wieczorek M, Siłka J, Woźniak M, Garg S, Hassan MM. Lightweig ht Convolution al Neural Network\nModel for Huma n Face Detecti on in Risk Situations. IEEE Transact ions on Industria l Informatic s. 2021;\n18(7):4820 –9. https://d oi.org/10.110 9/TII.2021.3 129629\n19. Wahid MF, Hasan MJ, Alom MS, editors. Deep Convolut ional Neural Network for Microscopic Bacteri a\nImage Classificat ion. 2019 5th Internat ional Conferen ce on Advances in Electrical Engineeri ng\n(ICAEE); 2019.\n20. Ouyang P, Hong H, Shi Z, editors. Plankton classifica tion with deep convolutio nal neural networks .\n2016 IEEE Informatio n Technolo gy, Networki ng, Electronic and Automation Control Conferen ce\n(ITNEC); 2016.\n21. Qin Z, Yu F, Liu C, Xiang C. How convolutio nal neural networks see the world—A survey of convolu-\ntional neural network visualization methods. Mathem atical Founda tions of Compu ting. 2018; 1(2):149–\n80. https://doi. org/10.3934/m fc.201800 8\n22. Kwon J, Kim J, Park H, Choi IK, editors. ASAM: Adaptive Sharpness -Aware Minimization for Scale-\nInvariant Learning of Deep Neural Networks. 2021.\n23. Zhang Z, Zhang H, Zhao L, Chen T, Pfister T. Aggregatin g Nested Transfor mers. 2021. https://doi.or g/\n10.48550/ arXiv.2105.12 723\n24. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attentio n is all you need.\nAdvances in neural informati on processing systems. 2017;30.\n25. Lan Z, Chen M, Goodm an S, Gimpel K, Sharma P, Soricut R. Albert: A lite bert for self-super vised learn-\ning of language represe ntations. arXiv preprint arXiv:1 90911942. 2019.\n26. Schmidt F, Fleming RW, Ronald V. Identifying shape transfor mations from photographs of real objects.\nPlos One. 2018; 13(8). https:// doi.org/10.13 71/journal.p one.020 2115 PMID: 3011420 2\n27. Wei L, Ma L, Qiu B, Cui M, Ding J. An efficient depth map preprocess ing method based on structure-\naided domain transform smoot hing for 3D view generation. Plos One. 2017; 12(4):e017 5910. https://\ndoi.org/10.13 71/journal.p one.0175910 PMID: 28407027\n28. Jordan TR, Sheen M, Abedipour L, Paterson KB. Visual Speech Perception in Foveal and Extrafovea l\nVision: Further Implicat ions for Divisions in Hemisp heric Projections . Plos One. 2014; 9(7):e9827 3.\nhttps://doi.or g/10.137 1/journal.po ne.00982 73 PMID: 25032950\n29. Sutskever I, Vinyals O, Le QV. Sequence to sequence learning with neural networks. Advances in neu-\nral information processing systems . 2014;27.\n30. Dosovitskiy A, Beyer L, Kolesniko v A, Weissenbor n D, Zhai X, Unterthine r T, et al. An image is worth\n16x16 words: Transfor mers for image recognit ion at scale. arXiv preprint arXiv:201011 929. 2020.\n31. He K, Zhang X, Ren S, Sun J, editors. Deep residua l learning for image recognition. Proceedings of the\nIEEE conferenc e on computer vision and pattern recognition; 2016.\n32. Huang G, Liu Z, Van Der Maaten L, Weinbe rger KQ, editors. Densely connected convolutio nal net-\nworks. Procee dings of the IEEE conferen ce on computer vision and pattern recognit ion; 2017.\n33. Zhao P, Li C, Rahaman MM, Xu H, Yang H, Sun H, et al. A comparative study of deep learning classifi-\ncation methods on a small environ mental microorg anism image dataset (EMDS- 6): from convolutio nal\nneural networks to visual transfor mers. Frontie rs in Microbiology . 2022;13. https:// doi.org/10.33 89/\nfmicb.2022.7 92166 PMID: 35308350\n34. Touvron H, Cord M, Douze M, Massa F, Sablayrolle s A, Je ´ gou H, editors. Training data-effic ient image\ntransforme rs & distillation throug h attention. Interna tional Conference on Machine Learning; 2021:\nPMLR.\n35. Yuan L, Chen Y, Wang T, Yu W, Shi Y, Jiang Z-H, et al., editors. Tokens-to- token vit: Training vision\ntransforme rs from scratch on imagenet. Proceedings of the IEEE/CVF Interna tional Conferen ce on\nComputer Vision; 2021.\n36. Srinivas A, Lin T-Y, Parmar N, Shlens J, Abbeel P, Vaswani A, editors. Bottleneck transforme rs for\nvisual recognition. Proceed ings of the IEEE/CV F conferen ce on computer vision and pattern recogni-\ntion; 2021.\n37. Zhao P, Li C, Rahaman MM, Xu H, Ma P, Yang H, et al. EMDS-6: Enviro nmental Microorg anism Image\nDataset Sixth Version for Image Denoising, Segmenta tion, Feature Extraction , Classifica tion, and\nDetection Method Evaluati on. Frontiers in Microbiol ogy. 2022:13 34. https://doi.or g/10.485 50/arXiv.\n2112.07111\n38. Chollet F, editor Xception : Deep learning with depthwise separab le convolutio ns. Proceedings of the\nIEEE conferenc e on computer vision and pattern recognition; 2017.\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 21 / 22\n39. Dou H, Wu X, editors. Coarse-to -fine trained multi-scale Convolution al Neural Networks for image clas-\nsification. Internation al Joint Conferen ce on Neural Networks; 2015.\n40. Gulati A, Qin J, Chiu C-C, Parmar N, Zhang Y, Yu J, et al. Conformer : Convolution -augme nted trans-\nformer for speech recognit ion. arXiv preprint arXiv:200508 100. 2020.\n41. Yuan K, Guo S, Liu Z, Zhou A, Yu F, Wu W, editors. Incorpor ating convolutio n designs into visual trans-\nformers. Proceedings of the IEEE/CVF International Conferen ce on Comput er Vision; 2021.\n42. Wu H, Xiao B, Codella N, Liu M, Dai X, Yuan L, et al., editors. Cvt: Introducing convolutio ns to vision\ntransforme rs. Proceed ings of the IEEE/CV F Internation al Conferen ce on Computer Vision; 2021.\n43. Kaiser L, Gomez AN, Chollet F. Depthwise separab le convolutio ns for neural machine translation . arXiv\npreprint arXiv:170603 059. 2017.\n44. Zhou D, Hou Q, Chen Y, Feng J, Yan S, editors. Rethinking bottleneck structure for efficient mobile net-\nwork design. European Conferen ce on Computer Vision; 2020: Springer.\n45. Kruk M, Kozera R, Osowski S, Trzciński P, Paszt LS, Sumorok B, et al., editors. Comput erized classifi-\ncation system for the identifi cation of soil microorganis ms. AIP conferen ce proceed ings; 2015: AIP\nPublishing LLC.\n46. Cunshe C, Xiaojuan L, editors. A new wastewate r bacteria classific ation with microscopic image analy-\nsis. Proceedings of the 12th WSEAS internationa l conferen ce on computers ; 2008.\n47. Xiaojuan L, Cunshe C. An improved BP neural network for wastewate r bacteria recognition based on\nmicroscopic image analysis. WSEA S Trans Comput. 2009; 8(2):237–4 7.\n48. Li C, Shirahama K, Grzegorzek M, Ma F, Zhou B, editors. Classifica tion of environm ental microorg an-\nisms in microsc opic images using shape features and support vector machines . 2013 IEEE interna tional\nconferen ce on image process ing; 2013: IEEE.\n49. Luo JY, Irisson JO, Graham B, Guigand C, Sarafraz A, Mader C, et al. Automate d plankton image anal-\nysis using convolutio nal neural networks . Limnology and Oceanograp hy: methods. 2018; 16(12):814 –\n27. https://doi. org/10.1002/l om3.1028 5\n50. Połap D, Woźniak M, editors. Bacteria shape classification by the use of region covariance and convolu-\ntional neural network. 2019 Interna tional Joint Conferen ce on Neural Networks (IJCNN); 2019: IEEE.\n51. Bliznuks D, Chizhov Y, Bondarenko A, Uteshe v D, Liepins J, Zolins S, et al., editors. Embedd ed neural\nnetwork system for microorganis ms growth analysis. Saratov Fall Meeting 2019: Optical and Nano-\nTechnolo gies for Biology and Medicine; 2020: SPIE.\n52. Li Z, Li C, Yao Y, Zhang J, Rahaman MM, Xu H, et al. EMDS-5: Environm ental Microorganis m image\ndataset Fifth Version for multiple image analysis tasks. Plos one. 2021; 16(5):e025 0631. https://doi.or g/\n10.1371/ journal.pon e.0250631 PMID: 339793 56\n53. Han K, Xiao A, Wu E, Guo J, Xu C, Wang Y. Transfor mer in transfor mer. Advances in Neural Informa-\ntion Processin g Systems. 2021; 34:1590 8–19.\n54. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, et al., editors. Swin transforme r: Hierarchic al vision trans-\nformer using shifted window s. Proceedings of the IEEE/CV F International Conferen ce on Computer\nVision; 2021.\n55. d’Ascoli S, Touvron H, Leavitt ML, Morcos AS, Biroli G, Sagun L, editors. Convit : Improving vision trans-\nformers with soft convolutio nal inductive biases. International Conferen ce on Machine Learning; 2021:\nPMLR.\n56. Yahui L, Sangineto E, Wei B, Sebe N, Bruno L, De Nadai M, editors. Efficient Training of Visual Trans-\nformers with Small-Siz e Datasets. NeurIPS; 2021: NeurIPS.\n57. Lumini A, Nanni L. Deep learning and transfer learning features for plankton classif ication. Ecologica l\ninformati cs. 2019; 51:33–43.\n58. Chu X, Zhang B, Tian Z, Wei X, Xia H. Do we really need explicit position encodin gs for vision transform-\ners. arXiv preprint arXiv:2 10210882. 2021; 3(8).\nPLOS ONE\nA novel hybrid Transfor mer-CNN architectur e for environmen tal microorganis m classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 57 November 11, 2022 22 / 22",
  "topic": "Architecture",
  "concepts": [
    {
      "name": "Architecture",
      "score": 0.5482308864593506
    },
    {
      "name": "Computer science",
      "score": 0.45559051632881165
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33176130056381226
    },
    {
      "name": "Computational biology",
      "score": 0.32812798023223877
    },
    {
      "name": "Biology",
      "score": 0.3013772964477539
    },
    {
      "name": "Geography",
      "score": 0.09540867805480957
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151727225",
      "name": "Harbin Engineering University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I145897649",
      "name": "Minzu University of China",
      "country": "CN"
    }
  ]
}