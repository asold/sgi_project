{
  "title": "DarkEmbed: Exploit Prediction With Neural Language Models",
  "url": "https://openalex.org/W2807143630",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2574719704",
      "name": "Nazgol Tavabi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2342562826",
      "name": "Palash Goyal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2775509944",
      "name": "Mohammed Almukaynizi",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A16615764",
      "name": "Paulo Shakarian",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2149625712",
      "name": "Kristina Lerman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W150078352",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W6821113053",
    "https://openalex.org/W2402739313",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W1415990210",
    "https://openalex.org/W6723140618",
    "https://openalex.org/W2614184875",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2472414028",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2494800716",
    "https://openalex.org/W6817491142",
    "https://openalex.org/W61482965",
    "https://openalex.org/W6735274614",
    "https://openalex.org/W308556676",
    "https://openalex.org/W1707806712",
    "https://openalex.org/W2554200766",
    "https://openalex.org/W6646744811",
    "https://openalex.org/W6722221299",
    "https://openalex.org/W1420268584",
    "https://openalex.org/W2484789065",
    "https://openalex.org/W2964051216",
    "https://openalex.org/W4239319433",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W1985324839",
    "https://openalex.org/W2735150897",
    "https://openalex.org/W3137883743",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2599027056",
    "https://openalex.org/W4236137412",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2963980822"
  ],
  "abstract": "Software vulnerabilities can expose computer systems to attacks by malicious actors. With the number of vulnerabilities discovered in the recent years surging, creating timely patches for every vulnerability is not always feasible. At the same time, not every vulnerability will be exploited by attackers; hence, prioritizing vulnerabilities by assessing the likelihood they will be exploited has become an important research problem. Recent works used machine learning techniques to predict exploited vulnerabilities by analyzing discussions about vulnerabilities on social media. These methods relied on traditional text processing techniques, which represent statistical features of words, but fail to capture their context. To address this challenge, we propose DarkEmbed, a neural language modeling approach that learns low dimensional distributed representations, i.e., embeddings, of darkweb/deepweb discussions to predict whether vulnerabilities will be exploited. By capturing linguistic regularities of human language, such as syntactic, semantic similarity and logic analogy, the learned embeddings are better able to classify discussions about exploited vulnerabilities than traditional text analysis methods. Evaluations demonstrate the efficacy of learned embeddings on both structured text (such as security blog posts) and unstructured text (darkweb/deepweb posts). DarkEmbed outperforms state-of-the-art approaches on the exploit prediction task with an F1-score of 0.74.",
  "full_text": "DarkEmbed: Exploit Prediction with Neural Language Models\nNazgol Tavabi\nUSC Information Sciences Institute\nnazgolta@isi.edu\nPalash Goyal\nUSC Information Sciences Institute\npalashgo@usc.edu\nMohammed Almukaynizi\nArizona State University\nmalmukay@asu.edu\nPaulo Shakarian\nArizona State University\nshak@asu.edu\nKristina Lerman\nUSC Information Sciences Institute\nlerman@isi.edu\nSoftware vulnerabilities can expose computer systems to\nattacks by malicious actors. With the number of vulner-\nabilities discovered in the recent years surging, creating\ntimely patches for every vulnerability is not always feasi-\nble. At the same time, not every vulnerability will be ex-\nploited by attackers; hence, prioritizing vulnerabilities by\nassessing the likelihood they will be exploited has become\nan important research problem. Recent works used ma-\nchine learning techniques to predict exploited vulnerabil-\nities by analyzing discussions about vulnerabilities on so-\ncial media. These methods relied on traditional text process-\ning techniques, which represent statistical features of words,\nbut fail to capture their context. To address this challenge,\nwe propose DarkEmbed, a neural language modeling ap-\nproach that learns low dimensional distributed representa-\ntions, i.e., embeddings, of darkweb/deepweb discussions to\npredict whether vulnerabilities will be exploited. By captur-\ning linguistic regularities of human language, such as syn-\ntactic, semantic similarity and logic analogy, the learned em-\nbeddings are better able to classify discussions about ex-\nploited vulnerabilities than traditional text analysis methods.\nEvaluations demonstrate the efﬁcacy of learned embeddings\non both structured text (such as security blog posts) and un-\nstructured text (darkweb/deepweb posts). DarkEmbed out-\nperforms state-of-the-art approaches on the exploit predic-\ntion task with anF\n1-score of 0.74.\nIntroduction\nVulnerabilities in software expose computer systems to at-\ntacks by cybercriminals. The consequences of an attack can\nbe severe, as demonstrated on May 12, 2017, when Wan-\nnacry ransomware (Martin, Kinross, and Hankin 2017), ex-\nploiting a vulnerability in Microsoft Windows operating\nsystem, crippled hundreds of thousands of computer sys-\ntems worldwide, including critical systems used by hospi-\ntals and others health services (Kostov, Neumann, and Woo\n2017). To avoid attacks on their software, vendors need\nto create patches for discovered vulnerabilities. However,\nnot every vulnerability is equally critical to patch. While\na growing number of vulnerabilities are discovered each\nyear—in the ﬁrst four months of 2017 alone more than\n5,000 vulnerabilities were disclosed by National Vulnerabil-\nCopyright © 2018, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All\nrights reserved.\nity Database (NVD)1—fewer than 3% of these have exploits\nthat exist in the wild (Sabottke, Suciu, and Dumitras 2015;\nAllodi and Massacci 2014). Given that so few vulnerabili-\nties are exploited, how should vendors prioritize which ones\nto patch? To address this problem, researchers have recently\nturned to machine learning techniques to analyze different\ndata sources about vulnerabilities for clues to exploitability\n(Bozorgi et al. 2010; Edkrantz and Said 2015). Along this\napproach (Sabottke, Suciu, and Dumitras 2015) used terms\nappearing in Twitter posts associated with vulnerabilities, as\nfeatures to train a classiﬁer to predict which ones will be\nexploited. However, traditional text mining approaches fail\nto capture the context of the discussions, and thereby have\na hard time distinguishing between potentially threatening\nposts and non-malicious discussions of vulnerabilities. The\ntwo posts below illustrate these differences.\n• “. . . ﬁrst advertise of this kit after several months of\nshutdown. rates for wm are 20/30%prices:100$/day600$/\nweek2000$/ month. . . exploits:cve-2015-5122cve-2015-\n5119cve-2015-3043cve-2015-2419cve-2015-2445cve-\n2015-0311cve-2014-6332 ...”\n• “. . . this is a really dangerous security ﬂaw. poc of cve-\n2014-0476 is available lookup google linux kernel vul-\nnerable to privilege escalation and dos attack”\nThe ﬁrst post advertises an exploit kit for sale on a dark-\nweb marketplace with a considerable price, which is a lead-\ning indicator of an attack. In contrast, the second post sim-\nply talks about a vulnerability. Given the words in the two\nposts, the second post seems more likely to be connected\nto a threat, but this is actually not the case. Traditional text\nmining methods that do not capture the context of words\nwill fail to detect the differences. Another disadvantage is\nthat they use sparse, high-dimensional features, which may\nlead to suboptimal performance in a classiﬁcation task.\nTo address these challenges, we describe a neural lan-\nguage model that analyzes discussions about vulnerabilities\nto predict whether they will be exploited in the wild. Specif-\nically, we use paragraph vector (Le and Mikolov 2014), an\nunsupervised algorithm that embeds variable-length texts in\na low-dimensional vector space, to learn distributed repre-\nsentations of discussions on the darkweb/deepweb (D2Web).\n1https://nvd.nist.gov\nThe Thirtieth AAAI Conference on Innovative Applications of Artificial Intelligence (IAAI-18)\n7849\nWe then train a classiﬁer to recognize posts discussing vul-\nnerabilities that will be exploited in the wild.\nThe paragraph vector is effective, because it captures the\nmeaning of discussions and their other characteristics, such\nas language, and indicator words. Evaluations show that it\noutperforms classiﬁers which use word frequencies by 10%\nin predicting exploited vulnerabilities. The method also de-\ncreases the dimension of the feature space by 0.001 of the\noriginal values. Moreover, we show that adding other fea-\ntures, such as CVSS score of the vulnerability and whether\nit appeared in ExploitDB, improves prediction performance\nby 12%.\nOverall, our paper makes the following contributions:\n• We propose DarkEmbed, an efﬁcient algorithm which uti-\nlizes neural language models to learn features of conver-\nsations on the D2Web.\n• We use DarkEmbed to predict whether a vulnerability dis-\ncussed on D2Web will be exploited.\n• We extend DarkEmbed to use other features of vulnerabil-\nities, such as CVSS score, and evaluated how much these\nfeatures improve predictions.\n• We use the same approach of DarkEmbed on security blog\nposts to detect new exploited vulnerabilities at the earliest\nopportunity.\n• Using DarkEmbed, we identify keywords in D2Web in-\ndicative of exploit probability, i.e., words which are asso-\nciated with high and low rates of exploitation.\nThe rest of the paper is organized as follows. We ﬁrst re-\nview existing research on estimating the likelihood that a\nvulnerability will be exploited. We then review neural lan-\nguage model which learns distributed representations of text,\nas well as the data set we use. We evaluate the learned repre-\nsentations of D2Web posts on the task of exploit prediction\nand conclude with the discussion of the implications of the\nresults.\nRelated work\nA great deal of the current research on cybersecurity defense\nhas focused on detecting emerging cyber threats. Although\nlimited, the work on predicting cybersecurity incidents is\ngaining larger attention in recent years (Liu et al. 2015;\nSoska and Christin 2014; Hao et al. 2016; Sabottke, Su-\nciu, and Dumitras 2015) - along the same line comes our\nwork. Several approaches to evaluating the severity of soft-\nware vulnerabilities and predicting whether they will be ex-\nploited have been pursued. The National Institute of Stan-\ndards and Technology, NIST, uses Common Vulnerability\nScoring System (CVSS) to assess the severity of the vul-\nnerability (Quinn et al. 2010). This metric assigns a score\nto vulnerabilities, which is formulated using different char-\nacteristics, such as ease of exploit and scale of damage it\nmay cause if exploited (Scarfone and Mell 2009). Unfortu-\nnately, this metric was proven not to be very effective, since\nit marks many vulnerabilities as exploitable though majority\nof them will never be attacked (Allodi and Massacci 2014).\nFigure 1: The framework of DarkEmbed.\nThis is also the shortcoming of other standard scoring sys-\ntems, such as Microsoft's exploitability index2 and Adobe\nPriority Rating3.\nWith the ever growing number of vulnerabilities discov-\nered and the threats they pose, different data sources have\nbeen generated and are publicly available to help enhance\ncybersecurity. Some previous works have combined these\ndifferent data sources to get more accurate predictions (Bo-\nzorgi et al. 2010; Edkrantz and Said 2015), specially since\nmachine learning algorithms can easily combine these dif-\nferent sources to achieve best results. NVD and ExploitDB\nare among those data sources used in different methods.\nNVD: NIST provides this database which has a list of vulner-\nabilities disclosed, it also contains descriptions, CVSS score\nand other metrics for each vulnerability.ExploitDB\n4:I ti s\na repository for exploits, reported by security researchers. It\nprovides proof of concept exploits which shows the vulner-\nability is exploitable but not necessary exploited. Another\ndata source is blog posts written by cyber security experts,\nsecurity analysts as well as white hat hackers, which has not\nbeen used in previous works and provides news and updated\ninformation about cyber security topics.\nIn other previous works it has been proposed that using\ndiscussions surrounding a vulnerability in social media like\nTwitter (Mittal et al. 2016) or marketplaces on the darkweb\n(Marin, Diab, and Shakarian 2016; Samtani et al. 2016) can\nhelp predict exploitation. Speciﬁcally, (Sabottke, Suciu, and\nDumitras 2015) was able to predict exploited vulnerabilities\nmore accurately than existing methods. However, it looks at\nthe words surrounding that vulnerability which fails to cap-\nture semantics of the words and leads to data sparsity and\nhigh dimensionality. The approach discussed in the current\npaper addresses some of the problems of existing methods\nby using neural embeddings of discussions about vulnera-\nbilities.\nMethodology\nOur framework for exploit prediction consists of two com-\nponents: (1) learning embeddings of D2Web posts, and (2)\nexploit classiﬁer. Figure 1 illustrates the juxtaposition of\nthese components. We learn the distributed representations\nof the D2Web posts and use them as features, potentially\nwith other features, such as CVSS score and exploitDB, in a\n2https://technet.microsoft.com/en-us/security/cc998259.aspx\n3https://helpx.adobe.com/security/severity-ratings.html\n4https://www.exploit-db.com\n7850\nclassiﬁer which predicts whether vulnerabilities mentioned\nin the posts will be exploited.\nD2Web Crawling Infrastructure\nTo collect data, we use the infrastructure for crawling the\ndarkweb and deepweb originally introduced in (Robert-\nson et al. 2017; Nunes et al. 2016). In this context,dark-\nweb refers to sites accessed through anonymization proto-\ncols such as Tor and I2P , while deepweb refers to non-\nindexed sites on the open Internet (Shakarian, Gunn, and\nShakarian 2016). The crawling infrastructure handles sites\nof both types. The framework consists of an infrastructure\nthat enables lightweight crawlers and parsers that are fo-\ncused on speciﬁc sites. At the time of this writing, we have\ncreated crawlers and parsers for a manually-compiled list of\nover 200 sites relating to malicious hacking and/or online ﬁ-\nnancial fraud, including ﬁshing, spear-ﬁshing, ransomware,\ncredit card frauds, etc. This framework also helps ensure\nthat the obtained data remains relevant to cyber-security: in-\ndeed, many darkweb and deepweb sites also create forums\nfor other illicit activities, such as drug markets and the sale\nof stolen goods.\nLearning Embeddings\nRecent works in natural language processing popularized\ndistributed representation learning and introduced a family\nof neural language models (Bengio et al. 2003; Mnih and\nHinton 2007) to model sequences of words in sentences\nand documents. These models embed words in a ﬁxed-\ndimension vector space, such that words in similar con-\ntexts tend to produce similar representations in vector space.\nThese distributed representations of words capture many lin-\nguistic regularities of human language, such as syntactic, se-\nmantic similarity and logical analogy. We learn a context-\nbased representation of D2Web posts in two steps. First, we\nlearn distributed representations of words using word em-\nbedding. To go from distributed representations of words to\ndistributed representations of variable-length D2Web posts,\nwe could simply aggregate vectors of all the words contained\nin a post and compute their average. However, this meth-\nods does not work as well as usingparagraph embeddingto\nlearn the global context of words in the entire post. We de-\nscribe these methods below. An embedding projects words\nin a lower-dimensional vector space withd dimensions, so\nthat each wordw\ni is represented by ad-dimensional vector\nvi. Words that are used in similar contexts will be closer to\none another in this vector space. While context usually im-\nplies semantic or meaning of the word, here it simply cap-\ntures how the word is used within a sequence of words. For\nexample, given two sentences—“The cat sat on the mat.”\nand “The dog sat on the ﬂoor.”—“dog” and “cat” are used in\nsimilar contexts, and thus, may be similar.\nOf the many proposed models for learning distributed\nrepresentations (Mikolov et al. 2013a; 2013b; Bengio et\nal. 2003; Collobert and Weston 2008), we use Skip-Gram\nwith Negative Sampling (SGNS) (Mikolov et al. 2013b).\nThe model takes as input a tokenized text corpus C =\n{w\n1,w 2,...,w n} and creates a context for each wordwi\nFigure 2: Framework for learning paragraph vectors.\nas {wi−k,...,w i−1,w i+1,...,w i+k} where k is the con-\ntext length. Given the embedding of word wi, vi, the\nmodel aims to reconstruct the embedding of the con-\ntext, {v\ni−k,...,v i−1,v i+1,...,v i+k}. It randomly samples\n“negative” examples i.e. words which do not co-occur to-\ngether and maximizes (minimizes) the probability of observ-\ning positive (negative) examples from the data.\nTo learn the distributed representation for the entire post,\nwe follow the intuition of learning word embeddings. Here,\ninstead of predicting a context for a particular word, the\nmodel samples multiple contexts from the paragraph and\npredicts the next word given the context (Figure 2). The\ncontext is obtained using a sliding window of lengthk over\nthe paragraph. The representation is learned using stochas-\ntic gradient descent (Rumelhart, Hinton, and Williams 1988)\nand gradients are calculated using back propagation.\nWe use all the posts to learn distributed representations,\nsince having a larger corpus helps to learn better embed-\ndings. One of the advantages of using the paragraph vec-\ntor is that it simpliﬁes the task of handling multiple lan-\nguages. Posts in different languages are embedded in the\nsame vector space, making their comparison easier. In ad-\ndition, since they may naturally fall into different clusters\nwithin this space, it is easy to identify the language of the\npost, which may help learn the language bias in D2Web vul-\nnerability posts leading to more accurate exploit prediction.\nClassiﬁcation\nWe formulate exploit prediction as a classiﬁcation task.\nGiven a set of posts discussing vulnerabilities andground\ntruth containing positive examples (vulnerabilities for which\nexploits exist in the wild), we train a classiﬁer to recog-\nnize posts that discuss exploited vulnerabilities. As features\nfor the classiﬁer, we use vectors representing post embed-\ndings and number of times a vulnerability was mentioned in\nD2Web. Then, given a new post mentioning a vulnerability,\nthe classiﬁer decides whether that vulnerability is exploited.\nFor this problem Support V ector Machines (SVM) (Cortes\nand V apnik 1995) with Radial basis function (RBF) kernel\nperforms better than other examined classiﬁers. SVM is a\nsupervised learning model which ﬁnds a set of hyperplanes\nthat best separate different classes by having the largest mar-\ngin. We also explored using Random Forest classiﬁer, a\ncombination of decision trees (Quinlan 1986), in which ran-\n7851\nFigure 3: Comparison of the performance of classiﬁers for\nvulnerability exploit prediction.\ndom selection of features are given and the ﬁnal output is\ndecided by taking a vote from individual tree predictors.\nResults\nWe used a dataset containing almost 2,500,000 messages\nposted on a variety of darkweb and deepweb sites over the\nperiod from 2010 through August 2017. These posts were\nin 17 different languages, with English, Arabic and Russian\nbeing the most common languages. We identiﬁed vulnera-\nbilities mentioned in D2Web posts using regular expression\npatterns to match CVEs, the unique identiﬁers of vulnera-\nbilities. Since our goal is to predict vulnerabilities that are\nlikely to be exploited, the posts referencing vulnerabilities\nafter the exploitation date were removed from the data. This\nﬁltering step left 4898 posts mentioning 1886 distinct CVEs,\nsome vulnerabilities were mentioned in more than one post.\nFor the posts mentioning more than one vulnerability, we\nonly considered the less frequently mentioned CVE. The\nground truth was obtained from two sources: (1)Symantec's\nanti-virus\n5 and Intrusion Detection Systems6 attack signa-\ntures and (2) a database of the exploits deployed forMetas-\nploit.\nSymantec attack signatures report exploits detected in the\nwild and their corresponding vulnerabilities, along with the\ntime the exploit was discovered. Metasploit is a popular\nopen source penetration testing framework which allows us-\nage of install-and-test exploits developed by the cybersecu-\nrity community and a company called Rapid7\n7. Each Metas-\nploit's exploit is reported with the date it was deployed. The\nvulnerabilities mentioned on D2Web were labeled positive,\nif they have a corresponding attack signature in Symantec's\nlist or exploits available on Rapid7's site, and negative other-\nwise. Of the CVE mentioned on D2Web, only 149 are clas-\nsiﬁed as exploited - these represent only 8% of the vulnera-\nbilities in our dataset.\n5https://www.symantec.com/security response/landing/azlisting.jsp\n6https://www.symantec.com/security response/attacksignatures/\n7https://www.rapid7.com/db/modules/\nExploit Prediction\nWe train a classiﬁer to recognize vulnerabilities discussed in\nposts that will be subsequently exploited. We useF\n1 score\nand AUC (area under the ROC-curve) to evaluate classiﬁ-\ncation performance. To optimize performance, we tune pa-\nrameters to the data. Most of the parameters are for learning\nthe embeddings, including dimension of the representations,\nwindow size, the degree of negative sampling, and frequency\nthreshold for words. Having a high dimension space gives\nthe model the ability to better represent the posts; however,\nit takes more space and might lead to sparse representations.\nWindow is the context referred to in previous sections, used\nfor predicting the next word. Higher window sizes takes\nlonger to train but it might be able to better capture the con-\ntext. Negative sampling means randomly sampling words\nwhich do not co-occur together, and minimizing the prob-\nability of observing those words together.\nComparison to Baseline\nAs an alternative to word embeddings, we use TF-IDF-based\nrepresentation of D2Web posts as the baseline for compar-\ning performance. This approach is similar in spirit to exist-\ning work that predicts exploits based on online discussions\nof vulnerabilities (Sabottke, Suciu, and Dumitras 2015). TF-\nIDF approach represents posts as vectors with the same\nlength as the vocabulary of the entire text corpus, i.e., posts.\nEach entry in the vector corresponds to a unique word, and\nits weight gives the frequency of that word in the post (TF)\ndivided by its document frequency (IDF), i.e., the number\nof posts in which the word appears. Since the TF-IDF vec-\ntors can be quite large, classiﬁcation methods using them\nwould experience slow processing time and large memory\nusage. To reduce the size of document vectors, instead of\nthe entire vocabulary, often a subset of the most frequent\nwords is used to represent the documents. These document\nvectors are then used in the classiﬁcation task. Also since\nTF-IDF results in high dimensional representations, random\nforest can usually perform better in these problems, hence\nwe used both classiﬁers (SVM and Random Forest) on TF-\nIDF features. Figure 3 reports the performance of (1) the ran-\ndom forest classiﬁer and (2) the SVM classiﬁer on TF-IDF\nvectors as features. The TF-IDF vectors were constructed\nfor words appearing more than once in the dataset (61,995\nwords). Finally, the ﬁgure also reports the performance of\nDarkEmbed using a 101 dimensional embedding space and\nSVM classiﬁer. DarkEmbed outperforms baseline.\nAdding Features\nPost embeddings can be combined with other features of\nvulnerabilities to improve performance of exploit prediction.\nFor example a binary feature indicating whether the vulnera-\nbility appears in ExploitDB, or its CVSS scores from NVD,\ncan be used by the classiﬁers to improve performance. To\nillustrate, we combined CVSS score for each vulnerability\nand a binary feature for ExploitDB with D2Web post's em-\nbeddings. The added features improved classiﬁcation perfor-\nmance fromF\n1 measure of 0.66 to 0.74. Figure 4 shows that\nincrementally adding each feature improves classiﬁer per-\nformance.\n7852\nFigure 4: Performance of classiﬁers using additional features\non the vulnerability exploit prediction task. Each new fea-\nture is added incrementally.\nTable 1: Classiﬁcation results on different methods\nMethod F1 AUC\nBaseline methods\nTF-ID(RF) 0.54 0.69\nTF-IDF(SVM) 0.60 0.78\nDarkEmbed\n101 dimensions 0.66 0.84\nAdding features to DarkEmbed(101 dim)\n+ExploitDB 0.66 0.87\n+CVSS-Score 0.74 0.92\nUsing blogs to detect exploited vulnerabilities\nBlogs 0.80 0.87\nUsing Security Blogs\nAs mentioned earlier, the ground truth for this task was ob-\ntained from Symantec and Metasploit penetration tools. Al-\nthough most cyber attacks are caused by a handful of vul-\nnerabilities, which are already included in our ground truth,\nthere are other exploited vulnerabilities that are not included\nin these sources. To address this gap in the ground truth, we\nused blogs written by cyber security experts to identify new\nexploited vulnerabilities. We collected blog posts from 218\ncyber security experts, covering period from 2001 to 2017.\nTo identify exploited vulnerabilities mentioned in blogs,\nwe applied the DarkEmbed approach to blogs by using em-\nbeddings of blogs, along with other features, to classify vul-\nnerabilities. Here, we did not ﬁlter out posts published af-\nter exploit date as we aim to detect exploited vulnerabilities\ninstead of predicting them. Also, we only considered posts\nmentioning a single vulnerability. We used embedding of\nsize 150 (blog posts are lengthier that darkweb posts), CVSS\nscore and number of times a vulnerability was mentioned\nin this dataset as features. Note that the optimal embedding\nsize was obtained through cross validation. With 1613 blog\nposts in our dataset, we were able to achieveF\n1 =0 .80 and\nAUC =0 .87.\nTable 2: Software related discriminative words identiﬁed by\nDarkEmbed\nCategory Words # of vul. # of exploits % exploits\nFlash 19 14 73.7%\nAdobe 21 14 66.7%\nPositive XP 16 10 62.5%\nMicrosoft 68 25 36.8%\nWindows 42 13 31.0%\niOS 4 0 0%\nSamba 7 0 0%\nNegative Kernel 16 0 0%\nAndroid 30 0 0%\nLinux 38 6 15.8%\nDistinctive Words\nIn order to better interpret DarkEmbed results, we identi-\nﬁed key words in D2Web indicative of exploitability. Using\nclassiﬁcations of our ﬁnal classiﬁer, D2Web posts were sep-\narated into two classes: posts mentioning exploited vulnera-\nbilities (positive) and other posts (negative). Frequencies of\nwords in a speciﬁc class relative to the size of the class were\ncalculated. The words with highest difference in relative fre-\nquencies between the two classes were marked as distinctive\nwords of that class. Since D2Web posts are in different lan-\nguages many of these words were not in English.\nThe distinctive words identiﬁed fall into two categories:\ngeneral purpose words and software related words. Some\ngeneral words indicative of exploitation identiﬁed by Dark-\nEmbed are “exploit”, “vulnerable” and “push” while those\nassociated with low exploitation probability are “long”,\n“char” and “local”. Table 2 shows words related to software\nidentiﬁed by our model to positively and negatively impact\nexploitability. We observe that the software detected corre-\nlate with the exploits in the wild. For example, more than\n50% of the vulnerabilities of Flash, Adobe and Microsoft\nwere exploited whereas none of vulnerabilities associated\nwith iOS, Samba and Android were exploited.\nDiscussion and Future Work\nLearning distributed representations of discussions on\nD2Web, by embedding them in a lower-dimensional space,\nleverages the ability of neural language models to capture\nﬁne-grained statistical relationships between words. Instead\nof treating each word independently, as traditional statistical\napproaches to text analysis do, embeddings capture deeper\nrelationships between words that represent their meaning in\ncontext. Another advantage of this approach is its compres-\nsion: for example, we used it to represent a corpus with over\na million posts and tens of thousands of unique words using\nvectors of length 100. In contrast, traditional text approaches\nrequire large vectors to represent frequencies of all words.\nWe showed that post embeddings allow us to learn fea-\ntures of discussions about vulnerabilities on the D2Web that\nare useful for predicting whether the vulnerabilities will be\nexploited in the wild. We did this by training a classiﬁer\nto use post embeddings to discriminate between discussions\nabout exploited vulnerabilities.\nFurther research is needed to understand the interpretation\n7853\nof features learned using distributed representations. An-\nother direction is applying this approach to other discussion\nforums where malicious actors may discuss software vulner-\nabilities. For example, social media posts are an interesting\napplication domain that poses challenges to text analysis due\nto the brevity of posts.\nAcknowledgements\nThis work was supported by the Ofﬁce of the Director of Na-\ntional Intelligence (ODNI) and the Intelligence Advanced\nResearch Projects Activity (IARPA) via the Air Force Re-\nsearch Laboratory (AFRL) contract number FA8750-16-C-\n0112. The U.S. Government is authorized to reproduce and\ndistribute reprints for Governmental purposes notwithstand-\ning any copyright annotation thereon. Disclaimer: The views\nand conclusions contained herein are those of the authors\nand should not be interpreted as necessarily representing\nthe ofﬁcial policies or endorsements, either expressed or im-\nplied, of ODNI, IARPA, AFRL, or the U.S. Government.\nReferences\nAllodi, L., and Massacci, F. 2014. Comparing vulnerability\nseverity and exploits using case-control studies.ACM Trans.\nInf. Syst. Secur .17(1):1:1–1:20.\nBengio, Y .; Ducharme, R.; Vincent, P .; and Jauvin, C. 2003.\nA neural probabilistic language model.Journal of Machine\nLearning Research3:1137–1155.\nBozorgi, M.; Saul, L. K.; Savage, S.; and V oelker, G. M.\n2010. Beyond heuristics: Learning to classify vulnerabilities\nand predict exploits. InKDD2010, 105–114.\nCollobert, R., and Weston, J. 2008. A uniﬁed architecture\nfor natural language processing: Deep neural networks with\nmultitask learning. InICML, 160–167.\nCortes, C., and V apnik, V . 1995. Support-vector networks.\nMach. Learn.20(3):273–297.\nEdkrantz, M., and Said, A. 2015. Predicting cyber vulnera-\nbility exploits with machine learning. InSCAI.\nHao, S.; Kantchelian, A.; Miller, B.; Paxson, V .; and Feam-\nster, N. 2016. Predator: Proactive recognition and elimina-\ntion of domain abuse at time-of-registration. InCCS2016,\n1568–1579.\nKostov, N.; Neumann, J.; and Woo, S. 2017. Cyberattack\nvictims begin to assess ﬁnancial damage. Wall Street Jour-\nnal.\nLe, Q., and Mikolov, T. 2014. Distributed representations\nof sentences and documents. In Xing, E. P ., and Jebara, T.,\neds., ICML, volume 32, 1188–1196.\nLiu, Y .; Sarabi, A.; Zhang, J.; Naghizadeh, P .; Karir, M.; Bai-\nley, M.; and Liu, M. 2015. Cloudy with a chance of breach:\nForecasting cyber security incidents. InUsenix Security.\nMarin, E.; Diab, A.; and Shakarian, P . 2016. Product offer-\nings in malicious hacker markets. InISI, 187–189.\nMartin, G.; Kinross, J.; and Hankin, C. 2017. Effective\ncybersecurity is fundamental to patient safety.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a.\nEfﬁcient estimation of word representations in vector space.\narXiv preprint arXiv:1301.3781.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and\nDean, J. 2013b. Distributed representations of words and\nphrases and their compositionality. InNIPS, 3111–3119.\nMittal, S.; Das, P . K.; Mulwad, V .; Joshi, A.; and Finin, T.\n2016. Cybertwitter: Using twitter to generate alerts for cy-\nbersecurity threats and vulnerabilities. In ASONAM, 860–\n867.\nMnih, A., and Hinton, G. 2007. Three new graphical models\nfor statistical language modelling. InICML, 641–648.\nNunes, E.; Diab, A.; Gunn, A.; Marin, E.; Mishra, V .;\nPaliath, V .; Robertson, J.; Shakarian, J.; Thart, A.; and\nShakarian, P . 2016. Darknet and deepnet mining for proac-\ntive cybersecurity threat intelligence. InISI, 7–12. IEEE.\nQuinlan, J. R. 1986. Induction of decision trees. Mach.\nLearn. 1(1):81–106.\nQuinn, S. D.; Scarfone, K. A.; Barrett, M.; and Johnson,\nC. S. 2010. Sp 800-117. guide to adopting and using the se-\ncurity content automation protocol (scap) version 1.0. Tech-\nnical report, Gaithersburg, MD, United States.\nRobertson, J.; Diab, A.; Marin, E.; Nunes, E.; Paliath, V .;\nShakarian, J.; and Shakarian, P . 2017. Darkweb Cyber\nThreat Intelligence Mining. Cambridge University Press.\nRumelhart, D. E.; Hinton, G. E.; and Williams, R. J. 1988.\nNeurocomputing: Foundations of research. Cambridge,\nMA, USA: MIT Press. chapter Learning Representations\nby Back-propagating Errors, 696–699.\nSabottke, C.; Suciu, O.; and Dumitras, T. 2015. Vulnerabil-\nity disclosure in the age of social media: Exploiting twitter\nfor predicting real-world exploits. InUSENIX, 1041–1056.\nSamtani, S.; Chinn, K.; Larson, C.; and Chen, H. 2016.\nAzsecure hacker assets portal: Cyber threat intelligence and\nmalware analysis. InISI, 19–24.\nScarfone, K., and Mell, P . 2009. An analysis of cvss version\n2 vulnerability scoring. InSESM, 516–525.\nShakarian, J.; Gunn, A. T.; and Shakarian, P . 2016. Explor-\ning malicious hacker forums. InCyber Deception. Springer.\n261–284.\nSoska, K., and Christin, N. 2014. Automatically detecting\nvulnerable websites before they turn malicious. InUsenix\nSecurity\n, 625–640.\n7854",
  "topic": "Exploit",
  "concepts": [
    {
      "name": "Exploit",
      "score": 0.8820565342903137
    },
    {
      "name": "Computer science",
      "score": 0.8416459560394287
    },
    {
      "name": "Vulnerability (computing)",
      "score": 0.6012938618659973
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5636915564537048
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5576554536819458
    },
    {
      "name": "Analogy",
      "score": 0.5520251393318176
    },
    {
      "name": "Task (project management)",
      "score": 0.5379278063774109
    },
    {
      "name": "Language model",
      "score": 0.48575359582901
    },
    {
      "name": "Machine learning",
      "score": 0.46429237723350525
    },
    {
      "name": "Natural language processing",
      "score": 0.39133235812187195
    },
    {
      "name": "Data science",
      "score": 0.3490593433380127
    },
    {
      "name": "Computer security",
      "score": 0.2768169343471527
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55732556",
      "name": "Arizona State University",
      "country": "US"
    }
  ],
  "cited_by": 85
}