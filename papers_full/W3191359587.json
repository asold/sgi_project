{
    "title": "Learning Fair Face Representation With Progressive Cross Transformer",
    "url": "https://openalex.org/W3191359587",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1990237803",
            "name": "Li Yong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2359469960",
            "name": "Sun Yufei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2128451743",
            "name": "Cui Zhen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2182446373",
            "name": "Shan, Shiguang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2008127672",
            "name": "Yang Jian",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3167651619",
        "https://openalex.org/W2963767627",
        "https://openalex.org/W3002638829",
        "https://openalex.org/W2984006054",
        "https://openalex.org/W3035232573",
        "https://openalex.org/W3090533328",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3181883364",
        "https://openalex.org/W1509966554",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2963010714",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W3035701170",
        "https://openalex.org/W2963212406",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W3175998650",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W3169737649",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3034552680",
        "https://openalex.org/W3214586131",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2969985801",
        "https://openalex.org/W3097003645",
        "https://openalex.org/W3134169675",
        "https://openalex.org/W2982232682",
        "https://openalex.org/W2910603373",
        "https://openalex.org/W1993766142",
        "https://openalex.org/W3123943836",
        "https://openalex.org/W2515770085",
        "https://openalex.org/W3034256900",
        "https://openalex.org/W2963466847",
        "https://openalex.org/W3099206234",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W1584317276",
        "https://openalex.org/W2981515171",
        "https://openalex.org/W2914454064",
        "https://openalex.org/W2214725774",
        "https://openalex.org/W2949386440",
        "https://openalex.org/W1978642336",
        "https://openalex.org/W3141497777",
        "https://openalex.org/W3005720838",
        "https://openalex.org/W3101998545",
        "https://openalex.org/W3151034650",
        "https://openalex.org/W3157525179",
        "https://openalex.org/W3171516518",
        "https://openalex.org/W2918252054",
        "https://openalex.org/W2962898354",
        "https://openalex.org/W3175851380",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W3119686997"
    ],
    "abstract": "Face recognition (FR) has made extraordinary progress owing to the advancement of deep convolutional neural networks. However, demographic bias among different racial cohorts still challenges the practical face recognition system. The race factor has been proven to be a dilemma for fair FR (FFR) as the subject-related specific attributes induce the classification bias whilst carrying some useful cues for FR. To mitigate racial bias and meantime preserve robust FR, we abstract face identity-related representation as a signal denoising problem and propose a progressive cross transformer (PCT) method for fair face recognition. Originating from the signal decomposition theory, we attempt to decouple face representation into i) identity-related components and ii) noisy/identity-unrelated components induced by race. As an extension of signal subspace decomposition, we formulate face decoupling as a generalized functional expression model to cross-predict face identity and race information. The face expression model is further concretized by designing dual cross-transformers to distill identity-related components and suppress racial noises. In order to refine face representation, we take a progressive face decoupling way to learn identity/race-specific transformations, so that identity-unrelated components induced by race could be better disentangled. We evaluate the proposed PCT on the public fair face recognition benchmarks (BFW, RFW) and verify that PCT is capable of mitigating bias in face recognition while achieving state-of-the-art FR performance. Besides, visualization results also show that the attention maps in PCT can well reveal the race-related/biased facial regions.",
    "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 1\nLearning Fair Face Representation With Progressive\nCross Transformer\nYong Li, Yufei Sun, Zhen Cui, Shiguang Shan, Jian Yang\nAbstract—Face recognition (FR) has made extraordinary\nprogress owing to the advancement of deep convolutional neural\nnetworks. However, demographic bias among different racial\ncohorts still challenges the practical face recognition system. The\nrace factor has been proven to be a dilemma for fair FR (FFR)\nas the subject-related speciﬁc attributes induce the classiﬁcation\nbias whilst carrying some useful cues for FR. To mitigate racial\nbias and meantime preserve robust FR, we abstract face identity-\nrelated representation as a signal denoising problem and propose\na progressive cross transformer (PCT) method for fair face\nrecognition. Originating from the signal decomposition theory,\nwe attempt to decouple face representation into i) identity-related\ncomponents and ii) noisy/identity-unrelated components induced\nby race. As an extension of signal subspace decomposition, we\nformulate face decoupling as a generalized functional expression\nmodel to cross-predict face identity and race information. The\nface expression model is further concretized by designing dual\ncross-transformers to distill identity-related components and\nsuppress racial noises. In order to reﬁne face representation,\nwe take a progressive face decoupling way to learn identity/race-\nspeciﬁc transformations, so that identity-unrelated components\ninduced by race could be better disentangled. We evaluate the\nproposed PCT on the public fair face recognition benchmarks\n(BFW, RFW) and verify that PCT is capable of mitigating bias in\nface recognition while achieving state-of-the-art FR performance.\nBesides, visualization results also show that the attention maps\nin PCT can well reveal the race-related/biased facial regions.\nIndex Terms—Face recognition, Transformer, Fair face recog-\nnition\nI. I NTRODUCTION\nA\nUTOMATIC face recognition has achieved considerable\nsuccess with the rapid developments of deep learning\nalgorithms [1]–[5]. However, Face recognition (FR) systems\nare found to exhibit discriminatory behaviors against certain\ndemographic groups [6]–[11]. Every face not only reﬂects\nindividual identity, but also exhibits many demographic at-\ntributes, such as race, ethnicity, age, gender and other visible\nforms of self-expression [12]. The face recognition systems\nare expected to work equally accurate for each of us. It means\nthe performance of a desired FR system should not vary\nfor different individuals or demographic groups. However, as\nCorresponding author: Zhen Cui, zhen.cui@njust.edu.cn\nYong Li, Yufei Song, Zhen Cui, Jian Yang are with the Key Laboratory\nof Intelligent Perception and Systems for High-Dimensional Information,\nMinistry of Education, School of Computer Science and Engineering, Nanjing\nUniversity of Science and Technology, Nanjing, China 210094 (e-mail:\n(yong.li, laolingjie, zhen.cui)@njust.edu.cn, csjyang@mail.njust.edu.cn).\nShiguang Shan is with the Key Laboratory of Intelligent Information Pro-\ncessing of Chinese Academy of Sciences, Institute of Computing Technology,\nCAS, Beijing 100190, China, and with the University of Chinese Academy of\nSciences, Beijing 100049, China, and also with CAS Center for Excellence\nin Brain Science and Intelligence Technology (e-mail: sgshan@ict.ac.cn).\nreported in the 2019 NIST Face Recognition Vendor Test\n[8], all participating FR algorithms exhibit different levels of\nbiased performances across various demographic (e.g., race,\ncountry of birth, gender, age) groups. Such FR systems with\nbias against speciﬁc minorities can lead to unjust or prejudicial\noutcomes. To prevent the unexpected side effects and to\nensure the long-term acceptance of the FR algorithms, the\ndevelopment and deployment of unbiased FR systems are vital\nand essential.\nCurrently, the popular FR models are with convolutional\nneural networks (CNNs) trained on large scale training data\nto represent face features in N-dimensions with minimal\ndistances between the same identity and maximum between\nunique identities. During the training process, the bias is\ninevitably introduced by both the imbalanced training data and\nthe mapping function that encodes the input face image into a\nlow-dimensional vector. Since the commonly used FR datasets\n(e.g., CASIA-WebFace [13], MS-Celeb-1M [14]) are collected\nfrom the Internet, face images are naturally imbalanced in\ndifferent demographic groups. To address the data imbalance\nissue, data re-sampling methods [15]–[17] have been carefully\nexploited to manually balance the data distribution by under-\nsampling the majority or oversampling the minority classes.\nHowever, naively training on a balanced dataset is beneﬁcial\nfor fair face recognition but can still lead to bias to some\ndegree [18].\nConsidering the limited beneﬁts brought by the balanced\ndatasets, a number of FR approaches have been proposed\n[9], [19]–[21] to mitigate the performance bias among dif-\nferent demographic groups. These methods aim to learn a\ndeep neural network that can fairly encodes the input faces\namong different demographic groups, regardless of whether\nthe training datasets are balanced or not. For example, Wang et\nal. [19] proposed a deep information maximization adaptation\nnetwork by considering Caucasian as the source domain and\nother races as target domains. Wang et al. [9] further proposed\na reinforcement learning-based network to ﬁnd the optimal\nmargins for different racial groups.\nIntuitively, in order to learn the fair face representation, the\nface recognition model needs to mitigate the bias caused by\nor related to race. This type of analysis comes up in multiple\nliterature [20], [22]. Among them, Gong et al [20] proposed a\nnovel de-biasing adversarial network (DebFace) that learns to\nencode disentangled feature representations for both unbiased\nface recognition and attribute estimation. The learned disen-\ntangled face representations exhibit less bias among different\ndemographic groups while show degraded FR performance\nsimultaneously. This is because some demographic attributes\narXiv:2108.04983v1  [cs.CV]  11 Aug 2021\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 2\nsuch as race and gender are critical components that constitute\nface patterns and are useful cues for FR [10], [20], [23], [24].\nWith similar motivation, Gong et al [25] proposed a group\nadaptive classiﬁer that mitigates FR bias with adaptive convo-\nlution kernels and attention mechanisms on faces according to\nthe demographic attributes.\nDifferent with previous fair FR methods that either transfer\nknowledge among different racial groups or learn the disentan-\ngled face representation, we aim to resolve this challenge from\na signal denoising perspective. Given an input face image, we\naim to distill the identity-related face representation by sup-\npressing the noisy/identity-unrelated representation induced by\nrace. To this end, we attempt to decompose the input face\nimage into two parts: 1) identity-related representation, and\n2) noisy/identity-unrelated representation induced by race. We\nabstract the process of face representation as a signal denoising\nproblem based on the signal subspace theory, and derive a\ngeneralized functional expression model for face decoupling.\nTo learn the face expression model, we further concretize the\nunsolved function with the transformer. Speciﬁcally, the dual\ncross-transformers are designed to bridge two streams of the\nencoding process, one is an identity-related distillation and the\nother is the identity-unrelated suppression induced by race. In\norder to better reﬁne face representation, we take a progressive\nface decoupling way to learn identity/race-speciﬁc transfor-\nmation, so that identity-unrelated representation introduced\nby race could be disentangled gradually. We ﬁnally frame\nthe proposed method into the progressive cross transformer\n(PCT) framework for fair face recognition. Concretely, PCT\nconsists of several cross transformer (CT) modules, each CT\nmodule learns the ethnicity-speciﬁc transformations to distil\nthe bias caused by their racial groups. The CT modules\ncan be integrated into the deep FR networks in a top-down\nmanner so that the face representation is transformed and\nde-biased gradually. By merely disentangling the identity-\nunrelated component induced by race, our proposed PCT is\ncapable of mitigating the racial bias while enhancing the\ndiscriminative ability of the learned face representation.\nOur contributions can be summarized as follows:\n• To suppress racial bias in face representation, we analyze\nface decoupling from a signal denoising perspective and\nabstract the distillation process of face representation as a\ngeneralized functional expression model, which falls into\nthe transformer framework.\n• We propose a progressive cross transformer method for\nfair face recognition, which extends the standard trans-\nformer as the cross dual streams of transformers. The\nmethod mitigates the bias components from the identity\nrepresentations by cross transforming the features of\nidentity and race progressively.\n• Extensive experiments on two popular race aware train-\ning datasets demonstrate the feasibility of our proposed\nPCT, and meantime report the state-of-the-art recognition\naccuracy with lower bias on different racial groups.\nII. R ELATED WORK\nA. Fair face recognition\nA number of studies have revealed that face recognition\nalgorithms perform unequally on different demographic at-\ntributes [6]–[11]. The 2002 NIST Face Recognition Vendor\nTest (FRVT) [26] is believed to be the ﬁrst study that showed\nthat non-deep FR algorithms show unequal FR accuracies\namong different races. The recent NIST Face Recognition\nVendor Test in 2019 [8] reveals that all participating FR\nalgorithms exhibit unequal performances on demographic at-\ntributes, e.g., gender, race, and age groups. Since then, some\nrepresentative FR datasets have been released to study the\nFR bias. Merler et al. [12] released the Diversity in Faces\n(DiF) dataset that contains annotations of 1 million human\nfacial images to advance the study of fairness in FR. The\nannotations in DiF dataset were generated using ten facial\ncoding schemes that provide human-interpretable quantitative\nmeasures of intrinsic facial features. Recently, Wang et al. [19]\ncontributed a popular Racial Faces in-the-Wild (RFW) dataset,\non which they validated the racial bias of various state-of-\nthe-art FR methods, and proposed the unsupervised domain\nadaptation approach to alleviate the racial bias. RFW dataset\nhas been served as a testbed to fairly measure the bias of\nvarious FR algorithms. Afterward, Wang et al. [9] released\nthe popular BUPT-Balancedface dataset where face images\nare balanced in various races, and BUPT-Globalface dataset\nthat reveals the real distribution of the world’s population.\nSimilarly, Robinson et al. [21] proposed the BFW benchmark\ndataset for evaluating the bias of current FR recognition\nsystems. They reduced the performance gaps among differ-\nent subgroups and showed a notable boost in overall FR\nperformance by learning subgroup-speciﬁc thresholds. In this\npaper, we adopt the BUPT-Balancedface and BUPT-Globalface\ndatasets for training and evaluate our proposed algorithm on\nRFW and BFW datasets.\nRecently, a lot of studies aim to learn fair face representation\nindependent of the balanced training datasets [9], [20], [21],\n[25], [27]. Inspired by the observations that the FR error\nrates on non-Caucasians are usually much higher than that\nof Caucasians, Wang et al. [9] proposed to manually select\nmargin for Caucasians and learn the optimal margins of the\nnon-Caucasians by Q-learning. Gong et al. [20] introduced\na debiasing adversarial network with four speciﬁc classiﬁers,\nin which one for identity classiﬁcation and the other three\nfor demographic attributes estimation. The disentangled face\nrepresentation in [20] shows more equal FR accuracies among\nvarious demographic groups and degraded FR performance at\nthe same time. This is because some demographic attributes\nsuch as race and gender are critical components that constitute\nface patterns and are beneﬁcial for FR [20], [23]. Gong et al.\n[25] proposed a group adaptive classiﬁer (GAC) that mitigates\nthe FR bias by using adaptive convolution kernels and attention\nmechanisms on faces based on their demographic attributes.\nThe adaptive convolution kernels and attention mechanisms\nin GAC help activate different facial regions for face identi-\nﬁcation and lead to more discriminative features according to\ntheir demographics.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 3\nDemographic Attribute Recognition\nFace Recognition\nCT\nIdentity Annotation\nStage 1\nStage 2\nRace Annotation\nAttention Maps in \nthe CT modules\nLow Bias High Bias\nCT\n CT\nCT\nStage 3 Stage 4\nConvolutional operation \n Face/Race representation\nFig. 1. Main idea of the proposed progressive cross transformer (PCT). PCT takes a face image as input and obtains the stage-wise identity ˜X(t)\nid and race\n˜X(t)\nra representation at t-th stage. PCT then suppresses the racial bias from the input identity representation ˜X(t)\nid via the CT module and obtains the de-biased\nrepresentation X(t)\nid . The learned attention maps in the stage-wise CT modules have been shown in the top rectangle, where the deep red means the biased\nand race-related facial regions. The attention maps mainly cover the facial texture, facial features, facial contour, which are race-related facial regions. More\nvisual examples and explanations can be seen in Section V-C.\nB. Transformer\nTransformer was proposed in [28] for sequence to sequence\ntasks such as language translation, to replace traditional re-\ncurrent models. Transformer architecture has become the de-\nfacto standard for natural language processing tasks. The main\nidea of the original transformer is to compute self-attention\nby comparing a feature to all other features in the sequence.\nIn detail, features are ﬁrst encoded to obtain query ( Q) and\nmemory (including key ( K) and value ( V)) embedding by\nlinear projections. The product of the query Q with keys\nK is used as the attention weights for value V. A position\nembedding is also exploited and added to these representations\nto incorporate the positional information which is lost in such\na non-convolutional paradigm. Transformers are especially\ngood at modeling long-range dependencies between elements\nof a sequence. Dosovitisky et al. [29] proposed the Vision\nTransformer (ViT) and showed the pure-transformer networks\ncan achieve state-of-the-art results for image classiﬁcation.\nSince then, there have been many attempts to adapt trans-\nformers towards vision tasks including object detection [30],\nsegmentation [31], [32], image classiﬁcation [33], multiple\nobject tracking [34], [35], video processing [36], [37].\nFor face-related tasks, Zhong et al. [38] investigated and\nveriﬁed the feasibility of applying transformer models for face\nrecognition. Jacob et al. [39] proposed a transformer encoder\narchitecture to capture the relationships between different\nfacial action units for the wide range of facial expressions.\nThe methods in [40], [41] translated the facial images into\nsequences of visual words and perform facial expression\nrecognition from a global perspective. Transformer has also\nbeen exploited to tackle the face clustering problem [42]\nthanks to its effective self-attention mechanism.\nIn this paper, we develop the progressive cross transformer\n(PCT) for fair face recognition. The representations from the\nfaces and demographics are cross transformed adaptively to\nlearn and suppress the bias induced by race. Unlike previous\ntransformer-based methods that learn a set of visual tokens\nand model the long-range dependencies via a self-attention\nmechanism, our proposed PCT implements the cross-attention\nmechanism globally across the face and ethnicity representa-\ntions in a top-down manner. PCT enables the model to mitigate\nthe ethnicity-biased information progressively and recognize\nthe input faces equally. We will formulate the transformation\nbetween the face and ethnicity representations for feature de-\nbiasing from a signal subspace decomposition perspective in\nthe next section.\nIII. M OTIVATION AND FORMULATION\nAn observed face signal ˜x may consist of the pure identity\nsignal xid, noise signal ϵ¯id and measuring error e. We assume\nthat the identity signal xid comes from the system H, i.e.,\nxid = fid(H,θH), where θH is the estimated parameter, the\nnoise signal ϵ¯id is generated by the other non-id system S,\ni.e., ϵ¯id = f¯id(S,θS) with the system parameter θS, and the\nmeasuring error e is assumed to be additive white Gaussian\nnoise. Formally, we can express the input face signal ˜x as\n˜x = xid + ϵ¯id + e = fid(H,θH) +f¯id(S,θS) +e. (1)\nIn the case of the plain linear system, the basic vectors (w.r.t\ncolumns) of H (S) are often assumed to be linearly indepen-\ndent, and accordingly the two corresponding spaces, Range(H)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 4\nand Range(S), are disjoint and often non-orthogonal. Thus, we\ncan rewrite Eqn. (1) as\n˜x = HθH + SθS + e. (2)\nSuppose the additive white Gaussian noise e and the structure\nnoise ϵ¯id are orthogonal, then we have ⟨SθS,e⟩= 0, and fur-\nther S⊺e = 0if θS ̸= 0. Likewise, if the desired signal xid is\northogonal to e, then we have H⊺e = 0. We premultiply with\nthe orthogonal projection matrix P⊥\nH to the space Range (H)\nin Eqn. (2), and can derive as P⊥\nH˜x = P⊥\nHSθS + e by using\nP⊥\nHH = 0 and P⊥\nHe = e. Again, after premultiplication\nwith the matrix S⊺, we can reach θS = (S⊺P⊥\nHS)−1S⊺P⊥\nH˜x.\nTherefore, we can express the noise signal as ϵ¯id = SθS =\nS(S⊺P⊥\nHS)−1S⊺P⊥\nH˜x = ES|H˜x, where ES|H is the oblique\nprojector from Range(H) to Range(S). In practice, the systems\nH and S are often unknown and also difﬁcult to be estimated\nexactly. For fair face recognition we focus in this work, the\nonly input signal ˜x and some corresponding annotations (e.g.,\nid and race information) can be used for training. Thus, it\nis intractable to decompose the input signal ˜x into the latent\nidentity signal xid and the non-id noise ϵ¯id. Moreover, the\ncalculation has a high computation burden due to the inverse\noperation of matrix. In fact, the high-dimension signals usually\nlie in the non-linear space.\nNow we rethink the estimation process of the noise signal\nϵ¯id, and abstract it as a more generalized model with learnable\nparameters. Formally, we deﬁne ϵ¯id\n.= C(H,S) ·PH(˜x) (=\n[S(S⊺P⊥\nHS)−1S⊺][P⊥\nH˜x] in the linear system case), where PH\ndenotes the projection function of ˜x to the space H and C\ndenotes the transformation function from the space H to the\nspace S. Due to the unknown spaces H and S, we take the\nparameterizing strategy, and deﬁne the estimation process of\nnoise signal ϵ¯id as\nϵ¯id\n.= CH(H(˜x),S(˜x)) ·PS(˜x), (3)\nwhere H,S are estimated from the input signal ˜x, and CS\nmeans the transformed target space is S. Accordingly, we can\ndeﬁne the identity signal xid as\nxid\n.= CS(H(˜x),S(˜x)) ·PH(˜x). (4)\nSo far, we have abstracted and modeled the basic decomposi-\ntion process for the input signal ˜x, as formulated in Eqn. (3)\nand (4). However, the estimation processes of xid and ϵ¯id\nshould be mutually dependent based on Eqn. (1). If ignoring\nthe measuring error e, we can derive them as\nxid ≃˜x −ϵ¯id, ϵ¯id ≃˜x −xid, (5)\nwhich are more suit for the progressive stage learning through\nthe interaction between xid and ϵ¯id as used in Section IV.\nInspired by the success of transformer [28], we may con-\ncretize the transformation functions CS,CH and the projection\nfunctions PS,PH. Given the multi-channel feature ˜X ∈Rn×d\nwith spatial position n= h×w and feature dimension d, we\ndeﬁne as follows,\nCS\n.= Softmax\n(\nGqry\nid ( ˜X) ×[Gkey\n¯id ( ˜X)]⊺\n√\nC\n)\n, (6)\nPH\n.= Gval\nid ( ˜X), (7)\nwhere Gqry\nid means an identity transformation function w.r.t the\nquery of transformer, Gkey\n¯id denotes a non-id function w.r.t the\nkey of transformer, Gval\nid is a identity projection function w.r.t\nthe value of transformer, and C is the normalization factor.\nSimilarly, we can have\nCH\n.= Softmax\n(\nGqry\n¯id ( ˜X) ×[Gkey\nid ( ˜X)]⊺\n√\nC\n)\n, (8)\nPS\n.= Gval\n¯id ( ˜X). (9)\nHereby, we derive the learnable parameterized model with\nthe transformer mechanism, where face identity signal can be\ndecomposed from input face signal.\nIV. P ROGRESSIVE CROSS TRANSFORMER\nA. The Network Framework\nFig. 1 shows the network framework of the proposed PCT.\nTo obtain the face and racial representation, the network\nstructure of PCT is deployed to consist of two branches: 1) the\ntop branch is used to learn the representation related to face\nidentity, and 2) the bottom branch aims to capture identity-\nunrelated feature induced by race factor. For a convenient\ndescription, below we abuse the term “race” as the identity-\nunrelated component because the identity-unrelated branch is\nconstrained by the race factor. To extract robust feature, we\ntake the typical face recognition network ResNet-34 [43] as\nthe infrastructure. A ResNet typically consists of several stages\nand each stage consists of multiple bottleneck blocks with\nresidual connections [44]. By virtue of the multiple stages\nwithin the FR and RC networks, we are capable of extracting\nthe top-down facial/racial representations based on the output\nof each stage within the FR and RC network, which facilitates\nprogressive learning naturally.\nGiven an input face image, suppose we obtain the decoupled\nface representation ˜X(t)\nid ∈ Rn×d,˜X(t)\n¯id ∈ Rn×d from the\nidentity-related/unrelated branches at the t-th stage respec-\ntively. As the identity-unrelated feature is mainly induced by\nrace factor, so we denote ˜X(t)\n¯id with the race feature ˜X(t)\nra for\nsimpliﬁcation. At the ﬁrst stage, we may use the convolu-\ntional feature as the input to produce the preliminary face\nrepresentations ˜X(1)\nid ,˜X(1)\nra with different convolution kernels.\nThe extracted features ˜X(t)\nid ,˜X(t)\nra are required for a further\nseparation in the progressive distillation of face representation.\nTo disentangle those identity-unrelated information, we design\nthe cross transformer (CT) module to infer less biased facial\nrepresentation by taking ˜X(t)\nid , ˜X(t)\nra as input, formally,\nX(t)\nid ,X(t)\nra ←CT( ˜X(t)\nid ,˜X(t)\nra ), (10)\n˜X(t+1)\nid ,˜X(t+1)\nra ←f(X(t)\nid ),f(X(t)\nra ), (11)\nwhere the cross transformer follows the basic idea as deﬁned\nin Eqn. (3)-(9) in Section III, and f is a function to match\nthe input of the next stage. More details of the CT module\nare presented in Section IV-B. With the evolution of face\ndecoupling, more identity information ˜Xid will be aggregated\ninto the identity-related branch while those identity-unrelated\nracial information is suppressed by the affect of the other\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 5\nbranch of network. At the last stage, we denote the output\ndecoupled features with\nXid,Xra ←X(T)\nid ,X(T)\nra , (12)\nwhere T is the number of progressive stages. For training, the\ndecoupled representations Xid,Xra are fed into the constraint\nloss functions w.r.t face identity and face race labels. When\ntesting, we only need perform the feeding-forward pass en-\ncoding to produce the representation related to face identity\nfor fair face recognition.\nB. Cross Transformer Module\nFig. 2. Illustration of proposed cross transformer (CT) module. The CT\nmodule takes ˜X(t)\nid and ˜X(t)\nra as the input and output the decoupled resenta-\ntions X(t)\nid and X(t)\nra . ⨂ means matrix multiplication. ⊖means element-wise\nsubtraction.\nFig. 2 shows the main idea of the proposed CT module. To\nsuppress identity-unrelated component induced by race, this\nmodule at t-th stage takes ˜X(t)\nid and ˜X(t)\nra as the input and\noutputs the purer representations X(t)\nid and X(t)\nra as identity\ninformation and noises based on the transformer mechanism.\nFormally, we concretize the deﬁnition in Eqns. (6)-\n(9). Following the basic idea of transformer, we deﬁne\nthree learnable matrices related to identity transformation:\nWkey\nid ,Wqry\nid ,Wval\nid ∈ Rd×d′\n. Likewise, we also introduce\nother three learnable weights on the branch of race, i.e.,\nWkey\nra ,Wqry\nra ,Wval\nra ∈Rd×d′\n. The function G (G as value) in\nEqns. (6)-(9) is rewritten as\nGkey\nid = ˜X(t)\nid Wkey\nid + bkey\nid , Gkey\nra = ˜X(t)\nra Wkey\nra + bkey\nra , (13)\nGqry\nid = ˜X(t)\nid Wqry\nid + bqry\nid , Gqry\nra = ˜X(t)\nra Wqry\nra + bqry\nra , (14)\nGval\nid = ˜X(t)\nid Wval\nid + bval\nid , Gval\nra = ˜X(t)\nra Wval\nra + bval\nra , (15)\nwhere b is the bias term as used in the network layers, and\nwe omit the subscript (t) under the clear context. Taking the\nprojected features as inputs, we illustrate how to peel those\nunrelated components from the current branch. Below we take\nface identity distillation (the top block in Fig. 2) as an example\nto introduce cross transformer.\nTo purify face identity representation, we expect to remove\nsome identity-unrelated noises (here induced by race). Taking\nthe current race components Gkey\nra as the reference, we project\nthe face representation Gqry\nid to the race space by computing\nthe attention weight as follows\nSi,j =\nexp( 1√\nd′ [Gqry\nid ]i[Gkey\nra ]⊺\nj)\n∑d′\nj=1 exp( 1√\nd′ [Gqry\nid ]i[Gkey\nra ]⊺\nj)\n, (16)\nwhere Si,j measures the impact of i-th position in Gqry\nid on\nj-th position in Gkey\nra . Intuitively, Gqry\nid and Gkey\nra are used to\nbuild the relationships between face and race features, thus\nthe resultant S reveals the race-related/biased facial regions,\nwhich will be visualized in Section V-C to investigate the race-\nrelated/-speciﬁc facial features qualitatively. With the cross\nattention, spatial dependencies between any two positions of\nthe face and race features can be captured. For the feature\nat a certain position, it is updated via aggregating features at\nall positions with weighted summation, where the attention\nweights are adaptively decided by the feature similarities\nbetween the corresponding two positions.\nTo further estimate the identity-unrelated component in-\nduced by race as formulated in Eqn. (3), we transform the\nSi,j with an identity projection function as follows\nϵ(t)\nra = S ×Gval\nid ( ˜X(t)\nid ), (17)\nwhere Gval\nid denotes the identity projection, and × denotes\nthe matrix multiplication. The output ϵt\nra corresponds to the\nidentity-unrelated components induced by race, so we can\nobtain the less biased face representation at stage t according\nto Eqn. (5), formally,\nX(t)\nid = ˜X(t)\nid −ϵ(t)\nra . (18)\nTo make cross transformer position-aware, we addition-\nally resort to the relative distance encodings [45]–[47]. The\nposition embedding takes into account the relative distances\nbetween features at different facial locations and effectively\nassociates information across the face and race features with\npositional awareness. Here we adopt the two dimensional\nrelative position self-attention as used in the literature [47].\nIn the advanced self-attention method [28], [29], the input\nrepresentation can be projected into different spaces via differ-\nent learnable parameters, which fall into the multi-head self-\nattention (MSA) paradigm. By projecting the input represen-\ntation into several sub-spaces, the model is capable of paying\nattention to different positions in the input representation.\nInspired by this observation, we may extend the above cross\ntransformer to the multi-head case. Let H denotes the number\nof heads, the feature dimension d′ of each sub-space is set\nas: d′ = d\nH. The output is the linear transformation of\nconcatenation of all the attention outputs:\nϵ(t)\nra = Concat([ϵ(t)\nra ]1,[ϵ(t)\nra ]2,..., [ϵ(t)\nra ]H), (19)\nwhere Concat denotes the concatenation operation in the\nfeature dimension, [ϵ(t)\nra ]i means the output of the i-th head\nin MSA.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 6\nConversely, we can remove some face identity information\nfrom the branch of noises/race, and obtain more identity-\nunrelated representation of noises (induced by race) in the\nbottom block as shown in Fig. 2. With the cross attention\nmechanism in the proposed CT module, the identity-unrelated\ncomponent induced by race could be washed out to some\nextent while the identity-related features would be reserved.\nUnlike other fair face recognition approaches, PCT is deployed\nto merely distill the identity-related component from the\nidentity-unrelated part induced by race. Thus, PCT is capable\nof suppressing the bias induced by race while enhancing the\ndiscriminability of face representations simultaneously.\nC. Progressive Learning\nIn order to purify face representation in a top-down manner,\nthe distilled face representation X(t)\nid at t-th stage is fed into\nthe input to the t+ 1stage for the further reﬁnement. To fuse\nthe multi-layer convolutional network, we employ the standard\nconvolutional operation to encode the input X(t)\nid to high-level\nfeature ˜X(t+1)\nid , where the size of feature map could be reduced\ninto one half when pooling is used. Formally, together with\nthe noise representation, we have the formulas\n˜X(t+1)\nid = conv(X(t)\nid ), ˜X(t+1)\nra = conv(X(t)\nra ), (20)\nwhere conv means the standard convolutional operation within\nthe t-th stage in the FR network, t = {1,2,··· ,T −1}\ndenotes the index of the T stages. With the evolution of\nface decoupling, more face identity information would be\naggregated into the identity-related network (top branch in\nFig. 1) while those identity-unrelated information of race\nfactor would be suppressed progressively. At the last stage,\nwe obtain the output decoupled features for face and race\nclassiﬁcation, as illustrated in the right part in Fig. 1.\nD. Loss Function\nFinally, we employ both the face recognition loss and race\nclassiﬁcation loss to learn the face and race representation in\nan end-to-end manner. For face recognition, we choose to use\nArcFace [48] or CosFace [4] for face recognition. The training\nobjective of face identity can be formulated as:\nLface = −1\nN\nN∑\ni=1\nlog e(s(cos(θyi +m)))\ne(s(cos(θyi +m))) + ∑n\nj=1,j̸=yi e(scos θj) ,\n(21)\nθyi = arccos ⟨wyi ,xi⟩\n∥wyi ∥·∥xi∥, (22)\nwhere xi denotes the deep feature of the i-th training sample\nbelonging to the yi classes, wj is the j-th column of the weight\nin the last fully-connected classiﬁcation layer, N is the batch\nsize, θj denotes the angle between weight wj and feature xi,\ns is a scale factor (64 as default), m is an additive angular\nmargin penalty between wj and xi to enhance the intra-\nclass compactness and inter-class discrepancy. We refer the\ninterested reader to [48] for more details and the explanation\nfor the ArcFace loss. Besides, we may also use CosFace\nfor face recognition whose details can be found in [4]. The\ncomparison of loss functions between ArcFace and CosFace is\nprovided in the experiment part. For race classiﬁcation (RC),\nwe adopt the cross entropy loss Lrace. Finally, we integrate the\ntwo object functions for training,\nLtotal = Lface + αLrace, (23)\nwhere the hyper-parameter α controls the importance of the\nRC term. We present the thorough experimental analysis of\nthe proposed PCT in the next section.\nV. E XPERIMENTS\nIn this section, we present the experimental evaluations of\nthe proposed PCT. Before showing the results, we will describe\nthe experimental settings, including the training and evaluation\ndatasets, the implementation details and the evaluation proto-\ncol. Then, we compare our method with the state-of-the-art\nface recognition methods that aim to mitigate the bias. Finally,\nwe provide the ablation analysis to verify the reasonability of\nthe proposed PCT.\nA. Experimental setup\nDatasets: We employ the popular BUPT-Balancedface [9]\nand BUPT-Globalface [9] datasets for training. BUPTBal-\nancedface contains 1.3M images of 28,000 celebrities and is\napproximately race-balanced with 7,000 identities per race.\nBUPT-Globalface contains 2M images of 38,000 celebrities,\nand its racial distribution is approximately the same as the real\ndistribution of the world’s population. The identities in the two\ndatasets are grouped into 4 categories, i.e. Caucasian, Indian,\nAsian and African, according to their races.\nFor fairness testing, we employ the RFW [19] and BFW\n[21] datasets. RFW dataset consists of four race subsets,\nnamely Caucasian, Asian, Indian, and African. Each subset\ncontains approximately 10K images of 3K individuals for face\nveriﬁcation. Compared with RFW, the BFW dataset contains\nbalanced face images with more attributes, including identity,\ngender, and race. The identities in the BFW dataset are\ncategorized into eight demographic groups according to two\ngenders and four ethnic groups (i.e., Black, White, Asian, and\nIndia). Each demographic group in the BFW dataset consists\nof 200 subjects with 2,500 face images.\nEvaluation Protocol: For the RFW dataset, we follow the\nRFW face veriﬁcation protocol with 6K pairs for each race.\nWe utilize the average face veriﬁcation accuracy (A VE) of four\nraces as the metric to evaluate the total performance of the FR\nmethods. To measure the fairness of the proposed PCT and\nthe compared FR methods, we exploit the standard deviation\n(STD) among different races. STD reﬂects the amount of\ndiscrepancy among various racial groups.\nFor the BFW dataset, we use FPR for the eight demo-\ngraphic groups (two genders, four races) to measure the\noverall performance of the fair FR methods. To measure\nthe fairness, we exploit STD among the eight groups. We\nutilize the ofﬁcially released face pairs and calculate the FPR\nfor each group at a global cosine similarity threshold which\nis determined by the speciﬁed global FPR. We obtain the\nSTD of the eight demographic groups on BFW dataset as:\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 7\nTABLE I\nFACE VERIFICATION PERFORMANCE (%) OF PCT, STATE-OF-THE -ART FAIR FACE RECOGNITION METHODS ON RFW ( TRAINING DATASET :\nBUPT-B ALANCEDFACE ). BOLD DENOTES THE BEST .\nMethods (%) African Asian Caucasian Indian A VE (↑) STD (↓)\nResNet-34 DebFace [20] 93.67 94.33 95.95 94.78 94.68 0.83\nPFE [49] 95.17 94.27 96.38 94.60 95.11 0.93\nResNet-34, ArcFace\nBaseline [48] 93.98 93.72 96.18 94.67 94.64 1.11\nMTL 94.82 94.47 96.60 95.23 95.28 0.93\nGAC [25] 94.12 94.10 96.02 94.22 94.62 0.81\nRL-RBN [9] 95.00 94.82 96.27 94.68 95.19 0.93\nPCT (Ours) 95.72 94.98 96.22 95.33 95.56 0.53\nResNet-34, CosFace\nBaseline [4] 92.93 92.98 95.12 93.93 93.74 1.03\nMTL 95.20 94.58 96.82 95.60 95.55 0.94\nRL-RBN [9] 95.27 94.52 95.47 95.15 95.10 0.41\nPCT (Ours) 96.02 94.87 96.72 96.02 95.91 0.77\nResNet-50, ArcFace MTL 96.05 95.25 97.20 96.05 96.14 0.80\nPCT (Ours) 96.22 95.73 97.00 96.38 96.33 0.52\nResNet-50, CosFace\nMTL 95.82 94.93 96.73 95.78 95.82 0.74\nGAC [25] 94.77 94.87 96.20 94.98 95.21 0.58\nPCT (Ours) 95.83 95.48 96.90 96.12 96.08 0.60\nTABLE II\nFACE VERIFICATION PERFORMANCE (%) OF PCT, STATE-OF-THE -ART FAIR FACE RECOGNITION METHODS ON RFW ( TRAINING DATASET :\nBUPT-G LOBALFACE ). BOLD DENOTES THE BEST .\nMethods (%) African Asian Caucasian Indian A VE (↑) STD (↓)\nResNet-34, ArcFace\nBaseline [48] 93.87 94.55 97.37 95.86 95.37 1.53\nMTL 95.13 95.92 97.92 96.05 96.26 1.18\nRL-RBN [9] 94.87 95.57 97.08 95.63 95.79 0.93\nPCT(Ours) 95.87 95.45 97.68 96.15 96.29 0.97\nResNet-34, CosFace\nBaseline [4] 92.17 93.50 96.63 94.68 94.25 1.90\nMTL 95.07 95.53 97.87 96.52 96.25 1.24\nRL-RBN [9] 94.27 94.58 96.03 95.15 95.01 0.77\nPCT(Ours) 96.43 95.88 97.97 96.50 96.70 0.89\nResNet-50, ArcFace MTL 96.30 95.97 98.03 96.53 96.71 0.91\nPCT(Ours) 96.58 96.05 98.15 96.93 96.93 0.89\nResNet-50, CosFace MTL 96.37 96.43 98.17 96.95 96.98 0.83\nPCT(Ours) 96.62 96.88 98.15 96.97 97.16 0.68\nδ= 1\nK\n√∑K\nk=1(Fk −µ)2. Fk means the group-speciﬁc FPRs.\nK means the number of groups, µ is the mean of the group-\nspeciﬁc FPRs. To normalize the scale for the group-speciﬁc\nFPRs, we divide δ by the speciﬁed global FPR.\nimplementation details: Following [9], [48], we cropped\nthe face images to 112 ×112 with ﬁve facial landmarks\ndetected by MTCNN [50]. We normalize the face images by\nsubtracting 127.5 and divided the pixel intensity by 128, then\nfeeding the normalized face images to the face recognition\nand race classiﬁcation networks. For the race classiﬁcation\nbackbone, we adopt the ResNet18 network as that in [25]. For\nthe face recognition backbone, we exploit the ResNet-34 and\nResNet-50 networks as they are popular network structures for\nface recognition. The ResNet-based networks consist of four\nstages, we embedded a CT module at each stage as shown in\nFig. 1.\nWe adopt a batch-based stochastic gradient descent method\nto optimize the model. The base learning rates for FR and\nRC were set as 0.1 and 0.01. We scaled the learning rates by\n0.1 at 16,24,28 epochs. The batch size was set as 256 for\nboth the two classiﬁcation tasks and the training process was\nﬁnished at 32 epochs. We set the momentum as 0.9 and the\nweight decay as 0.0005. The optimal setting for the loss weight\nbetween the face and race classiﬁcation tasks was set as 1 : 1\nby grid search. The training of the models was completed on\n8 NVIDIA Tesla V100 GPU with Pytorch framework [51].\nFor ResNet-34, it took about 10 hours to ﬁnish optimizing\nthe PCT model on the BUPTBalancedface dataset 18 hours\non the BUPT-Globalface dataset. The number of heads H\nin each CT module was set as 2 in our experiments. The\nselection of hyperparameter H is discussed in Section V-C.\nFor the ArcFace and CosFace loss functions, we follow the\ncommon setting as that in [9], [48] and set s = 64, then set\nm= 0.35 for ArcFace, m= 0.35 for CosFace. The proposed\nPCT models were trained on BUPT-Balancedface or BUPT-\nGlobalface datasets with ground truth race and identity labels.\nB. Comparisons with state-of-the-art methods\nWe compare PCT with other state-of-the-art fair face recog-\nnition methods, including the DebFace [20], Probabilistic Face\nEmbeddings (PFE) [49], GAC [25], RL-RBN [9]. Among\nthem, PFE [49] represents each face image as a Gaussian\ndistribution in the latent space. The mean of the distribution\nestimates the most likely feature values and the variance\nshows the uncertainty in the feature values. PFE is capable\nof handling the feature ambiguity dilemma for unconstrained\nface recognition. Debface [20] consists of a novel de-biasing\nadversarial network that learns to extract disentangled feature\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 8\nFig. 3. ROC curves for BFW (training dataset: BUPT-Balancedface). PCT outperforms or is comparable with other methods in the sub-ﬁgures (a), (b), (d),\n(e), (f), (h).\nFig. 4. ROC curves for BFW (training dataset: BUPT-Globalface). PCT obtains slight improvements in all the sub-ﬁgures (a-h).\nrepresentations for both unbiased face recognition and demo-\ngraphics estimation. GAC [25] mitigates the FR bias by using\nadaptive convolution kernels and attention mechanisms on\nfaces based on their demographic attributes. RL-RBN [9] con-\ntains a reinforcement learning-based race-balanced network\nthat is trained to ﬁnd the optimal margins for non-Caucasians.\nWe additionally compare PCT with a regular FR baseline\nwhich has the same architecture as the face recognition back-\nbone in PCT using the ArcFace [48] or CosFace [4] loss\nfunction. Besides, we trained the face recognition and race\nclassiﬁcation tasks in a multi-task manner, where the two\ntasks share a classiﬁcation backbone and have their respective\nclassiﬁcation heads. We term the multi-task learning method\nas MTL, as illustrated in Tab. I, II, III, IV.\n1) Experimental results on RFW dataset: Tab. I illus-\ntrates the face veriﬁcation performance with the BUPT-\nBalancedface as the training and RFW as the evaluation\ndataset. The ﬁrst column shows the loss function and network\nstructure conﬁgurations of different FR approaches.\nAs illustrated Tab. I, our method performs better than the\ncommon baselines with the ResNet-based network structures,\ni.e., ArcFace [48] , CosFace [4]. With ResNet-34, PCT outper-\nforms ArcFace with 0.92% improvements in the average face\nveriﬁcation accuracy. When measuring the bias of the face\nrepresentation, PCT signiﬁcantly mitigates the racial bias and\nhalves the standard deviation. Although the methods [4], [48]\nare veriﬁed to obtain good performance for face recognition,\nthey do not consider the demographic bias explicitly, thus\nthe biases in data are inevitably transmitted to the FR mod-\nels through network learning. With “ResNet-34 + ArcFace”\nconﬁguration, our proposed PCT obtains the best in both the\nbias and the average face veriﬁcation performance. Compared\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 9\nFig. 5. ROC curves for RFW dataset (training dataset: BUPT-Balancedface). PCT show its superiority in the sub-ﬁgures (a-c) and is comparable with the\nsecond best in the sub-ﬁgure (d).\nFig. 6. ROC curves for RFW (training dataset: BUPT-Globalface). PCT obtains slight improvements in all the sub-ﬁgures (a-d).\nwith DebFace [20] that disentangles demographic attributes\nfrom the face representation, PCT shows higher face veriﬁ-\ncation accuracies in all the four cohorts and lower standard\ndeviation simultaneously. It is because race represents subject-\nspeciﬁc intrinsic characteristics and is a critical component\nthat constitutes face patterns. Removing the race component\nfrom the face representation will reduce its discrimination [10],\n[20], [23]. Compared with GAC [25], PFE [49], RL-RBN [9],\nour proposed PCT achieves higher and fairer face veriﬁcation\nperformance and by mitigating the bias by cross transforming\nthe face and race representation so as to suppress the identity-\nunrelated component induced by race gradually. PCT also\noutperforms MTL because the MTL learns the face and race\nclassiﬁcation without extra constraint, thus the bias induced by\nrace naturally remains in the learned face representation. When\nit comes to “ResNet-34 + CosFace” conﬁguration, the STD of\nour proposed PCT lags behind RL-RBN [9]. However, PCT\noutperforms RL-RBN in both the individual racial cohort and\nthe average performance. It indicates the reduction of racial\nbias in our proposed PCT is obtained along with the accuracy\nimprovements of all the four races.\nWe also trained the ResNet-50 models on BUPT-\nBalanceface dataset with our proposed PCT, as shown in the\nbottom part in Tab. I. It shows that both the individual accuracy\nand the average accuracy of PCT is much better than that\nof GAC, while our standard deviation (0.60) is comparable\nwith that of GAC (0.58). It reveals the proposed PCT is\ncapable of equally representing the faces in different race\ncohorts while enhancing the discrimination of the learned face\nrepresentation. Compared with the conventional multi-task\nlearning method (MTL), PCT shows consistent improvements\nin the face recognition accuracy and the equality of the learned\nface representation. It indicates the feasibility of the proposed\nCT modules in PCT that mitigates the bias induced by race\ngradually.\nTab. II illustrates the face veriﬁcation performance with the\nBUPT-Globalface as the training and RFW as the evaluation\ndataset. Similar to the aforementioned comparisons in Tab. I,\nwe group the methods according to their loss functions and\nnetwork structures in the ﬁrst column. Compared with other\nmethods, our proposed PCT shows consistent high face ver-\niﬁcation accuracy and relatively low racial bias on various\ncombinations of loss functions and network structures. For\nexample, with the “ArcFace + ResNet-34” congﬁguration,\nPCT obtains 0.5% improvments in the average accuracy and\ncomparable standard deviation compared with RL-RBN [9].\nWhen it comes to “CosFace + ResNet-34” conﬁguration,\nthe performance gap between PCT and RL-RBN [9] reaches\n1.69%. The promotions of PCT verify the beneﬁts of sup-\npressing the identity-unrelated component induced by race in\nthe learned face representation. Besides, the improvements\nof PCT over other compared methods are in line with the\nobservation in [9] that colored faces are more susceptible to\nnoise and image quality than Caucasians. Our proposed PCT\nmitigates the bias/noise induced by race with the proposed CT\nmodule in each stage in the classiﬁcation network gradually,\nthus achieves balanced generalization ability of the learned\nface representation. The above results in Tab. I, II show that\nour method can achieve competitive performances on both race\nbalanced and unbalanced datasets, with regard to the mean and\nstandard deviation of accuracy.\nFrom the experimental results in Tab. I, II, we can con-\nclude that our proposed PCT obtains the highest accuracy on\nCaucasian, and the lowest performance on Asian, respectively.\nThis can be explained in two-fold: 1) Although the number\nof Asians is much larger than that of Indians and Africans in\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 10\nBUPT-Globalface dataset, this group still needs a larger margin\nbecause it is the most difﬁcult race to recognize even with\nbalanced training [9], [52]. 2) There are many low-resolution\nface images in the Asian group in BUPT-Balanceface dataset,\nwhile this is not the case for the images in other races. In order\nto go deep into this phenomenon, we used the RetinaFace\n[53] to detect the faces for the images in BUPT-Balancedface\ndataset, then calculated the average pixels in the facial regions\naccording to the race annotations. For the African, Asian,\nCaucasian, India cohorts, the average pixels within the facial\nregions are 49335, 26395, 48119, 38487. It is clear that the\nface images in the Asian group suffer from low resolution\ncompared with other racial groups, which can cause difﬁculty\nfor accurate FR.\nTABLE III\nBIAS DEGREE OF PROTOCOL ON BFW. L OW BIAS DEGREE MEAN FAIR\nFACE REPRESENTATION (TRAINING DATASET : BUPT-B ALANCEDFACE ).\nMethods (%) 10−5 10−4 10−3 10−2 10−1\nArcFace 0.86 0.72 0.58 0.42 0.21\nMTL 0.49 0.64 0.54 0.47 0.33\nPCT (Ours) 0.64 0.62 0.39 0.34 0.24\nCosFace 0.86 0.44 0.36 0.33 0.25\nMTL 0.75 0.70 0.66 0.51 0.37\nPCT (Ours) 0.58 0.44 0.34 0.30 0.22\nTABLE IV\nBIAS DEGREE OF PROTOCOL ON BFW. L OW BIAS DEGREE MEAN FAIR\nFACE REPRESENTATION (TRAINING DATASET : BUPT-G LOBALFACE ).\nMethods (%) 10−5 10−4 10−3 10−2 10−1\nArcFace 2.30 1.69 1.43 1.03 0.57\nMTL 1.03 0.72 0.61 0.44 0.22\nPCT (Ours) 1.03 0.76 0.49 0.34 0.15\nCosFace 0.63 0.74 0.58 0.41 0.21\nMTL 0.99 0.74 0.56 0.38 0.23\nPCT (Ours) 0.85 0.71 0.56 0.40 0.19\n2) Experimental results on BFW: The results are shown in\nTab. III, IV. The experimental results show that our PCT shows\nconsistent low bias than the compared methods at various\nFPRs. When we adopt the BUPT-Balanceface as the training\ndataset shown in Tab. III, our PCT shows improved equality\nat FAR = 0.01, 0.001, and 0.0001. When we set BUPT-\nGlobalface as the training data in Tab. IV, our PCT shows\nlow bias at FAR = 0.1, 0.01, 0.001. These comparisons clearly\nand convincingly show that PCT can signiﬁcantly improve the\nfairness of the learned face representation on both the race\nbalanced and unbalanced datasets.\nWe additionally illustrate Receiver Operating Characteristic\nCurve (ROC) curves in Fig. 3, 4 on the eight groups in BFW\ndataset, and Fig. 5, 6 on the four racial groups in RFW dataset\nwith the ResNet-34 based FR models. Compared with MTL\nand RL-RBN [9], PCT obtains slight improvements in many\nracial groups, e.g., (a), (b), (d), (e), (f) in Fig. 3. Compared\nwith ArcFace and MTL, PCT shows its superiority in all the\nsub-ﬁgures (a-h) in Fig. 4. PCT also outperforms the compared\nmethods in nearly all the sub-ﬁgures in Fig. 5, 6. MTL out-\nperforms ArcFace as race represents subject-speciﬁc intrinsic\ncharacteristics, explicitly learning face and race classiﬁcation\nwill enhance the discrimination of the facial features to some\ndegree. However, the race also induces bias into the facial\nrepresentation. To improve the robustness of features against\nrace attribute, our proposed PCT suppresses the bias induced\nby race with the adaptive stage-wise CT modules and achieves\nboth high FR accuracy and fairness simultaneously.\nTABLE V\nFACE VERIFICATION PERFORMANCE ON RFW DATASET BY ADDING NONE ,\nONE OR STAGE -WISE CT MODULES (TRAINING DATASET :\nBUPT-B ALANCEDFACE ). BOLD DENOTES THE BEST .\nMethods (%) African Asian Caucasian Indian A VE (↑) STD (↓)\nw/o CT 93.98 93.72 96.18 94.67 94.64 1.11\nStage 1 94.13 93.77 95.85 95.25 94.75 0.97\nStage 2 95.53 94.38 96.17 95.53 95.40 0.75\nStage 3 95.13 94.00 95.95 95.27 95.09 0.81\nStage 4 95.22 93.55 96.13 94.92 94.96 1.07\nPCT 95.72 94.98 96.22 95.33 95.56 0.53\nTABLE VI\nFACE VERIFICATION PERFORMANCE ON RFW DATASET BY ADDING NONE ,\nONE OR STAGE -WISE CT MODULES (TRAINING DATASET :\nBUPT-G LOBALFACE ). BOLD DENOTES THE BEST .\nMethods (%) African Asian Caucasian Indian A VE (↑) STD (↓)\nw/o CT 93.87 94.55 97.37 95.86 95.37 1.53\nstage 1 94.13 93.77 95.85 95.25 94.75 0.97\nStage 2 95.57 95.38 97.82 96.05 96.21 1.11\nStage 3 95.32 94.95 97.83 96.05 96.04 1.28\nStage 4 95.23 94.53 97.68 95.97 95.85 1.35\nPCT 95.87 95.45 97.68 96.15 96.29 0.97\nTABLE VII\nABLATION STUDY ON RFW DATASET. VERIFICATION PERFORMANCE (%)\nOF PROTOCOL ON RFW WITH STATE -OF-THE -ART METHODS . * MEANS\nTHE VALUES ARE REPORTED IN THE ORIGINAL PAPERS\n([BUPT-B ALANCEDFACE ]).\nMethods (%) African Asian Caucasian Indian A VE (↑) STD (↓)\nH = 1 95.23 93.92 96.45 94.42 95.13 1.04\nH = 2 95.72 94.98 96.22 95.33 95.56 0.53\nH = 4 94.67 93.40 96.52 93.92 94.63 1.37\nTABLE VIII\nABLATION STUDY ON RFW DATASET. VERIFICATION PERFORMANCE (%)\nOF PROTOCOL ON RFW WITH STATE -OF-THE -ART METHODS . * MEANS\nTHE VALUES ARE REPORTED IN THE ORIGINAL PAPERS\n([BUPT-G LOBALFACE ]).\nMethods (%) African Asian Caucasian Indian A VE (↑) STD (↓)\nH = 1 94.83 94.55 97.28 95.33 95.50 1.23\nH = 2 95.87 95.45 97.68 96.15 96.29 0.97\nH = 4 94.92 94.18 97.25 94.95 95.33 1.33\nC. Ablation Study\nTo investigate the effectiveness of PCT, we conduct a quan-\ntitative evaluation to verify the contribution of the proposed\nCT module in each stage in our proposed PCT. For a more\ndetailed analysis, we also visualize the learned attention maps\nto reveal the biased facial regions related to race.\nEvaluation of stage-wise CT module: We analyze the\nperformance variations of PCT by fully removing the race\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 11\nAfrican\nAsian\nCaucasian\nIndian\n(a) (b) (c) (d) (a) (b) (c) (d)Input Input\nFig. 7. Attention maps of the four stage-wise cross transformer units. PCT perceives the race-related facial regions and suppresses the racial bias gradually.\nThe sub-ﬁgures (a), (b), (c) mainly covers the facial texture, facial features (e.g., eyes in African, Indian, nose in Asian, Caucasian), facial contour, respectively.\nThis is in line with the observation in [54] that the three parts are the major determinants for different races. The deep red means the highly biased and\nrace-related facial regions. The deep blue in (d) denotes the face representation contains less racial bias. Better viewed in color and zoom in.\nclassiﬁcation branch (w/o CT in Tab.V, VI) or only adding one\nCT module at certain stage in the classiﬁcation network during\nthe training process. Tab.V, VI show the FR performance and\nbias variations.\nAs can be seen in Tab.V, VI, the baseline model without\nCT module shows the maximal standard deviation and it indi-\ncates the considerable bias in the learned face representation.\nAdding one CT module at certain stage in the classiﬁcation\nnetwork is capable of decreasing the standard deviation and\nsuppressing the bias induced by race. It is worth noting that\nadding a CT module at the early stage (e.g., stage 1 or stage\n1 in Tab.V, VI,) has more signiﬁcant effect than adding it\nin the later stage (e.g., stage 3 or stage 4 in Tab.V, VI). It\nmight be explained faces in different racial groups usually\nhave distinct facial texture distributions and morphological\ncharacteristics [54], [55], and it has been veriﬁed that CNN\ntends to capture the low-level features such as color con-\njunctions, edges, corners in the early layers [56]. Compared\nwith other conﬁgurations, our proposed stage-wise progressive\nCT module (PCT) obtains higher and fairer face veriﬁcation\nperformance. The experimental results reveal the effectiveness\nof the stage-wise CT modules in the proposed PCT.\nEvaluation of different head numbers: We analyze the\nperformance variations of our method by exploiting different\nnumber of cross-attention heads H. As illustrated in Tab. VII,\nVIII, our proposed PCT always obtains good trade-off between\nFR accuracy and fairness with H = 2. In Tab. VII, PCT\n(H = 2) halves the standard deviation compared with PCT\n(H = 1 ). In Tab. VII, PCT ( H = 2 ) nearly decreases\none third standard deviation compared with PCT ( H = 1).\nThis is in line with the observation in [28] that multi-head\nattention allows the model to jointly attend to information from\ndifferent representation subspaces at different positions. As a\ncomparison, the single attention head inhibits such beneﬁts.\nWith H = 4, PCT shows decreased FR accuracy and increased\nbias. It might be explained that H = 4incorporates too many\ntrainable parameters. Applying H = 4to each CT module is\nprone to overﬁtting. The comparisons in Tab. VII, VIII reveal\nH = 2makes good compromise between the model capability\nand the amount of trainable parameters.\nVisualization: To understand the learned fair face repre-\nsentation, we show the learned attention maps of some repre-\nsentative faces in Fig. 7. Although the individual differences\nexist in the learned attention maps, we can still observe some\nmeaningful phenomenons from the consistencies within the\nsame racial group and discrepancies between different races:\n(1) The columns (a), (b), (c) in Fig. 7 mainly covers the\nfacial texture, facial features (e.g., eyes in African, Indian,\nnose in Asian, Caucasian), and facial contour, respectively.\nThis phenomenon is in line with the observations in [54], [55]\nthat facial texture, facial features (e.g., eyes, nose, mouth) and\nfacial contour are the major determinants for different racial\ncohorts. It indicates the proposed PCT is capable of perceiving\nthe race-related facial regions and suppresses the racial bias\ngradually. (2) When it comes to the fourth stage (column (d)\nin each row in Fig. 7), the attention map tends to be overly\nsimilar with deep blue. It indicates the face representation in\nthis stage contains less biased component induced by race.\nIt can be explained in two aspects. Firstly, the racial bias\nhas been suppressed in the previous stages with the proposed\nCT modules. Secondly, the racial bias mainly resides in the\nearlies stages in the FR model. As evidenced in Tab.V, VI, the\nstandard deviation of FR accuracies show its maximum when\nmerely adding one CT module in the fourth stage among the\nstage-wise conﬁgurations.\nVI. C ONCLUSION\nWithin this paper we have presented a transformer-based\nfair face recognition approach. The proposed progressive cross\ntransformer (PCT) suppresses the identity-unrelated compo-\nnent induced by race from the identity-related representation\nfrom a signal subspace decomposition perspective. To dis-\ntill the identity-related component and suppress the identity-\nunrelated part, we abstract the distillation process of the face\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 12\nrepresentation as a signal denoising problem and propose a\ngeneralized parameterized model with the cross transformer\n(CT) mechanism. This cross transformer mechanism can be\napplied in a stage-wise manner within a face recognition\nnetwork so as to suppress the bias caused by race gradually.\nExperimental results show our proposed PCT is capable of\nequally representing the faces in different race cohorts while\nenhancing the discrimination of the learned face representa-\ntion. For future work, we will explore how to suppress the\nbias caused by race or other demographic attributes in a self-\nsupervised manner, as PCT relies on race annotation.\nREFERENCES\n[1] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed embed-\nding for face recognition and clustering,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2015, pp. 815–\n823.\n[2] W. Liu, Y . Wen, Z. Yu, M. Li, B. Raj, and L. Song, “Sphereface: Deep\nhypersphere embedding for face recognition,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2017, pp.\n212–220.\n[3] M. Wang and W. Deng, “Deep face recognition: A survey,” arXiv\npreprint arXiv:1804.06655, 2018.\n[4] H. Wang, Y . Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and\nW. Liu, “Cosface: Large margin cosine loss for deep face recognition,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 5265–5274.\n[5] Y . Huang, P. Shen, Y . Tai, S. Li, X. Liu, J. Li, F. Huang, and R. Ji, “Im-\nproving face recognition from hard samples via distribution distillation\nloss,” in European Conference on Computer Vision . Springer, 2020,\npp. 138–154.\n[6] B. F. Klare, M. J. Burge, J. C. Klontz, R. W. V . Bruegge, and A. K.\nJain, “Face recognition performance: Role of demographic information,”\nIEEE Transactions on Information Forensics and Security, vol. 7, no. 6,\npp. 1789–1801, 2012.\n[7] P. Drozdowski, C. Rathgeb, A. Dantcheva, N. Damer, and C. Busch,\n“Demographic bias in biometrics: A survey on an emerging challenge,”\nIEEE Transactions on Technology and Society, vol. 1, no. 2, pp. 89–103,\n2020.\n[8] P. Grother, M. Ngan, and K. Hanaoka, Face Recognition Vendor Test\n(FVRT): Part 3, Demographic Effects . National Institute of Standards\nand Technology, 2019.\n[9] M. Wang and W. Deng, “Mitigating bias in face recognition us-\ning skewness-aware reinforcement learning,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 9322–9331.\n[10] B. Lu, J.-C. Chen, C. D. Castillo, and R. Chellappa, “An experimental\nevaluation of covariates effects on unconstrained face veriﬁcation,”IEEE\nTransactions on Biometrics, Behavior, and Identity Science, vol. 1, no. 1,\npp. 42–55, 2019.\n[11] A. Acien, A. Morales, R. Vera-Rodriguez, I. Bartolome, and J. Fierrez,\n“Measuring the gender and ethnicity bias in deep models for face recog-\nnition,” in Iberoamerican Congress on Pattern Recognition . Springer,\n2018, pp. 584–593.\n[12] M. Merler, N. Ratha, R. S. Feris, and J. R. Smith, “Diversity in faces,”\narXiv preprint arXiv:1901.10436 , 2019.\n[13] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face representation from\nscratch,” arXiv preprint arXiv:1411.7923 , 2014.\n[14] Y . Guo, L. Zhang, Y . Hu, X. He, and J. Gao, “Ms-celeb-1m: A dataset\nand benchmark for large-scale face recognition,” inEuropean conference\non computer vision . Springer, 2016, pp. 87–102.\n[15] Y . Zhang and Z.-H. Zhou, “Cost-sensitive face recognition,” IEEE\ntransactions on pattern analysis and machine intelligence , vol. 32,\nno. 10, pp. 1758–1769, 2009.\n[16] C. Huang, Y . Li, C. C. Loy, and X. Tang, “Deep imbalanced learning for\nface recognition and attribute prediction,” IEEE transactions on pattern\nanalysis and machine intelligence, vol. 42, no. 11, pp. 2781–2794, 2019.\n[17] S. S. Mullick, S. Datta, and S. Das, “Generative adversarial minority\noversampling,” in Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision , 2019, pp. 1695–1704.\n[18] Y . Zhang and W. Deng, “Class-balanced training for deep face recogni-\ntion,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops, 2020, pp. 824–825.\n[19] M. Wang, W. Deng, J. Hu, X. Tao, and Y . Huang, “Racial faces in\nthe wild: Reducing racial bias by information maximization adaptation\nnetwork,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 2019, pp. 692–702.\n[20] S. Gong, X. Liu, and A. K. Jain, “Jointly de-biasing face recognition and\ndemographic attribute estimation,” inEuropean Conference on Computer\nVision. Springer, 2020, pp. 330–347.\n[21] J. P. Robinson, G. Livitz, Y . Henon, C. Qin, Y . Fu, and S. Timoner, “Face\nrecognition: too bias, or not too bias?” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops ,\n2020, pp. 0–1.\n[22] H. Wang, D. Gong, Z. Li, and W. Liu, “Decorrelated adversarial learning\nfor age-invariant face recognition,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n3527–3536.\n[23] Y . Liu, F. Wei, J. Shao, L. Sheng, J. Yan, and X. Wang, “Explor-\ning disentangled feature representation beyond face identiﬁcation,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 2080–2089.\n[24] I. Serna, A. Morales, J. Fierrez, M. Cebrian, N. Obradovich, and\nI. Rahwan, “Algorithmic discrimination: Formulation and exploration\nin deep learning-based face biometrics,” in AAAI Workshop on Artiﬁcial\nIntelligence Safety (SafeAI) , February 2020.\n[25] S. Gong, X. Liu, and A. K. Jain, “Mitigating face recognition bias via\ngroup adaptive classiﬁer,” arXiv preprint arXiv:2006.07576 , 2020.\n[26] P. J. Phillips, P. Grother, R. Micheals, D. M. Blackburn, E. Tabassi,\nand M. Bone, “Face recognition vendor test 2002,” in 2003 IEEE\nInternational SOI Conference. Proceedings (Cat. No. 03CH37443) .\nIEEE, 2003, p. 44.\n[27] J. G. Cavazos, P. J. Phillips, C. D. Castillo, and A. J. O’Toole,\n“Accuracy comparison across face recognition algorithms: Where are we\non measuring race bias?” IEEE Transactions on Biometrics, Behavior,\nand Identity Science , 2020.\n[28] A. Waswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in NIPS, 2017.\n[29] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[30] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213–\n229.\n[31] Y . Jin, D. Han, and H. Ko, “Trseg: Transformer for semantic segmen-\ntation,” Pattern Recognition Letters, vol. 148, pp. 29–35, 2021.\n[32] Y . Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia,\n“End-to-end video instance segmentation with transformers,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2021, pp. 8741–8750.\n[33] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani,\n“Bottleneck transformers for visual recognition,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2021, pp. 16 519–16 529.\n[34] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer,\n“Trackformer: Multi-object tracking with transformers,” arXiv preprint\narXiv:2101.02702, 2021.\n[35] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, “Transformer\ntracking,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2021, pp. 8126–8135.\n[36] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video action\ntransformer network,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 244–253.\n[37] Y . Fang, S. Gao, J. Li, W. Luo, L. He, and B. Hu, “Multi-level feature\nfusion based locality-constrained spatial transformer network for video\ncrowd counting,” Neurocomputing, vol. 392, pp. 98–107, 2020.\n[38] Y . Zhong and W. Deng, “Face transformer for recognition,” arXiv\npreprint arXiv:2103.14803, 2021.\n[39] G. M. Jacob and B. Stenger, “Facial action unit detection with trans-\nformers,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2021, pp. 7680–7689.\n[40] H. Li, M. Sui, F. Zhao, Z. Zha, and F. Wu, “Mvit: Mask vision\ntransformer for facial expression recognition in the wild,” arXiv preprint\narXiv:2106.04520, 2021.\n[41] F. Ma, B. Sun, and S. Li, “Robust facial expression recognition with\nconvolutional visual transformers,” arXiv preprint arXiv:2103.16854 ,\n2021.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2018 13\n[42] X.-B. Nguyen, D. T. Bui, C. N. Duong, T. D. Bui, and K. Luu,\n“Clusformer: A transformer based clustering approach to unsupervised\nlarge-scale face and visual landmark recognition,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2021, pp. 10 847–10 856.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770–778.\n[44] Y . Li, L. Lao, Z. Cui, S. Shan, and J. Yang, “Graph jigsaw learning for\ncartoon face recognition,” arXiv preprint arXiv:2107.06532 , 2021.\n[45] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2019, pp. 3286–3295.\n[46] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\nJ. Shlens, “Stand-alone self-attention in vision models,” arXiv preprint\narXiv:1906.05909, 2019.\n[47] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\nposition representations,” arXiv preprint arXiv:1803.02155 , 2018.\n[48] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular\nmargin loss for deep face recognition,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n4690–4699.\n[49] Y . Shi and A. K. Jain, “Probabilistic face embeddings,” in Proceedings\nof the IEEE/CVF International Conference on Computer Vision , 2019,\npp. 6902–6911.\n[50] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, “Joint face detection and\nalignment using multitask cascaded convolutional networks,” IEEE\nSignal Processing Letters , vol. 23, no. 10, pp. 1499–1503, 2016.\n[51] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in\npytorch,” 2017.\n[52] P. Terh ¨orst, J. N. Kolf, M. Huber, F. Kirchbuchner, N. Damer,\nA. Morales, J. Fierrez, and A. Kuijper, “A comprehensive study\non face recognition biases beyond demographics,” arXiv preprint\narXiv:2103.01592, 2021.\n[53] J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou, “Retinaface:\nSingle-shot multi-level face localisation in the wild,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2020.\n[54] I. B ¨ulthoff, W. Jung, R. G. Armann, and C. Wallraven, “Predominance\nof eyes and surface information for face race categorization,” Scientiﬁc\nreports, vol. 11, no. 1, pp. 1–9, 2021.\n[55] S. Fu, H. He, and Z.-G. Hou, “Learning race from face: A survey,”\nIEEE transactions on pattern analysis and machine intelligence, vol. 36,\nno. 12, pp. 2483–2509, 2014.\n[56] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-\ntional networks,” in European conference on computer vision. Springer,\n2014, pp. 818–833.\nYong Li received the Ph.D. degree from Institute of\nComputing Technology (ICT), Chinese Academy of\nSciences in 2020. He worked as a software engineer\nin Baidu company from 2015 to 2016. He has been\na assistant professor at School of Computer Sci-\nence and Engineering, Nanjing University of Science\nand Technology since 2020. His research interests\ninclude face-related deep learning, self-supervised\nlearning and affective computing.\nYufei Sun received the B.E. degree from Henan\nUniversity, Kaifeng, China in 2019. He is working\ntowards the M.S. degree in computer science and\ntechnology. His research interests include computer\nvision, fair face recognition.\nZhen Cui received the Ph.D. degree from Institute of\nComputing Technology (ICT), Chinese Academy of\nSciences in 2014. He was a Research Fellow in the\nDepartment of Electrical and Computer Engineering\nat National University of Singapore (NUS) from\nSep 2014 to Nov 2015. He also spent half a year\nas a Research Assistant on Nanyang Technological\nUniversity (NTU) from Jun 2012 to Dec 2012.\nCurrently, he is a Professor of Nanjing University of\nScience and Technology, China. His research inter-\nests cover computer vision, pattern recognition and\nmachine learning, especially focusing on vision perception and computation,\ngraph deep learning, etc.\nShiguang Shan received M.S. degree in computer\nscience from the Harbin Institute of Technology,\nHarbin, China, in 1999, and Ph.D. degree in com-\nputer science from the Institute of Computing Tech-\nnology (ICT), Chinese Academy of Sciences (CAS),\nBeijing, China, in 2004. He joined ICT, CAS in\n2002 and has been a Professor since 2010. He is\nnow the deputy director of the Key Lab of Intelli-\ngent Information Processing of CAS. His research\ninterests cover computer vision, pattern recognition,\nand machine learning. He especially focuses on face\nrecognition related research topics. He has published more than 200 papers\nin refereed journals and proceedings in the areas of computer vision and\npattern recognition. He has served as Area Chair for many international confer-\nences including ICCV’11, ICPR’12, ACCV’12, FG’13, ICPR’14, ICASSP’14,\nACCV’16, ACCV18, FG’18, and BTAS’18. He is Associate Editors of several\ninternational journals including IEEE Trans. on Image Processing, Computer\nVision and Image Understanding, Neurocomputing, and Pattern Recognition\nLetters. He is a recipient of the China’s State Natural Science Award in 2015,\nand the China’s State S&T Progress Award in 2005 for his research work. He\nis also personally interested in brain science, cognitive neuroscience, as well\nas their interdisciplinary researche topics with AI.\nJian Yang received the PhD degree from Nanjing\nUniversity of Science and Technology (NUST), on\nthe subject of pattern recognition and intelligence\nsystems in 2002. In 2003, he was a postdoctoral\nresearcher at the University of Zaragoza. From 2004\nto 2006, he was a Postdoctoral Fellow at Biometrics\nCentre of Hong Kong Polytechnic University. From\n2006 to 2007, he was a Postdoctoral Fellow at\nDepartment of Computer Science of New Jersey\nInstitute of Technology. Now, he is a Chang-Jiang\nprofessor in the School of Computer Science and\nTechnology of NUST. He is the author of more than 100 scientiﬁc papers in\npattern recognition and computer vision. His journal papers have been cited\nmore than 4000 times in the ISI Web of Science, and 9000 times in the Web of\nScholar Google. His research interests include pattern recognition, computer\nvision and machine learning. Currently, he is/was an associate editor of Pattern\nRecognition Letters, IEEE Trans. Neural Networks and Learning Systems, and\nNeurocomputing. He is a Fellow of IAPR."
}