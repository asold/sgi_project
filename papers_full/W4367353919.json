{
    "title": "Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT",
    "url": "https://openalex.org/W4367353919",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4367573278",
            "name": "Mostafa M. Amin",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2806062742",
            "name": "Erik Cambria",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2954012826",
            "name": "Björn W. Schuller",
            "affiliations": [
                "University of Augsburg"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6800751262",
        "https://openalex.org/W6810738896",
        "https://openalex.org/W6846930601",
        "https://openalex.org/W4375949262",
        "https://openalex.org/W6850462617",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W6850182213",
        "https://openalex.org/W2573933330",
        "https://openalex.org/W2735866559",
        "https://openalex.org/W2749525589",
        "https://openalex.org/W4285304296",
        "https://openalex.org/W6800546666",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W4212863985",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6636510571",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2144005487",
        "https://openalex.org/W4247934745",
        "https://openalex.org/W4294982692",
        "https://openalex.org/W2962692632",
        "https://openalex.org/W4283802865",
        "https://openalex.org/W4382463665",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4404658388",
        "https://openalex.org/W4307001524",
        "https://openalex.org/W4286970057",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4321472057"
    ],
    "abstract": "ChatGPT has shown the potential of emerging general artificial intelligence capabilities, as it has demonstrated competent performance across many natural language processing tasks. In this work, we evaluate the capabilities of ChatGPT to perform text classification on three affective computing problems, namely, big-five personality prediction, sentiment analysis, and suicide tendency detection. We utilize three baselines, a robust language model (RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and a simple bag-of-words (BoW) baseline. Results show that the RoBERTa model trained for a specific downstream task generally has a superior performance. On the other hand, ChatGPT provides decent results and is relatively comparable to the Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy data, where the Word2Vec model achieves worse results due to noise. Results indicate that ChatGPT is a good generalist model that is capable of achieving good results across various problems without any specialized training; however, it is not as good as a specialized model for a downstream task.",
    "full_text": "Will affective computing emerge from foundation models\nand general artificial intelligence? A first evaluation of\nChatGPT\nMostafa M. Amin, Erik Cambria, Björn W. Schuller\nAngaben zur Veröffentlichung / Publication details:\nAmin, Mostafa M., Erik Cambria, and Björn W. Schuller. 2023. “Will affective computing\nemerge from foundation models and general artificial intelligence? A first evaluation of\nChatGPT.” IEEE Intelligent Systems38 (2): 15–23.https://doi.org/10.1109/mis.2023.3254179.\nNutzungsbedingungen / Terms of use:\nDieses Dokument wird unter folgenden Bedingungen zur Verfügung gestellt: / This document is made available under these conditions:\nDeutsches Urheberrecht\nWeitere Informationen finden Sie unter: / For more information see:\nhttps://www.uni-augsburg.de/de/organisation/bibliothek/publizieren-zitieren-archivieren/publiz/\nlicgercopyright\n\nDEPARTMENT: AFFECTIVE COMPUTING AND SENTIMENT\nANALYSIS\nWill Affective Computing Emerge From\nFoundation Models and General Artificial\nIntelligence? A First Evaluation of ChatGPT\nMostafa M. Amin , University of Augsburg, 86159, Augsburg, Germany, and SyncPilot GmbH, 86156, Augsburg,\nGermany\nErik Cambria , Nanyang Technological University, 639798, Singapore\nBj€orn W. Schuller , University of Augsburg, 86159, Augsburg, Germany, and Imperial College London, SW7 2AZ,\nLondon, U.K.\nChatGPT has shown the potential of emerging general artiﬁcial intelligence capabilities,\nas it has demonstrated competent performance across many natural language\nprocessing tasks. In this work, we evaluate the capabilities of ChatGPT to perform text\nclassiﬁcation on three affective computing problems, namely, big-ﬁve personality\nprediction, sentiment analysis, and suicide tendency detection. We utilize three\nbaselines, a robust language model (RoBERTa-base), a legacy word model with\npretrained embeddings (Word2Vec), and a simple bag-of-words (BoW) baseline. Results\nshow that the RoBERTa model trained for a speciﬁc downstream task generally has a\nsuperior performance. On the other hand, ChatGPT provides decent results and is\nrelatively comparable to the Word2Vec and BoW baselines. ChatGPT further shows\nrobustness against noisy data, where the Word2Vec model achieves worse results due\nto noise. Results indicate that ChatGPT is a good generalist model that is capable of\nachieving good results across various problems without any specialized training;\nhowever, it is not as good as a specialized model for a downstream task.\nW\nith the advent of increasingly large-data-\ntrained general-purpose machine learning\nmodels, a new era of “foundation models”\nhas started. According to Bommasani et al.,1 these are\nmarked by having been trained on “broad” data— often\nself-supervised— at scale leading to 1) homogenization\n(i.e., most use the same model for ﬁne-tuning and train-\ning for downstream tasks, as they are effective across\nmany tasks and too cost-intensive to train individually)\nand 2) emergence (i.e., tasks can be solved that these\nmodels were not originally trained upon— potentially\neven without additional ﬁne-tuning or downstream\ntraining). However, at this time, much more research is\nneeded to understand the actual emergence abilities\nthat potentially lead to a massive shift of paradigm in\nmachine learning. Models might not need to be trained\nany more at all speciﬁcally for limited tasks, be it from\nthe upstream or downstream perspective. Here, we\nconsider the example of affective computing tasks\nseen from a natural language processing (NLP) end. In\nthe future, will we need to train extra models at all to\ntackle tasks such as personality, sentiment, or suicidal\ntendency recognition from text, or will “big” foundation\nmodels sufﬁce with their emergence of these?\nTo this end, we consider ChatGPT as our basis for a\n“big” foundation model to check for the full emergence\nof these three tasks. It was launched on 30 November\n2022 and gained more than 1 million users within one\nweek.2 It has shown very promising results as an inter-\nactive chatting bot that is capable, to a large extent, of\nunderstanding questions posed by humans and giving\nmeaningful answers to them. ChatGPT is one of the\n                                                                      15                                                                                                                                           \nnamed “foundation models” constructed by ﬁne-tuning\na large language model— namely, GPT-3— that can\ngenerate English text. The model is ﬁne-tuned using\nreinforcement learning from human feedback (RLHF),3\nwhich makes use of reward models that rank res-\nponses based on different criteria; the reward models\nare then used to sample a more general space of\nresponses.3,4 As a result, general artiﬁcial intelligence\n(AI) capabilities emerged from this training mechanism,\nwhich resulted in a very fast adoption of ChatGPT by\nmany users in a very short time.2 The effectiveness of\nthese capabilities is not exactly known yet; for exam-\nple, Borj5 explored many of the systematic failures of\nChatGPT. Zhou et al.6 explain the history of the devel-\nopment of the NLP literature until arriving at the point\nwhen ChatGPT was developed.\nIn summary, the aim of this article is to systematize\nan evaluation framework for evaluating the perfor-\nmance of ChatGPT on various classiﬁcation tasks to\nanswer the question of whether it shows full emer-\ngence features of other (affective computing-related)\nNLP tasks. We use this framework to show if ChatGPT\nhas general capabilities that could yield competent\nperformance on affective computing problems. The\nevaluation compares it against specialized models that\nare speciﬁcally trained on the downstream tasks. The\ncontributions of this article are as follows:\nWe evaluate whether NLP foundation models can\nlead to “full” (i.e., no need for ﬁne-tuning or down-\nstream training) emergence of other tasks, which\nwould usually be trained on speciﬁc data sources.\nTherefore, we introduce a method to evaluate\nChatGPT on classiﬁcation tasks.\nWe compare the results of ChatGPT on three\nclassiﬁcation problems in the ﬁeld of affective\ncomputing. The problems are big-ﬁve personal-\nity prediction, sentiment analysis, and suicide\nand depression detection.\nThe remainder of this article is organized as follows.\nWe begin by elaborating on the related work, then we\nintroduce our method, and then we present and dis-\ncuss the results. We ﬁnish with concluding remarks.\nRELATED WORK\nWe focus on related work within the key research ques-\ntion of potential emergence (in the text domain) by\nfoundation models. In particular, Qin et al.7 explore\nthe question of whether ChatGPT is a general NLP\nsolver that works for all problems. They explore a\nwide range of tasks, like reasoning, text summariza-\ntion, named entity recognition, and sentiment analysis.\nHendy et al.8 explore the capabilities of GPT language\nmodels (including ChatGPT) in machine translation.\nBorji5 explores the systematic errors of ChatGPT.\nMETHODS\nThe aim of this article is to evaluate the generalization\ncapabilities of ChatGPT across a wide range of affective\ncomputing tasks. To assess this, we utilize three datasets\ncorresponding to three different problems, as mentioned:\nbig-ﬁve personality prediction, sentiment analysis, and\nsuicide tendency assessment. For these tasks, we utilize\nthree datasets. On each of the introduced tasks, we\nattempt to get ChatGPT’s assessment about each of the\nexamples of the corresponding test set. Furthermore, we\ncompare ChatGPT against three baselines, namely, a\nlarge language model, a word model with pretrained\nembeddings, and a basic bag-of-words (BoW) model\nwithout making use of any external data. We describe\nthe datasets, querying procedure of ChatGPT, and base-\nlines in this section. Figure 1 demonstrates the pipelines\nof all methods (ChatGPT and the three baselines).\nDatasets\nWe introduce the three datasets in this section. A sum-\nmary of their statistics is presented in Table 1. We uti-\nlize publicly available datasets for reproducibility.\nPersonality Dataset\nWe utilize the First Impressions dataset9,10 for the per-\nsonality task.a Personality is represented by the big-\nﬁve personality traits, namely, openness to experience,\nconscientiousness, extraversion, agreeableness, and\nneuroticism. The dataset consists of 15-s videos with\none speaker, whose personality was manually labeled.\nSuch labeling was conducted by relative comparisons\nbetween pairs of videos by ranking which person\nscores higher on each one of the big-ﬁve personality\ntraits. A statistical model was then used to reduce the\nlabels into regression values within the range [0, 1]. The\npersonality labels were given based on the multiple\nmodalities of a video, namely, images, audio, and text\n(content). We utilize the transcriptions of these videos\nas the input to be used to predict personality. We use\nthe train/development (dev)/test split given by the\npublishers of the dataset.9,10 Like Kaya et al.,11 we train\nthe models on this dataset as a regression problem\n(by using mean absolute error as the loss function)\nsince the continuous ground truth values can give a\nmore granular estimation of the labels can give a more\naWe acquired the dataset on 3 February 2023 from https://\nchalearnlap.cvc.uab.cat/dataset/24/description/.\n16                                                                                                                                                                                 \ngranular estimation of the labels; then, we binarize\nthese to positive or negative using the threshold 0.5.\nSentiment Dataset\nWe adopt the Sentiment140 dataset12 for the sentiment\nanalysis task.b The dataset is collected from tweets on\nTwitter, which makes the text very noisy, which can\npose a challenge for many models (especially word\nmodels). The dataset consists of tweets and the corre-\nsponding sentiment labels (positive or negative). We\nsplit the training portion with a ratio of 9:1 to give the\ntrain and dev portionsc listed in Table 1. The test portion\nconsists of 497 tweets only; however, these were ﬁl-\ntered down to 359 because the remaining have a neu-\ntral label, which is not present in the training set.\nSuicide and Depression Dataset\nThe suicide and depression dataset13 is collected from\nthe Reddit platform, under different subreddit categories,\nnamely, “SuicideWatch,” “depression,” and “teenagers.”d\nThe texts of the posts from the “teenagers” category are\nlabeled as negative, while the texts from the other two\ncategories are labeled as positive. We excluded exam-\nples longer than 512 characters and then divided the\ndataset into three portions: train, dev, and test.\nChatGPT Querying Mechanism\nWe introduce the stages of querying ChatGPT as\nshown in Figure 1. The general mechanism to collect\nfor our experiments is achieved by the following proce-\ndure for each problem:\nText Subword\nencoding RoBERTa RoBERTa\npooling MLP label\nText Word\nencoding Word2Vec Average\npooling SVM label\nText Question\nformulation chatGPT Parse\nLabel label\nText Word\nencoding TF-IDF Scaling SVM label\nFIGURE 1. Pipelines of the ChatGPT (top row), RoBERTa baseline (second row), Word2Vec baseline (third row), and bag-of-words\nbaseline (bottom row) approaches. MLP: multilayer perceptron; SVM: support vector machine; TF-IDF: term frequency–inverse\ndocument frequency.\nTABLE 1. Statistics on the sizes of the datasets, with counts of positive and negative classes in the test set.\nDataset Train Dev Test Positive Negative\nO\n6000 2000 509\n333 176\nC 286 223\nE 214 295\nA 340 169\nN 274 235\nSent 1,440,144 159,856 359 182 177\nSui 138,479 6270 496 165 331\nThe “Test” column shows the ﬁnal number of samples used for evaluation (lower than the original sizes due to the limitation of manually collecting\nexamples from ChatGPT). A: agreeableness; C: conscientiousness; E: extraversion; N: neuroticism; O: openness to experience; Sent: sentiment; Sui:\nsuicide.\nbWe acquired the dataset from https://huggingface.co/\ndatasets/sentiment140 on 9 February 2023.\nc[Online]. Available at: https://github.com/senticnet/chatgpt-\naffect.\ndWe acquired the dataset on 28 January 2023 from https://\nwww.kaggle.com/datasets/nikhileswarkomati/suicide-watch.\n                                     17                                                                                                                                           \n1) Reformat all of the texts of the test portion of the\ndataset by using a format that asks ChatGPT\nwhat its guess is about the label of the text.\n2) Chunk the examples into 25 examples per chunk.\n3) For each chunk, open a new ChatGPT conversation.\n4) Ask ChatGPT (manually) the reformatted ques-\ntion for each example, one by one, and collect\nthe answers.\n5) Repeat the steps 3–4 until the predictions for\nthe whole test set are ﬁnished.\n6) Postprocess the results in case they need some\ncleanup.\nThe formatting in the ﬁrst step and the postpro-\ncessing in the last step are speciﬁed in the following\ntwo sections. We used the version of ChatGPT released\non 30 January 2023.e\nQuestion Formulation\nThe formats that are used for the three problems are\ngiven by the following snippets. The example text is\nsubstituted in place of the {text} part; however, the\nquotation marks are kept since it speciﬁes for ChatGPT\nthat this is a placeholder used by the question being\nasked. The formulations for the three problems are\ngiven as follows:\nFor the big-ﬁve personality traits, we formulate\nthe following question: “What is your guess for\nthe big-ﬁve personality traits of someone who\nsaid ‘{text}’? Answer ‘low’ or ‘high’ with bullet\npoints for the ﬁve traits. It does not have to be\nfully correct. You do not need to explain the\ntraits. Do not show any warning after.”\nFor the sentiment analysis, we formulate the\nfollowing question: “What is your guess for the\nsentiment of the text ‘{text}’? Answer ‘positive,’\n‘neutral,’ or ‘negative.’ It does not have to be\ncorrect. Do not show any warning after.”\nFor the suicide problem, we formulate the follow-\ning question: “What is your guess as to whether a\nperson saying ‘{text}’ has a suicide tendency or\nnot? Answer ‘yes’or ‘no.’It does not have to be\ncorrect. Do not show any warning after.”\nThe formulation of the question is of crucial impor-\ntance to the answer ChatGPT will give; we encoun-\ntered the following aspects:\nAsking the question directly without asking about\na guess made ChatGPT, in many instances, answer\nthat there is little information provided to answer\nthe question and that it cannot answer it exactly.\nHence, we ask it to guess the answer, and we\ndeclare that it is acceptable to be not fully\naccurate.\nIt is important to ask what the guess is and not\nCan you guess? because this can give a response\nsimilar to the previous point, where ChatGPT\nresponds with an answer that starts with No, I\ncannot accurately answer whether. . . . Therefore,\nthe question needs to be assertive and speciﬁc.\nWe need to specify the exact output format,\nbecause ChatGPT can get innovative otherwise\nabout the formatting of the answer, which can\nmake it hard to collect the answers for our\nexperiment. Despite specifying the format, it\nstill sometimes gave different formats. We elab-\norate on this in the next section.\nThe questions for the suicide assessment task trig-\ngered warnings in the responses of ChatGPT due\nto their sensitive content. We elaborate on the\nterms of use in the “Acknowledgments” section.\nParsing Responses\nThe responses of ChatGPT need to be parsed since\nChatGPT can give arbitrary formats for a given answer,\neven when the content is the same. This is predomi-\nnant in the personality traits since there are ﬁve traits.\nSometimes, the answers are listed as bullet points;\nother times, they are all in one comma-separated line.\nAlso, it uses different delimiters or order, e.g.,\n“Openness: Low,” “Low in Openness,” and “Low: Open-\nness.” Additionally, in all problems, it sometimes gives\nan introduction for the answer, for example, “Here is\nmy guess for . . .” or “Based on the statement. . . .” We\ncounter this issue by using regular expressions to cap-\nture the responses.\nBaselines\nTo compare the performance of ChatGPT on the differ-\nent tasks, we need to use baselines and train them on\nthe train portion (while validating on the dev portion).\nWe employ three baselines, which serve as the special-\nized models speciﬁcally tailored for the corresponding\ndownstream task. The ﬁrst baseline is a robust lan-\nguage model (RoBERTa) trained on a large amount of\ntext. The second is a simple baseline that uses a word\nmodel by employing pretrained Word2Vec embeddings\non the words of a sentence with a simple classiﬁer. The\nthird baseline is a simple BoW model that utilizes a lin-\near classiﬁer. The hyperparameters of all models are\noptimized by selecting the hyperparameters yielding\neChatGPT release notes: https://help.openai.com/en/articles/\n6825453-chatgpt-release-notes.\n18                                                                                                                                                                                 \nthe best performance on the dev portion. The hyper-\nparameters are tuned using the SMAC toolkit,14 which\nis based on Bayesian optimization. The selected hyper-\nparameters are listed in Table 2.\nRoBERTa Language Model\nThe baseline RoBERTa15 is a pretrained BERT model,\nwhich has a transformer architecture. Liu et al.15 trai-\nned two instances of RoBERTa; we use the smaller one,\nnamely, RoBERTa-base,f consisting of 110 million para-\nmeters. RoBERTa-base is pretrained on a mixture of\nseveral large datasets that included books, English\nWikipedia, English news, Reddit posts, and stories.15\nThe model starts by tokenizing a text using subword\nencoding, which is a hybrid representation between\ncharacter-based and word-based encodings. The tokens\nare then fed to RoBERTa to obtain a sequence of\nembeddings. The pooling layer of RoBERTa is then used\nto reduce the embeddings into one embedding only,\nhence acquiring a static feature vector of size 768 repre-\nsenting the text. We additionally train a multilayer per-\nceptron (MLP)16 to predict the ﬁnal label. The pipeline\nfor the model is shown in Figure 1.\nFor the training procedure, we use SMAC14 to select\nthe MLP speciﬁcations. We employ SMAC to sample a\ntotal of 100 models per task and train them with a batch\nsize 256 for 300 epochs with early stopping to prune\nthe ineffective models. Eventually, the model with the\nbest performance on the dev set is selected. The hyper-\nparameter space consists of four hyperparameters: the\nnumber of hidden layers N 2 [0, 3], the number of neu-\nrons in the ﬁrst hidden layer U 2 [64, 512] (log sampled),\nthe optimization algorithm [Adam17 or stochastic gradi-\nent descent (SGD)16], and the learning rate a 2 [10/C06, 10]\n(log sampled). The number of neurons in the hidden\nlayers is speciﬁed by the ﬁrst one as a hyperparameter;\nthen, the number of neurons is halved for each subse-\nquent hidden layer (clipped to be at least 32). The hid-\nden layers have rectiﬁed linear unit as an activation\nfunction. The ﬁnal layer has a sigmoid activation func-\ntion. The loss function for classiﬁcation is cross entropy\nand for regression is mean absolute error.\nWord2Vec Word Embeddings\nThe baseline Word2Vec18,19 makes use of pretrained\nword embeddings,g which are trained on a large amount\nof text from Google News. The model operates by toke-\nnizing a given text into words; each word is assigned an\nembedding from the pretrained embeddings. The\nembeddings are then averaged for all words to give a\nstatic feature vector of size 300 for the entire string. A\nsupport vector machine (SVM)16 is then used to predict\nthe given task. The pipeline of this model is shown\nin Figure 1.\nWe train the SVM model by tuning its hyperpara-\nmeter C using SMAC14 by sampling 20 values within\nthe range [10/C06, 104] (log sampled) and choosing the\nmodel that yields the best score on the dev set. We use\nthe radial basis function (RBF) kernel for the SVMs\nexcept for the sentiment dataset, where we apply a lin-\near kernel, as the sentiment dataset is much bigger (as\nshown in Table 1), which renders the RBF impractical\ndue to the computational efﬁciency.\nTABLE 2. Hyperparameters of the different baselines.\nTarget Label\nRoBERTa W2V BoW\nN U a C g\nO\n2 498 5.66 /C210/C04\n0.0378 2.47 /C210/C03\nC 0.0472 3.09 /C210/C06\nE 0.0069 1.09 /C210/C05\nA 0.0218 4.65 /C210/C04\nN 0.0657 2.21 /C210/C06\nSen 3 420 2.97 /C210/C05 0.0144 5.25 /C210/C06\nSui 3 497 8.04 /C210/C04 10.00 4.71 /C210/C06\nN is the number of hidden layers in the multilayer perceptron using RoBERTa representations, U is the number of neurons in the ﬁrst hidden layer\n(which is halved for each subsequent layer), and a is the learning rate. The Adam optimizer always yields the best results as compared to stochastic\ngradient descent (SGD). C is the support vector machine parameter for Word2Vec, and the sentiment model used linear kernel, while the other\nmodels used the radial basis function kernel. g is the learning rate of the SGD in the BoW model. Bow: bag of words; W2V: Word2Vec.\nfAcquired on 9 February 2023 from https://huggingface.co/\ndocs/transformers/modeldoc/roberta.\ngAcquired on 16 February 2023 from https://code.google.\ncom/archive/p/word2vec/.\n                                     19                                                                                                                                           \nBoW\nThe BoW model is a very simple baseline that does not\nrely on any knowledge transfer or large-scale training.\nIn particular, it uses only in-domain data for training\nand no other data for either up- or downstreaming.\nWe utilize the classical technique term frequency–\ninverse document frequency, which tokenizes the sen-\ntences into words; then, a sentence is represented by\na vector of the counts of the words it contains. The\nvector is then normalized by the term frequency across\nthe entire train set of the corresponding dataset. We\nrestrict the words to the most common 10,000 words\nin the train set; then, we scale each feature to be within\n[/C01, 1], by dividing by the maximum absolute value of\nthe feature across the train set. We optimize a linear\nkernel SVM, and we optimize using SGD16 due to the\nhigh number of features (10,000 features). We tune the\nlearning rate g of SGD using SMAC.14\nRESULTS\nIn this section, we review the results of our experi-\nments. In summary, we evaluate the performance\nof ChatGPT against the three baselines— RoBERTa,\nWord2Vec, and BoW— on three downstream classiﬁca-\ntion tasks, namely, personality traits, sentiment analy-\nsis, and suicide tendency assessment. We measure\nclassiﬁcation accuracy and unweighted average recall\n(UAR)20 as performance measures. UAR has the advan-\ntage of exposing whether a model is performing very\nwell on a class at the expense of the other class, espe-\ncially in imbalanced datasets. Additionally, we utilize\nthe randomized permutation test as a statistical signiﬁ-\ncance test.21 The main results of the experiments are\nshown in Table 3.\nDISCUSSION\nThe RoBERTa model is achieving the best performance\nfor the personality and suicide assessment tasks, with\na statistically signiﬁcant improvement of accuracy\nover ChatGPT. However, ChatGPT is the best in senti-\nment analysis, but only slightly better than RoBERTa.\nThe UAR for the personality traits points to similar con-\nclusions about the relative performance; however, it\nyields much lower values for all baselines on some of\nthe traits (openness and agreeableness). The UAR\nmeasure generally yields similar results on all models\nfor both sentiment analysis and suicide assessment.\nThe performance of ChatGPT on the personality assess-\nment is inferior to that of the three baselines on all of\nthe traits. It is signiﬁcantly worse than RoBERTa on all\ntraits and Word2Vec on three traits.\nChatGPT has the best performance in the senti-\nment analysis, where it is slightly better than RoBERTa\nand BoW and signiﬁcantly better than Word2Vec. One\nof the potential reasons for the inferiority of Word2Vec\nand BoW on the sentiment dataset is not using sub-\nword encodings. The reason is that the sentiment data-\nset is collected from Twitter, so it is very noisy, which\ncan lead to many mistakes in identifying the words\nand, hence, assigning them the proper embeddings.\nSubword encoding avoids many of these issues since a\nfew typos would still yield a meaningful subword repre-\nsentation of the given sentence.\nThe results on the suicide assessment problem\nshow the contrast between the aforementioned analy-\nses. The task is not as hard as the personality assess-\nment problem, with a much bigger amount of training\ndata. The suicide assessment can, rather, be thought\nof as classifying extreme negative sentiment, where\nTABLE 3. The classiﬁcation accuracy and unweighted average recall of ChatGPT against the baselines on the different\ntasks.\nTarget\nLabel\nAccuracy Unweighted Average Recall (%)\nChatGPT RoBERTa Word2Vec BoW ChatGPT RoBERTa Word2Vec BoW\nO 46.6 66.0*** 65.2*** 59.7*** 50.1 50.9 50.7 55.6\nC 57.4 63.7* 62.7 55.6 57.7 60.8 60.0 56.3\nE 55.2 66.0*** 59.9 55.2 54.0 62.3*** 55.5 53.7\nA 44.8 67.4*** 67.2*** 58.5*** 48.4 51.9 51.0 55.7*\nN 47.2 62.1*** 56.8*** 56.0*** 49.1 61.2*** 54.6 55.8*\nSen 85.5 85.0 79.4* 82.5 85.5 85.0 79.4** 82.4\nSui 92.7 97.4*** 92.1 92.7 91.2 97.4*** 91.2 90.9\n*Statistically signiﬁcant difference as compared to ChatGPT, with a p value of 0.05. **Statistically signiﬁcant difference as compared to ChatGPT,\nwith a p value of 0.02. ***Statistically signiﬁcant difference as compared to ChatGPT, with a p value of 0.01. Signiﬁcance tests are checked with a\nrandomized permutation test. The bold values refer to the best model (out of the four presented models, namely ChatGPT and the three baselines)\nfor a speciﬁc combination of target label and performance metric.\n20                                                                                                                                                                                 \nQin et al.7 showed that ChatGPT is better at predicting\nnegative sentiment than positive. However, the texts\nof the suicide dataset are much less noisy compared\nto those of the sentiment dataset. In that case, the per-\nformances of the Word2Vec and BoW models are more\nor less on par with that of the ChatGPT model, while\nRoBERTa is signiﬁcantly better than all of them.\nOur experiments indicate that ChatGPT has a decent\nperformance across many tasks (especially sentiment\nanalysis or similar tasks), which is comparable to simple\nspecialized models that solve a downstream task. How-\never, it is not competent enough as compared to the\nbest specialized model to solve the same downstream\ntask (e.g., ﬁne-tuned RoBERTa). The performance of\nChatGPT does not generally show statistically signiﬁ-\ncant differences when compared to the simplest base-\nline BoW, which does not make any use of pretraining.\nThis is further conﬁrmed by Hendy et al.8 in machine\ntranslation and other tasks.7 In summary, our study sug-\ngests that ChatGPT is a generalist model (in contrast to\na specialized model) that can decently solve many dif-\nferent problems without specialized training. However,\nto achieve the best results on speciﬁc downstream\ntasks, dedicated training is still required. This might be\nenhanced in future versions of ChatGPT and similar\nmodels by including more diverse tasks for the RLHF\ncomponent in the training.\nLimitations\nThe most crucial limitation of the presented results is\nthe small amount of data for evaluation (497, 362, and\n509 examples for the three tasks) since ChatGPT is\nonly available for manual entries by consumers and not\nfor automated large-scale testing. Additionally, it only\nresponds to approximately 25–35 requests per hour to\nreduce the computational cost and avoid brute forcing.\nAnother issue that may limit future experiments is\nparsing the responses. In our experiments, ChatGPT\nresponded with arbitrary formatting despite the fact\nthat we speciﬁed the desired format explicitly in the\nquestion prompt.\nCONCLUSION\nIn this article, we provided ﬁrst insight into the poten-\ntial “full” emergence of tasks by broad-data-trained\nfoundation models. We approached this from the per-\nspective of natural language tasks in the affective com-\nputing domain and chose ChatGPT as the exemplary\nfoundation model. To this end, we introduced a frame-\nwork to evaluate the performance of ChatGPT as a gen-\neralist foundation model against specialized models on\na total of seven classiﬁcation tasks from three affective\ncomputing problems, namely, personality assessment,\nsentiment analysis, and suicide tendency assessment.\nWe compared the results against three baselines,\nwhich reﬂect training the downstream tasks and using\nor not using additional data for the upstream task.\nThe ﬁrst model was RoBERTa, a large-scale-trained,\ntransformer-based language model; the second was\nWord2Vec, a deep learning model trained to recon-\nstruct the linguistic contexts of words; and the third\nwas a simple BoW model.\nThe experiments have shown that ChatGPT is a\ngeneralist model that has a decent performance on a\nwide range of problems without specialized training.\nChatGPT showed superior performance in sentiment\nanalysis, poor performance on personality assessment,\naverage performance on suicide assessment. In other\nwords, we could demonstrate genuine emergence pro-\nperties, potentially rendering future efforts to collect\ntask-speciﬁc databases increasingly obsolete.\nHowever, the performance of ChatGPT is not par-\nticularly impressive since it did not show statistically\nsigniﬁcant differences with a simple BoW model in\nalmost all cases. On the other hand, RoBERTa, ﬁne-\ntuned for a speciﬁc task, had signiﬁcantly better perfor-\nmance as compared to ChatGPT on the given tasks,\nwhich suggests that, despite the generalization abili-\nties of ChatGPT, specialized models are still the best\noption for optimal performance. However, this can be\ntaken into consideration in future developments of\nfoundation models like ChatGPT to yield wider explora-\ntion spaces for training.\nIn the near future, we will extend our experiments to\nmore metrics, e.g., explainability and computational efﬁ-\nciency, on top of accuracy and UAR. We also plan to\nexpand our comparative evaluation to more sophisti-\ncated models (e.g., prompt-based classiﬁcation22 and\nneurosymbolic AI23) and more advanced affective com-\nputing tasks (e.g., sarcasm detection,24 metaphor under-\nstanding,25 and conversational emotion recognition26)\nbut also more complex sentiment datasets requiring\ncommonsense reasoning and/or narrative understanding.\nACKNOWLEDGMENTS\nWe would like to thank OpenAI for the usage of\nChatGPT. We followed the policy of ChatGPT.h Our\nuse of ChatGPT is purely for research purposes to\nassess emerging capabilities of foundation models\nand does not promote the use of ChatGPT in any way\nthat violates the aforementioned usage policy; in par-\nticular, with regard to the subject of self-harm, note\nhUsage policy released on 15 February 2023: https://platform.\nopenai.com/docs/usage-policies/disallowed-usage.\n                                     21                                                                                                                                           \nthat some of the examples in the datasets we used\ntriggered a related warning by ChatGPT.\nREFERENCES\n1. R. Bommasani et al., “On the opportunities and risks\nof foundation models,” 2021, arXiv:2108.07258.\n2. S. Mollman. “ChatGPT gained 1 million users in under\na week. Here’s why the AI chatbot is primed to\ndisrupt search as we know it.” Yahoo! Finance.\nAccessed: Feb. 21, 2023. [Online]. Available: https://\nﬁnance.yahoo.com/news/chatgpt-gained-1-million-\nfollowers-224523258.html\n3. L. Ouyang et al., “Training language models to follow\ninstructions with human feedback,” 2022,\narXiv:2203.02155.\n4. L. Gao, J. Schulman, and J. Hilton, “Scaling laws\nfor reward model overoptimization,” 2022,\narXiv:2210.10760.\n5. A. Borji, “A categorical archive of ChatGPT failures,”\n2023, arXiv:2302.03494.\n6. C. Zhou et al., “A comprehensive survey on pretrained\nfoundation models: A history from BERT to ChatGPT,”\n2023, arXiv:2302.09419.\n7. C. Qin et al., “Is ChatGPT a general-purpose natural\nlanguage processing task solver?” 2023,\narXiv:2302.06476.\n8. A. Hendy et al., “How good are GPT models at\nmachine translation? A comprehensive evaluation,”\n2023, arXiv:2302.09210.\n9. V. Ponce-L/C19opez et al., “Chalearn lap 2016: First\nround challenge on ﬁrst impressions – Dataset\nand results,” in Proc. Eur. Conf. Comput. Vision,\nCham, Switzerland: Springer International\nPublishing, 2016, pp. 400–418, doi: 10.1007/978-3-319-\n49409-8_32.\n10. H. J. Escalante et al., “Design of an explainable\nmachine learning challenge for video interviews,” in\nProc. IEEE Int. Joint Conf. Neural Netw. (IJCNN),\nAnchorage, AK, USA, 2017, pp. 3688–3695,\ndoi: 10.1109/IJCNN.2017.7966320.\n11. H. Kaya, F. Gurpinar, and A. A. Salah, “Multi-modal\nscore fusion and decision trees for explainable\nautomatic job candidate screening from video\nCVs,” in Proc. IEEE Conf. Comput. Vision Pattern\nRecognit. (CVPR) Workshops, Honolulu, HI, USA, 2017,\npp. 1–9.\n12. A. Go, R. Bhayani, and L. Huang, “Twitter sentiment\nclassiﬁcation using distant supervision,” Stanford\nUniversity, Stanford, CA, USA, CS224N project report,\n2009.\n13. V. Desu et al., “Suicide and depression detection in\nsocial media forums,” in Proc. Smart Intell. Comput.\nAppl., Singapore: Springer Nature, 2022, vol. 2,\npp. 263–270.\n14. M. Lindauer et al., “SMAC3: A versatile bayesian\noptimization package for hyperparameter\noptimization,” J. Mach. Learn. Res., vol. 23, no. 54,\npp. 1–9, Jan. 2022.\n15. Y. Liu et al., “RoBERTa: A robustly optimized BERT\npretraining approach,” 2019, arXiv:1907.11692.\n16. C. M. Bishop, Pattern Recognition and Machine\nLearning. New York, NY, USA: Springer-Verlag, 2006.\n17. D. P. Kingma and J. Ba, “Adam: A method for\nstochastic optimization,” 2015, arXiv:1412.6980.\n18. T. Mikolov et al., “Efﬁcient estimation of word\nrepresentations in vector space,” 2013.\n19. T. Mikolov et al., “Distributed representations of\nwords and phrases and their compositionality,” in\nProc. Adv. Neural Inf. Process. Syst., Lake Tahoe, NV,\nUSA: Curran Associates, Inc., 2013, pp. 3111–3119.\n20. B. Schuller et al., “The INTERSPEECH 2013\ncomputational paralinguistics challenge: Social signals,\nconﬂict, emotion, autism,” in Proc. 14th Annu. Conf. Int.\nSpeech Commun. Assoc. (ISCA), Lyon, France, 2013,\npp. 148–152, doi: 10.21437/Interspeech.2013-56.\n21. P. Good, Permutation Tests: A Practical Guide to\nResampling Methods for Testing Hypotheses. New\nYork, NY, USA: Springer-Verlag, 1994.\n22. R. Mao et al., “The biases of pre-trained language\nmodels: An empirical study on prompt-based\nsentiment analysis and emotion detection,” IEEE\nTrans. Affective Comput., early access, 2023,\ndoi: 10.1109/TAFFC.2022.3204972.\n23. E. Cambria et al., “SenticNet 7: A commonsense-\nbased neurosymbolic AI framework for explainable\nsentiment analysis,” in Proc. 13th Lang. Resour. Eval.\nConf. (LREC), 2022, pp. 3829–3839.\n24. N. Majumder et al., “Sentiment and sarcasm\nclassiﬁcation with multitask learning,” IEEE Intell.\nSyst., vol. 34, no. 3, pp. 38–43, Jul. 2019, doi: 10.1109/\nMIS.2019.2904691.\n25. M. Ge, R. Mao, and E. Cambria, “Explainable metaphor\nidentiﬁcation inspired by conceptual metaphor\ntheory,” in Proc. 36th AAAI Conf. Artif. Intell., 2022,\npp. 10,681–10,689, doi: 10.1609/aaai.v36i10.21313.\n26. W. Li et al., “SKIER: A symbolic knowledge integrated\nmodel for conversational emotion recognition,” in\nProc. 37th AAAI Conf. Artif. Intell., 2023.\nMOSTAFA M. AMIN is working toward his Ph.D. with the\nChair of Embedded Intelligence for Health Care and Well-\nbeing, University of Augsburg, 86159, Augsburg, Germany, and\nis a senior research data scientist at SyncPilot GmbH, 86156,\nAugsburg, Germany. His research interests include affective\ncomputing and audio and text analytics. Amin received his\n22                                                                                                                                                                                 \nM.Sc. degree in computer science from the University of Frei-\nburg, Germany. Contact him at mostafa.mohamed@unia.de.\nERIK CAMBRIA is an associate professor at Nanyang Techno-\nlogical University, 639798, Singapore. His research interests\ninclude neurosymbolic artificial intelligence for explainable nat-\nural language processing in domains like sentiment analysis,\ndialogue systems, and financial forecasting. Cambria received\nhis Ph.D. degree in computing science and mathematics\nthrough a joint program between the University of Stirling and\nthe Massachusetts Institute of Technology Media Lab. He is a\nFellow of IEEE. Contact him at cambria@ntu.edu.sg.\nBJ€ORN W. SCHULLER is a professor of artificial intelligence\nwith the Department of Computing, Imperial College London,\nSW7 2AZ, London, U.K., where he heads the Group on Lan-\nguage, Audio, and Music. He is also a full professor and the\nhead of the Chair of Embedded Intelligence for Health Care\nand Wellbeing with the University of Augsburg, 86159, Augs-\nburg, Germany and the founding chief executive officer/chief\nscientific officer of audEERING. Schuller received his doc-\ntoral degree in electrical engineering and information tech-\nnology from the Technical University of Munich. Contact him\nat schuller@IEEE.org.\n                                     23                                                                                                                                           "
}