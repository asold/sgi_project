{
  "title": "Exploiting Transformer-Based Multitask Learning for the Detection of Media Bias in News Articles",
  "url": "https://openalex.org/W4212772002",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4202210999",
      "name": "Spinde, Timo",
      "affiliations": [
        "University of Wuppertal"
      ]
    },
    {
      "id": "https://openalex.org/A4281502070",
      "name": "Krieger, Jan-David",
      "affiliations": [
        "University of Konstanz"
      ]
    },
    {
      "id": "https://openalex.org/A4223315846",
      "name": "Ruas, Terry",
      "affiliations": [
        "University of Wuppertal"
      ]
    },
    {
      "id": "https://openalex.org/A2743198558",
      "name": "Mitrović Jelena",
      "affiliations": [
        "University of Passau"
      ]
    },
    {
      "id": "https://openalex.org/A4288883795",
      "name": "Götz-Hahn, Franz",
      "affiliations": [
        "University of Kassel"
      ]
    },
    {
      "id": "https://openalex.org/A2733320184",
      "name": "Aizawa, Akiko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3162852365",
      "name": "Gipp, Bela",
      "affiliations": [
        "University of Wuppertal"
      ]
    },
    {
      "id": null,
      "name": "Mitrovi\\'c, Jelena",
      "affiliations": []
    },
    {
      "id": null,
      "name": "G\\\"otz-Hahn, Franz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2293583072",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2007178835",
    "https://openalex.org/W3122710292",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W3100201982",
    "https://openalex.org/W2132083787",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970879410",
    "https://openalex.org/W2798279345",
    "https://openalex.org/W2901215301",
    "https://openalex.org/W3096259383",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W3033129824",
    "https://openalex.org/W4232050619",
    "https://openalex.org/W4205331174",
    "https://openalex.org/W3046904879",
    "https://openalex.org/W3127474531",
    "https://openalex.org/W4200342516",
    "https://openalex.org/W4200373081",
    "https://openalex.org/W3213844504",
    "https://openalex.org/W3138270512",
    "https://openalex.org/W3130090378",
    "https://openalex.org/W4287179421",
    "https://openalex.org/W4200312743",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4287194945",
    "https://openalex.org/W2991203612",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3156491329",
    "https://openalex.org/W136732505",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2145451908",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3029492690",
    "https://openalex.org/W2624871570"
  ],
  "abstract": "Media has a substantial impact on the public perception of events. A\\none-sided or polarizing perspective on any topic is usually described as media\\nbias. One of the ways how bias in news articles can be introduced is by\\naltering word choice. Biased word choices are not always obvious, nor do they\\nexhibit high context-dependency. Hence, detecting bias is often difficult. We\\npropose a Transformer-based deep learning architecture trained via Multi-Task\\nLearning using six bias-related data sets to tackle the media bias detection\\nproblem. Our best-performing implementation achieves a macro $F_{1}$ of 0.776,\\na performance boost of 3\\\\% compared to our baseline, outperforming existing\\nmethods. Our results indicate Multi-Task Learning as a promising alternative to\\nimprove existing baseline models in identifying slanted reporting.\\n",
  "full_text": "Exploiting Transformer-based Multitask\nLearning for the Detection of Media Bias in\nNews Articles\nTimo Spinde1[0000−0003−3471−4127], Jan-David Krieger2[0000−0002−5360−2078],\nTerry Ruas1[0000−0002−9440−780X], Jelena Mitrovi´ c3[0000−0003−3220−8749], Franz\nG¨ otz-Hahn4[0000−0003−3465−5040], Akiko Aizawa5[0000−0001−6544−5076], and Bela\nGipp1[0000−0001−6522−3019]\n1 University of Wuppertal, Germany\n{firstname.lastname}@uni-wuppertal.de\n2 University of Konstanz, Germany\nJan-David.Krieger@uni-konstanz.de\n3 University of Passau, Germany\nInstitute for Artiﬁcial Intelligence Research and Development of Serbia\njelena.mitrovic@Uni-Passau.de\n4 University of Kassel, Germany\nfranz.goetz-hahn@uni-kassel.de\n5 NII Tokyo, Japan\naizawa@nii.ac.jp\nAbstract. Media has a substantial impact on the public perception of\nevents. A one-sided or polarizing perspective on any topic is usually de-\nscribed as media bias. One of the ways how bias in news articles can be\nintroduced is by altering word choice. Biased word choices are not always\nobvious, nor do they exhibit high context-dependency. Hence, detecting\nbias is often diﬃcult. We propose a Transformer-based deep learning ar-\nchitecture trained via Multi-Task Learning using six bias-related data\nsets to tackle the media bias detection problem. Our best-performing\nimplementation achieves a macro F1 of 0.776, a performance boost of\n3% compared to our baseline, outperforming existing methods. Our re-\nsults indicate Multi-Task Learning as a promising alternative to improve\nexisting baseline models in identifying slanted reporting.\nKeywords: Media Bias · Text Analysis · Multi-Task Learning · News\nAnalysis\n1 Introduction\nMedia bias, i.e., slanted news coverage, has the potential to drastically change\nthe public opinion on any topic [32]. One of the forms bias can be expressed by is\nby word choice, e.g., depicting any content in a non-neutral way [23]. Detecting\nand highlighting media bias might be relevant for media analysis and mitigate\nthe eﬀects of biased reporting. To date, only a few research projects focus on\narXiv:2211.03491v1  [cs.CL]  7 Nov 2022\n2 Authors Suppressed Due to Excessive Length\nthe detection and aggregation of bias [6, 16]. One of the reasons that make the\ncreation of automated methods to detect media bias a complex task is often the\nsubtle nature of media bias, which represents a challenge for quantitative iden-\ntiﬁcation methods [10, 16, 30, 33]. While many current research projects focus\non collecting linguistic features to describe media bias [11, 23, 29, 34], we pro-\npose a Transformer-based [39] architecture for the classiﬁcation of media bias.\nSimilar models have recently shown to achieve performance increases in the me-\ndia bias domain, e.g., sentence-level bias detection [6, 12, 27, 32]. However, so\nfar, they rely on very limited resources. Data sets with bias gold standard an-\nnotations are, to date, only scarcely available, and exhibit various weaknesses,\nsuch as low inter annotator agreement, small size, or no information about the\nannotator background [31,32,36]. Additionally, state-of-the-art neural language\nmodels usually require large amounts of training data to yield meaningful rep-\nresentations [9, 24], which are incompatible with the size of current media bias\ndata sets [10,35]. To mitigate the lack of suitable data sets, our model incorpo-\nrates Multi-Task Learning (MTL) [24], which allows for increasing performance\nby sharing model representations between related tasks [13, 19, 37]. The use of\ncross-domain data sets in our model is particularly relevant for the media bias\ndomain as multiple sources can provide a more robust model. To the best of our\nknowledge, the MTL paradigm has not been explored in existing work on me-\ndia bias. Our research question is therefore to assess whether MTL can improve\nmodels to classify media bias automatically.\nThe main contribution of this paper is to incorporate Transformer-based\nMTL into a system to identify sentence-level media bias automatically. We ex-\nploit MTL in the media bias context by computing multiple models based on\ndiﬀerent combinations of auxiliary training data sets (section 2). All our models,\ndata, and code are publicly available on https://bit.ly/3cmiQgB.\n2 Related Work\nWhile there are multiple forms of media bias, e.g., bias by personal perception\nor by the omission of information [28], our focus is on bias by word choice, in\nwhich diﬀerent words refer to the same concept [23]. We will ﬁrst summarize\navailable media bias data sets and then present automated methods to identify\nbias as well as MTL.\nThe concept of media bias is covered by many data sets [1, 6, 10, 18, 35].\nHowever, they all exhibit speciﬁc deﬁciencies, such as (1) a low number of top-\nics [16, 18], (2) no annotations on the word level [18], (3) low inter-annotator\nagreement [1,17,18,35], (4) no background check for its participants (except [35]),\nand (5) only article-level annotations [6]. Also, some related papers focus on\nframing rather than on bias [1, 10], or on Wikipedia instead of news [12], and\nresults are only partially transferable. To the best of our knowledge, the most\nextensive and precise data set was presented recently [32]. The data set consists\nof 3700 sentences annotated by expert raters on sentence-level with an inter-\nTransformer-based Multitask Learning on Media Bias 3\nannotator agreement of 0.40 measured by Krippendorﬀ’s α[15], which is higher\nthan for all other available data sets.\nSeveral studies tackle the challenge of identifying media bias automatically\n[6, 11, 12, 23, 34]. Most of them use hand-crafted features to detect bias [11, 34].\nFor example, [34] identify and evaluate a wide range of linguistic, lexical, and\nsyntactic features serving as potential bias indicators. The existing work on\nneural models is based on the data sets mentioned above, which exhibit the\ndescribed weaknesses [6, 12]. Most media bias models focus on sentence-level\nbias [6, 10–12, 23]. Therefore, we follow the standard practice and construct a\nsentence-level classiﬁer.\nMTL approaches have shown to be helpful when high-quality data sets in\nthe domain are scarce, but text corpora covering general related concepts are\navailable [13,19,37,38,40]. For example, [13] report that MTL applied on BERT\nyields an accuracy increase of 1.03% compared to the baseline BERT in a subjec-\ntivity detection task. MTL might be a suitable training paradigm for media bias\nidentiﬁcation systems since suﬃciently sized bias corpora with qualitative hand-\ncrafted annotations do not exist. Therefore, we propose the ﬁrst neural MTL\nmedia bias classiﬁer composed of inter-domain and cross-domain data sets.\n3 Methodology\nWe explore how ﬁne-tuning a language model via MTL can improve the per-\nformance in detecting media bias on the sentence level. Computational costs\nare an important consideration for us since we train multiple large-scale MTL\nmodels. For this reason, we employ a distilled modiﬁcation of BERT [9], called\nDistilBERT [26], which achieves a 40% reduction in size while simultaneously\naccelerating the training process by 60% and retaining 97% of language under-\nstanding capabilities on NLP benchmark tasks [40]. DistilBERT represents an\nappropriate architecture, keeping resource consumption and performance bal-\nanced. The incorporation of larger models trained via MTL is left to future\nresearch.\nOur MTL technique is based on hard parameter sharingin which all hidden\nmodel layers are shared between auxiliary training tasks [24]. Task-speciﬁc layers\nare added on top of the last hidden state, accounting for the label structure of\nauxiliary data sets. The MTL paradigm we propose is architecture-independent\nand can be adjusted to future NLP architectures.\nFor our training procedure, we distinguish between models trained on in-\ndomain and cross-domain data sets. For in-domain data sets, the creation process\nincluded concepts related to media bias, such as subjectivity [21]. Conversely,\ncross-domain data sets include data points that are not directly annotated for or\nrelated to media bias, but are retrieved from tasks that bear some connection to\nit. The auxiliary data sets we use comprise a diverse set of NLP tasks requiring\ntwo diﬀerent losses for the learning process – the Cross-Entropy (CE) loss [8]\nand the Mean Squared Error (MSE) loss [25]. The origin and number of the\ndata used for the training of our models, as well as their respective original\n4 Authors Suppressed Due to Excessive Length\ntasks and used loss functions, are shown in Table 1. We use in-domain (ID) and\ncross-domain (CD) data sets used in other MTL studies within the language\nprocessing domain [13,19,37].\nData set Domain n Task Loss Description\nReddit data set (Reddit) [4] ID 6861 Single Sentence Regression MSEReddit comments labeled on a continuousscale ranging from 0 (supportive) to 1 (dis-criminatory)\nSubjectivity data set (Subj) [21] ID 10000 Single Sentence Classiﬁcation CEMovie reviews labeled asobjectiveoropinion-ated\nIMDb [20] ID 50000 Single Sentence Classiﬁcation CE Movie reviews containing positiveand negative sentiment labels\nWikipedia data set (Wiki)1[22] ID 180000 Single Sentence Classiﬁcation CENeutral and biased sentence pairs from articlesgoing against Wikipedia’s NPOV rule\nSemantic Textual SimilarityBenchmark (STS-B) data set [5]CD 10943 Pairwise Sentence Similarity MSEMultilingual and cross-lingual sentence pairslabeled in terms of similarity\nStanford Natural Language In-ference (SNLI) corpus1[2] CD 570000 Pairwise Sentence Classiﬁcation CESentence pairs labeled for linguistic relationswithin the labelsentailment, neutral, orcon-tradiction\n1We only use 50000 text instances from these corpora in our MTL approach to keep\nthe size of training sets balanced.\nTable 1. Auxiliary data sets incorporated in the MTL models ( n = number of in-\nstances)\nﬁg. 1 outlines our in-domain MTL model consisting of DistilBERT’s encoder,\nwhose parameters are shared across tasks, and the added task-speciﬁc layers 6.\nThe represented model is based on the maximum number of possible data sets\nwithin the approach. In our experiments on MTL, we try various combinations,\nincluding at least three in-domain and ﬁve cross-domain data sets, respectively.\nFor preprocessing and MTL training, we took the same approach as [19].\nInitially, pre-trained parameters are loaded from huggingface 7. We split up data\nfor a ﬁxed-size subset of tasks into batches, and batches are merged and shuﬄed\nto guarantee the model does not train on too many subsequent batches of a\nsingle task. The preprocessing step is repeated every epoch. Batches are then\npassed on by the data loader one by one to the model, which outputs task-\nspeciﬁc predictions and the respective loss. Finally, the loss is backpropagated,\nand parameters are updated.\n4 Experiments\nTo investigate the beneﬁt of MTL to identify media bias on a ﬁne-grained lin-\nguistic level, we train ten models using MTL, which we compare to ﬁve baseline\nmodels. As a consequence of a lack of robust guidelines for selection criteria for\n6The cross-domain model is not shown due to lack of space but is published at the\nrepository mentioned in Section section 1.\n7https://huggingface.co/transformers/model doc/distilbert.html\nTransformer-based Multitask Learning on Media Bias 5\nFig. 1.Outline of in-domain MTL model consisting of a shared encoder block and\ntask-speciﬁc layers. Note: We implement multiple MTL models based on diﬀerent com-\nbinations of the presented data sets.\nauxiliary corpora, we choose a variety of auxiliary tasks to ﬁne-tune the Dis-\ntilBERT model via MTL that have previously been used successfully in MTL\nstudies [13,19,37]. Each of our MTL models is trained using a diﬀerent combina-\ntion of a sample of six popular data sets, where IMDb [20], Subj [21], Wiki [22],\nand Reddit [4] are considered in-domain data sets, and STS-B [5], and SNLI [2]\ncomprise examples of cross-domain data sets 8.\nThe in-domain models are based on bias-related data sets 9. Combining the\nin-domain corpora yields ﬁve diﬀerent models (table 2, M1 - M5): four use triple\ncombinations, and one model relies on all in-domain data sets. The cross-domain\nmodels extend the pool of experiments by adding the STS-B and SNLI data sets\nto each of the ﬁve in-domain models (table 2, M6 - M10). The approaches are\noriented on the MTL ﬁne-tuning approaches applied in [37]. In their experi-\nments based on BERT, the authors apply MTL on domain-related and domain-\nunrelated data yielding a performance boost for sentiment classiﬁcation.\nAll experiments are performed on a Google Colab NVIDIA Tesla K8010.\nWe choose the AdamW optimizer [14] and a batch size of 32 . All downstream\ntask layers are based on a hidden state dimensionality of 768. All performance\nmetrics are calculated based on 5-fold cross-validation [3]. Thus, we divide the\n8A detailed description of the data sets is published at the repository mentioned in\nSection section 1.\n9IMDb, Subj, Wiki, Reddit\n10https://colab.research.google.com/notebooks/intro.ipynb\n6 Authors Suppressed Due to Excessive Length\nﬁnal bias data set containing 1700 instances into ﬁve diﬀerent train and tests 11.\nThe models are then iteratively trained on all ﬁve training sets and evaluated on\nthe respective held-out test set. Finally, the performance metrics on the test sets\nare averaged, yielding the cross-validated model performance. Each respective\nmodel is trained over four epochs with an early stopping criterion based on\nvalidation CE loss. In many cases, the model stops learning after two epochs.\nThe MTL ﬁne-tuning is based on a learning rate of 5 · 10−5.\nAs far as we know, there are no related works applying MTL in the media bias\ndomain. Therefore, we compare the performances of our MTL approaches to a set\nof baseline models (table 2; B1-B5). We report the performance scores achieved\nfrom pre-trained DistilBERT provided by huggingface (B1). Furthermore, we\ntrain four DistilBERT models on each of the in-domain data sets (B2-B5). Thus,\nwe can observe whether the assumed performance boost of our MTL models\nresults from MTL rather than domain-relatedness of the training data.\nWe expect that ﬁne-tuning via MTL leads to an improvement of DistilBERT’s\nbias identiﬁcation power. Mainly, we want to analyze whether the MTL technique\nyields a substantial performance boost compared to simple Transfer Learning\n(TL) approaches training the model on only a single data set. Therefore, we run\nseveral experiments.\n5 Results and Discussion\nWe show the performance indicators of our model on our expert-labeled media\nbias data set in table 2, according to F1, precision, recall, and loss. Since the\nhighest macro F1 score does not necessarily match with the lowest loss, we\nelaborate on the results from the perspective of both metrics.\nAmong all MTL-trained models the highest F1 score is achieved from the\nin-domain M4 model with 0.776. The best cross-domain model regarding macro\nF1 is reached by M8 with 0.771. Compared to DistilBERT, M4 achieves a 3%\nincrease in macro F1, while B5 achieves the highest macro F1 for TL-based\nmodels at 0.782, which is not surpassed by any MTL approach. Although all\nMTL models outperform DistilBERT, the highest macro F1 score of all MTL\nmodels is 0.6% lower than that of B5. Overall, MTL improves the B1 baseline\nmacro F1 score in a range from 0.3% (M9) to 3% (M4). When considering the\nmodels from a loss-based perspective, the performance ranks change slightly:\nM4 remains as the best in-domain MTL model, but M7 (the second to last\nin terms of macro F1 performance) reaches the lowest loss within the cross-\ndomain approaches. Compared to DistilBERT, M4 shows a decrease in loss of\n4.9%. B5 prevails as the best TL model with a CE loss of 0.466. In contrast to\nthe macro F1-based perspective, however, M4 achieves the lowest overall loss,\noutperforming B5 by 0.2%.\nIn general, our MTL approaches surpass the baseline methods. However,\nthe best overall model based on macro F1 was a TL model trained on a data\n11We use a subset of BABE [32], introduced in section 2, to evaluate the MTL\nmodels.\nTransformer-based Multitask Learning on Media Bias 7\nModel Data sets macro F1 microF1 binaryF1 Precision Recall CE Loss\nB1 huggingfaceDistilBERT 0.746 0.750 0.711 0.805 0.640 0.513\nTL\nB2 \u0013 0.744 0.744 0.730 0.744 0.716 0.545\nB3 \u0013 0.761 0.762 0.746 0.770 0.725 0.491\nB4 \u0013 0.743 0.746 0.709 0.790 0.646 0.497\nB5 \u0013 0.782 0.782 0.7695 0.785 0.754 0.466\nID MTL\nM1 \u0013 \u0013 \u0013 0.768 0.768 0.753 0.778 0.731 0.482\nM2 \u0013 \u0013 \u0013 0.760 0.760 0.746 0.766 0.729 0.495\nM3 \u0013 \u0013 \u0013 0.773 0.774 0.762 0.777 0.755 0.482\nM4 \u0013 \u0013 \u0013 0.776 0.777 0.759 0.794 0.727 0.464\nM5 \u0013 \u0013 \u0013 \u0013 0.772 0.771 0.757 0.778 0.737 0.473\nCD MTL\nM6 \u0013 \u0013 \u0013 \u0013 \u0013 0.766 0.766 0.758 0.756 0.763 0.492\nM7 \u0013 \u0013 \u0013 \u0013 \u0013 0.765 0.765 0.751 0.770 0.735 0.474\nM8 \u0013 \u0013 \u0013 \u0013 \u0013 0.771 0.771 0.762 0.765 0.761 0.491\nM9 \u0013 \u0013 \u0013 \u0013 \u0013 0.749 0.750 0.759 0.714 0.812 0.499\nM10 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 0.769 0.770 0.751 0.789 0.720 0.480\nSubj IMDb Reddit Wiki STS SNLI\nTable 2.Results for all baseline models, i.e., thehuggingfacemodel or models obtained by TL, as well as\nthe models trained using MTL considering only in-domain data sets or also incorporating cross-domain data.\nFor each metric we have denoted the best performance in bold.\nset containing revised Wikipedia excerpts (B5). Based on CE loss, only one\nMTL model slightly outperform this TL model. Thus, we cannot state whether\nTransformer-Based MTL improves media bias detection on the sentence level.\nWe assume that the strong performance of B5 results from the relatedness of\nthe underlying data set to our bias corpus. The Wikipedia data set contains\nbiased and neutral sentences extracted from revised Wikipedia passages. Hence,\nthe data set is similar to our bias corpus 12. The only diﬀerence to our ﬁne-\ntuning data set is the source from which the data is extracted. Pre-training a\nTransformer-based model on a highly bias-related corpus seems to hinder MTL’s\nrelative performance improvement. Furthermore, we assume that our selection of\nauxiliary data sets might not have been suﬃciently comprehensive. In our MTL\napproaches, updating DistilBERT’s parameters only required the computation\nand back-propagation of binary CE loss and MSE loss. [24] argues that well-\nperforming MTL approaches must be trained on NLP tasks, including multiple\nloss functions.\nExisting MTL studies [13, 19, 37] do not report diverse TL baseline models.\nThe MTL approaches are primarily compared to a pre-trained baseline model\nprovided by model libraries. Future research should incorporate a comprehensive\nset of baseline models allowing for a more robust analysis. Comparing our best\nMTL model to DistilBERT, the eﬀect of MTL is similar as in [13].\nConsidering our MTL-based media bias research, future work should include\nmore comprehensive sets of bias-related auxiliary data sets with multiple loss\nfunctions. Possible tasks could, for example, comprise the detection of bias-\ninducing linguistic features such as negative sentiment [34]. In this way, deep\n12Let us point out that none of the instances from the Wikipedia data set are\ncontained in our target media bias data set.\n8 Authors Suppressed Due to Excessive Length\nlearning techniques could beneﬁt from other types of tasks, such as classifying\nlinguistic features. Moreover, future MTL approaches could beneﬁt from larger\ntransformer models (e.g., XLNet [41], ELECTRA [7]). Our approach based on\nDistilBERT is the ﬁrst step towards balancing cloud-computing costs and per-\nformance. We note that a follow-up experiment about an improved model and\na larger exploratory data analysis are already in progress and will be published\nin the future.\n6 Final Considerations\nThis work proposes a Transformer-based MTL approach to identify media bias\nby word choice in news articles. The motivation for selecting the training tech-\nnique results from our observation that the size of available media bias data sets\nis not compatible with the requirements of state-of-the-art neural language mod-\nels. We train ten MTL models based on diﬀerent combinations of six auxiliary\ndata sets and compare them to ﬁve baseline models. Our results show that the\nbest performing MTL model partly surpasses the baseline models in terms of\nmacro F1 loss and CE loss. Yet, we can not ascertain a signiﬁcant superiority of\nthe MTL approach in classifying media bias instances. The main limitation of\nour work is the restricted inclusion of auxiliary tasks. In future work, we plan\nto incorporate more tasks based on bias-inducing linguistic features. We have\nto emphasize that any successful MTL implementation in the context of media\nbias identiﬁcation could decrease ﬁnancial burdens emerging from the collection\nof hand-crafted training data. Yet, at the same time, cloud computing requires\nsubstantial ﬁnancial resources. Costs of using larger models should therefore\nbe properly evaluated. We believe the MTL approach to be promising in the\narea and aim to continue the research on MTL in connection with media bias\nidentiﬁcation in the future.\nReferences\n1. Baumer, E., Elovic, E., Qin, Y., Polletta, F., Gay, G.: Testing and comparing com-\nputational approaches for identifying the language of framing in political news. In:\nProceedings of the 2015 Conference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies. pp. 1472–\n1482. Association for Computational Linguistics, Denver, Colorado (May–Jun\n2015). https://doi.org/10.3115/v1/N15-1171, https://www.aclweb.org/anthology/\nN15-1171\n2. Bowman, S.R., Angeli, G., Potts, C., Manning, C.D.: A large anno-\ntated corpus for learning natural language inference. In: Proceedings of\nthe 2015 Conference on Empirical Methods in Natural Language Process-\ning. pp. 632–642. Association for Computational Linguistics, Lisbon, Portu-\ngal (Sep 2015). https://doi.org/10.18653/v1/D15-1075, https://www.aclweb.org/\nanthology/D15-1075\n3. Browne, M.W.: Cross-validation methods. J. Math. Psychol. 44(1), 108–132\n(Mar 2000). https://doi.org/10.1006/jmps.1999.1279, https://doi.org/10.1006/\njmps.1999.1279\nTransformer-based Multitask Learning on Media Bias 9\n4. Cabot, P.H., Abadi, D., Fischer, A., Shutova, E.: Us vs. them: A dataset of populist\nattitudes, news bias and emotions. CoRR abs/2101.11956 (2021), https://arxiv.\norg/abs/2101.11956\n5. Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., Specia, L.: Semeval-2017 task\n1: Semantic textual similarity multilingual and crosslingual focused evaluation.\nProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-\n2017) (2017). https://doi.org/10.18653/v1/s17-2001, http://dx.doi.org/10.18653/\nv1/S17-2001\n6. Chen, W.F., Al Khatib, K., Wachsmuth, H., Stein, B.: Analyzing Political Bias\nand Unfairness in News Articles at Diﬀerent Levels of Granularity. In: Pro-\nceedings of the Fourth Workshop on Natural Language Processing and Com-\nputational Social Science. pp. 149–154. Association for Computational Linguis-\ntics, Online (2020). https://doi.org/10.18653/v1/2020.nlpcss-1.16, https://www.\naclweb.org/anthology/2020.nlpcss-1.16\n7. Clark, K., Luong, M.T., Le, Q.V., Manning, C.D.: ELECTRA: Pre-training Text\nEncoders as Discriminators Rather Than Generators. arXiv:2003.10555 [cs] (Mar\n2020)\n8. De Boer, P.T., Kroese, D.P., Mannor, S., Rubinstein, R.Y.: A tutorial on the cross-\nentropy method. Annals of operations research 134(1), 19–67 (2005)\n9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers). pp. 4171–4186. Association for Computational Linguistics, Minneapo-\nlis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://www.\naclweb.org/anthology/N19-1423\n10. Fan, L., White, M., Sharma, E., Su, R., Choubey, P.K., Huang, R., Wang, L.: In\nplain sight: Media bias through the lens of factual reporting. In: Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP). pp. 6343–6349. Association for Computational Linguistics, Hong Kong,\nChina (Nov 2019). https://doi.org/10.18653/v1/D19-1664, https://www.aclweb.\norg/anthology/D19-1664\n11. Hube, C., Fetahu, B.: Detecting biased statements in wikipedia. In: Compan-\nion Proceedings of the The Web Conference 2018. p. 1779–1786. WWW ’18,\nInternational World Wide Web Conferences Steering Committee, Republic and\nCanton of Geneva, CHE (2018). https://doi.org/10.1145/3184558.3191640, https:\n//doi.org/10.1145/3184558.3191640\n12. Hube, C., Fetahu, B.: Neural based statement classiﬁcation for biased language.\nIn: Proceedings of the Twelfth ACM International Conference on Web Search\nand Data Mining. p. 195–203. WSDM ’19, Association for Computing Machin-\nery, New York, NY, USA (2019). https://doi.org/10.1145/3289600.3291018, https:\n//doi.org/10.1145/3289600.3291018\n13. Huo, H., Iwaihara, M.: Utilizing bert pretrained models with various ﬁne-tune\nmethods for subjectivity detection. In: Asia-Paciﬁc Web (APWeb) and Web-Age\nInformation Management (WAIM) Joint International Conference on Web and Big\nData. pp. 270–284. Springer (2020)\n14. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio,\nY., LeCun, Y. (eds.) 3rd International Conference on Learning Representations,\n10 Authors Suppressed Due to Excessive Length\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings\n(2015), http://arxiv.org/abs/1412.6980\n15. Krippendorﬀ, K.: Computing krippendorﬀ’s alpha-reliability. Departmental Pa-\npers (ASC); University of Pennsylvania (2011), https://repository.upenn.edu/cgi/\nviewcontent.cgi?article=1043&context=asc papers\n16. Lim, S., Jatowt, A., F¨ arber, M., Yoshikawa, M.: Annotating and analyzing bi-\nased sentences in news articles using crowdsourcing. In: Proceedings of the 12th\nLanguage Resources and Evaluation Conference. pp. 1478–1484. European Lan-\nguage Resources Association, Marseille, France (May 2020), https://www.aclweb.\norg/anthology/2020.lrec-1.184\n17. Lim, S., Jatowt, A., F¨ arber, M., Yoshikawa, M.: Annotating and analyzing bi-\nased sentences in news articles using crowdsourcing. In: Proceedings of the 12th\nLanguage Resources and Evaluation Conference. pp. 1478–1484. European Lan-\nguage Resources Association, Marseille, France (May 2020), https://www.aclweb.\norg/anthology/2020.lrec-1.184\n18. Lim, Sora and Jatowt, Adam and Yoshikawa, Masatoshi: Understanding Char-\nacteristics of Biased Sentences in News Articles. In: CIKM Workshops (2018),\n{http://ceur-ws.org/Vol-2482/paper13.pdf}\n19. Liu, X., He, P., Chen, W., Gao, J.: Multi-task deep neural networks for natural\nlanguage understanding. In: Proceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics. pp. 4487–4496. Association for Computa-\ntional Linguistics, Florence, Italy (Jul 2019), https://www.aclweb.org/anthology/\nP19-1441\n20. Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C.: Learning\nword vectors for sentiment analysis. In: Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies.\npp. 142–150. Association for Computational Linguistics, Portland, Oregon, USA\n(Jun 2011), https://www.aclweb.org/anthology/P11-1015\n21. Pang, B., Lee, L.: A sentimental education: Sentiment analysis using sub-\njectivity summarization based on minimum cuts. In: Proceedings of the\n42nd Annual Meeting on Association for Computational Linguistics. p.\n271–es. ACL ’04, Association for Computational Linguistics, USA (2004).\nhttps://doi.org/10.3115/1218955.1218990, https://doi.org/10.3115/1218955.\n1218990\n22. Pryzant, R., Diehl Martinez, R., Dass, N., Kurohashi, S., Jurafsky, D.,\nYang, D.: Automatically neutralizing subjective bias in text. Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence 34(01), 480–489 (Apr 2020).\nhttps://doi.org/10.1609/aaai.v34i01.5385, https://ojs.aaai.org/index.php/AAAI/\narticle/view/5385\n23. Recasens, M., Danescu-Niculescu-Mizil, C., Jurafsky, D.: Linguistic models for an-\nalyzing and detecting biased language. In: Proceedings of the 51st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers). pp.\n1650–1659 (2013), https://www.aclweb.org/anthology/P13-1162.pdf\n24. Ruder, S.: An overview of multi-task learning in deep neural networks. CoRR\nabs/1706.05098 (2017), http://arxiv.org/abs/1706.05098\n25. Sammut, C., Webb, G.I. (eds.): Mean Squared Error, pp. 653–653. Springer US,\nBoston, MA (2010). https://doi.org/10.1007/978-0-387-30164-8 528, \\url{https://\ndoi.org/10.1007/978-0-387-30164-8 528}\n26. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of\nBERT: smaller, faster, cheaper and lighter. CoRR abs/1910.01108 (2019), http:\n//arxiv.org/abs/1910.01108\nTransformer-based Multitask Learning on Media Bias 11\n27. Spinde, T.: An interdisciplinary approach for the automated detection and visu-\nalization of media bias in news articles. In: 2021 IEEE International Conference\non Data Mining Workshops (ICDMW) (2021), https://media-bias-research.org/\nwp-content/uploads/2021/09/Spinde2021g.pdf\n28. Spinde, T., Hamborg, F., Donnay, K., Becerra, A., Gipp, B.: En-\nabling news consumers to view and understand biased news coverage:\nA study on the perception and visualization of media bias. In: Pro-\nceedings of the ACM/IEEE Joint Conference on Digital Libraries in\n2020. p. 389–392. JCDL ’20, Association for Computing Machinery,\nVirtual Event, China (2020). https://doi.org/10.1145/3383583.3398619,\nhttps://doi.org/10.1145/3383583.3398619\n29. Spinde, T., Hamborg, F., Gipp, B.: Media bias in german news articles: A combined\napproach. ECML PKDD 2020 Workshops: Workshops of the European Conference\non Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2020):\nINRA 2020, Ghent, Belgium, September 14–18, 2020, Proceedings 1323, 581–590\n(2020). https://doi.org/10.1007/978-3-030-65965-3 41, https://www.ncbi.nlm.nih.\ngov/pmc/articles/PMC7850083/\n30. Spinde, T., Kreuter, C., Gaissmaier, W., Hamborg, F., Gipp, B., Giese, H.: Do You\nThink It’s Biased? How To Ask For The Perception Of Media Bias. In: Proceedings\nof the ACM/IEEE Joint Conference on Digital Libraries (JCDL) (Sep 2021)\n31. Spinde, T., Krieger, D., Plank, M., Gipp, B.: Towards A Reliable Ground-Truth For\nBiased Language Detection. In: Proceedings of the ACM/IEEE Joint Conference\non Digital Libraries (JCDL) (Sep 2021)\n32. Spinde, T., Plank, M., Krieger, J.D., Ruas, T., Gipp, B., Aizawa, A.: Neural Media\nBias Detection Using Distant Supervision With BABE - Bias Annotations By\nExperts. In: Findings of the Association for Computational Linguistics: EMNLP\n2021. Dominican Republic (Nov 2021)\n33. Spinde, T., Rudnitckaia, L., Hamborg, F., Gipp, B.: Identiﬁcation of biased terms\nin news articles by comparison of outlet-speciﬁc word embeddings. In: Proceedings\nof the iConference 2021 (March 2021)\n34. Spinde, T., Rudnitckaia, L., Mitrovi´ c, J., Hamborg, F., Granitzer, M., Gipp, B.,\nDonnay, K.: Automated identiﬁcation of bias inducing words in news articles using\nlinguistic and context-oriented features. Information Processing & Management\n58(3), 102505 (2021). https://doi.org/10.1016/j.ipm.2021.102505, https://doi.org/\n10.1016/j.ipm.2021.102505\n35. Spinde, T., Rudnitckaia, L., Sinha, K., Hamborg, F., Gipp, B., Don-\nnay, K.: MBIC – A media bias annotation dataset including annotator\ncharacteristics. In: Proceedings of the iConference 2021. iSchools (2021).\nhttps://doi.org/10.5281/zenodo.4474336\n36. Spinde, T., Sinha, K., Meuschke, N., Gipp, B.: TASSY - A Text Annotation Survey\nSystem. In: Proceedings of the ACM/IEEE Joint Conference on Digital Libraries\n(JCDL) (Sep 2021)\n37. Sun, C., Qiu, X., Xu, Y., Huang, X.: How to ﬁne-tune bert for text classiﬁcation?\nIn: Sun, M., Huang, X., Ji, H., Liu, Z., Liu, Y. (eds.) Chinese Computational\nLinguistics. pp. 194–206. Springer International Publishing, Cham (2019)\n38. Sun, Y., Wang, S., Li, Y.K., Feng, S., Tian, H., Wu, H., Wang, H.: Ernie\n2.0: A continual pre-training framework for language understanding. Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence 34, 8968–8975 (04 2020).\nhttps://doi.org/10.1609/aaai.v34i05.6428\n12 Authors Suppressed Due to Excessive Length\n39. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I.,\nLuxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Gar-\nnett, R. (eds.) Advances in Neural Information Processing Systems. vol. 30.\nCurran Associates, Inc. (2017), https://proceedings.neurips.cc/paper/2017/ﬁle/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n40. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: Glue: A multi-\ntask benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461 (2018)\n41. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q.V.:\nXLNet: Generalized Autoregressive Pretraining for Language Understanding.\narXiv:1906.08237 [cs] (Jun 2019)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.855485200881958
    },
    {
      "name": "Transformer",
      "score": 0.6622892022132874
    },
    {
      "name": "Multi-task learning",
      "score": 0.5572108030319214
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4659024178981781
    },
    {
      "name": "Machine learning",
      "score": 0.3794479966163635
    },
    {
      "name": "Natural language processing",
      "score": 0.376914918422699
    },
    {
      "name": "Electrical engineering",
      "score": 0.1601462960243225
    },
    {
      "name": "Voltage",
      "score": 0.09791859984397888
    },
    {
      "name": "Task (project management)",
      "score": 0.04993855953216553
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}