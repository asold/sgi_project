{
  "title": "Teaching Large Language Models to Translate with Comparison",
  "url": "https://openalex.org/W4393152836",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2344356506",
      "name": "Jiali Zeng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2133392087",
      "name": "Fandong Meng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2890699089",
      "name": "Yongjing Yin",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2344356506",
      "name": "Jiali Zeng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2133392087",
      "name": "Fandong Meng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2890699089",
      "name": "Yongjing Yin",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4310831983",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W4319323306",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W4385565879",
    "https://openalex.org/W4376312026",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W6776094423",
    "https://openalex.org/W6782465632",
    "https://openalex.org/W2970059135",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W6810898478",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W4388512539",
    "https://openalex.org/W2542860122",
    "https://openalex.org/W4381711094",
    "https://openalex.org/W3200230461",
    "https://openalex.org/W3095962368",
    "https://openalex.org/W4364387438",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W3173343821",
    "https://openalex.org/W4385571264",
    "https://openalex.org/W4225116116",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging to tune smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves output comparison and preference comparison, presenting the model with carefully designed examples of correct and incorrect translations and an additional preference loss for better regularization. Empirical evaluation on four language directions of WMT2022 and FLORES-200 benchmarks shows the superiority of our proposed method over existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a promising solution for generating high-quality translations. Please refer to Github for more details: https://github.com/lemon0830/TIM.",
  "full_text": "Teaching Large Language Models to Translate with Comparison\nJiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou\nPattern Recognition Center, WeChat AI, Tencent Inc\n{lemonzeng,fandongmeng,yongjingyin,withtomzhou}@tencent.com\nAbstract\nOpen-sourced large language models (LLMs) have demon-\nstrated remarkable efficacy in various tasks with instruction\ntuning. However, these models can sometimes struggle with\ntasks that require more specialized knowledge such as trans-\nlation. One possible reason for such deficiency is that in-\nstruction tuning aims to generate fluent and coherent text that\ncontinues from a given instruction without being constrained\nby any task-specific requirements. Moreover, it can be more\nchallenging to tune smaller LLMs with lower-quality train-\ning data. To address this issue, we propose a novel frame-\nwork using examples in comparison to teach LLMs to learn\ntranslation. Our approach involves output comparison and\npreference comparison, presenting the model with carefully\ndesigned examples of correct and incorrect translations and\nan additional preference loss for better regularization. Em-\npirical evaluation on four language directions of WMT2022\nand FLORES-200 benchmarks shows the superiority of our\nproposed method over existing methods. Our findings of-\nfer a new perspective on fine-tuning LLMs for translation\ntasks and provide a promising solution for generating high-\nquality translations. Please refer to Github for more details:\nhttps://github.com/lemon0830/TIM.\nIntroduction\nGenerative large language models, like GPT models, have\nshown impressive performance in various NLP tasks (Brown\net al. 2020; Ouyang et al. 2022), including machine transla-\ntion (Hendy et al. 2023; Zhu et al. 2023), which opens up\nnew possibilities for building more effective translation sys-\ntems. It is impractical to deploy such large models for the\ntranslation task only, and using or tuning open-sourced gen-\nerative language models has become an attractive research\ndirection. In this regard, researchers have explored strate-\ngies for example selection and instruction design through\nIn-Context Learning (ICL) (Lin et al. 2022; Agrawal et al.\n2022). However, evaluations of open-sourced LLMs show\nthat they do not perform as well as strong multilingual su-\npervised baselines in most translation directions (Zhu et al.\n2023). Additionally, ICL can increase decoding latency due\nto longer context. Based on these observations, researchers\nsuggest tuning relatively small LLMs for translation with a\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nfew high-quality supervised instructions (Hendy et al. 2023;\nZeng et al. 2023; Jiao et al. 2023).\nInstruction tuning is an efficient method for making LLMs\nbetter aligned to the task descriptions preferred by humans\n(Stiennon et al. 2020; Ouyang et al. 2022; Chung et al. 2022;\nWang et al. 2023). The only requirement is to collect task-\nspecific data, on which LLMs will be fine-tuned with the lan-\nguage modeling loss. However, optimizing for simple next-\ntoken prediction loss will cause models to overlook context\ninformation, especially for low-capacity models. It is serious\nfor the tasks in which the specialized knowledge in context is\nnecessary for task completion (e.g., translation), and ignor-\ning such knowledge on translation can lead to inadequacy\nand hallucination. Therefore, there is a need to investigate\nthe limitations of LLMs and explore methods for improving\ntheir performance in specialized tasks.\nIn this paper, we propose to Teach the language models\nto learn translation wIth examples in coMparison, named\nTIM, aiming to make full use of a small amount of high-\nquality translation data. Based on the training data, we fur-\nther construct two kinds of comparisons: output comparison\nand preference comparison. Output comparison is used to\nlearn responses to different instructions for the same input.\nPreference comparison is used to maximize the gap between\ncorrect and incorrect translations. Specifically, to help iden-\ntify specific areas where the model may be making errors,\nwe introduce an additional preference loss, which is origi-\nnally used to learn reward models (Stiennon et al. 2020), as\nregularization to penalize unexpected outputs.\nWe evaluate our proposed method on WMT22 and\nFLORES-200 test sets (EN‚áîDE, EN‚áîZH), and the im-\nprovement over the baselines shows the effectiveness of our\nmethod. Our model shows better zero-shot translation per-\nformance and stability in prompt choice. Moreover, the per-\nformance of the models tuned by our TIM increases as the\nmodel size increases, with the improvement being more pro-\nnounced in the case of smaller models. In particular, the\ntuned LLaMA-2-13B (Touvron et al. 2023a) achieves top\n1 on quality estimation without references in the EN‚áîDE,\noutperforming the dedicated models for quality estimation.\nMethod\nIn brief, we tune generative language models to learn trans-\nlation with output comparison and preference comparison in\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19488\nDictionary-guided Data\nExample 1\nInstruction:\nInput:\nOutput:\nTranslate from Chinese to English.\nInstruction:\nInput:\nOutput:\nTranslate from Chinese to English.\nExample 2\nÂõΩÊúâ‰ºÅ‰∏öÂíå‰ºòÂäøÊ∞ëËê•‰ºÅ‰∏öËµ∞ËøõËµ£ÂçóÈù©ÂëΩËÄÅÂå∫„ÄÇ\\n\\n\n### Note: ‰ºÅ‰∏ö means enterprises . Èù©ÂëΩ means\nrevolution. Ëµ£Âçó means south Jiangxi.\nState -owned enterprises and advantageous private\nenterprises entered the revolutionary basearea of south\nJiangxi.\nÂõΩÊúâ‰ºÅ‰∏öÂíå‰ºòÂäøÊ∞ëËê•‰ºÅ‰∏öËµ∞ËøõËµ£ÂçóÈù©ÂëΩËÄÅÂå∫„ÄÇ\\n\\n\n### Note: ‰ºÅ‰∏ö means enterprises . Èù©ÂëΩ means\nrevolutionary. Ëµ£Âçó means Gannan.\nState-owned enterprises and advantageous private\nenterprises entered the old revolutionary area of\nGannan.\nOrder-guided Data\nExample 1 [Standard Translation]\nInstruction:\nInput:\nOutput:\nTranslate from Chinese to English.\nInstruction:\nInput:\nOutput:\nTranslate from Chinese to English.\nExample 2\nÂõΩÊúâ‰ºÅ‰∏öÂíå‰ºòÂäøÊ∞ëËê•‰ºÅ‰∏öËµ∞ËøõËµ£ÂçóÈù©ÂëΩËÄÅÂå∫„ÄÇ\\n\\n\nState-owned enterprises and advantageous private \nenterprises entered the revolutionary base area of south \nJiangxi.\nÂõΩÊúâ‰ºÅ‰∏öÂíå‰ºòÂäøÊ∞ëËê•‰ºÅ‰∏öËµ∞ËøõËµ£ÂçóÈù©ÂëΩËÄÅÂå∫„ÄÇ\\n\\n\n### Note: A translation thatis generated by reversing\nthe order of the original sentence.\nJiangxi. south of area base revolutionary the entered \nenterprises private advantageous and enterprises State-\nowned\nFigure 1: Illustration of two types of output comparison.\nThe text in blue highlights the difference between the added\nnotes and the resulting difference due to these specific notes.\nthe instruction tuning framework. First, we will give a for-\nmal introduction to instruction tuning. Then, we present the\ndetails of two kinds of comparisons of our method consist-\ning of output comparison and preference comparison and an\nadditional preference learning loss. Finally, we explore dif-\nferent approaches to parameter tuning.\nBackground: Instruction Tuning\nInstruction tuning aims to enhance the capacity of language\nmodels to handle instructions in natural languages. The con-\ncept is that the models can be trained to execute tasks spec-\nified in instructions, which would enable them to compre-\nhend the tasks and even process tasks not encountered be-\nfore.\nGenerally, each instance of instruction-following data\nstarts with ‚Äúinstructions‚Äù c describing a task, and a corre-\nsponding output y indicating the answer to the instruction.\nThe ‚Äúinput‚Äù x, the optional context or input for the task,\nis not necessary but is required for the machine translation\ntask. Given the instruction data, the language models are\noptimized by minimizing the negative log-likelihood of the\noutput y:\nLlm = ‚àí 1\n|y|\n|y|X\ni\nlogp(yi|c, x). (1)\nNotably, the objective is the same as that used in pretraining.\nOutput Comparison\nAn important ingredient of our method is the construction\nof samples used to provide comparison signals for model\nlearning. In addition to regular translation data, we construct\ndata used for comparison by introducing sequence ordering,\ndictionary information, or translation errors.\nOrder-guided data. We introduce a variation of the trans-\nlation process, and we reverse the translations of certain ex-\namples and provide an accompanying note indicating the\nreverse generation order (Order-guided Data in Figure 1).\nBy training on these reverse sentences, the model gains the\nability to capture dependencies that may not be evident in\nthe original sentence order. This helps improve the model‚Äôs\ncomprehension of instructions and enhances its capability to\ngenerate coherent and contextually appropriate translations.\nDictionary-guided Data. To make the model aware of the\nunderlying reasons for different translations, we inform the\nmodel of different correct outputs with the help of bilingual\ndictionaries1. Instead of synthesizing the comparison data,\nwe utilize an existing multi-reference corpus. By looking up\nthe bilingual dictionary, we establish word alignments be-\ntween a single source sentence and multiple references. The\nword alignments serve as annotations appended to the input.\nIllustrated in Figure 1, the notes contain distinct word align-\nments, and the outputs of Example 1and Example 2differ\ndespite the same input sentences.\nError-guided Data. We introduce translations with error\nannotations inspired by Jiao et al. (2023). The added notes\nindicate no mistakes in the references for correct input-\noutput pairs, while the notes of incorrect input-output pairs\nindicate detailed translation errors. As shown in the right\npart of Figure 3, the translation of Example 1 has a ma-\njor locale convention format mistake, corresponding to the\nadded note.\nPreference Comparison\nIn preference comparison, we assign contrastive outputs for\neach data type, denoted as Bad Output, and train the model\nwith an extra preference loss. As illustrated in Figure 3, we\npropose two types of the Bad Output: 1) Noisy-based, in\nwhich we intentionally introduce noise into the original out-\nput by randomly deleting words or swapping the positions of\ntwo words; 2) LM-based, in which we fine-tune a relatively\nsmall LM (e.g., BLOOM-1b7) and generate output using a\nsimple sampling strategy for each instance. With examples\nof correct and incorrect translations, the model is optimized\nto distinguish higher-quality translations, which can reduce\nthe resource requirement for training.\n1https://github.com/facebookresearch/MUSE\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19489\nùêø!\"\nD E F\nTraining Prompt\nWrite a response that appropriately \ncompletes the request. \\n\\n ### \nRequest: \\n {instruction} \\n \n{input} \n\\n\\n ### Response:\n Training Prompt\n Bad Output\nTraining Prompt\n Output\nLLMs\nùêø#!\nE > G\nF B>\nState-owned enterprises and advantageous \nprivate enterprisesentered the old \nrevolutionary area of Gannan.\nState-owned enterprises and dominant \nprivate enterprises entered the old \nrevolutionary area of South Gan South Gan.\nOutput\nBad Output D G B\n‚Ä¶\n‚Ä¶\n‚Ä¶\nFigure 2: Overall framework of our proposed TIM. Given the contrastive outputs of each instance, we optimize the LLMs with\nthe general language modeling loss and the token-level preference loss.\nError-guided Data\nInstruction:\nInput:\nOutput:\nTranslate from Chinese to English.\n(Noisy) Bad Output:\nExample 1\nÂõΩÊúâ‰ºÅ‰∏öÂíå‰ºòÂäøÊ∞ëËê•‰ºÅ‰∏öËµ∞ËøõËµ£ÂçóÈù©ÂëΩËÄÅÂå∫„ÄÇ\\n\\n\n### Note: A translation showing major locale\nconvention/name format mistakes\nState-owned enterprises and dominant private enterprises \nentered the old revolutionary area of  <v> South Gan </v>\nSouth Gan.\n(LM) Bad Output: State-owned enterprises and <v>advantageous</v> private \nenterprises entered the old revolutionary base area of \nsouthern Jiangxi.\nState-owned State-owned State-owned dominant private \nenterprises entered the old revolutionary area <v> South \nof Gan </v> Gan.\nFigure 3: An example of contrastive outputs for preference\nComparison. The ‚ÄúBad Output‚Äù denotes the noisy transla-\ntion used to be compared with the ‚ÄúOutput‚Äù.\nOne way to utilize the contrastive outputs is to train a re-\nward model and further fine-tune language models with the\nreward model using reinforcement learning, i.e., RLHF (Sti-\nennon et al. 2020; Ouyang et al. 2022). Instead of using such\na complex two-stage training process, we directly tune the\nlanguage model using a token-level preference loss:\nLpl = ‚àí 1\nN ‚àí I\nNX\ni=I\nmax(0, ‚àírŒ∏(h(0)\ni ) +rŒ∏(h(1)\ni ) + 1.0),\n(2)\nwhere N is the maximum length of two sequences, and h(0)\ni\nand h(1)\ni are the hidden state of the i-th token of the pre-\nferred output y0 and comparison output y1, respectively. I\nis the index starting from the segments different between y0\nand y1. As illustrated in Figure 3, the overlapping part of\nOutput and Bad Output ‚ÄúState-owned enterprises and‚Äù will\nnot contribute to the calculation of Lpl. Specifically, rŒ∏ is a\nlinear head that takes the hidden state of the top layer and\nreturns a scalar.\nThe overall loss function for tuning the model is\nL = Llm + ŒªLpl, (3)\nwhere Œª is a coefficient of the preference learning loss. We\nsimply set Œª as 1.0 in this paper.\nTuning Strategies\nIn this paper, we adopt three different strategies for fine-\ntuning, listed in descending order from the number of train-\nable parameters.\nLoRA: Tuning with Low-rank Matrices. LoRA (Hu\net al. 2022) is a technique that reduces the number of train-\nable parameters by introducing new low-rank matrices to\nany module in the model while keeping the original weights\nfrozen. This results in a significant reduction in storage re-\nquirements and efficient task-switching during deployment\nwithout impacting inference latency.\nFixEmb: Tuning with Embedding Fixed. LoRA-based\ntuning has a limitation where the limited number of trainable\nparameters may restrict its expressiveness. A simple solution\nto overcome this is to fine-tune the parameters of the model\nlayers while keeping the embeddings fixed. This allows the\nmodel to gain more flexibility in adjusting its performance\nwithout compromising the semantic information captured by\nthe embeddings.\nFull: Tuning Full Parameters.Full parameter tuning has\nrecently been demonstrated more effective than LORA. The\nmajor limitation of full parameter fine-tuning is the memory\nfootprint, but it is not serious for 7B models and little data.\nExperiments\nIn this section, we begin by conducting preliminary exper-\niments to investigate the impact of inference strategies and\nthe resilience of our TIM under varying instructions. Sub-\nsequently, we evaluate TIM on the WMT and FLORES-200\ndev-test tasks in four language directions.\nSettings\nTo avoid data leakage (Garcia et al. 2023), we use the\nlatest WMT22 test set and FLORES-200 dev-test: 1) We\nuse the test sets from WMT22 competition 2, which con-\nsist of more recent content from diverse domains such as\n2https://www.statmt.org/wmt22/translation-task.html\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19490\nnews, social, e-commerce, and conversational domains. The\ntest sets comprise 1984, 2037, 1875, and 2037 samples\nfor the German-to-English (De‚áíEn), English-to-German\n(En‚áíDe), Chinese-to-English (Zh‚áíEn), and English-to-\nChinese (En‚áíZh) language pairs, respectively. 2) We use\nthe dev-test split from the FLORES-200 benchmarks3. This\ndataset includes 1,012 sentences extracted from English\nWikipedia, covering a broad range of topics and domains.\nProfessional translators have carefully checked these sen-\ntences into approximately 200 languages.\nTo ensure a fair and consistent evaluation, we fine-tuned\nall models for 1 epoch with a batch size of 128, while im-\nposing a maximum text length of 512. The learning rates\nare 2e-5 for FixEmb and Full, and 3e-4 for LoRA, respec-\ntively. The weight decay parameter is set to 0.0. We con-\nducted fine-tuning on eight NVIDIA A100 GPUs, utilizing\nthe Deep-Speed ZeRO stage3 for model parallelism. The re-\nsults of the final checkpoints are reported. For automatic\nevaluations, we utilize two widely adopted metrics: BLEU\n(Papineni et al. 2002) implemented in SacreBLEU 4, and\nCOMET5 with Unbabel/wmt22-comet-da. BLEU is driven\nby n-gram similarity, while COMET relies on cross-lingual\npre-trained models.\nBaselines\nWe leverage BLOOMZ-7b-mt6 and LLaMA-2-7b7 (Tou-\nvron et al. 2023b) as the backbones and evaluate the follow-\ning baselines:\nAlpaca-(*) is a reproduction of the Alpaca model fine-\ntuned solely on the alpaca multi-task dataset8.\nMT-(*) is fine-tuned on the human-written validation data\nfrom previous WMT competitions, i.e., the newstest2017-\n2021 of Chinese‚áîEnglish and German‚áîEnglish, which\nconsist of 45,433 sentence pairs for all four directions.\nBesides, we report the results of WMT22 winners, and\nNLLB-3.3B (Costa-juss `a et al. 2022). The latter is a mul-\ntilingual translation model trained on a massive paral-\nlel corpus of over 200 languages 9. We use the nota-\ntion TIM-(*) to refer to LLMs fine-tuned using our pro-\nposed TIM approach. In practice, to construct the order-\nguided data, we utilize the WMT translation data. Besides,\nwe rely on the annotated data of newstest2020 Zh‚áíEn\nand En‚áíDe in the Multidimensional Quality Metrics\n(MQM) datasets. We use the mqm\nnewstest2020 ende.tsv\nand mam newstest2020 zhen.tsv to construct the ‚ÄúError-\nguided‚Äù data10. Specifically, we consider the column ‚Äúsever-\nity‚Äù, where treat the ‚ÄúNo-error‚Äù label as the translations\nwithout error and others as the translations with errors. The\n3https://github.com/facebookresearch/flores/blob/main/flores200\n4https://github.com/mjpost/sacrebleu\n5https://github.com/Unbabel/COMET\n6https://huggingface.co/bigscience/bloomz-7b1-mt\n7https://huggingface.co/meta-llama/Llama-2-7b\n8https://huggingface.co/datasets/tatsu-lab/alpaca\n9The results in (Zhang et al. 2023) are directly reported.\n10https://github.com/google/wmt-mqm-human-\nevaluation/tree/main/newstest2020\nMethod Zh‚áí En En‚áí Zh De‚áí En En‚áí De\nSample 22.75 34.98 24.72 19.09\nw/ No Err. 23.10 36.37 25.20 19.34\nw/ Dict. 21.28 34.55 24.37 18.19\nBeam-4 24.51 37.83 26.12 20.90\nw/ No Err. 24.26 38.17 26.24 21.10\nw/ Dict. 24.55 36.32 26.16 20.19\nTable 1: Effect of inference strategies. We fine-tune\nBLOOMZ-7b-mt with our TIM and report BLEU scores on\nfour language pairs.\n20\n25\n30\n35\n40\nZh-En En-Zh De-En En-De\nBLEU (%)\nFigure 4: Effect of instructions. We fine-tune BLOOMZ-7b-\nmt with our TIM and report BLEU scores of 10 different\ninstructions on four language pairs.\ntraining data for TIM-(*) consists of the alpaca dataset, the\nWMT translation data, the Dictionary-guided data, Order-\nguided data constructed from the WMT validation data, and\nError-guided data constructed from MQM data.\nPre-Experiments\nHere, we investigate the effect of inference strategies and in-\nstructions. We fine-tune theBLOOMZ-7b-mt with our TIM\nand conduct evaluations on the WMT22 test sets.\nEffect of Inference Strategies. We compare the perfor-\nmance of sampling and beam search, and the two search\nalgorithms are combined with the notes in our dictionary-\nguided and error-guided data. Table 1 presents the experi-\nmental results. First, we observe that instructing the model to\ngenerate translations without errors does not result in a sig-\nnificant performance gain. We speculate that the preference\nloss function implicitly allows the LLMs to learn to gener-\nate error-free translations, making the additional instructions\nunnecessary. Secondly, previous studies have shown that in-\ntroducing alignment information from dictionaries can im-\nprove translation performance (Lu et al. 2023; Zheng et al.\n2021; Zhang and Zong 2016). Surprisingly, adding align-\nment notes harms the performance, and this may be due to\nthat most of the words in the dictionaries we use are com-\nmon words, or that the wording styles of the dictionaries dif-\nfer greatly from the reference. How to better collect and use\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19491\nModel Zh‚áíEn En‚áíZh De‚áíEn En‚áíDe\nBLEU COMET BLEU COMET BLEU COMET BLEU COMET\nTest: WMT22 Test Sets Backbone: BLOOMZ-7b-mt\nWMT22 Winners‚àó 33.5 81.0 54.3 86.8 33.7 85.0 38.4 87.4\nNLLB-3.3b‚àó 21.07 76.92 32.52 81.56 29.54 83.42 33.98 86.23\nAlpaca-LoRA 12.61 76.36 24.30 81.18 16.04 71.17 8.05 57.54\nAlpaca-Full 13.01 75.95 20.65 78.69 16.98 72.46 2.28 36.91\nMT-LoRA 21.47 79.20 35.22 85.00 23.59 76.91 15.74 66.42\nMT-FixEmb 23.08 78.95 37.09 85.02 24.99 78.19 19.05 71.89\nMT-Full 22.81 79.15 34.49 84.26 24.72 77.84 18.79 71.65\nw/ Noisy-based Bad Output\nTIM-LoRA 22.11 78.89 35.70 84.90 23.55 76.70 16.46 66.80\nTIM-FixEmb 24.11 79.70 37.46 85.29 26.20 78.79 20.97 74.63\nTIM-Full 23.49 79.17 34.70 84.26 25.11 78.40 20.99 74.12\nw/ LM-based Bad Output\nTIM-LoRA 22.22 78.81 35.71 84.67 23.82 76.57 16.62 66.67\nTIM-FixEmb 24.51 79.71 37.83 85.10 26.12 78.94 20.90 74.91\nTIM-Full 23.81 79.33 35.57 84.75 25.43 78.19 20.74 74.24\nTest: FLORES-200 Backbone: LLaMA-2-7b\nMT-FixEmb 26.41 85.88 33.80 84.88 42.14 88.92 32.23 86.16\nMT-Full 26.06 85.81 33.75 84.92 41.56 88.77 31.71 85.93\nw/ Noisy-based Bad Output\nTIM-FixEmb 26.47 85.64 34.84 85.47 42.24 88.95 33.01 86.32\nTIM-Full 26.30 85.71 34.46 85.23 42.01 88.68 32.28 86.05\nw/ LM-based Bad Output\nTIM-FixEmb 26.13 85.61 35.15 85.27 42.91 88.84 33.32 86.20\nTIM-Full 26.25 85.81 34.53 85.18 41.96 88.82 32.79 86.05\nTable 2: Evaluation results of different LLMs on 4 language pairs from WMT22 test sets and Flores devsets. Methods with *\ndenote that we directly report the scores from the corresponding paper, and others are from our implementation.\na dictionary for machine translation (Thompson et al. 2019)\nis left for future work.\nEffect of Instructions. In human interaction scenarios, in-\nstructions provided by users may vary in styles and forms,\nand thus it is essential to evaluate the robustness of TIM un-\nder different instructions. We use ten distinct instructions\nand the result in Figure 4 indicates that our TIM achieves\nconsistent performance across all the tested instructions.\nMain Results\nBased on the observation in Section , we use a simple\ninstruction ‚ÄúTranslate from {src} to {tgt}.\\n{input}‚Äù and\nbeam search with a beam size of 4 for all models during in-\nference. Table 2 presents the translation performance on the\nWMT22 test sets and FLORES-200 dev-test.\nWe have the following observations: First, we observe sig-\nnificant performance fluctuations across different language\nmodels, training data, and language pairs for (*)-LoRA and\n(*)-Full. For example, with BLOOMZ-7b-mt as the back-\nbone, Alpaca-LoRA outperforms Alpaca-Full in most lan-\nguage pairs, while MT-LoRA underperforms MT-Full. We\nspeculate that LoRA can prevent LLMs from overfitting but\nis limited in the number of trainable parameters. In con-\ntrast, the experiment result of (*)-FixEmbindicates that fine-\ntuning with fixed embedding parameters can better leverage\nthe generalization of LLMs and prevent overfitting. Second,\ntraining LLMs with comparison can further enhance the un-\nderstanding of the translation task. Compared toAlpaca-(*),\nMT-(*) models, TIM-(*) exhibits notably better results on\nboth the WMT22 test sets and FLORES-200 dev-test.\nAnalysis\nEffect of Model Sizes\nWe present a comparison between TIM and instruction tun-\ning across different model sizes on the WMT22 test set.\nFigure 5 illustrates the consistent improvements achieved\nby TIM, indicating its generalizability. Besides, as the foun-\ndation LLM‚Äôs size increases, the translation performance\nof the LLMs fine-tuned with TIM improves. In particular,\nthe improvement is more significant when the model size is\nsmaller. This observation supports our hypothesis that the\nsmaller model has a weaker ability to comprehend instruc-\ntions, and it may not effectively learn task patterns with sim-\nple instruction tuning especially using a small amount of\ntraining data, By contrast, training LLMs with comparison\nhelp them to better identify the task‚Äôs requirements and bet-\nter leverage internal cross-lingual knowledge.\nZero-shot Translation\nTo evaluate TIM‚Äôs performance in translation directions\nnever seen previously, i.e., zero-shot multilingual capabil-\nity, we conduct experiments on the WMT22 multilingual-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19492\n33.36\n35.46\n37.83\n35.63\n37.84\n32.48\n34.61\n37.09\n33.34\n36.24\n32\n36\n40\n1b7 3b 7b 13b\nBLEU (%)\n14.27\n16.42\n19.05\n16.93\n19.13\n20.9\n27.85\n29.628.67\n30.09\n14\n18\n22\n26\n30\n1b7 3b 7b 13b\nBLEU (%)\n20.32\n22.1\n23.08\n24.36\n25.2\n21.01\n22.9\n24.51\n24.7\n25.6\n20\n23\n26\n1b7 3b 7b 13b\nBLEU (%)\n20.16\n22.6\n24.99\n22.00 \n23.64\n26.12\n30.47\n30.9\n31.07\n31.45\n20\n24\n28\n32\n1b7 3b 7b 13b\nBLEU (%)\nTIM-BLOOM MT-BLOOM TIM-LLaMA MT-LLaMA\n(a) Zh ‚áí En (c) De ‚áí En (d) En ‚áí De(b) En ‚áí Zh\nFigure 5: Effect of model sizes. We present a comparison between TIM and instruction tuning across LLMs with different\nmodel sizes including BLOOM-1b7, BLOOM-3b, BLOOMZ-7b-mt, LLaMA-2-7b, and LLaMA-2-13b.\n5\n15\n25\n35\n45\n55\nRu-En Cs-En Ja-En Uk-En\nBLEU (%)\n60\n70\n80\n90\nRu-En Cs-En Ja-En Uk-En\nCOMET (%)\nAlpaca-7B Vicuna-13b BayLing -7b BayLing -13b\nTIM-FixEmb-7b TIM-LoRA-13b TIM-FixEmb-13b\nNLLB-3.3b\nGPT-3.5-turbo GPT-4\nFigure 6: Zero-shot translation. We fine-tune LLaMA2 and compare our TIM-FixEmb-7b, TIM-LoRA-13b, and TIM-FixEmb-\n13b with the open-sourced models on WMT22 multilingual-to-English translation benchmark.\nto-English translation benchmark which encompasses 4\ntranslation directions: Czech-to-English (cs‚áíen), Japanese-\nto-English (ja ‚áíen), Russian-to-English (ru ‚áíen), and\nUkrainian-to-English (uk ‚áíen). We compare our method\nwith the following open-sourced models: Alpaca-7b 11,\nVicuna-13b12, BayLing-7b, -13b (Zhang et al. 2023),\nNLLB-3.3b (Costa-juss `a et al. 2022), ChatGPT, and GPT4\n(OpenAI 2023). We report the results of the above models\nin Zhang et al. (2023). Due to the better performance of\nLLaMA-2 in multilingual-to-English, we report the perfor-\nmance of fine-tuned LLaMA-2-7b and LLaMA-2-13b with\nour TIM, respectively.\nAs depicted in Figure 6, TIM-(*) (i.e., TIM-FixEmb-\n7b, TIM-LoRA-13b, and TIM-FixEmb-13b) exhibit good\nzero-shot multilingual capability on these translation direc-\ntions. Compared to Alpaca-7b, Vicuna-13B, BayLing-7b,\nand BayLing-13b, TIM-(*) exhibits superior translation abil-\nity, highlighting that aligning training languages strength-\nens the alignment of other languages as a by-product. Ad-\nditionally, TIM-(*) obtains comparative performance with\nNLLB-3.3B in most language pairs, and significantly bet-\nter on Ja ‚áíEn. These results demonstrate that adding care-\n11https://huggingface.co/tatsu-lab/alpaca-7b-wdiff\n12https://huggingface.co/lmsys/vicuna-13b-delta-v1.1\nfully constructed translation data, combined with an effec-\ntive training strategy such as our proposed TIM, can enhance\nthe overall task capability of LLMs.\nAblation Study\nTo analyze the impact of different components of TIM, we\ninvestigate variants of TIM-FixEmb taking BLOOMZ-7b-\nmt as the backbone: MT w/(*), where we add the (*)-guided\ncomparisons in training data; TIM[*], where we use noisy-\nbased or LM-based bad output for preference comparison;\nTIM w/oLpl, where we remove Lpl; and TIM w/o OutCom,\nwhere we remove output comparison. As a supplement to\nBLEU, we analyze the phenomenon of hallucination on the\nZh‚áíEn test set using the hallucination detector provided by\nZhou et al. (2021). The BLEU scores, sentence-level, and\ntoken-level hallucination scores are reported in Table 3.\nThe experimental results of 1, 2, 3, and 4 indicate a\nnoteworthy reduction in translation hallucination when out-\nput comparison is incorporated into language models. Par-\nticularly, the inclusion of dictionary-guided data is cru-\ncial among various data types. This suggests that providing\ntranslation-related information and instructing the model to\ngenerate corresponding translations during training can pro-\nmote the model to produce more faithful translations. Fur-\nthermore, the results of 1 and 8 indicate that LLMs can\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19493\nId Method BLEU‚Üë S-Hal.‚Üì T-Hal.‚Üì ‚àÜ% T-Hal.\n0 Alpaca 10.96 73.87 20.36 -\n1 MT 23.08 68.21 10.58 -9.78%\n2 w/ Rev 23.41 67.36 9.62 -10.74%\n3 w/ Dict 23.73 66.77 8.93 -11.43%\n4 w/ Error 23.94 66.61 9.59 -10.77%\n5 TIM[Noisy] 24.11 67.31 9.39 -10.97%\n6 TIM[LM] 24.51 66.03 8.83 -11.53%\n7 w/o Lpl 23.76 68.00 9.53 -10.83%\n8 w/o OutCom 23.21 67.46 9.69 -10.67%\nTable 3: Ablation study. We fine-tune BLOOMZ-7b-mt\nwith our TIM and report BLEU and hallucination scores on\nZh‚áíEn.\nlearn better translation output through preference compar-\nison, even without the requirement of any output compari-\nson data. Finally, although the performance of TIM[Noisy]\nproved to be competitive with TIM[LM] in terms of BLEU\nand COMET scores (Table 2), the results of 5 and 6 in Ta-\nble 3 indicate that incorporating bad examples based on ac-\ntual LM errors can provide more meaningful training signals\ncompared to artificial noisy data.\nMT Metrics Evaluation\nThe preference scores can reflect the quality of the model\noutput. To demonstrate how well they reflect quality assess-\nment, we use MTME 13 to evaluate the performance of our\npreference scores on standard test sets from the WMT22\nMetrics Shared Tasks in De‚áíEn and En‚áíDe. We compare\nours with some reference-free metrics: COMET-QE (Rei\net al. 2021), COMETKiwi (Rei et al. 2022), UniTE-src (Wan\net al. 2022), and HWTSC-Teacher-SIM (Liu et al. 2022));\nand reference-based metrics: metricx\nxxl MQM 2020 (Fre-\nitag et al. 2022), BLEURT-20 (Sellam, Das, and Parikh\n2020), COMET-22 (Rei et al. 2022), BLEU (Papineni et al.\n2002), and chrF (Popovic 2015).\nFor each pair consisting of a source sentence and the cor-\nresponding hypothesis, we wrap them with our Training\nPrompt, and use the score of the last token in the hypothe-\nsis as the final score. Table 4 shows the system-level accu-\nracy (Acc) and Pearson correlations (PCCs). In particular,\nour TIM-LLaMA-13b and TIM-BLOOMZ-7b outperform all\nthe reference-free metrics and achieve better Pearson cor-\nrelation on De‚áíEn than others. This demonstrates that the\nLLMs are implicitly a reward model that can be jointly op-\ntimized during instruction tuning (Rafailov et al. 2023).\nRelated Work\nResearch on machine translation based on Large Language\nModels (LLMs) can be divided into two categories: LLMs\nas interface and instruction tuning.\nThe studies of using LLMs as an interface focus on em-\npirical analysis. For example, Hendy et al. (2023) evalu-\nate ChatGPT, GPT3.5 (text-davinci-003), and text-davinci-\n002 in eighteen different translation directions involving\n13https://github.com/google-research/mt-metrics-eval\nMethod Acc. PCCs.\nDe‚áíEn En‚áíDe\nmetricx xxl MQM 2020 74.56 48.98 84.69\nBLEURT-20 73.68 45.84 71.89\nTIM-LLaMA-13b‚àó 72.81 50.37 62.67\nCOMET-22 72.81 44.63 77.06\nBERTScore 71.05 43.96 42.82\nTIM-BLOOMZ-7b‚àó 69.30 62.14 42.59\nCOMET-QE‚àó 69.30 44.32 50.21\nCOMETKiwi‚àó 68.42 40.95 67.35\nMS-COMET-QE-22‚àó 68.42 39.49 53.92\nBLEU 67.54 35.24 17.88\nchrF 65.79 35.45 34.63\nUniTE-src‚àó 64.91 40.20 50.91\nHWTSC-Teacher-Sim‚àó 60.52 32.17 38.53\nTable 4: Pearson correlation of all metrics with system-level\nMQM scores for De‚áîEn. Rows are sorted by the system-\nlevel pairwise accuracy across the two language pairs. The\nbest results are indicated in bold. Reference-free metrics are\nindicated using an asterisk.\nhigh and low resource languages. Zhu et al. (2023) further\nevaluate four popular LLMs (XGLM, BLOOMZ, OPT and\nChatGPT) on 202 directions and 102 languages, and com-\npare them with strong supervised baselines, which provides\na more comprehensive benchmark result. Many efforts are\nalso put into investigating translation exemplars selection\nstrategy of in-context learning (Lin et al. 2022; Agrawal\net al. 2022). Another line of work introduces knowledge,\nsuch as word alignments extracted from a dictionary, to\nLLMs for better translation (Lu et al. 2023).\nTuning smaller LLMs (e.g., 7B) for translation tasks is\na promising direction since they are better at English than\nsupervised translation models. However, even for directions\nfrom other languages to English, the gap between language\nmodels fine-tuned with translation data and supervised sys-\ntems is still evident (Jiao et al. 2023; Zhang et al. 2023).\nDifferent from them, we introduce output comparison and\npreference comparison data and present a preference reg-\nularization to alleviate hallucination and help LLMs learn\ntranslation better.\nConclusion\nWe propose TIM, a training method that fine-tunes open-\nsource large language models for the translation task with\nthe comparison of translations. Experiments and analyses\nvalidate the effectiveness of TIM in terms of translation\nquality and zero-shot translation ability. For the reference-\nfree MT metrics evaluation, TIM-LLaMA-13b even outper-\nforms representative metrics like COMET and BLEURT in\nDe‚áíEn, showing that our method can well learn the transla-\ntion and evaluation jointly. Future work can explore the use\nof more diverse references for output comparison, and more\nadvanced preference learning objectives.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19494\nReferences\nAgrawal, S.; Zhou, C.; Lewis, M.; Zettlemoyer, L.; and\nGhazvininejad, M. 2022. In-context Examples Selection for\nMachine Translation. CoRR, abs/2212.02437.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Infor-\nmation Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; Web-\nson, A.; Gu, S. S.; Dai, Z.; Suzgun, M.; Chen, X.; Chowdh-\nery, A.; Narang, S.; Mishra, G.; Yu, A.; Zhao, V . Y .; Huang,\nY .; Dai, A. M.; Yu, H.; Petrov, S.; Chi, E. H.; Dean, J.; De-\nvlin, J.; Roberts, A.; Zhou, D.; Le, Q. V .; and Wei, J. 2022.\nScaling Instruction-Finetuned Language Models. CoRR,\nabs/2210.11416.\nCosta-juss`a, M. R.; Cross, J.; C ¬∏ elebi, O.; Elbayad, M.;\nHeafield, K.; Heffernan, K.; Kalbassi, E.; Lam, J.; Licht, D.;\nMaillard, J.; Sun, A.; Wang, S.; Wenzek, G.; Youngblood,\nA.; Akula, B.; Barrault, L.; Gonzalez, G. M.; Hansanti, P.;\nHoffman, J.; Jarrett, S.; Sadagopan, K. R.; Rowe, D.; Spruit,\nS.; Tran, C.; Andrews, P.; Ayan, N. F.; Bhosale, S.; Edunov,\nS.; Fan, A.; Gao, C.; Goswami, V .; Guzm¬¥an, F.; Koehn, P.;\nMourachko, A.; Ropers, C.; Saleem, S.; Schwenk, H.; and\nWang, J. 2022. No Language Left Behind: Scaling Human-\nCentered Machine Translation. CoRR, abs/2207.04672.\nFreitag, M.; Rei, R.; Mathur, N.; Lo, C.; Stewart, C.;\nAvramidis, E.; Kocmi, T.; Foster, G. F.; Lavie, A.; and Mar-\ntins, A. F. T. 2022. Results of WMT22 Metrics Shared Task:\nStop Using BLEU - Neural Metrics Are Better and More\nRobust. In Koehn, P.; Barrault, L.; Bojar, O.; Bougares, F.;\nChatterjee, R.; Costa-juss `a, M. R.; Federmann, C.; Fishel,\nM.; Fraser, A.; Freitag, M.; Graham, Y .; Grundkiewicz,\nR.; Guzman, P.; Haddow, B.; Huck, M.; Jimeno-Yepes, A.;\nKocmi, T.; Martins, A.; Morishita, M.; Monz, C.; Nagata,\nM.; Nakazawa, T.; Negri, M.; N¬¥ev¬¥eol, A.; Neves, M.; Popel,\nM.; Turchi, M.; and Zampieri, M., eds., Proceedings of the\nSeventh Conference on Machine Translation, WMT 2022,\nAbu Dhabi, United Arab Emirates (Hybrid), December 7-8,\n2022, 46‚Äì68. Association for Computational Linguistics.\nGarcia, X.; Bansal, Y .; Cherry, C.; Foster, G. F.; Krikun, M.;\nFeng, F.; Johnson, M.; and Firat, O. 2023. The unreasonable\neffectiveness of few-shot learning for machine translation.\nCoRR, abs/2302.01398.\nHendy, A.; Abdelrehim, M.; Sharaf, A.; Raunak, V .; Gabr,\nM.; Matsushita, H.; Kim, Y . J.; Afify, M.; and Awadalla,\nH. H. 2023. How Good Are GPT Models at Ma-\nchine Translation? A Comprehensive Evaluation. CoRR,\nabs/2302.09210.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adapta-\ntion of Large Language Models. In The Tenth International\nConference on Learning Representations, ICLR 2022, Vir-\ntual Event, April 25-29, 2022. OpenReview.net.\nJiao, W.; Huang, J.; Wang, W.; Wang, X.; Shi, S.; and Tu,\nZ. 2023. ParroT: Translating During Chat Using Large Lan-\nguage Models. CoRR, abs/2304.02426.\nLin, X. V .; Mihaylov, T.; Artetxe, M.; Wang, T.; Chen, S.;\nSimig, D.; Ott, M.; Goyal, N.; Bhosale, S.; Du, J.; Pasunuru,\nR.; Shleifer, S.; Koura, P. S.; Chaudhary, V .; O‚ÄôHoro, B.;\nWang, J.; Zettlemoyer, L.; Kozareva, Z.; Diab, M. T.; Stoy-\nanov, V .; and Li, X. 2022. Few-shot Learning with Mul-\ntilingual Generative Language Models. In EMNLP 2022,\n9019‚Äì9052.\nLiu, Y .; Qiao, X.; Wu, Z.; Su, C.; Zhang, M.; Zhao, Y .; Peng,\nS.; Tao, S.; Yang, H.; Qin, Y .; Guo, J.; Wang, M.; Li, Y .; Li,\nP.; and Zhao, X. 2022. Partial Could Be Better than Whole.\nHW-TSC 2022 Submission for the Metrics Shared Task. In\nKoehn, P.; Barrault, L.; Bojar, O.; Bougares, F.; Chatterjee,\nR.; Costa-juss `a, M. R.; Federmann, C.; Fishel, M.; Fraser,\nA.; Freitag, M.; Graham, Y .; Grundkiewicz, R.; Guzman, P.;\nHaddow, B.; Huck, M.; Jimeno-Yepes, A.; Kocmi, T.; Mar-\ntins, A.; Morishita, M.; Monz, C.; Nagata, M.; Nakazawa,\nT.; Negri, M.; N ¬¥ev¬¥eol, A.; Neves, M.; Popel, M.; Turchi,\nM.; and Zampieri, M., eds.,Proceedings of the Seventh Con-\nference on Machine Translation, WMT 2022, Abu Dhabi,\nUnited Arab Emirates (Hybrid), December 7-8, 2022, 549‚Äì\n557. Association for Computational Linguistics.\nLu, H.; Huang, H.; Zhang, D.; Yang, H.; Lam, W.; and Wei,\nF. 2023. Chain-of-Dictionary Prompting Elicits Translation\nin Large Language Models. CoRR, abs/2305.06575.\nOpenAI. 2023. GPT-4 Technical Report. CoRR,\nabs/2303.08774.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and\nLowe, R. 2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine Trans-\nlation. In Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics, 311‚Äì318. Asso-\nciation for Computational Linguistics.\nPopovic, M. 2015. chrF: character n-gram F-score for auto-\nmatic MT evaluation. In Proceedings of the Tenth Workshop\non Statistical Machine Translation, WMT@EMNLP 2015,\n17-18 September 2015, Lisbon, Portugal, 392‚Äì395. The As-\nsociation for Computer Linguistics.\nRafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,\nC. D.; and Finn, C. 2023. Direct Preference Optimization:\nYour Language Model is Secretly a Reward Model. CoRR,\nabs/2305.18290.\nRei, R.; de Souza, J. G. C.; Alves, D. M.; Zerva, C.; Farinha,\nA. C.; Glushkova, T.; Lavie, A.; Coheur, L.; and Martins,\nA. F. T. 2022. COMET-22: Unbabel-IST 2022 Submission\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19495\nfor the Metrics Shared Task. In Koehn, P.; Barrault, L.; Bo-\njar, O.; Bougares, F.; Chatterjee, R.; Costa-juss`a, M. R.; Fe-\ndermann, C.; Fishel, M.; Fraser, A.; Freitag, M.; Graham,\nY .; Grundkiewicz, R.; Guzman, P.; Haddow, B.; Huck, M.;\nJimeno-Yepes, A.; Kocmi, T.; Martins, A.; Morishita, M.;\nMonz, C.; Nagata, M.; Nakazawa, T.; Negri, M.; N¬¥ev¬¥eol, A.;\nNeves, M.; Popel, M.; Turchi, M.; and Zampieri, M., eds.,\nProceedings of the Seventh Conference on Machine Trans-\nlation, WMT 2022, Abu Dhabi, United Arab Emirates (Hy-\nbrid), December 7-8, 2022, 578‚Äì585. Association for Com-\nputational Linguistics.\nRei, R.; Farinha, A. C.; Zerva, C.; van Stigt, D.; Stewart, C.;\nRamos, P. G.; Glushkova, T.; Martins, A. F. T.; and Lavie, A.\n2021. Are References Really Needed? Unbabel-IST 2021\nSubmission for the Metrics Shared Task. In Barrault, L.;\nBojar, O.; Bougares, F.; Chatterjee, R.; Costa-juss `a, M. R.;\nFedermann, C.; Fishel, M.; Fraser, A.; Freitag, M.; Graham,\nY .; Grundkiewicz, R.; Guzman, P.; Haddow, B.; Huck, M.;\nJimeno-Yepes, A.; Koehn, P.; Kocmi, T.; Martins, A.; Mor-\nishita, M.; and Monz, C., eds.,Proceedings of the Sixth Con-\nference on Machine Translation, WMT@EMNLP 2021, On-\nline Event, November 10-11, 2021, 1030‚Äì1040. Association\nfor Computational Linguistics.\nSellam, T.; Das, D.; and Parikh, A. 2020. BLEURT: Learn-\ning Robust Metrics for Text Generation. In Proceedings\nof the 58th Annual Meeting of the Association for Com-\nputational Linguistics, 7881‚Äì7892. Online: Association for\nComputational Linguistics.\nStiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D. M.; Lowe, R.;\nV oss, C.; Radford, A.; Amodei, D.; and Christiano, P. F.\n2020. Learning to summarize with human feedback. In\nLarochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and\nLin, H., eds., Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nThompson, B.; Knowles, R.; Zhang, X.; Khayrallah, H.;\nDuh, K.; and Koehn, P. 2019. HABLex: Human Annotated\nBilingual Lexicons for Experiments in Machine Translation.\nIn Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 1382‚Äì1387. Hong Kong, China: Asso-\nciation for Computational Linguistics.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar,\nF.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.\n2023a. LLaMA: Open and Efficient Foundation Language\nModels. CoRR, abs/2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Canton-Ferrer, C.; Chen, M.; Cu-\ncurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller,\nB.; Gao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hos-\nseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa,\nM.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b.\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\nCoRR, abs/2307.09288.\nWan, Y .; Liu, D.; Yang, B.; Zhang, H.; Chen, B.; Wong, D.;\nand Chao, L. 2022. UniTE: Unified Translation Evaluation.\nIn Proceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n8117‚Äì8127. Dublin, Ireland: Association for Computational\nLinguistics.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2023. Self-Instruct: Align-\ning Language Models with Self-Generated Instructions.\nZeng, J.; Meng, F.; Yin, Y .; and Zhou, J. 2023. Im-\nproving Machine Translation with Large Language Models:\nA Preliminary Study with Cooperative Decoding. CoRR,\nabs/2311.02851.\nZhang, J.; and Zong, C. 2016. Bridging Neural Ma-\nchine Translation and Bilingual Dictionaries. CoRR,\nabs/1610.07272.\nZhang, S.; Fang, Q.; Zhang, Z.; Ma, Z.; Zhou, Y .; Huang,\nL.; Bu, M.; Gui, S.; Chen, Y .; Chen, X.; and Feng, Y . 2023.\nBayLing: Bridging Cross-lingual Alignment and Instruction\nFollowing through Interactive Translation for Large Lan-\nguage Models. CoRR, abs/2306.10968.\nZheng, X.; Zhang, Z.; Huang, S.; Chen, B.; Xie, J.; Luo, W.;\nand Chen, J. 2021. Non-Parametric Unsupervised Domain\nAdaptation for Neural Machine Translation. In Findings\nof the Association for Computational Linguistics: EMNLP\n2021, 4234‚Äì4241. Punta Cana, Dominican Republic: Asso-\nciation for Computational Linguistics.\nZhou, C.; Neubig, G.; Gu, J.; Diab, M.; Guzm ¬¥an, F.; Zettle-\nmoyer, L.; and Ghazvininejad, M. 2021. Detecting Halluci-\nnated Content in Conditional Neural Sequence Generation.\nIn Findings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, 1393‚Äì1404. Online: Association\nfor Computational Linguistics.\nZhu, W.; Liu, H.; Dong, Q.; Xu, J.; Kong, L.; Chen, J.; Li, L.;\nand Huang, S. 2023. Multilingual Machine Translation with\nLarge Language Models: Empirical Results and Analysis.\nCoRR, abs/2304.04675.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19496",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4918553829193115
    },
    {
      "name": "Linguistics",
      "score": 0.3940278887748718
    },
    {
      "name": "Natural language processing",
      "score": 0.3824402987957001
    },
    {
      "name": "Mathematics education",
      "score": 0.38134488463401794
    },
    {
      "name": "Psychology",
      "score": 0.2889344096183777
    },
    {
      "name": "Philosophy",
      "score": 0.12554556131362915
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ]
}