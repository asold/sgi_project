{
    "title": "Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models",
    "url": "https://openalex.org/W2891068404",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2553704161",
            "name": "Wada Takashi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2005055567",
            "name": "Iwata, Tomoharu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2951184134",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2127589108",
        "https://openalex.org/W114517082",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2962824887",
        "https://openalex.org/W2760424551",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W2740132093",
        "https://openalex.org/W2741986357",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2171082019",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2765961751",
        "https://openalex.org/W2741602058",
        "https://openalex.org/W2762484717",
        "https://openalex.org/W2118090838",
        "https://openalex.org/W2739748921",
        "https://openalex.org/W2467585580",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W22168010"
    ],
    "abstract": "We propose an unsupervised method to obtain cross-lingual embeddings without any parallel data or pre-trained word embeddings. The proposed model, which we call multilingual neural language models, takes sentences of multiple languages as an input. The proposed model contains bidirectional LSTMs that perform as forward and backward language models, and these networks are shared among all the languages. The other parameters, i.e. word embeddings and linear transformation between hidden states and outputs, are specific to each language. The shared LSTMs can capture the common sentence structure among all languages. Accordingly, word embeddings of each language are mapped into a common latent space, making it possible to measure the similarity of words across multiple languages. We evaluate the quality of the cross-lingual word embeddings on a word alignment task. Our experiments demonstrate that our model can obtain cross-lingual embeddings of much higher quality than existing unsupervised models when only a small amount of monolingual data (i.e. 50k sentences) are available, or the domains of monolingual data are different across languages.",
    "full_text": "Unsupervised Cross-lingual Word Embedding\nby Multilingual Neural Language Models\nTakashi Wada\nNara Institute of Science and Technology,\nNara, Japan\nwada.takashi.wp7@is.naist.jp\nTomoharu Iwata\nNTT Communication Science Laboratories,\nKyoto, Japan\niwata.tomoharu@lab.ntt.co.jp\nAbstract\nWe propose an unsupervised method to obtain cross-lingual\nembeddings without any parallel data or pre-trained word\nembeddings. The proposed model, which we call multilin-\ngual neural language models, takes sentences of multiple\nlanguages as an input. The proposed model contains bidi-\nrectional LSTMs that perform as forward and backward lan-\nguage models, and these networks are shared among all the\nlanguages. The other parameters, i.e. word embeddings and\nlinear transformation between hidden states and outputs, are\nspeciﬁc to each language. The shared LSTMs can capture\nthe common sentence structure among all languages. Accord-\ningly, word embeddings of each language are mapped into\na common latent space, making it possible to measure the\nsimilarity of words across multiple languages. We evaluate\nthe quality of the cross-lingual word embeddings on a word\nalignment task. Our experiments demonstrate that our model\ncan obtain cross-lingual embeddings of much higher quality\nthan existing unsupervised models when only a small amount\nof monolingual data (i.e. 50k sentences) are available, or the\ndomains of monolingual data are different across languages.\n1 Introduction\nCross-lingual word representation learning has been recog-\nnized as a very important research topic in natural language\nprocessing (NLP). It aims to represent multilingual word\nembeddings in a common space, and has been applied to\nmany multilingual tasks, such as machine translation (Zou\net al. 2013) and bilingual named entity recognition (Rudra-\nmurthy, Khapra, and Bhattacharyya 2016). It also enables\nthe transfer of knowledge from one language into another\n(Xiao and Guo 2014; Adams et al. 2017).\nA number of methods have been proposed that obtain\nmultilingual word embeddings. The key idea is to learn a lin-\near transformation that maps word embedding spaces of dif-\nferent languages. Most of them utilize parallel data such as\nparallel corpus and bilingual dictionaries to learn a mapping\n(Mikolov et al. 2013a). However, such data are not read-\nily available for many language pairs, especially for low-\nresource languages.\nTo tackle this problem, a few unsupervised methods have\nbeen proposed that obtain cross-lingual word embeddings\nwithout any parallel data (Conneau et al. 2017; Zhang et\nal. 2017a; 2017b; Artetxe, Labaka, and Agirre 2017; 2018).\nTheir methods have opened up the possibility of perform-\ning unsupervised neural machine translation (Lample, De-\nnoyer, and Ranzato 2017; Artetxe et al. 2018). Conneau et al.\n(2017), Zhang et al. (2017a) propose a model based on ad-\nversarial training, and similarly Zhang et al. (2017b) propose\na model that employs Wasserstein GAN (Arjovsky, Chin-\ntala, and Bottou 2017). Surprisingly, these models have out-\nperformed some supervised methods in their experiments.\nRecently, however, Søgaard, Ruder, and Vuli ´c (2018) have\npointed out that the model of Conneau et al. (2017) is ef-\nfective only when the domain of monolingual corpora is\nthe same across languages and languages to align are lin-\nguistically similar. Artetxe, Labaka, and Agirre (2018), on\nthe other hand, have overcome this problem and proposed a\nmore robust method that enables to align word embeddings\nof distant language pairs such as Finnish and English. How-\never, all of these approaches still have a common signiﬁ-\ncant bottleneck: they require a large amount of monolingual\ncorpora to obtain cross-lingual word embedddings, and such\ndata are not readily available among minor languages.\nIn this work, we propose a new unsupervised method that\ncan obtain cross-lingual embeddings even in a low-resource\nsetting. We deﬁne our method as multilingual neural lan-\nguage model, that obtains cross-lingual embeddings by cap-\nturing a common structure among multiple languages. More\nspeciﬁcally, our model employs bidirectional LSTM net-\nworks (Schuster and Paliwal 1997; Hochreiter and Schmid-\nhuber 1997) that respectively perform as forward and back-\nward language models (Mikolov et al. 2010), and these pa-\nrameters are shared among multiple languages. The shared\nLSTM networks learn a common structure of multiple lan-\nguages, and the shared network encodes words of differ-\nent languages into a common space. Our model is signiﬁ-\ncantly different from the existing unsupervised methods in\nthat while they aim to align two pre-trained word embed-\nding spaces, ours jointly learns multilingual word embed-\ndings without any pre-training. Our experiments show that\nour model is more stable than the existing methods under a\nlow-resource condition, where it is difﬁcult to obtain ﬁne-\ngrained monolingual word embeddings.\narXiv:1809.02306v1  [cs.CL]  7 Sep 2018\nw!\"#ℓ   Eℓ   h$ ℓ   h% ℓ   EBOS\n   &$ ℓ   &'() ℓ\n<BOS>   EBOS… …… …\nw!ℓ w!ℓ\n   *\tℓ   *EOS   *\tℓ   *EOS\n… …\nShared:-,\t-, EBOS,*EOSSpecific to ℓ:   Eℓ,*\tℓ\nforwardLSTM -\nbackwardLSTM-\n<BOS>w!(#ℓ   Eℓ…\nFigure 1: Illustration of our proposed multilingual neu-\nral language model. The parameters shared among across\nmultiple languages are the ones of forward and backward\nLSTMs − →f and ← −f, the embedding of <BOS>, EBOS, and\nthe linear projection for<EOS>, WEOS. On the other hand,\nword embeddings, Eℓ, and linear projection Wℓ are speciﬁc\nto each language ℓ. The shared LSTMs capture a common\nstructure of multiple languages, and that enables us to map\nword embeddings Eℓ of multiple languages into a common\nspace.\n2 Our Model\n2.1 Overview\nWe propose a model called multi-lingual neural language\nmodel, which produces cross-lingual word embeddings in an\nunsupervised way. Figure 1 brieﬂy illustrates our proposed\nmodel. The model consists of the shared parameters among\nmultiple languages and the speciﬁc ones to each language.\nIn what follows, we ﬁrst summerize which parameters are\nshared or separate across languages:\n• Shared Parameters\n– − →f and ← −f : LSTM networks which perform as forward\nand backward language models, independently.\n– EBOS: The embedding of <BOS>, an initial input to\nthe language models.\n– WEOS: The linear mapping for<EOS>, which calcu-\nlates how likely it is that the next word is the end of a\nsentence.\n• Separate Parameters\n– Eℓ: Word embeddings of language ℓ\n– Wℓ: Linear projections of language ℓ, which is used to\ncalculate the probability distribution of the next word.\nThe LSTMs − →f and ← −f are shared among multiple lan-\nguages and capture a common language structure. On the\nother hand, the word embedding function Eℓ and liner pro-\njection Wℓ are speciﬁc to each language ℓ. Since different\nlanguages are encoded by the same LSTM functions, sim-\nilar words across different languages should have a similar\nrepresentation so that the shared LSTMs can encode them\neffectively. For instance, suppose our model encodes an En-\nglish sentence “He drives a car.” and its Spanish translation\n“El conduce un coche.” In these sentences, each English\nword corresponds to each Spanish one in the same order.\nTherefore, these equivalent words would have similar rep-\nresentations so that the shared language models can encode\nthe English and Spanish sentences effectively. Although in\ngeneral each language has its different grammar rule, the\nshared language models are trained to roughly capture the\ncommon structure such as a common basic word order rule\n(e.g. subject-verb-object) among different languages. Shar-\ning <BOS> and <EOS> symbols further helps to obtain\ncross-lingual representations, ensuring that the beginning\nand end of the hidden states are in the same space regard-\nless of language. In particular, sharing <EOS>symbol in-\ndicates that the same linear function predicts how likely it is\nthat the next word is the end of a sentence. In order for the\nforward and backward language models to predict the end\nof a sentence with high probability, the words that appear\nnear the end or beginning of a sentence such as punctuation\nmarks and conjunctions should have very close representa-\ntions among different languages.\n2.2 Network Structure\nSuppose a sentence with N words in language ℓ,\n⟨wℓ\n1,wℓ\n2,...,w ℓ\nN ⟩. The forward language model calculates\nthe probability of upcoming word wℓ\nt given the previous\nwords wℓ\n1,wℓ\n2,...,w ℓ\nt−1.\nP(wℓ\n1,wℓ\n2,...,w ℓ\nN ) =\nN∏\nt=1\np(wℓ\nt|wℓ\n1,wℓ\n2,...,w ℓ\nt−1). (1)\nThe backward language model is computed similarly given\nthe backward context:\nP(wℓ\n1,wℓ\n2,...,w ℓ\nN ) =\nN∏\nt=1\np(wℓ\nt|wℓ\nt+1,wℓ\nt+2,...,w ℓ\nN ). (2)\nThe tth hidden states hℓ\nt of the forward and backward\nLSTMs are calculated based on the previous hidden state\nand word embedding,\n− →hℓ\nt = − →f(− →hℓ\nt−1,xℓ\nt−1), (3)\n← −hℓ\nt = ← −f(← −hℓ\nt+1,xℓ\nt+1), (4)\nxℓ\nt =\n{EBOS if t = 0 or N+1,\nEℓ(wℓ\nt) otherwise, (5)\nwhere − →f(·) and ← −f(·) are the standard LSTM functions.\nEBOS is the embedding of <BOS>, which is shared among\nall the languages. Note that the same word embedding func-\ntion Eℓ is used among the forward and backward language\nmodels. The probability distribution of the upcoming word\nwℓ\nt is calculated by the forward and backward models inde-\npendently based on their current hidden state:\np(wℓ\nt|wℓ\n1,wℓ\n2,...,w ℓ\nt−1) = softmax(gℓ(− →hℓ\nt))), (6)\np(wℓ\nt|wℓ\nt+1,wℓ\nt+2,...,w ℓ\nN ).= softmax(gℓ(← −hℓ\nt)), (7)\ngℓ(hℓ\nt) = [WEOS(hℓ\nt),Wℓ(hℓ\nt)], (8)\nwhere [x,y] means the concatenation of xand y. WEOS is\na matrix with the size of ( 1 ×d), where dis the dimension\nof the hidden state. This matrix is a mapping function for\n<EOS>, and shared among all of the languages. Wℓ is a\nmatrix with the size of (Vℓ ×d), where Vℓ is the vocabulary\nsize of language ℓexcluding <EOS>. Therefore, gis a lin-\near transformation with the size of ((Vℓ + 1)×d). As with\nthe word embeddings, the same mapping functions are used\namong the forward and backward language models.\nThe largest difference between our model and a standard\nlanguage model is that our model shares LSTM networks\namong different languages, and the shared LSTMs cap-\nture a common structure of multiple languages. Our model\nalso shares <BOS>and <EOS>among languages, which\nencourages word embeddings of multiple languages to be\nmapped into a common space.\nThe proposed model is trained by maximizing the log\nlikelihood of the forward and backward directions for each\nlanguage ℓ:\nL∑\nl=1\nSℓ\n∑\ni=1\nNi\n∑\nt=1\nlog p(wℓ\ni,t|wℓ\ni,1,wℓ\ni,2,...wℓ\ni,t−1; − →θ)\n+ logp(wℓ\ni,t|wℓ\ni,t+1,wℓ\ni,t+2,...wℓ\ni,Ni; ← −θ),\nwhere L and Sℓ denote the number of languages and sen-\ntences of language ℓ. − →θ and ← −θ denote the parameters for\nthe forward and backward LSTMs − →f and ← −f, respectively.\n3 Related Work\n3.1 Unsupervised Word Mapping\nA few unsupervised methods have been proposed that ob-\ntain cross-lingual representations in an unsupervised way.\nTheir goal is to ﬁnd a linear transformation that aligns pre-\ntrained word embeddings of multiple languages. For in-\nstance, Artetxe, Labaka, and Agirre (2017) obtain a lin-\near mapping using a parallel vocabulary of automatically\naligned digits (i.e. 1-1, 2-2, 15-15...). In fact, their method\nis weakly supervised because they rely on the aligned in-\nformation of Arabic numerals across languages. Zhang et\nal. (2017a) and Conneau et al. (2017), on the other hand,\npropose fully unsupervised methods that do not make use\nof any parallel data. Their methods are based on adversar-\nial training (Goodfellow et al. 2014): during the training, a\ndiscriminator is trained to distinguish between the mapped\nsource and the target embeddings, while the mapping matrix\nis trained to fool the discriminator. Conneau et al. (2017) fur-\nther reﬁne the mapping obtained by the adversarial training.\nThey build a synthetic parallel vocabulary using the map-\nping, and apply a supervised method given the pseudo par-\nallel data. Zhang et al. (2017b) employ Wasserstein GAN\nand obtain cross-lingual representations by minimizing the\nearth-mover’s distance. Artetxe, Labaka, and Agirre (2018)\npropose an unsupervised method using a signiﬁcantly dif-\nferent approach from them. It ﬁrst roughly aligns words\nacross language using structural similarity of word embed-\nding spaces, and reﬁnes the word alignment by repeating\na robust self-learning method until convergence. They have\nfound that their approach is much more effective than Zhang\net al. (2017a) and Conneau et al. (2017) on realistic scenar-\nios, namely when languages to align are linguistically distant\nor training data are non-comparable across language.\nThe common objective among all these unsupervised\nmethods is to map word embeddings of multiple languages\ninto a common space. In their experiments. the word embed-\ndings are pre-trained on a large amount of monolingual data\nsuch as Wikipedia before their methods are applied. There-\nfore, they haven’t evaluated their method on the condition\nwhen only a small amount of data are available. That con-\ndition is very realistic for minor languages, and an unsuper-\nvised method can be very useful for these languages. In our\nexperiments, it turns out that existing approaches do not per-\nform well without enough data, while our proposed method\ncan align words with as small data as ﬁfty thousand sen-\ntences for each language.\n3.2 Siamese Neural Network\nOur model embeds words of multiple languages into a com-\nmon space by sharing LSTM parameters among the lan-\nguages. In general, the model architecture of sharing param-\neters among different domains is called the “Siamese Neu-\nral Network” (Bromley et al. 1993). It is known to be very\neffective at representing data of different domains in a com-\nmon space, and this technique has been employed in many\nNLP tasks. For example, Johnson et al. (2016) built a neural\nmachine translation model whose encoder and decoder pa-\nrameters are shared among multiple languages. They have\nobserved that sentences of multiple languages are mapped\ninto a common space, and that has made it possible to per-\nform zero-shot translation. Rudramurthy, Khapra, and Bhat-\ntacharyya (2016) share LSTM networks of their named en-\ntity recognition model across multiple languages, and im-\nprove the performance in resource-poor languages. Note that\nthese models are fully supervised and require parallel data to\nobtain cross-lingual representations. Our model, on the other\nhand, does not require any parallel or cross-lingual data, and\nit acquires cross-lingual word embeddings through ﬁnding a\ncommon language structure in an unsupervised way.\n4 Experiments\n4.1 Data sets\nWe considered two learning scenarios that we deem realistic\nfor low-resource languages:\n1. Only a small amount of monolingual data are available.\n2. The domains of monolingual corpora are different across\nlanguages.\nFor the ﬁrst case, we used the News Crawl 2012 mono-\nlingual corpus for every language except for Finnish, for\nwhich we used News Crawl 2014. These data are provided\nby WMT20131 and 20172. We randomly extracted 50k sen-\ntences in each language, and used them as training data.\n1http://www.statmt.org/wmt13/\ntranslation-task.html\n2http://www.statmt.org/wmt17/\ntranslation-task.html\nWe also extracted 100k, 150k, 200k, and 250k sentences\nand analyzed the impact of the data size. For the second\nscenario, we used the Europarl corpus (Koehn 2005) as an\nEnglish monolingual corpus, and the News Crawl corpus\nfor the other languages. We randomly extracted one mil-\nlion sentences from each corpus and used them as training\ndata. The full vocabulary sizes of the Europarl and News\nCrawl corpora in English were 79258 and 265368 respec-\ntively, indicating the large difference of the domains. We\ndid not use any validation data during the training. We to-\nkenized and lowercased these corpora using Moses toolkit3.\nWe evaluated models in the pairs of{French, German, Span-\nish, Finnish, Russian, Czech}-English.\n4.2 Evaluation\nIn this work, we evaluate our methods on a word align-\nment task. Given a list of M words in a source language\ns [x1,x2,...,x M ] and target language t[y1,y2,...,y M ], the\nword alignment task is to ﬁnd one-to-one correspondence\nbetween these words. If a model generates accurate cross-\nlingual word embeddings, it is possible to align words prop-\nerly by measuring the similarity of the embeddings. In our\nexperiment, we used the bilingual dictionary data published\nby Conneau et al. (2017), and extracted 1,000 unique pairs of\nwords that are included in the vocabulary of the News Crawl\ndata of from 50k to 300k sentences. As a measurement of\nthe word embeddings, we used cross-domain similarity local\nscaling (CSLS), which is also used in Conneau et al. (2017)\nand Artetxe, Labaka, and Agirre (2018) . CSLS can mitigate\nthe hubness problem in high-dimensional spaces, and can\ngenerally improve matching accuracy. It takes into account\nthe mean similarity of a source language embedding xto its\nKnearest neighbors in a target language:\nrT(x) = 1\nK\n∑\ny∈NT (x)\ncos(x,y), (9)\nwhere cos is the cosine similarity and NT (x) denotes the K\nclosest target embeddings to x. Following their suggestion,\nwe set K as 10. rR(y) is deﬁned in a similar way for any\ntarget language embedding y. CSLS(x, y) is then calculated\nas follows:\nCSLS(x,y) = 2cos(x,y) −rT(x) −rS(y). (10)\nFor each source word xi, we extracted the ktarget words\nthat have the highest CSLS scores ( k = 1 or 5). However,\nsince the value of rT(x) does not affect the result of this\nevaluation, we omit the score from CSLS in our experi-\nments. We report the precision p@k: how often the correct\ntranslation of a source word xi is included in the kextracted\ntarget words.\n4.3 Baseline\nAs baselines, we compared our model to that of Conneau\net al. (2017) and Artetxe, Labaka, and Agirre (2018). Con-\nneau et al. (2017) aim to ﬁnd a mapping matrix W based on\n3https://github.com/moses-smt/\nmosesDecoder\nadversarial training. The discriminator is trained to distin-\nguish the domains (i.e. language) of the embeddings, while\nthe mapping is trained to fool the discriminator. Then, W is\nused to match frequent source and target words, and induce\na bilingual dictionary. Given the pseudo dictionary, a new\nmapping matrix W is then trained in the same manner as a\nsupervised method, which solves the Orthogonal Procrustes\nproblem:\nW∗= arg min\nW\n∥WX −Y∥F = UV T,\ns.t. U\n∑\nVT = SVD(YXT).\nThis training can be iterated using the new matrix W to in-\nduce a new bilingual dictionary. This method assumes that\nthe frequent words can serve as reliable anchors to learn a\nmapping. Since they suggest normalizing word embeddings\nin some language pairs, we evaluated their method with and\nwithout normalization. Artetxe, Labaka, and Agirre (2018)\nuse a different approach and employ a robust self-learning\nmethod. First, they roughly align words based on the similar-\nity of word emebeddings. Then, they repeat the self-learning\napproach, where they alternatively update a mapping func-\ntion and word alignment.\nTo implement the baseline methods, we used the code\npublished by the authors 4,5 . To obtain monolingual word\nembeddings, we used word2vec (Mikolov et al. 2013b).\nNote that these embeddings were used only for the base-\nlines, but not for ours since our method does not require any\npre-trained embeddings. For a fair comparison, we used the\nsame monolingual corpus with the same vocabulary size for\nthe baselines and our model.\n4.4 Training Settings\nWe preprocessed monolingual data and generated mini-\nbatches for each language. For each iteration, our model al-\nternately read mini-batches of each language, and updated\nits parameters every time it read one mini-batch. We trained\nour model for 10 epochs with the mini-batch size 64. The\nsize of word embedding was set as 300, and the size of\nLSTM hidden states was also set as 300 for the forward\nand backward LSTMs, respectively. Dropout (Srivastava et\nal. 2014) is applied to the hidden state with its rate 0.3. We\nused SGD (Bottou 2010) as an optimizer with the learning\nrate 1.0. Our parameters, which include word embeddings,\nwere uniformly initialized in [-0.1, 0.1], and gradient clip-\nping (Pascanu, Mikolov, and Bengio 2013) was used with\nthe clipping value 5.0. We included in the vocabulary the\nwords that were used at least a certain number of times. For\nthe News Crawl corpus, we set the threshold as 3, 5, 5, 5\n,5, 10, and 20 for 50k, 100k, 150k, 200k, 250k, 300k and\n1m sentences. For the Europarl corpus, we set the value as\n10. We fed 10000 frequent words into the discriminator in\nConneau et al. (2017).\nAs a model selection criterion, we employed a similar\nstrategy used in the baseline. More speciﬁcally, we consid-\n4https://github.com/facebookresearch/MUSE\n5https://github.com/artetxem/vecmap\nfr-en de-en es-en ﬁ-en ru-en cs-en\np@1 p@5 p@1 p@5 p@1 p@5 p@1 p@5 p@1 p@5 p@1 p@5\nRANDOM 0.1 0.5 0.1 0.5 0.1 0.5 0.1 0.5 0.1 0.5 0.1 0.5\nConneau et al. (2017) 2.5 7.7 0.6 3.5 3.0 9.0 0.0 0.4 0.1 0.7 0.0 1.2\nConneau et al. (2017) + normalize 0.7 3.0 0.6 3.3 0.5 2.6 0.0 0.4 0.0 0.5 0.1 0.3\nArtetxe, Labaka, and Agirre (2018) 2.4 6.8 1.0 4.5 1.0 5.0 0.0 0.1 0.2 0.9 0.4 1.6\nOURS 7.3 16.5 4.6 12.0 8.2 18.0 2.7 7.3 2.7 6.9 3.7 10.2\nTable 1: Word alignment average precisions p@1 and 5 when models are trained on 50k sentences of source and target lan-\nguages.\nfr-en de-en es-en ﬁ-en ru-en cs-en\np@1 p@5 p@1 p@5 p@1 p@5 p@1 p@5 p@1 p@5 p@1 p@5\nRANDOM 0.1 0.5 0.1 0.5 0.1 0.5 0.1 0.5 0.1 0.5 0.1 0.5\nConneau et al. (2017) 0.8 4.2 0.2 1.3 1.4 4.6 0.1 0.6 0.6 2.1 0.5 1.3\nConneau et al. (2017) + normalize 0.2 1.2 0.1 0.8 0.2 1.0 0.2 1.1 0.3 1.1 0.3 1.2\nArtetxe, Labaka, and Agirre (2018) 6.1 14.7 1.1 5.0 29.9 45.3 0.5 2.2 0.1 1.2 0.5 2.2\nOURS 12.7 26.6 3.4 10.0 14.9 28.6 3.0 8.5 3.8 11.1 4.0 10.8\nTable 2: Word alignment average precisions p@1 and 5 when models are trained on one million sentences extracted from\ndifferent domains between source and target languages.\nsource word (es) OURS Artetxe, Labaka, and Agirre (2018)\ntop 1 top 2 top 3 top 1 top 2 top 3\nacusado accused designed captured english dark drama\nactor actor artist candidate appointment actor charlie\ncasi almost approximately about around age capita\naunque although but drafting about are been\nd´ıas days decades decade bodies both along\nactualmente currently clearly essentially comes continued candidates\ncontiene contains deﬁnes constitutes barrier etiquette commissioned\ncap´ıtulo chapter episode cause arriving bulls dawn\nTable 3: Some examples when Spanish and English words matched correctly by our model using 50k sentences, but not by\nArtetxe, Labaka, and Agirre (2018). Each column indicates 1st, 2nd, and 3rd most similar English words to each Spanish word.\nEnglish words in bold font are translations of each Spanish word.\nered the 3,000 most frequent source words, and used CSLS\nexcluding rT(x) to generate a translation for each of them\nin a target language. We then computed the average CSLS\nscores between these deemed translations, and used them as\na validation metric.\n5 Results\n5.1 Bilingual Word Embeddings\nFirst, we trained our model and obtained cross-lingual em-\nbeddings between two languages for each language pair. We\nreport our results under the two scenarios that we consid-\nerted realistic when dealing with minor languages. In the\nﬁrst scenario, we trained our model on a very small amount\nof data, and in the second scenario the model was trained\non a large amount of data extracted from different domains\nbetween source and target languages.\nTable 1 illustrates the results of the word alignment task\nunder the low-resource scenario. R ANDOM is the expected\naccuracy when words are aligned at random. The result\nshows that our model outperformed the baseline methods\nsigniﬁcantly in all of the language pairs, indicating that ours\nis more robust in a low-resource senario. On the other hand,\nthe baseline methods got poor performance, especially in the\nFinnish and English pair. Even though Artetxe, Labaka, and\nAgirre (2018) report that their method achieves good per-\nformance in that language pair, our experiment has demon-\nstrated that it does not perform well without a large amount\nof data.\nTable 2 shows the results when the domains of training\ndata used to obtain source and target embeddings are differ-\nent. Our method again outperformed the baselines to a large\nextent except for the Spanish-English pair. The poor perfor-\nmance of Conneau et al. (2017) in such a setting has also\nbeen observed in Søgaard, Ruder, and Vuli ´c (2018), even\nthough much larger data including Wikipedia were used for\ntraining in their experiments.\nTable 3 shows some examples when Spanish and En-\nglish words were correctly matched by our model, but\nnot by Artetxe, Labaka, and Agirre (2018) under the low-\nresource scenario. The table lists the three most similar\nEnglish words to each Spanish source word. Our method\nFigure 2: Comparison of p@1 accuracy of German-English\npair between supervised word mapping method and our\nmodel on 50k sentences. The x axis indicates the number\nof pairs of words n(= 0,50,100,150,..., 450, 500) that were\nused for the supervised method, but not for ours, to map\nword embedding spaces in two languages.\nsuccessfully matched similar or semantically related words\nto the source words, indicating that our method obtained\ngood cross-lingual embeddings. For example, to the Span-\nish source word “casi”, our model aligned its translation\n“almost” and also very similar words “approximately” and\n“about”. Indeed, many of the aligned target words in our\nmodel have the same part of speech tag as that of the source\nword, suggesting that our model captured a common lan-\nguage structure such as rules of word order and roles of\nvocabulary by sharing LSTMs. On the other hand, Artetxe,\nLabaka, and Agirre (2018) could not align words properly,\nand there do not seem to exist consistent relations between\nthe source and extracted words.\nComparison to Supervised Method To further inves-\ntigate the effectiveness of our model, we compared our\nmethod to a supervised method under the low-resource set-\nting. The method is a slight modiﬁcation of Conneau et\nal. (2017): it is trained using a bilingual dictionary, and\nlearns a mapping from the source to the target space us-\ning iterative Procrustes alignment. We used the code pro-\nvided by Conneau et al. (2017). Figure 2 compares p@1\naccuracy between the supervised method and our model in\nthe German-English pair. The x-axis denotes the number of\nseeds of the bilingual dictionary that were used for the super-\nvised method, but not for ours. The ﬁgure illustrates that our\nmethod achieved a better result than the supervised method\nwhen the number of seeds was less than 400, which is sur-\nprising given that our model is fully unsupervised.\nImpact of Data Size We changed the size of the training\ndata by 50k sentences, and analyzed how the performance\nof the baselines and our model changed. Figure 3 illustrates\nhow the performance changed depending on the data size.\nIt shows that our method achieved a comparable or bet-\nter result than the baseline methods in all of the language\npairs when the number of sentences was not more than\nFigure 3: Graphs show the change in p@1 accuracy of\neach language pair as the size of training data increases.\nThe x-axis denotes the number of sentences (thousand) in\nthe monolingual training data of the source and target lan-\nguages.\n100k. In the closely related language pairs such as {French,\nGerman, Spanish}-English, the baselines performed better\nwhen there were enough amount of data. Among the dis-\ntant languages such as {Finnish, Czech}-English, our model\nachieved better results overall, while the baseline methods,\nespecially Conneau et al. (2017) got very poor results.\n5.2 Quadrilingual Word Embeddings\nOur results of the word alignment task have shown that our\nmodel can jointly learn bilingual word embeddings by cap-\nturing the common structure of two languages. This suc-\ncess has raised another question: “Is it also possible to learn\na common structure of more than two languages?” To ex-\namine this intriguing question, we trained our model that\nfr-en de-en es-en\n50k 100k 300k 50k 100k 300k 50k 100k 300k\nOURS (BILINGUAL ) 7.3 12.8 23.1 4.6 5.7 13.1 8.2 14.0 27.9\nOURS (QUADRILINGUAL ) 8.4 12.3 25.6 4.7 8.4 16.6 7.6 13.8 25.3\nTable 4: Word alignment average precisions p@1 in each language pair when 50k, 100k, and 300k sentences were used for\ntraining. O URS (BILINGUAL ) denotes the accuracy of the models that read source and target languages, generating bilingual\nword embeddings. O URS (QUADRILINGUAL ) denotes the accuracy of one model that reads all four languages, producing\nquadrilingual word embeddings.\nEn (source) Fr top1 De top1 Es top1\ndeclared d´eclar´e erkl¨art declarado\nalways toujours immer siempre\nare sont sind est´an\nafter apr`es nachdem despu´es\ndied d´ec´ed´e starb murieron\nTable 5: Examples of words that were correctly aligned by\nOURS (QUADRILINGUAL ) among the four languages.\nencoded four linguistically similar languages, namely En-\nglish, French, Spanish, and German, and aimed to capture\nthe common structure among them. We expect that word\nembeddings of the four languages should be mapped into a\ncommon space, generating what we call quadrilingual word\nembeddings. Table 4 describes the result of the word align-\nment when using bilingual and quadrilingual word embed-\ndings of our model. While quadrilingual word embeddings\nperformed slightly worse than bilingual ones in the Spanish-\nEnglish pair, they brought large gains in the German-English\nalignment task and achieved comparable performance over-\nall. Our model successfully mapped word embeddings of\nthe four languages into a common space, making it possi-\nble to measure the similarity of words across the multiple\nlanguages.\nTo investigate whether quadrilingual embeddings were\nactually mapped into a common space, we aligned each En-\nglish word to French, German and Spanish words in the\nbilingual dictionary. Table 5 describes the words that were\ncorrectly aligned among the four languages. This result in-\ndicates that these equivalent words have very similar repre-\nsentations, and that means our model successfully embedded\nthese languages into a common space. Figure 4 illustrates\nthe scatter plot of the embeddings of the most 1,000 fre-\nquent words in each corpus of the four languages. It clearly\nshows that the word embeddings were clustered based on\ntheir meanings rather than their language. For example, the\nprepositions of the four languages (e.g. de (fr, es), of (en),\nvon (de)) were mapped into the bottom-right area, and de-\nterminers (e.g. la (fr, es), the (en), der, die, das (de)) were in\nthe bottom-left area. Near the area where the embedding of\n‘<BOS>’ was mapped, the words from which a new sen-\ntence often starts (‘,’ , et (fr), y(es), und (de), and (en)) were\nmapped.\n<BOS>\n<unk> (en, de. es, fr)\nde,del,a(es), de,des(fr), von(de), of(en) \nder(de)    la (es) the (en) la(fr)   die, das(de), \n. (en, de, es, fr)     ? (en, de, es, fr)\nand (en)und (de)et (fr)y (es)\n, (en,de,es, fr)\na (es) à (fr) to(en)\nque (es), que, qui(fr) ﬁrst (en)     last(en) \nyears(en)años(es)\nans(fr)\nFigure 4: Scatter plot of cross-lingual word embeddings\nof French, English, German and Spanish obtained by our\nmodel. The embeddings are reduced to 2D using tSNE\n(Maaten and Hinton 2008).\n6 Conclusion\nIn this paper, we proposed a new unsupervised method that\nlearns cross-lingual embeddings without any parallel data.\nOur experiments of a word alignment task in six language\npairs have demonstrated that our model signiﬁcantly outper-\nforms existing unsupervised word translation models in all\nthe language pairs under a low resource situation. Our model\nalso achieved better results in ﬁve language pairs when the\ndomains of monolingual data are different across language.\nWe also compared our unsupervised method to a supervised\none in the German-English word alignment task, and our\nmodel achieved a better result than the supervised method\nthat were trained with 350 pairs of words from a bilingual\ndictionary. Our model also succeeded in obtaining cross-\nlingual embeddings across four languages, which we call\nquadrilingual embeddings. These embeddings enabled us to\nalign equivalent words among four languages in an unsu-\npervised way. The visualization of the quadrilingual embed-\ndings showed that these embeddings were actually mapped\ninto a common space, and words with similar meanings had\nclose representations across different languages.\nPotential future work includes extending our approach to\na semi-supervised method that utilizes a bilingual dictionary.\nOne possible idea is to set an additional loss function in our\nmodel that decreases the distance of embeddings of equiva-\nlent words across languages.\nReferences\nAdams, O.; Makarucha, A.; Neubig, G.; Bird, S.; and Cohn,\nT. 2017. Cross-lingual word embeddings for low-resource\nlanguage modeling. In Proceedings of the 15th Conference\nof the European Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers, 937–947. Asso-\nciation for Computational Linguistics.\nArjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasser-\nstein generative adversarial networks. In Precup, D., and\nTeh, Y . W., eds.,Proceedings of the 34th International Con-\nference on Machine Learning, volume 70 of Proceedings of\nMachine Learning Research, 214–223. International Con-\nvention Centre, Sydney, Australia: PMLR.\nArtetxe, M.; Labaka, G.; Agirre, E.; and Cho, K. 2018. Un-\nsupervised neural machine translation. In Proceedings of\nthe Sixth International Conference on Learning Representa-\ntions.\nArtetxe, M.; Labaka, G.; and Agirre, E. 2017. Learning\nbilingual word embeddings with (almost) no bilingual data.\nIn Proceedings of the 55th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Pa-\npers), 451–462. Vancouver, Canada: Association for Com-\nputational Linguistics.\nArtetxe, M.; Labaka, G.; and Agirre, E. 2018. A robust self-\nlearning method for fully unsupervised cross-lingual map-\npings of word embeddings. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), 789–798. Association for\nComputational Linguistics.\nBottou, L. 2010. Large-scale machine learning with\nstochastic gradient descent. In Lechevallier, Y ., and Saporta,\nG., eds., Proceedings of the 19th International Conference\non Computational Statistics (COMPSTAT’2010), 177–187.\nParis, France: Springer.\nBromley, J.; Guyon, I.; LeCun, Y .; S¨ackinger, E.; and Shah,\nR. 1993. Signature veriﬁcation using a ”siamese” time\ndelay neural network. In Proceedings of the 6th Interna-\ntional Conference on Neural Information Processing Sys-\ntems, NIPS’93, 737–744. San Francisco, CA, USA: Morgan\nKaufmann Publishers Inc.\nConneau, A.; Lample, G.; Ranzato, M.; Denoyer, L.; and\nJ´egou, H. 2017. Word translation without parallel data.\narXiv preprint arXiv:1710.04087.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial nets. In Ghahramani, Z.;\nWelling, M.; Cortes, C.; Lawrence, N. D.; and Weinberger,\nK. Q., eds., Advances in Neural Information Processing Sys-\ntems 27. Curran Associates, Inc. 2672–2680.\nHochreiter, S., and Schmidhuber, J. 1997. Long short-term\nmemory. Neural Comput.9(8):1735–1780.\nJohnson, M.; Schuster, M.; Le, Q. V .; Krikun, M.; Wu, Y .;\nChen, Z.; Thorat, N.; Vi ´egas, F. B.; Wattenberg, M.; Cor-\nrado, G.; Hughes, M.; and Dean, J. 2016. Google’s multilin-\ngual neural machine translation system: Enabling zero-shot\ntranslation. CoRR abs/1611.04558.\nKoehn, P. 2005. Europarl: A Parallel Corpus for Statisti-\ncal Machine Translation. In Conference Proceedings: the\ntenth Machine Translation Summit, 79–86. Phuket, Thai-\nland: AAMT.\nLample, G.; Denoyer, L.; and Ranzato, M. 2017. Unsuper-\nvised machine translation using monolingual corpora only.\narXiv preprint arXiv:1711.00043.\nMaaten, L., and Hinton, G. 2008. Visualizing high-\ndimensional data using t-sne. 9:2579–2605.\nMikolov, T.; Karaﬁ´at, M.; Burget, L.; Cernock´y, J.; and Khu-\ndanpur, S. 2010. Recurrent neural network based language\nmodel. In INTERSPEECH.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a.\nEfﬁcient estimation of word representations in vector space.\nCoRR abs/1301.3781.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013b.\nEfﬁcient estimation of word representations in vector space.\nCoRR abs/1301.3781.\nPascanu, R.; Mikolov, T.; and Bengio, Y . 2013. On the difﬁ-\nculty of training recurrent neural networks. In Proceedings\nof the 30th International Conference on Machine Learning,\nICML 2013, Atlanta, GA, USA, 16-21 June 2013, 1310–\n1318.\nRudramurthy, V .; Khapra, M. M.; and Bhattacharyya, P.\n2016. Sharing network parameters for crosslingual named\nentity recognition. CoRR abs/1607.00198.\nSchuster, M., and Paliwal, K. 1997. Bidirectional recurrent\nneural networks. Trans. Sig. Proc.45(11):2673–2681.\nSøgaard, A.; Ruder, S.; and Vuli ´c, I. 2018. On the Limita-\ntions of Unsupervised Bilingual Dictionary Induction.ArXiv\ne-prints.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: A simple way to pre-\nvent neural networks from overﬁtting. Journal of Machine\nLearning Research15:1929–1958.\nXiao, M., and Guo, Y . 2014. Distributed word representation\nlearning for cross-lingual dependency parsing. In Proceed-\nings of the Eighteenth Conference on Computational Natu-\nral Language Learning, 119–129. Association for Compu-\ntational Linguistics.\nZhang, M.; Liu, Y .; Luan, H.; and Sun, M. 2017a. Adver-\nsarial training for unsupervised bilingual lexicon induction.\nIn Proceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n1959–1970. Vancouver, Canada: Association for Computa-\ntional Linguistics.\nZhang, M.; Liu, Y .; Luan, H.; and Sun, M. 2017b. Earth\nmover’s distance minimization for unsupervised bilingual\nlexicon induction. InProceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing, 1934–\n1945. Association for Computational Linguistics.\nZou, W. Y .; Socher, R.; Cer, D.; and Manning, C. D. 2013.\nBilingual word embeddings for phrase-based machine trans-\nlation. In Proceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing, 1393–1398. As-\nsociation for Computational Linguistics."
}