{
  "title": "Depression symptoms modelling from social media text: an LLM driven semi-supervised learning approach",
  "url": "https://openalex.org/W4393949820",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2990129773",
      "name": "Nawshad Farruque",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118780889",
      "name": "Randy Goebel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2314429266",
      "name": "Sudhakar Sivapalan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2308328903",
      "name": "Osmar R. Zaïane",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6628750762",
    "https://openalex.org/W2741004402",
    "https://openalex.org/W1989420684",
    "https://openalex.org/W2252031683",
    "https://openalex.org/W2182854643",
    "https://openalex.org/W2402700",
    "https://openalex.org/W4210461484",
    "https://openalex.org/W3022552813",
    "https://openalex.org/W2315409615",
    "https://openalex.org/W2613843855",
    "https://openalex.org/W2511501696",
    "https://openalex.org/W2618742246",
    "https://openalex.org/W2163568299",
    "https://openalex.org/W2592472466",
    "https://openalex.org/W2786643838",
    "https://openalex.org/W2315849398",
    "https://openalex.org/W2114595254",
    "https://openalex.org/W2112205306",
    "https://openalex.org/W2962848499",
    "https://openalex.org/W2058971120",
    "https://openalex.org/W3199956316",
    "https://openalex.org/W2801113291",
    "https://openalex.org/W2740966010",
    "https://openalex.org/W1985452637",
    "https://openalex.org/W2797568851",
    "https://openalex.org/W2790322417",
    "https://openalex.org/W3111490291",
    "https://openalex.org/W2766165088",
    "https://openalex.org/W3101267588"
  ],
  "abstract": "Abstract A fundamental component of user-level social media language based clinical depression modelling is depression symptoms detection (DSD). Unfortunately, there does not exist any DSD dataset that reflects both the clinical insights and the distribution of depression symptoms from the samples of self-disclosed depressed population. In our work, we describe a semi-supervised learning (SSL) framework which uses an initial supervised learning model that leverages (1) a state-of-the-art large mental health forum text pre-trained language model further fine-tuned on a clinician annotated DSD dataset, (2) a Zero-Shot learning model for DSD, and couples them together to harvest depression symptoms related samples from our large self-curated depressive tweets repository (DTR). Our clinician annotated dataset is the largest of its kind. Furthermore, DTR is created from the samples of tweets in self-disclosed depressed users Twitter timeline from two datasets, including one of the largest benchmark datasets for user-level depression detection from Twitter. This further helps preserve the depression symptoms distribution of self-disclosed tweets. Subsequently, we iteratively retrain our initial DSD model with the harvested data. We discuss the stopping criteria and limitations of this SSL process, and elaborate the underlying constructs which play a vital role in the overall SSL process. We show that we can produce a final dataset which is the largest of its kind. Furthermore, a DSD and a Depression Post Detection model trained on it achieves significantly better accuracy than their initial version.",
  "full_text": "Vol.:(0123456789)\nLanguage Resources and Evaluation (2024) 58:1013–1041\nhttps://doi.org/10.1007/s10579-024-09720-4\n1 3\nORIGINAL PAPER\nDepression symptoms modelling from social media text: \nan LLM driven semi‑supervised learning approach\nNawshad Farruque1 · Randy Goebel1 · Sudhakar Sivapalan2 · Osmar R. Zaïane1\nAccepted: 9 January 2024 / Published online: 4 April 2024 \n© The Author(s) 2024\nAbstract\nA fundamental component of user-level social media language based clinical depres-\nsion modelling is depression symptoms detection (DSD). Unfortunately, there does \nnot exist any DSD dataset that reflects both the clinical insights and the distribution \nof depression symptoms from the samples of self-disclosed depressed population. \nIn our work, we describe a semi-supervised learning (SSL) framework which uses \nan initial supervised learning model that leverages (1) a state-of-the-art large men-\ntal health forum text pre-trained language model further fine-tuned on a clinician \nannotated DSD dataset, (2) a Zero-Shot learning model for DSD, and couples them \ntogether to harvest depression symptoms related samples from our large self-curated \ndepressive tweets repository (DTR). Our clinician annotated dataset is the largest of \nits kind. Furthermore, DTR is created from the samples of tweets in self-disclosed \ndepressed users Twitter timeline from two datasets, including one of the largest \nbenchmark datasets for user-level depression detection from Twitter. This further \nhelps preserve the depression symptoms distribution of self-disclosed tweets. Sub-\nsequently, we iteratively retrain our initial DSD model with the harvested data. We \ndiscuss the stopping criteria and limitations of this SSL process, and elaborate the \nunderlying constructs which play a vital role in the overall SSL process. We show \nthat we can produce a final dataset which is the largest of its kind. Furthermore, a \nDSD and a Depression Post Detection model trained on it achieves significantly bet-\nter accuracy than their initial version.\nKeywords Semi-supervised learning · Zero-shot learning · Depression symptoms \ndetection · Depression detection · Bidirectional Encoder Representations from \nTransformers (BERT) · Mental-BERT\nExtended author information available on the last page of the article\n1014 N. Farruque et al.\n1 3\n1 Introduction\nAccording to Boyd et al. (1982), in developed countries, around 75% of all psy -\nchiatric admissions are young adults with depression. The fourth leading cause of \ndeath in young adults is suicide, which is closely related to untreated depression \n(World Health Organization, 2023). Moreover, traditional survey-based depres-\nsion screening may be in-effective due to the cognitive bias of the patients who \nmay not be truthful in revealing their depression condition. So there is a huge \nneed for an effective, inexpensive and almost real time intervention for depres-\nsion in this high risk population. Interestingly, among young adults, social media \nis very popular where they share their day to day activities and the availabil-\nity of social media services is growing exponentially year by year (O’Keeffe & \nClarke-Pearson, 2011). Moreover, according to the research (Gowen et al., 2012; \nNaslund et  al., 2014, 2016), it has been found that depressed people who are \notherwise socially aloof, show increased use of social media platforms to share \ntheir daily struggles, connect with others who might have experienced the same \nand seek help. So, in this research we focus on identifying depression symptoms \nfrom a user’s social media posts as one of the strategies for early identification of \ndepression. Earlier research confirms that signs of depression can be identified \nin the language used in social media posts (Coppersmith et al., 2015; De Choud-\nhury & De, 2014; De Choudhury et al., 2013; Losada & Crestani, 2016; Reece \net al., 2017; Rude et al., 2004; Seabrook et al., 2018; Shen et al., 2017; Trotzek \net al., 2018; Yadav et al., 2020; Yazdavar et al., 2017). Based on this background, \nlinguistic features, such as n-grams, psycholinguistic and sentiment lexicons, \nword and sentence embeddings extracted from the social media posts can be very \nuseful for detecting depression, especially when compared to other social media \nrelated features which are not language specific, such as social network struc-\nture of depressed users and their posting behavior. In addition, the majority of \nthis background research focused on public social media data, i.e., Twitter and \nReddit mental health forums for user-level depression detection, because of the \nrelative ease of accessing such datasets (unlike Facebook and other social media \nwhich have strict privacy policies). All this background placed emphasis on signs \nof depression detection, however, they lacked the inclusion of clinical depres-\nsion modelling; such requires extensive effort in building a depression symptoms \ndetection model (Sect.  4.2). Some of the earlier research (Ma et al., 2017; Mow -\nery et al., 2016; Safa et al., 2022; Tlelo-Coyotecatl et al., 2022; Yazdavar et al., \n2017; Yadav et  al., 2020) has focused on depression symptoms detection but \nthey do not attempt to create a clinician-annotated dataset, and later use existing \nstate-of-the-art language models to expand it. All the previous research does not \nattempt to curate the possible depression candidate dataset from self-disclosed \ndepressed users’ timelines. Therefore the main motivation of this work arises \nfrom the following: \n1. Clinician-annotated dataset creation from depressed users tweets: Through lev-\neraging our existing datasets from self disclosed depressed users and trained \n1015\n1 3\nDepression symptoms modelling from social media text: an LLM…\nDepression Post Detection (DPD) model (which is a binary model for detecting \nsigns of depression), we want to curate a clinician-annotated dataset for depres-\nsion symptoms. This is a more “in-situ” approach for harvesting depression symp-\ntoms posts compared to crawled tweets for depression symptoms using depression \nsymptoms keywords, as done in most of the earlier literature (Mowery et al., \n2016, 2017). We call it in-situ because this approach respects the natural distri-\nbution of depression symptoms samples found in the self-disclosed depressed \nusers’ timelines. Although Yadav et al. (2020) collected samples in-situ as well, \nour clinician-annotated dataset is much bigger and annotation is more rigorous \n(Sect. 5.1).\n2. Gather more data that reflects clinical insight: Starting from the small dataset \nfound at (1) and a DSD model trained on that, we want to iteratively harvest more \ndata and retrain our model for our depression symptoms modelling or DSD task.\nOur dataset made of both clinician annotated and harvested tweets with signs of \ndepression symptoms is the largest of its kind, to the best of our knowledge.\n2  Methodology\nTo achieve the goals mentioned earlier, we divide our depression symptoms \nmodelling into two parts: (1) Clinician annotated dataset curation: here we first \npropose a process to create our annotation candidate dataset from our existing \ndepressive tweets from self-disclosed depressed Twitter users. We later annotate \nthis dataset with the help of a clinician amongst others, that helps us achieve our \nfirst goal (Sect.  3) and (2) Semi-supervised Learning (SSL): we then describe how \nwe leverage that dataset to learn our first sets of DPD and DSD models and even-\ntually make them robust through iterative data harvesting and retraining or SSL \n(McClosky et al., 2006) (Sect. 4 ).\n3  Datasets\nWe create Depression-Candidate-Tweets dataset from the timeline of depressed \nusers in IJCAI-2017 (Shen et al., 2017) who disclosed their depression condition \nthrough a self-disclosure statement, such as: \"I (am / was / have) been diagnosed \nwith depression\" and UOttawa (Jamil et al., 2017) datasets where the users were \nverified by annotators about their ongoing depression episodes. Later, we further \nfilter it with a DPD model (discussed in Sect.  3.1) for depressive tweets and cre-\nate the depressive tweets repository (DTR) which is used in our SSL process to \nharvest in-situ tweets for depression symptoms. We also separate a portion of the \nDTR for clinician annotation for depression symptoms (Fig. 3 ).\n1016 N. Farruque et al.\n1 3\n3.1  Clinician annotated dataset curation\nIn the overall DSD framework, depicted in Fig.  1, we are ultimately interested \nin creating a robust DPD and a DSD model which are initially trained on human \nannotated samples, called “DPD-Human” model and “DSD-Clinician” model as \ndepicted in Fig.  2. The suffixes with these model names, such as “Human,” indi-\ncates that this model leverages the annotated samples from both non-clinicians \nand clinicians; “Clinician” indicates that this model leverages the samples for \nwhich the clinician’s annotation is taken as more important (more explanation is \nFig. 1  DSD modelling algorithm\nFig. 2  Semi-supervised learning process at a high level\n1017\n1 3\nDepression symptoms modelling from social media text: an LLM…\nprovided later in Sect.  3.4). At the beginning of this process, we have only a small \nhuman annotated dataset for depression symptoms augmented with depression \nposts from external organizations (i.e. D2S (Yadav et al., 2020) and DPD-Vioules \n(Vioulès et  al., 2018) datasets), no clinician annotated depression symptoms \nsamples, and a large dataset from self-disclosed depressed users (i.e IJCAI-2017 \ndataset). We take the following steps to create our first set of clinician annotated \ndepression symptoms dataset and DTR which we will use later for our SSL.\n1. We start the process with the help of a DPD model, which we call DPD Majority \nVoting model (DPD-MV). It consists of a group of DPD models (Farruque et al., \n2019), where each model leverages pre-trained word embedding (both augmented \n(ATE) and depression specific (DSE)) and sentence embedding (USE), further \ntrained on a small set of human annotated depressive tweets and a Zero-Shot \nLearning (ZSL) model (USE-SE-SSToT). This ZSL model helps determine the \nsemantic similarity between a tweet and all the possible depression symptoms \ndescriptors and returns the top-k corresponding labels. It also provides a score \nfor each label, based on cosine distance. More details are provided in a previous \npaper (Farruque et al., 2021). Subsequently, the DPD-MV model takes the major-\nity voting of these models for detecting depressive tweets.\n2. We then apply DPD-MV on the sets of tweets collected from depressed users’ \ntimelines (or Depression-Candidate-Tweets, (Fig. 3) to filter control tweets. The \nresultant samples, after applying DPD-MV is referred to as Depression Tweet \nRepository or DTR. We later separate a portion of this dataset, e.g., 1500 depres-\nsive tweets for human annotation which we call DSD-Clinician-Tweets dataset. \nDetails of the annotation process are described in Sect.  3.4.\n3. We train our first DSD model using this dataset, then use this model to harvest \nmore samples from DTR. An outline of the DTR and DSD-Clinician-Tweets \ncuration process is provided in Fig.  3. We describe the details of this process in \nFig. 3  DSD-Clinician-Tweets and DTR curation process\n1018 N. Farruque et al.\n1 3\nSect. 4.2, but describe each of its building blocks in the next sections. In Table 1 \nwe provide relevant datasets description.\n3.2  Annotation task description\nOur annotation task consists of labelling a tweet for either (1) one or more of 10 \nsymptoms of depression (See next section), (2) No Evidence of Depression (NoED), \n(3) Evidence of Depression (ED) or (4) Gibberish. We have 10 labels instead of the \ntraditional nine depression symptoms labels because we separate the symptom “Agi-\ntation / Retardation” into two categories so that our model can separately learn and \ndistinguish these labels, unlike previous research (Yadav et al., 2020). NoED indi-\ncates the absence of any depression symptoms expressed in a tweet. ED indicates \nmultiple symptoms of depression expressed in a tweet in a way so that it’s hard to \nspecifically pinpoint these combined depression symptoms in that tweet. Gibberish \nis a tweet less than three words long and, due to the result of crawling or data pre-\nprocessing, the tweet is not complete and it’s hard to infer any meaningful context.\n3.3  Annotation guideline creation\nTo create the annotation guideline for the task, we analyze the textual descriptions \nof depression symptoms from most of the major depression rating scales, such as, \nPHQ-9, CES-D, BDI, MADRS and HAM-D (The classification of depression, \n2010). We also use DSM-5 as our reference for symptom descriptions. Based on \nthese descriptions of the symptoms from these resources and several meetings with \nour clinicians, we consolidate some of the most confusing samples of tweets from \nDTR and map them to one or more of those depression symptoms. We then create \nan annotation guideline with a clear description of the clinical symptoms of depres-\nsion that an annotator should look for in the tweets followed by relevant tweet exam-\nples for them including the confusing ones previously noted. We then separate a \nportion of 1500 samples from our DTR and provide it to the annotators along with \nour annotation guideline. During the annotation, we randomly assign a set of tweets \nmultiple times to calculate test-retest reliability scores. We find annotators annotate \nthe tweets consistently with the same annotation with 83% reliability based on the \ntest-retest reliability score. Our detailed guideline description is provided in Appen-\ndix 3.\nTable 1  Dataset\nDataset Sample size Comment\nDepression-Candidate-Tweets 42,691 Depressed users’ tweets\nDTR 6077 Depressive tweets repository\nDSD-Clinician-Tweets 1500 Clinician annotated tweets\n1019\n1 3\nDepression symptoms modelling from social media text: an LLM…\n3.4  Depression symptoms annotation process\nWe provide a portion of 1500 tweets from DTR for depression symptoms anno-\ntation by four annotators.\nAmong these annotators two have a clinical understanding of depression: one \nis a practicing clinician and the other one has a Ph.D. in Psychiatry.\nOur annotation process is based on the clinical understanding of depression \nas outlined in our guidelines. We take majority voting to assign a label for the \ntweet. In the absence of a majority, we assign a label based on the clinician’s \njudgment, if present, otherwise, we do not assign a label to that tweet. We call \nthis scheme Majority Voting with Clinician Preference (MVCP). Table  2 reports \nthe average Cohen’s kappa scores for each label and Annotator-Annotator, Anno-\ntator-MVCP, and All pairs (i.e. avg. on both of the previous schemes). Through \nout the paper by kappa score, we mean Cohen’s kappa score.\nWe observe fair to moderate kappa agreement score (0.38–0.53) among our \nannotators for all the labels. We also find, “Suicidal thoughts” and “Change in \nSleep Patterns” are the labels for which inter-annotator agreement is the highest \nand agreement between each annotator and MVCP is substantial for the same. \nAmong the annotators the order of the labels based on descending order of \nagreement score is as follows: Suicidal thoughts, Change in Sleep Patterns, Feel-\nings of Worthlessness, Indecisiveness, Anhedonia, Retardation, Weight change, \nNoED, Fatigue, Low mood, Gibberish, Agitation and ED. However, with MVCP, \nwe find moderate to substantial agreement (0.56–0.66). For all labels and anno-\ntators, we find a global inter-annotator agreement score (Krippendorff’s alpha) \nof 0.3064.\nTable 2  Pairwise kappa scores among annotators and MVCP for all the labels\nDepression-Symptom-Labels Average (Annots.) Average (Annots.–MVCP) Average (All)\nSuicidal thoughts 0.5319(±0.1045) 0.6296(±0.1227) 0.5710(±0.1167 )\nChange in Sleep Pattern 0.5171(±0.0770 ) 0.6162(±0.1034) 0.5568(±0.0973)\nFeelings of Worthlessness 0.4517(±0.1978) 0.6589(±0.2347) 0.5346(±0.2271)\nIndecisiveness 0.4475(±0.2164) 0.6378(±0.2479) 0.5236(±0.2370)\nAnhedonia 0.4434(±0.2383 ) 0.6037(±0.0915) 0.5076(±0.2030)\nRetardation 0.4382(±0.3030 ) 0.5672(±0.2446) 0.4898(±0.2746)\nWeight Change 0.4358(±0.1589) 0.6155(±0.2149) 0.5077(±0.1951 )\nNoED 0.4321(±0.2119) 0.5946(±0.2631) 0.4971(±0.2346)\nFatigue 0.4297(±0.1136) 0.5975(±0.2375) 0.4968(±0.1830)\nLow Mood 0.4251(±0.3041) 0.6454(±0.3730) 0.5132(±0.3327)\nGibberish 0.4172(±0.2606) 0.6626(±0.3272 ) 0.5154(±0.2991)\nAgitation 0.4008(±0.2066 ) 0.6505(±0.2571) 0.5007(±0.2498)\nED 0.3877(±0.0878 ) 0.5765(±0.2742) 0.4632(±0.1971)\n1020 N. Farruque et al.\n1 3\n3.5  Distribution analysis of the depression symptoms data\nIn this section, we provide symptom distribution analysis for D2S and DSD-Cli-\nnician-Tweets datasets. DSD-Clinician-Tweets dataset contains 1500 tweets. We \nthen create a clean subset of this dataset which holds clinicians’ annotations and \nonly tweets with depression symptoms, which we call DSD-Clinician-Tweets-\nOriginal (further detail is in Sect.  4.2.1). For D2S, we have 1584 tweets with dif-\nferent depression symptom labels. In Fig.  4, the top 3 most populated labels for \nthe DSD dataset are \"Agitation\", \"Feeling of Worthlessness\", and \"Low Mood\". \nHowever, for the D2S dataset, \"Suicidal Thought\" is the most populated label fol-\nlowed by “Feelings of Worthlessness” and “Low Mood”, just like DSD. We use \nthe D2S dataset because D2S crawled tweets from self-reported depressed users’ \ntimeline. Although they did not confirm whether these users have also disclosed \ntheir depression diagnosis, they mention that they analyze their profile to ensure \nthat these users are going through depression. Since their annotation process \nis not as rigorous as ours, i.e., they did not develop an annotation guideline as \ndescribed in the earlier section and their  dataset may not contain all self-dis-\nclosed depressed users, we had to further filter those tweets before we could use \nthem. So we use DSD-Clinician-Original-Tweets for training our very first model \nin the SSL process, and later use that to re-label D2S samples.\nIn Sect.  4.2.6, we report the distribution on harvested data and another \napproach for increasing sample size for least populated labels.\n4  Experimental setup and evaluation\nOur experimental setup consists of iterative data harvesting and re-training of a \nDSD and a DPD model (Sect.  4.2), followed by observing their accuracy increase \nover each iteration coupled with incremental initial dataset size increase.\nSample Counts\n0\n200\n400\n600\n800\nD2S DSD Total DSD/D2S\nAnhedonia\nLow Mood\nChange in Sleep Pattern\nFatigue\nWeight Change\nFeelings of Worthlessness\nIndecisiveness\nAgitation\nRetardation\nSuicidal Thoughts\nFig. 4  Sample distribution and ratio analysis across D2S and DSD datasets\n1021\n1 3\nDepression symptoms modelling from social media text: an LLM…\nWe report the results separately for each of the steps of SSL in the next sec-\ntions. For the DSD task, which is a multi-class multi-label problem, we report Macro \nand  Weighted-Averaged Precision, Recall, and F1 along with label-wise Precision, \nRecall, and F1 scores as our accuracy scores. Macro-F1 is an average F1 score for all the \nlabels, whereas weighted F1-score is a measure that assigns more weight to the labels \nfor which we have the most samples. For the DPD task, which is a binary classification \nproblem, we report Macro-Averaged Precision, Recall, and F1 scores as our accuracy \nscores.\nFrom our clinician-annotated dataset, we separate a subset of depression symp-\ntoms stratified samples as a test-set for the DSD task. For the DPD task, we sepa-\nrate a 10% portion from the DPD-Human train-set as a test set. After each step of \nthe SSL process, we report the accuracy scores to evaluate the efficacy of that step \nbased on the DSD and DPD models’ performance on these test-sets respectively \n(Tables 3 and 4).\n4.1  Data preprocessing\nWe perform the following preprocessing steps for all our Twitter datasets, we use \nNLTK1 for tokenizing our tweets and also Ekphrasis2 for normalizing tweets. \nTable 3  Datasets in step 1\nDataset Sample size Comment\nDSD-Clinician-Tweets-Original 539 Tweets with depression \nsymptoms only\nDSD-Clinician-Tweets-Original-Train 377 Initial train dataset\nDSD-Clinician-Tweets-Original-Test 162 Overall test dataset\nDSD-Clinician-ED-Tweets 135 Depressive tweets\nDSD-Clinician-NoED-Tweets 785 Control tweets\nDSD-Clinician-Gibberish-Tweets 41 Gibberish tweets\nTable 4  Model details in step 1\nModel Train dataset Sample size Comment\nDSD-Clinician-1 DSD-Clinician-Tweets-Original-\nTrain\n377 DSD-Clini-\ncian model \nat SSL \niteration 1\nDPD-Human (DSD-Clinician-Tweets + D2S – \n(DSD-Gibberish-Tweets + DSD-\nNoED-Tweets + Tweets with \nself-disclosure)) + equal number \nof NoED tweets from DTR\n(1500 + 1584 - (785 + 41 + \n34)) + 2224 = 4448\nDPD-Human \nmodel at \nSSL itera-\ntion 1\n1 https:// www. nltk. org/ book/ ch06. html.\n2 https:// github. com/ cbazi otis/ ekphr asis.\n1022 N. Farruque et al.\n1 3\n 1. Lowercase each word.\n 2. Remove one character words and digits.\n 3. De-contract contracted words in a tweet. For example, “I’ve” is made “I have”.\n 4. Elongated words are converted to their original form. For example, “Looong” \nis turned into “Long”.\n 5. Remove tweets with self-disclosure, i.e. any tweet containing the word “diag-\nnosed” or “diagnosis” is removed.\n 6. Remove all punctuations except period, comma, question mark, and exclama-\ntion.\n 7. Remove URLs.\n 8. Remove non-ASCII characters from words.\n 9. Remove hashtags.\n 10. Remove emojis.\n4.2  Semi‑supervised learning (SSL) framework\nIn our SSL framework, we iteratively perform data harvesting and retraining of \nour DSD model, which is a multi-label text classifier utilizing pre-trained Men-\ntal-BERT,3 technical details of this model (i.e., the training hyper-parameters) \nare provided in Appendix 2 . We find Mental-BERT-based DSD performs signif-\nicantly better in terms of Macro-F1 and Weighted-F1 scores compared to base \nBERT-only models in the DSD task (Tables  5 and 6 ). In this section, we provide \nour step-by-step SSL process description, datasets utilized at each step, and the \nresulting models and/or datasets.\nAll our steps are depicted in points 11–25 in Fig.  5 and described further \nbelow.\nTable 5  DSD-Clinician-1 \n(BERT based) model accuracy Comment Precision Recall F1-score Support\nAnhedonia 0.00 0.00 0.00 5\nLow mood 0.00 0.00 0.00 26\nChange in sleep pattern 1.00 0.07 0.12 15\nFatigue 0.00 0.00 0.00 6\nWeight change 0.00 0.00 0.00 4\nFeelings of worthlessness 0.55 0.16 0.24 38\nIndecisiveness 0.00 0.00 0.00 11\nAgitation 0.55 0.73 0.62 66\nRetardation 0.00 0.00 0.00 12\nSuicidal thoughts 1.00 0.14 0.24 22\nMacro avg 0.31 0.11 0.12 205\nWeighted avg 0.46 0.28 0.28 205\n3 https:// huggi ngface. co/ mental/ mental- bert- base- uncas ed.\n1023\n1 3\nDepression symptoms modelling from social media text: an LLM…\n4.2.1  Step 1: creating first DSD model\nIn this step, we focus on the creation of a training dataset and a test dataset selected \nfrom our clinician-annotated samples. This dataset consists of tweets carrying \nat least one of the 10 depression symptoms. We use this training dataset to create \nour first DSD model, called DSD-Clinician-1. To do so, we follow the steps stated \nbelow. \n1. We first remove all the tweets with labels “Gibberish,” “Evidence of Depression” \n(ED) and “No Evidence of Depression” (NoED) from a subset of DSD-Clinician-\nTweets after applying MVCP. We call this dataset DSD-Clinician-Tweets-Origi-\nnal. Details of ED, NoED, and Gibberish are provided in Table 3.\n2. We save the tweets labelled as “Evidence of Depression,” which we call DSD-\nClinician-ED-Tweets, (Arrow 8 in Fig. 5). We later use those to harvest depression \nsymptoms-related tweets.\n3. Next, we separate 70% of the tweets from DSD-Clinician-Tweets-Original dataset \nand create DSD-Clinician-Tweets-Original-Train dataset for training our first ver-\nsion of DSD model, called DSD-Clinician-1 and the rest 30% of the tweets are \nused as an SSL evaluation set, also called, DSD-Clinician-Tweets-Original-Test, \n(Arrows 5 and 7 in the Fig. 5). We will use this evaluation set all through our SSL \nprocess to measure the performance of SSL, i.e., whether it helps increase accu-\nracy for DSD task or not. We report the datasets created in this step in Table  3, \nmodels in Table 4, and accuracy scores for each label and their average in Table 6. \nWe also report the accuracy for the DPD-Human model in this step in Table 7.\n4.2.2  Step 2: harvesting tweets using DSD‑Clinician‑1\nIn this step, we use DSD-Clinician-1 model created in the previous step to har -\nvest tweets that carry signs of depression symptoms from a set of tweets filtered \nTable 6  DSD-\nClinician-1 (Mental-BERT \nbased) model accuracy in step 1\nComment Precision Recall F1-score Support\nAnhedonia 0.00 0.00 0.00 5\nLow mood 0.61 0.42 0.50 26\nChange in sleep pattern 0.76 0.87 0.81 15\nFatigue 0.00 0.00 0.00 6\nWeight change 0.00 0.00 0.00 4\nFeelings of worthlessness 0.49 0.53 0.51 38\nIndecisiveness 0.00 0.00 0.00 11\nAgitation 0.63 0.77 0.69 66\nRetardation 0.00 0.00 0.00 12\nSuicidal thoughts 0.91 0.45 0.61 22\nMacro avg 0.34 0.30 0.31 205\nWeighted avg 0.52 0.51 0.51 205\n1024 N. Farruque et al.\n1 3\nFig. 5  Detailed SSL framework. Here, we show the interaction among our datasets and models. Data-\nsets are shown as cylinders, models are shown as rectangles. An arrow from a dataset to another dataset \nrepresents data subset creation; an arrow to another model means the provision of training data for that \nmodel; and an arrow from a model to a dataset means the use of that model to harvest samples from the \ndataset. All the arrow heads are marked, so that these can be easily referred while describing a particular \nscenario in the SSL framework\nTable 7  DPD-Human model \naccuracy in step 1 Precision Recall F1-score Support\n0.84 0.90 0.87 227\n1025\n1 3\nDepression symptoms modelling from social media text: an LLM…\nfor carrying signs of depression only by DPD-Human model from DTR, we call \nthis dataset DSD-Harvest-Candidate-Tweets (Arrows 10 and 12 in Fig.  5). Our \nDPD-Human model is trained on all available human annotated datasets, i.e., DSD-\nClinician-Tweets-Original, D2S, and an equal number of control tweets from DTR \n(Arrows 6 and 9 in Fig. 5 and more dataset details in Table 4). We use this model to \nleverage human insights to further filter DTR. In this step, we create two more data-\nsets from DSD-Harvest-Candidate-Tweets, (1) Harvested-DSD-Tweets: This dataset \ncontains the tweet samples for which the model is confident, i.e., it detects one of the \n10 depression symptoms and (2) Harvested-DSD-Tweets-Less-Confident: This data-\nset contains the tweet samples for which the model has no confident predictions or it \ndoes not predict any depression symptoms for harvested dataset (Table 8).\n4.2.3  Step 3: harvesting tweets using best ZSL Model\nIn this step, we use a ZSL model (USE-SE-SSToT) described in Farruque et  al. \n(2021) to harvest tweets carrying signs of depression symptoms from the DSD-Har -\nvest-Candidate-Tweets. We chose this model because it has reasonable accuracy in \nthe DSD task and it is fast. We also set a threshold while finding semantic similarity \nbetween the tweet and the label descriptor to be more on the conservative side so \nthat we reduce the number of false positive tweets. We find that a threshold < 1 is a \nreasonable choice because cosine-distance < 1 indicates higher semantic similarity. \nIn this step, we create two datasets: (1) Only-ZSL-Pred-on-Harvested-DSD-Tweets \n(step: 3a): This dataset is only ZSL predictions on DSD-Harvest-Candidate-Tweets. \n(2) ZSL-and-Harvested-DSD-Tweets (step: 3b): This dataset is a combination of \nZSL predictions and DSD-Clinician-1 predictions on DSD-Harvest-Candidate-\nTweets. We follow steps: 3a and 3b to compare whether datasets produced through \nthese steps help in accuracy gain after using them to retrain DSD-Clinician-1.\nCompared to step 1 (Table  6), we achieve 4% gain in Macro-F1 and 5% gain in \nWeighted-F1 using the combined dataset in step: 3b (Table 10). We achieve 1% gain \nin both the measures using Harvested-DSD-Tweets only in step: 2 (Table  9). With \nZSL only in step: 3a (Table 11), we lose 3% in Macro-F1 and 15% in Weighted-F1. \nWe also provide our produced datasets description in Table 12.\nTable 8  Datasets in step 2\nDataset Sample size Comment\nDSD-Harvest-Candidate-Tweets 3145 Harvestable tweets for DSD\nHarvested-DSD-Tweets 2491 First harvested dataset\nHarvested-DSD-Tweets-Less-Confident 654 First harvested less confident dataset\n1026 N. Farruque et al.\n1 3\n4.2.4  Step 4: creating a second DSD Model:\nTable 9  DSD-Clinician-1 model \naccuracy in step 2 Comment Precision Recall F1-score Support\nAnhedonia 0.00 0.00 0.00 5\nLow mood 0.71 0.46 0.56 26\nChange in sleep pattern 0.70 0.93 0.80 15\nFatigue 0.00 0.00 0.00 6\nWeight change 0.00 0.00 0.00 4\nFeelings of worthlessness 0.44 0.63 0.52 38\nIndecisiveness 0.00 0.00 0.00 11\nAgitation 0.62 0.77 0.69 66\nRetardation 0.00 0.00 0.00 12\nSuicidal thoughts 0.80 0.55 0.65 22\nMacro avg 0.33 0.33 0.32 205\nWeighted avg 0.51 0.55 0.52 205\nTable 10  DSD-Clinician-1 \nmodel accuracy in step 3b Comment Precision Recall F1-score Support\nAnhedonia 0.00 0.00 0.00 5\nLow mood 0.71 0.92 0.80 26\nChange in sleep pattern 0.68 0.87 0.76 15\nFatigue 0.00 0.00 0.00 6\nWeight change 0.00 0.00 0.00 4\nFeelings of worthlessness 0.34 0.82 0.48 38\nIndecisiveness 0.00 0.00 0.00 11\nAgitation 0.65 0.82 0.72 66\nRetardation 0.00 0.00 0.00 12\nSuicidal thoughts 0.76 0.73 0.74 22\nMacro avg 0.31 0.42 0.35 205\nWeighted avg 0.49 0.67 0.56 205\nTable 11  DSD-Clinician-1 \nmodel accuracy in step 3a Comment Precision Recall F1-score Support\nAnhedonia 0.00 0.00 0.00 5\nLow mood 0.56 0.85 0.68 26\nChange in sleep pattern 0.72 0.87 0.79 15\nFatigue 0.00 0.00 0.00 6\nWeight change 0.00 0.00 0.00 4\nFeelings of worthlessness 0.33 0.55 0.42 38\nIndecisiveness 0.00 0.00 0.00 11\nAgitation 1.00 0.11 0.19 66\nRetardation 0.00 0.00 0.00 12\nSuicidal thoughts 0.82 0.64 0.72 22\nMacro avg 0.34 0.30 0.28 205\nWeighted avg 0.60 0.38 0.36 205\n1027\n1 3\nDepression symptoms modelling from social media text: an LLM…\nFrom the previous experiments, we now create our second DSD model by retraining \nit with DSD-Clinician-Tweets-Original-Train and ZSL-and-Harvested-DSD-Tweets. \nThis results in our second DSD (DSD-Clinician-2) model (Table 13).\n4.2.5  Step 5: creating final DSD model\nIn this final step, we do the following: \n1. We create a combined dataset from D2S and DSD-Clinician-ED-Tweets and we \ncall this combined dataset DSD-Less-Confident-Tweets dataset (Arrows 15, 16, \n17, 20 in Fig.  5). D2S tweets are used here because the dataset was annotated \nexternally with a weak clinical annotation guideline. We use our model to further \nfilter this dataset.\n2. We use DSD-Clinician-2 model and ZSL to harvest depression symptoms tweets \nfrom DSD-Less-Confident-Tweets, we call this dataset Harvested-DSD-from-\nLess-Confident-Tweets. Finally, with this harvested data and the datasets used to \ntrain DSD-Clinician-2 model, we create our final dataset called Final-DSD-Cli-\nnician and by training with it, we learn our final DSD model called, Final-DSD-\nClinician. We also retrain our DPD-Human model to create Final-DPD-Human \nmodel. Datasets, models, and the relevant statistics are reported in Tables  14, \n15, 16 and 17. We reported the symptoms distribution for our DSD-Clinician-\nTweets-Original-Train dataset earlier, and here report depression symptoms dis-\ntribution in our SSL model harvested datasets (ZSL-and-Harvested-DSD-Tweets \n+ Harvested-DSD-from-Less-Confident-Tweets) only (Fig.  6). We see that the \nsample size for all the labels generally increases and reflects almost the same \ndistribution as our DSD-Clinician-Tweets-Original-Train dataset. Interestingly, \ndata harvesting increases the sample size of “Feelings of Worthlessness” and \n“Suicidal thoughts” while still maintaining the distribution of our original clini-\ncian annotated dataset (DSD-Clinician-Tweets-Original-Train) (Fig. 6). We also \nreport the top-10 bi-grams for each of the symptoms for our Final-DSD-Clinician-\nTweets dataset in Table 18. We see that top bi-grams convey the concepts of each \nsymptoms.\nTable 12  Datasets in step 3\nDataset Sample size Comment\nZSL-and-Harvested-DSD-\nTweets\n2491 Second harvest, sample size is same as Harvested-\nDSD-Tweets because harvesting is done on the \nsame data\nOnly-ZSL-Pred-on-Harvested-\nDSD-Tweets\n2248 Sample size less than the above because we are not \nusing samples with no labels predicted\n1028 N. Farruque et al.\n1 3\n4.2.6  Step 6: combating low accuracy for less populated labels\nHere we attempt to combat the low accuracy for the labels that have a very small \nsample size. In these cases, we analyze the co-occurrence of those labels with \nother labels through an associative rule mining (Apriori) algorithm (Agrawal et al., \n1994). Our idea is to use significant co-occurring labels and artificially predict one \nlabel if the other occurs. For that, we analyze a small human-annotated train data-\nset (DSD-Clinician-Tweets-Original-Train). However, since the support and confi-\ndence for association rules are not significant due to the small sample size, we con-\nsider all the “strong” rules with non-zero support and confidence scores for those \nlabels. The rules we consider have the form: (strong-label → weak-label), where the \nweak label (such as Anhedonia, Fatigue, Indecisiveness, and Retardation) means the \nlabels for which our model achieves either 0 F1 score or very low recall, i.e., less \nor equal to chance level. These are the candidate labels for which we would like to \nhave increased accuracy. On the other hand, strong labels are those for which we \nhave at least a good recall, i.e., beyond chance level. By emphasizing high recall, \nwe intend to prevent a depression symptom from being undetected by our model. \nAll the extracted strong rules are provided in Appendix 1. When we compare the \nsample distribution for Apriori-based harvested data and plain harvested data, we \nsee for the least populated class we have more samples (Fig. 7). This makes the clas-\nsification task more sensitive towards weak labels. However, with this method, we \ndo not achieve a better Macro-F1 score compared to our Final-DSD-Clinician model \n(Table 19).\n4.2.7  Stopping criteria for SSL\nThe following two observations lead us to stop the SSL: \n1. Our DTR consists of a total 6077 samples and we have finally harvested 4567 \nsamples, so for (6077 − 4567)= 1510 samples neither ZSL nor any version of \nDSD models have any predictions. We exhausted all our depression candidate \ntweets from all sources we have, therefore, we do not have any more depression \nsymptoms candidate tweets for moving on with SSL.\n2. We have another very noisy dataset, called IJCAI-2017-Unlabelled (Shen et al., \n2017), where we have tweets from possible depressed users, i.e., their self-dis-\nclosure contains the stem “depress” but it is not verified whether they are genuine \nself-disclosures of depression. Using our Final-DSD-Clinician model we harvest \n≈ 22K depression symptoms tweets from ≈ 0.4M depression candidate tweets \nTable 13  Model details in step 4\nModel Train dataset Sample size Comment\nDSD-Clinician-2 DSD-Clinician-Tweets-Original-\nTrain + ZSL-and-Harvested-\nDSD-Tweets\n(377+ 2491)= 2868 DSD model at SSL \niteration 2\n1029\n1 3\nDepression symptoms modelling from social media text: an LLM…\nidentified by the Final-DPD-Human model from that dataset. We then retrain the \nFinal-DSD-Clinician model on all the samples previously we harvested combined \nwith the newly harvested ≈ 22K tweets, which results in a total of ≈ 26k tweets \n( ≈ 6 times larger than the samples DSD-Final-model was trained on). However, \nwe did not see any significant accuracy increase, so we did not proceed (Table 20).\n5  Results analysis\nHere we analyse the efficacy of our SSL frameworks in three dimensions, as follows:\n5.1  Dataset size increase\nThrough the data harvesting process, we can increase our initial clinician annotated \n377 samples to 4567 samples, which is 12 times bigger than our initial dataset. In \naddition, we have access to an external organization-collected dataset (i.e., D2S), for \nwhich we could access around ≈ 1800 samples. Our final dataset is more than dou-\nble the size of that dataset.\n5.2  Accuracy improvement\nOur Final-DSD-Clinician model has Macro-F1 score of 45% which is 14% more than \nthat of our initial model and Weighted-F1 score increased by 5% from 51% to 56% \n(Table 21). The substantial gain in the Macro-F1 score indicates the efficacy of our data \nharvesting in increasing F1 scores for all the labels. We also find that the combination \nFig. 6  Sample distribution in harvested dataset vs original clinician annotated dataset\n1030 N. Farruque et al.\n1 3\nof DSD-Clinician-1 and ZSL models in step 3a helps achieve more accuracy than \nindividually; specifically, using only ZSL-harvested data for training is not ideal. \nTable 14  Datasets in step 5\nDataset Constituent datasets Sample size\nFinal-DSD-Clinician-Tweets DSD-Clinician-Tweets-Original-Train \n+ ZSL-and-Harvested-DSD-Tweets \n+ Harvested-DSD-from-Less-Confi-\ndent-Tweets\n(377+ 2491 + 1699)= 4567\nFinal-DPD-Human-Tweets Final-DSD-Clinician-Tweets which \nare not in DPD-Human model itera-\ntion 1 testset + DPD-Human model \niteration 1 trainset which are not \nin Final-DSD-Clinician-Tweets + \nEqual number of NoED tweets from \nDSD-Harvest-Candidates\n(2743+ 1997)× 2 = 9480\nTable 15  Model details in step 5\nModel Train dataset Comment\nFinal-DSD-Clinician Final-DSD-Clinician-Tweets DSD model at SSL Step 5\nFinal-DPD-Human Final-DPD-Human-Tweets DPD model at SSL step 5\nTable 16  Final-DSD-Clinician \nmodel accuracy in step 5 Comment Precision Recall F1-score Support\nAnhedonia 0.00 0.00 0.00 5\nLow mood 0.57 0.96 0.71 26\nChange in sleep pattern 0.68 0.87 0.76 15\nFatigue 1.00 0.17 0.29 6\nWeight change 1.00 0.75 0.86 4\nFeelings of worthlessness 0.35 0.76 0.48 38\nIndecisiveness 0.00 0.00 0.00 11\nAgitation 0.62 0.77 0.69 66\nRetardation 0.00 0.00 0.00 12\nSuicidal thoughts 0.64 0.82 0.72 22\nMacro avg 0.49 0.51 0.45 205\nWeighted avg 0.51 0.68 0.56 205\nTable 17  Final-DPD-Human \nmodel accuracy in step 5 Precision Recall F1-score Support\n0.83 0.97 0.89 227\n1031\n1 3\nDepression symptoms modelling from social media text: an LLM…\nWeighted-F1 has slow growth and does not increase after Step 3b. We also find that the \ncombined harvesting process on D2S samples helped us achieve further accuracy in a \nfew classes for which D2S had more samples, such as “Fatigue,” “Weight Change” and \n“Suicidal Thoughts.”\n5.3  Linguistic components distribution\nIn Table 18, we see that our harvested dataset contains important clues about depres-\nsion symptoms. Interestingly, there are some bi-grams, such as, “feel like” occur in \nmost of the labels; this signifies the frequent usage of that bi-gram in various language-\nbased expressions of depression symptoms. This also shows a pattern of how people \ndescribe their depression.\n5.4  Sample distribution\nCompared with the original clinician annotated dataset distribution (Fig.  6), we see \nsimilar trends in our harvested dataset, i.e., in Final-DSD-Clinician-Tweets. However, \ninstead of “Agitation” we have some more samples on “Feeling of Worthlessness,” \nalthough those are not surpassed by “Suicidal thoughts” as in the D2S dataset. Moreo-\nver, “Suicidal thoughts” samples have also a strong presence which is the result of inte-\ngrating the D2S dataset in our harvesting process. Since the majority of our samples are \ncoming from self-disclosed users’ tweets, and we apply our DSD model trained on that \nTable 18  Top-10 bi-grams for each symptom for Final-DSD-Clinician-Tweets dataset with the ones \nbolded occur exclusively to the corresponding symptoms\nDepression-Symptoms Bi-grams\nAnhedonia want go, dont care, go work, motivation anything, want die, want live, go \naway, im done, tired bored, getting bed\nLow Mood feel like, want cry, depression anxiety, feeling like, mental illness, want die, \nlike shit, want someone, feel alone, feels like\nChange in Sleep Pattern want sleep, go sleep, im tired, hours sleep, fall asleep, cant sleep, need \nsleep, back sleep, could sleep, going sleep\nFatigue im tired, f*cking tired, physically mentally, tired everything, tired tired, \nfeel tired, im f*cking, need break, tired yall, sad tired\nWeight Change eating disorder, fat fat, stop eating, feel like, keep eating, im gonna, lose \nweight, eating disorders, fat body, wish could\nFeelings of Worthlessness feel like, like shit, feeling like, fat fat, wish could, f*cking hate, good \nenough, ibs hate, hate ibs, makes feel\nIndecisiveness cant even, even know, says better, thoughts brain, seems like, feel like, \nbetter dead, assistant remember, remember things, time like\nAgitation feel like, mental illness, f*ck f*ck, depression anxiety, f*ck life, f*cking \nhate, fat fat, panic attacks, every time, hate body\nRetardation feel like, lay bed, ever get, committed bettering, sleepy kind, im tired, one \nmoods, talking going, well mind, motherf*ckers prove\nSuicidal thoughts want die, feel like, wanna die, want kill, want cut, f*cking die, better dead, \nself harm, hope die, want f*cking\n1032 N. Farruque et al.\n1 3\ndataset to the D2S dataset to harvest tweets, our final harvested dataset reflects mainly \nthe distribution of symptoms from the self-disclosed depressed users. However, D2S \nhas some impact, resulting in more samples in the most populated labels of the final \nharvested dataset.\nTable 19  Final-DSD-Clinician \nmodel with applied label \nassociation rules accuracy in \nstep 6\nComment Precision Recall F1-score Support\nAnhedonia 0.03 0.80 0.06 5\nLow mood 0.59 0.92 0.72 26\nChange in sleep pattern 0.71 1.00 0.83 15\nFatigue 0.04 0.83 0.08 6\nWeight change 1.00 0.50 0.67 4\nFeelings of worthlessness 0.34 0.79 0.47 38\nIndecisiveness 0.09 1.00 0.16 11\nAgitation 0.61 0.76 0.68 66\nRetardation 0.07 0.75 0.12 12\nSuicidal thoughts 0.72 0.82 0.77 22\nMacro avg 0.42 0.82 0.45 205\nWeighted avg 0.49 0.82 0.57 205\nFig. 7  Sample distribution in Apriori harvested dataset vs plain harvested dataset\n1033\n1 3\nDepression symptoms modelling from social media text: an LLM…\n5.5  Data harvesting in the wild\nWe use our final model on a bigger set of very loosely related data, but we do not see \nany increase in accuracy, which suggests that harvesting from irrelevant data is of no \nuse (Sect. 4.2.6).\n6  Limitations\n1. Our overall dataset size is still small, i.e. for some labels we have a very small \namount of data both for training and testing.\n2. In the iterative harvesting process we do not employ continuous human annotation \nor human-in-the-loop strategy since this process requires several such cycles and \ninvolving experts in such a framework is also very expensive.\nTable 20  DSD-Clinician \nmodel trained on IJCAI-2017-\nUnlabelled and all the harvested \ndataset\nComment Precision Recall F1-score Support\nAnhedonia 0.00 0.00 0.00 5\nLow mood 0.52 0.96 0.68 26\nChange in sleep pattern 0.71 1.00 0.83 15\nFatigue 1.00 0.17 0.29 6\nWeight change 1.00 0.75 0.8 4\nFeelings of worthlessness 0.32 0.82 0.46 38\nIndecisiveness 0.00 0.00 0.00 11\nAgitation 0.64 0.76 0.69 66\nRetardation 0.00 0.00 0.00 12\nSuicidal thoughts 0.60 0.82 0.69 22\nMacro avg 0.48 0.53 0.45 205\nWeighted avg 0.50 0.70 0.56 205\nTable 21  Summary of accuracy \nimprovements (DSD and DPD \ncorrespond to DSD-Clinician \nand DPD-Human models)\nStep Model Macro-F1 Weighted-F1 F1\n1 DSD 0.31 0.51 –\n1 DPD – – 0.87\n2 DSD 0.32 0.52 –\n3a DSD 0.28 0.36 –\n3b DSD 0.35 0.56 –\nFinal DSD 0.45 0.56 –\nFinal DPD – – 0.89\n1034 N. Farruque et al.\n1 3\n7  Conclusion\nWe have described a Semi-supervised Learning (SSL) framework, more specifically \nsemi-supervised co-training for gathering depression symptoms data in  situ from \nself-disclosed users’ Twitter timelines. We articulate each step of our data harvest-\ning process and model re-training process. We also discuss our integration of Zero-\nShot learning models in this process and their contribution. We show that each of \nthese steps provides moderate to significant accuracy gains. We discuss the effect \nof harvesting from the samples of an externally curated dataset, and we also try har -\nvesting samples in the wild, i.e., a large noisy dataset with our Final-DSD-Clinician \nmodel. In the former case, we find good improvement in the Macro-F1 score. In \nthe latter, we do not see any improvements indicating that there is room for further \nprogress to improve accuracy in those samples. Finally, we discuss the effect of our \nSSL process for curating small but distributionally relevant samples through both \nsample distribution and bi-gram distribution for all the labels.\nAppendix 1: Apriori rules\nHere we provide the strong rules mined from DSD-Clinician-Tweets-Original-Train \n(Table 22).\nAppendix 2: Mental‑BERT training configuration for DPD and DSD\nHere we report the training configuration for Mental-BERT based DPD and DSD \n(Table 23).\nFor DSD we use BCE Loss on the output of last layer of our Mental-BERT model \nwhich is based on sigmoid functions for each nodes corresponding to each depres-\nsion symptoms labels. For DPD, we use BCE loss on the softmaxed output for each \nTable 22  Strong Rules; indices for each labels are from Sect. 1\n(Strong-Label → Weak-Label)\n1 → 2\n1 → 6\n4 → 3\n4 → 8\n4 → 10\n7 → 6\n7 → 8\n9 → 6\n9 → 8\n9 → 10\n1035\n1 3\nDepression symptoms modelling from social media text: an LLM…\nbinary labels i.e. depression vs control. We do not freeze any layers in our fine-tun-\ning process because it turned out to be detrimental to the model accuracy.\nAppendix 3: Annotation guideline\nSocial media data annotation by human\nFor this annotation task, an annotator has to label or classify a social media post (i.e. \na tweet) in one or more of the following depression symptom categories which suit \nbest for that social media post through a web tool: \n 1. Inability to feel pleasure or Anhedonia\n 2. Low mood\n 3. Change in sleep pattern\n 4. Fatigue or loss of energy\n 5. Weight change or change in appetite\n 6. Feelings of worthlessness or excessive inappropriate guilt\n 7. Diminished ability to think or concentrate or indecisiveness\n 8. Psychomotor agitation or inner tension\n 9. Psychomotor retardation\n 10. Suicidal thoughts or self-harm\n 11. Evidence of clinical depression\n 12. No evidence of clinical depression\n 13. Gibberish\nDetailed description of these categories with examples are as follows:\nThe following sections need to be very carefully read to better understand what \neach category means. We divide the description under each category into three parts: \n“Lead”, “Elaboration”, and “Example”. “Lead” contains the summary or gist of the \nsymptomatology. “Elaboration” provide a broader description of the symptomatol-\nogy accompanied by a few relevant “Examples”. These sections have been devel-\noped with careful considerations of criteria defined in the DSM-5 and MADRS, \nBDI, CES-D and PHQ-9 depression rating scales.\nTable 23  DPD and DSD model \ntraining parameters Hyperparameters DPD DSD\n#Epochs 20 10\n#Batch 32 Same\nMAX sequence length 30 Same\nLearning rate 2 × 10−5 Same\n#GPUs 1 Same\nLoss function Binary Cross Entropy \n(BCE) Loss\nSame\n1036 N. Farruque et al.\n1 3\nDepression symptoms labels\n1 Inability to feel pleasure or anhedonia\n(a) Lead: Subjective experience of reduced interest in the surroundings or \nactivities, that normally give pleasure.\n(b) Elaboration: Dissatisfied and bored about everything. Not enjoying things \nas one would used to. Not enjoying life. Lost Interest in other people. Lost \ninterest in sex. Can’t cry anymore even though one wants to.\n(c) Example:\n (i) I feel numb.\n (ii) I am dead inside.\n (iii) I don’t give a damn to anything anymore.\n2 Diminished ability to think or concentrate or indecisiveness\n(a) Lead: Difficulties in collecting one’s thoughts mounting to incapacitating \nlack of concentration.\n(b) Elaboration: Can’t make decisions at all anymore. Trouble keeping one’s \nmind on what one was doing. Trouble concentrating on things.\n(c) Example:\n(i) I can’t make up my mind these days.\n3 Change in sleep pattern\n(a) Lead: Reduced duration or depth of sleep, or increased duration of sleep \ncompared to one’s normal pattern when well.\n(b) Elaboration: Trouble falling or staying asleep. Waking up earlier and cannot \ngo back to sleep. Sleep was restless (wake up not feeling rested). Sleeping \ntoo much.\n(c) Example:\n (i) It’s 3 am, and I am still awake.\n (ii) I sleep all day!\n4 Fatigue or loss of energy\n(a) Lead: Any physical manifestation of tiredness.\n(b) Elaboration: Feeling tired. Insufficient energy for tasks. Feeling too tired \nto do anything.\n(c) Example:\n (i) I feel tired all day.\n (ii) I feel sleepy all day.\n1037\n1 3\nDepression symptoms modelling from social media text: an LLM…\n (iii) I get exhausted very easily.\n5 Feelings of worthlessness or excessive inappropriate guilt\n(a) Lead:Representing thoughts of guilt, inferiority, self-reproach, sinfulness, \nand self-depreciation.\n(b) Elaboration: Feeling like a complete failure, Feeling guilty, Feeling of \nbeing punished. Self-hate. Disgusted and Disappointed in oneself. Self-\nblaming for everything bad happens. Believe that one looks ugly or unat-\ntractive. Having crying spells. Feeling lonely. People seem unfriendly. Felt \nlike all other people dislike oneself.\n(c) Example:\n (i) Leave me alone, I want to go somewhere where there is no one.\n (ii) I am so alone...\n (iii) Everything bad happens, happens because of me.\n6 Low mood\n(a) Lead: Despondency, Gloom, Despair, Depressed Mood, Low Spirits, Feel-\ning of being beyond help without hope.\n(b) Elaboration: Feeling down. Feeling sad. Discouraged about future. Hope-\nlessness. Feeling like it’s not possible to shake of the blues even with the \nhelp of family and friends.\n(c) Example:\n (i) Life will never get any better.\n (ii) I don’t know why but I feel so empty.\n (iii) I am so lost.\n (iv) There is no hope to get out of this bad situation.\n7 Psychomotor agitation or inner tension\n(a) Lead: Ill defined discomfort, edginess, inner-turmoil, mental tension \nmounting to either panic, dread or anguish.\n(b) Elaboration: Feeling irritated and annoyed all the time. Bothered by things \nthat usually don’t bother. Feeling fearful. Feeling Restless. Feeling Mental \nPain.\n(c) Example:\n (i) It’s my life so I decide what to do next, mind your own business, \ndon’t bother!\n (ii) You have no idea how much pain you gave me!\n1038 N. Farruque et al.\n1 3\n8 Psychomotor retardation or lassitude\n(a) Lead: Difficulty getting started or slowness initiating and performing eve-\nryday activities.\n(b) Elaboration: Feeling everything one do requires effort. Could not get going. \nTalked less than usual. Have to push oneself to do anything. Everything is \na struggle. Moving or talking slowly.\n(c) Example:\n(i) I don’t feel like moving from the bed.\n9 Suicidal thoughts or self-Harm\n(a) Lead: Feeling of Life is not worth living, suicidal thoughts, preparation for \nsuicide.\n(b) Elaboration: Recurrent thoughts of death (not just fear of dying), recurrent \nsuicidal ideation without specific plan, or suicide attempt, or a specific plan \nfor suicide. Thoughts of self-harm. Suicidal ideation. Drug abuse.\n(c) Example:\n (i) I want to leave for the good.\n (ii) 0 days clean.\n10 Weight change or change in appetite\n(a) Lead: Loss or gain of appetite or weight than usual.\n(b) Elaboration: Increase in weight. Decrease in weight. Increase in appetite. \nDecrease in appetite. Do not feel like eating. Poor appetite. Loss of desire \nto food, forcing oneself to eat. Eating a lot but not feeling satiated. Eating \neven if one is full. Eating in large amount of food quickly and repeatedly. \nDifficulty in stop eating.\n(c) Example:\n (i) I think I am over eating these days!\n (ii) I don’t feel like eating anything!\n11 Evidence of clinical depression\n(a) Elaboration: Any social media post which do not necessarily fit into any of \nthe above symptoms, however still carry signs of depression or representing \nmany symptoms at a time, so it’s very hard to fit it in a few symptoms.\n(b) Example:\n(i) I feel like I am drowning …\n1039\n1 3\nDepression symptoms modelling from social media text: an LLM…\n12 No evidence of clinical depression\n(a) Elaboration: Political stance or personal opinion, inspirational statement \nor advice, unsubstantiated claim or fact.\n(b) Example:\n(i) People who eat dark chocolate are less likely to be depressed.\n13 Gibberish\n(a) Elaboration: If you are not sure what a social media post means i.e. if a \nsocial media post does not make sense or it’s gibberish, then annotate it as \nGibberish.\nAcknowledgements We are grateful to the annotators for allocating their time for data annotation.\nAuthor contributions N.F. developed the original research idea, designed and conducted experiments, \nannotated samples, and wrote and reviewed the manuscript. R.G. reviewed the manuscript and managed \nfunding. S.S. helped in creating annotation guidelines, annotated samples, and reviewed the manuscript. \nO.Z. reviewed the manuscript.\nFunding We are grateful to Alberta Machine Intelligence Institute (AMII), Natural Sciences and Engi-\nneering Research Council of Canada (NSERC), and MITACS for their support.\nData availability The dataset generated and/or analyzed during the current study is not publicly available \ndue to the privacy and ethical implications regarding the identity of Twitter users and tweets. According \nto Benton et al. (2017) Twitter users may not expect their tweets to have a large audience, that’s why their \ntweets need to be protected as much as possible. There is also a Twitter policy in place for sharing data, \nwhere Twitter discourages sharing tweets directly with third party (Twitt er Data Shari ng Policy). There-\nfore,  we’re still trying to find the best way in which we may share data. We will not release  our dataset to \nany interested party, until they acquire the signed consent form from the external research organizations \nfrom which we collected our datasets and created our own, because, we are  not allowed to share datasets \ncollected by these organizations.\nDeclarations \nEthical approval We obtained ethics approval from the University of Alberta’s research ethics office for \n“Depression Detection from Social Media Language Usage” (Pro00099074), “Depression Dataset Collec-\ntion” (Pro00082738), and “Social Media Data Annotation by Human” (Pro00091801).\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permis-\nsion directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\nReferences\nAgrawal, R., & Srikant, R. (1994). Fast algorithms for mining association rules. In Proc. 20th int. conf. very \nlarge data bases, VLDB (Vol. 1215, pp. 487–499). Citeseer.\n1040 N. Farruque et al.\n1 3\nBenton, A., Coppersmith, G., & Dredze, M. (2017). Ethical research protocols for social media health research. \nIn Proceedings of the first ACL workshop on ethics in natural language processing (pp. 94–102).\nBoyd, J. H., Weissman, M. M., Thompson, W. D., & Myers, J. K. (1982). Screening for depression in a com-\nmunity sample: Understanding the discrepancies between depression symptom and diagnostic scales. \nArchives of General Psychiatry, 39(10), 1195–1200.\nCoppersmith, G., Dredze, M., Harman, C., Hollingshead, K., & Mitchell, M. (2015). Clpsych 2015 shared \ntask: Depression and ptsd on twitter. In Proceedings of the 2nd workshop on computational linguistics \nand clinical psychology: From linguistic signal to clinical reality (pp. 31–39).\nDe Choudhury, M., & De, S. (2014). Mental health discourse on reddit: Self-disclosure, social support, and \nanonymity. In ICWSM.\nDe Choudhury, M., Gamon, M., Counts, S., & Horvitz, E. (2013). Predicting depression via social media. In \nICWSM (p. 2).\nFarruque, N., Goebel, R., Zaïane, O. R., & Sivapalan, S. (2021). Explainable zero-shot modelling of clinical \ndepression symptoms from text. In 2021 20th IEEE international conference on machine learning and \napplications (ICMLA) (pp. 1472–1477). IEEE.\nFarruque, N., Zaiane, O., & Goebel, R. (2019). Augmenting semantic representation of depressive language: \nFrom forums to microblogs. In Joint European conference on machine learning and knowledge discov-\nery in databases (pp. 359–375). Springer.\nGowen, K., Deschaine, M., Gruttadara, D., & Markey, D. (2012). Young adults with mental health conditions \nand social networking websites: Seeking tools to build community. Psychiatric Rehabilitation Journal, \n35(3), 245.\nJamil, Z., Inkpen, D., Buddhitha, P., White, K. (2017). Monitoring tweets for depression to detect at-risk \nusers. In Proceedings of the 4th workshop on computational linguistics and clinical psychology—from \nlinguistic signal to clinical reality (pp. 32–40).\nLosada, D. E., & Crestani, F. (2016). A test collection for research on depression and language use. In Inter-\nnational conference of the cross-language evaluation forum for European languages (pp. 28–39). \nSpringer.\nMa, L., Wang, Z., & Zhang, Y. (2017). Extracting depression symptoms from social networks and web blogs \nvia text mining. In Proceedings of Bioinformatics research and applications: 13th international sympo-\nsium, ISBRA 2017, Honolulu, HI, USA, 29 May–2 June 2017  (Vol. 13, pp. 325–330). Springer.\nMcClosky, D., Charniak, E., & Johnson, M. (2006). Effective self-training for parsing. In Proceedings of the \nhuman language technology conference of the NAACL, main conference (pp. 152–159).\nMowery, D., Smith, H., Cheney, T., Stoddard, G., Coppersmith, G., Bryan, C., & Conway, M. (2017). Under-\nstanding depressive symptoms and psychosocial stressors on Twitter: A corpus-based study. Journal of \nMedical Internet Research, 19(2), e48.\nMowery, D. L., Park, Y. A., Bryan, C., & Conway, M. (2016). Towards automatically classifying depressive \nsymptoms from Twitter data for population health. In Proceedings of the workshop on computational \nmodeling of people’s opinions, personality, and emotions in social media (PEOPLES) (pp. 182–191).\nNaslund, J., Aschbrenner, K., Marsch, L., & Bartels, S. (2016). The future of mental health care: Peer-to-peer \nsupport and social media. Epidemiology and Psychiatric Sciences, 25(2), 113–122.\nNaslund, J. A., Grande, S. W., Aschbrenner, K. A., & Elwyn, G. (2014). Naturally occurring peer support \nthrough social media: The experiences of individuals with severe mental illness using youtube. PLoS \nONE, 9(10), 110171.\nO’Keeffe, G. S., & Clarke-Pearson, K. (2011). The impact of social media on children, adolescents, and \nfamilies. Pediatrics, 127(4), 800–804.\nReece, A. G., Reagan, A. J., Lix, K. L., Dodds, P. S., Danforth, C. M., & Langer, E. J. (2017). Forecasting the \nonset and course of mental illness with twitter data. Scientific Reports, 7(1), 13006.\nRude, S., Gortner, E.-M., & Pennebaker, J. (2004). Language use of depressed and depression-vulnerable \ncollege students. Cognition & Emotion, 18(8), 1121–1133.\nSafa, R., Bayat, P., & Moghtader, L. (2022). Automatic detection of depression symptoms in Twitter using \nmultimodal analysis. The Journal of Supercomputing, 78(4), 4709–4744.\nSeabrook, E. M., Kern, M. L., Fulcher, B. D., & Rickard, N. S. (2018) Predicting depression from language-\nbased emotion dynamics: Longitudinal analysis of Facebook and Twitter status updates. Journal of \nMedical Internet Research, 20(5), e168.\nShen, G., Jia, J., Nie, L., Feng, F., Zhang, C., Hu, T., Chua, T.-S., Zhu, W. (2017). Depression detection via \nharvesting social media: A multimodal dictionary learning solution. In IJCAI (pp. 3838–3844).\nThe classification of depression and depression rating scales/questionnaires. In Depression in adults with a \nchronic physical health problem: Treatment and management. British Psychological Society (2010)\n1041\n1 3\nDepression symptoms modelling from social media text: an LLM…\nTlelo-Coyotecatl, I., Escalante, H. J., & Montes y Gómez, M. (2022) Depression recognition in social media \nbased on symptoms’ detection. Procesamiento del Lenguaje Natural, Revista, 68, 25–37.\nTrotzek, M., Koitka, S., & Friedrich, C. M. (2018). Utilizing neural networks and linguistic metadata for \nearly detection of depression indications in text sequences. IEEE Transactions on Knowledge and Data \nEngineering, 32(3), 588–601.\nVioulès, M. J., Moulahi, B., Azé, J., & Bringay, S. (2018). Detection of suicide-related posts in Twitter data \nstreams. IBM Journal of Research and Development, 62(1), 7–1.\nWorld Health Organization. (2023). Suicide. Retrieved from https:// www. who. int/ news- room/ fact- sheets/ \ndetail/ suici de\nYadav, S., Chauhan, J., Sain, J.P., Thirunarayan, K., Sheth, A., & Schumm, J. (2020). Identifying depressive \nsymptoms from Tweets: Figurative language enabled multitask learning framework. arXiv preprint. \narXiv: 2011. 06149\nYazdavar, A. H., Al-Olimat, H. S., Ebrahimi, M., Bajaj, G., Banerjee, T., Thirunarayan, K., Pathak, J., & \nSheth, A. (2017). Semi-supervised approach to monitoring clinical depressive symptoms in social \nmedia. In Proceedings of the 2017 IEEE/ACM international conference on advances in social networks \nanalysis and mining 2017 (pp. 1191–1198). ACM.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nAuthors and Affiliations\nNawshad Farruque1 · Randy Goebel1 · Sudhakar Sivapalan2 · Osmar R. Zaïane1\n * Nawshad Farruque \n nawshad@ualberta.ca\n Randy Goebel \n rgoebel@ualberta.ca\n Sudhakar Sivapalan \n sivapala@ualberta.ca\n Osmar R. Zaïane \n zaiane@ualberta.ca\n1 Department of Computing Science, Faculty of Science, Alberta Machine Intelligence Institute \n(AMII), University of Alberta, Edmonton, AB T6G 2E8, Canada\n2 Department of Psychiatry, Faculty of Medicine and Dentistry, University of Alberta, Edmonton, \nAB T6G 2H5, Canada",
  "topic": "Social media",
  "concepts": [
    {
      "name": "Social media",
      "score": 0.6568934321403503
    },
    {
      "name": "Depression (economics)",
      "score": 0.5429458618164062
    },
    {
      "name": "Psychology",
      "score": 0.49732306599617004
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4242284297943115
    },
    {
      "name": "Computer science",
      "score": 0.4188610315322876
    },
    {
      "name": "Cognitive psychology",
      "score": 0.35406744480133057
    },
    {
      "name": "Natural language processing",
      "score": 0.32867568731307983
    },
    {
      "name": "World Wide Web",
      "score": 0.15829211473464966
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154425047",
      "name": "University of Alberta",
      "country": "CA"
    }
  ],
  "cited_by": 18
}