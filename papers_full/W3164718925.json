{
  "title": "Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery",
  "url": "https://openalex.org/W3164718925",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1919251676",
      "name": "Diane M Korngiebel",
      "affiliations": [
        "Hastings Center"
      ]
    },
    {
      "id": "https://openalex.org/A2117583478",
      "name": "Sean D. Mooney",
      "affiliations": [
        "University of Washington Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1919251676",
      "name": "Diane M Korngiebel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117583478",
      "name": "Sean D. Mooney",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W2169818249",
    "https://openalex.org/W2260558829",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2515682654",
    "https://openalex.org/W3092291878",
    "https://openalex.org/W2982435932",
    "https://openalex.org/W2892917680",
    "https://openalex.org/W2791549227",
    "https://openalex.org/W2801888051",
    "https://openalex.org/W2965481926",
    "https://openalex.org/W3049024208",
    "https://openalex.org/W2145482038",
    "https://openalex.org/W3086249591",
    "https://openalex.org/W3085332162",
    "https://openalex.org/W2912457762"
  ],
  "abstract": null,
  "full_text": "COMMENT OPEN\nConsidering the possibilities and pitfalls of Generative Pre-\ntrained Transformer 3 (GPT-3) in healthcare delivery\nDiane M. Korngiebel 1 ✉ and Sean D. Mooney2\nNatural language computer applications are becoming increasingly sophisticated and, with the recent release of Generative Pre-\ntrained Transformer 3, they could be deployed in healthcare-related contexts that have historically comprised human-to-human\ninteraction. However, for GPT-3 and similar applications to be considered for use in health-related contexts, possibilities and pitfalls\nneed thoughtful exploration. In this article, we brieﬂy introduce some opportunities and cautions that would accompany advanced\nNatural Language Processing applications deployed in eHealth.\nnpj Digital Medicine           (2021) 4:93 ; https://doi.org/10.1038/s41746-021-00464-x\nA seemingly sophisticated arti ﬁcial intelligence, OpenAI’s Gen-\nerative Pre-trained Transformer 3, or GPT-3, developed using\ncomputer-based processing of huge amounts of publicly available\ntextual data (natural language)\n1, may be coming to a healthcare\nclinic (or eHealth application) near you. This may sound fantastical,\nbut not too long ago so did a powerful computer so tiny it could\nﬁt in the palm of your hand. GPT-3 and other technologies are\ngetting close to passing a Turing Test, an assessment of whether\nthe language such applications generate is indistinguishable from\nlanguage produced by humans2,3. This possibility has generated\nboth excitement and caution4, and Microsoft Corporation recently\nacquired an exclusive license from OpenAI for GPT-35. As with so\nmany technologies and their potential use in eHealth, there are\ndevelopments and applications that are unrealistic, realistic, and\nrealistic but challenging— and perhaps unwise.\nNatural Language Processing (NLP) has a long history in clinical\ninformatics and includes groundbreaking work using computer-\nbased algorithms that compute on text and natural language.\nThere are many clinical applications of NLP including assisting\nwith provider documentation, automated structured chart\nabstraction, and in machine learning\n6.\nDespite the large amount of work in this area, AI that generates\ntext and conversations, such as GPT-3, will not replace a\nconversation with another human being for the foreseeable\nfuture in clinical settings\n7. This means that it cannot interact with\npatients in lieu of healthcare providers or healthcare support\npersonnel. Interactions with GPT-3 that look (or sound) like\ninteractions with a living, breathing— and empathetic or sympa-\nthetic— human being are not8. A recent example of this failing\nwas seen in testing the use of GTP-3 for mental health support\nusing a simulated patient; the model supported the patient ’s\nsuggestion of suicide9. Moreover, language models such as GPT-3\nare not grounded in input-diverse datasets (like visual and\nauditory data)1. GPT-3’s self-supervised prediction will, therefore,\nhit limits based on its pre-training data and cannot dynamically\nadjust a conversation or interaction for tone or body language.\nGPT-3 is an autoregressive language model trained with 175\nbillion parameters and then tested in“few-shot learning settings”(in\nwhich a new language task can be performed after only a few\nexamples). Autoregressive language models predict the next\nelement in a text, usually a word, based on previous natural\nlanguage texts. Although its developers at OpenAI think it performs\nwell on translation, question answering, and cloze tasks (e.g., aﬁll-in-\nthe-blank test to demonstrate comprehension of text by providing\nthe missing words in a sentence)\n1, it does not always predict a\ncorrect string of words that are believable as a conversation. And\nonce it has started a wrong prediction (ranging from a semantic\nmistake to using biased language), it does not go back and correct\nitself but continues to predict each word based on the preceding\nwords. Further, since it is based on real language, human biases are\npresent and, with inadequate priming of the application, may even\nbe ampliﬁed and cause serious harm in sensitive contexts, such as\nhealthcare. It is well-known that Internet-trained models reﬂect the\nscale of bias seen on the Internet, recently demonstrated by using\nthe Implicit Association Test (IAT) to measure biases in a machine\nlearning model trained on web-based content\n10. Therefore, it is\nunsurprising that GPT-3 showed associations between gender and\ncertain jobs; often the default was male. Negative sentiments were\nassociated with Black race and positive with Asian race. Islam was\nmore often associated with terrorism-related words than were other\nreligions\n1. Furthermore, according to recent research at the Center\non Terrorism, Extremism, and Counterterrorism, GPT-3 is easy to\nprime for harmful text generation promoting extremism and\nterrorist activities, including Nazism and QAnon11.\nIt is within this caveat-ﬁlled context that evaluation of AI health\nand healthcare applications that produce natural language should\nassess their risk, feasibility, and return on investment— including\nprioritizing improved patient care. Realistic applications of GPT-3\nmust start in areas of high value, high feasibility, and low risk for\nall stakeholders, including (at a minimum) patients, clinicians,\nadministrators, and payers. Applications with higher levels of risk\nor feasibility must be studied extensively and their actual and\nprojected short-, medium-, and long-term impact measured.\nRealistic but challenging or unwise applications include those\nthat are medium to high feasibility, medium to high risk, and\nmedium to high value.\nUNREALISTIC APPLICATIONS FOR GPT-3 APPLICATIONS IN\nHEALTHCARE\nGPT-3 is not an artiﬁcial general intelligence. It will not, and cannot\n(for now at least), replace a human interaction that requires\n1The Hastings CenterGarrison, New York, NY, USA. 2Department of Biomedical Informatics and Medical Education, University of Washington Seattle, Seattle, WA, USA.\n✉email: korngiebeld@thehastingscenter.org\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nhumanness12,13. Although GPT-3 performed well on free-form\nconversation assessments demonstrating reading comprehension,\nit performed worst on a dataset meant to mimic the dynamic give-\nand-take of student-teacher interactions, and it also did not score\nwell on multiple choice questions from middle and high school\nexaminations\n1. This makes sense because it does not “know”\nanything. One of the major limitations of GPT-3 is that it repeats\nitself semantically, loses coherence over long conversations, and\ncontradicts itself\n1,14. It would be unrealistic to consider GPT-3 as a\nstand-in for a healthcare provider or as a proxy in high-stakes\ninteractions, such as a health emergency or an emotionally fraught\ninteraction.\nREALISTIC AND FEASIBLE GPT-3 APPLICATIONS IN\nHEALTHCARE\nThere is compelling promise and serious hype in AI applications\nthat generate natural language. Some of that promise is realistic.\nRoutinizing tedious work for providers could productively improve\ntheir work satisfaction and reduce time interacting with computer\nsystems, a well-documented concern\n15. AI NLP applications could\nnavigate complex electronic health record (EHR) systems, auto-\nmate documentation with human review, prepare orders, or\nautomate other routine tasks.\nIt is, however, capable of more complexity in its text\nconversations than a chatbot, including more natural-sounding\nquestion and answer exchanges\n14. This could personalize the\nexperience of data collection in several non-critical healthcare\nsystem encounters, including online chat support for patients or\nassisting patients with setting up equipment in preparation for a\ntelehealth visit. In fact, its developer, OpenAI, originally intended\nthe software to be used by companies and organizations to\nimprove customer service chatbots or do other similar tasks\n16.\nHowever, each application must also include implementation\nguidance, including serious guardrails for all healthcare-related\ninteractions. For example, this could mean it would be primed,\nperhaps using few-shot training alongside imposed limitations, to\ndiscuss solely relevant topics— and only after excisions of harmful,\nprejudicial, or inappropriate vocabulary.\nREALISTIC BUT CHALLENGING GPT-3 APPLICATIONS IN\nHEALTHCARE\nImplementation guidance will be even more important in\nadapting GPT-3 technology for realistic but challenging healthcare\napplications. For example, using GPT-3 to assist with triaging non-\ncritical patients presenting in the emergency department might\nseem a good use of the technology, from both a patient\nexperience perspective and a resource allocation perspective. In\nthis example, the focus would be on collecting accurate data from\npatients in a user-friendly way, thereby improving the patient\nexperience (by making it easier to provide information) and\nenhancing patient care (by freeing up clinicians to spend more\ntime in meaningful clinical encounters rather than routine data\ncollection).\nFDA approval would likely be required in this type of application\nin the United States and any evaluation must consider a broadly\ndiverse population of patients. For instance, stakeholders, includ-\ning developers and implementers, will need to be mindful of\nallocational and representational harms\n17,18, particularly when a\nvirtual agent acts as a gatekeeper19— in which case the patient-\nuser has no other option than to interact ﬁrst with the virtual\nagent. In the triage example, the allocational harm occurs when\nthose who are more able to successfully interact with the GPT-3\ntext intake process or forms are more likely to be triaged\naccurately. Implementation should include another means of\ntriaging those patients who cannot, or do not wish to, use the\nconversational agent, which may also be too linguistically\nhomogenous to offer culturally mindful language use. Further-\nmore, alternatives should be readily apparent and easy to access.\nAlthough this may seem to duplicate work, it is a necessary step in\nensuring that harms are reduced and, as the GPT-3-driven\ninteraction improves, redundant effort should be needed less\noften. An appropriate staff member would also need to review all\ntriage forms completed using the GPT-3 application; it will be\nimportant to maintain a“human in the loop”. A representational\nharm in this triage example might be when the GPT-3 intake is\nonly available in one language. In such a case, one could explore\nGPT-3’s expanded languages and translation functions: though\nthe intake interaction could be in Spanish, form material could\nthen be translated into the language spoken by the triage staff.\nThere are real possibilities here, if done well, for language and\nrelated healthcare access barriers to be reduced. However,\nwithout careful implementation, including the step-wise process\nand oversight we describe, this triage application would be\nunwise with the potential to cause both immediate harm to\nindividual patients and broader harm to patient groups, exacer-\nbating healthcare disparities.\nMAKING SURE ANY REALISTIC APPLICATIONS ARE EQUITABLE\nAI software that generates natural language could be viewed as\njust another vendor-provided information technology tool, but it\nshould not be. The role of human-computer interactions in\ninforming the design of these conversational spaces, whether in\nan online chat or at an emergency department patient registration\nkiosk, will be critical to ensure not just that accurate and relevant\ndata are collected, but also that the experience is what diverse\nusers expect, want, and value. A broad range of stakeholders\nshould be involved from the earliest stages of development (or\ntailoring) through deployment and evaluation. Stakeholders\nshould be selected who can represent as comprehensive a view\nas possible on both the harms and beneﬁts of proposed uses of\nGPT-3 in eHealth applications.\nTransparency will be key to the appropriate use of GPT-3 types\nof technology. Human beings must be informed that the\ninteraction is with a computer-based text generator. Doing so\nwould address concerns that humans tend to anthropomorphize\ntechnology applications with human traits, assuming humanness\nand ascribing empathic emotional responses when there are\nnone\n20,21. Some applications are highly feasible and seem low-risk\nbut might harbor hidden hazards. For example, an innocuous\nnatural language clinic appointment scheduler could not, with\nexisting technology, detect a change of tone or social cues of\nnervousness a patient expresses and that might signal more\nurgent clinical needs.\nTransparency is also critical for datasets and to disclose the\nlimitations in language training activities. A GPT-3 application will\nneed to be given conversation endpoints so that it leads the\nprompts rather than having the patient control the focus of the\ninteraction; for form-completion tasks, it will also need additional\nguidance to determine whether the information a patient shares\nactually addresses the question posed. IT support personnel, or\nthose in similar roles, will need to learn how to shape the prompts\nthat will deliver the most relevant answers or results from a given\ninteraction. For GPT-3 priming using few-shot learning, a commit-\nment to transparency would require publishing any customized\nparameters. In high-risk applications in healthcare, including any\npatient-facing tools, such sharing must be mandatory.\nWe should have cautious optimism for the potential applica-\ntions of sophisticated natural language processing applications to\nimprove patient care. Additional concerns from our triage\nexample include many implementation issues, including the ways\nAI software would interface with clinical and healthcare support\nworkﬂows (a known concern for other AI applications\n22,23), how\nthe data will be analyzed in real-time on the backend to\nD.M. Korngiebel and S.D. Mooney\n2\nnpj Digital Medicine (2021)    93 Published in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nsuccessfully triage patients in a queue that prioritizes more\nconcerning symptoms, and the degree and type of liability\nassigned the health system or provider. The future is coming.\nRather than fear it, we should prepare for it— and prepare to\nbeneﬁt humanity using these applications. But for now, Dr. GPT-3\nis not coming to a clinic near you anytime soon.\nReceived: 15 January 2021; Accepted: 11 May 2021;\nREFERENCES\n1. Brown, T. B., et al. Language models are few-shot learners. Preprint athttps://\narxiv.org/abs/2005.14165 (2020).\n2. Turing, A. M. Computing machinery and intelligence.Mind LIX, 433–460 (1950).\n3. Lacker, K. Giving GPT-3 a turing test. Available athttps://lacker.io/ai/2020/07/06/\ngiving-gpt-3-a-turing-test.html (2020).\n4. Metz, C. Meet GPT-3. It has learned to code (and Blog and Argue). Available at\nhttps://www.nytimes.com/2020/11/24/science/artiﬁcial-intelligence-ai-gpt3.html\n(2020).\n5. Scott, K. Microsoft teams up with OpenAI to exclusively license GPT-3 language\nmodel. Available at https://blogs.microsoft.com/blog/2020/09/22/microsoft-\nteams-up-with-openai-to-exclusively-license-gpt-3-language-model/ (2020).\n6. Nadkarni, P. M., Ohno-Machado, L. & Chapman, W. W. Natural language proces-\nsing: an introduction.J. Am. Med. Inform. Assoc.18, 544–551 (2011).\n7. Warwick, K. & Shah, H. Passing the turing test does not mean the end of\nhumanity. Cogn. Comput. 8, 409–419 (2016).\n8. Marcus, G. & Davis, E. GPT-3, Bloviator: OpenAI’s language generator has no idea\nwhat it’s talking about. Available athttps://www.technologyreview.com/2020/08/\n22/1007539/gpt3-openai-language-generator-artiﬁcial-intelligence-ai-opinion/\n(2020).\n9. Daws, R. Medical chatbot using OpenAI ’s GPT-3 told a fake patient to kill\nthemselves. Available at https://artiﬁcialintelligence-news.com/2020/10/28/\nmedical-chatbot-openai-gpt3-patient-kill-themselves/ (2020).\n10. Caliskan, A., Bryson, J. J. & Narayanan, A. Semantics derived automatically from\nlanguage corpora contain human-like biases.Science 356, 183–186 (2017).\n11. McGufﬁe, K. & Newhouse, A. The radicalization risks of GPT-3 and advanced\nneural language models. Preprint athttps://arxiv.org/abs/2009.06807 (2020).\n12. Floridi, L. & Chiriatti, M. GPT-3: its nature, scope, limits, and consequences.Minds\nMachines 30, 681–694 (2020).\n13. Heaven, W. D. OpenAI’s new language generator GPT-3 is shockingly good— and\ncompletely mindless. Available at https://www.technologyreview.com/2020/07/\n20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/ (2020).\n14. Elkins, K. & Chun, J. Can GPT-3 pass a writer’s Turing test. J. Cultural Analytics\n2371, 4549 (2020).\n15. Sinsky, C. et al. Allocation of physician time in ambulatory practice: a time and\nmotion study in 4 specialties.Ann. Intern. Med.165, 753–760 (2016).\n16. LaGrandeur, K. How safe is our reliance on AI, and should we regulate it?AI Ethics,\n1–7 https://link.springer.com/article/10.1007/s43681-020-00010-7#citeas. (2020).\n17. Abbasi, M., Friedler, S. A., Scheidegger, C. & Venkatasubramanian, S. Fairness in\nrepresentation: quantifying stereotyping as a representational harm. in Pro-\nceedings of the 2019 SIAM International Conference on Data Mining (SDM)801–809\n(2019).\n18. Suresh, H. & Guttag, J. V. A framework for understanding unintended con-\nsequences of machine learning. Preprint available at https://arxiv.org/abs/\n1901.10002 (2019).\n19. Scherr, S., Haim, M. & Arendt, F. Equal access to online information? Google’s\nsuicide-prevention disparities may amplify a global digital divide.N. Media Soc.\n21, 562–582 (2019).\n20. Damiano, L. & Dumouchel, P. Anthropomorphism in human-robot co-evolution.\nFront. Psychol. 9, 468 (2018).\n21. Hortensius, R. & Cross, E. S. From automata to animate beings: the scope and\nlimits of attributing socialness to artiﬁcial agents. Ann. N. Y. Acad. Sci. 1426,\n93–110 (2018).\n22. Serag, A. et al. Translational AI and deep learning in diagnostic pathology.Front.\nMed. 6, 185 (2019).\n23. Kotter, E. & Ranschaert, E. Challenges and solutions for introducing arti ﬁcial\nintelligence (AI) in daily clinical workﬂow. Eur. Radiol. 31,5 –7 (2021).\nACKNOWLEDGEMENTS\nThe authors thank Kenneth W. Goodman for his feedback on a draft of the article. The\nthoughts in this article are informed by work supported by the National Human\nGenome Research Institute and the Ofﬁce of the Director of the National Institutes of\nHealth under Award Number R21HG011277 (Korngiebel). The content is solely the\nresponsibility of the authors and does not necessarily represent the ofﬁcial views of\ntheir respective institutions or of the National Institutes of Health.\nAUTHOR CONTRIBUTIONS\nBoth authors contributed equally to drafting and revisions of the manuscript and\nhave approved theﬁnal version.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nCorrespondence and requests for materials should be addressed to D.M.K.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visithttp://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2021\nD.M. Korngiebel and S.D. Mooney\n3\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2021)    93 ",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7595316171646118
    },
    {
      "name": "Transformer",
      "score": 0.6337459087371826
    },
    {
      "name": "eHealth",
      "score": 0.6130112409591675
    },
    {
      "name": "Computer science",
      "score": 0.5403194427490234
    },
    {
      "name": "Health care",
      "score": 0.4974117577075958
    },
    {
      "name": "Human health",
      "score": 0.4505050480365753
    },
    {
      "name": "Data science",
      "score": 0.3473401665687561
    },
    {
      "name": "Human–computer interaction",
      "score": 0.33284008502960205
    },
    {
      "name": "Engineering",
      "score": 0.3040560483932495
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27109187841415405
    },
    {
      "name": "Medicine",
      "score": 0.18527686595916748
    },
    {
      "name": "Electrical engineering",
      "score": 0.10792773962020874
    },
    {
      "name": "Political science",
      "score": 0.07885041832923889
    },
    {
      "name": "Environmental health",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1308814778",
      "name": "Hastings Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801852214",
      "name": "University of Washington Medical Center",
      "country": "US"
    }
  ],
  "cited_by": 245
}