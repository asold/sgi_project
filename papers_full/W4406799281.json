{
    "title": "ollamar: An R package for running large language models",
    "url": "https://openalex.org/W4406799281",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2255186063",
            "name": "Hause Lin",
            "affiliations": [
                "Massachusetts Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5106714257",
            "name": "Tawab Safi",
            "affiliations": [
                "Massachusetts Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4405418891",
        "https://openalex.org/W4391513992",
        "https://openalex.org/W4394781960",
        "https://openalex.org/W4402387210",
        "https://openalex.org/W4393026942",
        "https://openalex.org/W4405621269",
        "https://openalex.org/W4403286813"
    ],
    "abstract": null,
    "full_text": "ollamar: An R package for running large language\nmodels\nHause Lin 1 and Tawab Safi 1\n1 Massachusetts Institute of Technology, USA\nDOI: 10.21105/joss.07211\nSoftware\n• Review\n• Repository\n• Archive\nEditor: Chris Vernon\nReviewers:\n• @KennethEnevoldsen\n• @elenlefoll\nSubmitted: 24 August 2024\nPublished: 24 January 2025\nLicense\nAuthors of papers retain copyright\nand release the work under a\nCreative Commons Attribution 4.0\nInternational License ( CC BY 4.0 ).\nSummary\nLarge language models (LLMs) have transformed natural language processing and AI applica-\ntions across numerous domains. While cloud-based LLMs are common, locally deployed models\noffer distinct advantages in reproducibility, data privacy, security, and customization. ollamar\nis an R package that provides an interface to Ollama, enabling researchers and data scientists\nto integrate locally-hosted LLMs into their R workflows seamlessly. It implements a consis-\ntent API design that aligns with other programming languages and follows established LLM\nusage conventions. It further distinguishes itself by offering flexible output formats and easy\nmanagement of conversation history. ollamar is maintained on GitHub and available through\nthe Comprehensive R Archive Network (CRAN), where it regularly undergoes comprehensive\ncontinuous integration testing across multiple platforms.\nState of the Field\nThe increasing importance of LLMs in various fields has created a demand for accessible tools\nthat allow researchers and practitioners to leverage LLMs within their preferred programming\nenvironments. Locally deployed LLMs offer advantages in terms of data privacy, security,\nreproducibility, and customization, making them an attractive option for many users ( Chan et al.,\n2024; Liu et al., 2024 ; Lytvyn, 2024; Shostack, 2024). Currently, Ollama (https://ollama.com/)\nis one of the most popular tools for running locally hosted LLMs, offering access to a range of\nmodels with different sizes and capabilities. Several R packages currently facilitate interaction\nwith locally deployed LLMs through Ollama, each with distinct approaches, capabilities, and\nlimitations.\nThe rollama (Gruber & Weber, 2024 ) and tidyllm (Brüll, 2024 ) libraries focus on text\ngeneration, conversations, and text embedding, but their core functions do not always or\nnecessarily mirror the official Ollama API endpoints, which lead to inconsistencies and confusion\nfor users familiar with the official API. Additionally, these libraries may not support all Ollama\nendpoints and features. Another popular R library is tidychatmodels (Albert, 2024 ), which\nallows users to chat with different LLMs, but it is not available on CRAN, and therefore is not\nsubject to the same level of testing and quality assurance as CRAN packages.\nAll these libraries also adopt the tidyverse workflow, which some may find restrictive, opinionated,\nor unfamiliar. While the tidyverse is popular in the R community, it may not align with the\nprogramming style or workflow of all users, especially those coming from other programming\nlanguages or domains. This limitation can hinder the accessibility and usability of these libraries\nfor a broader audience of R users. Thus, the R ecosystem lacks a simple and reliable library to\ninterface with Ollama, even though R is a popular and crucial tool in statistics, data science,\nand various research domains ( Hill et al., 2024 ).\nLin, & Safi. (2025). ollamar: An R package for running large language models. Journal of Open Source Software , 10(105), 7211. https:\n//doi.org/10.21105/joss.07211.\n1\nStatement of Need\nollamar addresses the limitations of existing R libraries by providing a consistent API design\nthat mirrors official Ollama endpoints, enabling seamless integration across programming\nenvironments. It also offers flexible output formats supporting dataframes, JSON lists, raw\nstrings, and text vectors, allowing users to choose the format that best suits their needs. The\npackage also includes independent conversation management tools that align with industry-\nstandard chat formats, streamlining the integration of locally deployed LLMs into R workflows.\nThese features make ollamar a versatile and user-friendly tool for researchers and data scientists\nworking with LLMs in R. It fills a critical gap in the R ecosystem by providing a native interface\nto run locally deployed LLMs, and is already being used by researchers and practitioners ( Turner,\n2024).\nDesign\nollamar implements a modular, non-opinionated, and consistent approach that aligns with\nestablished software engineering principles, where breaking down complex systems into man-\nageable components enhances maintainability, reusability, and overall performance. It also\navoids feature bloat, ensuring the library remains focused on its core functionality. The key\ndesign philosophy of ollamar are described below.\nConsistency and maintainability : It provides an interface to the Ollama server and all API\nendpoints, closely following the official API design, where each function corresponds to a\nspecific endpoint. This implementation ensures the library is consistent with the official Ollama\nAPI and easy to maintain and extend as new features are added to Ollama. It also makes it\neasy for R users to understand how similar libraries (such as in Python and JavaScript) work\nwhile allowing users familiar with other programming languages to adapt to and use this library\nquickly. The consistent API structure across languages facilitates seamless transitions and\nknowledge transfer for developers working in multi-language environments.\nConsistent and flexible output formats : All functions that call API endpoints return\nhttr2::httr2_response objects by default, which provides a consistent interface for flexible\ndownstream processing. If preferred, users have the option to specify different output formats\nwhen calling the endpoints, such as dataframes ( \"df\"), lists (of JSON objects) ( \"jsonlist\"),\nraw strings ( \"raw\"), text vectors ( \"text\"), or request objects ( \"req\"). Alternatively, use\nthe resp_process() function to convert and process the response object as needed. This\nflexibility allows users to choose the format that best suits their needs, such as when working\nwith different data structures, integrating the output with other R packages, or allowing\nparallelization via the popular httr2 library. It also allows users to easily build applications or\npipelines on top of the library, without being constrained by a specific output format.\nEasy management of LLM conversation history : LLM APIs often expect conversation/chat\nhistory data as input, often nested lists or JSON objects. Note that this data format is\nstandard for chat-based applications and APIs (not limited to Ollama), such as those provided\nby OpenAI and Anthropic. ollamar simplifies preparing and processing conversational data for\ninput to different LLMs, focusing on streamlining the workflow for the most popular chat-based\napplications.\nAutomated regular testing : ollamar is hosted on GitHub and available through CRAN, where\nit undergoes comprehensive continuous integration testing across multiple platforms to ensure\nreliability. Daily automated quality checks maintain long-term stability, and scheduled tests\nverify version compatibility.\nLin, & Safi. (2025). ollamar: An R package for running large language models. Journal of Open Source Software , 10(105), 7211. https:\n//doi.org/10.21105/joss.07211.\n2\nConclusion\nollamar bridges a crucial gap in the R ecosystem by providing seamless access to large language\nmodels through Ollama. Its focus on consistency, flexibility, and maintainability makes it a\nversatile and user-friendly tool for researchers and data scientists working with LLMs in R.\nThese design choices ensure ollamar is a user-friendly and reliable tool for integrating locally\ndeployed LLMs into R workflows, accelerating research and development in fields relying on R\nfor data analysis and machine learning.\nAcknowledgements\nThis project was partially supported by the Canadian Social Sciences & Humanities Research\nCouncil Tri-Agency Funding (funding reference: 192324).\nReferences\nAlbert, R. (2024). Tidychatmodels: Chat with all kinds of AI models through a common\ninterface. https://github.com/AlbertRapp/tidychatmodels\nBrüll, E. (2024). Tidyllm: Tidy integration of large language models. CRAN: Contributed\nPackages. https://doi.org/10.32614/cran.package.tidyllm\nChan, R. S.-Y., Nanni, F., Brown, E., Chapman, E., Williams, A. R., Bright, J., & Gabasova,\nE. (2024). Prompto: An open source library for asynchronous querying of LLM endpoints.\narXiv. https://doi.org/10.48550/arXiv.2408.11847\nGruber, J. B., & Weber, M. (2024). rollama: An R package for using generative large language\nmodels through Ollama. arXiv. https://doi.org/10.48550/arXiv.2404.07654\nHill, C., Du, L., Johnson, M., & McCullough, B. D. (2024). Comparing programming languages\nfor data analytics: Accuracy of estimation in Python and R. Wiley Interdisciplinary Reviews:\nData Mining and Knowledge Discovery , 14(3), e1531. https://doi.org/10.1002/widm.1531\nLiu, F., Kang, Z., & Han, X. (2024). Optimizing RAG techniques for automotive industry\nPDF chatbots: A case study with locally deployed Ollama models. arXiv. https://doi.org/\n10.48550/arXiv.2408.05933\nLytvyn, O. (2024). Enhancing propaganda detection with open source language models: A\ncomparative study. Proceedings of the MEi:CogSci Conference , 18(1). https://journals.\nphl.univie.ac.at/meicogsci/article/view/822\nShostack, A. (2024). The boy who survived: Removing Harry Potter from an LLM is harder\nthan reported. arXiv. https://doi.org/10.48550/arXiv.2403.12082\nTurner, S. D. (2024). biorecap: an R package for summarizing bioRxiv preprints with a local\nLLM. arXiv. https://doi.org/10.48550/arXiv.2408.11707\nLin, & Safi. (2025). ollamar: An R package for running large language models. Journal of Open Source Software , 10(105), 7211. https:\n//doi.org/10.21105/joss.07211.\n3"
}