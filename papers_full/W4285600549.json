{
  "title": "Geometric Transformer for End-to-End Molecule Properties Prediction",
  "url": "https://openalex.org/W4285600549",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2553538888",
      "name": "Yoni Choukroun",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2059111593",
      "name": "Lior Wolf",
      "affiliations": [
        "Tel Aviv University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2798416089",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W4287586570",
    "https://openalex.org/W3036737467",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W2083415705",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2778051509",
    "https://openalex.org/W2923693308",
    "https://openalex.org/W3003486042",
    "https://openalex.org/W2891365537",
    "https://openalex.org/W1981049088",
    "https://openalex.org/W2962876364",
    "https://openalex.org/W2097308346",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2948990653",
    "https://openalex.org/W4287828570",
    "https://openalex.org/W3102659967",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W1504770578",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W3106310231",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2585152223",
    "https://openalex.org/W3177270411",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W3203710967",
    "https://openalex.org/W3007488165",
    "https://openalex.org/W4287585714",
    "https://openalex.org/W4286905681",
    "https://openalex.org/W3103736477"
  ],
  "abstract": "Transformers have become methods of choice in many applications thanks to their ability to represent complex interactions between elements. However, extending the Transformer architecture to non-sequential data such as molecules and enabling its training on small datasets remains a challenge. In this work, we introduce a Transformer-based architecture for molecule property prediction, which is able to capture the geometry of the molecule. We modify the classical positional encoder by an initial encoding of the molecule geometry, as well as a learned gated self-attention mechanism. We further suggest an augmentation scheme for molecular data capable of avoiding the overfitting induced by the overparameterized architecture. The proposed framework outperforms the state-of-the-art methods while being based on pure machine learning solely, i.e. the method does not incorporate domain knowledge from quantum chemistry and does not use extended geometric inputs besides the pairwise atomic distances.",
  "full_text": "Geometric Transformer for End-to-End Molecule Properties Prediction\nYoni Choukrounand Lior Wolf\nSchool of Computer Science, Tel Aviv University\nchoukroun.yoni@gmail.com, wolf@cs.tau.ac.il\nAbstract\nTransformers have become methods of choice in\nmany applications thanks to their ability to repre-\nsent complex interactions between elements. How-\never, extending the Transformer architecture to\nnon-sequential data such as molecules and enabling\nits training on small datasets remains a challenge.\nIn this work, we introduce a Transformer-based ar-\nchitecture for molecule property prediction, which\nis able to capture the geometry of the molecule. We\nmodify the classical positional encoder by an ini-\ntial encoding of the molecule geometry, as well as\na learned gated self-attention mechanism. We fur-\nther suggest an augmentation scheme for molecular\ndata capable of avoiding the overfitting induced by\nthe overparameterized architecture. The proposed\nframework outperforms the state-of-the-art meth-\nods while being based on pure machine learning\nsolely, i.e. the method does not incorporate domain\nknowledge from quantum chemistry and does not\nuse extended geometric inputs besides the pairwise\natomic distances.\n1 Introduction\nProperties of chemical compounds can generally be estimated\nusing methods such as density functional theory (DFT) or ab\ninitio quantum chemistry [Jensen, 2017]. However, these can\nbe computationally expensive and therefore have a limited\napplicability, especially for larger systems. In recent years,\nmany approaches have started leveraging machine learning to\nreduce the computational complexity required for efficiently\npredicting molecular properties.\nIn this vein, many contributions have focused on the cre-\nation of handcrafted representations at the atomic or molec-\nular level [Christensen and others, 2020; Huang and oth-\ners, 2016 ] as input for various machine learning methods.\nSchr¨odinger’s equation indicates that the system variables\nthat define the ground-state properties of a given molecule\nare a function of the inter-atomic distances and the nuclear\ncharges solely. [Jensen, 2017 ]. Based on this observation,\nseveral recent methods predict molecular properties in an\nend-to-end fashion where the input is defined by the atoms’\ntype and spatial position. Such methods often incorporate\nquantum chemistry knowledge and rely on extensive hyper-\nparameter tuning.\nSince atomic interactions are challenging to simulate,\nmany recent works make use of graph neural networks as a\nnatural tool to model molecules [Gilmer et al., 2017 ]. Re-\nlated to graph neural networks, Transformers [Vaswaniet al.,\n2017] have recently become extremely popular in numerous\napplication domains.\nIn this paper we extend the ubiquitous Transformer to\nchemical compounds data in order to predict their ground-\nstate properties. Our work does not employ extended domain\nknowledge, and is based solely on the simple distance rel-\nevance assumption, namely, that the bigger the distance be-\ntween atomic elements, the lower the interaction.\nContrary to other works, the framework does not assume\nany extended input, e.g. quantum mechanical properties[Qiao\nand others, 2020], complex geometric constants such as bend-\ning or torsion angles [Klicpera et al., 2020b ], additional\nsolvers, e.g. fast DFT as residual solvers (i.e. delta learn-\ning) [Unke and others, 2019; Qiao and others, 2020 ]), or\neven knowledge adaptation from quantum chemistry into\nthe machine learning model design [Sch¨utt et al., 2018a;\nKlicpera et al., 2020b].\nThe Transformer we design is endowed with an adapted\npositional encoder, and with learned inter-atomic geomet-\nric embedding at the different levels of the model, allow-\ning increased representational power, while maintaining the\nmolecule invariance to rigid transformations and permutation.\nFor better regularization, we suggest to augment the training\nset by merging pairs of molecules positioned far apart.\nThe experimental results demonstrate the representational\npower of the end to end model, potentially allowing removal\nof undesirable inductive bias [Goyal and Bengio, 2020 ]\nsuch as handcrafted envelope functions. Also, by allow-\ning the model to fit the data while minimizing the domain-\nknowledge, scientific insights can emerge, such as effective\natomic cut-off radius, or most relevant molecular interactions\nfor a given property.\nRelated Work Classical molecule property prediction\nmethods combined handcrafted features generally based on\nforce field methods at the atom level[Christensen and others,\n2020] or molecular level[Huang and others, 2016], integrated\ninto various machine learning models such as Kernel methods\n[Christensen and others, 2020 ] , Gaussian processes [Bartok\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2895\nFigure 1: The proposed Transformer architecture (a). The main ar-\nchitectural modifications are the initial positional encoding based\non the pairwise distance matrix (c), and the metric learning module\ncoupled with the augmented self-attention module (b). The model is\ncomposed of M encoding blocks and the output block is composed\nof a normalized feed-forward neural network followed by a per atom\ni summation to accumulate the contribution of each atom.\net al., 2010] or Neural networks [Sch¨utt et al., 2018a].\nThese methods have recently been superseded by end-to-\nend neural networks, alleviating the need for handcrafted sig-\nnatures. The most popular and powerful models are based\non graph neural networks, which allow a natural representa-\ntion of the molecular graph [Sch¨utt et al., 2018a; Unke and\nothers, 2019; Anderson et al., 2019; Klicpera et al., 2020b;\nQiao and others, 2020 ]. These architectures are generally\ndesigned based on the message-passing mechanism, by ag-\ngregating features obtained from atom types, geometric in-\nvariants such as pairwise inter-atomic distances [Sch¨utt et\nal., 2018a; Unke and others, 2019 ], bending or torsion an-\ngles [Klicpera et al., 2020b ], or handcrafted atomic features\nderived from quantum mechanics [Qiao and others, 2020 ].\nExisting methods generally involve a deep understanding\nand a cautious adaptation of the underlying physics in or-\nder to provide better preconditioning [Klicpera et al., 2020b;\nQiao and others, 2020].\nTransformer neural networks were originally introduced\nfor machine translation [Vaswani et al., 2017] and they now\ndominate most applications in the field of Natural Language\nProcessing. Transformer encoders primarily rely on the self-\nattention operation in conjunction with feed-forward layers,\nallowing manipulation of variable-size sequences and learn-\ning of long-range dependencies. Many works have aug-\nmented the self-attention mechanism using domain-specific\nknowledge [Chen et al., 2017; Bello et al., 2019].\nRecently, a transformer architecture called MAT has been\nproposed for chemical molecules [Maziarka et al., 2020;\nMaziarka et al., 2021]. MAT modifies the self-attention mod-\nule by summing the inverse exponent of the pairwise dis-\ntance matrix to the self-attention tensor. As one contribution\nof our work, we show that the MAT self-attention architec-\nture is sub-optimal in modelling interactions, and we propose\na better self-attention module capable of capturing the con-\nnectivity of the graph. Our method also outperforms con-\ncurrent efforts [Wu et al., 2021; Kwak et al., 2021 ] which\nintegrate mollifiers from the literature [Sch¨utt et al., 2018a;\nKlicpera et al., 2020b] to the key element of the self-attention.\n2 Method\nA molecule is defined by the atomic numbers\nz = {z1, . . . , zN }∈ Z+, which serve to identify each\ntype of atom, and the three dimensional positions\nX = {x1, . . . , xN } ∈ R3 of the N atoms composing\nit. Molecular predictions must satisfy fundamental symme-\ntries and invariance of physical laws such as invariance to\nrigid spatial transformation (rotation and translation) and\npermutation (atoms of the same type are indistinguishable).\nTherefore, the positional input is transformed to interatomic\nEuclidean distances D = {dij}N\ni,j=1 where dij = ∥xi −xj∥2\nfor rigid transform invariance, while permutation invari-\nance is obtained via equal initial atomic representations of\nidentical particles.\nIn this work, we design a parameterized deep neural net-\nwork fθ for scalar regression of properties p ∈R such that\nfθ : {z, D} →R. We do not include any further auxil-\niary features in the input (e.g. adjacency, bonds type, hy-\nbridization [Gilmer et al., 2017; Maziarka et al., 2020], DFT\nsolver [Unke and others, 2019; Qiao and others, 2020 ]),\nneither any kind of knowledge adaptation from quantum\nphysics/chemistry [Unke and others, 2019; Klicpera et al.,\n2020b; Klicpera et al., 2020a; Qiao and others, 2020 ]. Fig-\nure 1 depicts the proposed architecture. The positional en-\ncoding provides an initial geometry aware embedding of the\natoms while the self-attention mechanism enables the accu-\nrate learning of the molecule geometry as well as the determi-\nnation of the complex geometric interactions that are modeled\nin order to perform the regression task.\nTransformer Transformer was introduced by [Vaswani et\nal., 2017] as a novel, attention-based building block for ma-\nchine translation. The input sequence is first embedded into\na high-dimensional space, coupled with positional embed-\nding for each element. The embeddings are then propagated\nthrough multiple normalized self-attention and feed-forward\nblocks.\nThe self-attention mechanism introduced by Transformers\nis based on a trainable associative memory with (key, value)\nvector pairs where a query vector q ∈Rd is matched against\na set of k key vectors using scaled inner products as follows\nA(Q, K, V) = Softmax\n\u0012QKT\n√\nd\n\u0013\nV, (1)\nwhere Q ∈ RN×d, K ∈ Rk×d and V ∈ Rk×d repre-\nsent the packed N queries, k keys and values tensors re-\nspectively. Keys, queries and values are obtained using lin-\near transformations of the sequence’ elements. A multi-head\nself-attention layer is defined by extending the self-attention\nusing h attention heads, i.e. h self-attention functions applied\nto the input, reprojected to values via a dh ×D linear layer.\nGeometric Positional Encoding The initial embedding of\nthe molecule is based solely on the atoms’ type and thus is\nunable to differentiate similar atoms since the molecule ge-\nometry is omitted. The original Transformer’s positional en-\ncoding module aims to transfer a measure of proximity of\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2896\nFigure 2: The impact of the initial positional encoder on the em-\nbedding for two different molecules (left and right). We show the\npairwise distance matrix D (left), pairwise distance of the initial\nembedding Emb(z) (middle), and the pairwise distance of the fi-\nnal geometric embedding (right). The marks on the map represent\nthe type of the atoms zi. Cold and warm colors represent low and\nhigh values respectively.\nFigure 3: The validation L1 loss (U0 property) of a regular trans-\nformer without any geometric input (red), the same transformer with\nour Laplacian extension (green), and with the proposed initial geo-\nmetric positional encoding (GPE).\nthe sequence elements to the initial embedding. In our case,\nsince the input is defined as a set rather than a sequence, the\npositional encoder needs to be adapted in order to provide\na geometry-aware initial embedding. Here we propose to use\nthe pairwise inter-atomic distance matrix in order to bring po-\nsitional information to each atom. For each atom we first em-\nbed its pairwise distance vector to a single scalar in order to\nkeep the module invariant to the size of the molecule, and then\nproject it to the initial embedding space. Formally, denoting\nthe atom embedding Emb(zi) : Z+ →Rd and the pairwise\ndistance matrix D ∈R+n×n such that (D)ij := (Di)j = dij,\nwe have\nyi = Emb(zi) + W\nX\nj\nfpos(dij). (2)\nHere, yi denotes the obtained initial positioned embedding\nof atom zi, fpos : R+ →R denotes the mapping of the pair-\nwise distance parameterized as a shallow neural network , and\nW ∈ Rd denotes the projection matrix of the atomic one-\ndimensional embedding onto the embedding space. Figure\n2 depicts the impact of the positional encoder on the initial\nembedding. As can be observed, the initial embedding does\nnot differentiate between atoms of the same type, while posi-\ntional encoding brings information about the geometry. The\nconvergence plot in Figure 3 demonstrates that the proposed\npositional encoding allows the transformer to learn the molec-\nular geometry and thus be able to predict properties of the\nmolecule. We also compare our method with the positional\nencoder of [Dwivedi and Bresson, 2020] where, since in our\nsetting no graph is given, we propose to compute the 15 (half\nthe biggest molecule size) first Laplacian eigenmaps [Belkin\nand Niyogi, 2003 ] instead of the combinatorial Laplacian.\nOur approach allows to encode the global geometry of the\nmolecule with respect to each atom, while the permutation-\ninvariant aggregation part in Eq. (2) maintains the symmetry\ninvariance of the embedding. This preconditioning enables\nfaster convergence and slightly better performance.\nGeometric Self-Attention In order to maintain the invari-\nant properties of molecules, we propose to augment the initial\npositional encoding layer and import the pairwise information\ndirectly into the self-attention layer. This is a natural choice\nsince the self-attention layer already computes pairwise sim-\nilarity of the atoms’ representations via the normalized inner\nproduct. The self-attention layer (Eq. 1) is then extended to\u0010\n˜A(Q, K, V, D)\n\u0011\ni\n= φω(Qi, K, Di)V\n=\nkX\nj=1\nφω(Qi, Kj, Dij)Vj,\n(3)\nwhere D denote the pairwise distance matrix, and φω a pa-\nrameterized, potentially learned, similarity function. Sev-\neral formulations can be conceived, however every extension\nshould satisfy thedistance relevance assumptionsuch that φω\nis a vanishing mapping with respect to distanceDij, such that\nfor a given positive δ scalar for all Dij > δwe would have\nφω(Qi, Kj, Dij) →0.\nThese assumptions are certainly reminiscent of cut-off dis-\ntance (radii) and of many inter-atomic formulations such as\nthe Van der Waals force or the Axilrod-Teller-Muto potential\n[Axilrod and Teller, 1943; Muto, 1943].\nAssuming a vanishing mapping ψω(D) is given and ap-\nplied element-wise, several approaches have been proposed\nin order to extend the self-attention mechanism. One pop-\nular extension is performed inside the softmax function as in\n[Wang and others, 2020] such that\nφω(Q, Kj, D) = Softmax\n\u0012QKT\n√\nd\n+ ψω(D)\n\u0013\n, (4)\nThis extension has the ability to fulfill the relevance assump-\ntion requirement if the function ψω(D), because of the soft-\nmax function, assigns negative values to distant atoms (at the\nlimit limd→∞ψω(d) = −∞). However, such a requirement\ncan be hard to design or even to learn with parameterized\nrepresentation. Another option is to extend the transformer\noutside of the normalization such that\nφω(Q, Kj, D) = Softmax\n\u0012QKT\n√\nd\n\u0013\n+ ψω(D). (5)\nIn this vein and concurrent to our work, [Maziarka et al.,\n2020] suggested MAT, a transformer architecture where the\nself-attention mechanism is defined as ψω(D) = ωexp(−D),\nwith hyper-parameter ω ∈R. This approach clearly fails in\nfulfilling the assumption presented above since distant atoms\nstill impact the self-attention via the softmax similarity.\nWe propose to directly multiply the distance relation by the\nsimilarity tensor as follows\nφω(Q, Kj, D) = Softmax\n\u0012QKT\n√\nd\n\u0013\n⊙ψω(D), (6)\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2897\nFigure 4: 1D cuts of the learned spherical metric at different blocks of the proposed Transformer. The distance is in ˚A. One can observe\ndifferent low-, band- and high-pass filters, which demonstrates the model’s ability to learn the connectivity that is relevant for each level of\nthe network. Notice the different dynamic ranges of the filtered values, which lead to different levels of impact on the self-similarity matrix.\nwith ⊙denoting the Hadamard product. This way, the in-\nteratomic distance has a direct gating impact on the pair-\nwise atomic contribution obtained from the embedding. Such\nrepresentation is especially important for the augmentation\nscheme we present later.\nOther multiplicative alternatives are possible, such as\ntransferring the Hadamard product inside the softmax [Wu\net al., 2021; Fuchs et al., 2020; Kwak et al., 2021 ]. How-\never there is a computationally demanding and hard to train\nneed to couple the input ofψ with the similarity tensor values\nin order to ensure the desired values of the softmax function,\ni.e., in that case ϕω(Qi, Kj, Dij) →0 for large Dij implies\nthat ψω(Dij) → ±∞and that QT\ni Kj and ψω(Dij) are of\nopposite signs.)\nLearning the Graph Geometry Many methods have\nstruggled to model the interaction function ψω. From force\nfield methods to the recent learning-based approach, there\nis a need to empirically redefine the Euclidean pairwise dis-\ntance in order to satisfy physical experimentation and/or per-\nformance.\nMany handcrafted methods adopt molecular mechanics ap-\nproximations of un/bonded interactions (e.g. stretch, bend-\ning, electrostatic or Van der Waals energies [Christensen and\nothers, 2020 ]) where molecular properties are obtained via\nthe modification of the interatomic distance, generally using\nexponential mapping of the distance.\nNeural network-based methods adopt a similar approach,\nwhere atomic embedding is obtained by modifying the pair-\nwise distance using various learned handcrafted mollifiers,\nGaussian radial basis functions, or complex basis in the cor-\nresponding function space [Sch¨utt et al., 2018a; Klicpera et\nal., 2020b; Qiao and others, 2020].\nOne of the most critical hyper-parameters present in every\nmethod is the cut-off distance parameter, connecting only\natoms which lie within the cut-off sphere. This hyper-\nparameter may also change according to the property to be\npredicted [Schutt et al., 2018b].\nHere we propose to learn the pairwise metric and the\nmolecule connectivity at each level of the Transformer. The\ntransformation of the Euclidean distance coupled with the\nself-attention mechanism suggested above allows us to di-\nrectly optimize the inter-atomic representation as well as the\ncut-off distance according to the prediction objective, re-\nmoving cumbersome hyper-parameterization, and allowing\nsoft and differentiable construction of the adjacency matrix\nlearned in a self-adaptive fashion. The similarity function is\nnow simply given by\nφω(Q, K, D) = Softmax\n\u0012QKT\n√\nd\n\u0013\n⊙ψω(D−1)2, (7)\nwhere ψω : R →R is an element-wise learnable function.\nWe parameterize ψω as a shallow, fully connected neural\nnetwork, and we further enforce the positiveness of the new\nsimilarity map by squaring the filtered distances. Transform-\ning the (element-wise) inverse of the distance D−1 instead\nof D speeds up the training since ψω transforms an already\nvanishing function (i.e. the multiplicative inverse function).\nIn contrast with existing methods that use envelopes[Unke\nand others, 2019; Klicperaet al., 2020b] or distance mollifiers\n[Sch¨utt et al., 2018a], as well as cut-off parameters, all induc-\ning a handcrafted graph connectivity, we optimally unify the\nlearning of the pairwise metric and of the graph adjacency.\nWe present the learned metrics for several of the Trans-\nformer blocks in Figure 4. As can be seen, the metric ob-\ntained is both more complex and more abstract than mono-\ntonic or Gaussian functions used in previous works [Sch¨utt\net al., 2018a; Unke and others, 2019; Maziarka et al., 2020].\nIt is interesting to notice that some of the obtained cut-off\ndistances lie around values empirically set in other works\n(2 −6 ˚A). The diversified filter bank demonstrates the dy-\nnamic graph connectivity the network learns via the induced\nmasking of the similarity map.\nRegularization via Molecule Augmentation Transform-\ners are generally extremely large and over-parameterized\nmodels. Dropout layers commonly used in Transformers in\norder to avoid overfitting cannot be used in our setting, be-\ncause of the permutation-invariance requirement as described\nin [Lee et al., 2019 ]. One of the most efficient techniques\nfor reducing overfitting is data augmentation. However it is\nnot straightforward to augment molecular data (elements of a\nset), especially not for regression tasks since modification of\none atom type or its spatial positions has unpredictable effects\non molecular properties.\nWhile many augmentation methods modify each datum,\nMixup strategies [Zhang and others, 2017 ] intend to create\nnew data samples from pairs (or more) of data. Here, follow-\ning our initial distance relevance assumption, we propose to\nextend the Mixup idea to molecules where a new data sam-\nple is obtained by creating a new system of two molecules\npositioned far apart.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2898\nFigure 5: The proposed augmentation scheme. Given two molecular\nsystems S1 and S2, we create a new system S1 ∪ S2 by performing\nrandom rigid transformation on the system S1.\nIn our mixup scheme, we constrain the property of the\nnew system to be the sum of the properties of the two sub-\nmolecules, even if the predicted property is intensive, i.e.,\nunlike extensive properties it is not physically additive. The\nsum of the contribution of each atom in the output module\nallows the model to learn to disentangle the two distanced\nsub-systems and reduce overfitting.\nFormally, for a given property p and given two centered\nmolecules Mi, Mj such that Mk = {zk, Xk}we have\n˜Xj := {R ·x1 + T, . . . , R·xNj + T}\nMij = Mi ∪Mj :=\nn\nz = {zi, zj}, X= {Xi, ˜Xj}\no\n,\n(8)\nwith a rotation matrix R ∈SO(3), and T = t ·1, t∈R a spa-\ntial translation vector ensuring large enough distance between\nthe molecules so that the interaction is null or negligible (e.g.\nt >103 ˚A). Thus, based on inter-atomic distances, we want\nour model to be able to differentiate the two systems such that\nthe target properties p(Mi) and p(Mj) sum, thus\np(Mij) = p(Mi ∪Mj) = p(Mi) + p(Mj). (9)\nAn illustration of the augmentation scheme is given in Fig-\nure 5. We note that methods relying on cut-off distances can-\nnot use this augmentation strategy in a straightforward man-\nner since the cut-off parameters would automatically sepa-\nrate the two molecules. Also, this approach can be extended\nto more than just pairs of atoms. The main drawback of\nthis method is an increase in training time since the created\nmolecules can be up to twice as large as the largest molecule\nof the dataset. However, this augmentation method enables a\nsignificant improvement of generalization, even on relatively\nsmall datasets, as demonstrated in the Discussion section.\n3 Experiments\nThe experimental setup including the architecture details and\nthe training procedure is provided in the Appendix1.\nQM9 The popular QM9 dataset [Ramakrishnan et al.,\n2014] contains 130, 831 molecules with up to 9 atoms of the\ntype C,N,O, and F saturated with hydrogen atoms in their\nequilibrium geometries, with chemical properties computed\n1https://arxiv.org/pdf/2110.13721.pdf\nwith DFT solvers. Following previous work, we split the\ndataset to 110, 000, 10, 000 and 10, 831 molecules for the\ntraining, validation and testing sets respectively. We use the\natomization energy for U0, U, H,and G.\nIn Table 1 (left) we report the mean absolute error (MAE)\non all QM9 targets and compare it to the state-of-the-art mod-\nels SchNet [Sch¨utt et al., 2018a ], PhysNet [Unke and oth-\ners, 2019], MGCN [Lu et al., 2019], Cormorant [Anderson et\nal., 2019 ], and DimeNet [Klicpera et al., 2020b ]. We also\ncompare with concurrent molecular data Transformers, R-\nMAT[Maziarka et al., 2021], 3D-T [Wu et al., 2021], SE(3)-T\n[Fuchs et al., 2020] and GeoT [Kwak et al., 2021].\nThe method outperforms or is similar to state-of-the-art\nmethods for most of the properties and surpasses Dimenet\nby 5.22% average performance ratio, and other Transformers\nmethods by large margins. In contrast to other works [Schutt\net al., 2018b; Klicpera et al., 2020a ], the same model and\ntraining procedure were applied for all properties. The pro-\nposed framework does not require tuning of physical hyper-\nparameters or chemical approximations, making it a true end-\nto-end framework.\nThe recent SOTA work DimeNet++ [Klicpera et al.,\n2020a] substantially outperforms DimeNet (by 9% in aver-\nage) with careful initialization and architectural modifica-\ntions, and outperforms our framework by 0.78% (average\nperformance ratio). We believe that a similar thorough ar-\nchitecture search may have similar impact on the proposed\napproach, depending on available computational resources\nsince Transformers are computationally intensive. It is also\nimportant to notice that the proposed method does not take\ninto account computationally heavy bond angles between\ntriplets of atoms (i.e. potentially inducing cubic complexity)\nas in many recent frameworks.\nMD17 We use MD17 [Chmiela et al., 2017] to test model\nperformance in molecular dynamics simulations. The goal\nof this benchmark is to predict, for eight small organic\nmolecules, the Cartesian atomic forces acting on each atom\ndue to the overall potential energy. A separate model is to\nbe trained for each molecule, in order to provide accurate in-\ndividual predictions. We test our model in the challenging\n1000 training samples setting [Chmiela et al., 2018 ]. The\noriginal training objective is extended to molecular dynamics\npredictions by backpropagating to the atom coordinates X as\nfollows\nL= ||fθ(z, D) −p||1 + ρ\n3∥−∂Xfθ(z, X) −F∥1, (10)\nwhere F denotes the nuclear three-dimensional Cartesian\nforces to be predicted and ρ is the forces’ loss coefficient.\nIn our experiments ρ is set to 103, the augmentation scheme\nis extended straightforwardly to forces (i.e. concatenation)\ndue to their translation invariance, and ReLU activations are\nreplaced with GELU non-linearities to enforce twice con-\ntinuous differentiability. As shown in Table 1 (right) our\nframework sets or reaches SOTA performances even when the\ntraining size remains extremely small, a challenging setting\nfor Transformer models. It demonstrates our method’s flexi-\nbility and its ability to generalize to other tasks and datasets.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2899\nTar\nget Unit Schnet Physnet MGCN Cormorant DimeNet SE(3)-T R-MA\nT 3D-T GeoT Our\nµ D 0.0330\n0.0529 0.0560 0.038 0.0286 0.051 0.110\n0.045 0.0297* 0.0264\nα a 3\n0 0.235 0.0615 0.085 0.0681 0.0469 0.142 0.082\n0.086 0.052 0.051\nϵHOMO meV 41.0\n32.9 42.1 34 27.8 35 31 21 25* 27.5\nϵLUMO meV\n34.0 24.7 57.4 38 19.7 33 29\n26 20.2* 20.4\n∆ϵ meV 63.0\n42.5 64.2 38 34.8 53 48\n39 43 36.1\n⟨R2⟩ a3\no 0.073 0.765 0.110\n0.961 0.331 - 0.676\n- 0.30 0.157\nZPVE meV\n1.70 1.39 1.12 2.03 1.29 - 2.23\n- 1.7* 1.24\nU0 meV 14.0 8.15 12.9 22 8.02 - 12\n- 11.1 7.35\nU meV 19.0 8.34 14.4 21 7.89 - 10\n- 11.7 7.55\nH meV 14.0 8.42 14.6 21 8.11 - 10\n- 11.3 7.73\nG meV 14.0 9.40 16.2 20 8.98 - 10\n- 11.7 8.21\ncv cal\nmol K 0.0330\n0.0280 0.0380 0.026 0.0249 0.054 0.036\n- 0.0276 0.0280\nTable 1: MAE on QM9. Best in bold and second underlined.\nTransformer based architectures are to the right of the vertical line.\nTar\nget Schnet DimeNet GeoT Our\nAspirin 1.35\n0.499 0.85 0.451\nBenzene 0.31\n0.187 0.135 0.28\nEthanol 0.39\n0.230 0.225 0.212\nMalonaldehyde\n0.66 0.383 0.402 0.369\nNaphthalene 0.58 0.215 - 0.44\nSalicylic\nacid 0.85 0.374 - 0.372\nToluene\n0.57 0.216 0.328 0.24\nUracil 0.56 0.301 - 0.301\nTable 2: MAE on MD17 forces using 1000\ntraining samples.\n3.1 Discussion\nGeometric Self-attention Analysis Figure 6, presents typ-\nical impact of the proposed self-attention mechanism on the\nsimilarity map. As can be seen, different filters applied to\nthe pairwise distance have a major impact on the similarity\nmatrix and redefine the adjacency matrix at each level of the\nnetwork. One can observe the drastic impact (color flipping)\nof the learned metric on the similarity map.\nFigure 6: Regular self-attention similarity map (left), the pairwise\ndistance matrix (center), and the resulting geometric self-attention\nsimilarity map (right) of two different molecules (left and right) at\nlayers 2 and 9 respectively.\nComparison and Ablation Studies Our ablation study\ncompares typical impact of the different self-attention mod-\nules. We present the convergence curves of our method from\nEq. (7), the concurrent Transformers based MAT method\n[Maziarka et al., 2020] from Eq. (5), the MAT method with a\nlearned metric ψω, and the sum self-attention from Eq. (4)\n[Wang and others, 2020 ] with learned metric; we refer to\nthe last method as SUM SA. The compared networks and\nthe training procedure are exactly the same, except for the\naforementioned self-attention equation itself and the distance\nmapping module. The results are presented in Figure 7 (left).\nAs can be seen, the MAT architecture presents the worse con-\nvergence, while the metric learning module significantly im-\nproves the performance. The proposed self-attention mecha-\nnism greatly surpasses all other architectures.\nFinally, we present the typical effect of the data augmen-\ntation procedure on the generalization of the network during\ntraining. Figure 7 (right) presents the impact of the augmen-\ntation on the MAE convergence of the model for both an ex-\ntensive and intensive property, namely U0 and µ. As can be\nseen, in both cases, when applying augmentation, the gener-\nalization gap is extremely reduced while the validation loss is\nmuch lower. The gain for the property µ is 175% in terms of\ntesting MAE and 75% for U0.\nFigure 7: Left: Comparison of validation MAE losses between\nMAT, MAT with the proposed learned metric (MAT+LM), the\nsummed self-attention mechanism with learned metric (SUM SA\n+LM), and our method. Right: The impact of data augmenta-\ntion on the MAE convergence and generalization of the network\n(val=validation) for U0 and µ. Continuous lines are consistently\nbelow their corresponding dashed ones.\n4 Conclusion\nWe introduce a new Transformer architecture and a training\nscheme for molecular predictions. The proposed model al-\nlows effective representation of interactions based solely on\npairwise distances. The graph geometry and connectivity can\nbe learned in a soft fashion by the network via the geometric\nself-attention module. We further propose a molecular data\naugmentation procedure based on mixing strategies, which\nleads to a clear improvement in generalization for the pro-\nposed scheme. Our results indicate that our method is the first\nTransformer, as far as we can ascertain, that is able to model\nmolecular data successfully without requiring any assump-\ntions on the underlying physical model or involving complex\ngeometric priors. We believe the advent of new datasets and\nnew transformer architectures will allow the development of\nmore efficient models also less prone to overfitting, making it\na tool of predilection for the analysis of molecular data.\nAcknowledgments\nThis project has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Horizon\n2020 research and innovation programme (grant ERC CoG\n725974). The contribution of the first author is part of a PhD\nthesis research conducted at Tel Aviv University.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2900\nReferences\n[Anderson et al., 2019] B Anderson, T Son Hy, and R Kon-\ndor. Cormorant: Covariant molecular neural networks.\narXiv:1906.04015, 2019.\n[Axilrod and Teller, 1943] BM Axilrod and Ei Teller. Inter-\naction of the van der waals type between three atoms. The\nJournal of Chemical Physics, 1943.\n[Bartok et al., 2010] A Bartok, M Payne, et al. Gaussian ap-\nproximation potentials: The accuracy of quantum mechan-\nics, without the electrons. Physical review letters, 2010.\n[Belkin and Niyogi, 2003] M Belkin and P Niyogi. Lapla-\ncian eigenmaps for dimensionality reduction and data rep-\nresentation. Neural computation, 2003.\n[Bello et al., 2019] I Bello, B Zoph, A Vaswani, et al. Atten-\ntion augmented convolutional networks. In ICCV, 2019.\n[Chen et al., 2017] Q Chen, X Zhu, Z Ling, et al. Neural\nnatural language inference models enhanced with external\nknowledge. arXiv:1711.04289, 2017.\n[Chmiela et al., 2017] S Chmiela, A Tkatchenko, et al. Ma-\nchine learning of accurate energy-conserving molecular\nforce fields. Science, 2017.\n[Chmiela et al., 2018] S Chmiela, H Sauceda, K-R M ¨uller,\net al. Towards exact molecular dynamics simulations with\nmachine-learned force fields. Nature, 2018.\n[Christensen and others, 2020] A Christensen et al. Fchl re-\nvisited: Faster and more accurate quantum machine learn-\ning. The Journal of Chemical Physics, 2020.\n[Dwivedi and Bresson, 2020] VP Dwivedi and X Bresson.\nA generalization of transformer networks to graphs.\narXiv:2012.09699, 2020.\n[Fuchs et al., 2020] F B Fuchs, D E Worrall, V Fischer, and\nM Welling. Se (3)-transformers: 3d roto-translation equiv-\nariant attention networks. arXiv:2006.10503, 2020.\n[Gilmer et al., 2017] J Gilmer, S Schoenholz, P Riley,\nO Vinyals, and G Dahl. Neural message passing for quan-\ntum chemistry. arXiv:1704.01212, 2017.\n[Goyal and Bengio, 2020] A Goyal and Y Bengio. Induc-\ntive biases for deep learning of higher-level cognition.\narXiv:2011.15091, 2020.\n[Huang and others, 2016] Bing Huang et al. Communica-\ntion: Understanding molecular representations in machine\nlearning: The role of uniqueness and target similarity. J.\nof Chemical Physics, 2016.\n[Jensen, 2017] Frank Jensen. Introduction to computational\nchemistry. John wiley & sons, 2017.\n[Klicpera et al., 2020a] J Klicpera, S Giri, JT Margraf, et al.\nFast and uncertainty-aware directional message passing\nfor non-equilibrium molecules. arXiv:2011.14115, 2020.\n[Klicpera et al., 2020b] J Klicpera, J Groß, and\nS G¨unnemann. Directional message passing for molecular\ngraphs. arXiv:2003.03123, 2020.\n[Kwak et al., 2021] B Kwak, J Jo, B Lee, and S Yoon.\nGeometry-aware transformer for molecular property pre-\ndiction. arXiv:2106.15516, 2021.\n[Lee et al., 2019] J Lee, Y Lee, J Kim, et al. Set trans-\nformer: A framework for attention-based permutation-\ninvariant neural networks. In ICML, 2019.\n[Lu et al., 2019] C Lu, Q Liu, C Wang, et al. Molecular\nproperty prediction: A multilevel quantum interactions\nmodeling perspective. In AAAI, 2019.\n[Maziarka et al., 2020] L Maziarka, T Danel, S Mucha, et al.\nMolecule attention transformer. arXiv:2002.08264, 2020.\n[Maziarka et al., 2021] L Maziarka, D Majchrowski,\nT Danel, et al. Relative molecule self-attention trans-\nformer. arXiv:2110.05841, 2021.\n[Muto, 1943] Yoshio Muto. Force between nonpolar\nmolecules. J. Phys. Math. Soc. Japan, 1943.\n[Qiao and others, 2020] Z Qiao et al. Orbnet: Deep learning\nfor quantum chemistry using symmetry-adapted atomic-\norbital features. J. of Chemical Physics, 2020.\n[Ramakrishnan et al., 2014] R Ramakrishnan, P Dral,\nM Rupp, et al. Quantum chemistry structures and\nproperties of 134 kilo molecules. Scientific data, 2014.\n[Sch¨utt et al., 2018a] K Sch¨utt, H Sauceda, P-J Kindermans,\net al. Schnet–a deep learning architecture for molecules\nand materials. The Journal of Chemical Physics, 2018.\n[Schutt et al., 2018b] KT Schutt, P Kessel, et al. Schnetpack:\nA deep learning toolbox for atomistic systems. Journal of\nchemical theory and computation, 2018.\n[Unke and others, 2019] O Unke et al. PhysNet: A neural\nnetwork for predicting energies, forces, dipole moments,\nand partial charges. J. chemical theory and computation,\n2019.\n[Vaswani et al., 2017] A Vaswani, N Shazeer, N Parmar,\net al. Attention is all you need. In NeurIPS, 2017.\n[Wang and others, 2020] H Wang et al. Axial-deeplab:\nStand-alone axial-attention for panoptic segmentation. In\nECCV, 2020.\n[Wu et al., 2021] F Wu, Q Zhang, D Radev, et al. 3d-\ntransformer: Molecular representation with transformer in\n3d space. arXiv:2110.01191, 2021.\n[Zhang and others, 2017] H Zhang et al. mixup: Beyond em-\npirical risk minimization. arXiv:1710.09412, 2017.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2901",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.7311933636665344
    },
    {
      "name": "Computer science",
      "score": 0.6561431288719177
    },
    {
      "name": "Transformer",
      "score": 0.6221784353256226
    },
    {
      "name": "Pairwise comparison",
      "score": 0.6201288104057312
    },
    {
      "name": "Encoder",
      "score": 0.5639812350273132
    },
    {
      "name": "Architecture",
      "score": 0.49915194511413574
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4526101350784302
    },
    {
      "name": "Algorithm",
      "score": 0.42239853739738464
    },
    {
      "name": "Theoretical computer science",
      "score": 0.35557886958122253
    },
    {
      "name": "Electrical engineering",
      "score": 0.15325874090194702
    },
    {
      "name": "Artificial neural network",
      "score": 0.1468794345855713
    },
    {
      "name": "Engineering",
      "score": 0.13375887274742126
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    }
  ],
  "cited_by": 16
}