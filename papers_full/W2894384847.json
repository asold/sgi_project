{
  "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
  "url": "https://openalex.org/W2894384847",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2752421016",
      "name": "Lee, Juho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A878788227",
      "name": "Lee, Yoonho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3132791220",
      "name": "Kim Jungtaek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227877655",
      "name": "Kosiorek, Adam R.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966182110",
      "name": "Choi Seungjin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223290457",
      "name": "Teh, Yee Whye",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963716836",
    "https://openalex.org/W2909878113",
    "https://openalex.org/W2594466397",
    "https://openalex.org/W2962908092",
    "https://openalex.org/W2963580001",
    "https://openalex.org/W2951775809",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2951544594",
    "https://openalex.org/W2154318594",
    "https://openalex.org/W2116810533",
    "https://openalex.org/W2787501667",
    "https://openalex.org/W2963699792",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W1644641054",
    "https://openalex.org/W2963528347",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W1571870753",
    "https://openalex.org/W2963557251",
    "https://openalex.org/W2753798143",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W1629010235",
    "https://openalex.org/W99485931",
    "https://openalex.org/W2952753997",
    "https://openalex.org/W2136440798",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2110119381",
    "https://openalex.org/W2099768828",
    "https://openalex.org/W2964208960",
    "https://openalex.org/W1750591676",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2964069537",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W2968370607",
    "https://openalex.org/W1686810756"
  ],
  "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.",
  "full_text": "Set Transformer: A Framework for Attention-based\nPermutation-Invariant Neural Networks\nJuho Lee 1 2 Yoonho Lee3 Jungtaek Kim 4 Adam R. Kosiorek1 5 Seungjin Choi 4 Yee Whye Teh1\nAbstract\nMany machine learning tasks such as multiple\ninstance learning, 3D shape recognition and few-\nshot image classiﬁcation are deﬁned on sets of in-\nstances. Since solutions to such problems do not\ndepend on the order of elements of the set, mod-\nels used to address them should be permutation\ninvariant. We present an attention-based neural\nnetwork module, the Set Transformer, speciﬁcally\ndesigned to model interactions among elements\nin the input set. The model consists of an encoder\nand a decoder, both of which rely on attention\nmechanisms. In an effort to reduce computational\ncomplexity, we introduce an attention scheme in-\nspired by inducing point methods from sparse\nGaussian process literature. It reduces computa-\ntion time of self-attention from quadratic to linear\nin the number of elements in the set. We show\nthat our model is theoretically attractive and we\nevaluate it on a range of tasks, demonstrating in-\ncreased performance compared to recent methods\nfor set-structured data.\n1. Introduction\nLearning representations has proven to be an essential prob-\nlem for deep learning and its many success stories. The\nmajority of problems tackled by deep learning are instance-\nbased and take the form of mapping a ﬁxed-dimensional\ninput tensor to its corresponding target value (Krizhevsky\net al., 2012; Graves et al., 2013).\nFor some applications, we are required to process set-\nstructured data. Multiple instance learning (Dietterich et al.,\n1Department of Statistics, University of Oxford, United King-\ndom 2AITRICS, Republic of Korea 3Kakao Corporation, Repub-\nlic of Korea 4Department of Computer Science and Engineering,\nPOSTECH, Republic of Korea 5Oxford Robotics Institute, Univer-\nsity of Oxford, United Kingdom. Correspondence to: Juho Lee\n<juho.lee@stats.ox.ac.uk>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\n1997; Maron & Lozano-P´erez, 1998) is an example of such\na set-input problem, where a set of instances is given as an\ninput and the corresponding target is a label for the entire\nset. Other problems such as 3D shape recognition (Wu et al.,\n2015; Shi et al., 2015; Su et al., 2015; Charles et al., 2017),\nsequence ordering (Vinyals et al., 2016), and various set op-\nerations (Muandet et al., 2012; Oliva et al., 2013; Edwards &\nStorkey, 2017; Zaheer et al., 2017) can also be viewed as the\nset-input problems. Moreover, many meta-learning (Thrun\n& Pratt, 1998; Schmidhuber, 1987) problems which learn\nusing different, but related tasks may also be treated as set-\ninput tasks where an input set corresponds to the training\ndataset of a single task. For example, few-shot image clas-\nsiﬁcation (Finn et al., 2017; Snell et al., 2017; Lee & Choi,\n2018) operates by building a classiﬁer using a support set\nof images, which is evaluated with query images.\nA model for set-input problems should satisfy two critical\nrequirements. First, it should be permutation invariant —\nthe output of the model should not change under any permu-\ntation of the elements in the input set. Second, such a model\nshould be able to process input sets of any size. While these\nrequirements stem from the deﬁnition of a set, they are not\neasily satisﬁed in neural-network-based models: classical\nfeed-forward neural networks violate both requirements,\nand RNNs are sensitive to input order.\nRecently, Edwards & Storkey (2017) and Zaheer et al.\n(2017) propose neural network architectures which meet\nboth criteria, which we call set pooling methods. In this\nmodel, each element in a set is ﬁrst independently fed into\na feed-forward neural network that takes ﬁxed-size inputs.\nResulting feature-space embeddings are then aggregated\nusing a pooling operation (mean, sum, max or similar).\nThe ﬁnal output is obtained by further non-linear processing\nof the aggregated embedding. This remarkably simple ar-\nchitecture satisﬁes both aforementioned requirements, and\nmore importantly, is proven to be a universal approximator\nfor any set function (Zaheer et al., 2017). Thanks to this\nproperty, it is possible to learn a complex mapping between\ninput sets and their target outputs in a black-box fashion,\nmuch like with feed-forward or recurrent neural networks.\nEven though this set pooling approach is theoretically at-\ntractive, it remains unclear whether we can approximate\narXiv:1810.00825v3  [cs.LG]  26 May 2019\nSet Transformer\ncomplex mappings well using only instance-based feature\nextractors and simple pooling operations. Since every el-\nement in a set is processed independently in a set pooling\noperation, some information regarding interactions between\nelements has to be necessarily discarded. This can make\nsome problems unnecessarily difﬁcult to solve.\nConsider the problem of amortized clustering, where we\nwould like to learn a parametric mapping from an input\nset of points to the centers of clusters of points inside the\nset. Even for a toy dataset in 2D space, this is not an easy\nproblem. The main difﬁculty is that the parametric mapping\nmust assign each point to its corresponding cluster while\nmodelling the explaining away pattern such that the resulting\nclusters do not attempt to explain overlapping subsets of\nthe input set. Due to this innate difﬁculty, clustering is\ntypically solved via iterative algorithms that reﬁne randomly\ninitialized clusters until convergence. Even though a neural\nnetwork with a set poling operation can approximate such an\namortized mapping by learning to quantize space, a crucial\nshortcoming is that this quantization cannot depend on the\ncontents of the set. This limits the quality of the solution\nand also may make optimization of such a model more\ndifﬁcult; we show empirically in Section 5 that such pooling\narchitectures suffer from under-ﬁtting.\nIn this paper, we propose a novel set-input deep neural\nnetwork architecture called the Set Transformer, (cf. Trans-\nformer, (Vaswani et al., 2017)). The novelty of the Set\nTransformer is in three important design choices:\n1. We use a self-attention mechanism to process every\nelement in an input set, which allows our approach to\nnaturally encode pairwise- or higher-order interactions\nbetween elements in the set.\n2. We propose a method to reduce theO(n2) computation\ntime of full self-attention (e.g. the Transformer) to\nO(nm) where mis a ﬁxed hyperparameter, allowing\nour method to scale to large input sets.\n3. We use a self-attention mechanism to aggregate fea-\ntures, which is especially beneﬁcial when the prob-\nlem requires multiple outputs which depend on each\nother, such as the problem of meta-clustering, where\nthe meaning of each cluster center heavily depends its\nlocation relative to the other clusters.\nWe apply the Set Transformer to several set-input problems\nand empirically demonstrate the importance and effective-\nness of these design choices, and show that we can achieve\nthe state-of-the-art performances for the most of the tasks.\n2. Background\n2.1. Pooling Architecture for Sets\nProblems involving a set of objects have the permutation\ninvariance property: the target value for a given set is the\nsame regardless of the order of objects in the set. A sim-\nple example of a permutation invariant model is a network\nthat performs pooling over embeddings extracted from the\nelements of a set. More formally,\nnet({x1,...,x n}) =ρ(pool({φ(x1),...,φ (xn)})). (1)\nZaheer et al. (2017) have proven that all permutation in-\nvariant functions can be represented as (1) when pool is\nthe sum operator and ρ,φ any continuous functions, thus\njustifying the use of this architecture for set-input problems.\nNote that we can deconstruct (1) into two parts: an encoder\n(φ) which independently acts on each element of a set of n\nitems, and a decoder (ρ(pool(·))) which aggregates these\nencoded features and produces our desired output. Most\nnetwork architectures for set-structured data follow this\nencoder-decoder structure.\nZaheer et al. (2017) additionally observed that the model\nremains permutation invariant even if the encoder is a stack\nof permutation-equivariant layers:\nDeﬁnition 1. Let Snbe the set of all permutations of indices\n{1,...,n }. A function f : Xn →Yn is permutation equiv-\nariant iff for any permutation π∈Sn, f(πx) =πf(x).\nAn example of a permutation-equivariant layer is\nfi(x; {x1,...,x n}) =σi(λx+ γpool({x1,...,x n}))\n(2)\nwhere pool is the pooling operation,λ,γ are learnable scalar\nvariables, and σ(·) is a nonlinear activation function.\n2.2. Attention\nAssume we have nquery vectors (corresponding to a set\nwith n elements) each with dimension dq: Q ∈ Rn×dq .\nAn attention function Att(Q,K,V ) is a function that\nmaps queries Qto outputs using nv key-value pairs K ∈\nRnv×dq ,V ∈Rnv×dv .\nAtt(Q,K,V ; ω) =ω\n(\nQK⊤)\nV. (3)\nThe pairwise dot product QK⊤ ∈Rn×nv measures how\nsimilar each pair of query and key vectors is, with weights\ncomputed with an activation function ω. The output\nω(QK⊤)V is a weighted sum of V where a value gets\nmore weight if its corresponding key has larger dot product\nwith the query.\nMulti-head attention , originally introduced in Vaswani\net al. (2017), is an extension of the previous attention\nSet Transformer\n(a) Our model\n (b) MAB\n (c) SAB\n (d) ISAB\nFigure 1.Diagrams of our attention-based set operations.\nscheme. Instead of computing a single attention func-\ntion, this method ﬁrst projects Q,K,V onto h different\ndM\nq ,dM\nq ,dM\nv -dimensional vectors, respectively. An atten-\ntion function (Att(·; ωj)) is applied to each of these hpro-\njections. The output is a linear transformation of the con-\ncatenation of all attention outputs:\nMultihead(Q,K,V ; λ,ω) = concat(O1,··· ,Oh)WO,\n(4)\nwhere Oj = Att(QWQ\nj ,KW K\nj ,VW V\nj ; ωj) (5)\nNote that Multihead(·,·,·; λ) has learnable parameters\nλ = {WQ\nj ,WK\nj ,WV\nj }h\nj=1, where WQ\nj ,WK\nj ∈Rdq×dM\nq ,\nWV\nj ∈Rdv×dM\nv , WO ∈RhdM\nv ×d. A typical choice for the\ndimension hyperparameters is dM\nq = dq/h, dM\nv = dv/h,\nd= dq. For brevity, we set dq = dv = d, dM\nq = dM\nv = d/h\nthroughout the rest of the paper. Unless otherwise speciﬁed,\nwe use a scaled softmax ωj(·) = softmax(·/\n√\nd), which\nour experiments were worked robustly in most settings.\n3. Set Transformer\nIn this section, we motivate and describe the Set Trans-\nformer: an attention-based neural network that is designed\nto process sets of data. Similar to other architectures, a Set\nTransformer consists of an encoder followed by a decoder\n(cf. Section 2.1), but a distinguishing feature is that each\nlayer in the encoder and decoder attends to their inputs to\nproduce activations. Additionally, instead of a ﬁxed pooling\noperation such as mean, our aggregating function pool(·)\nis parameterized and can thus adapt to the problem at hand.\n3.1. Permutation Equivariant (Induced) Set Attention\nBlocks\nWe begin by deﬁning our attention-based set operations,\nwhich we call SAB and ISAB. While existing pooling meth-\nods for sets obtain instance features independently of other\ninstances, we use self-attention to concurrently encode the\nwhole set. This gives the Set Transformer the ability to com-\npute pairwise as well as higher-order interactions among\ninstances during the encoding process. For this purpose,\nwe adapt the multihead attention mechanism used in Trans-\nformer. We emphasize that all blocks introduced here are\nneural network blocks with their own parameters, and not\nﬁxed functions.\nGiven matrices X,Y ∈Rn×d which represent two sets of\nd-dimensional vectors, we deﬁne the Multihead Attention\nBlock (MAB) with parameters ωas follows:\nMAB(X,Y ) = LayerNorm(H+ rFF(H)), (6)\nwhere H = LayerNorm(X+ Multihead(X,Y,Y ; ω)),\n(7)\nrFF is any row-wise feedforward layer (i.e., it pro-\ncesses each instance independently and identically), and\nLayerNorm is layer normalization (Ba et al., 2016). The\nMAB is an adaptation of the encoder block of the Trans-\nformer (Vaswani et al., 2017) without positional encoding\nand dropout. Using the MAB, we deﬁne the Set Attention\nBlock (SAB) as\nSAB(X) := MAB(X,X). (8)\nIn other words, an SAB takes a set and performs self-\nattention between the elements in the set, resulting in a set\nof equal size. Since the output of SAB contains information\nabout pairwise interactions among the elements in the input\nset X, we can stack multiple SABs to encode higher order\ninteractions. Note that while the SAB (8) involves a multi-\nhead attention operation (7), where Q = K = V = X, it\ncould reduce to applying a residual block on X. In practice,\nit learns more complicated functions due to linear projec-\ntions of X inside attention heads, (3) and (5).\nA potential problem with using SABs for set-structured\ndata is the quadratic time complexity O(n2), which may be\ntoo expensive for large sets ( n ≫1). We thus introduce\nthe Induced Set Attention Block (ISAB), which bypasses\nthis problem. Along with the set X ∈Rn×d, additionally\ndeﬁne md-dimensional vectors I ∈Rm×d, which we call\ninducing points. Inducing points I are part of the ISAB\nitself, and they are trainable parameters which we train\nalong with other parameters of the network. An ISAB with\nminducing points I is deﬁned as:\nISABm(X) = MAB(X,H) ∈Rn×d, (9)\nwhere H = MAB(I,X) ∈Rm×d. (10)\nThe ISAB ﬁrst transforms I into H by attending to the\ninput set. The set of transformed inducing points H, which\nSet Transformer\ncontains information about the input setX, is again attended\nto by the input set X to ﬁnally produce a set of nelements.\nThis is analogous to low-rank projection or autoencoder\nmodels, where inputs ( X) are ﬁrst projected onto a low-\ndimensional object (H) and then reconstructed to produce\noutputs. The difference is that the goal of these methods is\nreconstruction whereas ISAB aims to obtain good features\nfor the ﬁnal task. We expect the learned inducing points\nto encode some global structure which helps explain the\ninputs X. For example, in the amortized clustering problem\non a 2D plane, the inducing points could be appropriately\ndistributed points on the 2D plane so that the encoder can\ncompare elements in the query dataset indirectly through\ntheir proximity to these grid points.\nNote that in (9) and (10), attention was computed between\na set of size m and a set of size n. Therefore, the time\ncomplexity of ISABm(X; λ) is O(nm) where mis a (typ-\nically small) hyperparameter — an improvement over the\nquadratic complexity of the SAB. We also emphasize that\nboth of our set operations (SAB and ISAB) are permutation\nequivariant (deﬁnition in Section 2.1):\nProperty 1. Both SAB(X) and ISABm(X) are permuta-\ntion equivariant.\n3.2. Pooling by Multihead Attention\nA common aggregation scheme in permutation invariant\nnetworks is a dimension-wise average or maximum of the\nfeature vectors (cf. Section 1). We instead propose to aggre-\ngate features by applying multihead attention on a learnable\nset of kseed vectors S ∈Rk×d. Let Z ∈Rn×d be the set of\nfeatures constructed from an encoder. Pooling by Multihead\nAttention (PMA) with kseed vectors is deﬁned as\nPMAk(Z) = MAB(S,rFF(Z)). (11)\nNote that the output of PMAk is a set of kitems. We use\none seed vector (k= 1) in most cases, but for problems such\nas amortized clustering which requires kcorrelated outputs,\nthe natural thing to do is to use kseed vectors. To further\nmodel the interactions among the k outputs, we apply an\nSAB afterwards:\nH = SAB(PMAk(Z)). (12)\nWe later empirically show that such self-attention after pool-\ning helps in modeling explaining-away (e.g., among clusters\nin an amortized clustering problem).\nIntuitively, feature aggregation using attention should be\nbeneﬁcial because the inﬂuence of each instance on the\ntarget is not necessarily equal. For example, consider a\nproblem where the target value is the maximum value of a\nset of real numbers. Since the target can be recovered using\nonly a single instance (the largest), ﬁnding and attending to\nthat instance during aggregation will be advantageous.\n3.3. Overall Architecture\nUsing the ingredients explained above, we describe how we\nwould construct a set transformer consists of an encoder and\na decoder. The encoder Encoder : X ↦→Z ∈Rn×d is a\nstack of SABs or ISABs, for example:\nEncoder(X) = SAB(SAB(X)) (13)\nEncoder(X) = ISABm(ISABm(X)). (14)\nWe point out again that the time complexity for ℓ stacks\nof SABs and ISABs are O(ℓn2) and O(ℓnm), respectively.\nThis can result in much lower processing times when using\nISAB (as compared to SAB), while still maintaining high\nrepresentational power. After the encoder transforms data\nX ∈Rn×dx into features Z ∈Rn×d, the decoder aggre-\ngates them into a single or a set of vectors which is fed into\na feed-forward network to get ﬁnal outputs. Note that PMA\nwith k >1 seed vectors should be followed by SABs to\nmodel the correlation between koutputs.\nDecoder(Z; λ) = rFF(SAB(PMAk(Z))) ∈Rk×d (15)\nwhere PMAk(Z) = MAB(S,rFF(Z)) ∈Rk×d, (16)\n3.4. Analysis\nSince the blocks used to construct the encoder (i.e., SAB,\nISAB) are permutation equivariant, the mapping of the en-\ncoder X →Zis permutation equivariant as well. Combined\nwith the fact that the PMA in the decoder is a permutation\ninvariant transformation, we have the following:\nProposition 1. The Set Transformer is permutation invari-\nant.\nBeing able to approximate any function is a desirable prop-\nerty, especially for black-box models such as deep neural\nnetworks. Building on previous results about the universal\napproximation of permutation invariant functions, we prove\nthe universality of Set Transformers:\nProposition 2. The Set Transformer is a universal approxi-\nmator of permutation invariant functions.\nProof. See supplementary material.\n4. Related Works\nPooling architectures for permutation invariant map-\npings Pooling architectures for sets have been used in\nvarious problems such as 3D shape recognition (Shi et al.,\n2015; Su et al., 2015), discovering causality (Lopez-Paz\net al., 2017), learning the statistics of a set (Edwards &\nStorkey, 2017), few-shot image classiﬁcation (Snell et al.,\n2017), and conditional regression and classiﬁcation (Gar-\nnelo et al., 2018). Zaheer et al. (2017) discuss the structure\nSet Transformer\nin general and provides a partial proof of the universality\nof the pooling architecture, and Wagstaff et al. (2019) fur-\nther discuss the limitation of pooling architectures. Bloem-\nReddy & Teh (2019) provides a link between probabilistic\nexchangeability and pooling architectures.\nAttention-based approaches for sets Several recent\nworks have highlighted the competency of attention mecha-\nnisms in modeling sets. Vinyals et al. (2016) pool elements\nin a set by a weighted average with weights computed using\nan attention mechanism. Yang et al. (2018) propose AttSets\nfor multi-view 3D reconstruction, where dot-product atten-\ntion is applied to compute the weights used to pool the\nencoded features via weighted sums. Similarly, Ilse et al.\n(2018) use attention-based weighted sum-pooling for multi-\nple instance learning. Compared to these approaches, ours\nuse multihead attention in aggregation, and more impor-\ntantly, we propose to apply self-attention after pooling to\nmodel correlation among multiple outputs. PMA withk= 1\nseed vector and single-head attention roughly corresponds\nto these previous approaches. Although not permutation\ninvariant, Mishra et al. (2018) has attention as one of its\ncore components to meta-learn to solve various tasks using\nsequences of inputs. Kim et al. (2019) proposed attention-\nbased conditional regression, where self-attention is applied\nto the query sets.\nModeling interactions between elements in sets An im-\nportant reason to use the Transformer is to explicitly model\nhigher-order interactions among the elements in a set. San-\ntoro et al. (2017) propose the relational network, a simple\narchitecture that sum-pools all pairwise interactions of el-\nements in a given set, but not higher-order interactions.\nSimilarly to our work, Ma et al. (2018) use the Transformer\nto model interactions between the objects in a video. They\nuse mean-pooling to obtain aggregated features which they\nfed into an LSTM.\nInducing point methods The idea of letting trainable vec-\ntors I directly interact with data points is loosely based on\nthe inducing point methods used in sparse Gaussian pro-\ncesses (Snelson & Ghahramani, 2005) and the Nystr ¨om\nmethod for matrix decomposition (Fowlkes et al., 2004). m\ntrainable inducing points can also be seen as mindependent\nmemory cells accessed with an attention mechanism. The\ndifferential neural dictionary (Pritzel et al., 2017) stores pre-\nvious experience as key-value pairs and uses this to process\nqueries. One can view the ISAB is the inversion of this idea,\nwhere queries Iare stored and the input features are used as\nkey-value pairs.\n5. Experiments\nTo evaluate the Set Transformer, we apply it to a suite of\ntasks involving sets of data points. We repeat all experi-\nTable 1.Mean absolute errors on the max regression task.\nArchitecture MAE\nrFF + Pooling (mean) 2.133 ±0.190\nrFF + Pooling (sum) 1.902 ±0.137\nrFF + Pooling (max) 0.1355 ±0.0074\nSAB + PMA (ours) 0.2085 ±0.0127\nments ﬁve times and report performance metrics evaluated\non corresponding test datasets. Along with baselines, we\ncompared various architectures arising from the combina-\ntion of the choices of having attention in encoders and de-\ncoders. Unless speciﬁed otherwise, “simple pooling” means\naverage pooling.\n• rFF + Pooling (Zaheer et al., 2017): rFF layers in\nencoder and simple pooling + rFF layers in decoder.\n• rFFp-mean/rFFp-max + Pooling (Zaheer et al., 2017):\nrFF layers with permutation equivariant variants in\nencoder (Zaheer et al., 2017, (4)) and simple pooling +\nrFF layers in decoder.\n• rFF + Dotprod (Yang et al., 2018; Ilse et al., 2018):\nrFF layers in encoder and dot product attention based\nweighted sum pooling + rFF layers in decoder.\n• SAB (ISAB) + Pooling (ours): Stack of SABs (ISABs)\nin encoder and simple pooling + rFF layers in decoder.\n• rFF + PMA (ours): rFF layers in encoder and PMA\n(followed by stack of SABs) in decoder.\n• SAB (ISAB) + PMA (ours): Stack of SABs (ISABs)\nin encoder and PMA (followed by stack of SABs) in\ndecoder.\n5.1. Toy Problem: Maximum Value Regression\nTo demonstrate the advantage of attention-based set aggre-\ngation over simple pooling operations, we consider a toy\nproblem: regression to the maximum value of a given set.\nGiven a set of real numbers {x1,...,x n}, the goal is to\nreturn max(x1,··· ,xn). Given prediction p, we use the\nmean absolute error |p−max(x1,··· ,xn)|as the loss func-\ntion. We constructed simple pooling architectures with three\ndifferent pooling operations: max, mean, and sum. We\nreport loss values after training in Table 1. Mean- and sum-\npooling architectures result in a high mean absolute error\n(MAE). The model with max-pooling can predict the output\nperfectly by learning its encoder to be an identity function,\nand thus achieves the highest performance. Notably, the\nSet Transformer achieves performance comparable to the\nmax-pooling model, which underlines the importance of\nadditional ﬂexibility granted by attention mechanisms — it\ncan learn to ﬁnd and attend to the maximum element.\nSet Transformer\nFigure 2.Counting unique characters: this is a randomly sampled\nset of 20 images from the Omniglot dataset. There are 14 different\ncharacters inside this set.\nTable 2.Accuracy on the unique character counting task.\nArchitecture Accuracy\nrFF + Pooling 0.4382 ±0.0072\nrFFp-mean + Pooling 0.4617 ±0.0076\nrFFp-max + Pooling 0.4359 ±0.0077\nrFF + Dotprod 0.4471 ±0.0076\nrFF + PMA (ours) 0.4572 ±0.0076\nSAB + Pooling (ours) 0.5659 ±0.0077\nSAB + PMA (ours) 0.6037 ±0.0075\n5.2. Counting Unique Characters\nIn order to test the ability of modelling interactions between\nobjects in a set, we introduce a new task of counting unique\nelements in an input set. We use the Omniglot (Lake et al.,\n2015) dataset, which consists of 1,623 different handwritten\ncharacters from various alphabets, where each character is\nrepresented by 20 different images.\nWe split all characters (and corresponding images) into train,\nvalidation, and test sets and only train using images from the\ntrain character classes. We generate input sets by sampling\nbetween 6 and 10 images and we train the model to predict\nthe number of different characters inside the set. We used\na Poisson regression model to predict this number, with\nthe rate λ given as the output of a neural network. We\nmaximized the log likelihood of this model using stochastic\ngradient ascent.\nWe evaluated model performance using sets of images sam-\npled from the test set of characters. Table 2 reports accuracy,\nmeasured as the frequency at which the mode of the Poisson\ndistribution chosen by the network is equal to the number\nof characters inside the input set.\nWe additionally performed experiments to see how the num-\nber of incuding points affects performance. We trained\nISABn+ PMAon this task while varying the number of in-\nducing points (n). Accuracies are shown in Figure 3, where\nother architectures are shown as horizontal lines for compar-\nison. Note ﬁrst that even the accuracy of ISAB1 + PMA\nsurpasses that of both rFF+Pooling and rFF+PMA , and\nthat performance tends to increase as we increase n.\n1 2 3 4 5 6 7 8 9 10 11\nNumber of Inducing Points (n)\n0.45\n0.50\n0.55\n0.60\nAccuracy\n ISAB(n)+PMA\nSAB+PMA\nSAB + Pooling\nrFF + PMA\nrFF + Pooling\nFigure 3. Accuracy of ISABn + PMAon the unique character\ncounting task. x-axis is nand y-axis is accuracy.\n5.3. Amortized Clustering with Mixture of Gaussians\nWe applied the set-input networks to the task of maxi-\nmum likelihood of mixture of Gaussians (MoGs). The\nlog-likelihood of a dataset X = {x1,...,x n}generated\nfrom an MoG with kcomponents is\nlog p(X; θ) =\nn∑\ni=1\nlog\nk∑\nj=1\nπjN(xi; µj,diag(σ2\nj)). (17)\nThe goal is to learn the optimal parameters θ∗(X) =\narg maxθlog p(X; θ). The typical approach to this prob-\nlem is to run an iterative algorithm such as Expectation-\nMaximisation (EM) until convergence. Instead, we aim\nto learn a generic meta-algorithm that directly maps the\ninput set X to θ∗(X). One can also view this as amor-\ntized maximum likelihood learning. Speciﬁcally, given a\ndataset X, we train a neural network to output parameters\nf(X; λ) ={π(X),{µj(X),σj(X)}k\nj=1}which maximize\nEX\n\n\n|X|∑\ni=1\nlog\nk∑\nj=1\nπj(X)N(xi; µj(X),diag(σ2\nj (X)))\n\n. (18)\nWe structured f(·; λ) as a set-input neural network and\nlearned its parameters λusing stochastic gradient ascent,\nwhere we approximate gradients using minibatches of\ndatasets.\nWe tested Set Transformers along with other set-input net-\nworks on two datasets. We used four seed vectors for the\nPMA (S ∈R4×d) so that each seed vector generates the\nparameters of a cluster.\nSynthetic 2D mixtures of Gaussians : Each dataset con-\ntains n ∈[100,500] points on a 2D plane, each sampled\nfrom one of four Gaussians.\nCIFAR-100: Each dataset contains n∈[100,500] images\nsampled from four random classes in the CIFAR-100 dataset.\nEach image is represented by a 512-dim vector obtained\nfrom a pretrained VGG network (Simonyan & Zisserman,\n2014).\nSet Transformer\nTable 3.Meta clustering results. The number inside parenthesis indicates the number of inducing points used in ISABs of encoders. We\nshow average likelihood per data for the synthetic dataset and the adjusted rand index (ARI) for the CIFAR-100 experiment. LL1/data,\nARI1 are the evaluation metrics after a single EM update step. The oracle for the synthetic dataset is the log likelihood of the actual\nparameters used to generate the set, and the CIFAR oracle was computed by running EM until convergence.\nSynthetic CIFAR-100\nArchitecture LL0/data LL1/data ARI0 ARI1\nOracle -1.4726 0.9150\nrFF + Pooling -2.0006 ±0.0123 -1.6186 ±0.0042 0.5593 ±0.0149 0.5693 ±0.0171\nrFFp-mean + Pooling -1.7606 ±0.0213 -1.5191 ±0.0026 0.5673 ±0.0053 0.5798 ±0.0058\nrFFp-max + Pooling -1.7692 ±0.0130 -1.5103 ±0.0035 0.5369 ±0.0154 0.5536 ±0.0186\nrFF + Dotprod -1.8549 ±0.0128 -1.5621 ±0.0046 0.5666 ±0.0221 0.5763 ±0.0212\nSAB + Pooling (ours) -1.6772 ±0.0066 -1.5070 ±0.0115 0.5831 ±0.0341 0.5943 ±0.0337\nISAB (16) + Pooling (ours) -1.6955 ±0.0730 -1.4742 ±0.0158 0.5672 ±0.0124 0.5805 ±0.0122\nrFF + PMA (ours) -1.6680 ±0.0040 -1.5409 ±0.0037 0.7612 ±0.0237 0.7670 ±0.0231\nSAB + PMA (ours) -1.5145 ±0.0046 -1.4619 ±0.0048 0.9015 ±0.0097 0.9024 ±0.0097\nISAB (16) + PMA (ours) -1.5009 ±0.0068 -1.4530 ±0.0037 0.9210 ±0.0055 0.9223 ±0.0056\nFigure 4. Clustering results for 10 test datasets, along with centers and covariance matrices. rFF+Pooling (top-left), SAB+Pooling\n(top-right), rFF+PMA (bottom-left), Set Transformer (bottom-right). Best viewed magniﬁed in color.\nWe report the performance of the oracle along with the set-\ninput neural networks in Table 3. We additionally report\nscores of all models after a single EM update. Overall,\nthe Set Transformer found accurate parameters and even\noutperformed the oracles after a single EM update. This\nmay be due to the relatively small size of the input sets;\nsome clusters have fewer than 10 points. In this regime,\nsample statistics can differ substantially from population\nstatistics, which limits the performance of the oracle while\nthe Set Transformer can adapt accordingly. Notably, the\nSet Transformer with only 16 inducing points showed the\nbest performance, even outperforming the full Set Trans-\nformer. We believe this is due to the knowledge transfer\nand regularization via inducing points, helping the network\nto learn global structures. Our results also imply that the\nimprovement from using the PMA is more signiﬁcant than\nthat of the SAB, supporting our claim of the importance\nof attention-based decoders. We provide detailed genera-\ntive processes, network architectures, and training schemes\nalong with additional experiments with various numbers of\ninducing points in the supplementary material.\n5.4. Set Anomaly Detection\nWe evaluate our methods on the task of meta-anomaly de-\ntection within a set using the CelebA dataset. The dataset\nconsists of 202,599 images with the total of 40 attributes.\nWe randomly sample 1,000 sets of images. For every set,\nwe select two attributes at random and construct the set\nby selecting seven images containing both attributes and\none image with neither. The goal of this task is to ﬁnd the\nimage that does not belong to the set. We give a detailed\ndescription of the experimental setup in the supplementary\nmaterial. We report the area under receiver operating char-\nacteristic curve (AUROC) and area under precision-recall\ncurve (AUPR) in Table 5. Set Transformers outperformed\nall other methods by a signiﬁcant margin.\nSet Transformer\nTable 4. Test accuracy for the point cloud classiﬁcation task using 100,1000,5000 points.\nArchitecture 100 pts 1000 pts 5000 pts\nrFF + Pooling (Zaheer et al., 2017) - 0.83 ±0.01 -\nrFFp-max + Pooling (Zaheer et al., 2017) 0.82 ±0.02 0.87 ±0.01 0.90 ±0.003\nrFF + Pooling 0.7951 ±0.0166 0.8551 ±0.0142 0.8933 ±0.0156\nrFF + PMA (ours) 0.8076 ±0.0160 0.8534 ±0.0152 0.8628 ±0.0136\nISAB (16) + Pooling (ours) 0.8273 ±0.0159 0.8915 ±0.0144 0.9040 ±0.0173\nISAB (16) + PMA (ours) 0.8454 ±0.0144 0.8662 ±0.0149 0.8779 ±0.0122\nFigure 5. Sampled datasets. Each row is a dataset, consisting of\n7 normal images and 1 anomaly (red box). In each subsampled\ndataset, a normal image has two attributes (rightmost column)\nwhich anomalies do not.\nTable 5.Meta set anomaly results. Each architecture is evaluated\nusing average of test AUROC and test AUPR.\nArchitecture Test AUROC Test AUPR\nRandom guess 0.5 0.125\nrFF + Pooling 0.5643 ±0.0139 0.4126 ±0.0108\nrFFp-mean + Pooling 0.5687 ±0.0061 0.4125 ±0.0127\nrFFp-max + Pooling 0.5717 ±0.0117 0.4135 ±0.0162\nrFF + Dotprod 0.5671 ±0.0139 0.4155 ±0.0115\nSAB + Pooling (ours) 0.5757 ±0.0143 0.4189 ±0.0167\nrFF + PMA (ours) 0.5756 ±0.0130 0.4227 ±0.0127\nSAB + PMA (ours) 0.5941 ±0.0170 0.4386 ±0.0089\n5.5. Point Cloud Classiﬁcation\nWe evaluated Set Transformers on a classiﬁcation task using\nthe ModelNet40 (Chang et al., 2015) dataset1, which con-\ntains three-dimensional objects in 40 different categories.\nEach object is represented as a point cloud, which we treat\nas a set of nvectors in R3. We performed experiments with\ninput sets of size n ∈{100,1000,5000}. Because of the\nlarge set sizes, MABs are prohibitively time-consuming due\nto their O(n2) time complexity.\nTable 4 shows classiﬁcation accuracies. We point out that\nZaheer et al. (2017) used signiﬁcantly more engineering\nfor the 5000 point experiment. For this experiment only,\n1The point-cloud dataset used in this experiment was obtained\ndirectly from the authors of Zaheer et al. (2017).\nthey augmented data (scaling, rotation) and used a differ-\nent optimizer (Adamax) and learning rate schedule. Set\nTransformers were superior when given small sets, but were\noutperformed by ISAB (16) + Pooling on larger sets. First\nnote that classiﬁcation is harder when given fewer points.\nWe think Set Transformers were outperformed in the prob-\nlems with large sets because such sets already had sufﬁcient\ninformation for classiﬁcation, diminishing the need to model\ncomplex interactions among points. We point out that PMA\noutperformed simple pooling in all other experiments.\n6. Conclusion\nIn this paper, we introduced the Set Transformer, an\nattention-based set-input neural network architecture. Our\nproposed method uses attention mechanisms for both en-\ncoding and aggregating features, and we have empirically\nvalidated that both of them are necessary for modelling\ncomplicated interactions among elements of a set. We also\nproposed an inducing point method for self-attention, which\nmakes our approach scalable to large sets. We also showed\nuseful theoretical properties of our model, including the fact\nthat it is a universal approximator for permutation invariant\nfunctions. An interesting future work would be to apply\nSet Transformers to meta-learning problems. In particular,\nusing Set Transformers to meta-learn posterior inference in\nBayesian models seems like a promising line of research.\nAnother exciting extension of our work would be to model\nthe uncertainty in set functions by injecting noise variables\ninto Set Transformers in a principled way.\nAcknowledgments JL and YWT’s research leading to\nthese results has received funding from the European Re-\nsearch Council under the European Union’s Seventh Frame-\nwork Programme (FP7/2007-2013) ERC grant agreement no.\n617071. JL has also received funding from EPSRC under\ngrant EP/P026753/1. JL acknowledges support from IITP\ngrant funded by the Korea government(MSIT) (No.2017-\n0-01779, XAI) and Samsung Research Funding & Incuba-\ntion Center of Samsung Electronics under Project Number\nSRFC-IT1702-15.\nSet Transformer\nReferences\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv e-prints, arXiv:1607.06450, 2016.\nBloem-Reddy, B. and Teh, Y .-W. Probabilistic sym-\nmetry and invariant neural networks. arXiv e-prints ,\narXiv:1901.06082, 2019.\nChang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P.,\nHuang, Q., Li, Z., Savarese, S., Savva, M., Song, S.,\nSu, H., Xiao, J., Yi, L., and Yu, F. ShapeNet: An\ninformation-rich 3D model repository. arXiv e-prints,\narXiv:1512.03012, 2015.\nCharles, R. Q., Su, H., Kaichun, M., and Guibas, L. J. Point-\nNet: Deep learning on point sets for 3D classiﬁcation and\nsegmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017.\nDietterich, T. G., Lathrop Richard, H., and Lozano-P´erez, T.\nSolving the multiple instance problem with axis-parallel\nrectangles. Artiﬁcial intelligence, 89(1-2):31–71, 1997.\nEdwards, H. and Storkey, A. Towards a neural statistician. In\nProceedings of the International Conference on Learning\nRepresentations (ICLR), 2017.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Pro-\nceedings of the International Conference on Machine\nLearning (ICML), 2017.\nFowlkes, C., Belongie, S., Chung, F., and Malik, J. Spectral\ngrouping using the Nystr¨om method. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 25(2):215–\n225, 2004.\nGarnelo, M., Rosenbaum, D., Maddison, C. J., Ramalho,\nT., Saxton, D., Shanahan, M., Teh, Y . W., Rezende, D. J.,\nand Eslami, S. M. A. Conditional neural processes. In\nProceedings of the International Conference on Machine\nLearning (ICML), 2018.\nGraves, A., Mohamed, A.-r., and Hinton, G. E. Speech\nrecognition with deep recurrent neural networks. In Pro-\nceedings of the IEEE International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP), 2013.\nIlse, M., Tomczak, J. M., and Welling, M. Attention-based\ndeep multiple instance learning. In Proceedings of the\nInternational Conference on Machine Learning (ICML),\n2018.\nKim, H., Mnih, A., Schwarz, J., Garnelo, M., Eslami, A.,\nRosenbaum, D., Vinyals, O., and Teh, Y . W. Attentive\nneural processes. In Proceedings of International Confer-\nence on Learning Representations, 2019.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet\nclassiﬁcation with deep convolutional neural networks.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2012.\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B.\nHuman-level concept learning through probabilistic pro-\ngram induction. Science, 350(6266):1332–1338, 2015.\nLee, Y . and Choi, S. Gradient-based meta-learning with\nlearned layerwise metric and subspace. In Proceedings\nof the International Conference on Machine Learning\n(ICML), 2018.\nLopez-Paz, D., Nishihara, R., Chintala, S., Sch ¨olkopf, B.,\nand Bottou, L. Discovering causal signals in images. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017.\nMa, C.-Y ., Kadav, A., Melvin, I., Kira, Z., AlRegib, G., and\nPeter Graf, H. Attend and interact: higher-order object\ninteractions for video understanding. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\nMaron, O. and Lozano-P´erez, T. A framework for multiple-\ninstance learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 1998.\nMishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.\nA simple neural attentive meta-learner. In Proceedings\nof the International Conference on Machine Learning\n(ICML), 2018.\nMuandet, K., Fukumizu, K., Dinuzzo, F., and Sch ¨olkopf,\nB. Learning from distributions via support measure ma-\nchines. In Advances in Neural Information Processing\nSystems (NeurIPS), 2012.\nOliva, J., P´oczos, B., and Schneider, J. Distribution to dis-\ntribution regression. In Proceedings of the International\nConference on Machine Learning (ICML), 2013.\nPritzel, A., Uria, B., Srinivasan, S., Puigdomenech, A.,\nVinyals, O., Hassabis, D., Wierstra, D., and Blundell,\nC. Neural episodic control. In Proceedings of the In-\nternational Conference on Machine Learning (ICML) ,\n2017.\nSantoro, A., Raposo, D., Barret, D. G. T., Malinowski, M.,\nPascanu, R., and Battaglia, P. A simple neural network\nmodule for relational reasoning. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2017.\nSchmidhuber, J. Evolutionary Principles in Self-Referential\nLearning. PhD thesis, Technical University of Munich,\n1987.\nSet Transformer\nShi, B., Bai, S., Zhou, Z., and Bai, X. DeepPano:\ndeep panoramic representation for 3-D shape recogni-\ntion. IEEE Signal Processing Letters, 22(12):2339–2343,\n2015.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition. arXiv e-\nprints, arXiv:1409.1556, 2014.\nSnell, J., Swersky, K., and Zemel, R. Prototypical networks\nfor few-shot learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nSnelson, E. and Ghahramani, Z. Sparse Gaussian processes\nusing pseudo-inputs. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2005.\nSu, H., Maji, S., Kalogerakis, E., and Learned-Miller, E.\nMulti-view convolutional neural networks for 3D shape\nrecognition. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2015.\nThrun, S. and Pratt, L.Learning to Learn. Kluwer Academic\nPublishers, 1998.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nVinyals, O., Bengio, S., and Kudlur, M. Order matters:\nsequence to sequence for sets. In Proceedings of the\nInternational Conference on Learning Representations\n(ICLR), 2016.\nWagstaff, E., Fuchs, F. B., Engelcke, M., Posner, I., and\nOsborne, M. On the limitations of representing functions\non sets. arXiv:1901.09006, 2019.\nWu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X.,\nand Xiao, J. 3D ShapeNets: a deep representation for vol-\numetric shapes. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) ,\n2015.\nYang, B., Wang, S., Markham, A., and Trigoni, N. Atten-\ntional aggregation of deep feature sets for multi-view 3D\nreconstruction. arXiv e-prints, arXiv:1808.00758, 2018.\nZaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B.,\nSalakhutdinov, R. R., and Smola, A. J. Deep sets.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\nSupplementary Material for Set Transformer\nJuho Lee 1 2 Yoonho Lee3 Jungtaek Kim 4 Adam R. Kosiorek1 5 Seungjin Choi 4 Yee Whye Teh1\n1. Proofs\nLemma 1. The mean operatormean({x1,...,x n}) = 1\nn\n∑n\ni=1 xi is a special case of dot-product attention with softmax.\nProof. Let s= 0 ∈Rd and X ∈Rn×d.\nAtt(s,X,X ; softmax) = softmax\n(sX⊤\n√\nd\n)\nX = 1\nn\nn∑\ni=1\nxi\nLemma 2. The decoder of a Set Transformer, given enough nodes, can express any element-wise function of the form(1\nn\n∑n\ni=1 zp\ni\n)1\np .\nProof. We ﬁrst note that we can view the decoder as the composition of functions\nDecoder(Z) = rFF(H) (1)\nwhere H = rFF(MAB(Z,rFF(Z))) (2)\nWe focus on H in (2). Since feed-forward networks are universal function approximators at the limit of inﬁnite nodes, let\nthe feed-forward layers in front and back of the MAB encode the element-wise functions z→zp and z→z\n1\np , respectively.\nWe let h= d, so the number of heads is the same as the dimensionality of the inputs, and each head is one-dimensional. Let\nthe projection matrices in multi-head attention (WQ\nj ,WK\nj ,WV\nj ) represent projections onto the jth dimension and the output\nmatrix (WO) the identity matrix. Since the mean operator is a special case of dot-product attention, by simple composition,\nwe see that an MAB can express any dimension-wise function of the form\nMp(z1,··· ,zn) =\n(\n1\nn\nn∑\ni=1\nzp\ni\n)1\np\n. (3)\nLemma 3. A PMA, given enough nodes, can express sum pooling(∑n\ni=1 zi).\nProof. We prove this by construction.\nSet the seed sto a zero vector and let ω(·) = 1 +f(·), where f is any activation function such that f(0) = 0. The identiy,\nsigmoid, or relu functions are suitable choices for f. The output of the multihead attention is then simply a sum of the\nvalues, which is Zin this case.\nWe additionally have the following universality theorem for pooling architectures:\nTheorem 1. Models of the formrFF(sum(rFF(·))) are universal function approximators in the space of permutation\ninvariant functions.\nProof. See Appendix A of ?.\narXiv:1810.00825v3  [cs.LG]  26 May 2019\nSupplementary Material for Set Transformer\nBy Lemma 3, we know that decoder(Z) can express any function of the form rFF(sum(Z)). Using this fact along with\nTheorem 1, we can prove the universality of Set Transformers:\nProposition 1. The Set Transformer is a universal function approximator in the space of permutation invariant functions.\nProof. By setting the matrix WO to a zero matrix in every SAB and ISAB, we can ignore all pairwise interaction terms\nin the encoder. Therefore, the encoder(X) can express any instance-wise feed-forward network (Z = rFF(X)). Directly\ninvoking Theorem 1 concludes this proof.\nWhile this proof required us to ignore the pairwise interaction terms inside the SABs and ISABs to prove that Set\nTransformers are universal function approximators, our experiments indicated that self-attention in the encoder was crucial\nfor good performance.\n2. Experiment Details\nIn all implementations, we omit the feed-forward layer in the beginning of the decoder (rFF(Z)) because the end of the\nprevious block contains a feed-forward layer. All MABs (inside SAB, ISAB and PMA) use fully-connected layers with\nReLU activations for rFF layers.\nIn the architecture descriptions,FC(d,f) denotes the fully-connected layer withdunits and activation functionf. SAB(d,h)\ndenotes the SAB with dunits and hheads. ISABm(d,h) denotes the ISAB with dunits, hheads and minducing points.\nPMAk(d,h) denotes the PMA with dunits, hheads and kvectors. All MABs used in SAB and PMA uses FC layers with\nReLU activations for FF layers.\n2.1. Max Regression\nGiven a set of real numbers {x1,...,x n}, the goal of this task is to return the maximum value in the set max(x1,··· ,xn).\nWe construct training data as follows. We ﬁrst sample a dataset size nuniformly from the set of integers {1,··· ,10}. We\nthen sample real numbers xi independently from the interval [0,100]. Given the network’s predictionp, we use the actual\nmaximum value max(x1,··· ,xn) to compute the mean absolute error |p−max(x1,··· ,xn)|. We don’t explicitly consider\nsplits of train and test data, since we sample a new set {x1,...,x n}at each time step.\nTable 1.Detailed architectures used in the max regression experiments.\nEncoder Decoder\nFF SAB Pooling PMA\nFC(64, ReLU) SAB(64 , 4) mean , sum, max PMA 1(64, 4)\nFC(64, ReLU) SAB(64 , 4) FC(64 , ReLU) FC(1 , −)\nFC(64, ReLU) FC(1 , −)\nFC(64, −)\nWe show the detailed architectures used for the experiments in Table 1. We trained all networks using the Adam optimizer (?)\nwith a constant learning rate of 10−3 and a batch size of 128 for 20,000 batches, after which loss converged for all\narchitectures.\n2.2. Counting Unique Characters\nThe task generation procedure is as follows. We ﬁrst sample a set size nuniformly from the set of integers {6,..., 10}.\nWe then sample the number of characters cuniformly from {1,...,n }. We sample ccharacters from the training set of\ncharacters, and randomly sample instances of each character so that the total number of instances sums to nand each set of\ncharacters has at least one instance in the resulting set.\nWe show the detailed architectures used for the experiments in Table 3. For both architectures, the resulting1-dimensional\noutput is passed through a softplus activation to produce the Poisson parameter γ. The role of softplus is to ensure that γis\nalways positive.\nSupplementary Material for Set Transformer\nTable 2.Detailed results for the unique character counting experiment.\nArchitecture Accuracy\nrFF + Pooling 0.4366 ±0.0071\nrFF + PMA 0.4617 ±0.0073\nrFFp-mean + Pooling 0.4617 ±0.0076\nrFFp-max + Pooling 0.4359 ±0.0077\nrFF + Dotprod 0.4471 ±0.0076\nSAB + Pooling 0.5659 ±0.0067\nSAB + Dotprod 0.5888 ±0.0072\nSAB + PMA (1) 0.6037 ±0.0072\nSAB + PMA (2) 0.5806 ±0.0075\nSAB + PMA (4) 0.5945 ±0.0072\nSAB + PMA (8) 0.6001 ±0.0078\nTable 3.Detailed architectures used in the unique character counting experiments.\nEncoder Decoder\nrFF SAB Pooling PMA\nConv(64, 3, 2, BN, ReLU) Conv(64 , 3, 2, BN, ReLU) mean PMA 1(8, 8)\nConv(64, 3, 2, BN, ReLU) Conv(64 , 3, 2, BN, ReLU) FC(64 , ReLU) FC(1 , softplus)\nConv(64, 3, 2, BN, ReLU) Conv(64 , 3, 2, BN, ReLU) FC(1 , softplus)\nConv(64, 3, 2, BN, ReLU) Conv(64 , 3, 2, BN, ReLU)\nFC(64, ReLU) SAB(64 , 4)\nFC(64, ReLU) SAB(64 , 4)\nFC(64, ReLU)\nFC(64, −)\nThe loss function we optimize, as previously mentioned, is the log likelihood log p(x|γ) =xlog(γ) −γ−log(x!). We\nchose this loss function over mean squared error or mean absolute error because it seemed like the more logical choice when\ntrying to make a real number match a target integer. Early experiments showed that directly optimizing for mean absolute\nerror had roughly the same result as optimizing γin this way and measuring |γ−x|. We train using the Adam optimizer\nwith a constant learning rate of 10−4 for 200,000 batches each with batch size 32.\n2.3. Solving maximum likelihood problems for mixture of Gaussians\n2.3.1. D ETAILS FOR 2D SYNTHETIC MIXTURES OF GAUSSIANS EXPERIMENT\nWe generated the datasets according to the following generative process.\n1. Generate the number of data points, n∼Unif(100,500).\n2. Generate kcenters.\nµj,d ∼Unif(−4,4), j = 1,..., 4, d= 1,2. (4)\n3. Generate cluster labels.\nπ∼Dir([1,1]⊤), z i ∼Categorical(π), i= 1,...,n. (5)\n4. Generate data from spherical Gaussian.\nxi ∼N(µzi ,(0.3)2I). (6)\nSupplementary Material for Set Transformer\nTable 4 summarizes the architectures used for the experiments. For all architectures, at each training step, we generate 10\nrandom datasets according to the above generative process, and updated the parameters via Adam optimizer with initial\nlearning rate 10−3. We trained all the algorithms for 50k steps, and decayed the learning rate to 10−4 after 35k steps.\nTable 5 summarizes the detailed results with various number of inducing points in the ISAB. Figure?? shows the actual\nclustering results based on the predicted parameters.\nTable 4.Detailed architectures used in 2D synthetic experiments.\nEncoder Decoder\nrFF SAB ISAB Pooling PMA\nFC(128, ReLU) SAB(128 , 4) ISAB m(128, 4) mean PMA 4(128, 4)\nFC(128, ReLU) SAB(128 , 4) ISAB m(128, 4) FC(128 , ReLU) SAB(128 , 4)\nFC(128, ReLU) FC(128 , ReLU) FC(4 · (1 + 2· 2), −)\nFC(128, ReLU) FC(128 , ReLU)\nFC(4 · (1 + 2· 2), −)\nTable 5.Average log-likelihood/data (LL0/data) and average log-likelihood/data after single EM iteration (LL1/data) the clustering\nexperiment. The number inside parenthesis indicates the number of inducing points used in the SABs of encoder. For all PMAs, four seed\nvectors were used.\nArchitecture LL0/data LL1/data\nOracle -1.4726\nrFF + Pooling -2.0006 ±0.0123 -1.6186 ±0.0042\nrFFp-mean + Pooling -1.7606 ±0.0213 -1.5191 ±0.0026\nrFFp-max + Pooling -1.7692 ±0.0130 -1.5103 ±0.0035\nrFF+Dotprod -1.8549 ±0.0128 -1.5621 ±0.0046\nSAB + Pooling -1.6772 ±0.0066 -1.5070 ±0.0115\nISAB (16) + Pooling -1.6955 ±0.0730 -1.4742 ±0.0158\nISAB (32) + Pooling -1.6353 ±0.0182 -1.4681 ±0.0038\nISAB (64) + Pooling -1.6349 ±0.0429 -1.4664 ±0.0080\nrFF + PMA -1.6680 ±0.0040 -1.5409 ±0.0037\nSAB + PMA -1.5145 ±0.0046 -1.4619 ±0.0048\nISAB (16) + PMA -1.5009 ±0.0068 -1.4530 ±0.0037\nISAB (32) + PMA -1.4963 ±0.0064 -1.4524 ±0.0044\nISAB (64) + PMA -1.5042 ±0.0158 -1.4535 ±0.0053\n2.3.2. 2D S YNTHETIC MIXTURES OF GAUSSIANS EXPERIMENT ON LARGE -SCALE DATA\nTo show the scalability of the set transformer, we conducted additional experiments on large-scale 2D synthetic clustering\ndataset. We generated the synthetic data as before, except that we sample the number of data points nUnif(1000,5000)\nand set k= 6. We report the clustering accuracy of a subset of comparing methods in Table 6. The set transformer with only\n32 inducing points works extremely well, demonstrating its scalability and efﬁciency.\n2.3.3. D ETAILS FOR CIFAR-100 AMORTIZED CLUTERING EXPERIMENT\nWe pretrained VGG net (?) with CIFAR-100, and obtained the test accuracy 68.54%. Then, we extracted feature vectors of\n50k training images of CIFAR-100 from the 512-dimensional hidden layers of the VGG net (the layer just before the last\nlayer). Given these feature vectors, the generative process of datasets is as follows.\n1. Generate the number of data points, n∼Unif(100,500).\n2. Uniformly sample four classes among 100 classes.\n3. Uniformly sample ndata points among four sampled classes.\nSupplementary Material for Set Transformer\nTable 6.Average log-likelihood/data (LL0/data) and average log-likelihood/data after single EM iteration (LL1/data) the clustering\nexperiment on large-scale data. The number inside parenthesis indicates the number of inducing points used in the SABs of encoder. For\nall PMAs, six seed vectors were used.\nArchitecture LL0/data LL1/data\nOracle -1.8202\nrFF + Pooling -2.5195 ±0.0105 -2.0709 ±0.0062\nrFFp-mean + Pooling -2.3126 ±0.0154 -1.9749 ±0.0062\nrFF + PMA (6) -2.0515 ±0.0067 -1.9424 ±0.0047\nSAB (32) + PMA (6) -1.8928 ±0.0076 -1.8549 ±0.0024\nTable 7.Detailed architectures used in CIFAR-100 meta clustering experiments.\nEncoder Decoder\nrFF SAB ISAB rFF PMA\nFC(256, ReLU) SAB(256 , 4) ISAB m(256, 4) mean PMA 4(128, 4)\nFC(256, ReLU) SAB(256 , 4) ISAB m(256, 4) FC(256 , ReLU) SAB(256 , 4)\nFC(256, ReLU) SAB(256 , 4) ISAB m(256, 4) FC(256 , ReLU) SAB(256 , 4)\nFC(256, ReLU) FC(256 , ReLU)) FC(4 · (1 + 2· 512), −)\nFC(256, ReLU) FC(256 , ReLU)\nFC(256, −) FC(256 , ReLU)\nFC(4 · (1 + 2· 512), −)\nTable 7 summarizes the architectures used for the experiments. For all architectures, at each training step, we generate 10\nrandom datasets according to the above generative process, and updated the parameters via Adam optimizer with initial\nlearning rate 10−4. We trained all the algorithms for 50k steps, and decayed the learning rate to 10−5 after 35k steps.\nTable 8 summarizes the detailed results with various number of inducing points in the ISAB.\n2.4. Set Anomaly Detection\nTable 9 describes the architecture for meta set anomaly experiments. We trained all models via Adam optimizer with\nlearning rate 10−4 and exponential decay of learning rate for 1,000 iterations. 1,000 datasets subsampled from CelebA\ndataset (see Figure ??) are used to train and test all the methods. We split 800 training datasets and 200 test datasets for the\nsubsampled datasets.\n2.5. Point Cloud Classiﬁcation\nWe used the ModelNet40 dataset for our point cloud classiﬁcation experiments. This dataset consists of a three-dimensional\nrepresentation of 9,843 training and 2,468 test data which each belong to one of 40 object classes. As input to our\narchitectures, we produce point clouds with n = 100,1000,5000 points each (each point is represented by (x,y,z )\ncoordinates). For generalization, we randomly rotate and scale each set during training.\nWe show results our architectures in Table 10 and additional experiments which usedn= 100,5000 points in Table ??. We\ntrained using the Adam optimizer with an initial learning rate of 10−3 which we decayed by a factor of 0.3 every 20,000\nsteps. For the experiment with 5,000 points (Table ??), we increased the dimension of the attention blocks (ISAB16(512,4)\ninstead of ISAB16(128,4)) and also decayed the weights by a factor of 10−7. We also only used one ISAB block in the\nencoder because using two lead to overﬁtting in this setting.\n3. Additional Experiments\n3.1. Runtime of SAB and ISAB\nWe measured the runtime of SAB and ISAB on a simple benchmark (Figure 1). We used a single GPU (Tesla P40) for this\nexperiment. The input data was a constant (zero) tensor of nthree-dimensional vectors. We report the number of seconds it\nSupplementary Material for Set Transformer\nTable 8.Average clustering accuracies measured by Adjusted Rand Index (ARI) for CIFAR100 clustering experiments. The number\ninside parenthesis indicates the number of inducing points used in the SABs of encoder. For all PMAs, four seed vectors were used.\nArchitecture ARI0 ARI1\nOracle 0.9151\nrFF + Pooling 0.5593 ±0.0149 0.5693 ±0.0171\nrFFp-mean + Pooling 0.5673 ±0.0053 0.5798 ±0.0058\nrFFp-max + Pooling 0.5369 ±0.0154 0.5536 ±0.0186\nrFF+Dotprod 0.5666 ±0.0221 0.5763 ±0.0212\nSAB + Pooling 0.5831 ±0.0341 0.5943 ±0.0337\nISAB (16) + Pooling 0.5672 ±0.0124 0.5805 ±0.0122\nISAB (32) + Pooling 0.5587 ±0.0104 0.5700 ±0.0134\nISAB (64) + Pooling 0.5586 ±0.0205 0.5708 ±0.0183\nrFF + PMA 0.7612 ±0.0237 0.7670 ±0.0231\nSAB + PMA 0.9015 ±0.0097 0.9024 ±0.0097\nISAB (16) + PMA 0.9210 ±0.0055 0.9223 ±0.0056\nISAB (32) + PMA 0.9103 ±0.0061 0.9119 ±0.0052\nISAB (64) + PMA 0.9141 ±0.0040 0.9153 ±0.0041\nTable 9.Detailed architectures used in CelebA meta set anomaly experiments. Conv(d, k, s, r, f) is a convolutional layer with d output\nchannels, k kernel size, s stride size, r regularization method, and activation function f. If d is a list, each element in the list is distributed.\nFC(d, f, r) denotes a fully-connected layer with d units, activation function f and r regularization method. If d is a list, each element in\nthe list is distributed. SAB(d, h) denotes the SAB with d units and h heads. PMA(d, h, nseed) denotes the PMA with d units, h heads\nand nseed vectors. All MABs used in SAB and PMA uses FC layers with ReLU activations for rFF layers.\nEncoder Decoder\nrFF SAB Pooling PMA\nConv([32, 64, 128], 3, 2, Dropout, ReLU) mean PMA 4(128, 4)\nFC([1024, 512, 256], −, Dropout) FC(128 , ReLU, −) SAB(128 , 4)\nFC(256, −, −) FC(128 , ReLU, −) FC(256 · 8, −, −)\nFC([128, 128, 128], ReLU, −) SAB(128 , 4) FC(128 , ReLU, −)\nFC([128, 128, 128], ReLU, −) SAB(128 , 4) FC(256 · 8, −, −)\nFC(128, ReLU, −) SAB(128 , 4)\nFC(128, −, −) SAB(128 , 4)\ntook to process 10,000 sets of each size. The maximum set size we report for SAB is 2,000 because the computation graph\nof bigger sets could not ﬁt on our GPU. The speciﬁc attention blocks used are ISAB4(64,8) and SAB(64,8).\nSupplementary Material for Set Transformer\nTable 10.Detailed architectures used in the point cloud classiﬁcation experiments.\nEncoder Decoder\nrFF ISAB Pooling PMA\nFC(256, ReLU) ISAB(256 , 4) max Dropout(0 .5)\nFC(256, ReLU) ISAB(256 , 4) Dropout(0 .5) PMA 1(256, 4)\nFC(256, ReLU) FC(256 , ReLU) Dropout(0 .5)\nFC(256, −) Dropout(0 .5) FC(40 , −)\nFC(40, −)\nFigure 1. Runtime of a single SAB/ISAB block on dummy data. x axis is the size of the input set and y axis is time (seconds). Note that\nthe x-axis is log-scale.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7077537775039673
    },
    {
      "name": "Transformer",
      "score": 0.6336965560913086
    },
    {
      "name": "Quadratic equation",
      "score": 0.5663480758666992
    },
    {
      "name": "Invariant (physics)",
      "score": 0.5617039203643799
    },
    {
      "name": "Permutation (music)",
      "score": 0.5429202914237976
    },
    {
      "name": "Computation",
      "score": 0.5210111141204834
    },
    {
      "name": "Encoder",
      "score": 0.4939449429512024
    },
    {
      "name": "Artificial neural network",
      "score": 0.48090195655822754
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4785369336605072
    },
    {
      "name": "Embedding",
      "score": 0.4704536199569702
    },
    {
      "name": "Algorithm",
      "score": 0.43877533078193665
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4304419159889221
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4149722456932068
    },
    {
      "name": "Machine learning",
      "score": 0.3686698079109192
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3311731815338135
    },
    {
      "name": "Mathematics",
      "score": 0.1751435399055481
    },
    {
      "name": "Mathematical physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}