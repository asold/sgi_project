{
  "title": "CMTNet: a hybrid CNN-transformer network for UAV-based hyperspectral crop classification in precision agriculture",
  "url": "https://openalex.org/W4409320928",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5112909675",
      "name": "Guo Xi-hong",
      "affiliations": [
        "Dingxi City People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5101789762",
      "name": "Quan Feng",
      "affiliations": [
        "Gansu Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A5101217329",
      "name": "Faxu Guo",
      "affiliations": [
        "Gansu Agricultural University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2983376237",
    "https://openalex.org/W2793272303",
    "https://openalex.org/W4387803853",
    "https://openalex.org/W4366310361",
    "https://openalex.org/W4281869972",
    "https://openalex.org/W3206465089",
    "https://openalex.org/W4387829157",
    "https://openalex.org/W4287510400",
    "https://openalex.org/W4211019776",
    "https://openalex.org/W2127199143",
    "https://openalex.org/W2346557146",
    "https://openalex.org/W2998198695",
    "https://openalex.org/W4312469371",
    "https://openalex.org/W2090424610",
    "https://openalex.org/W2600746131",
    "https://openalex.org/W3012135036",
    "https://openalex.org/W2991616716",
    "https://openalex.org/W2765739551",
    "https://openalex.org/W3095012029",
    "https://openalex.org/W4308067883",
    "https://openalex.org/W3031868989",
    "https://openalex.org/W4224211678",
    "https://openalex.org/W4318149008",
    "https://openalex.org/W4289654392",
    "https://openalex.org/W4362519158",
    "https://openalex.org/W6842430276",
    "https://openalex.org/W4367598041",
    "https://openalex.org/W3209859545",
    "https://openalex.org/W6803105104",
    "https://openalex.org/W1493160505",
    "https://openalex.org/W4387968135",
    "https://openalex.org/W4402769086",
    "https://openalex.org/W4404914914",
    "https://openalex.org/W3017845037",
    "https://openalex.org/W4402351971",
    "https://openalex.org/W2500751094",
    "https://openalex.org/W1950365613",
    "https://openalex.org/W2992919850",
    "https://openalex.org/W2267317359",
    "https://openalex.org/W2809113079",
    "https://openalex.org/W2572303978",
    "https://openalex.org/W4211129314",
    "https://openalex.org/W2614326984",
    "https://openalex.org/W2519653196",
    "https://openalex.org/W3116252251",
    "https://openalex.org/W3213181250",
    "https://openalex.org/W4402305045",
    "https://openalex.org/W2950266692",
    "https://openalex.org/W3043183554",
    "https://openalex.org/W3114720220",
    "https://openalex.org/W4407590516",
    "https://openalex.org/W4400375283",
    "https://openalex.org/W4409366412",
    "https://openalex.org/W6736210250",
    "https://openalex.org/W3180637455",
    "https://openalex.org/W4285187901",
    "https://openalex.org/W2971432438",
    "https://openalex.org/W3171853541",
    "https://openalex.org/W3202179271",
    "https://openalex.org/W4312846227",
    "https://openalex.org/W4283662170",
    "https://openalex.org/W4292553536",
    "https://openalex.org/W2405365025",
    "https://openalex.org/W2036195817",
    "https://openalex.org/W4313229413",
    "https://openalex.org/W2904698365",
    "https://openalex.org/W3049655825",
    "https://openalex.org/W4286008192",
    "https://openalex.org/W3134136658",
    "https://openalex.org/W4402041082"
  ],
  "abstract": null,
  "full_text": "CMTNet: a hybrid CNN-transformer \nnetwork for UAV-based \nhyperspectral crop classification in \nprecision agriculture\nXihong Guo1, Quan Feng2 & Faxu Guo2\nHyperspectral imaging acquired from unmanned aerial vehicles (UAVs) offers detailed spectral and \nspatial data that holds transformative potential for precision agriculture applications, such as crop \nclassification, health monitoring, and yield estimation. However, traditional methods struggle to \neffectively capture both local and global features, particularly in complex agricultural environments \nwith diverse crop types, varying growth stages, and imbalanced data distributions. To address these \nchallenges, we propose CMTNet, an innovative deep learning framework that integrates convolutional \nneural networks (CNNs) and Transformers for hyperspectral crop classification. The model combines \na spectral-spatial feature extraction module to capture shallow features, a dual-branch architecture \nthat extracts both local and global features simultaneously, and a multi-output constraint module to \nenhance classification accuracy through cross-constraints among multiple feature levels. Extensive \nexperiments were conducted on three UAV-acquired datasets: WHU-Hi-LongKou, WHU-Hi-HanChuan, \nand WHU-Hi-HongHu. The experimental results demonstrate that CMTNet achieved overall accuracy \n(OA) values of 99.58%, 97.29%, and 98.31% on these three datasets, surpassing the current state-of-\nthe-art method (CTMixer) by 0.19% (LongKou), 1.75% (HanChuan), and 2.52% (HongHu) in OA values, \nrespectively. These findings indicate its superior potential for UAV-based agricultural monitoring \nin complex environments. These results advance the precision and reliability of hyperspectral crop \nclassification, offering a valuable solution for precision agriculture challenges.\nKeywords Hyperspectral imaging, Crop classification, Multi-output feature constraints, Convolutional \nneural networks, Transformer\nAccurate identification of crop types is crucial for agricultural monitoring, crop yield estimation, growth analysis, \nand determining the spatial distribution and area of crops1. It also provides essential reference information for \nresource allocation, agricultural structure adjustment, and the formulation of economic development strategies \nin the agricultural production process. In recent years, with the continuous improvement of spectral imaging \ntechnology, hyperspectral imaging (HSI) has become a research hotspot for remote sensing data analysis2,3. HSI \nimages consist of hundreds or even thousands of spectral channels containing abundant spatial and spectral \ninformation. The high spatial resolution of HSI provides new opportunities for detecting subtle spectrial \ndifferences between crops, which is beneficial for the fine classification of crops. In addition, HSI is widely used \nin areas such as plant disease detection4, food inspection5, reidentification6, and geological exploration7.\nTraditional methods for HSI classification typically include the designed loss 8 and the designed model 9. \nIn addition, scholars have also introduced several methods for HSI spectral dimension reduction and \ninformation extraction, including principal component analysis, minimum noise fraction transformation, linear \ndiscriminant analysis, independent component analysis, and others. However, these methods only consider \nthe spectral information of HSI, ignoring the spatial correlation between pixels in the spatial dimension. This \nignores the spatial features contained in the HSI data and ignored rich spatial contextual information, leading \nto variability in the spectral features of target objects, thus affecting classification performance. To utilize spatial \ninformation in the images, scholars have studied various mathematical morphology operators suitable for HSI \nto extract spatial features from the images, including morphological profile features, extended morphological \nprofile features, extended multi-attribute profile features (EMAP), and extinction profile features10,11. Although \n1Dingxi Sanniu Agricultural Machinery Manufacturing Co., Ltd., Dingxi 743000, China. 2College of Mechanical and \nElectrical Engineering, Gansu Agriculture University, Lanzhou 730070, China. email: guofax@gsau.edu.cn\nOPEN\nScientific Reports |        (2025) 15:12383 1| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports\n\nhyperspectral image classification methods based on spatial features can effectively capture the spatial \ninformation such as the position, structure, and contours of target objects, they neglect the spectral dimension \ninformation of hyperspectral imaging, resulting in less than ideal classification results. The generalization and \nversatility of traditional HSI classification methods are weak and susceptible to salt and pepper noise, which \naffects classification accuracy.\nIn recent years, many deep learning-based methods have been applied to HSI classification12,13, as illustrated \nin Fig. 1. Initially, deep belief networks (DBN) 14, recurrent neural networks (RNN) 15, and one-dimensional \nconvolutional neural networks (1D-CNN) 16 was introduced into the HSI classification field. However, these \nmethods only utilize spectral information and ignore the neighborhood information in the spatial dimension, \nleading to lower classification accuracy 17. To address this issue, researchers proposed an architecture based \non two-dimensional convolutional neural networks (2D-CNN) 18. Subsequently, Xu et al. 19 combined \n1D-CNN and 2D-CNN, constructing a dual-branch network structure to extract spectral and spatial features. \nHowever, this approach extracts spectral and spatial features separately and cannot effectively utilize the 3D \nspectral-spatial features of HSI. In order to better extract spectral-spatial features, researchers developed the \n3D-CNN18 architecture and applied it to HSI classification. To overcome the limitation of CNN in capturing \nglobal information, scholars have proposed two approaches to improve CNN. One approach is to improve \nthe perceptual range directly of the convolutional kernel, including the use of dilated convolutions 20 and the \nconstruction of a multiscale feature pyramid 21; the other method is to embed an attention module 22 that can \ncapture global contextual information into the CNN structure23,24, including spectral attention, spatial attention, \nand spatiotemporal attention. However, these methods still rely on convolutional operations in the backbone \nnetwork to encode dense features, thus tending to local semantic information interaction 25. Capturing long-\nrange dependencies becomes a pivotal breakthrough in overcoming the CNN performance bottleneck.\nRecently, a visual transformer (ViT) 26 has been applied to various image processing tasks and has been \npreliminarily applied to the HSI classification field27. ViT originates from the field of natural language processing \n(NLP) and is a new type of deep neural network composed of a multi-head attention mechanism and feedforward \nneural network, which can capture long-range dependency relationships in sequences through the multi-head \nattention mechanism28,29. Compared to CNN, the self-attention mechanism of the Transformer imitates the \nsaliency detection and selective attention of biological vision, and can establish long-distance dependency \nrelationships, solving the limited receptive field problem of convolutional neural networks30. However, ViT is not \ngood at capturing local features. Given this, some scholars have begun to combine CNN and ViT to jointly capture \nlocal information, sequence features, and long-range dependency relationships. Existing HSI classification \nmethods based on CNN-Transformer hybrid architectures25 usually adopt manually specified hybrid strategies, \nsuch as using convolution to extract local features in the shallow layers and using a Transformer to extract global \nfeatures in the deep layers29,31, or directly adding the features extracted by CNN and Transformer32. Currently, \nFig. 1. HSI classification using deep learning approach.\n \nScientific Reports |        (2025) 15:12383 2| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nmany hyperspectral image classification methods based on hybrid CNN-Transformer architectures employ \nshallow convolutional layers to extract local features, while introducing Transformers at deeper layers to capture \nglobal features. Although this design successfully integrates local and global information, it presents several \nlimitations. First, the direct concatenation or sequential stacking of convolutional layers and Transformers \noften lacks flexibility in feature fusion, leading to insufficient interaction between local and global information. \nSecond, such methods typically struggle with fine-grained classification in complex scenarios, particularly in \nagricultural applications where the spectral features of different crops can be highly similar. This similarity \nmakes it challenging to differentiate between crops using global features alone. These limitations highlight the \nneed for a more flexible and integrated approach to feature extraction and fusion. In recent years, hyperspectral \nimaging has gained attention for crop classification in precision agriculture. However, traditional methods often \nface limitations due to the use of single-source data. To address this, fusion-based strategies, combining multi-\nspectral, hyperspectral, and LiDAR data, have been explored to enhance classification accuracy33. For example, \nAnandakrishnan et al.34 emphasized the effectiveness of UAV-based hyperspectral imaging in crop monitoring \nand classification.\nIn response to these limitations, this study proposes a novel hyperspectral crop classification approach, the \nConvolutional Meets Transformer Network (CMTNet). CMTNet employs a unique two-branch architecture: \na CNN branch extracts local spectral-spatial features, while a Transformer branch captures global spectral-\nspatial features. This parallel dual-branch design not only mitigates the separation between local and global \nfeatures seen in traditional methods but also excels in fine-grained classification tasks, particularly in complex \nagricultural environments. Furthermore, CMTNet enhances the efficiency of feature fusion through a multi-\noutput constraint module, with experimental results demonstrating significant improvements in classification \naccuracy and generalization capabilities.\nThe main contributions of this article are given as follows.\n• The CMTNet network proposed in this study features a unique dual-branch design that enables parallel ex -\ntraction and dynamic fusion of local and global features. In contrast to existing hybrid CNN-Transformer \nmethods, which typically stack local and global features sequentially, this design effectively addresses the \nlimitations of traditional feature fusion methods. This innovation not only enhances the model’s performance \nin fine-grained classification tasks but also improves its adaptability in complex agricultural environments.\n• A novel multi-constraint module is introduced to enhance classification accuracy by applying cross-con -\nstraints on local, global, and combined features. Unlike traditional decision-level fusion, our approach impos-\nes constraints at multiple stages of feature extraction, improving the utilization of spectral-spatial features and \nenabling better differentiation of fine-grained classes in complex agricultural scenarios.\n• The proposed CMTNet employs a dual-branch structure with CNN and Transformer components to extract \nboth local and global spectral-spatial features. Our approach introduces enhancements, such as a multi-out-\nput constraint module and optimized feature extraction, leading to significant improvements in classification \naccuracy. Experimental results on three datasets demonstrate that our method outperforms several state-of-\nthe-art networks, particularly in complex, low-resolution hyperspectral scenarios in agricultural applications.\nThe rest of the article is organized as follows: Section 2 reviews related work in hyperspectral image classification \nand UAV-based precision agriculture. Section 3 describes the architecture and key components of the proposed \nCMTNet. Section 4 presents the experimental setup and datasets used, followed by the results and discussions. \nFinally, Sect. 5 concludes the article and outlines potential future directions.\nRelated work\nCNN-based methods\nCNN is a powerful tool for analyzing HSI images because they can accurately represent the spectral and spatial \ncontextual information contained in the HSI data cube, extracting highly abstract features from the raw data \nand achieving excellent classification results 35. HSI classification tasks are categorized into three based on the \ndistinct features CNN processes. The initial category involves 1D-CNN, focusing on spectral features. The data \ninput for 1D-CNN is typically a single pixel. Li et al. 36 proposed a n feature extraction module and feature \ninteraction in the frequency domain to enhance salient features. Chen et al. 37 used a multi-layer convolutional \nnetwork to extract deep features of HSI, improving the classification results with a few training samples. Yue et \nal.38 utilized principal component analysis for HSI preprocessing before feature extraction. The second category \ninvolves 2D-CNN, focusing on spatial features. Li et al. 39 used two 2D-CNN networks to extract high spectral \nand spatial frequency information simultaneously. Zhao et al.40 proposed a 2D-CNN model that initially reduces \ndimensionality using PCA or another method, followed by data input into the model, where the data undergo \ninitial processing by 2D-CNN to extract spatial information, subsequently combined with spectral information. \nHaut et al.41 developed a novel classification model guided by active learning, employing a Bayesian approach. The \nlast category is based on spectral-spatial feature methods. In this case, there are two ways of feature processing. \nOne approach involves the use of 3D-CNN. For instance, Li et al. 42 introduced a 3D-CNN framework for the \nefficient extraction of deep spectral-spatial combined features from HSI cube data without preprocessing or \npost-processing. Another approach involves hybrid CNN, with significant research applying this method 43–45. \nXu et al. 19 integrated multi-source remote sensing data to enhance classification performance, employing \n1D-CNN and 2D-CNN for the extraction of spectral and spatial features, respectively. Diakite et al.46 suggested \na hybrid network combining 3D-CNN and 2D-CNN. However, the current CNN-based methods overlook \nimportant differences between spatial pixels and unequal contributions of spectral bands. Convolutional kernels \nwith limited receptive fields are independent of content, resulting in less accurate recognition of ground objects \nwith local contextual similarity and large-scale variations.\nScientific Reports |        (2025) 15:12383 3| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nSubsequently, the attention mechanism has been widely integrated with CNN frameworks 43,47–49 due to its \ncapability to assign varying weights to input features, enabling the model to concentrate more on crucial task-\nrelated information. Haut et al. 50 introduced a dual data-path attention module as the basic building block, \nconsidering both bottom-up and top-down visual factors to enhance the network’s feature extraction capability. \nLiu et al. 43, based on the widely used convolutional block attention module (CBAM) improved accuracy by \nchanging the way the attention module is connected. Tang et al.51 presented two attention models from spatial \nand spectral dimensions to emphasize crucial spatial regions and specific spectral bands, offering significant \ninformation for the classification task. Additionally, Roy et al.52 suggested an attention-based adaptive spectral-\nspatial kernel to enhance the residual network, capturing discriminative spectral-spatial features through end-\nto-end training for HSI classification. These attention-based methods are essentially enhanced versions of CNN, \nyet they are restricted by the inherent constraints of local convolutional kernels. These approaches emphasize \nlocal features while neglecting global information, consequently inadequately addressing the remote dependency \nbetween spectral sequences and spatial pixels.\nTransformer-based methods\nThe initial design of the Transformer was focused on sequence modeling and transduction tasks. Its remarkable \nsuccess in natural language processing has prompted researchers to explore its application in the visual domain, \nwhere it has demonstrated exceptional performance in tasks such as image classification and joint visual-\nlinguistic modeling. Recent advances in diffusion models53,54 have significantly enhanced various image synthesis \ntasks. For instance, Shen et al.55 proposed a progressive conditional diffusion model for story generation , while \ntheir later work56 introduced a customizable virtual dressing model using diffusion-based approaches, further \ndemonstrating the versatility of diffusion models in dynamic and interactive applications. In their work, Hong \net al.57 were the first to apply the ViT to HSI classification and achieved impressive results on commonly used \nhyperspectral image datasets. Additionally, He et al.58 utilized a well-trained bidirectional encoder transformer \nstructure for hyperspectral image classification. Furthermore, Qing et al. 59 introduced the self-attention-based \ntransformer network (SAT-Net) for HSI classification, employing multiple Transformer encoders to extract \nimage features. The encoder modules are directly connected using a multi-level residual structure to address the \nissues of vanishing gradients and overfitting. Tan et al.60 introduced the transformer-in-transformer module for \nend-to-end classification, building a deep network model that fully utilizes global and local information in the \ninput spectral cube. Sun et al.24 proposed the spatial and spectral attention mechanism fusion network (SSAMF) \nfor HSI classification, which incorporates channel self-attention into the Swin Transformer to better encode \nthe rich spectral-spatial information of HSI, contributing to improved classification by the network. Mei et al.61 \nproposed the Group-Aware Hierarchical Transformer (GAHT) for HSI classification, applying multi-head self-\nattention to local spatial-spectral context and hierarchically constructing the network to improve classifying \naccuracy. Zhong et al.62 developed a spectral-spatial transformer network (SSTN) to overcome the constraints of \nconvolutional kernels. Additionally, stable and efficient network architecture optimization is achieved through \nfast architecture search. It is evident that these previous studies primarily utilize Transformer to learn strong \ninteractions between comprehensive label information through multiple self-attention modules. However, they \nare troubled by slow processing speed during inference and high memory usage, and these methods have yet to \nexploit the rich spatial features of HSI fully.\nHybrid methods\nRecently, multiple endeavors have sought to integrate CNN and Transformer to build HSI classification networks \nthat leverage the strengths of both architectures. Zhang et al.63 proposed a dual-branch structure, incorporating \nboth CNN and Transformer branches to capture local-global hyperspectral features. In the multi-head self-\nattention mechanism, convolutional operations were introduced skillfully to unite CNN and Transformer, \nfurther enhancing the classification progress. Liang et al. 64 integrated multi-head self-attention mechanisms in \nthe spatial and spectral domains, applying them to context through uniform sampling and embedding 1D-CNN \nand 2D-CNN operations. Y ang et al.65 integrated CNN and Transformer sequentially and in parallel to fully utilize \nthe features of HSI. Qi et al. 31 developed the global-local spatial convolution transformer (GACT) to exploit \nlocal spatial context features and global interaction between different pixels. Additionally, through the weighted \nmulti-scale spectral-spatial feature interaction (WMSFI) module, trainable adaptive fusion of multi-scale global-\nlocal spectral-spatial information is achieved. Song et al.66 presented a dual-branch HSI classification framework \nutilizing 3D-CNN and bottleneck spatial-spectral transformer (B2ST), where both branches use a combination \nof shallow CNN and deep Transformer. Y ang et al.67 embedded CNN operations into the Transformer structure \nto capture subtle spectral differences and convey local spatial context information, then encoded spatial-spectral \nrepresentation along multiple dimensions through a novel convolution displacer. In our earlier work published \non arXiv68, we proposed a convolutional and transformer hybrid model for hyperspectral image classification, \nwhich serves as the foundation for the improvements presented in this manuscript. However, while these \nmethods have been adapted from natural image processing, challenges remain in effectively integrating CNN’s \nstrength in local context exploration with the Transformer’s capability in global spectral-spatial modeling, \nparticularly in achieving adaptive fusion of spectral-spatial features across multiple attributes and scales in low \nspatial resolution HSI.\nProposed method\nThe proposed method CMTNet’s framework is illustrated in Fig. 2. CMTNet comprises a spectral-spatial feature \nextraction module, a local-global feature extraction module, and a multi-scale output constraint module. The \nspectral-space feature extraction module initially extracts shallow features from hyperspectral images by solely \nutilizing the spectral-space information present in the images. Subsequently, a parallel local-global feature \nScientific Reports |        (2025) 15:12383 4| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nextraction module, consisting of a Transformer branch and a CNN branch, is employed to deeply extract local \nand global features from the hyperspectral images. Finally, the classification results are generated using the \nmulti-output constraint module, which calculates multi-output losses and cross-constraints on local, global, and \njoint features from various feature perspectives.\nSpectral-spatial feature extraction module\nThe structure of the spectral-spatial feature extraction module outlined in this study is illustrated in Fig. 2. This \nmodule primarily utilizes convolutional neural networks to process the segmented hyperspectral image block. It \nbegins by employing a 3D convolutional layer to extract spectral-spatial features, followed by a 2D convolutional \nlayer to capture shallow spatial features.Let the hyperspectral dataset be denoted as H ∈ Rh×w×d, with the \nspatial dimensions’ height and width represented as h and w, respectively, and the number of spectral bands \nas d. Each pixel in H comprises d spectral dimensions, with its corresponding class label vector denoted as \nV =( v1, v2,..., vn) , where n signifies the number of land cover categories in the hyperspectral scene. To \nmanage the extensive hyperspectral image data, block division is necessary during model training to accommodate \nthe computer’s computational limitations. Following partitioning, each hyperspectral image block is denoted as \nX ∈ Rm×m×d , with its dimensions specified. Each training image block sample is then inputted into the initial \n3D convolutional layer. The convolution kernel within the 3D convolution calculates new convolutional feature \nmaps by summing the dot product between the convolution of the entire spatial dimension and the kernel. The \ncalculation formula is presented in Eq. (1):\n \nvp,q,u\ni,j =\nη∑\nη=1\nh∑\nh=0\nw∑\nw=0\nc∑\nc=0\nωh,w,c\ni,j,η × v(p+h),(q+w),(u+c)\ni−1,η + bi,j (1)\nWhere η represents the feature related to the j-th convolutional feature cube of the i − 1 th layer; vp,q,u\ni,j  represents \nthe convolution output value at position (p, q, u) of the j-th convolutional feature cube of the i-th layer, with the \nconvolution kernel size of (h, w, c); ωh,w,c\ni,j,η  and bi,j  represent the weight parameters and bias at position (h, w, c) \nrelated to the η-th convolutional feature cube.\nSimilar to the 3D convolutional layer, the 2D convolutional layer operates by convolving a two-dimensional \nkernel to produce new feature maps. The calculation formula for this process is depicted in Eq. (2):\n \nvp,q\ni,j =\nη∑\nη=1\nh∑\nh=0\nw∑\nw=0\nωh,w\ni,j,η × v(p+h),(q+w)\ni−1,η + bi,j (2)\nvp,q\ni,j  represents the convolution output value at position ( p, q) of the j-th convolutional feature cube of the \ni-th layer, with the convolution kernel size of (h, w) ;(h, w); ωh,w\ni,j,η and bi,j  represent the weight parameters and \nbias at position (h, w) related to the η-th convolutional feature cube.\nThis module consists of two convolutional layers, two batch normalization layers, and two activation layers \nusing the ReLU activation function. The extraction process and calculation formulas of this module are detailed \nin Eqs. (3) and (4):\n vp,q\ni,j =Φ\n(\ng1\n(\nvp,q·u\ni,j\n))\n (3)\n y =Φ\n(\ng2\n(\nvp,q\ni,j\n))\n (4)\nFig. 2. CMTNet overall network framework. CMTNet consists of three main modules, spectral-spatial feature \nextraction module, local-global feature extraction module and multi-output constraint module.\n \nScientific Reports |        (2025) 15:12383 5| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nWhere Φ(·) represents the ReLU activation function, g1 and g2 , respectively, represent three-dimensional batch \nnormalization and two-dimensional batch normalization.\nLocal-global feature extraction module\n (1) Transformer encoder branch: As shown in Fig. 2, the Transformer encoder branch mainly consists of posi-\ntional encoding embeddings, multi-head self-attention (MHSA) (Fig. 3a), a multilayer perceptron (MLP), \nand two normalization layers. Residual connections are designed in front of MHSA and MLP . The output \nfeatures of the spectral-spatial feature extraction module are flattened and linearly mapped to a sequence \nvector T ∈ Rn×z of length s and channel dimension z. Then, a relative positional information vector \nPs ∈ Rn×z of length s is embedded into N sequence vectors as the input feature Tin of the Transformer \nencoder branch.\n Tin =\n[\nT 1; T 2; ...T N ]\n+ Ps (5)\nThe Transformer encoder’s exceptional performance can be attributed to its MHSA mechanism. MHSA \nefficiently captures the relationships between feature sequences by utilizing self-attention (SA) (see Fig. 3b). \nInitially, the Q, K, and V values derived from the convolution mapping are passed to MHSA via SA to extract \nglobal features. Within this process, Q and K are used to calculate attention scores, and the softmax function is \napplied to determine the weights of these attention scores. The formula for SA can be expressed as follows:\n \nTSA = Attention(Q ,K ,V) = soft max\n(\nQKT\n√dK\n)\nV  (6)\nWhere TSA represents the output of the SA module, and dK is the dimension of K. MHSA uses multiple sets \nof weight matrices to generate Q, K, and V, and through a consistent computation process, multiple attention \ndistributions are obtained. These distributions are then aggregated to obtain a comprehensive attention value. \nFinally, the features obtained by MHSA are passed to the MLP layer.\nTraditional 3D-CNNs are constrained by fixed receptive fields, which limit their ability to model long-range \nspectral dependencies-a critical factor in distinguishing crops with subtle spectral differences. In contrast, the \nself-attention mechanism in CMTNet’s Transformer branch dynamically captures global spectral relationships \nacross all bands and spatial positions.\n (2) CNN branch: As shown in Fig. 2, the CNN branch mainly consists of a 3 ×3 convolutional layer, two 1×1 \nconvolutional layers, and residual connections, aiming to extract local features of hyperspectral images.\nMulti-output constraint module\nWhen calculating the loss, traditional feature constraints are typically restricted to the highest-level network \noutput features, leading to suboptimal utilization of valuable spatial and spectral information. In hyperspectral \nFig. 3. Attention mechanism of the transformer. (a) Multi-head attention mechanism. (b) Self-attention \nmechanism.\n \nScientific Reports |        (2025) 15:12383 6| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nimage classification, high-level spatial and spectral semantic features play a pivotal role, and preserving \nthis valuable information across different scales during multi-scale feature fusion can significantly impact \nclassification performance. To address this limitation, we propose a multi-output constraint module in CMTNet \nthat applies feature constraints at multiple stages (as illustrated in Fig. 4), rather than solely at the final output.\nSpecifically, the multi-output constraint module independently constrains the local features (CNN branch), \nglobal features (Transformer branch), and integration features (fused high-level semantic features). Each feature \nset FL, FG, FI  is sent to the softmax activation function for classification, resulting in the probability outputs \nPL, PG, PI .The corresponding losses are computed using the categorical cross-entropy loss function:\n \nLL = −\nN∑\ni=1\nyi log (PL(i)) (7)\n \nLG = −\nN∑\ni=1\nyi log (PG(i)) (8)\n \nLI = −\nN∑\ni=1\nyi log (PI(i)) (9)\nwhere LL is the loss associated with the local features, LG is the loss associated with the global features, LI  \nis the loss associated with the integration features, yi represents the true class label, and N is the number of \nsamples. The total loss is then computed as a weighted sum of the individual losses:\n LT = λ1LL + λ2LG + λ3LI (10)\nwhere LT  is the total loss that combines the contributions from all feature branches, and λ1, λ2, λ3 are weighting \nfactors that balance the contribution of each feature type. To determine their optimal values, we conducted \na random search over the parameter space λ1,λ 2,λ 3 ∈ [0.1, 1.0], constrained by λ1 + λ2 + λ3 =1 . During \nbackpropagation, these combined losses guide the optimization of network parameters across all branches, \nensuring that local, global, and integration features are simultaneously refined. This enhances the interaction \nand fusion of local and global information, ultimately leading to a more robust feature representation.\nFig. 4. Multi-output constraint module.\n \nScientific Reports |        (2025) 15:12383 7| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nThis approach contrasts with traditional methods that constrain only the final output, allowing CMTNet \nto dynamically adjust the importance of each feature type throughout the training process. The multi-output \nconstraint module thus improves gradient flow and convergence during backpropagation, leading to higher \nclassification accuracy and better performance in fine-grained classification tasks, especially in complex \nagricultural scenarios where differentiating between spectrally similar crops is challenging.\nExperiment and analysis\nTo validate the proposed CMTNet method’s superiority, it is compared with multiple state-of-the-art RF 69, \nSVM70, 2D-CNN 18, 3D-CNN 18, Resnet 52, ViT 26, SSFTT 71 and CTMixer 63 approaches on three large-scale \ndatasets, namely, WHU-Hi-LongKou, WHU-Hi-HanChuan and WHU-Hi-HongHu.\nDatasets\nThis study used the publicly available HSI datasets. The WHU-Hi dataset72,73 produced by Wuhan University from \na research study located on the Jianghan plain of Hubei Province, China, with flat topography and abundant crop \nspecies (Fig. 5).The WHU-Hi LongKou dataset was acquired using the Headwall Nano-Hyperspec unmanned \naerial UAV in LongKou town, Hubei Province, China, on July 17, 2018. The image size is 550 × 400 pixels, \nwith 270 bands between 400 nm and 1000 nm, and a spatial resolution of approximately 0.463 m. The study \narea includes 9 land cover types.The image cube and ground-truth image are shown in Fig. 5a. The WHU-Hi \nHanChuan dataset was acquired using the Headwall Nano-Hyperspec unmanned aerial vehicle hyperspectral \nimager in Hanchuan City, Hubei Province, China, on June 17, 2016. The image size is 1217 × 303 pixels, with \n274 bands between 400 and 1000 nm and a spatial resolution of approximately 0.109 m. The study area includes \n16 land cover types. The image cube and ground-truth image are shown in Fig. 5b. The WHU-Hi HongHu \ndataset was acquired using the Headwall Nano-Hyperspec unmanned aerial vehicle hyperspectral imager in \nHonghu City, Hubei Province, China, on November 20, 2017. The image size is 940 × 475 pixels, with 270 bands \nbetween 400 and 1000 nm and a spatial resolution of approximately 0.043 m. The study area includes 22 land \ncover types.The image cube and ground-truth image are shown in Fig. 5c. Table 1 lists the overall crop category \nnames, number of training samples, and number of test samples for these three datasets. Each dataset is divided \ninto training and sample sets, with 0.5 % randomly selected from the total samples as the training set.\nEvaluation metrics\nThis study uses overall classification accuracy (OA), average classification accuracy (AA), kappa coefficient, and \naccuracy under individual categories as evaluation metrics. It also visually presents classification diagrams as a \nvisualization of the results.\nExperimental setup\nThe experiment utilized the SITONHOLY IW4202 rack server, equipped with an Intel® Xeon® CPU E5-2620 \nv4 @ 2.10GHz and four NVIDIA TITAN Xp GPUs, each with 12 GB of memory and a total of 128 GB running \nmemory. The software platform included Ubuntu 16.04.6 LTS 64-bit OS, CUDA Toolkit 10.1, CUDNN v7.6.5, \nFig. 5. Wuhan UAV-borne hyperspectral image. A and B denote image cube and ground-truth image \nrespectively. (a) WHU-Hi-LongKou dataset. (b) WHU-Hi-HanChuan dataset. (c) WHU-Hi-HongHu dataset.\n \nScientific Reports |        (2025) 15:12383 8| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nPython 3.8, and Pytorch 1.7.0. Each experiment was repeated ten times independently, with the average value \ntaken as the final result to mitigate the impact of random factors. The number of iterations was set to 100, using \ncross-entropy loss as the loss function and the Adam optimizer for model optimization.\nThe network, which combines CNN and Transformer, emphasizes global spatial information. To investigate \nthe impact of various input image patch sizes (s) on classification performance, experiments were conducted \nusing image patch sizes ranging from 5 to 15 on three datasets, with adjacent spatial sizes differing by 2. The \nexperimental results are illustrated in Fig. 6. The figure demonstrates that the classification accuracy of the \nWHU-Hi LongKou dataset increases as the input image s increases. OA initially increases and then stabilizes. \nThe WHU-Hi HanChuan and WHU-Hi HongHu datasets exhibit greater sensitivity to different input image \npatch sizes, with OA initially increasing and then decreasing with s. When s = 13, the OA of all three datasets \napproaches the maximum value. Consequently, s = 13 is chosen as the input image block size for the network \nproposed in this study.\nIn order to determine the optimal configuration of the proposed network architecture in terms of learning \nrate and batch size, a series of targeted experiments were conducted. The experimental results are illustrated \nin Fig. 7, with Fig. 7a–c representing the WHU-Hi LongKou, WHU-Hi HanChuan, and WHU-Hi HongHu \ndatasets. Different colors in the figures indicate various ranges of OA. It is evident that different learning rates \nand batch sizes result in different OA values for the same dataset. For the WHU-Hi LongKou dataset (Fig. 7a), \nthe impact of learning rate and batch size on OA is minimal, but there is some interaction between the two. The \noptimal learning rate and batch size were found to be 1e-3 and 100, respectively. On the other hand, the WHU-Hi \nHanChuan and WHU-Hi HongHu datasets show significant sensitivity to learning rate due to variations in crop \ntypes used for training. Increasing the learning rate initially boosts OA and then decreases, while increasing the \nbatch size also shows a similar trend. For the WHU-Hi HanChuan dataset Fig. 7b), a batch size of 100 resulted in \nimproved classification performance with the selected learning rate. Similarly, for the WHU-Hi HongHu dataset \n(Fig. 7c), the best classification performance was achieved with a learning rate of 1e-3 and a batch size of 100. \nConsequently, based on the parameter experiments, the optimal learning rate and batch size for the proposed \nclassification network were determined to be 1e-3 and 100, respectively.\nThis study investigates how the number of encoder layers and attention heads can impact the model’s \nrobustness and stability. Experimental results, as shown in Fig. 8, demonstrate the effects on the WHU-Hi \nLongKou, WHU-Hi HanChuan, and WHU-Hi HongHu datasets. The histograms in Fig. 8a–c reveal that the \ndifferences in OA histograms across different layers and heads are minimal, with OAs remaining stable within \nspecific ranges: LongKou: 99.52–99.68, HanChuan: 97.42–97.59, HongHu: 98.51–98.62. To ensure uniformity \nin the network structure across all datasets, this study opts for the CMTNet with one transformer layer and four \nattention heads as the final network configuration.\nNO.\nWHU-Hi LongKou WHU-Hi HanChuan WHU-Hi HongHu\nClass Training Test Class Training Test Class Training Test\n1 Corn 172 34339 Strawberry 223 44521 Red roof 70 13971\n2 Cotton 41 8333 Cowpea 113 22640 Road 17 3495\n3 Sesame 15 3016 Soybean 51 10236 Bare soil 109 21712\n4 Broad-leaf soybean 316 62896 Sorghum 26 5327 Cotton 816 162469\n5 Narrow-leaf soybean 20 4131 Water spinach 6 1194 Cotton firewood 31 6187\n6 Rice 59 11795 Watermelon 22 4511 Rape 222 44335\n7 Water 335 66721 Greens 29 5974 Chinese cabbage 120 23983\n8 Roads and houses 35 7089 Trees 89 17889 Pakchoi 20 4034\n9 Mixed weed 26 5203 Grass 47 9422 Cabbage 54 10765\n10 Red roof 52 10464 Tuber mustard 61 12333\n11 Gray roof 84 16827 Brassica parachinensis 55 10960\n12 Plastic 18 3661 Brassica chinensis 44 8910\n13 Bare soil 45 9071 Small Brassica chinensis 112 22395\n14 Road 92 18468 Lactuca sativa 36 7320\n14 Bright object 5 1131 Celtuce 5 997\n16 Water 377 75024 Film covered lettuce 36 7226\n17 Romaine lettuce 15 2995\n18 Carrot 16 3201\n19 White radish 43 8669\n20 Garlic sprout 17 3469\n21 Broad bean 6 1322\n22 Tree 20 4020\n/ Total 1019 203523 Total 1289 256241 Total 1925 384678\nTable 1. Training and test sample numbers in the WHU-HiLongKou dataset, theWHU-HiHanChuan dataset, \nand the WHU-HiHongHu dataset.\n \nScientific Reports |        (2025) 15:12383 9| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nFig. 8. OA under different numbers of transformer encoder layers and MHSA heads on (a) WHU-Hi \nLongKou, (b) WHU-Hi HanChuan, and (c) WHU-Hi HongHu, respectively.\n \nFig. 7. Effect of different learning rates and batch sizes on performance accuracy OA. (a) Experimental results \non WHU-Hi LongKou dataset. (b) Experimental results on WHU-Hi HanChuan dataset. (c) Experimental \nresults on WHU-Hi HongHu dataset.\n \nFig. 6. Impact of different input space sizes on OA.\n \nScientific Reports |        (2025) 15:12383 10| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nComparison of experimental results with SOTA\nThe OA, AA, and Kappa values of CMTNet and other comparative methods on the WHU-Hi LongKou, WHU-\nHi HanChuan, and WHU-Hi HongHu datasets are presented in Tables 2, 3 and 4, accompanied by visual \nrepresentations in Figs. 9, 10 and 11. The best values are highlighted in bold in the tables, clearly indicating \nthe superior performance of the proposed CMTNet method. Analysis of Table 2 reveals that CMTNet excels \nin capturing both global and local spectral features of hyperspectral imaging separately, effectively integrating \nhigh-dimensional information to achieve outstanding classification results across different land cover targets. \nWhen compared to the CNN and Transformer hybrid networks SSFTT and CTMixer, CMTNet outperforms \nin final classification results, showing an increase in OA of 0.21 and 0.19, respectively. This improvement can \nbe attributed to the multi-output constraint module of CMTNet, which optimally reallocates feature weights. \nHowever, the performance of RF and SVM could be enhanced, particularly in the classification of cotton and \nsoybeans with limited training samples, where individual accuracies fall below 47%. Examination of Fig. 9 \ndemonstrates that CMTNet significantly enhances classification performance, reducing misclassifications and \nensuring complete classification edges through the fusion of local-global spectral features.\nThe WHU-Hi HanChuan dataset captured images in the afternoon with a lower sun angle, resulting in \nnumerous shadow patches. The classification results for the RF and SVM methods show many misclassifications. \nBoth 2D-CNN and 3-DCNN models display significant fragmentation, highlighting the necessity for methods \nto enhance model generalization. The SSFTT synthesizes the use of 3D convolutional layers and attention \nNO. RF SVM 2D-CNN 3D-CNN Resnet ViT SSFTT CTMixer CMTNet\n1 77.94 70.74 94.72 87.16 86.61 90.87 95.97 95.65 96.09\n2 72.55 49.21 87.18 93.39 78.74 75.90 97.90 95.60 97.66\n3 38.76 71.1 91.33 87.66 87.35 89.31 95.54 92.16 94.30\n4 86.13 94.12 94.25 92.53 90.35 92.67 94.72 98.24 98.28\n5 14.23 81.35 94.39 64.22 89.68 85.3 84.03 97.41 98.35\n6 22.75 47.53 64.42 75.41 85.02 83.37 82.82 89.89 96.11\n7 49.02 88.82 85.59 74.36 86.40 85.21 85.84 89.16 96.29\n8 35.31 59.46 85.32 90.43 77.78 77.02 85.92 91.18 94.63\n9 87.62 61.63 84.18 87.84 87.43 80.21 81.32 89.64 92.70\n10 87.94 87.94 89.44 95.32 87.03 88.86 97.62 98.39 98.15\n11 47.85 92.05 91.35 89.71 90.06 86.84 92.04 95.59 96.86\n12 26.17 61.61 55.8 75.31 85.87 84.86 77.92 93.41 97.30\n13 68.66 56.39 71.28 82.21 84.09 80.36 87.63 88.39 94.25\n14 95.06 63.48 86.77 89.09 83.73 83.79 89.98 92.84 97.96\n15 38.07 70.93 36.68 89.65 90.54 88.46 90.6 97.10 96.01\n16 93.25 94 94.72 97.79 89.57 91.07 99.66 99.36 99.79\nOA(%) 71.03 76.05 89.47 91.03 87.36 92.86 94.17 95.54 97.29\nAA(%) 69.83 71.9 81.71 85.35 81.33 84.67 87.22 89.34 94.01\nk×100 70.71 72.58 88.52 89.43 85.04 91.63 93.18 94.77 96.83\nTable 3. Classification performance obtained by different methods for WHU-Hi HanChuan dataset (optimal \nresults are bolded).\n \nNO. RF SVM 2D-CNN 3D-CNN Resnet ViT SSFTT CTMixer CMTNet\n1 89.04 90.99 94.83 98.56 88.09 92.3 99.94 99.62 99.75\n2 45.10 46.25 58.63 65.53 89.97 72.22 98.32 99.69 99.53\n3 90.23 89.87 95.47 97.73 88.31 97.63 99.93 100.00 99.83\n4 87.56 87.83 84 93.91 90.07 90.15 99.36 99.31 99.67\n5 32.59 42.57 55.11 75.11 86.88 92.35 98.94 98.38 98.61\n6 83.85 85.18 90.79 96.62 85.26 85.53 99.89 99.95 99.94\n7 86.86 86.83 97.66 97.61 86.83 86.65 99.90 99.79 99.84\n8 64.61 65.55 66.35 81.44 82.83 81.71 95.6 95.51 99.32\n9 44.06 65.87 71.83 90.21 84.56 87.58 95.1 97.83 98.55\nOA(%) 84.65 85.21 88.75 93.93 90.43 94.05 99.37 99.39 99.58\nAA(%) 75.33 75.26 79.41 88.53 90.68 93.84 97.66 97.60 98.62\nk×100 86.24 87.56 85.36 92.38 88.67 94.99 99.18 99.20 99.45\nTable 2. Classification performance obtained by different methods for WHU-Hi LongKou dataset (optimal \nresults are bolded).\n \nScientific Reports |        (2025) 15:12383 11| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nFig. 9. Classification visualization maps of all methods on the WHU-Hi LongKou dataset. (a)–(i) \nClassification map of RF , SVM, 2-DCNN, 3-D CNN, Resnet, ViT, SSFTT, SSTN, CTMixer, and CMTNet, \nrespectively. (j) Real ground feature map. CMTNet model classification results (OA = 99.58%, Kappa = 99.45).\n \nNO. RF SVM 2D-CNN 3D-CNN Resnet ViT SSFTT CTMixer CMTNet\n1 92.35 83.93 85.67 94.6 98.25 97.22 96.23 96.90 98.32\n2 49.67 97.34 76.96 85.17 80.08 98.62 81.87 90.30 96.14\n3 97.97 72.85 98.08 98.62 99.96 99.31 92.19 92.81 97.31\n4 96.20 78.96 92.83 97.48 99.75 98.43 98.89 98.96 99.79\n5 22.34 77.25 67.59 80.09 51.02 72.64 84.47 91.17 97.60\n6 22.91 81.95 82.35 76.71 80.21 87.18 98.51 97.68 98.98\n7 46.88 59.25 62.94 92.24 82.01 92.70 88.21 91.71 94.41\n8 14.44 41.63 49.68 40.38 39.35 63.25 92.35 88.71 96.23\n9 82.68 90.86 86.23 100 99.79 100.00 97.33 97.93 97.92\n10 30.46 54.08 84.83 59.51 76.25 86.25 94.62 94.13 98.26\n11 14.36 48.31 73.53 85.25 83.84 82.45 89.34 92.65 96.54\n12 14.46 61.31 64.91 67.92 38.14 48.18 90.25 87.4 98.36\n13 21.21 49.86 43.02 30.89 29.95 38.6 90.8 87.79 96.25\n14 57.30 63.78 62.46 59.17 94.37 94.02 98.37 98.19 98.08\n15 9.90 85.92 58.00 76.92 100.00 95.24 90.14 99.01 97.70\n16 78.29 78.01 99.17 96.25 98.94 98.55 97.76 96.65 99.10\n17 57.71 70.65 100 91.51 84.27 99.49 80.68 91.45 99.73\n18 18.07 79.24 82.63 67.70 56.69 63.49 95.21 94.07 97.29\n19 47.87 68.22 78.91 53.06 71.58 63.06 95.20 93.90 97.99\n20 26.86 77.85 17.87 72.76 69.72 81.72 84.9 87.67 96.31\n21 16.52 74.67 98.78 48.80 55.00 92.77 84.13 66.16 90.86\n22 10.14 81.14 74.64 54.84 47.99 46.55 92.76 90.17 95.91\nOA(%) 54.06 73.55 87.81 89.48 85.14 91.53 95.56 95.79 98.31\nAA(%) 49.32 71.23 82.14 85.99 83.54 87.26 89.02 89.58 95.26\nk×100 52.38 68.05 86.63 88.20 84.43 91.51 94.37 94.68 97.87\nTable 4. Classification performance obtained by different methods for WHU-Hi HongHu dataset (optimal \nresults are bolded).\n \nScientific Reports |        (2025) 15:12383 12| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nFig. 11. Classification visualization maps of all methods on the WHU-Hi HanChuan dataset. (a)–(i) \nClassification map of RF , SVM, 2-DCNN, 3-D CNN, Resnet, ViT, SSFTT, SSTN, CTMixer, and CMTNet, \nrespectively. (j) Real ground feature map. CMTNet model classification results (OA = 98.31%, Kappa = 97.87).\n \nFig. 10. Classification visualization maps of all methods on the WHU-Hi HanChuan dataset. (a)–(i) \nClassification map of RF , SVM, 2-DCNN, 3-D CNN, Resnet, ViT, SSFTT, SSTN, CTMixer, and CMTNet, \nrespectively. (j) Real ground feature map. CMTNet model classification results (OA = 97.29%, Kappa = 96.83).\n \nScientific Reports |        (2025) 15:12383 13| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nmechanism modules to realize the abstraction extraction of joint spectral-spatial features, effectively mitigating \nclassification errors caused by ‘same material different spectrum, different materials same spectrum’ . However, \nowing to its serial extraction of spectral-spatial features without effective selection, there is still a problem \nof performance plummeting in the classification of small-sample targets, with the OA for categories such as \nWatermelon and Plastic (NO.6 and 12, respectively) being only 82.42% and 77.92%. ResNet exhibits clear \nmisclassifications of soybeans and gray rooftops. On the other hand, ViT and CTMixer methods achieve high-\nprecision classifications overall, but errors persist in shadow-covered areas. Despite this, CMTNet outperforms \nin identifying similar spectral features through multi-feature fusion extraction, leading to reduced fragmentation \ncompared to other methods.\nIn the WHU-Hi HongHu dataset, traditional classification algorithms struggle with misclassifications due to \nslight spectral differences among crops of the same type. Specifically, Brassica parachinensis, Brassica chinensis, \nand Small Brassica chinensis exhibit low classification accuracy. Deep learning methods have notably enhanced \nhyperspectral classification over traditional approaches. However, 2D-CNN and 3D-CNN tend to only capture \nlocal features in hyperspectral images, resulting in fragmented classification outcomes. The ViT model, on the \nother hand, leverages global perceptual spectral features to mitigate this fragmentation. While models like SSFTT \nand CTMixer combine CNN and Transformer architectures to effectively utilize spectral-spatial information for \nimproved classification, they still struggle with misclassifications in land cover categories with limited samples.\nCMTNet demonstrates the best effectiveness in categorizing various terrestrial objects due to its capability \nto capture spatial and spectral characteristics separately, and efficiently filter and integrate high-dimensional \ninformation. It delivers exceptionally good results for different categories, with OA for Red roof, Cotton, Rape, \nTuber mustard, and Lactuca sativa reaching 98.32%, 99.7%, 98.98%, 98.26%, and 98.08% respectively, and the \noverall OA and Kappa coefficient being 98.31% and 97.87%. CTMixer focuses on the effective use of global and \nlocal multi-scale features, achieving better outcomes in mixed terrestrial feature regions, yet its OA and Kappa \ncoefficients are reduced by 2.52% and 3.19% compared to CMTNet. Visual and quantitative analyses reveal that \nCMTNet achieves the highest accuracy and excels at classifying land cover categories with limited samples. \nThis suggests that incorporating the multi-output constraint module can enhance the model’s robustness and \nstability.\nAblation experiments\nTo thoroughly verify the effectiveness of the proposed method, ablation experiments were conducted on three \ndatasets using different components of the network model. The baseline network was Transformer, with modules \nfrom CMTNet sequentially added to assess their contributions. Five combined models were analyzed, and the \nimpact of each component on the OA was measured. The results of all ablation experiments are presented in \nTable 5. The checkmark symbol “ ✓” indicates module usage, while the cross symbol “ ×” indicates non-usage. \nAnalysis revealed that using only the Transformer module resulted in relatively low OAs across the datasets, \nsuggesting its limitations in extracting local features for hyperspectral image classification. Addition of the \nspectral-spatial feature extraction module in Case 2 and Case 3 led to an increase in OA. Case 4 introduced a \nCNN branch in parallel with the Transformer branch to enhance local feature extraction, resulting in a significant \nOA improvement. Case 5 further improved the integration of features from each branch by incorporating the \nmulti-output constraint module. Experimental findings demonstrated that Case 5 consistently outperformed \nCase 4 on all three datasets, highlighting the effectiveness of the Multi-Output Constraint Module (MOCM).\nModel efficiency analysis\nTo evaluate the computational efficiency of the proposed method, we conducted efficiency tests on all \napproaches, with Table 6 presenting the experimental results. As shown in Table 6, traditional machine learning \nmodels (such as RF and SVM) exhibited the fastest running speeds, with RF requiring 36.82, 62.66, and 74.54 s \non the three datasets, respectively. In contrast, deep learning models like 3D-CNN, Resnet, and ViT demand \nsignificantly more computational resources. For instance, ViT required 857.20, 1458.26, and 1735.34 s on the \nLongkou, Hanchuan, and Honghu datasets, respectively, reflecting its inherent complexity. Compared with \nthe SSFTT method, the training and testing times for CMTNet are slightly longer, which can be attributed \nto its dual-branch Transformer architecture that enhances feature representation while introducing additional \ncomputational overhead during optimization. In contrast to the CTMixer method, CMTNet requires less \nrunning time. Overall, Transformer-based methods demonstrate significantly higher efficiency than CNN-\nbased methods. While CMTNet achieves state-of-the-art classification accuracy, its computational cost is higher \nthan traditional machine learning methods. This trade-off is critical for precision agriculture applications where \naccuracy is prioritized. Future work will focus on model compression and edge deployment frameworks to \nfurther bridge the efficiency gap.\nCase CNN Branch Conv3D Conv2D MOCM LongKou HanChuan HongHu\n1 × × × × 94.97 92.86 91.53\n2 ✓ × × × 96.25 95.21 93.89\n3 ✓ ✓ × × 98.77 95.76 95.34\n4 ✓ ✓ ✓ × 99.12 96.05 97.29\n5 ✓ ✓ ✓ ✓ 99.58 97.29 98.31\nTable 5. Impact of different modules on network OA value (%).\n \nScientific Reports |        (2025) 15:12383 14| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nDiscussion\nWhile CMTNet achieves state-of-the-art performance on diverse datasets, its accuracy on shadow-affected \nregions (e.g., WHU-Hi HanChuan) reveals a dependency on consistent illumination conditions. Shadows \nintroduce spectral ambiguities that challenge the current feature extraction modules. To address this, future \niterations could integrate shadow-invariant feature learning techniques, such as normalization based on \nillumination-invariant indices74, or leverage multi-temporal data to disentangle shadow effects from intrinsic \nspectral signatures.\nTo evaluate the risk of overfitting in classes with limited training samples, we specifically analyze the \nperformance of Cotton (NO.2) and Narrow-leaf soybean (NO.5) in the WHU-Hi LongKou dataset. As shown \nin Table 2, these classes have only 41 and 20 training samples, respectively. Despite the small sample size, \nCMTNet achieves OA values of 99.53% for Cotton and 98.61% for Narrow-leaf soybean. However, compared \nto classes with abundant samples (e.g., Broad-leaf soybean (NO.4) with 316 training samples and 99.67% OA), \nthe accuracy gaps (0.14% and 1.06%) indicate potential overfitting risks. Although MOCM partially alleviates \noverfitting, the performance of extremely small sample classes (e.g., Narrow-leaf soybean) has not yet reached \nits optimal level. To address this issue, in future work, we will initialize the feature extractor using a pre-trained \nmodel on large-scale hyperspectral datasets (such as WHU-Hi HongHu) and then fine-tune it on the target \nsmall-sample classes75. Additionally, semi-supervised learning techniques, such as consistency regularization or \npseudo-labeling76, will be employed to incorporate unlabeled data, which can enhance the model’s generalization \nwithout requiring additional labeled samples. These methods can synergistically improve the robustness of \nCMTNet’s dual-branch architecture in imbalanced agricultural scenarios.\nConclusions\nIn order to enhance the precision and efficiency of crop classification in areas with imbalanced samples and \ndiverse land cover types, this study introduces a novel method called CMTNet. This method incorporates a \ndual-branch structure featuring parallel CNN and Transformer components, enabling the extraction of local-\nglobal features from hyperspectral images. A convolutional layer combination spectral-spatial feature extraction \nmodule is employed to capture low-level spectral-spatial features, while a multi-output constraint module \neffectively addresses information loss post multi-scale feature fusion. Experimental results demonstrate the \nmethod’s effectiveness in enhancing classification performance. Although CMTNet demonstrates excellent \nperformance in complex agricultural scenarios, the current study still exhibits several limitations. Specifically, \nthe model’s robustness against extreme shadows and occlusions requires improvement, and its computational \ncost restricts deployment on resource-constrained devices. To address these issues, future research will focus on \nthe following directions. First, by integrating multimodal data (e.g., LiDAR elevation information), we aim to \nenhance classification robustness in complex environments characterized by shadows and occlusions. Second, \nwe plan to optimize real-time performance and explore model compression techniques (such as pruning and \nquantization) to reduce computational costs. Finally, by incorporating edge computing frameworks, we seek \nto achieve efficient deployment on drone platforms, thereby advancing real-time monitoring applications in \nprecision agriculture.\nData availability\nThe datasets analyzed during the current study are publicly available in the WHU-Hi repository at  h t t p :   /  / r s i d e  a \n. w h  u . e  d u .   c n / r e  s o u  r c e  _ W H  U H  i _ s h a   r i n g . h t m.\nReceived: 21 January 2025; Accepted: 2 April 2025\nReferences\n 1. Weiss, M., Jacob, F . & Duveiller, G. Remote sensing for agricultural applications: A meta-review. Remote Sens. Environ. 236, 111402 \n(2020).\n 2. Khan, M. J., Khan, H. S., Y ousaf, A., Khurshid, K. & Abbas, A. Modern trends in hyperspectral image analysis: A review. IEEE \nAccess 6, 14118–14129 (2018).\n 3. Hu, J., Huang, Z., Shen, F ., He, D. & Xian, Q. A rubust method for roof extraction and height estimation. In IGARSS 2023-2023 \nIEEE International Geoscience and Remote Sensing Symposium (IEEE, 2023).\n 4. Qiao, C. et al. A novel multi-frequency coordinated module for sar ship detection. In 2022 IEEE 34th International Conference on \nTools with Artificial Intelligence (ICTAI) 804–811 (IEEE, 2022).\n 5. Soni, A., Dixit, Y ., Reis, M. M. & Brightwell, G. Hyperspectral imaging and machine learning in food microbiology: Developments \nand challenges in detection of bacterial, fungal, and viral contaminants. Compr. Rev. Food Sci. Food Saf. 21, 3717–3745 (2022).\n 6. Shen, F . et al. An efficient multiresolution network for vehicle reidentification. IEEE Internet Things J. 9, 9049–9059 (2021).\n 7. Hu, J., Huang, Z., Shen, F ., He, D. & Xian, Q. A bag of tricks for fine-grained roof extraction. In IGARSS 2023-2023 IEEE \nInternational Geoscience and Remote Sensing Symposium (IEEE, 2023).\nDatasets RF SVM 2D-CNN 3D-CNN Resnet ViT SSFTT CTMixer CMTNet\nWHU-Hi LongKou 36.82 163.31 591.22 658.82 726.10 857.20 701.21 1149.38 931.63\nWHU-Hi HanChuan 62.66 277.65 1005.48 1120.93 1235.35 1458.26 1192.85 1955.42 1584.60\nWHU-Hi HongHu 74.54 330.32 1196.46 1333.98 1469.82 1735.34 1419.31 2327.04 1885.32\nTable 6. Running time of different methods on three datasets (s).\n \nScientific Reports |        (2025) 15:12383 15| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\n 8. Wu, H. et al. A sample-proxy dual triplet loss function for object re-identification. IET Image Proc. 16, 3781–3789 (2022).\n 9. Xu, R., Shen, F ., Wu, H., Zhu, J. & Zeng, H. Dual modal meta metric learning for attribute-image person re-identification. In 2021 \nIEEE International Conference on Networking, Sensing and Control (ICNSC), vol. 1, 1–6 (IEEE, 2021).\n 10. Dalla Mura, M., Benediktsson, J. A., Waske, B. & Bruzzone, L. Morphological attribute profiles for the analysis of very high \nresolution images. IEEE Trans. Geosci. Remote Sens. 48, 3747–3762 (2010).\n 11. Ghamisi, P . et al. Extinction profiles for the classification of remote sensing data. IEEE Trans. Geosci. Remote Sens. 54, 5631–5645 \n(2016).\n 12. Hennessy, A., Clarke, K. & Lewis, M. Hyperspectral classification of plants: A review of waveband selection generalisability. Remote \nSens. 12, 113 (2020).\n 13. Ranjan, P . & Girdhar, A. A comprehensive systematic review of deep learning methods for hyperspectral images classification. Int. \nJ. Remote Sens. 43, 6221–6306 (2022).\n 14. Chen, Y ., Zhao, X. & Jia, X. Spectral-spatial classification of hyperspectral data based on deep belief network. IEEE J. Sel. Top. Appl. \nEarth Obs. Remote Sens. 8, 2381–2392 (2015).\n 15. Wu, H. & Prasad, S. Convolutional recurrent neural networks for hyperspectral data classification. Remote Sens. 9, 298 (2017).\n 16. Hsieh, T.-H. & Kiang, J.-F . Comparison of CNN algorithms on hyperspectral image classification in agricultural lands. Sensors 20, \n1734 (2020).\n 17. Paoletti, M., Haut, J., Plaza, J. & Plaza, A. Deep learning classifiers for hyperspectral imaging: A review. ISPRS J. Photogramm. \nRemote. Sens. 158, 279–317 (2019).\n 18. Bera, S., Shrivastava, V . K. & Satapathy, S. C. Advances in hyperspectral image classification based on convolutional neural \nnetworks: A review. CMES-Comput. Model. Eng. Sci. 133, 219–250 (2022).\n 19. Xu, X. et al. Multisource remote sensing data classification based on convolutional neural network. IEEE Trans. Geosci. Remote \nSens. 56, 937–949 (2017).\n 20. Shi, C., Liao, D., Zhang, T. & Wang, L. Hyperspectral image classification based on expansion convolution network. IEEE Trans. \nGeosci. Remote Sens. 60, 1–16 (2022).\n 21. Xu, Q., Yuan, X., Ouyang, C. & Zeng, Y . Attention-based pyramid network for segmentation and classification of high-resolution \nand hyperspectral remote sensing images. Remote Sens. 12, 3501 (2020).\n 22. Li, M., Wei, M., He, X. & Shen, F . Enhancing part features via contrastive attention module for vehicle re-identification. In 2022 \nIEEE International Conference on Image Processing (ICIP) 1816–1820 (IEEE, 2022).\n 23. Shen, F ., Zhu, J., Zhu, X., Xie, Y . & Huang, J. Exploring spatial significance via hybrid pyramidal graph network for vehicle re-\nidentification. IEEE Trans. Intell. Transp. Syst. 23, 8793–8804 (2021).\n 24. Sun, J. et al. Fusing spatial attention with spectral-channel attention mechanism for hyperspectral image classification via encoder-\ndecoder networks. Remote Sens. 14, 1968 (2022).\n 25. Shen, F ., Xie, Y ., Zhu, J., Zhu, X. & Zeng, H. Git: Graph interactive transformer for vehicle re-identification. IEEE Trans. Image \nProcess. 32, 1039–1051 (2023).\n 26. Ming, Y . et al. Visuals to text: A comprehensive review on automatic image captioning. IEEE/CAA J. Autom. Sin. 9, 1339–1365 \n(2022).\n 27. Aleissaee, A. A. et al. Transformers in remote sensing: A survey. Remote Sens. 15, 1860 (2023).\n 28. Maurício, J., Domingues, I. & Bernardino, J. Comparing vision transformers and convolutional neural networks for image \nclassification: A literature review. Appl. Sci. 13, 5521 (2023).\n 29. Weng, W ., Wei, M., Ren, J. & Shen, F . Enhancing aerial object detection with selective frequency interaction network. IEEE Trans. \nArtif. Intell. 1, 1–12 (2024).\n 30. Xu, Y . et al. Transformers in computational visual media: A survey. Comput. Vis. Media 8, 33–62 (2022).\n 31. Qi, W . et al. Global-local three-dimensional convolutional transformer network for hyperspectral image classification. IEEE Trans. \nGeosci. Remote Sens. 61, 1–20 (2023).\n 32. Shen, F ., Shu, X., Du, X. & Tang, J. Pedestrian-specific bipartite-aware similarity learning for text-based person retrieval. In \nProceedings of the 31th ACM International Conference on Multimedia (2023).\n 33. Sangaiah, A. K. et al. Edge-IoT-UAV adaptation toward precision agriculture using 3d-lidar point clouds. IEEE Internet Things \nMag. 8, 19–25 (2024).\n 34. Anandakrishnan, J., Sundaram, V . M. & Paneer, P . STA-AgriNet: A spatio-temporal attention framework for crop type mapping \nfrom fused multi-sensor multi-temporal sits. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 18, 1817–1826 (2024).\n 35. Mohan, A. & Venkatesan, M. HybridCNN based hyperspectral image classification using multiscale spatiospectral features. \nInfrared Phys. Technol. 108, 103326 (2020).\n 36. Li, H., Zhang, R., Pan, Y ., Ren, J. & Shen, F . Lr-fpn: Enhancing remote sensing object detection with location refined feature \npyramid network. Preprint at arXiv:2404.01614 (2024).\n 37. Chen, Y ., Jiang, H., Li, C., Jia, X. & Ghamisi, P . Deep feature extraction and classification of hyperspectral images based on \nconvolutional neural networks. IEEE Trans. Geosci. Remote Sens. 54, 6232–6251 (2016).\n 38. Yue, J., Zhao, W ., Mao, S. & Liu, H. Spectral-spatial classification of hyperspectral images using deep convolutional neural networks. \nRemote Sens. Lett. 6, 468–477 (2015).\n 39. Li, X., Ding, M. & Pižurica, A. Deep feature fusion via two-stream convolutional neural network for hyperspectral image \nclassification. IEEE Trans. Geosci. Remote Sens. 58, 2615–2629 (2019).\n 40. Zhao, W . & Du, S. Learning multiscale and deep representations for classifying remotely sensed imagery. ISPRS J. Photogramm. \nRemote. Sens. 113, 155–165 (2016).\n 41. Haut, J. M., Paoletti, M. E., Plaza, J., Li, J. & Plaza, A. Active learning with convolutional neural networks for hyperspectral image \nclassification using a new Bayesian approach. IEEE Trans. Geosci. Remote Sens. 56, 6440–6461 (2018).\n 42. Li, Y ., Zhang, H. & Shen, Q. Spectral-spatial classification of hyperspectral imagery with 3D convolutional neural network. Remote \nSens. 9, 67 (2017).\n 43. Liu, J. et al. An investigation of a multidimensional CNN combined with an attention mechanism model to resolve small-sample \nproblems in hyperspectral image classification. Remote Sens. 14, 785 (2022).\n 44. Y ang, J., Zhao, Y .-Q. & Chan, J.C.-W . Learning and transferring deep joint spectral-spatial features for hyperspectral classification. \nIEEE Trans. Geosci. Remote Sens. 55, 4729–4742 (2017).\n 45. Yu, S., Jia, S. & Xu, C. Convolutional neural networks for hyperspectral image classification. Neurocomputing 219, 88–98 (2017).\n 46. Diakite, A., Jiangsheng, G. & Xiaping, F . Hyperspectral image classification using 3D 2D CNN. IET Image Process. 15 (2020).\n 47. Shen, F ., Wei, M. & Ren, J. HSGNet: Object re-identification with hierarchical similarity graph network. Preprint at arXiv:2211.05486 \n(2022).\n 48. Hu, X., Wang, X., Zhong, Y . & Zhang, L. S3ANet: Spectral-spatial-scale attention network for end-to-end precise crop classification \nbased on UAV-borne H2 imagery. ISPRS J. Photogramm. Remote. Sens. 183, 147–163 (2022).\n 49. Shen, F ., Du, X., Zhang, L. & Tang, J. Triplet contrastive learning for unsupervised vehicle re-identification. Preprint at \narXiv:2301.09498 (2023).\n 50. Haut, J. M., Paoletti, M. E., Plaza, J., Plaza, A. & Li, J. Visual attention-driven hyperspectral image classification. IEEE Trans. Geosci. \nRemote Sens. 57, 8065–8080 (2019).\n 51. Tang, X. et al. Hyperspectral image classification based on 3-d octave convolution with spatial-spectral attention network. IEEE \nTrans. Geosci. Remote Sens. 59, 2430–2447 (2020).\nScientific Reports |        (2025) 15:12383 16| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\n 52. Roy, S. K., Manna, S., Song, T. & Bruzzone, L. Attention-based adaptive spectral-spatial kernel ResNet for hyperspectral image \nclassification. IEEE Trans. Geosci. Remote Sens. 59, 7831–7843 (2020).\n 53. Shen, F . & Tang, J. Imagpose: A unified conditional framework for pose-guided person generation. In The Thirty-eighth Annual \nConference on Neural Information Processing Systems (2024).\n 54. Shen, F . et al.  Long-term talkingface generation via motion-prior conditional diffusion model. Preprint at arXiv:2502.09533 \n(2025).\n 55. Shen, F . et al.  Boosting consistency in story visualization with rich-contextual conditional diffusion models. Preprint at \narXiv:2407.02482 (2024).\n 56. Shen, F . et al. Imagdressing-v1: Customizable virtual dressing. Preprint at arXiv:2407.12705 (2024).\n 57. Hong, D. et al. SpectralFormer: Rethinking hyperspectral image classification with transformers. IEEE Trans. Geosci. Remote Sens. \n60, 1–15 (2021).\n 58. He, J., Zhao, L., Y ang, H., Zhang, M. & Li, W . HSI-BERT: Hyperspectral image classification using the bidirectional encoder \nrepresentation from transformers. IEEE Trans. Geosci. Remote Sens. 58, 165–178 (2019).\n 59. Qing, Y ., Liu, W ., Feng, L. & Gao, W . Improved transformer net for hyperspectral image classification. Remote Sens. 13, 2216 \n(2021).\n 60. Tan, X., Gao, K., Liu, B., Fu, Y . & Kang, L. Deep global-local transformer network combined with extended morphological profiles \nfor hyperspectral image classification. J. Appl. Remote Sens. 15, 038509–038509 (2021).\n 61. Mei, S., Song, C., Ma, M. & Xu, F . Hyperspectral image classification using group-aware hierarchical transformer. IEEE Trans. \nGeosci. Remote Sens. 60, 1–14 (2022).\n 62. Zhong, Z., Li, Y ., Ma, L., Li, J. & Zheng, W .-S. Spectral-spatial transformer network for hyperspectral image classification: A \nfactorized architecture search framework. IEEE Trans. Geosci. Remote Sens. 60, 1–15 (2021).\n 63. Zhang, J., Meng, Z., Zhao, F ., Liu, H. & Chang, Z. Convolution transformer mixer for hyperspectral image classification. IEEE \nGeosci. Remote Sens. Lett. 19, 1–5 (2022).\n 64. Liang, M. et al. A dual multi-head contextual attention network for hyperspectral image classification. Remote Sens. 14, 3091 \n(2022).\n 65. Y ang, L. et al. FusionNet: A convolution-transformer fusion network for hyperspectral image classification. Remote Sens. 14, 4066 \n(2022).\n 66. Song, R., Feng, Y ., Cheng, W ., Mu, Z. & Wang, X. BS2T: Bottleneck spatial-spectral transformer for hyperspectral image \nclassification. IEEE Trans. Geosci. Remote Sens. 60, 1–17 (2022).\n 67. Y ang, X., Cao, W ., Lu, Y . & Zhou, Y . Hyperspectral image transformer classification networks. IEEE Trans. Geosci. Remote Sens. 60, \n1–15 (2022).\n 68. Guo, F ., Feng, Q., Y ang, S. & Y ang, W . CMTNet: Convolutional meets transformer network for hyperspectral images classification. \nPreprint at arXiv:2406.14080 (2024).\n 69. Ballanti, L., Blesius, L., Hines, E. & Kruse, B. Tree species classification using hyperspectral imagery: A comparison of two \nclassifiers. Remote Sens. 8, 445 (2016).\n 70. Chen, Y ., Zhao, X. & Lin, Z. Optimizing subspace SVM ensemble for hyperspectral imagery classification. IEEE J. Sel. Top. Appl. \nEarth Obs. Remote Sens. 7, 1295–1305 (2014).\n 71. Sun, L., Zhao, G., Zheng, Y . & Wu, Z. Spectral-spatial feature tokenization transformer for hyperspectral image classification. IEEE \nTrans. Geosci. Remote Sens. 60, 1–14 (2022).\n 72. Zhong, Y . et al. Mini-UAV-borne hyperspectral remote sensing: From observation and processing to applications. IEEE Geosci. \nRemote Sens. Mag. 6, 46–62 (2018).\n 73. Zhong, Y . et al. WHU-Hi: UAV-borne hyperspectral with high spatial resolution (H2) benchmark datasets and classifier for precise \ncrop identification based on deep convolutional neural network with CRF . Remote Sens. Environ. 250, 112012 (2020).\n 74. Guo, D. et al. Face illumination normalization based on generative adversarial network. Nat. Comput. 22, 105–117 (2023).\n 75. Xie, F ., Gao, Q., Jin, C. & Zhao, F . Hyperspectral image classification based on superpixel pooling convolutional neural network \nwith transfer learning. Remote Sens. 13, 930 (2021).\n 76. Li, Z. et al. Pseudo-labelling contrastive learning for semi-supervised hyperspectral and LiDAR data classification. IEEE J. Sel. Top. \nAppl. Earth Obs. Remote Sens. 17, 17099–17116 (2024).\nAcknowledgements\nThis research was funded by the National Natural Science Foundation of China (32160421) and Major Science \nand Technology Project of Gansu Province (24ZDNJ001).\nAuthor contributions\nX.G. and Q.F . conceptualized the study. X.G. developed the methodology. F .G. wrote the software. X.G. drafted \nthe original manuscript. Q.F . and F .G. reviewed and edited the manuscript. Q.F . and F .G. provided supervision. \nX.G. and Q.F . curated the data. X.G. and Q.F . conducted the investigation. Q.F . provided resources. All authors \nreviewed the manuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to F .G.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nScientific Reports |        (2025) 15:12383 17| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:12383 18| https://doi.org/10.1038/s41598-025-97052-w\nwww.nature.com/scientificreports/",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.848384439945221
    },
    {
      "name": "Computer science",
      "score": 0.658601701259613
    },
    {
      "name": "Precision agriculture",
      "score": 0.6552814841270447
    },
    {
      "name": "Transformer",
      "score": 0.5208361744880676
    },
    {
      "name": "Agriculture",
      "score": 0.5050981640815735
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46351468563079834
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3682361841201782
    },
    {
      "name": "Agricultural engineering",
      "score": 0.35347461700439453
    },
    {
      "name": "Biology",
      "score": 0.15014109015464783
    },
    {
      "name": "Ecology",
      "score": 0.1073225736618042
    },
    {
      "name": "Engineering",
      "score": 0.07773515582084656
    },
    {
      "name": "Electrical engineering",
      "score": 0.0749254822731018
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}