{
  "title": "Optimizing Large Language Models for Discharge Prediction: Best Practices in Leveraging Electronic Health Record Audit Logs",
  "url": "https://openalex.org/W4402809646",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2105481316",
      "name": "Xinmeng Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097325630",
      "name": "Chao Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161968505",
      "name": "Yuyang Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2528449914",
      "name": "Zhuohang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109857036",
      "name": "Feng Yubo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2145238237",
      "name": "Bradley A Malin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106573198",
      "name": "You Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4367668173"
  ],
  "abstract": "Electronic Health Record (EHR) audit log data are increasingly utilized for clinical tasks, from workflow modeling to predictive analyses of discharge events, adverse kidney outcomes, and hospital readmissions. These data encapsulate user-EHR interactions, reflecting both healthcare professionals' behavior and patients' health statuses. To harness this temporal information effectively, this study explores the application of Large Language Models (LLMs) in leveraging audit log data for clinical prediction tasks, specifically focusing on discharge predictions. Utilizing a year's worth of EHR data from Vanderbilt University Medical Center, we fine-tuned LLMs with randomly selected 10,000 training examples. Our findings reveal that LLaMA-2 70B, with an AUROC of 0.80 [0.77-0.82], outperforms both GPT-4 128K in a zero-shot, with an AUROC of 0.68 [0.65-0.71], and DeBERTa, with an AUROC of 0.78 [0.75-0.82]. Among various serialization methods, the first-occurrence approach — wherein only the initial appearance of each event in a sequence is retained — shows superior performance. Furthermore, for the fine-tuned LLaMA-2 70B, logit outputs yield a higher AUROC of 0.80 [0.77-0.82] compared to text outputs, with an AUROC of 0.69 [0.67-0.72]. This study underscores the potential of fine-tuned LLMs, particularly when combined with strategic sequence serialization, in advancing clinical prediction tasks.",
  "full_text": null,
  "topic": "Audit",
  "concepts": [
    {
      "name": "Audit",
      "score": 0.7421345710754395
    },
    {
      "name": "Workflow",
      "score": 0.5952582359313965
    },
    {
      "name": "Serialization",
      "score": 0.5641341209411621
    },
    {
      "name": "Health records",
      "score": 0.45996978878974915
    },
    {
      "name": "Electronic health record",
      "score": 0.44491779804229736
    },
    {
      "name": "Computer science",
      "score": 0.4364524483680725
    },
    {
      "name": "Medical record",
      "score": 0.42168089747428894
    },
    {
      "name": "Health care",
      "score": 0.40874385833740234
    },
    {
      "name": "Medical emergency",
      "score": 0.36684179306030273
    },
    {
      "name": "Medicine",
      "score": 0.36540424823760986
    },
    {
      "name": "Business",
      "score": 0.24020376801490784
    },
    {
      "name": "Database",
      "score": 0.21978804469108582
    },
    {
      "name": "Internal medicine",
      "score": 0.1515870988368988
    },
    {
      "name": "Accounting",
      "score": 0.12884357571601868
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}