{
    "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
    "url": "https://openalex.org/W3154596443",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2268987810",
            "name": "Akbari, Hassan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4200861171",
            "name": "Yuan, Liangzhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2136113646",
            "name": "Qian Rui",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Chuang, Wei-Hong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221774442",
            "name": "Chang, Shih-Fu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096630757",
            "name": "Cui, Yin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2743107656",
            "name": "Gong, Boqing",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2944828972",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2326925005",
        "https://openalex.org/W2593179621",
        "https://openalex.org/W2964233791",
        "https://openalex.org/W2550462002",
        "https://openalex.org/W3145450063",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W3147387781",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W2887051120",
        "https://openalex.org/W3010874390",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W2991151250",
        "https://openalex.org/W2770804203",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W2984287396",
        "https://openalex.org/W2996086858",
        "https://openalex.org/W2979579363",
        "https://openalex.org/W3173621652",
        "https://openalex.org/W2948242301",
        "https://openalex.org/W2963814513",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W2116435618",
        "https://openalex.org/W2990408345",
        "https://openalex.org/W2962742544",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W3145385912",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2962711930",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W2990152177",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2946957821",
        "https://openalex.org/W2126579184",
        "https://openalex.org/W2295739661",
        "https://openalex.org/W2783139164",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2052666245",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W3034572008",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2593116425",
        "https://openalex.org/W3100177202",
        "https://openalex.org/W2529337537",
        "https://openalex.org/W3009812836",
        "https://openalex.org/W2964037671",
        "https://openalex.org/W2963180316",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2507009361",
        "https://openalex.org/W2948048211",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W2884797191",
        "https://openalex.org/W24089286",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3006320872",
        "https://openalex.org/W2784025607",
        "https://openalex.org/W2963495494",
        "https://openalex.org/W3108655343",
        "https://openalex.org/W2964048159",
        "https://openalex.org/W2619697695",
        "https://openalex.org/W3035635319",
        "https://openalex.org/W2798991696",
        "https://openalex.org/W343636949",
        "https://openalex.org/W3047740097",
        "https://openalex.org/W3034978746",
        "https://openalex.org/W2990503944",
        "https://openalex.org/W2321533354",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963631366",
        "https://openalex.org/W3020933450",
        "https://openalex.org/W3137120824",
        "https://openalex.org/W3035118106",
        "https://openalex.org/W2401640538",
        "https://openalex.org/W2883429621",
        "https://openalex.org/W3095121901",
        "https://openalex.org/W2963420272",
        "https://openalex.org/W3034215340",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2963115079",
        "https://openalex.org/W3010094231",
        "https://openalex.org/W2964891022",
        "https://openalex.org/W2975357369",
        "https://openalex.org/W3048918001",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2889614720",
        "https://openalex.org/W3151935374",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2988396473",
        "https://openalex.org/W3109304426",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2425121537",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W3036559109",
        "https://openalex.org/W3094550259",
        "https://openalex.org/W2984008963",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2462831000"
    ],
    "abstract": "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.",
    "full_text": "V ATT: Transformers for Multimodal Self-Supervised\nLearning from Raw Video, Audio and Text\nHassan Akbari∗\nColumbia University\nha2436@columbia.edu\nLiangzhe Yuan\nGoogle\nlzyuan@google.com\nRui Qian∗\nCornell University\nrq49@cornell.edu\nWei-Hong Chuang\nGoogle\nwhchuang@google.com\nShih-Fu Chang\nColumbia University\nsc250@columbia.edu\nYin Cui\nGoogle\nyincui@google.com\nBoqing Gong\nGoogle\nbgong@google.com\nAbstract\nWe present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Speciﬁcally, our Video-\nAudio-Text Transformer (V ATT) takes raw signals as inputs and extracts multi-\nmodal representations that are rich enough to beneﬁt a variety of downstream tasks.\nWe train V ATT end-to-end from scratch using multimodal contrastive losses and\nevaluate its performance by the downstream tasks of video action recognition, audio\nevent classiﬁcation, image classiﬁcation, and text-to-video retrieval. Furthermore,\nwe study a modality-agnostic, single-backbone Transformer by sharing weights\namong the three modalities. We show that the convolution-free V ATT outperforms\nstate-of-the-art ConvNet-based architectures in the downstream tasks. Especially,\nV ATT’s vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400,\n83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time,\nnew records while avoiding supervised pre-training. Transferring to image classiﬁ-\ncation leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training\nthe same Transformer from scratch, showing the generalizability of our model\ndespite the domain gap between videos and images. V ATT’s audio Transformer\nalso sets a new record on waveform-based audio event recognition by achieving the\nmAP of 39.4% on AudioSet without any supervised pre-training. V ATT’s source\ncode is publicly available.2\n1 Introduction\nConvolutional neural networks (CNNs) [53, 51] have triumphed over various computer vision tasks.\nThe inductive bias induced by convolutions, namely translation invariance and locality, are proven\neffective for the visual data. In the meantime, however, we witness in the natural language processing\n(NLP) community a paradigm shift from the models with strong inductive biases, such as recurrent\nneural networks [43, 7] and CNNs [104, 32], to more general architectures constructed upon self-\nattention. Particularly, Transformers [ 88] have become the de facto model architecture for NLP\n∗Work done during an internship at Google.\n2https://github.com/google-research/google-research/tree/master/vatt\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2104.11178v3  [cs.CV]  7 Dec 2021\nNCE\nloss\n“Sled dogs running on the \nsnow pulling the sled.”\nInput Video\n Input Audio Waveform Input Text\nLinear Projection \n(3D RGB voxels)\nLinear Projection \n(1D waveform)\nLinear Projection \n(1-hot word vectors)\nTransformer Encoder\nModality-Specific   OR    Modality-Agnostic\n···Extra Learnable \n[AGG] Embedding\nModality-Specific Patch + Position Embedding\nMultimodal Projection Head\n+\nMulti-Head \nAttention\nNorm\n+\nNorm\nMLP\nEmbedding\nL ✕\nTransformer EncoderVATT Multimodal\nProjection Head\nvideo \nfeature\naudio \nfeature\ntext \nfeature\nMIL-NCE\nloss\nFigure 1: Overview of the V ATT architecture and the self-supervised, multimodal learning\nstrategy. V ATT linearly projects each modality into a feature vector and feeds it into a Transformer\nencoder. We deﬁne a semantically hierarchical common space to account for the granularity of\ndifferent modalities and employ the Noise Contrastive Estimation (NCE) to train the model.\ntasks [23, 70, 71, 10]. Pre-training a Transformer on large text corpora followed by ﬁne-tuning gives\nrise to state-of-the-art results for different downstream tasks.\nIn view of the success of the attention mechanism in NLP, there has been a rich line of works exploring\nits potential in computer vision. Early work studied hybrid models consisting of both convolutions and\nattention modules [89, 94, 36, 105]. Recent studies showed that convolution-free, specially designed\nall-attention models can match CNNs’ performance on image recognition tasks [106, 44, 73]. Most\nrecently, [ 25] achieved impressive performance on several image recognition tasks, including\nImageNet [22], using a pre-trained Transformer with minimal architecture changes. Their work\ndelivered a compelling message that “large scale (supervised) training trumps inductive bias (for\nimage classiﬁcation).” This conclusion was further extended to video recognition tasks by [9, 5].\nHowever, the large-scale supervised training of Transformers is essentially troubling for two main\nreasons. First, it rules out the much larger other part of “big visual data,” i.e, the vast amount of\nunlabeled, unstructured visual data. As a result, the supervised training strategy could produce\nbiased systems that require even more labeled data to correct their biases. Second, this strategy\nfundamentally limits the application scope of Transformers in computer vision because it is costly\nand extremely time-consuming to collect enough labeled images or videos for training the millions of\nparameters, choosing hyper-parameters, and validating their expected generalization.\nHence, this work poses another pressing question about the Transformers that take raw signals as\ninput. How to empower them with large-scale, unlabeled visual data? To answer this question, we\ndraw insights from NLP. BERT [23] and GPT [70, 71, 10] use masked language modeling as their\npre-training tasks. Natural languages are organic supervision for Transformers. They sequentially\nplace words, phrases, and sentences into context, granting them semantics and syntax. For visual\ndata, the most organic supervision is arguably the multimodal videos. They are abundantly available\nin the digital world, and their temporal, cross-modality regulation, and therefore supervision, requires\nno human annotation. The extreme scale of multimodal videos is potentially capable to teach\nTransformers necessary priors, as opposed to predeﬁned inductive biases, to model the visual world.\nTo this end, we study self-supervised, multimodal pre-training of three Transformers [ 88], which\ntake as input the raw RGB frames of internet videos, audio waveforms, and text transcripts of the\nspeech audio, respectively. We call the video, audio, text Transformers V ATT. Figure 1 illustrates the\narchitecture. V ATT borrows the exact architecture from BERT [23] and ViT [25] except the layer of\ntokenization and linear projection reserved for each modality separately. This design shares the same\nspirit as ViT that we make the minimal changes to the architecture so that the learned model can\ntransfer its weights to various frameworks and tasks. Furthermore, the self-supervised, multimodal\nlearning strategy resonates the spirit of BERT and GPT that the pre-training requires minimal human\ncurated labels.\nWe evaluate the pre-trained Transformers on a variety of downstream tasks: image classiﬁcation,\nvideo action recognition, audio event classiﬁcation, and zero-shot text-to-video retrieval. Fine-tuning\n2\nthe vision-modality Transformer on ImageNet [22] obtains the top-1 accuracy of 78.7%, which is\ncomparable to 79.9% achieved by ViT. This result is especially appealing considering the domain\ngap between videos and images, and that ViT is pre-trained using a large-scale, human-curated image\ndataset. Furthermore, we set new records on Kinetics-400 [ 14], Kinetics-600 [ 15], Moments in\nTime [61], and AudioSet [33] without supervised pre-training.\nOur V ATT results, along with others reported for NLP tasks [23, 10], image recognition [25], semantic\nsegmentation [108], point cloud classiﬁcation [107], and action recoginition [9], demonstrate that\nTransformer is a versatile general-purpose architecture for different types of data.\nTo move one step forward, we challenge the Transformers in V ATT by a seemingly too strong\nconstraint: sharing weights among the video, audio, and text modalities. The idea is to test whether\nthere exists a single, general-purpose model for all the modalities — of course, they still have\ntheir own layers of tokenization and linear projection. Preliminary results are encouraging. This\nmodality-agnostic Transformer is on par with three modality-speciﬁc ones of slightly smaller sizes.\nFinally, another contribution of this work is DropToken, a simple and yet effective technique to reduce\nthe training complexity with a minor reduction of the end Transformers’ performance. DropToken\nrandomly drops a portion of the video and audio tokens from each input sequence during training, al-\nlowing for high-resolution inputs and leveraging their abundance. This is signiﬁcant for Transformers\nbecause their computational complexity is quadratic with respect to the number of input tokens.\n2 Related work\n2.1 Transformers in Vision\nTransformer was originally built for NLP tasks [88] and the design of multi-head attention shows its\neffectiveness on modeling long-term correlation of words. A few attempts have been made to use\nTransformer for vision tasks like image super-resolution [99], object detection [11] and multimodal\nvideo understanding [84, 19, 57]. However these methods still rely on the feature extracted by CNNs.\nRecently, [25] proposes a set of convolution-free vision Transformers which directly work on raw\nimages and obtain competitive performance with CNNs. [86] improves the training data efﬁciency\nof [25] by using stronger data augmentations and knowledge distillation. Since then, the pure\nTransformer design has been adopted to various vision tasks including semantic segmentation [108],\npoint cloud classiﬁcation [ 107], action recoginition [9, 78, 5]. To the best of our knowledge, our\nV ATT is the ﬁrst Transformer model on raw multimodal inputs of video, audio and text.\n2.2 Self-Supervised Learning\nSingle vision modality. Early work of self-supervised visual representation learning usually learns\nfrom unlabeled images via manually speciﬁed pretext tasks, like auto-encoding [ 64, 102, 103],\npatch location prediction [24], solving jigsaw puzzles [63], and image rotation prediction [35]. [95]\npropose a novel instance discrimination objective. The recent trend of contrastive learning [40, 17,\n100, 37, 41, 85] integrates data augmentations and instance discrimination by maintaining relative\nconsistency between representations of an image and its augmented view. Clustering can also provide\nan effective addition [12]. Recently, [ 18] conduct contrastive learning using ViT [25] and achieve\nimpressive results. As for the video domain, it is natural to exploit the temporal signals as the pretext\ntask. Examples include predicting the future frame [ 82], motion and appearance statistics [ 90],\nspeed [8, 91] and encodings [56, 38, 39], sorting frames or video clips [ 54, 97, 45, 31]. Recently,\n[68] apply contrastive learning to videos with a temporal sampling strategy and temporally consistent\nspatial augmentation.\nMultimodal video. Video is a natural source of multimodal data. Multimodal self-supervised\nlearning can be achieved by predicting whether a video has correspondence with an audio stream [3,\n4, 62, 50], cross-modality clustering [2], and evolving losses [67]. Recently, [ 1] use contrastive loss\nto learn from video, audio and text; [ 74] learn to predict a broad view that spans a longer temporal\ncontext from a narrow view. V ATT serves as a ﬁrst work combining the strength of convolution-free\nTransformer and multimodal contrastive learning.\n3\n3 Approach\nIn this section, we introduce our convolution-free V ATT architecture and elaborate on the self-\nsupervised multimodal objectives for training V ATT from scratch.\nFigure 1 is an overview of the architecture. We feed each modality to a tokenization layer, where\nthe raw input is projected to an embedding vector followed by a Transformer. There are two major\nsettings: 1) The backbone Transformers are separate and have speciﬁc weights for each modality, and\n2) The Transformers share weights, namely, there is a single backbone Transformer applied to any of\nthe modalities. In either setting, the backbone extracts modality-speciﬁc representations, which are\nthen mapped to common spaces to be compared with each other by contrastive losses. We describe\neach module in the following.\n3.1 Tokenization and Positional Encoding\nV ATT operates on raw signals. The vision-modality input consists of 3-channel RGB pixels of video\nframes, the audio input is in the form of air density amplitudes (waveforms), and the text input is a\nsequence of words. We ﬁrst deﬁne a modality-speciﬁc tokenization layer that takes as input the raw\nsignals and returns a sequence of vectors to be fed to the Transformers. Besides, each modality has\nits own positional encoding, which injects the order of tokens into Transformers [88]. We partition\nan entire video clip of size T ×H×W to a sequence of ⌈T/t⌉·⌈H/h⌉·⌈W/w⌉patches, where\neach patch contains t×h×w×3 voxels. We apply a linear projection on the entire voxels in\neach patch to get a d-dimensional vector representation. This projection is performed by a learnable\nweight Wvp ∈Rt·h·w·3×d. This can be seen as a 3D extension of the patching mechanism proposed\nin [25]. To encode the position of these patches, we deﬁne a dimension-speciﬁc sequence of learnable\nembeddings as follows:\nei,j,k = eTemporali+eHorizontalj + eVerticalk,\nETemporal ∈R⌈T/t⌉×d, EHorizontal ∈R⌈H/h⌉×d, EVertical ∈R⌈W/w⌉×d (1)\nwhere ei is the i-th row of E. This scheme allows us to use ⌈T/t⌉+ ⌈H/h⌉+ ⌈W/w⌉positional\nembeddings to encode all the ⌈T/t⌉·⌈ H/h⌉·⌈ W/w⌉patches in a video clip. The raw audio\nwaveform is a 1D input with length T′, and we partition it to ⌈T′/t′⌉segments each containing\nt′waveform amplitudes. Similar to video, we apply a linear projection with a learnable weight\nWap ∈Rt′×d to all elements in a patch to get a d-dimensional vector representation. We use ⌈T′/t′⌉\nlearnable embeddings to encode the position of each waveform segment. For text, we ﬁrst construct a\nvocabulary of size vout of all words in our training dataset. For an input text sequence, we then map\neach word to a v-dimensional one-hot vector followed by a linear projection with a learnable weight\nWtp ∈Rv×d. This is equivalent to an embedding dictionary lookup, which has been widely used in\nnatural language understanding [60].\n3.1.1 DropToken\nWe introduce DropToken, a simple and yet effective strategy to reduce the computational complexity\nduring training. Once we get the token sequence for the video or audio modality, we randomly sample\na portion of the tokens and then feed the sampled sequence, not the complete set of tokens, to the\nTransformer. This is crucial for reducing the computational cost because a Transformer’s computation\ncomplexity is quadratic, O(N2), where N is number of tokens in the input sequence. Any effort on\nreducing the input length would reduce the number of FLOPs quadratically. This has an immediate\nimpact on the wall clock time for training these models and makes it possible to host large models in\nlimited hardware. We argue that instead of reducing the resolution or dimension of the raw inputs, it\nis better to keep a high-ﬁdelity input and randomly sample the tokens via DropToken. DropToken is\nappealing especially with the raw video and audio inputs, which may contain high redundancies.\n3.2 The Transformer Architecture\nFor simplicity, we adopt the most established Transformer architecture [23], which has been widely\nused in NLP. Similar to ViT [25], we do not tweak the architecture so that our weights can be easily\ntransferred to any standard Transformer implementation. We will brieﬂy elaborate on the pipeline\n(also illustrated in Figure 1 middle panel) and refer the reader to [ 25, 23] for more details of the\n4\nstandard Transformer architecture. The sequence of input tokens to the Transformer follows the\nbelow formulation:\nzin = [xAGG; x0WP ; x1WP ; ... ; xN WP ] +ePOS (2)\nwhere xn is the input patches sequence and xAGG is the learnable embedding of a special aggregation\ntoken whose corresponding output in the Transformer (z0\nout) is used as the aggregated representation\nfor the entire input sequence. This will be later used for classiﬁcation and common space mapping.\nWe use a standard self-attention [88] as the Multi-Head-Attention (MHA) module, and GeLU [42]\nas the activation in the MLP layer. We also use Layer Normalization [6] before the MHA and MLP\nmodules. In our text model, we remove the position encoding ePOS and add a learnable relative bias\nto each attention score of the ﬁrst layer in the MHA module. This simple change makes our text\nmodel’s weights directly transferable to the state-of-the-art text model T5 [72].\n3.3 Common Space Projection\nWe use common space projection and contrastive learning in that common space to train our networks.\nMore speciﬁcally, given a video-audio-text triplet, we deﬁne a semantically hierarchical common\nspace mapping that enables us to directly compare video-audio pairs as well as video-text pairs by the\ncosine similarity. As argued in [1], such comparison is more feasible if we assume there are different\nlevels of semantic granularity for these modalities. To achieve this, we deﬁne multi-level projections\nas follows:\nzv,va = gv→va(zvideo\nout ), za,va = ga→va(zaudio\nout )\nzt,vt = gt→vt(ztext\nout ), zv,vt = gv→vt(zv,va) (3)\nwhere gv→va and ga→va are the projection heads to respectively map the video and audio Trans-\nformers’ outputs to the video-audio common spaceSva. Moreover, gt→vt and gv→vt project the text\nTransformer’s outputs and the video embedding in theSva space to video-text common space, Svt.\nThis multi-level common space projection is depicted in Figure 1 (the rightmost panel). The main\nintuition behind this hierarchy is that different modalities have different levels of semantic granularity,\nso we should impose this as an inductive bias in the common space projection. Similar to [1], we use\na linear projection for ga→va(.), gt→vt(.), and gv→vt(.), and a two-layer projection with ReLU in\nbetween for gv→va(.). To ease the training, a batch normalization is used after each linear layer.\n3.4 Multimodal Contrastive Learning\nInspired by [1, 3, 59], we use Noise Contrastive Estimation (NCE) to align video-audio pairs and\nMultiple Instance Learning NCE (MIL-NCE) to align video-text pairs. The pairs are composed from\ndifferent temporal locations in the video-audio-text stream. Positive pairs from two modalities are\nconstructed by sampling their corresponding streams from the same location in the video, and negative\npairs are constructed by sampling from any non-matching locations in the video [ 1]. Concretely,\ngiven the common space speciﬁed in Section 3, the loss objectives can be written as follows:\nNCE(zv,va,za,va) =−log\n(\nexp(z⊤\nv,vaza,va/τ)\nexp(z⊤v,vaza,va/τ) +∑\nz′∈Nexp(z′⊤\nv,vaz′a,va/τ)\n)\n, (4)\nMIL-NCE(zv,vt,{zt,vt}) =−log\n( ∑\nzt,vt∈Pexp(z⊤\nv,vtzt,vt/τ)\n∑\nzt,vt∈Pexp(z⊤\nv,vtzt,vt/τ) +∑\nz′∈Nexp(z′⊤\nv,vtz′t,vt/τ)\n)\n,\n(5)\nwhere Ncontains all non-matching pairs in a batch. In Equation 5, Pcontains ﬁve text clips that are\nnearest neighbors to the video clip in time. τ is a temperature to adjust the softness of the objectives\nin distinguishing the positive pairs from the negative pairs.\nThe overall per-sample objective for training the entire V ATT model end-to-end is as follows:\nL= NCE(zv,va,za,va) +λMIL-NCE(zv,vt,{zt,vt}), (6)\nwhere λbalances the two losses. The model is optimized based on the back-propagation of the\naverage loss calculated over a batch of samples.\n5\n4 Experiments\nIn this section, we ﬁrst brieﬂy describe the experimental setup for the pre-training and downstream\nevaluation, and then present the results and analytic interpretation of V ATT in different tasks. We\nrefer the reader to the Appendix for a more detailed description of all experimental settings.\n4.1 Experimental Setup\nPre-train: we use a combination of AudioSet [ 33] and HowTo100M [ 58] datasets to pre-train\nV ATT— we use only a subset of the HowTo100M dataset in compliance with Youtube’s policies.\nFollowing [1], we use video-audio-text triplets from HowTo100M clips while only using video-audio\npairs from AudioSet. We sample 32 frames at 10 fps with a spatial size of 224 ×224 following a\nrandom crop, horizontal ﬂip and color augmentation (details in A.2.1). Accordingly, we sample audio\nwaveforms in sync at 48kHz. Both video and audio are normalized between [-1,1]. We use patch\nsizes of 4 ×16 ×16 and 128 for video and raw waveform tokenization, respectively (ablation in A.5).\nWe use one-hot vectors to encode text sequences (capped to 16 tokens) with the vocabulary size of\n216. In all pre-training experiments, we use DropToken with drop rate 50%. We train our models\nusing the Adam optimizer [46] with a quarter-period cosine scheduled learning rate from 1e-4 to 5e-5\nand 10k warmup steps. Optimization is performed on totally 500k steps with batch size 2048 (512\nin exploration experiments). Following the previously established practice [1] for the projection to\nthe common spaces Sva and Svt, we use dva = 512and dvt = 256. We also use the temperature of\nτ = 0.07 and the weight ofλ= 1in the loss in Equation 6. We use 4 network sizes in our experiments\n(details in A.2.2). We use the Medium model (155M parameters) for our modality-agnostic variant\n(V ATT-MA), and 3 variants for the modality-speciﬁc video-audio-text backbones: Base-Base-Small\n(BBS; 197M), Medium-Base-Small (MBS; 264M), and Large-Base-Small (LBS; 415M). Pre-training\nan MBS V ATT with batch size 2048 on 256 TPUs (v3) takes less than 3 days. Pre-training with batch\nsize 512 takes less than 1 day.\nDownstream: we evaluate the pre-trained V ATT models on 4 major downstream tasks using a total\nof 10 datasets. We use UCF101 [ 81], HMDB51 [ 52], Kinetics-400 [ 14], Kinetics-600 [ 15], and\nMoments in Time [61] for video action recognition. We use ESC50 [66] and AudioSet [33] for audio\nevent classiﬁcation, and we evaluate the quality of our video-text common space representations\nby zero-shot text-to-video retrieval on YouCook2 [109] and MSR-VTT [98]. Finally, we evaluate\nthe transferability of the vision backbone by ﬁne-tuning it on ImageNet classiﬁcation [ 22]. Since\nHMDB51, UCF101, and ESC50 are very small datasets compared to the size of our networks, we\nonly use them to train a linear classiﬁer on top of the frozen pre-trained backbones. In our exploration\nexperiments, we report linear classiﬁcation accuracy and zero-shot video retrieval metrics. We refer\nto the Appendix for a detailed description of the datasets and the experimental setup.\n4.2 Results\n4.2.1 Fine-tuning for video action recognition\nWe ﬁne-tune V ATT’s vision Transformer on Kinetics-400, Kinetics-600, and Moments in Time, three\nof the arguably most established large-scale datasets for video action recognition. We use the ﬁnal\ncheckpoints of four pre-train settings for these experiments: three modality-speciﬁc variations (LBS,\nMBS, BBS), and one modality-agnostic ( Medium). Table 1 shows the results compared with the\nstate-of-the-art video models. On all three datasets, we achieve higher accuracy than previous works\nincluding TimeSFormer [9], a recent effort in ﬁne-tuning the ViT checkpoints obtained by supervised\npre-training. In contrast, our pre-training does not rely on any labels curated by humans. To the best\nof our knowledge, V ATT provides the ﬁrst vision Transformer backbone that is pre-trained from\nscratch using self-supervision on multimodal videos and achieves state-of-the-art results on video\naction recognition. It is also worth mentioning that ﬁne-tuning V ATT on the most recent Kinetics-700\ndataset results in a top-1 accuracy of 72.7%, which outperforms the state-of-the-art top-1 accuracy of\n72.4% in [47].\nTo further quantify how much the multimodal self-supervised pre-training helps in achieving these\nnumbers, we train a variant from scratch without any pre-training and observe the top-1 and top-5\naccuracies of 26.4% and 51.8% on Kinetics-400, respectively. The low accuracies verify the efﬁcacy\nof our pre-training strategy for V ATT. Finally, we ﬁnd that V ATT-MA-Medium, the modality-agnostic\n6\nKinetics-400 Kinetics-600 Moments in Time\nMETHOD TOP-1 T OP-5 T OP-1 T OP-5 T OP-1 T OP-5 TFLOP S\nI3D [13] 71.1 89.3 71.9 90.1 29.5 56.1 -\nR(2+1)D [26] 72.0 90.0 - - - - 17.5\nbLVNet [27] 73.5 91.2 - - 31.4 59.3 0.84\nS3D-G [96] 74.7 93.4 - - - - -\nOct-I3D+NL [20] 75.7 - 76.0 - - - 0.84\nD3D [83] 75.9 - 77.9 - - - -\nI3D+NL [93] 77.7 93.3 - - - - 10.8\nip-CSN-152 [87] 77.8 92.8 - - - - 3.3\nAttentionNAS [92] - - 79.8 94.4 32.5 60.3 1.0\nAssembleNet-101 [77] - - - - 34.3 62.7 -\nMoViNet-A5 [47] 78.2 - 82.7 - 39.1 - 0.29\nLGD-3D-101 [69] 79.4 94.4 81.5 95.6 - - -\nSlowFast-R101-NL [30] 79.8 93.9 81.8 95.1 - - 7.0\nX3D-XL [29] 79.1 93.9 81.9 95.5 - - 1.5\nX3D-XXL [29] 80.4 94.6 - - - - 5.8\nTimeSFormer-L [9] 80.7 94.7 82.2 95.6 - - 7.14\nV ATT-Base 79.6 94.9 80.5 95.5 38.7 67.5 9.09\nV ATT-Medium 81.1 95.6 82.4 96.1 39.5 68.2 15.02\nV ATT-Large 82.1 95.5 83.6 96.6 41.1 67.7 29.80\nV ATT-MA-Medium 79.9 94.9 80.8 95.5 37.8 65.9 15.02\nTable 1: Video action recognition accuracy on Kinetics-400, Kinetics-600, and Moments in Time.\nbackbone shared by the video, audio, and text modalities, is on par with the modality-speciﬁc V ATT-\nBase when ﬁne-tuned for the video action recognition. This result is encouraging as it indicates the\npotential of unifying three data modalities by a single Transformer backbone.\n4.2.2 Fine-tuning for audio event classiﬁcation\nWe ﬁne-tune V ATT’s audio Transformer on AudioSet, which benchmarks the task of multi-label audio\nevent classiﬁcation. We use the ﬁnal checkpoints of two pre-train settings: one modality-speciﬁc\n(BBS), and one modality-agnostic (Medium). Table 2 shows the results compared to state-of-the-art\nmodels. Following common practice [34, 48], we report mean Average Precision (mAP), Area Under\nCurve (AUC), and d-prime (based on AUC) [34]. Our audio Transformer consistently outperforms\nthe existing CNN-based models in all metrics. More interestingly, ﬁne-tuning the modality-agnostic\nbackbone (V ATT-MA-Medium) is on par with ﬁne-tuning the modality-speciﬁc one (V ATT-Base).\nTo the best of our knowledge, V ATT is the ﬁrst Transformer that outperforms CNN-based models\nin audio event recognition. V ATT operates on raw waveforms and does not utilize any handcrafted\nfeatures.\n4.2.3 Fine-tuning for image classiﬁcation\nIn this section, we show that our pipeline is capable of transferring the learned knowledge into another\ndomain by performing the image classiﬁcation task, even though the models are pre-trained in the\nmultimodal video domain. We ﬁne-tune the vision Transformer in V ATT-BBS on ImageNet without\nany modiﬁcation to the backbone architecture. Instead, to satisfy the voxel-to-patch layer we replicate\nthe input image 4 times and feed it to the network. The network sees the input as a single-frame\nvideo clip and performs spatial self-attention. Table 3 shows the results for ﬁne-tuning the vision\nTransformer end-to-end on ImageNet. We can see that our pre-training leads to a signiﬁcant boost in\nthe accuracy compared to training from scratch. We also observe that even though the self-supervised\npre-training happens in the video domain, we still achieve competitive results to the supervised\npre-training using large-scale image data [25].\n4.2.4 Zero-shot text-to-video retrieval\nWe feed video-text pairs to V ATT-MBS, and extract representations in the Svt space. We then\ncalculate the similarity between each video-text pair from YouCook2 and MSR-VTT. Given a text\nquery, we rank the videos based on their similarities to the text. We then measure the recall for the\n7\nMETHOD mAP AUC d-prime\nDaiNet [21] 29.5 95.8 2.437\nLeeNet11 [55] 26.6 95.3 2.371\nLeeNet24 [55] 33.6 96.3 2.525\nRes1dNet31 [49] 36.5 95.8 2.444\nRes1dNet51 [49] 35.5 94.8 2.295\nWavegram-CNN [49] 38.9 96.8 2.612\nV ATT-Base 39.4 97.1 2.895\nV ATT-MA-Medium 39.3 97.0 2.884\nTable 2: Finetuning results for AudioSet\nevent classiﬁcation.\nMETHOD PRE-TRAINING DATA TOP-1 T OP-5\niGPT-L [16] ImageNet 72.6 -\nViT-Base [25] JFT 79.9 -\nV ATT-Base - 64.7 83.9\nV ATT-Base HowTo100M 78.7 93.9\nTable 3: Finetuning results for ImageNet classiﬁcation.\nYouCook2 MSR-VTT\nMETHOD BATCH EPOCH R@10 MedR R@10 MedR\nMIL-NCE [59] 8192 27 51.2 10 32.4 30\nMMV [1] 4096 8 45.4 13 31.1 38\nV ATT-MBS 2048 4 45.5 13 29.7 49\nV ATT-MA-Medium 2048 4 40.6 17 23.6 67\nTable 4: Zero-shot text-to-video retrieval.\ncorrect video in the top-10 videos. We also measure the median of the rank of the correct video.\nTable 4 compares our video retrieval results to two baselines. In our experiments we observe that the\nzero-shot retrieval results are heavily affected by the batch size and number of epochs, conﬁrming the\nobservation made in [1]. That said, our model still delivers comparable results to MMV [1] while\nbeing pre-trained with a half number of epochs and a half batch size of theirs. We also experiment\nwith a larger batch size 8192 and longer pre-training for 6 epochs, arriving at exactly the same results\nas MIL-NCE [59] on YouCook2 and the R@10 of 29.2 and MedR of 42 on MSR-VTT. We also\nnotice that, probably due to the noisy nature of text transcripts, a sophisticated language model like\nours is underrated. As shown in [1], using a simple linear projection would still perform reasonably\nwell. It is worth exploring other, higher-quality text sources in future work.\n4.2.5 Feature visualization\nWe take our modality-speciﬁc and modality-agnostic V ATT ﬁne-tuned on Kinetics-400 and visual-\nize their output feature representations using t-SNE. For comparison, we also include the feature\nvisualization of the vision Transformer trained from scratch on Kinetics-400. From Figure 2, we\nobserve that the ﬁne-tuned V ATT yields a much better separation than the model trained from scratch.\nFurthermore, it is worth noting that there is no clear difference between the modality-agnostic features\nand the modality-speciﬁc ones.\nWe further investigate the V ATT backbones without any ﬁne-tuning. We randomly choose 1k video\nclips from the YouCook2 dataset and store the representations from two points of a pre-trained V ATT\nmodel. One is after the tokenization layer (input space of the Transformer), and the other is after the\ncommon space projection (output space), where the loss is computed. Figure 3-top visualizes the\nrepresentations, comparing modality-speciﬁc V ATT to modality-agnostic V ATT. Interestingly, we\nobserve that the representations are slightly more mixed together in the modality-agnostic setting\ncompared to the modality-speciﬁc ones, implying that the modality-agnostic backbone sees different\nmodalities as different symbols describing the same concept. This is analogous to a uniﬁed language\nmodel in NLP that supports multiple languages.\nTo see how well V ATT distinguishes positive video-text pairs from randomly sampled pairs, we\ncalculate pair-wise similarities for all possible pairs and perform a Kernel Density Estimation (KDE)\nto visualize the distributions of the similarities of the positive pairs vs. negative pairs. We perform this\nprocedure for both input and output spaces of the modality-speciﬁc and modality-agnostic backbones.\nFigure 3-bottom shows the KDE curves of these similarities. We can see that V ATT in both settings\nseparates the positive and negative pairs in its output space. This veriﬁes V ATT’s efﬁcacy in learning\na semantic common space for different modalities, even if we share the backbone across modalities.\n4.2.6 Model Activations\nWe measure the average activation of the modality-agnostic V ATT when a full multimodal input is fed\nto the model. More speciﬁcally, we sample 100k short video clips from the test split of HowTo100M\nalong with their corresponding audio and text and feed them to the model separately. For each\n8\nt-SNE-1\n8\n 6\n 4\n 2\n 0 2 4 6 8\nt-SNE-2\n6\n4\n2\n0\n2\n4\n6\nt-SNE-3\n8\n6\n4\n2\n0\n2\n4\n6\n8\nFrom Scratch\nt-SNE-1\n8\n 6\n 4\n 2\n 0 2 4 6 8\nt-SNE-2\n6\n4\n2\n0\n2\n4\n6\nt-SNE-3\n8\n6\n4\n2\n0\n2\n4\n6\n8\nModality-Specific\nt-SNE-1\n8\n 6\n 4\n 2\n 0 2 4 6 8\nt-SNE-2\n6\n4\n2\n0\n2\n4\n6\nt-SNE-3\n8\n6\n4\n2\n0\n2\n4\n6\n8\nModality-Agnostic\nFigure 2: t-SNE visualization of the feature representations extracted by the vision Transformer in\ndifferent training settings. For better visualization, we show 100 random classes from Kinetics-400.\n15\n 10\n 5\n 0 5 10\nt-SNE-1\n15\n10\n5\n0\n5\n10\n15\nt-SNE-2\nInput / M-Specific\nModalities\nVideo\nText\n15\n 10\n 5\n 0 5 10 15\nt-SNE-1\n15\n10\n5\n0\n5\n10\n15\nt-SNE-2\nOutput / M-Specific\nModalities\nVideo\nText\n10\n 5\n 0 5 10\nt-SNE-1\n15\n10\n5\n0\n5\n10\n15\nt-SNE-2\nInput / M-Agnostic\nModalities\nVideo\nText\n15\n 10\n 5\n 0 5 10 15\nt-SNE-1\n15\n10\n5\n0\n5\n10\nt-SNE-2\nOutput / M-Agnostic\nModalities\nVideo\nText\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0\nSimilarities\n0.0\n0.5\n1.0\n1.5\n2.0\nDensity\nPairs\nPositive\nNegative\n0.0 0.2 0.4 0.6 0.8 1.0\nSimilarities\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nDensity\nPairs\nPositive\nNegative\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0\nSimilarities\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nDensity\nPairs\nPositive\nNegative\n0.0 0.2 0.4 0.6 0.8 1.0\nSimilarities\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nDensity\nPairs\nPositive\nNegative\nFigure 3: t-SNE visualization and distribution of pair-wise similarities of the input space vs. output\nspace for modality-speciﬁc and modality-agnostic backbones when different modalities are fed.\nmodality, we calculate the average activation of each node at the output of the MLP module, before\nthe residual addition (Figure 1-Transformer Encoder). Figure 4 shows the average activations across\nall nodes in a Medium-size model. We observe that earlier nodes in the model are activated with the\ntext inputs, while the middle-to-later nodes are activated with video and audio modalities. However,\nthe nodes in the last layers of the network are activated with all modalities almost equally. This might\nsuggest that the model allocates different nodes to certain modalities while reaching the same level of\nsemantic perception for all modalities in the later layers. Such observation encourages further studies\non the possibility of utilizing Mixture-of-Experts [79, 28, 76] to increase the model’s capacity for\nsimultaneous multimodal perception. We leave this direction of research for future work.\n4.2.7 Effect of DropToken\nWe introduced a new method to reduce the redundancy in high-resolution data. To study the effect\nof the proposed DropToken method on downstream applications and the pre-training computation,\nwe perform pre-training by randomly dropping 75%, 50%, 25%, and 0% (no drop) of the tokens\nfrom the video and audio inputs. Table 5 shows the accuracy of linear classiﬁcation on HMDB51,\nUCF101, ESC50 and R@10 on YouCook2 and MSR-VTT vs. the drop rate along with GFLOPs\nduring a forward call. We choose 50% sampling rate for our large-scale pre-training as it offers a\ngood trade-off between accuracy and computational costs. We then take the ﬁnal checkpoint of the\npre-trained V ATT with50% DropToken rate and perform ﬁne-tuning on Kinetics-400 at different\nDropToken rates and at different spatial and temporal resolutions to see how high-resolution inputs\ncoupled with DropToken compare to low-resolution inputs with no tokens dropped during ﬁne-tuning.\nTable 6 shows the top-1 accuracy on Kinetics-400. We argue against using low-resolution inputs,\nwhich is the most common approach to reduce the computational cost during training. Instead,\nwe suggest using high-resolution inputs with DropToken, whose accuracy and training cost are\ncomparable to or better than low-resolution counterparts.\n9\nEarly Activation for Text\nLate Activation for Video & Audio\nModality-Agnostic Activation\nNode #\nInput Modalities\nVideo\nAudio\nText\nVideo\nAudio\nText\nModality\n0         2000     4000      6000     8000     10000    12000   14000   16000\nFigure 4: The average node activation across the Modality-Agnostic-Medium V ATT while feeding a\nmultimodal video-audio-text triplet to the model.\nDropToken Drop Rate\n75% 50% 25% 0%\nMultimodal GFLOPs 188.1 375.4 574.2 784.8\nHMDB51 62.5 64.8 65.6 66.4\nUCF101 84.0 85.5 87.2 87.6\nESC50 78.9 84.1 84.6 84.9\nYouCookII 17.9 20.7 24.2 23.1\nMSR-VTT 14.1 14.6 15.1 15.2\nTable 5: Top-1 accuracy of linear classiﬁca-\ntion and R@10 of video retrieval vs. drop rate\nvs. inference GFLOPs in the V ATT-MBS.\nResolution/ DropToken Drop Rate\nFLOPs 75% 50% 25% 0%\n32 × 224 × 224 - - - 79.9\nInference (GFLOPs) - - - 548.1\n64 × 224 × 224 - - - 80.8\nInference (GFLOPs) - - - 1222.1\n32 × 320 × 320 79.3 80.2 80.7 81.1\nInference (GFLOPs) 279.8 572.5 898.9 1252.3\nTable 6: Top-1 accuracy of video action recogni-\ntion on Kinetics400 using high-resolution inputs\ncoupled with DropToken vs. low-resolution inputs.\n5 Conclusion and Discussion\nIn this paper, we present a self-supervised multimodal representation learning framework based\non Transformers. Our study suggests that Transformers are effective for learning semantic\nvideo/audio/text representations — even if one model is shared across modalities — and multi-\nmodal self-supervised pre-training is promising for reducing their dependency on large-scale labeled\ndata. We show that DropToken can signiﬁcantly reduce the pre-training complexity with video and\naudio modalities and have minor impact on the models’ generalization. We report new records of\nresults on video action recognition and audio event classiﬁcation and competitive performance on\nimage classiﬁcation and video retrieval. Having these results, we still see some limitations in our\nwork. Firstly, not all videos have organic audio or speech, while our approach depends on meaningful\nmultimodal correspondences. Besides, the text modality currently consists of speech transcripts,\nwhich are noisy and sometimes sparse. Potential negative Societal Impacts are mainly concerned\nwith applications. The models could be biased if one applies our approach to the multimodal videos\nthat are not representative enough. Finally, our method is still demanding in computation, though we\nmanaged to avoid the need for human labels. Future work can improve upon these limitations.\nAcknowledgments and Disclosure of Funding\nWe would like to thank Min-Hsuan Tsai, Jean-Baptise Alayrac, Andrew Audibert, Yeqing Li, Vidush\nMukund, and the TensorFlow team for their help with codes, infrastructure, and insightful discussions.\n10\nReferences\n[1] Jean-Baptiste Alayrac, Adrià Recasens, Rosalia Schneider, Relja Arandjelovi´c, Jason Rama-\npuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-\nsupervised multimodal versatile networks. In NeurIPS, 2020. 3, 5, 6, 8, 17, 18, 19, 20\n[2] Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and\nDu Tran. Self-supervised learning by cross-modal audio-video clustering. arXiv preprint\narXiv:1911.12667, 2019. 3, 20\n[3] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In CVPR, 2017. 3, 5\n[4] Relja Arandjelovic and Andrew Zisserman. Objects that sound. In ECCV, 2018. 3\n[5] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia\nSchmid. Vivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021. 2, 3\n[6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016. 5\n[7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In ICLR, 2015. 1\n[8] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, William T Freeman, Michael Rubin-\nstein, Michal Irani, and Tali Dekel. Speednet: Learning the speediness in videos. In CVPR,\n2020. 3\n[9] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for\nvideo understanding? arXiv preprint arXiv:2102.05095, 2021. 2, 3, 6, 7\n[10] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 2, 3\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 3\n[12] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.\n3\n[13] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the\nkinetics dataset. In CVPR, 2017. 7\n[14] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the\nkinetics dataset. In CVPR, 2017. 3, 6, 17\n[15] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A\nshort note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018. 3, 6, 17\n[16] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In ICML, 2020. 8\n[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020. 3\n[18] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised\nvisual transformers. arXiv preprint arXiv:2104.02057, 2021. 3\n[19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,\nand Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020. 3\n[20] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach,\nShuicheng Yan, and Jiashi Feng. Drop an octave: Reducing spatial redundancy in convolutional\nneural networks with octave convolution. In ICCV, 2019. 7\n11\n[21] Wei Dai, Chia Dai, Shuhui Qu, Juncheng Li, and Samarjit Das. Very deep convolutional neural\nnetworks for raw waveforms. In ICASSP, 2017. 8\n[22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009. 2, 3, 6, 17\n[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL, 2019. 2, 3, 4\n[24] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning\nby context prediction. In ICCV, 2015. 3\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR, 2021. 2, 3, 4, 7, 8\n[26] Heng Wang Du Tran, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A\ncloser look at spatiotemporal convolutions for action recognition. 2018 ieee. In CVPR, 2017. 7\n[27] Quanfu Fan, Chun-Fu (Ricarhd) Chen, Hilde Kuehne, Marco Pistoia, and David Cox. More\nIs Less: Learning Efﬁcient Video Representations by Temporal Aggregation Modules. In\nNeurIPS. 2019. 7\n[28] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\nparameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021. 9\n[29] Christoph Feichtenhofer. X3d: Expanding architectures for efﬁcient video recognition. In\nCVPR, 2020. 7\n[30] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for\nvideo recognition. In ICCV, 2019. 7, 18\n[31] Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video\nrepresentation learning with odd-one-out networks. In CVPR, 2017. 3\n[32] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolu-\ntional sequence to sequence learning. In ICML, 2017. 1\n[33] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Chan-\nning Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled\ndataset for audio events. In ICASSP, 2017. 3, 6, 17\n[34] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing\nMoore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset\nfor audio events. In ICASSP, 2017. 7\n[35] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning\nby predicting image rotations. In ICLR, 2018. 3\n[36] Rohit Girdhar and Deva Ramanan. Attentional pooling for action recognition. In NeurIPS,\n2017. 2\n[37] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In\nNeurIPS, 2020. 3\n[38] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense\npredictive coding. In ICCV Workshops, 2019. 3\n[39] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-augmented dense predictive coding\nfor video representation learning. In ECCV, 2020. 3\n12\n[40] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In CVPR, 2020. 3\n[41] Olivier J Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami,\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding.\narXiv preprint arXiv:1905.09272, 2019. 3\n[42] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016. 5\n[43] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n1997. 1\n[44] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image\nrecognition. In ICCV, 2019. 2\n[45] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-supervised video representation learning\nwith space-time cubic puzzles. In AAAI, 2019. 3\n[46] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014. 6, 18\n[47] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, and\nBoqing Gong. Movinets: Mobile video networks for efﬁcient video recognition. In CVPR,\n2021. 6, 7\n[48] Qiuqiang Kong, Changsong Yu, Yong Xu, Turab Iqbal, Wenwu Wang, and Mark D Plumbley.\nWeakly labelled audioset tagging with attention neural networks. TASLP, 2019. 7\n[49] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley.\nPanns: Large-scale pretrained audio neural networks for audio pattern recognition. TASLP,\n2020. 8, 19\n[50] Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video\nmodels from self-supervised synchronization. NeurIPS, 2018. 3, 20\n[51] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In NeurIPS, 2012. 1\n[52] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for\nhuman motion recognition. In ICCV, 2011. 6, 17\n[53] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 1998. 1\n[54] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised repre-\nsentation learning by sorting sequences. In ICCV, 2017. 3\n[55] Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim, and Juhan Nam. Sample-level deep\nconvolutional neural networks for music auto-tagging using raw waveforms. arXiv preprint\narXiv:1703.01789, 2017. 8\n[56] William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video\nprediction and unsupervised learning. arXiv preprint arXiv:1605.08104, 2016. 3\n[57] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, and\nMing Zhou. Univilm: A uniﬁed video and language pre-training model for multimodal\nunderstanding and generation. arXiv preprint arXiv:2002.06353, 2020. 3\n[58] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and\nJosef Sivic. Howto100m: Learning a text-video embedding by watching hundred million\nnarrated video clips. In ICCV, 2019. 6, 17\n13\n[59] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew\nZisserman. End-to-end learning of visual representations from uncurated instructional videos.\nIn CVPR, 2020. 5, 8, 17, 20\n[60] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013. 4, 18\n[61] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal,\nTom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl V ondrick, et al. Moments in time\ndataset: one million videos for event understanding. TPAMI, 2019. 3, 6, 17\n[62] Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-visual instance discrimination\nwith cross-modal agreement. arXiv preprint arXiv:2004.12943, 2020. 3\n[63] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving\njigsaw puzzles. In ECCV, 2016. 3\n[64] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\nencoders: Feature learning by inpainting. In CVPR, 2016. 3\n[65] Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth Fong, João F Henriques, Geoffrey\nZweig, and Andrea Vedaldi. Multi-modal self-supervision from generalized data transforma-\ntions. arXiv preprint arXiv:2003.04298, 2020. 20\n[66] Karol J. Piczak. ESC: Dataset for Environmental Sound Classiﬁcation. In ACM MM, 2015. 6,\n17\n[67] AJ Piergiovanni, Anelia Angelova, and Michael S Ryoo. Evolving losses for unsupervised\nvideo representation learning. In CVPR, 2020. 3, 20\n[68] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie,\nand Yin Cui. Spatiotemporal contrastive video representation learning. In CVPR, 2021. 3\n[69] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal\nrepresentation with local and global diffusion. In CVPR, 2019. 7\n[70] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018. 2\n[71] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI blog, 2019. 2\n[72] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. JMLR, 2020. 5\n[73] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and\nJonathon Shlens. Stand-alone self-attention in vision models.arXiv preprint arXiv:1906.05909,\n2019. 2\n[74] Adrià Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin\nTallec, Mateusz Malinowski, Viorica Patraucean, Florent Altché, Michal Valko, et al. Broaden\nyour views for self-supervised video learning. arXiv preprint arXiv:2103.16559, 2021. 3\n[75] Steffen Rendle. Factorization machines. In ICDM, 2010. 19\n[76] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton,\nAndré Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of\nexperts. arXiv preprint arXiv:2106.05974, 2021. 9\n[77] Michael S Ryoo, AJ Piergiovanni, Mingxing Tan, and Anelia Angelova. Assemblenet:\nSearching for multi-stream neural connectivity in video architectures. arXiv preprint\narXiv:1905.13209, 2019. 7\n14\n[78] Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a\nvideo worth? arXiv preprint arXiv:2103.13915, 2021. 3\n[79] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017. 9\n[80] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine. Avid: Learning\nmulti-stage tasks via pixel-level translation of human videos.arXiv preprint arXiv:1912.04443,\n2019. 20\n[81] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human\nactions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 6, 17\n[82] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of\nvideo representations using lstms. In ICML, 2015. 3\n[83] Jonathan Stroud, David Ross, Chen Sun, Jia Deng, and Rahul Sukthankar. D3d: Distilled 3d\nnetworks for video action recognition. In WACV, 2020. 7\n[84] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video represen-\ntations using contrastive bidirectional transformer. arXiv preprint arXiv:1906.05743, 2019.\n3\n[85] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV,\n2020. 3\n[86] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,\nand Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention.\narXiv preprint arXiv:2012.12877, 2020. 3\n[87] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classiﬁcation with channel-\nseparated convolutional networks. In ICCV, 2019. 7\n[88] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 1, 2, 3, 4, 5\n[89] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\nWang, and Xiaoou Tang. Residual attention network for image classiﬁcation. In CVPR, 2017.\n2\n[90] Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu. Self-\nsupervised spatio-temporal representation learning for videos by predicting motion and ap-\npearance statistics. In CVPR, 2019. 3\n[91] Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu. Self-supervised video representation learning\nby pace prediction. In ECCV, 2020. 3\n[92] Xiaofang Wang, Xuehan Xiong, Maxim Neumann, AJ Piergiovanni, Michael S Ryoo, Anelia\nAngelova, Kris M Kitani, and Wei Hua. Attentionnas: Spatiotemporal attention cell search for\nvideo classiﬁcation. In ECCV, 2020. 7\n[93] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIn CVPR, 2018. 7\n[94] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional\nblock attention module. In ECCV, 2018. 2\n[95] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via\nnon-parametric instance discrimination. In CVPR, 2018. 3\n[96] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking\nspatiotemporal feature learning: Speed-accuracy trade-offs in video classiﬁcation. In ECCV,\n2018. 7\n15\n[97] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised\nspatiotemporal learning via video clip order prediction. In CVPR, 2019. 3\n[98] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for\nbridging video and language. In CVPR, 2016. 6, 17\n[99] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture\ntransformer network for image super-resolution. In CVPR, 2020. 3\n[100] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning\nvia invariant and spreading instance feature. In CVPR, 2019. 3\n[101] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 19\n[102] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV,\n2016. 3\n[103] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised\nlearning by cross-channel prediction. In CVPR, 2017. 3\n[104] Xiang Zhang, Junbo Zhao, and Yann Lecun. Character-level convolutional networks for text\nclassiﬁcation. NeurIPS, 2015. 1\n[105] Y Zhang, K Li, K Li, B Zhong, and Y Fu. Residual non-local attention networks for image\nrestoration. In ICLR, 2019. 2\n[106] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recogni-\ntion. In CVPR, 2020. 2\n[107] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer.\narXiv preprint arXiv:2012.09164, 2020. 3\n[108] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\nsequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n3\n[109] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures\nfrom web instructional videos. In AAAI, 2018. 6, 17\n16\nA Appendix\nAppendix contains more detailed explanations about datasets (A.1) and the experimental setup (A.2)\nfor both pre-training and downstream tasks. We also cover linear evaluation results compared to\nstate-of-the-art (A.4) and an ablation study on the input parameters (A.5).\nA.1 Datasets\nA.1.1 Pre-training\nFollowing [1, 59], we use HowTo100M [ 58] and AudioSet [ 33] to pre-train V ATT. The former\ncontains 1.2M unique videos, each providing multiple clips with audio and narration scripts resulting\nin 136M video-audio-text triplets in total. The narration scripts are extracted from speech audio using\nan off-the-shelf ASR. We use a subset of HowTo100M to comply with Youtube’s policies, which\nresults in having almost 1M unique videos and less than 100M clips. AudioSet consists of 10-second\nclips sampled from two million videos from YouTube. The dataset contains a variety of audio events\nwith their corresponding video without any narration, so we do not have any text input from this\ndataset. We do not use any labels from the datasets. We uniformly sample clips from these datasets; a\nmini-batch in the pre-training contains samples from both datasets. In order to ﬁll in the empty text\nin AudioSet, we feed a sequence of zeros to the text Transformer and exclude those samples from the\nMIL-NCE loss.\nA.1.2 Downstream\nWe evaluate the pre-trained V ATT on a set of diverse, representative downstream tasks to test different\naspects of the learned representations.\nVideo action recognition: We evaluate the visual representations on UCF101 [ 81] (101 classes,\n13,320 videos), HMDB51 [52] (51 classes, 6,766 videos), Kinetics-400 [14] (400 classes, 234,584\nvideos), Kinetics-600 [15] (600 classes, 366,016 videos), and Moments in Time [61] (339 classes,\n791,297 videos). Since UCF101 and HMDB51 are small datasets compared to the size of our model,\nwe freeze the vision backbone and use its outputs to train a linear classiﬁer. We use the split #1 results\nof the two datasets as a reference in our design exploration. For Kinetics-400, Kinetics-600, and\nMoments in Time, we ﬁne-tune our vision backbone initialized from the pre-trained checkpoint.\nAudio event classiﬁcation: We use ESC50 [66] (50 classes, 2000 audio clips) and AudioSet [33]\n(527 classes, ∼2M audio clips) to evaluate our audio Transformer on audio event classiﬁcation. We\nuse ESC50 to train a linear classiﬁer on top of the frozen audio Transformer. We use the split #1\nresults of this dataset as a reference in our design exploration. We also use AudioSet to ﬁne-tune our\naudio backbone initialized from the pre-trained checkpoint.\nZero-shot video retrieval: We evaluate the quality of our video-text common space represen-\ntations by zero-shot text-to-video retrieval on two of the most established datasets in this area:\nYouCook2 [109] and MSR-VTT [98] with 3.1k and 1k video-text pairs, respectively. We follow the\nsame evaluation pipeline described in [1] and report the Recall at 10 (R@10).\nImage classiﬁcation: Although there exists a domain gap between images and the video datasets\nused for pre-training V ATT, we test the learned vision Transformer in the image domain. We ﬁne-\ntune the last checkpoint of the vision Transformer on ImageNet [ 22] with no modiﬁcation to our\narchitecture or the tokenization pipeline. We will elaborate on this in the sequel.\nA.2 Experimental Setup\nA.2.1 Inputs\nDuring pre-training, we sample 32 frames at 10 fps for both pre-training datasets. For these frames,\nwe randomly crop a temporally consistent spatial region whose relative area is in the range of [0.08,\n1] and its aspect ratio in [0.5, 2]. These crops are then resized to 224 ×224, followed by a horizontal\nﬂip and color augmentation. The color augmentation follows [1] and randomizes brightness (max\ndelta = 32/255), saturation (max delta = 0.4), contrast (max delta=0.4), and hue (max delta=0.2). We\n17\nclip values to ensure the RGB is in [0, 1]. The audio waveforms are sampled in sync with the video\nframes at 48kHz. Both video and audio inputs are normalized between [-1, 1] for numerical stability.\nWe use patch sizes of 4 ×16 ×16 and 128 for video and raw waveform tokenization, respectively.\nWe use one-hot vectors to encode text sequences with the vocabulary size of216, which is the same\nas word2vec [60]. The resulting sequence retains a maximum of 16 words by either clipping or\npadding. We use DropToken with a drop rate of 50% during pre-training. For video ﬁne-tuning and\nevaluation, 32 frames with a temporal stride of 2 are sampled at 25 fps (2.56 seconds) with a crop\nsize of 320 ×320 (with similar video augmentation during pre-training), and we do not drop any\ntokens. We do not change the input size for audio and text during evaluation.\nA.2.2 Network setup in V ATT\nWe use the same Transformer architecture described in the main paper with various sizes shown\nin Table 7. We use the Medium model for our modality-agnostic variant (V ATT-MA). For the\nexperiments with modality-speciﬁc Transformers, we use the Small and Base models for the text\nand audio modalities, respectively, while varying the model sizes for the video modality. This\nresults in 3 variants for the modality-speciﬁc video-audio-text backbones: Base-Base-Small (BBS),\nMedium-Base-Small (MBS), and Large-Base-Small (LBS).\nModel Layers Hidden Size MLP Size Heads Params\nSmall 6 512 2048 8 20.9 M\nBase 12 768 3072 12 87.9 M\nMedium 12 1024 4096 16 155.0 M\nLarge 24 1024 4096 16 306.1 M\nTable 7: Details of the Transformer architectures in V ATT.\nA.2.3 Projection heads and contrastive losses\nWe use dva = 512and dvt = 256for the projection to the common spaces Sva and Svt, respectively.\nWe normalize the vectors before calculating the NCE and MIL-NCE objectives and use the tempera-\nture of τ = 0.07 and the weight of λ= 1in the loss deﬁned in the paper. We choose these values\nfollowing the previously established practice [ 1]; we may achieve better results by varying these\nhyper-parameters.\nA.2.4 Pre-training setup\nWe pre-train V ATT from scratch using Adam [46] with an initial learning rate of 1e-4, 10k warmup\nsteps, 500k steps in total, a batch size of 2048, and a quarter-period cosine schedule to anneal the\nlearning rate from 1e-4 to 5e-5. In the exploration experiments, we use a batch size of 512 while\nkeeping the rest of the training parameters the same. Our pipeline is implemented in Tensorﬂow\n(v2.4), and our models are trained for 3 days using 256 TPUs (v3).\nA.2.5 Video ﬁne-tuning setup\nFor video action recognition, we use the SGD with a momentum of 0.9 and an initial learning rate of\n0.005, 2.5k warmup steps, a batch size of 64, 100k steps in total, and a half-period cosine schedule\nto anneal the learning rate to 0. We use label smoothing with smoothing factor α= 0.1. The video\nframe resolution is 320 ×320, which results in an increase in the number of positional encoding\nweights. This increase is due to the fact that, in the pre-train time, we have 8+14+14 positional\nencoding buckets, while 8+20+20 positional buckets are required to completely encode 320/16\nhorizontal and 320/16 vertical locations in ﬁne-tune. To generate the new positional embeddings, we\ncreate a new set of positional encoding buckets by bi-cubic interpolation from the original buckets.\nAfter this step, we ﬁne-tune the entire network, including the positional encoding buckets, end-to-end.\nWe tried ﬁxed positional embeddings (solely based on interpolation for the missing locations) and did\nnot observe signiﬁcant improvements. We uniformly sample 4 clips to cover the entire 10 seconds of\nthe video and apply a standard 3-crop evaluation following [30]. We average the logits across the\nresulting 12 views before having the ﬁnal class predictions.\n18\nA.3 Audio ﬁne-tuning setup\nFor audio event classiﬁcation, we use the SGD with a momentum of 0.9, an initial learning rate of\n0.2, 5k warmup steps, a batch size of 1024, 50k steps in total, and a half-period cosine schedule to\nanneal the learning rate to 0. We observe that increasing the effective receptive ﬁeld improves the\noverall performance. We suggest that this might be due to the fact that the AudioSet annotations\nare multi-label and each event might occur in different temporal positions. Hence, we employ the\nduration of 6.4s with 24kHz sampling rate (153.6k total input samples). Similar to [ 49], we use\nmixup [101] on input-label (x-y) pairs in a mini-batch as below:\nx = αx1 + (1−α)x2, y = αy1 + (1−α)y2,\nwhere the input-label pairs are randomly sampled from a mini-batch, and the mixing rateαis sampled\nfrom a Beta(5,5) distribution. We also perform data balancing by penalizing the loss value of a\nsample with the inverse of the per-batch number of repetitive labels it carries. This is crucial for\navoiding over-ﬁtting since AudioSet has a long-tailed distribution, and a few dominant classes may\ndisrupt the training [49].\nA.3.1 Image ﬁne-tuning setup\nWe ﬁnetune the pre-trained V ATT on ImageNet for 50 epochs with384 ×384 input resolution, 512\nbatch size, SGD with momentum of 0.9, cosine learning rate decay with an initial learning rate of\n8e-2, and label smoothing of 0.1. No weight decay is used.\nA.3.2 Linear evaluation setup\nWe use a linear classiﬁer with ﬁxed backbones across all datasets and tasks. We observe that using\nmatrix factorization on the classiﬁer weight [75] leads to a more stable result across experiments. More\nspeciﬁcally, we use a factorized weight C = UV ∈Rd×c, where U ∈Rd×n and V ∈Rn×c are\nlearnable weights. During training this classiﬁer, we randomly choose a subset of the ncomponents\nin U and V , hence leading to a low-rank classiﬁer weight, C. The classiﬁer weight, C, is trained\nusing the Adam optimizer with a learning rate of 5e-4, a batch size of 64, a total of 50k training steps,\nand a sampling rate of 10% on its n= 128components.\nA.3.3 Zero-shot retrieval setup\nFor zero-shot text-to-video retrieval, we use the 1k split of MSR-VTT and the entire test split of\nYouCook2 as the pool for retrieval. We use224 ×224 central crops for 32 frames with a temporal\nstride of 2 sampled at 25 fps. Since each input clip covers 2.56 seconds, and the full clip length is 10\nseconds, we average the embeddings over 4 uniformly sampled clips before calculating the similarity\nwith a text query’s embedding. Weℓ2-normalize each vector to assure that a dot product results in the\ncosine similarity.\nA.4 Linear evaluation on frozen V ATT\nWe also test V ATT’s ability to generalize to other datasets when the entire backbone is frozen. In\nthis setting, we focus on the video and audio modalities and train a linear classiﬁer on the outputs\nof the frozen backbones. In addition to the low-rank classiﬁer (LRC) described in Section A.2, we\nalso report the results of a SVM classiﬁer following the same pipeline as [ 1]. Table 8 shows the\nperformance of our model on three datasets. We observe that V ATT does not outperform the best\nCNN counterparts in [1], and achieves comparable numbers to other baselines. This could suggest\nthat V ATT’s backbones learn less-linearly-separable feature, especially given that the contrastive\nestimation head includes non-linear projections.\nA.5 Ablation study on input parameters\nSince V ATT takes raw multimodal signals as inputs, the choice of input size and how they are patched\nhas a signiﬁcant impact on the ﬁnal performance. First, we alter the frame crop size and the number\nof sampled frames from each video clip while keeping the patch size ﬁxed to 5 ×16 ×16. Table 9\nshows that using a small frame crop size and a larger number of frames hurts the video-related results,\nbut it does not signiﬁcantly change the audio classiﬁcation numbers.\n19\nMETHOD UCF101 HMDB51 ESC50\nMIL-NCE [59] 83.4 54.8 -\nA VTS [50] - - 82.3\nXDC [2] - - 84.8\nELo [67] - 64.5 -\nA VID [80] - - 89.2\nGDT [65] - - 88.5\nMMV [1] 91.8 67.1 88.9\nV ATT-Medium + SVM 89.2 63.3 82.5\nV ATT-Medium + LRC 89.6 65.2 84.7\nV ATT-MA-Medium + LRC 84.4 63.1 81.2\nTable 8: Linear evaluation results for video action recognition on UCF101 and HMDB51 and audio\nevent classiﬁcation on ESC50. MA refers to the Modality-Agnostic backbone.\nFrame Size Patch Size UCF HMDB YC2 MSRVTT ESC\n32×224×224 4 ×16×16 87.8 67.7 27.53 17.99 87\n32×200×200 5 ×16×16 87.16 67.08 23.98 17.84 86.25\n32×224×224 5 ×16×16 87.74 67.6 27.47 17.96 87\n64×224×224 5 ×16×16 86.57 63.09 18.52 12.5 86.25\n32×224×224 8 ×16×16 86.52 65.64 23.43 16.14 84\n32×224×224 8 ×32×32 82.68 60.73 15.27 13.79 87\nTable 9: Effect of video frame and patch size on downstream results.\nThen, we keep the best frame size (32 ×224 ×224) and vary the video patch size. We ﬁnd going\nbeyond 4 ×16 ×16 along either the time or spatial dimensions is not helpful. We avoid patches that\nare smaller than 4 ×16 ×16 because of the signiﬁcantly increaseed wall clock time in experiments.\nFinally, we compare different audio patch sizes and perform an experiment using spectrograms, as\nopposed to the raw waveforms, as audio input. The goal is to see how the raw waveforms compare to\nthe handcrafted spectrograms. We use the MEL spectrogram with 80 bins, the STFT length of 42\nms, and the STFT step of 21 ms following a similar setup in [1]. Tables 10 summarize the results, in\nwhich we observe that the patch size of 128 gives rise to the best waveform-based results, and using\nspectrogram does not lead to any conclusive improvement. The experiment with the spectrograms\ndemonstrates that V ATT is able to learn semantic representations from raw audios. To the best of our\nknowledge, this is the ﬁrst time that raw audio waveforms are used for multimodal self-supervised\nlearning.\nInput Patch Size UCF HMDB YC2 MSRVTT ESC\nWaveform 128 88.14 68.13 25.72 17.31 87.75\nWaveform 256 87.74 66.1 24.19 16.55 83.75\nWaveform 512 87.21 67.34 26.11 16.91 82.5\nWaveform 1024 86.41 66.36 24.46 16.38 82.5\nSpectrogram 16 × 5 88.3 67.52 26.62 16.86 88\nTable 10: Effect of the audio input type and patch size on downstream results.\n20"
}