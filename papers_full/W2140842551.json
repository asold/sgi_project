{
    "title": "Aggregate and mixed-order Markov models for statistical language processing",
    "url": "https://openalex.org/W2140842551",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4297573010",
            "name": "Saul, Lawrence",
            "affiliations": [
                "AT&T (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2750143916",
            "name": "Pereira Fernando",
            "affiliations": [
                "AT&T (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2049633694",
        "https://openalex.org/W1597533204",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2075201173",
        "https://openalex.org/W1963571628",
        "https://openalex.org/W1996903695",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2149671658",
        "https://openalex.org/W2170120409",
        "https://openalex.org/W2950186769",
        "https://openalex.org/W2013196554",
        "https://openalex.org/W2127314673"
    ],
    "abstract": "We consider the use of language models whose size and accuracy are intermediate between different order n-gram models. Two types of models are studied in particular. Aggregate Markov models are class-based bigram models in which the mapping from words to classes is probabilistic. Mixed-order Markov models combine bigram models whose predictions are conditioned on different words. Both types of models are trained by Expectation-Maximization (EM) algorithms for maximum likelihood estimation. We examine smoothing procedures in which these models are interposed between different order n-grams. This is found to significantly reduce the perplexity of unseen word combinations.",
    "full_text": "arXiv:cmp-lg/9706007v1  9 Jun 1997\nAggregate and mixed-order Markov models for\nstatistical language processing\nLawrence Saul and Fernando Pereira\n{lsaul,pereira}@research.att.com\nAT&T Labs – Research\n180 Park Ave, D–130\nFlorham Park, NJ 07932\nAbstract\nWe consider the use of language models\nwhose size and accuracy are intermedi-\nate between diﬀerent order n-gram models.\nTwo types of models are studied in partic-\nular. Aggregate Markov models are class-\nbased bigram models in which the map-\nping from words to classes is probabilis-\ntic. Mixed-order Markov models combine\nbigram models whose predictions are con-\nditioned on diﬀerent words. Both types\nof models are trained by Expectation-\nMaximization (EM) algorithms for maxi-\nmum likelihood estimation. We examine\nsmoothing procedures in which these mod-\nels are interposed between diﬀerent order\nn-grams. This is found to signiﬁcantly re-\nduce the perplexity of unseen word combi-\nnations.\n1 Introduction\nThe purpose of a statistical language model is to as-\nsign high probabilities to likely word sequences and\nlow probabilities to unlikely ones. The challenge\nhere arises from the combinatorially large number\nof possibilities, only a fraction of which can ever be\nobserved. In general, language models must learn\nto recognize word sequences that are functionally\nsimilar but lexically distinct. The learning problem,\none of generalizing from sparse data, is particularly\nacute for large-sized vocabularies (Jelinek, Mercer,\nand Roukos, 1992).\nThe simplest models of natural language are n-\ngram Markov models. In these models, the prob-\nability of each word depends on the n −1 words\nthat precede it. The problems in estimating ro-\nbust models of this form are well-documented. The\nnumber of parameters—or transition probabilities—\nscales as V n, where V is the vocabulary size. For\ntypical models (e.g., n = 3, V = 10 4), this num-\nber exceeds by many orders of magnitude the total\nnumber of words in any feasible training corpus.\nThe transition probabilities in n-gram models are\nestimated from the counts of word combinations in\nthe training corpus. Maximum likelihood (ML) esti-\nmation leads to zero-valued probabilities for unseen\nn-grams. In practice, one adjusts or smoothes (Chen\nand Goodman, 1996) the ML estimates so that\nthe language model can generalize to new phrases.\nSmoothing can be done in many ways—for example,\nby introducing artiﬁcial counts, backing oﬀ to lower-\norder models (Katz, 1987), or combining models by\ninterpolation (Jelinek and Mercer, 1980).\nOften a great deal of information is lost in the\nsmoothing procedure. This is due to the great dis-\ncrepancy between n-gram models of diﬀerent order.\nThe goal of this paper is to investigate models that\nare intermediate, in both size and accuracy, between\ndiﬀerent order n-gram models. We show that such\nmodels can “intervene” between diﬀerent order n-\ngrams in the smoothing procedure. Experimentally,\nwe ﬁnd that this signiﬁcantly reduces the perplexity\nof unseen word combinations.\nThe language models in this paper were evalu-\nated on the ARPA North American Business News\n(NAB) corpus. All our experiments used a vo-\ncabulary of sixty-thousand words, including tokens\nfor punctuation, sentence boundaries, and an un-\nknown word token standing for all out-of-vocabulary\nwords. The training data consisted of approxi-\nmately 78 million words (three million sentences);\nthe test data, 13 million words (one-half million\nsentences). All sentences were drawn randomly\nwithout replacement from the NAB corpus. All\nperplexity ﬁgures given in the paper are com-\nputed by combining sentence probabilities; the prob-\nability of sentence w0w1 ··· wnwn+1 is given by∏ n+1\ni=1 P (wi|w0 ··· wi−1), where w0 and wn+1 are\nthe start- and end-of-sentence markers, respectively.\nThough not reported below, we also conﬁrmed that\nthe results did not vary signiﬁcantly for diﬀerent ran-\ndomly drawn test sets of the same size.\nThe organization of this paper is as follows.\nIn Section 2, we examine aggregate Markov mod-\nels, or class-based bigram models (Brown et al.,\n1992) in which the mapping from words to classes\nis probabilistic. We describe an iterative algo-\nrithm for discovering “soft” word classes, based on\nthe Expectation-Maximization (EM) procedure for\nmaximum likelihood estimation (Dempster, Laird,\nand Rubin, 1977). Several features make this algo-\nrithm attractive for large-vocabulary language mod-\neling: it has no tuning parameters, converges mono-\ntonically in the log-likelihood, and handles proba-\nbilistic constraints in a natural way. The number\nof classes, C, can be small or large depending on\nthe constraints of the modeler. Varying the number\nof classes leads to models that are intermediate be-\ntween unigram ( C = 1) and bigram ( C = V ) models.\nIn Section 3, we examine another sort of “inter-\nmediate” model, one that arises from combinations\nof non-adjacent words. Language models using such\ncombinations have been proposed by Huang et al.\n(1993), Ney, Essen, and Kneser (1994), and Rosen-\nfeld (1996), among others. We consider speciﬁcally\nthe skip-k transition matrices, M(wt−k, wt), whose\npredictions are conditioned on the kth previous word\nin the sentence. (The value of k determines how\nmany words one “skips” back to make the predic-\ntion.) These predictions, conditioned on only a sin-\ngle previous word in the sentence, are inherently\nweaker than those conditioned on all k previous\nwords. Nevertheless, by combining several predic-\ntions of this form (for diﬀerent values of k), we can\ncreate a model that is intermediate in size and ac-\ncuracy between bigram and trigram models.\nMixed-order Markov models express the predic-\ntions P (wt|wt−1, wt−2, . . . , w t−m) as a convex com-\nbination of skip- k transition matrices, M(wt−k, wt).\nWe derive an EM algorithm to learn the mixing co-\neﬃcients, as well as the elements of the transition\nmatrices. The number of transition probabilities in\nthese models scales as mV 2, as opposed to V m+1.\nMixed-order models are not as powerful as trigram\nmodels, but they can make much stronger predic-\ntions than bigram models. The reason is that quite\noften the immediately preceding word has less pre-\ndictive value than earlier words in the same sentence.\nIn Section 4, we use aggregate and mixed-order\nmodels to improve the probability estimates from\nn-grams. This is done by interposing these models\nbetween diﬀerent order n-grams in the smoothing\nprocedure. We compare our results to a baseline tri-\ngram model that backs oﬀ to bigram and unigram\nmodels. The use of “intermediate” models is found\nto reduce the perplexity of unseen word combina-\ntions by over 50%.\nIn Section 5, we discuss some extensions to these\nmodels and some open problems for future research.\nWe conclude that aggregate and mixed-order models\nprovide a compelling alternative to language models\nbased exclusively on n-grams.\n2 Aggregate Markov models\nIn this section we consider how to construct class-\nbased bigram models (Brown et al., 1992). The\nproblem is naturally formulated as one of hidden\nvariable density estimation. Let P (c|w1) denote the\nprobability that word w1 is mapped into class c.\nLikewise, let P (w2|c) denote the probability that\nwords in class c are followed by the word w2. The\nclass-based bigram model predicts that word w1 is\nfollowed by word w2 with probability\nP (w2|w1) =\nC∑\nc=1\nP (w2|c)P (c|w1), (1)\nwhere C is the total number of classes. The hidden\nvariable in this problem is the class label c, which\nis unknown for each word w1. Note that eq. (1)\nrepresents the V 2 elements of the transition matrix\nP (w2|w1) in terms of the 2 CV elements of P (w2|c)\nand P (c|w1).\nThe Expectation-Maximization (EM) algorithm\n(Dempster, Laird, and Rubin, 1977) is an iterative\nprocedure for estimating the parameters of hidden\nvariable models. Each iteration consists of two steps:\nan E-step which computes statistics over the hidden\nvariables, and an M-step which updates the param-\neters to reﬂect these statistics.\nThe EM algorithm for aggregate Markov models\nis particularly simple. The E-step is to compute, for\neach bigram w1w2 in the training set, the posterior\nprobability\nP (c|w1, w2) = P (w2|c)P (c|w1)\n∑\nc′ P (w2|c′)P (c′|w1). (2)\nEq. (2) gives the probability that word w1 was as-\nsigned to class c, based on the observation that it\nwas followed by word w2. The M-step uses these\nposterior probabilities to re-estimate the model pa-\nrameters. The updates for aggregate Markov models\nare:\nP (c|w1) ←\n∑\nw N(w1, w)P (c|w1, w)\n∑\nwc′ N(w1, w)P (c′|w1, w) , (3)\nP (w2|c) ←\n∑\nw N(w, w2)P (c|w, w2)∑\nww′ N(w, w′)P (c|w, w′), (4)\nC train test\n1 964.7 964.9\n2 771.2 772.2\n4 541.9 543.6\n8 399.5 401.5\n16 328.8 331.8\n32 278.9 283.2\nV 123.6 —\nTable 1: Perplexities of aggregate Markov models on\nthe training and test sets; C is the number of classes.\nThe case C = 1 corresponds to a ML unigram model;\nC = V , to a ML bigram model.\n0 0.2 0.4 0.6 0.8 1\nwinning assignment probability\nFigure 2: Histogram of the winning assignment\nprobabilities, max c P (c|w), for the three hundred\nmost commonly occurring words.\nwhere N(w1, w2) denotes the number of counts of\nw1w2 in the training set. These updates are guar-\nanteed to increase the overall log-likelihood,\nℓ =\n∑\nw1w2\nN(w1, w2) ln P (w2|w1), (5)\nat each iteration. In general, they converge to a local\n(though not global) maximum of the log-likelihood.\nThe perplexity V ∗ is related to the log-likelihood by\nV ∗ = e−ℓ/N, where N is the total number of words\nprocessed.\nThough several algorithms (Brown et al., 1992;\nPereira, Tishby, and Lee, 1993) have been proposed\nfor performing the decomposition in eq. (1), it is\nworth noting that only the EM algorithm directly\noptimizes the log-likelihood in eq. (5). This has ob-\nvious advantages if the goal of ﬁnding word classes is\nto improve the perplexity of a language model. The\nEM algorithm also handles probabilistic constraints\nin a natural way, allowing words to belong to more\nthan one class if this increases the overall likelihood.\nOur approach diﬀers in important ways from the\nuse of hidden Markov models (HMMs) for class-\nbased language modeling (Jelinek et al. , 1992).\nWhile HMMs also use hidden variables to represent\nword classes, the dynamics are fundamentally dif-\nferent. In HMMs, the hidden state at time t + 1 is\npredicted (via the state transition matrix) from the\nhidden state at time t. On the other hand, in aggre-\ngate Markov models, the hidden state at time t + 1\nis predicted (via the matrix P (ct+1|wt)) from the\nword at time t. The state-to-state versus word-to-\nstate dynamics lead to diﬀerent learning algorithms.\nFor example, the Baum-Welch algorithm for HMMs\nrequires forward and backward passes through each\ntraining sentence, while the EM algorithm we use\ndoes not.\nWe trained aggregate Markov models with 2, 4,\n8, 16, and 32 classes. Figure 1 shows typical plots\nof the training and test set perplexities versus the\nnumber of iterations of the EM algorithm. Clearly,\nthe two curves are very close, and the monotonic\ndecrease in test set perplexity strongly suggests lit-\ntle if any overﬁtting, at least when the number of\nclasses is small compared to the number of words in\nthe vocabulary. Table 1 shows the ﬁnal perplexities\n(after thirty-two iterations of EM) for various ag-\ngregate Markov models. These results conﬁrm that\naggregate Markov models are intermediate in accu-\nracy between unigram ( C = 1) and bigram ( C = V )\nmodels.\nThe aggregate Markov models were also observed\nto discover meaningful word classes. Table 2 shows,\nfor the aggregate model with C = 32 classes, the\nmost probable class assignments of the three hun-\ndred most commonly occurring words. To be precise,\nfor each class c∗, we have listed the words for which\nc∗ = arg max c P (c|w). Figure 2 shows a histogram of\nthe winning assignment probabilities, max c P (c|w),\nfor these words. Note that the winning assignment\nprobabilities are distributed broadly over the inter-\nval [ 1\nC , 1]. This demonstrates the utility of allowing\n“soft” membership classes: for most words, the max-\nimum likelihood estimates of P (c|w) do not corre-\nspond to a winner-take-all assignment, and therefore\nany method that assigns each word to a single class\n(“hard” clustering), such as those used by Brown et\nal. (1992) or Ney, Essen, and Kneser (1994), would\nlose information.\nWe conclude this section with some ﬁnal com-\nments on overﬁtting. Our models were trained by\nthirty-two iterations of EM, allowing for nearly com-\nplete convergence in the log-likelihood. Moreover,\nwe did not implement any ﬂooring constraints 1 on\n1It is worth noting, in this regard, that individual\nzeros in the matrices P (w2|c) and P (c|w1) do not nec-\nessarily give rise to zeros in the matrix P (w2|w1), as\ncomputed from eq. (1).\n0 5 10 15 20 25 30200\n300\n400\n500\n600\n700\n800\n900\n1000\niteration of EM\nperplexity (train)\n0 5 10 15 20 25 30200\n300\n400\n500\n600\n700\n800\n900\n1000\niteration of EM\nperplexity (test)\n(a) (b)\nFigure 1: Plots of (a) training and (b) test perplexity versus numbe r of iterations of the EM algorithm, for\nthe aggregate Markov model with C = 32 classes.\n1 as cents made make take\n2\nago day earlier Friday Monday month quarter\nreported said Thursday trading Tuesday\nWednesday ⟨. . . ⟩\n3 even get to\n4 based days down home months up work years\n⟨%⟩\n5 those ⟨,⟩ ⟨— ⟩\n6 ⟨.⟩ ⟨?⟩\n7 eighty ﬁfty forty ninety seventy sixty thirty\ntwenty ⟨(⟩ ⟨·⟩\n8 can could may should to will would\n9 about at just only or than ⟨&⟩ ⟨;⟩\n10 economic high interest much no such tax united\nwell\n11 president\n12 because do how if most say so then think very\nwhat when where\n13 according back expected going him plan used way\n15 don’t I people they we you\n16 Bush company court department more oﬃcials\npolice retort spokesman\n17 former the\n18 American big city federal general house military\nnational party political state union York\n19 billion hundred million nineteen\n20 did ⟨”⟩ ⟨’⟩\n21 but called San ⟨:⟩ ⟨start-of-sentence⟩\n22\nbank board chairman end group members\nnumber oﬃce out part percent price prices rate\nsales shares use\n23 a an another any dollar each ﬁrst good her his its\nmy old our their this\n24 long Mr. year\n25\nbusiness California case companies corporation\ndollars incorporated industry law money\nthousand time today war week ⟨)⟩ ⟨unknown⟩\n26 also government he it market she that there\nwhich who\n27 A. B. C. D. E. F. G. I. L. M. N. P. R. S. T. U.\n28 both foreign international major many new oil\nother some Soviet stock these west world\n29\nafter all among and before between by during for\nfrom in including into like of oﬀ on over since\nthrough told under until while with\n30\neight ﬁfteen ﬁve four half last next nine oh one\nsecond seven several six ten third three twelve\ntwo zero ⟨-⟩\n31 are be been being had has have is it’s not still\nwas were\n32 chief exchange news public service trade\nTable 2: Most probable assignments for the 300 most frequent wor ds in an aggregate Markov model with\nC = 32 classes. Class 14 is absent because it is not the most probable cla ss for any of the selected words.)\nthe probabilities P (c|w1) or P (w2|c). Nevertheless,\nin all our experiments, the ML aggregate Markov\nmodels assigned non-zero probability to all the bi-\ngrams in the test set. This suggests that for large\nvocabularies there is a useful regime 1 ≪C ≪V\nin which aggregate models do not suﬀer much from\noverﬁtting. In this regime, aggregate models can be\nrelied upon to compute the probabilities of unseen\nword combinations. We will return to this point in\nSection 4, when we consider how to smooth n-gram\nlanguage models.\n3 Mixed-order Markov models\nOne of the drawbacks of n-gram models is that their\nsize grows rapidly with their order. In this section,\nwe consider how to make predictions based on a con-\nvex combination of pairwise correlations. This leads\nto language models whose size grows linearly in the\nnumber of words used for each prediction.\nFor each k > 0, the skip-k transition matrix\nM(wt−k, wt) predicts the current word from the\nkth previous word in the sentence. A mixed-order\nMarkov model combines the information in these\nmatrices for diﬀerent values of k. Let m denote\nthe number of bigram models being combined. The\nprobability distribution for these models has the\nform:\nP (wt|wt−1, . . . , w t−m) = (6)\nm∑\nk=1\nλk(wt−k) Mk(wt−k, wt)\nk−1∏\nj=1\n[1 −λj (wt−j )].\nThe terms in this equation have a simple interpreta-\ntion. The V ×V matrices Mk(w, w′) in eq. (6) de-\nﬁne the skip- k stochastic dependency of w′ at some\nposition t on w at position t −k; the parameters\nλk(w) are mixing coeﬃcients that weight the predic-\ntions from these diﬀerent dependencies. The value of\nλk(w) can be interpreted as the probability that the\nmodel, upon seeing the word wt−k, looks no further\nback to make its prediction (Singer, 1996). Thus the\nmodel predicts from wt−1 with probability λ1(wt−1),\nfrom wt−2 with probability [1 −λ1(wt−1)]λ2(wt−2),\nand so on. Though included in eq. (6) for cosmetic\nreasons, the parameters λm(w) are actually ﬁxed to\nunity so that the model never looks further than m\nwords back.\nWe can view eq. (6) as a hidden variable model.\nImagine that we adopt the following strategy to pre-\ndict the word at time t. Starting with the previous\nword, we toss a coin (with bias λ1(wt−1)) to see if\nthis word has high predictive value. If the answer\nis yes, then we predict from the skip-1 transition\nmatrix, M1(wt−1, wt). Otherwise, we shift our at-\ntention one word to the left and repeat the process.\nIf after m −1 tosses we have not settled on a pre-\ndiction, then as a last resort, we make a prediction\nusing Mm(wt−m, wt). The hidden variables in this\nprocess are the outcomes of the coin tosses, which\nare unknown for each word wt−k.\nViewing the model in this way, we can derive an\nEM algorithm to learn the mixing coeﬃcients λk(w)\nand the transition matrices 2 Mk(w, w′). The E-step\nof the algorithm is to compute, for each word in the\n2Note that the ML estimates of Mk(w, w ′) do not\ndepend only on the raw counts of k-separated bigrams;\nthey are also coupled to the values of the mixing coef-\nﬁcients, λk(w). In particular, the EM algorithm adapts\nthe matrix elements to the weighting of word combina-\ntions in eq. (6). The raw counts of k-separated bigrams,\nhowever, do give good initial estimates.\n0 1 2 3 470\n75\n80\n85\n90\n95\n100\n105\n110\niteration of EM\nperplexity (train)\nFigure 3: Plot of (training set) perplexity versus\nnumber of iterations of the EM algorithm. The re-\nsults are for the m = 4 mixed-order Markov model.\ntraining set, the posterior probability that it was\ngenerated by Mk(wt−k, wt). Denoting these poste-\nrior probabilities by φk(t), we have:\nφk(t) = (7)\nλk(wt−k)Mk(wt−k, wt) ∏ k−1\nj=1 [1−λj(wt−j )]\nP (wt|wt−1, wt−2, . . . , w t−m) ,\nwhere the denominator is given by eq. (6). The\nM-step of the algorithm is to update the parame-\nters λk(w) and Mk(w, w′) to reﬂect the statistics in\neq. (7). The updates for mixed-order Markov models\nare given by:\nλk(w) ←\n∑\nt δ(w, wt−k)φk(t)\n∑\nt\n∑ m\nj=k δ(w, wt−k)φj (t), (8)\nMk(w, w′) ←\n∑\nt δ(w, wt−k)δ(w′, wt)φk(t)\n∑\nt δ(w, wt−k )φk(t) , (9)\nwhere the sums are over all the sentences in the\ntraining set, and δ(w, w′) = 1 iﬀ w = w′.\nWe trained mixed-order Markov models with 2 ≤\nm ≤4. Figure 3 shows a typical plot of the train-\ning set perplexity as a function of the number of\niterations of the EM algorithm. Table 3 shows the\nﬁnal perplexities on the training set (after four iter-\nations of EM). Mixed-order models cannot be used\ndirectly on the test set because they predict zero\nprobability for unseen word combinations. Unlike\nstandard n-gram models, however, the number of\nunseen word combinations actually decreases with\nthe order of the model. The reason for this is that\nmixed-order models assign ﬁnite probability to all n-\ngrams w1w2 . . . w n for which any of the k-separated\nbigrams wkwn are observed in the training set. To\nillustrate this point, Table 3 shows the fraction of\nwords in the test set that were assigned zero proba-\nbility by the mixed-order model. As expected, this\nfraction decreases monotonically with the number of\nbigrams that are mixed into each prediction.\nm train missing\n1 123.2 0.045\n2 89.4 0.014\n3 77.9 0.0063\n4 72.4 0.0037\nTable 3: Results for ML mixed-order models; m de-\nnotes the number of bigrams that were mixed into\neach prediction. The ﬁrst column shows the per-\nplexities on the training set. The second shows the\nfraction of words in the test set that were assigned\nzero probability. The case m = 1 corresponds to a\nML bigram model.\nClearly, the success of mixed-order models de-\npends on the ability to gauge the predictive value\nof each word, relative to earlier words in the same\nsentence. Let us see how this plays out for the\nsecond-order ( m = 2) model in Table 3. In this\nmodel, a small value for λ1(w) indicates that the\nword w typically carries less information that the\nword that precedes it. On the other hand, a large\nvalue for λ1(w) indicates that the word w is highly\npredictive. The ability to learn these relationships\nis conﬁrmed by the results in Table 4. Of the three-\nhundred most common words, Table 4 shows the\nﬁfty with the lowest and highest values of λ1(w).\nNote how low values of λ1(w) are associated with\nprepositions, mid-sentence punctuation marks, and\nconjunctions, while high values are associated with\n“contentful” words and end-of-sentence markers. (A\nparticularly interesting dichotomy arises for the two\nforms “a” and “an” of the indeﬁnite article; the lat-\nter, because it always precedes a word that begins\nwith a vowel, is inherently more predictive.) These\nresults underscore the importance of allowing the\ncoeﬃcients λ1(w) to depend on the context w, as\nopposed to being context-independent (Ney, Essen,\nand Kneser, 1994).\n4 Smoothing\nSmoothing plays an essential role in language models\nwhere ML predictions are unreliable for rare events.\nIn n-gram modeling, it is common to adopt a re-\ncursive strategy, smoothing bigrams by unigrams,\ntrigrams by bigrams, and so on. Here we adopt a\nsimilar strategy, using the ( m −1)th mixed-order\nmodel to smooth the mth one. At the “root” of\nour smoothing procedure, however, lies not a uni-\ngram model, but an aggregate Markov model with\nC > 1 classes. As shown in Section 2, these models\nassign ﬁnite probability to all word combinations,\n0. 1 < λ 1(w) < 0. 7\n⟨-⟩ and of ⟨”⟩ or ⟨;⟩ to ⟨,⟩ ⟨&⟩ by with S. from\nnine were for that eight low seven the ⟨(⟩ ⟨:⟩ six\nare not against was four between a their two\nthree its ⟨unknown⟩ B. on as is ⟨— ⟩ ﬁve ⟨)⟩ into\nC. M. her him over than A.\n0. 96 < λ 1(w) ≤ 1\noﬃcials prices which go way he last they earlier\nan Tuesday there foreign quarter she former\nfederal don’t days Friday next Wednesday ⟨%⟩\nThursday I Monday Mr. we half based part\nUnited it’s years going nineteen thousand months\n⟨·⟩ million very cents San ago U. percent billion\n⟨?⟩ according ⟨.⟩\nTable 4: Words with low and high values of λ1(w)\nin an m = 2 mixed order model.\nC validation test unseen\n1 163.615 167.112 293175\n2 162.982 166.193 259360\n4 161.513 164.363 200067\n8 161.327 164.104 190178\n16 160.034 162.686 164673\n32 159.247 161.683 150958\nTable 5: Perplexities of bigram models smoothed by\naggregate Markov models with diﬀerent numbers of\nclasses ( C).\neven those that are not observed in the training set.\nHence, they can legitimately replace unigrams as the\nbase model in the smoothing procedure.\nLet us ﬁrst examine the impact of replacing uni-\ngram models by aggregate models at the root of the\nsmoothing procedure. To this end, a held-out inter-\npolation algorithm (Jelinek and Mercer, 1980) was\nused to smooth an ML bigram model with the aggre-\ngate Markov models from Section 2. The smoothing\nparameters, one for each row of the bigram transi-\ntion matrix, were estimated from a validation set the\nsame size as the test set. Table 5 gives the ﬁnal per-\nplexities on the validation set, the test set, and the\nunseen bigrams in the test set. Note that smooth-\ning with the C = 32 aggregate Markov model has\nnearly halved the perplexity of unseen bigrams, as\ncompared to smoothing with the unigram model.\nLet us now examine the recursive use of mixed-\norder models to obtain smoothed probability esti-\nmates. Again, a held-out interpolation algorithm\nwas used to smooth the mixed-order Markov models\nfrom Section 3. The mth mixed-order model had\nmV smoothing parameters σk(w), corresponding to\nthe V rows in each skip- k transition matrix. The\nm validation test\n1 160.1 161.3\n2 135.3 136.9\n3 131.4 133.5\n4 131.2 133.7\nTable 6: Perplexities of smoothed mixed-order mod-\nels on the validation and test sets.\nmth mixed-order model was smoothed by discount-\ning the weight of each skip- k prediction, then ﬁll-\ning in the leftover probability mass by a lower-order\nmodel. In particular, the discounted weight of the\nskip-k prediction was given by\n[1 −σk(wt−k)]λk(wt−k)\nk−1∏\nj=1\n[1 −λj (wt−j )] , (10)\nleaving a total mass of\nm∑\nk=1\nσk(wt−k)λk(wt−k)\nk−1∏\nj=1\n[1 −λj (wt−j )] (11)\nfor the ( m −1)th mixed-order model. (Note that\nthe m = 1 mixed-order model corresponds to a ML\nbigram model.)\nTable 6 shows the perplexities of the smoothed\nmixed-order models on the validation and test sets.\nAn aggregate Markov model with C = 32 classes\nwas used as the base model in the smoothing proce-\ndure. The ﬁrst row corresponds to a bigram model\nsmoothed by a aggregate Markov model; the second\nrow corresponds to an m = 2 mixed-order model,\nsmoothed by a ML bigram model, smoothed by an\naggregate Markov model; the third row corresponds\nto an m = 3 mixed-order model, smoothed by a\nm = 2 mixed-order model, smoothed by a ML bi-\ngram model, etc. A signiﬁcant decrease in perplex-\nity occurs in moving to the smoothed m = 2 mixed-\norder model. On the other hand, the diﬀerence in\nperplexity for higher values of m is not very dra-\nmatic.\nOur last experiment looked at the smoothing of\na trigram model. Our baseline was a ML trigram\nmodel that backed oﬀ 3 to bigrams (and when nec-\nessary, unigrams) using the Katz backoﬀ procedure\n(Katz, 1987). In this procedure, the predictions of\nthe ML trigram model are discounted by an amount\ndetermined by the Good-Turing coeﬃcients; the left-\nover probability mass is then ﬁlled in by the backoﬀ\n3We used a backoﬀ procedure (instead of interpo-\nlation) to avoid the estimation of trigram smoothing\nparameters.\nbackoﬀ test unseen\nbaseline 95.2 2799\nmixed 79.8 1363\nTable 7: Perplexities of two smoothed trigram mod-\nels on the test set and the subset of unseen word\ncombinations. The baseline model backed oﬀ to bi-\ngrams and unigrams; the other backed oﬀ to the\nm = 2 model in Table 6.\nmodel. We compared this to a trigram model that\nbacked oﬀ to the m = 2 model in Table 6. This was\nhandled by a slight variant of the Katz procedure\n(Dagan, Pereira, and Lee, 1994) in which the mixed-\norder model substituted for the backoﬀ model.\nOne advantage of this smoothing procedure is that\nit is straightforward to assess the performance of dif-\nferent backoﬀ models. Because the backoﬀ models\nare only consulted for unseen word combinations,\nthe perplexity on these word combinations serves as\na reasonable ﬁgure-of-merit.\nTable 7 shows those perplexities for the two\nsmoothed trigram models (baseline and backoﬀ).\nThe mixed-order smoothing was found to reduce\nthe perplexity of unseen word combinations by 51%.\nAlso shown in the table are the perplexities on the\nentire test set. The overall perplexity decreased\nby 16%—a signiﬁcant amount considering that only\n24% of the predictions involved unseen word com-\nbinations and required backing oﬀ from the trigram\nmodel.\nThe models in Table 7 were constructed from all\nn-grams (1 ≤n ≤3) observed in the training data.\nBecause many n-grams occur very infrequently, a\nnatural question is whether truncated models, which\nomit low-frequency n-grams from the training set,\ncan perform as well as untruncated ones. The ad-\nvantage of truncated models is that they do not need\nto store nearly as many non-zero parameters as un-\ntruncated models. The results in Table 8 were ob-\ntained by dropping trigrams that occurred less than\nt times in the training corpus. The t = 1 row cor-\nresponds to the models in Table 7. The most in-\nteresting observation from the table is that omitting\nvery low-frequency trigrams does not decrease the\nquality of the mixed-order model, and may in fact\nslightly improve it. This contrasts with the standard\nbackoﬀ model, in which truncation causes signiﬁcant\nincreases in perplexity.\nt baseline mixed trigrams(×106) missing\n1 95.2 79.8 25.4 0.24\n2 98.6 78.3 6.1 0.32\n3 101.7 79.6 3.3 0.36\n4 104.2 81.1 2.3 0.38\n5 106.2 82.4 1.7 0.41\nTable 8: Eﬀect of truncating trigrams that occur\nless than t times. The table shows the baseline and\nmixed-order perplexities on the test set, the num-\nber of distinct trigrams with t or more counts, and\nthe fraction of trigrams in the test set that required\nbacking oﬀ.\n5 Discussion\nOur results demonstrate the utility of language mod-\nels that are intermediate in size and accuracy be-\ntween diﬀerent order n-gram models. The two\nmodels considered in this paper were hidden vari-\nable Markov models trained by EM algorithms for\nmaximum likelihood estimation. Combinations of\nintermediate-order models were also investigated by\nRosenfeld (1996). His experiments used the 20,000-\nword vocabulary Wall Street Journalcorpus, a pre-\ndecessor of the NAB corpus. He trained a maximum-\nentropy model consisting of unigrams, bigrams, tri-\ngrams, skip-2 bigrams and trigrams; after selecting\nlong-distance bigrams (word triggers) on 38 million\nwords, the model was tested on a held-out 325 thou-\nsand word sample. Rosenfeld reported a test-set\nperplexity of 86, a 19% reduction from the 105 per-\nplexity of a baseline trigram backoﬀ model. In our\nexperiments, the perplexity gain of the mixed-order\nmodel ranged from 16% to 22%, depending on the\namount of truncation in the trigram model.\nWhile Rosenfeld’s results and ours are not di-\nrectly comparable, both demonstrate the utility of\nmixed-order models. It is worth discussing, how-\never, the diﬀerent approaches to combining infor-\nmation from non-adjacent words. Unlike the max-\nimum entropy approach, which allows one to com-\nbine many non-independent features, ours calls for\na careful Markovian decomposition. Rosenfeld ar-\ngues at length against na¨ ıve linear combinations in\nfavor of maximum entropy methods. His arguments\ndo not apply to our work for several reasons. First,\nwe use a large number of context-dependent mixing\nparameters to optimize the overall likelihood of the\ncombined model. Thus, the weighting in eq. (6) en-\nsures that the skip- k predictions are only invoked\nwhen the context is appropriate. Second, we adjust\nthe predictions of the skip- k transition matrices (by\nEM) so that they match the contexts in which they\nare invoked. Hence, the count-based models are in-\nterpolated in a way that is “consistent” with their\neventual use.\nTraining eﬃciency is another issue in evaluating\nlanguage models. The maximum entropy method\nrequires very long training times: e.g., 200 CPU-\ndays in Rosenfeld’s experiments. Our methods re-\nquire signiﬁcantly less; for example, we trained the\nsmoothed m = 2 mixed-order model, from start to\nﬁnish, in less than 12 CPU-hours (while using a\nlarger training corpus). Even accounting for diﬀer-\nences in processor speed, this amounts to a signiﬁ-\ncant mismatch in overall training time.\nIn conclusion, let us mention some open problems\nfor further research. Aggregate Markov models can\nbe viewed as approximating the full bigram tran-\nsition matrix by a matrix of lower rank. (From\neq. (1), it should be clear that the rank of the class-\nbased transition matrix is bounded by the num-\nber of classes, C.) As such, there are interesting\nparallels between Expectation-Maximization (EM),\nwhich minimizes the approximation error as mea-\nsured by the KL divergence, and singular value de-\ncomposition (SVD), which minimizes the approxi-\nmation error as measured by the L2 norm (Press\net al., 1988; Sch¨ utze, 1992). Whereas SVD ﬁnds a\nglobal minimum in its error measure, however, EM\nonly ﬁnds a local one. It would clearly be desirable\nto improve our understanding of this fundamental\nproblem.\nIn this paper we have focused on bigram models,\nbut the ideas and algorithms generalize in a straight-\nforward way to higher-order n-grams. Aggregate\nmodels based on higher-order n-grams (Brown et al.,\n1992) might be able to capture multi-word struc-\ntures such as noun phrases. Likewise, trigram-based\nmixed-order models would be useful complements to\n4-gram and 5-gram models, which are not uncom-\nmon in large-vocabulary language modeling.\nA ﬁnal issue that needs to be addressed is\nscaling—that is, how the performance of these mod-\nels depends on the vocabulary size and amount\nof training data. Generally, one expects that the\nsparser the data, the more helpful are models that\ncan intervene between diﬀerent order n-grams. Nev-\nertheless, it would be interesting to see exactly how\nthis relationship plays out for aggregate and mixed-\norder Markov models.\nAcknowledgments\nWe thank Michael Kearns and Yoram Singer for use-\nful discussions, the anonymous reviewers for ques-\ntions and suggestions that helped improve the paper,\nand Don Hindle for help with his language modeling\ntools, which we used to build the baseline models\nconsidered in the paper.\nReferences\nP. Brown, V. Della Pietra, P. deSouza, J. Lai, and R.\nMercer. 1992. Class-based n-gram models of natural\nlanguage. Computational Linguistics18(4):467–479.\nS. Chen and J. Goodman. 1996. An empirical study\nof smoothing techniques for language modeling. In\nProceedings of the 34th Meeting of the Association\nfor Computational Linguistics.\nI. Dagan, F. Pereira, and L. Lee. 1994. Similarity-\nbased estimation of word co-occurrence probabili-\nties. In Proceedings of the 32nd Annual Meeting of\nthe Association for Computational Linguistics.\nA. Dempster, N. Laird, and D. Rubin. 1977. Max-\nimum likelihood from incomplete data via the EM\nalgorithm. Journal of the Royal Statistical Society\nB39:1–38.\nX. Huang, F. Alleva, H. Hon, M.-Y. Hwang, K.-F.\nLee, and R. Rosenfeld. 1993. The sphinx-ii speech\nrecognition system: an overview. Computer Speech\nand Language, 2:137–148.\nF. Jelinek and R. Mercer. 1980. Interpolated es-\ntimation of Markov source parameters from sparse\ndata. In Proceedings of the Workshop on Pattern\nRecognition in Practice.\nF. Jelinek, R. Mercer, and S. Roukos. 1992. Princi-\nples of lexical language modeling for speech recogni-\ntion. In S. Furui and M. Sondhi, eds. Advances in\nSpeech Signal Processing. Mercer Dekker, Inc.\nS. Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of\na speech recognizer. IEEE Transactions on ASSP\n35(3):400–401.\nH. Ney, U. Essen, and R. Kneser. 1994. On structur-\ning probabilistic dependences in stochastic language\nmodeling. Computer Speech and Language8:1–38.\nF. Pereira, N. Tishby, and L. Lee. 1993. Distribu-\ntional clustering of English words. In Proceedings\nof the 30th Annual Meeting of the Association for\nComputational Linguistics.\nW. Press, B. Flannery, S. Teukolsky, and W. Vet-\nterling. 1988. Numerical Recipes in C. Cambridge\nUniversity Press: Cambridge.\nR. Rosenfeld. 1996. A Maximum Entropy Approach\nto Adaptive Statistical Language Modeling. Com-\nputer Speech and Language, 10:187–228.\nH. Sch¨ utze. 1992. Dimensions of Meaning. In Pro-\nceedings of Supercomputing, 787–796. Minneapolis\nMN.\nY. Singer. 1996. Adaptive Mixtures of Probabilistic\nTransducers. In D. Touretzky, M. Mozer, and M.\nHasselmo (eds). Advances in Neural Information\nProcessing Systems 8:381–387. MIT Press: Cam-\nbridge, MA."
}