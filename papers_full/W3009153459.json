{
  "title": "TranS^3: A Transformer-based Framework for Unifying Code Summarization and Code Search",
  "url": "https://openalex.org/W3009153459",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1657155912",
      "name": "Wang Wenhua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2084027238",
      "name": "Zhang Yu-qun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4380139250",
      "name": "Zeng, Zhengran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2331390008",
      "name": "Xu Guandong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2888651608",
    "https://openalex.org/W2911550516",
    "https://openalex.org/W3090608524",
    "https://openalex.org/W2084887015",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W2247374552",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2547932336",
    "https://openalex.org/W2129632406",
    "https://openalex.org/W2944464415",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2921535292",
    "https://openalex.org/W2514538448",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2906946247",
    "https://openalex.org/W2888307014",
    "https://openalex.org/W2941326118",
    "https://openalex.org/W3121414853",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2008398646",
    "https://openalex.org/W2034209539",
    "https://openalex.org/W2949297108",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2735706718",
    "https://openalex.org/W2851896161",
    "https://openalex.org/W2082160726",
    "https://openalex.org/W2954491573",
    "https://openalex.org/W2622620450",
    "https://openalex.org/W2156737235",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W2802729828",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2956048495",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2964300898",
    "https://openalex.org/W1984184199",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2023925487",
    "https://openalex.org/W2728773317",
    "https://openalex.org/W2791003924",
    "https://openalex.org/W2794601162",
    "https://openalex.org/W2803933017",
    "https://openalex.org/W2051167396",
    "https://openalex.org/W1515851193",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2514588627",
    "https://openalex.org/W2807964941",
    "https://openalex.org/W1520352740",
    "https://openalex.org/W2805937249",
    "https://openalex.org/W2133333349",
    "https://openalex.org/W2352511489",
    "https://openalex.org/W2126793110",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2998879364",
    "https://openalex.org/W3089397612",
    "https://openalex.org/W3104874136"
  ],
  "abstract": "Code summarization and code search have been widely adopted in sofwaredevelopmentandmaintenance. However, fewstudieshave explored the efcacy of unifying them. In this paper, we propose TranS^3 , a transformer-based framework to integrate code summarization with code search. Specifcally, for code summarization,TranS^3 enables an actor-critic network, where in the actor network, we encode the collected code snippets via transformer- and tree-transformer-based encoder and decode the given code snippet to generate its comment. Meanwhile, we iteratively tune the actor network via the feedback from the critic network for enhancing the quality of the generated comments. Furthermore, we import the generated comments to code search for enhancing its accuracy. To evaluatetheefectivenessof TranS^3 , we conduct a set of experimental studies and case studies where the experimental results suggest that TranS^3 can signifcantly outperform multiple state-of-the-art approaches in both code summarization and code search and the study results further strengthen the efcacy of TranS^3 from the developers' points of view.",
  "full_text": "TranS 3: A Transformer-based Framework for Unifying Code\nSummarization and Code Search\nWenhua Wang\n11760006@mail.sustech.edu.cn\nSouthern University of Science and Technology\nP.O. Box 1212\nShenzhen, Guangzhou, China 43017-6221\nYuqun Zhang\nzhangyq@sustech.edu.cn\nSouthern University of Science and Technology\nP.O. Box 1212\nShenzhen, Guangzhou, China 43017-6221\nZhengran Zeng\n11612527@mail.sustech.edu.cn\nSouthern University of Science and Technology\nP.O. Box 1212\nShenzhen, Guangzhou, China 43017-6221\nGuandong Xu\nGuandong.Xu@uts.edu.au\nUniversity of Technology, Sydney\nSydney, NSW, Australia 43017-6221\nABSTRACT\nCode summarization and code search have been widely adopted in\nso/f_tware development and maintenance. However, few studies have\nexplored the eﬃcacy of unifying them. In this paper, we propose\nTranS 3, a transformer-based framework to integrate code summa-\nrization with code search. Speci/f_ically, for code summarization,\nTranS 3 enables an actor-critic network, where in the actor net-\nwork, we encode the collected code snippets via transformer- and\ntree-transformer-based encoder and decode the given code snippet\nto generate its comment. Meanwhile, we iteratively tune the actor\nnetwork via the feedback from the critic network for enhancing the\nquality of the generated comments. Furthermore, we import the\ngenerated comments to code search for enhancing its accuracy. To\nevaluate the eﬀectiveness ofTranS 3, we conduct a set of experimen-\ntal studies and case studies where the experimental results suggest\nthat TranS 3 can signi/f_icantly outperform multiple state-of-the-art\napproaches in both code summarization and code search and the\nstudy results further strengthen the eﬃcacy of TranS 3 from the\ndevelopers’ points of view.\n1 INTRODUCTION\nCode summarization and code search have become increasingly\npopular in so/f_tware development and maintenance [1–6], because\nthey can help developers understand and reuse billions of lines of\ncode from online open-source repositories and thus signi/f_icantly\nenhance so/f_tware development and maintenance process [6]. In\nparticular, since much of the so/f_tware maintenance eﬀort is spent\non understanding the maintenance task and related so/f_tware source\ncode [7], eﬀective and eﬃcient documentation is quite essential\nto provide high-level descriptions of program tasks for so/f_tware\nmaintenance. To this end, code summarization aims to automati-\ncally generate natural language comments for documenting code\nsnippets [8]. On the other hand, over years various open-source\nand industrial so/f_tware systems have been rapidly developed where\nthe source code of these systems is typically stored in source code\nrepositories. Such source code can be treated as important reusable\nassets for developers because they can help developers understand\nhow others addressed similar problems for completing their pro-\ngram tasks, e.g., testing [9–15], fault localization [16–18], program\nrepair and synthesis [19–22], in multiple so/f_tware development do-\nmains [23–25]. Correspondingly, there also raises a strong demand\nfor an eﬃcient search process through a large codebase to /f_ind\nrelevant code for helping programming tasks. To this end, code\nsearch refers to automatically retrieving relevant code snippets\nfrom a large code corpus given natural language queries.\n/T_he recent research progress towards code summarization and\ncode search can mainly be categorized to two classes: information-\nretrieval-based approaches and deep-learning-based approaches. To\nbe speci/f_ic, theinformation-retrieval-based approaches derive the\nnatural language clues from source code, compute and rank the\nsimilarity scores between them and source code/natural language\nqueries for recommending comments/search results [2, 4, 26, 27].\n/T_hedeep-learning-based approaches use deep neural networks to\nencode source code/natural language into a hidden space, and utilize\nneural machine translation models for generating code comments\nand computing similarity distance to derive search results [5, 6, 28–\n30].\nBased on the respective development of code summarization and\ncode search techniques, we infer that developing a uni/f_ied technique\nfor optimizing both domains simultaneously is not only mutually\nbene/f_icial but also feasible. In particular, on one hand, since the\nnatural-language-based code comments can re/f_lect program seman-\ntics to strengthen the understanding of the programs [1], adopting\nthem in code search can improve the matching process with nat-\nural language queries [6]. Accordingly, injecting code comments\nfor code search is expected to enhance the search results [31]. On\nthe other hand, the returned search results can be utilized as an\nindicator of the accuracy of the generated code comments to guide\ntheir optimization process. Moreover, since code summarization\nand code search can share the same technical basis as mentioned\nabove, it can be inferred that it is feasible to build a framework to\nunify and advance both the domains. /T_herefore, it is essential to\nintegrate code summarization with code search.\nAlthough integrating code summarization with code search can\nbe promising, there remains the following challenges that may\ncompromise its performance: (1) state-of-the-art code summariza-\ntion techniques render inferior accuracy. According to the recent\nadvances in code summarization [26, 32, 33], the accuracy of the\ngenerated code comments appears to be inferior for real-world\narXiv:2003.03238v2  [cs.SE]  9 Mar 2020\napplicability (around 20% in BLEU-1 with many well-recognized\nbenchmarks). Integrating such code comments might lead to in-\naccuracies of matching natural language queries and further com-\npromise the performance of code search. (2) how to eﬀectively and\neﬃciently integrate code summarization with code search remains\nchallenging. Ideally, the goal of integrating code summarization\nwith code search is to optimize the performance of both domains\nrather than causing trade-oﬀs. Moreover, such integration is ex-\npected to introduce minimum overhead. To this end, it is essential\nto propose an eﬀective and eﬃcient integration approach.\nTo tackle the aforementioned problems, in this paper, we propose\na framework, namely TranS 3 for optimizing both code summariza-\ntion and code search based on a recent NLP technique—transformer\n[34]. Unlike the traditional CNN-based approaches that suﬀer from\nlong-distance dependency problem [35] and RNN-based approaches\nthat suﬀer from excessive load imposed by sequential computation\n[36], transformer advances in applying the self-a/t_tention mecha-\nnism which can parallelize the computation and preserve the inte-\ngral textual weights for encoding to achieve the optimal accuracy\nof text representation [34].\nTranS 3 consists of two components: the code summarization\ncomponent and code search component. Speci/f_ically, the code\nsummarization component is initialized by preparing a large-scale\ncorpus of annotated < code;comment > pairs to record all the\ncode snippets with their corresponding comments as the training\ndata. Next, we extract the semantic granularity of the training\nprograms for constructing a tree-transformer to encode the source\ncode into hidden vectors. Furthermore, such annotated pair vectors\nare injected into our deep reinforcement learning model, i.e., the\nactor-critic framework, for the training process, where the actor\nnetwork is a formal encoder-decoder model to generate comments\ngiven the input code snippets; and the critic network evaluates\nthe accuracy of the generated comments according to the ground\ntruth (the input comments) and give feedback to the actor network.\nAt last, given the resulting trained actor network and a code snip-\npet, its corresponding comment can be generated. Given a natural\nlanguage query, the code search component is launched by en-\ncoding the natural language query, the generated code comments,\nand the code snippets into the vectors respectively via transformer\nand tree-transformer. Next, we compute similarity scores between\nquery/code vectors and query/comment vectors for deriving and\noptimizing their weighted scores . Eventually, we rank all the code\nsnippets according to their score s for recommending the search\nresults. /T_he underlying transformer inTranS 3 can enhance the\nquality of the generated code and thus strengthen the code search\nresults by importing the impact from the generated comments.\nMoreover, since the code search component applies the encoder\ntrained by the code summarization component without incurring\nextra training process, its computing overhead can be maintained\nminimum.\nTo evaluate the eﬀectiveness and eﬃciency of TranS 3, we con-\nduct a set of experiments based on the GitHub dataset in [37] which\nincludes over 120,000 code snippets of Python functions and their\ncorresponding comments. /T_he experimental results suggest that\nTranS 3 can outperform multiple state-of-the-art approaches in both\ncode summarization and code search, e.g., TranS 3 can signi/f_icantly\nimprove the code summarization accuracy from 47.2% to 141.6%\nin terms of BLEU-1 and the code search accuracy from 5.1% to\n28.8% in terms of MRR compared with the selected state-of-the-art\napproaches. In addition, we also conduct case studies for both code\nsummarization and code search where the study results further\nverify the eﬀectiveness of TranS 3.\nIn summary, the main contributions of this paper are listed as\nfollows:\n• Idea. To the best of our knowledge, we build the /f_irst\ntransformer-based framework for integrating code summa-\nrization and code search, namelyTranS 3, that can optimize\nthe accuracy of both domains.\n• Technique. To precisely represent the source code, we de-\nsign a transformer-based encoder and a tree-transformer-\nbased encoder for encoding code and comments by in-\njecting the impact from the semantic granularity of well-\nformed programs.\n• Evaluation. To evaluateTranS 3, we conduct a substantial\nnumber of experiments based on real-world benchmarks.\n/T_he experimental results suggest thatTranS 3 can outper-\nform several existing approaches in terms of accuracy of\nboth code summarization and code search. In addition, we\nalso conduct empirical studies with developers. /T_he results\nsuggest that the quality of the generated comments and\nsearch results are widely acknowledged by developers.\n/T_he reminder of this paper is organized as follows. Section 2\nillustrates some preliminary background techniques. Section 3\ngives an example to illustrate our motivation for unifying code\nsummarization and code search. Section 4 elaborates the details of\nour proposed approach. Section 5 demonstrates the experimental\nand study results and analysis. Section 6 introduces the threats to\nvalidity. Section 7 reviews the related work. Section 8 concludes\nthis paper.\n2 BACKGROUND\nIn this section, we present the preliminary background techniques\nrelevant to TranS 3, including language model, transformer, and\nreinforcement learning, which are initialized by introducing basic\nnotations and terminologies. Let x = (x1, x2, . . . ,x|x|)denote the\ncode sequence of one function, where xt represents a token of the\ncode, e.g., … “def ”, “fact”, or “i” in a Python statement “def fact(i):”.\nLet y = (/y.alt1,/y.alt2, . . . ,/y.alt|y|)denote the sequence of the generated\ncomments, where |y|denotes the sequence length. Let T denote\nthe maximum step of decoding in the encoder-decoder framework.\nWe use notation /y.altl . . .m to represent the comment subsequence\n/y.altl , . . . ,/y.altm and D= {(xN , yN )}as the training dataset, where N\nis the size of training set.\n2.1 Language Model\nA language model refers to the decoder of neural machine trans-\nlation which is usually constructed as the probability distribution\nover a particular sequence of words. Assuming such sequence with\nits length T , the language model de/f_inesp(/y.alt1:T )as its occurrence\nprobability which is usually computed based on the conditional\nprobability from a window of n predecessor words, known as n-\ngram [38], as shown in Equation 1.\nInput\nMulti-\nHead\nAttention\nAdd\n& \nNorm\nAdd\n& \nNorm\nFeed\nForward\nInput\nEmbedding Output\n+\nPositional Encoding N ꓫ\nFigure 1: /T_he Transformer Model Architecture.\np(/y.alt1:T )=\ni=TÖ\nt=1\np(/y.altt |/y.alt1:t−1)≈\nt=TÖ\nt=1\np(/y.altt |/y.altt−(n−1):t−1) (1)\nWhile the n-gram model can only predict a word based on a\n/f_ixed number of predecessor words, a neural language model can\nuse predecessor words with longer distance to predict a word based\non deep neural networks which include three layers: an input layer\nwhich maps each word xt to a vector, a recurrent hidden layer\nwhich recurrently computes and updates a hidden state ht a/f_ter\nreading xt , and an output layer which estimates the probabilities\nof the subsequent words given the current hidden state. In par-\nticular, the neural network reads individual words from the input\nsentence, and predicts the subsequent word in turn. For the word\n/y.altt , the probability of its subsequent word /y.altt+1, p(/y.altt+1|/y.alt1:t )can be\ncomputed as in Equation 2:\np(/y.altt+1|/y.alt1:t )= /afii10069.ital (ht ) (2)\nwhere /afii10069.ital is a stochastic output layer (e.g., a so/f_tmax for discrete\noutputs) that generates output tokens with the hidden state ht\ncomputed as Equation 3:\nht = f (ht−1, w(xt )) (3)\nwhere w(xt )denotes the weight of the token xt .\n2.2 Transformer\nMany neural machine translation approaches integrate the a/t_ten-\ntion mechanism with sequence transduction models for enhancing\nthe accuracy. However, the encoding networks are still exposed\nwith challenges. To be speci/f_ic, the CNN-based encoding networks\nare subjected to long-distance dependency issues and the RNN-\nbased encoding networks are subjected to the long-time compu-\ntation. To address such issues, transformer [ 34] is proposed to\neﬀectively and eﬃciently improve the sequence representation by\nadopting the self-a/t_tention mechanism only. Many transformer-\nbased models e.g., BERT [39], ERNIE [40], XLNET [41], have been\nproposed and veri/f_ied to dramatically enhance the performance of\nvarious NLP tasks such as natural language inference , text classi/f_i-\ncation, and retrieval question answering [42, 43].\nTransformer consists of N identical layers where each layer con-\nsists of two sub-layers. /T_he /f_irst sub-layer realizes a multi-head\nself-a/t_tention mechanism, and the second sub-layer is a simple,\nposition-wise fully connected feed-forward neural network, as\nshown in Figure 1. Note that the output of the /f_irst sub-layer is\ninput to the second sub-layer and the outputs of both the sub-layers\nneed to be normalized prior to the subsequent process.\n2.2.1 Self-a/t_tention Mechanism./T_he a/t_tention function can be\ndescribed as mapping a query and a set of key-value pairs to an\noutput, where the query, keys, values, and output are all vectors.\n/T_he output is computed as a weighted sum of the values, where\nthe weight assigned to each value is computed by a compatibility\nfunction of the query with the corresponding key.\n/T_he input consists of queries, keys and values of the dimension\ndk . Accordingly, transformer computes the dot products of the\nquery with all keys, divides each resulting element by\n√\ndk , and\napplies a so/f_tmax function to obtain the weights on the values. In\npractice, we simultaneously compute the a/t_tention function on a set\nof queries which are packed together into a matrix Q. In addition,\nthe keys and values are also packed together into matrices K and\nV . /T_herefore, the matrix of outputs can be computed as:\nAttention (Q, K,V )= sof tmax(QKT\n√\ndk\n)V (4)\nInstead of implementing a single a/t_tention function, transformer\nadopts a multi-head a/t_tention which allows the model to jointly\na/t_tend to information from diﬀerent representation subspaces at\ndiﬀerent positions.\n/T_he self-a/t_tention mechanism derives the relationships between\nthe current input token and all the other tokens to determine the\ncurrent token vector for the /f_inal input representation. By taking\nadvantage of the overall token weights, such mechanism can dra-\nmatically alleviate the long-distance dependency problem caused\nby the CNN-based transduction models, i.e., compromising the con-\ntributions of the long-distance tokens. Moreover, the multi-head\nself-a/t_tention mechanism can parallelize the computation and thus\nresolve the excessive computing overhead caused by the RNN-based\ntransduction models which sequentially encode the input tokens.\n2.2.2 Position-wise Feed-Forward Neural Network. In addition\nto multi-head self-a/t_tention sub-layers, each of the layers contains\na fully connected feed-forward neural network, which is applied\nto each position separately. Since transformer contains no recur-\nrence or convolution, in order to utilize the order of the sequence,\ntransformer injects “positional encodings” to the input embedding.\nSince transformer has been veri/f_ied to be dramatically eﬀective\nand eﬃcient in encoding word sequences, we infer that by rep-\nresenting code as a sequence, transformer can also be expected\nto excel in the encoding eﬃcacy. /T_herefore, inTranS 3, we adopt\ntransformer as the encoder.\n2.3 Reinforcement Learning for Code\nSummarization\nIn code summarization, reinforcement learning (RL)[44] refers to\ninteracting with the ground truth, learning the optimal policy from\nthe reward signals, and generating texts in the testing phase. It can\npotentially solve the exposure bias problem introduced by the max-\nimum likelihood approaches which is used to train the RNN model.\nSpeci/f_ically in the inference stage, a typical RNN model generates a\nsequence iteratively and predicts next token conditioned on its pre-\nviously predicted ones that may never be observed in the training\ndata [45]. Such a discrepancy between training and inference can\nbecome cumulative along with the sequence and thus prominent\nas the length of sequence increases. While in the reinforcement-\nlearning-based framework, the reward, other than the probability\nof the generated sequence, is calculated to give feedback to train\nthe model to alleviate such exposure bias problem. Such text gener-\nation process can be viewed as a Markov Decision Process (MDP)\n{state , action, polic/y.alt, reward }. Speci/f_ically in the MDP se/t_tings,\nstate st at time t consists of the code snippets x and the predicted\nwords /y.alt0,/y.alt1, . . . ,/y.altt . /T_heaction space is de/f_ined as the dictionary\nYwhere the words are drawn, i.e., /y.altt ⊂Y. Correspondingly, the\nstate transition function P is de/f_ined asst+1 = {st ,/y.altt }, where the\naction (word) /y.altt becomes a part of the subsequent state st+1 and\nthe reward rt+1 can be derived. /T_he objective of the generation\nprocess is to /f_ind apolic/y.alt that iteratively maximizes the expected\nreward of the generated sentence sampled from the model’spolic/y.alt,\nas shown in Equation 5,\nmax\nθ\nL(θ)= max\nθ\nE x∼D\nˆy∼Pθ(·|x)\n[R(ˆy, x)] (5)\nwhere θ is the policy parameter to be learned, Dis the training\nset, ˆy denotes the predicted action s/words, and R is the reward\nfunction.\nTo learn the policy, many approaches have been proposed, which\nare mainly categorized into two classes [46]: (1) the policy-based\napproaches (e.g., Policy gradients [47]) which optimize the policy\ndirectly via policy gradient and (2) the value-based approaches\n(e.g., Q-learning [ 48]) which learn the Q-function, and at each\ntime the agent selects the action with the highest Q-value. It has\nbeen veri/f_ied that the policy-based approaches may suﬀer from\na variance issue and the value-based approaches suﬀer from a\nbias issue [49]. To address such issues, the Actor-Critic learning\napproach is proposed [50] to combine the strengths of both policy-\nand value-based approaches where the actor chooses an action\naccording to the probability of each action and the critic assigns\nthe value to the chosen action for speeding up the learning process\nfor the original policy-based approaches.\nIn this paper, we adopt the actor-critic learning model for code\nsummarization of TranS 3.\n3 ILLUSTRATIVE EXAMPLE\nIn this section, we use a sample Python code snippet to illustrate\nour motivation for unifying code summarization and code search.\nFigure 2 shows the Python code snippet, the comment generated\nby our approach and its associated natural language query in our\ndataset. Traditional code search approaches usually compute the\nsimilarity scores of the query vector and the code snippet vectors\nfor recommending and returning the relevant code snippets. On the\nother hand, provided the comment information, it is plausible to\nenhance the code search results by enabling an additional mapping\nprocess between the query and the comments corresponding to the\ncode snippets. For example, in Figure 2, given the query “get the\nrecursive list of target dependencies”, although the code snippet can\nprovide some information such as “dependencies”, “target”, which\nmight be helpful for being recommended, its eﬃcacy can be com-\npromised due to the disturbing information such as “dicts”, “set”,\n“pending” in the code snippet. It is expected to enhance the search\nresult by integrating the comment information during the search-\ning process when it has the identical “target”, “dependencies” with\nCode Snippet:\n1.def DeepDependencyTargets(target_dicts, roots):\n2.  dependencies = set() \n3. pending = set(roots)\n4. while pending:\n5.     r = pending.pop()\n6.     if (r in dependencies):\n7.     continue\n8. dependencies.add(r)\n9. spec = target_dicts[r]\n10.    pending.updata(set(spec.get(‘dependencies’,[])))\n11. pending.updata(set(spec.get(‘dependencies_original’,[])))\n12.  return list((dependencies – set(roots)))\nGenerated comment: returns the path of the target dependencies. \nQuery:  get the recursive list of target dependencies.\nFigure 2: An example Python code snippet and the corre-\nsponding query and generated comment.\nthe query. To this end, we infer that a be/t_ter code search result can\nbe expected if high-quality comment information can be integrated\nin the code search process.\n4 THE APPROACH OFTRANS 3\nWe formulate the research problem of integrating code summariza-\ntion with code searchas as follows:\n• First, we a/t_tempt to /f_ind a policy that generates a sequence\nof words y = (/y.alt1,/y.alt2, . . . ,/y.alt|y|)from dictionary Yto an-\nnotate the code snippets in the corpus as their comments.\nNext, given a natural language query x = (x1, x2, . . . ,x|x|),\nwe aim to /f_ind the code snippets that can satisfy the query\nunder the assistance of the generated comments.\nTo address such research problem, we proposeTranS 3 with its\nframework shown in Figure 3, where Figure 3(a) presents the code\nsummarization part and Figure 3(b) presents the code search part.\n4.1 Transformer- and Tree-Transformer-based\nEncoder\nIn TranS 3, we utilize transformer to build the encoder. Speci/f_ically,\nwe develop the transformer-based encoder to encode the comments,\nthe query and each program statement. Moreover, we develop a\ntree-transformer-based encoder that exploits the semantic granular-\nity information of programs for enhancing the program encoding\naccuracy.\nTransformer-based Encoder./T_he transformer-based encoder\nis initialized by embedding the input tokens into vectors via word\nembedding [51]. Speci/f_ically, we tokenize the natural language\ncomments/queries based on their intervals and the code based\non a set of symbols, i.e., { . , ” ’ : * () ! - (space)}. Next, we\napply word embedding to derive each token vector in one input\nsequence. Furthermore, for each token vector xi , we derive its\nrepresentation according to the self-a/t_tention mechanism as follows:\n(1) deriving the query vector qi , the key vector ki , and the value\nvector /v.alti by multiplying xi with a randomly-generated matrix, (2)\ncomputing the scores of xi from all the input token vectors by the\ndot product of qi ·kj , where j/uni03F5 [1, n]and n denotes the number of\ninput tokens, (3) dividing the scores ofxi by\n√\ndk where dk denotes\nthe dimension number ofki and normalizing the results by so/f_tmax\nCode \nSnippet\nSummary\nCode \nSnippet\nComment\n(a)   Code Summarization\nActor\nQuery\n(b)   Code search\nvc\nTree-transformer vc\nvq\nvsReward\nSim(vq, vc)\nSim(vq, vs)\nTransformer\nTransformer\nRanking\nFigure 3: /T_he Overview of Our Proposed approachTranS 3. (a) the code summarization part; (b) the code search part.\n1\nInput\nEmbedding\nQuery\nKey\nValue\nDot product\nDivide \nSoftmax\nWeighted\nAdd\n2\n 1\n 2\n8\n 8\n 8\n5\n 2\n 7\n18\n 10\n 7\n1\n 2\n 1\n 1\n7\n 7\n 7\n4\n 2\n 6\n17\n 10\n 5\n8.8\n 1.2\n 0.6\n10\n 17\n 10\n 5\nx1\nq1\nk1\nv1\nz1\nx2\nq2\nk2\nv2\nv'2\nz2\nSoftware Engineering\nq1·k2 = 96\n14 12\n0.88 0.12\nv'1\nq1·k1 = 112\n15.84 6.16 2.04\n6.7617.88\nFigure 4: An example of Self-Attention Mechanism.\nto obtain the weights (contributions) of all the input token vectors,\n(4) multiplying such weights and their corresponding value vectors\nto obtain an interim vector space/v.alt\n′\n, and (5) summing all the vectors\nin /v.alt\n′\nfor deriving the /f_inal vector ofxi , zi . As a result, all the token\nvectors are input to the feed-forward neural network to obtain\nthe /f_inal representation vector of the input sequence of natural\nlanguage comment.\nWe use Figure 4 as an example to illustrate how the transformer-\nbased encoder works, where the token vectors of “So/f_tware” and\n“Engineering” are embedded as x1 and x2 respectively. For x1, its\ncorresponding q1, k1, and /v.alt1 are derived in the beginning. Next, its\nscores from all the token vectors, i.e., x1 and x2, can be computed\nby q1 ·k1 (112) and q1 ·k2 (96). Assuming dk is 64, by dividing the\nresulting dot products by\n√\ndk and normalizing, the weights of x1\nand x2 can be computed as 0.88 and 0.12. At last, z1 can be derived\nby 0.88*/v.alt1 + 0.12*/v.alt2.\nTree-Transformer-based Code Encoder.It can be observed\nthat well-formed source code can re/f_lect the program semantics\nthrough its representations, e.g., the indents of Python. In general,\nin a well-formed program, the statement with fewer indents tends\nto indicate more abstracted semantics than the one with longer\nindents. /T_herefore, we infer that incorporating the indent-based\nsemantic granularity information for encoding can inject program\nsemantics for program comprehension and thus be promising to\nAlgorithm 1Tree Transformer Encoding Algorithm.\nInput : ordered tree ()\nOutput: vector representation of the tree\n1: function P/o.sc/s.sc/t.sc/o.sc/r.sc/d.sc/e.sc/r.scT/r.sc/a.sc/v.sc/e.sc/r.sc/s.sc/e.sc\n2: node list ←root node\n3: if isLeaf(root node) then\n4: return Transformer(node list);\n5: else\n6: for i in range(len(root node’s children))do\n7: node list.append(PostOrderTraverse(i’s children))\n8: return Transformer(node list)\nenhance the encoding accuracy. Such injection can potentially\nbe advanced when leveraging transformer. In particular, in addi-\ntion to the original self-a/t_tention mechanism which determines\nthe token vector score by only importing the token-level weights,\nstatement-level impacts can be injected by analyzing statement\nindents, obtaining the semantic hierarchy of the code, and realizing\nthe hierarchical encoding process.\nIn this paper, we design a tree-transformer-based encoder that\nincorporates indent-based semantic granularity for encoding pro-\ngrams. Firstly, we construct an ordered tree according to the indent\ninformation of a well-formed program. In particular, by reading the\nprogram statements in turn, we initialize the tree by building the\nroot node out of the function de/f_inition statement. Next, we itera-\ntively label each of the subsequent statements with an indent index\nassigned by counting the indents such that the statements with the\nsame indent index i are constructed as the ordered sibling nodes\nand the preceding statement above such statement block with the\nindent index i −1 is constructed as their parent node. Secondly,\nwe encode each node (i.e., each statement) of the tree into a vector\nby transformer. At last, we build the tree-transformer accordingly\nto further encode all the vector nodes of the tree for obtaining the\ncode snippet representations. Speci/f_ically, we traverse the tree in a\npost-order manner. Assuming a nodeni and its parent nodenj , if ni\nis a leaf node, we replace the vector ofnj , namely Vnj by the vector\nlist {Vni , Vnj } and subsequently traverse nj ’s other child nodes;\notherwise, we traverse ni ’s child nodes. Next, we encode nodenj\nwith the updated vector list {Vni , Vnj } by transformer when it has\ndef DeepDependencyTargets\n(target_discts, roots):\ndependencies\n=set()\npending=\nset(roots)\nwhile\npending:\nr=pending\n.pop()\nreturn\nlist(…)\nif (r in \ndependencies):\ncontinue\n...\nFigure 5: /T_he Source Code and the Tree Structure.\nno child nodes. /T_he tree-transformer encoding process is shown as\nAlgorithm 1.\nFigure 5 illustrates indent-based tree representation of the code\nsnippet given in Figure 2. We use this example to describe how\nthe tree-transformer-based encoder works. Speci/f_ically in Figure 2,\nwe construct nodes “Dependencies = set()”, “pending=set(roots)”,\n“while pending:” and “return list(…)” as siblings because they are\nassigned with the same indents and one-shorter-indent preced-\ning statement “def DeepDependencyTargets(targetdicts, roots):”,\nwhich is constructed as their parent node. /T_hen, we encode all the\nstatement nodes into vectors by transformer respectively. Next, as\nthe root’s child nodes “Dependencies = set()” and “pending=set(roots)”\nare leaf nodes, we replace the root vector by the vector list of them\nthree. /T_hen, since the root’s child node “while pending:” is not the\nleaf node, we /f_irst encode its child node “if (r in dependencies):”\nwith “continue” by transformer, and then encode the resulting vec-\ntor with the siblings of “while pending:” and “if (r in dependencies):”\ntogether by transformer. At last, we encode the root node with\nall its child nodes to obtain the /f_inal representation of this code\nsnippet.\n4.2 Code Summarization\nInitialized by collecting code snippets with their associated com-\nments and forming < code;comment > pairs for training the code\nsummarization model, the code summarization component is imple-\nmented via reinforcement learning (i.e., the actor-critic framework),\nwhere the actor network establishes an encoder-decoder mecha-\nnism to derive code comments and the critic network iteratively pro-\nvides feedback for tuning the actor network. In particular, the actor\nnetwork leverages a transformer-based or a tree-transformer-based\nencoder to encode the collected code into hidden space vectors\nand applies a transformer-based decoder to decode them to natural\nlanguage comments. Next, by computing the similarity between the\ngenerated and the ground-truth comments, the critic network iter-\natively provides feedback for tuning the actor network. As a result,\ngiven a code snippet, its corresponding natural language comment\ncan be generated based on the trained code summarization model.\n4.2.1 Actor Network. /T_he actor network is composed of an\nencoder and decoder.\nEncoder. We construct the tree representation of the source\ncode and establish a tree-transformer, described as Section 4.1, to\nencode the source code into hidden space vectors for the code\nrepresentation.\nDecoder. A/f_ter obtaining the code snippet representations,\nTranS 3 implements the decoding process for them, i.e., generating\ncomments from the hidden space, to derive their associated natural\nlanguage comments.\n/T_he decoding process is launched by generating an initial de-\ncoding state s0 = {x}by encoding the given code snippet. At\nstep t, state st is generated to maintain the source code snippet\nand the previously generated words /y.alt1. . .t−1, i.e., st = {x,/y.alt1. . .t−1}.\nSpeci/f_ically, the previously generated words/y.alt1. . .t−1 are encoded\ninto a vector by transformer and subsequently concatenated with\nstate st−1. Our approach predicts the tth word by using a so/f_tmax\nfunction. Let p(/y.altt |st )denote the probability distribution of the tth\nword /y.altt in the state st , we can obtain the following equation:\np(/y.altt |st )= sof tmax(Ws st + bs ) (6)\nNext, we update st to st+1 to generate the next word. /T_his process\nis iterated till it exceeds the max-step or generates the end-of-\nsequence (EOS) token for generating the whole comment corre-\nsponding to the code snippet.\n4.2.2 Critic Network. To enhance the accuracy of the generated\ncode comments,TranS 3 applies a critic network to approximate the\nvalue of the generated comments at timet to issue a feedback to tune\nthe network iteratively. Unlike the actor network which outputs a\nprobability distribution, the critic network outputs a single value\non each decoding step. To illustrate, given the generated comments\nand the reward functionr, the value functionV is de/f_ined to predict\nthe total reward from the state st at time t, which is formulated as\nfollows,\nV (st )= Est+1:T ,\n/y.altt:T\n[T −tÕ\nl=0\nrt+l |/y.altt+1, ··· ,/y.altT , h\n]\n(7)\nwhere T is the max step of decoding and h is the representation of\ncode snippet. By applying the reward function, we can obtain an\nevaluation score (e.g., BLEU) when the generation process of the\ncomment sequences is completed. Such process is terminated when\nthe associated step exceeds T or generates the end-of-sequence\n(EOS) token. For instance, a BLEU-based reward function can be\ncalculated as:\nr = exp (1\nN ∗\nNÕ\ni=1\nlo/afii10069.ital pn) (8)\nwhere pn =\nÍ\nn−/afii10069.italr am∈c count (n−/afii10069.ital ram )Í\nn−/afii10069.italr am∈c′count (n−/afii10069.ital ram ), and c is the generated com-\nment an c\n′\nis the ground truth.\n4.2.3 Model Training. For the actor network, the training objec-\ntive is to minimize the negative expected reward, which is de/f_ined\nas L(θ)= −E/y.alt1, . . .,T ∼π(ÍT\nl=t rt ), where θ is the parameter set of\nthe actor network. De/f_ining policy as the probability of a generated\ncomment, we adopt the policy gradient approach to optimize the\npolicy directly, which is widely used in reinforcement learning.\n/T_he critic network a/t_tempts to minimize the following loss func-\ntion,\nL(ϕ)= 1\n2\n\r\r\rV (st )−Vϕ(st )\n\r\r\r\n2\n(9)\nwhere V (st )is the target value, Vϕ(st )is the value predicted by the\ncritic network with its parameter set ϕ. Eventually, the training for\ncomment generation is completed a/f_terL(ϕ)converges.\nDenoting all the parameters as Θ = {θ, ϕ}, the total loss of our\nmodel can be represented as L(Θ)= L(θ)+ L(ϕ). We employ\nstochastic gradient descend with the diagonal variant of AdaGrad\n[52] to tune the parameters of TranS 3 for optimizing the code\nsummarization model.\n4.3 Code Search\nGiven a natural language query, TranS 3 encodes all the code snip-\npets and the generated comments into vector sets by tree-transformer-\nbased encoder and transformer-based encoder respectively, and en-\ncodes the query into a vector by transformer-based encoder. Next,\nwe compute the similarity scores between the query vector and\nthe vectors in both the code snippets vector set and the generated\ncomments vector set. At last, we rank all the code snippets to rec-\nommend the search results derived from the linear combination of\nthe two similarity score sets which are trained for optimality.\nAs shown in Figure 3 (b), we encode the code snippets and\nthe generated comments into vector spaces {Vc }and {Vs }by the\ntree-transformer-based encoder and transformer-based encoder\nrespectively. We also encode the given natural language query into\na vector Vq by transformer-based encoder. Next, we compute the\nsimilarity scores between Vq and the vectors of the code snippet i\nfrom both {Vc }and {Vs }as sim(Vq,Vci )and sim(Vq,Vsi ). Further-\nmore, we derive the weighted score of the code snippet i, score i ,\nby linearly combining sim(Vq,Vci )and sim(Vq,Vsi ), as shown in\nEquation 10. Eventually, we rank all the code snippets according to\ntheir score s for recommending the search results,\nscore (Q,C)= β ∗sim(Vq,Vci )+ (1 −β)∗sim(Vq,Vsi ) (10)\nwhere β is a parameter that ranges from 0 to 1 and determined\na/f_ter training,sim()is computed by consine. Specially, given the\nquery qi , the training objective is to ensurescore (qi , ci )> score (qi , cj ),\nwhere cj demonstrates the code snippets in the dataset expect ci .\n5 EVALUATION\nWe conduct a set of extensive experiments on the eﬀectiveness\nand eﬃciency of TranS 3 in terms of both the code summariza-\ntion and code search components compared with state-of-the-art\napproaches.\n5.1 Experimental Setups\nTo evaluate the performance of our proposed approach, we use\nthe dataset presented in [37] where over 120,000 <code;comment>\npairs are collected from various Python projects in GitHub [53] with\n50,400 code tokens and 31,350 comment tokens in its vocabulary\nrespectively. For cross validation, we shuﬄe the original dataset\nand use the /f_irst 60% for training, 20% for validation, and the rest\nfor testing.\nIn our experiments, the word embedding size is set to 1280, the\nbatch size is set to 2048, the layer size is set to 6, and the head\nnumber is set to 8. We pretrain both actor network and critic net-\nwork with 20000 steps each, and train the actor-critic network with\n100000 steps simultaneously. For the code search part, the com-\nments in the dataset are utilized for the query. All the experiments\nin this paper are implemented with Python 3.5, and run on a com-\nputer with a 2.8 GHz Intel Core i7 CPU, 64 GB 1600 MHz DDR3\nRAM, and a Saturn XT GPU with 24 GB memory running RHEL\n7.5.\n5.2 Result Analysis\n5.2.1 Code summarization. To evaluate the code summarization\ncomponent ofTranS 3, we select several state-of-the-art approaches,\ni.e., Hybrid-DeepCom [54], CoaCor [6], and AutoSum [55] for per-\nformance comparison withTranS 3. In particular, Hybrid-DeepCom\n[54] utilizes the AST sequence converted by traversing the AST as\nthe code representation and input the AST sequence to the GRU-\nbased NMT for code summarization via combining lexical and struc-\nture information. CoaCor [6] utilizes the plain text of source code\nand an LSTM-based encoder-decoder framework for code summa-\nrization. AutoSum [ 55] utilizes a tree-LSTM-based NMT model\nand inputs the code snippet as plain text to the code-generation\nmodel with reinforcement learning for performance enhancement.\nSimilarly, the evaluation for TranS 3 is also designed to explore\nthe performance of its diﬀerent components, where TranS 3base\nadopts the transformer-based encoder; TranS 3tree adopts the tree-\ntransformer-based encoder for source code; andTranS 3tree +RL , i.e.,\nthe complete TranS 3, utilizes the tree-transformer-based encoder\nfor source code and reinforcement learning for further enhancing\nthe code summarization model.\nWe evaluate the performance of all the approaches based on four\nwidely-used evaluation metrics adopted in neural machine transla-\ntion and image captioning: BLEU [56], METEOR [57], ROUGE [58]\nand CIDER [59]. In particular, BLEU measures the average n-gram\nprecision on a set of reference sentences with a penalty for short\nsentences. METEOR evaluates how well the results capture content\nfrom the references via recall which is computed via stemming and\nsynonymy matching. ROUGE-L imports account sentence level\nstructure similarity and identi/f_ies the longest co-occurrence in se-\nquential n-grams. CIDER is a consensus-based evaluation protocol\nfor image captioning that evaluates the similarity of the generated\ncomments and the ground truth.\nTable 1 demonstrates the code summarization results of all the\napproaches in terms of the selected metrics. While the compared\napproaches achieve close performances, e.g., around 20% in terms of\nBLEU-1, TranS 3 can approximate 38%. In particular, we can obtain\nthe following detailed /f_indings. First, we can observe thatTranS 3\ncan signi/f_icantly outperform all the compared approaches in terms\nof all the evaluated metrics. For instance, the complete TranS 3,\ni.e., TranS 3tree +RL can outperform all the compared approaches\nfrom 47.2% to 141.6% in terms of BLEU-1. Such performance ad-\nvantages can indicate the superiority of the transformer-enabled\nself-a/t_tention mechanism over the mechanisms, including the a/t_ten-\ntion mechanism, that are adopted in other RNN-based approaches,\nbecause the self-a/t_tention mechanism can eﬀectively capture the\nTable 1: Code summarization results with diﬀerent metrics.\n(Best scores are in boldface.)\nApproaches BLEU-1 METEOR ROUGE-L CIDER\nHybrid-\nDeepCom\n15.60 6.09 14.33 51.88\nCoaCor 25.60 9.52 29.38 78.11\nAutoSum 25.27 9.29 39.13 75.01\nTranS 3base 27.69 10.26 41.87 81.01\nTranS 3tree 32.05 11.74 45.92 84.56\nTranS 3tree +RL 37.69 13.52 51.27 87.24\nimpacts of the overall text on all the tokens of the input sequences\nfor be/t_ter re/f_lecting their semantics and thus optimizing the lan-\nguage model weights. Next, we can verify that each component of\nTranS 3 is eﬀective for enhancing the performance. For instance, by\napplying the tree-transformer-based encoder,TranS 3tree can dra-\nmatically outperformTranS 3base that only applies the transformer-\nbased encoder by 15.7% in terms of BLEU-1. We can verify that our\ntree transformer based on identifying and leveraging the indent-\nbased program semantic granularity can eﬀectively strengthen\nthe language model by augmenting the semantic level informa-\ntion for tokens. Moreover, by applying reinforcement learning,\nTranS 3tree +RL outperforms TranS 3, i.e., TranS 3tree by 17.5% in\nterms of BLEU-1, which can further verify the strength of rein-\nforcement learning as veri/f_ied in [6, 55]. Note that the performance\nof certain approaches, e.g., Hybrid-DeepCom, dramatically diﬀers\nfrom its original performance in [54] mainly because of the training\ndata and programming language diﬀerences.\nWe also conduct a set of case studies to further evaluate the ef-\nfectiveness of TranS 3. In particular, we /f_irst collect Python projects\nfrom GitHub and input them to ourTranS 3-trained model for gener-\nating their corresponding comments. Next, we issue such generated\ncomments to the corresponding developers for their evaluations\non the quality of the generated comments. In total, we received 24\nresponses, among which 11 developers con/f_irmed the correctness\nof the generated comments to summarize their code snippets. In\naddition, 5 developers extended detailed explanations of the as-\nsociated code which also expose their support to our generated\ncomments. /T_he rest responses are unrelated to the correctness of\nour generated comments. Table 2 presents selected examples of the\ndeveloper feedback where the /f_irst and second case indicate that\nthe developers con/f_irm the correctness of our generated comments\nwhile the third case reveals that the developer is supportive to the\ngenerated comment though he did not directly present it.\n5.2.2 Code search. To evaluate the eﬀectiveness of the code\nsearch component of TranS 3, we select several state-of-the-art\napproaches for comparison. Firstly, for the aforementioned ap-\nproaches Hybrid-DeepCom, AutoSum, and CoaCor, we utilize the\ngenerated comments of those approaches as the input for the code\nsearch part ofTranS 3. In addition to further utilizing them for code\nsearch, we also compare TranS 3 with DeepCS [5] which utilizes\nRNN to encode code and query and compute the distance between\nthe code vector and the query vector for returning the code snip-\npets with the closest vectors. /T_he performance of code search is\nevaluated in terms of four widely-used metrics: MRR (Mean Recip-\nrocal Rank) [60], nDCG (normalized Discounted Cumulative Gain)\n[61] and Success Rate@k [62], where MRR measures the average\nreciprocal ranks of results given a set of queries and the reciprocal\nrank of a query is computed as the inverse of the rank of the /f_irst\nhit result; nDCG considers the ranking of the search results which\nevaluates the usefulness of result based on its position in the result\nlist; and Success Rate@k measures the percentage of queries for\nwhich more than one correct result exist in the topk ranked results.\nTable 3 shows the code search result comparisons between our\nproposed approach and the aforementioned baselines where we\ncan observe that TranS 3 can outperform all the other approaches\nin terms of all the evaluate metrics. Speci/f_ically, in terms of MRR,\nTranS 3tree +RL can outperform all the other approaches from 5.12%\nto 28.8%. Compared with the code summarization results, the ad-\nvantages of TranS 3 over the same adopted approaches on code\nsearch dramatically shrinks which can be discussed as follows: (1)\nthe code search metrics are naturally subject to less distinguishable\nresults than the code summarization metrics. For Hybrid-DeepCom,\nAutoSum, and TranS 3 which all utilize the generated comments\nto strengthen their code search performance, their adopted code\nsummarization metrics are essentially based on word frequency\nwhich generally are /f_ine-grained, e.g., BLEU-based metrics, while\ntheir code search metrics are generally based on coarse-grained\nquery-wise comparisons. /T_herefore, the code summarization met-\nrics tend to result in distinguishable results for diﬀerent techniques\nbecause they are likely to re/f_lect the trivial diﬀerence between two\ngenerated comments. However, their corresponding code search\nresults might not be that distinguishable because the two generated\ncomments might be trained to result in the result in the identi-\ncal code rankings. For instance, suppose two code summarization\napproaches generate the comments “returns the path of the target de-\npendencies” and “derive a target-dependency list” respectively. While\nthey can be used to represent the same code snippets, they may\nresult in diﬀerent BLEU scores because they consist of diﬀerent\nwords. However, if they are used for code search, they can both\nrank the code snippet of Figure 2 on the top and thus result in\nthe identical score in terms of the code search metrics. (2) Coa-\nCor [6] can approach a close performance to TranS 3 because its\nrewarding mechanism utilizes the search accuracy to guide the\ncode annotation generation and search modeling directly. However,\nWe can observe thatTranS 3 signi/f_icantly outperforms CoaCor in\nterms of code summarization (by 47.2%). /T_herefore, to bridge such\nperformance gap, CoaCor has to pay extra eﬀort for enhancing its\nmodeling process while TranS 3 can limit its eﬀort in training the\nmodel once and for all for optimizing both code summarization and\ncode search.\nWe also conduct a case study to evaluate the eﬀectiveness of\nTranS 3. We organized 5 postgraduate students and 5 developers\nwith certain Python background. We designed 15 programming\ntasks where each participant is asked to choose 3 tasks for code\nsearch using TranS 3 as well as our benchmark. Two example tasks\nare listed as follows:\nTable 2: Sample issues for code summarization case study\nIssue link\nGenerated\ncomment\nFeedback\n1 h/t_tps://github.com/\nmikunit567/GAE/issues/1\nValidate a given xsrf to-\nken by retrieving it.\n“Yes, this is correct. Validate a retrieved XSRF from the memory cache and\nthen with the token perform an associated action. ”\n2 h/t_tps://github.com/\nhamzafaisaljarral/scoop/\nissues/1\nIterates through the\nglob nodes.\n“Yup you have got that right but for be/t_ter understanding you have to look\ninto django-shop documentation and look into django-cms documentation\nas well. ”\n3 h/t_tps://github.com/\nrumd3x/PSP-POC/issues/\n1\nCombine two lists in a\nlist.\n“/T_hepstats package is used for creating reports from data generated by\nthe Pro/f_iles class. /T_headd callers function is supposed to take asource\nlist, and a target list, and return new callers by combining the call\nresults of both target and source by adding the call time. ”\nTable 3: Code search accuracy compared with baselines.\n(Best scores are in boldface.)\nApproaches MRR nDCG SR@5 SR@10\nDeepCS 48.41 58.85 57.44 66.78\nCoaCor 59.33 67.51 67.05 73.58\nHybrid-DeepCom 50.92 59.92 60.52 68.35\nAutoSum 57.68 63.52 63.43 70.16\nTranS 3base 58.43 65.13 63.28 70.85\nTranS 3tree 60.57 68.43 65.16 74.13\nTranS 3tree +RL 62.37 70.62 66.95 75.21\n• Task 1: Remove all the /f_iles in a directory.\n• Task 2: Sends a message to the admins.\n/T_hen, they are asked to evaluate if the searched code snippets\ncan solve the tasks or are helpful for solving them, by giving a\nscore on a /f_ive-point Likert scale (strongly agree is 5 and strongly\ndisagree is 1). For the 10 participants, the average Likert score is\n3.167 (with standard deviation of 1.472), which indicates that in\ngeneral, the eﬃcacy of TranS 3 can be acceptable.\n6 THREATS TO VALIDITY\n/T_here are several threats to validity of our proposed approach and\nits results, which are presented as follows.\n/T_he main threat to internal validity is the potential defects in\nthe implementation of our techniques. To reduce such threat, we\nadopted a commonly-used benchmark with over 120,000 Python\nfunctions for evaluating the eﬀectiveness and eﬃciency of our\nproposed approach and several existing approaches for comparison.\nMoreover, to ensure the fair comparison, we directly downloaded\nthe optimized models of the existing approaches for comparison.\n/T_he threats to external validity mainly lie in the dataset qual-\nity and the evaluation metrics of our experiments. On one hand,\nthe quality of the training data, i.e., the < code;comment > pairs\nadopted in our experiment was not evaluated. Among the over\n120,000 python functions, it is likely that part of the poor-quality\ndata can taint the training results. However, since (1) all the ap-\nproaches were evaluated in the identical benchmark, and (2) the\nadopted evaluation metrics measure the performance of the ap-\nproaches by word frequency where the corresponding performance\ndiﬀerence among the approaches can indicate their word map-\nping levels, we can also infer that given high-quality training data,\nthe performance distribution of all the approaches are likely to\nmaintain consistency, whereTranS 3 can still outperform the other\napproaches in terms of the word-frequency metrics. Moreover,\nthe performance of the tree-transformer-based encoder heavily\nrelies on the quality of program forms. However, the experimen-\ntal results indicate that the tree-transformer-based encoder can\nachieve be/t_ter performance than the transformer-based encoder\nregardless the quality of the program forms. On the other hand, the\nword-frequency-based metrics cannot fully re/f_lect the the semantic\ncorrectness of the approaches. To reduce such threat, we adopted a\nset of empirical studies such that developers can feedback for the\nquality of our code summarization and code search results. /T_he pos-\nitive study results can strengthen the validity of the eﬀectiveness\nand eﬃciency of TranS 3.\n7 RELATED WORK\n7.1 Code Summarization\n/T_he code summarization techniques can be mainly categorized as\ninformation-retrieval-based approaches and deep-learning-based\napproaches.\nInformation-retrieval-based approaches. Wong et al. [26]\nproposed AutoComment which leverages code-description map-\npings to generate description comments for similar code segments\nmatched in open-source projects. Similarly they also apply code\nclone detection techniques to /f_ind similar code snippets and extract\ncomments from the similar code snippets [63]. Movshovitz-A/t_tias\net al. [ 2] predicted comments from Java source /f_iles using topic\nmodels and n-grams. Haiduc et al. [ 64] combined IR techniques,\ni.e., Vector Space Model and Latent Semantic Indexing, to generate\nterms-based summaries for Jave classes and methods.\nDeep-learning-based approaches. /T_he deep-learning-based\napproaches usually leverage Recurrent Neural Networks (RNNs)\nor Convolution neural networks (CNNs) with the a/t_tention mecha-\nnism. For instance, Iyer et al. [ 32] proposed to use RNN with an\na/t_tention mechanism—CODE-NN to produce comments for C# code\nsnippets and SQL queries. Allamanis et al. [65] proposed an a/t_ten-\ntional CNN on the input tokens to detect local time-invariant and\nlong-range topical a/t_tention features to summarize code snippets\ninto function name-like summaries. Considering the API informa-\ntion, Hu et al. [28] proposed TL-CodeSum to generate summaries\nby capturing semantics from the source code with the assistance\nof API knowledge. Chen et al. [29] proposed BVAE which utilizes\nC-VAE to encode code and L-VAE to encode natural language. In\naddition to such encoder-decoder-based approaches, Wan et al.\n[55, 66] drew on the insights of deep reinforcement learning to\nalleviate the exposure bias issue by integrating exploration and\nexploitation into the whole framework. Hu et al. [ 33] proposed\nDeepCom which takes AST sequence converted by traversing the\nAST as the input of NMT and they also extended this work by con-\nsidering hybrid lexical and syntactical information in [54]. Leclair\net al. [1] combined words from code with code structure from AST,\nwhich allows the model to learn code structure independent of the\ntext in code.\n7.2 Code Search\nCode search techniques also mainly consists of information-retrieval-\nbased approaches and deep-learning-based approaches.\nInformation-retrieval-based approaches. Hill et al. [ 67]\nproposed CONQUER which integrates multiple feedback mecha-\nnisms into the search results view. Some approaches proposed to\nextend the queries, for example, Lu et al. [27] proposed to extend\nqueries with synonyms generated from WordNet and then match\nthem with phrases extracting from code identi/f_iers to obtain the\nsearch results. Lv et al. [4] designed a API understanding compo-\nnent to /f_igure out the potential APIs and then expand the query\nwith the potential APIs and retrieve relevant code snippets from the\ncodebase. Similarly, Raghothaman et al. [68] proposed swim, which\n/f_irst suggests an API set given a query by the natural language\nto API mapper that is extracted from clickthrough data in search\nengine, and then generates code using the suggested APIs by the\nsynthesizer.\nDeep-learning-based approaches /T_he deep learning-based\napproaches usually encode the code snippets and natural language\nquery into a hidden vector space, and then train a model to make\nthe corresponding code and query vector more similar in the hidden\nspace. Gu et al. [5] proposed DeepCS, which reads code snippets\nand embeds them into vectors. /T_hen, given a query, it returns the\ncode snippets with the nearest vectors to the query. Luan et al. [69]\nproposed Aroma, which takes a code snippet as input, assembles a\nlist of method bodies that contain the snippet, clusters and inter-\nsects those method bodies to oﬀer code recommendations. Diﬀerent\nfrom the above approaches, Akbar et al. [ 30] presented a frame-\nwork that incorporates both ordering and semantic relationships\nbetween the terms and builds one-hot encoding model to rank the\nretrieval results. Chen et al. [29] proposed BVAE, which includes\nC-VAE and L-VAE to encode code and query respectively, based\non which semantic vector for both code and description and gen-\nerate completely. Yao et al. [6] proposed CoaCor, which designs a\nrewarding mechanism to guide the code annotation model directly\nbased on how eﬀectively the generated annotation distinguishes\nthe code snippet for code retrieval.\nOther approaches. Sivaraman et al. [ 70] proposed ALICE,\nwhich integrates active learning and inductive logic programming\nto incorporate partial user feedback and re/f_ine code search pa/t_terns.\nTakuya et al. [71] proposed Selene, which uses the entire editing\ncode as query and recommends code based on a associative search\nengine. Lemons et al. [72] proposed a test-driven code search and\nreuse approach, which searches code according to the behavior of\nthe desired feature to be searched.\n8 CONCLUSION\nIn this paper, we propose TranS 3, which is a transformer-based\nframework to integrate code summarization with code search. Specif-\nically, TranS 3 enables an actor-critic network for code summa-\nrization. In the actor network, we build transformer- and tree-\ntransformer-based encoder to encode code snippets and decode\nthe given code snippet to generate their comments. Meanwhile,\nwe utilize the feedback from the critic network to iteratively tune\nthe actor network for enhancing the quality of the generated com-\nments. Furthermore, we import the generated comments to code\nsearch for enhancing its accuracy. We conduct a set of experi-\nmental studies and case studies to evaluate the eﬀectiveness of\nTranS 3, where the experimental results suggest that TranS 3 can\nsigni/f_icantly outperform multiple state-of-the-art approaches in\nboth code summarization and code search and the study results fur-\nther strengthen the eﬃcacy of TranS 3 from the developers’ points\nof view.\nREFERENCES\n[1] A. LeClair, S. Jiang, and C. McMillan, “A neural model for generating natural lan-\nguage summaries of program subroutines, ” inProceedings of the 41st International\nConference on So/f_tware Engineering. IEEE Press, 2019, pp. 795–806.\n[2] D. Movshovitz-A/t_tias and W. W. Cohen, “Natural language models for predict-\ning programming comments, ” inProceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), vol. 2, 2013,\npp. 35–40.\n[3] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker, “Towards\nautomatically generating summary comments for java methods, ” inProceedings\nof the IEEE/ACM international conference on Automated so/f_tware engineering.\nACM, 2010, pp. 43–52.\n[4] F. Lv, H. Zhang, J.-g. Lou, S. Wang, D. Zhang, and J. Zhao, “Codehow: Eﬀective\ncode search based on api understanding and extended boolean model (e), ” in\n2015 30th IEEE/ACM International Conference on Automated So/f_tware Engineering\n(ASE). IEEE, 2015, pp. 260–270.\n[5] X. Gu, H. Zhang, and S. Kim, “Deep code search, ” in2018 IEEE/ACM 40th Interna-\ntional Conference on So/f_tware Engineering (ICSE). IEEE, 2018, pp. 933–944.\n[6] Z. Yao, J. R. Peddamail, and H. Sun, “Coacor: Code annotation for code retrieval\nwith reinforcement learning, ” in/T_he World Wide Web Conference. ACM, 2019,\npp. 2203–2214.\n[7] B. P. Lientz and E. B. Swanson, “So/f_tware maintenance management, ”IEE Pro-\nceedings E Computers and Digital Techniques Transactions on So/f_tware Engineering,\nvol. 127, no. 6, 1980.\n[8] L. Moreno and A. Marcus, “Automatic so/f_tware summarization: the state of the\nart, ” in2017 IEEE/ACM 39th International Conference on So/f_tware Engineering\nCompanion (ICSE-C). IEEE, 2017, pp. 511–512.\n[9] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, “Deeproad: Gan-based\nmetamorphic testing and input validation framework for autonomous driving\nsystems, ” in Proceedings of the 33rd ACM/IEEE International Conference on\nAutomated So/f_tware Engineering, ASE 2018, Montpellier, France, September 3-7,\n2018, 2018, pp. 132–142. [Online]. Available: h/t_tps://doi.org/10.1145/3238147.\n3238187\n[10] L. Chen and L. Zhang, “Speeding up mutation testing via regression test\nselection: An extensive study, ” in11th IEEE International Conference on So/f_tware\nTesting, Veri/f_ication and Validation, ICST 2018, V¨aster˚as, Sweden, April 9-13, 2018,\n2018, pp. 58–69. [Online]. Available: h/t_tps://doi.org/10.1109/ICST.2018.00016\n[11] Y. Lu, Y. Lou, S. Cheng, L. Zhang, D. Hao, Y. Zhou, and L. Zhang, “How does\nregression test prioritization perform in real-world so/f_tware evolution?” in\nProceedings of the 38th International Conference on So/f_tware Engineering, ICSE\n2016, Austin, TX, USA, May 14-22, 2016, 2016, pp. 535–546. [Online]. Available:\nh/t_tps://doi.org/10.1145/2884781.2884874\n[12] M. Wu, H. Zhou, L. Zhang, C. Liu, and Y. Zhang, “Characterizing and detecting\nCUDA program bugs, ”CoRR, vol. abs/1905.01833, 2019. [Online]. Available:\nh/t_tp://arxiv.org/abs/1905.01833\n[13] H. Zhou, W. Li, Z. Kong, J. Guo, Y. Zhang, B. Yu, L. Zhang, and C. Liu, “Deep-\nbillboard: Systematic physical-world testing of autonomous driving systems, ” in\nProceedings of the 42nd International Conference on So/f_tware Engineering, ICSE\n2020, Seoul, Korea, May 23 - 29, 2020, 2020.\n[14] I. Yen, S. Zhang, F. Bastani, and Y. Zhang, “A framework for iot-based monitoring\nand diagnosis of manufacturing systems, ” in2017 IEEE Symposium on Service-\nOriented System Engineering (SOSE), April 2017, pp. 1–8.\n[15] M. Wu, Y. Ouyang, H. Zhou, L. Zhang, C. Liu, and Y. Zhang, “Simulee: Detecting\ncuda synchronization bugs via memory-access modeling, ” inProceedings of the\n42nd International Conference on So/f_tware Engineering, ICSE 2020, Seoul, Korea,\nMay 23 - 29, 2020, 2020.\n[16] X. Li, W. Li, Y. Zhang, and L. Zhang, “Deep/f_l: integrating multiple fault\ndiagnosis dimensions for deep fault localization, ” in Proceedings of the 28th\nACM SIGSOFT International Symposium on So/f_tware Testing and Analysis, ISSTA\n2019, Beijing, China, July 15-19, 2019 , 2019, pp. 169–180. [Online]. Available:\nh/t_tps://doi.org/10.1145/3293882.3330574\n[17] M. Zhang, Y. Li, X. Li, L. Chen, Y. Zhang, L. Zhang, and S. Khurshid, “An em-\npirical study of boosting spectrum-based fault localization via pagerank, ”IEEE\nTransactions on So/f_tware Engineering, pp. 1–1, 2019.\n[18] M. Zhang, X. Li, L. Zhang, and S. Khurshid, “Boosting spectrum-based\nfault localization using pagerank, ” in Proceedings of the 26th ACM SIGSOFT\nInternational Symposium on So/f_tware Testing and Analysis, Santa Barbara,\nCA, USA, July 10 - 14, 2017 , 2017, pp. 261–272. [Online]. Available:\nh/t_tps://doi.org/10.1145/3092703.3092731\n[19] A. Ghanbari, S. Benton, and L. Zhang, “Practical program repair via bytecode\nmutation, ” inProceedings of the 28th ACM SIGSOFT International Symposium on\nSo/f_tware Testing and Analysis, ISSTA 2019, Beijing, China, July 15-19, 2019, 2019,\npp. 19–30. [Online]. Available: h/t_tps://doi.org/10.1145/3293882.3330559\n[20] Y. Lou, J. Chen, L. Zhang, D. Hao, and L. Zhang, “History-driven build\nfailure /f_ixing: how far are we?” in Proceedings of the 28th ACM\nSIGSOFT International Symposium on So/f_tware Testing and Analysis, ISSTA\n2019, Beijing, China, July 15-19, 2019 , 2019, pp. 43–54. [Online]. Available:\nh/t_tps://doi.org/10.1145/3293882.3330578\n[21] J. Hua, Y. Zhang, Y. Zhang, and S. Khurshid, “Edsketch: execution-driven\nsketching for java, ”STTT, vol. 21, no. 3, pp. 249–265, 2019. [Online]. Available:\nh/t_tps://doi.org/10.1007/s10009-019-00512-8\n[22] M. Wu, L. Zhang, C. Liu, S. H. Tan, and Y. Zhang, “Automating cuda synchroniza-\ntion via program transformation, ” in2019 34th IEEE/ACM International Conference\non Automated So/f_tware Engineering (ASE), Nov 2019, pp. 748–759.\n[23] T. Zheng, X. Zheng, Y. Zhang, Y. Deng, E. Dong, R. Zhang, and X. Liu, “Smartvm:\na sla-aware microservice deployment framework, ”World Wide Web, vol. 22, no. 1,\npp. 275–293, 2019. [Online]. Available: h/t_tps://doi.org/10.1007/s11280-018-0562-5\n[24] D. Yu, Y. Jin, Y. Zhang, and X. Zheng, “A survey on security issues in\nservices communication of microservices-enabled fog applications, ” Concurr.\nComput. Pract. Exp. , vol. 31, no. 22, 2019. [Online]. Available: h/t_tps:\n//doi.org/10.1002/cpe.4436\n[25] T. Zheng, Y. Zhang, X. Zheng, M. Fu, and X. Liu, “Bigvm: A multi-\nlayer-microservice-based platform for deploying saas, ” in Fi/f_th International\nConference on Advanced Cloud and Big Data, CBD 2017, Shanghai, China, August\n13-16, 2017 . IEEE Computer Society, 2017, pp. 45–50. [Online]. Available:\nh/t_tps://doi.org/10.1109/CBD.2017.16\n[26] E. Wong, J. Yang, and L. Tan, “Autocomment: Mining question and answer\nsites for automatic comment generation, ” inProceedings of the 28th IEEE/ACM\nInternational Conference on Automated So/f_tware Engineering. IEEE Press, 2013,\npp. 562–567.\n[27] M. Lu, X. Sun, S. Wang, D. Lo, and Y. Duan, “/Q_uery expansion via wordnet for\neﬀective code search, ” in2015 IEEE 22nd International Conference on So/f_tware\nAnalysis, Evolution, and Reengineering (SANER). IEEE, 2015, pp. 545–549.\n[28] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, “Summarizing source code with\ntransferred api knowledge, ” inInternational Joint Conference on Arti/f_icial Intelli-\ngence 2018. International Joint Conferences on Arti/f_icial Intelligence, 2018, pp.\n2269–2275.\n[29] Q. Chen and M. Zhou, “A neural framework for retrieval and summarization of\nsource code, ” inProceedings of the 33rd ACM/IEEE International Conference on\nAutomated So/f_tware Engineering. ACM, 2018, pp. 826–831.\n[30] S. A. Akbar and A. C. Kak, “Scor: source code retrieval with semantics and order, ”\nin Proceedings of the 16th International Conference on Mining So/f_tware Repositories.\nIEEE Press, 2019, pp. 1–12.\n[31] F. Scholer, H. E. Williams, and A. Turpin, “/Q_uery association surrogates for web\nsearch, ”Journal of the American Society for Information Science & Technology ,\nvol. 55, no. 7, pp. 637–650, 2014.\n[32] S. Iyer, I. Konstas, A. Cheung, and L. Ze/t_tlemoyer, “Summarizing source code\nusing a neural a/t_tention model. ” inProceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (1), 2016, pp. 2073–2083.\n[33] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment generation, ” in\nProceedings of the 26th Conference on Program Comprehension. ACM, 2018, pp.\n200–210.\n[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\nand I. Polosukhin, “A/t_tention is all you need, ” inAdvances in neural information\nprocessing systems, 2017, pp. 5998–6008.\n[35] J. Wang, L.-C. Yu, K. R. Lai, and X. Zhang, “Dimensional sentiment analysis\nusing a regional cnn-lstm model, ” inProceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Volume 2: Short Papers), 2016, pp.\n225–230.\n[36] Z. Huang, G. Zweig, M. Levit, B. Dumoulin, B. Oguz, and S. Chang, “Accelerating\nrecurrent neural network training via two stage classes and parallelization, ” in\n2013 IEEE Workshop on Automatic Speech Recognition and Understanding. IEEE,\n2013, pp. 326–331.\n[37] A. V. M. Barone and R. Sennrich, “A parallel corpus of python functions and\ndocumentation strings for automated code documentation and code generation, ”\narXiv preprint arXiv:1707.02275, 2017.\n[38] S. Wang, D. Chollak, D. Movshovitz-A/t_tias, and L. Tan, “Bugram: bug detection\nwith n-gram language models, ” inProceedings of the 31st IEEE/ACM International\nConference on Automated So/f_tware Engineering. ACM, 2016, pp. 708–719.\n[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding, ” arXiv preprint\narXiv:1810.04805, 2018.\n[40] Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu, H. Tian, and\nH. Wu, “Ernie: Enhanced representation through knowledge integration, ”arXiv\npreprint arXiv:1904.09223, 2019.\n[41] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le, “Xlnet: Gen-\neralized autoregressive pretraining for language understanding, ”arXiv preprint\narXiv:1906.08237, 2019.\n[42] S. Bowman, G. Angeli, C. Po/t_ts, and C. Manning, “A large annotated corpus for\nlearning natural language inference, ” inEMNLP, 2015.\n[43] E. M. Voorhees, “/T_he trec question answering track, ”Natural Language Engineer-\ning, vol. 7, no. 4, pp. 361–378, 2001.\n[44] S. /T_hrun and M. L. Li/t_tman, “Reinforcement learning: An introduction, ”IEEE\nTransactions on Neural Networks, vol. 16, no. 1, pp. 285–286, 2005.\n[45] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence generative adversarial\nnets with policy gradient, ” in/T_hirty-First AAAI Conference on Arti/f_icial Intelligence,\n2017.\n[46] R. S. Su/t_ton and A. G. Barto,Introduction to reinforcement learning. MIT press\nCambridge, 1998, vol. 135.\n[47] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist\nreinforcement learning, ” inReinforcement Learning. Springer, 1992, pp. 5–32.\n[48] C. J. Watkins and P. Dayan, “Q-learning, ”Machine learning, vol. 8, no. 3-4, pp.\n279–292, 1992.\n[49] Y. Keneshloo, T. Shi, C. K. Reddy, and N. Ramakrishnan, “Deep reinforcement\nlearning for sequence to sequence models, ”arXiv preprint arXiv:1805.09461, 2018.\n[50] V. Konda, “Actor-critic algorithms, ”Siam Journal on Control & Optimization ,\nvol. 42, no. 4, pp. 1143–1166, 2003.\n[51] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Eﬃcient estimation of word\nrepresentations in vector space, ”Computer Science, 2013.\n[52] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online\nlearning and stochastic optimization, ”Journal of Machine Learning Research ,\nvol. 12, no. Jul, pp. 2121–2159, 2011.\n[53] “Github, ”h/t_tps://github.com/.\n[54] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment generation with\nhybrid lexical and syntactical information, ”Empirical So/f_tware Engineering, pp.\n1–39, 2019.\n[55] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu, “Improving\nautomatic source code summarization via deep reinforcement learning, ” inPro-\nceedings of the 33rd ACM/IEEE International Conference on Automated So/f_tware\nEngineering. ACM, 2018, pp. 397–407.\n[56] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu, “Bleu: a method for automatic\nevaluation of machine translation, ” inProceedings of the 40th annual meeting\non association for computational linguistics . Association for Computational\nLinguistics, 2002, pp. 311–318.\n[57] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evaluation with\nimproved correlation with human judgments, ” inProceedings of the acl workshop\non intrinsic and extrinsic evaluation measures for machine translation and/or\nsummarization, vol. 29, 2005, pp. 65–72.\n[58] C. Y. Lin, “Rouge: A package for automatic evaluation of summaries, ” Text\nSummarization Branches Out, 2004.\n[59] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-based\nimage description evaluation, ” inProceedings of the IEEE conference on computer\nvision and pa/t_tern recognition, 2015, pp. 4566–4575.\n[60] N. Craswell, “Mean reciprocal rank, ” 2009.\n[61] Y. Wang, L. Wang, Y. Li, D. He, T. Y. Liu, and W. Chen, “A theoretical analysis of\nndcg type ranking measures, ”Journal of Machine Learning Research, vol. 30, pp.\n25–54, 2013.\n[62] L. Xuan, Z. Wang, Q. Wang, S. Yan, X. Tao, and M. Hong, “Relationship-aware\ncode search for javascript frameworks, ” inAcm Sigso/f_t International Symposium\non Foundations of So/f_tware Engineering, 2016.\n[63] E. Wong, T. Liu, and L. Tan, “Clocom: Mining existing source code for automatic\ncomment generation, ” in2015 IEEE 22nd International Conference on So/f_tware\nAnalysis, Evolution, and Reengineering (SANER). IEEE, 2015, pp. 380–389.\n[64] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus, “On the use of automated text\nsummarization techniques for summarizing source code, ” inReverse Engineering,\n2010.\n[65] M. Allamanis, H. Peng, and C. Su/t_ton, “A convolutional a/t_tention network for\nextreme summarization of source code, ” inInternational Conference on Machine\nLearning, 2016, pp. 2091–2100.\n[66] W. Wang, Y. Zhang, Y. Sui, Y. Wan, Z. Zhao, J. Wu, P. Yu, and G. Xu,\n“Reinforcement-learning-guided source code summarization using hierarchical\na/t_tention, ”IEEE Transactions on So/f_tware Engineering, pp. 1–1, 2020.\n[67] E. Hill, M. Roldanvega, J. A. Fails, and G. Mallet, NL-Based /Q_uery Re/f_inement and\nContextualized Code Search Results: A User Study, 2014.\n[68] M. Raghothaman, Y. Wei, and Y. Hamadi, “Swim: Synthesizing what i mean-code\nsearch and idiomatic snippet synthesis, ” in2016 IEEE/ACM 38th International\nConference on So/f_tware Engineering (ICSE). IEEE, 2016, pp. 357–367.\n[69] S. Luan, D. Yang, K. Sen, and S. Chandra, “Aroma: Code recommendation via\nstructural code search, ”arXiv preprint arXiv:1812.01158, 2018.\n[70] A. Sivaraman, T. Zhang, G. Van den Broeck, and M. Kim, “Active inductive logic\nprogramming for code search, ” inProceedings of the 41st International Conference\non So/f_tware Engineering. IEEE Press, 2019, pp. 292–303.\n[71] W. Takuya and H. Masuhara, “A spontaneous code recommendation tool based\non associative search, ” inProceedings of the 3rd International Workshop on Search-\nDriven Development: Users, Infrastructure, Tools, and Evaluation. ACM, 2011, pp.\n17–20.\n[72] O. A. L. Lemos, S. K. Bajracharya, J. Ossher, R. S. Morla, P. C. Masiero, P. Baldi,\nand C. V. Lopes, “Codegenie: using test-cases to search and reuse source code. ”\nAse, pp. 525–526, 2007.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8589482307434082
    },
    {
      "name": "Computer science",
      "score": 0.7764526605606079
    },
    {
      "name": "Transformer",
      "score": 0.565167248249054
    },
    {
      "name": "Encoder",
      "score": 0.5492243766784668
    },
    {
      "name": "Code (set theory)",
      "score": 0.5423243641853333
    },
    {
      "name": "Snippet",
      "score": 0.5264875888824463
    },
    {
      "name": "Source code",
      "score": 0.4559631049633026
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3375242054462433
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.2921101450920105
    },
    {
      "name": "Information retrieval",
      "score": 0.28519928455352783
    },
    {
      "name": "Programming language",
      "score": 0.26123300194740295
    },
    {
      "name": "Engineering",
      "score": 0.08451920747756958
    },
    {
      "name": "Operating system",
      "score": 0.07529142498970032
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}