{
  "title": "Real-Time 3D Single Object Tracking With Transformer",
  "url": "https://openalex.org/W4210786150",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4212387993",
      "name": "Shan Jiayao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2244971966",
      "name": "Zhou, Sifan",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2131075666",
      "name": "Cui Yu-bo",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2061772064",
      "name": "Fang Zheng",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2971306974",
    "https://openalex.org/W2972392244",
    "https://openalex.org/W3131716792",
    "https://openalex.org/W3167536469",
    "https://openalex.org/W2962922818",
    "https://openalex.org/W2988715931",
    "https://openalex.org/W3114753236",
    "https://openalex.org/W6739778489",
    "https://openalex.org/W6787985011",
    "https://openalex.org/W3166470370",
    "https://openalex.org/W3132607695",
    "https://openalex.org/W6792915775",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W6788023325",
    "https://openalex.org/W3107400217",
    "https://openalex.org/W3035641096",
    "https://openalex.org/W2949708697",
    "https://openalex.org/W3093537218",
    "https://openalex.org/W3111535274",
    "https://openalex.org/W6802372869",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6763367864",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W3026441479",
    "https://openalex.org/W2997941866",
    "https://openalex.org/W6760655697",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3129373298",
    "https://openalex.org/W6788305448",
    "https://openalex.org/W2965376383",
    "https://openalex.org/W3008681115",
    "https://openalex.org/W2435777129",
    "https://openalex.org/W6631884009",
    "https://openalex.org/W2912429050",
    "https://openalex.org/W2917353338",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W3035672751",
    "https://openalex.org/W2963227409",
    "https://openalex.org/W3175891031",
    "https://openalex.org/W3009294634",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W6754654878",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W2964071664",
    "https://openalex.org/W2151295993",
    "https://openalex.org/W2990211277",
    "https://openalex.org/W3102817716",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W1964846093",
    "https://openalex.org/W3194700450",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2158827467",
    "https://openalex.org/W3208501016",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6757585730",
    "https://openalex.org/W2557641257",
    "https://openalex.org/W2154889144",
    "https://openalex.org/W1892578678",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963400571",
    "https://openalex.org/W3035346742",
    "https://openalex.org/W4394652702",
    "https://openalex.org/W4297780875",
    "https://openalex.org/W3149936330",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3006754479",
    "https://openalex.org/W2922625254",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W4298395628",
    "https://openalex.org/W2894176037",
    "https://openalex.org/W4287557165",
    "https://openalex.org/W1531069015",
    "https://openalex.org/W3119686997"
  ],
  "abstract": "LiDAR-based 3D single object tracking is a challenging issue in robotics and autonomous driving. Currently, existing approaches usually suffer from the problem that objects at long distance often have very sparse or partially-occluded point clouds, which makes the features extracted by the model ambiguous. Ambiguous features will make it hard to locate the target object and finally lead to bad tracking results. To solve this problem, we utilize the powerful Transformer architecture and propose a Point-Track-Transformer (PTT) module for point cloud-based 3D single object tracking task. Specifically, PTT module generates fine-tuned attention features by computing attention weights, which guides the tracker focusing on the important features of the target and improves the tracking ability in complex scenarios. To evaluate our PTT module, we embed PTT into the dominant method and construct a novel 3D SOT tracker named PTT-Net. In PTT-Net, we embed PTT into the voting stage and proposal generation stage, respectively. PTT module in the voting stage could model the interactions among point patches, which learns context-dependent features. Meanwhile, PTT module in the proposal generation stage could capture the contextual information between object and background. We evaluate our PTT-Net on KITTI and NuScenes datasets. Experimental results demonstrate the effectiveness of PTT module and the superiority of PTT-Net, which surpasses the baseline by a noticeable margin, ~10% in the Car category. Meanwhile, our method also has a significant performance improvement in sparse scenarios. In general, the combination of transformer and tracking pipeline enables our PTT-Net to achieve state-of-the-art performance on both two datasets. Additionally, PTT-Net could run in real-time at 40FPS on NVIDIA 1080Ti GPU. Our code is open-sourced for the research community at https://github.com/shanjiayao/PTT.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nReal-time 3D Single Object Tracking\nwith Transformer\nJiayao Shan†, Sifan Zhou †, Yubo Cui†, Zheng Fang*, Member, IEEE,\nAbstract—LiDAR-based 3D single object tracking is a challeng-\ning issue in robotics and autonomous driving. Currently, existing\napproaches usually suffer from the problem that objects at long\ndistance often have very sparse or partially-occluded point clouds,\nwhich makes the features extracted by the model ambiguous.\nAmbiguous features will make it hard to locate the target object\nand ﬁnally lead to bad tracking results. To solve this problem,\nwe utilize the powerful Transformer architecture and propose a\nPoint-Track-Transformer (PTT) module for point cloud-based 3D\nsingle object tracking task. Speciﬁcally, PTT module generates\nﬁne-tuned attention features by computing attention weights,\nwhich guides the tracker focusing on the important features of\nthe target and improves the tracking ability in complex scenarios.\nTo evaluate our PTT module, we embed PTT into the dominant\nmethod and construct a novel 3D SOT tracker named PTT-\nNet. In PTT-Net, we embed PTT into the voting stage and\nproposal generation stage, respectively. PTT module in the voting\nstage could model the interactions among point patches, which\nlearns context-dependent features. Meanwhile, PTT module in\nthe proposal generation stage could capture the contextual\ninformation between object and background. We evaluate our\nPTT-Net on KITTI and NuScenes datasets. Experimental results\ndemonstrate the effectiveness of PTT module and the superiority\nof PTT-Net, which surpasses the baseline by a noticeable margin,\n∼10% in the Car category. Meanwhile, our method also has\na signiﬁcant performance improvement in sparse scenarios. In\ngeneral, the combination of transformer and tracking pipeline\nenables our PTT-Net to achieve state-of-the-art performance on\nboth two datasets. Additionally, PTT-Net could run in real-time\nat 40FPS on NVIDIA 1080Ti GPU. Our code is open-sourced for\nthe research community at https://github.com/shanjiayao/PTT.\nIndex Terms—3D single object tracking, Lidar point-cloud,\nSiamese network, Transformer, Self attention.\nI. I NTRODUCTION\nS\nINGLE object tracking (SOT) using LiDAR points has a\nwide range of applications in robotics and autonomous\ndriving [1]–[3]. For example, the autonomous pedestrian fol-\nlowing robot should robustly track its master and localize\nhim/her accurately for efﬁcient following control in the crowd.\nAnother example is autonomous landing of unmanned aerial\nvehicles, where the drone needs to track the target and know\nthe accurate distance and pose of the target for safe landing.\nHowever, most existing 3D SOT methods are usually using\nvisual or RGB-D cameras [4]–[6], which may fail in visually\n†Authors with equal contribution.\nThe authors are with the Faculty of Robot Science and Engineering, North-\neastern University, Shenyang, China; Corresponding author: Zheng Fang, e-\nmail: fangzheng@mail.neu.edu.cn\n2Jiayao Shan is also with Science and Technology on Near-Surface Detec-\ntion Laboratory, Wuxi, China\nThis paper has supplementary downloadable material available at\nhttp://ieeexplore.ieee.org, provided by the authors.\nImage of the scene Tracking target\nSearch Area Attention points Voting and Bbox\nscore\nFig. 1. Exempliﬁed illustration to show the attention weights and voting\nresult. Given raw point cloud, we specify search area and extract robust key-\npoints ﬁrst. Then, the transformer and attention mechanism focus on the key-\npoints which carry rich and robust information. Finally, the voting module\nwill generate votes according to the key-points and the predict bounding box.\ndegraded or illumination changing environments due to that\nthey mainly depend on the dense images for target tracking.\nIn addition to visual or RGB-D sensors, 3D LiDAR sen-\nsors are also widely used in object tracking tasks [7]–[9]\nbecause they are less sensitive to illumination changes and\ncould directly capture geometric and distance information\nmore accurately. However, LiDAR-based 3D SOT has its own\nchallenges. First, point data is sparse and disordered [10],\nwhich requires the network to be permutation-invariant to\nhandle points well. Second, point cloud is spatially discrete,\nwhich is naturally different from dense image.Third, 3D object\ntracking needs to estimate higher space dimension information\n(e.g., x,y,z,w,h,l,ry ) than 2D visual tracking, which brings\nmore computational complexity. All these problems bring\ngreat challenges to realize a robust and real-time LiDAR-based\ntracking method.\nDifferent from the existing LiDAR-based Multi-object\nTracking (MOT) methods [11]–[15], LiDAR-based 3D SOT\nmethods need to model the similarity function between target\ntemplate and search area to localize the target object. Although\nthey both need to compute the similarity, MOT methods\ncompute the object-level similarity to association the detection\nresults and tracklets, while SOT methods compute the intra-\nobject-level similarity to localize the target object. Therefore,\ncompared to 3D MOT, 3D SOT has its own challenges. SC3D\n[16] is the pioneer LiDAR-based 3D Siamese tracker which is\nbased on the shape completion network. However, the method\nonly uses an encoder consisting of 3 layers of 1D-convolutions\nto process the input point cloud, which makes it difﬁcult to\nextract the robust point cloud feature representation. Besides,\nSC3D could not run in real-time and be trained end-to-end.\narXiv:2209.00860v1  [cs.CV]  2 Sep 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nQi et al. [17] also proposed a point-to-box (P2B) network to\nestimate the target bounding box from the raw point cloud.\nHowever, their approach usually fails to track in sparse point\ncloud scenarios. Meanwhile, P2B gives no preference to non-\naccidental coincidences [18] which have more contribution to\nlocating the target center. Recently, Fang et al. [19] combined\nSiamese network and LiDAR-based RPN network [20] to\ntackle 3D object tracking. Nevertheless, they directly use the\nclassiﬁcation scores to sort regression results, ignoring the\ninconsistency between the localization and classiﬁcation. It\nis worth noting that points located in different geometric\npositions often have different importance in representing tar-\ngets. However, these aforementioned methods do not weigh\npoint cloud features based on this characteristic. Besides,\nthe point cloud features extracted from the template and the\nsearch area contain less potential object information and more\nbackground noise due to the sparsity and occlusion of point\nclouds. Therefore, how to pay attention to the spatial clues is\nthe key to improve the performance of the 3D object tracker.\nRecently, Transformer has shown amazing performance in\nfeature encoding due to its powerful self-attention module\n[21]–[23]. Transformer usually consists of three main modules,\nincluding input (word) embedding, position encoding and\nattention module. Compared with the convolution network,\ncontent-adaptive property and unique position module of trans-\nformer make it more suitable for processing 3D point clouds.\nIn addition, the 3D SOT task only focuses on local areas,\nwhich makes the transformer suitable for this task, although\nthe transformer is sensitive to time and space cost.\nIn this paper, we propose a Point-Track-Transformer (PTT)\nmodule for 3D single object tracking to learn features more\neffectively by leveraging the superiority of the transformer\nmodels on set-structured point clouds. The core idea is to\nfocus on the important features of the target object by utilizing\nthe self-attention and position encoding mechanism to weigh\nthe point cloud features. PTT module contains three blocks\nfor feature embedding, position encoding, and self-attention\nfeature computation, respectively. Feature embedding aims to\nplace features closer in the embedding space if they have\nsimilar semantic information. Position encoding is used to\nencode the coordinates of point cloud into high dimension\ndistinguishable features. Self-attention generates reﬁned atten-\ntion features by computing attention weights. Furthermore, to\nevaluate the effectiveness of our PTT module, we embed the\nPTT module into the dominant P2B [17] to construct a novel\n3D SOT tracker termed PTT-Net. In PTT-Net, we add PTT into\nthe voting stage and proposal generation stage, respectively.\nPTT embedded in the voting stage could model interactions\namong point patches located in different geometric positions,\nwhich learns context-dependent features and helps the network\nfocus on more representative features of objects. Meanwhile,\nPTT embedded in the proposal generation stage could capture\nthe contextual information between object and background,\nand help the network to effectively suppress background noise.\nThese modiﬁcations can efﬁciently improve the performance\nof 3D object tracker. The experimental results of our PTT-Net\non KITTI tracking dataset [24] demonstrate the superiority of\nour method (∼10%’s improvement compared to the baseline).\nWe further evaluate our PTT-Net on NuScenes dataset [25], the\nresults show that our method could achieve new state-of-the-\nart performance. Additionally, PTT-Net could run in real-time\nat 40FPS on a single NVIDIA 1080Ti GPU.\nOverall, our main contributions are as follows:\n• PTT module : we propose a Point-Track-Transformer\n(PTT) module for 3D single object tracking using only raw\npoint clouds, which could weigh point cloud features to focus\non deeper-level object clues during tracking.\n•PTT-Net: we construct a 3D single object tracking net-\nwork embedded with PTT modules which can be trained end-\nto-end. To the best of our knowledge, this is the ﬁrst work\nto apply transformer to 3D object tracking task using point\nclouds.\n• Open-source: extensive experiments on KITTI and\nNuScenes datasets show that our method outperforms the state-\nof-the-art methods with remarkable margins at the speed of 40\nFPS. Besides, we open source of our method to the research\ncommunity.\nApart from that, as an extended work of our conference\npaper [26], we add more detailed descriptions on the network\narchitecture and dataset. Besides, we also carry out more\nqualitative experiments and visualizations of our PTT-Net to\nanalyze the effectiveness of transformer in 3D single object\ntracking task. For the extended experiments on the more\nchallenging NuScenes dataset, our method still achieves state-\nof-the-art performance. The results indicate that our method\ncould be adapted to more complex scenes, further conﬁrming\nthe effectiveness of our method.\nThe rest of this paper is organized as follows. In Sec. II,\nwe discuss the related work. Section III describes the proposed\nPTT module and PTT-Net. We validate the performance of our\nmethod on KITTI and NuScenes datasets in Sec. IV and we\nconclude in section V .\nII. RELATED WORK\nThis section will brieﬂy discuss the related works in 2D\nsiamese trackers, 3D single object tracking, transformer, and\nself-attention mechanism.\nA. 2D Siamese Tracking\nEarly 2D visual trackers mainly focused on correlation\nﬁltering [27]–[30]. However, these methods are based on the\ntracking template matching mechanism, so they cannot cope\nwith the rapid deformation of the tracking target. Recently, the\nrealization of 2D object tracking tasks based on Siamese net-\nworks has become the mainstream with the rapid development\nof deep learning [31]–[40]. Luca et al. [31] proposed SiamFC\nwhich was the ﬁrst pioneer work of Siamese trackers. The\nvisual tracking task was handled as a similarity problem, and\nthe cross-correlation module was introduced into the network\nstructure. Subsequently, a large number variants of SiamFC\n[31] were proposed. Li et al. [32] introduced the Region\nProposal Network (RPN) into the Siamese network. SiamRPN\ncould regress more accurate 2D bounding box than SiamFC.\nBesides, Li et al. [33] explored the relationship between the\nnumber of network layers and tracker performance, optimized\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nthe network depth of the tracker to improve the tracking\naccuracy. Furthermore, Wang et al. [35] integrated the task\nframework of image segmentation and image tracking, and\nused the mask to improve the accuracy of the tracker. Paul\net al. [36] proposed a two-stage network for visual tracking,\nwhich used the re-detection of the ﬁrst frame template and\nthe previous frame template to modify the tracking target.\nTheir method surpassed all previous methods on six short-term\ntracking benchmarks and four long-term tracking benchmarks,\nand achieved amazing results. [37] introduced an informative\nenhanced loss, which can enable the network to capture infor-\nmation from an overall perspective. Han et al. [38] proposed\nan asymmetric convolution module, which could capture the\nsemantic correlation information well. To address the problem\nof decisive samples missing during ofﬂine training, Dong et\nal. [39] proposed a compact latent network to make the model\ncould quickly adapt to new scenes. And Dong et al. [40]\nintroduced a novel hyper-parameter optimization method by\nusing deep reinforcement learning. Chen et al. [41] proposed\na tracking framework by fusing the template and search\nfeatures with transformer. In summary, the 2D visual tracking\nmethod has made great progress in the past decade and has\nbeen applied to many practical scenarios. However, limited\nby the sensor, the 2D visual trackers are still very sensitive\nto illumination changes. In addition, most of the 2D visual\ntracking methods only obtain the pixel coordinates of the\ntracking target, but sometimes it is necessary to know the\naccurate three-dimensional pose of the tracking target.\nB. 3D SOT Using Point Cloud\nGiancola et al. [16] proposed the ﬁrst pioneer LiDAR-\nbased 3D single object tracker which utilized the Kalman\nFilter to generate massive target proposals. They exploited\nshape completion to learn the shape information of target,\nbut their method has a poor generalization ability and could\nnot run in real-time. Zarzar et al. [42] leveraged 2D Siamese\nnetwork which works on Bird-Eye-View (BEV) representation\nto generate 3D proposals. However, this method may lose ﬁne-\ngrained geometry details which are important for tracking tiny\nobjects. Cui et al. [43] also adopted a 3D Siamese tracker only\nusing point cloud, but they could not estimate the orientation\nand size information of the target. Fang et al. [19] combined\n3D Siamese network and 3D RPN network to track targets,\nwhile the performance is limited by the RPN network. Besides,\nZou et al. [44] integrated 2D image and 3D point cloud\ninformation for 3D object tracking. However, this method\nrelies more on 2D tracker and uses the ground truth to track\nobjects, which is unreasonable for realistic application. Qi\net al. [17] proposed P2B which used deep hough voting to\nobtain the potential centers (votes) and estimated the target\ncenter based on those votes. However, they ignore the fact\nof points in different positions have different contributions in\ntracking. Furthermore, its random sampling mechanism loses\nthe location distribution information of the raw point cloud.\nTo deal with these shortcomings, we propose a PTT module\nto capture the feature correlations among the neighbor point\naround the target object by weighing different point features.\nMoreover, we use farthest point sampling instead of random\nsampling to obtain more raw point cloud information.\nC. Transformer and Self-attention\nRecently, transformer has revolutionized natural language\nprocessing and image analysis [21], [45]–[48]. Hu et al. [45]\nand Ramachandran et al. [46] applied scalar dot product self-\nattention to local pixel neighbors. Zhao et al. [49] applied\nvector self-attention operations to image tasks. These works\ncombined or replaced CNNs with self-attention layers and\nconﬁrmed the transformer’s great potential in visual tasks.\nInspired by those works, Zhao et al. [50] used a Point\nTransformer layer by applying vector self-attention operations,\nwhich had a great performance improvement in point cloud\nclassiﬁcation and segmentation tasks. Because self-attention\noperator, which is the core of transformer networks, is intrin-\nsically a set operator: positional information is provided as\nattributes of elements that are processed as a set [21], [49].\nTherefore, transformer is suitable for point cloud processing\ndue to its positional attributes. Besides, Nico et al. [51]\nproposed SortNet as a part of Point Transformer and achieved\ncompetitive performance on point cloud classiﬁcation and part\nsegmentation tasks. Meanwhile, Guo et al. [52] also introduced\nPoint Cloud Transformer (PCT), which performed well on\nshape classiﬁcation, part segmentation, and normal estimation\ntasks. Recently, Pan et al. [53] proposed a PointFormer as\nthe drop-in replacement backbone for 3D object detection and\ngained state-of-the-art performance. Obviously, transformer\nhas unique advantages for point cloud feature learning.\nIn addition, transformer and attention mechanism have\nrecently been widely used in 2D tracking tasks [41], [54]–\n[58]. The tracker using the transformer or attention also shows\nsuperior performance with the help of the transformer’s power-\nful attention mechanism for features. Therefore, we apply the\ntransformer to the 3D point cloud tracking task to improve the\nperformance of the tracker.\nIII. METHODOLOGY\nIn this section, we ﬁrst analyze the challenges and bot-\ntlenecks of current LiDAR-based single object trackers and\ndiscuss feasible solutions. Then, we revisit the transformer\nand present our PTT module for LiDAR-based object tracking.\nFinally, we introduce our PTT-Net in detail.\nA. Baseline\nP2B [17] is the dominant 3D SOT method using point\nclouds. In this work, we use P2B as the baseline. The main\nidea of P2B is to localize the target center in 3D search area\nand execute the proposal generation and veriﬁcation jointly.\nHence we can divide P2B into two parts. The ﬁrst part is\nfeature enhancement. The input template and search point\nclouds are extracted through a shared weight backbone net-\nwork [59], then the corresponding similarity could be obtained\nby calculating the point-wise cosine similarity in an implicitly\nembedded space. The second part is the region proposal\nnetwork which generates the proposals by deep hough voting\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nmechanism [60] from the semantic features. Besides, P2B\nalso utilizes the proposal clustering network to leverage the\nensemble power and obtain accurate target proposals.\nHowever, P2B tends to suffer from the defects that it gives\nno preference to non-accidental coincidences [18] which have\nmore contribution to locate the target center. Therefore, we\nwould like to explore the differences among the augmented\nfeatures by using the transformer architecture.\nB. Challenges\nIn Sec. I, we had pointed out several challenges for pro-\ncessing point cloud data. For 3D single object tracking task,\nthere are also several challenges as follows.\n1) error accumulation and propagation:In tasks involving\npoint clouds and deep learning, tracking is naturally different\nfrom detection and segmentation due to its spatial-temporal\ncontinuity. Since the tracking target is a sequence, which\nmakes two frames similar in spatial and temporal terms.\nThus, the dominant tracking algorithms utilize the spatial and\ntemporal prior information to initialize the search area. In spite\nof its effectiveness in reducing computational complexity, bad\ntracking results will lead to large tracking error in challenging\nscenarios. This is because the consecutive tracking predictions\nwill accumulate over the historical error and propagate it to the\nnext frame. And when error accumulates enough, the tracker\nwill fails.\n2) sparsity sensitive: The Siamese network from VOT is\ngenerally adopted in existing algorithms for point cloud object\ntracking. However, the image data is usually dense, while point\ncloud is naturally sparse. This causes gaps in the effect of\napplying Siamese network in image and point cloud. Sparse\npoint cloud data makes it difﬁcult for the backbone network to\nextract robust point cloud features. Hence, the existing LiDAR-\nbased tracking methods are sensitive to the sparsity of point\ncloud data.\n3) feature ambiguity: The sparsity of point clouds limits\nnetworks on modeling interactions among point patches lo-\ncated in different geometric positions and capturing contextual\ninformation, which makes the features extracted by the models\nambiguous. The points at locations where the object surface\nhave a low-dimensional structure, such as a plane, contribute\nambiguous features [18]. The feature ambiguity makes the\ntracker hard to classify the fore-background points and regress\nthe box center, and ﬁnally leads to bad tracking results.\nThese challenges motivate us to propose new algorithm to\ndeal with those problems. To this end, we propose our PTT\n(Point-Track-Transformer) module and PTT-Net. PTT module\ncan handle the sparsity of points with the help of the attention\nmechanism in Transformer, where the contribution of each\npoint is automatically learned in the network training. Besides,\nthe ambiguity of features will be suppressed through self-\nattention mechanism because of the role of attention is to reﬁne\nfeatures and guide the network to focus on more representative\nfeatures of the tracking target. Finally, our PTT-Net embedded\nwith PTT module could capture sparse dependencies even\nfrom a few points, thus reducing the accumulation of errors.\nC. Revisiting Transformer\nTransformer [21] is ﬁrstly introduced to aggregate informa-\ntion from the entire input sequence for machine translation. It\ncan handle sequential tasks well due to its attention mecha-\nnism. The core architecture of transformer can be divided into\nthree parts: input feature embedding, position encoding, and\nself-attention. Self-attention is the core module, which mainly\nfocuses on the differences of input features and generates\nreﬁned attention features based on global or local context.\nGiven the input feature G= {gi}N\ni=1 after feature embedding,\nthe general formula of self-attention is:\nQ,K,V = α(G),β(G),γ(G)\nA= ρ(σ(Q,K) +P) ⊙(V) (1)\nwhere α, β and γ are point-wise feature transformations (e.g.\nlinear layers or MLPs). Q, K, and V are the query, key\nand value matrices, respectively. σ is the relation function\nbetween Q and K. P is the position encoding feature. ρ is\na normalization function (e.g. softmax). ⊙means Hadamard\nproduct, which is used to obtain the output features from the\nattention weights and V. Ais the attention feature produced\nby the self-attention module. For the relation function σ, the\nregular form in machine translation [21] is:\nσ(Q,K) =QKT (2)\nD. PTT Module\nTo further integrate the self-attention mechanism into the\npoint cloud tracking task, we modify the transformer module\nproposed in [50] to capture point cloud features better. The\npoint transformer layer in [50] is proposed to process the raw\npoint cloud for classiﬁcation and segmentation tasks. However,\nwe utilize the point transformer to beneﬁt the tracking task\nand enable the tracker to capture spatial relations and object\ngeometry shape information. All these modiﬁcations construct\nthe PTT module, which is used to reﬁne the features from\nraw sparse point clouds and eliminate the ambiguity among\nfeatures. The architecture of PTT module is shown in Fig. 2.\nFor 3D SOT task, given an input of M points with XYZ\ncoordinates, a backbone network is used to extract the point\ncloud and learn deep features. It outputs a subset of the\ninput containing N interest points (seeds) S = {si}N\ni=1. Here,\nsi = (ci,fi) is composed of a vector ci of 3D coordinate and a\nD-dimensional descriptor fi of the local object geometry. Our\ngoal of using transformer is to perform an attention weighting\noperation on the feature space of fi, and output reﬁned features\nfi\n∗ with the same dimension.\nPTT module processes features by utilizing shape and\ngeometry information. Given a point set S = {si}N\ni=1, si =\n(ci,fi), ci ∈R3 and fi ∈RD. ci and fi represent 3D coor-\ndinates and descriptor of point si. Feature embedding module\nmaps input features into embedding space RM : fi → gi,\ngi ∈RM . Position encoding module extracts higher-level M-\ndimensional features pi from input coordinates ci: ci →pi,\npi ∈ RK×M . Finally, the self-attention module calculates\nattention weights and attention features f∗\ni ,f∗\ni ∈RD by taking\nembedding features and position features as inputs. To avoid\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nCoordinates(N x 3)\nInput Feature (N x D)\nD\nN\nM\nN\nK\nPos Encoding\nSelf-Attention\nValue(NxKxM)\nKey(NxKxM)\nQuery(NxM)\nKNN_XYZ(NxKx3)\nKNN\nPos_Feat(NxKxM)\nMLP\nPos_Feat\nK\nK\nM\nPos_Feat\nAttention Weights\n(NxKxM)\nM\nN\nK\nOutput\nResidual Feat\n(NxD)\nM\nN\nInput Feat\nAttention Feat\n(NxD)\n• • •\n• • •\n• • •\n• • •\n• • •\n• • •\n• • •\n• • •\nLinear\nMLP\nLinear\nInput\nMLP\n KNN Point-wise Summation Point-wise Subtraction Hadamard product\nLinear\nN x M\nEmbedding\nEmbedding-Feat\n• • •\n• • •\nLinear + KNN\nLinear + KNN\nFig. 2. PTT module architecture. It consists of three blocks: feature embedding, position encoding, and self-attention. The whole input are the coordinates\nand their corresponding features. Feature embedding module maps input features into embedding space. In position encoding module, the k-nearest neighbor\nalgorithm is used to obtain local position information, then the encoded position features will be learned by an MLP layer. The self-attention module learns\nreﬁned attention features for input features based on local context. The output features of PTT module are the sum of input and residual features.\nthe vanishing gradient problem in the training stage, we also\nadopt the residual architecture proposed in [61], and take the\nsum of the attention features and input features as output\nfeatures.\n1) Feature Embedding: The original feature embedding\nmodule in natural language processing is to map each word in\nthe input sequence to a high-dimensional vector. In this work,\nwe use the linear layer to complete the feature embedding\noperation, and map the input point cloud feature dimension\nfrom DTo M: RD →RM , which can place the feature closer\nin the embedding space when the semantics are more similar\nand make the network have a stronger ﬁtting ability.\n2) Position Encoding: Position encoding module plays a\ncrucial role in transformer, which allows operators to adapt to\nthe local structure of the input data [21]. 3D point coordinates\nare valuable features indicating the local structures. Therefore,\nwe utilize the coordinates directly as the input of the position\nencoding module. Compared to the techniques used in natural\nlanguage processing, we use a simple and yet efﬁcient ap-\nproach by mapping the coordinates of each point to the feature\ndimension and the resulting position encoding is added to the\nattention matrix. We adopt the relative coordinates to make the\nnetwork better capture the spatial correlation between points\nand local geometric shape information. Since the feature fi\nextracted by [59] can provide the local context information,\nwe obtain the position encoding features P = {pi}N\ni=1 with\nfunction η. For input point set S including N points, the\nposition encoding feature for each point is:\npi = η(ci −cj) (3)\nwhere ci is the coordinate of the i-th point in S. cj is the\nj-th coordinate in local neighborhood region of ci. η is an\nMLP with two linear layers and one ReLU non-linearity. Here,\nwe use KNN to capture the local context and set k = 16 by\ninheriting the experimental results in [50].\n3) Self-Attention: As Fig. 2 shows, self-attention module\ncomputes three vectors for each point: Q, K, V through α,\nβ, γ, where α, β, γ are all shared linear layers. It is worth\nnoting that K and V are aggregated from the features of the k\nneighborhood points, which aims to encode more local context\ninformation. Here, Q ∈RM , K ∈Rk×M , and V ∈Rk×M .\nFor relation function σ, we use σ(Q,K) to obtain point-wise\nattention weights, the detail implementation of σ(Q,K) will\nbe introduced in next part. And an MLP layer γ is used to\nintroduce additional trainable transformations and match the\noutput dimension. Then, we add the position encoding features\nP to both the attention vector σ and the transformed features\nK. Finally, the residual features recorded as Aare deﬁned as\nthe weighted sum of the attention weights with all V vectors.\nThe formula is as follows:\nA= ρ\n(\nγ(σ(Q,K) +P)\n)\n⊙\n(\nV + P\n)\n(4)\nwhere ρis a normalization function (softmax) and γ is a non-\nlinear mapping function (MLP) that includes two linear layers\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nINPUT\nSearch Area\nTemplate\nBETTER VOTES\nTRANSFORMER \nBLOCK\nBACKBONE\nPointNET++\nPointNET++\nTARGET-SPECIFIC \nFEATURE\nAUGMENTATION\nTemplate Seeds\nSearch Seeds\nAugmented   Feature\nClassifying\nVoting\nRefine Feature\nRefine Feature\nClustering\nBETTER PROPOSALSTRANSFORMER \nBLOCK\nCluster   Feature\nRefine   Feature\nProposals Generating\nBETTER BOX\nProposals Verification\nFinal 3D Box\nFig. 3. The pipeline of PTT-Net. In order to verify the effect of our PTT module, we embed two PTT modules into seeds voting and proposals generation\nstage of the deep hough voting framework.\nand one ReLU non-linearity. σis the relation function between\nQ and K. Ais output attention features.\n4) Relations Functions: The relation function σ(Q,K) is\nthe core of self-attention. Different ways of obtaining the\nrelation between Query vector and Key vector could construct\ndifferent types of attention modules. As mentioned in [50],\nself-attention operators can be classiﬁed into two types: scalar\nattention [21] and vector attention [49]. In scalar attention,\nthe relation function σ(Q,K) can be expressed in the form\nof Eq. 2, which computes the scalar product between Query\nvector and Key vector. The vector attention obtains vector\nattention weights by using channel-wise subtraction operation.\nBesides, it has been conﬁrmed in [50] that vector attention is a\nnatural ﬁt for point cloud than scalar attention since it supports\nadaptive modulation of individual feature channels, not just\nwhole feature vectors. Therefore, we use the vector attention\nstructure. The relation function σ(Q,K) is a subtraction\noperation. The formula is as follows:\nσ(Q,K) =Q−K (5)\nE. PTT-Net\nThis section details our PTT-Net which is a more accurate\nand robust target tracking based on the existing open-source\nmethod P2B [17]. In the following, we ﬁrst explore the effect\nof different sampling methods on point cloud-based tracking\ntasks. Then, the position where the PTT module is embedded\nand the loss function of the network training are described.\n1) Sampling Strategies:Here, we ﬁrst discuss the impact of\ndifferent sampling strategies on tracking task. The purpose of\nsampling is to extract key points and ensure that the number\nof points in the template or search point cloud is aligned with\nthe input dimension of network. However, different sampling\nmethods will lead to different degrees of target information\nloss. At the same time, we ﬁnd that in the existing tracking\npipeline, the more foreground points left in the search area\nafter sampling, the more accurate the regression results of\nthe network. In contrast, due to the unbalanced distribution of\nforeground and background points, the classiﬁcation accuracy\nof the network will decrease. Therefore, a suitable sampling\nstrategy can not only achieve input alignment, but also improve\nthe tracking performance of the tracking network.\nRandom Sample Feature-FPS\nFPSSearch Area\nFig. 4. Visualization of different sampling methods . The input points and\nsampled points are labeled as black and red respectively. As shown in the\nﬁgure, the result of RS depends on the distribution density. And FPS can better\nretain the geometric information. However, the Feat-FPS mainly focuses on\nthe foreground points, which will cause the unbalanced distribution between\nforeground and background points.\nCommon 3D point cloud sampling methods include Random\nSampling (RS), Farthest Point Sampling (FPS), and Farthest\nPoint Sampling in Feature dimension (Feat-FPS). RS can\nachieve high sampling efﬁciency, but the sampling result de-\npends on the distribution density of point cloud. FPS can better\nretain the geometric information of the original point cloud,\nand is a more balanced sampling method. In addition, Feat-\nFPS proposed in 3D-SSD [62], which can sample in feature\nspace, has better sampling results for targets of different\nsemantic categories. The visualization of the three different\nsampling methods is shown in Fig. 4. We ﬁnd that FPS has\nmore uniform sampling results, while Feat-FPS pays more\nattention to points on the target. P2B uses RS, although it\nhas a higher computational efﬁciency, it leads to the loss of\nsome key information, which limits their tracking accuracy.\nIn summary, for the sampling strategy of tracking task,\nwe adopted FPS which can retain the geometric information\nof the original point cloud and make the foreground and\nbackground points balanced. A demonstration of its good\ntracking performance will be shown in Sec. IV-D.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nGT Box Predict \nBox\nForeground \nPoints\nBackground \nPoints\n(a) Car (b) Car  \n(c) Pedestrian (d) Pedestrian\nWithout PTT With PTT\nFig. 5. Visualization of classiﬁcation(a-b) and tracking(c-d) results with\nor without PTT module . The point will be paid more attention if it has\na higher score. Compared (a) with (b), PTT module pays more attention to\nthe foreground points. Compared (c) with (d), PTT module could still track\ntargets robustly in crowded scenes(with multiple pedestrians).\n2) Embedding Position of PTT:The ability of transformer\nto learn self-attention weights inspires us to try it on 3D SOT\ntask. We formulate the problem of focusing on the differences\nof features as self-attention weighting. In order to verify the\neffect of our method, we embed our PTT module into the\nprevious open-source state-of-the-art (SOTA) LiDAR-based\n3D SOT work P2B [17]. More speciﬁcally, the PTT modules\nare inserted in seeds voting stage and proposals generation\nstage of P2B respectively. In the seeds voting stage, P2B\ngenerates votes by utilizing the augmented features, which are\nobtained from the backbone network (in Fig. 3). We notice\nthat [17] ignores the differences among different point cloud\nfeatures in the search area, and gives no preference to the\npoints in different locations when generating votes. However,\nit is important to focus on the points which contain more geo-\nmetric information. Therefore, we apply PTT module to weigh\nthe augmented features and model interactions among point\npatches to learn the context-dependent feature (in Fig. 5(a)(b)).\nIn proposals generation stage, P2B generates proposals\nbased on local context features. However, their method ignores\nthe global semantic features of the targets, so that they\ncould not distinguish similar objects (e.g., two pedestrians,\nin Fig. 5(c)(d)). Therefore, we use the PTT module to fur-\nther weigh the target-wise context features obtained by the\naggregation network in P2B for tracking deeper-level target\nclues to capture the contextual information between object and\nbackground.\nAs shown in Fig. 3, we embed our PTT module in the\nopen-source SOTA method P2B [17] to build PTT-Net. We\nadd PTT module to the seeds voting and proposal generation\nstages, and weigh the augmented features and cluster features\nrespectively. Experiments show that our PTT-Net outperforms\nthe SOTA method with remarkable margins.\n3) Loss Function:The PTT module is trained with the other\nsub-networks in [17]. Therefore, we follow [17] to design our\n0 – 20 20 - 100 100 - 500 > 500\n26.10%\n31.69%\n24.95%\n17.25%\nNumber \nof points\nPercentage\nFig. 6. The percentages and visualizations of the different number\nintervals of point cloud in the car category. The number of frames\ncontaining less than 20 points accounts for 26.10% of the total. The number of\nframes containing 20-100 points accounts for 31.69% of the total. In addition,\nthe number of 100-500 and more than 500 frames are accounting for 24.95%\nand 17.25% of the total, respectively. For four different intervals, we visualize\nfour point clouds with 11, 52, 293, and 883 points, respectively.\nloss function. The overall loss consists of two parts as follows:\nLall = Lcv + λ1Lcb + λ2Lrv + λ3Lrb (6)\nwhere λ1, λ2, λ3 represent the weighting coefﬁcient of each\nloss. Classiﬁcation loss includes voting classiﬁcation loss Lcv\nand proposal box classiﬁcation loss Lcb. The regression loss\nincludes the voting loss Lrv and the proposal box regression\nloss Lrb.\nIV. EXPERIMENTS\nWe used KITTI tracking dataset [24] and NuScenes [25]\ndataset as the benchmark. Similar to [16], [17], [19], [44], we\nmainly focused on rigid car tracking and performed ablation\nstudies on KITTI. We also conducted extended experiments\nwith other three target categories (Pedestrian, Van, Cyclist) to\ncomprehensively evaluate the performance of our method for\nnon-rigid objects (Pedestrian) tracking on KITTI. Besides, we\nalso follow BAT [64] to evaluate our PTT-Net on NuScenes.\nThe experiments show that PTT-Net outperforms previous\nSOTA methods with remarkable margins and can run at 40\nfps. 1\nA. Experimental Protocols\n1) Datasets: We used the training set of KITTI and\nNuScenes dataset to train and evaluate our method. For KITTI\ndataset, there are more than 20,000 manually labeled 3D\nobjects using Velodyne HDL-64E 3D lidar (10HZ). Following\n[16], [17], [19], [44], we split the dataset as follows: 0-16 for\ntraining, 17-18 for validation and 19-20 for testing. Specially,\nwe ﬁrst extract each frame label from every scene label of\nKITTI tracking benchmark. Then, we further extract each ID\nlabel from every frame label, and ﬁnally concatenate the labels\nof the same ID to obtain each tracklet label from its ﬁrst frame\nto its ﬁnal frame. By this way, we convert the KITTI MOT\nlabel to SOT label. Furthermore, for each tracklet, only the\n1Our experiment video is available at https://youtu.be/z5Vkm8r9Wus.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE I\nPERFORMANCE COMPARISON ON KITTI FOR THE CAR CATEGORY . RED AND BLUE MEAN THE PERFORMANCE SCORE IS RANKED FIRST AND SECOND\nRESPECTIVELY .\nModule Modality 3D Success 3D Precision FPS\nA VOD-Tracking [63] RGB+LiDAR 63.1 69.7 -\nF-Siamese [44] RGB+LIDAR 37.1 50.6 -\nSC3D [16] LiDAR only 41.3 57.9 1.8\nETP2D-3D [42] LiDAR only 36.3 51.0 -\nP2B [17] LiDAR only 56.2 72.8 45.5\n3D-SiamRPN [19] LiDAR only 58.2 76.2 20.8\nPTT-Net(Ours) LiDAR only 67.8 81.8 40.0\nﬁrst frame includes the 3D ground truth bounding box (bbox)\nin the testing phase. Therefore, we initialize our tracker with\nthe ﬁrst frame 3D ground truth bbox during tracking, and then\ntrack the object in the sequence frames by our tracker. In order\nto better illustrate that the point cloud in KITTI is challenging\nfor tracking task, we choose the Car category and count the\nnumber of foreground points in each frame, then calculate\nthe percentage referred to all frames from 00-20 sequence in\nFig. 6. The total number of frames is 27292. The number of\nframes that are less than 20 points is 7123, accounting for\n26.10% of the total. The number of frames between 20-100\nis 8650, accounting for 31.69% of the total. In addition, the\nnumber of 100-500 and more than 500 frames are 6810 and\n4709, respectively, accounting for 24.95% and 17.25% of the\ntotal. This fully shows that about half of the frames in KITTI\nare sparse scenes. Hence the KITTI dataset is challenging\nfor tracking task. This will be the bottleneck of limiting the\ntracking accuracy as mentioned in Sec. III-B. Furthermore, we\nexemplify the visualization results of point clouds at various\nnumber intervals in Fig. 6.\nFor NuScenes dataset, there are 1000 driving scenes and 23\nobject categories, which make it is more challenging because\nof more complex scenes. We follow the same settings with the\nBAT [64] to obtain a fair comparison, and directly refer to the\nresults reported in BAT [64] for comparison.\n2) Evaluation Metric: Following previous work [16], [17],\n[19], [44], we report Success and Precision metrics deﬁned by\nOne Pass Evaluation (OPE) [65], which represent the overlap\nand error Area Under the Curve (AUC) respectively.\n3) Implementation Details: We use FPS in our implemen-\ntation instead of RS used in original P2B [17] for point cloud\nsampling. In the training stage, we use the Adam optimizer\nand set the initial learning rate to 0.001 and decrease by 5\ntimes after 12 epochs. The batch size is 48 and the training\nepoch is 60. Besides, we extend the offset from (x, y, θ) to\n(x, y, z, θ) when generating more template samples during\ndata augmentation in [17]. In the testing stage, we also add\nZ axis offset to generate the predicted box. Other parameters\nare consistent with settings of [17]. Meanwhile, we also follow\nthe tracking setting of P2B [17]. Specially, we initialize the\ntemplate point cloud with the point cloud of the ﬁrst frame\nground truth and update the template point cloud by fusing the\npoint cloud of ﬁrst frame ground truth with previous result.\nThe search point cloud is updated based on the point cloud of\nthe previous result, which can better meet the requirement of\nreal scenes.\nB. Quantitative Experiment\nTo better evaluate our method, we designed two quantitative\nexperiments. In the ﬁrst experiment, we quantitatively evalu-\nated our method for 3D car tracking. In the second experiment,\nwe further compared PTT-Net with the previous methods\namong different categories on both KITTI and NuScenes\ndatasets.\n1) Comparisons on car category:We compared the perfor-\nmance of our PTT-Net with the existing methods on the KITTI\ndataset and reported the results for 3D car tracking in Tab. I. To\nmeet the requirement of real scenarios, we generate the search\narea centered on the previous tracking result. The results show\nthat our PTT-Net has achieved SOTA performance in all eval-\nuation metrics with remarkable margins. Compared with the\nbaseline algorithm P2B [17], our performance has been greatly\nimproved by ∼11% in 3D Success. Besides, compared with\nthe previous SOTA method 3D-SiamRPN [19], our method\nperforms better by a margin of ∼9% and ∼5% in 3D Success\nand 3D Precision respectively. It veriﬁes the superiority of\nour method, and shows that PTT-Net could work better in\nchallenging scenarios like sparse or occlusion scenarios, while\nother trackers often fail to track in these scenes. Additionally,\ncompared with [63] and [44] which both use RGB+LIDAR\nfusion information, the Success/Precision results of PTT-Net\noutperform them 4.7%/12.1% and 30.7%/31.2% respectively.\nWe think that this is because our method can capture important\nfeature representations on tracking targets even if it is only\nbased on raw point cloud data. More importantly, compared\nwith other methods, our proposed method not only has a good\nperformance, but also could run in real time with 40 FPS.\n2) Comparisons on other categories:For four categories in\nKITTI tracking dataset, the Car category is the rigid object.\nTo evaluate the tracking performance in more scenes and\nespecially containing non-rigid objects, we further compared\nour method with previous methods on Pedestrian, Van, and\nCyclist (Tab. II). Except for F-Siamese [44], which fuses image\nand point cloud information, all other methods adopt the same\nexperimental settings.\nAs shown in Tab. II, the average performance of PTT-Net\noutperforms SC3D [16], P2B [17] and 3D-SiamRPN [19] by\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE II\nEXTENSIVE COMPARISONS WITH DIFFERENT CATEGORIES ON KITTI (LEFT ) AND NUSCENES (RIGHT ) DATASET. RED AND BLUE MEAN THE\nPERFORMANCE SCORE IS RANKED FIRST AND SECOND RESPECTIVELY . AND FRAME NUMBER INDICATES THE INSTANCE NUMBER OF EACH CATEGORY .\nDataset KITTI NuScenes\nCategory Car Pedestrian Van Cyclist Mean Car Truck Trailer Bus Mean\nFrame Number 6424 6088 1248 308 14068 64159 13587 3352 2953 84051\n3D Success\nSC3D [16] 41.3 18.2 40.4 41.5 31.2 22.31 30.67 35.28 29.35 24.43\nP2B [17] 56.2 28.7 40.8 32.1 42.4 38.81 42.95 48.96 32.95 39.68\nF-Siamese [44] 37.1 16.2 - 47.0 - - - - - -\n3D-SiamRPN [19] 58.2 35.2 45.6 36.1 46.6 - - - - -\nBAT [64] 65.4 45.7 52.4 33.7 55.0 40.73 45.34 52.59 35.44 41.76\nPTT-Net(Ours) 67.8 44.9 43.6 37.2 55.1 41.22 50.23 61.66 43.86 43.58\n3D Precision\nSC3D [16] 57.9 37.8 47.0 70.4 48.5 21.93 27.73 28.12 24.08 23.19\nP2B [17] 72.8 49.6 48.4 44.7 60.0 43.18 41.59 40.05 27.41 42.24\nF-Siamese [44] 50.6 32.2 - 77.2 - - - - - -\n3D-SiamRPN [19] 76.2 56.2 52.8 49.0 64.9 - - - - -\nBAT [64] 78.9 74.5 67.0 45.4 75.2 43.29 42.58 44.89 28.01 42.70\nPTT-Net(Ours) 81.8 72.0 52.5 47.3 74.2 45.26 48.56 56.05 39.96 46.04\n35\n40\n45\n50\n55\n60\n65\n70\n0\n10\n20\n30\n40\n50\nFPS\nSC3D\n3D\n-\nSiamRPN\nOurs\nP2B\n3D\nSuccess\n55\n60\n65\n70\n75\n80\n85\n0\n10\n20\n30\n40\n50\n3D\nPrecision\nFPS\nSC3D\n3D\n-\nSiamRPN\nOurs\nP2B\nFig. 7. Tracking Success and Precision vs. speed of the dominant trackers\non Car category. The proposed PTT-Net is superior than SC3D [16], P2B\n[17], 3D SiamRPN [19] at 3D Success and 3D Precision, and could maintain\na good inference speed.\n∼24%, ∼13% and ∼9% respectively. It is worth noting that the\nSuccess/Precision results of PTT-Net show an improvement\n(9.7%/15.8%) on non-rigid object (Pedestrian) tracking. The\nresult also veriﬁes that our PTT module can help the network\nunderstand and learn the important features of the target\nbetter. Additionally, we notice that there are performance gaps\nbetween our method and the best method F-Siamese [44] in\nVan and Cyclist categories. We believe there are two reasons.\nFirst, the cyclist has the least training samples (only 1529\nsamples for training), which may limit the performance of the\nFig. 8. The comparison of different points number intervals in the ﬁrst\nframe between PTT and P2B . The Success and Precision results are shown\nin two line charts separately. Only one case that the performance of PTT\nis lower than P2B when the points number interval is in [50,100]. And the\naverage accuracy of PTT is much higher than that of P2B.\ntransformer. And we did not do any extra data augmentation\nfor cyclist because we would like to use a fair setting among\nall categories. Second, F-Siamese ﬁrstly utilizes a 2D siamese\ntracker in the front end only using dense image data, and\ngenerates a search area based on the results of the 2D siamese\ntracker, which provides prior information for subsequent 3D\nSOT. We believe that the data fusion in F-Siamese may result\nin the better performance. Additionally, we also notice that\nthe Success results of P2B achieves 28.7%/32.1% in pedestrian\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nOriginal Point Cloud Search Area with PTT Module\nGT Box Predict Box Attention Points Attention Score\n0 1\nwithout PTT Module\nFig. 9. Comparison of visualization results with or without PTT module. We compared the attention points and corresponding scores in the third column\nand the fourth column when the network had the PTT module or not. Furthermore, the tracking difﬁculty is increasing as the number of initial points in the\nsearch area decreases from top to bottom.\nand cyclist respectively, and our method brings +16.2%/+5.2%\ngains in the two categories. Because our method is based on\nP2B, we believe the results also verify the effectiveness of our\nmethod. Besides, our performance is much higher than that\nof F-Siamese method in the categories with more abundant\ndata, such as vehicles and pedestrians. The Success/Precision\nresults of PTT-Net outperform F-siamese by 30.7%/31.2% and\n28.7%/39.8% in vehicles and pedestrian category respectively.\nBesides, we also evaluated our PTT-Net on NuScenes\ndataset to further conﬁrm the effectiveness of our method.\nAlthough KITTI dataset is commonly used by previous 3D\nSOT methods, its scale is too small and this may limit the\nperformance of proposed network. Recently, BAT [64] reports\nthe results of the dominant methods on NuScenes dataset.\nHence we follow the settings of BAT and evaluate our PTT-\nNet on NuScenes, and report the results in Tab II. As shown\nin Tab II, our PTT-Net outperforms BAT in all categories.\nThis indicates that our PTT-Net could address the challenging\nscenes in NuScenes more effectively.\n3) Comparison of speed and performance:We further show\nthe comparisons with previous trackers in terms of speed,\n3D Success and 3D Precision on Car category. As shown in\nFig. 7, our PTT-Net achieves SOTA performance on both 3D\nSuccess and 3D Precision. Meanwhile, our method has less\ncomputational burden. In other words, our method has both\nhigh accuracy and fast running speed.\n4) Comparisons in different points intervals: Here, we\nchoose the Car category and divide the tracking sequence\naccording to the number of points in the ﬁrst frame, which\nis used to initialize the template point cloud and plot the cor-\nresponding tracking performance curve (Fig. 8). As mentioned\nin Sec. III-B, the sparsity of point cloud limits the performance\nof 3D SOT trackers. Generally, in extremely sparse scenes (the\nnumber of points on the target is less than 50), most existing\ntrackers often track off or fail. However, as shown in Fig. 8, the\nperformance of our method outperforms P2B in both Success\nand Precision with a large margin when points number is less\nthan 50. This shows that our method could deal with sparse\nscenes better and has more robust performance.\nC. Qualitative Experiment\n1) PTT Module show:To show the effects of PTT modules\nin PTT-Net, we exempliﬁed the attention points and scores\nwhen PTT-Net utilizes PTT Module or not in Fig. 9. For\nattention points and scores, we set the voting scores in the\nvoting stage as attention scores. The higher the score, the\nmore attention the corresponding points have been focused\non. As shown in Fig. 9, from the ﬁrst row to the fourth row,\nthe point cloud of tracking target becomes more sparse, which\nmakes the tracking difﬁculty higher. However, the results show\nthat our PTT module can achieve robust tracking in all these\ndifferent scenarios. And the attention scores tend to be higher\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nframe=85First frame frame=55 frame=65\nGround Truth PTT-Net (Ours) P2B\nframe=45 frame=75\nTimeline\nframe=30First frame frame=5 frame=10\nTimeline\nGround Truth PTT-Net (Ours) SC3D P2B\nframe=1 frame=15\na)\nb)\nFig. 10. Advantageous cases of PTT-Net compared with SC3D and P2B (a-b). In (a) and (b), the number of point clouds in the ﬁrst frame is less than\n50. Our method can track the target accurately. However, in scenario (a), both P2B and SC3D failed to track. In scenario (b), even though P2B could track\nthe target, it still has an inaccurate z-axis estimation for the target. Meanwhile, SC3D has failed to track. These results show the robustness of our method in\nsparse point cloud scenarios. Please see our experiment video for more details.\nframe=10\nTracking results show with search points\nFirst frame frame=7frame=3frame=1\nTimeline\nGround Truth PTT-Net(Ours)\nFig. 11. Failure cases. no points in the initial search area\nin the location where the target features are rich. Meanwhile,\nwe also observe that our PTT module helps network ﬁlter\nthe background noise better and focus on the tracking target.\nEspecially, from the visualization results in the fourth row of\nFig. 9, even if there are few points, our PTT module could\nstill help the network focus more on the foreground points,\nwhich once again proves the power of our PTT module.\n2) Advantageous cases: We visualized our advantageous\ncases over P2B and SC3D in Fig. 10. We can observe from\nFig. 10 (a) that in the sparse scenarios (less than 50 points)\nwhere both SC3D and P2B tracked off course or even failed,\nour PTT-Net still tracks the target robustly. In Fig. 10 (b),\neven though P2B can track the target, their position estimation\nstill has a large deviation in z axis. This also shows the\neffectiveness of our method. We could not only track the\ntarget robustly in sparse scenes, but also estimate the location\ninformation of the target more accurately.\n3) Failure cases:The Fig. 10 shows that PTT-Net can work\nwell in most of scenes compared to SC3D and P2B. However,\nthe sparsity of points still inﬂuences the performance of PTT-\nNet. Our method tends to fail when the points are extremely\nsparse. To show the failure case of our method in more detail,\nwe visualized the failed tracking result when the points are less\nthan 20. As shown in Fig. 11, our PTT-Net could not learn\neffective object features since there are almost no points in the\ninitial search area. In addition, due to the sparse point cloud,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nTABLE III\nDIFFERENT WAYS FOR TEMPLATE GENERATION . “FIRST & PREVIOUS \" DENOTES “THE FIRST GROUND TRUTH (GT) AND PREVIOUS RESULT \".\nSource of Success Precision\ntemplate points PTT-Net 3D-SiamRPN [19] P2B [17] SC3D [16] PTT-Net 3D-SiamRPN [19] P2B [17] SC3D [16]\nThe First GT 62.9 57.2 46.7 31.6 76.5 75.0 59.7 44.4\nPrevious result 64.9 - 53.1 25.7 77.5 - 68.9 35.1\nFirst & Preivous 67.8 58.2 56.2 34.9 81.8 76.2 72.8 49.8\nAll previous 59.8 - 51.4 41.3 74.5 - 66.8 57.9\nTABLE IV\nDIFFERENT WAYS FOR SEARCH AREA GENERATION .\nSuccess Precesion\nSC3D [16] P2B [17] PTT-Net(Ours) SC3D [16] P2B [17] PTT-Net(Ours)\nPrevious Result 41.3 56.2 67.8 57.9 72.8 81.8\nPrevious GT 64.6 82.4 75.9 74.5 90.1 88.9\nCurrent GT 76.9 84.0 76.1 81.3 90.3 89.1\nthe feature ambiguity also leads to the inaccurate estimation\nof the bounding box, which causes the propagation of errors\nand failure case ﬁnally.\nD. Ablation Study\nHere, we ablate the network architecture on KITTI dataset.\nFirst, we discuss different ways for seeds sampling, template\ngeneration, and search area generation. Then we ablate the\ndifferent embedded position of our PTT module. Finally, dif-\nferent parameters selection of PTT module are also discussed.\n1) Ways for seeds sampling:The ﬁrst ablation study pre-\nsented is designed to support our claim that FPS could beneﬁt\nthe classiﬁcation task. As mentioned in Sec. III-E1, different\nsampling methods will lead to different degrees of target\ninformation loss. Here, we compared the effects of three\ndifferent down-sampling methods: Random Sampling (RS),\nFarthest Point Sampling (FPS), and Feat-FPS proposed by\n[62] (Tab. V) on the performance of our method. We also\nshowed the visualizations of three sampling methods (Fig. 4).\nWe found that RS had the worst performance, and FPS could\nobtain the best performance, which was ∼7% higher than RS.\nBesides, Feat-FPS also had good tracking performance, which\nwas only ∼1% lower than the FPS. We attribute this result\nto the fact that FPS can obtain seeds which belong to the\nforeground and background points uniformly. Meanwhile, FPS\ncould keep the distribution probability of the original input\npoint cloud to the greatest extent while reducing the dimension\nof the input, which will be beneﬁcial to the classiﬁcation and\nregression tasks of the tracking network.\n2) Ways for template generation:Our method is consistent\nwith P2B when generating template point clouds. Therefore,\nwe explored the different way of template point cloud genera-\ntion, including the ﬁrst ground truth, previous result, the fusion\nof the ﬁrst ground truth and previous result, and all previous\nresults. Specially, the fusion between ﬁrst frame ground truth\nand previous results means the fusion of two point clouds\nTABLE V\nPERFORMANCE OF DIFFERENT SAMPLING METHODS .\nRandom Sample Feat-Fps Fps\n3D Success 60.4 66.1 67.8\n3D Precision 73.7 80.0 81.8\nTABLE VI\nDIFFERENT EMBEDDED POSITIONS OF PTT MODULE .\nAblation 3D Success 3D Precision\nbaseline [17] 56.2 72.8\nOnly PTT in V ote 62.1 76.9\nOnly PTT in Prop 65.7 78.9\nPTT in all (PTT-Net) 67.8 67.867.8 81.8 81.881.8\nwithin the two 3D bounding boxes respectively. First, we\nextract the points in the box from the point cloud according\nto the ﬁrst frame ground-truth box and the predicted box in\nthe previous frame. Second, according to the angle of the box\nand the coordinates of the center point, the two frames of\npoint clouds are normalized to the same coordinate system\nby rotating and translating respectively. Finally, the updated\ntemplate point clouds could be obtained by concatenating two\npoint clouds directly. We reported the results in Tab. III. We\nfound that our PTT-Net outperformed 3D-SiamRPN, P2B and\nSC3D in all settings. Besides, the proposed method has the\nbest performance by fusing the ﬁrst ground truth and the\nprevious result. We believe that this is because our method\nhas more robust tracking results in sparse scenes. Therefore,\nif we fuse the ﬁrst ground truth and the previous result to\nupdate the template points, it could further enhance the target\ninformation and improve the algorithm performance.\n3) Ways for search area generation:The generation strat-\negy of search area in object tracking task directly determines\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nTABLE VII\nABLATION STUDY OF HYPER -PARAMETERS IN PTT MODULE .\n3D Success 3D Precision\nHead Number\n1 67.8 81.8\n2 67.2 80.9\n4 65.6 79.1\n8 65.5 78.1\nLayer Number\n1 67.8 81.8\n2 64.7 78.5\n4 64.0 76.8\n8 62.6 76.5\nthe feature scale and quality that the network can learn. In\nprevious work SC3D [16], P2B [17], there are performance\ncomparisons in different search area situations. To further\nexplore the performance of our method, we also conducted\nexperiments on the search area generation strategies, and\ncompared them with SC3D [16] and P2B [17]. The experi-\nmental results are shown in Tab. IV. Speciﬁcally, we compared\nthree different search area generation methods: 1) centered\non previous result; 2) centered on previous ground truth; 3)\ncentered on current ground truth. The results show that the\nperformance of the three methods has been greatly improved\nwith search area generated by ground-truth. The reason is that\neach frame uses the ground-truth result, which can effectively\navoid the accumulation of errors caused over time. However,\nit is worth noting that our method PTT is slightly lower than\nP2B after using the ground-truth, but the performance is still\nat a similar level.\n4) Positions for PTT module embedding: To verify our\ndesign in Sec. III-E of positions where PTT modules are em-\nbedded, we tried different schemes (Tab. VI). The results show\nthat embedding PTT module in both stages of [17] can obtain\nthe best improvement. Comparing (a) with (b) in Fig. 5, PTT-\nNet has better point cloud classiﬁcation results which focus\non foreground points. Comparing (c) with (d), PTT-Net could\nstill track the target pedestrian robustly when more proposal\ncenters are generated from another pedestrian. Besides, as\nshown in Fig. 9, PTT-Net can focus on foreground points with\nthe help of PTT module. This result effectively shows that the\ntransformer can learn more target-wise information.\n5) Parameters selection for PTT module:Here we discuss\nthe details of our PTT module, including the number of heads\nand the number of attention layers, as shown in Tab. VII. For\nthe number of heads, we observe that head= 1and layer = 1\nachieves the best performance, and stacking more heads or\nlayers could not bring in performance improvement but more\nparameters and lower speed. We believe that since our PTT\nmodule is directly applied on the fusion feature, which already\nhas the similarity representations, more heads or layers in PTT\nmay make the feature focus on other unimportant features, thus\ndistracting the already fused similarity features. Therefore, we\nset both the heads and layers to 1.\nE. Timing Breakdow\nWe calculated the average running time of all test frames for\ncar to measure PTT-Net’s speed. PTT-Net achieved 40 FPS on\na single NVIDIA 1080Ti GPU, including 8.3 ms for preparing\npoint cloud, 16.2 ms for model forward propagation, and 0.5\nms for post-processing. The running time of SC3D [16], P2B\n[17] and 3D-SiamRPN [19] on the same platform are 1.8FPS,\n45.5FPS and 20.8FPS, respectively.\nV. C ONCLUSIONS\nIn this work, we explored the application of transformer\nnetwork in 3D SOT task and proposed PTT module. The\nPTT module aims at weighing point cloud features to focus\non the important features of objects. We also embedded\nthe PTT module into the open-source state-of-the-art method\n[17] to build a PTT-Net 3D tracker. Experiments show that\nPTT-Net outperforms previous state-of-the-art methods with\nremarkable margins. We hope that our work will inspire further\ninvestigation of the application of transformers to 3D object\ntracking.\nACKNOWLEDGMENT\nThis work was supported by National Natural Sci-\nence Foundation of China (62073066, U20A20197), Sci-\nence and Technology on Near-Surface Detection Labora-\ntory (6142414200208), the Fundamental Research Funds\nfor the Central Universities (N182608003), Major Spe-\ncial Science and Technology Project of Liaoning Province\n(No.2019JH1/10100026), and Aeronautical Science Founda-\ntion of China (No.201941050001).\nREFERENCES\n[1] A. I. Comport, É. Marchand, and F. Chaumette, “Robust model-based\ntracking for robot vision,” IEEE/RSJ International Conference on Intel-\nligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), vol. 1,\npp. 692–697 vol.1, 2004.\n[2] U.Kart, A.Lukezic, M.Kristan, J.K.Kamarainen, and J.Matas, “Object\ntracking by reconstruction with view-speciﬁc discriminative correlation\nﬁlters,” IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pp. 1339–1348, 2019.\n[3] E. Machida, M. Cao, T. Murao, and H. Hashimoto, “Human motion\ntracking of mobile robot with kinect 3d sensor,” Proceedings of SICE\nAnnual Conference (SICE), pp. 2207–2211, 2012.\n[4] A. Bibi, T. Zhang, and B. Ghanem, “3d part-based sparse tracker\nwith automatic synchronization and registration,” IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , pp. 1439–1448,\n2016.\n[5] Y . Liu, X.-Y . Jing, J. Nie, H. Gao, J. Liu, and G.-P. Jiang, “Context-\naware three-dimensional mean-shift with occlusion handling for robust\nobject tracking in rgb-d videos,” IEEE Transactions on Multimedia, pp.\n664–677, 2018.\n[6] U.Kart, J.K.Kamarainen, and J.Matas, “How to make an rgbd tracker?”\nProc. European Conference on Computer Vision (ECCV), pp. 148–161,\n01 2018.\n[7] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time end-\nto-end 3d detection, tracking and motion forecasting with a single\nconvolutional net,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018, pp. 3569–3577.\n[8] M. Simon, K. Amende, A. Kraus, J. Honer, T. Sämann, H. Kaulbersch,\nS. Milz, and H. Groß, “Complexer-yolo: Real-time 3d object detection\nand tracking on semantic point clouds,” IEEE Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), pp. 1190–1199,\n2019.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\n[9] S. Wang, Y . Sun, C. Liu, and M. Liu, “Pointtracknet: An end-to-end\nnetwork for 3-d object detection and tracking from point clouds,” IEEE\nRobotics and Automation Letters, vol. 5, no. 2, pp. 3206–3212, 2020.\n[10] R. Q. Charles, S. Hao, K. Mo, and L. J. Guibas, “Pointnet: Deep learning\non point sets for 3d classiﬁcation and segmentation,” inIEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2017, pp. 652–\n660.\n[11] X. Weng, J. Wang, D. Held, and K. Kitani, “3d multi-object tracking:\nA baseline and new evaluation metrics,” IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS), pp. 10 359–10 366,\n2020.\n[12] S. Wang, Y . Sun, C. Liu, and M. Liu, “Pointtracknet: An end-to-end\nnetwork for 3-d object detection and tracking from point clouds,” IEEE\nRobotics and Automation Letters, vol. 5, pp. 3206–3212, 2020.\n[13] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatoﬁghi,\nR. M. Martin, and S. Savarese, “Jrmot: A real-time 3d multi-object\ntracker and a new large-scale dataset,” IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS), pp. 10 335–10 342,\n2020.\n[14] E. Baser, V . Balasubramanian, P. Bhattacharyya, and K. Czarnecki,\n“Fantrack: 3d multi-object tracking with feature association network,”\nIEEE Intelligent Vehicles Symposium (IV), pp. 1426–1433, 2019.\n[15] W. Luo, B. Yang, and R. Urtasun, “Fast and Furious: Real Time End-\nto-End 3D Detection, Tracking and Motion Forecasting with a Single\nConvolutional Net,” in Proceedings of the IEEE Computer Society\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n3569–3577.\n[16] S. Giancola, J. Zarzar, and B. Ghanem, “Leveraging shape completion\nfor 3d siamese tracking,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019, pp. 1359–1368.\n[17] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y . Xiao, “P2b: Point-to-box\nnetwork for 3d object tracking in point clouds,” IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 6328–6337,\n2020.\n[18] H. Du, L. Li, B. Liu, and N. Vasconcelos, “SPOT: Selective Point\nCloud V oting for Better Proposal in Point Cloud Object Detection,” in\nEuropean Conference on Computer Vision (ECCV), 11 2020, pp. 230–\n247.\n[19] Z. Fang, S. Zhou, Y . Cui, and S. Scherer, “3d-siamrpn: An end-to-end\nlearning method for real-time 3d single object tracking using raw point\ncloud,” IEEE Sensors Journal, vol. 21, no. 4, pp. 4995–5011, 2021.\n[20] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation\nand detection from point cloud,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019, pp. 770–779.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems, 2017.\n[22] F. Wu, A. Fan, A. Baevski, Y . N. Dauphin, and M. Auli, “Pay less atten-\ntion with lightweight and dynamic convolutions,” in 7th International\nConference on Learning Representations, ICLR 2019, 2019.\n[23] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,”\nin Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies - Proceed-\nings of the Conference, 2019.\n[24] A. Geiger, “Are we ready for autonomous driving? the kitti vision\nbenchmark suite,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2012, pp. 3354–3361.\n[25] H. Caesar, V . Bankiti, A. H. Lang, S. V ora, V . E. Liong, Q. Xu, A. Kr-\nishnan, Y . Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal\ndataset for autonomous driving,” IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 11 618–11 628, 2020.\n[26] J. Shan, S. Zhou, Z. Fang, and Y . Cui, “Ptt: Point-track-transformer\nmodule for 3d single object tracking in point clouds,” 2021 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pp.\n1310–1316, 2021.\n[27] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y . M. Lui, “Visual\nobject tracking using adaptive correlation ﬁlters,” in IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE, jun 2010, pp. 2544–2550. [Online]. Available:\nhttp://ieeexplore.ieee.org/document/5539960/\n[28] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “High-speed\ntracking with kernelized correlation ﬁlters,” IEEE Transactions on\nPattern Analysis and Machine Intelligence (TPAMI), vol. 37, no. 3, pp.\n583–596, 2014.\n[29] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg, “Eco: Efﬁcient\nconvolution operators for tracking,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2017, pp. 6638–6646.\n[30] H. K. Galoogahi, T. Sim, and S. Lucey, “Correlation ﬁlters with\nlimited boundaries,” IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 4630–4638, 2015.\n[31] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. S. Torr,\n“Fully-convolutional siamese networks for object tracking,” inEuropean\nConference on Computer Vision (ECCV), 2016, pp. 850–865.\n[32] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual\ntracking with siamese region proposal network,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2018, pp. 8971–\n8980.\n[33] B. Li, W. Wu, Q. Wang, F. Zhang, and J. Yan, “Siamrpn++: Evolution of\nsiamese visual tracking with very deep networks,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4282–\n4291.\n[34] X. Dong and J. Shen, “Triplet loss in siamese network for object\ntracking,” in ECCV, 2018.\n[35] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. S. Torr, “Fast\nonline object tracking and segmentation: An unifying approach,” inIEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2019,\npp. 1328–1338.\n[36] P. V oigtlaender, J. Luiten, P. H. S. Torr, and B. Leibe, “Siam r-cnn:\nVisual tracking by re-detection,” IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 6577–6587, 2020.\n[37] S. Tian, X. Liu, M. Liu, S. Li, and B. Yin, “Siamese tracking network\nwith informative enhanced loss,” IEEE Transactions on Multimedia,\nvol. 23, pp. 120–132, 2021.\n[38] W. Han, X. Dong, F. S. Khan, L. Shao, and J. Shen, “Learning to fuse\nasymmetric feature maps in siamese trackers,”IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 16 565–16 575,\n2021.\n[39] X. Dong, J. Shen, L. Shao, and F. M. Porikli, “Clnet: A compact latent\nnetwork for fast adjusting siamese trackers,” in ECCV, 2020.\n[40] X. Dong, J. Shen, W. Wang, L. Shao, H. Ling, and F. M. Porikli,\n“Dynamical hyperparameter optimization via deep reinforcement learn-\ning in tracking,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 43, pp. 1515–1529, 2021.\n[41] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, “Transformer\ntracking,” IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 8122–8131, 2021.\n[42] J. Zarzar, S. Giancola, and B. Ghanem, “Efﬁcient tracking proposals us-\ning 2d-3d siamese networks on lidar,” arXiv preprint arXiv:1903.10168,\n2019.\n[43] Y . Cui, Z. Fang, and S. Zhou, “Point Siamese Network for Person\nTracking Using 3D Point Clouds,” Sensors, vol. 20, no. 1, p. 143, dec\n2019. [Online]. Available: https://www.mdpi.com/1424-8220/20/1/143\n[44] H. Zou, J. Cui, X. Kong, C. Zhang, Y . Liu, F. Wen, and W. Li, “F-siamese\ntracker: A frustum-based double siamese network for 3d single object\ntracking,” IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS), pp. 8133–8139, 2020.\n[45] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image\nrecognition,” IEEE/CVF International Conference on Computer Vision\n(ICCV), pp. 3463–3472, 2019.\n[46] P. Ramachandran, I. Bello, N. Parmar, A. Levskaya, A. Vaswani, and\nJ. Shlens, “Stand-alone self-attention in vision models,” in Advances in\nNeural Information Processing Systems, 2019.\n[47] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-End Object Detection with Transformers,” in\nEuropean Conference on Computer Vision (ECCV), vol. 12346 LNCS,\n11 2020, pp. 213–229.\n[48] H. Zhong, J. Chen, C. Shen, H. Zhang, J. Huang, and X.-S. Hua, “Self-\nadaptive neural module transformer for visual question answering,”IEEE\nTransactions on Multimedia, vol. 23, pp. 1264–1273, 2021.\n[49] H. Zhao, J. Jia, and V . Koltun, “Exploring self-attention for image\nrecognition,” IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 10 073–10 082, 2020.\n[50] H. Zhao, L. Jiang, J. Jia, P. H. S. Torr, and V . Koltun, “Point transformer,”\narXiv preprint arXiv:2012.09164, 2020.\n[51] V . B. Nico Engel and K. Dietmayer, “Point transformer,” arXiv preprint\narXiv:2011.00931, 2020.\n[52] M. H. Guo, J. X. Cai, Z. N. Liu, T. J. Mu, R. R. Martin, and S. M.\nHu, “Pct: Point cloud transformer,” Computational Visual Media, vol. 7,\nno. 2, pp. 187–199, 2021.\n[53] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, “3D object detection\nwith pointformer,” in arXiv, 2020.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15\n[54] T. Meinhardt, A. Kirillov, L. Leal-Taixé, and C. Feichtenhofer,\n“Trackformer: Multi-object tracking with transformers,” ArXiv, vol.\nabs/2101.02702, 2021.\n[55] P. Sun, Y . Jiang, R. Zhang, E. Xie, J. Cao, X. Hu, T. Kong, Z. Yuan,\nC. Wang, and P. Luo, “Transtrack: Multiple-object tracking with trans-\nformer,” ArXiv, vol. abs/2012.15460, 2020.\n[56] P. Chu, J. Wang, Q. You, H. Ling, and Z. Liu, “Transmot: Spatial-\ntemporal graph transformer for multiple object tracking,” ArXiv, vol.\nabs/2104.00194, 2021.\n[57] N. Wang, W. gang Zhou, J. Wang, and H. Li, “Transformer meets\ntracker: Exploiting temporal context for robust visual tracking,”\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 1571–1580, 2021.\n[58] J. Shen, X. Tang, X. Dong, and L. Shao, “Visual object tracking by hier-\narchical attention siamese network,” IEEE Transactions on Cybernetics,\nvol. 50, pp. 3068–3080, 2020.\n[59] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical\nfeature learning on point sets in a metric space,” in Advances in neural\ninformation processing systems, 2017, pp. 5099–5108.\n[60] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for\n3d object detection in point clouds,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019, pp. 9277–9286.\n[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 770–778, 2016.\n[62] Z. Yang, Y . Sun, S. Liu, and J. Jia, “3DSSD: Point-based 3d single stage\nobject detector,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020, pp. 11 037–11 045.\n[63] J. Ku, M. Moziﬁan, J. Lee, A. Harakeh, and S. Waslander, “Joint 3d\nproposal generation and object detection from view aggregation,” in\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), 2018, pp. 1–8.\n[64] C. Zheng, X. Yan, J. Gao, W. Zhao, W. Zhang, Z. Li, and S. Cui, “Box-\naware feature enhancement for single object tracking on point clouds,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 13 199–13 208.\n[65] M. Kristian, J. Matas, A. Leonardis, and F. Porikli, “A novel performance\nevaluation methodology for single-target trackers,” IEEE Transactions\non Pattern Analysis and Machine Intelligence (TPAMI), pp. 2137–2155,\n2016.\nREFERENCES\n[1] A. I. Comport, É. Marchand, and F. Chaumette, “Robust model-based\ntracking for robot vision,” IEEE/RSJ International Conference on Intel-\nligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), vol. 1,\npp. 692–697 vol.1, 2004.\n[2] U.Kart, A.Lukezic, M.Kristan, J.K.Kamarainen, and J.Matas, “Object\ntracking by reconstruction with view-speciﬁc discriminative correlation\nﬁlters,” IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pp. 1339–1348, 2019.\n[3] E. Machida, M. Cao, T. Murao, and H. Hashimoto, “Human motion\ntracking of mobile robot with kinect 3d sensor,” Proceedings of SICE\nAnnual Conference (SICE), pp. 2207–2211, 2012.\n[4] A. Bibi, T. Zhang, and B. Ghanem, “3d part-based sparse tracker\nwith automatic synchronization and registration,” IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , pp. 1439–1448,\n2016.\n[5] Y . Liu, X.-Y . Jing, J. Nie, H. Gao, J. Liu, and G.-P. Jiang, “Context-\naware three-dimensional mean-shift with occlusion handling for robust\nobject tracking in rgb-d videos,” IEEE Transactions on Multimedia, pp.\n664–677, 2018.\n[6] U.Kart, J.K.Kamarainen, and J.Matas, “How to make an rgbd tracker?”\nProc. European Conference on Computer Vision (ECCV), pp. 148–161,\n01 2018.\n[7] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time end-\nto-end 3d detection, tracking and motion forecasting with a single\nconvolutional net,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018, pp. 3569–3577.\n[8] M. Simon, K. Amende, A. Kraus, J. Honer, T. Sämann, H. Kaulbersch,\nS. Milz, and H. Groß, “Complexer-yolo: Real-time 3d object detection\nand tracking on semantic point clouds,” IEEE Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), pp. 1190–1199,\n2019.\n[9] S. Wang, Y . Sun, C. Liu, and M. Liu, “Pointtracknet: An end-to-end\nnetwork for 3-d object detection and tracking from point clouds,” IEEE\nRobotics and Automation Letters, vol. 5, no. 2, pp. 3206–3212, 2020.\n[10] R. Q. Charles, S. Hao, K. Mo, and L. J. Guibas, “Pointnet: Deep learning\non point sets for 3d classiﬁcation and segmentation,” inIEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2017, pp. 652–\n660.\n[11] X. Weng, J. Wang, D. Held, and K. Kitani, “3d multi-object tracking:\nA baseline and new evaluation metrics,” IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS), pp. 10 359–10 366,\n2020.\n[12] S. Wang, Y . Sun, C. Liu, and M. Liu, “Pointtracknet: An end-to-end\nnetwork for 3-d object detection and tracking from point clouds,” IEEE\nRobotics and Automation Letters, vol. 5, pp. 3206–3212, 2020.\n[13] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatoﬁghi,\nR. M. Martin, and S. Savarese, “Jrmot: A real-time 3d multi-object\ntracker and a new large-scale dataset,” IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS), pp. 10 335–10 342,\n2020.\n[14] E. Baser, V . Balasubramanian, P. Bhattacharyya, and K. Czarnecki,\n“Fantrack: 3d multi-object tracking with feature association network,”\nIEEE Intelligent Vehicles Symposium (IV), pp. 1426–1433, 2019.\n[15] W. Luo, B. Yang, and R. Urtasun, “Fast and Furious: Real Time End-\nto-End 3D Detection, Tracking and Motion Forecasting with a Single\nConvolutional Net,” in Proceedings of the IEEE Computer Society\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n3569–3577.\n[16] S. Giancola, J. Zarzar, and B. Ghanem, “Leveraging shape completion\nfor 3d siamese tracking,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019, pp. 1359–1368.\n[17] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y . Xiao, “P2b: Point-to-box\nnetwork for 3d object tracking in point clouds,” IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 6328–6337,\n2020.\n[18] H. Du, L. Li, B. Liu, and N. Vasconcelos, “SPOT: Selective Point\nCloud V oting for Better Proposal in Point Cloud Object Detection,” in\nEuropean Conference on Computer Vision (ECCV), 11 2020, pp. 230–\n247.\n[19] Z. Fang, S. Zhou, Y . Cui, and S. Scherer, “3d-siamrpn: An end-to-end\nlearning method for real-time 3d single object tracking using raw point\ncloud,” IEEE Sensors Journal, vol. 21, no. 4, pp. 4995–5011, 2021.\n[20] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation\nand detection from point cloud,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019, pp. 770–779.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems, 2017.\n[22] F. Wu, A. Fan, A. Baevski, Y . N. Dauphin, and M. Auli, “Pay less atten-\ntion with lightweight and dynamic convolutions,” in 7th International\nConference on Learning Representations, ICLR 2019, 2019.\n[23] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,”\nin Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies - Proceed-\nings of the Conference, 2019.\n[24] A. Geiger, “Are we ready for autonomous driving? the kitti vision\nbenchmark suite,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2012, pp. 3354–3361.\n[25] H. Caesar, V . Bankiti, A. H. Lang, S. V ora, V . E. Liong, Q. Xu, A. Kr-\nishnan, Y . Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal\ndataset for autonomous driving,” IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 11 618–11 628, 2020.\n[26] J. Shan, S. Zhou, Z. Fang, and Y . Cui, “Ptt: Point-track-transformer\nmodule for 3d single object tracking in point clouds,” 2021 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pp.\n1310–1316, 2021.\n[27] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y . M. Lui, “Visual\nobject tracking using adaptive correlation ﬁlters,” in IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE, jun 2010, pp. 2544–2550. [Online]. Available:\nhttp://ieeexplore.ieee.org/document/5539960/\n[28] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “High-speed\ntracking with kernelized correlation ﬁlters,” IEEE Transactions on\nPattern Analysis and Machine Intelligence (TPAMI), vol. 37, no. 3, pp.\n583–596, 2014.\n[29] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg, “Eco: Efﬁcient\nconvolution operators for tracking,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2017, pp. 6638–6646.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16\n[30] H. K. Galoogahi, T. Sim, and S. Lucey, “Correlation ﬁlters with\nlimited boundaries,” IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 4630–4638, 2015.\n[31] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. S. Torr,\n“Fully-convolutional siamese networks for object tracking,” inEuropean\nConference on Computer Vision (ECCV), 2016, pp. 850–865.\n[32] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual\ntracking with siamese region proposal network,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2018, pp. 8971–\n8980.\n[33] B. Li, W. Wu, Q. Wang, F. Zhang, and J. Yan, “Siamrpn++: Evolution of\nsiamese visual tracking with very deep networks,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4282–\n4291.\n[34] X. Dong and J. Shen, “Triplet loss in siamese network for object\ntracking,” in ECCV, 2018.\n[35] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. S. Torr, “Fast\nonline object tracking and segmentation: An unifying approach,” inIEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2019,\npp. 1328–1338.\n[36] P. V oigtlaender, J. Luiten, P. H. S. Torr, and B. Leibe, “Siam r-cnn:\nVisual tracking by re-detection,” IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 6577–6587, 2020.\n[37] S. Tian, X. Liu, M. Liu, S. Li, and B. Yin, “Siamese tracking network\nwith informative enhanced loss,” IEEE Transactions on Multimedia,\nvol. 23, pp. 120–132, 2021.\n[38] W. Han, X. Dong, F. S. Khan, L. Shao, and J. Shen, “Learning to fuse\nasymmetric feature maps in siamese trackers,”IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 16 565–16 575,\n2021.\n[39] X. Dong, J. Shen, L. Shao, and F. M. Porikli, “Clnet: A compact latent\nnetwork for fast adjusting siamese trackers,” in ECCV, 2020.\n[40] X. Dong, J. Shen, W. Wang, L. Shao, H. Ling, and F. M. Porikli,\n“Dynamical hyperparameter optimization via deep reinforcement learn-\ning in tracking,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 43, pp. 1515–1529, 2021.\n[41] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, “Transformer\ntracking,” IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 8122–8131, 2021.\n[42] J. Zarzar, S. Giancola, and B. Ghanem, “Efﬁcient tracking proposals us-\ning 2d-3d siamese networks on lidar,” arXiv preprint arXiv:1903.10168,\n2019.\n[43] Y . Cui, Z. Fang, and S. Zhou, “Point Siamese Network for Person\nTracking Using 3D Point Clouds,” Sensors, vol. 20, no. 1, p. 143, dec\n2019. [Online]. Available: https://www.mdpi.com/1424-8220/20/1/143\n[44] H. Zou, J. Cui, X. Kong, C. Zhang, Y . Liu, F. Wen, and W. Li, “F-siamese\ntracker: A frustum-based double siamese network for 3d single object\ntracking,” IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS), pp. 8133–8139, 2020.\n[45] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image\nrecognition,” IEEE/CVF International Conference on Computer Vision\n(ICCV), pp. 3463–3472, 2019.\n[46] P. Ramachandran, I. Bello, N. Parmar, A. Levskaya, A. Vaswani, and\nJ. Shlens, “Stand-alone self-attention in vision models,” in Advances in\nNeural Information Processing Systems, 2019.\n[47] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-End Object Detection with Transformers,” in\nEuropean Conference on Computer Vision (ECCV), vol. 12346 LNCS,\n11 2020, pp. 213–229.\n[48] H. Zhong, J. Chen, C. Shen, H. Zhang, J. Huang, and X.-S. Hua, “Self-\nadaptive neural module transformer for visual question answering,”IEEE\nTransactions on Multimedia, vol. 23, pp. 1264–1273, 2021.\n[49] H. Zhao, J. Jia, and V . Koltun, “Exploring self-attention for image\nrecognition,” IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 10 073–10 082, 2020.\n[50] H. Zhao, L. Jiang, J. Jia, P. H. S. Torr, and V . Koltun, “Point transformer,”\narXiv preprint arXiv:2012.09164, 2020.\n[51] V . B. Nico Engel and K. Dietmayer, “Point transformer,” arXiv preprint\narXiv:2011.00931, 2020.\n[52] M. H. Guo, J. X. Cai, Z. N. Liu, T. J. Mu, R. R. Martin, and S. M.\nHu, “Pct: Point cloud transformer,” Computational Visual Media, vol. 7,\nno. 2, pp. 187–199, 2021.\n[53] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, “3D object detection\nwith pointformer,” in arXiv, 2020.\n[54] T. Meinhardt, A. Kirillov, L. Leal-Taixé, and C. Feichtenhofer,\n“Trackformer: Multi-object tracking with transformers,” ArXiv, vol.\nabs/2101.02702, 2021.\n[55] P. Sun, Y . Jiang, R. Zhang, E. Xie, J. Cao, X. Hu, T. Kong, Z. Yuan,\nC. Wang, and P. Luo, “Transtrack: Multiple-object tracking with trans-\nformer,” ArXiv, vol. abs/2012.15460, 2020.\n[56] P. Chu, J. Wang, Q. You, H. Ling, and Z. Liu, “Transmot: Spatial-\ntemporal graph transformer for multiple object tracking,” ArXiv, vol.\nabs/2104.00194, 2021.\n[57] N. Wang, W. gang Zhou, J. Wang, and H. Li, “Transformer meets\ntracker: Exploiting temporal context for robust visual tracking,”\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 1571–1580, 2021.\n[58] J. Shen, X. Tang, X. Dong, and L. Shao, “Visual object tracking by hier-\narchical attention siamese network,” IEEE Transactions on Cybernetics,\nvol. 50, pp. 3068–3080, 2020.\n[59] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical\nfeature learning on point sets in a metric space,” in Advances in neural\ninformation processing systems, 2017, pp. 5099–5108.\n[60] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for\n3d object detection in point clouds,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019, pp. 9277–9286.\n[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 770–778, 2016.\n[62] Z. Yang, Y . Sun, S. Liu, and J. Jia, “3DSSD: Point-based 3d single stage\nobject detector,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020, pp. 11 037–11 045.\n[63] J. Ku, M. Moziﬁan, J. Lee, A. Harakeh, and S. Waslander, “Joint 3d\nproposal generation and object detection from view aggregation,” in\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), 2018, pp. 1–8.\n[64] C. Zheng, X. Yan, J. Gao, W. Zhao, W. Zhang, Z. Li, and S. Cui, “Box-\naware feature enhancement for single object tracking on point clouds,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 13 199–13 208.\n[65] M. Kristian, J. Matas, A. Leonardis, and F. Porikli, “A novel performance\nevaluation methodology for single-target trackers,” IEEE Transactions\non Pattern Analysis and Machine Intelligence (TPAMI), pp. 2137–2155,\n2016.\nJiayao Shan received the B.S. degree from the\nDalian Jiaotong University of China. He is currently\npursuing the M.S. degree from the Northeastern\nUniversity. His research interests include deep learn-\ning, lidar perception and 3D object detection and\ntracking.\nSifan Zhou received the B.S. degree from the\nCivil Aviation University of China. He is currently\npursuing the M.S. degree from the Northeastern Uni-\nversity. His research interests include deep learning,\nvisual perception and 3D object tracking.\nYubo Cui received the B.S. degree in Automation\nand the M.S. degree in Robot Science and Engi-\nneering from Northeastern University, China in 2017\nand 2020 respectively. His research interests include\ndeep learning, 3D object tracking, and 3D detection.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17\nZheng Fang received the B.S. degree in Automation\nand Ph.D. degree in Pattern Recognition and Intelli-\ngent Systems from Northeastern University, China in\n2002 and 2006 respectively. He was a postdoctoral\nresearch fellow at Carnegie Mellon University from\n2013 to 2015. He is now a full professor in Faculty\nof Robot Science and Engineering at Northeastern\nUniversity, China. His research interests include\nvisual/ Laser SLAM, perception and autonomous\nnavigation of various mobile robots.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8146710395812988
    },
    {
      "name": "Video tracking",
      "score": 0.48184359073638916
    },
    {
      "name": "Transformer",
      "score": 0.4356442987918854
    },
    {
      "name": "Computer vision",
      "score": 0.4104694128036499
    },
    {
      "name": "Object (grammar)",
      "score": 0.39047902822494507
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37653493881225586
    },
    {
      "name": "Electrical engineering",
      "score": 0.08473023772239685
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9224756",
      "name": "Northeastern University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210140031",
      "name": "Science and Technology on Surface Physics and Chemistry Laboratory",
      "country": "CN"
    }
  ]
}