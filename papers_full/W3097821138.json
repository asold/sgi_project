{
  "title": "Indic-Transformers: An Analysis of Transformer Language Models for Indian Languages",
  "url": "https://openalex.org/W3097821138",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3161530172",
      "name": "Jain, Kushal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2488616190",
      "name": "Deshpande Adwait",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2138441558",
      "name": "Shridhar Kumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288397429",
      "name": "Laumann, Felix",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288397430",
      "name": "Dash, Ayushman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2548506194",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W3015650676",
    "https://openalex.org/W2328234756",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2790142169",
    "https://openalex.org/W3011663485",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W318113118",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2798422354",
    "https://openalex.org/W1978489870",
    "https://openalex.org/W2806157494",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2584229400",
    "https://openalex.org/W3099919888",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2095333428",
    "https://openalex.org/W3104382433",
    "https://openalex.org/W2969873034",
    "https://openalex.org/W2952566282",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W3022847728",
    "https://openalex.org/W3033841510",
    "https://openalex.org/W2990069357",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W116003086",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W3100403097",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W3027825632"
  ],
  "abstract": "Language models based on the Transformer architecture have achieved state-of-the-art performance on a wide range of NLP tasks such as text classification, question-answering, and token classification. However, this performance is usually tested and reported on high-resource languages, like English, French, Spanish, and German. Indian languages, on the other hand, are underrepresented in such benchmarks. Despite some Indian languages being included in training multilingual Transformer models, they have not been the primary focus of such work. In order to evaluate the performance on Indian languages specifically, we analyze these language models through extensive experiments on multiple downstream tasks in Hindi, Bengali, and Telugu language. Here, we compare the efficacy of fine-tuning model parameters of pre-trained models against that of training a language model from scratch. Moreover, we empirically argue against the strict dependency between the dataset size and model performance, but rather encourage task-specific model and method selection. We achieve state-of-the-art performance on Hindi and Bengali languages for text classification task. Finally, we present effective strategies for handling the modeling of Indian languages and we release our model checkpoints for the community : https://huggingface.co/neuralspace-reverie.",
  "full_text": "Indic-Transformers: An Analysis of Transformer\nLanguage Models for Indian Languages\nKushal Jain1∗, Adwait Deshpande 2∗, Kumar Shridhar 1 , Felix Laumann 1 , Ayushman Dash 1\n1NeuralSpace, 2Reverie Language Technologies,\n{kushal,kumar,felix,ayushman}@neuralspace.ai, adwait.deshpande@reverieinc.com\nAbstract\nLanguage models based on the Transformer architecture [1] have achieved state-of-\nthe-art performance on a wide range of natural language processing (NLP ) tasks\nsuch as text classiﬁcation, question-answering, and token classiﬁcation. How-\never, this performance is usually tested and reported on high-resource languages,\nlike English, French, Spanish, and German. Indian languages, on the other hand,\nare underrepresented in such benchmarks. Despite some Indian languages be-\ning included in training multilingual Transformer models, they have not been\nthe primary focus of such work. In order to evaluate the performance on Indian\nlanguages speciﬁcally, we analyze these language models through extensive ex-\nperiments on multiple downstream tasks in Hindi, Bengali, and Telugu language.\nHere, we compare the efﬁcacy of ﬁne-tuning model parameters of pre-trained\nmodels against that of training a language model from scratch. Moreover, we\nempirically argue against the strict dependency between the dataset size and model\nperformance, but rather encourage task-speciﬁc model and method selection. We\nachieve state-of-the-art performance on Hindi and Bengali languages for text clas-\nsiﬁcation task. Finally, we present effective strategies for handling the modeling\nof Indian languages and we release our model checkpoints for the community :\nhttps://huggingface.co/neuralspace-reverie.\n1 Introduction\nNatural Language Processing (NLP ) has witnessed a paradigm shift from employing task-speciﬁc\narchitectures to ﬁne-tuning the same pre-trained language models for various downstream tasks [2, 3].\nThese language models are trained on large corpora of unlabelled text in an unsupervised manner.\nWith the advent of Transformer-based architectures and various pre-training techniques in the last two\nyears [4, 5, 6, 7], the state-of-the-art results have improved on various downstream tasks. However,\nmuch of this research has been limited to high-resource languages such as English, French, Spanish,\nand German [8]. There are 7000+ languages spoken around the world and yet most of the NLP\nsystems are largely evaluated on a handful of high-resource languages. Hence, there is a dire need\nto work on NLP beyond resource-rich languages [9]. Our work contributes to ﬁlling this gap, as we\nfocus on NLP for Indian languages.\nIndia is a multilingual country with only 10 percent of its population speaking English [10]. There\nhave been some concerted and noteworthy efforts to advance research in Indian languages [11], but\nvery little work has been done with respect to the recently proposed Transformer-based models [4,\n12]. Most of the recent works that have tried using these models for their applications have preferred\nusing a multilingual version of Bidirectional Encoder Representations from Transformers (BERT ),\nproposed as mBERT [4], rather than training monolingual language models from scratch, i.e., without\ntransferring any parameters from other pre-trained models. However, [8] shows that mBERT does\n∗ Equal contribution\nPreprint. Under review.\narXiv:2011.02323v1  [cs.CL]  4 Nov 2020\nnot have high quality representations for all languages. For example, Multilingual BERT (mBERT )\nperforms worse than non-BERT models on a number of downstream tasks for the bottom 30 percent of\nlanguages in terms of dataset-size upon which mBERT is trained [8]. Therefore, we see it as important\nto evaluate language models in the monolingual setting for low-resource languages. In this paper, we\nevaluate Transformer-based architectures for three Indian languages (Hindi, Bengali, and Telugu). To\nthis end, our contributions are as follows:\n• We train four variants of contextual monolingual language models, namelyBERT , DistilBERT ,\nRoBERT a and XLM -RoBERT a, for three Indian languages (Hindi, Bengali and Telugu), which\nare spoken by more than 60 percent of the Indian population [10]. These languages have\ndifferent scripts based on an alphasyllabary system [13].\n• We present an exhaustive analysis of these models by evaluating them on multipleNLP tasks:\nQuestion Answering ( QA), Parts-of-Speech ( POS ) Tagging, and Text Classiﬁcation. We\nconduct a wide range of experiments with three different setups that enable us to compare\nour language model variants with their multilingual counterparts and understand different\nfactors that lead to better results on downstream tasks.\n• For each setup, we evaluate models under two training settings which include ﬁne-tuning\nthe entire model and using the language model output as contextual embeddings. We use\ndifferent layers, namely Long Short-Term Memory (LSTM ) [14], BiLSTM , fully connected\nand Transformer, on top of these embeddings.\n• We present a correlation between the available dataset size for training vs the results\nobtained and empirically demonstrate that using Transformers as feature extractors can lead\nto competitive results in several downstream tasks.\n• We plan to release our language model checkpoints2 to aid further research in NLP for Indian\nlanguages. Finally, we also intend to open-source mergedQuAD3, a combination of XQuAD\nand MLQA datasets for Hindi, that allows training and comparison of QA models for Hindi.\nOur paper is structured as follows. In the next Section 2, we place our research in context with related\nwork in the downstream tasks we examine here. In Section 3, we provide the necessary theoretical\nbackground to the various Transformer-based models we anticipate to employ, and the subsequent\nSections 4 and 5 explain our experimental setup and our results, respectively. We conclude our work\nwith a brief discussion in Section 6.\n2 Background & Related Work\nIn this section, we brieﬂy discuss some relevant work in NLP for Indian languages, separately for\neach downstream task. We start with QA before we review existing methods for POS tagging and text\nclassiﬁcation for the three Indian languages we investigate.\n2.1 Question Answering\nThe QA task involves extracting the answer for a given question from a corresponding passage.\nTraditional QA systems for Indian languages were largely based on multiple moving components,\neach designed for a speciﬁc task. For instance, [15] outlined a pipeline for QA systems in Hindi with\nseparate components for query generation/processing, document retrieval, and answer extraction.\nThe question types were limited to four and had separate answer selection algorithms for each. A\nsimilar component-based method was proposed by [16] for the Bengali language. While [15] only\nused static data for evaluation, [16] collected data from Wikipedia and annotated it for QA. As\nresearch signiﬁcantly progressed in recent years, more advanced algorithms for different components\nwere proposed in these systems. [17] used Convolutional Neural Networks (CNN s) and Recurrent\nNeural Networks (RNN s) for the classiﬁcation of a question, the software Lucene4 for the retrieval of\nthe document, and similarity features such as term-coverage and proximity-score for extracting the\nanswers. Similarly, [18] used a language-independent graph-based algorithm for document retrieval\nand bidirectional Gated Recurrent Units (GRU) [19] for answer extraction in a multilingual setting for\n2https://huggingface.co/neuralspace-reverie\n3https://github.com/Neural-Space/indic-transformers\n4https://github.com/apache/lucene-solr\n2\nEnglish and Hindi. More recently, Transformer-based models have been applied to the task ofQA,\nbut this has been restricted to ﬁne-tuning of mBERT [4] over translated datasets [20]. A major issue\nfor all the approaches discussed above is that they use different datasets or translate existing datasets,\nand hence lack a common benchmark. QA in English and other high-resource languages has become\nmore end-to-end largely because of the SQuAD dataset [21] and a very clear deﬁnition of the task.\nLately there have been efforts to make NLP more multilingual and many SQuAD-like datasets have\nbeen curated for low-resource languages [22, 23, 24].\n2.2 POS Tagging\nPOS tagging for Indian languages has been a challenging task not only due to the lack of annotated\ndata but also because of their rich morphology. Most of the earlier attempts [25] made to tag such\nlanguages relied heavily on morphological rules and linguistic features of a language and did not\ngeneralize to other languages. [26] tried to counter this by proposing a simple POS tagger based on\na Hidden Markov Model (HMM ) which achieved reasonable results. Amongst Indian languages, a\nlot of work has been done on tagging Hindi when compared to other low-resource languages. [27]\nproposed a method to transfer features from a comparatively high-resource Indian language like Hindi\nusing parallel translation corpora which were more easily available. POS tagging for code-mixed5\n[28] Indian languages was another effort in this direction. It involved tagging code-mixed data in\nmultiple language pairs including English-Bengali, English-Telugu, and English-Hindi, collected from\nvarious social media platforms [29, 30]. More recently, a lot of surveys have focused on evaluating\ncontextualized embeddings for POS tagging across many languages using the Universal Dependencies\n[31] treebanks. For example, [32] present a comparative analysis of 54 languages on tasks like POS\ntagging, dependency parsing and lemmatization using Embeddings from Language Models (ELMo)\n[3], ﬂair [33] and BERT embeddings. Furthermore, [34] analyse how mBERT performs by evaluating\nit on tasks of POS tagging and Named-Entity Recognition (NER ).\n2.3 Text Classiﬁcation\nAdvancements in NLP for Indian languages mirrors the progress made for English albeit it has been\nslower due to the lack of annotated datasets, pre-trained word embeddings, and, more recently, pre-\ntrained language models. Nonetheless, inltk [11] and indic-NLP [35] have curated datasets, trained\nembeddings and created benchmarks for classiﬁcation. Indic- NLP provides pre-trained FastText [36]\nword embeddings in multiple Indian languages. They have also curated classiﬁcation datasets for\nnine languages providing a common benchmark for future research. Apart from these contributions\ntowards the classiﬁcation task, we brieﬂy discuss some other notable work. Similar to tagging\ncode-mixed text, sentiment analysis of code-mixed social media text has also drawn attention [37],\ninvolving English-Hindi and English-Bengali datasets that saw various machine learning and deep\nlearning approaches being applied to classiﬁcation. [38] presented a comparative study of various\nclassiﬁcation techniques for Hindi. They train CNN and LSTM architectures built upon FastText\nembeddings. They also evaluate mBERT sentence embeddings on their translated datasets.\n3 Transformer Language Models\nThe Transformer model by [1] uses stacked encoder and decoder layers comprised of multi-headed\nself-attention layers without using any recurrent or convolutional modules. Despite being a departure\nfrom the then state-of-the-art RNN -based approaches to translation, the Transformer model showed\nsigniﬁcant performance improvements on the WMT 2014 translation task6. Inspired by this use of\nattention mechanism, the language models described below make use of the encoder layers from this\nmodel architecture.\n3.1 Bidirectional Encoder Representations from Transformers\nBERT [4], a bidirectional Transformer model, showed signiﬁcant improvements in natural language\nunderstanding tasks. A version of the model was trained on English and another on 104 languages\nusing a masked language modeling approach. The latter version, called mBERT , was pre-trained on\n5Code-mixing is the mixing of two or more languages or language varieties in speech.\n6A detailed task description and the dataset are available at (http://www.statmt.org/wmt14/)\n3\nWikipedia articles. The pre-trained version could be used to further ﬁne-tune the performance on\ndownstream tasks such as QA and token classiﬁcation.\n[39] observed signiﬁcant improvements in performance over mBERT by training a language model\nfrom scratch on data in German from multiple sources. We expect to see similar performance\nimprovements in low-resource languages by ﬁne-tuning on monolingual data (Setup 2) as well as\ntraining from scratch (Setup 3).\n3.2 DistilBERT\nAlthough the BERT language model achieves state-of-the-art performance, it comes at the cost of a\nlarge model size having more than 340 million parameters [4]. This makes it harder to deploy and\nuse it on resource-constrained devices. [40] proposed a method to reduce BERT ’s model size by 40%\nusing distillation. The model, called DistilBERT and consisting of only 66 million parameters runs\n60% faster and retains 97% language understanding capabilities of the original model. Since we aim\nto show the effectiveness of Transformer-based language models in a production setting as well, we\nhave included DistilBERT (DistilBERT ) in our experiments.\n3.3 RoBERT a\n[12] performed an analysis of the training procedure of BERT and showed that BERT ’s performance\ncan be improved by training BERT on a larger dataset for a longer duration. This model, called\nRoBERT a, shows an improvement of 4-5% over BERT on natural language inference and QA tasks.\nAnother interesting modiﬁcation that the authors make is the use of a byte-level BPE (Byte Pair\nEncoding) tokenizer, instead of a character-level BPE tokenizer used in BERT , to have the advantage\nof a universal encoding scheme at the cost of a minor degradation in performance. CamemBERT [41],\na RoBERT a language model trained from scratch on French language data, achieves state-of-the-art\nperformance on downstream tasks using a relatively small web crawled dataset. We aim to verify\nthis in our experiments by including a similar monolingual RoBERT a model trained on a web crawled\nIndian language dataset.\n3.4 XLM -RoBERT a\nUsing over 2 terabytes of web-crawled data, the XLM -RoBERT a model [42] achieves state-of-the-\nart performance on natural language inference and several downstream tasks. More interestingly,\ntheir work shows that lower resource languages such as Urdu beneﬁt signiﬁcantly through cross-\nlingual training at a large scale. Unlike the language-speciﬁc tokenization employed by mBERT ,\nXLM -RoBERT a uses Sentencepiece [43] tokenization on raw text without any degradation in perfor-\nmance.\n4 Experimental Setup\nAll our experiments are run on BERT -based Transformer language models. We use the Huggingface\nTransformers library [44] for setting up our experiments and for downloading pre-trained models.\nFor each language model, we use three setups:\n1. Setup A: We directly use the pre-trained multilingual version of the models released along\nwith research papers for BERT , DistilBERT and XLM -RoBERT a. RoBERT a was only trained\non English language data and has no multilingual variant. These models form the baseline\nfor all our experiments. The pre-trained models are downloaded from the Huggingface\nmodel hub7.\n2. Setup B: We use the pre-trained models from above and train them further, or ﬁne-tune\nthem, keeping the vocabulary unchanged. We ﬁne-tune separately for Hindi, Bengali, and\nTelugu using monolingual corpora of each language. With this setup, we want to see the\neffect that increasing the amount of language data has on the original multilingual model.\nWe train these models on the monolingual data for 5 epochs using approximately 300MB of\ndata for each language.\n7https://huggingface.co/models\n4\n3. Setup C: As observed in [39], training the German language BERT model from scratch\nresults in a better performance than mBERT . For each of the architectures, including\nRoBERT a, we train Hindi, Bengali, and Telugu models on monolingual data. We do not\ntransfer any parameters from either the pre-trained (Setup 1) or the ﬁne-tuned models (Setup\n2). The models are trained for 5 epochs on monolingual data for each of the languages.\nWe compare this setup with previous setups to see what impact, if any, does cross-lingual\ntraining have on performance.\n4.1 Language Selection\nWe choose to evaluate Transformer models on languages from three different regions — North, West,\nand South — of India. These languages also show a great variation in their linguistic features as well\nas their scripts. This makes them well suited for a comprehensive evaluation of language models.\n1. Hindi: The Hindi language belongs to the Indo-Aryan family [45] and is spoken by around\n322 million people in India [10] making it the most widely spoken language in the country.\nMost speakers of Hindi live in the Northern part of India. Hindi is a verb-ﬁnal language\nwith its verbs and nouns being inﬂected for gender and number. It is written in the phonetic\nDevanagari script with spaces being used to separate words.\n2. Bengali: Spoken by more than 96 million people in the Eastern part of India, Bengali is the\nnext most popularly spoken Indian language after Hindi. It also belongs to the Indo-Aryan\nfamily, but unlike Hindi it has no grammatical gender [46]. Bengali is also written in a\nphonetic script but is distinct from Devanagari.\n3. Telugu: While Bengali and Hindi belong to the same family of languages, Telugu belongs\nto a separate family of Dravidian languages. It is spoken by 81 million people, largely in\nthe Indian states of Telangana and Andhra Pradesh in the South. Telugu is an agglutinative\nlanguage [47], meaning morphemes are combined without a change in form to produce\ncomplex words. Its nouns and verbs are inﬂected for both number and gender. Telugu, like\nHindi and Bengali, is written using a phonetic script but with a separate orthography.\n4.2 Datasets\nWe evaluate the performance of different model architectures on QA, POS tagging, and Text Clas-\nsiﬁcation tasks. While training our language models from scratch (Setup 3), we use monolingual\nunlabelled datasets. As described in Setup 1 and Setup 3, we train all the language models from\nscratch to compare the effect of pre-training. The Open Super-large Crawled ALMAnaCH coRpus\n(OSCAR) dataset [48] is a ﬁltered version of the CommonCrawl dataset and has monolingual corpora\nfor 166 languages. Prior to training, we normalize the OSCAR dataset for Hindi, Bengali, and Telugu\nusing the inltk library.\nThe Stanford Question Answering Dataset (SQuAD) [21], which is commonly used in the evaluation\nof Transformer language models, has all its examples in English. Inspired by the original SQuAD\ndataset, there are now a few multilingual datasets available which fulﬁl the same purpose. The\nTyDi QA dataset by [23] covers 11 languages with diverse linguistic features in order to evaluate\nthe performance of language models on languages beyond English. The dataset contains 204K\nquestion-answer pairs and is divided into two sets of tasks. The primary tasks include selecting\nthe relevant passage and the minimal answer span from the entire context based on the question.\nHowever, we use the dataset for the secondary ‘Gold Passage’ task where the relevant passage is\ndirectly provided. The secondary task is similar to SQuAD and allows us to readily test existing\nlanguage models.\nHindi is more widely spoken than Bengali and Telugu, yet, interestingly, does not have a comprehen-\nsive QA dataset available for training. Both the MLQA [49] and the XQuAD [24] datasets contain\nvalidation and test sets for Hindi but not for training. The XQuAD dataset contains a machine-\ntranslated training dataset that has not been human-veriﬁed. We instead combine the training and\nthe evaluation sets from XQuAD and MLQA datasets and split the training and test sets in the ratio\n90:10. We refer to this dataset in our paper as mergedQuAD. We release the training and test split for\nmergedQuAD dataset for future development.\n5\nPOS tagging occurs at the word level and focuses on the relationship between words within a sentence.\nWe use open-source treebanks annotated by the Universal Dependencies [31] framework for Hindi\nand Telugu. We use the UPOS8 tags to evaluate our models on POS Tagging. The treebank for Hindi\ncomprises of 16 tags and the Telugu treebank is tagged using 14 such tags. Finally, for Bengali, we\nuse the tagged sentences which are a part of the Indian corpus in nltk. There are 887 examples overall\nwhich we split manually into a training and a validation set. 29 unique tags are used in this dataset.\nWhile QA and POS tagging look at parts of documents and sentences, text classiﬁcation considers\nthe entire document before placing it in a category. We have, therefore, included this task for\ncompleteness in our evaluation of Transformer-based language models. For Hindi, we use the BBC\nHindi News Articles9 which contains annotated news articles classiﬁed into 14 different categories.\nSince the dataset only has train and validation splits, we evaluate on the validation set only. We report\nour results on Bengali on a dataset released by Indic-NLP [35]. The examples are categorized into 2\ndifferent classes. For Telugu10 too, we use the classiﬁcation dataset provided by Indic- NLP . Each\nexample in the dataset can be classiﬁed into any one of 3 different categories. We summarize the\nexact dataset splits for all the languages and tasks in Table 1.\nCLASSIFICATION POS TAGGING QUESTIONANSWERING\nDATASET HINDI BENGALI TELUGU HINDI BENGALI TELUGU HINDI BENGALI TELUGU\nNAME BBC Indic- NLP Indic-NLP UD NLTK UD mergedQuAD TyDi-QA TyDi-QA\nTrain 3,468 11,200 19,199 13,304 665 1051 2072 2390 5563\nValidation 867 1,400 2,399 1659 222 131 231 113 669\nTest - 1,400 2,399 1684 - 146 - - -\nTable 1: Detailed break-down of all dataset-splits for the various tasks over which we evaluate our\nmodels. Each cell represents the number of examples in each split for every language. For cases\nwhere test split is missing, we used validation split for testing and validation was performed on a\nsubset of training data.\n4.3 Evaluation settings\nFor each setup explained above, we perform experiments under two settings:\n1. Freezing. In this setting, we use Transformer models as feature extractors. Speciﬁcally, we\nfreeze the weights of the language model and use its output as feature-based embeddings\nto train different layers on top. For simplicity we refer to the language model as the base\nmodel and the trainable layers that we add on top as the head model. We experiment with\nmultiple head types which include:\n(a) Linear. This simply involves passing the base model outputs to a linear layer.\n(b) Multilinear. Two linear layers are added on top of the base model.\n(c) LSTM . We keep the conﬁguration of LSTM layers added on top as ﬂexible as possible\nby varying all its parameters along with other hyperparameters. As such, the head\nmodels can have multiple LSTM layers, can be bidirectional, or both.\n(d) Transformer. A Transformer encoder layer was added above the base model.\n2. Fine-tuning. In this setting, the entire language model is trained along with a single linear\nlayer on top of the model.\n• We report our results for our text-classiﬁcation experiments in terms of the accuracy obtained\non the validation/test set.\n• For POS tagging, we report F1 scores for all our experiments. This is calculated using\nthe seqeval package11. Before feeding the labels and our model’s prediction to the metric\nfunction, we ensure that we only consider the non-padded elements of the inputs.\n8https://universaldependencies.org/u/pos/\n9https://github.com/NirantK/hindi2vec/releases/tag/bbc-hindi-v0.1\n10https://github.com/AI4Bharat/indicnlp_corpus\n11https://github.com/chakki-works/seqeval\n6\n• For QA, we report the F1 score on the validation set. We use the functions from the SQuAD\nv2.0 Evalutaion Script12 to calculate this score.\n5 Results and Analysis\n5.1 Setup A\nCLASSIFICATION(ACC) POS T AGGING(F1) Q UESTIONANSWERING(F1)MODEL HINDI BENGALI TELUGU HINDI BENGALI TELUGU HINDI BENGALI TELUGU\nmDistilBERT(frozen) 74.86 98.28 99.01 95.83 78.45 89.98 17.5 23.76 47.40\nmDistilBERT(ﬁne-tuned) 76.89 98.58 99.13 97.02 85.78 95.67 42.16 54.93 76.08\nmBERT(frozen) 78.60 98.29 99.30 96.60 83.38 95.30 33.32 27.10 47.05\nmBERT(ﬁne-tuned) 78.97 97.79 99.38 97.51 86.45 96.20 56.96 72.79 82.90\nXLM-RoBERTa (frozen) 80.63 98.36 99.59 97.25 87.55 95.34 19.65 33.14 53.40\nXLM-RoBERTa (ﬁne-tuned) 79.20 98.01 99.58 97.98 92.60 97.12 59.70 74.31 81.12\nTable 2: Summary of the results from experiments based on Setup A, where we evaluate multilingual\nTransformer models on downstream tasks under two training settings, which are speciﬁed for each\nmodel variant. The training settings are (a) frozen: we use Transformer models as feature extractors\nand (b) ﬁne-tuned: we ﬁne-tune the entire model. The highlighted metrics show the highest\nperformance across the models that we compare for every language. A similar tabular structure is\nused for subsequent tables in this section.\nA general trend (Table 2) across all three languages is that the results improve as we move from\nmDistilBERT to mBERT and then from mBERT to XLM -RoBERT a. Although the Multilingual DistilBERT\n(mDistilBERT ) for Bengali performs better than XLM -RoBERT a, the improvement is marginal. For Hindi, this\ntrend is most evident. XLM -RoBERT a improves the validation accuracy on mDistilBERT approximately by 7%.\nSuch an increase is less evident for Bengali and Telugu where all model variants achieve high metrics rather\neasily. For each task and each model variant, we further report results in two different training settings: frozen\nand ﬁne-tuned. In the former setting, we ﬁnd that the best performing head model for all experiments (across\nall tasks and languages) is generally a multilayer LSTM or Bi-directional Long Short-Term Memory (BiLSTM )\nwhich is shown speciﬁcally for POS tagging in [4]. While the current norm in NLP literature is to ﬁne-tune\nthe entire language model, the classiﬁcation results show that freezing gives comparable to better results than\nﬁne-tuning.\nThe trend of XLM -RoBERT a outperforming other variants across all languages is apparent when we compare\nPOS tagging results. The performance gain with XLM -RoBERT a against mDistilBERT is more evident for Bengali\n(+8%) and Telugu (+6%). For POS tagging, the best results are achieved consistently with ﬁne-tuning. Moreover,\nﬁne-tuned XLM -RoBERT a performs better by approximately 6% for Bengali when compared to the frozen\nsetting.\nXLM -RoBERT a gives the highest accuracy, except for Telugu(-1.78%), in theQA task as well. The pre-trained\nvariant of XLM -RoBERT a shows a performance gain in Hindi (+2.74%) and Bengali (+1.52%) compared to the\nnext best performing model.\n5.2 Setup B\nCLASSIFICATION(ACC) POS T AGGING(F1) Q UESTIONANSWERING(F1)MODEL HINDI BENGALI TELUGU HINDI BENGALI TELUGU HINDI BENGALI TELUGU\nmBERT(frozen) 78.18 98.08 99.30 97.40 87.97 96.34 15.89 28.10 43.08\nmBERT(ﬁne-tuned) 77.94 98.07 99.63 97.68 91.36 95.85 30.86 68.42 58.36\nTable 3: Summary of results from the experiments based on setup B, where we augment multilingual\nTransformer models with monolingual data for each language. More speciﬁcally, we ﬁne-tunemBERT\nwith 300 MB of monolingual data.\nIn this setup, we augment the multilingual Transformer models by ﬁne-tuning the language model on 300 MB of\nmonolingual data for each language. We did not test this hypothesis on all model variants because the initial\nresults (Table 3) of doing so withmBERT were not encouraging. Augmenting the mBERT with data from a single\n12https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/\ncontents/blob/\n7\nlanguage (Hindi, Bengali, and Telugu in our case) does not lead to massive improvements in performance on\ndownstream tasks, and the results are similar or at best comparable tomBERT . We believe that one of the possible\nreasons for this is that the dataset size with which we ﬁne-tune the model is small and that the performances\nmight improve with larger dataset size. However, for QA, there is a considerable drop in performance with\nthis setup when compared to mBERT . While this drop could be task-dependent, there might be other reasons\nwhich need to be studied and analyzed in much more detail. We plan to conduct a more comprehensive analysis\npertaining to this experimental setup alone in the future, where we would also include other multilingual models\nand vary the dataset sizes to understand these behaviours more concretely.\n5.3 Setup C\nCLASSIFICATION(ACC) POS T AGGING(F1) Q UESTIONANSWERING(F1)MODEL HINDI BENGALI TELUGU HINDI BENGALI TELUGU HINDI BENGALI TELUGU\nDistilBERT(frozen) 79.39 98.14 99.37 97.63 92.09 95.67 13.01 24.59 32.07\nDistilBERT(ﬁne-tuned) 81.83 98.19 99.5 97.93 91.05 96.14 22.03 42.73 55.74\nBERT(frozen) 79.77 98.58 99.42 97.72 89.96 95.43 12.68 19.44 34.61\nBERT(ﬁne-tuned) 81.01 98.45 99.62 98.08 91.14 96.14 18.61 41.12 53.86\nXLM-RoBERTa (frozen) 74.08 98.43 99.47 96.56 88.80 96.31 11.85 27.48 49.16\nXLM-RoBERTa (ﬁne-tuned) 78.75 98.36 99.50 96.73 90.61 95.60 15.93 53.03 70.80\nRoBERTa (frozen) 78.22 97.08 97.86 96.13 84.28 87.15 8.82 20.95 34.61\nRoBERTa (ﬁne-tuned) 76.82 97.72 98.88 96.74 86.85 90.92 24.74 39.13 58.02\nTable 4: Summary of results from experiments based on setup C, where we train four variants of\ncontextual monolingual models for three languages.\nIn setup C, we train monolingual language models from scratch and evaluate their performance on down-\nstream tasks. We observe (Table 4) that our model variants forBERT and DistilBERT perform better than their\nmultilingual counterparts (setup A) across all tasks and languages. While our XLM -RoBERT a variants post\ncompetitive results for Bengali and Telugu, they still fall short of the multilingual model by a small margin.\nXLM -RoBERT a-Hindi performs poorly than expected on classiﬁcation, POS tagging and QA under both the\ntraining settings. We believe this behaviour can be attributed to the fact that we put some constraints on our\ntraining setup. We trained all our model variants uniformly for the same number of epochs. Essentially, we\nconjecture that our XLM -RoBERT a model for Hindi is under-trained and can be trained more to furnish better\nresults on downstream tasks.\nAnother observation that warrants our attention is the relatively poor performance of RoBERT a models across all\ntasks and languages. RoBERT a uses a Byte-Level BPE tokenizer, which is known to have poorer morphological\nalignment with the original text compared to a simple unigram tokenizer, and that this results in a higher\nperformance gap on SQuAD and MNLI tasks [50]. The authors of RoBERT a [12] choose the Byte-Level BPE\ntokenizer for higher coverage and notice no drop in performance when it comes to English. However, dealing\nwith morphologically rich languages, as in our case, clearly seems to impact the performance.\nFine-tuning the model for the QA tasks results in improvements in Setup C as well (more than 11% improvement\nfor Bengali and Telugu). Across all setups, there is a marked performance gain when a model is ﬁne-tuned\nagainst the dataset compared to when only the classiﬁer head is used. This gain is more marked in the case of\nQA task than it is for POS tagging or Text classiﬁcation. Especially apparent from the results for the QA task is\nthat the pre-trained models always perform signiﬁcantly better than the models trained from scratch.\nCLASSIFICATION(ACC) POS T AGGING(F1) Q UESTIONANSWERING(F1)MODEL HINDI BENGALI TELUGU HINDI BENGALI TELUGU HINDI BENGALI TELUGU\nSetup A (frozen) 80.63 98.36 99.59 97.25 87.55 95.34 33.32 33.14 53.40\nSetup A (ﬁne-tuned) 79.20 98.58 99.58 97.98 92.60 97.12 59.70 74.31 82.9\nSetup C (frozen) 79.39 98.58 99.47 97.63 92.09 96.31 13.01 27.48 49.16\nSetup C (ﬁne-tuned)81.83 98.45 99.50 98.08 91.14 96.14 24.74 53.03 70.80\nIndicBERT - 97.14 99.67 - - - - - -\ninltk 78.75 - - - - - - - -\nTyDiQA - - - - - - - 75.4 83.3\nTable 5: Direct comparison between the best results from setup A and setup C and related work. The\nempty cells denote models were either not evaluated on that particular task/language (e.g., IndicBERT\non POS Tagging) or that the model was only evaluated on one speciﬁc task (e.g., TyDiQA).\nFinally, we compare our best results from setup A and setup C with other relevant work in the ﬁeld. The most\nrecent and relevant work related to ours is by [51], who released the IndicBERT model, a multilingual ALBERT\n8\nmodel [52] pre-trained on 11 Indian languages. They, however, do not report results for POS tagging and\nSQuAD-based QA tasks. For Hindi classiﬁcation, we compare our results with inltk [11] that uses a ﬁne-tuned\nULMFiT language model. We compare our results with the baseline results for the TyDi QA Gold Passage task\nfor Bengali and Telugu. We do not have a comparable dataset and baseline for HindiQA tasks and we publish\nour results with our merged dataset. We report state-of-the-art results for Hindi and Bengali classiﬁcation, both\nattained by our language models in setup C. Even for Telugu, the difference between our model and IndicBERT\nis marginal. As for the QA task, our results do not match up to the baselines of TyDi QA Gold Passage task.\nThe baseline scores for TyDi QA were calculated by ﬁne-tuning over the entire dataset, and not on individual\nlanguages. This strongly suggests that the QA tasks beneﬁts a lot from cross-lingual transfer learning.\nSetup A Setup B Setup C\n0\n20\n40\n60\n80\n100\nClassification Accuracy (Te)\nPOS F1 (Te)\nQA F1 (Te)\nClassification Accuracy (Hi)\nPOS F1 (Hi)\nQA F1 (Hi)\nClassification Accuracy (Bn)\nPOS F1 (Bn)\nQA F1 (Bn)\nFigure 1: Relationship between the dataset size that the language model was trained on and its\nperformance on various downstream task. The Y-axis here denotes the generic metric which is F1\nscore in case of POS tagging and QA and accuracy in case of text classiﬁcation.\n6 Conclusion and Future Scope\nIn this work, we have investigated the efﬁcacy of state-of-the-art Transformer language models on languages\nother than English. We trained 4 variants of monolingual contextual language models with different sizes and\ncapabilities for 3 Indian languages that cover more than 60% of the country’s population. We evaluated our\nmodels on three downstream tasks (POS Tagging, text classiﬁcation, and QA) under two distinct training settings\nand across three carefully planned experimental setups. By doing so, we present a highly exhaustive analysis of\nthe performance of Transformer-based models on Indian languages, something which has been lacking in this\nresearch space. Our best models reach or improve the state-of-the-art results across multiple tasks and languages.\nOne of the most important aspects of our analysis is that we directly compare our trained language models\nwith their existing multilingual counterparts. Upon comparison, our results demonstrate that while monolingual\nmodels perform better for some tasks/languages, the improvement attained, if at all, is marginal at best. Our\nanalysis also shows that some variants of Transformer models might be better suited to your needs depending on\nthe available resources, training-dataset size, and downstream task. Furthermore, our experiments also show that\ncompetitive results can be achieved by using Transformer models as feature extractors and training different\nlayers on top (LSTM for best results).\nWe observed that a Byte-Level BPE (in RoBERT a) affects model performance especially in QA tasks. In our\nfollow up work, we aim to explore the impact of tokenizers choice on Indian languages. We notice that the\nTelugu language model tends to perform well in the QA task despite a lower monolingual dataset size (Table 6).\nThis encourages us to explore more on what combination of monolingual dataset size and task level dataset size\nis sufﬁcient to train models with high accuracy on language inference tasks (an area where there is a lot of room\nfor improvement).\nWhile there is still a lot of work to do in this area, we outline a few ideas that we hope to pursue in the\nfuture. Firstly, we need to create larger corpora for under-resourced languages in order to train more powerful\nand efﬁcient language models. This work shows that monolingual models trained from the data currently\navailable perform only marginally better than multilingual models. Secondly, we also establish the need of\ncurating annotated datasets for various downstream tasks like POS Tagging and QA in Indian languages and\nmore importantly, make them easily accessible to the research community. At the same time, we need to create\nuniformly annotated datasets and benchmarks for Indian languages so that new approaches can be compared\n9\neasily. [51] very recently released such a benchmark for Indian languages called the IndicGLUE, which we\nbelieve is a step in the right direction.\nReferences\n[1] Ashish Vaswani et al. “Attention Is All You Need”. In: Advances in Neural Information\nProcessing Systems 30. Ed. by I. Guyon et al. Curran Associates, Inc., 2017.\n[2] Jeremy Howard and Sebastian Ruder. “Universal Language Model Fine-tuning for Text\nClassiﬁcation”. In: arXiv:1801.06146 [cs, stat](May 2018). arXiv: 1801.06146. URL : http:\n//arxiv.org/abs/1801.06146.\n[3] Matthew E. Peters et al. “Deep contextualized word representations”. In:arXiv:1802.05365\n[cs] (Mar. 2018). arXiv: 1802.05365. URL : http://arxiv.org/abs/1802.05365.\n[4] Jacob Devlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding”. In: arXiv:1810.04805 [cs] (May 2019). arXiv: 1810.04805. URL : http:\n//arxiv.org/abs/1810.04805.\n[5] Kevin Clark et al. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\nGenerators. 2020. arXiv: 2003.10555 [cs.CL].\n[6] Zhilin Yang et al. “XLNet: Generalized Autoregressive Pretraining for Language Understand-\ning”. In: arXiv:1906.08237 [cs](Jan. 2020). arXiv: 1906.08237. URL : http://arxiv.org/\nabs/1906.08237.\n[7] Yinhan Liu et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach”. In:\narXiv:1907.11692 [cs](July 2019). arXiv: 1907.11692. URL : http://arxiv.org/abs/\n1907.11692.\n[8] Shijie Wu and Mark Dredze. “Are All Languages Created Equal in Multilingual BERT?” In:\narXiv:2005.09093 [cs](May 2020). arXiv: 2005.09093. URL : http://arxiv.org/abs/\n2005.09093.\n[9] Sebastian Ruder. Why You Should Do NLP Beyond English. http : / / ruder . io / nlp -\nbeyond-english. 2020.\n[10] Ofﬁce of the Registrar General & Census Commissioner India. C-17 POPULATION BY\nBILINGUALISM AND TRILINGUALISM. URL : https://www.censusindia.gov.in/\n2011census/C-17.html.\n[11] Gaurav Arora. iNLTK: Natural Language Toolkit for Indic Languages. 2020. arXiv: 2009.\n12534 [cs.CL].\n[12] Yinhan Liu et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach”. In:\narXiv:1907.11692 [cs](July 2019).\n[13] William Bright. “A matter of typology: Alphasyllabaries and abugidas”. In:Written Language\n& Literacy2.1 (1999), pp. 45–55.\n[14] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Mar. 2006. DOI : 10.\n1162/neco.1997.9.8.1735. URL : https://www.mitpressjournals.org/doix/abs/\n10.1162/neco.1997.9.8.1735.\n[15] Shriya Sahu. “Prashnottar: A Hindi Question Answering System”. In: International Journal of\nComputer Science and Information Technology4.2 (Apr. 2012), pp. 149–158. ISSN : 09754660.\nDOI : 10.5121/ijcsit.2012.4213.\n[16] Somnath Banerjee, Sudip Kumar Naskar, and Sivaji Bandyopadhyay. “BFQA: A Bengali\nFactoid Question Answering System”. In: Text, Speech and Dialogue. Ed. by Petr Sojka\net al. V ol. 8655. Lecture Notes in Computer Science. Springer International Publishing, 2014,\npp. 217–224. ISBN : 978-3-319-10815-5. DOI : 10.1007/978-3-319-10816-2_27 . URL :\nhttp://link.springer.com/10.1007/978-3-319-10816-2_27 .\n[17] Deepak Gupta et al. “MMQA: A Multi-domain Multi-lingual Question-Answering Framework\nfor English and Hindi”. In:Proceedings of the Eleventh International Conference on Language\nResources and Evaluation (LREC 2018). Miyazaki, Japan: European Language Resources\nAssociation (ELRA), May 2018. URL : https://www.aclweb.org/anthology/L18-1440.\n[18] Deepak Gupta, Asif Ekbal, and Pushpak Bhattacharyya. “A Deep Neural Network Framework\nfor English Hindi Question Answering”. In:ACM Trans. Asian Low-Resour. Lang. Inf. Process.\n19.2 (Nov. 2019). ISSN : 2375-4699. DOI : 10.1145/3359988. URL : https://doi.org/10.\n1145/3359988.\n10\n[19] Kyunghyun Cho et al. “Learning Phrase Representations using RNN Encoder-Decoder for\nStatistical Machine Translation”. In: CoRR abs/1406.1078 (2014). arXiv: 1406.1078. URL :\nhttp://arxiv.org/abs/1406.1078.\n[20] Somil Gupta and Nilesh Khade. “BERT Based Multilingual Machine Comprehension in\nEnglish and Hindi”. In: (June 2020).\n[21] Pranav Rajpurkar et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text”.\nIn: arXiv:1606.05250 [cs](Oct. 2016).\n[22] Patrick Lewis et al. “MLQA: Evaluating Cross-lingual Extractive Question Answering”. In:\narXiv:1910.07475 [cs](May 2020). arXiv: 1910.07475. URL : http://arxiv.org/abs/\n1910.07475.\n[23] Jonathan H. Clark et al. “TyDi QA: A Benchmark for Information-Seeking Question Answering\nin Typologically Diverse Languages”. In:arXiv:2003.05002 [cs](Mar. 2020).\n[24] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. “On the Cross-lingual Transferability of\nMonolingual Representations”. In: arXiv:1910.11856 [cs](May 2020). arXiv: 1910.11856.\nURL : http://arxiv.org/abs/1910.11856.\n[25] Jan Hajiˇc, Barbora Hladka, and Petr Pajas. “The Prague Dependency Treebank: Annotation\nStructure and Support.” In: Jan. 2001, pp. 105–114.\n[26] Manish Shrivastava and P. Bhattacharyya. “Hindi POS Tagger Using Naive Stemming : Har-\nnessing Morphological Information Without Extensive Linguistic Knowledge”. In: 2008.\n[27] Pruthwik Mishra, Vandan Mujadia, and Dipti Sharma. “POS Tagging For Resource Poor Indian\nLanguages Through Feature Projection”. In: Feb. 2018.\n[28] Amitava Das. URL : https://amitavadas.com/Code-Mixing.html.\n[29] Prakash B. Pimpale and Raj Nath Patel. “Experiments with POS Tagging Code-mixed Indian\nSocial Media Text”. In:arXiv:1610.09799 [cs](Oct. 2016). arXiv: 1610.09799. URL : http:\n//arxiv.org/abs/1610.09799.\n[30] Sree Harsha Ramesh and Raveena R. Kumar. “A POS Tagger for Code Mixed Indian Social\nMedia Text - ICON-2016 NLP Tools Contest Entry from Surukam”. In: arXiv:1701.00066\n[cs] (Dec. 2016). arXiv: 1701.00066. URL : http://arxiv.org/abs/1701.00066.\n[31] Joakim Nivre et al. Universal Dependencies v2: An Evergrowing Multilingual Treebank\nCollection. 2020. arXiv: 2004.10643 [cs.CL].\n[32] Milan Straka, Jana Straková, and Jan Hajiˇc. “Evaluating Contextualized Embeddings on 54\nLanguages in POS Tagging, Lemmatization and Dependency Parsing”. In:arXiv:1908.07448\n[cs] (Aug. 2019). arXiv: 1908.07448. URL : http://arxiv.org/abs/1908.07448.\n[33] Alan Akbik, Duncan Blythe, and Roland V ollgraf. “Contextual String Embeddings for Se-\nquence Labeling”. In: Proceedings of the 27th International Conference on Computational\nLinguistics. Santa Fe, New Mexico, USA: Association for Computational Linguistics, Aug.\n2018, pp. 1638–1649. URL : https://www.aclweb.org/anthology/C18-1139.\n[34] Telmo Pires, Eva Schlinger, and Dan Garrette. “How Multilingual is Multilingual BERT?” In:\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\nAssociation for Computational Linguistics, 2019, pp. 4996–5001. DOI : 10.18653/v1/P19-\n1493. URL : https://www.aclweb.org/anthology/P19-1493.\n[35] Anoop Kunchukuttan et al. “AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word\nEmbeddings for Indic Languages”. In: arXiv preprint arXiv:2005.00085(2020).\n[36] Piotr Bojanowski et al. “Enriching Word Vectors with Subword Information”. In: CoRR\nabs/1607.04606 (2016). arXiv: 1607.04606. URL : http://arxiv.org/abs/1607.04606.\n[37] Braja Gopal Patra, Dipankar Das, and Amitava Das. “Sentiment Analysis of Code-Mixed\nIndian Languages: An Overview of SAIL_CodeMixed Shared Task @ICON-2017”. In:\narXiv:1803.06745 [cs](Mar. 2018). arXiv: 1803.06745. URL : http://arxiv.org/abs/\n1803.06745.\n[38] Ramchandra Joshi, Purvi Goel, and Raviraj Joshi. “Deep Learning for Hindi Text Classiﬁcation:\nA Comparison”. In: arXiv:2001.10340 [cs, stat]11886 (2020). arXiv: 2001.10340, pp. 94–101.\nDOI : 10.1007/978-3-030-44689-5_9 .\n[39] Chan, Branden et al. Deepset - Open Sourcing German BERT. https://deepset.ai/german-bert.\n[40] Victor Sanh et al. “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and\nLighter”. In: arXiv:1910.01108 [cs](Feb. 2020).\n11\n[41] Louis Martin et al. “CamemBERT: A Tasty French Language Model”. In:Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics(2020).\n[42] Alexis Conneau et al. “Unsupervised Cross-Lingual Representation Learning at Scale”. In:\narXiv:1911.02116 [cs](Apr. 2020).\n[43] Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for Neural Text Processing. 2018. arXiv: 1808.06226 [cs.CL].\n[44] Thomas Wolf et al. “HuggingFace’s Transformers: State-of-the-art Natural Language Process-\ning”. In: ArXiv (2019), arXiv–1910.\n[45] Omkar N Koul. Modern Hindi Grammar. Dunwoody Press Springﬁeld, USA, 2008.\n[46] Hanne-Ruth Thompson. Bengali. V ol. 18. John Benjamins Publishing, 2012.\n[47] Bhadriraju Krishnamurti. The dravidian languages. Cambridge University Press, 2003.\n[48] Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. “A Monolingual Approach to\nContextualized Word Embeddings for Mid-Resource Languages”. In:Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics. Online: Association for\nComputational Linguistics, July 2020, pp. 1703–1714. URL : https://www.aclweb.org/\nanthology/2020.acl-main.156.\n[49] Patrick Lewis et al. “MLQA: Evaluating Cross-lingual Extractive Question Answering”. In:\narXiv preprint arXiv:1910.07475(2019).\n[50] Kaj Bostrom and Greg Durrett. Byte Pair Encoding is Suboptimal for Language Model\nPretraining. 2020. arXiv: 2004.03720 [cs.CL].\n[51] Divyanshu Kakwani et al. “IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and\nPre-trained Multilingual Language Models for Indian Languages”. In: Findings of EMNLP.\n2020.\n[52] Zhenzhong Lan et al. ALBERT: A Lite BERT for Self-supervised Learning of Language\nRepresentations. 2020. arXiv: 1909.11942 [cs.CL].\n[53] Lukas Biewald. Experiment Tracking with Weights and Biases. Software available from\nwandb.com. 2020. URL : https://www.wandb.com/.\n12\n7 Appendix\nA Training details and metrics for Language Modeling\nThe techniques and setups that we have used for our different language models vary depending on the setup.\nIn our future work we plan to cover in the missing combination of dataset sizes and model architectures for\ncompleteness.\nS ETUP B S ETUP C\nM ODEL\nH INDI B ENGALI T ELUGU H INDI B ENGALI T ELUGU\nDistil BERT 300 MB 300 MB 300 M 10 GB 6 GB 2 GB\nBERT 300 MB 300 MB 300 M 3 GB 3.1 GB 1.6 GB\nR oBERT a - - - 10 GB 6 GB 2 GB\nXLM -R oBERT a 300 MB 300 MB 300 MB 3 GB 3.1 GB 1.6 GB\nTable 6: A comparison of the dataset sizes used for training different language models\nB Training metrics for POS Tagging\nIn this section we show some of the plots for our experiments for each language. These plots clearly show the\ndifferent head layers that we train as part of our training setting described in the paper. We used Weights and\nBiases [53] to run extensive hyperparameter search for our models. The plots in this section are for DistilBERT\nin setup C.\nB.1 Telugu\nB.2 Bengali\n13\nB.3 Hindi\nC Training metrics for Text Classiﬁcation\nThis section shows similar graphs for text classiﬁcation. Here we have taken plots for mDistilBERT from setup A\nto demonstrate our experimental setups.\nC.1 Telugu\nC.2 Bengali\nC.3 Hindi\n14",
  "topic": "Telugu",
  "concepts": [
    {
      "name": "Telugu",
      "score": 0.7945520877838135
    },
    {
      "name": "Computer science",
      "score": 0.7907748222351074
    },
    {
      "name": "Transformer",
      "score": 0.7442718744277954
    },
    {
      "name": "Bengali",
      "score": 0.7430068850517273
    },
    {
      "name": "Natural language processing",
      "score": 0.6867979168891907
    },
    {
      "name": "Language model",
      "score": 0.6866629123687744
    },
    {
      "name": "Hindi",
      "score": 0.6517166495323181
    },
    {
      "name": "Artificial intelligence",
      "score": 0.601677656173706
    },
    {
      "name": "German",
      "score": 0.4481724798679352
    },
    {
      "name": "Security token",
      "score": 0.44207826256752014
    },
    {
      "name": "Linguistics",
      "score": 0.28307482600212097
    },
    {
      "name": "Engineering",
      "score": 0.10315930843353271
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}