{
  "title": "Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition",
  "url": "https://openalex.org/W3181555446",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287231652",
      "name": "Aouayeb, Mouath",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744803961",
      "name": "Hamidouche, Wassim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287231654",
      "name": "Soladie, Catherine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A52995560",
      "name": "Kpalma Kidiyo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224176360",
      "name": "Séguier, Renaud",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3023130389",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2106115875",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2103943262",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W1977005561",
    "https://openalex.org/W2738672149",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2481681431",
    "https://openalex.org/W2243226955",
    "https://openalex.org/W2746314669",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3157999215",
    "https://openalex.org/W2745497104",
    "https://openalex.org/W2963712289",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2014185685",
    "https://openalex.org/W2150283722",
    "https://openalex.org/W3046768359",
    "https://openalex.org/W2168341643",
    "https://openalex.org/W2251198138",
    "https://openalex.org/W3118530108",
    "https://openalex.org/W3105536522",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2593367875",
    "https://openalex.org/W3091535944",
    "https://openalex.org/W2125434646",
    "https://openalex.org/W2083021723",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3003720578",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3141497777",
    "https://openalex.org/W3136617080",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2963085784",
    "https://openalex.org/W3138109342",
    "https://openalex.org/W3035253074"
  ],
  "abstract": "As various databases of facial expressions have been made accessible over the last few decades, the Facial Expression Recognition (FER) task has gotten a lot of interest. The multiple sources of the available databases raised several challenges for facial recognition task. These challenges are usually addressed by Convolution Neural Network (CNN) architectures. Different from CNN models, a Transformer model based on attention mechanism has been presented recently to address vision tasks. One of the major issue with Transformers is the need of a large data for training, while most FER databases are limited compared to other vision applications. Therefore, we propose in this paper to learn a vision Transformer jointly with a Squeeze and Excitation (SE) block for FER task. The proposed method is evaluated on different publicly available FER databases including CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model outperforms state-of-the-art methods on CK+ and SFEW and achieves competitive results on JAFFE and RAF-DB.",
  "full_text": "LEARNING VISION TRANSFORMER WITH SQUEEZE AND\nEXCITATION FOR FACIAL EXPRESSION RECOGNITION\nMouath Aouayeb\nUniv. Rennes, INSA Rennes, CentraleSupélec,\nCNRS, IETR - UMR 6164,\nRennes, France\naouayeb.mouath@insa-rennes.fr\nWassim Hamidouche\nUniv. Rennes, INSA Rennes,\nCNRS, IETR - UMR 6164,\nRennes, France\nwassim.hamidouche@insa-rennes.fr\nCatherine Soladie\nUniv. Rennes, CentraleSupélec,\nCNRS, IETR - UMR 6164,\nRennes, France\ncatherine.soladie@centralesupelec.fr\nKidiyo Kpalma\nUniv. Rennes, INSA Rennes,\nCNRS, IETR - UMR 6164,\nRennes, France\nkidiyo.kpalma@insa-rennes.fr\nRenaud Seguier\nUniv. Rennes, CentraleSupélec,\nCNRS, IETR - UMR 6164,\nRennes, France\nrenaud.seguier@centralesupelec.fr\nABSTRACT\nAs various databases of facial expressions have been made accessible over the last few decades, the\nFacial Expression Recognition (FER) task has gotten a lot of interest. The multiple sources of the\navailable databases raised several challenges for facial recognition task. These challenges are usually\naddressed by Convolution Neural Network (CNN) architectures. Different from CNN models, a\nTransformer model based on attention mechanism has been presented recently to address vision tasks.\nOne of the major issue with Transformers is the need of a large data for training, while most FER\ndatabases are limited compared to other vision applications. Therefore, we propose in this paper\nto learn a vision Transformer jointly with a Squeeze and Excitation (SE) block for FER task. The\nproposed method is evaluated on different publicly available FER databases including CK+, JAFFE,\nRAF-DB and SFEW. Experiments demonstrate that our model outperforms state-of-the-art methods\non CK+ and SFEW and achieves competitive results on JAFFE and RAF-DB.\nKeywords ViT ·Squeeze and Excitation ·Facial Expressions Recognition\n1 Introduction\nYear after year, human life is increasingly intertwined with Artiﬁcial Intelligence (AI)-based systems. As a result, there\nis a growing attention in technologies that can understand and interact with humans, or that can provide improved\ncontact between humans. To that end, more researchers are involved in developing automated FER methods that can be\nsummarised in three categories including Handcrafted, Deep Learning and Hybrid. Main handcrafted solutions [1, 2, 3]\nare based on techniques like local binary pattern (LBP), Histogram of Oriented Gradients (HOG) and Optical Flow (OF).\nThey present good results on lab-made databases (CK+ [4] and JAFFE [5]), in contrast, they perform modestly on wild\ndatabases (SFEW [6] and RAF-DB [7]). Some researchers [8, 9, 10] have taken advantage of advancements in deep\nlearning techniques, especially in CNN architectures, to outperform previous hand-crafted solutions. Others [11, 12]\narXiv:2107.03107v4  [cs.CV]  16 Jul 2021\nLearning ViT with SE for FER\npropose solutions that mix the handcrafted techniques with deep learning techniques to address speciﬁc challenges in\nFER.\nImpressive results [13, 14, 15] from Transformer models on Natural Language Processing (NLP) tasks have motivated\nvision community to study the application of Transformers to computer vision problems. The idea is to represent an\nimage as a sequence of patches in analogy of a sequence of words in a sentence in NLP domain. Transformers are made\nto learn parallel relation between sequence inputs through an attention mechanism which makes them theoretically\nsuitable for both tasks NLP and image processing. The Transformer was ﬁrstly introduced by Vaswani et al.[13] as a\nmachine translation model, and then multiple variants [13, 14, 15] were proposed to increase the model accuracy and\novercome various NLP challenges. Recently, a Vision Transformer (ViT) is presented for different computer vision\ntasks from image classiﬁcation [16], object detection [17] to image data generation [18]. The Transformer proves its\ncapability and overcomes state-of-the-art performance in different NLP applications as well as in vision applications.\nHowever, these attention-based architectures are computationally more demanding than CNNs and training data hunger.\nIn this paper, we propose to alleviate the problem, that ViT has, caused by the lack of training data for FER with a block\nof SE. We also provide an internal representations analysis of the ViT on facial expressions. The contribution of this\npaper can be summarized in four-folds:\n• Introduction of a SE block to optimize the learning of the ViT.\n• Fine-tuning of the ViT on FER-2013 [19] database for FER task.\n• Test of the model on four different databases (CK+ [4], JAFFE [5], RAF-DB [7], SFEW [6]).\n• Analysis of the attention mechanism of the ViT and the effect of the SE block.\nThe remaining of this paper is organized as follows. Section 2 reviews the related work. Section 3 ﬁrstly gives an\noverview of the proposed method and then describes the details of the ViT and the SE block. Section 4 presents the\nexperimental results. Finally, Section 5 concludes the paper.\n2 Related Works\nIn this section, we brieﬂy review some related works on ViT and facial expression recognition solutions.\n2.1 Vision Transformer (ViT)\nThe ViT is ﬁrst proposed by Dosovitskiy et al.[16] for image classiﬁcation. The main part of the model is the encoder\npart of the Transformer as ﬁrst introduced for machine translation by Vaswani et al.[13]. To transform the images\ninto a sequence of patches they use a linear projection, and for the classiﬁcation, they use only the token class vector.\nThe model achieves state-of-the-art performance on ImageNet [20] classiﬁcation using ﬁne-tuning on JFT-300M [21].\nFrom that and the fact that this model contains much more parameters (about 100M) than CNNs, we can say that ViT\nare data-hungry models. To address this heavily relying on large-scale databases, Touvronet al.[22] proposed DEIT\nmodel. It’s a ViT with two classiﬁcation tokens. The ﬁrst one is fed to an Multiple Layer Perceptron (MLP) head\nfor the classiﬁcation and the other one is used on the distillation process with a CNN teacher model pretrained on\nImageNet [20]. The DEIT was only trained on ImageNet and outperforms both the ViT model and the teacher model.\nYuan et al.[23] overcome the same limitation of ViT using novel tokenization process. The proposed T2T-ViT [23]\nmodel has two modules: 1) the T2T tokenization module that consists in two steps: re-structurization and soft split, to\nmodel the local information and reduce the length of tokens progressively, and 2) the Transformer encoder module. It\nachieves state-of-the-art performance on ImageNet [20] classiﬁcation without a pretraining on JFT-300M [21].\n2.2 Facial Expression Recognition\nThe FER task has progressed from handcrafted [1, 2, 3] solutions to deep learning [8, 24, 10, 25] and Hybrid [11, 12, 26]\nsolutions. In 2014, Turan et al.[2] proposed a region-based handcrafted system for FER. They extracted features from\nthe eye and mouth regions using Local Phase Quantization (LPQ) and Pyramid of Histogram of Oriented Gradients\n(PHOG). A Principal Component Analysis (PCA) is used as a tool for features selection. They fused the two groups of\nfeatures with a Canonical Correlation Analysis (CCA) and ﬁnally, a Support Vector Machine (SVM) is applied as a\nclassiﬁer. More recent work [3], proposed an automatic FER system based on LBP and HOG as features extractor. A\nlocal linear embedding technique is used to reduce features dimensionality and a SVM for the classiﬁcation part. They\nreached state-of-the-art performance for handcrafted solutions on JAFFE [5], KDEF [27] and RafD [28]. Recently,\nmore challenging and rich data have been made publicly available and with the progress of deep learning architectures,\nmany deep learning solutions based on CNN models are revealed. Otberdout et al.[24] proposed to use Symmetric\n2\nLearning ViT with SE for FER\nPositive Deﬁnite (SPC) to replace the fully connected layer in CNN architecture for facial expression classiﬁcation.\nWang et al. [25] proposed a region-based solution with a CNN model with two blocks of attention. They perform\ndifferent crop of the same image and apply a CNN on each patch. A self-attention module is then applied followed by a\nrelation attention module. On the self-attention block, they use a loss function in a way that one of the cropped image\nmay have a weight larger than the weight given to the input image. More recently, Farzaneh et al.[10] have integrated\nan attention block to estimate the weights of features with a sparse center loss to achieve intra-class compactness and\ninter-class separation. Deep learning based solutions have widely outperformed handcrafted solutions especially on\nwild databases like RAF-DB [7], SFEW[6], AffectNet [29] and others.\nOther researchers have though about combining deep learning techniques with handcrafted techniques into a hybrid\nsystem. Levi et al.[11] proposed to apply CNN on the image, its LBP and the mapped LBP to a 3D space using Multi\nDimensional Scaling (MDS). Xu et al.[12] proposed to fuse CNN features with LBP features and they used PCA as\nfeatures selector. Newly, many Transformer models have been introduced for different computer vision tasks and in that\ncontext Ma et al.[26] proposed a convolutional vision Transformer. They extract features from the input image as well\nas form its LBP using a ResNet18. Then, they fuse the extracted features with an attentional selective fusion module\nand fed the output to a Transformer encoder with a MLP head to perform the classiﬁcation. To our knowledge, [26]\nis considered as the ﬁrst solution based on Transformer architecture for FER. However, our proposed solution differs\nin applying the Transformer encoder directly on the image and not on the extracted features which may reduce the\ncomplexity of the proposed system and aid to study and analyse the application of ViT on FER problem as one of the\ninteresting vision tasks.\nTable 8 (presented in the Supplementary Material) summarizes some state-of-the-art approaches with details on the used\narchitecture and databases. We can notice that different databases are used to address different issues and challenges.\nFrom these databases we selected 4 of them to study our proposed solution and compare it with state-of-the-art works.\nThe selected databases are described in the experiments and comparison Section 4. In the next section we will describe\nour proposed solution.\n3 Proposed Method\nIn this section, we introduce the proposed solution in three separate paragraphs: an overview, then some details of the\nViT architecture and the attention mechanism, and ﬁnally the SE block.\n3.1 Architecture overview\nThe proposed solution contains two main parts, a vision Transformer to extract local attention features and a SE block to\nextract global relation from the extracted features which may optimize the learning process on small facial expressions\ndatabases.\n3.2 Vision Transformer\nThe vision Transformer consists of two steps: the tokenization and the Transformer encoder. In the tokenization step,\nthe image is cropped onto Lequal (h×h) dimension patches and then ﬂattened to a vector. An extra learnable vector\nis added as a token for classiﬁcation called \"cls_tkn\". Each vector is marked with a position value. To summarize, the\ninput of the Transformer encoder is L+ 1vectors of length h2 + 1.\nAs shown in Figure 1, the Transformer encoder is a sequence of N blocks of the attention module. The main part of\nthe attention block is the Multi-Head Attention (MHA). The MHA is build with zheads of self-Attention, also called\nintra-attention. According to [13], the idea of the self-attention is to relate different positions of a single sequence in\norder to compute a representation of the sequence. For a given sequence, 3 layers are used: Q-layer, K-layer and V-layer\nand the self-attention function will be a mapping of a query (Q or Q-layer) and a set of key-value (K or K-layer; V or\nV-layer) pairs to an output. The self-attention function is summarized by Equation (1):\nAttention(Q,K,V ) =softmax( QKT\n√dk\n)V. (1)\nAnd so the MHA Equation (2) will be:\nMHA(Q,K,V ) =Concat(head0,...,head z)WO,\nheadi = Attention(QWQ\ni ,KW K\ni ,VW V\ni ).\n(2)\nwhere the projections WO,WQ\ni ,WK\ni and WV\ni are parameters’ matrices.\n3\nLearning ViT with SE for FER\nFigure 1: Overview of the proposed solution. The used ViT is the base version with 14 layers of Transformer\nencoder and patch dimension of (16 ×16). The ViT is already trained on JFT-300M [21] database and ﬁne-tuned to\nImageNet-1K [20] database.\n3.3 Squeeze and Excitation (SE)\nThe Squeeze and Excitation block, shown on the right of the Figure 1, is also an attention mechanism. It contains\nwidely fewer parameters than self-attention block as shown by Equation (3) where two fully connected layers are used\nwith only one operation of pointwise multiplication. It is ﬁrstly introduced in [30] to optimize CNN architecture as a\nchannel-wise attention module, concretely we use only the excitation part since the squeeze part is a pooling layer build\nto reduce the dimension of the 2d-CNN layers.\nSE(cls_tkn) =cls_tkn⊙Excitaion(cls_tkn),\nExcitaion(cls_tkn) =Sigmoid(FCLγ(ReLU(FCLγ/4(cls_tkn)))). (3)\nwhere FCLγ and FCLγ/4 are fully connected layers with respectively γneurons and γ/4 neurons, γis the length of\nthe cls_tkn which is the classiﬁcation token vector and ⊙is a pointwise multiplication. The idea of using SE in our\narchitecture is to optimize the learning of the ViT by learning more global attention relations between extracted local\nattention features. Thus, the SE is introduced on top of the Transformer encoder more precisely on the classiﬁcation\ntoken vector. Different from the self-attention block where it is used inside the Transformer encoder to encode the\ninput sequence and extract features through cls_tkn, the SE is applied to recalibrate the feature responses by explicitly\nmodelling inter-dependencies among cls_tkn channels.\n4 Experiments and Comparison\nIn this section, we ﬁrst describe the used databases, and then provide an ablation study for different contributions with\nother details on the proposed solution and an analysis of additional visualisation for in-depth understanding of the ViT\napplied on FER task. Finally, we present a comparison with state-of-the-art works.\n4.1 FER Databases\nCK+ [4] : published on 2010, and it is an extended version of Cohne-Kanade (CK) database. It contains 593 sequences\ntaken in lab environment with two data formats (640 ×490) and (640 ×480). It encompasses the 7 basic expressions\nwhich are : Angry, Disgust, Fear, Happy, Neutral, sad and Surprise, plus the Contempt expression. In our case, we only\nworked on the 7 basic expressions to have a fair study with other databases and with most state-of-the-art solutions.\nJAFFE [5]: The Japanese Female Facial Expression (JAFFE) database is a 213 gray scale images of acted Japanese\nfemale facial expressions. All the images are resized onto (256 ×256). It contains the 7 basic expressions.\nFER-2013 [19]: The FER-2013 database, or sometimes referred as FERPlus, is almost 35k facial expressions database\n4\nLearning ViT with SE for FER\non 7 basic expressions. Published in 2013 in a challenge on Kaggle plate-form1. The images are collected from the web\nconverted to gray scale model and resized to (48 ×48). Theoretically, this database could suffer from mislabeling since\na 68% ±5% human accuracy is reported. However, since it is a large spontaneous databases of facial expressions we\nused it as a pre-training data for our model.\nSFEW [6]: The Static Facial Expression in the Wild (SFEW) is a very challenging databases with images captured\nfrom different movies. It contains 1,766 RGB images with size of (720 ×576). It is also labeled with the 7 basic\nexpressions.\nRAF-DB [7]: The Real-world Affective Faces Database (RAF-DB) is a recent database with nearly 30K of mixed RGB\nand gray scale images collected from different internet websites. This database contains two separate sub-data: one\nwith 7 basic expressions and the other with 12 compound facial expressions. In the experiments, we used the 7 basic\nexpressions version.\nTable 7 (presented in the Supplementary Material) summarizes previous presented databases with reference to the year\nand the publication conference and some other details. For FER task there are other publicly available databases that\naddress different issues, but we restrained our choices on these databases because they are in the center of interest of\nmajor state-of-the-art solutions.\n4.2 Architecture and training parameters\nIn all experiments, we use a pretrained ViT-B16-224 (weights 2), the base version of the ViT with (16 ×16) patch\nsize and (224 ×224) input image size. Since ViT training needs large data to reach good performance we used\nthe following list of data augmentation: Random Horizontal ﬂip, Random GrayScale conversion, different values of\nbrightness, contrast and saturation. All images are converted to 3 channels, resized to (224 ×224) and normalized. The\nregularisation methods we used in this work are Cutout [31] and Mixup [32]. The training is performed with categorical\ncross entropy as a loss function and AdamW [33] as an optimizer. The learning rate is ﬁxed to 1.6 ×10−4 with a batch\nsize of 16. When training on FER-2013 database, the number of epochs is ﬁxed to 8 and for the rest of databases it is\nﬁxed to 10. The training process is carried-out on a Tesla K80 TPU with 8 cores using Pytorch1.7.\n4.3 Ablation Study\nIn the ablation study, we assess the performance of the ViT architecture, the added SE block and the use of FER-\n2013 [19] as a pre-training data. Table 1 shows the result of different experiments on CK+, JAFFE, RAF-DB and SFEW.\nFrom ﬁrst line, we can notice that ViT can reach state-of-the-art performance on lab-made databases like CK+ [ 4]\nTable 1: Ablation Study\nModel Pre-train CK+ [4] JAFFE [5] RAF-DB [7] SFEW [6]\nViT %* 0.9857 0.8823 0.8595 0.3828\nViT + SE %* 0.9949 0.9061 0.8618 0.4084\nViT FER-2013[19]* 0.9817 0.9483 0.8703 0.5035\nViT + SE FER-2013[19]* 0.9980 0.9292 0.8722 0.5429\n* The used ViT model is already trained on ImageNet [20].\nand JAFFE [5], however on SFEW [ 6] the Transformer is less effective. In all cases, we can notice that there is a\nbeneﬁt of using SE and the pre-training phase on FER-2013 [19]. The two contributions may not be complementary on\nlab-made data (CK++ [4] and JAFFE [5]). For example, on CK++ [4] we can notice that the pre-training improves the\nperformance only when combined with the SE. On JAFFE [5], the best solution is the one that relies on pre-training\nwithout the SE. Although, on wild databases (RAF-DB [7] and SFEW [6]) the added value of both contributions is\nmore noticeable, specially on SFEW [ 6] we can obtain a 16% gain on accuracy compared to the ViT without a SE\nneither a pre-training on FER-2013 [19].\nThe confusion matrices of the proposed ViT+SE pre-trained on FER-2013 are reported in Figure 2, the left plot is\nfor the validation set of RAF-DB [7] and the right plot is for the validation set of SFEW [6]. The Happy and Neutral\nexpressions are the best recognized on the SFEW [6] database with respectively an accuracy of 69% and 57%. For\nRAF-DB [7], the Happy expression has the best accuracy with 93% followed by the Sad expression with 89% accuracy.\n1https://www.kaggle.com/msambare/FER-2013\n2https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n5\nLearning ViT with SE for FER\nFigure 2: Confusion matrices of ViT+SE on the validation set of RAF-DB (left) and the validation set of SFEW (right).\nFigure 3: t-SNE plots corresponding to the 768-dimensional features from the ViT, ViT+SE before and after the SE\nblock and the 512-dimensional features from the ResNet50. The features correspond to the RAF-DB images. The\naccuracy of ResNet50, ViT and ViT+SE on RAF-DB are respectively: 0.8061, 0.8595 and 0.8618.\nOn the two confusion matrices, we can notice that our model confront difﬁculties in recognizing the Fear expression,\nand that may be due to the less amount of data provided for that expression compared to the rest of expressions.\n4.4 Transformer visualisation and analysis\nIn this section, we have conducted a various set of experiments in RAF-DB database. Specially, we evaluate the\nclassiﬁcation outputs of the model through t-SNE and we provide a visual analysis of the ViT model performance with\nthe SE in comparison with CNN.\nFigure 3 shows the t-SNE of the extracted features form the ViT model without SE, the features of the ViT + SE after\nthe SE block and before SE, and compared with t-SNE of ResNet50 [34] features trained also on RAF-DB. Based on\nt-SNE, the ViT architectures enable better separation of classes compared to CNN base-line architecture (ResNet50).\n6\nLearning ViT with SE for FER\nFigure 4: GRAD-CAM, Score-CAM, Eigen-CAM maps of the last layer before the classiﬁcation block for the Happy\nexpression (image from the validation set of RAF-DB [7]).\nIn addition, the SE block enhances ViT model robustness, as the intra-distances between clusters are maximized.\nInterestingly, the features before the SE form a more compact clusters with inter-distance lower than the features after\nthe SE, which may interpret the features before SE are more robust than those after the SE. However, we tried to use\nthe before SE features directly in the classiﬁcation task and no performance gain has been reported. Figure 4 shows\ndifferent maps of attention of the ViT, the ViT+SE and the ResNet50, using Grad-Cam [ 35], Score-Cam [36] and\nEigen-Cam [37] tools. This visualisation shows that ViT architectures succeed to focus more locally which conﬁrm the\ninterest of using the self-attention blocks for computer vision tasks. Once again, we can notice the gain of using the SE\nblock with different tools but mostly using Eigen-CAM [37].\nOther investigations of the ViT architecture are presented in the Supplementary Material (Figure 5) that shows the\nevolution of the attention form ﬁrst attention block to a deeper attention blocks and we can notice that the focus of\nthe ViT goes from global attention to more local attention. This particular behaviour of the ViT on FER task is the\nmotivation of using SE block on top of it to build a calibrated relation between different local focuses. In Figure 6\n(Supplementary Material), we show the focus of the ViT compared to the ViT + SE for different facial expressions and\nit shows how the SE can rectify the local attention feature extracted with the ViT, by searching for a global attention\nrelations.\n4.5 Comparison with state-of-the-art\nIn this paper, we compare our proposed model ViT+SE pre-trained on FER-2013 [19] database with state-of-the-art\nsolution on 2 lab-made databases (CK+ [4] and JAFFE [5]) and 2 wild databases (RAF-DB [7] and SFEW [6]). Table 2\nshows that we have the highest accuracy on CK+ [4] with a 99.80% using a 10-fold cross-validation protocol. Table 5\nshows that we set the new state-of-the-art performance for single models on SFEW [6] with 54.29% accuracy, however a\nhigher accuracy (56.4%) is reported in [25] using ensemble models. Furthermore, in Table 3 the proposed solution have\na good 10-fold cross validation accuracy on JAFFE [5] with 92.92%. To our knowledge, it is the highest performance\nwith a deep learning based solution but still less by almost 3% than the highest obtained accuracy with newly handcrafted\nproposed solution [3]. Table 4 shows that our solution has a good result on RAF-DB [7] with an accuracy of 87.22%, to\nposition as the third best solution among state-of-the-art on this database, less than the best record by nearly 3%.\nTable 2: Comparison on CK+ [ 4] with 10-fold cross\nvalidation.\nRef. Model Type Accuracy\n[2] 2014 Handcrafted 0.9503\n[9] 2020 Deep Learning 0.9759\n[38] 2021 Deep Learning 0.9800\nViT + SE Deep Learning 0.9980\nTable 3: Comparison on JAFFE [5] with 10-fold cross\nvalidation.\nRef. Model Type Accuracy\n[39] 2015 Handcrafted 0.9180\n[3] 2020 Handcrafted 0.9600\n[38] 2021 Deep Learning 0.9280\nViT + SE Deep Learning 0.9292\n7\nLearning ViT with SE for FER\nTable 4: Comparison on the validation set of RAF-\nDB [7]\nRef. Model Type Accuracy\n[25] 2020 Deep Learning 0.8690\n[26] 2021 Hybrid 0.8814\n[40] 2021 Deep Learning 0.9055\nViT + SE Deep Learning 0.8722\nTable 5: Comparison on the validation set of SFEW [6]\nRef. Model Type Accuracy\n[24] 2018 Deep Learning 0.4918\n[41] 2018 Deep Learning 0.5252\n[25] 2020 Deep Learning 0.5419\nViT + SE Deep Learning 0.5429\n5 Conclusion\nIn this work, we introduced the ViT+SE, a simple scheme that optimize the learning of the ViT by an attention block\ncalled Squeeze and Excitation. It performs impressively well for improving the performance of ViT in FER task.\nFurthermore, it also improves the robustness of the model as shown in the t-SNE representation of the extracted features\nand in the attention maps. We have presented the classiﬁcation performance on lab-made databases (CK+ and JAFFE)\nand wild databases (RAF-DB and SFEW) to evaluate the gain of the SE block and the use of FER-2013 as a pre-training\ndatabase. By comparing to different state-of-the-art solutions, we have shown that our proposed solution achieves the\nhighest performance with a single model on CK+ and SFEW, and competitive results on JAFFE and RAF-DB. As\nfuture work, we aim to extend the ViT architecture to address the temporal aspect for a more competitive task like\nmicro-expressions recognition.\nReferences\n[1] Y . Yacoob and L.S. Davis. Recognizing human facial expressions from long image sequences using optical ﬂow.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 18(6):636–642, 1996.\n[2] Cigdem Turan and Kin-Man Lam. Region-based feature fusion for facial-expression recognition. In 2014 IEEE\nInternational Conference on Image Processing (ICIP), pages 5966–5970, 2014.\n[3] Y . Yaddaden, M. Adda, and A. Bouzouane. Facial expression recognition using locally linear embedding with lbp\nand hog descriptors. In 2020 2nd International Workshop on Human-Centric Smart Environments for Health and\nWell-being (IHSH), pages 221–226, 2021.\n[4] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews. The extended cohn-kanade dataset\n(ck+): A complete dataset for action unit and emotion-speciﬁed expression. In 2010 IEEE Computer Society\nConference on Computer Vision and Pattern Recognition - Workshops, pages 94–101, 2010.\n[5] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba. Coding facial expressions with gabor wavelets. InProceedings\nThird IEEE International Conference on Automatic Face and Gesture Recognition, pages 200–205, 1998.\n[6] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static facial expression analysis in tough conditions: Data,\nevaluation protocol and benchmark. In 2011 IEEE International Conference on Computer Vision Workshops\n(ICCV Workshops), pages 2106–2112, 2011.\n[7] Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-preserving learning for\nexpression recognition in the wild. In 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 2584–2593. IEEE, 2017.\n[8] J. Zhou, X. Zhang, Y . Liu, and X. Lan. Facial expression recognition using spatial-temporal semantic graph\nnetwork. In 2020 IEEE International Conference on Image Processing (ICIP), pages 1961–1965, 2020.\n[9] Zijun Cui, Tengfei Song, Yuru Wang, and Qiang Ji. Knowledge augmented deep neural networks for joint facial\nexpression and action unit recognition. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin,\neditors, Advances in Neural Information Processing Systems, volume 33, pages 14338–14349. Curran Associates,\nInc., 2020.\n[10] Amir Hossein Farzaneh and Xiaojun Qi. Facial expression recognition in the wild via deep attentive center loss. In\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2402–2411,\nJanuary 2021.\n[11] Gil Levi and Tal Hassner. Emotion recognition in the wild via convolutional neural networks and mapped binary\npatterns. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, ICMI ’15,\npage 503–510, New York, NY , USA, 2015. Association for Computing Machinery.\n8\nLearning ViT with SE for FER\n[12] Q. Xu and N. Zhao. A facial expression recognition algorithm based on cnn and lbp feature. In 2020 IEEE 4th\nInformation Technology, Networking, Electronic and Automation Control Conference (ITNEC), volume 1, pages\n2304–2308, 2020.\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran\nAssociates, Inc., 2017.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[15] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. cite\narxiv:1907.11692.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929,\n2020.\n[17] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In European Conference on Computer Vision, pages 213–229.\nSpringer, 2020.\n[18] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two transformers can make one strong gan. arXiv\npreprint arXiv:2102.07074, 2021.\n[19] Pierre-Luc Carrier, Aaron Courville, Ian J Goodfellow, Medhi Mirza, and Yoshua Bengio. Fer-2013 face database.\nUniversit de Montral, 2013.\n[20] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009.\n[21] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of\ndata in deep learning era. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 843–852,\n2017.\n[22] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.\nTraining data-efﬁcient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877,\n2020.\n[23] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan.\nTokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986,\n2021.\n[24] Naima Otberdout, Anis Kacem, M. Daoudi, Lahoucine Ballihi, and S. Berretti. Deep covariance descriptors for\nfacial expression recognition. In BMVC, 2018.\n[25] K. Wang, Xiaojiang Peng, Jianfei Yang, Debin Meng, and Yu Qiao. Region attention networks for pose and\nocclusion robust facial expression recognition. IEEE Transactions on Image Processing, 29:4057–4069, 2020.\n[26] Fuyan Ma, Bin Sun, and Shutao Li. Robust facial expression recognition with convolutional visual transformers.\nArXiv, abs/2103.16854, 2021.\n[27] D. Lundqvist, A. Flykt, and A. Öhma. kdef, cd rom from department of clinical neuroscience,psychology section.\nkarolinska institutet, 1998.\n[28] Oliver Langner, Ron Dotsch, Gijsbert Bijlstra, Daniel H. J. Wigboldus, Skyler T. Hawk, and Ad van Knippenberg.\nPresentation and validation of the radboud faces database. Cognition and Emotion, 24(8):1377–1388, 2010.\n[29] A. Mollahosseini, B. Hasani, and M. H. Mahoor. Affectnet: A database for facial expression, valence, and arousal\ncomputing in the wild. IEEE Transactions on Affective Computing, 10(01):18–31, jan 2019.\n[30] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7132–7141, 2018.\n[31] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout.\narXiv preprint arXiv:1708.04552, 2017.\n9\nLearning ViT with SE for FER\n[32] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. International Conference on Learning Representations, 2018.\n[33] Ilya Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.\n[35] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv\nBatra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE\nInternational Conference on Computer Vision (ICCV), pages 618–626, 2017.\n[36] Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu.\nScore-cam: Score-weighted visual explanations for convolutional neural networks. In 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops (CVPRW), pages 111–119, 2020.\n[37] Mohammed Bany Muhammad and M. Yeasin. Eigen-cam: Class activation map using principal components.\n2020 International Joint Conference on Neural Networks (IJCNN), pages 1–7, 2020.\n[38] Shervin Minaee and AmirAli Abdolrashidi. Deep-emotion: Facial expression recognition using attentional\nconvolutional network. Sensors (Basel, Switzerland), 21, 2021.\n[39] S L Happy and Aurobinda Routray. Automatic facial expression recognition using features of salient facial patches.\nIEEE Transactions on Affective Computing, 6(1):1–12, 2015.\n[40] Jiawei Shi and Songhao Zhu. Learning to amend facial expression representation via de-albino and afﬁnity. ArXiv,\nabs/2103.10189, 2021.\n[41] Jie Cai, Zibo Meng, Ahmed-Shehab Khan, Zhiyuan Li, James O’Reilly, and Yan Tong. Island loss for learning\ndiscriminative features in facial expression recognition. 2018 13th IEEE International Conference on Automatic\nFace & Gesture Recognition (FG 2018), pages 302–309, 2018.\n[42] M. Taini, G. Zhao, S. Z. Li, and M. Pietikainen. Facial expression recognition from near-infrared video sequences.\nIn 2008 19th International Conference on Pattern Recognition, pages 1–4, 2008.\n[43] Abhinav Dhall, O.V . Ramana Murthy, Roland Goecke, Jyoti Joshi, and Tom Gedeon. Video and image based\nemotion recognition challenges in the wild: Emotiw 2015. In Proceedings of the 2015 ACM on International\nConference on Multimodal Interaction, ICMI ’15, page 423–426, New York, NY , USA, 2015. Association for\nComputing Machinery.\n[44] Deepali Aneja, Alex Colburn, Gary Faigin, Linda Shapiro, and Barbara Mones. Modeling stylized character\nexpressions via deep learning. In Asian conference on computer vision, pages 136–153. Springer, 2016.\n[45] Emad Barsoum, Cha Zhang, C. Canton-Ferrer, and Zhengyou Zhang. Training deep networks for facial expression\nrecognition with crowd-sourced label distribution. Proceedings of the 18th ACM International Conference on\nMultimodal Interaction, 2016.\n10\nLearning ViT with SE for FER\nLearning Vision Transformer with Squeeze and Excitation for Facial\nExpression Recognition (Supplementary Material)\nABSTRACT\nIn this supplementary material, we give further details on the conducted experiments and present a\nsummary of the state-of-the-art solutions. In particular, we provide a visual illustrations attention\nmaps for different expressions and at different attention layers. Besides, we support our set of\nexperiments with confusion matrices on RAF-DB and cross database evaluation on CK+. Finally, we\nprovide additional tables that summarize both state-of-the-art solutions and used databases.\n1 Cross-database evaluation and visual illustrations\nCross-database evaluation:To verify the generalisation ability of our model, we conduct a cross-database evaluation\non CK+. The results are summarized in Table 6. It shows that the ViT generalizes better than a baseline CNN\n(ResNet50), and the proposed ViT+SE model enables the best generalization from different training databases when\ntested on CK+. However, the generalization ability is still modest and we aim to improve it in a future work.\nTable 6: Crass-database evaluation on CK+.\nModel Train Test Accuracy\nResNet50\nCK+ CK+ 0.9488\nRAf-DB CK+ 0.3517\nSFEW CK+ 0.2905\nFER2013 CK+ 0.3456\nViT\nCK+ CK+ 0.9817\nRAf-DB CK+ 0.5443\nSFEW CK+ 0.3812\nFER2013 CK+ 0.4098\nViT+SE\nCK+ CK+ 0.9980\nRAf-DB CK+ 0.5576\nSFEW CK+ 0.5341\nFER2013 CK+ 0.6514\nAttention Maps:In this work, we used Grad-Cam [35], Score-Cam [36] and Eigen-Cam [37] as tools to provide visual\nanalysis of the proposed deep learning architectures. (code available in3).\nGrad-CAM [35] :the Gradient-weighted Class Activation Mapping (CAM) uses the gradient of any target following\nto the selected layer in the model to generate a heat map that highlight the important region in the image for predicting\nthe target.\nScore-CAM [36] :the Score-weighted CAM is a linear combination of weights and activation maps. The weights are\nobtained by passing score of each activation map forward on target class.\nEigen-CAM [37] :it computes the principal components of the learned features from the model layers.\nConfusion matrices: Figure 7 shows the confusion matrices of the validation set of RAF-DB for ResNet50, ViT\nand ViT+SE. ViT and ViT+SE have better performance on all expressions except the Happy expression compared\nto ResNet50 performance. Although, the ViT+SE is 0.19% more accurate than ViT, it only outperforms in 4 facial\nexpressions out of 7 basic expressions, which are Fear, Happy, Sad and Surprise. The ViT performs better in Angry,\nDisgust and Neutral expressions.\n3https://github.com/jacobgil/pytorch-grad-cam\n11\nLearning ViT with SE for FER\nFigure 5: score-CAM maps and the guided back-propagation (GBP) at different layers of attention of the ViT for fear\nexpression (image from the validation set of RAF-DB).\nFigure 6: Attention maps based on GRAD-CAM for different expressions (images from the validation set of RAF-DB).\nFigure 7: Confusion Matrices of RAF-DB for ResNet50 (0.8061), ViT (0.8703) and ViT+SE (0.8722).\n2 State-of-the-art\nSurvey on the used databases:Table 7 shows an overview of the facial experiments databases that are used in our\nexperiments.\n12\nLearning ViT with SE for FER\nSummary of state-of-the-art:In Table 8 we summarize different proposed solutions in literature into 3 different\napproaches: Handcrafted, Hybrid and Deep Learning. The Table gives details about the year, the core of the proposed\narchitecture and the databases used for the evaluation.\nTable 7: Survey on databases of Macro-Expressions. BE: Basic Expressions, CE: Compound Expressions, Publ.:\nPublications, Condit.: Conditions.\nDatabase Publ. Year Annotation Condit. Data format Classes\nCK+ [4] CVPRW 2010 593 sequences∗† Lab 640 ×490,\n640 ×480 8BE‡\nJAFFE [5] FG 1998 213 images† Lab 256 ×256 7 BE\nFER-2013 [19] ICONIP 2013 35,887 images† Web 48 ×48 7 BE\nSFEW [6] ICCV 2011 1,766 images∗ Movie 720 ×576 7 BE\nRAF-DB [7] CVPR 2017 29,672 images∗† Internet 256 ×256 7BE\n†Gray scale, ∗RGB, ‡7BE + Contempt\nTable 8: Summary of representative approaches for facial expressions recognition.\nMethods Publ. Year Architecture Databases\nHandcrafted\n[1] TPAMI 1996 OF Private database\n[2] ICIP 2014 PHOG, LPQ CK+[4]\n[39] Trans. AC. 2015 LBP JAFFE[5], CK+[4]\n[3] IHSH 2020 LBP, HOG JAFFE[5], KDEF[ 27],\nRafD[28]\n[24] BMVC 2018 CNN Oulu-CASIA[42], SFEW [6]\nDeep learning\n[25] Trans. IP. 2020 CNN FER-2013[19], RAF-DB[7],\nSFEW[6], AffectNet[29]\n[10] W ACV 2021 CNN RAF-DB[7], AffectNet[29]\n[38] Sensors 2021 CNN FER-2013[19], CK+[ 4] ,\nFERG[44], and JAFFE[5]\n[40] arXiv 2021 CNN FER-2013[19], RAF-DB[7],\nAffectNet[29]\n[11] ICMI 2015 LBP, CNN EmotiW 2015[43]\nHybrid\n[12] ITNEC 2020 LBP, CNN FER-2013 [19]\n[26] arXiv 2021 LBP, CNN,\nViT\nFERPlus [45], RAF-DB [7],\nAffectNet [29], CK+ [4]\n13",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.799758791923523
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7377824187278748
    },
    {
      "name": "Computer science",
      "score": 0.6947182416915894
    },
    {
      "name": "Artificial intelligence",
      "score": 0.564890444278717
    },
    {
      "name": "Facial expression",
      "score": 0.552500307559967
    },
    {
      "name": "Facial expression recognition",
      "score": 0.48805010318756104
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40810421109199524
    },
    {
      "name": "Speech recognition",
      "score": 0.3785974979400635
    },
    {
      "name": "Facial recognition system",
      "score": 0.3476887345314026
    },
    {
      "name": "Machine learning",
      "score": 0.34305498003959656
    },
    {
      "name": "Engineering",
      "score": 0.1566213071346283
    },
    {
      "name": "Voltage",
      "score": 0.060435950756073
    },
    {
      "name": "Electrical engineering",
      "score": 0.05333670973777771
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28221208",
      "name": "Institut National des Sciences Appliquées de Rennes",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210116658",
      "name": "École Centrale d'Électronique",
      "country": "FR"
    }
  ],
  "cited_by": 8
}