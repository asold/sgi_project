{
  "title": "Transformer-Based Under-Sampled Single-Pixel Imaging",
  "url": "https://openalex.org/W4387063747",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096525004",
      "name": "Ye Tian",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109509426",
      "name": "Ying Fu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096219712",
      "name": "Jun Zhang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2800209654",
    "https://openalex.org/W6798837711",
    "https://openalex.org/W2772609332",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3113950706",
    "https://openalex.org/W3151876601",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3213402869",
    "https://openalex.org/W3186573928",
    "https://openalex.org/W3092557781",
    "https://openalex.org/W2122548617",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3082973566",
    "https://openalex.org/W2901589656",
    "https://openalex.org/W3212386989",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W1998688147",
    "https://openalex.org/W1964746825",
    "https://openalex.org/W3035579754",
    "https://openalex.org/W2970610508",
    "https://openalex.org/W3167568784",
    "https://openalex.org/W4281654241",
    "https://openalex.org/W3097187472",
    "https://openalex.org/W2734545110",
    "https://openalex.org/W3085924178",
    "https://openalex.org/W2342370921",
    "https://openalex.org/W2788109278",
    "https://openalex.org/W2137136191",
    "https://openalex.org/W2467052516",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3092457796",
    "https://openalex.org/W3044084828",
    "https://openalex.org/W3163860955",
    "https://openalex.org/W2115451126",
    "https://openalex.org/W3089257391",
    "https://openalex.org/W2564877147",
    "https://openalex.org/W4300961734",
    "https://openalex.org/W2912269594",
    "https://openalex.org/W2587649600",
    "https://openalex.org/W2291054886",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W59557121",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3213165621",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Single-pixel imaging, as an innovative imaging technique, has attracted much attention during the last decades. However, it is still a challenging task for single-pixel imaging to reconstruct high-quality images with fewer measurements. Recently, deep learning techniques have shown great potential in single-pixel imaging especially for under-sampling cases. Despite outperforming traditional model-based methods, the existing deep learning-based methods usually utilize fully convolutional networks to model the imaging process which have limitations in long-range dependencies capturing, leading to limited reconstruction performance. In this paper, we present a transformer-based single-pixel imaging method to realize high-quality image reconstruction in under-sampled situation. By taking advantage of self-attention mechanism, the proposed method is good at modeling the imaging process and directly reconstructs high-quality images from the measured one-dimensional light intensity sequence. Numerical simulations and real optical experiments demonstrate that the proposed method outper-forms the state-of-the-art single-pixel imaging methods in terms of reconstruction performance and noise robustness.",
  "full_text": "Transformer-Based Under-sampled\nSingle-Pixel Imaging\nTIAN Ye1,4, FU Ying2,3, and ZHANG Jun1,2,4\n(1. School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China)\n(2. Yangtze Delta Region Academy of Beijing Institute of Technology, Jiaxing 314019, China)\n(3. School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China)\n(4. Advanced Research Institute of Multidisciplinary Science, Beijing Institute of Technology, Beijing 100081, China)\n \n   Abstract — Single-pixel  imaging,  as  an  innovative\nimaging  technique,  has  attracted  much  attention  during\nthe last decades. However, it is still a challenging task for\nsingle-pixel  imaging  to  reconstruct  high-quality  images\nwith fewer  measurements.  Recently,  deep  learning  tech-\nniques have shown great potential in single-pixel imaging\nespecially for  under-sampling  cases.  Despite  outperform-\ning  traditional  model-based  methods,  the  existing  deep\nlearning-based methods usually utilize fully convolutional\nnetworks to model the imaging process which have limita-\ntions  in  long-range  dependencies  capturing,  leading  to\nlimited  reconstruction  performance.  In  this  paper,  we\npresent  a  transformer-based  single-pixel  imaging  method\nto  realize  high-quality  image  reconstruction  in  under-\nsampled  situation.  By  taking  advantage  of  self-attention\nmechanism, the proposed method is good at modeling the\nimaging process and directly reconstructs high-quality im-\nages  from  the  measured  one-dimensional  light  intensity\nsequence. Numerical  simulations  and  real  optical  experi-\nments demonstrate  that  the  proposed  method  outper-\nforms the state-of-the-art single-pixel imaging methods in\nterms of reconstruction performance and noise robustness.\n   Key words — Computational  imaging, Single-pixel\nimaging, Vision transformer, Under-sampled ratio.\n I. Introduction\nSingle-pixel imaging (SPI) [1]–[3], as a novel com-\nputational  imaging  technique,  has  attracted  a  wide\nrange of  attention  recently  due  to  its  ability  of  repla-\ncing conventional pixel-rich detectors with a single-pixel\ndetector.  Different  from  the  conventional  single-shot\nimaging scheme,  single-pixel  imaging  reconstructs  ob-\nject  image  with  multiple  measurements.  To  be  more\nspecific, SPI  uses  a  sequence  of  modulation  light  pat-\nterns to  illuminate  the  object  image  and  the  corres-\nponding reflected or transmitted light is captured by a\nsingle-element photodetector  as  one-dimensional  meas-\nurement data. The object image can be recovered from\nthe recorded measurements by using various computa-\ntional  imaging  algorithms.  SPI  has  unique  advantages\nsuch as high signal-to-noise ratio, low cost, broadband,\nand  flexible  light-path  configuration  [4].  Therefore,  it\nhas been applied to three-dimensional imaging [5], gas\nimaging  [6],  terahertz  imaging  [7],  remote  sensing  [8],\nand many other fields [9] where pixelated detectors are\nnot accessible due to cost or technical constraints.\nIn  SPI,  high-quality  image  reconstruction  needs  a\nlarge amount of measurements, which greatly increases\nthe  imaging  time  and  limits  the  practical  application.\nTherefore, it is very important for SPI to balance ima-\nging quality and efficiency. Various methods have been\nproposed to solve this problem. One of the most com-\nmonly used methods is to adopt the compressed sens-\ning  (CS)  which  incorporates  the  prior  knowledge  that\nmost  natural  images  have  [10],  [11].  However,  these\nhand-crafted priors are insufficient to represent the di-\nverse  range  of  real-world  images.  Moreover,  these  CS-\nbased methods are very time-consuming because of the\niterative process. Recently, deep learning methods have\nbeen proposed for SPI to improve the reconstructed im-\nage  quality  [12]–[14].  In  comparison  to  the  CS-based\nmethods, deep learning (DL)-based methods can recov-\ner higher quality images with fewer measurements. But\nmost of  these  methods  need  data  preprocessing  to  re-\ncover  the  approximant  first,  which  ignores  the  SPI\nphysical model and is more like image denoising. More\nimportantly,  for  SPI,  the  measured  data  has  a  strong \nManuscript Received Aug. 22, 2022; Accepted Jan. 18, 2023. This work was supported by the National Key R&D Program of China\n(2022YFC3300704) and the National Natural Science Foundation of China (62171038, 61827901, 62088101).\n© 2023 Chinese Institute of Electronics. DOI:10.23919/cje.2022.00.284\nChinese Journal of Electronics\nVol.32, No.5, Sept. 2023\nrelationship because it is obtained from the same scene,\nindicating that long-range dependencies are crucial for\neffective SPI modeling. But the current DL-based meth-\nods all rely on convolution filters to model the depend-\nencies across the input and desirable reconstructed im-\nage. These convolutional neural network (CNN) based\nmodels often focus on local features and ignore the long-\nrange  dependencies  of  SPI  measurements.  Thus,  these\nDL-based  methods  exhibit  satisfactory  reconstruction\nresults only for simple objects and show little perform-\nance improvement as the number of measurements in-\ncreases.\nInspired by the great success in the field of natural\nlanguage processing (NLP) [15], transformer models are\napplied  to  computer  vision  (CV)  tasks  recently  and\nshow valuable potential to substitute for CNN models\n[16], [17]. Different from CNN models, transformer mod-\nels  are  good  at  extracting  global  features  and  flexibly\nmodeling  long-range  dependencies  of  data  at  various\nscales.  Benefiting  from  these  advantages,  transformer\nmodels have the potential to overcome the limitations\nof CNN-based models in SPI reconstruction.\nIn this paper, we propose an under-sampled trans-\nformer-based SPI method, which can realize high-qual-\nity image reconstruction with the one-dimensional light\nintensity sequence. By developing an end-to-end trans-\nformer  network,  the  long-range  dependencies  in  the\nmeasured data can be effectively utilized to improve the\nreconstruction performance of under-sampled SPI. Sim-\nulated and experimental results show that our method\ncan achieve better performance than the state-of-the-art\nSPI methods in terms of fidelity and robustness. This\nwork provides a novel solution for high-quality under-\nsampled single-pixel imaging.\n II. Related Work\nIn this section, we review the researches most rel-\nevant  to  our  work,  including  single-pixel  imaging  and\nvision transformer. Through this review, the innovation\nof our proposed method is clarified.\n 1. Single-pixel imaging\nExisting SPI methods can be divided into conven-\ntional  model-based  methods  and  DL-based  methods.\nAmong them, the conventional model-based SPI meth-\nods  can  be  classified  into  two  categories:  non-iterative\nSPI methods and iterative SPI methods. The non-iter-\native  SPI  methods  [18]  directly  utilize  the  correlation\nbetween modulation light patterns and object image to\nreconstruct the object image without iteration, such as\nthe differential ghost imaging (DGI) method [19]. How-\never,  these  non-iterative  SPI  methods  can  successfully\nreconstruct images only in the fully-sampled situation.\nThe  iterative  SPI  methods,  including  gradient  descent\nSPI methods [20], alternating projection SPI methods [21]\nand  CS-based  SPI  methods  [10],  [11],  combine  convex\noptimization theory and various of hand-crafted priors\nto  reconstruct  the  object  images.  In  [20],  the  authors\ncompared all  model-based  SPI  methods  and  demon-\nstrated  that  the  CS-based  methods  outperform  other\nmodel-based methods in under-sampled cases. Neverthe-\nless,  the  CS-based  methods  are  with  higher  algorithm\ncomplexity and not suitable for real-time imaging.\nRecently, DL-based  SPI  methods  are  widely  con-\ncerned due  to  their  superior  SPI  reconstruction  per-\nformance  in  the  under-sampled  situation.  In  [12], re-\nsearchers  reported  a  DL-based  ghost  imaging  method\n(GIDL) with  a  two-step  process.  It  first  uses  a  tradi-\ntional correlation-based  method  to  reconstruct  the  ap-\nproximate image  from  the  under-sampled  measure-\nments, then uses a deep neural network (DNN) to im-\nprove the reconstruction performance. Similarly, in [13],\nresearchers  proposed  a  deep  learning  ghost  imaging\nmethod (DLGI). Different from the GIDL, it uses a CS-\nbased method  in  the  first  step  and  a  CNN-based  net-\nwork in  the  second  step.  However,  these  methods  re-\nduce the reconstruction efficiency due to the long time\nconsumed in the first step. To improve the reconstruc-\ntion efficiency, in [22], researchers proposed a one-step\nSPI  method  based  on  a  deep  convolutional  auto-en-\ncoder  network  (DCAN),  which  can  reconstruct  image\ndirectly  from  the  under-sampled  SPI  measurements.\nAfter that, researchers proposed several other one-step\nSPI methods that employ different end-to-end SPI re-\nconstruct  networks  [23]–[26].  However,  these  CNN-\nbased  network  structures  are  inefficient  in  reasoning\nlong-range dependencies  in  SPI  measurements.  There-\nfore,  these  methods  are  more  suitable  for  MNIST-like\nsimple object  imaging.  To  further  enhance  the  recon-\nstruction performance,  researchers  prefer  making  ef-\nforts to  improve  the  two-step  DL-based  methods  re-\ncently  [27]–[29].  These  methods  use  non-iterative  SPI\nmethods  to  reconstruct  the  approximate  image  in  the\nfirst step and more complicated CNN-based networks in\nthe second step, which can obtain better reconstruction\nefficiency and quality compared with the previous two-\nstep  methods.  Besides,  researchers  applied  a  physics-\nenhanced framework with fine-tuning process [30] to im-\nprove  the  reconstruction  quality  and  generalization  of\nthe two-step method [31]. In summary, the majority of\nDL-based  SPI  methods  adopt  a  two-step  process  for\nimaging  [26]–[31]  and  heavily  rely  on  CNN  structures\nfor modeling.\n 2. Vision transformer\nTransformer  was  first  proposed  in  [32]  and  has\ngained  extensive  application  for  NLP  tasks  [33].  The\nkey component of transformer is attention mechanism,\n1152 Chinese Journal of Electronics 2023\nwhich can  capture  long-term  information  between  se-\nquence  elements.  Recently,  many  efforts  have  been\nmade by researchers to explore its applicability in CV\ntasks. Compared  with  CNN-based  architecture,  trans-\nformer  shows  more  appealing  performance  in  various\napplications, such as image classification [34], segmenta-\ntion [35], object detection [36], and human pose estima-\ntion [37]. Among them, ViT [38] is the first work that\nuses transformer in place of the standard convolution.\nTo adapt to visual tasks, the 2D image patches are con-\nverted into a vector and fed into the transformer. After\nthat, many kinds of transformers are developed for dif-\nferent demands. For example, to further reduce compu-\ntation expense  and  improve  the  efficiency  of  trans-\nformer, pyramid  vision  transformer  (PVT)  was  pro-\nposed in [39], which makes full use of spatial-reduction\nattention (SRA) to learn multiscale and high-resolution\nfeatures. To improve the modeling capacity of local in-\nformation, shifted windows (Swin) transformer was pro-\nposed  in  [40]  which  has  the  advantage  of  processing\nlarge-size  images  on  the  strength  of  shifted  windows\nmechanism. In summary, transformer now is becoming\nan upgraded alternative for original CNNs in CV tasks\ndue to its outstanding performance.\n III. Principles and Methods\nIn this section, we first formulate the problem for\nunder-sampled  SPI  reconstruction  and  illustrate  the\nmotivation of our work. Then we describe the proposed\ntransformer-based single-pixel imaging method in detail.\n 1. Problem formulation\nThe process of single-pixel imaging consists of two\nstages.  First,  the  object  is  illuminated  by  a  series  of\nmodulation light patterns and the corresponding reflec-\nted light is collected as measurement data with a single-\npixel detector. Mathematically, this process can be ex-\npressed as\n \nBi =\n\u0002\nPi(x; y)T(x; y)dxdy (1)\n \nT(x; y)\nN\n(x; y)\nPi(x; y)\ni\ni = 1; 2; : : : ; M\nM\nN\nM\nM/N\nBi\ni\nwhere  denotes  the  object  image  and  the  total\nnumber of pixels is ,  is the transverse coordin-\nates at the object plane.  denotes the -th modu-\nlation pattern, where  and  is the total\nnumber of modulation patterns. In under-sampled SPI\ncondition,  is  larger  than  and  is  the\nsampling ratio.  is the reflected light intensity under\nthe -th modulation pattern. For convenience, we gener-\nally express the above process in matrix form as\n \nB = PT (2)\n \nSecondly, various kinds of SPI algorithms are used\nT\nP\nto  reconstruct  the  object  image  according  to  the\nknown  modulation  light  patterns .  In  the  under-\nsampled situation, the reconstruction of object image is\nan  ill-posed  problem.  For  CS-based  SPI  methods,  the\nimage  reconstruction  is  regarded  as  an  optimization\nproblem, which can be expressed as\n \n^T = arg min\nT\n∥B \u0000 PT∥2\n2 + \u001cR(T) (3)\n \nR(T)\n\u001c\nB\nwhere  is the prior that most natural images pos-\nsess, such as sparsity, total variation and low rankness.\n  is a trade-off parameter. By combining the convex op-\ntimization  theory  [41],  [42],  the  above  optimization\nproblem can be solved and the object image can be ob-\ntained. However, these CS-based methods need to tune\nparameters  manually  and  have  slow  reconstruction\nspeed  due  to  their  iterative  process.  By  comparison,\nDL-based  SPI  methods  adopt  a  CNN-based  network\nthat can implicitly learn the prior to reconstruct the ob-\nject images. Most of these methods need preprocessing\nalgorithms  [12],  [13]  to  recover  the  approximant  first\nfrom measurements , and then send the noisy image\nto the CNN-based network to get a higher quality re-\nconstructed image. This process can be expressed as\n \n^T = fcnn(SPI(B)) (4)\n \nfcnn(\u0001)\nSPI(\u0001)\nwhere  denotes the  CNN-based  SPI  reconstruc-\ntion  network.  denotes the  preprocessed  al-\ngorithm, such as DGI algorithm [19]. However, due to\nthe limitation of CNN structure, these CNN-based SPI\nmethods perform well only in reconstructing binary im-\nages and  sparse  gray  images.  Besides,  when  the  num-\nber of  measurements  increases,  the  performance  im-\nprovement of these methods is very limited. It is still a\nchallenging task for SPI to reconstruct high-quality im-\nages  directly  from  fewer  one-dimensional  measurement\ndata. Therefore, we develop a transformed-based single-\npixel  imaging  method  to  reconstruct  the  high-quality\nimage directly from one-dimensional light intensity se-\nquence in the under-sampled situation.\n 2. Transformer-based single-pixel imaging\nB\nThe proposed method employs a transformer-based\nnetwork  to  reconstruct  the  object  image  directly  from\nthe one-dimensional measurements . This reconstruc-\ntion process can be mathematically expressed as an im-\nplicit function:\n \n^T = ftspi(B) (5)\n \nftspi(\u0001)\n^T\nwhere  denotes  our  proposed  transformed-based\nSPI  network  that  models  the  dependencies  across  the\nSPI  measurements  to  the  reconstructed  image.  de-\nnotes  the  reconstructed  object  image.  The  mapping\nTransformer-Based Under-sampled Single-Pixel Imaging 1153\nK\nTk\nBk\nk = 1;\n2; : : : ; K\nfrom  one-dimensional  measurements  to  a  two-dimen-\nsional image without knowing the transformation mat-\nrix is highly ill-posed. Therefore, we train the proposed\ntransformer-based network from  pairs of labeled data\neach of which pairs up a known object image  and\nthe corresponding SPI measurements , where \n . The training stage can be expressed as:\n \n~ftspi = arg min\nw\nK∑\nk=1\nL(Tk; ftspi;w(Bk)) (6)\n \nw\nL(\u0001)\nftspi(Bk)\nTk\nwhere  is the all learnable parameters in our proposed\nnetwork,  is  a  loss  function  to  measure  the  error\nbetween the network output  and the ground-\ntruth . We use the mean squared error (MSE) as the\nloss in this work. After training is completed, the arbit-\nrary object image can be reconstructed in terms of its\nSPI measurements:\n \n^T\n′\n= ~ftspi(B\n′\n) (7)\n \nThen, we  illustrate  the  architecture  of  the  pro-\nposed  transformed-based  SPI  network  in Fig.1.  As\nshown in Fig.1, the input of the network is the one-di-\nmensional  measurement  data  obtained  by  the  single-\npixel detector. To effectively exploit the long-range de-\nj\nXj\u00001\nXj\npendencies in  measurements,  we  first  add  a  fully  con-\nnected layer at the input which reshapes the measure-\nment  data  into  a  two-dimensional  feature  map.  Then,\nwe  use  a  convolutional  layer  to  extract  the  low-level\nfeatures. Next, we use four transformer blocks and one\nconvolutional layer to extract the deep features. Finally,\nwe adopt one convolution layer to reconstruct the ob-\nject image from the extracted features. In order to ob-\ntain better image quality, we adopt a long skip connec-\ntion that can aggregate both low-level features and deep\nfeatures to the last convolution layer. For more details,\neach transformer block consists of four transformer lay-\ners and one convolutional layer. Inspired by the advant-\nages  of  the  Swin  transformer,  each  transformer  layer\nconsists  of  LayerNorm  (LN),  multi-layer  perceptron\n(MLP) and  multi-head  self-attention  (MSA).  In  addi-\ntion,  the  shifted  window  mechanism  is  used  in  the\nMSA, which is expressed as S-MSA. For the -th trans-\nformer  layer,  assuming  is  the  input,  the  output\n  can be expressed as:\n \nX′\nj = S-MSA(LN(Xj\u00001)) +Xj\u00001 (8)\n \n \nXj = MLP(LN(Xj′)) +X′\nj (9)\n \nX′\nj\nwhere  denotes the output of the S-MSA.\n \nTransformer block\nSPI measurements\n……\nFully connected Reconstructed\nimage\nTransformed-based single-pixel imaging\nTransformer layer\nConv.\nTransformer block\nTransformer layer\nTransformer layer\nTransformer layer\nTransformer layer\nConv.\nLN\nLN\nS-MSA\nMLP\nTransformer block\nTransformer block\nTransformer block\nConv.\nConv.\n  \nFig. 1. Architecture of the proposed transformer-based SPI network.\n \n IV. Simulation and Experimental\nResults\nIn this section, we conduct several simulations and\nreal  experiments  to  evaluate  the  performance  of  our\nmethod. We first describe the settings for our simula-\ntions and real experiments, including dataset, quantitat-\nive  evaluation  metrics  and  competing  methods.  Then,\nwe compare our proposed method with the state-of-the-\nart methods by simulation, in which both the noiseless\nand noisy situations are considered. Finally, we imple-\nment  our  proposed  method  on  the  real  SPI  captured\ndata which further verifies the effectiveness of our pro-\nposed method.\n 1. Metrics and setups\nWe  conduct  a  comparative  analysis  between  our\nproposed transformer-based SPI method and the state-\nof-the-art SPI methods, which includes traditional CS-\n1154 Chinese Journal of Electronics 2023\n+1\n\u00001\n1 \u0002 10\u00004\n16\n150\nbased  methods  (i.e.,  CS-sparse  [10]  and  CS-TV  [11]),\nand DL-based methods (i.e., DCAN [22], RNN [25], and\nphysics-enhanced method [31]). Following [22], the op-\ntimized binary patterns are selected as the modulation\nlight  patterns  because  they  are  more  practical  and\nhardware-friendly. More specifically, the values of mod-\nulation patterns  are  optimized  together  with  the  pro-\nposed network in the training stage and restricted to \nor  approximately by regularization function. We use\nAdam optimizer [43] to train the proposed transformer-\nbased SPI network. The learning rate is . The\nbatch size is set to  with  epochs. All DL-based\nmethods  are  trained  using  STL-10  dataset  [22]  on  a\nserver equipped with a GeForce RTX 3090. All compet-\ning  methods  are  tested  on  a  computer  with  NVIDIA\nGeForce GTX 1660 SUPER GPU, 16 GB RAM, and 64\nbit  Windows  10  operating  system.  To  quantitatively\nevaluate all methods, we employ two performance met-\nrics,  including  peak  signal-to-noise  ratio  (PSNR)  and\nstructure  similarity  (SSIM)  index  [44].  In  general,  the\nlarger the PSNR and SSIM values, the better the recon-\nstruction performance of SPI.\n 2. Simulations\nWe compare the performance of all competing SPI\n5%\n8%\n10%\n20%\n30%\nalgorithms  based  on  Set12  [45]  dataset.  The  sampling\nratios  are  set  as , , , ,  and .  The\nquantitative results of SPI reconstruction are shown in\nTable  1.  The  proposed  transformer-based  SPI  method\nconsistently  outperforms  both  CS-based  and  DL-based\nSPI methods in terms of PSNR and SSIM, demonstrat-\ning  superior  reconstruction  quality  across  various\nsampling ratios.\n64 \u0002 64\nTo facilitate a visual comparison of all competing\nmethods, we present the reconstructed images of “cam-\neraman” at various sampling ratios for each competing\nmethods,  as  shown  in Fig.2.  The  image  have \npixels. From Fig.2, it is apparent that more details and\nsharper edges can be obtained by our proposed method,\nwhich further demonstrates the effectiveness of our pro-\nposed method.\n1 \u0002 10\u00005\n1 \u0002 10\u00003\nIn practical SPI, the measurements inevitably con-\ntain various noise. To test the noise robustness of our\nmethod, we simulate the case that Gaussian white noise\nis  included  in  SPI  measurements.  Following  [20],  the\nnoise level  added  in  SPI  measurements  can  be  calcu-\nlated  by  the  dividing  of  noise  standard  deviation  and\ntotal pixel number. According to the above definition,\nwe set the noise level as  to . The im-\n   \nTable 1. Average PSNR (dB) and SSIM of the reconstructed results at various sampling ratios on Set12 dataset\n\u0002\nImage size: 32  32\nAlgorithm Metrics Sampling ratio\n2% 5% 8% 10% 20% 30%\nCS-sparse PSNR 11.80 12.54 12.82 13.05 14.73 16.21\nSSIM 0.0527 0.0824 0.1233 0.1480 0.3194 0.4501\nCS-TV PSNR 13.83 15.03 15.80 16.23 18.05 19.38\nSSIM 0.1776 0.2672 0.3464 0.3907 0.5551 0.6522\nDCAN PSNR 17.85 19.16 19.91 20.98 22.78 22.45\nSSIM 0.2662 0.4149 0.4945 0.5586 0.7112 0.6848\nRNN PSNR 18.00 19.82 20.69 21.09 23.20 23.08\nSSIM 0.3085 0.5040 0.5934 0.6307 0.7822 0.7830\nPhysics-enhanced PSNR 16.78 18.23 19.27 19.97 22.26 23.50\nSSIM 0.2735 0.4211 0.5068 0.5426 0.7074 0.7693\nOurs PSNR 18.72 20.60 21.55 22.40 23.64 25.04\nSSIM 0.3350 0.5322 0.6145 0.6811 0.7853 0.8301\n\u0002\nImage size: 64  64\nAlgorithm Metrics Sampling ratio\n2% 5% 8% 10% 20% 30%\nCS-sparse PSNR 12.24 13.24 14.02 14.54 16.51 18.12\nSSIM 0.0554 0.1127 0.1525 0.1882 0.3440 0.4661\nCS-TV PSNR 15.17 16.60 17.51 17.93 19.89 21.39\nSSIM 0.2584 0.3470 0.4138 0.4478 0.5807 0.6662\nDCAN PSNR 18.77 19.97 20.12 20.68 21.00 21.97\nSSIM 0.3531 0.4720 0.4816 0.5305 0.5449 0.6308\nRNN PSNR 16.53 20.86 21.69 21.52 21.45 23.16\nSSIM 0.3504 0.5819 0.6461 0.6372 0.6819 0.7428\nPhysics-enhanced PSNR 16.85 19.41 20.73 21.36 23.20 24.74\nSSIM 0.3233 0.4678 0.5614 0.6021 0.7052 0.7752\nOurs PSNR 19.57 21.29 22.00 22.49 24.30 25.34\nSSIM 0.4366 0.5928 0.6532 0.6785 0.7870 0.8280 \nTransformer-Based Under-sampled Single-Pixel Imaging 1155\n64 \u0002 64\n8%\nage have  pixels. The sampling ratio is . The\nsimulated  results  are  presented  in Table  2. It  is  obvi-\nous that our method maintains the best SPI reconstruc-\ntion performance, even the noise level increases. It in-\ndicates  our  method  has  superior  noise  robustness  and\nmuch suitable for practical SPI applications.\n  \nTable 2. Average PSNR (dB) and SSIM of the reconstructed results on Set12 dataset at different noise levels\nAlgorithm Metrics Noise level\n1E−3 5E−4 1E−4 5E−5 1E−5\nCS-sparse PSNR 12.99 13.79 14.02 14.02 14.02\nSSIM 0.1100 0.1393 0.1503 0.1523 0.1525\nCS-TV PSNR 16.33 17.15 17.48 17.50 17.51\nSSIM 0.3348 0.3874 0.4122 0.4133 0.4138\nDCAN PSNR 19.86 20.04 20.11 20.12 20.12\nSSIM 0.4710 0.4780 0.4813 0.4815 0.4816\nRNN PSNR 21.19 21.54 21.67 21.68 21.69\nSSIM 0.6188 0.6384 0.6455 0.6459 0.6461\nPhysics-enhanced PSNR 19.54 20.40 20.72 20.73 20.73\nSSIM 0.4953 0.5404 0.5571 0.5577 0.5578\nOurs PSNR 21.31 21.83 21.99 22.00 22.00\nSSIM 0.6190 0.6457 0.6530 0.6532 0.6532\n \n \n64 \u0002 64\n10%\nAdditionally, we conduct a comparison of computa-\ntional costs associated with all competing DL-based SPI\nmethods,  where  the  images  have  pixels  with\n  sampling ratio. From Table 3, it can be seen that\nthe one-step DL-based SPI methods consume shorter in-\nference  time  than  the  two-step  method,  indicating  a\nhigher  reconstruction  efficiency.  Among  the  one-step\nmethods, our proposed method has a comparable infer-\nence  time  but  higher  training  consumption.  However,\nthe  relatively  high  training  consumption  is  acceptable\nbecause it does not affect the reconstruction efficiency.\nIn addition, our method has a few more network para-\nmeters than DCAN and RNN methods but realizes bet-\nter  reconstruction  performance  than  other  competing\nmethods.\n 3. Real experiments\nTo further  validate  the  effectiveness  of  the  trans-\n \nCS-TV DCAN RNN Ours\n5%\n10%\n30%\n13.84/0.1035\n15.22/0.1541\n19.02/0.3603\n17.55/0.3682\n18.97/0.4264\n22.28/0.5634\n20.33/0.5174\n21.33/0.5962\n22.84/0.6604\n21.52/0.6336\n22.11/0.6697\n23.78/0.7644\n21.78/0.6612\n22.86/0.7450\n25.97/0.8421\nPhysics-enhanced\n19.99/0.4760\n22.31/0.6398\n25.61/0.7776\nCS-sparse\n \n \nFig. 2. Reconstructed images of “cameraman” (PSNR(dB)/SSIM) with different methods. \n   \nTable 3. The comparison of computational cost among\ndifferent DL-based SPI methods\nAlgorithm Params. Training consumption Inference time\nDCAN 3.37 M\n2 hours single-GPU\n0.1031 s\n(1545 MiB)\nRNN 5.61 M\n27 hours single-GPU\n0.1485 s\n(1106 MiB)\nPhysics-enhanced 12.87 M\n64 hours single-GPU\n7.9219 s\n(2093 MiB)\nOurs 6.50 M\n59 hours single-GPU\n0.1401 s\n(15723 MiB) \n1156 Chinese Journal of Electronics 2023\n64 \u0002 64\n5%\n10%\n30%\nformer-based SPI method, we build up an optical sys-\ntem to acquire real SPI measurement data. The experi-\nmental setup is shown in Fig.3. Specifically, we gener-\nate the  binary modulation patterns using a pro-\njector (Panasonic, X416C XGA) and project them onto\nthe object. Then we capture the reflection light using a\nSi amplified photodetector (Thorlabs, PDA100A2). We\nset the sampling ratio as , , and . Fig.4 illus-\ntrates  the  optical  reconstruction  results.  It  is  obvious\nthat our method outperforms the model-based methods\nand DL-based methods, producing higher quality image\nreconstructions. These experimental results are in good\nagreement  with  the  simulation  results.  Furthermore,\nthese  results  demonstrate  that  our  method  is  better\nsuited for practical applications.\n \nComputer\nProjector\nSingle-pixel detector\nObject\nBinary patterns\n  \nFig. 3. The SPI experimental setup.\n \n5%\n10%\n30%\nCS-sparse CS-TV DCAN RNN Ours\n5%\n10%\n30%\n(a)\n(b)\nPhysics-enhanced\n  \nFig. 4. The  images  reconstructed  by  all  competing  methods  using  the  real  SPI  system.  (a)  Imaging  of  a “ghost” picture;\n(b) Imaging of a tea caddy with a “SPI” letter background.\n \n V. Conclusions\nIn  this  study,  we  present  a  novel  one-step  single-\npixel  imaging  method  that  enables  high-performance\nSPI  reconstruction  based  on  one-dimensional  under-\nsampled  measurements.  By  taking  advantage  of  the\nself-attention mechanism  and  shifted  window  mechan-\nism,  the  proposed  transformer-based  SPI  network  can\nwell exploit the intrinsic features of SPI. Numerous sim-\nulation and  experimental  results  show  that  the  pro-\nposed  method  achieves  higher  image  quality  and\nstronger noise  robustness  than  the  existing  SPI  meth-\nTransformer-Based Under-sampled Single-Pixel Imaging 1157\nods. In  addition,  our  work  indicates  that  the  trans-\nformer  architecture  has  great  potential  to  replace  the\nCNN  architecture  in  single-pixel  imaging,  which  could\nprovide a  new  insight  into  optical  computational  ima-\nging.\nReferences\n M. P. Edgar, G. M. Gibson, and M. J. Padgett, “Principles\nand  prospects  for  single-pixel  imaging,” Nature  Photonics,\nvol.13, no.1, pp.13–20, 2019.\n[1]\n G. M. Gibson, S. D. Johnson, and M. J. Padgett, “Single-\npixel  imaging  12  years  on:  A  review,” Optics  Express,\nvol.28, no.19, pp.28190–28208, 2020.\n[2]\n Q. H. Dai, J. M. Wu, J. T. Fan, et al., “Recent advances in\ncomputational photography,” Chinese Journal of Electron-\nics, vol.28, no.1, pp.1–5, 2019.\n[3]\n Y. Tian, Y. Fu, and J. Zhang, “Plug-and-play algorithm for\nunder-sampling Fourier single-pixel imaging,” Science China\nInformation Sciences, vol.65, no.10, article no.209303, 2022.\n[4]\n M. J. Sun, M. P. Edgar, G. M. Gibson, et al., “Single-pixel\nthree-dimensional  imaging  with  time-based  depth\nresolution,” Nature  Communications,  vol.7,  article\nno.12010, 2016.\n[5]\n G. M. Gibson, B. Q. Sun, M. P. Edgar, et al., “Real-time\nimaging of methane gas leaks using a single-pixel camera,”\nOptics Express, vol.25, no.4, pp.2998–3005, 2017.\n[6]\n L. Zanotto, R. Piccoli, J. L. Dong, et al., “Single-pixel tera-\nhertz imaging: A review,” Opto-Electronic Advances, vol.3,\nno.9, article no.200012, 2020.\n[7]\n J. W. Ma, “Single-pixel remote sensing,” IEEE Geoscience\nand Remote Sensing Letters, vol.6, no.2, pp.199–203, 2009.\n[8]\n Z. B. Zhang, X. Y. Wang, G. A. Zheng, et al., “Fast Fouri-\ner  single-pixel  imaging  via  binary  illumination,” Scientific\nReports, vol.7, no.1, article no.12029, 2017.\n[9]\n M. F. Duarte, M. A. Davenport, D. Takhar, et al., “Single-\npixel imaging via compressive sampling,” IEEE Signal Pro-\ncessing Magazine, vol.25, no.2, pp.83–91, 2008.\n[10]\n C. B. Li, “An efficient algorithm for total variation regular-\nization  with  applications  to  the  single  pixel  camera  and\ncompressive sensing,” Master Thesis, Rice University, Hous-\nton, TX, USA, 2010.\n[11]\n M.  Lyu,  W.  Wang,  H.  Wang, et al., “Deep-learning-based\nghost  imaging,” Scientific  Reports,  vol.7,  no.1,  article\nno.17865, 2017.\n[12]\n Y.  C.  He,  G.  Wang,  G.  X.  Dong, et  al., “Ghost  imaging\nbased on deep learning,” Scientific Reports, vol.8, no.1, art-\nicle no.6469, 2018.\n[13]\n L. S. Sui, L. W. Zhang, Y. Cheng, et al., “Computational\nghost  imaging  based  on  the  conditional  adversarial\nnetwork,” Optics  Communications,  vol.492,  article\nno.126982, 2021.\n[14]\n T. Wolf, L. Debut, V. Sanh, et al., “Transformers: state-of-\nthe-art natural language processing,” in Proceedings of the\n2020 Conference  on  Empirical  Methods  in  Natural  Lan-\nguage  Processing:  System  Demonstrations,  Online,\npp.38–45, 2020.\n[15]\n K. Han, Y. H. Wang, H. T. Chen, et al., “A survey on vis-\nion  transformer,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol.45, no.1, pp.87–110, 2023.\n[16]\n M. H. Guo, T. X. Xu, J. J. Liu, et al., “Attention mechan-\nisms in computer vision: A survey,” Computational Visual\nMedia, vol.8, no.3, pp.331–368, 2022.\n[17]\n W. L. Gong and S. S. Han, “A method to improve the visib-\nility  of  ghost  images  obtained  by  thermal  light,” Physics\nLetters A, vol.374, no.8, pp.1005–1008, 2010.\n[18]\n F.  Ferri,  D.  Magatti,  L.  A.  Lugiato, et  al., “Differential\nghost imaging,” Physical Review Letters, vol.104, no.25, art-\nicle no.253603, 2010.\n[19]\n L. H. Bian, J. L. Suo, Q. H. Dai, et al., “Experimental com-\nparison  of  single-pixel  imaging  algorithms,” Journal of the\nOptical Society of America A, vol.35, no.1, pp.78–87, 2018.\n[20]\n K.  K.  Guo,  S.  W.  Jiang,  and  G.  A.  Zheng, “Multilayer\nfluorescence imaging on a single-pixel detector,” Biomedical\nOptics Express, vol.7, no.7, pp.2425–2431, 2016.\n[21]\n C.  F.  Higham,  R.  Murray-Smith,  M.  J.  Padgett, et  al.,\n“Deep  learning  for  real-time  single-pixel  video,” Scientific\nReports, vol.8, no.1, article no.2369, 2018.\n[22]\n F. Wang, H. Wang, H. C. Wang, et al., “Learning from sim-\nulation: An end-to-end deep-learning approach for computa-\ntional  ghost  imaging,” Optics  Express,  vol.27,  no.18,\npp.25560–25572, 2019.\n[23]\n H. Wu, R. Z. Wang, G. P. Zhao, et al., “Deep-learning de-\nnoising computational ghost imaging,” Optics and Lasers in\nEngineering, vol.134, article no.106183, 2020.\n[24]\n I. Hoshi, T. Shimobaba, T. Kakue, et al., “Single-pixel ima-\nging using a recurrent neural network combined with convo-\nlutional  layers,” Optics  Express,  vol.28,  no.23,  pp.34069–\n34078, 2020.\n[25]\n X. G. Wang, A. G. Zhu, S. S. Lin, et al., “Learning-based\nhigh-quality  image  recovery  from  1D  signals  obtained  by\nsingle-pixel imaging,” Optics Communications, vol.521, art-\nicle no.128571, 2022.\n[26]\n T. Bian, Y. X. Yi, J. L. Hu, et al., “A residual-based deep\nlearning  approach  for  ghost  imaging,” Scientific  Reports,\nvol.10, no.1, article no.12149, 2020.\n[27]\n T. Bian, Y. M. Dai, J. L. Hu, et al., “Ghost imaging based\non  asymmetric  learning,” Applied  Optics,  vol.59,  no.30,\npp.9548–9552, 2020.\n[28]\n W. Feng, X. Y. Sun, X. H. Li, et al., “High-speed computa-\ntional ghost imaging based on an auto-encoder network un-\nder  low  sampling  rate,” Applied  Optics,  vol.60,  no.16,\npp.4591–4598, 2021.\n[29]\n Y. Fu, T. Zhang, L. Z. Wang, et al., “Coded hyperspectral\nimage reconstruction using deep external and internal learn-\ning,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol.44, no.7, pp.3404–3420, 2021.\n[30]\n F. Wang, C. L. Wang, C. J. Deng, et al., “Single-pixel ima-\nging using physics enhanced deep learning,” Photonics Re-\nsearch, vol.10, no.1, pp.104–110, 2022.\n[31]\n A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all\nyou need,” in Proceedings of the 31st International Confer-\nence  on  Neural  Information  Processing  Systems,  Long\nBeach, CA, USA, pp.6000–6010, 2017.\n[32]\n A. Gillioz, J. Casas, E. Mugellini, et al., “Overview of the\ntransformer-based models for NLP tasks,” in Proceedings of\nthe 2020  15th  Conference  on  Computer  Science  and  In-\nformation Systems, Sofia, Bulgaria, pp.179–183, 2020.\n[33]\n C. F. R. Chen, Q. F. Fan, and R. Panda, “CrossViT: Cross-\nattention multi-scale vision transformer for image classifica-\ntion,” in Proceedings of the IEEE/CVF International Con-\nference  on  Computer  Vision,  Montreal,  QC,  Canada,\npp.347–356, 2021.\n[34]\n B.  W.  Cheng,  A.  G.  Schwing,  and  A.  Kirillov, “Per-pixel\nclassification is not all you need for semantic segmentation,”\nin Proceedings of  the  35th  Advances  in  Neural  Informa-\ntion  Processing  Systems,  Virtual  Conference,  pp.17864–\n17875, 2021.\n[35]\n1158 Chinese Journal of Electronics 2023\n N. Carion, F. Massa, G. Synnaeve, et al., “End-to-end ob-\nject  detection  with  transformers,” in Proceedings  of the\n16th European Conference on Computer Vision, Glasgow,\nUK, pp.213–229, 2020.\n[36]\n Y. H. Cai, Z. C. Wang, Z. X. Luo, et al., “Learning delicate\nlocal  representations  for  multi-person  pose  estimation,” in\nProceedings  of the 16th  European  Conference  on  Com-\nputer Vision, Glasgow, UK, pp.455–472, 2020.\n[37]\n A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al., “An image\nis worth 16×16 words: Transformers for image recognition\nat scale,” arXiv preprint, arXiv: 2010.11929, 2020.\n[38]\n W. H. Wang, E. Z. Xie, X. Li, et al., “Pyramid vision trans-\nformer:  A  versatile  backbone  for  dense  prediction  without\nconvolutions,” in Proceedings  of the IEEE/CVF  Interna-\ntional  Conference  on  Computer  Vision,  Montreal,  QC,\nCanada, pp.548–558, 2021.\n[39]\n Z. Liu, Y. T. Lin, Y. Cao, et al., “Swin transformer: Hier-\narchical vision transformer using shifted windows,” in Pro-\nceedings  of the  IEEE/CVF  International  Conference  on\nComputer  Vision,  Montreal,  QC,  Canada,  pp.9992–10002,\n2021.\n[40]\n S.  Wu,  J.  Tian,  and  W.  Cui, “A novel  parameter  estima-\ntion algorithm for DSSS signals based on compressed sens-\ning,” Chinese  Journal  of  Electronics,  vol.24,  no.2,\npp.434–438, 2015.\n[41]\n X. S. Wang, Y. H. Cheng, and J. Ji, “Semi-supervised re-\ngression  algorithm  based  on  optimal  combined  graph,”\nChinese  Journal  of  Electronics,  vol.22,  no.4,  pp.724–728,\n2013.\n[42]\n D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” in Proceedings of the 3rd International Con-\nference on Learning Representations, San Diego, CA, USA,\nAvailable at: https://arxiv.org/pdf/1412.6980v9.pdf, 2015.\n[43]\n S. S. Channappayya, A. C. Bovik, and R. W. Heath, “Rate\nbounds on SSIM index of quantized images,” IEEE Trans-\nactions  on  Image  Processing,  vol.17,  no.9,  pp.1624–1639,\n2008.\n[44]\n K. Zhang, Y. W. Li, W. M. Zuo, et al., “Plug-and-play im-\nage  restoration  with  deep  denoiser  prior,” IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, vol.44,\nno.10, pp.6360–6376, 2022.\n[45]\nTIAN Ye    received the B.S. de-\ngree  from  School  of  Information  Science\nand  Engineering,  Lanzhou  University,\nLanzhou, China,  in  2017,  the  M.S.  de-\ngree  from  Peking  University,  Beijing,\nChina, in 2020. She is currently pursuing\nthe Ph.D. degree with the School of In-\nformation and  Electronics,  Beijing  Insti-\ntute  of  Technology,  Beijing,  China.  Her\ncurrent research interests include deep learning, image processing,\nand computational imaging. (Email: 3220205110@bit.edu.cn)\nFU Ying   (corresponding  author)\nreceived the B.S. degree in electronic en-\ngineering from Xidian University in 2009,\nthe  M.S.  degree  in  automation  from\nTsinghua  University  in  2012,  and  the\nPh.D.  degree  in  information  science  and\ntechnology from the University of Tokyo\nin  2015.  She  is  currently  a  Professor  at\nthe  School  of  Computer  Science  and\nTechnology, Beijing  Institute  of  Technology.  Her  research  in-\nterests include physics-based vision, image processing, and com-\nputational photography. (Email: fuying@bit.edu.cn)\nZHANG Jun   received  the  B.S.,\nM.S., and  Ph.D.  degrees  in  communica-\ntions and  electronic  systems  from  Bei-\nhang University, Beijing, China, in 1987,\n1991,  and  2001,  respectively.  He  was  a\nProfessor  with  Beihang  University.  He\nhas served as the Dean for the School of\nElectronic  and  Information  Engineering,\nand the  Vice  President  and  the  Secret-\nary for the Party Committee, Beihang University. He is currently\na Professor with Beijing Institute of Technology, where he is also\nthe President. His research interests are networked and collabor-\native air traffic management systems, covering signal processing,\nintegrated and heterogeneous networks, and wireless communica-\ntions. He is a Member of the Chinese Academy of Engineering. He\nhas  won  the  awards  for  science  and  technology  in  China  many\ntimes. (Email: buaazhangjun@vip.sina.com)\nTransformer-Based Under-sampled Single-Pixel Imaging 1159",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8051230907440186
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7453721165657043
    },
    {
      "name": "Pixel",
      "score": 0.7353178858757019
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6942969560623169
    },
    {
      "name": "Deep learning",
      "score": 0.5849161744117737
    },
    {
      "name": "Computer vision",
      "score": 0.5662764310836792
    },
    {
      "name": "Transformer",
      "score": 0.49724724888801575
    },
    {
      "name": "Image quality",
      "score": 0.47713303565979004
    },
    {
      "name": "Process (computing)",
      "score": 0.4463556706905365
    },
    {
      "name": "Iterative reconstruction",
      "score": 0.44450944662094116
    },
    {
      "name": "Image (mathematics)",
      "score": 0.20542070269584656
    },
    {
      "name": "Voltage",
      "score": 0.10447105765342712
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    }
  ]
}