{
  "title": "EVALUATING LARGE LANGUAGE MODELS FOR MEDICAL INFORMATION EXTRACTION: A COMPARATIVE STUDY OF ZERO-SHOT AND SCHEMA-BASED METHODS",
  "url": "https://openalex.org/W4405933018",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5008780656",
      "name": "Zakaria Kaddari",
      "affiliations": [
        "Premier University"
      ]
    },
    {
      "id": "https://openalex.org/A5115719540",
      "name": "Ikram El HACHMI",
      "affiliations": [
        "Premier University"
      ]
    },
    {
      "id": "https://openalex.org/A5032252694",
      "name": "Jamal Berrich",
      "affiliations": [
        "Premier University"
      ]
    },
    {
      "id": "https://openalex.org/A5024648833",
      "name": "Rim Amrani",
      "affiliations": [
        "Premier University"
      ]
    },
    {
      "id": "https://openalex.org/A5073383075",
      "name": "Toumi Bouchentouf",
      "affiliations": [
        "Premier University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4393335435",
    "https://openalex.org/W4391093878",
    "https://openalex.org/W4396553888",
    "https://openalex.org/W4404534210",
    "https://openalex.org/W3043096329",
    "https://openalex.org/W4392632345",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4403524605",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W3203765809",
    "https://openalex.org/W1747861911",
    "https://openalex.org/W4392353733",
    "https://openalex.org/W4313527208",
    "https://openalex.org/W3177034450",
    "https://openalex.org/W4400035322"
  ],
  "abstract": "This study investigates the application of large language models, particularly ChatGPT, in the extraction and structuring of medical information from free-text patient reports. The authors explore two distinct methods: a zero-shot extraction approach and a schema-based extraction approach. The dataset, consisting of 1230 anonymized French medical reports from the Department of Neonatology of the Mohammed VI University Hospital, served as the basis for these experiments. The findings indicate that while ChatGPT demonstrates a significant capability in structuring medical data, certain challenges remain, particularly with complex and non-standardized text formats. The authors evaluate the model's performance using precision, recall, and F1 score metrics, providing a comprehensive assessment of its applicability in clinical settings.",
  "full_text": " \nApplied Computer Science, vol. 20, no. 4, pp. 138‚Äì148 \ndoi: https://doi.org/10.35784/acs-2024-44 \n \n138 \nSubmitted: 2024-08-30 | Revised: 2024-11-11 | Accepted: 2024-11-18 \nKeywords: medical information extraction, Large Language Models, ChatGPT, schema-based \nextraction \nZakaria KADDARI [0000-0003-4034-5612]*, Ikram El HACHMI [0009-0008-7928-3088]**, \nJamal BERRICH [0000-0001-8443-7223]*, Rim AMRANI [0000-0003-3906-5533]**, \nToumi BOUCHENTOUF [0000-0002-2689-8678]* \nEVALUATING LARGE LANGUAGE MODELS FOR \nMEDICAL INFORMATION EXTRACTION: A \nCOMPARATIVE STUDY OF ZERO-SHOT AND \nSCHEMA-BASED METHODS \nAbstract \nThis study investigates the application of large language models, particularly \nChatGPT, in the extraction and structuring of medical information from free -text \npatient reports. The authors  explore two distinct methods: a zero -shot extraction \napproach and a schema -based extraction approach. The dataset, consisting of 1230 \nanonymized French medical reports from the Department of Neonatology of the \nMohammed VI University Hospital, served as th e basis for these experiments. The \nfindings indicate that while Chat GPT demonstrates a significant capability in \nstructuring medical data, certain challenges remain, particularly with complex and \nnon-standardized text formats. The authors  evaluate the model's performance using \nprecision, recall, and F1 score metrics, providing a comprehensive assessment of its \napplicability in clinical settings. \n1. INTRODUCTION \nIn recent years, the healthcare industry has seen a significant shift towards the \ndigitalization of medical records, driven by the need for improved patient care, data \nmanagement, and operational efficiency. With the proliferation of electronic health records \n(EHRs), there has been an exponential increase in the amount of unstructured textual data \ngenerated by healthcare providers. This unstructured data, which includes clinical notes, \npatient reports, and other free-text medical documentation, contains vital information that is \ncrucial for patient diagnosis, treatment planning, and research. However, extracting \nstructured, actionable insights from this vast amount of unstructured text remains a \nformidable challenge. \n \n* Universit√© Mohammed Premier, National School of Applied Sciences, LaRSA laboratory, AIRES team,  \nMorocco, z.kaddari@ump.ac.ma, j.berrich@ump.ac.ma, t.bouchentouf@ump.ac.ma \n** Universit√© Mohammed Premier, Faculty of Medicine and Pharmacy Oujda , Morocco, \nikram.elhachmi@ump.ac.ma, r.amrani@ump.ac.ma \n \n139 \nTraditional natural language processing (NLP) (Kaddari et al., 2021) techniques have \nbeen employed to address this challenge, offering various methods to convert free-text data \ninto structured formats. These methods typically rely on rule -based systems  (RBS), \nstatistical models, or a combination of both. While they have shown success in specific \napplications, these approaches often struggle with the complexity and variability of clinical \nlanguage, which is characterized by domain-specific terminology, abbreviations, and diverse \nlinguistic patterns (Zhan et al., 2021). Moreover, traditional NLP systems require extensive \ndomain knowledge and manual rule crafting, making them less adaptable to new or evolving \ndatasets. \nThe advent o f large language models (LLMs) (Yifan et al., 2024) , such as OpenAI's \nChatGPT (Ray, 2023), has introduced a new paradigm in the field of NLP. These models, \ntrained on vast amounts of text data, are capable of understanding and generating human -\nlike language, making them well-suited for a wide range of text processing tasks. ChatGPT, \nin particular, has demonstrated impressive performance in tasks like text summarization, \ntranslation, and question answering. Its ability to perform zero -shot learning‚Äîwhere the \nmodel is applied to tasks without specific trainin g on those tasks ‚Äîpresents a unique \nopportunity to leverage its capabilities in the medical domain, particularly for the extraction \nand structuring of clinical information. \nDespite the potential of LLMs, their application in the healthcare sector is still in its \ninfancy, and several challenges remain. The highly specialized and sensitive nature of \nmedical data requires models that not only perform accurately but also maintain the highest \nstandards of reliability and interpretability. Furthermore, the risk of hallucination (Huang et \nal., 2024) where the model generates incorrect or nonsensical information, poses a \nsignificant concern in clinical settings, where such errors can have serious consequences. \nThis study aims to explore the effectiveness of LLMs, particularly ChatGPT, in extracting \nand structuring information from free-text medical reports, specifically focusing on French-\nlanguage neonatal patient reports. The authors propose two distinct methods for information \nextraction: a zero -shot extraction approach, where the model i s tasked with extracting \npredefined attributes without prior domain-specific training, and a schema-based extraction \napproach, which involves defining a structured schema to gu ide the extraction process. By \ncomparing the performance of these methods, the authors seek to evaluate the feasibility of \nusing ChatGPT for clinical data structuring and identify areas where further refinement is \nneeded. \nThrough this research, the authors contribute to the growing body of knowledge on the \napplication of LLMs in the healthcare domain, offering insights into their strengths and \nlimitations. The findings are intended to inform future efforts in the development of NLP \ntools for clinical applications, with the ultimate goal of improving patient care through more \neffective use of healthcare data. \n2. RELATED WORKS \nDue to the difficulty of obtaining medical notes, little research was done on automatic \nmedical notes structuring. Still, varieties of techniques were  explored, from rule -based \nsystems (RBS) to Large Language Models, each presenting unique strengths and limitations \nin handling clinical data. \n \n140 \nRule-Based Approaches: Early methods, such  as rule -based NLP s ystems, have \ndemonstrated robustness in specific scenarios. F or instance, Patra et al. (2024) utilized an \nRBS to extract social information from psychiatry notes, outperforming LLMs in accuracy \nacross all metrics. These systems excel due to their deterministic nature, ensuring \nconsistency in extraction when rules are clearly defined. However, their rigidity makes them \nless adaptable to diverse and evolving clinical contexts. \nLLMs for Information Extraction: With the rise of LLMs like ChatGPT, new paradigms \nhave emerged. LLMs leverage vast amounts of training data, allowing them to generalize \nacross tasks with minimal domain -specific adjustments. In Huang et al. (2024), ChatGPT-\n3.5 demonstrated an 89% accuracy in extracting structured data from lung cancer notes, \nsurpassing traditional NLP methods. This study highlighted the potential of zero -shot \ncapabilities in clinical settings, showing that LLMs could achieve significant results without \ndomain-specific training. Similarly, in (Kernberg et al., 2024), the authors used ChatGPT-4 \nto create structured medical notes from audio recordings of physician -patient encounters. \nThe research indicated significant differences in error rates, accuracy, and the quality of \nnotes produced by ChatGPT-4. It was found that longer transcripts and more complex data \nnegatively impacted note accuracy, highlighting potential issues with the model's capability \nin managing intricate medical cases. \nThis work aligns with this trend by employing both a zero-shot extraction method and a \nschema-based extraction approach . The zero -shot method is similar to Huang et al.'s, \nleveraging ChatGPT‚Äôs ability to perform information extraction without specialized fine -\ntuning. However, the schema-based method introduces additional structure, which enhances \nextraction accuracy, particularly for complex attributes like \"Mother's medical history.\" \nTransformer-Based Models: Other research focused on smaller transformer models \ntailored to specific datasets. For instance, (Bergomi et al., 2024) used a compact transformer \nto structure radiology notes, achieving results comparable to GPT -3.5 despite being a \nthousand times smaller. Similarly, (Zelina et al., 2022) applied a fine-tuned RoBERTa model \nRobeCzech (Straka et al., 2021)  to Czech clinical data, illustrating that domain -specific \ntransformers can perform effectively with localized datasets. \nThe presented  schema-based extraction method parallels these domain -specific \napproaches by providing structured guidance. Unlike smaller transformers, we employed \nChatGPT models (3.5 and 4) with a schema that closely mirrors the attribute structure in \npatient reports. This structured guidance led to superior performance in precision and recall, \nespecially with ChatGPT-4, which outperformed the zero-shot method in all metrics. \nZero-shot and Few-shot Learning: The adaptability of LLMs has also been tested in few-\nshot learning contexts. (Agrawal et al., 2022)  demonstrated that LLMs like I nstructGPT \n(Ouyang, 2022)  excel in zero -shot and few -shot scenarios even without clinical domain \ntraining. Similarly, (Bhate et al., 2023)  employed a minimal instruction setup to extract \nsocial determinants from clinical notes using a GPT-based model. \nThis research builds on these findings by showing that structured extraction (schema -\nbased) can mitigate some of the limitations of zero-shot approaches, particularly in handling \ncomplex, multi -word fields. While our zero -shot method displayed competence in \nstraightforward extractions, the schema -based approach provided a consistent boost in \naccuracy, highlighting the benefits of a structured methodology when dealing with nuanced \nclinical data. \n \n141 \n3. DATA \nThe dataset used in this study consists of 1230 free-text patient medical reports from 2021 \nto 2023 written in French, supplied by the Department of Neonatology of the Mohammed \nVI University Hospital of Oujda, Morocco. The provided reports were completely \nanonymized. The formatting was not consistent, as multiple templates were used during the \nthree-year period. In general, each report contained the following five sections: patient \ninformation, information about the mother, clinical exam, how to proceed wit h the patient, \nand patient stay evolution. Figure 1 lists the most important clinical information in each \nsection. On average, a report contains 505 words, with the most complete report containing \n1049 words. \nTable 1 shows an example of the desired medical information to extract for two sample \nclinical reports. \n \nFig. 1. Patient reports most important information \nTab. 1. Example of the desired medical information to extract for two sample clinical reports \nReport \nnumber \nGender Mother‚Äôs \nage \nDate of \nadmission \nWeight PNN Type of marriage Height \n1 F√©minin 43 14/07/2023 3.6 7895 consanguin 50cm \n2 masculin 19 13/04/2023 2.6 11256 Non  consanguin 48cm \nReport 1 : Date d‚Äôentr√©e: 14/07/2023 ‚Ä¶ de sexe F√©minin ‚Ä¶ m√®re √¢g√©e de 43 ans ‚Ä¶ de mariage consanguin‚Ä¶ Poids: 3.6kg \nTaille: 50cm ‚Ä¶ PNN 7895 \nReport 2 : Date d‚Äôentr√©e: 13/04/2023 ‚Ä¶ de sexe masculin ‚Ä¶  √¢g√©e de 19 ans ‚Ä¶ de mariage non consanguin ‚Ä¶ poids: 2kg 600 \nTaille 48cm PNN 11256 \n\n \n142 \n4. METHODS \nDue to its superiority compared to the other available LLMs, the authors based their \nmethods on ChatGPT. They used the model via the proposed Application Programming \nInterface (API) by OpenAI in a zero -shot setting. A ChatGPT API call requires users to \nprovide instructions via two role variables. \n‚àí System: defines task instructions. \n‚àí User: provides an input text for zero-shot learning. \nThe specific models that were used are described in the Results section. \nThe authors  used two different methods in their experiments. First, they prompted \nChatGPT in a zero-shot setting by asking it to extract a list of predefined attributes from a \ngiven patient report. Table 2 shows the exact prompt that was used. \nTab. 2. Exact prompt used in the first method \nRole Content \nSystem Please extract the following attributes from the provided patient report: \n- Date of admission \n- Date of birth \n- Gender of newborn \n- Type of marriage \n- Admission diagnosis \n- Mother's medical history \n- Mother's age \n- PNN (polynucl√©aires neutrophiles) \n- Pregnancy follow-up \n- Pregnancy carried to term \n- Weight \n- Height \nUser Patient report \n \nFor the second method, the authors defined a schema model for each report section. With \neach model containing the medical attributes to be extracted. A model is defined as a Python \nclass, and medical attributes are defined as regular typed class attributes. To aid the model \nin the extraction, the authors used the same attribute names in the patient reports, and they \nprovided a description for certain attributes to further aid the model in extracting the desired \nattributes with specific unit measures for example. Thay then instructed ChatGPT to extract \nthe required information as objects of the defined classes.  \nIn order to mitigate hallucination, the authors allowed the model for an additional retry \nin case it did not succeed in extracting a given attribute. \nThe results of the experiments using the two methods are presented in section V. \n5. RESULTS \nIn order to experiment with these two methods, the authors created a test set from their \ndataset, first by randomly selecting 100 patient reports. Then, by manually extracting a \n \n143 \nsubset of the desired medical attributes. They made sure to include attributes from all \nrelevant types (strings, dates, integers, floats, and booleans) in their experiments. \nIn the evaluation, the authors used a relaxed match metric, similar to the fragment match \napproach used in biomedical named entity recognition (Tsai et al., 2006). In relaxed match, \nall extracted concepts are br oken down into individual words . Unlike in fragment match, \nwhere each token in a biomedical concept is considered separately . Relaxed match is more \nsuited for clinical notes, and represents a middle ground between the more rigorous strict \nmatch, and more tolerant measures like fragment match. Once the individual words were \nextracted, the precision and value returned based on the words were calculated.  Words \npresent in both the annotated ground truth and the model output are counted as True Positives \n(TP). Words found in the model output but not in the annotated ground truth are counted as \nFalse Positives (FP), and words in the annotated ground truth but missing from the model \noutput are counted as False Negatives (FN). Equations 1 to 3 detail the calculations for recall \n(R), precision (P), and F1 score. \n ùëÉ =\nùëáùëÉ\nùëáùëÉ+ùêπùëÉ                                 (1) \n ùëÖ =\nùëáùëÉ\nùëáùëÉ+ùêπùëÅ                                  (2) \n ùêπ1 =\n2ùëÉùëÖ\nùëÉ+ùëÖ                                 (3) \nThe results of these experiments are shown in Table 3 and Table 4. The authors  \nexperimented with their two methods using the two models ChatGPT 3.5 and ChatGPT 4. \nTable 3 shows the overall results of their experiments with the two methods using the two \nChatGPT models. Whereas Table 4 shows the detailed results of the experiments carried out \nwith the scheme-based extraction method. This is the best performing of the two proposed \nmethods. \nTab. 3. Overall results of our experiments using the two proposed methods \nMethod ChatGPT 3.5 ChatGPT 4 \nP R F1 P R F1 \nzero-shot extraction 70.13 71.45 70.35 74.69 74.98 73.14 \nschema-based extraction 76.55 77.99 76.69 81.93 82.76 81.00 \n \nThe authors wanted to study the effectiveness of their best-performing method (schema-\nbased extraction) by attribute type. They experimented with all attribute types in patient \nreports, which are dates, single word strings, long sentences, integers, floats, and booleans. \nIn order to confirm their findings, the authors  experimented on two attributes by each \nattribute type. Table 4 shows the results of these experiments using the two ChatGPT \nmodels. \n \n \n \n \n144 \nTab. 4. Detailed results of the schema-based extraction method \nAttribute \nType \nAttribute ChatGPT 3.5 ChatGPT 4 \nP R F1 P R F1 \nDate Date of admission 76.67 76.67 76.67 77.36 77.36 77.36 \nDate of birth 83.33 83.33 83.33 85 85 85 \nSingle word string Gender of newborn 93.33 93.33 93.33 95.75 95.75 95.75 \nType of marriage 80.00 80.00 80.00 83.16 83.16 83.16 \nLong sentence Admission diagnosis 77.78 78.33 78.00 78.67 78.67 78.67 \nMother's medical history 54.13 70.90 55.59 61.23 71.23 60.12 \nInteger Mother's age 96.67 96.67 96.67 96.67 96.67 96.67 \nPNN 56.67 56.67 56.67 59.33 59.33 59.33 \nBoolean Pregnancy follow-up 90.00 90.00 90.00 93.33 93.33 93.33 \nPregnancy carried to term 53.33 53.33 53.33 86.67 86.67 86.67 \nFloat Weight 70.00 70.00 70.00 77.00 77.00 77.00 \nHeight 86.67 86.67 86.67 89.00 89.00 89.00 \nAverage 76.55 77.99 76.69 81.93 82.76 81.00 \n \nIn Figure 2, the performance of the schema-based extraction method by number of tries \nis presented. Allowing for an additional try does mitigate the effect of hallucination, yielding \noverall performance gains. \n \nFig. 2. Performance of the schema-based extraction method by number of tries \nThe next section discusses the results, first comparing the performance of the two \nproposed methods. Then, the performance of the most efficient method is investigated for \ndifferent types of attributes, and finally, comparing the performance of the ChatGPT model. \n\n \n145 \n6. DISCUSSION \nThe results of this study indicate a clear difference in the performance of the two methods \nemployed‚Äîzero-shot extraction and schema -based extraction ‚Äîacross various attribute \ntypes and ChatGPT models. \n6.1. Comparison of methods \nThe schema -based extraction method outperformed zero -shot extraction in all \nperformance metrics across both ChatGPT -3.5 and ChatGPT -4 models. Specifically, for \nChatGPT-3.5, the schema-based method achieved an F1 score of 76.69 compared to 70.35 \nfor the zer o-shot method. This trend was consistent with ChatGPT -4, where the schema -\nbased approach recorded an F1 score of 81.00, significantly higher than the 73.14 observed \nwith zero-shot extraction. These findings underscore the importance of providing structured \nguidance to the model in extracting specific medical attributes, which appears to enhance \nthe accuracy and consistency of the results. \n6.2.  Comparison by attribute type \nWhen analyzing the results by attribute type, it is evident that both methods struggled \nmore with certain field types, particularly complex or multi -word fields such as \"Mother's \nmedical history\". For instance, the F1 score for \"Mother's medical history\" w as notably \nlower compared to simpler fields like \"Gender of newborn\" and \"Mother's age\". This \nsuggests that while LLMs like ChatGPT are adept at handling straightforward, single-word \nextractions, they encounter challenges with longer and more nuanced text segments. \nInterestingly, the schema-based method provided more reliable results for complex fields \ncompared to zero-shot extraction, particularly with ChatGPT-4. For example, the F1 score \nfor \"Mother's medical history\" improved from 55.59 (ChatGPT -3.5) to 60.12 (ChatGPT-4) \nin the schema -based approach, demonstrating the method's relative effectiveness in \nmanaging complex attributes when provided with explicit structure and schema. \nBoolean fields such as \"Pregnancy follow -up\" and \"Pregnancy carried to term\" showed \nvarying results. While \"Pregnancy follow-up\" achieved high F1 scores (90.00 for ChatGPT-\n3.5 and 93.33 for ChatGPT -4), \"Pregnancy carried to term\" had lower scores, particul arly \nwith ChatGPT -3.5 (53.33). This discrepancy suggests that the model's performance on \nboolean fields may depend on the specific context and phrasing of the attribute. \nNumeric fields like \"Weight\" and \"Height\" showed good performance, with F1 scores \nranging from 70.00 to 89.00. This indicates that both methods are reasonably effective at \nextracting quantitative data from medical reports. \n6.3. Comparison by ChatGPT model \nThe comparison between ChatGPT -3.5 and ChatGPT -4 further highlights the \nadvancements made in the latter model. ChatGPT -4 consistently outperformed ChatGPT -\n3.5 across all metrics and methods, with a noticeable increase in precision, recall, and F1 \nscores. For example, the F1 score for zero-shot extraction improved from 70.35 (ChatGPT-\n3.5) to 73.14 (ChatGPT -4), and for schema -based extraction, it increased from 76.69 \n(ChatGPT-3.5) to 81.00 (ChatGPT-4). This improvement is particularly pronounced in the \n \n146 \nschema-based method, indicating that ChatGPT-4 is better equipped to leverage structured \nguidance for extracting medical information, further validating the evolution in LLM \ncapabilities. \nThe performance gap between the two models was most evident in complex fields. For \ninstance, in the schema-based extraction of \"Mother's medical history\", ChatGPT-4 achieved \nan F1 score of 60.12, compared to 55.59 for ChatGPT-3.5. This suggests that the advanced \ncapabilities of ChatGPT-4 are particularly beneficial when dealing with more challenging, \ncontext-dependent information extraction tasks. \nIt's worth noting that even for fields where ChatGPT-3.5 performed well, such as \"Gender \nof newborn\" (F1 score of 93.33), ChatGPT-4 still managed to show improvement (F1 score \nof 95.75). This consistent enhancement across various field types underscores th e overall \nsuperiority of the newer model in medical information extraction tasks. \n6.4. Common extraction errors \nThe analysis of the extraction methods revealed several common errors, each of which \ncarries specific implications for clinical applications. These errors were observed across both \nzero-shot and schema-based approaches, highlighting areas where current methods can be \nimproved. \nFalse positives in e xtraction: One common issue involved the over -identification of \nattributes, leading to false positives. This error was particularly evident in fields with \noverlapping terminology, such as \"Mother's medical history\" and \"Admission diagnosis,\" \nwhere context was crucial for accurate interpretation. In clinical scenarios, false positives \ncan generate misleading information, potentially affecting clinical decisions and leading to \nunnecessary follow-up actions. \nIncomplete data due to false n egatives: False negatives, where relevant attributes were \nmissed by the extraction model, were also a notable source of error. Complex, multi -word \nentities, particularly those involving numerical or categorical values like \"Pregnancy carried \nto term,\" were  frequently affected. Missing critical data can compromise the integrity of \npatient records, leading to gaps in medical history that could affect future treatment plans. \nMisinterpretation of contextual i ndicators: Both extraction methods occasionally failed \nto interpret negations or contextual cues accurately. This was particularly problematic with \nphrases involving temporal elements (e.g., \"recent history of illness\") or negations (\"no \nprevious complications\"). Errors in contextual understanding could lead to incorrect patient \nprofiles, potentially influencing clinical risk assessments and eligibility for specific \ntreatments. \nThese extraction errors have a direct impact on the reliability of structured data in clinical \nsettings. For instance, a false negative in the extraction of a family history related to genetic \nconditions could lead to underestimating patient risks, while f alse positives may result in \nunnecessary diagnostic procedures. Additionally, incomplete or misinterpreted data \ncomplicates clinical documentation, affecting the quality of Electronic Health Records \n(EHRs) and their use in decision-making systems. \nTo address these issues, future work will focus on refining the schema -based extraction \nmethodology, emphasizing improved context recognition and handling of ambiguous \nterminology. Adjustments to the schema structure and the incorporation of domain-specific \nlexicons could enhance precision. Additionally, developing post-processing validation steps \n \n147 \nthat involve clinician feedback may help filter out inaccuracies, ensuring that the structured \ndata aligns closely with clinical needs. \n6.5. General observations \nOverall, the schema -based extraction method, particularly when paired with the more \nadvanced ChatGPT-4, shows considerable promise for medical information extraction tasks. \nThe structured nature of this approach mitigates some of the challenges associated with \ncomplex and non -standardized medical texts, which were more problematic for the zero -\nshot extraction method. Furthermore, the superior performance of ChatGPT -4 across all \ntasks suggests that future work in this area would benefit from utilizing the mo st recent \niterations of LLMs, coupled with robust schema -based methodologies, to enhance the \naccuracy and reliability of medical data extraction. \nHowever, it's important to note that even with the best-performing method (schema-based \nextraction with ChatGPT -4), there is still room for improvement, particularly in handling \ncomplex, multi-word fields and ensuring consistent performance across all types of medical \nattributes. The variability in performance across different field types highlights the need for \nspecialized approaches that can adapt to the diverse nature of medical information. \nThese findings contribute valuable insights into the application of LLMs in clinical \nsettings, highlighting both their potential and the areas where additional refinement is \nnecessary. The results underscore the importance of continuous model development and the \nneed for domain-specific fine-tuning to address the unique challenges presented by medical \ndata extraction. \n7. CONCLUSIONS \nThis research demonstrates the potential of large language models like ChatGPT in \nextracting and structuring medical information from unstructured clinical texts. While the \nmodel shows considerable promise, particularly in its zero -shot capabilities, it al so \nhighlights the need for more specialized approaches to handle the unique challenges \npresented by medical data. Future work should focus on enhancing the model's ability to \nmanage complex and non -standardized text formats, as well as reducing the inciden ce of \nhallucinations. The continued development and application of LLMs in this field could \nultimately contribute to more efficient and accurate clinical data management, improving \noutcomes for healthcare providers and patients alike. \nCode and data availability \nThe code is available from the corresponding author upon request. As for the data, due \nto privacy, ethical and legal concerns, we cannot share our dataset, but can provide more \ndetails to interested parties on request. \n \n \n \n \n148 \nREFERENCES \nAgrawal, M., Hegselmann, S., Lang, H., Kim, Y., & Sontag, D. (2022). Large Language Models are few -shot \nclinical information extractors. ArXiv, abs/2205.12689. https://doi.org/10.48550/arXiv.2205.12689 \nBergomi, L., Tommaso, M., Antonazzo, P., Alberghi, L., Bellazzi, R., Preda, L., Bortolotto, C., & Parimbelli, E. (2024). \nReshaping free-text radiology notes into structured reports with generative question answering transformers. \nArtificial Intelligence in Medicine, 154, 102924. https://doi.org/10.1016/j.artmed.2024.102924 \nBhate, N., Mittal, A., He, Z., & Luo, X. (2023). Zero -shot learning with minimum instruction to extract social \ndeterminants and family history from clinical notes using GPT Model. IEEE International Conference \non Big Data (BigData) (pp. 1476-1480). IEEE. https://doi.org/10.1109/BigData59044.2023.10386811 \nHuang, J., Yang, D. M., Rong, R., Nezafati, K., Treager, C., Chi, Z., Wang, S., Cheng, X., Guo, Y., Klesse, L. J., \nXiao, G., Peterson, E. D., Zhan, X., & Xie, Y. (2024). A critical assessment of using ChatGPT for extracting \nstructured data from clinical notes. Npj Digital Medicine, 7(1), 106. https://doi.org/10.1038/s41746-024-\n01079-8 \nHuang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., & Liu, T. \n(2024). A Survey on hallucination in Large Language Models: Principles, taxonomy, challenges, and \nopen questions. ACM Transactions on Information Systems, 3703155. https://doi.org/10.1145/3703155 \nKaddari, Z., Mellah, Y., Berrich, J., Belkasmi, M. G., & Bouchentouf, T. (2021). Natural language processing: \nchallenges and future directions. In T. Masrour, I. El Hassani, & A. Cherrafi (Eds.), Artificial Intelligence \nand Industrial Applications  (Vol. 144, pp. 236 ‚Äì246). Springer International Publishing. \nhttps://doi.org/10.1007/978-3-030-53970-2_22 \nKernberg, A., Gold, J., & Mohan, V. (2024). Using ChatGPT -4 to create structured medical notes from audio \nrecordings of physician -patient encounters: Comparative study. Journal of Medical Internet Research , \n26, e54419. https://doi.org/10.2196/54419 \nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., \nRay, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, \nP., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback. \nArXiv, abs/2203.02155. https://doi.org/10.48550/arXiv.2203.02155 \nPatra, B. G., Lepow, L. A., Kasi Reddy Jagadeesh Kumar, P., Vekaria, V., Sharma, M. M., Adekkanattu, P., \nFennessy, B., Hynes, G., Landi, I., Sanchez -Ruiz, J. A., Ryu, E., Biernacka, J. M., Nadkarni, G. N., \nTalati, A., Weissman, M., Olfson, M., Mann, J. J.,  Zhang, Y., Charney, A. W., & Pathak, J. (2024). \nExtracting social support and social isolation information from clinical psychiatry notes: Comparing a \nrule-based natural language processing system and a large language model. Journal of the American \nMedical Informatics Association. https://doi.org/10.1093/jamia/ocae260 \nRay, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, \nlimitations and future scope. Internet of Things and Cyber -Physical Systems , 3, 121 -154. \nhttps://doi.org/10.1016/j.iotcps.2023.04.003 \nStraka, M., N√°plava, J., Strakov√°, J., & Samuel, D. (2021). RobeCzech: Czech RoBERTa, a monolingual \ncontextualized language representation model. In K. Ek≈°tein, F. P√°rtl, & M. Konop√≠k (Eds.), Text, \nSpeech, and Dialogue  (Vol. 12848, pp. 197 -209). Springer International Publishing. \nhttps://doi.org/10.1007/978-3-030-83527-9_17 \nTsai, R. T.-H., Wu, S.-H., Chou, W.-C., Lin, Y.-C., He, D., Hsiang, J., Sung, T.-Y., & Hsu, W.-L. (2006). Various \ncriteria in the evaluation of biomedical named entity recognition. BMC Bioinformatics , 7, 92. \nhttps://doi.org/10.1186/1471-2105-7-92 \nYifan, Y., Jinhao, D., Kaidi, X., Yuanfang, C., Zhibo, S., & Yue, Z. (2024). A survey on large language model \n(LLM) security and privacy: The Good, The Bad, and The Ugly. High-Confidence Computing, 4 (2), \n100211. https://doi.org/10.1016/j.hcc.2024.100211 \nZelina, P., Halamkova, J., & Novacek, V. (2022). Unsupervised extraction, labelling and clustering of segments \nfrom clinical notes. 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)  \n(pp. 1362-1368). IEEE. http://dx.doi.org/10.1109/BIBM55620.2022.9995229 \nZhan, X., Humbert-Droz, M., Mukherjee, P., & Gevaert, O. (2021). Structuring clinical text with AI: Old versus \nnew natural language processing techniques evaluated on eight common cardiovascular diseases. \nPatterns, 2(7), 100289. https://doi.org/10.1016/j.patter.2021.100289 ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8726483583450317
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.7237774133682251
    },
    {
      "name": "Structuring",
      "score": 0.7127693891525269
    },
    {
      "name": "Information extraction",
      "score": 0.6315762996673584
    },
    {
      "name": "Relationship extraction",
      "score": 0.48634082078933716
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.47480645775794983
    },
    {
      "name": "Information retrieval",
      "score": 0.46607017517089844
    },
    {
      "name": "Natural language processing",
      "score": 0.45853787660598755
    },
    {
      "name": "Medical information",
      "score": 0.43346428871154785
    },
    {
      "name": "Text messaging",
      "score": 0.4279231131076813
    },
    {
      "name": "Precision and recall",
      "score": 0.42098289728164673
    },
    {
      "name": "Recall",
      "score": 0.4190460443496704
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3862944543361664
    },
    {
      "name": "Data mining",
      "score": 0.33908647298812866
    },
    {
      "name": "World Wide Web",
      "score": 0.12342473864555359
    },
    {
      "name": "Linguistics",
      "score": 0.07563051581382751
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76129166",
      "name": "Premier University",
      "country": "BD"
    }
  ],
  "cited_by": 1
}