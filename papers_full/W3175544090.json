{
    "title": "Early Convolutions Help Transformers See Better",
    "url": "https://openalex.org/W3175544090",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4226777912",
            "name": "Xiao, Tete",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221790079",
            "name": "Singh, Mannat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4365304084",
            "name": "Mintun, Eric",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744191946",
            "name": "Darrell, Trevor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3024273107",
            "name": "Dollár, Piotr",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2749197864",
            "name": "Girshick, Ross",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2613718673",
        "https://openalex.org/W3034098129",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W3137278571",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2081580037",
        "https://openalex.org/W2994759459",
        "https://openalex.org/W1665214252",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2962900737",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963399829",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2804047946",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2911925209",
        "https://openalex.org/W3147352887",
        "https://openalex.org/W2622263826",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W3204076343",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3204619080",
        "https://openalex.org/W3172064272",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3203166921",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3204575409",
        "https://openalex.org/W2981563141",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2097073572",
        "https://openalex.org/W2914611487",
        "https://openalex.org/W2147800946",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2101926813",
        "https://openalex.org/W3202578961",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W3203606893",
        "https://openalex.org/W3202698164",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3194591991",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W3202863625",
        "https://openalex.org/W3202406646",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W3146097248",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W3174068320",
        "https://openalex.org/W2970389371"
    ],
    "abstract": "Vision transformer (ViT) models exhibit substandard optimizability. In particular, they are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperparameters, and training schedule length. In comparison, modern convolutional neural networks are easier to optimize. Why is this the case? In this work, we conjecture that the issue lies with the patchify stem of ViT models, which is implemented by a stride-p p*p convolution (p=16 by default) applied to the input image. This large-kernel plus large-stride convolution runs counter to typical design choices of convolutional layers in neural networks. To test whether this atypical design choice causes an issue, we analyze the optimization behavior of ViT models with their original patchify stem versus a simple counterpart where we replace the ViT stem by a small number of stacked stride-two 3*3 convolutions. While the vast majority of computation in the two ViT designs is identical, we find that this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the final model accuracy. Using a convolutional stem in ViT dramatically increases optimization stability and also improves peak performance (by ~1-2% top-1 accuracy on ImageNet-1k), while maintaining flops and runtime. The improvement can be observed across the wide spectrum of model complexities (from 1G to 36G flops) and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us to recommend using a standard, lightweight convolutional stem for ViT models in this regime as a more robust architectural choice compared to the original ViT model design.",
    "full_text": "Early Convolutions Help Transformers See Better\nTete Xiao1,2 Mannat Singh1 Eric Mintun1 Trevor Darrell2 Piotr Dollár1∗ Ross Girshick1∗\n1Facebook AI Research (FAIR) 2UC Berkeley\nAbstract\nVision transformer (ViT) models exhibit substandard optimizability. In particular,\nthey are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperpa-\nrameters, and training schedule length. In comparison, modern convolutional neural\nnetworks are easier to optimize. Why is this the case? In this work, we conjecture\nthat the issue lies with the patchify stem of ViT models, which is implemented by\na stride-pp×pconvolution (p= 16 by default) applied to the input image. This\nlarge-kernel plus large-stride convolution runs counter to typical design choices\nof convolutional layers in neural networks. To test whether this atypical design\nchoice causes an issue, we analyze the optimization behavior of ViT models with\ntheir original patchify stem versus a simple counterpart where we replace the ViT\nstem by a small number of stacked stride-two 3×3 convolutions. While the vast\nmajority of computation in the two ViT designs is identical, we ﬁnd that this small\nchange in early visual processing results in markedly different training behavior in\nterms of the sensitivity to optimization settings as well as the ﬁnal model accuracy.\nUsing a convolutional stem in ViT dramatically increases optimization stability\nand also improves peak performance (by ∼1-2% top-1 accuracy on ImageNet-1k),\nwhile maintaining ﬂops and runtime. The improvement can be observed across the\nwide spectrum of model complexities (from 1G to 36G ﬂops) and dataset scales\n(from ImageNet-1k to ImageNet-21k). These ﬁndings lead us to recommend using\na standard, lightweight convolutional stem for ViT models in this regime as a more\nrobust architectural choice compared to the original ViT model design.\n1 Introduction\nVision transformer (ViT) models [13] offer an alternative design paradigm to convolutional neural\nnetworks (CNNs) [24]. ViTs replace the inductive bias towards local processing inherent in con-\nvolutions with global processing performed by multi-headed self-attention [ 43]. The hope is that\nthis design has the potential to improve performance on vision tasks, akin to the trends observed\nin natural language processing [11]. While investigating this conjecture, researchers face another\nunexpected difference between ViTs and CNNs: ViT models exhibit substandard optimizability.\nViTs are sensitive to the choice of optimizer [41] (AdamW [27] vs. SGD), to the selection of dataset\nspeciﬁc learning hyperparameters [13, 41], to training schedule length, to network depth [42], etc.\nThese issues render former training recipes and intuitions ineffective and impede research.\nConvolutional neural networks, in contrast, are exceptionally easy and robust to optimize. Simple\ntraining recipes based on SGD, basic data augmentation, and standard hyperparameter values have\nbeen widely used for years [19]. Why does this difference exist between ViT and CNN models? In\nthis paper we hypothesize that the issues lies primarily in the early visual processing performed by\nViT. ViT “patchiﬁes” the input image into p×pnon-overlapping patches to form the transformer\nencoder’s input set. Thispatchify stem is implemented as a stride-pp×pconvolution, with p= 16\nas a default value. This large-kernel plus large-stride convolution runs counter to the typical design\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.14881v3  [cs.CV]  25 Oct 2021\n16x16 conv, stride 163x3 conv, stride 1 or 2\n1x1 conv, stride 1\nNorm\nMulti-Head Attention\nNormMLP+ +(L-1) ×\nNorm\nMulti-Head Attention\nNormMLP+ +L ×\nOurs (termed ViTC, same runtime):üRobust tolrand wd choiceüConverges quicklyüWorks with AdamW, and alsoSGDüOutperformssotaCNNs on ImageNet\nOriginal ViT(baseline, termed ViTP):oSensitive tolrand wd choiceoConverges slowlyoWorks with AdamW, but not SGDoUnderperformssotaCNNs on ImageNet\npatchify(P) stem\n×nconvolutional (C) stem\nstem flops ≈1 transformer block\ntransformer block\ntransformer block\nFigure 1: Early convolutions help transformers see better: We hypothesize that the substandard\noptimizability of ViT models compared to CNNs primarily arises from the early visual processing\nperformed by its patchify stem, which is implemented by a non-overlapping stride-pp×pconvolution,\nwith p= 16 by default. We minimally replace the patchify stem in ViT with a standard convolutional\nstem of only ∼5 convolutions that has approximately the same complexity as a single transformer\nblock. We reduce the number of transformer blocks by one ( i.e., L− 1 vs. L) to maintain parity\nin ﬂops, parameters, and runtime. We refer to the resulting model as ViTC and the original ViT as\nViTP . The vast majority of computation performed by these two models is identical, yet surprisingly\nwe observe that ViTC (i) converges faster, (ii) enables, for the ﬁrst time, the use of either AdamW\nor SGD without a signiﬁcant accuracy drop, (iii) shows greater stability to learning rate and weight\ndecay choice, and (iv) yields improvements in ImageNet top-1 error allowing ViTC to outperform\nstate-of-the-art CNNs, whereas ViTP does not.\nchoices used in CNNs, where best-practices have converged to a small stack of stride-two3×3 kernels\nas the network’s stem (e.g., [30, 36, 39]).\nTo test this hypothesis, we minimally change the early visual processing of ViT by replacing its\npatchify stem with a standard convolutional stem consisting of only ∼5 convolutions, see Figure 1.\nTo compensate for the small addition in ﬂops, we remove one transformer block to maintain parity in\nﬂops and runtime. We observe that even though the vast majority of the computation in the two ViT\ndesigns is identical, this small change in early visual processing results in markedly different training\nbehavior in terms of the sensitivity to optimization settings as well as the ﬁnal model accuracy.\nIn extensive experiments we show that replacing the ViT patchify stem with a more standard\nconvolutional stem (i) allows ViT to converge faster (§5.1), (ii) enables, for the ﬁrst time, the\nuse of either AdamW or SGD without a signiﬁcant drop in accuracy (§5.2), (iii) brings ViT’s\nstability w.r.t. learning rate and weight decay closer to that of modern CNNs (§5.3), and (iv) yields\nimprovements in ImageNet [10] top-1 error of ∼1-2 percentage points (§6). We consistently observe\nthese improvements across a wide spectrum of model complexities (from 1G ﬂops to 36G ﬂops) and\ndataset scales (ImageNet-1k to ImageNet-21k).\nThese results show that injecting some convolutional inductive bias into ViTs can be beneﬁcial under\ncommonly studied settings. We did not observe evidence that the hard locality constraint in early\nlayers hampers the representational capacity of the network, as might be feared [ 9]. In fact we\nobserved the opposite, as ImageNet results improve even with larger-scale models and larger-scale\ndata when using a convolution stem. Moreover, under carefully controlled comparisons, we ﬁnd that\nViTs are only able to surpass state-of-the-art CNNs when equipped with a convolutional stem (§6).\nWe conjecture that restricting convolutions in ViT toearly visual processing may be a crucial design\nchoice that strikes a balance between (hard) inductive biases and the representation learning ability of\ntransformer blocks. Evidence comes by comparison to the “hybrid ViT” presented in [13], which\nuses 40 convolutional layers (most of a ResNet-50) and shows no improvement over the default ViT.\nThis perspective resonates with the ﬁndings of [9], who observe that early transformer blocks prefer\nto learn more local attention patterns than later blocks. Finally we note that exploring the design\nof hybrid CNN/ViT models is not a goal of this work; rather we demonstrate that simply using a\nminimal convolutional stem with ViT is sufﬁcient to dramatically change its optimization behavior.\nIn summary, the ﬁndings presented in this paper lead us to recommend using a standard, lightweight\nconvolutional stem for ViT models in the analyzed dataset scale and model complexity spectrum as a\nmore robust and higher performing architectural choice compared to the original ViT model design.\n2\n2 Related Work\nConvolutional neural networks (CNNs). The breakthrough performance of the AlexNet [ 23]\nCNN [ 15, 24] on ImageNet classiﬁcation [ 10] transformed the ﬁeld of recognition, leading to\nthe development of higher performing architectures, e.g., [19, 36, 37, 48], and scalable training\nmethods [16, 21]. These architectures are now core components in object detection ( e.g., [34]),\ninstance segmentation (e.g., [18]), and semantic segmentation (e.g., [26]). CNNs are typically trained\nwith stochastic gradient descent (SGD) and are widely considered to be easy to optimize.\nSelf-attention in vision models. Transformers [43] are revolutionizing natural language processing\nby enabling scalable training. Transformers use multi-headed self-attention, which performs global\ninformation processing and is strictly more general than convolution [ 6]. Wang et al. [46] show\nthat (single-headed) self-attention is a form of non-local means [ 2] and that integrating it into a\nResNet [19] improves several tasks. Ramachandran et al. [32] explore this direction further with\nstand-alone self-attention networks for vision. They report difﬁculties in designing an attention-based\nnetwork stem and present a bespoke solution that avoids convolutions. In contrast, we demonstrate\nthe beneﬁts of a convolutional stem. Zhao et al. [53] explore a broader set of self-attention operations\nwith hard-coded locality constraints, more similar to standard CNNs.\nVision transformer (ViT).Dosovitskiy et al. [13] apply a transformer encoder to image classiﬁcation\nwith minimal vision-speciﬁc modiﬁcations. As the counterpart of input token embeddings, they\npartition the input image into, e.g., 16×16 pixel, non-overlapping patches and linearly project them\nto the encoder’s input dimension. They report lackluster results when training on ImageNet-1k,\nbut demonstrate state-of-the-art transfer learning when using large-scale pretraining data. ViTs are\nsensitive to many details of the training recipe, e.g., they beneﬁt greatly from AdamW [27] compared\nto SGD and require careful learning rate and weight decay selection. ViTs are generally considered\nto be difﬁcult to optimize compared to CNNs (e.g., see [13, 41, 42]). Further evidence of challenges\ncomes from Chen et al. [4] who report ViT optimization instability in self-supervised learning (unlike\nwith CNNs), and ﬁnd that freezing the patchify stem at its random initialization improves stability.\nViT improvements. ViTs are gaining rapid interest in part because they may offer a novel direction\naway from CNNs. Touvron et al. [41] show that with more regularization and stronger data aug-\nmentation ViT models achieve competitive accuracy on ImageNet-1k alone (cf . [13]). Subsequently,\nworks concurrent with our own explore numerous other ViT improvements. Dominant themes include\nmulti-scale networks [14, 17, 25, 45, 50], increasing depth [42], and locality priors [5, 9, 17, 47, 49].\nIn [9], d’Ascoliet al. modify multi-head self-attention with a convolutional bias at initialization and\nshow that this prior improves sample efﬁciency and ImageNet accuracy. Resonating with our work,\n[5, 17, 47, 49] present models with convolutional stems, but do not analyze optimizability (our focus).\nDiscussion. Unlike the concurrent work on locality priors in ViT, our focus is studyingoptimizability\nunder minimal ViT modiﬁcations in order to derive crisp conclusions. Our perspective brings several\nnovel observations: by adding only ∼5 convolutions to the stem, ViT can be optimized well with\neither AdamW or SGD (cf . all prior works use AdamW to avoid large drops in accuracy [ 41]), it\nbecomes less sensitive to the speciﬁc choice of learning rate and weight decay, and training converges\nfaster. We also observe a consistent improvement in ImageNet top-1 accuracy across a wide spectrum\nof model complexities (1G ﬂops to 36G ﬂops) and dataset scales (ImageNet-1k to ImageNet-21k).\nThese results suggest that a (hard) convolutional bias early in the network does not compromise\nrepresentational capacity, as conjectured in [9], and is beneﬁcial within the scope of this study.\n3 Vision Transformer Architectures\nNext, we review vision transformers [13] and describe the convolutional stems used in our work.\nThe vision transformer (ViT).ViT ﬁrst partitions an input image into non-overlapping p×ppatches\nand linearly projects each patch to a d-dimensional feature vector using a learned weight matrix. A\npatch size of p= 16 and an image size of 224×224 are typical. The resulting patch embeddings (plus\npositional embeddings and a learned classiﬁcation token embedding) are processed by a standard\ntransformer encoder [43, 44] followed by a classiﬁcation head. Using common network nomenclature,\nwe refer to the portion of ViT before the transformer blocks as the network’sstem. ViT’s stem is a\n3\nmodel ref hidden MLP num numﬂops params acts time\nmodel size mult heads blocks(B) (M) (M) (min)\nViTP-1GF ∼ViT-T192 3 3 12 1.1 4.8 5.5 2.6\nViTP-4GF ∼ViT-S384 3 6 12 3.9 18.5 11.1 3.8\nViTP-18GF=ViT-B768 4 12 12 17.5 86.7 24.0 11.5\nViTP-36GF3\n5ViT-L1024 4 16 14 35.9 178.4 37.3 18.8\nmodel hidden MLP num numﬂops params acts time\nsize mult heads blocks(B) (M) (M) (min)\nViTC-1GF 192 3 3 11 1.1 4.6 5.7 2.7\nViTC-4GF 384 3 6 11 4.0 17.8 11.3 3.9\nViTC-18GF 768 4 12 11 17.7 81.6 24.1 11.4\nViTC-36GF1024 4 16 13 35.0 167.8 36.7 18.6\nTable 1: Model deﬁnitions: Left: Our ViTP models at various complexities, which use the original\npatchify stem and closely resemble the original ViT models [ 13]. To facilitate comparisons with\nCNNs, we modify the original ViT-Tiny, -Small, -Base, -Large models to obtain models at 1GF, 4GF,\n18GF, and 36GF, respectively. The modiﬁcations are indicated in blue and include reducing the MLP\nmultiplier from 4× to 3× for the 1GF and 4GF models, and reducing the number of transformer\nblocks from 24 to 14 for the 36GF model. Right: Our ViTC models at various complexities that use\nthe convolutional stem. The only additional modiﬁcation relative to the corresponding ViTP models\nis the removal of 1 transformer block to compensate for the increased ﬂops of the convolutional stem.\nWe show complexity measures for all models (ﬂops, parameters, activations, and epoch training time\non ImageNet-1k); the corresponding ViTP and ViTC models match closely on all metrics.\nspeciﬁc case of convolution (stride-p, p×pkernel), but we will refer to it as the patchify stem and\nreserve the terminology of convolutional stem for stems with a more conventional CNN design with\nmultiple layers of overlapping convolutions (i.e., with stride smaller than the kernel size).\nViTP models. Prior work proposes ViT models of various sizes, such as ViT-Tiny, ViT-Small,\nViT-Base,etc. [13, 41]. To facilitate comparisons with CNNs, which are typically standardized to 1\ngigaﬂop (GF), 2GF, 4GF, 8GF,etc., we modify the original ViT models to obtain models at about\nthese complexities. Details are given in Table 1 (left). For easier comparison with CNNs of similar\nﬂops, and to avoid subjective size names, we refer the models by their ﬂops, e.g., ViTP -4GF in place\nof ViT-Small. We use theP subscript to indicate that these models use the original patchify stem.\nConvolutional stem design. We adopt a typical minimalist convolutional stem design by stacking\n3×3 convolutions [36], followed by a single 1×1 convolution at the end to match the d-dimensional\ninput of the transformer encoder. These stems quickly downsample a 224×224 input image using\noverlapping strided convolutions to 14×14, matching the number of inputs created by the standard\npatchify stem. We follow a simple design pattern: all 3×3 convolutions either have stride 2 and\ndouble the number of output channels or stride 1 and keep the number of output channels constant.\nWe enforce that the stem accounts for approximately the computation of one transformer block of\nthe corresponding model so that we can easily control for ﬂops by removing one transformer block\nwhen using the convolutional stem instead of the patchify stem. Our stem design was chosen to be\npurposefully simple and we emphasize that it was not designed to maximize model accuracy.\nViTC models. To form a ViT model with a convolutional stem, we simply replace the patchify stem\nwith its counterpart convolutional stem and remove one transformer block to compensate for the\nconvolutional stem’s extra ﬂops (see Figure 1). We refer to the modiﬁed ViT with a convolutional stem\nas ViTC. Conﬁgurations for ViTC at various complexities are given in Table 1 (right); corresponding\nViTP and ViTC models match closely on all complexity metrics including ﬂops and runtime.\nConvolutional stem details. Our convolutional stem designs use four, four, and six3×3 convolutions\nfor the 1GF, 4GF, and 18GF models, respectively. The output channels are [24, 48, 96, 192], [48,\n96, 192, 384], and [64, 128, 128, 256, 256, 512], respectively. All 3×3 convolutions are followed by\nbatch norm (BN) [21] and then ReLU [29], while the ﬁnal 1×1 convolution is not, to be consistent\nwith the original patchify stem. Eventually, matching stem ﬂops to transformer block ﬂops results in\nan unreasonably large stem, thus ViTC-36GF uses the same stem as ViTC-18GF.\nConvolutions in ViT.Dosovitskiy et al. [13] also introduced a “hybrid ViT” architecture that blends\na modiﬁed ResNet [19] (BiT-ResNet [22]) with a transformer encoder. In their hybrid model, the\npatchify stem is replaced by a partial BiT-ResNet-50 that terminates at the output of the conv4 stage\nor the output of an extended conv3 stage. These image embeddings replace the standard patchify\nstem embeddings. This partial BiT-ResNet-50 stem is deep, with 40 convolutional layers. In this\nwork, we explore lightweight convolutional stems that consist of only 5 to 7 convolutions in total,\ninstead of the 40 used by the hybrid ViT. Moreover, we emphasize that the goal of our work isnot to\nexplore the hybrid ViT design space, but rather to study the optimizability effects of simply replacing\nthe patchify stem with a minimal convolutional stem that follows standard CNN design practices.\n4\n4 Measuring Optimizability\nIt has been noted in the literature that ViT models are challenging to optimize, e.g., they may achieve\nonly modest performance when trained on a mid-size dataset (ImageNet-1k) [13], are sensitive to data\naugmentation [41] and optimizer choice [41], and may perform poorly when made deeper [42]. We\nempirically observed the general presence of such difﬁculties through the course of our experiments\nand informally refer to such optimization characteristics collectively as optimizability.\nModels with poor optimizability can yield very different results when hyperparameters are varied,\nwhich can lead to seemingly bizarre observations, e.g., removing erasing data augmentation [54]\ncauses a catastrophic drop in ImageNet accuracy in [41]. Quantitative metrics to measure optimiz-\nability are needed to allow for more robust comparisons. In this section, we establish the foundations\nof such comparisons; we extensively test various models using these optimizability measures in §5.\nTraining length stability. Prior works train ViT models for lengthy schedules, e.g., 300 to 400\nepochs on ImageNet is typical (at the extreme, [17] trains models for 1000 epochs), since results at a\nformerly common 100-epoch schedule are substantially worse (2-4% lower top-1 accuracy, see §5.1).\nIn the context of ImageNet, we deﬁne top-1 accuracy at 400 epochs as an approximate asymptotic\nresult, i.e., training for longer will not meaningfully improve top-1 accuracy, and we compare it to the\naccuracy of models trained for only 50, 100, or 200 epochs. We deﬁne training length stability as the\ngap to asymptotic accuracy. Intuitively, it’s a measure of convergence speed. Models that converge\nfaster offer obvious practical beneﬁts, especially when training many model variants.\nOptimizer stability. Prior works use AdamW [27] to optimize ViT models from random initialization.\nResults of SGD are not typically presented and we are only aware of Touvron et al. [41]’s report of a\ndramatic ∼7% drop in ImageNet top-1 accuracy. In contrast, widely used CNNs, such as ResNets, can\nbe optimized equally well with either SGD or AdamW (see §5.2) and SGD (always with momentum)\nis typically used in practice. SGD has the practical beneﬁt of having fewer hyperparameters ( e.g.,\ntuning AdamW’sβ2 can be important [3]) and requiring 50% less optimizer state memory, which\ncan ease scaling. We deﬁne optimizer stability as the accuracy gap between AdamW and SGD. Like\ntraining length stability, we use optimizer stability as a proxy for the ease of optimization of a model.\nHyperparameter (lr, wd) stability. Learning rate (lr) and weight decay (wd) are among the most\nimportant hyperparameters governing optimization with SGD and AdamW. New models and datasets\noften require a search for their optimal values as the choice can dramatically affect results. It is\ndesirable to have a model and optimizer that yield good results for a wide range of learning rate\nand weight decay values. We will explore this hyperparameter stability by comparing the error\ndistribution functions (EDFs) [ 30] of models trained with various choices of lr and wd. In this\nsetting, to create an EDF for a model we randomly sample values of lr and wd and train the model\naccordingly. Distributional estimates, like those provided by EDFs, give a more complete view of the\ncharacteristics of models that point estimates cannot reveal [30, 31]. We will review EDFs in §5.3.\nPeak performance. The maximum possible performance of each model is the most commonly used\nmetric in previous literature and it is often provided without carefully controlling training details\nsuch as data augmentations, regularization methods, number of epochs, and lr, wd tuning. To make\nmore robust comparisons, we deﬁne peak performance as the result of a model at 400 epochs using\nits best-performing optimizer and parsimoniously tuned lr and wd values (details in §6), while ﬁxing\njustiﬁably good values for all other variables that have a known impact on training. Peak performance\nresults for ViTs and CNNs under these carefully controlled training settings are presented in §6.\n5 Stability Experiments\nIn this section we test the stability of ViT models with the original patchify ( P) stem vs. the\nconvolutional (C) stem deﬁned in §3. For reference, we also train RegNetY [12, 31], a state-of-the-art\nCNN that is easy to optimize and serves as a reference point for good stability.\nWe conduct experiments using ImageNet-1k [10]’s standard training and validation sets, and report\ntop-1 error. Following [12], for all results, we carefully control training settings and we use a minimal\nset of data augmentations that still yields strong results, for details see §5.4. In this section, unless\nnoted, for each model we use the optimal lr and wd found under a 50 epoch schedule (see Appendix).\n5\n50 100 200 400\ntraining epochs\n0\n2\n4\n6\n8\n10top-1 error\n1GF models\nViTP\nViTC\nRegNetY\n50 100 200 400\ntraining epochs\n4GF models\nViTP\nViTC\nRegNetY\n50 100 200 400\ntraining epochs\n18GF models\nViTP\nViTC\nRegNetY\nname\nFigure 2: Training length stability: We train 9 models for 50 to 400 epochs on ImageNet-1k and plot\nthe ∆top-1 error to the 400 epoch result for each. ViTC demonstrates faster convergence than ViTP\nacross the model complexity spectrum, and helps close the gap to CNNs (represented by RegNetY).\n50 100 200 400\ntraining epochs\n15\n20\n25\n30\n35\n40\n45\n50top-1 error\n1GF models\nViTP\nViTC\nRegNetY\n50 100 200 400\ntraining epochs\n4GF models\nViTP\nViTC\nRegNetY\n50 100 200 400\ntraining epochs\n18GF models\nViTP\nViTC\nRegNetY\nname\nFigure 3: Optimizer stability: We train each model for 50 to 400 epochs with AdamW (upward\ntriangle ▲) and SGD (downward triangle ▼). For the baseline ViTP , SGD yields signiﬁcantly worse\nresults than AdamW. In contrast, ViTC and RegNetY models exhibit a much smaller gap between\nSGD and AdamW across all settings. Note that for long schedules, ViTP often fails to converge with\nSGD (i.e., loss goes to NaN), in such cases we copy the best results from a shorter schedule of the\nsame model (and show the results via a dashed line).\n5.1 Training Length Stability\nWe ﬁrst explore how rapidly networks converge to their asymptotic error on ImageNet-1k,i.e., the\nhighest possible accuracy achievable by training for many epochs. We approximate asymptotic error\nas a model’s error using a 400 epoch schedule based on observing diminishing returns from 200 to\n400. We consider a grid of 24 experiments for ViT: {P, C} stems × {1, 4, 18} GF model sizes ×\n{50, 100, 200, 400} epochs. For reference we also train RegNetY at {1, 4, 16} GF. We use the best\noptimizer choice for each model (AdamW for ViT models and SGD for RegNetY models).\nResults. Figure 2 shows the absolute errordeltas (∆top-1) between 50, 100, and 200 epoch schedules\nand asymptotic performance (at 400 epochs). ViT C demonstrates faster convergence than ViTP\nacross the model complexity spectrum, and closes much of the gap to the rate of CNN convergence.\nThe improvement is most signiﬁcant in the shortest training schedule (50 epoch), e.g., ViTP -1GF has\na 10% error delta, while ViTC-1GF reduces this to about 6%. This opens the door to applications\nthat execute a large number of short-scheduled experiments, such as neural architecture search.\n5.2 Optimizer Stability\nWe next explore how well AdamW and SGD optimize ViT models with the two stem types. We\nconsider the following grid of 48 ViT experiments: {P, C} stems × {1, 4, 18} GF sizes × {50, 100,\n200, 400} epochs × {AdamW, SGD} optimizers. As a reference, we also train 24 RegNetY baselines,\none for each complexity regime, epoch length, and optimizer.\nResults. Figure 3 shows the results. As a baseline, RegNetY models show virtually no gap when\ntrained using either SGD or AdamW (the difference ∼0.1-0.2% is within noise). On the other hand,\nViTP models suffer a dramatic drop when trained with SGD across all settings (of up to 10% for\nlarger models and longer training schedules). With a convolutional stem, ViTC models exhibit much\nsmaller error gaps between SGD and AdamW across all training schedules and model complexities,\nincluding in larger models and longer schedules, where the gap is reduced to less than 0.2%. In other\nwords, both RegNetY and ViTC can be easily trained via either SGD or AdamW, but ViTP cannot.\n6\n20 24 28 32 36 40\ntop-1 error\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0cumulative prob.\n1GF models\nViTP\nViTC\nRegNetY\n20 24 28 32 36 40\ntop-1 error\n4GF models\nViTP\nViTC\nRegNetY\n20 24 28 32 36 40\ntop-1 error\n18GF models\nViTP\nViTC\nRegNetY\n0 2 4 6 8 10\ntop-1 error\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0cumulative prob.\n1GF models\nViTP\nViTC\nRegNetY\n0 2 4 6 8 10\ntop-1 error\n4GF models\nViTP\nViTC\nRegNetY\n0 2 4 6 8 10\ntop-1 error\n18GF models\nViTP\nViTC\nRegNetY\nFigure 4: Hyperparameter stability for AdamW (lr and wd): For each model, we train 64 instances\nof the model for 50 epochs each with a randomlr and wd (in a ﬁxed width interval around the optimal\nvalue for each model). Top: Scatterplots of the lr, wd, and lr ·wd for three 4GF models. Vertical bars\nindicate optimal lr, wd, and lr ·wd values for each model. Bottom: For each model, we generate an\nEDF of the errors by plotting the cumulative distribution of the ∆top-1 errors (∆ to the optimal error\nfor each model). A steeper EDF indicates better stability to lr and wd variation. ViTC signiﬁcantly\nimproves the stability over the baseline ViTP across the model complexity spectrum, and matches or\neven outperforms the stability of the CNN model (RegNetY).\n20 24 28 32 36 40\ntop-1 error\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0cumulative prob.\n1GF models\nViTP\nViTC\nRegNetY\n20 24 28 32 36 40\ntop-1 error\n4GF models\nViTP\nViTC\nRegNetY\n20 24 28 32 36 40\ntop-1 error\n18GF models\nViTP\nViTC\nRegNetY\n0 2 4 6 8 10\ntop-1 error\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0cumulative prob.\n1GF models\nViTP\nViTC\nRegNetY\n0 2 4 6 8 10\ntop-1 error\n4GF models\nViTP\nViTC\nRegNetY\n0 2 4 6 8 10\ntop-1 error\n18GF models\nViTP\nViTC\nRegNetY\nFigure 5: Hyperparameter stability for SGD (lr and wd): We repeat the setup from Figure 4 using\nSGD instead of AdamW. The stability improvement of ViTC over the baseline ViTP is even larger\nthan with AdamW. E.g., ∼60% of ViTC-18GF models are within 4% ∆top-1 error of the best result,\nwhile less than 20% of ViTP -18GF models are (in fact most ViTP -18GF runs don’t converge).\n5.3 Learning Rate and Weight Decay Stability\nNext, we characterize how sensitive different model families are to changes in learning rate (lr) and\nweight decay (wd) under both AdamW and SGD optimizers. To quantify this, we make use of error\ndistribution functions (EDFs) [30]. An EDF is computed by sorting a set of results from low-to-high\nerror and plotting the cumulative proportion of results as error increases, see [ 30] for details. In\nparticular, we generate EDFs of a model as a function of lr and wd. The intuition is that if a model is\nrobust to these hyperparameter choices, the EDF will be steep (all models will perform similarly),\nwhile if the model is sensitive, the EDF will be shallow (performance will be spread out).\nWe test 6 ViT models ({P, C} × {1, 4, 18} GF) and 3 RegNetY models ({1, 4, 16} GF). For each\nmodel and each optimizer, we compute an EDF by randomly sampling 64 (lr, wd) pairs with learning\nrate and weight decay sampled in a ﬁxed width interval around their optimal values for that model\nand optimizer (see the Appendix for sampling details). Rather than plotting absolute error in the EDF,\nwe plot ∆top-1 error between the best result (obtained with the optimal lr and wd) and the observed\nresult. Due to the large number of models, we train each for only 50 epochs.\nResults. Figure 4 shows scatterplots and EDFs for models trained by AdamW. Figure 5 shows SGD\nresults. In all cases we see that ViTC signiﬁcantly improves the lr and wd stability over ViTP for\nboth optimizers. This indicates that the lr and wd are easier to optimize for ViTC than for ViTP .\n7\n5.4 Experimental Details\nIn all experiments we train with a single half-period cosine learning rate decay schedule with a\n5-epoch linear learning rate warm-up [16]. We use a minibatch size of 2048. Crucially, weight decay\nis not applied to the gain factors found in normalization layers nor to bias parameters anywhere in the\nmodel; we found that decaying these parameters can dramatically reduce top-1 accuracy for small\nmodels and short schedules. For inference, we use an exponential moving average (EMA) of the\nmodel weights (e.g., [8]). The lr and wd used in this section are reported in the Appendix. Other\nhyperparameters use defaults: SGD momentum is 0.9 and AdamW’sβ1 = 0.9 and β2 = 0.999.\nRegularization and data augmentation. We use a simpliﬁed training recipe compared to recent\nwork such as DeiT [41], which we found to be equally effective across a wide spectrum of model\ncomplexities and dataset scales. We use AutoAugment [ 7], mixup [ 52] (α = 0.8), CutMix [ 51]\n(α= 1.0), and label smoothing [38] (ϵ= 0.1). We prefer this setup because it is similar to common\nsettings for CNNs (e.g., [ 12]) except for stronger mixup and the addition of CutMix (ViTs beneﬁt\nfrom both, while CNNs are not harmed). We compare this recipe to the one used for DeiT models in\nthe Appendix, and observe that our setup provides substantially faster training convergencelikely\nbecause we remove repeating augmentation [1, 20], which is known to slow training [1].\n6 Peak Performance\nA model’s peak performance is the most commonly used metric in network design. It represents\nwhat is possible with the best-known-so-far settings and naturally evolves over time. Making fair\ncomparisons between different models is desirable but fraught with difﬁculty. Simply citing results\nfrom prior work may be negatively biased against that work as it was unable to incorporate newer,\nyet applicable improvements. Here, we strive to provide a fairer comparison between state-of-the-art\nCNNs, ViTP , and ViTC. We identify a set of factors and then strike a pragmatic balance between\nwhich subset to optimize for each model vs. which subset share a constant value across all models.\nIn our comparison, all models share the same epochs (400), use of model weight EMA, and set of\nregularization and augmentation methods (as speciﬁed in §5.4). All CNNs are trained with SGD with\nlr of 2.54 and wd of 2.4e−5; we found this single choice worked well across all models, as similarly\nobserved in [12]. For all ViT models we found AdamW with a lr/wd of 1.0e−3/0.24 was effective,\nexcept for the 36GF models. For these larger models we tested a few settings and found a lr/wd of\n6.0e−4/0.28 to be more effective for both ViTP -36GF and ViTC-36GF models. For training and\ninference, ViTs use 224×224 resolution (we do not ﬁne-tune at higher resolutions), while the CNNs\nuse (often larger) optimized resolutions speciﬁed in [12, 39]. Given this protocol, we compare ViTP ,\nViTC, and CNNs across a spectrum of model complexities (1GF to 36GF) and dataset scales (directly\ntraining on ImageNet-1k vs. pretraining on ImageNet-21k and then ﬁne-tuning on ImageNet-1k).\nResults. Figure 6 shows a progression of results. Each plot shows ImageNet-1k val top-1 error vs.\nImageNet-1k epoch training time.1 The left plot compares several state-of-the-art CNNs. RegNetY\nand RegNetZ [12] achieve similar results across the training speed spectrum and outperform Efﬁ-\ncientNets [39]. Surprisingly, ResNets [19] are highly competitive at fast runtimes, showing that under\na fairer comparison these years-old models perform substantially better than often reported (cf . [39]).\nThe middle plot compares two representative CNNs (ResNet and RegNetY) to ViTs, still using\nonly ImageNet-1k training. The baseline ViT P underperforms RegNetY across the entire model\ncomplexity spectrum. To our surprise, ViTP also underperforms ResNets in this regime. ViTC is\nmore competitive and outperforms CNNs in the middle-complexity range.\nThe right plot compares the same models but with ImageNet-21k pretraining (details in Appendix).\nIn this setting ViT models demonstrates a greater capacity to beneﬁt from the larger-scale data:\nnow ViTC strictly outperforms both ViTP and RegNetY . Interestingly,the original ViTP does not\noutperform a state-of-the-art CNN even when trained on this much larger dataset. Numerical results\nare presented in Table 2 for reference to exact values. This table also highlights that ﬂop counts are\nnot signiﬁcantly correlated with runtime, but that activations are (see Appendix for more details), as\nalso observed by [12]. E.g., EfﬁcientNets are slow relative to their ﬂops while ViTs are fast.\n1We time models in PyTorch on 8 32GB V olta GPUs. We note that batch inference time is highly correlated\nwith training time, but we report epoch time as it is easy to interpret and does not depend on the use case.\n8\n2.5 5.0 10 20 40\ntraining speed (min.)\n14\n16\n18\n20\n22\n24top-1 error\nResNet\nRegNetY\nRegNetZ\nEfficientNet\n2.5 5.0 10 20 40\ntraining speed (min.)\nViTP\nViTC\nResNet\nRegNetY\n2.5 5.0 10 20 40\ntraining speed (min.)\nImageNet 21k\nViTP\nViTC\nResNet\nRegNetY\nname\nFigure 6: Peak performance (epoch training time vs. ImageNet-1k val top-1 error) : Results\nof a fair, controlled comparison of ViT P , ViTC, and CNNs. Each curve corresponds to a model\ncomplexity sweep resulting in a training speed spectrum (minutes per ImageNet-1k epoch). Left:\nState-of-the-art CNNs. Equipped with a modern training recipe, ResNets are highly competitive in\nthe faster regime, while RegNetY and Z perform similarly, and better than EfﬁcientNets. Middle:\nSelected CNNs compared to ViTs. With access to only ImageNet-1k training data, RegNetY and\nResNet outperform ViTP across the board. ViTC is more competitive with CNNs. Right: Pretraining\non ImageNet-21k improves the ViT models more than the CNNs, making ViTP competitive. Here,\nthe proposed ViTC outperforms all other models across the full training speed spectrum.\nmodel ﬂops params acts time batch epochs IN\n(B) (M) (M) (min) size 100 200 400 21k\nResNet-50 4.1 25.6 11.3 3.4 2048 22.5 21.2 20.721.6\nResNet-101 7.8 44.5 16.4 5.5 2048 20.3 19.1 18.519.2\nResNet-152 11.5 60.2 22.8 7.7 2048 19.5 18.4 17.718.2\nResNet-200 15.0 64.7 32.3 10.7 102419.5 18.3 17.617.7\nRegNetY-1GF1.0 9.6 6.2 3.1 2048 23.2 22.2 21.5-\nRegNetY-4GF4.1 22.4 14.5 7.6 2048 19.4 18.3 17.918.4\nRegNetY-16GF15.5 72.3 30.7 17.9 102417.1 16.4 16.315.6\nRegNetY-32GF31.1 128.6 46.2 35.1 51216.2 15.9 15.915.0\nRegNetZ-1GF1.0 11.0 8.8 4.2 2048 20.8 20.2 19.6-\nRegNetZ-4GF4.0 28.1 24.3 12.9 1024 17.4 16.9 16.6-\nRegNetZ-16GF16.0 95.3 51.3 32.0 512 16.0 15.9 15.9-\nRegNetZ-32GF32.0 175.1 79.6 55.3 25616.3 16.2 16.1-\nmodel ﬂops params acts time batch epochs IN\n(B) (M) (M) (min) size 100 200 400 21k\nEffNet-B2 1.0 9.1 13.8 5.9 2048 21.4 20.5 19.9-\nEffNet-B4 4.4 19.3 49.5 19.4 512 18.5 17.8 17.5-\nEffNet-B510.3 30.4 98.9 41.7 256 17.3 17.0 17.0-\nViTP-1GF 1.1 4.8 5.5 2.6 2048 33.2 29.7 27.7-\nViTP-4GF 3.9 18.5 11.1 3.8 2048 23.3 20.8 19.620.6\nViTP-18GF17.5 86.6 24.0 11.5 102419.9 18.4 17.916.4\nViTP-36GF35.9 178.4 37.3 18.8 51219.9 18.8 18.215.1\nViTC-1GF 1.1 4.6 5.7 2.7 2048 28.6 26.1 24.7-\nViTC-4GF 4.0 17.8 11.3 3.9 2048 20.9 19.2 18.618.8\nViTC-18GF17.7 81.6 24.1 11.4 102418.4 17.5 17.015.1\nViTC-36GF35.0 167.8 36.7 18.6 51218.3 17.6 16.814.2\nTable 2: Peak performance (grouped by model family): Model complexity and validation top-1\nerror at 100, 200, and 400 epoch schedules on ImageNet-1k, and the top-1 error after pretraining\non ImageNet-21k (IN 21k) and ﬁne-tuning on ImageNet-1k. This table serves as reference for the\nresults shown in Figure 6. Blue numbers: best model trainable under 20 minutes per ImageNet-1k\nepoch. Batch sizes and training times are reported normalized to 8 32GB V olta GPUs (see Appendix).\nAdditional results on the ImageNet-V2 [33] test set are presented in the Appendix.\nThese results verify that ViTC’s convolutional stem improves not only optimization stability, as seen in\nthe previous section, but also peak performance. Moreover, this beneﬁt can be seen across the model\ncomplexity and dataset scale spectrum. Perhaps surprisingly, given the recent excitement over ViT,\nwe ﬁnd that ViTP struggles to compete with state-of-the-art CNNs. We only observe improvements\nover CNNs when using both large-scale pretraining data and the proposed convolutional stem.\n7 Conclusion\nIn this work we demonstrated that the optimization challenges of ViT models are linked to the large-\nstride, large-kernel convolution in ViT’s patchify stem. The seemingly trivial change of replacing this\npatchify stem with a simple convolutional stem leads to a remarkable change in optimization behavior.\nWith the convolutional stem, ViT (termed ViT C) converges faster than the original ViT (termed\nViTP ) (§5.1), trains well with either AdamW or SGD (§5.2), improves learning rate and weight decay\nstability (§5.3), and improves ImageNet top-1 error by ∼1-2% (§6). These results are consistent\nacross a wide spectrum of model complexities (1GF to 36GF) and dataset scales (ImageNet-1k to\nImageNet-21k). Our results indicate that injecting a small dose of convolutional inductive bias into\nthe early stages of ViTs can be hugely beneﬁcial. Looking forward, we are interested in the theoretical\nfoundation of why such a minimal architectural modiﬁcation can have such large (positive) impact on\noptimizability. We are also interested in studying larger models. Our preliminary explorations into\n72GF models reveal that the convolutional stem still improves top-1 error, however we also ﬁnd that\na new form of instability arises that causes training error to randomly spike, especially for ViTC.\nAcknowledgements. We thank Hervé Jegou, Hugo Touvron, and Kaiming He for valuable feedback.\n9\nstem kernel size stride padding channels ﬂops params acts top-1 error ∆(M) (M) (M) AdamW SGD\nP [16] [16] [0] [384] 58 0.3 0.8 27.7 33.0 5.3\nC [3, 3, 3, 3, 1] [2, 2, 2, 2, 1] [1, 1, 1, 1, 0] [48, 96, 192, 384, 384] 435 1.0 1.2 24.0 24.7 0.7\nS1 [3, 3, 3, 2, 1] [2, 2, 2, 2, 1] [1, 1, 1, 0, 0] [42, 104, 208, 416, 384] 422 0.8 1.3 24.3 25.1 0.8\nS2 [3, 3, 3, 4, 1] [2, 2, 1, 4, 1] [1, 1, 1, 0, 0] [32, 64, 128, 256, 384] 422 0.7 1.1 24.3 25.3 1.0\nS3 [3, 3, 3, 8, 1] [2, 1, 1, 8, 1] [1, 1, 1, 0, 0] [17, 34, 68, 136, 384] 458 0.7 1.6 25.1 26.2 1.1\nS4 [3, 3, 3, 16, 1] [1, 1, 1, 16, 1] [1, 1, 1, 0, 0] [8, 16, 32, 64, 384] 407 0.6 2.9 26.2 27.9 1.3\nTable 3: Stem designs: We compare ViT’s standard patchify stem (P) and our convolutional stem\n(C) to four alternatives (S1 - S4) that each include a patchify layer, i.e., a convolution with kernel\nsize (>1) equal to stride (highlighted in blue). Results use 50 epoch training, 4GF model size, and\noptimal lr and wd values for all models. We observe that increasing the pixel size of the patchify\nlayer (S1 - S4) systematically degrades both top-1 error and optimizer stability (∆) relative to C.\nmodel top-1 err.\nViT P -4GF 23.2\nViT P ( bn ) -4GF 23.3\nViT C -4GF 20.9\nViT C ( ln ) -4GF 21.1\nTable 7: TMP :A\n11\nFigure 7: Stem normalization and non-linearity: We apply BN and ReLU after the patchify stem\nand train ViTP -4GF (left plot), or replace BN with layer norm (LN) in the convolutional stem of\nViTC-4GF (middle plot). EDFs are computed by sampling lr and wd values and training for 50\nepochs. The table (right) shows 100 epoch results using bestlr and wd values found at 50 epochs. The\nminor gap in error in the EDFs and at 100 epochs indicates that these choices are fairly insigniﬁcant.\nAppendix A: Stem Design Ablation Experiments\nViT’s patchify stem differs from the proposed convolutional stem in the type of convolution used and\nthe use of normalization and a non-linear activation function. We investigate these factors next.\nStem design. The focus of this paper is studying the large, positive impact of changing ViT’s default\npatchify stem to a simple, standard convolutional stem constructed from stacked stride-two 3×3\nconvolutions. Exploring the stem design space, and more broadly “hybrid ViT” models [ 13], to\nmaximize peak performance is an explicit anti-goal because we want to study the impact under\nminimal modiﬁcations. However, we can gain additional insight by considering alternative stem\ndesigns that fall between the patchify stem (P) the standard convolutional stem (C). Four alternative\ndesigns (S1 - S4) are presented in Table 3. The stems are designed so that overall model ﬂops remain\ncomparable. Stem S1 modiﬁes C to include a small 2×2 patchify layer, which slightly worsens\nresults. Stems S2 - S4 systematically increase the pixel size pof the patchify layer from p= 2 up\nto 16, matching the size used in stem P. Increasing preliably degrades both error and optimizer\nstability. Although we selected the Cdesign a priori based on existing best-practices for CNNs, we\nsee ex post facto that it outperforms four alternative designs that each include one patchify layer.\nStem normalization and non-linearity. We investigate normalization and non-linearity from two\ndirections: (1) adding BN and ReLU to the default patchify stem of ViT, and (2) changing the\nnormalization in the proposed convolutional stem. In the ﬁrst case, we simply apply BN and ReLU\nafter the patchify stem and train ViTP -4GF (termed ViTP(bn)-4GF) for 50 and 100 epochs. For the\nsecond case, we run four experiments with ViTC-4GF: {50, 100} epochs × {BN, layer norm (LN)}.\nAs before, we tune lr and wd for each experiment using the 50-epoch schedule and reuse those values\nfor the 100-epoch schedule. We use AdamW for all experiments. Figure 7 shows the results. From\nthe EDFs, which use a 50 epoch schedule, we see that the addition of BN and ReLU to the patchify\nstem slightly worsens the best top-1 error but does not affect lr and wd stability (left). Replacing BN\nwith LN in the convolutional stem marginally degrades both best top-1 error and stability (middle).\nThe table (right) shows 100 epoch results using optimal lr and wd values chosen from the 50 epoch\nruns. At 100 epochs the error gap is small indicating that these factors are likely insigniﬁcant.\n10\n20 25 30 35 40\ntop-1 error\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0cumulative prob. ViTP-16GF (48 blocks)\nViTC-16GF (47 blocks)\n20 25 30 35 40\ntop-1 error\nViTP-16GF (48 blocks)\nViTP-4GF (12 blocks)\nViTP-18GF (12 blocks)\n20 25 30 35 40\ntop-1 error\nViTC-16GF (47 blocks)\nViTC-4GF (11 blocks)\nViTC-18GF (11 blocks)\nFigure 8: Deeper models: We increase the depth of ViT P -4GF from 12 to 48 blocks, termed as\nViTP -16GF (48 blocks), and create a counterpart with a convolutional stem, ViTC-16GF (47 blocks);\nall models are trained for 50 epochs. Left: The convolutional stem signiﬁcantly improves error and\nstability despite accounting for only ∼2% total ﬂops. Middle, Right: The deeper 16GF ViTs clearly\noutperform the shallower 4GF models and achieve similar (slightly worse) error to the shallower and\nwider 18GF models. The deeper ViTP also has better lr/wd stability than the shallower ViTP models.\n0 50 100 150 200 250 300 350\nparameters (M)\n0\n10\n20\n30\n40\n50\n60training time (min.) r = 0.71\nResNet\nRegNetY\nRegNetZ\nEfficientNet\nViTP\nViTC\n0 10 20 30 40 50 60 70\nflops (B)\nr = 0.75\nResNet\nRegNetY\nRegNetZ\nEfficientNet\nViTP\nViTC\n20 40 60 80 100\nactivations (M)\nr = 0.93\nResNet\nRegNetY\nRegNetZ\nEfficientNet\nViTP\nViTC\nname\nFigure 9: Complexity measures vs. runtime: We plot the GPU runtime of models versus three\ncommonly used complexity measures: parameters, ﬂops, and activations. For all models, including\nViT,runtime is most correlated with activations, not ﬂops, as was previously shown for CNNs [12].\nAppendix B: Deeper Model Ablation Experiments\nTouvron et al. [42] found that deeper ViT models are more unstable, e.g., increasing the number of\ntransformer blocks from 12 to 36 may cause a ∼10 point drop in top-1 accuracy given a ﬁxed choice\nof lr and wd. They demonstrate that stochastic depth and/or their proposed LayerScale can remedy\nthis training failure. Here, we explore deeper models by looking at EDFs created by sampling lr and\nwd. We increase the depth of a ViTP -4GF model from 12 blocks to 48 blocks, termed ViTP -16GF\n(48 blocks). We then remove one block and use the convolutional stem from ViTC-4GF, yielding a\ncounterpart ViTC-16GF (47 blocks) model. Figure 8 shows the EDFs of the two models and shallower\nmodels for comparison, following the setup in §5.3. Despite the convolutional stem accounting for\nonly 1/48 (∼2%) total ﬂops, it shows solid improvement over its patchify counterpart. We ﬁnd that\na variety of lr and wd choices allow deeper ViT models to be trained without a large drop in top-1\nperformance and without additional modiﬁcations. In fact, the deeper ViTP -16GF (48 blocks) has\nbetter lr and wd stability than ViTP -4GF and ViTP -18GF over the sampling range (Figure 8, middle).\nAppendix C: Larger Model ImageNet-21k Experiments\nIn Table 2 we reported the peak performance of ViT models on ImageNet-21k up to 36GF. To study\nlarger models, we construct a 72GF ViTP by using 22 blocks, 1152 hidden size, 18 heads, and 4 MLP\nmultiplier. For ViTC-72GF, we use the same C-stem design used for ViTC-18GF and ViTC-36GF,\nbut without removing one transformer block since the ﬂops increase from the C-stem is marginal in\nthis complexity regime.\nOur preliminary explorations into 72GF ViT models directly adopted hyperparameters used for 36GF\nViT models. Under this setting, we observed that the convolutional stem still improves top-1 error,\nhowever, we also found that a new form of instability arises, which causes training error to randomly\nspike. Sometimes training may recover within the same epoch, and subsequently the ﬁnal accuracy\nis not impacted; or, it may take several epochs to recover from the error spike, and in this case we\nobserve suboptimal ﬁnal accuracy. The ﬁrst type of error spike is more common for ViT P -72GF,\nwhile the latter type of error spike is more common for ViTC-72GF.\n11\nmodel AdamW SGD\nlr wd lr wd\nRegNetY-∗ 3.8e-3 0.1 2.54 2.4e-5\nViTP-1GF 2.0e-3 0.20 1.9 1.3e-5\nViTP-4GF 2.0e-3 0.20 1.9 1.3e-5\nViTP-18GF 1.0e-3 0.24 1.1 1.2e-5\nViTC-1GF 2.5e-3 0.19 1.9 1.3e-5\nViTC-4GF 1.0e-3 0.24 1.3 2.2e-5\nViTC-18GF 1.0e-3 0.24 1.1 2.7e-5\nmodel AdamW\nlr wd\nViT-∗ (2.5e−4,8.0e−3) (0 .02,0.8)\nRegNetY-∗ (1.25e−3,4.0e−2) (0 .0075,0.24)\nmodel SGD\nlr wd\nViT-∗ (0.1,3.2) (4 .0e−6,1.2e−4)\nRegNetY-∗ (0.25,8.0) (3 .0e−6,8.0e−5)\nTable 4: Learning rate and weight decay used in §5: Left: Per-model lr and wd values used for the\nexperiments in §5.1 and §5.2, optimized for ImageNet-1k at 50 epochs. Right: Per-model lr and wd\nranges used for the experiments in §5.3. Note that for our ﬁnal experiments in §6, we constrained the\nlr and wd values further, using a single setting for all CNN models, and just two settings for all ViT\nmodels. We recommend using this simpliﬁed set of values in §6 when comparing models for fair and\neasily reproducible comparisons. All lr values are normalized w.r.t. a minibatch size of2048 [16].\nTo mitigate this instability, we adopt two measures: (i) For both models, we lower wd from 0.28\nto 0.15 as we found that it signiﬁcantly reduces the chance of error spikes. (ii) For ViT C-72GF,\nwe initialize its stem from the ImageNet-21k pre-trained ViTC-36GF and keep it frozen throughout\ntraining. These modiﬁcations make training ViT-72GF models on ImageNet-21k feasible. When ﬁne-\ntuned on ImageNet-1k, ViTP -72GF reaches 14.2% top-1 error and ViTC-72GF reaches 13.6% top-1\nerror, showing that ViTC still outperforms its ViTP counterpart. Increasing ﬁne-tuning resolution\nfrom 224 to 384 boosts the performance of ViTC-72GF to 12.6% top-1 error, while signiﬁcantly\nincreasing the ﬁne-tuning model complexity from 72GF to 224GF.\nAppendix D: Model Complexity and Runtime\nIn previous sections, we reported error vs. training time. Other commonly used complexity measures\ninclude parameters, ﬂops, and activations. Indeed, it is most typical to report accuracy as a function\nof model ﬂops or parameters. However, ﬂops may fail to reﬂect the bottleneck on modern memory-\nbandwidth limited accelerators (e.g., GPUs, TPUs). Likewise, parameters are an even more unreliable\npredictor of model runtime. Instead, activations have recently been shown to be a better proxy of\nruntime on GPUs (see [12, 31]). We next explore if similar results hold for ViT models.\nFor CNNs, previous studies [12, 31] deﬁned activations as the total size of all output tensors of the\nconvolutional layers, while disregarding normalization and non-linear layers (which are typically\npaired with convolutions and would only change the activation count by a constant factor). In this\nspirit, for transformers, we deﬁne activations as the size of output tensors of all matrix multiplications,\nand likewise disregard element-wise layers and normalizations. For models that use both types of\noperations, we simply measure the output size of all convolutional and vision transformer layers.\nFigure 9 shows the runtime as a function of these model complexity measures. The Pearson correlation\ncoefﬁcient (r) conﬁrms that activations have a much stronger linear correlation with actual runtime\n(r= 0.93) than ﬂops (r= 0.75) or parameters (r= 0.71), conﬁrming that the ﬁndings of [12] for\nCNNs also apply to ViTs. While ﬂops are somewhat predictive of runtime, models with a large ratio\nof activations to ﬂops, such as EfﬁcientNet, have much higher runtime than expected based on ﬂops.\nFinally, we note that ViTP and ViTC are nearly identical on all complexity measures and runtime.\nTiming. Throughout the paper we report normalized training time, as if the model were trained on\na single 8 V100 GPU server, by multiplying the actual training time by the number of GPUs used\nand dividing by 8. (Due to different memory requirements of different models, we may be required\nto scale up the number of GPUs to accommodate the target minibatch size.) We use the number of\nminutes taken to process one ImageNet-1k epoch as a standard unit of measure. We prefer training\ntime over inference time because inference time depends heavily on the use case (e.g., a streaming,\nlatency-oriented setting requires a batch size of 1 vs. a throughput-oriented setting that allows for\nbatch size ≫ 1) and the hardware platform (e.g., smartphone, accelerator, server CPU).\n12\nmodel\nAugment\nMixup\nCutMix\nLabel Smooth\nModel EMA\nErasing\nStoch Depth\nRepeating\n100 epochs\n400 epochs\n300 epochs [41]\nViTP-4GF\nAuto \u0013 \u0013 \u0013 \u0013 23.2 20.5 -\nRand \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 25.4 20.7 -\nRand \u0013 \u0013 \u0013 \u0013 \u0013 24.9 20.5 -\nRand \u0013 \u0013 \u0013 \u0013 23.6 20.4 -\nRand \u0013 \u0013 \u0013 23.5 20.3 -\nAuto \u0013 \u0013 \u0013 23.0 20.3 -\nViTP-18GF\nAuto \u0013 \u0013 \u0013 \u0013 19.9 17.9 -\nRand \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 22.5 18.6 18.2\nRand \u0013 \u0013 \u0013 \u0013 \u0013 25.1 19.2 96.6\nRand \u0013 \u0013 \u0013 \u0013 21.2 19.9 -\nRand \u0013 \u0013 \u0013 20.9 19.7 -\nAuto \u0013 \u0013 \u0013 20.4 20.0 -\nRand \u0013 \u0013 \u0013 \u0013 \u0013 - - 22.6\nRand \u0013 \u0013 \u0013 \u0013 \u0013 - - 95.7\nRand \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 - - 18.1\nTable 5: Ablation of data augmentation and regularization: We use the lr and wd from Table 4\n(left), except for ViTP -18GF models with RandAugment which beneﬁt from strongerwd (we increase\nwd to 0.5). Original DeiT ablation results [41] are copied for reference in gray (last column); these\nuse a lr/wd of 1e−3/0.05 (lr normalized to minibatch size 2048), which leads to some training\nfailures (we note our wd is 5-10× higher). Our default training setup ( ﬁrst row in each set) uses\nAutoAugment, mixup, CutMix, label smoothing, and model EMA. Compared to the DeiT setup\n(second row in each set), we do not use erasing, stochastic depth, or repeating. Although our setup is\nequally effective, it is simpler and also converges much faster (see Figure 10).\nAppendix E: Additional Experimental Details\nStability experiments. For the experiments in §5.1 and §5.2, we allow each CNN and ViT model to\nselect a different lr and wd. We ﬁnd that all CNNs select nearly identical values, so we normalize\nthem to a single choice as done in [12]. ViT models prefer somewhat more varied choices. Table 4\n(left) lists the selected values. For the experiments in §5.3, we use lr and wd intervals shown in\nTable 4 (right). These ranges are constructed by (i) obtaining initial good lr and wd choices for each\nmodel family; and then (ii) multiplying them by 1/8 and 4.0 for left and right interval endpoints (we\nuse an asymmetric interval because models are trainable with smaller but not larger values). Finally\nwe note that if we were to redo the experiments, the setting used in §5.1/§5.2 could be simpliﬁed.\nPeak performance on ImageNet-1k. We note that in later experiments we found tuning lr and wd\nper model is not necessary to obtain competitive results. Therefore, for our ﬁnal experiments in §6,\nwe constrained the lr and wd values further, using a single setting for all CNN models, and just two\nsettings for all ViT models, as discussed in §6. We recommend using this simpliﬁed set of values\nwhen comparing models for fair and easily reproducible comparisons. Finally, for these experiments,\nwhen training is memory constrained (i.e., for EfﬁcientNet-{B4,B5}, RegNetZ-{4,16,32}GF), we\nreduce the minibatch size from 2048 and linearly scale the lr according to [16].\nPeak performance on ImageNet-21k. For ImageNet-21k, a dataset of 14M images and ∼21k\nclasses, we pretrain models for 90 (ImageNet-21k) epochs, following [13]. We do not search for the\noptimal settings for ImageNet-21k and instead use the identical training recipe (up to minibatch size)\nused for ImageNet-1k. To reduce training time, we distribute training over more GPUs and use a\nlarger minibatch size of 4096 with the lr scaled accordingly. For simplicity and reproducibility, we\nuse a single label per image, unlike some prior work (e.g., [35, 40]) that uses WordNet [28] to expand\nsingle labels to multiple labels. After pretraining, we ﬁne-tune for 20 epochs on ImageNet-1k and\nuse a small-scale grid search of lr while keeping wd at 0, similar to [13, 40].\nAppendix F: Regularization and Data Augmentation\nAt this study’s outset, we developed a simpliﬁed training setup for ViT models. Our goals were to\ndesign a training setup that is as simple as possible, resembles the setup used for state-of-the-art\nCNNs [12], and maintains competitive accuracy with DeiT [41]. Here, we document this exploration\n13\n50 100 200 400\ntraining epochs\n0\n2\n4\n6\n8\n10top-1 error\n4GF models\nViTP (DeiT)\nViTP (Ours)\nViTC (Ours)\n50 100 200 400\ntraining epochs\n0\n2\n4\n6\n8\n10 18GF models\nViTP (DeiT)\nViTP (Ours)\nViTC (Ours)\n50 100 200 400\ntraining epochs\n18\n21\n24\n27\n30top-1 error\n18GF models\nViTP (DeiT)\nViTP (Ours)\nViTC (Ours)\nname\nFigure 10: Impact of training recipes on convergence: We train ViT models using the DeiT recipe\nvs. our simpliﬁed counterpart. Left and middle: ∆top-1 error of 4GF and 18GF models at 50, 100\nand 200 epoch schedules, and asymptotic performance at 400 epochs. Right: Absolute top-1 error of\n18GF models. Removing augmentations and using model EMA accelerates convergence for both\nViTP and ViTC models while slightly improving upon our reproduction of DeiT’s top-1 error.\n15 16 17 18 19 20 21\nImageNet-OG (error)\n24\n26\n28\n30\n32ImageNet-V2 (error)r = 0.99\nResNet\nRegNetY\nRegNetZ\nEfficientNet\nViTP\nViTC\nname\nFigure 11: ImageNet-V2 performance: We take the models from Table 2 and benchmark them on the\nImageNet-V2 test set. Top-1 errors are plotted for the original (OG) ImageNet validation set (x-axis)\nand the ImageNet-V2 test set (y-axis). Rankings are mostly preserved up to one standard deviation of\nnoise (estimated at ∼0.1-0.2%) and the two testing sets exhibit linear correlation (Pearson’sr= 0.99).\nMarker size corresponds to model ﬂops.\nby considering the baseline ViT P -4GF and ViTP -18GF models. Beyond simpliﬁcation, we also\nobserve that our training setup yields faster convergence than the DeiT setup, as discussed below.\nTable 5 compares our setup to that of DeiT [ 41]. Under their lr/wd choice, [ 41] report failed\ntraining when removing erasing and stochastic depth, as well as signiﬁcant drop of accuracy when\nremoving repeating. We ﬁnd that they can be safely disabled as long as a higher wd is used (our wd\nis 5-10× higher). We observe that we can remove model EMA for ViTP -4GF, but that it is essential\nfor the larger ViTP -18GF model, especially at 400 epochs. Without model EMA, ViTP -18GF can\nstill be trained effectively, but this requires additional augmentation and regularization (as in DeiT).\nFigure 10 shows that our training setup accelerates convergence for both ViTP and ViTC models, as\ncan be seen by comparing the error deltas (∆top-1) between the DeiT baseline and ours ( left and\nmiddle plots). Our training setup also yields slightly better top-1 error than our reproduction of DeiT\n(right plot). We conjecture that faster convergence is due to removing repeating augmentation [1, 20],\nwhich was shown in [1] to slow convergence. Under some conditions repeating augmentation may\nimprove accuracy, however we did not observe such improvements in our experiments.\nAppendix G: ImageNet-V2 Evaluation\nIn the main paper and previous appendix sections we benchmarked all models on the original (OG)\nImageNet validation set [10]. Here we benchmark our models on the ImageNet-V2 [33], a new test\nset collected following the original procedure. We take the 400-epoch or ImageNet-21k models from\nTable 2, depending on which one is better, and evaluate them on ImageNet-V2 to collect top-1 errors.\nFigure 11 shows that rankings are mostly preserved up to one standard deviation of noise (estimated\nat ∼0.1-0.2%). The two testing sets exhibit linear correlation, as conﬁrmed by the Pearson correlation\ncoefﬁcient r= 0.99, despite ImageNet-V2 results showing higher absolute error. The parameters of\nthe ﬁt line are given by y= 1.31x+ 5.0.\n14\nReferences\n[1] Maxim Berman, Hervé Jégou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a\nuniﬁed image embedding for classes and instances. arXiv:1902.05509, 2019. 8, 14\n[2] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In CVPR,\n2005. 3\n[3] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In ICML, 2020. 5\n[4] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers. In ICCV, 2021. 3\n[5] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Visformer: The\nvision-friendly transformer. In ICCV, 2021. 3\n[6] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention\nand convolutional layers. ICLR, 2020. 3\n[7] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. AutoAugment: Learning\naugmentation policies from data. In CVPR, 2019. 8\n[8] Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong Tian,\nMatthew Yu, Peter Vajda, et al. FBNetV3: Joint architecture-recipe search using neural acquisition function.\narXiv:2006.02049, 2020. 8\n[9] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. ConViT:\nImproving vision transformers with soft convolutional inductive biases. In ICML, 2021. 2, 3\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical\nimage database. In CVPR, 2009. 2, 3, 5, 14\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NACCL, 2019. 1\n[12] Piotr Dollár, Mannat Singh, and Ross Girshick. Fast and accurate model scaling. In CVPR, 2021. 5, 8, 11,\n12, 13\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1, 2, 3, 4, 5, 10, 13\n[14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph\nFeichtenhofer. Multiscale vision transformers. In ICCV, 2021. 3\n[15] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern\nrecognition unaffected by shift in position. Biological cybernetics, 36(4):193–202, 1980. 3\n[16] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew\nTulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour.\narXiv:1706.02677, 2017. 3, 8, 12, 13\n[17] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and Matthijs\nDouze. LeViT: a vision transformer in ConvNet’s clothing for faster inference. InICCV, 2021. 3, 5\n[18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 3\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016. 1, 3, 4, 8\n[20] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your\nbatch: better training with larger batches. arXiv:1901.09335, 2019. 8, 14\n[21] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In ICML, 2015. 3, 4\n[22] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil\nHoulsby. Big Transfer (BiT): General visual representation learning. In ECCV, 2020. 4\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classiﬁcation with deep convolutional\nneural networks. In NeurIPS, 2012. 3\n[24] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation,\n1989. 1, 3\n[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 3\n[26] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-\ntion. In CVPR, 2015. 3\n15\n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 1, 3, 5\n[28] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 1995. 13\n[29] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In ICML,\n2010. 4\n[30] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces\nfor visual recognition. In ICCV, 2019. 2, 5, 7\n[31] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing network\ndesign spaces. In CVPR, 2020. 5, 12\n[32] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. NeurIPS, 2019. 3\n[33] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers\ngeneralize to imagenet? In ICML, 2019. 9, 14\n[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object\ndetection with region proposal networks. In NeurIPS, 2015. 3\n[35] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the\nmasses. In NeurIPS, 2021. 13\n[36] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\ntion. In ICLR, 2015. 2, 3, 4\n[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 3\n[38] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking\nthe inception architecture for computer vision. In CVPR, 2016. 8\n[39] Mingxing Tan and Quoc V Le. EfﬁcientNet: Rethinking model scaling for convolutional neural networks.\nICML, 2019. 2, 8\n[40] Mingxing Tan and Quoc V Le. Efﬁcientnetv2: Smaller models and faster training. In ICML, 2021. 13\n[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. In ICML, 2021. 1, 3, 4,\n5, 8, 13, 14\n[42] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper\nwith image transformers. arXiv:2103.17239, 2021. 1, 3, 5, 11\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 3\n[44] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning\ndeep transformer models for machine translation. In ACL, 2019. 3\n[45] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\nIn ICCV, 2021. 3\n[46] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,\n2018. 3\n[47] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. CvT:\nIntroducing convolutions to vision transformers. In ICCV, 2021. 3\n[48] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-\ntions for deep neural networks. In CVPR, 2017. 3\n[49] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution\ndesigns into visual transformers. In ICCV, 2021. 3\n[50] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token ViT: Training vision transformers from scratch on ImageNet. In ICCV,\n2021. 3\n[51] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutMix: Regularization strategy to train strong classiﬁers with localizable features. In CVPR, 2019. 8\n[52] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond empirical risk\nminimization. In ICLR, 2018. 8\n[53] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR,\n2020. 3\n[54] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.\nIn AAAI, 2020. 5\n16"
}