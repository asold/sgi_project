{
  "title": "C5T5: Controllable Generation of Organic Molecules with Transformers",
  "url": "https://openalex.org/W3194210660",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226570100",
      "name": "Rothchild, Daniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202102312",
      "name": "Tamkin, Alex",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287037545",
      "name": "Yu, Julie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4311410397",
      "name": "Misra, Ujval",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223304968",
      "name": "Gonzalez, Joseph",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1981622697",
    "https://openalex.org/W2618625858",
    "https://openalex.org/W2735574368",
    "https://openalex.org/W2972608805",
    "https://openalex.org/W3133498786",
    "https://openalex.org/W3158543275",
    "https://openalex.org/W2999242200",
    "https://openalex.org/W2034354062",
    "https://openalex.org/W2803526748",
    "https://openalex.org/W2962753250",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2786722833",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2746340587",
    "https://openalex.org/W2981540061",
    "https://openalex.org/W1500036797",
    "https://openalex.org/W2963609389",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2963215859",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2949378066",
    "https://openalex.org/W3137038343",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W3160789623",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W3137903636",
    "https://openalex.org/W568917201",
    "https://openalex.org/W2806351858",
    "https://openalex.org/W2805177834",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W2992072991",
    "https://openalex.org/W2052907531",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2009979602"
  ],
  "abstract": "Methods for designing organic materials with desired properties have high potential impact across fields such as medicine, renewable energy, petrochemical engineering, and agriculture. However, using generative modeling to design substances with desired properties is difficult because candidate compounds must satisfy multiple constraints, including synthetic accessibility and other metrics that are intuitive to domain experts but challenging to quantify. We propose C5T5, a novel self-supervised pretraining method that enables transformers to make zero-shot select-and-replace edits, altering organic substances towards desired property values. C5T5 operates on IUPAC names -- a standardized molecular representation that intuitively encodes rich structural information for organic chemists but that has been largely ignored by the ML community. Our technique requires no edited molecule pairs to train and only a rough estimate of molecular properties, and it has the potential to model long-range dependencies and symmetric molecular structures more easily than graph-based methods. C5T5 also provides a powerful interface to domain experts: it grants users fine-grained control over the generative process by selecting and replacing IUPAC name fragments, which enables experts to leverage their intuitions about structure-activity relationships. We demonstrate C5T5's effectiveness on four physical properties relevant for drug discovery, showing that it learns successful and chemically intuitive strategies for altering molecules towards desired property values.",
  "full_text": "C5T5: Controllable Generation of Organic Molecules\nwith Transformers\nDaniel Rothchild\nEECS\nUC Berkeley\ndrothchild@berkeley.edu\nAlex Tamkin\nComputer Science\nStanford University\natamkin@stanford.edu\nJulie Yu\nData Science & Chemistry\nUC Berkeley\njulieyu@berkeley.edu\nUjval Misra\nEECS\nUC Berkeley\nujval@berkeley.edu\nJoseph Gonzalez\nEECS\nUC Berkeley\njegonzal@berkeley.edu\nAbstract\nMethods for designing organic materials with desired properties have high potential\nimpact across ﬁelds such as medicine, renewable energy, petrochemical engineering,\nand agriculture. However, using generative modeling to design substances with\ndesired properties is difﬁcult because candidate compounds must satisfy multiple\nconstraints, including synthetic accessibility and other metrics that are intuitive\nto domain experts but challenging to quantify. We propose C5T5, a novel self-\nsupervised pretraining method that enables transformers to make zero-shot select-\nand-replace edits, altering organic substances towards desired property values.\nC5T5 operates on IUPAC names—a standardized molecular representation that\nintuitively encodes rich structural information for organic chemists but that has\nbeen largely ignored by the ML community. Our technique requires no edited\nmolecule pairs to train and only a rough estimate of molecular properties, and it has\nthe potential to model long-range dependencies and symmetric molecular structures\nmore easily than graph-based methods. C5T5 also provides a powerful interface to\ndomain experts: it grants users ﬁne-grained control over the generative process by\nselecting and replacing IUPAC name fragments, which enables experts to leverage\ntheir intuitions about structure-activity relationships. We demonstrate C5T5’s\neffectiveness on four physical properties relevant for drug discovery, showing\nthat it learns successful and chemically intuitive strategies for altering molecules\ntowards desired property values.\n1 Introduction\nOrganic molecules are used in countless applications across human society: as medicines, industrial\nchemicals, fuels, pesticides, plastics, television screens, solar cells, and many others. Traditionally,\nnew molecules are designed for particular tasks by hand, but the space of all possible molecules is so\nvast (e.g. the total number of drug-like molecules may be as high as 1060) that most useful materials\nare probably still undiscovered [1]. To automate materials discovery, domain experts have turned\nto high-throughput screening, in which a large library of potentially useful molecules is generated\nheuristically, and the most promising molecules are chosen for further study using computational\nmodels that estimate how effective each substance will be for the target application [2]. Unfortunately,\nhigh-throughput screening faces the fundamental limitation that the total number of molecules that\ncan be screened is still only a tiny fraction of all possible molecules.\nPreprint. Under review.\narXiv:2108.10307v1  [cs.LG]  23 Aug 2021\nFigure 1: Increasing a molecule’s octanol-water partition coefﬁcient with C5T5. (1) A molecular\nfragment (acetyloxy) is identiﬁed in a molecule of interest. (2) The molecular fragment is replaced\nwith a sentinel token (<s1>) and the property value is set to the desired bucket (<high>). (3) Sampling\nfrom C5T5 produces a new fragment (decyl). Substituting in the fragment yields a new molecule.\nThe long chain of carbons added to the molecule increases its solubility in octanol while decreasing\nits solubility in water.\nGenerating molecules directly using machine learning addresses this limitation, butde novo molecular\ndesign using machine learning can be of limited use in domains like drug discovery, where experts’\nintuitions about structure-activity relationships and external factors like patentability are important to\nconsider in the design process. These constraints can often be expressed by providing known portions\nof the molecular structure; for example a domain expert may be interested in a particular scaffold\nbecause it has favorable intellectual property attributes, or certain parts of a drug may be needed for\nthe desired biological activity, while other parts can be modiﬁed to increase bioavailability.\nTo address this real-world setting, we consider the problem of learning to make localized modiﬁcations\nto a molecule that change its physical properties in a desired way. We propose C5T5: Controllable\nCharacteristic-Conditioned Chemical Changer with T5 [3], a novel method for generative modeling\nof organic molecules that gives domain experts ﬁne-grained control over the molecular optimization\nprocess while also providing more understandable predictions than prior methods (Figure 1). Our\ntwo key contributions are 1) recasting molecular modeling as language modeling on the semantically\nrich IUPAC name base representation, and 2) the development of a novel conditional language\nmodeling strategy using transformers that supports targeted modiﬁcations to existing molecules.\nIUPAC Names. The IUPAC naming system is a systematic way of naming organic molecules\nbased on functional groups and moieties, which are commonly occurring clusters of connected\natoms that have known chemical behaviors. Organic chemists have discovered countless chemical\nreactions that operate on functional groups, and they use these reactions to develop synthesis routes\nfor novel molecules. Despite this, existing generative methods for organic molecules have ignored\nIUPAC names as a representation, instead opting for atom-based representations like SMILES [4] and\nmolecular graphs [5]. We argue that these representations are less suitable for molecular optimization\nbecause adding or removing arbitrary atoms has no intuitive meaning to chemists and is unlikely to\nallow for easy synthesis; see Figure 2 for a comparison of IUPAC names and SMILES. To the best of\nour knowledge we are the ﬁrst to use IUPAC names as a base representation for molecular modeling.\nSelf-Supervised Objective for Zero-Shot Editing. To enable targeted modiﬁcations of molecules\nwithout predeﬁned edit pairs, we train transformers with a conditional variant of a self-supervised\ninﬁlling task by masking out certain tokens in the IUPAC name and training the model to replace\nthese missing tokens. Crucially, we prepend the IUPAC names with discretized property values\nduring training, enabling the model to learn the conditional relationships between the property value\nand molecular structure. During inference, we replace the true property values with desired property\nvalues, mask out the portion of the molecule we want replaced, and ask the model to inﬁll the\nmasked tokens as usual. To the best of our knowledge, this sort of self-supervision to enable guided\nselect-and-replace editing has not been explored previously; we anticipate this method could be\nbroadly applied in other controlled generation contexts, such as modeling various aspects of text like\naffect, politeness, or topic [6–9].\nAs we show in Section 4, C5T5 is able to make interpretable targeted modiﬁcations to molecules that\nlead to desired changes across several physical properties important in drug design.\n2\n2-acetyloxybenzoic acid\n1 234\nCC(=O)OC1=C(C=CC=C1)C(O)=O\n1 2    3  4 5     6  7   8 9  10  11 12  13 \n13 12\n11\n6 1\n2\n3\n4\n57\n8 9 10\n       IUPAC Name     SMILES\nFigure 2: Visual representations of IUPAC\nnames and SMILES. Tokens in IUPAC names\ncorrespond to well-known functional groups and\nmoieties. In contrast, tokens in SMILES corre-\nspond to individual atoms and bonds.\n2 Related Work\nModeling. A number of machine learning methods have been developed for the task of designing\norganic molecules, but most do not allow a user to make targeted modiﬁcations to a molecule. Some\nmethods, like generative adversarial networks and unconditional sequence models, provide no control\nover a generated molecule’s structure [10–14], and are therefore more useful for generating candidate\nlibraries than optimizing a particular molecule. Other methods, like variational autoencoders or\nsequence models that are conditioned on a base molecule, allow specifying that generated molecules\nshould be similar to a starting molecule in some learned space, but there is no way to speciﬁcally\ntarget a certain part of the molecule to modify [15–29]. Recognizing the importance of leveraging\ndomain experts’ intuition about structure-activity relationships, several methods, published mostly\nin chemistry venues, have explored constraining generated molecules to contain a scaffold, or a\nsubgraph of the full molecular graph [30–32]. However, these methods append to scaffolds arbitrarily\ninstead of allowing domain experts to specify which part of the molecule they would like to modify\nor append to, limiting their utility for human-in-the-loop molecular optimization.\nA few methods have explored allowing targeted modiﬁcations, where a domain expert can mask\nout a portion of a starting molecule and ask the model to replace the mask with a novel side chain\n[33–35]. These methods are limited because they only support masking parts of the molecule that\ncan be truncated by cutting a single bond, and because they require a dataset of paired molecules\n(scaffolds & decorators) that must be constructed using hand-crafted rules. In contrast, C5T5 learns\nin an entirely unsupervised fashion and therefore requires no paired data; the only limit to what can\nbe masked is what can be represented using IUPAC tokens.\nRepresentation. Existing methods all use SMILES (or a derivative representation) or graphs to\nrepresent molecules. There are a number of drawbacks to using the SMILES representation: a small\nchange in a molecule can lead to a large change in the SMILES string [24]; ﬂattening the graph into a\nlist of atoms artiﬁcially creates variable- and long-range dependencies between bonded atoms; and it\nis difﬁcult to reason about common substructures, because the same structure can be represented in\nmany different ways depending on how the graph was ﬂattened. And although graphs seem like a\nnatural representation for molecules, graphs do a poor job encoding symmetry, long-range interactions\nbetween atoms that are many bonds apart but nearby in 3D space, and long-range interactions that\narise from conjugated systems [ 5]. C5T5 operates instead of IUPAC names, which we argue in\nSection 3.1 is a more suitable representation for molecular optimization because tokens have much\nmore semantic meaning. See Appendix A for more details on how C5T5 relates to prior work.\nTransformers for Molecular Modeling Outside of molecular optimization, transformers have\nfound a number of applications in molecular modeling tasks, including property prediction [36, 37],\nchemical reaction prediction [38], retrosynthesis [39] and generating proteins [40, 41]. A few works\nhave explored using transformers for generative modeling of organic molecules [15, 17, 22]. Some\nworks have also proposed using transformers for scaffold-conditioned generative modeling [27, 35].\nThis work extends these efforts by proposing a simple yet effective training and zero-shot adaptation\nmethod, and by using IUPAC names instead of SMILES strings.\nIUPAC Names Although we are unaware of prior work using IUPAC names as a base representation\nfor molecular modeling, several works have explored using machine learning to convert between\nIUPAC names and other molecular representations [42–44].\n3\n3 Method\nMolecular optimization is a difﬁcult problem because it requires modifying a molecule that already\nsatisﬁes a number of requirements. Modiﬁcations need to improve a particular aspect of the molecule\nwithout degrading its performance on other metrics, and without making it too difﬁcult to synthesize.\nWe argue that by using IUPAC names (Section 3.1) and by allowing users to target particular\nparts of a molecule to modify (Section 3.2), C5T5 has the potential to support human-in-the-loop\nmolecular editing that complements domain experts’ intuitions about structure-activity relationships\nand synthetic accessibility.\n3.1 IUPAC Naming\nThe International Union of Pure and Applied Chemistry (IUPAC) publishes a set of rules that allow\nsystematic conversion between a chemical structure and a human-readable name [45]. For example,\n2-chloropentane refers unambiguously to ﬁve carbons (“pent”) connected by single bonds (“ane”)\nwith a chlorine atom (“chloro”) bonded to the second carbon from one end (“2-”). IUPAC names\nare used ubiquitously in scholarly articles, patents, and educational materials. In contrast to other\nlinear molecular representations like SMILES and its derivatives, where single tokens mostly refer to\nindividual atoms and bonds, tokens in IUPAC names generally have a rich semantic meaning. For\nexample, the token “ic acid” denotes a carboxylic acid, which is a common functional group that\nhas well-known physical and chemical properties; there are many known chemical reactions that\neither start with or produce carboxylic acids. Other tokens denote additional functional groups (e.g.\n“imide,” “imine,” “al,” “one”), locants (e.g. “1,” “2,” “N”), which indicate connectivity, alkanes (e.g.\n“meth,” “eth,” “prop”), which denote the lengths of carbon chains, polycyclic rings (e.g. “naphthalene,”\n“anthracene”), stereochemistry markers (“R,” “S”), and multipliers (e.g. “di,” “tri”), which concisely\nrepresent duplicated and symmetric structures. Figure 2 shows the relationships between IUPAC\nnames, graph representations, and SMILES.\nFor molecular optimization, C5T5 supports qualitatively different molecular edits compared to graph-\nand SMILES-based methods by virtue of its use of IUPAC names. In particular, editing a locant token\ncorresponds to moving a functional group along a carbon backbone or changing the connectivity of a\nfused ring system. And editing a multiplier token corresponds to creating or eliminating duplicated\nand symmetric structures. For example, changing “ethylbenzene“ to “hexaethylbenzene” replicates\nthe ethyl structure around the entire benzene ring with a single token edit. These sorts of modiﬁcations\nrequire much more extensive editing for SMILES- and graph-based methods.1\nWe argue that IUPAC names are especially attractive for molecular optimization, since the process\nrequires interaction between the algorithm and a domain expert, so interpretability is paramount.\nCompared to graph- or SMILES-based models, C5T5 makes predictions that can be traced back to\nmoieties and functional groups that domain experts are more likely to understand, trust, and know\nhow to synthesize than arbitrary collections of atoms and bonds.\nIn addition to improved interpretability, we argue that using IUPAC names has advantages purely\nfrom the standpoint of modeling data, since moving from SMILES to IUPAC names is akin to moving\nfrom a character-based to a word-based sequence model. Modeling at this higher level of abstraction\nenables the network to direct more of its capacity to structure at the relevant semantic level, instead\nof relearning lower-level details like the speciﬁc atomic composition of functional groups. In this\nvein, we demonstrate the potential of IUPAC names by learning word2vec representations of IUPAC\nname tokens [46], drawn from a list of over 100 million names in the PubChem repository [ 47]\nand tokenized using a list of tokens in OPSIN—an open-source IUPAC Name parser library (MIT\nLicense) [48]. For example, as shown in Figure 2, the chemical “2-acetyloxybenzoic acid” gets\ntokenized to [“2”, “-”, “acet”, “yl”, “oxy”, “benzo”, “ic acid”]. As with natural language modeling,\nwe ﬁnd that the embedding space learned by word2vec encodes the semantic meaning of the tokens,\nas shown in Figure 3. Different classes of tokens tend to be clustered together, and similar tokens\nwithin clusters are located nearby. For example, aromatic compounds with two rings are clearly\nseparated from those with three, locants are ordered roughly correctly from 1 to 100, and multiplier\n1Moving the attachment point of a functional group requires only a small edit of a graph (i.e. changing one\nbond), but most graph-based molecular generation methods sequentially generate one node at a time, followed\nby any bonds that connect the new node to the molecular graph so far. Moving a side chain therefore requires\nremoving the entire chain and regenerating it node by node.\n4\ntokens are also roughly in order (zoom not shown). Following Mikolov et al. [46], we also ﬁnd\nthat simple arithmetic operations in the embedding vector space correspond to semantic analogies\nbetween tokens. For example, the nearest neighbor of “phosphonous acid” - “nitrous acid” + “nitroso”\nis the embedding for “phosphoroso.”2 The nearest neighbor of “diphosphate” - “disulfate” + “sulfate”\nis “phosphate.” Likewise for “selenate” - “tellurate” + “tellurite” being closest to “selenite.”\n3.2 Conditional Language Modeling with Transformers\nWe now present the C5T5 objective, which trains a model to alter the properties of a molecule\nthrough localized edits. Importantly, this behavior does not require training on human-deﬁned edit\npairs, a strategy limited by either a ﬁxed set of hand-speciﬁed chemical alterations or an expensive\nexperimentally-derived training set. Instead, this editing behavior emerges as a zero-shot side-effect\nof our conditional language modeling objective, requiring only a simple forward pass using our\npretrained model without additional gradient updates.\nGiven a property value P, a fragment of a molecule F, and the rest of the molecule C (the context),\nwe wish to learn the conditional distribution P(F|C, P). Then, for a new molecule, one could\nalter the molecule towards a desired property value by redacting the original F, changing P to P′\nand sampling a new F′ ∼P(F|C, P′). Intuitively, this asks our model what kinds of molecular\nfragments the model would expect given the context and the new property value.\nTo learn this conditional distribution, we propose a conditional generalization of the inﬁlling objective\nused to train T5 [3] and ILM [49]. This process consists of several steps, also illustrated in Figure 1:\n1. Replacing random spans of the tokenized IUPAC name with sentinel tokens.\n2. Prepending the resulting sequence with a token indicating the original molecule’s computed\nproperty value. To obtain these property value tokens, we discretize the distribution of\nproperty values into three buckets, speciﬁed in Table 1.\n3. Training the model as in T5 to produce the sequence of redacted tokens, prepended by their\ncorresponding sentinel tokens.\nProperty <low> <med> <high>\nOctanol-water partition coeff. (logP) (−∞,−0.4) ( −0.4,5.6) (5 .6,∞)\nOctanol-water distribution coeff. (logD) (−∞,−0.4) ( −0.4,5.6) (5 .6,∞)\nPolar surface area (PSA) (0,90) (90 ,140) (140 ,∞)\nRefractivity (0,40) (40 ,130) (130 ,∞)\nTable 1: Numerical ranges across properties for each property value token. Cutoffs for logP,\nPSA, and refractivity were chosen as common thresholds for druglikeness screening following\n[50–52]. We use the same cutoff for logD as logP.\nThis conditional inﬁlling objective incentivizes the model to learn the relationship between the\ncomputed property value and the missing tokens. To make a localized edit to a molecule, we then\nreplace the desired fragments with sentinel tokens, change the property value token, and sample\nautoregressively from the predictive distribution. Thus, our approach hybridizes the ﬂexible editing\ncapabilities of ILM [49] with the controllability of CTRL [ 9]. See Appendix C for experimental\ndetails. Code is available at https://github.com/dhroth/c5t5.\n4 Results\nTo demonstrate the promise of combining IUPAC names with conditional modeling using T5, we\nexplore several molecular optimization tasks relevant to drug discovery. Speciﬁcally, we train\nC5T5 to make localized changes that affect the octanol-water partition and distribution coefﬁcients\n(logP, logD), polar surface area (PSA), and refractivity—four properties commonly used to estimate\nbioavailability of a candidate drug [50, 51, 53]. logP and logD measure the ratio of a compound’s\n2ignoring the embedding for “nitroso”\n5\n0 2 4 6 8\n7\n6\n5\n4\n3\n2\n20\n 10\n 0 10 20 30 40 50\n60\n40\n20\n0\n20\n40\n60\nOther\nCharge\nGroup\nCount/Mult.\nRing loc.\nStereo\nLocants\nElements\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\n5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n55\n50\n45\n40\n18192021222324\n25262728293031\n333234353637\n393840\n42\n4143\n45\n46 44\n484749\n505152\n55535456575859\n64\n26 27 28 29 30\n12\n11\n10\n9\n8\n7\n6\nFigure 3: T-SNE visualization of the word2vec embedding space. “Charge”: tokens that indicate\nformal charge. “Group”: functional groups and moieties. “Count/Mult”: multipliers. “Ring loc.”:\nfused-ring locants. “Stereo”: stereochemistry markers. “Locants”: simple locants. “Elements”:\nsingle-atom tokens. As shown, the 2D location of tokens carries high semantic meaning; for example,\nlocants are not only collocated, but are approximately in order.\n6\n2\n 0 2 4 6\nOrig. logP\n5\n0\n5\n10\n15\nGenerated logP\nT arget: <high>\nT arget: <low>\n2\n 0 2 4 6\nOrig. logD\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nGenerated logD\nTarget: <high>\nTarget: <low>\n50 75 100 125 150 175 200\nOrig. Refractivity\n50\n100\n150\n200\n250Generated Refractivity\nTarget: <high>\nTarget: <low>\n50 100 150 200\nOrig. Polar SA\n50\n100\n150\n200\n250Generated Polar SA\nTarget: <high>\nTarget: <low>\nFigure 4: Calculated property values of optimized molecules vs. original. Values computed for\n30 randomly chosen starting molecules. Top left: octanol-water partition coefﬁcient. Top right:\noctanol-water distribution coefﬁcient at pH of 7. Bottom left: molar refractivity. Bottom right: polar\nsurface area. Blue violins show the distribution of generated molecule properties when the model\nwas asked to complete molecules to achieve a <high> property value, and red for <low>. Cutoffs\nbetween <high> and <med> and between <med> and <low> are shown as dashed blue and red lines,\nrespectively. The black dashed line is y=x.\nsolubility in octanol, a lipid-like structure, and water; drugs need to be somewhat soluble in both to\nbe orally absorbed. PSA and refractivity both relate to charge separation within the molecule. As\nshown in Appendix B, C5T5 generates mostly valid and novel molecules, with values of logP that lie\noutside of the range of a “best in dataset” baseline for targeted modiﬁcations.\n4.1 C5T5 Successfully Modiﬁes Properties\nFirst, we demonstrate that the localized changes proposed by C5T5 do in fact control the property\nvalue as desired. C5T5 allows domain experts to choose where to make changes to a molecule based\non their intuition and particular application: the user masks out tokens in the IUPAC name that can be\nmodiﬁed, and the model ﬁlls in the masked spans in a way that changes the property value as directed.\nThere is no canonical way to choose particular tokens to mask for evaluation purposes, so we simply\nchoose a number of starting molecules randomly from PubChem [47], and then we iteratively mask\nall length-one to length-ﬁve spans (to match the training distribution) and run inference. In some\ninstances, there is only one way to ﬁll in the mask that yields a valid IUPAC name, regardless of the\ndesired property value (e.g. a single-token mask that removes a parenthesis, a comma between two\nlocants, or a hyphen after a locant). This would not be an issue in real usage, so in our evaluation\nwe simply ignore cases when the model generates the starting molecule. We also experiment with\nmasking multiple spans per molecule during inference, as is done during training. This is useful in\npractice when there are multiple areas of the molecule that can be changed in tandem to achieve the\ndesired property value, but in our evaluation we observe qualitatively similar results when masking\nonly a single span, so for computational efﬁciency we limit ourselves to single spans. We expect\nmulti-span masks to be much more important when optimizing for more complex properties.\n7\nFigure 4 shows that C5T5 successfully generates molecules with higher property values when passed\n<high>, and with lower property values when passed <low>. The model is much more successful at\nraising property values than lowering them, especially for refractivity and polar surface area. For both\nof these properties, increasing the property value is straightforward: just add polar groups to replace\nwhatever tokens were masked. In contrast, lowering these properties is only possible when the mask\ncoincides with a polar group, in which case the model must ﬁnd a non-polar substitute while still\nmaintaining the molecule’s validity. Even if unsuccessful at lowering these two property values on\naverage, C5T5 can still be used in this case to suggest a number of candidate edits, and the one with\nthe lowest property value can be selected using a property prediction model. This is an improvement\nover high-throughput screening and untargeted machine-learning methods for molecular optimization,\nsince it isn’t restricted to a predeﬁned library of candidate molecules, and it still allows the user to\nchoose particular parts of the molecule to modify.\nTable 2: Tokens most preferentially added when C5T5 is asked to make modiﬁcations resulting\nhigh vs. low logP values. Multipliers compare the actual rate of adding tokens compared to the\nexpected number if the model drew randomly from the data distribution independent of property\nvalue. Blue tokens are hydrocarbons (i.e. lipophilic groups). Red tokens contain hydrogen bonding\ndonors or acceptors (i.e. hydrophilic groups)\nTarget: <high> Target: <low>\ntrityl 77.0x phospho 48.1x\npentadecyl 20.2x phosphonato 44.7x\nZ 17.7x sulﬁnam 41.2x\nperylen 11.8x hydrazon 34.3x\nundecyl 8.1x sulﬁnato 26.3x\nheptadecyl 7.9x Z 17.9x\nylium 7.6x oxonium 10.3x\nisoindolo 5.9x amoyl 9.3x\nbH 5.9x carbamic acid 8.6x\niod 5.8x sulﬁn 6.9x\n4.2 Modiﬁed Tokens are Chemically Intuitive\nOne main advantage of C5T5 is that the changes it suggests are in the intuitive language of IUPAC\nnames, rather than suggesting arbitrary combinations of atoms. Table 2 shows the tokens that the\nmodel most preferentially adds to a molecule when asked to produce low vs. high logP. Unsurprisingly,\nthe most common tokens added when increasing logP are generally long carbon chains (pentadecyl,\nundecyl, heptadecyl) and other hydrocarbons (trityl, perylen). Conversely, when the model is asked\nto produce low-logP modiﬁcations, hydrophilic groups are added. LogP is a simple metric, and by\nproposing molecular edits for logP that are obvious and easily understandable to domain experts, we\nexpect users to gain conﬁdence that C5T5 will suggest reasonable edits for more complex properties.\nTo further investigate the types of optimizations C5T5 suggests, Figure 5 visualizes two of the starting\nmolecules from Figure 4 with logP values of−2.4 and 5.8. For each molecule, we mask spans as usual\nand generate after prepending with <low> (molecules on the left) and <high> (molecules on the right).\nThe IUPAC name of the top starting molecule is “3,3-bis(aminomethyl)pentane-1,5-diol,” where “bis”\nsigniﬁes that the CNH2 group should be duplicated, and “diol” means duplicate OH groups at the\nends. By virtue of the IUPAC name encoding symmetry, C5T5 is easily able to generate similarly\nsymmetric molecules. For example, the molecule in the top left is “3,3-bis(aminomethyl)pentane-\n1,5-disulﬁnic acid,” where the “ol” has been replaced with “sulﬁnic acid.” Although symmetry is\noften highly desired, C5T5 is not limited to generating symmetric molecules. For example, the\nmiddle molecule on the right is “3,3-bis(aminomethyl)-1-heptadecylpentane-1,5-diol”—C5T5 added\nan additional carbon chain non-symmetrically at the end of the pentane.3\nSometimes C5T5 generates valid but unstable compounds. For example, the neighboring NH2 groups\nin the bottom left molecule in the top half of Figure 5 are unstable, and would turn into aldehydes in an\n3Although this is not a preferred IUPAC name, it is still unambiguous, and therefore valid and parseable.\n8\nDecrease logP Increase logP\nOriginal logP: -2.4\nlogP: -5\nlogP: -3.5\nlogP: -3\nlogP: 10.9\nlogP: 5.2\nlogP: 14.2\nlogP: 1.1\nOriginal logP: 5.8\nlogP: 1.4\nlogP: 1.4\nlogP: 10.8\nlogP: 8.1\nlogP: 8.1\nFigure 5: Visualizations of base and logP-optimized molecules. Two molecules from the logP plot\nin Figure 4, with three molecules after optimizing with C5T5 for each of <low> and <high> logP.\naqueous solution. All machine learning methods are susceptible to this sort of mistake, underscoring\nthe importance of the type of human-in-the-loop optimization that C5T5 enables.\n5 Discussion and Conclusion\nWe propose C5T5, a simple and effective zero-shot method for targeted control of molecular properties\nwith transformers. Unlike prior approaches that make user-targeted modiﬁcations, our method requires\nno database of paired edits; instead, it simply trains in a self-supervised fashion on a large dataset\nof molecules and coarse estimates of their molecular property values. Core to our method is the\nuse of IUPAC names as a data representation, which captures molecular structure at an appropriate\nlevel of abstraction and enables an intuitive editing interface for domain experts. C5T5 successfully\nrediscovers chemically-intuitive strategies for altering four drug-related properties in molecules, a\nnotable feat given the absence of any human demonstration of these editing strategies.\nOur work also has several limitations. The select-and-replace interface provided by the inﬁlling\nobjective may not always match the needs or preferred design process of biochemists who might wish\nto use it. The interface also only suggests how to ﬁll in missing parts of a molecule, and it relies on\ndomain expertise or enumeration to decide which parts of the molecule should be changed to begin\nwith. In addition, we explore only a coarse-grained bucketing of property values, leaving a more\nﬁne-grained treatment for future work. IUPAC names might also be too limiting in cases where a user\nwants to edit a subgraph of a molecule that does not correspond neatly to a small number of IUPAC\ntokens. Finally, although there are many potential positive impacts to better drug design, including\nsafer and more effective medicines to prolong the length and quality of people’s lives, the generality\nof this method means that bad actors could optimize molecules towards harmful properties as well.\nFuture work will investigate using C5T5 with more molecular properties, such as the power conversion\nefﬁciency of solar cells. We also leave to future work extending C5T5 to jointly model multiple\nproperties, and adding a more ﬂexible editing interface.\n9\n6 Acknowledgements\nChemAxon’s calculator was used to compute IUPAC names, chemical descriptors, and molecular\nproperties (version 20.17.0, https://www.chemaxon.com).\nWe thank ACD/Labs for computing IUPAC names for several datasets.\nThis material is based upon work supported by the National Science Foundation Graduate Research\nFellowship Program under Grant No. DGE 1752814. Any opinions, ﬁndings, and conclusions or\nrecommendations expressed in this material are those of the author(s) and do not necessarily reﬂect\nthe views of the National Science Foundation.\nAT is supported by an OpenPhil AI fellowship.\nIn addition to NSF CISE Expeditions Award CCF-1730628, this research is supported by gifts from\nAmazon Web Services, Ant Group, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia,\nScotiabank, Splunk and VMware.\n10\nReferences\n[1] Jean-Louis Reymond, Lars Ruddigkeit, Lorenz Blum, and Ruud van Deursen. The enumeration\nof chemical space. Wiley Interdisciplinary Reviews: Computational Molecular Science, 2(5):\n717–733, 2012.\n[2] James P Hughes, Stephen Rees, S Barrett Kalindjian, and Karen L Philpott. Principles of early\ndrug discovery. British journal of pharmacology, 162(6):1239–1249, 2011.\n[3] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n[4] David Weininger. Smiles, a chemical language and information system. 1. introduction to\nmethodology and encoding rules. Journal of chemical information and computer sciences, 28\n(1):31–36, 1988.\n[5] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli,\nTimothy Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs\nfor learning molecular ﬁngerprints. In Proceedings of the 28th International Conference on\nNeural Information Processing Systems - Volume 2, NIPS’15, page 2224–2232, Cambridge,\nMA, USA, 2015. MIT Press.\n[6] Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, and Stefan Scherer.\nAffect-lm: A neural language model for customizable affective text generation. arXiv preprint\narXiv:1704.06851, 2017.\n[7] Jessica Ficler and Y . Goldberg. Controlling linguistic style aspects in neural language generation.\nArXiv, abs/1707.02633, 2017.\n[8] Tong Niu and Mohit Bansal. Polite dialogue generation without parallel data. Transactions of\nthe Association for Computational Linguistics, 6:373–389, 2018.\n[9] N. Keskar, Bryan McCann, L. Varshney, Caiming Xiong, and R. Socher. Ctrl: A conditional\ntransformer language model for controllable generation. ArXiv, abs/1909.05858, 2019.\n[10] Francesca Grisoni, Michael Moret, Robin Lingwood, and Gisbert Schneider. Bidirectional\nmolecule generation with recurrent neural networks. Journal of chemical information and\nmodeling, 60(3):1175–1183, 2020.\n[11] Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha\nFarias, and Alán Aspuru-Guzik. Objective-reinforced generative adversarial networks (organ)\nfor sequence generation models. arXiv preprint arXiv:1705.10843, 2017.\n[12] Benjamin Sanchez-Lengeling, Carlos Outeiral, Gabriel L Guimaraes, and Alan Aspuru-Guzik.\nOptimizing distributions over molecular space. an objective-reinforced generative adversarial\nnetwork for inverse-design chemistry (organic).\n[13] Nicola De Cao and Thomas Kipf. MolGAN: An implicit generative model for small molecular\ngraphs. ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative\nModels, 2018.\n[14] Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization.\nIn International Conference on Machine Learning, pages 3183–3191. PMLR, 2019.\n[15] Jiazhen He, Huifang You, Emil Sandström, Eva Nittinger, Esben Jannik Bjerrum, Christian Tyr-\nchan, Werngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemist’s\nintuition using deep neural networks. Journal of cheminformatics, 13(1):1–17, 2021.\n[16] Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal\ngraph-to-graph translation for molecule optimization. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=B1xJAsA5F7.\n11\n[17] Bonggun Shin, Sungsoo Park, JinYeong Bak, and Joyce C Ho. Controlled molecule generator\nfor optimizing multiple chemical properties. In Proceedings of the Conference on Health,\nInference, and Learning, pages 146–153, 2021.\n[18] Kevin Yang, Wengong Jin, Kyle Swanson, Regina Barzilay, and Tommi Jaakkola. Improving\nmolecular design by stochastic iterative target augmentation. In International Conference on\nMachine Learning, pages 10716–10726. PMLR, 2020.\n[19] Panagiotis-Christos Kotsias, Josep Arús-Pous, Hongming Chen, Ola Engkvist, Christian Tyr-\nchan, and Esben Jannik Bjerrum. Direct steering of de novo molecular generation with descriptor\nconditional recurrent neural networks. Nature Machine Intelligence, 2(5):254–265, 2020.\n[20] Rafael Gómez-Bombarelli, Jennifer N. Wei, David Duvenaud, José Miguel Hernández-Lobato,\nBenjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel,\nRyan P. Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven\ncontinuous representation of molecules. ACS Central Science, 4(2):268–276, 2018. doi: 10.\n1021/acscentsci.7b00572. URL https://doi.org/10.1021/acscentsci.7b00572. PMID:\n29532027.\n[21] Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative\nmodel based on conditional variational autoencoder for de novo molecular design. Journal of\ncheminformatics, 10(1):1–9, 2018.\n[22] Orion Dollar, Nisarg Joshi, David Beck, and Jim Pfaendtner. Attention-based generative models\nfor de novo molecular design. Chemical Science, 2021.\n[23] Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Constrained\ngraph variational autoencoders for molecule design. In Proceedings of the 32nd International\nConference on Neural Information Processing Systems, NIPS’18, page 7806–7815, Red Hook,\nNY , USA, 2018. Curran Associates Inc.\n[24] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder\nfor molecular graph generation. In International Conference on Machine Learning , pages\n2323–2332. PMLR, 2018.\n[25] Łukasz Maziarka, Agnieszka Pocha, Jan Kaczmarczyk, Krzysztof Rataj, Tomasz Danel, and\nMichał Warchoł. Mol-cyclegan: a generative model for molecular optimization. Journal of\nCheminformatics, 12(1):1–18, 2020.\n[26] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo\ndesign through deep reinforcement learning. Journal of cheminformatics, 9(1):1–14, 2017.\n[27] Viraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. Liggpt: Molecular generation\nusing a transformer-decoder model. 2021.\n[28] Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy\nnetwork for goal-directed molecular graph generation. In Proceedings of the 32nd International\nConference on Neural Information Processing Systems, NIPS’18, page 6412–6422, Red Hook,\nNY , USA, 2018. Curran Associates Inc.\n[29] Chence Shi*, Minkai Xu*, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang.\nGraphaf: a ﬂow-based autoregressive model for molecular graph generation. In International\nConference on Learning Representations, 2020. URL https://openreview.net/forum?\nid=S1esMkHYPr.\n[30] Yibo Li, Jianxing Hu, Yanxing Wang, Jielong Zhou, Liangren Zhang, and Zhenming Liu.\nDeepscaffold: A comprehensive tool for scaffold-based de novo drug discovery using deep\nlearning. Journal of chemical information and modeling, 60(1):77–91, 2019.\n[31] Jaechang Lim, Sang-Yeon Hwang, Seokhyun Moon, Seungsu Kim, and Woo Youn Kim.\nScaffold-based molecular design with a graph generative model. Chemical Science, 11(4):\n1153–1164, 2020.\n12\n[32] Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schnei-\nder, Nikolaus Stieﬂ, and Marc Brockschmidt. Learning to extend molecular scaffolds with\nstructural motifs. arXiv preprint arXiv:2103.03864, 2021.\n[33] Josep Arús-Pous, Atanas Patronov, Esben Jannik Bjerrum, Christian Tyrchan, Jean-Louis\nReymond, Hongming Chen, and Ola Engkvist. Smiles-based deep generative scaffold decorator\nfor de-novo drug design. Journal of cheminformatics, 12:1–18, 2020.\n[34] Maxime Langevin, Hervé Minoux, Maximilien Levesque, and Marc Bianciotto. Scaffold-\nconstrained molecular generation. Journal of Chemical Information and Modeling, 2020.\n[35] Jiazhen He, Felix Mattsson, Marcus Forsberg, Esben Jannik Bjerrum, Ola Engkvist, Christian\nTyrchan, Werngard Czechtizky, et al. Transformer neural network for structure constrained\nmolecular optimization. 2021.\n[36] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large\nscale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th\nACM international conference on bioinformatics, computational biology and health informatics,\npages 429–436, 2019.\n[37] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou\nHuang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural\nInformation Processing Systems, 33, 2020.\n[38] Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter,\nCostas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated\nchemical reaction prediction. ACS central science, 5(9):1572–1583, 2019.\n[39] Pavel Karpov, Guillaume Godin, and Igor V Tetko. A transformer model for retrosynthesis. In\nInternational Conference on Artiﬁcial Neural Networks, pages 817–830. Springer, 2019.\n[40] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion\nJones, Tom Gibbs, Tamas Feher, Christoph Angerer, Debsindhu Bhowmik, and Burkhard Rost.\nProttrans: Towards cracking the language of life’s code through self-supervised deep learning\nand high performance computing. bioRxiv, 2020. doi: 10.1101/2020.07.12.199554. URL\nhttps://www.biorxiv.org/content/early/2020/07/12/2020.07.12.199554.\n[41] Daria Grechishnikova. Transformer neural network for protein-speciﬁc de novo drug generation\nas a machine translation problem. Scientiﬁc reports, 11(1):1–13, 2021.\n[42] Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Stout: Smiles to iupac names using\nneural machine translation. Journal of Cheminformatics, 13(1):1–14, 2021.\n[43] Jennifer Handsel, Brian Matthews, Nicola Knight, and Simon Coles. Translating the molecules:\nadapting neural machine translation to predict iupac names from a chemical identiﬁer. 2021.\n[44] Lev Krasnov, Ivan Khokhlov, Maxim Fedorov, and Sergey Sosnin. Struct2iupac–transformer-\nbased artiﬁcial neural network for the conversion between chemical notations. 2021.\n[45] Henri A Favre and Warren H Powell. Nomenclature of organic chemistry: IUPAC recommenda-\ntions and preferred names 2013. Royal Society of Chemistry, 2013.\n[46] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[47] Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi\nHan, Jane He, Siqian He, Benjamin A Shoemaker, et al. Pubchem substance and compound\ndatabases. Nucleic acids research, 44(D1):D1202–D1213, 2016.\n[48] Daniel M Lowe, Peter T Corbett, Peter Murray-Rust, and Robert C Glen. Chemical name to\nstructure: Opsin, an open source solution, 2011.\n[49] Chris Donahue, Mina Lee, and Percy Liang. Enabling language models to ﬁll in the blanks. In\nACL, 2020.\n13\n[50] Arup K Ghose, Vellarkad N Viswanadhan, and John J Wendoloski. A knowledge-based approach\nin designing combinatorial or medicinal chemistry libraries for drug discovery. 1. a qualitative\nand quantitative characterization of known drug databases. Journal of combinatorial chemistry,\n1(1):55–68, 1999.\n[51] Daniel F Veber, Stephen R Johnson, Hung-Yuan Cheng, Brian R Smith, Keith W Ward, and Ken-\nneth D Kopple. Molecular properties that inﬂuence the oral bioavailability of drug candidates.\nJournal of medicinal chemistry, 45(12):2615–2623, 2002.\n[52] Stephen A Hitchcock and Lewis D Pennington. Structure- brain exposure relationships. Journal\nof medicinal chemistry, 49(26):7559–7583, 2006.\n[53] Sanjivanjit K Bhal, Karim Kassam, Ian G Peirson, and Greg M Pearl. The rule of ﬁve revisited:\napplying log d in place of log p in drug-likeness ﬁlters.Molecular pharmaceutics, 4(4):556–560,\n2007.\n[54] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In EMNLP (Demonstration), 2018.\n[55] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying\nthe carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.\n[56] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transform-\ners: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n[57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.\n[58] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,\nOlivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-\nlearn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830,\n2011.\n14\nMethod Base Repr. Model T? a UD?b CO?c\nC5T5 IUPAC T5 ✓ ✓ ✓\n[15] SMILES Seq2Seq/Transformer X X ✓\n[34] SMILES Any ✓ X X\n[33] SMILES LSTM ✓ X X\n[16] Graph/Motifs JT-V AE+GAN X X X\n[30] Graph/Atoms GNN+V AE Xd X X\n[31] Graph/Atoms V AE+GNN X X ✓\n[32] Graph/Motifs V AE+GNN X ✓ ✓\n[27] SMILES GPT X ✓ ✓\n[17] SMILES Transformer+LSTM X X ✓\n[35] SMILES Transformer ✓ X ✓\n[19] SMILES cRNN X ✓ ✓\n[20] SMILES V AE+ConvNet+GRU X ✓ X\n[21] SMILES cV AE X ✓ ✓\n[22] SMILES V AE+Transformer X ✓ X\n[23] Graph/Atoms V AE+GNN X ✓ X\n[28] Graph/Atoms GCN+GAN+RL X ✓ X\n[24] Graph/Motifs JT-V AE X ✓ X\n[25] Graph/Motifs CycleGAN+JT-V AE X ✓ ✓\n[29] Graph/Atoms Flow+RL X ✓ X\n[26] SMILES RNN+RL X ✓ X\nTable 3: Comparison of C5T5 to prior methods\naWhether or not the model makes targeted modiﬁcations – i.e. whether the user can specify which part of the\nmolecule the model should modify.\nbWhether or not the model can train without using paired molecular data.\ncWhether or not the model can switch objectives without re-training or re-optimizing.\ndThe authors propose ﬁltering results based on where the user wants to add a side chain, but the method itself\ndoes not target speciﬁc attachment points.\nA Comparison To Prior Methods\nTable 3 shows how C5T5 and prior methods for molecular optimization differ along several axes.\nB Additional Experiments\nSection 4.1 shows that C5T5 successfully modiﬁes property values. Here, for molecules generated to\noptimize logP, we show the novelty and validity of the generated molecules, along with a comparison\nto a baseline of the best eligible compound in PubChem. To match how the model was trained and\nhow new molecules were generated, eligible compounds are those that could be generated by masking\nany consecutive span of at most 5 IUPAC name tokens and replacing the masked tokens with any\nnumber of replacement tokens.4 Results are shown in Tables 4 and 5. Percent validity is the fraction\nof generated molecules that T5 generated with valid sentinel tokens that were considered chemically\nvalid by the ChemAxon logP calculator. Percent novelty is the fraction of distinct generated molecules\nthat do not appear in PubChem (excluding when C5T5 re-generated the source molecule). As shown,\ndespite the comparative difﬁculty of learning IUPAC name syntax compared to SMILES syntax,\nC5T5 consistently ﬁnds novel and valid molecules that signiﬁcantly outperform the best-in-dataset\nbaseline.\n4For computational efﬁciency, we ﬁlter out molecules that differ in length by more than 15 tokens, or that\nhave more than 15 non-overlapping tokens in their bag of tokens, before checking whether they could indeed be\ngenerated by masking some length-5 sequence of tokens.\n15\nTable 4: For each source molecule, we show the percent of generated molecules that are novel (not\nin PubChem, including invalid names) and valid (can be parsed by ChemAxon), the number of\ngenerated molecules, the min/max logP of any generated molecule, the number of eligible compounds\nin PubChem and the min/max logP of all molecules in PubChem that could be generated by masking\nup to 5 consecutive tokens. IUPAC names of source molecules are listed in Table 5.\nSrc. # gen. % novel % valid max gen. min gen. # elig. max PC min PC\n1 82 95.1% 81.7% 14.22 -5.04 26 10.81 -2.7\n2 133 93.2% 91.7% 9.41 -3.82 28 1.92 -1.46\n3 217 98.6% 91.7% 9.15 -2.17 4 3.01 1.66\n4 140 100.0% 94.3% 10.21 1.15 3 8.18 3.78\n5 128 92.2% 88.3% 6.91 0.86 19 4.24 2.35\n6 160 100.0% 75.6% 8.16 0.47 4 2.56 1.6\n7 159 99.4% 86.2% 9.05 -2.22 8 3.91 -1.34\n8 137 99.3% 84.7% 14.74 -2.97 1 1.08 1.08\n9 122 95.1% 81.1% 8.57 -4.89 34 4.75 -0.07\n10 112 92.9% 88.4% 8.44 -2.96 112 5.03 -2.3\n11 127 97.6% 81.1% 9.52 0.11 9 5.13 2.28\n12 114 90.4% 85.1% 7.71 -3.32 515 6.13 -2.95\n13 115 94.8% 90.4% 8.12 -1.91 36 4.05 0.3\n14 149 97.3% 85.2% 14.09 -4.3 7 2.1 0.44\n15 135 100.0% 83.0% 7.52 -0.7 2 5.08 4.12\n16 214 100.0% 97.7% 6.86 -1.83 3 1.91 -0.05\n17 156 99.4% 92.3% 10.58 -2.88 2 0.76 0.76\n18 231 98.7% 83.1% 9.79 -1.35 6 5.6 3.46\n19 148 93.2% 82.4% 9.98 -0.19 63 7.01 0.76\n20 143 96.5% 72.7% 14.12 0.33 14 4.61 1.31\n21 232 99.1% 85.3% 9.9 -1.6 6 3.15 1.14\n22 150 96.0% 92.7% 7.96 -0.74 22 4.95 2.71\n23 127 94.5% 87.4% 7.54 -3.1 18 4.67 0.37\n24 160 99.4% 82.5% 7.54 -1.53 2 1.28 1.28\n25 274 100.0% 95.6% 10.82 1.12 2 6.12 5.82\nC Experimental Details\nDataset Preparation We download PubChem from ftp.ncbi.nlm.nih.gov/pubchem/\nCompound/CURRENT-Full/XML/ and extracted each molecule’s Preferred IUPAC Name and com-\nputed octanol-water partition coefﬁcient (logP). There are 109M total compounds in the version of\nPubChem we downloaded in January 2021. For experiments using logP, we used the XLogP3 values\nfrom PubChem, which were computed using OpenEye’s software. For logD (pH=7), refractivity,\nand polar surface area, we computed values using ChemAxon’s calculator with default parameters.\nSeparately for each target property, we excluded chemicals that had no logP value in PubChem or\nthat were not parseable by ChemAxon’s calculator. Of the remaining molecules, we randomly split\ninto a training set with 90M compounds and a validation set with ∼10M-19M compounds.\nTokenization We use HuggingFace’s T5Tokenizer, which is based on the SentencePiece algorithm\n[54]. Because our goal is to have tokens that domain experts are familiar with, we do not train\nSentencePiece on the IUPAC names, since doing so learns both combinations of and substrings of\nmoiety names. Instead, we manually specify the tokens to be all the keywords in the Opsin IUPAC\nname parsing library [48]. To these keywords, we add locants 1–100, stereochemistry markers (R, S,\nE, Z,...), and a few miscellaneous tokens to e.g. handle spiro centers. This leads to a total of 1274\ntokens. After tokenization, we truncate all names to at most 128 tokens.\nTraining We train a t5-large model (∼700M params) available from HuggingFace that was pre-\ntrained on English text. We keep the ﬁrst 1274 embeddings from the pretrained embedding table,\nalong with the pretrained embeddings for the 100 sentinel tokens. When training, we mask 15%\nof the tokens in each input in spans of mean length 3 tokens, with a minimum span length of 1.\n16\nTable 5: IUPAC Name lookup table for Table 4\nID Source Molecule\n1 3,3-bis(aminomethyl)pentane-1,5-diol\n2 1-(3-hydroxypropyl)-N-(1-methoxybutan-2-yl)pyrazole-4-sulfonamide\n3 4-chloro-N-[2-[[2-(4-ﬂuorophenyl)acetyl]amino]ethyl]-1,3-thiazole-5-carboxamide\n4 1-[(1S)-1-(3-ﬂuorophenyl)propyl]-3-iodoindole\n5 4-(4-ﬂuorophenyl)-N-[(1R,2R)-2-methylcyclohexyl]piperazine-1-carbothioamide\n6 N’-(3-ethyl-4-oxophthalazine-1-carbonyl)-4-methyl-2-phenyl-1,3-thiazole-5-carbohydrazide\n7 (E)-2-methoxy-3-methylhex-4-en-1-ol\n8 N-methyl-1-[2-(4-methylthiadiazol-5-yl)-1,3-thiazol-4-yl]methanamine\n9 2-[(7-methyl-[1,2,4]triazolo[1,5-a]pyridin-2-yl)amino]ethylurea\n10 4-[[2-(2-oxopyridin-1-yl)acetyl]amino]benzoic acid\n11 [6-prop-2-enoxy-4-(triﬂuoromethyl)pyridin-2-yl]hydrazine\n12 4-(2-methylphenyl)sulfonylpiperidin-3-amine\n13 3-[ethyl(2-methylpropyl)amino]propane-1-thiol\n14 6-methoxy-4-N-methyl-4-N-[(2-methylfuran-3-yl)methyl]pyrimidine-4,5-diamine\n15 3-phenylmethoxy-5-(triﬂuoromethoxy)quinoline-2-carboxylic acid\n16 3-[4-[acetamido-[3-methoxy-4-[(2-methylphenyl)carbamoylamino]phenyl]methyl]piperidin-1-yl]-\n3-phenylpropanoic acid\n17 (3R)-3-[[(2S)-2-[benzyl(methyl)amino]butanoyl]amino]pyrrolidine-1-carboxamide\n18 6-cyclobutyl-2-N-[3-(1-ethylsulﬁnylethyl)phenyl]-5-(triﬂuoromethyl)pyrimidine-2,4-diamine\n19 6-ﬂuoro-2-(4-phenylpyridin-2-yl)-1H-benzimidazole\n20 4-chloro-3-(2-oxo-1,3-dihydroindol-5-yl)benzonitrile\n21 1-(6-tert-butylpyridazin-3-yl)-N-methyl-N-[(2-methyl-1,3-oxazol-4-yl)methyl]azetidin-3-amine\n22 2-[(4aR,8aS)-3,4,4a,5,6,7,8,8a-octahydro-1H-isoquinolin-2-yl]-N-(2,4-dimethoxyphenyl)acetamide\n23 2-[2-(2,4-dichlorophenoxy)ethoxy]-4-methoxybenzoic acid\n24 (2Z)-2-[(1,7-dimethylquinolin-1-ium-2-yl)methylidene]-1-ethyl-7-methylquinoline\n25 N’-[(3S)-1-[[3-(2,4-dichlorophenyl)phenyl]methyl]-2-oxoazepan-3-yl]-\n3-(2-methylpropyl)-2-prop-2-enylbutanediamide\nWe use a linear warmup of the learning rate for 10,000 steps followed by a1/T decay. All models\nwere trained using the AdamW optimizer. We train the logP model for 2.5M iterations using a max\nlearning rate of 10−3. We train the refractivity/logD/Polar SA models using a maximum learning rate\nof 2 ×10−4/10−4/2 ×10−4 starting from the logP model after 1M/2.5M/2.5M iterations. (We trained\nusing the latest model available when we started a run.) All models were trained on 8 NVIDIA A100s\nwith a batch size of 16 per GPU.\nGeneration We generate novel molecules greedily from the output of T5’s decoder. We discard\nany generations where the sentinel tokens do not line up, and we further discard any molecules that\ncannot be parsed by ChemAxon’s calculators.\nCloud Computing Cost We train T5-Large models [3] on PubChem for each investigated property\nvalue using AWS p4d.24xlarge instances in the us-east-1 region. The logP model was trained for\n2.5M iterations over 8 days; the logD and refractivity models for 350k iterations over 1 day each\n(initialized from the logP model after 2.5M iterations); and the Polar SA model for 1.6M iterations\nover 5 days (initialized from the logP model after 1M iterations). Total on-demand AWS cost for the\nmodels presented here is ∼$12,000, and total carbon dioxide-equivalent emissions is∼320 kg [55].5\nSoftware We use HuggingFace 4.2.2 (Apache 2.0 license) [ 56], PyTorch 1.8.0 (BSD) [ 57],\nChemAxon 20.17.0 (Academic License), scikit-learn 0.24.2 (New BSD) [58] and python 3.9 (PSFL).\n5The emissions calculator does not yet support NVIDIA A100 GPUs, so we calculated with V100-32GB\ninstead.\n17",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6713925004005432
    },
    {
      "name": "Organic molecules",
      "score": 0.5151037573814392
    },
    {
      "name": "Molecule",
      "score": 0.41393423080444336
    },
    {
      "name": "Materials science",
      "score": 0.403967022895813
    },
    {
      "name": "Chemistry",
      "score": 0.3014116585254669
    },
    {
      "name": "Electrical engineering",
      "score": 0.28045278787612915
    },
    {
      "name": "Engineering",
      "score": 0.18241021037101746
    },
    {
      "name": "Organic chemistry",
      "score": 0.09825935959815979
    },
    {
      "name": "Voltage",
      "score": 0.0895959734916687
    }
  ]
}