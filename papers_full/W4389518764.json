{
  "title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models",
  "url": "https://openalex.org/W4389518764",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2990069950",
      "name": "Jingyuan Qi",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2096148238",
      "name": "Zhiyang Xu",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095840125",
      "name": "Ying Shen",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2101408962",
      "name": "Minqian Liu",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2114807075",
      "name": "Di Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127284166",
      "name": "Qifan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166497444",
      "name": "Lifu Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4385569922",
    "https://openalex.org/W4368755685",
    "https://openalex.org/W4385572980",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W2250958092",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W2963609889",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W3199958362",
    "https://openalex.org/W2949134692",
    "https://openalex.org/W4375958700",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W3092945658",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4327526607",
    "https://openalex.org/W4226352076",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W3100436891",
    "https://openalex.org/W4312846625",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4385570412",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4225303417",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4385573161",
    "https://openalex.org/W3034457116"
  ],
  "abstract": "Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e. iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, and is more robust towards errors in the thinking process. Extensive experiments on several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual question-answering demonstrate significant performance improvements over the state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans’ recursively thinking process of complex reasoning problems.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4177–4199\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nThe Art of SOCRATIC QUESTIONING : Recursive Thinking with Large\nLanguage Models\nJingyuan Qi∗♠ Zhiyang Xu∗♠ Ying Shen†♠ Minqian Liu†♠\nDi Jin♡ Qifan Wang♣ Lifu Huang♠\n♠Virginia Tech ♡Amazon Inc. ♣Meta AI\n{jingyq1,zhiyangx,yings,minqianliu,lifuh}@vt.edu\ndjinamzn@amazon.com wqfcr@meta.com\nAbstract\nChain-of-Thought (CoT) prompting enables\nlarge language models to solve complex rea-\nsoning problems by generating intermediate\nsteps. However, confined by its inherent single-\npass and sequential generation process, CoT\nheavily relies on the initial decisions, caus-\ning errors in early steps to accumulate and im-\npact the final answers. In contrast, humans\nadopt recursive thinking when tackling com-\nplex reasoning problems, i.e., iteratively break-\ning the original problem into approachable sub-\nproblems and aggregating their answers to re-\nsolve the original one. Inspired by the hu-\nman cognitive process, we propose SOCRATIC\nQUESTIONING , a divide-and-conquer style al-\ngorithm that mimics the recursive thinking pro-\ncess. Specifically, SOCRATIC QUESTIONING\nleverages large language models to raise and\nanswer sub-questions until collecting enough\ninformation to tackle the original question. Un-\nlike CoT, SOCRATIC QUESTIONING explicitly\nnavigates the thinking space, stimulates effec-\ntive recursive thinking, and is more robust to-\nwards errors in the thinking process. Exten-\nsive experiments on several complex reasoning\ntasks, including MMLU, MATH, LogiQA, and\nvisual question-answering demonstrate signifi-\ncant performance improvements over the state-\nof-the-art prompting methods, such as CoT,\nand Tree-of-Thought. The qualitative analysis\nclearly shows that the intermediate reasoning\nsteps elicited by SOCRATIC QUESTIONING are\nsimilar to humans’ recursively thinking process\nof complex reasoning problems12 .\n1 Introduction\nThe art of Socratic Questioning is\nimportant for critical thinkers and\nexcellence of thought. What Socratic\nadds is systematicity, depth, and a keen\n1* Co-first Authors, †Co-second Authors\n2All the programs and necessary resources are released in\nhttps://github.com/VT-NLP/SOCRATIC-QUESTIONING\nQuestion:\n A ball slides down a landslide of height h,\nthe length of the landslide is l, ball math is m, and the\nangle between the landslide and the ground is \nθ. \nFind\nthe velocity v of the ball at the landslide bottom.\nIgnore friction force and air resistance.\n v = sqrt(2 * g * h * sin( θ) / (1 + ( l * sin( θ)) / (2 * h)))\nStandard Prompting\nChain-of-Though (CoT)\n The kinetic energy theorem states that the net work\ndone on an object equals its change in kinetic energy.\nIn this case, the net work done is done by the\nfrictional force.\n The work done by friction can be\ncalculated as W = f * l, where l is the landslide length\nand f= \nμ * \nN. Write the equation as: \nμ * \nm * g *\ncos(\nθ) * \nl = (1/2) * m * v^2\n. Solve that equation, \nv =\nsqrt((2 * \nμ * \ng * cos(\nθ) * \nl) / 1)\n.\nSocratic Questioning\n Solve 0 + mg * h = 1/2 * m * v² + 0, we get \nv = sqrt(2 * g * h)\n.\n 1. Does this problem obey the energy conservation\nlaw?\n2.\nWhat is the mechanical energy of the initial state?\n3.\nWhat is the mechanical energy in the final state?\nAgain\n, find the velocity V of the ball at the bottom of\nthe landslide. Ignore friction force.\n I do not know.\n 1. Because there is no external force doing work, so\nthis question follows engergy conservation law.\n2.\nAt begining, ball's velocity is 0, so its kinetic\nenergy is 0. And the potential energy is mg*h.\n3.\nAt the end, ball's height is 0, so the potential\nenergy is 0. And kinetic energy is 1/2 * m * v².\nFigure 1: Example of a complex question solved by the\nStandard Prompting, Chain-of-Thought, and SOCRATIC\nQUESTIONING . Accumulated incorrect reasoning are\nhighlighted in red.\ninterest in assessing the plausibility of\nthings.\n- L. ELDER and R. PAUL, 1998\nOne unique capability that allows humans to\nexcel at solving complex reasoning problems is\nrecursive thinking. If the answer is not immediately\nachievable, humans think deeper by recursively\ndecomposing the complex problem into simpler\nand solvable sub-problems.\nRecently, by scaling up the parameters, large-\n4177\nInput\nOutput\nInput\nOutput\nInput\nOutput\nInput\nOutput\n… … …\n(a)Input-OutputPrompting(b)ChainofThought(CoT) (d)TreeofThought(ToT) (e)SOCRATIC QUESTIONING(Ours)\nThoughtExplorationBacktracking\nInput\nOutput\n…\n(c)Self-ConsistencyChainofThought(CoT)\n… …\nFigure 2: Schematic comparison of various prompting methods. Each blue rectangle box represents a thought\nserving as an intermediate reasoning step in the problem-solving process. SOCRATIC QUESTIONING incorporates\nboth a top-down exploration process (in red line) to deconstruct complex problems into smaller sub-questions and a\nbottom-up backtracking process (in green line) to recursively solve these sub-questions and gather solutions for\nhigher-level problems.\nscale language models (LLMs) (Brown et al., 2020;\nChung et al., 2022; OpenAI, 2022; Touvron et al.,\n2023) gain emerging capabilities, such as Chain-\nof-Thought (CoT) (Wei et al., 2022) which decom-\nposes the complex problem and solves it step by\nstep. Though CoT has been proven to be effective\non various complex reasoning tasks, it’s in nature\na single-pass and sequential thinking process that\ngenerates the next step based on previous steps,\nthus only exploring a single way of thinking to ap-\nproach a problem and easily accumulating errors\nfrom previous steps (Turpin et al., 2023). In ad-\ndition, CoT lacks the ability to refine the already\ngenerated reasoning path, as shown in Figure 1.\nInspired by the recursive thinking of humans,\nwe propose SOCRATIC QUESTIONING , a novel\ndivide-and-conquer fashion algorithm that prompts\nlanguage models to solve complex reasoning prob-\nlems. As shown in Figure 2 (e), SOCRATIC QUES -\nTIONING consists of a top-down exploration pro-\ncess and a bottom-up backtracking process. Specif-\nically, in the top-down exploration process, the\noriginal complex problem is decomposed into sim-\npler or related sub-problems until the sub-problems\ncan be solved. In the bottom-up backtracking pro-\ncess, the solutions to the sub-problems are returned\nand selectively used to solve the original prob-\nlem. The fundamental component that drives SO-\nCRATIC QUESTIONING is a SELF -QUESTIONING\n(SQ) module, that leverages large-scale language\nmodels to proactively raise and answer questions\nthat are essential to solving the target question.\nSOCRATIC QUESTIONING recursively backtracks\nand tailors the intermediate thoughts acquired\nfrom SELF -QUESTIONING until reaching an an-\nswer to the original input question. It explic-\nitly navigates the thinking space and is more ro-\nbust towards thinking errors compared with pre-\nvious prompting methods including CoT, Self-\nConsistency Chain-of-Thought (Wang et al., 2023),\nand Tree-of-Thought (Yao et al., 2023), as shown\nin Figure 2.\nTo show the effectiveness ofSOCRATIC QUES -\nTIONING , we conduct extensive experiments on var-\nious complex reasoning tasks including the chem-\nistry and physics tasks (Hendrycks et al., 2020),\nmathematical tasks (Hendrycks et al., 2021), and\nreading comprehension tasks (Liu et al., 2020). Ad-\nditionally, we showcase the generalizability of our\nmethod by conducting experiments with few-shot\nmultimodal reasoning on VQA-V2 (Goyal et al.,\n2017), OK-VQA (Marino et al., 2019), and AOK-\nVQA (Schwenk et al., 2022) datasets. Experimen-\ntal results indicate that SOCRATIC QUESTIONING\nsubstantially improves performance over CoT, SC-\nCoT, and ToT across all language tasks and out-\nperforms several strong baselines in few-shot mul-\ntimodal reasoning. The qualitative analysis fur-\nther demonstrates that SOCRATIC QUESTIONING\nis capable of eliciting the intermediate reasoning\nsteps through SELF -QUESTIONING , like a critical\nthinker, and solving complex reasoning problems.\nThe main contributions of our paper are as follows:\n• We propose SOCRATIC QUESTIONING , a\nnovel prompting algorithm that can navigate\nthe cognitive thinking space in a recursive\nmanner.\n• We introduce the SELF -QUESTIONING mod-\nule, a core component that actively probes\ncomplex problems from various perspectives\nby raising and addressing questions essential\nfor solving the main problem.\n• Our approach achieves significant improve-\nments over the previous prompting methods\nin various complex reasoning tasks.\n4178\n2 Related Work\nPrompting Large Language Models With the\nscaling of both modal size and corpus size, large\nlanguage models (LLMs) such as GPT-3 (Brown\net al., 2020) and ChatGPT (OpenAI, 2022) have\nexhibited emergent abilities, including prompting\n(Brown et al., 2020), in-context learning (Dong\net al., 2023), and commonsense reasoning (Wei\net al.). One notable example of emergent abilities\nis the Chain-of-Thought (CoT) (Wei et al., 2022)\nwhich steers large language models to resolve com-\nplex problems by guiding them to produce a se-\nquence of intermediate steps before giving the final\nanswer. Self-Consistency Chain-of-Thought (SC-\nCoT) (Wang et al., 2023) improves naive CoT by\nsampling multiple reasoning paths and selecting the\nmost consistent answer. SC-CoT is based on the as-\nsumption that given a complex reasoning problem,\nmultiple reasoning paths can lead to the unique\ncorrect answer. Tree-of-Thought (ToT) (Yao et al.,\n2023) proposes to break the thinking process into\nsmall steps and at each step, the language model\ndeliberately decides a set of next steps to try.\nMultimodal Reasoning with Large Language\nModels Recent studies have explored the collab-\noration among diverse language and visual models\n(Yang et al., 2022; Zeng et al., 2022; Huang et al.,\n2022). For example, PICa (Yang et al., 2022) uti-\nlize image captions as the bridge between visual\nmodel and GPT-3 to peform few-shot knowledge-\nbased VQA. Socratic models (Zeng et al., 2022)\npresent a modular framework that utilizes language-\nbased exchange between pre-trained models and\nother modules. However, these studies only rely on\ntext as the shared interface, which can inevitably\nlead to information loss when translating visual\ninformation into language. In addition, several con-\ncurrent studies (Wu et al., 2023; Surís et al., 2023;\nLu et al., 2023) have also explored the utilization\nof large language models for composing various\nlanguage and visual models.\nQuestion Decomposition Recent research has\nunderscored the effectiveness of question decom-\nposition and sub-question generation techniques\nin tackling complex tasks. DECOMPRC (Min\net al., 2019), for instance, utilizes a limited\namount of human-labeled data to train a span-\nbased sub-question generator and simplifies multi-\nhop questions into single-hop questions. Similarly,\n(Nogueira and Cho, 2017) leverages reinforcement\nlearning for weakly supervised question generation\nand (Perez et al., 2020) introduces ONUS, an algo-\nrithm that harnesses large-scale questions sourced\nfrom the internet to perform unsupervised question\ndecomposition. More recently, (Patel et al., 2022)\nproposes an alternative approach to enhance the\nperformance of LLMs by decomposing challeng-\ning questions into simpler sub-questions on various\ntasks. Notably, the efficacy of question decom-\nposition has been demonstrated across a range of\ntasks and domains, including solving mathematical\nproblems (Shridhar et al., 2022), medical question\nanswering (Roberts et al., 2014), and factual cor-\nrection (Huang et al., 2023).\n3 Method\n3.1 S OCRATIC QUESTIONING\nFigure 3 shows the overview of the SOCRATIC\nQUESTIONING approach, which is essentially a\nrecursive thinking process involving a top-down\nexploration process (in red line) and a bottom-\nup backtracking process (in green line). The\ntop-down exploration process proactively breaks\ndown the question into simpler sub-questions until\nthe sub-questions are answered with high confi-\ndence. The bottom-up backtracking process re-\ncursively solves questions in which the answers to\nsub-questions are collected to solve the higher-level\nmore complex questions.\nIn the beginning, we are given a target question\nQ0,0\n1 , the context C (if provided), and an optional\nhint H0,0\n1 . The hint is initially Null but will be\nupdated and enriched as the recursive thinking pro-\ncess continues and results from sub-questions are\naggregated. We first run the top-down process to\nexplore the thinking space by invoking the SELF -\nQUESTIONING module. We use depth d and turn\nt to identify the node in our reasoning tree. Depth\nd refers to the traditional depth of the recursion\nalgorithm. Turn t refers to the times of SOCRATIC\nQUESTIONING invoking the SELF -QUESTIONING\nmodule for each question. For example, at depth d,\nturn t, SELF -QUESTIONING takes in the ith ques-\ntion Qd,t\ni , hint Hd,t\ni , the context C, and decides\nif it can answer the question Qd,t\ni : (1) If SELF -\nQUESTIONING can directly output the answer Ad,t\ni\nfor the question Qd,t\ni with high confidence, the\nbottom-up backtracking process starts by convert-\ning the answer Ad,t\ni to a hint ˜Hd,t\ni with a QA-to-\nHint module ( ˜H0,t\ni equals A0,t\ni directly when d =\n4179\nFigure 3: Overview of our SOCRATIC QUESTIONING algorithm.\n0) and adding ˜Hd,t\ni into the hints Hd−1,t of the\nparent question Qd−1. (2) If SELF -QUESTIONING\ncannot directly output an answer with high con-\nfidence, it outputs a set of sub-questions Qd+1,t\nrelated to Qd,t\ni . Then we run SELF -QUESTIONING\non each newly generated sub-question Qd+1,t\nj un-\ntil it’s answered with high confidence. Once we\nobtain the answers to all the sub-questions Qd+1,t,\nwe convert the answers into hints and incorporate\nthem to update Hd,t\ni to Hd,t+1\ni . We then run SELF -\nQUESTIONING on Qd,t+1\ni again with updated hints\nHd,t+1\ni . This recursive process continues until we\nreach the tree’s root and the original question Q0\n1\nis answered by ˜H0\n1 . We provide the pseudo-code\nof SOCRATIC QUESTIONING in Algorithm 1.\n3.2 S ELF -QUESTIONING\nSELF -QUESTIONING is designed to answer the\ngiven question, self-check the answer, and raise\nsub-questions. At depth d, turn t, SELF -\nQUESTIONING takes in the ith question Qd,t\ni , the\ncontext C (if available), and hints Hd,t\ni (if avail-\nable) and tries to generate an answer or a set of\nrelated sub-questions. SELF -QUESTIONING con-\nsists of two modules, a Question-Answering (QA)\nModule that outputs an answer Ad,t\ni for Qd,t\ni based\non C and Hd,t\ni , and an associated confidence level:\nhigh, medium, or low. If the confidence of the an-\nswer is high, or either depth d or turn t met the\npre-defined limit dm and tm, SELF -QUESTIONING\ninvokes the QA2H module to merge the question\nQd,t\ni and answer Ad,t\ni to hint ˜Hd,t\ni as output (when\nd = 0, we skip the merging process because the\nAlgorithm 1: SOCRATIC QUESTIONING\nInput: Question Qd,t\ni , Hint Hd,t\ni , Context C, Current\nDepth d, Max Depth dm, Current Turn t, Max\nTurn tm, Question Answer Prompt PQA,\nQuestion Generate Prompt PQG, QA to Hint\nPrompt PQA2H\nOutput: Hint ˜Hd,t\ni\n1 for t ≤tm do\n// call self-questioning\n2 < Qd+1,t, Hd+1,t, C >←\nSELF -QUESTIONING (Qd,t\ni , Hd,t\ni , C,\nd, dm, t, tm, PQA, PQG) ;\n3 if Qd+1,t ̸= ∅then\n4 for each Qd+1,t\nj ∈Qd+1,t do\n// recursively answer\nsub-questions\n5 ˜Hd+1,t\nj ←\nSOCRATIC QUESTIONING (Qd+1\nj ,\nHd+1,t\nj , C, d + 1, dm, t, tm, PQA,\nPQG);\n// gather hint\n6 Hd.insert( ˜Hd+1,t\nj ));\n7 else\n8 ˜Hd,t\nj ←Hd+1,t[0];\n9 return ˜Hd,t\nj ;\n10 t ←t + 1;\nanswer A0,\n1 is the final answer and does not need\nto be rewritten to hint). Both Max Depth dm and\nMax Turn tm prevent SOCRATIC QUESTIONING\nfrom infinite recursion. On the other hand, if the\nconfidence of the answer is lower than high, a\nQuestion-Generation (QG) Module is called to\ngenerate a set of sub-questions{Qd+1,t\n0 , .., Qd+1,t\nn }\nto collect more information based on Qd,t\ni , C, and\n4180\nAlgorithm 2: SELF -QUESTIONING\nInput: Question Qd,t\ni , Hint Hd,t\ni , Context C, Current\nDepth d, Max Depth dm, Current Turn t, Max\nTurn tm, Question Answer Prompt PQA,\nQuestion Generate Prompt PQG, QA to Hint\nPrompt PQA2H\nOutput: < Qd+1,t, Hd+1,t, C >\n1 Must_Answer ←False;\n2 if d = dm or t = tm then\n3 Must_Answer ←True;\n// call the Question-Answering module\n4 < Ad,t\ni , confidence >←\nQA(Qd,t\ni , Hd,t\ni , C, PQA) ;\n5 if confidence = high or Must_Answer then\n6 if d ̸= 0then\n// merge QA to a hint\n7 ˜Hd,t\ni ←QA2H(Qd,t\ni , Ad,t\ni , PQA2H) ;\n8 else\n9 ˜Hd,t\ni ←Ad ;\n10 Qd+1 ←∅;\n11 Hd+1,t ←{ ˜Hd,t\ni };\n12 else\n// call the Question-Generation\nmodule\n13 Qd+1,t ←QG(Qd,t\ni , Hd,t\ni , C, PQG) ;\n14 Hd+1,t ←∅;\n15 return < Qd+1,t, Hd+1,t, C >;\nHd,t\ni , where n < nm and nm denotes the max-\nimum number of sub-questions to be generated.\nAlgorithm 2 shows the pseudo-code of the SELF -\nQUESTIONING algorithm.\n3.2.1 Question-Answering (QA) Module\nThe QA module aims to answer either the target\nquestion or a sub-question asked by the SELF -\nQUESTIONING module, based on the optional con-\ntext and hints. We propose to leverage a large-scale\nlanguage model (LLM), such as GPT-3 or Chat-\nGPT (OpenAI, 2022), to answer the question given\ntheir superior reasoning capabilities demonstrated\nin previous studies (Brown et al., 2020; Zhang et al.,\n2022; Wei et al., 2022; Touvron et al., 2023; Yao\net al., 2023).\nSpecifically, the input to the QA module consists\nof the given question Qd,t\ni , the context C, the op-\ntional hints Hd,t\ni , and a prompt PQA designed to\nguide the QA module to generate an answer Ad,t\ni\nbased on the inputs and output a confidence level.\nWhen the hints Hd,t\ni are available, PQA also asks\nthe QA module to indicate which hints ared used\nto produce the answer.\nAd,t\ni , confidence= QA(Qd,t\ni , Hd,t\ni , C, PQA), (1)\nwhere confidence ∈{high, medium, low}.\n3.2.2 Question-Generation (QG) Module\nWhen the QA module outputs an answer for ques-\ntion Qd,t\ni with low confidence, it’s very likely that\nthe answer is not correct and we need to collect\nadditional hints to help the QA module produce\na more confident answer. To do so, we design a\nQuestion-Generation (QG) module to raise a set\nof sub-questions that are related to Qd,t\ni . The QG\nmodule is also based on a large language model,\nsuch as ChatGPT, that takes the question Qd,t\ni , op-\ntional hints Hd,t\ni , the context C, and a prompt PQG\nas input and outputs a set of sub-questions:\n{Qd+1\n0 , ...,Qd+1\nn }= QG(Qd,t\ni , Hd,t\ni ,C, PQG), (2)\nwhere n < nm. Intuitively, the sub-questions\nshould be simpler than Qd,t\ni and more likely to be\nanswered by the QA module with high confidence.\n3.2.3 QA-to-Hint (QA2H) Module\nSince the answers to sub-questions may not be self-\ncontained, we further design a QA-to-Hint module\n(QA2H) to merge each sub-question with its answer\ninto a statement. Specifically, we feed the sub-\nquestion Qd,t\ni and its answer Ad,t\ni to an LLM with\nthe prompt PQA2H which asks the LLM to rewrite\nthe question to a statement by incorporating the\nanswer:\n˜Hd = QA2H(Qd,t\ni , Ad,t\ni , PQA2H), (3)\n4 SOCRATIC QUESTIONING for Few-Shot\nMultimodal Reasoning\nSOCRATIC QUESTIONING can be naturally applied\nto text-based complex reasoning tasks as all the key\ncomponents are based on large language models,\nsuch as ChatGPT. There are two critical challenges\nwhen applying SOCRATIC QUESTIONING to mul-\ntimodal reasoning: (1) the language model cannot\nprocess visual information, and (2) simply apply-\ning a generic captioning model to convert visual\ncontent to natural language may not capture the key\ninformation required to answer a question.\nConverting Visual Information into Context\nWe propose to leverage LLMs to answer vi-\nsual questions since some of the visual questions\nare knowledge-demanding (Marino et al., 2019;\nSchwenk et al., 2022) and LLMs are capable of\nstoring commonsense knowledge and excel in com-\nplex reasoning tasks (Brown et al., 2020; Wei et al.,\n2022; Wang et al., 2023). To overcome the LLMs’\n4181\nshortcomings that they cannot perceive visual in-\nformation, previous works (Yang et al., 2022; Zeng\net al., 2022) leverage an image captioning model\nto convert visual information into text and use\nLLMs to perform few-shot visual question answer-\ning (VQA) tasks. However, considering the rich-\nness and density of the information contained in\nan image, a generic caption may not be able to\ncapture the key information that is necessary to\nanswer a question. Thus, in order to adapt our\nSOCRATIC QUESTIONING , we employ a visual\nperception model, BLIP-2 (Li et al., 2023), to de-\nscribe the content of the image that is specific to\na prompt. The input to BLIP-2 is an image I (i.e.,\nthe image input of the VQA task) and a text prompt\nQ, and the output is an image caption C describ-\ning the part of the image related to the prompt:\nC = BLIP-2(I, Q), where the text prompt Q cor-\nresponds to Qd in Equation (1) and the caption\nC corresponds to the context C in Equation (1).\nBy leveraging the visual perception model, we are\nable to resolve the hindrance and adopt our SO-\nCRATIC QUESTIONING framework on VQA. We\nshow more details on how we adapt SOCRATIC\nQUESTIONING to VQA in Appendix A.\n5 Experiment Setups\nLanguage-Only Tasks We leverage ChatGPT\nas the LLM for QA, QG, and QA2H modules,\nand provide detailed prompts for each module\nin Appendix K. We evaluate SOCRATIC QUES -\nTIONING on several complex reasoning tasks,\nincluding the Physics and Chemistry tasks in\nMassive Multitask Language Understanding\n(MMLU) (Hendrycks et al., 2020) , Mathemat-\nical tasks in MATH (Hendrycks et al., 2021), and\nlogical reasoning tasks based on LogiQA (Liu\net al., 2020). We adopt several state-of-the-art\nprompting methods as baselines, including Stan-\ndard Prompting (SP) that directly prompts Chat-\nGPT to answers a question with a few in-context\nexamples. Chain-of-Thought (CoT) (Wei et al.,\n2022), Self-Consistency Chain-of-Thought (SC-\nCoT) (Wang et al., 2023), and Tree-of-Thought\n(ToT) (Yao et al., 2023). Following previous stud-\nies (Chowdhery et al., 2023; Hoffmann et al., 2022),\nwe use exact match to measure the accuracy for all\nlanguage-only tasks. More details for the baselines,\nevaluation metrics, and evaluation datasets are dis-\ncussed in Appendix C.1.\nMultimodal Tasks We use blip2-flan-t5-xl as\nour Visual Perception module. We leverage Chat-\nGPT (OpenAI, 2022) for Factual/Visual Ques-\ntion Generation and Factual Question Answering\nand GPT-3 (GPT-3-davinci-003) for Visual Ques-\ntion Answering 3, motivated by the observation\nthat ChatGPT tends to be excessively cautious\nand neutral, and avoids answering some ques-\ntions. We provide detailed sample prompts for each\nmodule in Appendix K. We evaluate SOCRATIC\nQUESTIONING on several visual question answer-\ning datasets, including VQA-V2 (Goyal et al.,\n2017), OK-VQA (Marino et al., 2019) and AOK-\nVQA (Schwenk et al., 2022) , and compare our\napproach with several baselines, including BLIP-\n2 (Li et al., 2023) and. PICa (Yang et al., 2022).\nMore details for implementation, baselines, and\ndatasets are discussed in Appendix C.2. For evalu-\nation, we employ the conventional VQA accuracy\nmetric (Goyal et al., 2017) to measure the perfor-\nmance. To alleviate stringent penalization for mi-\nnor discrepancies between predicted answers and\nground truth, we normalize the answers by convert-\ning plural forms to singular forms and changing\nthe tense of verbs to present tense. In addition, to\naddress the conventional metric’s limitation due\nto synonyms and expression differences, we de-\nsign semantic-based accuracy by employing Chat-\nGPT to evaluate the correctness of the predicted\nanswers (Fu et al., 2023; Liu et al., 2023b). We\nprovide ChatGPT with the visual question, the pre-\ndicted answer and the ground-truth answer, and ask\nif the ground-truth answer and the predicted answer\ncan support each other. If the answer is \"Yes\", we\ntreat the predicted answer as correct. We show the\nexact prompts used for ChatGPT in Appendix K.8.\n6 Results and Discussions\n6.1 Quantitative Results\nLanguage-only Tasks Table 1 shows the quan-\ntitative results in terms of accuracy for language-\nonly reasoning tasks. Our method substantially\noutperforms previous state-of-the-art methods by\n4.34%, 2.98%, 4.22%, and 4.66% absolute gains\nin MATH, Physics, Chemistry, and Logic bench-\nmarks, respectively. This effectively demonstrates\nthe superiority of our approach. We also conduct an\nexperiment on how the maximum number of turns\ntm affects the performance. Specifically, we ex-\nperiment with the setting where tm = 2(2-Turns)\n3These components are detailed in Appendix A.\n4182\nMATH (DA) MMLU Physics MMLU Chemistry LogiQA Avg\nStandard-Prompting 7.00 65.11 53.20 54.67 45.00\nCoT (Wei et al., 2022) 7.33 67.66 57.14 48.33 45.12\nSC-CoT (Wei et al., 2022) 7.00 68.51 59.33 49.00 46.03\nToT (Yao et al., 2023) 0.00 40.00 26.60 22.22 29.46\nSOCRATICQUESTIONING(2-Turns) 7.67 71.49 63.55 59.33 50.51\nSOCRATICQUESTIONING(3-Turns) 11.67 69.36 63.55 58.00 50.65\nTable 1: Accuracy (%) using Exact Match. The best performance is highlighted in bold and the second best\nperformance is highlighted with underline.\nQuestion\n: A heavy rock and a light rock in free fall (zero air resistance) have the same acceleration. The heavy rock doesn't have a\ngreater acceleration because the \nOption\n: [\"A. force due to gravity is the same on each.\", \"B. air resistance is always zero in free fall.\", \"C. inertia of both rocks is\nthe same.\", \"D. ratio of force to mass is the same.\"]\n The force of gravity depends on the mass of each object\nand the distance between them, but since both objects are\nin free fall, they are at the same distance from the center\nof the Earth and \nexperience the same force of gravity\n.\nThus, option A, \"force due to gravity is the same on\neach,\" is the correct answer. Thus, the final answer is: \nA\n.\nChain-of-Thought\n To answer the question, we can use Newton's second law\nof motion, which states that force equals mass times\nacceleration. Since both rocks are in free fall, they are\nonly under the influence of gravity, which means that \nthe\nforce due to gravity is the same on each rock\n. Therefore,\noption \nA\n is the correct answer.\nTree-of-Thought\n - Hints:\n    1. The acceleration due to gravity is 9.81 m/s^2.\n    2. The formula for force due to gravity is F = (G*m1*m2) / r^2.\n    3. The mass of an object does not affect its acceleration in free\nfall.\n- Answer:\nBy hints 1 and 3, the \nheavy rock does not have a greater\nacceleration than the light rock\n because their force to mass ratio is\nthe same. Option A is incorrect because according to hint 2, \nthe\nheavy rock experiences a greater gravitational force due to its\nlarger mass\n. Option B is incorrect because air resistance is not a\nfactor for objects in free fall. Option C is incorrect because the\ninertia of an object is proportional to its mass, so the heavy rock\nhas greater inertia than the light rock. Thus, the final answer is: \nD\nSocratic Questioning:\nFigure 4: Qualitative results of CoT, ToT, andSOCRATIC QUESTIONING on the Physics task. The correct answer of\nthis example is D.\nModel VQA-V2 OK-VQA AOK-VQA\nBLIP-2 (Li et al., 2023)36.7 21.14 0\nPICa (Yang et al., 2022)43.18 29.94 28.6\nSOCRATICQUESTIONING 46.64 31.24 29.58\nTable 2: Traditional VQA Accuracy (%) based on Ex-\nactly Match. The best performance is highlighted in\nbold and the second best performance is highlighted\nwith underline.\nand tm = 3(3-Turns). From Table 1, the model\nwith maximum 2 turns achieves better performance\non Physics and LogiQA datasets, while the model\nwith tm = 3performs better on the MATH dataset.\nOne possible reason is that the Physics and LogiQA\nbenchmarks may not be challenging enough and\nreasoning within 2 turns is sufficient to answer most\nof the questions. We provide a concrete example\nin Appendix G.1.\nMultimodal Tasks Table 2 and 3 show the quan-\ntitative results using traditional VQA accuracy and\nsemantic-based accuracy, respectively. For both\nresults, our SOCRATIC QUESTIONING method out-\nperforms the previous state-of-the-art approaches\nModel VQA-V2 OK-VQA AOK-VQA\nBLIP-2 (Li et al., 2023)57.2 46.75 43.29\nPICa (Yang et al., 2022)49.8 48.05 46.85\nSOCRATICQUESTIONING 54.4 53.03 49.55\nTable 3: Semantic-based VQA Accuracy (%) using NLI.\nThe best performance is highlighted in bold and the\nsecond best performance is highlighted with underline.\non most benchmarks, often by a large margin. The\nonly exception is semantic-based accuracy on the\nVQA-V2 dataset. A possible reason is that the\ntasks on VQA-V2 focus more on the visual recog-\nnition and detection aspect and do not require much\nreasoning capability and external knowledge.\n6.2 Qualitative Result\nLanguage-only Tasks Figure 4 shows the qual-\nitative results of SOCRATIC QUESTIONING and\nbaselines on the Physics task. As one can observe,\nSOCRATIC QUESTIONING can effectively prompt\nhints containing the necessary information to solve\nthe original problem and selectively use the hints\nto reach the correct final answer. On the other hand,\nCoT and ToT reach the wrong answer due to the\n4183\nQuestion:\nWhy are the\nchildren\nwearing hats?\n They are on a bus\nBLIP2\nImage Caption:\nA bus driving down a street\nwith people walking around\nAnswer: \nSun\nPICa\nImage Caption:\nA bus driving down a street\nwith people walking around\nHints:\nChildren wear hats for\nvarious reasons including\nprotecting their head and\nface from the sun, \nkeeping\nwarm in cold weather\n, and\nas a fashion accessory. \nThe \nweather conditions in\nthe image are cold\n.\nAnswer:\nWarmth\nSocratic Questioning:\nQuestion:\nThe line the horse is\nwalking on separates\nwhat?\n A car from a truck\nBLIP2\nImage Caption:\nA horse walking through a\nparking lot next to cars\nAnswer: \nPavement\nPICa\nImage Caption:\nA horse walking through a\nparking lot next to cars\nHints:\nThe purpose of the lines in a\nparking lot is to \nmark the\nparking spaces\n, direct the flow\nof traffic, etc.\nThe horse is walking on a\nwhite line\n.\nThe horse is walking in an area\nof the \nparking lot where none\ntype of vehicles are parked.\nAnswer:\nParking spaces\nSocratic Questioning:\nFigure 5: Qualitative results of few-shot VQA using\nBLIP-2, PICa, and SOCRATIC QUESTIONING (2-Depth\n2-Turn).\nAnswered CorrectlyAnswered Incorrectly\nAvg. Hints 3.28 3.68\nAvg. Depth 2.89 2.92\nTable 4: Averaged numbers of hints and depth of SO-\nCRATIC QUESTIONING used for questions answered\ncorrectly and incorrectly, respectively.\npoorly sampled reasoning path.\nMultimodal Tasks Figure 5 shows several exam-\nples of few-shot VQA tasks from the baselines and\nSOCRATIC QUESTIONING . We demonstrate that\nthe hints acquired via the sub-problems are highly\nrelated to the original problem (e.g., \"weather con-\nditions are cold\"), and by considering the collected\nhints, the SOCRATIC QUESTIONING reaches the\ncorrect final answer (e.g., \"warmth\"). In contrast,\nthe answer from BLIP-2 is irrelevant to the given\nquestion, due to the generic caption.\n6.3 How do the Numbers of Turns and Depths\nAffect the Model?\nPerformance Breakdown w/ Number of Turns\nTo study how the number of reasoning turns affects\nthe performance across different benchmarks, we\ninvestigate how the baselines and our method per-\n0.00\n20.00\n40.00\n60.00\n80.00\nPhysics LogiQA Chemistry Math(DA) Avg\nStandard-Prompting CoT SC-CoT ToT\nSOCRATIC QUESTIONING (2-Turns)\nFigure 6: Accuracy (%) on the examples that triggered\n2 turns of reasoning by SOCRATIC QUESTIONING .\n0.00\n20.00\n40.00\n60.00\n80.00\nPhysics LogiQA Chemistry Math(DA) Avg\nStandard-Prompting CoT SC-CoT ToT\nSOCRATIC QUESTIONING (2-Turns)\nSOCRATIC QUESTIONING (3-Turns)\nFigure 7: Accuracy (%) on the examples that triggered\n3 turns of reasoning by SOCRATIC QUESTIONING .\nform on the examples that triggered 2 and 3 turns of\nreasoning by SOCRATIC QUESTIONING in Figure 6\nand Figure 7, respectively. This experiment can be\nconsidered as breaking down the results in Table 1\ninto two groups based on the number of reasoning\nturns. From Figure 6, our approach outperforms the\nbaselines on all benchmarks except for the MATH\ndataset. From Figure 7, our approach outperforms\nthe baselines on relatively challenging tasks such\nas MATH but performs more poorly on easier tasks\nsuch as Physics. This indicates SOCRATIC QUES -\nTIONING with more turns can tackle challenging\nproblems more effectively.\nThe Effect of Hyperparameters tm and dm In\naddition to the discussion in 6.1, we conduct a more\nin-depth analysis of how the maximum number of\nturns tm and maximum number of depthsdm affect\nthe performance of our SOCRATIC QUESTIONING .\nIn Figure 8, we show the heat map under different\nhyperparameter settings, where the number in each\ncell is the accuracy (%) given a specific combina-\ntion of tm and dm. We observe two general trends:\n(1) the accuracy increases when tm gets larger, and\n4184\n8.00\n8.00\n11.67\n9.33\n10.00\n12.33\n10.00\n11.67\n11.00\n2\n3\n4\n2\n3\n4\nMaximum Turn\nMaximum Depth\nFigure 8: Quantitative results SOCRATIC QUESTION -\nING on the MATH dataset with different values of the\nhyperparameters tm and dm.\n(2) the accuracy decreases when dm gets larger.\nThese results imply that our approach can bene-\nfit from raising more questions directly related to\nthe original question. Also, performing reasoning\nwith a larger maximum depth does not yield bet-\nter performance since the benchmark may not be\nchallenging enough, and exploring at a deeper level\nmay introduce irrelevant information. We provide\na concrete example in Appendix G.2. In addition,\nwe analyze the computational cost of SOCRATIC\nQUESTIONING compared to other baselines in Ap-\npendix H, and show that while achieving stronger\nperformance, our proposed algorithm enjoys higher\nefficiency than most of baselines.\n6.4 How does the Difficulty of Questions\nAffect the Model?\nTable 4 presents the averaged numbers of hints\nand depth used to answer the original questions\nfor correct and incorrect answers. As one can ob-\nserve, for incorrect answers, the LLM raises more\nsub-questions, which demonstrates that the LLM\ntends to explore more thinking space when tackling\nquestions that it does not know the answers. This\ntrend also agrees with the depth. If the question\nis hard for the LLM, the model tends to break the\nsub-questions into even more basic questions.\n7 Conclusion\nWe present SOCRATIC QUESTIONING , a novel\ndivide-and-conquer fashion algorithm that is in-\nspired by human’s recursive thinking processes.\nSOCRATIC QUESTIONING consists of a top-down\nreasoning phase that decomposes a complex prob-\nlem into simpler sub-problems and a bottom-top\nphase where the solutions to the sub-problems are\nrecursively returned and used to solve the original\nproblem at higher levels. Extensive experiments\non four challenging language-only tasks and the\nfew-shot VQA task validate the effectiveness of\nour SOCRATIC QUESTIONING . Moreover, quali-\ntative analysis demonstrates our approach can ef-\nfectively elicit intermediate reasoning steps and\nconsequently yield a correct final answer while en-\njoying transparency and interpretability.\nLimitation\nThe self-checking functionality lacks sufficient sen-\nsitivity to incorrect responses, as its confidence esti-\nmation heavily relies on LLMs themselves. While\nwe employed ChatGPT as the backbone for our al-\ngorithm, its tendency towards overconfidence leads\nto a low frequency of sub-question generation.\nOur study exhibits a lack of diversity in visual\nmodels used to extract information from images.\nWe only use BLIP-2 (Li et al., 2023) as an im-\nage caption model in current experiments. How-\never, the incorporation of diverse visual models,\nsuch as dense caption models, Optical Character\nRecognition (OCR), or scene graph models, may\npotentially yield a broader spectrum of image in-\nformation, thus facilitating the resolution of sub-\nquestions. In addition, to help BLIP-2 to better fol-\nlow instructions from LLMs, we propose to lever-\nage recent techniques developed in visual instruc-\ntion tuning (Liu et al., 2023a; Xu et al., 2023b,a;\nDai et al., 2023).\nAdditionally, our experiments were constrained\nto the English language datasets and we only con-\nsider the VQA task to showcase the multi-modal\nperformance. However, given the generality of our\nalgorithm, we plan to test its functionality with\nmultilingual datasets and experiment it on other\ndomains, such as speech (You et al., 2020, 2022),\nand video (Rose et al., 2023).\nAcknowledgments\nThis research is based upon work supported by the\nU.S. DARPA ECOLE Program # HR001122S0052.\nThe views and conclusions contained herein are\nthose of the authors and should not be interpreted as\nnecessarily representing the official policies, either\nexpressed or implied, of DARPA or the U.S. Gov-\nernment. The U.S. Government is authorized to\nreproduce and distribute reprints for governmental\npurposes notwithstanding any copyright annotation\ntherein.\n4185\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2023. Palm: Scaling language mod-\neling with pathways. J. Mach. Learn. Res., 24:240:1–\n240:113.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven C. H. Hoi.\n2023. Instructblip: Towards general-purpose vision-\nlanguage models with instruction tuning. CoRR,\nabs/2305.06500.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey for in-context learning.\nCoRR, abs/2301.00234.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. CoRR,\nabs/2302.04166.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904–6913.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. CoRR, abs/2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the MATH dataset. CoRR,\nabs/2103.03874.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nKung-Hsiang Huang, Hou Pong Chan, and Heng Ji.\n2023. Zero-shot faithful factual error correction. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5660–5676, Toronto, Canada.\nAssociation for Computational Linguistics.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, Pierre Sermanet,\nTomas Jackson, Noah Brown, Linda Luu, Sergey\nLevine, Karol Hausman, and Brian Ichter. 2022. In-\nner monologue: Embodied reasoning through plan-\nning with language models. In Conference on Robot\nLearning, CoRL 2022, 14-18 December 2022, Auck-\nland, New Zealand , volume 205 of Proceedings\nof Machine Learning Research , pages 1769–1782.\nPMLR.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962–977.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\n4186\nKaplan. 2022. Language models (mostly) know what\nthey know. CoRR, abs/2207.05221.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi. 2023. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models. In International Conference on\nMachine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings\nof Machine Learning Research, pages 19730–19742.\nPMLR.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023a. Visual instruction tuning.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,\nYile Wang, and Yue Zhang. 2020. Logiqa: A chal-\nlenge dataset for machine reading comprehension\nwith logical reasoning. In Proceedings of the Twenty-\nNinth International Joint Conference on Artificial\nIntelligence, IJCAI 2020, pages 3622–3628. ijcai.org.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNLG evaluation using GPT-4 with better human\nalignment. CoRR, abs/2303.16634.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models.CoRR,\nabs/2304.09842.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Conference on Computer Vision and Pattern\nRecognition (CVPR).\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Multi-hop reading compre-\nhension through question decomposition and rescor-\ning. CoRR, abs/1906.02916.\nRodrigo Nogueira and Kyunghyun Cho. 2017. Task-\noriented query reformulation with reinforcement\nlearning. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 574–583, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nOpenAI. 2022. ChatGPT: Optimizing language mod-\nels for dialogue. https://openai.com/blog/\nchatgpt/.\nPruthvi Patel, Swaroop Mishra, Mihir Parmar, and\nChitta Baral. 2022. Is a question decomposition unit\nall we need? In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4553–4569, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nEthan Perez, Patrick S. H. Lewis, Wen-tau Yih,\nKyunghyun Cho, and Douwe Kiela. 2020. Unsuper-\nvised question decomposition for question answering.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 8864–\n8880. Association for Computational Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver?\nKirk Roberts, Kate Masterton, Marcelo Fiszman, Halil\nKilicoglu, and Dina Demner-Fushman. 2014. Anno-\ntating question decomposition on complex medical\nquestions. In Proceedings of the Ninth International\nConference on Language Resources and Evaluation,\nLREC 2014, Reykjavik, Iceland, May 26-31, 2014 ,\npages 2598–2602. European Language Resources\nAssociation (ELRA).\nDaniel Rose, Vaishnavi Himakunthala, Andy Ouyang,\nRyan He, Alex Mei, Yujie Lu, Michael Saxon, Chin-\nmay Sonar, Diba Mirza, and William Yang Wang.\n2023. Visual chain of thought: Bridging logical gaps\nwith multimodal infillings. CoRR, abs/2305.02317.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-okvqa: A benchmark for visual question answering\nusing world knowledge. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23–27, 2022, Proceedings, Part VIII, pages\n146–162. Springer.\nKumar Shridhar, Jakub Macina, Mennatallah El-Assady,\nTanmay Sinha, Manu Kapur, and Mrinmaya Sachan.\n2022. Automatic generation of socratic subquestions\nfor teaching math word problems. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 4136–4149. Association for Computational\nLinguistics.\nDídac Surís, Sachit Menon, and Carl V ondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning. CoRR, abs/2303.08128.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R. Bowman. 2023. Language models\ndon’t always say what they think: Unfaithful ex-\nplanations in chain-of-thought prompting. CoRR,\nabs/2305.04388.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2023. Self-consistency\nimproves chain of thought reasoning in language\n4187\nmodels. In The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\nEmergent abilities of large language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023. Visual\nchatgpt: Talking, drawing and editing with visual\nfoundation models. CoRR, abs/2303.04671.\nZhiyang Xu, Trevor Ashby, Chao Feng, Rulin Shao,\nYing Shen, Di Jin, Qifan Wang, and Lifu Huang.\n2023a. Vision-flan: Scaling visual instruction tuning.\nZhiyang Xu, Ying Shen, and Lifu Huang. 2023b. Multi-\ninstruct: Improving multi-modal zero-shot learning\nvia instruction tuning. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2023,\nToronto, Canada, July 9-14, 2023 , pages 11445–\n11465. Association for Computational Linguistics.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nYumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An\nempirical study of GPT-3 for few-shot knowledge-\nbased VQA. In Thirty-Sixth AAAI Conference on\nArtificial Intelligence, AAAI 2022, Thirty-Fourth Con-\nference on Innovative Applications of Artificial In-\ntelligence, IAAI 2022, The Twelveth Symposium on\nEducational Advances in Artificial Intelligence, EAAI\n2022 Virtual Event, February 22 - March 1, 2022 ,\npages 3081–3089. AAAI Press.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. CoRR,\nabs/2305.10601.\nChenyu You, Nuo Chen, Fenglin Liu, Shen Ge, Xian\nWu, and Yuexian Zou. 2022. End-to-end spoken\nconversational question answering: Task, dataset and\nmodel. In In Findings of NAACL 2022.\nChenyu You, Nuo Chen, Fenglin Liu, Dongchao Yang,\nand Yuexian Zou. 2020. Towards data distillation for\nend-to-end spoken conversational question answer-\ning. arXiv preprint arXiv:2010.08923.\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof\nChoromanski, Federico Tombari, Aveek Purohit,\nMichael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vin-\ncent Vanhoucke, and Pete Florence. 2022. Socratic\nmodels: Composing zero-shot multimodal reasoning\nwith language. CoRR, abs/2204.00598.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nA Adapting SOCRATIC QUESTIONING to\nVisual Question Answering\nQuestion-Generation (QG) Module Some tasks\n(e.g., OK-VQA, AOK-VQA) require commonsense\nknowledge. Although LLMs can retrieve knowl-\nedge from its parameter, they are prone to halluci-\nnation and the black-box retrieving process is hard\nto debug. In order to gain a clear understanding of\nthe factual knowledge used in answering a question,\nwe divide the QG module in Section 3.2.2 into two\nsub-modules: A Fact-Question-Generation (FQG)\nsub-module which generates factual questions re-\nlated to background knowledge of the given ques-\ntion, and a Visual-Question-Generation (VQG) sub-\nmodule generates visual questions, which aims to\nguide the Visual Perception module to focus on\nquestion-related image regions and seek more im-\nage information.\nQuestion-Answering (QA) Module To accom-\nmodate the two question types, we also divide\nthe QA module in section 3.2.1 into two sub-\nmodules: A Factual-Question-Answering module\n(FQA) and a Visual-Question-Answering module\n(VQA). Both FQA and VQA modules follow the\nsame formulation in Equation (1). The input C to\nVQA is the caption related to the question Qd and\nis prompted via the Equation of BLIP-2.\nSELF -QUESTIONING Figure 9 demonstrates the\ndetailed step of the SELF -QUESTIONING algo-\nrithm in the multimodal setting. At depth d, SELF -\nQUESTIONING algorithm takes in a visual ques-\ntion Qd which can be the original visual question\n(d = 0) or a sub-question generated by VQG, a\nquestion-related caption C, and hints Hd (if it is\navailable), and try to generate an answer Ad via\nVQA. If the confidence level of Ad is not high,\nthe SELF -QUESTIONING algorithm starts to raise\nsub-questions. First, the FQG module takes in Qd,\ncontext C, and hints Hd as input and raises a set\nof factual questions Qf . Each question in Qf is\nanswered by the FQA module and we denote the an-\nswer as Af . Each Qf and its answer Af is mearged\ninto a factual statement hf via the QA2H module\n4188\nand the statement is appended to hints Hd to form\nHd+1. Second, the VQG module takes in Qd, con-\ntext C, and hints Hd+1 and raises a set of visual\nquestions Qd+1.\nB Visualization of Recursive Thinking\nProcess\nFigure 10 shows a complete recursive thinking pro-\ncess of our SOCRATIC QUESTIONING method. It\ninvolves 4 additional questions to acquire addi-\ntional information to answer the target question.\nFrom this example, we see that LLMs, such as\nGPT-3 or ChatGPT, have strong capabilities not\nonly in reasoning but also self-questioning. Given\nthe target question to be answered, “ Why are the\nchildren wearing hats?”, LLMs are able to proac-\ntively acquire additional commonsense knowledge\nthrough factual questions, e.g., “What are the com-\nmon reasons why children wear hats?”, and fine-\ngrained visual information from the input image,\ne.g., “ What’s the position of the sun in the sky\nat the time the children are shown wearing hats”,\n“Are the weather conditions in the image cold or\nhot”. By combining the additional knowledge, e.g.,\n“cold weather makes people wear hats” and visual\ninformation, e.g., “ it is cold ”, acquired from the\nrecursive Self-Questioning process, the model fi-\nnally achieves the answer “warmth”. This analysis\ndemonstrates that the recursive thinking process\nof our approach is highly transparent and inter-\npretable.\nC Implementation Details\nC.1 Language-only Tasks\nImplementation Details We leverage Chat-\nGPT (OpenAI, 2022) as the LLM for QA, QG,\nand QA2H modules. We provide detailed prompts\nfor each module in Appendix K.\nBaselines Standard Prompting (SP) prompts\nChatGPT to directly answers a question with\na few in-context examples. Chain-of-Thought\n(CoT) (Wei et al., 2022) prompts ChatGPT to first\ngenerate the thinking process and then generate the\nanswer. We also add the thinking process into the\nin-context examples. Self-Consistency Chain-of-\nThought (SC-CoT) (Wang et al., 2023) proposes\nto run chain-of-thought multiple times on Chat-\nGPT and marginalize the thinking process by tak-\ning the most consistent answer. Tree-of-Thought\n(ToT) (Yao et al., 2023) is a recently proposed\nframework for improving the reasoning capability\nof language models. We follow their implemen-\ntation 4 which leverages tree-search algorithms to\nexplore the thinking space and select the best think-\ning path. 5\nEvaluation Metrics For a fair comparison,\nwe use exact match and measure the accuracy\nfor all language-only tasks following previous\nworks (Chowdhery et al., 2023; Hoffmann et al.,\n2022).\nAll questions in MMLU Physics, MMLU Chem-\nistry, and LogiQA are multiple-choice questions\nand the answer is always a single letter like “A”,\n“B” or “C”. To easily parse the model’s final output,\nwe use “Thus, the final answer is:” as the prefix for\nthe final answers (A or B or C or D, ect.) in the in-\ncontext examples for all methods. When we parse\nthe output, we first run a template-based method\nto extract the answers after “Thus, the final answer\nis:”. For a few instances (12.52% in CoT, 16.4%\nin ToT and 11.64% in Socratic Questioning on av-\nerage) that do not match the template as shown in\nFigure 4 ToT, the authors manually compare the\nmodel’s predictions to the ground truth answers.\nThus, we assure that the final performance of all\nmethods is not affected by the output formats.\nDatasets Massive Multitask Language Under-\nstanding (MMLU) (Hendrycks et al., 2020)\ndataset contains 57 diverse tasks and is used to\nmeasure the model’s complex reasoning capabil-\nity. In this work, we use the physics and chem-\nistry tasks which contain conceptual physics and\nchemistry multiple-choice questions, respectively.\nMATH (Hendrycks et al., 2021) dataset consists\nof challenging competition-level mathematics prob-\nlems which require strong mathematical reasoning\nability. LogiQA (Liu et al., 2020) dataset contains\nexpert-written questions for testing the logical rea-\nsoning capability of humans. For each task, we\nuse the validation set to make design decisions and\nmeasure the model’s performance on the test set.\nThe detailed statistics of all datasets can be found\nin Table 5.\n4https://github.com/kyegomez/\ntree-of-thoughts\n5By the time we submit the work, we don’t have access to\nGPT4 so we use ChatGPT for ToT.\n4189\nMATH MMLU\n(Physics)\nMMLU\n(Chemistry) LogiQA\nDev 60 22 26 60\nTest 300 235 203 300\nTable 5: Statistic of datasets for language-only tasks.\nC.2 Multimodal Tasks\nImplementation Details We use blip2-flan-t5-\nxl6 as our Visual Perception module. We leverage\nChatGPT (OpenAI, 2022) for the FQG, VQG, and\nFQA modules and GPT-3 (GPT-3-davinci-003) for\nthe VQA module. This decision is motivated by the\nobservation that ChatGPT tends to be excessively\ncautious and neutral, and avoids answering some\nquestions. We provide detailed sample prompts for\neach module in Appendix K.\nBaselines BLIP-2 (Li et al., 2023) is a pre-\ntrained vision-language model that leverages an\nefficient and generic pre-training strategy and is\nable to follow text prompts. We use the released\nblip2-flan-t5-xl checkpoint. PICa (Yang et al.,\n2022) prompts GPT-3 with generic image captions\nto solve VQA in an in-context learning manner.\nIn our experiments, we implement PICa by using\nblip2-flan-t5-xl as the image captioning model and\nGPT-3-davinci-003 as the LLM.\nEvaluation Metrics We employ the conventional\nVQA accuracy metric (Goyal et al., 2017) to mea-\nsure the performance. To alleviate stringent penal-\nization for minor discrepancies between predicted\nanswers and ground truth, we normalize the an-\nswers by converting plural forms to singular forms\nand changing the tense of verbs to present tense.\nIn addition, to address the limitation due to syn-\nonyms and expression differences, we employ Chat-\nGPT to evaluate the correctness of the predicted\nanswers (Fu et al., 2023; Liu et al., 2023b). We\nprovide ChatGPT with the visual question, the pre-\ndicted answer and the ground-truth answer, and ask\nif the ground-truth answer and the predicted answer\ncan support each other. If the answer is \"Yes\", we\ntreat the predicted answer as correct. We show the\nexact prompts used for ChatGPT in Appendix K.8.\nDatasets VQA-V2 (Goyal et al., 2017) is a\ndataset containing open-ended questions about im-\nages. OK-VQA (Marino et al., 2019) requires\n6https://huggingface.co/Salesforce/\nblip2-flan-t5-xl\nVQA-v2 OK-VQA AOK-VQA\nDev 100 100 100\nTest 500 462 444\nTable 6: Statistic of datasets for multi-modalities tasks.\nmodel to leverage external knowledge to answer vi-\nsual questions. AOK-VQA (Schwenk et al., 2022)\nis an augmented successor of OK-VQA, which\nrequire commonsense knowledge and strong rea-\nsoning capabilities to answer its questions. For\neach task, we use the validation set to make design\ndecisions and measure the model’s performance on\nthe test set. The detailed statistics of all datasets\ncan be found in Table 6 and Appendix E.\nD S ELF -QUESTIONING in the\nMultimodal Setting\nSee Figure 9.\nE Data Leakage in BLIP-2 and GPT-3\nModel VQA-V2 OK-VQA AOK-VQA\nBLIP-2 1.46 2.93 28.08\nGPT-3 35.88 23.95 20.4\nTable 7: Traditional VQA Accuracy (%) under the set-\nting where no image is provided in the input.\nIn our preliminary experiments, we discovered\nan issue that pre-trained models could be subject\nto data leakage during their pre-training stage. We\nobserved that the baseline models (i.e., BLIP-2 and\nGPT-3) achieved unjustifiably high performance\nacross all three VQA datasets even without taking\nimages as inputs (see Table 7). To address this\nissue, we applied a filtering process to remove such\ncontaminated instances. We first test the BLIP-2\nand GPT-3 on zero-shot VQA tasks while replacing\nthe original input image with an image composed\nentirely of black pixels of the same size. Then, we\nonly retain the samples where the models failed\nto yield a correct answer when the original image\nis not given. After the filtering, we adopt the 500,\n462, and 444 test samples for VQA-V2, OK-VQA,\nand AOK-VQA, respectively. We use these clean\nexamples for the evaluation throughout the rest of\nour experiments.\n4190\nMultimodal\nReasoning\n QA to Hint\n Factual Question Answering\n Factual Question Generation\nAnswer (A\nLOI \n):\nLack of information\nAnswer (A):\nJoss Whedon\nHint (H\nd+1\ni\n):\nJoss Whedon\nFactual Questions (Q\nf\n):\n1.\nWhat movies came in\n2012?\n2.\nWhich famous directors\nhad movies came in 2012?\nFactual Answers (A\nf\n):\n1.\nAvengers, Skyfall, ...\n2.\nJoss Whedon, Gary Ross, ...\nFactual Hints (H\nf\n):\n1.\nMovies cames in 2012 are:\nThe Avengers, ......\n2.\nFamous directors had\nmovie in 2012 are: Gary\nRoss, ...... Visual Question Generation\n QA to Hint\nQuestion (Q\nd\ni\n):\nWho directed\nthe 2012 movie\nshown on the\nTV in the\npicture?\nHints (H\nd \n):\nN/A\nImage (I):\n If  d = 1\nVisual Question 1 (Q\nd+1\n1\n):\nIs there any text on TV?\nVisual Question 2 (Q\nd+1\n2\n):\nIs there any movie character on\nTV?\nHints (H\nd+1\n):\n1.\nMovies cames in 2012 are:\nThe Avengers, ......\n......\nFigure 9: The overview of the SELF -QUESTIONING Algorithm.\nF Visualization of Complete S OCRATIC\nQUESTIONING\nSee Figure 10.\nG Concrete Example\nG.1 Large Maximum Number of Turn\nDue to the calibration error in LLMs (Jiang et al.,\n2021), sometimes the pre-trained model’s confi-\ndence is not aligned with the answer’s correctness.\nThus, in such cases, the model predicts “low” or\n“medium” confidence in correct answers in the\nearly turns and hence misses the correct answers. If\nwe use fewer turns, we can keep the answer in the\nearly turn regardless of the confidence and hence\nalleviate the calibration error. Below we show a\nconcrete example in which the model predicts the\ncorrect answer in 2 turns and predicts the incorrect\nanswer in 3 turns. When we increase the num-\nber of turns, Socratic Questioning may raise some\nless relevant sub-questions and hence introduce\nnoisy information in the reasoning process. This\nnoisy information can confuse the model, leading\nto incorrect responses to the original question. For\nexample, consider a simple physics question:\nThe speed of sound is slightly greater on a [\n\"A. cold day\", \"B. hot day\", \"C. day with steady\ntemperature\", \"D. None of these\"]?\nIn a 2-turn setting, our approach obtains hints:\n(1) \"The speed of sound increases with increasing\ntemperature.\", and (2) \"Humidity is a factor in the\nspeed of sound.\" According to the hints, it is obvi-\nous that the correct answer is B, which is chosen by\nour approach in the second turn with the \"middle\"\nconfidence. In a 3-turn setting, since the LLM does\nnot assign “high” confidence to the answer in the\n2 turn, our approach goes deeper in the third turn\nand gets more information (e.g., (3) \"The speed of\nsound can be affected by several factors, including\ntemperature, humidity and density of the medium.\",\n(4) \"The speed of sound depends on the density and\nelasticity of the medium it is traveling through, in\nterms of physical properties.\", (5) \"The speed of\nsound increases with humidity as a result of in-\ncreased air density.\") As a result, by considering\nmore hints, we potentially introduce less relevant\ninformation to the LLM and the noisy information\ncauses the LLM to change its answer to D.\nG.2 Large Maximum Number of Depth\nWe observe that as the depth increases, the context\ninformation in the original questions start to vanish\nand the answers to the sub-questions may be inac-\ncurate in the context of the original question. Thus,\nby adding the answers to sub-question in larger\ndepth as hints, we can introduce noises to the rea-\nsoning process of the LLM which results in wrong\nanswers. Consider a physics question example:\nWhen a spinning system contracts in the absence\nof an external torque, its rotational speed increases,\nand its angular momentum [ A. decreases, B. in-\ncreases, C. remains unchanged, D. may increase\nor decrease ]\"?\nSocratic Questioning raises a sub-question:\n\"What affects the rotational speed of a spinning\nsystem?\" The initial answer to this sub-question\n4191\nis “Conservation of angular momentum”, which\nprovides enough information to answer the origi-\nnal question. In a larger depth setting, the Socratic\nQuestioning raises a deeper sub-question: “What is\nthe relationship between rotational speed and angu-\nlar momentum in a spinning system?” The answer\nto this question is: “The angular momentum is di-\nrectly proportional to the rotational speed”. Incor-\nporate this hint, the Socratic Questioning changes\nthe answer of the first sub-question to: “The an-\ngular momentum is directly proportional to the\nrotational speed. ”, which results in an incorrect\nfinal answer B.\nH Evaluation of Computational Cost\nIn Table 8, we provide the theoretical number of\ncalls in CoT, SC-CoT, ToT and Socratic Question-\ning in 2 and 3 turns settings. We also provide the\nempirical results of the average number of calls\nper instance and average running time per instance\nin seconds for all methods. For SC-CoT, we fix\nthe number of calls to 20 times on all the datasets\nbased on the performance curve in (Wang et al.,\n2023). In ToT, k represents the number of thoughts\nallowed to be generated per step, T represents the\nmaximum number of steps and b represents the\nmaximum number of states to keep at each step\nin BFS. Following (Yao et al., 2023), we set k=5,\nT=3, and b=4. In Socratic Questioning, q repre-\nsents the maximum number of raised sub-questions\nfor a parent node.\nAs one can observe, Socratic Questioning with\n2 turns and 3 turns achieves better efficiency com-\npared to SC-CoT and ToT. The main reason is that,\nin the experimental datasets, most questions do not\nrequire a large amount of thinking steps to reach\nthe correct answers. Socratic Questioning, adap-\ntively raises sub-questions based on the complexity\nof the original question and arrives at the correct\nanswer without reaching the theoretical maximum\nnumber of turns or depth. In contrast, both SC-\nCOT and ToT employ fixed settings for the num-\nber of thoughts generated per step. For relatively\nstraightforward questions, these fixed settings in-\ntroduce high computational overhead, making the\nalgorithms less efficient in these questions.\nI Experimental Results on Other QA and\nMath Datasets\nTable 9 provides the performance of our method\nand two strong baselines on GSM8K and Strate-\ngyQA datasets. As one can observe, our method\nhas significant performance improvement com-\npared to baselines. We use ChatGPT with tempera-\nture 0.7 for all methods. For SC-CoT, we sample\n20 reasoning paths.\nWe tried our best to reproduce the results of CoT\nand SC-CoT reported in (Wang et al., 2023) on\nStrategyQA. Following (Wang et al., 2022), we\nuse the question-only set from BIG-bench collabo-\nration (2021) and use the exact same prompt tem-\nplate and in-context examples in SC-CoT. However,\nwe cannot reproduce the results on StrategyQA\nin (Geva et al., 2021) since Code-davinci-002 and\nCode-davinci-001 are no longer publicly available.\nIn addition, our results of ChatGPT on StrategyQA\nalso agree with more recent studies in (Qin et al.,\n2023).\nJ Experiment Results based on GPT-4\nTo showcase the generalizability of our approach,\nwe have run CoT and Socratic Questioning on\nMMLU Chemistry and LogiQA based on GPT-\n4. The experimental results show that our So-\ncratic Questioning approach still significantly out-\nperforms CoT.\nK Prmopt Templates\nTo make our method generalize to other reasoning\ndomains, we carefully design in-context demon-\nstrations to guide the LLM to generate more basic\nsub-questions in an efficient manner. More con-\ncretely, to create high-quality sub-questions in the\nin-context examples, we take the human reasoning\nprocess and domain knowledge into account and\ncarefully annotate the sub-questions by ensuring\nthat they are more basic questions compared to the\noriginal question and their solutions can contribute\nto the reasoning process of the original questions.\nFor examples of sub-questions, please refer to Fig-\nure 12. Based on our experiments in math, physics,\nchemistry and VQA domains, we argue that with a\nfew examples (5 in all our experiments) Socratic-\nQuestioning can generalize to a new domain.\nFollowing (Kadavath et al., 2022), we ask the\nLLM itself to output a confidence level, “high”,\n“middle”, or “low”, towards its answer. In the in-\ncontext demonstrations, we label the correct an-\nswers with supportive hints in the context as “high”\nconfidence, label the correct answers without sup-\nportive hints as “middle” confidence, and label in-\ncorrect answers as “low” confidence. In this way,\n4192\nStandard-\nPrompting\nCoT SC-\nCoT\nToT Socratic Questioning\n(2 turns)\nSocratic Questioning\n(3 turns)\nTheoretical Number of Calls1 1 20 k + b*k*(T-1) 3×∑d−1\ni=1[q×(t−1)]i 3×∑d−1\ni=1[q×(t−1)]i\nAvg. Calls per Instance 1 1 20 31.1 9.22 18.7\nAvg. Running Time per In-\nstance (second)\n0.33 3.35 67.09 77.99 34.15 53.65\nTable 8: Evaluation of computational cost of different methods.\nGSM8K StrategyQA\nCoT 79.0 59.7\nSC-CoT 86.0 63.0\nSocratic-Questioning 89.33 65.33\nTable 9: Accuracy (%) on GSM8K and StrategyQA\nusing Exact Match. The best performance is highlighted\nin bold.\nMMLU ChemistryLogiQA\nCoT 80.2 70.3\nSocratic-Questioning 85.73 75.3\nTable 10: Accuracy (%) of GPT-4 based approaches\nusing Exact Match. The best performance is highlighted\nin bold.\nwe can guide the model to align its confidence to\nthe correctness of the predicted answers. Our al-\ngorithm will continue raising sub-questions if the\nestimated confidence is not “high”. Please refer to\nFigure 11 for more examples.\nK.1 Prompts template of QA module\nSee Figure 11.\nK.2 Prompts template of QG module\nSee Figure 12.\nK.3 Prompts template of FQG\nSee Figure 13.\nK.4 Prompts template of FQA\nSee Figure 14.\nK.5 Prompts template of VQG\nSee Figure 15.\nK.6 Prompts template of VQA\nSee Figure 16 and 17.\nK.7 Prompts template of QA-to-Hint\nSee Figure 18.\nK.8 Prompt for chatGPT for semantic-based\naccuracy evaluation\nSee Figure 19.\n4193\nDepth\n:1 \nTurn\n:1 \nID\n: v1 \nType\n: Visual\nAnswer\n: lack of information\nDepth\n:1 \nTurn\n:1 \nID\n: v1 \nType\n: Factual\nOri Question\n: Why are the children wearing hats?\nContext\n: Image Caption: a bus driving down a\nstreet with people walking around\nDepth\n:2 \nTurn\n:1 \nType\n: Factual\nIdea 1: \nChildren may wear hats for different reasons\ndepending on various circumstances.\nFactual Question 1\n: What are the common reasons why\nchildren wear hats?\nDepth\n:2 \nTurn\n:1 \nID\n: f1 \nType\n: Factual\nQuestion\n: What are the common reasons why\nchildren wear hats?\nDepth\n:2 \nTurn\n:1 \nID\n: f1 \nType\n: Factual\nAnswer\n: Children wear hats to protect their head and face\nfrom the sun, to keep warm in cold weather, and as a fashion\naccessory.\nStat: \nChildren wear hats for various reasons including\nprotecting their head and face from the sun, keeping warm in\ncold weather, and as a fashion accessory.\nDepth\n:1 \nTurn\n:1 \nID\n: v1 \nType\n: Visual\nOri Question\n: Why are the children wearing hats?\nContext\n: Image Caption: a bus driving down a\nstreet with people walking around\nHint: \n1.\nChildren wear hats for various reasons including\nprotecting their head and face from the sun,\nkeeping warm in cold weather, and as a fashion\naccessory.\nDepth\n:2 \nTurn\n:1 \nType\n: Visual\nIdea 1: \nChildren wear hats to protect themselves from the sun.\nVisual Prompt 1\n: What is the position of the sun in the sky at\nthe time the children are shown wearing hats?\nIdea 2:  \nChildren wear hats to keep themselves warm in cold\nweather.\nVisual Prompt 2: \nAre the weather conditions in the image cold\nor hot?\nIdea 3: \nChildren wear hats as a fashion accessory.\nVisual Prompt 3:\n What are the different styles of hats being\nworn by the children?\nDepth\n:1 \nTurn\n:1 \nID\n: v1 \nType\n: Visual\nQuestion\n: Why are the children wearing hats?\nContext\n: Image Caption: a bus driving down a\nstreet with people walking around\nDepth\n:2 \nTurn\n:1 \nID\n: v1 \nType\n: Visual\nQuestion\n: What is the position of the sun in the sky\nat the time the children are shown wearing hats?\nContext\n:  Image Caption: the sun is high in the sky\nat the time the children are shown wearing hats\nHint\n:\n1.\nChildren wear hats for various reasons including\nprotecting their head and face from the sun,\nkeeping warm in cold weather, and as a fashion\naccessory\nDepth\n:2 \nTurn\n:1 \nID\n: v1 \nType\n: Visual\nAnswer\n: The answer is: high\nStat:\n The position of the sun in the sky is high at the time the\nchildren are shown wearing hats.\nDepth\n:2 \nTurn\n:1 \nID\n: v2 \nType\n: Visual\nAnswer\n: The answer is: cold\nStat: \nThe weather conditions in the image are cold.\nDepth\n:2 \nTurn\n:1 \nID\n: v3 \nType\n: Visual\nAnswer\n: The answer is: fashion\nStat: \nUnknown. The answer doesn't provide enough\ninformation.\nDepth\n:1 \nTurn\n:2 \nID\n: v1 \nType\n: Visual\nAnswer\n: Hints 1,3 are useful. The answer is: warmth\nDepth\n:2 \nTurn\n:1 \nID\n: v2 \nType\n: Visual\nQuestion\n: Are the weather conditions in the image\ncold or hot?\nContext\n:  Image Caption: the weather conditions in\nthe image are cold\nHint\n:\n1.\nChildren wear hats for various reasons including\nprotecting their head and face from the sun,\nkeeping warm in cold weather, and as a fashion\naccessory\nDepth\n:2 \nTurn\n:1 \nID\n: v3 \nType\n: Visual\nQuestion\n: What are the different styles of hats\nbeing worn by the children?\nContext\n:  Image Caption: the children are wearing\ndifferent styles of hats\nHint\n:\n1.\nChildren wear hats for various reasons including\nprotecting their head and face from the sun,\nkeeping warm in cold weather, and as a fashion\naccessory\nDepth\n:1 \nTurn\n:2 \nID\n: v1 \nType\n: Visual\nQuestion\n: What are the different styles of hats\nbeing worn by the children?\nContext\n: Image Caption: a bus driving down a\nstreet with people walking aroun\nHint\n:\n1.\nChildren wear hats for various reasons including\nprotecting their head and face from the sun,\nkeeping warm in cold weather, and as a fashion\naccessory\n2.\nThe position of the sun in the sky is high at the\ntime the children are shown wearing hats.\n3.\nThe weather conditions in the image are cold.\n4.\nUnknown. The answer doesn't provide enough\ninformation.\nFigure 10: Visualization of a complete recursive thinking process of SOCRATIC QUESTIONING (2-Depth 2-Turn).\nThe process is initialized on the left and is continued on the right.\n4194\n// System define\nImagine you are a thoughtful and logical student. You are\ngiven a question. Please use your best judgment to\nanswer the question step by step, and give the\nconfidence. If there are some hints, consider the hints.\nNote that, the final answer has to be a single letter, which\nis the ID of the correct option. If there are hints, present\nwhich hints you use.\n// Demonstration\nQuestion\n: \nThe plates of a capacitor are charged to a potential\ndifference of 5 V. If the capacitance is 2 mF, what is the\ncharge on the positive plate?\nOption\n: \n[A. 0.005 C, B. 0.01 C, C. 0.02 C, D. 0.5 C]\nNote: Form of output is: explanation + [Answer: \"option\nletter\"; Used hints: \"hints ID or None\"; Confidence: \"low,\nmiddle, or high\"].\nAnswer\n: \nTo find the charge on the positive plate of the capacitor,\nwe can use the formula Q = CV, where Q represents the\ncharge, C represents the capacitance, and V represents\nthe potential difference. Given that the potential\ndifference V is 5 V and the capacitance C is 2 mF\n(millifarads), we can substitute these values into the\nformula to find the charge Q. Q = (2 mF) * (5 V) To\nsimplify the calculation, we need to convert millifarads\n(mF) to farads (F). Since 1 mF is equal to 0.001 F, we\ncan rewrite the capacitance as 2 * 0.001 F. Q = (2 *\n0.001 F) * (5 V) = 0.01 C Thus, the charge on the\npositive plate of the capacitor is 0.01 C. Thus, the final\nanswer is: [Answer: B; Used hints: None; Confidence:\nlow]\nQuestion\n: \nHow much work is required to charge a 10 µF capacitor\nto a potential difference of 100 V ?\nOption\n: \n[A. 0.005 J, B. 0.01 J, C. 0.05 J, D. 0.1 J]\nNote: Form of output is: explanation + [Answer: \"option\nletter\"; Used hints: \"hints ID or None\"; Confidence: \"low,\nmiddle, or high\"].\nAnswer\n: \nTo calculate the work required to charge a capacitor, we\ncan use the formula: Work = 0.5 * C * V^2, where C\nrepresents the capacitance and V represents the potential\ndifference. Given that the capacitance C is 10 µF\n(microfarads) and the potential difference V is 100 V, we\ncan substitute these values into the formula: Work = 0.5\n* (10 µF) * (100 V)^2. To simplify the calculation, we\nneed to convert microfarads (µF) to farads (F). Since 1\nµF is equal to 1 × 10^-6 F, we can rewrite the\ncapacitance as 10 × 10^-6 F. Work = 0.5 * (10 ×\n10^-6 F) * (100 V)^2 = 5 * 10^-5 F * 10^4 V^2 =\n0.5 * 10^-1 F * 1 V^2 = 5 * 10^-2J. Thus, the final\nanswer is: [Answer: C; Used hints: None; Confidence:\nlow]\nQuestion\n: \nUltraviolet light has a wavelength of about 6 × 10^-8 m.\nWhat is the frequency of this light?\nOption: [A. 5 × 10^15 Hz, B. 0.5 Hz, C. 2 Hz, D. 20\nHz]\nHints\n: \n(1) Wavelength is inversely proportional to the frequency.\nNote: Form of output is: explanation + [Answer: \"option\nletter\"; Used hints: \"hints ID or None\"; Confidence: \"low,\nmiddle, or high\"].\nAnswer\n: \nTo find the frequency of ultraviolet light with a given\nwavelength, we can use the relationship between\nwavelength and frequency, which is hint 1: wavelength is\ninversely proportional to frequency. Mathematically, this\nrelationship can be expressed as: frequency = speed of\nlight / wavelength. The speed of light in a vacuum is\napproximately 3 × 10^8 meters per second (m/s). Given\nthat the wavelength of the ultraviolet light is 6 × 10^-8 m,\nwe can substitute these values into the formula: frequency\n= (3 × 10^8 m/s) / (6 × 10^-8 m) = 0.5 x 10^16 Hz= 5\nx 10^15 Hz. Thus, the frequency of ultraviolet light with\na wavelength of 6 × 10^-8 m is 0.5 Hz. Thus, the final\nanswer is: [Answer: A; Used hints: 1; Confidence: high]\nQuestion\n: \nA whiffle ball is tossed straight up, reaches the highest\npoint, and falls back down. Air resistance is not\nnegligible. Which of the following statements are true? I.\nThe ball’s speed is zero at the highest point. II. The ball’s\nacceleration is zero at the highest point. III. The ball\ntakes a longer time to travel up to the highest point than\nto fall back down.\nOption\n: \n[A. I only, B. II only, C. I & II only, D. I & III only]\nNote: Form of output is: explanation + [Answer: \"option\nletter\"; Used hints: \"hints ID or None\"; Confidence: \"low,\nmiddle, or high\"].\nAnswer\n: \nLet's evaluate each statement one by one: I. The ball's\nspeed is zero at the highest point. This statement is true.\nAt the highest point of the ball's trajectory, its vertical\nvelocity becomes zero. Since speed is the magnitude of\nvelocity, which is a vector quantity, the speed of the ball\nindeed becomes zero at the highest point. This is because\nthe ball momentarily stops moving upward before it starts\ndescending. II. The ball's acceleration is zero at the\nhighest point. This statement is false. The ball's\nacceleration is not zero at the highest point. Even though\nthe ball momentarily stops changing its direction (from\nupward to downward motion) at the highest point, it still\nexperiences the force of gravity acting downward. The\npresence of gravity causes the ball's acceleration to\nremain constant throughout its motion, regardless of the\npoint in its trajectory. III. The ball takes a longer time to\ntravel up to the highest point than to fall back down. This\nstatement is false. \nDue to the presence of air resistance, the ball\nexperiences a drag force opposing its motion. As a\nresult, the ball takes longer to reach the highest point of\nits trajectory compared to the time it takes to fall back\ndown. Air resistance acts as a damping force, reducing\nthe upward speed of the ball and increasing the time it\ntakes to reach the peak. When falling back down, the\nball's downward speed is increased by the force of\ngravity, making the descent faster than the ascent. Based\non the evaluations, we can conclude that only Statement\nI is true. Thus, the final answer is: [Answer: A; Used\nhints: None; Confidence: middle]\nQuestion\n: \nAn object of volume 2 × 10^-3 m^3 and weight 6 N is\nplaced into a tank of water, where it floats. What\npercentage of the object's volume is above the surface of\nthe water?\nOption\n: \n[A. 12%, B. 30%, C. 60%, D. 70%]\nHints\n: \n(1) Density of water is 997 kg/m^3; (2) Object density is\n306 kg/m^3.\nNote\n: \nForm of output is: explanation + [Answer: \"option letter\";\nUsed hints: \"hints ID or None\"; Confidence: \"low,\nmiddle, or high\"].\nAnswer\n: \nThe percentage of the object's volume that is above the\nsurface of the water can be calculated using the formula:\nPercentage above surface = [(Density of water - Density\nof object) / Density of water] × 100. In this question, the\npercentage above surface = [(997 kg/m^3 - 306 kg/\nm^3) / 997 kg/m^3] × 100 = (691 kg/m^3 / 997kg/\nm^3) × 100 = 0.693 × 100 ≈ 70%. Thus, the final\nanswer is: [Answer: D; Used hints: 1, 2; Confidence:\nhigh]\nFigure 11: Prompt template of QA module.\n4195\n// System define\nImagine you are a thoughtful and logical question-raiser.\nYou are given a physics question. However, the question\nis too complex or lack of information to answer. You\nneed to raise some questions to decompose the original\nquestion into several simpler sub-questions, or to seek\nadditional information that helps you answer the original\nquestion. Important notes: do not use pronouns or\nindefinite pronoun phrases in your generated questions.\nThe raised question has to be the self-contain question,\nwhich means including context if it is needed.  Each\nquestion can only contain one argument. Do not just ask\nYes/No questions.\n// Demonstration\nQuestion\n: \nAn object of volume 2 × 10^-3 m^3 and weight 6 N is\nplaced into a tank of water, where it floats. What\npercentage of the object's volume is above the surface of\nthe water?\nOption\n:\n[A. 12%, B. 30%, C. 60%, D. 70%]\nNote\n: \nThe raised question has to be a self-contain question. Do\nnot use pronouns or indefinite pronoun phrases in the\ngenerated questions. Copy context from the original\nquestion if needed.\nDeep Questions:\n1. When an object floats, what function describes the\nrelationship between the object's volume and weight?\n2. What is the density of water?\n3. An object of volume 2 × 10^-3 m^3 and weight 6 N,\nwhat is the object's density?\nQuestion\n: \nCompared with the mass of a uranium atom undergoing\nfission, the combined masses of the products after fission\nare\nOption\n: \n[A. less, B. more, C. the same, D. zero]\nNote\n:\nThe raised question has to be a self-contain question. Do\nnot use pronouns or indefinite pronoun phrases in the\ngenerated questions. Copy context from the original\nquestion if needed.\nDeep Questions:\n1. What causes the change in mass of a particle before\nand after fission?\nQuestion\n: \nThings that are equivalent according to the equivalence\nprinciple are\nOption\n: \n[A. space and time, B. a traveling twin and a stay-at-\nhome twin, C. gravity and acceleration, D. mass and\nenergy]\nNote\n: \nThe raised question has to be a self-contain question. Do\nnot use pronouns or indefinite pronoun phrases in the\ngenerated questions. Copy context from the original\nquestion if needed.\nDeep Questions:\n1. What is the equivalence principle?\nQuestion\n: \nWhich of these three elements has the most mass per\nnucleon?\nOption\n: \n[A. Hydrogen, B. Iron, C. Uranium, D. Same in each]\nNote\n: \nThe raised question has to be a self-contain question. Do\nnot use pronouns or indefinite pronoun phrases in the\ngenerated questions. Copy context from the original\nquestion if needed.\nDeep Questions:\n1. What is the nucleon mass of hydrogen?\n2. What is the nucleon mass of iron?\n3. What is the nucleon mass of uranium?\nQuestion\n: \nA microwave oven is connected to an outlet, 120000\nmV, and draws a current of 2 amps. At what rate is\nenergy being used by the microwave oven?\nOption\n: \n[A. 10 W, B. 30 W, C. 60 W, D. 240 W]\nNote: The raised question has to be a self-contain\nquestion. Do not use pronouns or indefinite pronoun\nphrases in the generated questions. Copy context from\nthe original question if needed.\nDeep Questions:\n1. Given a given voltage and current, how to calculate\nthe power?\n2. How many volts equal 12000 microvolts?\nFigure 12: Prompt template of QG module.\n4196\n// System define\nImagine you are a blind but intelligent asker only given a\nquestion and description of an image. You need to ask\nme (at most 5) questions about facts or commonsense\nknowledge to help you get more information about the\nimage to help answer the given question. Important\nnotes: do not use pronouns in your generated questions.\nEach question can only contain one argument. Do not\njust ask Yes/No questions.\n// Demonstration\nImage Caption:\n \nA bowl of oranges in a bowl.\nQuestion:\n \nWhat states are these grown in?\nFactual Questions:\n1.\nReason: Each US state has different specialties.\nQuestion: In which state in the USA are oranges\ngrown?\n2.\nReason: Oranges grow in states where the\nenvironment is good for them.\nQuestion: What environments are suitable for orange\ngrowth?\n// Input\nImage Caption: \nA bedroom with a bed and a canopy.\nQuestion: \nIs this a room for a boy or a girl?\nFactual Questions:\nFigure 13: Prompt template of FQG module.\n// System define\nImagine you are a polymath familiar with encyclopedias\nand all kinds of common-sense knowledge. You need to\nanswer a question about some facts or some common-\nsense knowledge in short sentence.\n// Demonstration\nQuestion:\n \nWhat is human life expectancy in the United States?\nAnswer:\nHuman life expectancy in the United States is 78\nyears.\n// Input\nQuestion: \nIn which state in the USA are oranges grown?\nAnswer:\nFigure 14: Prompt template of FQA module.\n// System define\nImagine you are a blind but intelligent system. You are\ngiven the context of an image and a question about the\nimage. However, the current context is insufficient to\nanswer the question. You should ask me at least two\nshort questions about visual information in the image to\nhelp you answer the question. Important notes: do not\nuse pronouns in your generated questions. Each question\ncan only contain one argument. Do not just ask Yes/No\nquestions.\n// Demonstration\nImage Caption:\n \nTwo women walking on a sidewalk with an umbrella.\nQuestion:\n \nAre the ladies friends?\nVisual Prompts:\n1.\nReason: People with close relationships, such as\nfriends, walk closer.\nPrompt: Are the two women walking close to each\nother?\n2.\nReason: The body language between friends will be\nmore intimate, such as hugging, holding hands, etc.\nPrompt: What's the body language of the two\nwomen?\nImage Caption: \nA horse pulling a carriage with two people in it.\nHints:\n1.\nPeople generally use tools like bridles to force horses\nto work.\nQuestion: \nDoes the horse do this because it wants to?\nVisual Prompts:\n1.\nReason: When animals are forced to work, they show\nfacial expressions such as anger and sadness.\nPrompt: What is the expression on the horse's face\nwhile it's pulling the carriage?\n2.\nReason: Humans often use tools such as bridles to\ncontrol animals and force them to work.\nPrompt: What type of tools or equipment is being\nused to control the horse while it pulls the carriage?\n// Input\nImage Caption: \nA bedroom with a bed and a canopy.\nQuestion: \nIs this a room for a boy or a girl?\nHints: \n1.\nThe commonly used colors for boys' interior\ndecoration are blue and gray.\n2.\nThe commonly used colors for girls' interior\ndecoration are pink and white.\nVisual Prompts:\nFigure 15: Prompt template of VQG module.\n4197\n// System define\nImagine you are a blind but intelligent question answering\nsystem. You are asked a visual question about an image.\nI will provide you the caption of the image and some\nuseful visual hints. Please use your best judgement to\nanswer the visual question.\n// Demonstration\nImage Caption:\n \nA man holding a dog on his back.\nHints:\n1.\nDogs usually use mouth to catch objects\n2.\nThe popular game people play with dog is frisbee\n3.\nThe man is holding a frisbee\nQuestion:\n \nWhich part of this animal would be in use of it was\nplaying the game that is played with the items the man\nis holding? (If the information is not enough to answer\nthe question, answer \"lack of information\")\nAnswer:\nHints 1,2,3 are useful. The answer is: mouth\nImage Caption\n: \nA busy city street with many people walking around.\nQuestion: \nWhy might someone go to this place? (If the\ninformation is not enough to answer the question,\nanswer \"lack of information\")\nAnswer:\nShop\nImage Caption: \nA bowl of oranges in a bowl.\nQuestion: \nWhat states are these grown in? (If the information is\nnot enough to answer the question, answer \"lack of\ninformation\")\nAnswer:\nLack of information\n// Input\nImage Caption: \nA  bathroom with a toilet and a sink.\nHint:\n1.\nToilet could be used by both man and woman\n2.\nThere is a razor near the sink\nQuestion: \nWho leaves a toilet like this? (If the information is not\nenough to answer the question, answer \"lack of\ninformation\")\nAnswer:\nFigure 16: Prompt template of VQA module.\n// System define\nImagine you are a blind but intelligent question answering\nsystem. You are asked a visual question about an image.\nI will provide you the caption of the image and some\nuseful visual hints. Please use your best judgement to\nanswer the visual question.\n// Demonstration\nImage Caption:\n \nA man holding a dog on his back.\nHints:\n1.\nDogs usually use mouth to catch objects\n2.\nThe popular game people play with dog is frisbee\n3.\nThe man is holding a frisbee\nQuestion:\n \nWhich part of this animal would be in use of it was\nplaying the game that is played with the items the man\nis holding? (Must return an answer. The final answer\nshould be 1 or 2 words (maximum 2 words). If you\nare not sure, you can guess the most plausible\nanswer)\nAnswer:\nHints 1,2,3 are useful. The answer is: mouth\nImage Caption\n: \nA busy city street with many people walking around.\nQuestion: \nWhy might someone go to this place? (Must return an\nanswer. The final answer should be 1 or 2 words\n(maximum 2 words). If you are not sure, you can\nguess the most plausible answer)\nAnswer:\nShop\n// Input\nImage Caption: \nA  bathroom with a toilet and a sink.\nHint:\n1.\nToilet could be used by both man and woman\n2.\nThere is a razor near the sink\nQuestion: \nWho leaves a toilet like this? (Must return an answer.\nThe final answer should be 1 or 2 words (maximum 2\nwords). If you are not sure, you can guess the most\nplausible answer)\nAnswer:\nFigure 17: Prompt template of VQA module (force\nanswer).\n4198\n// System define\nYou are given a question-and-answer pair, can you help\nme to merge the question and answer into a statement\nsentence. If the question or the answer is ambiguous you\ncan just output the token \"unknown\". If the merged\nsentence is ambiguous, you can just output the token\n\"unknown\". If you can merge the question-and-answer\npair, just output the sentence.\n// Input\nQuestion: \nIn which state in the USA are oranges grown?\nAnswer:\nCalifornia\nStatement:\nFigure 18: Prompt template of QA-to-Hint module.\n// System define\nImagine you are a strict marking teacher or grader. I will\ngive you a question, a correct answer, and a student\nanswer. You need to tell me \"1\" or \"0\" (where 1 means\ncorrect, 0 means incorect). \"1\" does not mean the\nstudent's answer must exactly match the correct answer.\nIf they have the same meaning for the given question,\nthen it is also \"1\". However, an ambiguous answer is \"0\"\n(e.g., correct answer: \"1789\", student answer: \"long long\nago\").\n// Input\nQuestion: \nIn which state in the USA are oranges grown?\nCorrect Answer:\nCalifornia\nStudent Answer:\nCalifornia state\nGrade:\nFigure 19: Prompt template for chatGPT of semantic-\nbased evaluation.\n4199",
  "topic": "Socratic method",
  "concepts": [
    {
      "name": "Socratic method",
      "score": 0.8543308973312378
    },
    {
      "name": "Socratic questioning",
      "score": 0.7271593809127808
    },
    {
      "name": "Computer science",
      "score": 0.6604050993919373
    },
    {
      "name": "Process (computing)",
      "score": 0.4814873933792114
    },
    {
      "name": "Cognitive science",
      "score": 0.4520551562309265
    },
    {
      "name": "Qualitative reasoning",
      "score": 0.4515409469604492
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43929606676101685
    },
    {
      "name": "Epistemology",
      "score": 0.3533756732940674
    },
    {
      "name": "Theoretical computer science",
      "score": 0.32522809505462646
    },
    {
      "name": "Programming language",
      "score": 0.18928521871566772
    },
    {
      "name": "Psychology",
      "score": 0.1748276948928833
    },
    {
      "name": "Philosophy",
      "score": 0.07824119925498962
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I859038795",
      "name": "Virginia Tech",
      "country": "US"
    }
  ]
}