{
  "title": "Your fairness may vary: Group fairness of pretrained language models in toxic text classification.",
  "url": "https://openalex.org/W3190655802",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2086578199",
      "name": "Ioana Baldini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2952881425",
      "name": "Dennis Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2030434656",
      "name": "Karthikeyan Natesan Ramamurthy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2543265794",
      "name": "Mikhail Yurochkin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139222655",
      "name": "Moninder Singh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3177189402",
    "https://openalex.org/W3037314525",
    "https://openalex.org/W2809878087",
    "https://openalex.org/W2920807444",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W3105645800",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W3091818438",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2530395818",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W3133702157"
  ],
  "abstract": "We study the performance-fairness trade-off in more than a dozen fine-tuned LMs for toxic text classification. We empirically show that no blanket statement can be made with respect to the bias of large versus regular versus compressed models. Moreover, we find that focusing on fairness-agnostic performance metrics can lead to models with varied fairness characteristics.",
  "full_text": "Your fairness may vary:\nPretrained language model fairness in toxic text classiﬁcation\nIoana Baldini Dennis Wei Karthikeyan Natesan Ramamurthy\nMikhail Yurochkin Moninder Singh\nIBM Research\n{ioana,dwei,knatesa,moninder}@us.ibm.com\nmikhail.yurochkin@ibm.com\nAbstract\nThe popularity of pretrained language models\nin natural language processing systems calls\nfor a careful evaluation of such models in\ndown-stream tasks, which have a higher\npotential for societal impact. The evaluation\nof such systems usually focuses on accuracy\nmeasures. Our ﬁndings in this paper call for\nattention to be paid to fairness measures as\nwell. Through the analysis of more than a\ndozen pretrained language models of varying\nsizes on two toxic text classiﬁcation tasks\n(English), we demonstrate that focusing on\naccuracy measures alone can lead to models\nwith wide variation in fairness characteristics.\nSpeciﬁcally, we observe that fairness can\nvary even more than accuracy with increasing\ntraining data size and different random\ninitializations. At the same time, we ﬁnd that\nlittle of the fairness variation is explained by\nmodel size, despite claims in the literature.\nTo improve model fairness without retraining,\nwe show that two post-processing methods\ndeveloped for structured, tabular data can be\nsuccessfully applied to a range of pretrained\nlanguage models. Warning: This paper\ncontains samples of offensive text.\n1 Introduction\nPre-trained, bidirectional language models (De-\nvlin et al., 2019; Liu et al., 2019; Radford et al.,\n2019; Clark et al., 2020; He et al., 2021) 1 have\nrevolutionized natural language processing (NLP)\nresearch. LMs have provided a route to signiﬁ-\ncant performance increases in several NLP tasks\nas demonstrated by NLP leaderboards (Rajpurkar\net al., 2018; Wang et al., 2019a,b; AI2, 2021).\nMore importantly, LMs have been applied to prac-\ntical problems, leading to improved results for web\nsearch (Nayak, 2019) and have become an asset in\n1We use the acronym LM(s) to refer to language model(s)\nthroughout the paper.\nﬁelds such as medical evidence inference (Lehman\net al., 2019; Subramanian et al., 2020) and chem-\nistry (Schwaller et al., 2021). While the progress in\nNLP tasks due to LMs is clear, the reasons behind\nthis success are not as well understood (Rogers\net al., 2021; McCoy et al., 2019), and there are also\nimportant downsides. In particular, several stud-\nies have documented the bias of LMs (Bolukbasi\net al., 2016; Hutchinson et al., 2020; Webster et al.,\n2020; Borkan et al., 2019; de Vassimon Manela\net al., 2021) and others discuss potential societal\nharms (Blodgett et al., 2020; Bender et al., 2021)\nfor individuals or groups. We use the term bias\nto refer to systematic disparity in representation\nor outcomes for individuals based on their mem-\nbership in certain protected groups such as gender,\nrace, and ethnicity.\nIn this work, we focus on one important ap-\nplication of ﬁne-tuned LMs, toxic text classiﬁ-\ncation. Text toxicity predictors are already used\nin deployed systems (Perspective API, 2021) and\nthey are a crucial component for content modera-\ntion since online harassment is on the rise (V ogels,\n2021). In downstream applications such as toxic\ntext classiﬁcation, it is important to examine the\nbehavior of LMs in terms of measures other than\ntask-speciﬁc accuracy. This provides a more holis-\ntic understanding of model performance and appro-\npriate uses of LMs for these tasks. As a ﬁrst step\ntoward this goal, we provide herein an empirical\ncharacterization of LMs for the task of toxic text\nclassiﬁcation using a combination of accuracy and\nbias measures, and study two post-processing meth-\nods for bias mitigation that have proved successful\nfor structured, tabular data. For assessing bias, in\nthis paper, we focus on group fairness, which we\nexplain in Section 2 as it applies in general in ma-\nchine learning, and discuss what it means in the\ncontext of NLP tasks in the same section. The\nimplications of measuring group fairness for the\ntoxicity classiﬁcation task studied in this paper are\narXiv:2108.01250v3  [cs.CL]  14 Apr 2022\ndescribed in Section 3.\nOne aspect of LMs that is hard to ignore is the\nincrease in their size, as measured by the number\nof parameters in their architectures. In general,\nlarger LMs seem to perform better on NLP tasks\nas they have the capacity to capture more complex\ncorrelations present in the training data. Bender\net al. (2021) claim that this same property may also\nlead to more pronounced biases in their predictions,\nas the large data that LMs are trained on is not\ncurated. On the other hand, for image classiﬁca-\ntion models that use large neural networks, Hooker\net al. (2020) discuss how model pruning can lead to\nmore biased predictions. In this work, we consider\na wide variety of model architectures and sizes. We\nacknowledge that size is relative and what we con-\nsider large in this paper may not be considered as\nsuch in a different context.\nWe address the following questions regarding\nthe effect of various factors on model performance:\n1. Model size: How do the accuracy and group\nfairness of ﬁne-tuned LM-based classiﬁers\nvary with their size?\n2. Random seeds: LMs that start from different\nrandom initializations can behave differently\nin classiﬁcation. What is the effect of random\nseeds on the accuracy-fairness relationship?\n3. Data size: The size of ﬁne-tuning data is also\nan important dimension alongside model size.\nWhat happens to accuracy and fairness when\nmore/less data is used for ﬁne-tuning?\n4. Bias mitigation via post-processing : Given\nthe expense of training and ﬁne-tuning large\nLMs, to what extent can we mitigate bias by\nonly post-processing LM outputs?\nWe study the accuracy-fairness relationship in\nmore than a dozen ﬁne-tuned LMs for two different\ndatasets that deal with prediction of text toxicity.\nThe key contributions of our analysis are:\n1. We empirically show that no blanket state-\nment can be made regarding the fairness char-\nacteristics of ﬁne-tuned LMs with respect to\ntheir size. It really depends on the combina-\ntion of LM, task, and dataset.\n2. We ﬁnd that optimizing for accuracy measures\nalone can lead to models with wide variation\nin fairness characteristics. Speciﬁcally:\n(a) While increasing data size for ﬁne-tuning\ndoes not improve accuracy much beyond\na point, the improvement in fairness is\nmore signiﬁcant and may continue after\nthe improvement in accuracy has stopped\nfor certain datasets and tasks. This sug-\ngests that choosing data sizes based on\naccuracy alone could lead to suboptimal\nperformance with respect to fairness.\n(b) While accuracy measures are known to\nvary with different random initializa-\ntions (Dodge et al., 2020), the variation\nin fairness measures can be even greater.\n3. We demonstrate that post-processing bias mit-\nigation is an effective, computationally afford-\nable solution to enhance fairness in ﬁne-tuned\nLMs. In particular, one of the methods we\nexperimented with allows for a large accuracy-\nfairness tradeoff space, leading to relative im-\nprovements of 50% for fairness, as measured\nby equalized odds, while reducing accuracy\nonly by 2% (see Figure 8 religion group).\nOur observations strengthen the chorus of recent\nwork addressing bias mitigation in NLP in calling\nfor a careful empirical analysis of fairness with\nﬁne-tuned LMs in the context of their application.\nTo allow group fairness analysis, annotations of\ngroup membership are preferred and sometimes re-\nquired, and, thus, we urge the research community\nto include protected group annotations in datasets\nto enable extrinsic fairness evaluations that are as\nclose as possible to the point of deployment.\n2 Background and related work\n2.1 Fairness in machine learning\nAs machine learning models have become routinely\ndeployed in practice, many studies noticed their\ntendency to perform unfairly in various contexts\n(Angwin et al., 2016, 2017; Buolamwini and Ge-\nbru, 2018; Park et al., 2021). To understand and\nmeasure model bias, researchers have proposed\nmany deﬁnitions of algorithmic fairness. Broadly\nspeaking, they fall into two categories: group fair-\nness (Chouldechova and Roth, 2018) and individ-\nual fairness (Dwork et al., 2012). At a high level,\ngroup fairness requires similar average outcomes\non different groups of individuals considered, for\nexample comparable university acceptance rates\nacross ethnicities. Individual fairness requires sim-\nilar outcomes for similar individuals, e.g. two uni-\nversity applicants with similar credentials, but dif-\nferent ethnicity, gender, family background, etc.,\nshould either be both accepted or both rejected. In\nthis paper we consider group fairness, noting that\nboth have their pros and cons (Chouldechova and\nRoth, 2018; Dwork et al., 2012).\nThere are many deﬁnitions of group fairness and\nwe refer to Verma and Rubin (2018) for a compre-\nhensive overview and to Czarnowska et al. (2021)\nfor a discussion of metrics in the context of mea-\nsuring social biases in NLP. Statistical parity (SP)\nis one of the earlier deﬁnitions which requires the\noutput of a model to be independent of the sen-\nsitive attribute, such as race or gender. In other\nwords, the average outcome (e.g. prediction) across\ngroups deﬁned by the sensitive attribute needs to be\nsimilar. An alternative measure is equalized odds\n(EO) (Hardt et al., 2016), which requires the model\noutput conditioned on the true label to be indepen-\ndent of the sensitive attribute. The violation of con-\nditional independence for a given label (positive or\nnegative) can be measured by the difference in ac-\ncuracy across sensitive groups conditioned on that\nlabel. Taking the maximum or an average (average\nEO) of these label-speciﬁc differences quantiﬁes\nthe overall EO violation.\nMany methods for achieving group fairness have\nbeen proposed. These methods are typically cate-\ngorized as follows: (a) modifying the training data\n(pre-processing), (b) incorporating fairness con-\nstraints while training the model (in-processing),\nand (c) transforming the model output to enhance\nfairness (post-processing). A summary and im-\nplementation of group bias mitigation approaches\nare discussed in Bellamy et al. (2019). In this\nstudy, we investigate the use of post-processing\nmethods to enhance fairness in classiﬁcation tasks.\nWe chose post-processing approaches since they\ndo not require modiﬁcation of training data or\nmodel training procedures, and, hence, can be ef-\nﬁciently applied to all LMs we consider. In addi-\ntion, post-processing approaches could minimize\nthe environmental impact of re-training/ﬁne-tuning\nLMs (Patterson et al., 2021; Strubell et al., 2019).\nWe consider two post-processing approaches pro-\nposed by Wei et al. (2020) and Hardt et al. (2016),\nwhich have shown considerable success in mitigat-\ning bias for tabular data. Wei et al. (2020) optimize\na score (predicted probability) transformation func-\ntion to satisfy fairness constraints that are linear\nin conditional means of scores while minimizing a\ncross-entropy objective. Hardt et al. (2016) propose\nto solve a linear program to ﬁnd probabilities with\nwhich to change the predicted output labels such\nthat the equalized odds violation is minimized.\n2.2 Fairness in Natural Language Processing\nIn NLP systems, bias is broadly understood in two\ncategories, intrinsic and extrinsic. Intrinsic bias\nrefers to bias inherent in the representations, e.g.\nword embeddings used in NLP (Bolukbasi et al.,\n2016). Extrinsic bias refers to bias in downstream\ntasks, such as disparity in false positive rates across\ngroups deﬁned by sensitive attributes in a speci-\nﬁed application/task. The concepts of intrinsic and\nextrinsic bias also correlate well with the notions\nof representational and allocative harms. While\nallocative harms arise from disparities across differ-\nent groups in terms of decisions that lead to alloca-\ntion of beneﬁts/harms, representational harms are\nthose perpetuated by representation of individuals\nin the feature space (Crawford, 2017). Abbasi et al.\n(2019) discuss how harms from stereotypical repre-\nsentations manifest as allocative harms later in the\nML pipeline. However, probably because of the\ncomplexity of LMs, measuring intrinsic bias in the\nrepresentations created by LMs may not necessarily\nreﬂect the behavior of models built by ﬁne-tuning\nLMs. Goldfarb-Tarrant et al. (2021) discuss how\nintrinsic measures of bias do not correlate with ex-\ntrinsic, application-speciﬁc, bias measures. Since\nwe are concerned with the application of LMs to the\nspeciﬁc task of toxic text classiﬁcation, we restrict\nour focus to group fairness measures, which fall\nunder the category of extrinsic bias. Previous work\non bias mitigation in NLP has been focused on\npre- and in-processing methods (Sun et al., 2019;\nBall-Burack et al., 2021) and to the best of our\nknowledge, we are the ﬁrst to use post-processing\nmethods with NLP tasks.\n3 Methodology\nWe are interested in studying how group fairness\nvaries across different ﬁne-tuned LMs for binary\nclassiﬁcation. We choose to focus on text toxicity\nas the prediction task. Due to an increase in online\nharassment (V ogels, 2021) and the potential of both\npropagating harmful stereotypes of minority groups\nand/or inadvertently reducing their voices, the task\nof predicting toxicity in text has received increased\nattention in recent years (Kiritchenko et al., 2021).\nWhile we acknowledge that text toxicity presents\ndifferent complex nuances (e.g., offensive text, ha-\nrassment, hate speech), we focus on a binary task\nformulation. We adopt the deﬁnition of toxicity\ndescribed in Borkan et al. (2019) as “anything that\nis rude, disrespectful, or unreasonable that would\nmake someone want to leave a conversation”.\n3.1 Datasets\nWe used two datasets that deal with toxic text clas-\nsiﬁcation: 1) Jigsaw, a large dataset released for the\n“Unintended Bias in Toxicity Classiﬁcation” Kag-\ngle competition (Jigsaw, 2019) that contains online\ncomments on news articles, and 2) HateXplain,\na dataset recently introduced with the intent of\nstudying explanations for offensive and hate speech\nin Twitter and Twitter-like data (i.e., gab.com).\nBoth datasets have ﬁne-grained annotations for reli-\ngion, race and gender. We used as sensitive groups\nthe coarse-grained groups (e.g., mention of any\nreligion, see Section 3.3) as opposed to the ﬁner-\ngrained annotations (e.g., Muslim). Details about\nthe sizes of the datasets, the splits we used and text\nsamples can be found in Appendix A.1.\n3.2 Language models, ﬁne-tuning and\ncomputation infrastructure\nWe consider more than a dozen LMs that cover a\nlarge spectrum of sizes. We selected the models\nto not only represent various sizes but also differ-\nent styles of architecture and training. The mod-\nels in our study are shown in Table 1 along with\nthe number of parameters and the size of the Py-\nTorch (Paszke et al., 2019) model on disk. If not\nspeciﬁed, the version of the model used is base.\nFor all our experiments, we used the Hugging Face\nimplementation of Transformers (Wolf et al., 2020)\nand the corresponding implementations for all LMs\nin our study. In particular, we use the text sequence\nclassiﬁer without any modiﬁcations to increase re-\nproducibility.\nWe run model ﬁne-tuning for 1-3 epochs and\nchoose the best model based on the highest accu-\nracy obtained on the dev split. When presenting ex-\nperimental results, we focus primarily on balanced\naccuracy as the Jigsaw dataset is highly imbalanced\nand reporting only accuracy may be misleading. In\ngeneral, higher accuracy leads to higher balanced\naccuracy, with the exception of two LMs – GPT2\nand SqueezeBERT. For these two, the best balanced\naccuracy is less than 2 percentage points higher\nthan the balanced accuracy resulting from choos-\ning the highest overall accuracy across the various\nhyper-parameter runs. We experiment with two\nlearning rates (2e−6 and 2e−5) and observe that\nthe large models tend to prefer smaller learning rate,\ndegenerating for higher learning rates. For large\nLMs with Jigsaw we ﬁne-tune for one epoch to\nkeep the compute time under 24 hours. The model\naccuracy we obtained are in line with state-of-the-\nart results for these types of tasks. The large LMs\nare ﬁne-tuned on A100 Nvidia GPUs, while the\nrest of the models are ﬁne-tuned on V100 Nvidia\nGPUs. The experiments for HateXplain run from\n10 minutes to under an hour, while the experiments\nfor the large models with Jigsaw can take up to 24\nhours.\n3.3 Sensitive groups and fairness measures\nIn all our measurements, we considered the fol-\nlowing topics as sensitive: religion, race and gen-\nder. We categorize a text sample as belonging to\na sensitive group if it mentions one of these topics\n(e.g., religion), and otherwise to the complemen-\ntary group (no religion). Except in Section 5.5, we\ndo not analyze ﬁner-grained subgroups (e.g., Jew-\nish), but consider larger groups (any reference to\nreligion, such as Muslim, Jewish, atheist). There\nare several reasons that justify this choice. First,\nunlike tabular data where each sample corresponds\nto an individual belonging to one identity (e.g., ei-\nther female or male), we do not have information\non the demographics of the person producing the\ntext. Our categorization is based on the content. In\naddition, for the datasets we used, most subgroups\naccount for signiﬁcantly less than 1% of the data.\nMoreover, there is considerable overlap between\nsubgroups. For example, in the test split for Jigsaw,\n40% of the text belonging to the male subgroup also\nbelongs to the female subgroup. To summarize, we\nanalyze the bias/fairness of toxic text prediction in\nthe presence or absence of information that refers\nto religion, race or gender, respectively. The intent\nis to not have the performance of the predictor be\ninﬂuenced by these sensitive topics.\nWe use equalized odds as the group fairness mea-\nsure. Equalized odds is deﬁned as the maximum\nof the absolute true positive rate difference and\nfalse positive rate difference, where these differ-\nences are between a sensitive group and its com-\nplementary group. In toxic text classiﬁcation, a\ntrue positive means that a toxic text is correctly\nidentiﬁed as such, while a false positive means\nthat a benign piece of text is marked as toxic. In\nterms of harms, a false negative (toxic text that is\nmissed) may cause individuals to feel threatened or\nTable 1: The size (number of parameters, size on disk) for the language models considered in this study.\nSize Group Language Model # of parameters Size on disk\nSmall\nALBERT (Lan et al., 2020) 12M 45MB\nMobileBERT (Sun et al., 2020) 25.3M 95MB\nSqueezeBERT (Iandola et al., 2020) 51M ∗ 196MB\nDistilBERT (Sanh et al., 2020) 66M 256MB\nRegular\nBERT (Devlin et al., 2019) 110M 418MB\nELECTRA (Clark et al., 2020) 110M 418MB\nFunnel (small) (Dai et al., 2020) 117M ∗ 444MB\nRoBERTa (Liu et al., 2019) 125M 476MB\nGPT2 (Radford et al., 2019) 117M 487MB\nDeBERTa (He et al., 2021) 140M 532MB\nLarge\nELECTRA-large 335M 1.3GB\nBERT-large 340M 1.3GB\nRoBERTa-large 355M 1.4GB\nDeBERTa-large 400M 1.6GB\n∗Approximate number of parameters.\ndisrespected, while a false positive may be seen as\ncensoring, which is particularly problematic if it re-\nduces the voices of minority protected groups from\nonline conversations. By using the sensitive groups\nof religion/race/gender mentioned above, we aim\nto analyze and reduce the effect of the presence or\nabsence of religion/race/gender terms on the false\nnegative and false positive rates. By taking the max-\nimum, we are emphasizing the larger discrepancy\nas opposed to other studies that take the average of\nthe two rate differences (average equalized odds).\nNote that unlike statistical parity, equalized odds\ndoes allow the sensitive (e.g., mention of religion)\nand complementary (no religion) groups to have\ndifferent toxicity (positive prediction) rates.\n4 Bias mitigation post-processing\nWe investigated the use of post-processing methods\nto mitigate violations of equalized odds. By post-\nprocessing, we mean methods that operate only\non the outputs of the ﬁne-tuned LMs and do not\nmodify the models themselves 2. The ability to\navoid retraining models is a major advantage of\npost-processing due to the large computational cost\nof ﬁne-tuning LMs. Post-processing also targets\nunfairness at a point closest to deployment and\nhence can have a direct impact on downstream\noperations that use the model predictions.\nHardt, Price, Srebro (2016) (HPS): The ﬁrst\npost-processing method that we consider is by\nHardt et al. (2016) (abbreviated HPS, using the last\nnames of the authors), who were the original pro-\n2This is not to be confused with the post-processing of LM\nembeddings, before they are passed to classiﬁcation layers. In\nthis case, the classiﬁcation layers must be retrained to account\nfor the modiﬁed embeddings.\nposers of the equalized odds criterion for fairness.\nWe used the open-source implementation of their\nmethod from Bellamy et al. (2019), which post-\nprocesses binary predictions to satisfy EO while\nminimizing classiﬁcation loss. While this method\nis effective in enforcing EO, one limitation is that\nit does not offer a trade-off between minimizing\nthe deviation from EO and reducing the loss in\naccuracy.\nFair Score Transformer (FST): We study the\nFST method of Wei et al. (2020), in part to provide\nthe above-mentioned trade-off, and in part because\nit is a recent post-processing method shown to be\ncompetitive with several other methods (including\nin-processing). FST takes predicted probabilities\n(referred to as scores) as input and post-processes\nthem to satisfy a fairness criterion. We choose\ngeneralized equalized odds (GEO), a score-based\nvariant of EO, as the fairness criterion and then\nthreshold the output score to produce a binary pre-\ndiction. The application of FST required attention\nto three issues: 1) its ability to work with input\nscores that may not be calibrated probabilities; 2)\nthe choice of fairness parameter ϵ, which bounds\nthe allowed GEO on the data used to ﬁt FST; 3)\nthe choice of binary classiﬁcation threshold t. We\nconsider a range of ϵand tvalues to explore the\ntrade-off between EO and accuracy. Due to numer-\nical instability of the FST implementation in the\noriginal paper (occasional non-convergence in rea-\nsonable time for the Jigsaw dataset), we obtained a\nclosed-form solution for one step in the optimiza-\ntion that leads to a more efﬁcient implementation,\nrunning in minutes for all models and all datasets\nconsidered. More details on this implementation\nand the tuning of the parameters can be found in\nAppendix A.3.\nThreshold post-processing (TPP): We also\ntested the effect of thresholding alone, without\nfairness-enhancing transformations. We refer to\nthis as threshold post-processing (TPP). This sim-\nple method corresponds to FST without calibrating\nthe LM outputs, choosing ϵlarge enough so that\nFST yields an identity transformation, and thresh-\nolding at level t.\nJigsaw HateXplain\nreligion\nrace\ngender\nFigure 1: Balanced accuracy versus equalized odds for\nﬁne-tuned LMs on the Jigsaw and HateXplain datasets.\n5 The accuracy-fairness relationship in\ntoxic text classiﬁcation\nWe report on the performance and fairness charac-\nteristics of several LMs while varying parameters\nsuch as random seeds and training data size. We\nalso experiment with post-processing methods for\ngroup bias mitigation and show that it is possible to\nreduce some of the bias presented by these models.\n5.1 Characterization of language models of\nvaried sizes\nThe ﬁrst set of experiments present how perfor-\nmance and fairness measures vary across models.\nIn Figure 1 we show the performance as measured\nby balanced accuracy3 and the group fairness as\nmeasured by equalized odds on the x-axis (lower\nEO is better). The models are color-coded by their\nsize - dark blue for small models, orange for regu-\nlar size models and light blue for large models. The\nvariation in balanced accuracy is not as wide as the\nvariation in equalized odds. For the HateXplain\ndataset, the gap between balanced accuracy and\nfairness variability is more prominent. In terms of\naccuracy (not balanced), the models perform even\ncloser as shown in the plots in Appendix A.2. For\nEO, the spread is signiﬁcant, with gaps of 0.10 be-\ntween the largest and smallest values for Jigsaw,\nand 0.15 for HateXplain. Depending on the dataset\nand sensitive group, some larger models seem to\nlead to lower EO; for example, ELECTRA-large\nachieves the best accuracy-EO results for religion\nas the sensitive group (Jigsaw). For race, Squeeze-\nBERT, which is one of the small models in the\nstudy, achieves one of the best balanced accuracy-\nEO operating points for Jigsaw (considering it is\nhalf the size of RoBERTa which has better balanced\naccuracy but similar EO), hinting that size is not\nwell correlated with the fairness of the model. Sim-\nilarly, for HateXplain (religion), DistilBERT, again\na small model, obtains the best balanced accuracy-\nEO operating point. In the next section, we analyze\nmodels trained using various random seeds and ﬁnd\na low correlation between EO and model size.\nThese results strongly suggest that fairness mea-\nsures should be included in the evaluation of LMs.\nIn the next sections, we demonstrate that, if fair-\nness is not carefully considered, we can end up with\nmodels with widely varying fairness characteristics\ndepending on the training conditions.\n5.2 The inﬂuence of random seeds\nFine-tuning LMs depends on a random seed used\nfor mini-batch sampling and for initializing the\nweights in the last layers of the network responsible\nfor the binary classiﬁcation. It is well documented\nin the literature that this random seed may inﬂu-\nence the accuracy of the resulting model (Dodge\net al., 2020). In Figure 2 we show that while bal-\nanced accuracy is somewhat stable, fairness can\nvary widely by only changing the random seed. In\nfact, if we were to plot the accuracy instead of the\n3We use balanced accuracy as a measure for performance\nas it is more informative, especially for the imbalanced Jigsaw\ndataset where a trivial predictor that always outputs the label\n“normal” would achieve∼92% accuracy.\nbalanced accuracy, all points would be virtually on\na horizontal line for Jigsaw, as shown in Figure A.2.\nThe variations for EO are larger. For Jigsaw, we\nobserve a variation of up to 0.05 in equalized odds\nfor some cases. For HateXplain, the variation is\nconsiderably larger, with several models presenting\na spread of 0.15 or more for the sensitive group of\nreligion. For example, for DeBERTa-L, depending\non the random seed, one could get one of the best\nmodels with respect to performance-fairness trade-\noffs, or one of the worst (balanced accuracy varies\nwithin 0.79-0.80, while EO varies over 0.11-0.30).\nThe results in our experiments align with the ones\ndiscussed in a recent study on underspeciﬁcation\nin machine learning (D’Amour et al., 2020), where\ndifferent random seeds lead to small variations in\naccuracy, but considerable variations in intrinsic\nbias as measured by gendered correlations.\nTo further probe whether there is a correlation be-\ntween fairness and model size, we used the results\nfor multiple random seeds to compute Pearson’s co-\nefﬁcient of correlation. These values are -0.357 for\nJigsaw and -0.188 for HateXplain, with p-values\nof 5e-6 and 0.017, respectively. These results show\na low correlation between fairness as measured by\nEO and model size.\n5.3 Low data regime\nIn general, it is well known that more training data\nimproves model accuracy. We experiment with ﬁne-\ntuning the models using a fraction of the training\ndataset, while keeping the test set the same. When\nthe smaller datasets are subsampled from the orig-\ninal dataset, we ensure that the larger datasets in-\nclude the smaller ones to simulate situations when\nmore data is collected and used for training. The\nresults are shown for one small/regular/large model\nin Figure 3. Each data point in the graph represents\nthe average of eleven runs performed with different\nrandom seeds, one for each run. In very few cases,\nthe random seed led to a degenerate model and we\ndid not include these runs in the averaged results.\nOverall, there were up to ﬁve degenerate runs for\neach dataset (across all 14 models in this study, not\nonly the ones presented in the ﬁgure).\nWe observe that in the case of Jigsaw, equalized\nodds generally keeps improving even when the\naccuracy plateaus, suggesting that, from a fairness\npoint of view, it may be beneﬁcial to collect more\ndata for ﬁne-tuning. This does not seem to be the\ncase for the HateXplain dataset, where the accuracy\nJigsaw HateXplain\nreligion\nrace\ngender\nFigure 2: Balanced accuracy versus equalized odds for\nﬁne-tuned LMs when varying the random seed used in\nﬁne-tuning.\ndoes not plateau and the fairness measure oscillates.\nA reason could be that HateXplain is much smaller\nin size than Jigsaw and hence Jigsaw’s training is\nmore stable. Similar trends are observed for the\nrest of the models in our study.\n5.4 Bias mitigation through post-processing\nIn this section, we experiment with applying post-\nprocessing methods for group bias mitigation. We\nﬁrst discuss the results of parameter tuning for Fair\nScore Transformer (FST) (Wei et al., 2020). More\ndetails can be found in Appendix A.3. The FST\nmethod has one tunable parameter, ϵ. Using the\ntransformed scores from FST, we also investigate\ntuning the threshold used in the binary classiﬁer,\ninstead of using the default value of 0.5, as ex-\nplained in Section 4. Figure 4 depicts the data\npoints obtained by varying ϵand the classiﬁcation\nthreshold 4. Note that we plot EO decreasingly on\nthe x-axis, and overall better operating points are\n4All points are shown for the dev set as this plot illustrates\nthe tuning of FST parameters.\nReligion Christian Jewish Muslim Race White Black Gender Female Male LGBT\nBaseline 0.18 0.10 0.06 0.20 0.10 0.12 0.13 0.10 0.12 0.13 0.15\nFST 0.08 0.03 0.06 0.11 0.09 0.11 0.11 0.05 0.07 0.07 0.15\nTable 2: BERT (Jigsaw): Equalized odds before and after applying FST for all sensitive groups and their subgroups.\nJigsaw HateXplain\nDistilBERT\nBERT\nELECTRA-large\nFigure 3: Accuracy, balanced accuracy and equalized\nodds (religion) for ﬁne-tuned LMs when varying the\nﬁne-tuning data size and the random seeds. Error bars\ndenote ±1 SE (standard error) of the mean.\ncloser to the top right corner. When choosing an op-\nerating point, the points on the black Pareto frontier\nare the most interesting points: highest balanced\naccuracy and lowest equalized odds. For reference,\nwe also show the baseline points without bias mit-\nigation for the dev and test sets. All data points\nare plotted for ﬁne-tuned BERT. Similar trends are\nobserved for the rest of the models considered in\nthis study and for the HateXplain dataset.\nWe also experimented with calibrating the scores\nusing logistic regression before post-processing. In\nFigure 5, we plot the Pareto frontiers of bias miti-\ngation when applying FST, with and without cali-\nbration, along with the threshold post-processing\n(TPP) method. We also show the result of HPS,\nwhich yields a single operating point, as well as\nFigure 4: FST tuning for BERT: Balanced accuracy ver-\nsus equalized odds on the Jigsaw dataset when varying\nfairness parameter ϵ and classiﬁcation threshold t for\nthe FST method for group bias mitigation (religion).\nFigure 5: BERT: Balanced accuracy versus equalized\nodds on the Jigsaw dataset when applying the FST and\nHPS methods for group bias mitigation and threshold\npost-processing (TPP) alone (religion).\nthe baselines without bias mitigation. In general on\nthe Jigsaw dataset, FST is successful in reducing\nEO with different degrees of success depending\non the model/group (see Appendix A.4 for addi-\ntional plots), offering an interesting set of points\nwith different accuracy-EO trade-offs. For refer-\nence, we show the corresponding point for the test\nset (orange x) for the operating point in dev that\nachieves an equalized odds of at most 0.05 (orange\nsquare). In certain cases, FST manages to lower\nthe equalized odds with minimal or no decrease in\naccuracy, as seen for religion in Figure 5. Note that\nall points in the plots except for the x points are\nplotted using the dev split.\nIn comparison, HPS seems particularly effective\nin lowering the equalized odds and thus improving\nthe fairness of the model, with some penalty on\nthe accuracy. For Jigsaw, applying only TPP (i.e.,\ntuning the threshold used in the binary classiﬁca-\ntion) also offers some interesting operating points.\nTPP has a small search space compared to FST\nand sometimes the Pareto frontier is reduced to one\npoint, as is the case in Figure 5. In general, FST has\nsuperior Pareto frontiers compared to TPP alone.\nIn addition, as we discuss in Appendix A.4, TPP\nproved inefﬁcient for the HateXplain dataset. Last,\nusing score calibration before feeding the scores\nto FST does not seem to offer signiﬁcant improve-\nments. Similar trends can be observed for the rest\nof the models.\nOverall, we ﬁnd the post-processing methods\nfor bias mitigation worth considering. They are\nstraightforward to apply, run in the order of sec-\nonds or minutes on the CPU of a regular laptop\nand they offer interesting operating points. On the\nother hand, pre-processing or in-processing tech-\nniques for bias elimination would incur signiﬁcant\ncomputational cost. Obtaining the Pareto frontiers\nis instantaneous as the search space for FST is not\nthat large. For more results and discussion of bias\nmitigation, we refer the reader to Appendix A.4.\n5.5 Sensitive groups and subgroups\nIn our analysis so far, we looked at sensitive groups\nthat refer to religion, race and gender. In this sec-\ntion we use the Jigsaw dataset to zoom in and ana-\nlyze the equalized odds for a sensitive group and\nits constituent subgroups. We select all subgroups\nthat have at least 100 samples in the test split. We\ncontinue to apply FST only at the larger group level\n(e.g., religion) and examine its effect on subgroups.\nIn Table 2, we show the EO measure for BERT be-\nfore and after applying FST for all sensitive groups\nand subgroups. FST consistently manages to lower\nEO for individual subgroups, without overly favor-\ning one subgroup over another. There are a few\ninstances that do not observe any change, mostly\nthe smallest subgroups. Note that subgroups can\nbe overlapping since they do not represent iden-\ntities of individuals, instead they derive from the\ntext which may mention multiple subgroups. One\nnotable example is that male and female subgroups\nhave similar EO, both baseline and after FST. This\njustiﬁes using larger sensitive groups for ﬁtting FST\nsince it seems the discussion of gender overall is\nproblematic as opposed to one gender in particular.\n6 Limitations\nIn our study, we covered a series of different mod-\nels that varied in network architecture, size as num-\nber of parameters, training procedures, and pretrain-\ning data. As we did not keep any of the elements\nconstant (e.g., architecture) while varying the rest\n(e.g., pretraining data, size, training procedure), it\nis hard to draw insights on how each individual\nelement affects the fairness of the resulting predic-\ntion outcomes. We would like to emphasize that\nidentifying toxic text is not an easy task, not even\nfor humans. As such, we expect the datasets to\nbe noisy and contain samples that are not anno-\ntated correctly. Upon manual inspection, we could\nidentify some samples for which we did not agree\nwith their labels. Motivated by this observation, we\nstarted looking into understanding the quality of\ndatasets used in toxic text prediction (Arhin et al.,\n2021). As a consequence, while we expect the\ntrends shown in this paper to hold, the actual abso-\nlute numbers may vary with datasets/tasks. More\nobservations and limitations can be found in Sec-\ntion 8.\n7 Conclusions\nIn this work, we addressed the following research\nquestions for language models: how do model size,\ntraining size, random seeds affect the relationship\nbetween performance and fairness (as measured by\nequalized odds)? Can post-processing methods for\nbias mitigation lead to better operating points for\nboth accuracy and fairness? We ﬁnd these ques-\ntions important in the context of the ethics of us-\ning language models in text toxicity prediction, in\nparticular, and in NLP research, in general. We\npresented a comprehensive study of language mod-\nels and their performance/fairness relationship. We\nchose several models to cover different sizes and\ndifferent architectures. While we did not consider\nsome of the largest recent models available, we\nbelieve we have experimented with a wide vari-\nety of models that have been discussed well in the\nliterature. We hope that this study can drive the\nfollowing point across: we cannot make a blanket\nstatement on the fairness of language models with\nrespect to their size or architecture, while training\nfactors such as data size and random seeds can\nmake a large difference. This makes it all the more\nimportant for researchers/practitioners to make fair-\nness an integral part of the performance evaluation\nof language models.\n8 Ethics Statement\nThis research used a considerable amount of com-\nputational resources and this is our main ethics\nconcern for conducting this work. We did try to\nkeep the number and the size of models we experi-\nmented with limited, to reduce the carbon footprint\nof the experiments. We hope the results we show\nin this paper are worth the computational resources\nused.\nIn this study, we looked at coarse-grained\ngroups deﬁned by the text content mentioning re-\nligion/race/gender, which may obfuscate the be-\nhavior of the models with respect to ﬁner-grained\ngroups, such as females and males. Similarly, we\ndid not consider intersectionality.\nBias mitigation can lead to undesirable outcomes.\nFor example, one aspect we did not look into is\nwhat happens with other groups when the miti-\ngation is applied only for one of the groups. In\naddition, we focused only on group fairness and\ndo not provide any insights into individual fairness.\nWe also recognize that abstract metrics have limita-\ntions and the societal impacts resulting from bias\nmitigation are not well understood (Olteanu et al.,\n2017). These issues are universal to bias mitigation\ntechniques and not particular to our use case.\nLast, but not least, the datasets we used are En-\nglish only. We acknowledge the importance of per-\nforming similar studies on multi-lingual datasets.\nReferences\nMohsen Abbasi, Sorelle A Friedler, Carlos Scheideg-\nger, and Suresh Venkatasubramanian. 2019. Fair-\nness in representation: quantifying stereotyping as\na representational harm. In Proceedings of the 2019\nSIAM International Conference on Data Mining.\nAllen Institute for AI AI2. 2021. Leaderboards.\nJulia Angwin, Jeff Larson, Lauren Kirchner,\nand Surya Mattu. 2017. Minority Neigh-\nborhoods Pay Higher Car Insurance Premi-\nums Than White Areas With the Same Risk.\nhttps://www.propublica.org/article/minority-\nneighborhoods-higher-car-insurance-premiums-\nwhite-areas-same-risk.\nJulia Angwin, Jeff Larson, Surya Mattu, and\nLauren Kirchner. 2016. Machine Bias.\nwww.propublica.org/article/machine-bias-risk-\nassessments-in-criminal-sentencing.\nKoﬁ Arhin, Ioana Baldini, Dennis Wei,\nKarthikeyan Natesan Ramamurthy, and Monin-\nder Singh. 2021. Ground-truth, whose truth?\nexaming the difﬁculties with annotating toxic text\ndatasets. In Data-Centric AI Workshop colocated\nwith NeurIPS 2021.\nPranjal Awasthi, Matthäus Kleindessner, and Jamie\nMorgenstern. 2020. Equalized odds postprocessing\nunder imperfect group information. In The 23rd In-\nternational Conference on Artiﬁcial Intelligence and\nStatistics, AISTATS 2020.\nAri Ball-Burack, Michelle Seng Ah Lee, Jennifer\nCobbe, and Jatinder Singh. 2021. Differential tweet-\nment: Mitigating racial dialect bias in harmful tweet\ndetection. In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency.\nRachel KE Bellamy, Kuntal Dey, Michael Hind,\nSamuel C Hoffman, Stephanie Houde, Kalapriya\nKannan, Pranay Lohia, Jacquelyn Martino, Sameep\nMehta, Aleksandra Mojsilovi´c, et al. 2019. AI Fair-\nness 360: An extensible toolkit for detecting and mit-\nigating algorithmic bias. IBM Journal of Research\nand Development.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big?\n . In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics.\nAja Bogdanoff. 2017. Saying goodbye to Civil Com-\nments. [Online; accessed 21-July-2021].\nTolga Bolukbasi, Kai-Wei Chang, James Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In\nProceedings of the 30th International Conference\non Neural Information Processing Systems.\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum\nThain, and Lucy Vasserman. 2019. Nuanced met-\nrics for measuring unintended bias with real data for\ntext classiﬁcation. In Companion of The 2019 World\nWide Web Conference, WWW.\nJoy Buolamwini and Timnit Gebru. 2018. Gender\nshades: Intersectional accuracy disparities in com-\nmercial gender classiﬁcation. In Conference on fair-\nness, accountability and transparency.\nAlexandra Chouldechova and Aaron Roth. 2018. The\nfrontiers of fairness in machine learning. arXiv\npreprint arXiv:1810.08810.\nEvgenii Chzhen, Christophe Denis, Mohamed Hebiri,\nLuca Oneto, and Massimiliano Pontil. 2019. Lever-\naging labeled and unlabeled data for consistent fair\nbinary classiﬁcation. Advances in Neural Informa-\ntion Processing Systems, 32.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nKate Crawford. 2017. The trouble with bias.\nhttps://www.youtube.com/watch?v=\nfMym_BKWQzk.\nPaula Czarnowska, Yogarshi Vyas, and Kashif Shah.\n2021. Quantifying social biases in NLP: A gener-\nalization and empirical comparison of extrinsic fair-\nness metrics. Transactions of the Association for\nComputational Linguistics.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.\n2020. Funnel-Transformer: Filtering out sequential\nredundancy for efﬁcient language processing. In An-\nnual Conference on Neural Information Processing\nSystems 2020.\nAlexander D’Amour, Katherine A. Heller, Dan\nMoldovan, Ben Adlam, Babak Alipanahi, Alex Beu-\ntel, Christina Chen, Jonathan Deaton, Jacob Eisen-\nstein, Matthew D. Hoffman, Farhad Hormozdiari,\nNeil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan\nKarthikesalingam, Mario Lucic, Yi-An Ma, Cory Y .\nMcLean, Diana Mincu, Akinori Mitani, Andrea\nMontanari, Zachary Nado, Vivek Natarajan, Christo-\npher Nielson, Thomas F. Osborne, Rajiv Raman,\nKim Ramasamy, Rory Sayres, Jessica Schrouff, Mar-\ntin Seneviratne, Shannon Sequeira, Harini Suresh,\nVictor Veitch, Max Vladymyrov, Xuezhi Wang, Kel-\nlie Webster, Steve Yadlowsky, Taedong Yun, Xiao-\nhua Zhai, and D. Sculley. 2020. Underspeciﬁca-\ntion presents challenges for credibility in modern\nmachine learning. CoRR, abs/2011.03395.\nDaniel de Vassimon Manela, David Errington, Thomas\nFisher, Boris van Breugel, and Pasquale Minervini.\n2021. Stereotype and skew: Quantifying gender\nbias in pre-trained and ﬁne-tuned language models.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics. Association for Computational Linguis-\ntics.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah A. Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. CoRR, abs/2002.06305.\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer\nReingold, and Richard Zemel. 2012. Fairness\nthrough awareness. In Proceedings of the 3rd inno-\nvations in theoretical computer science conference.\nBenjamin Fish, Jeremy Kun, and Ádám D Lelkes. 2016.\nA conﬁdence-based approach for balancing fairness\nand accuracy. In Proceedings of the 2016 SIAM In-\nternational Conference on Data Mining. SIAM.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-\ncardo Muñoz Sanchez, Mugdha Pandya, and Adam\nLopez. 2021. Intrinsic bias metrics do not correlate\nwith application bias. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguis-\ntics.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In Proceedings of the 34th International\nConference on Machine Learning, ICML 2017, Syd-\nney, NSW, Australia, 6-11 August 2017.\nMoritz Hardt, Eric Price, and Nati Srebro. 2016. Equal-\nity of opportunity in supervised learning. Advances\nin neural information processing systems.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: decoding-enhanced\nBERT with disentangled attention. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nSara Hooker, Nyalleng Moorosi, Gregory Clark,\nS. Bengio, and Emily L. Denton. 2020. Character-\nising bias in compressed models. ArXiv.\nBen Hutchinson, Vinodkumar Prabhakaran, Emily\nDenton, Kellie Webster, Yu Zhong, and Stephen De-\nnuyl. 2020. Social biases in NLP models as barriers\nfor persons with disabilities. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics.\nForrest Iandola, Albert Shaw, Ravi Krishna, and Kurt\nKeutzer. 2020. SqueezeBERT: What can computer\nvision teach NLP about efﬁcient neural networks?\nIn Proceedings of SustaiNLP: Workshop on Simple\nand Efﬁcient Natural Language Processing.\nRay Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich\nJiang, and Silvia Chiappa. 2020. Wasserstein fair\nclassiﬁcation. In Uncertainty in Artiﬁcial Intelli-\ngence.\nKaggle Jigsaw. 2019. Jigsaw Unintended Bias in Toxic-\nity Classiﬁcation. [Online; accessed 21-July-2021].\nFaisal Kamiran, Asim Karim, and Xiangliang Zhang.\n2012. Decision theory for discrimination-aware\nclassiﬁcation. In 2012 IEEE 12th International Con-\nference on Data Mining. IEEE.\nMichael P Kim, Amirata Ghorbani, and James Zou.\n2019. Multiaccuracy: Black-box post-processing\nfor fairness in classiﬁcation. In Proceedings of the\n2019 AAAI/ACM Conference on AI, Ethics, and So-\nciety.\nSvetlana Kiritchenko, Isar Nejadgholi, and Kathleen C.\nFraser. 2021. Confronting abusive language online:\nA survey from the ethical and human rights perspec-\ntive. Journal of Artiﬁcial Intelligence Research.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nEric Lehman, Jay DeYoung, Regina Barzilay, and By-\nron C Wallace. 2019. Inferring which medical treat-\nments work from reports of clinical trials. In Pro-\nceedings of the North American Chapter of the As-\nsociation for Computational Linguistics (NAACL).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nBinny Mathew, Punyajoy Saha, Seid Muhie Yi-\nmam, Chris Biemann, Pawan Goyal, and Animesh\nMukherjee. 2021. HateXplain: A benchmark\ndataset for explainable hate speech detection. In\nThirty-Fifth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2021.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, Florence, Italy. Asso-\nciation for Computational Linguistics.\nPandu Nayak. 2019. Understanding searches better\nthan ever before.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. BERTweet: A pre-trained language model\nfor english tweets. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, EMNLP 2020 -\nDemos.\nAlexandra Olteanu, Kartik Talamadupula, and Kush R.\nVarshney. 2017. The limits of abstract evaluation\nmetrics: The case of hate speech detection. In Pro-\nceedings of the 2017 ACM on Web Science Confer-\nence, WebSci 2017, Troy, NY, USA, June 25 - 28,\n2017.\nYoonyoung Park, Jianying Hu, Moninder Singh, Issa\nSylla, Irene Dankwa-Mullan, Eileen Koski, and\nAmar K. Das. 2021. Comparison of Methods to Re-\nduce Bias From Clinical Prediction Models of Post-\npartum Depression. JAMA Network Open.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems 32.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\nbon emissions and large neural network training.\narXiv preprint arXiv:2104.10350.\nPerspective API. 2021. Using Machine Learning to\nReduce Toxicity Online. [Online; accessed 21-July-\n2021].\nGeoff Pleiss, Manish Raghavan, Felix Wu, Jon Klein-\nberg, and Kilian Q Weinberger. 2017. On fairness\nand calibration. arXiv preprint arXiv:1709.02012.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2021. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nPhilippe Schwaller, Daniel Probst, Alain C. Vaucher,\nVishnu H. Nair, David Kreutter, Teodoro Laino, and\nJean-Louis Reymond. 2021. Mapping the space of\nchemical reactions using attention-based neural net-\nworks. Nature Machine Intelligence.\nIrene Solaiman and Christy Dennison. 2021. Process\nfor adapting language models to society (PALMS)\nwith values-targeted datasets. In Annual Conference\non Neural Information Processing Systems.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nConference of the Association for Computational\nLinguistics, ACL.\nShivashankar Subramanian, Ioana Baldini, Sushma\nRavichandran, Dmitriy Katz-Rogozhnikov,\nKarthikeyan Natesan Ramamurthy, Prasanna\nSattigeri, Varshney Kush R, Annmarie Wang,\nPradeep Mangalath, and Laura Kleiman. 2020. A\nnatural language processing system for extracting\nevidence of drug repurposing from scientiﬁc pub-\nlications. Proceedings of the AAAI Conference on\nInnovative Applications of Artiﬁcial Intelligence.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\nBelding, Kai-Wei Chang, and William Yang Wang.\n2019. Mitigating gender bias in natural language\nprocessing: Literature review. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020.\nSahil Verma and Julia Rubin. 2018. Fairness deﬁ-\nnitions explained. In Proceedings of the Interna-\ntional Workshop on Software Fairness , New York,\nNY , USA. Association for Computing Machinery.\nEmily A. V ogels. 2021. The State of Online Harass-\nment. [Online; accessed 21-July-2021].\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint 1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of ICLR.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beu-\ntel, Emily Pitler, Ellie Pavlick, Jilin Chen, and\nSlav Petrov. 2020. Measuring and reducing gen-\ndered correlations in pre-trained models. CoRR,\nabs/2010.06032.\nDennis Wei, Karthikeyan Natesan Ramamurthy, and\nFlavio P. Calmon. 2021. Optimized score transfor-\nmation for consistent fair classiﬁcation. Journal of\nMachine Learning Research.\nDennis Wei, Karthikeyan Natesan Ramamurthy, and\nFlávio du Pin Calmon. 2020. Optimized score trans-\nformation for fair classiﬁcation. In The 23rd Inter-\nnational Conference on Artiﬁcial Intelligence and\nStatistics, AISTATS 2020, 26-28 August 2020.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations . Association for Computa-\ntional Linguistics.\nBlake Woodworth, Suriya Gunasekar, Mesrob I Ohan-\nnessian, and Nathan Srebro. 2017. Learning non-\ndiscriminatory predictors. In Conference on Learn-\ning Theory, pages 1920–1953. PMLR.\nForest Yang, Mouhamadou Cisse, and Oluwasanmi O\nKoyejo. 2020. Fairness with overlapping groups; a\nprobabilistic perspective. Advances in Neural Infor-\nmation Processing Systems, 33.\nA Appendix\nIn this appendix, we discuss the datasets we used in\nour experiments, include additional experimental\nresults and provide more details on post-processing\nmethods for bias mitigation. We conclude with\nremarks on the reproducibility of this study.\nA.1 Datasets\nA.1.1 Jigsaw Unintended Bias in Toxicity\nClassiﬁcation\nIn 2019, Jigsaw released a large dataset as part\nof the “Unintended Bias in Toxicity Classiﬁcation”\nKaggle competition (Jigsaw, 2019). The dataset\nis a collection of roughly two million samples of\ntext from online discussions (Bogdanoff, 2017).\nThe samples are rated for toxicity and annotated\nwith attributes for sensitive groups. Table 3 shows\nthe groups we considered in our analysis and the\navailable ﬁne-grained group annotations. Note that\nwe considered the coarser groups; a sample text\nbelongs to a sensitive (coarse) group if any (ﬁne-\ngrained) annotation for the sample text exists. We\nused the original training dataset split in a 80/20\nratio for training and development (dev) tuning,\nrespectively. For reporting test results, we used the\nprivate test split released on Kaggle. Statistics for\nthe dataset splits are shown in Table 5. Each sample\nin the dataset (see Table 4 for a few samples from\nthe dataset) has a toxicity score and we consider\nanything higher than 0.5 to be toxic.\nFor the Jigsaw dataset, a combination of automa-\ntion and crowdsourcing was used to ensure that\nidentity (i.e., sensitive group) labels are a reason-\nable approximation of true identity-related content\n(see Jigsaw FAQ). Not all the dataset was labeled\nfor identity terms. While these labels are imperfect,\nwe do not believe that the degree of imperfection\ninvalidates our study. We note that the problem of\nprotected attribute labels being imperfect is well-\naccepted and studied (Awasthi et al., 2020).\nNoisy and incomplete sensitive group labels are\nanother reason why we chose equalized odds as the\nfairness measure. EO is a valid fairness measure\neven when there is overlap between the protected\ngroups (e.g., the group labeled “non-religion” still\nhas samples mentioning religion). To see this, re-\ncall that EO requires that the prediction conditioned\non the true label be independent of the protected\nattribute and its violation can be measured by the\ndifference |E[ ˆY|Y = 1,A = 1]−E[ ˆY|Y = 1]|\n(similarly for Y = 0). The ﬁrst term in the differ-\nence is measured on a subset of comments (A= 1)\nthat contain identity information. This is a good es-\ntimate if a sufﬁcient number of samples were anno-\ntated, regardless of the potentially missing identity\nannotations on the remaining samples. The second\nterm does not depend on annotations at all. Thus,\nthe estimate of EO is not affected by the lack of\nannotations on some of the comments.\nTable 3: The sensitive groups for Jigsaw dataset with\ntheir corresponding ﬁne-grained annotations.\nGroup Fine-grained annotation\nreligion atheist, buddhist, christian, hindu,\njewish, other religion\nrace white, asian, black, latino, other race\nor ethnicity\ngender and sexual\norientation∗\nbisexual, female, male, heterosexual,\nhomosexual gay or lesbian, transgen-\nder, other gender, other sexual orien-\ntation\n∗Throughout the paper, we use “gender” for short.\nA.1.2 HateXplain: Toxic text in Twitter and\nTwitter-like text\nHateXplain (Mathew et al., 2021) was recently in-\ntroduced with the intent of studying explanations\nin offensive and hate speech in Twitter and Twitter\nlike data (i.e., gab.com). For the purposes of our\nstudy, we collapse the annotations for offensive and\nhate speech into one class of toxic text. Similar to\nthe Jigsaw dataset, HateXplain samples have ﬁne-\ngrained annotations for sensitive groups. We use\nas groups the coarse-level annotations, as we did\nfor the Jigsaw dataset. The groups that we consider\nare presented in Table 6 and a few examples from\nthe dataset are shown in Table 7. Note the text in\neach sample is represented in the dataset as a list\nof tokens; in the table, we concatenated them with\nspaces and this is the way we use them as inputs\nfor the classiﬁers as well. We used the splits as\nprovided in the dataset; dataset statistics are shown\nin Table 8.\nA.2 The inﬂuence of random seeds on\naccuracy and equalized odds\nIn this section we present graphs similar to the ones\nin Section 5.2 using accuracy as a measure of per-\nformance instead of balanced accuracy. These plots\nmakes it obvious how close in performance all mod-\nels are and emphasize the gap in fairness measure\nobserved across different random seeds for each\nﬁne-tuned model. The results are shown in Fig-\nure 6. Note that all Jigsaw models get an accuracy\nin performance of approximately 95% with a gap\nof approximately .05 for equalized odds. HateX-\nplain models exhibit a higher variance in accuracy\n(4-5%) across all models with an even larger gap\nof 0.15 for equalized odds for most models. Note\nthat each LM has a modest variation in accuracy\nthat spans approximately 1%.\nFor HateXplain, we also experimented with\nBERTweet (Nguyen et al., 2020), a BERT-base\nsized model following the RoBERTa pretraining\nprocedure that is further trained on Twitter data,\nusing the checkpoint available in the Hugging Face\nmodel hub. In our experiments, BERTweet pre-\nsented the largest variation for accuracy (results\nnot shown), achieving both the best and the worst\naccuracy across all models (across the 11 random\nseeds we used), spanning a spread of 4.5%. The\nEO measure for BERTweet exhibited a variation\nof 0.12 for religion. We acknowledge that a more\nthorough analysis is required to better understand\nthe effects of in-domain pretraining (in this case on\ntweets) for both accuracy and fairness. For exam-\nple, recent work showed that model behavior can\nbe adjusted to a set of “target values” if the model is\ntrained on a small, well-behaved dataset (Solaiman\nand Dennison, 2021).\nA.3 Fair Score Transformer (FST)\nIn this section, we expand on our discussion of the\napplication of FST in this work.\nThe generalized equalized odds (GEO) criterion\ntargeted by FST is computed as the maximum of\nthe between-group absolute differences in average\nscores for positively-labeled and negatively-labeled\ninstances (Wei et al., 2020). It is analogous to EO\nwhere instead of the predicted label, the correspond-\ning probability for the label is used instead.\nRegarding issue 1) mentioned in Section 4 (cal-\nibration of input scores), we found that the distri-\nbutions of softmax outputs of the tested LMs are\nbimodal and highly concentrated near values of\nTable 4: Jigsaw dataset samples.\nComment text Toxicity Group\nThe Atwood fable is Donald, is it? My impression of this noise (over Atwood) is that\nit’s a gimmick by Atwood and her publisher to cash in on the Donald effect. As if we\nneeded slaves in bonnets to remind us that Donald is a jerk (and where was Atwood’s\nnovel when Monica was being pawed over?). A word to defenders of women: don’t spend\nyour political capital on stupid analogies.\nToxic Gender\nI got a question for you, dear, and it is a fair question: We all know what is happening\nin Syria; where are all the women’s marches over the slaughter in that country?. And,\nwhy has Trudeau been silent, like his pal Barry Obama, on taking effective military action\nagainst Syria? All you lefties are the same: you have no side vision.\nNormal Gender\nJigsaw Dataset\nHateXplain Dataset\nreligion race gender\nFigure 6: Accuracy versus equalized odds for ﬁne-tuned LMs when varying the random seed used in ﬁne-tuning.\nTable 5: Jigsaw dataset statistics: sample counts per\ndataset split and sensitive group.\nSplit Total Religion Race Gender\nTrain 1443899 50748 31022 70703\nDev 360975 12769 7999 17869\nTest 97320 3316 1911 4367\n0 and 1 (as commonly observed with deep neural\nnetworks). Such skewed distributions appear to vio-\nlate FST’s expectation of probabilities as input and\nare typically not encountered on tabular datasets\non which FST was previously tested. Thus we ex-\nperimented with calibrating the LM outputs. We\nconsidered both logistic regression of the class la-\nbel on the logit outputs of the LMs (a generalization\nof temperature scaling (Guo et al., 2017)), as well\nTable 6: The sensitive groups for HateXplain dataset\nwith their corresponding ﬁne-grained annotations.\nGroup Fine-grained annotation\nrace African, Arab, Asian, Cau-\ncasian, Hispanic\nreligion Islam, Buddhism, Jewish,\nHindu, Christian\ngender Men, Women\nas linear regression on the logit outputs followed by\nclipping of the resulting values to the interval[0,1].\nIn general, logistic regression proved somewhat\nbeneﬁcial for the Jigsaw dataset and we included it\nin our results.\nRegarding issue 2) (choice of fairness parame-\nter), we found, as noted by Wei et al. (2020), that\nwhile the parameter ϵcontrols the deviation from\nTable 7: HateXplain dataset samples.\nText Toxicity Group\nwhy is anna acting like she black Normal Race\nlike brazil many are descendants of west africa social constructs labels classiﬁcations have\nruined a many people who falsely self identify as white or just do not even know abt their\nnative land all b c of them bum ass caucazoids aka caucasians\nToxic Race\nreligion race gender\nFigure 7: FST tuning for BERT: Balanced accuracy versus equalized odds on the Jigsaw dataset when varying\nfairness parameter ϵand binary classiﬁcation threshold tafter applying the FST method for group bias mitigation.\nTable 8: HateXplain dataset statistics: sample counts\nper dataset split and sensitive group.\nSplit Total Religion Race Gender\nTrain 15383 3924 5418 3102\nDev 1922 481 672 396\nTest 1924 468 685 375\nGEO (i.e. the “GEO difference”), this is not always\ncorrelated with the EO difference, which is a func-\ntion of the output after thresholding. Regarding 3)\n(classiﬁcation threshold), we found that varying the\nthreshold tcan signiﬁcantly affect equalized odds\nas well as accuracy and balanced accuracy, and can\nsometimes even produce a reasonable trade-off be-\ntween them. For this reason, we included a version\nof post-processing (see “Threshold post-processing\n(TPP)” in Section 4. This effect of the prediction\nthreshold on fairness has not been explored in pre-\nvious work to our knowledge.\nAs a result of our observations regarding 2) and\n3), we used the following procedure to select a\nset of (ϵ,t) pairs to map out a trade-off between\nfairness and accuracy. The training set used to\nﬁne-tune the LMs is never seen by FST. The de-\nvelopment dataset (“dev”) is used to both tune the\nFST parameters and evaluate the resulting transfor-\nmation. As such, the dev dataset was further split\ninto a dev-train set and a dev-eval set. Given an\nϵvalue, FST was ﬁt on the dev-train set to ensure\na GEO difference of at most ϵ. Then on the dev-\neval set, given ϵand t, scores were transformed by\nFST with parameter ϵ, thresholded at level tto pro-\nduce a binary label, and ﬁnally evaluated for both\nfairness and accuracy. Each (ϵ,t) pair thus yields\none point in the equalized odds-accuracy plane, as\nseen in Figure 7. We selected (ϵ,t) pairs that are\nPareto-efﬁcient on the dev-eval set, to ensure the\nbest fairness-accuracy trade-off.\nThis is the ﬁrst time FST is used with unstruc-\ntured, text data and with large datasets in the order\nof millions of samples. First, we implemented FST\nfollowing the proposed implementation in Wei et al.\n(2020). This ﬁrst implementation ended up with\nnumerical instabilities that lead to either slow run-\nning times (in the order of hours) or even situations\nwhen the method did not converge. We managed\nto improve upon the computational cost of FST,\nwhich was instrumental in scaling to the large Jig-\nsaw dataset and allowing rapid experimentation.\nSpeciﬁcally, in the dual ADMM algorithm of Wei\net al. (2020), the ﬁrst step (eq. (14) therein) consists\nof nparallel optimizations, each involving a single\nvariable. We observed that these optimizations can\nbe done in closed form by solving a cubic equation.\nWe refer to Wei et al. (2021, Appendix B.1) for\ndetails of the closed-form solution as it is not the\nfocus of the present paper. The replacement of an\niterative optimization with a closed-form solution\ngreatly reduces the computational cost of FST. The\nimproved FST runs in the order of 1-2 minutes for\nthe Jigsaw dataset and in seconds for HateXplain.\nreligion race gender\nFigure 8: BERT: Balanced accuracy versus equalized odds on the Jigsaw dataset when applying the FST and HPS\nmethods for group bias mitigation and threshold post-processing (TPP) alone.\nEqually important, it also eliminates instances of\nthe iterative optimization failing to converge.\nA.4 Bias mitigation through post-processing\nmethods\nIn this section we present additional results on ap-\nplying post-processing methods for group bias mit-\nigation. We ﬁrst discuss the results of parameter\ntuning for Fair Score Transformer (FST) (Wei et al.,\n2020). More details about FST itself can be found\nin the Appendix A.3. The FST method has one\nparameter, ϵ, that can be ﬁne-tuned. Using the\ntransformed scores from the FST, we also investi-\ngate tuning the threshold used in the binary clas-\nsiﬁer, instead of using the default value of 0.5, as\nexplained in Section 4. Figure 7 depicts the data\npoints obtained by varying epsilon and for each ep-\nsilon value, varying the classiﬁcation threshold. 5\nWhen choosing an operating point, the points on\nthe black Pareto frontier are the most interesting\npoints: highest balanced accuracy and lowest equal-\nized odds. For reference, we also show the base-\nline points without bias mitigation for the dev and\ntest sets. All data points are plotted for ﬁne-tuned\nBERT. Similar trends are observed for the rest of\nthe models considered in this study and for the\nHateXplain dataset.\nWe also experimented with calibrating the scores\nusing logistic regression before post-processing. In\nFigure 8, we plot the Pareto frontiers of bias miti-\ngation when applying FST, with and without cali-\nbration, along with the threshold post-processing\n(TPP) method. We also show the result of HPS,\nwhich yields a single operating point, as well as the\nbaselines without bias mitigation. In general, on\nthe Jigsaw dataset, FST is successful in reducing\nEO with different degrees of success depending on\n5All points are shown for the dev set as this plot corre-\nsponds to tuning FST parameters.\nthe model/group. It thus offers an interesting set of\npoints with different accuracy-EO trade-offs. For\nreference, we show the equivalent point for the test\nset (orange x) for the operating point in dev that\nachieves an equalized odds of at most 0.05 (orange\nsquare). In certain cases, FST manages to lower the\nequalized odds with minimal or no decrease in ac-\ncuracy, as seen in the religion and gender columns\nin Figure 8. Note that all points in the plots except\nfor the xpoints are plotted using the dev dataset\nsplit, the xpoints are test points corresponding to\ndev points that obtain an EO of at most 0.05.\nIn comparison, HPS seems particularly effective\nin lowering the equalized odds and thus improv-\ning the fairness of the model, with some penalty\non the accuracy. For Jigsaw, applying only TPP\n(i.e., tuning the threshold used in the binary clas-\nsiﬁcation) also offers some interesting operating\npoints. TPP has a small search space compared to\nFST and sometimes the Pareto frontier is reduced\nto one point, as is the case for the religion group.\nIn general, FST has superior Pareto frontiers com-\npared to TPP alone. In addition, as we will discuss\nshortly, TPP proved inefﬁcient for the HateXplain\ndataset. Last, using score calibration before feeding\nthe scores to FST does not seem to offer signiﬁcant\nimprovements. Similar trends can be observed for\nthe rest of the models.\nIn Figure 9, we show the results of applying bias\nmitigation techniques for a few LMs, one for each\nsize category, on the HateXplain dataset with re-\nligion as the sensitive group. Unlike Jigsaw, the\nresults of the bias mitigation techniques follow dif-\nferent trends. HPS still manages to substantially\nreduce the EO for all models, but with a consider-\nable decrease in balanced accuracy (in some cases,\nmore than six percentage points). For FST, the ﬁne-\ntuning for epsilon and classiﬁcation threshold does\nnot lead to a large search space as observed in the\nDistilBERT BERT DEBERTA large\nFigure 9: Balanced accuracy versus equalized odds for ﬁne-tuned LMs (religion) on the HateXplain dataset when\napplying the FST and HPS methods for group bias mitigation and threshold post-processing (TPP) alone.\nJigsaw case. Moreover, the reduction in EO is more\nlimited and sometimes the improvement observed\nfor the dev set disappears in test. There are cases,\nthough, such as BERT, where FST successfully re-\nduces EO and the reduction is maintained or even\nimproved in test. Across the board, tuning only the\nthreshold used in classiﬁcation (TPP) did not lead\nto improved results and we omit showing them in\nthe plots.\nOverall, we ﬁnd the post-processing methods\nfor bias mitigation worth considering. They are\nstraightforward to apply, run in the order of sec-\nonds or minutes on the CPU of a laptop and they\noffer interesting operating points when other meth-\nods for bias elimination would incur a signiﬁcant\ncomputational cost, such as pre-processing or in-\nprocessing techniques. Obtaining the Pareto fron-\ntiers is instantaneous as the search space for FST\nis not that large.\nA.5 Other post-processing methods for bias\nmitigation\nIn addition to the two post-processing methods that\nwe considered in our study, other post-processing\nmethods for bias mitigation include assigning fa-\nvorable labels to unprivileged groups in regions of\nhigh classiﬁer uncertainty (Kamiran et al., 2012),\nminimizing error disparity while maintaining clas-\nsiﬁer calibration (Pleiss et al., 2017), a relaxed\nnearly-optimal procedure for optimizing equalized\nodds (Woodworth et al., 2017), shifting the deci-\nsion boundary for the protected group (Fish et al.,\n2016), iterative post-processing to achieve unbi-\nased predictions on every identiﬁable subpopula-\ntion (Kim et al., 2019), recalibrating a classiﬁer us-\ning a group-dependent threshold to optimize equal-\nity of opportunity (deﬁned as the difference be-\ntween the group-wise true positive rates) (Chzhen\net al., 2019), using optimal transport to ensure sim-\nilarity in group-wise predicted score distributions\n(Jiang et al., 2020), and a plug-in approach for\ntransforming the predicted probabilities to satisfy\nfairness constraints (Yang et al., 2020).\nA.6 Reproducibility statement\nThe data processing we performed for the datasets\nwe used is brieﬂy explained in Appendix A.1. In all\nour experiments we used unmodiﬁed versions of\nthe model implementations from the Hugging Face\ntransformers library (version 4.3.3) and the main\nscripts to tune the models are modiﬁed versions of\nthe sequence text classiﬁcation examples accom-\npanying the library. The hyper-parameter tuning\nwe performed was minimal (varying the number\nof epochs from 1-3, two values for learning rates\n2e−6 and 2e−5, 11 values for random seeds).\nMore details on the experimental infrastructure can\nbe found in Section 3.2. The main limiting factor\nin reproducing the results presented in this study is\nhaving access to GPUs such as the NVIDIA V100\nand A100 and generous, parallel compute time. At\nthe time of this writing, the implementation of FST\nthat we used is evolving proprietary code that may\nbecome available for external consumption. More\ndetails are provided in Appendix A.3. For HPS,\nwe used the open-source implementation that can\nbe found as part of the AIF360 toolkit, “equalized\nodds post-processing” method.",
  "topic": "Statement (logic)",
  "concepts": [
    {
      "name": "Statement (logic)",
      "score": 0.7155579328536987
    },
    {
      "name": "Computer science",
      "score": 0.6233208775520325
    },
    {
      "name": "Dozen",
      "score": 0.5855335593223572
    },
    {
      "name": "Group (periodic table)",
      "score": 0.5174739956855774
    },
    {
      "name": "Blanket",
      "score": 0.4832192659378052
    },
    {
      "name": "Fairness measure",
      "score": 0.4133976101875305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39011502265930176
    },
    {
      "name": "Econometrics",
      "score": 0.3359122574329376
    },
    {
      "name": "Natural language processing",
      "score": 0.3321380019187927
    },
    {
      "name": "Mathematics",
      "score": 0.16463366150856018
    },
    {
      "name": "Linguistics",
      "score": 0.14447882771492004
    },
    {
      "name": "Political science",
      "score": 0.12866178154945374
    },
    {
      "name": "Arithmetic",
      "score": 0.08667469024658203
    },
    {
      "name": "Chemistry",
      "score": 0.0861186683177948
    },
    {
      "name": "Operating system",
      "score": 0.07934948801994324
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Wireless",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Throughput",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}