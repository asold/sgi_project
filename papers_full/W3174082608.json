{
  "title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?",
  "url": "https://openalex.org/W3174082608",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2397341542",
      "name": "Asahi Ushio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2319266098",
      "name": "Luis Espinosa Anke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2189566537",
      "name": "Steven Schockaert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2565581196",
      "name": "Jose Camacho Collados",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3091759883",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2645731340",
    "https://openalex.org/W2963176474",
    "https://openalex.org/W2572487292",
    "https://openalex.org/W1983578042",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3100404639",
    "https://openalex.org/W2250189634",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4214784425",
    "https://openalex.org/W3102226577",
    "https://openalex.org/W2912225506",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2294507219",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W3016970897",
    "https://openalex.org/W2913433659",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3125506016",
    "https://openalex.org/W2741613777",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2311143338",
    "https://openalex.org/W3037697022",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W3102488924",
    "https://openalex.org/W2921633540",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2850240473",
    "https://openalex.org/W2128084896",
    "https://openalex.org/W2963462013",
    "https://openalex.org/W1593045043",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2964046079",
    "https://openalex.org/W2803176955",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2005603639",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3035267217",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2751143192",
    "https://openalex.org/W2865541675",
    "https://openalex.org/W1654905138",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2460442863",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1994335990",
    "https://openalex.org/W3082478769",
    "https://openalex.org/W145476170",
    "https://openalex.org/W4252434862",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2578463623",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3104432410",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W1660519191",
    "https://openalex.org/W2963239938",
    "https://openalex.org/W3013547323",
    "https://openalex.org/W2649778229",
    "https://openalex.org/W1535654961",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2045466646",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2518186251",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2960311305",
    "https://openalex.org/W4288620981",
    "https://openalex.org/W2327501763"
  ],
  "abstract": "Asahi Ushio, Luis Espinosa Anke, Steven Schockaert, Jose Camacho-Collados. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3609–3624\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3609\nBERT is to NLP what AlexNet is to CV:\nCan Pre-Trained Language Models Identify Analogies?\nAsahi Ushio, Luis Espinosa-Anke, Steven Schockaert, Jose Camacho-Collados\nCardiff NLP, School of Computer Science and Informatics\nCardiff University, United Kingdom\n{UshioA,Espinosa-AnkeL,SchockaertS1,CamachoColladosJ}@cardiff.ac.uk\nAbstract\nAnalogies play a central role in human com-\nmonsense reasoning. The ability to recognize\nanalogies such as “eye is to seeing what ear is\nto hearing”, sometimes referred to as analogi-\ncal proportions, shape how we structure knowl-\nedge and understand language. Surprisingly,\nhowever, the task of identifying such analogies\nhas not yet received much attention in the lan-\nguage model era. In this paper, we analyze\nthe capabilities of transformer-based language\nmodels on this unsupervised task, using bench-\nmarks obtained from educational settings, as\nwell as more commonly used datasets. We ﬁnd\nthat off-the-shelf language models can identify\nanalogies to a certain extent, but struggle with\nabstract and complex relations, and results are\nhighly sensitive to model architecture and hy-\nperparameters. Overall the best results were\nobtained with GPT-2 and RoBERTa, while\nconﬁgurations using BERT were not able to\noutperform word embedding models. Our re-\nsults raise important questions for future work\nabout how, and to what extent, pre-trained\nlanguage models capture knowledge about ab-\nstract semantic relations.1\n1 Introduction\nOne of the most widely discussed properties of\nword embeddings has been their surprising abil-\nity to model certain types of relational similari-\nties in terms of word vector differences (Mikolov\nWhile the title is probably self-explanatory, this is a small\nnote explaining it. BERT is to NLP what AlexNet is to CV is\nmaking an analogy on what the BERT and AlexNet models\nrepresented for Natural Language Processing (NLP) and Com-\nputer Vision (CV), respectively. They both brought a paradigm\nshift in how research was undertaken in their corresponding\ndisciplines and this is what the analogy refers to.\n1Source code and data to reproduce our ex-\nperimental results are available in the following\nrepository: https://github.com/asahi417/\nanalogy-language-model\nQuery: word:language\nCandidates: (1) paint:portrait\n(2) poetry:rhythm\n(3) note:music\n(4) tale:story\n(5) week:year\nTable 1: An example analogy task from the SAT\ndataset. The third candidate is the answer to the query.\net al., 2013a; Vylomova et al., 2016; Allen and\nHospedales, 2019; Ethayarajh et al., 2019). The\nunderlying assumption is that when “ais to bwhat\ncis to d” the word vector differences b −a and\nd −c are expected to be similar, where we write x\nfor the embedding of a word x. While this assump-\ntion holds for some types of syntactic relations,\nfor semantic relations this holds to a much more\nlimited degree than was suggested in early work\n(Linzen, 2016; Schluter, 2018). Moreover, the most\ncommonly used benchmarks have focused on spe-\nciﬁc and well-deﬁned semantic relations such as\n“capital of”, rather than the more abstract notion of\nrelational similarity that is often needed for solving\nthe kind of psychometric analogy problems that\ncan be found in IQ tests and educational settings.\nAn example of such a problem is shown in Table 1.\nGiven the central role of analogy in human cog-\nnition, it is nonetheless important to understand the\nextent to which NLP models are able to solve these\nmore abstract analogy problems. Besides its value\nas an intrinsic benchmark for lexical semantics,\nthe ability to recognize analogies is indeed impor-\ntant in the contexts of human creativity (Holyoak\net al., 1996), innovation (Hope et al., 2017), com-\nputational creativity (Goel, 2019) and education\n(Pardos and Nam, 2020). Analogies are also a\nprerequisite to build AI systems for the legal do-\nmain (Ashley, 1988; Walton, 2010) and are used in\nmachine learning (Miclet et al., 2008; Hug et al.,\n3610\n2016; H¨ullermeier, 2020) and for ontology align-\nment (Raad and Evermann, 2015), among others.\nWithin NLP, however, the task of recognizing\nanalogies has received relatively little attention. To\nsolve such problems, Turney (2005) proposed La-\ntent Relational Analysis (LRA), which was essen-\ntially designed as a relational counterpart to Latent\nSemantic Analysis (Landauer and Dumais, 1997).\nSomewhat surprisingly, perhaps, despite the sub-\nstantial progress that word embeddings and lan-\nguage models (LMs) have enabled in NLP, LRA\nstill represents the current state-of-the-art in solv-\ning abstract word analogy problems. When go-\ning beyond a purely unsupervised setting, however,\nGPT-3 was recently found to obtain slightly better\nresults (Brown et al., 2020).\nThe aim of this paper is to analyze the ability of\npre-trained LMs to recognize analogies. Our focus\nis on the zero-shot setting, where LMs are used\nwithout ﬁne-tuning. To predict whether two word\npairs (a,b) and (c,d) are likely to be analogical,\nwe need a prompt, i.e. a template that is used to con-\nstruct the input to the LM, and a scoring function.\nWe extensively analyze the impact of both of these\nchoices, as well as the differences between differ-\nent LMs. When the prompt and scoring function\nare carefully calibrated, we ﬁnd that GPT-2 can out-\nperform LRA, standard word embeddings as well\nas the published results for GPT-3 in the zero-shot\nsetting. However, we also ﬁnd that these results\nare highly sensitive to the choice of the prompt, as\nwell as two hyperparameters in our scoring func-\ntion, with the optimal choices not being consistent\nacross different datasets. Moreover, using BERT\nleads to considerably weaker results, underperform-\ning even standard word embeddings in all of the\nconsidered conﬁgurations. These ﬁndings suggest\nthat while transformer-based LMs learn relational\nknowledge to a meaningful extent, more work is\nneeded to understand how such knowledge is en-\ncoded, and how it can be exploited.\n2 Related work\n2.1 Understanding Pre-trained LMs\nSince their recent dominance in standard NLP\nbenchmarks (Peters et al., 2018a; Devlin et al.,\n2019; Liu et al., 2019), pre-trained language mod-\nels have been extensively studied. This has mainly\nbeen done through probing tasks, which are aimed\nat understanding the knowledge that is implicitly\ncaptured by their parameters. After the initial focus\non understanding pre-trained LSTM-based LMs\n(Peters et al., 2018b), attention has now shifted to-\nward transformer-based models. The main aspects\nthat have been studied in recent years are syntax\n(Goldberg, 2019; Saphra and Lopez, 2019; Hewitt\nand Manning, 2019; van Schijndel et al., 2019;\nJawahar et al., 2019; Tenney et al., 2019b) and se-\nmantics (Ettinger, 2019; Tenney et al., 2019a). For\na more complete overview on analyses of the differ-\nent properties of transformer-based LMs, we refer\nto Rogers et al. (2021).\nDespite the rise in probing analyses for LMs\nand the importance of analogical reasoning in hu-\nman cognition, understanding the analogical capa-\nbilities of LMs remains understudied. The most\nsimilar works have focused on capturing relational\nknowledge from LMs (in particular the type of\ninformation available in knowledge graphs). For\ninstance, Petroni et al. (2019) analyzed to what\nextent LMs could ﬁll manually-deﬁned templates\nsuch as “Dante was born in [MASK]”. Follow-up\nworks extended this initial approach by automat-\nically generating templates and ﬁne-tuning LMs\non them (Bouraoui et al., 2020; Jiang et al., 2020),\nshowing an improved performance. In this paper,\nwe focus on the analogical knowledge that is en-\ncoded in pre-trained LMs, without the extra step of\nﬁne-tuning on additional data.\n2.2 Word Analogy Probing\nWord analogies have been used as a standard in-\ntrinsic evaluation task for measuring the quality of\nword embeddings. Mikolov et al. (2013b) showed\nthat word embeddings, in particular Word2vec em-\nbeddings, were able to solve analogy problems by\nsimple vector operations (e.g. king - man + woman\n= queen). The motivation for this task dates back\nto the connectionism theory (Feldman and Ballard,\n1982) in cognitive science. In particular, neural\nnetworks were thought to be able to model emer-\ngent concepts (Hopﬁeld, 1982; Hinton, 1986) by\nlearning distributed representations across an em-\nbedding space (Hinton et al., 1986), similar to the\nproperties that word embeddings displayed in the\nanalogy task. More recent works have proposed\nnew mathematical theories and experiments to un-\nderstand the analogical capabilities of word embed-\ndings, attempting to understand their linear alge-\nbraic structure (Arora et al., 2016; Gittens et al.,\n2017; Allen and Hospedales, 2019) or by explic-\nitly studying their compositional nature (Levy and\n3611\nGoldberg, 2014; Paperno and Baroni, 2016; Etha-\nyarajh et al., 2019; Chiang et al., 2020).\nHowever, recent works have questioned the im-\npressive results displayed by word embeddings\nin this task. In many cases simple baselines ex-\ncluding the input pair (or query) were competitive\n(Linzen, 2016). Simultaneously, some researchers\nhave found that many relationships may not be\nretrieved in the embedding space by simple linear\ntransformations (Drozd et al., 2016; Bouraoui et al.,\n2018) and others argued that the standard evalu-\nation procedure has limitations (Schluter, 2018).\nNew datasets and measures have also been intro-\nduced to address some of these issues (Gladkova\net al., 2016; Fournier et al., 2020). Finally, in the\ncontext of bias detection, for which analogies have\nbeen used as a proxy (Bolukbasi et al., 2016), it has\nalso been found that word analogies may misguide\nor hide the real relationships existing in the vector\nspace (Gonen and Goldberg, 2019; Nissim et al.,\n2020).\nAs far as language models are concerned, word\nanalogies have not been explored to the same ex-\ntent as for word embeddings. Recently, Brown et al.\n(2020) evaluated the unsupervised capabilities of\nGPT-3 by evaluating it on the SAT analogies dataset\n(Turney et al., 2003), which we also include in our\nevaluation (see Section 3.2). However, the evalu-\nation is limited to a single dataset (i.e., SAT) and\nmodel (i.e., GPT-3), and the general capabilities of\nlanguage models were not investigated.\nDespite their limitations, analogy tests remain\nappealing for evaluating the ability of embeddings\nand language models to identify abstract relation-\nships. To mitigate the aforementioned methodolog-\nical issues, in this work we rely on analogy tests\nfrom educational resources, where the task is to\ncomplete analogical proportions, given only the\nﬁrst word pair. In contrast, word embedding mod-\nels have mostly been evaluated using a predictive\ntask, in which three of the four words are given.\nMoreover, the considered datasets are focused on\nabstract analogies, whereas the most commonly\nused datasets only include well-deﬁned semantic\nrelations such as “capital of”. For completeness,\nhowever, we also show results on these standard\ndatasets. We furthermore experiment with several\nsimple baselines to understand possible artifacts\npresent in the different datasets.\n3 Word Analogies\nIn this section, we describe the word analogy for-\nmulation that is used for our experiments (Section\n3.1). Subsequently, we provide an overview of the\ndatasets used in our experiments (Section 3.2).\n3.1 Task Description\nWe frame the analogy task in terms of analogical\nproportions (Prade and Richard, 2017). Given a\nquery word pair (hq,tq) and a list of candidate\nanswer pairs {(hi,ti)}n\ni=1, the goal is to ﬁnd the\ncandidate answer pair that has the most similar\nrelation to the query pair. Table 1 shows a sample\nquery and candidate answers drawn from one of the\ndatasets used in our evaluation (see Section 3.2).\n3.2 Analogy Datasets\nWe split analogy datasets in two types, based on\nhow the analogy problems were constructed.\n3.2.1 Psychometric Analogy Tests\nWord analogy tests are commonly used in assess-\nments of linguistic and cognitive ability. For in-\nstance, in the past, such tests were included in the\nSAT exams, which are a US college admission\ntest. Turney et al. (2003) collected a benchmark\nof 374 word analogy problems, consisting primar-\nily of problems from these SAT tests. Aimed at\ncollege applicants, these problems are designed to\nbe challenging for humans. A key challenge for\nNLP systems is that solving these problems often\nrequires identifying ﬁne-grained semantic differ-\nences between word pairs that belong to the same\ncoarse-grained relation. For instance, in the case\nof Table 1, we could say that “a year consists of\nweeks” like “language consists of words”, but the\nweek-year pair is nonetheless less similar to word-\nlanguage than note-music.\nAnother analogy benchmark was constructed by\nBoteanu and Chernova (2015), who used word anal-\nogy problems from an educational resource2. They\nused in particular UNIT 2 of the analogy problems\nfrom the educational site. These problems have\nthe same form as those from the SAT benchmark,\nbut rather than college applicants, they are aimed\nat children in grades 4 to 12 from the US school\nsystem (i.e. from age 9 onwards). In this paper, we\nwill also include this UNIT 2 benchmark. More-\nover, we have collected another benchmark from\n2https://www.englishforeveryone.org/\nTopics/Analogies.html\n3612\nDataset Data size No. No.\n(val / test) candidates groups\nSAT 37 / 337 5 2\nUNIT 2 24 / 228 5,4,3 9\nUNIT 4 48 / 432 5,4,3 5\nGoogle 50 / 500 4 2\nBATS 199 / 1799 4 3\nTable 2: High-level statistics of the analogy datasets\nafter uniﬁcation: data size, number of candidates and\nnumber of group partitions.\nthe UNIT 4 problems on the same website. These\nUNIT 4 problems are organised in 5 difﬁculty\nlevels: high-beginning, low-intermediate, high-\nintermediate, low-advanced and high-advanced.\nThe low-advanced level is stated to be at the level\nof the SAT tests, whereas the high-advanced level\nis stated to be at the level of the GRE test (which is\nused for admission into graduate schools).\n3.2.2 Lexical Semantics Benchmarks\nSince the introduction of Word2vec (Mikolov et al.,\n2013a), the problem of modelling analogies has\nbeen commonly used as an intrinsic benchmark for\nword embedding models. However, the datasets\nthat have been used in that context are focused\non well-deﬁned and relatively coarse-grained rela-\ntions. The Google analogy dataset (Mikolov et al.,\n2013b) has been one of the most commonly used\nbenchmarks for intrinsic evaluation of word em-\nbeddings. This dataset contains a mix of semantic\nand morphological relations such as capital-of and\nsingular-plural, respectively. However, its cover-\nage has been shown to be limiting, and BATS (Glad-\nkova et al., 2016) was developed in an attempt to\naddress its main shortcomings. BATS includes a\nlarger number of concepts and relations, which are\nsplit into four categories: lexicographic, encyclope-\ndic, and derivational and inﬂectional morphology.\nAs pointed out above, these datasets were tai-\nlored to the evaluation of word embeddings in a\npredictive setting. To provide an evaluation set-\nting which is comparable to the benchmarks ob-\ntained from human analogy tests, we constructed\nword analogy problems from the Google and BATS\ndatasets, by choosing for each correct analogy\npair a number of negative examples. The result-\ning benchmark thus follows the same format as\ndescribed in Section 3.1. To obtain sufﬁciently\nchallenging negative examples, for each query pair\n(e.g. Paris-France) we extracted three negative in-\nFigure 1: Solving a word analogy problem by selecting\none with the highest LM score among the candidates.\nstances: (1) two random words from the head of the\ninput relation type (e.g. Rome-Oslo); (2) two ran-\ndom words from the tail of the input relation type\n(e.g. Germany-Canada); (3) a random word pair\nfrom a relation type of the same high-level category\nas the input relation type (e.g. Argentina-peso).3\n3.2.3 Uniﬁcation and Statistics\nTable 2 provides an overview of our datasets. The\ninstances from each dataset are organised into\ngroups. In the case of Google and BATS, these\ngroups refer to the relation types (e.g. semantic or\nmorphological in the case of Google). In the case\nof UNIT 2 and UNIT 4, the groups refer to the dif-\nﬁculty level. For the SAT dataset, we consider two\ngroups, capturing whether the instances come from\nan actual SAT test or not. Finally, we randomly\nsample 10% of each group in each dataset to con-\nstruct a validation set, and regard the remaining\ndata as the test set.\n4 Methodology\nIn this section, we explain our strategy for using\npretrained LMs to solve analogy problems without\nﬁne-tuning. First, in Section 4.1 we explain how\neach relation pair is converted into a natural sen-\ntence to be fed into the LM. In Section 4.2, we then\ndiscuss a number of scoring functions that can be\nused to select the most plausible answer candidate.\nFinally, we take advantage of the fact that analog-\nical proportion is invariant to particular permuta-\ntions, which allows for a natural extension of the\nproposed scoring functions (Section 4.3). Figure 1\nshows a high-level overview of our methodology.\n4.1 Relation Pair Prompting\nWe deﬁne a prompting function Tt(w1,w2,w3,w4)\nthat takes four placeholders and a template type t,\n3In order to avoid adding various correct answers to the\nquery, we avoided adding negative pairs from allcountry-of\ntype relations, and from similar lexicographic relations in\nthe BATS dataset with more than one relation type, namely\nantonyms, synonyms, meronyms and hyponyms.\n3613\nand returns a sentence in which the placeholders\nwere replaced by the words w1, w2, w3, and w4.\nFor instance, given a query “word:language” and\na candidate “note:music”, the prompting function\nproduces\nTto-as(“word”,“language”,“note”,“music”) =\n“word is to language as note is to music”\nwhere we use the template type to-as here.\nUsing manually speciﬁed template types can re-\nsult in a sub-optimal textual representation. For\nthis reason, recent studies have proposed auto-\nprompting strategies, which optimize the template\ntype on a training set (Shin et al., 2020), paraphras-\ning (Jiang et al., 2020), additional prompt genera-\ntion model (Gao et al., 2020), and corpus-driven\ntemplate mining (Bouraoui et al., 2020). How-\never, none of these approaches can be applied to\nunsupervised settings. Thus, we do not explore\nauto-prompting methods in this work. Instead, we\nwill consider a number of different template types\nin the experiments, and assess the sensitivity of the\nresults to the choice of template type.\n4.2 Scoring Function\nPerplexity. We ﬁrst deﬁne perplexity, which is\nwidely used as a sentence re-ranking metric (Chan\net al., 2016; Gulcehre et al., 2015). Given a sen-\ntence x, for autoregressive LMs such as LSTM\nbased models (Zaremba et al., 2014) and GPTs\n(Radford et al., 2018, 2019; Brown et al., 2020),\nperplexity can be computed as\nf(x) = exp\n\n−\nm∑\nj=1\nlog Pauto(xj|xj−1)\n\n (1)\nwhere x is tokenized as [x1...xm] and Pauto(x|x)\nis the likelihood from an autoregressive LM’s\nnext token prediction. For masked LMs such\nas BERT (Devlin et al., 2019) and RoBERTa\n(Liu et al., 2019), we instead use pseudo-\nperplexity, which is deﬁned as in (1) but\nwith Pmask(xj|x\\j) instead of Pauto(xj|xj−1),\nwhere x\\j = [x1 ...x j1 〈mask〉xj+1 ...x m] and\nPmask(xj|x\\j) is the pseudo-likelihood (Wang and\nCho, 2019) that the masked token is xj.\nPMI. Although perplexity is well-suited to capture\nthe ﬂuency of a sentence, it may not be the best\nchoice to test the plausibility of a given analogical\nproportion candidate. As an alternative, we pro-\npose a scoring function that focuses speciﬁcally\nFigure 2: Positive and negative permutations for a rela-\ntion pair (a:b)-(c:d).\non words from the two given pairs. To this end,\nwe propose to use an approximation of point-wise\nmutual information (PMI), based on perplexity.\nPMI is deﬁned as the difference between a condi-\ntional and marginal log-likelihood. In our case, we\nconsider the conditional likelihood of ti given hi\nand the query pair (recall from Section 3.1 that\nh and t represent the head and tail of a given\nword pair, respectively), i.e. P(ti|hq,tq,hi), and\nthe marginal likelihood over hi, i.e. P(ti|hq,tq).\nSubsequently, the PMI-inspired scoring function is\ndeﬁned as\nr(ti|hi,hq,tq) = logP(ti|hi,hq,tq)\n−α·log P(ti|hq,tq) (2)\nwhere αis a hyperparameter to control the effect\nof the marginal likelihood. The PMI score corre-\nsponds to the speciﬁc case where α= 1. However,\nDavison et al. (2019) found that using a hyperpa-\nrameter to balance the impact of the conditional and\nmarginal probabilities can signiﬁcantly improve the\nresults. The probabilities in (2) are estimated by\nassuming that the answer candidates are the only\npossible word pairs that need to be considered. By\nrelying on this closed-world assumption, we can\nestimate marginal probabilities based on perplex-\nity, which we found to give better results than the\nmasking based strategy from Davison et al. (2019).\nIn particular, we estimate these probabilities as\nP(ti|hq,tq,hi) =− f(Tt(hq,tq,hi,ti))\nn∑\nk=1\nf(Tt(hq,tq,hi,tk))\nP(ti|hq,tq) =−\nn∑\nk=1\nf(Tt(hq,tq,hk,ti))\nn∑\nk=1\nn∑\nl=1\nf(Tt(hq,tq,hk,tl))\n3614\nwhere n is the number of answer candidates for\nthe given query. Equivalently, since PMI is sym-\nmetric, we can consider the difference between the\nlogs of P(hi|hq,tq,ti) and P(hi|hq,tq). While\nthis leads to the same PMI value in theory, due to\nthe way in which we approximate the probabilities,\nthis symmetric approach will lead to a different\nscore. We thus combine both scores with an ag-\ngregation function Ag. This aggregation function\ntakes a list of scores and outputs an aggregated\nvalue. As an example, given a list [1,2,3,4], we\nwrite Amean([1,2,3,4]) = 2.5 for the mean and\nAval1 ([1,2,3,4]) = 1for the ﬁrst element. Given\nsuch an aggregation function, we deﬁne the follow-\ning PMI-based score\nsPMI(ti,hi|hq,tq) =Ag (r) (3)\nwhere we consider basic aggregation operations\nover the list r = [r(ti|hi,hq,tq),r(hi|ti,hq,tq)],\nsuch as the mean, max, and min value. The choice\nof using only one of the scores r(ti|hi,hq,tq),\nr(hi|ti,hq,tq) is viewed as a special case, in which\nthe aggregation function gsimply returns the ﬁrst\nor the second item.\nmPPL. We also experiment with a third scoring\nfunction, which borrows ideas from both perplexity\nand PMI. In particular, we propose the marginal\nlikelihood biased perplexity (mPPL) deﬁned as\nsmPPL(ti,hi|hq,tq) = logsPPL(ti,hi|hq,tq)\n−αt ·log P(ti|hq,tq)\n−αh ·log P(hi|hq,tq)\nwhere αt and αh are hyperparameters, and sPPL is\na normalized perplexity deﬁned as\nsPPL(ti,hi|hq,tq) =− f(Tt(hq,tq,hi,ti))\nn∑\nk=1\nf(Tt(hq,tq,hk,tk))\n.\nThe mPPL score extends perplexity with two bias\nterms. It is motivated from the insight that treating\nα as a hyperparameter in (2) can lead to better\nresults than ﬁxing α = 1. By tuning αt and αh,\nwe can essentially inﬂuence to what extent answer\ncandidates involving semantically similar words to\nthe query pair should be favored.\n4.3 Permutation Invariance\nThe formalization of analogical proportions dates\nback to Aristotle (Barbot et al., 2019). According\nto the standard axiomatic characterization, when-\never we have an analogical proportion a: b:: c: d\n(meaning “ais to bwhat cis to d”), it also holds\nthat c : d :: a : b and a : c :: b : d are ana-\nlogical proportions. It follows from this that for\nany given analogical proportion a : b :: c : d\nthere are eight permutations of the four elements\na,b,c,d that form analogical proportions. These\neight permutations, along with the 16 “negative\npermutations”, are shown in Figure 2.\nTo take advantage of the different permutations\nof analogical proportions, we propose the following\nAnalogical Proportion (AP) score:\nAP(hq,tq,hi,ti) =Agpos (p) −β·Agneg (n) (4)\np = [s(a,b|c,d)](a:b,c:d)∈P\nn = [s(a,b|c,d)](a:b,c:d)∈N\nwhere P and N correspond to the list of positive\nand negative permutations of the candidate ana-\nlogical proportion hq : tq :: hi : ti in the order\nshown in Figure 2, β is a hyperparameter to con-\ntrol the impact of the negative permutations, and\ns(a,b|c,d) is a scoring function as described in\nSection 4.2. Here Agpos and Agneg refer to the ag-\ngregation functions that are used to combine the\nscores for the positive and negative permutations\nrespectively, where these aggregation functions are\ndeﬁned as in Section 4.2. To solve an analogy prob-\nlem, we simply choose the answer candidate that\nresults in the highest value of AP(ti,hi,hq,tq).\n5 Evaluation\nIn this section, we evaluate language models on the\nﬁve analogy datasets presented in Section 3.\n5.1 Experimental Setting\nWe consider three transformer-based LMs of a dif-\nferent nature: two masked LMs, namely BERT (De-\nvlin et al., 2019) and RoBERTa (Liu et al., 2019),\nand GPT-2, as a prominent example of an auto-\nregressive language model. Each pretrained model\nwas fetched from the Huggingface transformers\nlibrary (Wolf et al., 2019), from which we use\nbert-large-cased, roberta-large, and\ngpt2-xl respectively. For parameter selection,\nwe run grid search on β, α, αh, αt, t, g, gpos, and\ngneg for each model and select the conﬁguration\nwhich achieves the best accuracy on each validation\nset. We experiment with the three scoring functions\npresented in Section 4.2, i.e., sPPL (perplexity),\n3615\nModel Score Tuned SAT U2 U4 Google BATS Avg\nLM\nBERT\nsPPL\n32.9 32.9 34.0 80.8 61.5 48.4\n✓ 39.8 41.7 41.0 86.8 67.9 55.4\nsPMI\n27.0 32.0 31.2 74.0 59.1 44.7\n✓ 40.4 42.5 27.8 87.0 68.1 53.2\nsmPPL ✓ 41.8 44.7 41.2 88.8 67.9 56.9\nGPT-2\nsPPL\n35.9 41.2 44.9 80.4 63.5 53.2\n✓ 50.4 48.7 51.2 93.2 75.9 63.9\nsPMI\n34.4 44.7 43.3 62.8 62.8 49.6\n✓ 51.0 37.7 50.5 91.0 79.8 62.0\nsmPPL ✓ 56.7 50.9 49.5 95.2 81.2 66.7\nRoBERTa\nsPPL\n42.4 49.1 49.1 90.8 69.7 60.2\n✓ 53.7 57.0 55.8 93.6 80.5 68.1\nsPMI\n35.9 42.5 44.0 60.8 60.8 48.8\n✓ 51.3 49.1 38.7 92.4 77.2 61.7\nsmPPL ✓ 53.4 58.3 57.4 93.6 78.4 68.2\nWE\nFastText - 47.8 43.0 40.7 96.6 72.0 60.0\nGloVe - 47.8 46.5 39.8 96.0 68.7 59.8\nWord2vec - 41.8 40.4 39.6 93.2 63.8 55.8\nBase\nPMI - 23.3 32.9 39.1 57.4 42.7 39.1\nRandom - 20.0 23.6 24.2 25.0 25.0 23.6\nTable 3: Accuracy results on each analogy dataset, categorized into language models (LM), word embeddings\n(WE), and baselines (Base). All LMs use the analogical proportion (AP) function described in Section 4.3. The\ndefault conﬁguration for AP includes α = αh = αt = β = 0, gpos = g = val1, and t = to-as. Note that\nsPPL = smPPL with the default conﬁguration. Average accuracy (Avg) across datasets is included in the last column.\nsPMI and smPPL. Possible values for each hyperpa-\nrameter (including the selection of six prompts and\nan ablation test on the scoring function) and the\nbest conﬁgurations that were found by grid search\nare provided in the appendix.\nAs baseline methods, we also consider three\npre-trained word embedding models, which have\nbeen shown to provide competitive results in anal-\nogy tasks, as explained in Section 2.2: Word2vec\n(Mikolov et al., 2013a), GloVe (Pennington et al.,\n2014), and FastText (Bojanowski et al., 2017). For\nthe word embedding models, we simply represent\nword pairs by taking the difference between their\nembeddings4. We then choose the answer candi-\ndate with the highest cosine similarity to the query\nin terms of this vector difference. To put the results\ninto context, we also include two simple statisti-\ncal baselines. First, we report the expected ran-\ndom performance. Second, we use a method based\non each word pair’s PMI in a given corpus. We\nthen select the answer candidate with the highest\n4Vector differences have been found to be the most robust\nencoding method in the context of word analogies (Hakami\nand Bollegala, 2017).\nPMI as the prediction. Note that the query word\npair is completely ignored in this case. This PMI\nscore is the well-known word-pair association met-\nric introduced by Church and Hanks (1990) for\nlexicographic purposes (speciﬁcally, collocation\nextraction), which compares the probability of ob-\nserving two words together with the probabilities of\nobserving them independently (chance). The PMI\nscores in our experiments were computed using the\nEnglish Wikipedia with a ﬁxed window size 10.\n5.2 Results\nTable 3 shows our main results. As far as the com-\nparison among LMs is concerned, RoBERTa and\nGPT-2 consistently outperform BERT. Among the\nAP variants, smPPL achieves substantially better re-\nsults than sPMI or sPPL in most cases. We also\nobserve that word embeddings perform surpris-\ningly well, with FastText and GloVe outperform-\ning BERT on most datasets, as well as GPT-2 and\nRoBERTa with default hyperparameters. FastText\nachieves the best overall accuracy on the Google\ndataset, conﬁrming that this dataset is particularly\nwell-suited to word embeddings (see Section 2.2).\n3616\nModel Score Tuned Accuracy\nLM\nBERT\nsPPL\n32.6\n✓ 40.4*\nsPMI\n26.8\n✓ 41.2*\nsmPPL ✓ 42.8*\nGPT-2\nsPPL\n41.4\n✓ 56.2*\nsPMI\n34.7\n✓ 56.8*\nsPPL ✓ 57.8*\nRoBERTa\nsPPL\n49.6\n✓ 55.8*\nsPMI\n42.5\n✓ 54.0*\nsmPPL ✓ 55.8*\nGPT-3 Zero-shot 53.7\nFew-shot ✓ 65.2*\n- LRA - 56.4\nWE\nFastText - 49.7\nGloVe - 48.9\nWord2vec - 42.8\nBase PMI - 23.3\nRandom - 20.0\nTable 4: Accuracy results for the full SAT dataset. Re-\nsults marked with * are not directly comparable as they\nwere tuned on full data (for our models) or use training\ndata (for GPT-3 few-shot). These results are included\nto provide an upper bound only. Results in italics were\ntaken from the original papers.\nIn order to compare with published results from\nprior work, we carried out an additional experiment\non the full SAT dataset (i.e., without splitting it into\nvalidation and test). Table 4 shows the results. GPT-\n3 (Brown et al., 2020) and LRA (Turney, 2005) are\nadded for comparison. Given the variability of the\nresults depending on the tuning procedure, we have\nalso reported results of conﬁgurations that were\ntuned on the entire set, to provide an upper bound\non what is possible within the proposed unsuper-\nvised setting. This result shows that even with\noptimal hyperparameter values, LMs barely outper-\nform the performance of the simpler LRA model.\nGPT-3 similarly fails to outperform LRA in the\nzero-shot setting.\n6 Analysis\nWe now take a closer look into our results to investi-\ngate parameter sensitivity, the correlation between\nmodel performance and human difﬁculty levels,\nand possible dataset artifacts. The following analy-\nsis focuses on smPPL as it achieved the best results\namong the LM based scoring functions.\nFigure 3: Box plot of the relative improvement on\ntest accuracy in each dataset over all conﬁgurations of\nsmPPL grouped by gpos. Here val k corresponds to kth\npositive permutation shown in Figure 2.\nParameter Sensitivity We found that optimal\nvalues of the parameters αand βare highly depen-\ndent on the dataset, while other parameters such\nas the template type t vary across LMs. On the\nother hand, as shown in Figure 3, the optimal per-\nmutations of the templates are relatively consistent,\nwith the original ordering a : b :: c : dtypically\nachieving the best results. The results degrade most\nfor permutations that mix the two word pairs (e.g.\na: c:: b: d). In the appendix we include an abla-\ntion study for the sensitivity and relevance of other\nparameters and design choices.\nDifﬁculty Levels To increase our understanding\nof what makes an analogy problem difﬁcult for\nLMs, we compare the results for each difﬁculty\nlevel.5 Recall from Section 3.2 that the U2 and\nU4 datasets come from educational resources and\nare split by difﬁculty level. Figure 4 shows the\nresults of all LMs (tuned setting), FastText and\nthe PMI baseline according to these difﬁculty lev-\nels. Broadly speaking, we can see that instances\nthat are harder for humans are also harder for the\nconsidered models. The analogies in the most\ndifﬁcult levels are generally more abstract (e.g.\nwitness : testimony :: generator : electricity), or\ncontain obscure or infrequent words (e.g. grouch :\ncantakerous :: palace : ornate).6\n5For SAT, Google and BATS, there are no difﬁculty levels\navailable, but we show the results split by high-level categories\nin the appendix. We also note that the number of candidates\nin U2 and U4 vary from three to ﬁve, so results per difﬁculty\nlevel are not fully comparable. However, they do reﬂect the\nactual difﬁculty of the educational tests.\n6In the appendix we include more examples with errors\nmade by RoBERTa ineasy instances.\n3617\nFigure 4: Test accuracy in U2 and U4 per difﬁculty\nlevel. LMs use smPPL with the best conﬁguration tuned\nin the corresponding validation sets.\nHypothesis Only Recently, several researchers\nhave found that standard NLP benchmarks, such\nas SNLI (Bowman et al., 2015) for language in-\nference, contain several annotation artifacts that\nmakes the task simpler for automatic models (Po-\nliak et al., 2018; Gururangan et al., 2018). One of\ntheir most relevant ﬁndings is that models which do\nnot even consider the premise can reach high accu-\nracy. More generally, these issues have been found\nto be problematic in NLP models (Linzen, 2020)\nand neural networks more generally (Geirhos et al.,\n2020). According to the results shown in Table 3,\nwe already found that the PMI baseline achieved a\nnon-trivial performance, even outperforming BERT\nin a few settings and datasets. This suggests that\nseveral implausible negative examples are included\nin the analogy datasets. As a further exploration of\nsuch artifacts, here we analyse the analogue of a\nhypothesis-only baseline. In particular, for this anal-\nysis, we masked the head or tail of the candidate\nanswer in all evaluation instances. Then, we test\nthe masked language models with the same AP con-\nMask SAT U2 U4 Google BATS\nBERT\nfull 41.8 44.7 41.2 88.8 67.9\nhead 31.8 28.1 34.3 72.0 62.4\ntail 33.5 31.6 38.2 64.2 63.1\nRoBERTa\nfull 53.4 58.3 57.4 93.6 78.4\nhead 38.6 37.7 41.0 60.6 54.5\ntail 35.6 37.3 40.5 55.8 64.2\nTable 5: Accuracy results by masking head or tail of the\ncandidate answers. Results in the top row correspond\nto the full model without masking.\nﬁguration and tuning on these artiﬁcially-modiﬁed\ndatasets.As can be seen in Table 5, a non-trivial\nperformance is achieved for all datasets, which sug-\ngests that the words from the answer pair tend to\nbe more similar to the words from the query than\nthe words from negative examples.\n7 Conclusion\nIn this paper, we have presented an extensive anal-\nysis of the ability of language models to identify\nanalogies. To this end, we ﬁrst compiled datasets\nwith psychometric analogy problems from educa-\ntional resources, covering a wide range of difﬁ-\nculty levels and topics. We also recast two stan-\ndard benchmarks, the Google and BATS analogy\ndatasets, into the same style of problems. Then, we\nproposed standard techniques to apply language\nmodels to the unsupervised task of solving these\nanalogy problems. Our empirical results shed light\non the strengths and limitations of various models.\nTo directly answer the question posed in the title,\nour conclusion is that language models can identify\nanalogies to a certain extent, but not all language\nmodels are able to achieve a meaningful improve-\nment over word embeddings (whose limitations in\nanalogy tasks are well documented). On the other\nhand, when carefully tuned, some language mod-\nels are able to achieve state-of-the-art results. We\nemphasize that results are highly sensitive to the\nchosen hyperparameters (which deﬁne the scoring\nfunction and the prompt among others). Further\nresearch could focus on the selection of these opti-\nmal hyperparameters, including automatizing the\nsearch or generation of prompts, along the lines\nof Bouraoui et al. (2020) and Shin et al. (2020),\nrespectively. Finally, clearly LMs might still be\nable to learn to solve analogy tasks when given\nappropriate training data, which is an aspect that\nwe leave for future work.\n3618\nReferences\nCarl Allen and Timothy Hospedales. 2019. Analo-\ngies explained: Towards understanding word em-\nbeddings. In International Conference on Machine\nLearning, pages 223–231.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A latent variable model\napproach to pmi-based word embeddings. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:385–399.\nKevin D Ashley. 1988. Arguing by analogy in law: A\ncase-based model. In Analogical reasoning, pages\n205–224. Springer.\nNelly Barbot, Laurent Miclet, and Henri Prade. 2019.\nAnalogy between concepts. Artiﬁcial Intelligence ,\n275:487–539.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion of Computational Linguistics, 5(1):135–146.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Ad-\nvances in Neural Information Processing Systems ,\npages 4349–4357.\nAdrian Boteanu and Sonia Chernova. 2015. Solving\nand explaining analogy questions using semantic\nnetworks. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 7456–\n7463.\nZied Bouraoui, Shoaib Jameel, and Steven Schockaert.\n2018. Relation induction in word embeddings revis-\nited. In Proceedings of the 27th International Con-\nference on Computational Linguistics , pages 1627–\n1637, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Annual Conference on Neural Information\nProcessing Systems.\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals. 2016. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech\nrecognition. In 2016 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 4960–4964. IEEE.\nHsiao-Yu Chiang, Jose Camacho-Collados, and\nZachary Pardos. 2020. Understanding the source of\nsemantic regularities in word embeddings. In Pro-\nceedings of the 24th Conference on Computational\nNatural Language Learning, pages 119–131, Online.\nAssociation for Computational Linguistics.\nKenneth Church and Patrick Hanks. 1990. Word as-\nsociation norms, mutual information, and lexicogra-\nphy. Computational linguistics, 16(1):22–29.\nJoe Davison, Joshua Feldman, and Alexander M Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, pages 1173–\n1178.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nAleksandr Drozd, Anna Gladkova, and Satoshi Mat-\nsuoka. 2016. Word embeddings, analogies, and\nmachine learning: Beyond king-man+ woman=\nqueen. In Proceedings of coling 2016, the 26th in-\nternational conference on computational linguistics:\nTechnical papers, pages 3519–3530.\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst.\n2019. Towards understanding linear word analo-\ngies. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 3253–3262.\nAllyson Ettinger. 2019. What bert is not: Lessons from\na new suite of psycholinguistic diagnostics for lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nJerome A. Feldman and Dana H. Ballard. 1982. Con-\nnectionist models and their properties. Cognitive\nScience, 6(3):205–254.\n3619\nLouis Fournier, Emmanuel Dupoux, and Ewan Dun-\nbar. 2020. Analogies minus analogy test: measur-\ning regularities in word embeddings. InProceedings\nof the 24th Conference on Computational Natural\nLanguage Learning, pages 365–375, Online. Asso-\nciation for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nRobert Geirhos, J ¨orn-Henrik Jacobsen, Claudio\nMichaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. 2020.\nShortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665–673.\nAlex Gittens, Dimitris Achlioptas, and Michael W Ma-\nhoney. 2017. Skip-gram- zipf+ uniform= vector ad-\nditivity. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 69–76.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Mat-\nsuoka. 2016. Analogy-based detection of morpho-\nlogical and semantic relations with word embed-\ndings: what works and what doesn’t. InProceedings\nof the Student Research Workshop at NAACL, pages\n8–15.\nAshok Goel. 2019. Computational design, analogy,\nand creativity. In Computational Creativity, pages\n141–158. Springer.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 609–614.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv preprint arXiv:1503.03535.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A.\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 107–112, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nHuda Hakami and Danushka Bollegala. 2017. Com-\npositional approaches for representing relations be-\ntween words: A comparative study. Knowledge-\nBased Systems, 136:172–182.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nGeoffrey E. Hinton. 1986. Learning distributed repre-\nsentations of concepts. In Proceedings of the eighth\nannual conference of the cognitive science society ,\nvolume 1, page 12. Amherst, MA.\nGeoffrey E. Hinton, James L. McClelland, and David E.\nRumelhart. 1986. Distributed representations. Par-\nallel distributed processing: explorations in the mi-\ncrostructure of cognition, vol. 1, pages 77–109.\nKeith J Holyoak, Keith James Holyoak, and Paul Tha-\ngard. 1996. Mental leaps: Analogy in creative\nthought. MIT press.\nTom Hope, Joel Chan, Aniket Kittur, and Dafna Sha-\nhaf. 2017. Accelerating innovation through analogy\nmining. In Proceedings of the 23rd ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining, pages 235–243.\nJohn J. Hopﬁeld. 1982. Neural networks and physi-\ncal systems with emergent collective computational\nabilities. Proceedings of the National Academy of\nSciences, 79(8):2554–2558.\nNicolas Hug, Henri Prade, Gilles Richard, and Math-\nieu Serrurier. 2016. Analogical classiﬁers: a theo-\nretical perspective. In Proceedings of the Twenty-\nsecond European Conference on Artiﬁcial Intelli-\ngence, pages 689–697.\nEyke H ¨ullermeier. 2020. Towards analogy-based ex-\nplanations in machine learning. In International\nConference on Modeling Decisions for Artiﬁcial In-\ntelligence, pages 205–217.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nThomas K. Landauer and Susan T. Dumais. 1997. A\nsolution to Plato’s problem: The latent semantic\nanalysis theory of acquisition, induction, and rep-\nresentation of knowledge. Psychological Review ,\n104(2):211.\nOmer Levy and Yoav Goldberg. 2014. Linguistic\nregularities in sparse and explicit word representa-\ntions. In Proceedings of the Eighteenth Confer-\nence on Computational Natural Language Learning,\n3620\npages 171–180, Ann Arbor, Michigan. Association\nfor Computational Linguistics.\nTal Linzen. 2016. Issues in evaluating semantic spaces\nusing word analogies. In Proceedings of the 1st\nWorkshop on Evaluating Vector-Space Representa-\ntions for NLP, pages 13–18.\nTal Linzen. 2020. How can we accelerate progress to-\nwards human-like linguistic generalization? In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5210–\n5217, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nLaurent Miclet, Sabri Bayoudh, and Arnaud Delhay.\n2008. Analogical dissimilarity: deﬁnition, algo-\nrithms and two experiments in machine learning.\nJournal of Artiﬁcial Intelligence Research , 32:793–\n824.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013a. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of HLT-\nNAACL, pages 746–751.\nMalvina Nissim, Rik van Noord, and Rob van der Goot.\n2020. Fair is better than sensational: Man is to doc-\ntor as woman is to doctor. Computational Linguis-\ntics, 46(2):487–497.\nDenis Paperno and Marco Baroni. 2016. When the\nwhole is less than the sum of its parts: How compo-\nsition affects pmi values in distributional semantic\nvectors. Computational Linguistics, 42(2):345–350.\nZachary A. Pardos and Andrew J. H. Nam. 2020. A\nuniversity map of course knowledge. PLoS ONE ,\n15(9).\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of EMNLP , pages\n1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 1499–1509, Brussels, Belgium. Association\nfor Computational Linguistics.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language in-\nference. In Proceedings of the Seventh Joint Con-\nference on Lexical and Computational Semantics ,\npages 180–191, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nHenri Prade and Gilles Richard. 2017. Analogical pro-\nportions and analogical reasoning-an introduction.\nIn International Conference on Case-Based Reason-\ning, pages 16–32. Springer.\nElie Raad and Joerg Evermann. 2015. The role of anal-\nogy in ontology alignment: A study on lisa. Cogni-\ntive Systems Research, 33:1–16.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2021. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nNaomi Saphra and Adam Lopez. 2019. Understand-\ning learning dynamics of language models with\nSVCCA. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 3257–3267, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with\nneural language models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5831–5837, Hong Kong,\nChina. Association for Computational Linguistics.\n3621\nNatalie Schluter. 2018. The word analogy testing\ncaveat. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 242–246.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019b. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In Proceed-\ning of the 7th International Conference on Learning\nRepresentations (ICLR).\nPeter D. Turney. 2005. Measuring semantic similar-\nity by latent relational analysis. In Proc. of IJCAI ,\npages 1136–1141.\nPeter D. Turney, Michael L. Littman, Jeffrey Bigham,\nand Victor Shnayder. 2003. Combining independent\nmodules in lexical multiple-choice problems. In Re-\ncent Advances in Natural Language Processing III ,\npages 101–110.\nEkaterina Vylomova, Laura Rimell, Trevor Cohn, and\nTimothy Baldwin. 2016. Take and took, gaggle and\ngoose, book and read: Evaluating the utility of vec-\ntor differences for lexical relation learning. In Pro-\nceedings of the 54th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1671–\n1682.\nDouglas Walton. 2010. Similarity, precedent and argu-\nment from analogy. Artiﬁcial Intelligence and Law,\n18(3):217–246.\nAlex Wang and Kyunghyun Cho. 2019. BERT has\na mouth, and it must speak: BERT as a Markov\nrandom ﬁeld language model. In Proceedings of\nthe Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation , pages 30–36,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nA Experimental Details\nIn our grid search to ﬁnd the optimal conﬁgura-\ntion for each dataset and language model, each\nparameter was selected within the values shown in\nTable 6. As the coefﬁcient of marginal likelihood\nα,αh,αt, we considered negative values as well as\nwe hypothesized that the marginal likelihood could\nbe beneﬁcial for LMs as a way to leverage lexical\nknowledge of the head and tail words.\nAdditionally, Table 7 shows the set of custom\ntemplates (or prompts) used in our experiments. Fi-\nnally, Tables 8, 9, and 10 include the best conﬁgura-\ntion based on each validation set in for sPMI, smPPL\nand the hypothesis-only baseline, respectively.\nParameter Value\nα -0.4, -0.2, 0, 0.2, 0.4\nαh -0.4, -0.2, 0, 0.2, 0.4\nαt -0.4, -0.2, 0, 0.2, 0.4\nβ 0, 0.2, 0.4, 0.6, 0.8, 1.0\ng max,mean,min,val1,val2\ngpos max,mean,min,val1,...,val8\ngneg max,mean,min,val1,...,val16\nTable 6: Hyperparameters with each search space.\nType Template\nto-as [w1] is to [w2] as [w3] is to [w4]\nto-what [w1] is to [w2] What [w3] is to [w4]\nrel-same\nThe relation between [w1] and [w2]\nis the same as the relation between\n[w3] and [w4].\nwhat-to what [w1] is to [w2], [w3] is to [w4]\nshe-as She explained to him that [w1] is\nto [w2] as [w3] is to [w4]\nas-what\nAs I explained earlier, what [w1] is\nto [w2] is essentially the same as\nwhat [w3] is to [w4].\nTable 7: Custom templates used in our experiments.\nEach has four placeholders[w1,...,w 4] and they are ful-\nﬁlled by words from a relation pair.\n3622\nData g α g pos gneg β t\nBERT\nSAT val2 -0.4 val 5 val12 0.4 what-to\nU2 val2 -0.4 mean mean 0.6 what-to\nU4 val1 0.4 max val 7 1.0 rel-same\nGoogle val1 -0.4 val 1 val11 0.4 she-as\nBATS val1 -0.4 val 11 val1 0.4 she-as\nGPT-2\nSAT val2 -0.4 val 3 val1 0.6 rel-same\nU2 val2 0.0 val 4 val4 0.6 rel-same\nU4 val2 -0.4 mean mean 0.6 rel-same\nGoogle val1 0.0 mean val 11 0.4 as-what\nBATS val1 -0.4 val 1 val6 0.4 rel-same\nRoBERTa\nSAT min -0.4 min val 7 0.2 as-what\nU2 min 0.4 mean val 4 0.6 what-to\nU4 val2 0.0 mean val 4 0.8 to-as\nGoogle val1 -0.4 val 1 val6 0.4 what-to\nBATS max -0.4 mean val 11 0.6 what-to\nTable 8: The best conﬁguration of sPMI score.\nData αh αt gpos gneg β t\nBERT\nSAT -0.2 -0.4 val 5 val5 0.2 what-to\nU2 0.0 -0.2 mean mean 0.8 she-as\nU4 -0.2 0.4 val 7 min 0.4 to-as\nGoogle 0.4 -0.2 val 5 val12 0.6 she-as\nBATS 0.0 0.0 val 8 min 0.4 what-to\nGPT-2\nSAT -0.4 0.2 val 3 val1 0.8 rel-same\nU2 -0.2 0.2 mean mean 0.8 as-what\nU4 -0.2 0.2 mean mean 0.8 rel-same\nGoogle -0.2 -0.4 mean mean 0.8 rel-same\nBATS 0.4 -0.4 val 1 val5 0.8 rel-same\nRoBERTa\nSAT 0.2 0.2 val 5 val11 0.2 as-what\nU2 0.4 0.4 val 1 val4 0.4 what-to\nU4 0.2 0.2 val 1 val1 0.4 as-what\nGoogle 0.2 0.2 val 1 val6 0.2 what-to\nBATS 0.2 -0.2 val 5 val11 0.4 what-to\nTable 9: The best conﬁguration of smPPL score.\nB Additional Ablation Results\nWe show a few more complementary results to our\nmain experiments.\nB.1 Alternative Scoring Functions\nAs alternative scoring functions for LM, we have\ntried two other scores: PMI score based on masked\ntoken prediction (Davison et al., 2019) (Mask PMI)\nand cosine similarity between the embedding dif-\nference of a relation pair similar to what used in\nword-embedding models. For embedding method,\nwe give a prompted sentence to LM to get the last\nlayer’s hidden state for each word in the given pair\nand we take the difference between them, which we\nregard as the embedding vector for the pair. Finally\nwe pick up the most similar candidate in terms of\nthe cosine similarity with the query embedding. Ta-\nMask Data gpos t\nBERT\nhead\nSAT val 5 to-what\nU2 val 5 to-as\nU4 mean to-as\nGoogle val 5 she-as\nBATS val 5 to-as\ntail\nSAT val 3 what-to\nU2 val 7 to-what\nU4 val 4 rel-same\nGoogle val 7 as-what\nBATS val 7 to-as\nRoBERTa\nhead\nSAT val 5 as-what\nU2 val 5 rel-same\nU4 val 7 she-as\nGoogle val 5 what-to\nBATS val 5 she-as\ntail\nSAT mean what-to\nU2 val 7 rel-same\nU4 mean what-to\nGoogle val 7 as-what\nBATS val 7 what-to\nTable 10: The best conﬁgurations for hypothesis-only\nscores.\nble 11 shows the test accuracy on each dataset. As\none can see, AP scores outperform other methods\nwith a great margin.\nScore SAT U2 U4 Google BATS\nBERT\nembedding 24.0 22.4 26.6 28.2 28.3\nMask PMI 25.2 23.3 31.5 61.2 46.2\nsPMI 40.4 42.5 27.8 87.0 68.1\nsmPPL 41.8 44.7 41.2 88.8 67.9\nRoBERTa\nembedding 40.4 42.5 27.8 87.0 68.1\nMask PMI 43.0 36.8 39.4 69.2 58.3\nsPMI 51.3 49.1 38.7 92.4 77.2\nsmPPL 53.4 58.3 57.4 93.6 78.4\nTable 11: Test accuracy tuned on each validation set.\nB.2 Parameter Sensitivity: template type t\nFigure 5 shows the box plot of relative improve-\nment across all datasets grouped by tand the re-\nsults indicate that there is a mild trend that certain\ntemplates tend to perform well, but not signiﬁcant\nuniversal selectivity can be found across datasets.\nB.3 Parameter Sensitivity: aggregation\nmethod gneg\nFigure 6 shows the box plot of relative improve-\nment across all datasets grouped by gneg. Unlike\ngpos we show in Figure 3, they do not give a strong\nsignals over datasets.\n3623\nFigure 5: Box plot of the relative improvement on\ntest accuracy in each dataset over all conﬁgurations of\nsmPPL grouped by template type.\nFigure 6: Box plot of the relative improvement on\ntest accuracy in each dataset over all conﬁgurations of\nsmPPL grouped by gneg. Here val k corresponds to kth\npositive permutation shown in Figure 2.\nB.4 Relation Types in BATS/Google\nFigure 7 shows the results of different language\nmodels with the smPPL scoring function on the dif-\nferent categories of the BATS and Google datasets.\nC Error Analysis\nTable 12 shows all examples from the U2 dataset\nof the easiest difﬁculy (i.e. grade 4), which were\nmisclassiﬁed by RoBERTa, with smPPL tuned on\nthe validation set. We can see a few typical issues\nwith word embeddings and language models. For\ninstance, in the ﬁrst example, the model confuses\nthe antonym pair right:wrong with synonymy. In\nthe second example, we have that someone who is\npoor lacks money, while someone who is hungry\nlacks food. However, the selected candidate pair\nis hungy:water rather than hungry:food, which is\nFigure 7: BATS (top) and Google (bottom) results split\nby high-level categories.\npresumably chosen because water is assumed to\nbe a near-synonym of food. In the third example\n(wrench:tool), the hypnernymy relation is confused\nwith a meronymy relation in the selected candidate\ntree:forest. In the last three examples, the model\nhas selected answers which seem reasonable. In the\nfourth example, beautiful:pretty, terrible:bad and\nbrave:valiant can all be considered to be synonym\npairs. In the ﬁfth example, vehicle:transport is\nclearly the correct answer, but the pair song:sing\nis nonetheless relationally similar to shield:protect.\nIn the last example, we can think of being sad as\nan emotional state, like being sick is a health state,\nwhich provides some justiﬁcation for the predicted\nanswer. On the other hand, the gold answer is based\non the argument that someone who is sick lacks\nhealth like someone who is scared lacks courage.\n3624\nQuery Candidates\nhilarious:funny right:wrong , hard:boring, nice:crazy,\ngreat:good\npoor:money tired:energy, angry:emotion, hot:ice,\nhungry:water\nwrench:tool cow:milk, radio:sound, tree:forest ,\ncarrot:vegetable\nbeautiful:pretty terrible:bad, brave:valiant, new:old,\ntall:skinny\nshield:protect computer:talk, vehicle:transport,\npencil:make, song:sing\nsick:health sad:emotion , tall:intelligence,\nscared:courage, smart:energy\nTable 12: Model prediction examples from RoBERTa\nwith smPPL tuned on the validation set. Gold answers\nare shown in bold, while the model predictions are un-\nderlined.",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.7171511650085449
    },
    {
      "name": "Computer science",
      "score": 0.7006794214248657
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6422479748725891
    },
    {
      "name": "Computational linguistics",
      "score": 0.6204823851585388
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.46603697538375854
    },
    {
      "name": "Joint (building)",
      "score": 0.4559493660926819
    },
    {
      "name": "Linguistics",
      "score": 0.4391494691371918
    },
    {
      "name": "Engineering",
      "score": 0.09050893783569336
    },
    {
      "name": "Philosophy",
      "score": 0.07135003805160522
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79510175",
      "name": "Cardiff University",
      "country": "GB"
    }
  ]
}