{
  "title": "Vision transformer architecture and applications in digital health: a tutorial and survey",
  "url": "https://openalex.org/W4383710235",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4309608615",
      "name": "Khalid Al-hammuri",
      "affiliations": [
        "University of Victoria"
      ]
    },
    {
      "id": "https://openalex.org/A2008328290",
      "name": "Fayez Gebali",
      "affiliations": [
        "University of Victoria"
      ]
    },
    {
      "id": "https://openalex.org/A2899200480",
      "name": "Awos Kanan",
      "affiliations": [
        "Princess Sumaya University for Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2047895960",
      "name": "Ilamparithi Thirumarai Chelvan",
      "affiliations": [
        "University of Victoria"
      ]
    },
    {
      "id": "https://openalex.org/A4309608615",
      "name": "Khalid Al-hammuri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2008328290",
      "name": "Fayez Gebali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2899200480",
      "name": "Awos Kanan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047895960",
      "name": "Ilamparithi Thirumarai Chelvan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4315705623",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4206693420",
    "https://openalex.org/W2886943560",
    "https://openalex.org/W4292825881",
    "https://openalex.org/W2807122651",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W3168491317",
    "https://openalex.org/W2884721730",
    "https://openalex.org/W2944851425",
    "https://openalex.org/W6600654476",
    "https://openalex.org/W6725794755",
    "https://openalex.org/W6815217517",
    "https://openalex.org/W3195473712",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W2766585573",
    "https://openalex.org/W4286587621",
    "https://openalex.org/W4297491535",
    "https://openalex.org/W4296693015",
    "https://openalex.org/W4309607309",
    "https://openalex.org/W4286008658",
    "https://openalex.org/W4296139711",
    "https://openalex.org/W4281254982",
    "https://openalex.org/W4292318628",
    "https://openalex.org/W2954011593",
    "https://openalex.org/W4362603432",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W2613456556",
    "https://openalex.org/W4226524654",
    "https://openalex.org/W6600208142",
    "https://openalex.org/W6603957568",
    "https://openalex.org/W4296123084",
    "https://openalex.org/W4295936749",
    "https://openalex.org/W3202006954",
    "https://openalex.org/W4287225564",
    "https://openalex.org/W4221166280",
    "https://openalex.org/W3092462072",
    "https://openalex.org/W2980998394",
    "https://openalex.org/W4221091790",
    "https://openalex.org/W3165793043",
    "https://openalex.org/W4295934705",
    "https://openalex.org/W4225426831",
    "https://openalex.org/W6630853386",
    "https://openalex.org/W3188404242",
    "https://openalex.org/W6600579670",
    "https://openalex.org/W4281853941",
    "https://openalex.org/W3213783748",
    "https://openalex.org/W3202941780",
    "https://openalex.org/W4319299841",
    "https://openalex.org/W4313134585",
    "https://openalex.org/W4295747653",
    "https://openalex.org/W2106059936",
    "https://openalex.org/W3203149165",
    "https://openalex.org/W4288442702",
    "https://openalex.org/W4223997512",
    "https://openalex.org/W4280514269",
    "https://openalex.org/W4295936537",
    "https://openalex.org/W4226281901",
    "https://openalex.org/W4213065378",
    "https://openalex.org/W4226133625",
    "https://openalex.org/W3137849220",
    "https://openalex.org/W1758598986",
    "https://openalex.org/W1186184816",
    "https://openalex.org/W3035596626",
    "https://openalex.org/W3039236647",
    "https://openalex.org/W2972862926",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W3202624106",
    "https://openalex.org/W6745514887",
    "https://openalex.org/W4221148807",
    "https://openalex.org/W3203467945",
    "https://openalex.org/W6600741150",
    "https://openalex.org/W4226227406",
    "https://openalex.org/W4285021577",
    "https://openalex.org/W4212822134",
    "https://openalex.org/W4229000188",
    "https://openalex.org/W2979956313",
    "https://openalex.org/W3166845084",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W4285531589",
    "https://openalex.org/W3207576362",
    "https://openalex.org/W2924551358",
    "https://openalex.org/W2819321284",
    "https://openalex.org/W4226227293",
    "https://openalex.org/W4226016066",
    "https://openalex.org/W4292387419",
    "https://openalex.org/W3183943918",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6860043847",
    "https://openalex.org/W4288809219",
    "https://openalex.org/W3132130152",
    "https://openalex.org/W4312604272",
    "https://openalex.org/W2898085636",
    "https://openalex.org/W2991357568",
    "https://openalex.org/W6603242443",
    "https://openalex.org/W4229439289",
    "https://openalex.org/W3204685218",
    "https://openalex.org/W4283736397",
    "https://openalex.org/W3101704389",
    "https://openalex.org/W4229021932",
    "https://openalex.org/W4225323865",
    "https://openalex.org/W4226085666",
    "https://openalex.org/W3105636987",
    "https://openalex.org/W4213099919",
    "https://openalex.org/W2068296444",
    "https://openalex.org/W2125729588"
  ],
  "abstract": null,
  "full_text": "Al‑hammuri et al. \nVisual Computing for Industry, Biomedicine, and Art            (2023) 6:14  \nhttps://doi.org/10.1186/s42492‑023‑00140‑9\nREVIEW Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nVisual Computing for Industry,\nBiomedicine, and Art\nVision transformer architecture \nand applications in digital health: a tutorial \nand survey\nKhalid Al‑hammuri1*  , Fayez Gebali1, Awos Kanan2 and Ilamparithi Thirumarai Chelvan1 \nAbstract \nThe vision transformer (ViT) is a state ‑of‑the‑art architecture for image recognition tasks that plays an important role \nin digital health applications. Medical images account for 90% of the data in digital medicine applications. This article \ndiscusses the core foundations of the ViT architecture and its digital health applications. These applications include \nimage segmentation, classification, detection, prediction, reconstruction, synthesis, and telehealth such as report \ngeneration and security. This article also presents a roadmap for implementing the ViT in digital health systems and \ndiscusses its limitations and challenges.\nKeywords Vision transformer, Digital health, Telehealth, Artificial intelligence, Medical imaging\nIntroduction\nThe coronavirus disease 2019 (COVID-19) pandemic \ndemonstrated how artificial intelligence (AI) can help \nscale a system during emergencies with limited medical \nstaff or existing safety concerns. AI algorithms are widely \nused in digital medicine solutions, mainly in image and \ntext recognition tasks, to analyze medical data stored \nin clinical information systems and generate medical \nreports, and to assist in other technical operations such \nas robotic surgery. Among the various AI-assisted tools \nfor analyzing medical images, the vision transformer \n(ViT) has emerged as a state-of-the-art algorithm that \nreplaces or combines traditional techniques such as con -\nvolutional neural networks (CNNs). This article discusses \nthe foundations and applications of the ViT in digital \nhealth.\nThe ViT [1, 2] is a type of neural network for image pro-\ncessing in computer vision tasks [3]. The backbone of the \nViT is a self-attention mechanism typically used in natu -\nral language processing (NLP). The ViT was introduced \nto deal with the image processing limitations of common \nmachine learning architectures such as CNNs [4], recur -\nrent neural networks (RNNs) [5], and even the traditional \ntransformers for language models [1, 6]. The ViT pro -\nvides a strong representation of image features and trains \ndata using fewer computational resources compared with \nCNNs [1].\nCNNs are widely used in the machine learning field \nand are suitable for feature extraction in specific local \nregions. However, they are unable to capture the con -\ntextual relationship between image features in the global \ncontext. In contrast, the ViT applies an attention mech -\nanism to understand the global relationships among \nfeatures.\nRNNs are used to obtain inferences about sequence-\nto-sequence relationships and memorizes some past \ndata. However, they require a large memory and are \nunsuitable for extracting image features compared \nwith the ViT or CNNs. Bidirectional encoder represen -\ntations from transformers (BERT) was developed by \n*Correspondence:\nKhalid Al‑hammuri\nkhalidalhammuri@uvic.ca\n1 Electrical and Computer Engineering, University of Victoria, Victoria V8W \n2Y2, Canada\n2 Computer Engineering, Princess Sumaya University for Technology, \nAmman 11941, Jordan\nPage 2 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nGoogle to process language models [7 ] based on atten -\ntion mechanisms [8 ]. BERT can efficiently process \nsequence-to-sequence models but requires a larger \nmemory compared with an RNN or a long short-term \nmemory (LSTM) [9 ].\nBERT has limitations in processing imaging data \nand is effective only for flattened data in a sequential \nshape. To deal with this issue, the ViT splits images  \ninto patches then and flattens them for analysis as linear \nsequences [ 1] in a parallel processing mechanism.\nThe applications of the ViT in medical imaging \ninclude segmentation, classification, reconstruction, \nprognosis prediction, and telehealth (e.g., report gen -\neration and security).\nThe remainder of this paper is organized as follows.  \nViT architecture section describes the foundations \nof the ViT architecture. Applications of the ViT in \ndigital health section presents an overview of the \nimportant applications of the ViT in medical imag -\ning. Roadmap for implementing ViT section presents \na roadmap for the end-to-end implementation of \nthe ViT. Limitations and challenges of ViT in digital \nhealth section discusses the limitations and challenges \nof using the ViT, and Conclusions section concludes \nthe paper.\nViT architecture\nThis section discusses the core principles and founda -\ntions of the ViT based on the attention mechanism. \nThe ViT architecture consists of a hierarchy of different \nfunctional blocks, which will be explained in the follow -\ning subsections. Figure  1 shows the typical transformer \narchitecture proposed by ref. [8] based on the attention \nmechanism.\nResearchers have proposed various modifications for \ntypical transformer designs [8] (Fig. 18 for a typical trans-\nformer architecture in Appendix) for applications other \nthan NLP tasks. The changes focus on the design frame -\nwork of encoder-decoder blocks in the transformer archi-\ntecture. In vision tasks, the transformer splits the image \ninto patches and flattens them into sequential forms to \nbe processed like time-series data, which is more suited \nto the nature of transformers (Fig.  19 in Appendix). To \nensure that an image can be reconstructed without any \ndata loss, positional encoding was utilized for the embed-\nded features in a vector shape. The embedded features \nwere fed into the encoder for image classification and \nthen classified by multilayer perception [1]. However, in \nthe segmentation task, the transformer is combined with \nthe CNN either in the encoder stage, similar to the Tran -\nsUNet architecture (Fig. 20 in Appendix) [10], or in both \nFig. 1 Transformer architecture [1]\nPage 3 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \nthe encoder and decoder stages, such as the Ds-TransU -\nNet [11] (Fig. 21 in Appendix).\nEncoder architecture\nFigure  2 shows a typical encoder architecture [ 8] that \nconsists of a stack of N identical layers, with each layer \ncontaining two sublayers. The first sublayer performs the \nmultihead self-attention (MSA), while the second sub -\nlayer normalizes the output of the first sublayer and feeds \nit into the multilayer perceptron (MLP), which is a type \nof feedforward network. See Appendix  A.1 for an exam -\nple of the transformer architecture in ref. [1].\nImage patches embedding\nOwing to computer memory limitations, the simultane -\nous processing of an entire image is difficult. Therefore, \nthe image is divided into different patches and processed \nsequentially. To conduct a detailed analysis of each image \npatch, each one was embedded into a set of feature values \nin the form of a vector.\nThe concept of image patch embedding in the ViT was \ninspired by the term ‘embedding’ in ref. [ 12]. The feature \nvectors were then graphically visualized in an embedding \nspace. Visualizing the features in the embedding space is \nbeneficial to identify the image patches with similar features \n[13]. The distance between each feature can be measured in \nthe features map to determine the degree of similarity [14].\nFigure 3 shows the feature embedding process, which \nbegins by creating an embedding layer from the embed -\nding vectors of each input feature. Random embedding \nvalues are initially assigned and updated during training \ninside the embedding layer. During training, similar fea -\ntures become closer to each other in the embedding or \nlatent space. This is important to classify or extract simi -\nlar features. However, not knowing the position of each \nfeature makes it difficult to determine the relationship \nbetween them. In medical imaging applications, posi -\ntional encoding and feature embedding enable accurate \nfeature selection in a specific-use case.\nPositional encoding\nThe transformer model has the advantage of simultane -\nously processing inputs in parallel, unlike the well-known \nLSTM algorithm [15, 16]. However, parallel processing is \ndifficult because of the risk of information loss due to the \ninability to reconstruct the processed sequences in their \noriginal positions.\nFigure 4 shows the positional encoding process for fea -\nture representation. Positional encoding was proposed \nto solve this problem and encode each feature vector to \nits accurate position [ 8, 17]. The feature vector and posi -\ntional encoding values were added to form a new vector \nin the embedding space. In this study, sine and cosine \nfunctions were used as examples to derive the positional \nencoding values at different frequencies, expressed as \nEqs.(1) and (2) [8], respectively.\nwhere P is the positional encoding, d is the vector dimen-\nsion, x is the position, and i is the index dimension. The \nsinusoidal function is beneficial for encoding the feature \n(1)P (x,2i) = sin\n( x\n10000\n2i\nd\n)\n(2)P (x,2i+ 1) = cos x\n10000\n2i\nd\nFig. 2 Encoder block in the transformer architecture [1]\nPage 4 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nposition in the embedding space using frequencies rang -\ning from 2π to 10000. In Eqs. (1) and (2), the frequencies \nresembled the index dimension i [8].\nMSA\nFigure  5 shows the MSA process, which calculates the \nweighted average of feature representations based on \nthe similarity scores between pairs of representations. \nGiven the input sequence X  of L  tokens or entries with \nthe dimension d , X ∈ RL×d was projected using three \nmatrices: WK ∈ Rd×dk, WQ ∈ Rd×dq, and WV ∈ Rd×dv with \nthe same dimensions to derive the representation of \nthe features. Equation (3 ) presents the formulas used to \nderive the Key (K ), Query (Q ), and Value (V ).\nThe final embedding layer that includes the position \nencoding was copied into the three linear layers K , Q, \nand V. To derive the similarity between the input fea -\ntures, matrix multiplication between K  and Q  was per -\nformed using self-attention. The output was then scaled \nand normalized using SoftMax. The self-attention [3 ] \nprocess is explained in the following steps:\n1. Calculate the score from the input of Q and K.\n(3)K = XW K, Q = XW Q, V = XW V\n2. Normalize the score to stabilize the training.\n3. Calculate the probabilities of the normalized score \nusing SoftMax.\n4. Compute the self-attention filter by multiplying P \nand V.\nThe multiplication of the outputs of K and Q were scaled \nby the square root of the input vector dimension, and then \nnormalized by the SoftMax function to generate the proba-\nbilities. Equation (8) presents the SoftMax function, where \nx is the input data point. Equation (9) computes the atten-\ntion filter.\n(4)S = QK T\n(5)N s = S\n√\nd\n(6)P = SoftMax(Ns)\n(7)Self − attention= PV\n(8)SoftMax (xi) = exp(xi)∑\nj exp(xj)\n(9)\nSelf− attention(Q , K , V ) = SoftMax\n(\nQ ·K T\n√ d\n)\n.V\nFig. 3 a Illustration of splitting ultrasound images into patches and flattening them in a linear sequence; b Image patch vectorization and linear \nprojection; c Patch embedding in multidimensional space\nPage 5 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \nThe output probabilities from SoftMax and the value \nlayer were multiplied to obtain the desired output with \nemphasis on the desired features to filter out unneces -\nsary data. The principle behind a multihead is to con -\ncatenate the results of different attention filters, with \neach one focusing on the desired features. The self-\nattention process is repeated multiple times to form \nthe MSA. The final output of the concatenated MSA \nwas passed through a linear layer and resized to a single \nhead. Equation (10) presents the MSA formula.\nwhere C is the concatenation of the multiheads; W 0 is \nthe projection weight; Q, K and V denote the Query, Key \nand Value, respectively; and h resembles each head in the \nself-attention process and was replicated n times. The \nnumber of replications was dependent on the amount \nof attention or the desired features needed to extract the \nrequired information. Figure 5 shows the MSA process in \n(10)MSA(Q,K,V) = C(h1 , ... ,hn)W 0\nthe ViT architecture. Detailed information on the scaled \ndot products between K, Q, and V are also presented.\nLayer normalization and residual connections\nA residual connection is required to directly feed the \noutput from the position encoding layer into the nor -\nmalization layer by bypassing the MSA layer [18]. The \nresidual connection is essential for knowledge preser -\nvation and to avoid vanishing gradient problems [19, \n20]. The MSA layer is vital for extracting useful features \nfrom the input. However, it could also lead to the dis -\nregard of helpful information of lesser weight in the \nattention filter. Minimizing the value of the feature \nweight may cause a vanishing gradient during the model \ntraining stage. A vanishing gradient occurs when the \ngradient of the loss function is depleted and becomes \nalmost or equal to zero while optimizing the weight in \nthe backpropagation algorithm. The residual connection \ndirectly feeds information from the initial layers into the \nFig. 4 Positional encoding for the feature representations. Top: Sinusoidal representation for the positional encoding (P0‑P3) at different indices \nand dimensions. Bottom: Vector representation for the positional encoding and feature embedding; P is the position encoding and E is the \nembedding vector\nPage 6 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nlayers at the end of the neural network to preserve fea -\ntures and retain important information.\nThe add and normalize layer [21] combine the input \nfrom the position encoding and MSA layers, and then \nnormalize them. The normalization layer is essential \nduring training to speed up and stabilize the loss con -\nvergence. Normalization can be achieved by stand -\nardizing the activation of neurons along the axis of \nthe features. Equations (11) and (12) are the statistical \ncomponents of layer normalization over all the hidden \nunits in the same layer [21].\nwhere a l\ni is the normalized value of the sum of the input \nfeatures along the ith hidden units in the lth layers. H is the \ntotal number of hidden units in the layer. µ is the mean or \naverage values of features along the axis in the normaliza-\ntion layer, σ is the standard deviation of the values of the \nfeatures along the axis.\nMLP\nFigure  6 shows the MLP diagram, which is part of the \nViT architecture. The MLP is a feedforward artificial \n(11)µ l = 1\nH (\n∑H\ni=1 al\ni)\n(12)σl =\n√\n1\nH\n∑ H\ni=1\n(\nal\ni − µl\n)2\nneural network that combines a series of fully connected \nlayers including the input, one or more hidden layers in \nthe middle, and the output [22].\nFully connected layers are a type of layer in which the \noutput of each neuron is connected to all the neurons in \nthe next hidden layer. The diagram shows that each neu -\nron from the layer in the feedforward neural network is \nconnected to all the neurons in the next layer through an \nactivation function. The residual connection preserves \nthe knowledge from the initial layers and minimizes the \nvanishing gradient problem. Typical MLP layers include \nthe input, output, and hidden layers.\nDecoder and mask MSA\nFigure  7 shows the decoder and mask MSA in the ViT \narchitecture used to extract the final image. The decoder \nwas stacked for N layers, the same as the number of \nencoder layers. The decoder includes the same sublay -\ners as the encoder and mask MSA stacked on them. The \nmask MSA works similarly as the MSA, but focuses on \nthe desired features in position i and ignores the unde -\nsirable features from the embedding layer by using the \nmask-only features before i. This is important to obtain \nan inference from the relationship between different fea -\ntures in the embedding space and a prediction from the \nfeatures relevant to the desired position.\nFig. 5 MSA process. a MSA process with several attention layers in parallel; b Scaled dot product [8]. The diagram flows upwards from the bottom \naccording to the direction of the arrow\nPage 7 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \nThe decoder obtains the V , Q, and K  as inputs. The \nV was obtained from the previous embedding space, \nwhile Q and K  were obtained from the encoder  \noutput. There are other MSA and normalization \nlayers inside the decoder, which is common in ViT \ndesigns. Despite modifications to the decoder-encoder \ndesign, the core principle remains the same. The  \ndifferent architectures for different applications are \nexplained in Applications of the ViT in digital health  \nsection.\nIn the image recognition task, the decoder output was \nflattened as a linear or dense layer. Then, SoftMax was \nused to derive the probability of the weight of each neu -\nron in the dense layer. The final probability was used to \nclassify or segment the features based on the training \ndata to detect the final object or image.\nFig. 6 MLP\nFig. 7 Decoder and mask multihead attention block to produce the final image\nPage 8 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nApplications of the ViT in digital health\nComputer vision and machine learning algorithms have \nbeen employed in recent medical studies on brain and \nbreast tumors [23, 24], histopathology [25], speech rec -\nognition [26, 27], rheumatology [28], automatic cap -\ntioning [29], endoscopy [30], fundus imaging [31], and \ntelemedicine [32]. The ViT has emerged as the state-of-\nthe-art in AI-based algorithms that use computer vision \nand machine learning for digital health solutions.\nFigure 8 shows the distribution of ViT applications in \nthe medical field. These include medical segmentation, \ndetection, classification, report generation, registration, \nprognosis prediction, and telehealth.\nApplications of ViT in medical image segmentation\nTransUNet [10] is one of the earliest attempts to apply \nthe ViT in medical imaging segmentation by combining \nit with the UNet [34] architecture. UNet is well known \nin the area of biomedical image segmentation. It is effi -\ncient in object segmentation tasks and can preserve \nthe quality of fine image details after reconstruction. \nThe UNet inherited the localization ability of a CNN \nfor feature extraction. Although localization is essen -\ntial in a segmentation task, it has limitations in process -\ning sequence-to-sequence image frames or extracting \nglobal features within the same image outside a specific \nregion. In contrast, the ViT has the advantages of pro -\ncessing sequence-to-sequence features and extracting \nthe global relationships between them. However, the ViT \nhas limitations in feature localization compared with \nCNNs. TransUNet proposed a robust architecture that \ncombined the capabilities of the ViT and UNet in a single \nmodel.\nTransUNet is a powerful tool for multiorgan segmen -\ntation. Segmenting different objects is essential to ana -\nlyze complex structures in magnetic resonance imaging \n(MRI) and computed tomography (CT) images. Figure  9 \nshows an example of image segmentation of the abdo -\nmen in a CT scan using TransUNet, which was compared \nwith ground truth (GT) images to validate the results.\nTo further improve the TransUNet architecture, a \nDual-TransUNet was implemented in ref. [11]. The main \ndifference is that the Dual-TransUNet used the trans -\nformer in the encoder to extract features and the decoder \nto reconstruct the desired image, while the TransUNet \nonly used the transformer in the encoder stage. The Swin \ntransformer [35] is another architecture for implement -\ning the ViT in combination with Unet [34, 36] in medical \nimaging.\nThe ViT was also used in iSegFormer [37], which was \nproposed for the interactive segmentation of three-\ndimensional (3D) MRI images of the knee. The 3D UX-\nnet [38] could segment brain tissues from the entire body \nin an MRI scan. UNesT [39] developed a hierarchical \ntransformer using local spatial representation for brain, \nkidney, and abdominal multiorgan image segmentation. \nSimilarly, the NestedFormer [40] was proposed to seg -\nment brain tumors in MRI images.\nRECIST [ 41] used the ViT to automatically segment \nbrain tumors to measure the size of the lesions in CT \nFig. 8 Distribution of medical imaging applications of the ViT according to the survey [33]\nPage 9 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \nimages. GT U-Net [42] was used for tooth therapy by \nsegmenting the root canal in X-ray images. Colorectal \ncancer (CRC) images were segmented by the fully convo -\nlutional network (FCN) transformer [43] during a colo -\nnoscopy. The ViT was also used in the TraSeTR [44] to \nassist in robotic surgery by segmenting the image and \ngenerating instructions based on previous knowledge. \nTable  1 lists examples of ViT applications in medical \nimage segmentation.\nApplications of ViT in medical image detection\nImage detection plays a key role in digital health and \nimaging analysis to identify objects in complex structures \nand share that information within the healthcare infor -\nmation system for further analysis. This is important to \nmeasure the cell size and count the number of suspicious \nobjects or malignant tissues.\nObject detection is essential in cancer screening when \ncell labeling or classification is difficult, and a careful \nanalysis is required to identify cancers. The detection \ntransformer (DETR) was proposed to detect lymphopro -\nliferative diseases in MRI T2 images [48]. In MRI scans, \nthe metastatic lymph nodes are small and difficult to \nidentify. The application of the DETR can reduce false \npositives as well as improve the precision and sensitivity \nby 65.41% and 91.66%, respectively.\nThe convolutional transformer (COTR) [49] detects \npolyp lesions in colonoscopy images to diagnose CRC, \nwhich has the second highest cancer-related mortality \nrisk worldwide. The COTR architecture employs a CNN \nAorta Gallbladder Left Kidney Right Kidney\nLiver Pancreas Spleen Stomach\n(a)\n(b)\nFig. 9 Comparison of TransUNet and GT using output segmentation results of different organs: a GT (expert reference) and b TransUNet [10]\nTable 1 Examples of ViT applications in medical image segmentation\nMethod Category Medical application\nTransUnet [10] MRI, CT CT and MRI cardiac segmentation\nDual‑TransUnet [11] Microscopy Skin lesion analysis [45]; gland segmentation in histology \n[46]; nuclei in divergent images [47]\nSwin‑Unet [35] CT Abdominal multiorgan segmentation\niSegFormer [37] 3D MRI Knee image segmentation\n3D UX‑net [38] 3D MRI Brain tissue segmentation\nUNesT [39] MRI, CT Abdominal multiorgan segmentation + kidney segmenta‑\ntion + whole brain segmentation\nNestedFormer [40] MRI Brain tumor segmentation\nRECIST [41] CT Automatic tumor segmentation and diameter size prediction\nGT U‑Net [42] X‑ray Tooth therapy: root canal segmentation\nFCN‑transformer [43] Colonoscopy CRC segmentation\nTraSeTR [44] Endoscopy Robot‑assisted surgery\nPage 10 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nfor feature extraction and convergence acceleration. A \ntransformer encoder is used to encode and recalibrate \nthe features, a transformer decoder for object querying, \nand a feedforward network for object detection.\nGlobal lesion detection in CT scans was performed \nusing a slice attention transformer (SATr) [50]. The \nbackbone of the SATr is a combination of convolution  \nand transformer attention that detects log-distance fea -\nture dependencies while preserving the local features.\nLung nodule detection was investigated using \nan unsupervised contrastive learning-based trans -\nformer (UCLT) [51]. Lung nodules are small cancer -\nous masses that are difficult to detect in complex lung \nstructures because of their size. This study harnessed \ncontrastive learning (CL) and the ViT to break down \nthe volume of CT images into small patches of non-\noverlapping cubes, and extract the embedded fea -\ntures for processing using the transformer attention \nmechanism.\nTo predict the hemorrhage category of brain inju -\nries in CT scans, a transformer-based architecture was \nused for intracranial hemorrhage detection (IHD) [52]. \nTable  2 lists examples of ViT applications in image \nclassification.\nApplications of ViT in medical image classification\nClassification is an important digital health solution in \nmedical imaging analysis that helps medical practitioners \nidentify objects within a complex structure to immedi -\nately categorize medical cases. Utilizing AI while working \nin remote areas and using telehealth systems with limited \nmedical resources ensures the accuracy of final clinical \ndecisions. The importance of AI emerged during the pan-\ndemic when the pressure on healthcare systems exceeded \nthe capacity of the healthcare infrastructure. The ViT has \ndifferent applications in medical imaging classification.\nTransMed [53] uses a combination of the ViT and a \nCNN to classify multimodal data for medical analysis. \nThe classification system includes disease and lesion \nidentification. Figure 10 shows an example of the applica-\ntion of TransMed in image classification.\nShoulder implant manufacturers [54] use a transformer \nin orthopedic applications to assist in shoulder replace -\nment surgery with artificial implants and joints. Before \nsurgery, shoulder X-ray images were used to detect \nand classify the shoulder implant manufacturer ven -\ndor to determine the required accessories. The GasHis-\ntransformer [55] is a multiscale visual transformer for \ndetecting and classifying gastric cancer images using his -\ntopathological images of hematoxylin and eosin obtained \nby a microscope. Table  3 lists examples of ViT applica -\ntions in image classification.\nA comparative analysis of cervical cancer classifications \nusing various deep learning (DL) algorithms, including \nthe ViT, was conducted using cytopathological images \n[56]. A transformer-based model was used in brain \nmetastases classification [57] from an MRI of the brain. \nBrain metastases are among the main causes of malignant \ntumors in the central nervous system [61]. ScoreNet [58] \nTable 2 Examples of ViT applications in medical image detection\nMethod Category Medical application\nDETR [48] MRI Lymphoproliferative diseases detection\nCOTR [49] Colonoscopy CRC detection\nSATr [50] CT Universal lesion detection\nUCLT [51] CT Lung nodule detection\nIHD [52] CT Brain injury hemorrhage detection\nFig. 10 Example of using the ViT for tumor classification in MRI images using TransMed [53]. The tumor is enclosed by the dashed circle indicated \nby the yellow arrow\nPage 11 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \nis a transformer-based model that classifies breast can -\ncer using histopathology images. RadioTransformer [59] \nclassifies COVID-19 cases based on chest X-rays. Trac -\ntoFormer [60] classifies brain images based on tractogra -\nphy, which is a 3D model of the brain nerve tracts using \ndiffusion MRI. TractoFormer discriminates between 3D \nfiber spatial relationships. It has proven to be accurate in \nclassifying patients with schizophrenia vs controls.\nApplications of ViT in medical imaging prognosis \npredication\nThe ability of the ViT to analyze time-series sequence \ndata and obtain insights from previous data allows the \nprediction of future behaviors or patterns. In medical \nimaging, it is important to help healthcare practition -\ners predict the effects of diseases or cancers to treat \nthem before they spread. Figure  11 shows the use of the \ntransformer for surgical instructions, which are also \nimplemented in Surgical Instruction Generation Trans -\nformer (SIGT) algorithm for surgical robots [62]. The \nalgorithm used the ViT to analyze the visual scene dur -\ning surgery and update the reinforcement learning (RL), \nreward, and status to predict the instructions for the \nrobot.\nThe Sig-Former [63] can predict surgical instruc -\ntions during an operation using the transformer atten -\ntion mechanism to analyze the input image. The dataset \nincludes images acquired during surgeries such as lapa -\nroscopic sleeve gastrectomy and laparoscopic ventral \nhernia repair.\nThe 3D Shuffle Mixer [64] analyzes 3D volumetric \nimages from CT and MRI using context-aware dense \npredictions for different diseases, such as hemorrhagic \nstroke, abdominal CT images, and brain tumors.\nTable 3 Examples of ViT applications in medical image classification\nMethod Category Medical application\nTransMed [53] MRI Multi‑modal classification: disease classification, lesion identification\nShoulder implant manufacture [54] X‑ray Orthopedics: Shoulder implant manufacture classification\nGasHis‑transformer [55] Histopathology microscopic images Gastric cancer classification and detection\nMulti‑scale\ncytopathology [56]\nCytopathological images Cervical cancer classification\nBrain metastases classification [57] MRI Classification of the brain tumor of central nervous system\nScoreNet [58] Histology\nDatasets of haematoxylin\n + eosin\nBreast cancer classification\nRadioTransformer [59] X‑ray COVID‑19 classification using chest X‑ray images\nTractoFormer [60] Diffusion MRI Nerve tracts modelling and 3D fiber representation\nTransformer Prediction:\nRetract peritoneum incise with \nscissors and electrocautery.\nGround Truth: \nWhile retracting peritoneum, \nidentify correct plane by thin \nareolar.\nTransformer Prediction:\nContinue tying.\nGround Truth: \nTie knots in each suture with tails.\nTransformer Prediction:\nProcess completed at first \nhorizontal axis position.\nGround Truth: \nRepeat process at horizontal \naxis positions.\nFig. 11 Examples of using ViT for surgical instruction prediction. Transformer prediction is based on the SIGT method [62]. GT is used as a reference \nfor comparison and validation\nPage 12 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nGraph-based transformer models [65] predict genetic \nalteration. Ultrasound recordings are used for fetal \nweight prediction by the residual transformer model \n[66]. CLIMAT [67] forecasts the trajectory of knee oste -\noarthritis based on X-ray images from specialized radi -\nologists. Table  4 lists examples of ViT applications in \nmedical image prediction.\nApplications of ViT in image reconstruction and synthesis\nAfter acquiring data from medical imaging modalities \nsuch as MRI, CT, and digital X-ray, the images are stored \nas raw data in an unstructured format. To make this raw \ndata readable, a reconstruction process is applied to \nretrieve images without any loss. However, this process \nis computationally expensive because of the size and \ncomplexity of reconstruction algorithms. The use of DL \nsignificantly improves the reconstruction performance \nby enhancing the preservation of fine image details \nwithin a reconstruction time of a few seconds. In con -\ntrast, traditional techniques such as image reconstruc -\ntion using compressed sensing require more time [68]. \nReconstructing magnetic resonance images is a chal -\nlenge because of the size, complexity, and sparsity of the \nK-space matrix, in which the raw images are stored in the \nfrequency domain.\nSLATER [69] is a zero-shot adversarial transformer \nthat performs the unsupervised reconstruction MRI \nimages. SLATER maps the noise and latent represen -\ntation to the MR coil-combined images. To maximize \nthe consistency of the images, the operator input and \nmaximum optimized prior information were combined \nusing a zero-shot reconstruction algorithm. Figure  12 \nshows different methods for reconstructing fast MRI \nand the reconstruction error map using SLATER (ViT-\nbased method) from T 1 weighted images. These were \nthen compared with other techniques based on non-ViT \nmethods.\nThe Task Transformer (T 2Net) [77] proposed an \narchitecture to simultaneously reconstruct and \nenhance images using a super-resolution method \nfor MRI. The T  2Net process can be divided into two \nparts. First, two CNN subtasks were used to extract \nTable 4 Examples of ViT applications in medical image prediction\nMethod Category Medical application\n3D‑SMx [64] 3D (MRI, CT) Context‑aware dense prediction for different diseases that \nincludes hemorrhagic stroke, abdominal CT images, brain \ntumor\nGBT [65] Cancer genome (TCGA) Computation pathology: genetic alteration\nRTM [66] Ultrasound Fetal weigh at birth prediction\nCLIMAT [67] X‑ray Forecasts knee osteoarthritis trajectory\nSig‑Former [63] Laparoscopy Surgical instructions prediction\nSIGT [62] Robot camera Surgical instruction prediction and image captioning\nFig. 12 Top: Different reconstruction methods from T1 weighted acquisition of the fast MRI using different methods. ZF is a traditional Fourier \nmethod [70]. LORKAS [71, 72],  GANsub [73], SSDU [74],  GANprior [75], and SAGAN [76] are generative adversarial network (GAN) reconstruction‑based \nmethods. SLATER is a ViT‑based method [69]. Bottom: Reconstruction error map [69]\nPage 13 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \ndomain-specific features. Second, T  2Net was embed -\nded and the relationship between the two subtasks \nwas synthesized. ReconFormer addresses the problem \nof under sampled K-space data by utilizing recurrent \npyramid transformer layers to rapidly and efficiently \nretrieve the data [78]. Transformer-based methods for \nfast MRI reconstruction were evaluated in ref. [79]. The \nresults showed that the combination of GANs and ViT \nachieved the best performance, i.e., a 30% improvement \nover standard methods such as the Swin transformer. \nTable  5 lists examples of ViT applications in image \nreconstruction.\nA ViT-based (stereo transformer) was utilized in \nefficient dynamic surgical scene reconstruction [80] \nto reconstruct a robotic surgery scene acquired by \nan endoscope. This application is essential for surgi -\ncal education, robotic guidance, and context-aware \nrepresentation.\nDuTrans adopted a Swin transformer as the core of \ntheir architectural design to reconstruct the sinograms of \nCT scans from the attenuation coefficient of the Houns -\nfield unit [81, 83]. The accurate reconstruction of CT \nscans is essential to obtain high-quality images, reduce \nradiation doses, and distinguish fine details to facilitate \nthe early detection of cancers.\nMIST-net proposed a multidomain transformer model \nto reconstruct CT scans [82]. MIST-net can reduce \nradiation doses without compromising image quality. \nMIST-net incorporates the Swin transformer architec -\nture, residual features, and an edge enhancement filter to \nreconstruct the desired CT image.\nApplications of ViT in telehealth\nThere is an increasing need for efficient techniques to \nprocess all medical information within the healthcare \necosystem. This is because of the complex nature of the \nunstructured format of medical data, such as images, \nclinical reports, and laboratory results. The ViT provides \na comprehensive solution as it can process medical data \nin different formats and automatically generate reports or \ninstructions. Figure  13 shows the main components of a \ntelehealth ecosystem: the data source, ingestion, machine \nlearning, and data analysis.\nThe hospital information system (HIS) and radiology \ninformation system register the patient and store data \nin electronic health records (EHRs) and picture archiv -\ning and communication systems (PACS) to be shared \nwithin the telehealth ecosystem. The HIS relies on \nstandards such as Health Level 7 and Fast Healthcare \nInteroperability Resources for the exchange of patient \nmetadata or EHRs [84, 85]. PACS is used to store and \ntransfer medical images, mainly in the Digital Imag -\ning [86] and Communications in Medicine [87] format, \nwhich are available to medical staff for further clinical \nanalysis.\nPatient data are shared in a cloud or server, either in \nreal-time streaming or in batches from a data storage \nwarehouse or data lake. The ViT or any other machine \nlearning model is used to train the system on the ingested \ndata. Once the model has been deployed, the ViT can \nbe used to analyze medical data, approximately 90% of \nwhich are in an image format. Once the data have been \nanalyzed, the results are sent to update patient records in \nthe EHR or other storage systems.\nApplications of ViT in report generation\nThe ViT provides a unified solution that processes text \nalong with unstructured data, such as images. The advan-\ntage of using the ViT is that it can process and generate \nradiology reports, surgical instructions, and other clinical \nreports in a global context by retrieving huge amounts of \ninformation stored in health information systems.\nFigure 14 shows the image capture, report consistency, \ncompleteness, and report generation by the Real Time \nMeasurement, Instrumentation & Control (RTMIC) \n[88] and International Federation of Clinical Chemistry \n(IFCC) algorithms [89] from an input of medical images. \nThe RTMIC is a ViT-based algorithm used for medi -\ncal image captioning [88]. The GT is a manual reference \nwritten by an expert. Att2in is an attention-based method \nused for comparison [90]. The quality standards for \nhealth information systems state that the transferred data \nTable 5 Examples of ViT applications in medical image reconstruction\nMethod Category Medical application\nSLATER [69] MRI MRI unsupervised reconstruction\nT 2Net [77] MRI Image reconstruction and super‑\nresolution enhancement\nReconFormer[78], FastMRIRecon [79] MRI Accelerated MRI reconstruction\nE‑DSSR [80] Endoscopy Surgical robot scene reconstruction\nDuTrans [81], MIST‑net [82] CT CT sinograms reconstruction\nPage 14 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nshould be consistent and complete. The IFCC algorithm \n[89] improves the factual completeness and consistency \nin image-to-text radiology report generation. The algo -\nrithm uses a combination of transformers to extract fea -\ntures and RL to optimize the results.\nThe transformer efficiently addresses the challenges of \nhandling biased medical data and long and inconsistent \nparagraphs. The AlignTransformer can produce a long \ndescriptive and coherent paragraph based on the analysis \nof medical images [91]. It mainly operates in two stages. \nFirst, it aligns the medical tags with the related medi -\ncal images to extract the features. Second, the extracted \nfeatures are used to generate a long report based on the \ntraining data for each medical tag.\nThe transformer is also used to generate surgical  \nreports during robot-assisted surgery by learning  \ndomain adaptation in the Learning Domain Adaption  \nSurgical Robot (LDASR) [92]. The LDASR uses a \nFig. 13 Schematic of the components of the ViT in a telehealth ecosystem\nPage 15 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \ntransformer to learn the relationships between the \ndesired region of interest, surgical instruments, and \nimages to generate image captions and reports during \nsurgery. Table  6 lists examples of ViT applications in \nimage generation.\nApplications of ViT in telehealth security\nTelehealth security is receiving significant attention \nfrom healthcare providers owing to the emerging \nrisks associated with leveraging advanced technolo -\ngies such as machine learning. In healthcare, there is a \nserious risk of misdiagnosing a patient with the wrong \ndisease or even diagnosing a healthy person with a \ndisease.\nAn adversarial attack refers to a malicious attack \nagainst the machine learning algorithm or data vulner -\nability. These attacks may include modifying the data or \nalgorithm code, resulting in incorrect outputs [93, 94]. \nThe accuracy of the algorithm may also be affected by \nthe manipulation of the code or labeled data. Cyber -\ncriminals attempt to extort money from healthcare \nproviders by threatening to publish patient information \nand encrypt the database. Figure  15 shows the effects \nof data poisoning by adversarial attacks on medi -\ncal images that attempt to disrupt the behavior of the \ntrained machine learning model.\nResearchers have developed the following counter -\nmeasures against cybercrime:\nFig. 14 Examples of report generation from the input image using the ViT. a Sample of results by the IFCC algorithm [89] for report completeness \nand consistency; b Example of report generation results by the RTMIC algorithm [88]\nTable 6 Examples of ViT applications in medical report generation\nMethod Category Medical application\nRTMIC [88] Medical images general Report generation from medical images (e.g., MRI, \nCT, PET and X‑ray)\nIFCC [89] Medical images general Medical report completeness and consistency\nAlignTransformer [91] Medical images general Long report generation from medical images tags\nLDASR [92] Surgical robot camera Surgical report generation\nPage 16 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n1. Implement a context-aware system to ensure that the \ncode is safe and not jeopardized.\n2. Store data in an encrypted cloud environment and \nensure that these are backed up.\n3. Federated learning is another measure that uses a \ndistributed computing engine to process data in geo -\ngraphically distributed environments that maintain \ndata in different locations, making them difficult to \nhack.\n4. Embrace a zero-trust policy when managing access \ncontrol systems in digital health applications. This \nprovides an additional authentication measure by \nconsidering different attributes before granting \naccess instead of just relying on a role-based access \nsystem.\nUnlike the ViT, traditional CNN-based algorithms are \nnot robust against adversarial attacks because of the sim -\nplicity of their architecture [95]. The complexity of the \nViT algorithm and its ability to extract features in a global \ncontext are solid grounds for detecting irregularities in \ndata entry. The ViT has been used for data encryption \n[96], anomaly detection [97], network intrusion system \ndetection [98], anti-spoofing [99], and patch processing \n[100]. Table  7 lists examples of the applications use of \nViT in information system security.\nRoadmap for implementing ViT\nFigure 16 shows the four stages in the end-to-end imple -\nmentation of the ViT model pipeline. These are problem \nformulation, data processing; model implementation, \ntraining, and validation; and model deployment and \nquality assurance, respectively.\nProblem formulation\n Before implementing a machine learning model, the \nproblem must be understood and formulated to fit the \ncontext of the desired product-use case.\nData preparation\nOnce the problem is understood, high-quality data must \nbe prepared for the AI algorithm. The data must be rel -\nevant, accurate, statistically balanced, and sufficient for \nFig. 15 Illustration of data poisoning by an adversarial attack that fools learning‑based models trained on medical image datasets\nTable 7 Examples of ViT applications in security\nMethod Application\nJigsaw block‑based encryption [96] Data encryption\nMFVT [97] Anomaly detection\nImage conversion from network data‑flow [98] Network intrusion \nsystem detection\nZero‑shot face [99] Anti‑spoofing\nBackdoor defender [100] Patch processing\nPage 17 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \ntraining. The data should also be verified by different \nqualitative and qualitative measures to ensure their valid-\nity. This helps stabilize the model during training and \nspeeds up convergence to obtain the optimal solution.\nModel and code implementation\nThere is no master algorithm that fits everything; each \nhas its own advantages and disadvantages. The suitable \nViT model or architecture is selected based on the avail -\nable data and application to achieve the desired success \nmetrics. The model hyperparameters are fine-tuned dur -\ning the training stage to achieve the desired accuracy and \nprevent overfitting or underfitting. The model should \nalso be validated and tested on datasets other than those \nused for training.\nModel deployment and testing\nFinally, once the model passes all the end-to-end testing \nand verification processes, it should be ready for deploy -\nment. Different environments can be used to deploy the \nfinal product in different cloud or on-premise applica -\ntions. The recommended environment is a cloud-based \nsystem because it can automatically generate a model \non a scale that fits the computational resources for dif -\nferent applications. The deployed model should undergo \ndifferent quality assurance and monitoring processes \nto ensure that the target performance of the system is \nmet during tests outside the laboratory or development \nenvironment. Any bugs found in the code should be \nfixed. If the performance of the trained model is insuf -\nficient, then a new dataset should be used for training.\nLimitations and challenges of ViT in digital health\nTransformer-based algorithms are emerging as the \nstate-of-art in vision tasks to replace traditional stan -\ndalone CNN architectures. However, transformer-\nbased models have disadvantages in terms of technical \nor regulatory compliance requirements. These include \ndata size and labeling, the need for a hybrid model, \ndata bias and model fairness, and ethical and privacy \nchallenges.\nDataset size and labeling challenges\nSimilar to other attention-based mechanisms, trans -\nformers inherently require a huge amount of data to \ntrain the model. The transformer achieved the best \nperformance compared with the well-known ResNet \narchitecture when trained on the JFT dataset [101], \nwhich contains 300 million images and 18000 classes. \nHowever, when trained on the ImageNet-21  k dataset \n[102], which contains approximately 14 million images \nand 21000 classes, the transformer performance did not \nsurpass that of the ResNet architecture trained on the \nsame dataset ImageNet-1 k [103, 104] with 1.28 million \nimages and 1000 classes. Figure  17 shows the perfor -\nmance of the ViT and ResNet architectures with respect \nto the data size.\nThe results show that ResNet performed better \nwhen the dataset was small. ResNet and ViT exhib -\nited almost the same performance when the trained \non approximately 100 million samples. However, the \nViT achieved superior performance compared with \nFig. 16 Roadmap for ViT implementation\nPage 18 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nResNet when the dataset size was larger than 100 mil -\nlion images [1 ].\nThe limited dataset size is challenging in medical \napplications because it is difficult to obtain a clean and \nhigh-quality dataset that is feasible for clinical applica -\ntion standards. Moreover, finding qualified specialists to \nannotate millions of images is difficult, expensive, and \ntime-consuming.\nTransfer learning, data augmentation, adversarial \nimaging synthesis, and automatic data labeling are \namong the best practices to deal with the problem of \ninsufficient dataset size. The researchers in ref. [105] \nsuggested that the ViT model outperformed ResNet \nwhen trained from scratch on the large ImageNet \ndataset without using data augmentation or a large \npretrained model. Thus, there is a tradeoff between \ndataset size limitations and performance because \nhaving a large dataset but sufficient computational \nresources for training remains a challenge. The use of \ncloud-based data training could be a solution to lim -\nited resources. However, this is an expensive option \nfor academia and more suitable for industrial appli -\ncations. Similarly, ref. [106] proposed an effective \nweight initialization scheme to fine-tune the ViT using \nself-supervised inductive biases learned directly from \nsmall-scale datasets. This reduced the need for huge \ndatasets for training, and hence required less compu -\ntational resources.\nCL is beneficial in medical image applications because \nit can minimize the difference between similar object \nrepresentations in the latent space, while maximizing the \ndifference between dissimilar objects [107]. CL has been \nused with ViT in medical histopathology to classify large \nimages (in gigapixels) and obtain inferences to distinguish \nbetween multilabel cancer cells for classification [108].\nThe need for hybrid model with transformer\nThe transformer was initially designed to process lan -\nguage models in a sequential format. Since then, it has \nbeen modified to process vision tasks by splitting the \nimage into small patches and processing them sequen -\ntially as a text-like model. The transformer can obtain \ninferences about the information in a global context to \ncapture a wide range of dependencies between objects; \nhowever, it has a limited feature localization capacity. \nWhile the standalone transformer model is sufficient \nfor most classification tasks, in the case of image seg -\nmentation for critical medical applications that require \na high-quality image, the transformer performance \nis insufficient and must be combined with a hybrid \nmodel.\nUnet or ResNet architectures are widely used as \nstandard models for medical image segmentation that \ncan preserve image details owing to the nature of the \nencoder-decoder architecture with residual connections. \nHowever, Unet and ResNet have inherited the limitation \nof CNNs in failing to capture a wide range of dependen -\ncies by having only local feature extraction capabilities. \nTransUNet was the first architecture proposed for medi -\ncal imaging segmentation that combined the transformer \nPre-training data size in (Millions)\n9\n30\n90\n300\n02 04 06 08 0\nViT-L/16\nViT-L/32\nViT-B/32\nViT-b/32\nResNet50x1 (BiT)\nResNet152x2 (BiT)\nTop1% accuracy\nFig. 17 Comparison between ViT and ResNet (BiT) architecture accuracies on different sizes of training data. The y‑axis is the size of pretraining data \nin the ImageNet dataset. The x‑axis is the accuracy selected from the top 1% of the selected five‑shots of ImageNet. Results according to the study \nin ref. [1]\nPage 19 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \n[10] and Unet architectures for local and global feature \nextraction.\nThe transformer was also combined with RL to gener -\nate instructions for surgical robots [62, 63]. The trans -\nformer can capture features to update the state-reward \nstatus in the RL to automate robot tasks. The RL-trans -\nformer combination has also been used in medical \nimage captioning [88] to automatically generate medical \nreports within the hospital system.\nData bias and fairness\nTraining machine learning models using huge datasets \n(in millions or billions of examples) requires resources \nwith sufficient computational power and storage. There -\nfore, many algorithms tend to apply dimensionality \nreduction to minimize model parameters, which reduces \nthe extracted features. This allows model training with \nreduced computational and memory requirements. \nHowever, there is a possibility of losing information \nwith less representation in the feature map or dataset. \nConsequently, the model may be biased toward labels \nor classes with the largest amount of training data. The \nbias in the results could be significant, particularly when \nlabel balancing was not performed before training. In \nmedical applications, rare diseases and outliers could be \ndisregarded from the model prediction.\nIn ref. [109], the fairness and interpretability of DL \nmodels were evaluated using the largest publicly avail -\nable dataset, the Medical Information Mart for Inten -\nsive Care, version IV. The study found that some DL \nmodels lacked fairness when relying on demographics \nand ethnicity to predict mortality rates. In contrast, \nDL models that used proper and balanced critical fea -\ntures for training were not biased and tended to be fair. \nIn many models, racial attributes were used unequally \nacross subgroups. This resulted in inconsistent recom -\nmendations on the use of mechanical ventilators for \ntreatments or in intensive care units when relying on \ndemographic and racial categories such as gender, mari -\ntal status, age, insurance type, and ethnicity. Figure  22 \nin Appendix  A.5 shows examples of the global features \nimportance scores used to predict mortality rates using \ndifferent machine learning methods. The figure shows \nthe bias of the importance score toward certain features \nwhen machine learning algorithms were changed.\nEthical and privacy challenges\nInformation-sharing in healthcare information systems \nis regulated, although privacy and ethical regulations \nmay differ across jurisdictions. For example, the Health \nInsurance Portability and Accountability Act (HIPAA) \nof the United States regulates healthcare information \nsystems to protect sensitive patient information. The \nHIPAA states that such information cannot be dis -\nclosed without patient consent. Patients also have the \nright to access their data, ask for modifications, and \nknow who accesses them. While such regulations help \npreserve patient privacy, collecting health-related data -\nsets or making them available to the public is a chal -\nlenge. This is a critical issue in the case of the ViT as \nmillions of examples are required to train the model \nand obtain accurate results. Using the ViT or any other \nmachine learning model trained on a large dataset \nhas a higher risk of errors, and the results are subject \nto ethical concerns. Many large datasets are obtained \nfrom the Internet; hence, the sources may be unknown \nor untrustworthy, and there is no previous consent to \ncollect these data. Training the ViT from untrusted \nsources could generate false results, which could lead \nto errors or offensive content in generated patient \nreports. The consequences may be worse in the case of \ndata breaches or cyberattacks on the healthcare infor -\nmation system as these could alter patient records, \nimages, or the data streaming performance of the tele -\nhealth system. Although the ViT is more robust against \nadversarial attacks, there is no guarantee that the ViT-\nbased model will not generate inappropriate content. \nThis raises concerns regarding the need to regulate the \ncurrent AI industry as well as applications in health -\ncare to ensure that the input and output of the systems \nare clean and valid for clinical applications. Federated \nlearning from different healthcare facilities and edge \ndevices or servers can help maintain a high level of data \nprivacy. However, the research in ref. [110] reported \nvulnerabilities in retrieving original data from the \nshared model weights.\nConclusions\nThe ViT has emerged as the state-of-the-art in image \nrecognition tasks, replacing traditional standalone  \nmachine learning algorithms such as CNN-based models.  \nThe ViT can extract information in a global context using \nan attention-based mechanism and analyze images, texts, \npatterns, and instructions.\nThe superior performance of the ViT makes it prac -\ntical for various digital medicine applications such as \nsegmentation, classification, image reconstruction, image \nenhancement, data prognosis prediction, and telehealth \nsecurity.\nPage 20 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nAppendix\nViT common architectures\nA.1 Typical transformer architecture\nFig. 18 Transformer typical architecture [8]\n\nPage 21 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \nA.2 Architecture example of using transformer in image recognition\nFig. 19 Example of using Transformer architecture for image recognition [1]\nPage 22 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nA.3 TransUnet architecture diagram\nFig. 20 a Transformer layer diagram; b TransUnet architecture [10]\nPage 23 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \nA.4 Swin‑transUnet architecture diagram\nFig. 21 Swin TransUnet architecture [11]\n\nPage 24 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nA.5 Example of global features importance rank (Fig. 22)\nFig. 22 Examples of global features that are used for mortality predictions are numbered from (112‑139). The numbers in the table depicts the rank \nsore and each column represents a feature and its importance score by different methods on the horizontal line [109]. AutoInt [111], LSTM [112], TCN \n[113], Transformer [8], IMVLSTM [114] are the machine learning methodologies\nAbbreviations\n3D  Three ‑dimensional\nAI  Artificial intelligence\nBERT  Bidirectional encoder representations from transformers\nCNN  Convolutional neural networks\nCOVID‑19  Coronavirus disease 2019\nCT  Computed tomography\nCL  Contrastive learning\nCRC   Colorectal cancer\nCOTR  Convolutional transformer\nDL  Deep learning\nEHR  Electronic health record\nFCN  Fully convolutional network\nGT  Ground truth\nGAN  Generative adversarial network\nHIS  Hospital information system\nHIPAA  Health Insurance Portability and Accountability Act\nK  Key\nLSTM  Long short‑term memory\nMLP  Multilayer perceptron\nMSA  Multihead self‑attention\nMRI  Magnetic resonance imaging\nNLP  Natural language processing\nPACS  Picture archiving and communication system\nQ  Query\nRL  Reinforcement learning\nRNN  Recurrent neural network\nV  Value\nViT  Vision transformer\nDETR  Detection transformer\nSATr  Slice attention transformer\nUCLT  Unsupervised contrastive learning‑based transformer\nIHD  Intracranial hemorrhage detection\nSIGT  Surgical Instruction Generation Transformer\nRTMIC  Real Time Measurement, Instrumentation & Control\nIFCC  International Federation of Clinical Chemistry\nLDASR  Learning Domain Adaption Surgical Robot\nAcknowledgements\nNot applicable.\nAuthors’ contributions\nKA‑h, FG and AK provided the conception; KA‑h provided the methodology \nand investigation; KA‑h and AK made the data analysis; KH prepared the origi‑\nnal draft; FG, ITC, AK and KA‑h reviewed and edited the manuscript; FG, AK and \nITC provided the supervision; FG provided the funding acquisition. All authors \nhave read and agreed to the published version of the manuscript.\nFunding\nThis research was supported by a grant from the National Research Council of \nCanada through the Collaborative Research and Development Initiative.\nAvailability of data and materials\nThe data underlying this manuscript is based on existing publications and is \navailable in the referenced literature or from the corresponding authors upon \nreasonable request.\nDeclarations\nCompeting interests\nThe authors declare no competing financial or non‑financial interests.\nReceived: 21 March 2023   Accepted: 30 May 2023\n\nPage 25 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \nReferences\n 1. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai XH, Unterthiner \nT et al (2021) An image is worth 16x16 words: transformers for image \nrecognition at scale. In: Proceedings of the 9th international conference \non learning representations, OpenReview.net, Vienna, 3‑7 May 2021\n 2. Zhang QM, Xu YF, Zhang J, Tao DC (2023) ViTAEv2: vision transformer \nadvanced by exploring inductive bias for image recognition and \nbeyond. Int J Comput Vis 131(5):1141‑1162. https:// doi. org/ 10. 1007/ \ns11263‑ 022‑ 01739‑w\n 3. Han K, Wang YH, Chen HT, Chen XH, Guo JY, Liu ZH et al (2023) A survey \non vision transformer. IEEE Trans Pattern Anal Mach Intell 45(1):87‑\n110.  https:// doi. org/ 10. 1109/ TPAMI. 2022. 31522 47\n 4. Wang RS, Lei T, Cui RX, Zhang BT, Meng HY, Nandi AK (2022) Medical \nimage segmentation using deep learning: a survey. IET Image Process \n16(5):1243‑1267. https:// doi. org/ 10. 1049/ ipr2. 12419\n 5. Bai WJ, Suzuki H, Qin C, Tarroni G, Oktay O, Matthews PM et al (2018) \nRecurrent neural networks for aortic image sequence segmenta‑\ntion with sparse annotations. In: Frangi AF, Schnabel JA, Davatzikos C, \nAlberola‑López C, Fichtinger G (eds) Medical image computing and \ncomputer assisted intervention. 21st international conference, Granada, \nSeptember 2018. Lecture notes in computer science (Image processing, \ncomputer vision, pattern recognition, and graphics), vol 11073. Springer, \nCham, pp 586‑594. https:// doi. org/ 10. 1007/ 978‑3‑ 030‑ 00937‑3_ 67\n 6. Wang YX, Xie HT, Fang SC, Xing MT, Wang J, Zhu SG et al (2022) PETR: \nrethinking the capability of transformer‑based language model in \nscene text recognition. IEEE Trans Image Process 31:5585‑5598.  https:// \ndoi. org/ 10. 1109/ TIP . 2022. 31979 81\n 7. Devlin J, Chang MW, Lee K, Toutanova K (2019) BERT: pre‑training of \ndeep bidirectional transformers for language understanding. In: Pro‑\nceedings of the 2019 conference of the North American chapter of the \nassociation for computational linguistics: human language technolo‑\ngies, volume 1 (long and short papers), Association for Computational \nLinguistics, Minneapolis, 2‑7 June 2019\n 8. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN et al \n(2017) Attention is all you need. In: Proceedings of the 31st interna‑\ntional conference on neural information processing systems, Curran \nAssociates Inc., Long Beach, 4‑9 December 2017\n 9. Gao Y, Phillips JM, Zheng Y, Min RQ, Fletcher PT, Gerig G (2018) Fully \nconvolutional structured LSTM networks for joint 4D medical image \nsegmentation. In: Proceedings of the 15th international symposium on \nbiomedical imaging, IEEE, Washington, 4‑7 April 2018. https:// doi. org/ \n10. 1109/ ISBI. 2018. 83637 64\n 10. Chen JN, Lu YY, Yu QH, Luo XD, Adeli E, Wang Y et al (2021) TransUNet: \ntransformers make strong encoders for medical image segmentation. \narXiv preprint arXiv: 2102.04306\n 11. Lin AL, Chen BZ, Xu JY, Zhang Z, Lu GM, Zhang D (2022) DS‑TransUNet: \ndual Swin transformer U‑Net for medical image segmentation. IEEE \nTrans Instrum Meas 71:4005615. https:// doi. org/ 10. 1109/ TIM. 2022. \n31789 91\n 12. Mikolov T, Chen K, Corrado G, Dean J (2013) Efficient estimation of word \nrepresentations in vector space. In: Proceedings of the 1st international \nconference on learning representations, ICLR, Scottsdale, 2‑4 May 2013\n 13. Maeda Y, Fukushima N, Matsuo H (2018) Taxonomy of vectorization \npatterns of programming for fir image filters using kernel subsampling \nand new one. Appl Sci 8(8):1235. https:// doi. org/ 10. 3390/ app80 81235\n 14. Jain P , Vijayanarasimhan S, Grauman K (2010) Hashing hyperplane \nqueries to near points with applications to large‑scale active learn‑\ning. In: Proceedings of the 23rd international conference on neural \ninformation processing systems, Curran Associates Inc., Vancouver, 6‑9 \nDecember 2010\n 15. Yu Y, Si XS, Hu CH, Zhang JX (2019) A review of recurrent neural \nnetworks: LSTM cells and network architectures. Neural Comput \n31(7):1235‑1270.  https:// doi. org/ 10. 1162/ neco_a_ 01199\n 16. Huang ZH, Xu W, Yu K (2015) Bidirectional LSTM‑CRF models for \nsequence tagging. arXiv preprint arXiv: 1508.01991\n 17. Gehring J, Auli M, Grangier D, Yarats D, Dauphin YN (2017) Convolu‑\ntional sequence to sequence learning. In: Proceedings of the 34th \ninternational conference on machine learning, PMLR, Sydney, 6‑11 \nAugust 2017\n 18. Takase S, Kiyono S, Kobayashi S, Suzuki J (2022) On layer normaliza‑\ntions and residual connections in transformers. arXiv preprint arXiv: \n2206.00330\n 19. Topal MO, Bas A, van Heerden I (2021) Exploring transformers in natural \nlanguage generation: GPT, BERT, and XLNet. arXiv preprint arXiv: \n2102.08036\n 20. Wang SL, Liu F, Liu B (2021) Escaping the gradient vanishing: periodic \nalternatives of softmax in attention mechanism. IEEE Access 9:168749‑\n168759.  https:// doi. org/ 10. 1109/ ACCESS. 2021. 31382 01\n 21. Ba JL, Kiros JR, Hinton GE (2016) Layer normalization. arXiv preprint \narXiv: 1607.06450\n 22. Taud H, Mas JF (2018) Multilayer perceptron (MLP). In: Camacho \nOlmedo M, Paegelow M, Mas JF, Escobar F (eds) Geomatic approaches \nfor modeling land change scenarios. Lecture notes in geoinformation \nand cartography. Springer, Cham, pp 451‑455. https:// doi. org/ 10. 1007/ \n978‑3‑ 319‑ 60801‑3_ 27\n 23. Akinyelu AA, Zaccagna F, Grist JT, Castelli M, Rundo L (2022) Brain tumor \ndiagnosis using machine learning, convolutional neural networks, cap‑\nsule neural networks and vision transformers, applied to MRI: a survey. J \nImaging 8(8):205.  https:// doi. org/ 10. 3390/ jimag ing80 80205\n 24. Mahoro E, Akhloufi MA (2022) Breast cancer classification on thermo‑\ngrams using deep CNN and transformers. Quant Infrared Thermogr \nJ.  https:// doi. org/ 10. 1080/ 17686 733. 2022. 21291 35\n 25. Shmatko A, Ghaffari Laleh N, Gerstung M, Kather JN (2022) Artificial \nintelligence in histopathology: enhancing cancer research and clini‑\ncal oncology. Nat Cancer 3(9):1026‑1038. https:// doi. org/ 10. 1038/ \ns43018‑ 022‑ 00436‑4\n 26. Al‑Hammuri K, Gebali F, Thirumarai Chelvan I, Kanan A (2022) Tongue \ncontour tracking and segmentation in lingual ultrasound for speech \nrecognition: a review. Diagnostics 12(11):2811. https:// doi. org/ 10. 3390/ \ndiagn ostic s1211 2811\n 27. Al‑Hammuri K (2019) Computer vision‑based tracking and feature \nextraction for lingual ultrasound. Dissertation, University of Victoria\n 28. McMaster C, Bird A, Liew DFL, Buchanan RR, Owen CE, Chapman WW \net al (2022) Artificial intelligence and deep learning for rheumatolo‑\ngists. Arthritis Rheumatol 74(12):1893‑1905. https:// doi. org/ 10. 1002/ art. \n42296\n 29. Beddiar DR, Oussalah M, Seppänen T (2023) Automatic captioning \nfor medical imaging (MIC): a rapid review of literature. Artif Intell Rev \n56(5):4019‑4076. https:// doi. org/ 10. 1007/ s10462‑ 022‑ 10270‑w\n 30. Renna F, Martins M, Neto A, Cunha A, Libânio D, Dinis‑Ribeiro M et al \n(2022) Artificial intelligence for upper gastrointestinal endoscopy: a \nroadmap from technology development to clinical practice. Diagnos‑\ntics 12(5):1278.  https:// doi. org/ 10. 3390/ diagn ostic s1205 1278\n 31. Coan LJ, Williams BM, Adithya VK, Upadhyaya S, Alkafri A, Czanner S \net al (2023) Automatic detection of glaucoma via fundus imaging and \nartificial intelligence: a review. Surv Ophthal 68(1):17‑41.  https:// doi. \norg/ 10. 1016/j. survo phthal. 2022. 08. 005\n 32. Chang A (2020) The role of artificial intelligence in digital health. In: \nWulfovich S, Meyers A (eds) Digital health entrepreneurship. Health \ninformatics. Springer, Cham, pp 71‑81. https:// doi. org/ 10. 1007/ \n978‑3‑ 030‑ 12719‑0_7\n 33. Shamshad F, Khan S, Zamir SW, Khan MH, Hayat M, Khan FS et al \n(2022) Transformers in medical imaging: a survey. arXiv preprint arXiv: \n2201.09873. https:// doi. org/ 10. 1016/j. media. 2023. 102802\n 34. Ronneberger O, Fischer P , Brox T (2015) U‑Net: convolutional networks \nfor biomedical image segmentation. In: Navab N, Hornegger J, Wells \nW, Frangi A (eds) Medical image computing and computer‑assisted \nintervention. 18th international conference, Munich, October 2015. \nLecture notes in computer science (Image processing, computer vision, \npattern recognition, and graphics), vol 9351. Springer, Cham, pp 234‑\n241. https:// doi. org/ 10. 1007/ 978‑3‑ 319‑ 24574‑4_ 28\n 35. Cao H, Wang YY, Chen J, Jiang DS, Zhang XP , Tian Q et al (2023) Swin‑\nUnet: unet‑like pure transformer for medical image segmentation. \nIn: Karlinsky L, Michaeli T, Nishino K (eds) Computer vision. Tel Aviv, \nOctober 2022. Lecture notes in computer science, vol 13803. Springer, \nCham, 205‑218. https:// doi. org/ 10. 1007/ 978‑3‑ 031‑ 25066‑8_9\n 36. Dong H, Yang G, Liu FD, Mo YH, Guo YK (2017) Automatic brain tumor \ndetection and segmentation using U‑Net based fully convolutional \nPage 26 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \nnetworks. In: Valdés Hernández M, González‑Castro V (eds) Medical image \nunderstanding and analysis. 21st annual conference, Edinburgh, July 2017. \nCommunications in computer and information science, vol 723. Springer, \nCham, pp 506‑517. https:// doi. org/ 10. 1007/ 978‑3‑ 319‑ 60964‑5_ 44\n 37. Liu Q, Xu ZL, Jiao YN, Niethammer M (2022) iSegFormer: interactive \nsegmentation via transformers with application to 3D knee MR images. \nIn: Wang LW, Dou Q, Fletcher PT, Speidel S, Li S (eds) Medical image \ncomputing and computer‑assisted intervention. 25th international \nconference, Singapore, September 2022. Lecture notes in computer sci‑\nence, vol 13435. Springer, Cham, pp 464‑474. https:// doi. org/ 10. 1007/ \n978‑3‑ 031‑ 16443‑9_ 45\n 38. Lee HH, Bao SX, Huo YK, Landman BA (2022) 3D UX‑Net: a large kernel \nvolumetric convnet modernizing hierarchical transformer for medical \nimage segmentation. arXiv preprint arXiv: 2209.15076\n 39. Yu X, Yang Q, Zhou YC, Cai LY, Gao RQ, Lee HH et al (2022) UNesT: \nlocal spatial representation learning with hierarchical transformer for \nefficient medical segmentation. arXiv preprint arXiv: 2209.14378\n 40. Xing ZH, Yu LQ, Wan L, Han T, Zhu L (2022) NestedFormer: nested \nmodality‑aware transformer for brain tumor segmentation. In: Wang \nLW, Dou Q, Fletcher PT, Speidel S, Li S (eds) Medical image computing \nand computer‑assisted intervention. 25th international conference, \nSingapore, September 2022. Lecture notes in computer science, vol \n13435. Springer, Cham, pp 140‑150. https:// doi. org/ 10. 1007/ 978‑3‑ 031‑ \n16443‑9_ 14\n 41. Tang YB, Zhang N, Wang YR, He SH, Han M, Xiao J et al (2022) Accurate \nand robust lesion RECIST diameter prediction and segmentation with \ntransformers. In: Wang LW, Dou Q, Fletcher PT, Speidel S, Li S (eds) \nMedical image computing and computer assisted intervention. 25th \ninternational conference, Singapore, September 2022. Lecture notes in \ncomputer science, vol 13434. Springer, Cham, pp 535‑544. https:// doi. \norg/ 10. 1007/ 978‑3‑ 031‑ 16440‑8_ 51\n 42. Li YX, Wang S, Wang J, Zeng GD, Liu WJ, Zhang QN et al (2021) GT U‑Net: \na U‑Net like group transformer network for tooth root segmentation. In: \nLian CF, Cao XH, Rekik I, Xu XN, Yan PK (eds) Machine learning in medical \nimaging. 12th international workshop, Strasbourg, September 2021. \nLecture notes in computer science (Image processing, computer vision, \npattern recognition, and graphics), vol 12966. Springer, Cham, pp 386‑\n395. https:// doi. org/ 10. 1007/ 978‑3‑ 030‑ 87589‑3_ 40\n 43. Sanderson E, Matuszewski BJ (2022) FCN‑transformer feature fusion for \npolyp segmentation. In: Yang G, Aviles‑Rivero A, Roberts M, Schönlieb \nCB (eds) Medical image understanding and analysis. 26th annual \nconference, Cambridge, July 2022. Lecture notes in computer science, \nvol 13413. Springer, Cham, pp 892‑907. https:// doi. org/ 10. 1007/ 978‑3‑ \n031‑ 12053‑4_ 65\n 44. Zhao ZX, Jin YM, Heng PA (2022) TraSeTR: track‑to‑segment transformer \nwith contrastive query for instance‑level instrument segmentation \nin robotic surgery. In: Proceedings of the 2022 international confer‑\nence on robotics and automation, IEEE, Philadelphia, 23‑27 May 2022. \nhttps:// doi. org/ 10. 1109/ ICRA4 6639. 2022. 98118 73\n 45. Codella N, Rotemberg V, Tschandl P , Celebi ME, Dusza S, Gutman D \net al (2019) Skin lesion analysis toward melanoma detection 2018: a \nchallenge hosted by the international skin imaging collaboration (ISIC). \narXiv preprint arXiv: 1902.03368\n 46. Valanarasu JMJ, Sindagi VA, Hacihaliloglu I, Patel VM (2020) KiU‑Net: \ntowards accurate segmentation of biomedical images using over‑com‑\nplete representations. In: Martel AL, Abolmaesumi P , Stoyanov D, Mateus \nD, Zuluaga MA, Zhou SK et al (eds) Medical image computing and \ncomputer‑assisted intervention. 23rd international conference, Lima, \nOctober 2020. Lecture notes in computer science (Image processing, \ncomputer vision, pattern recognition, and graphics), vol 12264. Springer, \nCham, pp 363‑373. https:// doi. org/ 10. 1007/ 978‑3‑ 030‑ 59719‑1_ 36\n 47. Caicedo JC, Goodman A, Karhohs KW, Cimini BA, Ackerman J, Haghighi \nM et al (2019) Nucleus segmentation across imaging experiments: the \n2018 data science bowl. Nat Methods 16(12):1247‑1253. https:// doi. \norg/ 10. 1038/ s41592‑ 019‑ 0612‑7\n 48. Mathai TS, Lee S, Elton DC, Shen TC, Peng YF, Lu ZY et al (2022) Lymph \nnode detection in T2 MRI with transformers. In: Proceedings of the SPIE \n12033, Medical imaging 2022: computer‑aided diagnosis, SPIE, San \nDiego, 20 February‑28 March 2022. https:// doi. org/ 10. 1117/ 12. 26132 73\n 49. Shen ZQ, Fu RD, Lin CN, Zheng SH (2021) COTR: convolution in trans‑\nformer network for end to end polyp detection. In: Proceedings of the \n7th international conference on computer and communications, IEEE, \nChengdu, 10‑13 December 2021. https:// doi. org/ 10. 1109/ ICCC5 4389. \n2021. 96742 67\n 50. Li H, Chen L, Han H, Zhou SK (2022) SATr: slice attention with trans‑\nformer for universal lesion detection. In: Wang LW, Dou Q, Fletcher PT, \nSpeidel S, Li S (eds) Medical image computing and computer assisted \nintervention. 25th international conference, Singapore, September \n2022. Lecture notes in computer science, vol 13433. Springer, Cham, pp \n163‑174. https:// doi. org/ 10. 1007/ 978‑3‑ 031‑ 16437‑8_ 16\n 51. Niu C, Wang G (2022) Unsupervised contrastive learning based trans‑\nformer for lung nodule detection. Phys Med Biol 67(20):204001. https:// \ndoi. org/ 10. 1088/ 1361‑ 6560/ ac92ba\n 52. Shang FX, Wang SQ, Wang XR, Yang YH (2022) An effective transformer‑\nbased solution for RSNA intracranial hemorrhage detection competi‑\ntion. arXiv preprint arXiv: 2205.07556\n 53. Dai Y, Gao YF, Liu FY (2021) TransMed: transformers advance multi‑\nmodal medical image classification. Diagnostics 11(8):1384.  https:// doi. \norg/ 10. 3390/ diagn ostic s1108 1384\n 54. Zhou M, Mo SL (2021) Shoulder implant X‑ray manufacturer classifica‑\ntion: exploring with vision transformer. arXiv preprint arXiv: 2104.07667\n 55. Chen HY, Li C, Wang G, Li XY, Rahaman M, Sun HZ et al (2022) GasHis‑\ntransformer: a multi‑scale visual transformer approach for gastric histo‑\npathological image detection. Pattern Recognit 130:108827.https:// doi. \norg/ 10. 1016/j. patcog. 2022. 108827\n 56. Liu WL, Li C, Rahaman MM, Jiang T, Sun HZ, Wu XC et al (2022) Is the \naspect ratio of cells important in deep learning? A robust comparison \nof deep learning methods for multi‑scale cytopathology cell image \nclassification: from convolutional neural networks to visual transform‑\ners. Comput Biol Med 141:105026.https:// doi. org/ 10. 1016/j. compb \niomed. 2021. 105026\n 57. Lyu Q, Namjoshi SV, McTyre E, Topaloglu U, Barcus R, Chan MD et al (2022) \nA transformer‑based deep‑learning approach for classifying brain metas‑\ntases into primary organ sites using clinical whole‑brain MRI images. \nPatterns 3(11):100613.https:// doi. org/ 10. 1016/j. patter. 2022. 100613\n 58. Stegmüller T, Bozorgtabar B, Spahr A, Thiran JP (2023) ScoreNet: \nlearning non‑uniform attention and augmentation for transformer ‑\nbased histopathological image classification. In: Proceedings of the \n2023 IEEE/CVF winter conference on applications of computer vision, \nIEEE, Waikoloa, 2‑7 January 2023. https:// doi. org/ 10. 1109/ WACV5 \n6688. 2023. 00611\n 59. Bhattacharya M, Jain S, Prasanna P (2022) RadioTransformer: a \ncascaded global‑focal transformer for visual attention‑ guided dis‑\nease classification. In: Avidan S, Brostow G, Cissé M, Farinella GM, \nHassner T (eds) Computer vision. 17th European conference, Tel \nAviv, October 2022. Lecture notes in computer science, vol 13681. \nSpringer, Cham, pp 679‑698. https:// doi. org/ 10. 1007/ 978‑3‑ 031‑ \n19803‑8_ 40\n 60. Zhang F, Xue TF, Cai WD, Rathi Y, Westin CF, O’Donnell LJ (2022) \nTractoFormer: a novel fiber ‑level whole brain tractography analysis \nframework using spectral embedding and vision transformers. In: \nWang LW, Dou Q, Fletcher PT, Speidel S, Li S (eds) Medical image \ncomputing and computer assisted intervention. 25th international \nconference, Singapore, September 2022. Lecture notes in computer \nscience, vol 13431. Springer, Cham, pp 196‑206. https:// doi. org/ 10.  \n1007/ 978‑3‑ 031‑ 16431‑6_ 19\n 61. Bertolini F, Spallanzani A, Fontana A, Depenni R, Luppi G (2015) Brain \nmetastases: an overview. CNS Oncol 4(1):37‑46. https:// doi. org/ 10.  \n2217/ cns. 14. 51\n 62. Zhang JL, Nie YY, Chang J, Zhang JJ (2021) Surgical instruction \ngeneration with transformers. In: de Bruijne M, Cattin PC, Cotin S, \nPadoy N, Speidel S, Zheng YF et al (eds) Medical image computing \nand computer assisted intervention. 24th international conference, \nStrasbourg, September 2021. Lecture notes in computer science \n(Image processing, computer vision, pattern recognition, and graph‑\nics), vol 12904. Springer, Cham, pp 290‑299. https:// doi. org/ 10. 1007/ \n978‑3‑ 030‑ 87202‑1_ 28\n 63. Zhang JL, Nie YY, Chang J, Zhang JJ (2022) SIG‑Former: monocular \nsurgical instruction generation with transformers. Int J Comput \nAssisted Radiol Surg 17(12):2203‑2210. https:// doi. org/ 10. 1007/ \ns11548‑ 022‑ 02718‑9\nPage 27 of 28\nAl‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n \n 64. Pang JY, Jiang C, Chen YH, Chang JB, Feng M, Wang RZ et al (2022) 3D \nshuffle‑mixer: an efficient context ‑aware vision learner of trans‑\nformer‑MLP paradigm for dense prediction in medical volume. IEEE \nTrans Med Imaging.  https:// doi. org/ 10. 1109/ TMI. 2022. 31919 74\n 65. Reisenbüchler D, Wagner SJ, Boxberg M, Peng TY (2022) Local atten‑\ntion graph‑based transformer for multi‑target genetic alteration \nprediction. In: Wang LW, Dou Q, Fletcher PT, Speidel S, Li S (eds) \nMedical image computing and computer assisted intervention. 25th \ninternational conference, Singapore, September 2022. Lecture notes \nin computer science, vol 13432. Springer, Cham, pp 377‑386. https://  \ndoi. org/ 10. 1007/ 978‑3‑ 031‑ 16434‑7_ 37\n 66. Płotka S, Grzeszczyk MK, Brawura‑Biskupski‑Samaha R, Gutaj P , Lipa \nM, Trzciński T et al (2022) BabyNet: residual transformer module \nfor birth weight prediction on fetal ultrasound video. In: Wang LW, \nDou Q, Fletcher PT, Speidel S, Li S (eds) Medical image computing \nand computer ‑assisted intervention. 25th international conference, \nSingapore, September 2022. Lecture notes in computer science, vol \n13434. Springer, Cham, pp 350‑359. https:// doi. org/ 10. 1007/ 978‑3‑ \n031‑ 16440‑8_ 34\n 67. Nguyen HH, Saarakkala S, Blaschko MB, Tiulpin A (2021) CLIMAT: \nclinically‑inspired multi‑agent transformers for knee osteoarthritis \ntrajectory forecasting. arXiv preprint arXiv: 2104.03642. https:// doi.  \norg/ 10. 1109/ ISBI5 2829. 2022. 97615 45\n 68. Xie YT, Li QZ (2022) A review of deep learning methods for com‑\npressed sensing image reconstruction and its medical applications. \nElectronics 11(4):586.  https:// doi. org/ 10. 3390/ elect ronic s1104 0586\n 69. Korkmaz Y, Dar SUH, Yurt M, Özbey M, Çukur T (2022) Unsupervised \nMRI reconstruction via zero ‑shot learned adversarial transformers. \nIEEE Trans Med Imaging 41(7):1747‑1763. https:// doi. org/ 10. 1109/ \nTMI. 2022. 31474 26\n 70. Huang W, Hand P , Heckel R, Voroninski V (2021) A provably con‑\nvergent scheme for compressive sensing under random genera‑\ntive priors. J Fourier Anal Appl 27(2):19. https:// doi. org/ 10. 1007/ \ns00041‑ 021‑ 09830‑5\n 71. Haldar JP , Zhuo JW (2016) P‑LORAKS: low‑rank modeling of local \nk‑space neighborhoods with parallel imaging data. Magn Reson Med \n75(4):1499‑1514. https:// doi. org/ 10. 1002/ mrm. 25717\n 72. Haldar JP (2015) Low‑rank modeling of local k‑space neighborhoods: \nfrom phase and support constraints to structured sparsity. In: Proceed‑\nings of the SPIE Optical Engineering + Applications, SPIE, San Diego, 2 \nSeptember 2015. https:// doi. org/ 10. 1117/ 12. 21867 05\n 73. Dar SUH, Yurt M, Shahdloo M, Ildız ME, Tınaz B, Çukur T (2020) Prior‑\nguided image reconstruction for accelerated multi‑contrast MRI \nvia generative adversarial networks. IEEE J Sel Top Signal Process \n14(6):1072‑1087.  https:// doi. org/ 10. 1109/ JSTSP . 2020. 30017 37\n 74. Yaman B, Hosseini SAH, Moeller S, Ellermann J, Uğurbil K, Akçakaya M \n(2020) Self‑supervised learning of physics‑guided reconstruction neural \nnetworks without fully sampled reference data. Magn Reson Med \n84(6):3172‑3191.  https:// doi. org/ 10. 1002/ mrm. 28378\n 75. Narnhofer D, Hammernik K, Knoll F, Pock T (2019) Inverse GANs for \naccelerated MRI reconstruction. In: Proceedings of the SPIE 11138, \nwavelets and sparsity XVIII, SPIE, San Diego, 11‑15 August 2019. https:// \ndoi. org/ 10. 1117/ 12. 25277 53\n 76. Karras T, Laine S, Aittala M, Hellsten J, Lehtinen J, Aila T (2020) Analyzing \nand improving the image quality of StyleGAN. In: Proceedings of the \nIEEE/CVF conference on computer vision and pattern recognition, IEEE, \nSeattle, 13‑19 June 2020. https:// doi. org/ 10. 1109/ CVPR4 2600. 2020. \n00813\n 77. Feng CM, Yan YL, Fu HZ, Chen L, Xu Y (2021) Task transformer network \nfor joint MRI reconstruction and super‑resolution. In: de Bruijne M, Cat‑\ntin PC, Cotin S, Padoy N, Speidel S, Zheng YF et al (eds) Medical image \ncomputing and computer‑assisted intervention. 24th international \nconference, Strasbourg, September 2021. Lecture notes in computer \nscience, (Image processing, computer vision, pattern recognition, and \ngraphics), vol. 12906. Springer, Cham, pp 307‑317. https:// doi. org/ 10. \n1007/ 978‑3‑ 030‑ 87231‑1_ 30\n 78. Guo PF, Mei YQ, Zhou JY, Jiang SS, Patel VM (2022) ReconFormer: accel‑\nerated MRI reconstruction using recurrent transformer. arXiv preprint \narXiv: 2201.09376\n 79. Huang JH, Wu YZ, Wu HJ, Yang G (2022) Fast MRI reconstruction: \nhow powerful transformers are? In: Proceedings of the 44th annual \ninternational conference of the IEEE engineering in medicine & biology \nsociety, IEEE, Glasgow, 11‑15 July 2022. https:// doi. org/ 10. 1109/ EMBC4 \n8229. 2022. 98714 75\n 80. Long YH, Li ZS, Yee CH, Ng CF, Taylor RH, Unberath M et al (2021) \nE‑DSSR: efficient dynamic surgical scene reconstruction with \ntransformer‑based stereoscopic depth perception. In: de Bruijne M, Cat‑\ntin PC, Cotin S, Padoy N, Speidel S, Zheng YF et al (eds) Medical image \ncomputing and computer assisted intervention. 24th international \nconference, Strasbourg, September, 2021. Lecture notes in computer \nscience, (Image processing, computer vision, pattern recognition, and \ngraphics), vol 12904. Springer, Cham, pp 415‑425. https:// doi. org/ 10. \n1007/ 978‑3‑ 030‑ 87202‑1_ 40\n 81. Wang C, Shang K, Zhang HM, Li Q, Hui Y, Zhou SK (2021) DuDoTrans: \ndual‑domain transformer provides more attention for sinogram \nrestoration in sparse ‑view CT reconstruction. arXiv preprint arXiv: \n2111.10790\n 82. Pan JY, Zhang HY, Wu WF, Gao ZF, Wu WW (2022) Multi‑domain integra‑\ntive Swin transformer network for sparse‑view tomographic reconstruc‑\ntion. Patterns 3(6):100498.https:// doi. org/ 10. 1016/j. patter. 2022. 100498\n 83. Razi T, Niknami M, Ghazani FA (2014) Relationship between Hounsfield \nunit in CT scan and gray scale in CBCT. J Dent Res Dent Clin Dent \nProspects 8(2):107‑110\n 84. Duda SN, Kennedy N, Conway D, Cheng AC, Nguyen V, Zayas‑Cabán \nT et al (2022) HL7 FHIR‑based tools and initiatives to support clinical \nresearch: a scoping review. J Am Med Inf Assoc 29(9):1642‑1653. \nhttps:// doi. org/ 10. 1093/ jamia/ ocac1 05\n 85. Auer F, Abdykalykova Z, Müller D, Kramer F (2022) Adaptation of HL7 \nFHIR for the Exchange of Patients’ Gene Expression Profiles. Stud Health \nTechnol Inform 295:332‑335. https:// doi. org/ 10. 1101/ 2022. 02. 11. 22270 \n850\n 86. Carter C, Veale B (2022) Digital radiography and PACS, 4th edn. Elsevier, \nAmsterdam\n 87. Twa MD, Johnson CA (2022) Digital imaging and communication \nstandards. Optom Vis Sci 99(5):423. https:// doi. org/ 10. 1097/ OPX. 00000 \n00000 001909\n 88. Xiong YX, Du B, Yan PK (2019) Reinforced transformer for medi‑\ncal image captioning. In: Suk HI, Liu M, Yan P , Lian C (eds) Machine \nlearning in medical imaging. 10th international workshop, Shenzhen, \nOctober 2019. Lecture notes in computer science (Image process‑\ning, computer vision, pattern recognition, and graphics), vol 11861. \nSpringer, Cham, pp 673‑680. https:// doi. org/ 10. 1007/ 978‑3‑ 030‑ \n32692‑0_ 77\n 89. Miura Y, Zhang YH, Tsai E, Langlotz C, Jurafsky D (2021) Improving \nfactual completeness and consistency of image ‑to‑text radiol‑\nogy report generation. In: Proceedings of the 2021 conference of \nthe North American chapter of the association for computational \nlinguistics: human language technologies, Association for Computa‑\ntional Linguistics, Online, 6‑11 June 2021. https:// doi. org/ 10. 18653/  \nv1/ 2021. naacl‑ main. 416\n 90. Rennie SJ, Marcheret E, Mroueh Y, Ross J, Goel V (2017) Self‑ critical \nsequence training for image captioning. In: Proceedings of the 2017 \nIEEE conference on computer vision and pattern recognition, IEEE, \nHonolulu, 21‑26 July 2017. https:// doi. org/ 10. 1109/ CVPR. 2017. 131\n 91. You D, Liu FL, Ge S, Xie XX, Zhang J, Wu X (2021) AlignTransformer: hier‑\narchical alignment of visual regions and disease tags for medical report \ngeneration. In: de Bruijne M, Cattin PC, Cotin S, Padoy N, Speidel S, \nZheng YF et al (eds) Medical image computing and computer assisted \nintervention. 24th international conference, Strasbourg, September \n2021. Lecture notes in computer science, (Image processing, computer \nvision, pattern recognition, and graphics), vol 12903. Springer, Cham, \npp 72‑82. https:// doi. org/ 10. 1007/ 978‑3‑ 030‑ 87199‑4_7\n 92. Xu MY, Islam M, Lim CM, Ren HL (2021) Learning domain adaptation \nwith model calibration for surgical report generation in robotic surgery. \nIn: Proceedings of the 2021 IEEE international conference on robotics \nand automation, IEEE, Xi’an, 30 May‑5 June 2021. https:// doi. org/ 10. \n1109/ ICRA4 8506. 2021. 95615 69\nPage 28 of 28Al‑hammuri et al. Visual Computing for Industry, Biomedicine, and Art            (2023) 6:14 \n 93. Finlayson SG, Bowers JD, Ito J, Zittrain JL, Beam AL, Kohane IS \n(2019) Adversarial attacks on medical machine learning. Science \n363(6433):1287‑1289.  https:// doi. org/ 10. 1126/ scien ce. aaw43 99\n 94. Papangelou K, Sechidis K, Weatherall J, Brown G (2019) Toward an \nunderstanding of adversarial examples in clinical trials. In: Berlingerio \nM, Bonchi F, Gärtner T, Hurley N, Ifrim G (eds) Machine learning and \nknowledge discovery in databases. European conference, Dublin, \nSeptember 2018. Lecture notes in computer science (Lecture notes in \nartificial intelligence), vol 11051. Springer, Cham, pp 35‑51. https:// doi. \norg/ 10. 1007/ 978‑3‑ 030‑ 10925‑7_3\n 95. Benz P , Ham S, Zhang CN, Karjauv A, Kweon IS (2021) Adversarial robust‑\nness comparison of vision transformer and MLP‑mixer to CNNs. In: \nProceedings of the 32nd british machine vision conference 2021, BMVA \nPress, Online, 22‑25 November 2021\n 96. Chuman T, Kiya H (2022) Security evaluation of block‑based image \nencryption for vision transformer against jigsaw puzzle solver attack. \nIn: Proceedings of the 4th global conference on life sciences and \ntechnologies (LifeTech), IEEE, Osaka, 7‑9 March 2022. https:// doi. org/ 10. \n1109/ LifeT ech53 646. 2022. 97549 37\n 97. Li M, Han DZ, Li D, Liu H, Chang CC (2022) MFVT: an anomaly traffic \ndetection method merging feature fusion network and vision trans‑\nformer architecture. EURASIP J Wirel Commun Netw 2022(1):39. https:// \ndoi. org/ 10. 1186/ s13638‑ 022‑ 02103‑9\n 98. Ho CMK, Yow KC, Zhu ZW, Aravamuthan S (2022) Network intrusion \ndetection via flow‑to‑image conversion and vision transformer clas‑\nsification. IEEE Access 10:97780‑97793. https:// doi. org/ 10. 1109/ ACCESS. \n2022. 32000 34\n 99. George A, Marcel S (2021) On the effectiveness of vision transform‑\ners for zero‑shot face anti‑spoofing. In: Proceedings of the 2021 IEEE \ninternational joint conference on biometrics, IEEE, Shenzhen, 4‑7 \nAugust 2021. https:// doi. org/ 10. 1109/ IJCB5 2358. 2021. 94843 33\n 100. Doan KD, Lao YJ, Yang P , Li P (2022) Defending backdoor attacks \non vision transformer via patch processing. arXiv preprint arXiv: \n2206.12381\n 101. Riquelme C, Puigcerver J, Mustafa B, Neumann M, Jenatton R, Susano \nPinto A et al (2021) Scaling vision with sparse mixture of experts. \nAdvances in Neural Information Processing Systems 34: 8583‑8595\n 102. Ridnik T, Ben‑Baruch E, Noy A, Zelnik‑Manor L (2021) ImageNet‑21K \npretraining for the masses. arXiv preprint arXiv: 2104.10972\n 103. Deng J, Dong W, Socher R, Li LJ, Li K, Fei‑Fei L (2009) ImageNet: a large‑\nscale hierarchical image database. In: Proceedings of the 2009 IEEE con‑\nference on computer vision and pattern recognition, IEEE, Miami, 20‑25 \nJune 2009. https:// doi. org/ 10. 1109/ CVPR. 2009. 52068 48\n 104. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma SA et al (2015) \nImageNet large scale visual recognition challenge. Int J Comput Vis \n115(3):211‑252. https:// doi. org/ 10. 1007/ s11263‑ 015‑ 0816‑y\n 105. Chen XN, Hsieh CJ, Gong BQ (2022) When vision transformers outper‑\nform ResNets without pre‑training or strong data augmentations. In: \nProceedings of the 10th international conference on learning represen‑\ntations, OpenReview.net, 25‑29 April 2022\n 106. Gani H, Naseer M, Yaqub M (2022) How to train vision transformer on \nsmall‑scale datasets? arXiv preprint arXiv: 2210.07240\n 107. Chen T, Kornblith S, Norouzi M, Hinton G (2020) A simple framework \nfor contrastive learning of visual representations. In: Proceedings of \nthe 37th international conference on machine learning, PMLR, Online, \n13‑18 July 2020\n 108. Wang XY, Yang S, Zhang J, Wang MH, Zhang J, Yang W et al (2022) \nTransformer‑based unsupervised contrastive learning for histopatho‑\nlogical image classification. Med Image Anal 81:102559.https:// doi. org/ \n10. 1016/j. media. 2022. 102559\n 109. Meng CZ, Trinh L, Xu N, Liu Y (2021) MIMIC‑IF: interpretability and fair‑\nness evaluation of deep learning models on MIMIC‑IV dataset. https:// \ndoi. org/ 10. 21203/ rs.3. rs‑ 402058/ v1\n 110. Lu JH, Zhang XS, Zhao TL, He XY, Cheng J (2022) APRIL: finding the \nAchilles’ heel on privacy for vision transformers. In: Proceedings of the \nIEEE/CVF conference on computer vision and pattern recognition, IEEE, \nNew Orleans, 18‑24 June 2022. https:// doi. org/ 10. 1109/ CVPR5 2688. \n2022. 00981\n 111. Song WP , Shi CC, Xiao ZP , Duan ZJ, Xu YW, Zhang M et al (2019) \nAutoInt: automatic feature interaction learning via self‑attentive neural \nnetworks. In: Proceedings of the 28th ACM international conference on \ninformation and knowledge management, ACM, Beijing, 3‑7 Novem‑\nber 2019. https:// doi. org/ 10. 1145/ 33573 84. 33579 25\n 112. Yu K, Zhang MD, Cui TY, Hauskrecht M (2019) Monitoring ICU mortal‑\nity risk with a long short‑term memory recurrent neural network. In: \nProceedings of the pacific symposium on Biocomputing 2020, World \nScientific, Kohala Coast, 3‑7 January 2020. https:// doi. org/ 10. 1142/ \n97898 11215 636_ 0010\n 113. Bai SJ, Kolter JZ, Koltun V (2018) An empirical evaluation of generic \nconvolutional and recurrent networks for sequence modeling. arXiv \npreprint arXiv: 1803.01271\n 114. Guo T, Lin T, Antulov‑Fantulin N (2019) Exploring interpretable LSTM \nneural networks over multi‑variable data. In: Proceedings of the 36th \ninternational conference on machine learning, PMLR, Long Beach, 9‑15 \nJune 2019\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub‑\nlished maps and institutional affiliations.",
  "topic": "Architecture",
  "concepts": [
    {
      "name": "Architecture",
      "score": 0.7053414583206177
    },
    {
      "name": "Computer science",
      "score": 0.6496157646179199
    },
    {
      "name": "Digital health",
      "score": 0.5461685061454773
    },
    {
      "name": "Telehealth",
      "score": 0.47736597061157227
    },
    {
      "name": "Transformer",
      "score": 0.4707389771938324
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4447477459907532
    },
    {
      "name": "Digital image processing",
      "score": 0.4435618221759796
    },
    {
      "name": "Digital image",
      "score": 0.4409372806549072
    },
    {
      "name": "Segmentation",
      "score": 0.440066933631897
    },
    {
      "name": "Telemedicine",
      "score": 0.425426185131073
    },
    {
      "name": "Digital imaging",
      "score": 0.42034095525741577
    },
    {
      "name": "Data science",
      "score": 0.3914332091808319
    },
    {
      "name": "Health care",
      "score": 0.3794965147972107
    },
    {
      "name": "Multimedia",
      "score": 0.35161930322647095
    },
    {
      "name": "Computer vision",
      "score": 0.35122284293174744
    },
    {
      "name": "Computer architecture",
      "score": 0.3481755256652832
    },
    {
      "name": "Image processing",
      "score": 0.3358490467071533
    },
    {
      "name": "Engineering",
      "score": 0.200486421585083
    },
    {
      "name": "Image (mathematics)",
      "score": 0.19096526503562927
    },
    {
      "name": "Electrical engineering",
      "score": 0.12295952439308167
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}