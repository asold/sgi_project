{
  "title": "Arithmetic-Based Pretraining Improving Numeracy of Pretrained Language Models",
  "url": "https://openalex.org/W4385718025",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5036267079",
      "name": "Dominic Petrak",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A1966671982",
      "name": "Nafise Sadat Moosavi",
      "affiliations": [
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A40109512",
      "name": "Iryna Gurevych",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3173783447",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3176108833",
    "https://openalex.org/W3034556525",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W3100995786",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W3176047188",
    "https://openalex.org/W3205396199",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3035275890",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W4287332285",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2611029872",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3100879603",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3158196282",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3166298099",
    "https://openalex.org/W2952523122",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W2970609357",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W3100618740",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3176395279",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W2525778437"
  ],
  "abstract": "State-of-the-art pretrained language models tend to perform below their capabilities when applied out-of-the-box on tasks that require understanding and working with numbers (usually referred to as numeracy). Recent work suggests two main reasons for this: (1) popular tokenisation algorithms have limited expressiveness for numbers, and (2) common pretraining objectives do not target numeracy. Approaches that address these shortcomings usually require architectural changes or pretraining from scratch. In this paper, we propose a new extended pretraining approach called Arithmetic-Based Pretraining that jointly addresses both in one extended pretraining step without requiring architectural changes or pretraining from scratch. Arithmetic-Based Pretraining combines contrastive learning to improve the number representation, and a novel extended pretraining objective called Inferable Number Prediction Task to improve numeracy. Our experiments show the effectiveness of Arithmetic-Based Pretraining in three different tasks that require improved numeracy, i.e., reading comprehension in the DROP dataset, inference-on-tables in the InfoTabs dataset, and table-to-text generation in the WikiBio and SciGen datasets.",
  "full_text": "Proceedings of the The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 477–493\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nArithmetic-Based Pretraining – Improving Numeracy of Pretrained\nLanguage Models\nDominic Petrak† , Nafise Sadat Moosavi‡, Iryna Gurevych†\n†Ubiquitous Knowledge Processing Lab (UKP Lab),\nDepartment of Computer Science and Hessian Center for AI (hessian.AI),\nTechnical University of Darmstadt, Germany\nhttps://www.ukp.tu-darmstadt.de\n‡Department of Computer Science, The University of Sheffield, UK\nAbstract\nState-of-the-art pretrained language models\ntend to perform below their capabilities when\napplied out-of-the-box on tasks that require un-\nderstanding and working with numbers. Recent\nwork suggests two main reasons for this: (1)\npopular tokenisation algorithms have limited\nexpressiveness for numbers, and (2) common\npretraining objectives do not target numeracy.\nApproaches that address these shortcomings\nusually require architectural changes or pre-\ntraining from scratch. In this paper, we propose\na new extended pretraining approach called\nArithmetic-Based Pretraining that jointly ad-\ndresses both in one extended pretraining step\nwithout requiring architectural changes or pre-\ntraining from scratch. Arithmetic-Based Pre-\ntraining combines contrastive learning to im-\nprove the number representation, and a novel\nextended pretraining objective called Inferable\nNumber Prediction Task to improve numer-\nacy. Our experiments show the effectiveness\nof Arithmetic-Based Pretraining in three differ-\nent tasks that require improved numeracy, i.e.,\nreading comprehension in the DROP dataset,\ninference-on-tables in the InfoTabs dataset, and\ntable-to-text generation in the WikiBio and Sci-\nGen datasets1.\n1 Introduction\nNumbers are ubiquitous in natural language. There-\nfore, understanding and working with numbers\n(usually referred to as numeracy) is a critical ca-\npability for pretrained language models such as\nBART (Lewis et al., 2020) or T5 (Raffel et al.,\n2019), cornerstones of modern NLP, in order to\nutilize quantitative information for various NLP\ntasks. Recent works question whether these mod-\nels meet this requirement out-of-the-box (Wallace\net al., 2019; Zhang et al., 2020): Common pretrain-\ning objectives such as the denoising autoencoder of\n1Code, data, and models trained using Arithmetic-\nBased Pretraining are available here: https://github.com/\nUKPLab/starsem2023-arithmetic-based-pretraining .\nBART (Lewis et al., 2020), the masked language\nmodeling objective of BERT (Devlin et al., 2019),\nor the span-corruption objective of T5 (Raffel et al.,\n2019), are designed for understanding structure\nand semantic meaning of language and not to learn\nworking with numbers. Furthermore, commonly\nused subword-based tokenisation algorithms such\nas Byte Pair Encoding (Sennrich et al., 2016) or\nWordPiece (Wu et al., 2016) are designed to handle\npatterns that are frequently observed during train-\ning, which is disadvantageous for numbers. For\ninstance, 0.72 and 0.73 are two similar numbers.\nThey should be processed similarly, but according\nto their frequency in the pretraining data they might\nbe tokenised very differently, e.g., [0, ., 72] and [0,\n., 7, 3], which will have an impact on their repre-\nsentation in embedding space. To address these\nshortcomings, various approaches have been pro-\nposed recently. However, most of them introduce\nadditional components or rely on predefined fea-\ntures that limit their application, e.g., they are only\napplicable in a specific task like reading compre-\nhension (Andor et al., 2019; Geva et al., 2020) or\nrequire architectural changes (Herzig et al., 2020).\nIn this paper, we propose a new extended pre-\ntraining approach called Arithmetic-Based Pretrain-\ning that targets both shortcomings for pretrained\nlanguage models in one extended pretraining step\nwithout introducing new components or requiring\npretraining from scratch. It consists of:\n• A contrastive loss that combines subword-\nbased with character-level tokenisation to im-\nprove the representation of numbers.\n• A denoising pretraining objective, called the\nInferable Number Prediction Task, to im-\nprove the model’s capability of working with\nnumbers.\nOur experiments show that Arithmetic-Based\nPretraining has a positive impact on BART (Lewis\n477\net al., 2020), T5 (Raffel et al., 2019) and Flan-\nT5 (Chung et al., 2022) in various tasks. It im-\nproves the accuracy in case of reading compre-\nhension and inference-on-tables, and the factual\ncorrectness in case of table-to-text generation.\n2 Related Work\nNumber Representations in Language Models.\nState-of-the-art language models like BART (Lewis\net al., 2020) or T5 (Raffel et al., 2019) use subword-\nbased tokenisation algorithms (such as Byte Pair\nEncoding (Sennrich et al., 2016)) to build vocab-\nularies based on frequently observed sequences\nin a text corpus. While this is effective for com-\nmon words, it is problematic for numbers. In an\nextensive study, Wallace et al. (2019) shows that\nmodels using character-level tokenisation, such as\nELMo (Peters et al., 2018), usually achieve better\nresults in numerical probing tasks and extrapolate\nbetter to unseen numbers compared to models us-\ning subword-based tokenisation. Thawani et al.\n(2021), Peng et al. (2021) and Zhang et al. (2020)\nreport similar findings. In our work, we use the\ncharacter-level tokenisation for numbers to address\nthis shortcoming in BART, T5, and Flan-T5 (Chung\net al., 2022).\nApproaches for Improving Numeracy. Numer-\nacy requires to understand and work with num-\nbers, i.e., to do artihmetic operations, in order\nto generate the expected result. To improve this\ncapability, recent approaches propose pretraining\nfrom scratch or architectural changes to tailor pre-\ntrained language models towards specific tasks.\nTAPAS (Herzig et al., 2020) targets question an-\nswering with tabular data. It is pretrained from\nscratch and extends BERT (Devlin et al., 2019) by\nintroducing additional embeddings for capturing\ntabular structure. GenBERT (Geva et al., 2020)\nreuses a pretrained BERT model and adds a de-\ncoder on top. It is then further trained using math\nword problems and arithmetic operations for (1)\nincorporating the character-level tokenisation for\nnumbers, and (2) to improve the numerical rea-\nsoning skills. It achieves state-of-the-art results\nin the DROP (Dua et al., 2019) and SQUAD (Ra-\njpurkar et al., 2016) datasets. Andor et al. (2019)\nalso reuses the pretrained BERT model and tar-\ngets reading comprehension. They add a new layer\non top that predicts and executes arithmetic op-\nerations. Suadaa et al. (2021) target table-to-text\ngeneration and propose a framework that uses the\ntemplate-guided text generation from Kale and Ras-\ntogi (2020) to inject pre-executed numerical oper-\nations into the pretrained GPT-2 (Radford et al.,\n2019) and T5 (Raffel et al., 2019) models.\nIn their experiments, all of these works show\nthat much of their performance improvements are\ndue to specific design decisions or multi-level pre-\ntraining setups which result in new or task-specific\nmodels. With Arithmetic-Based Pretraining, we\npropose an approach that improves a model’s nu-\nmeracy with just one extended pretraining step and\nwithout changing its architecture.\nDomain-Adaptive Pretraining. The idea of\ndomain-adaptive pretraining is to bridge the gap\nbetween the vocabulary of a model’s original pre-\ntraining corpus and the target domain by continu-\ning pretraining using in-domain data (Gururangan\net al., 2020). In this work, we propose the Infer-\nable Number Prediction Task which is similar to\ndomain-adaptive pretraining if the data used is from\nthe same domain as that of finetuning. However,\nwe show that this is not the only reason for perfor-\nmance improvements (Section 5.3).\nContrastive Learning. Contrastive learning is a\ngeneral way to learn to map vector representations\nof similar data points (usually called anchor and\npositive) close to each other while pushing non-\nsimilar data points apart. In NLP, it is commonly\nused for learning sentence representations (Kim\net al., 2021; Giorgi et al., 2021) or semantic simi-\nlarities (Wang et al., 2021). In this work, we use\ncontrastive learning to improve the representation\nof numbers.\n3 Arithmetic-Based Pretraining\nIn this section, we propose Arithmetic-Based Pre-\ntraining. It combines different tokenisation al-\ngorithms, i.e., character-level and subword-based,\nwith contrastive learning to improve the represen-\ntation of numbers in pretrained language models\n(Section 3.1), while training on the Inferable Num-\nber Prediction Task (Section 3.2) to improve the\ncapability of working with numbers. Section 3.3\ndescribes the joint loss function.\n3.1 Contrastive Learning\nWe propose to use a contrastive loss as additional\ntraining signal to improve the representation of\nnumbers. For example, the model should learn\na similar representation for the number 108.89,\n478\nwhether it is initially tokenised as [1, 0, 8, ., 8, 9]\n(character-level) or [10, 8, ., 89] (subword-based).\nIf a number frequently occurs in the pretraining\ncorpus, its corresponding subword-based encoding\nmay be more informative. If this is not the case, its\ncharacter-level tokenisation may be more informa-\ntive. Therefore, our motivation is to benefit from\nboth embedding spaces for learning better number\nrepresentations. For implementation, we use the\nMultiple Negative Ranking Loss as proposed by\nHenderson et al. (2017)2:\nLC = −1\nN\nN∑\ni=1\nesim(avg(ˆpi),avg(ˆp′i))\n∑\nj esim(avg(ˆpi),avg(ˆpneg)) (1)\nFor the contrastive loss, we consider all numbers\nin the batch independently of the input sequences.\nEach number is used twice, once in character-level\ntokenisation (anchor), and once in subword-based\ntokenisation3. Assume p is a list of all numbers\nin the batch in character-level tokenisation. p′is a\nlist of all numbers in the batch in subword-based\ntokenisation. We consider pi and p′i as a positive\npair. Every other number in p and p′is considered\nas negative sample to pi (denoted as pneg). ˆpi,\nˆp′i, and ˆpneg are the corresponding embeddings\nafter the encoder pass. sim represents the cosine\nsimilarity and avg represents the mean-average of\nthe embedding. Averaging is a simple and effective\nform of aggregation which is necessary at this point,\nas the numbers are split into multiple tokens during\ntokenisation.\n3.2 The Inferable Number Prediction Task\nThe Inferable Number Prediction Task is a varia-\ntion of the classic masked language modeling ob-\njective (Devlin et al., 2019), but aims on improving\na model’s capability on working with numbers by\nfocusing on data that requires arithmetic operations.\nThe task consists of input C and the correspond-\ning target sequence D. C consists of a pair of text\nsequences, C1 and C2, that are separated with a\nspecial character. C2 equals to D, but contains\na masked number that can be inferred from C1.\nGiven C, the task is to reconstruct D by correctly\n2We use the implementation from the sentence-transformer\nlibrary (Reimers and Gurevych, 2019).\n3Note that we use both only for Arithmetic-Based Pre-\ntraining. For finetuning and during inference, we only use\ncharacter-level tokenisation for numbers.\npredicting the masked number inC24. For instance,\nfor the task of table-to-text generation, C consists\nof the linearized form of the input table (C1) and\nits description with one masked number (C2). We\nselect data with the following criteria:\n• D (C2 in C) and C1 should have at least one\noverlapping entity, e.g., D should contain at\nleast one of the entities that appear in the row\nor column headers of C1 if C1 is a table. This\nensures that D is relevant to the information\ngiven in C1.\n• D (C2 in C) should contain at least one num-\nber that either occurs in C1 or is inferable by\nsummation, subtraction, multiplication, divi-\nsion or ordering. This ensures that the masked\nnumber in C2 is arithmetically related to the\nnumbers given in C1.\nNext, we reduce C to the necessary information.\nIf C1 is an extensive text or paragraph, we apply\neach of these heuristics to each of the sentences and\nretain only the matching ones (the same applies to\nC2). If C1 is a table, we remove rows and columns\nthat do not share entities with C2 (see Appendix B\nfor further details and illustrations).\nFor training, we use the cross-entropy loss func-\ntion:\nLINP (x, y) = 1\nN\nN∑\nn=1\n−log\n(\ne(xn,yn)\n∑K\nk=1 e(xn,k)\n)\n(2)\nwhere x represents the logits of the predicted input\nsequence, and y = y1, ..., yN represents the indices\nof the tokens of the output sequence. N is the size\nof the target sequence. xn,yn is the logit of the xn\ntoken corresponding to the output token yn. K is\nthe size of the model’s vocabulary.\n3.3 Joint Loss Function\nWe combine the contrastive loss LC (Equation 1)\nand the loss for the Inferable Number Prediction\nTask LINP (Equation 2) as weighted sum in a joint\nloss function:\nL= LC\n2 + LINP\n2 (3)\n4Preliminary experiments revealed that just reconstructing\nthe masked number, without its context, has a negative impact\non a model’s text generation capabilities.\n479\n4 Experimental Setup\nWe implement our approach using Python 3.10, Py-\nTorch (Paszke et al., 2019) and Huggingface (Wolf\net al., 2020). As pretrained language models, we\nuse the large variant of BART (Lewis et al., 2020)\nand the base variant of T5 (Raffel et al., 2019) and\nFlan-T5 (Chung et al., 2022) as provided by the\nHuggingface platform (see Appendix A for details\non hyperparameters)5. All models are pretrained\nTransformer-based encoder-decoder models, but\ndifferent in size. BART-large consists of a total of\n24 layers and 406M parameters. T5-base and Flan-\nT5-base consist of 12 layers and 220M parameters.\nFlan-T5 is based on T5, but trained on more tasks,\ne.g., arithmetic reasoning, and chain-of-thought\ndata (instructions). It significantly improves the\nresults of the original model in many tasks (Chung\net al., 2022). We conduct all experiments on a Tesla\nV100-SXM3 GPU with 32 GB memory. For ex-\nperiments using table-to-text datasets, we represent\ntables as linearized sequence. We report the results\nof the best single runs.\n4.1 Original Datasets\nReading Comprehension. The task of reading\ncomprehension is to answer a question by reason-\ning over a related text passage. DROP (Dua et al.,\n2019) is such a dataset. It contains over 96,567\nopen-domain question-answer pairs and 6,735 para-\ngraphs. According to the authors, 59.1% of an-\nswers consist of numbers and therefore implicitly\nrequire performing arithmetic operations to be pre-\ndicted correctly. Each paragraph consists of 9.19%\nnumbers on average. We split the dev data into two\nequally-sized subsets and use one for testing. Each\nsubset contains 4,828 question-answer pairs.\nInference-on-Tables. Given a premise and a hy-\npothesis, natural language inference (NLI) is the\ntask of deciding whether the hypothesis is en-\ntailed, contradictory, or neutral to the premise. In-\nfoTabs (Gupta et al., 2020) extends NLI to using\nsemi-structured data, i.e., tables, as hypothesis. It\nconsists of 23,738 hypothesis for 2,540 Wikipedia\ninfoboxes from a variety of domains and provides\nthree different test sets: in-domain, cross-domain,\nand an adversarial test set. The cross-domain test\nset uses premises from domains not used for train-\ning. The adversarial test set uses a different set\n5We could not use the large variant of T5 and Flan-T5 due\nto hardware limitations (each model has 770M parameters).\nof source tables. Furthermore, the wording of hy-\npotheses was slightly changed by expert annotators.\nAccording to the authors, InfoTabs requires nu-\nmerical and temporal reasoning (which implicitly\nrequires performing arithmetic operations) across\nmultiple rows and to a large extent. Each table\nconsists on average of 13, 89% numbers.\nTable-to-Text Generation. Table-to-text genera-\ntion is the task of summarizing tabular data (which\nis often numerical) in a descriptive text. It requires\nto implicitly perform arithmetic operations such\nas ordering, summation or subtraction, or to cap-\nture magnitudes. SciGen (Moosavi et al., 2021)\nis a table-to-text generation dataset that requires\nto generate descriptions for scientific tables6. It is\ndesigned for arithmetic reasoning and consists of\n53,136 table-description pairs. Each table consists\nof 41.55% numbers on average.\nWikiBio (Lebret et al., 2016) is a dataset from\nthe biographical domain. It consists of 728,321\ntable-description pairs. The task is to reproduce the\nfirst paragraph of biographical Wikipedia articles,\ngiven the corresponding infobox. According to\nthe authors, dates, ages, and other quantities play\nan important role. Each table consists of 16.83%\nnumbers on average. However, most values can be\ndirectly copied from the tables and do not require\narithmetic operations.\n4.2 Preprocessing for the Inferable Number\nPrediction Task\nTo fulfill the requirements of the Inferable Number\nPrediction Task, we apply the criterias described\nin Section 3.2 to all datasets in an offline prepro-\ncessing step. In case of InfoTabs (Gupta et al.,\n2020), we only use the data labeled with entailed\nin order to exclude contradictions (see Appendix B\nfor examples and illustrations). Table 1 shows the\nresulting datasets.\nTrain Dev Test\nSciGen 4,859 1,473 55\nWikiBio 412,053 51,424 51,657\nDROP 8,336 849 850\nInfoTabs 1,981 1,800 1,800\nTable 1: Data distribution for the Inferable Number Pre-\ndiction Task after applying the criterias to the original\ndataset splits.\n6NumericNLG (Suadaa et al., 2021) is a similar dataset.\nAs SciGen (Moosavi et al., 2021) provides more unsupervised\ntraining pairs that we can use for Arithmetic-Based Pretraining,\nwe use SciGen in our experiments.\n480\nWe also find that the resulting datasets have\nslightly different number-to-word ratios. In the\ncase of DROP (Dua et al., 2019) and InfoTabs,\npreprocessing increases the portion of numbers up\nto 18.98% and 17.25% in paragraphs and tables.\nIn the case of WikiBio (Lebret et al., 2016) the\nratio remains unchanged and in the case of Sci-\nGen (Moosavi et al., 2021) it reduces the numbers\nper table to 33.88%.\nOCC ORD SUM SUB MUL DIV\nDROP 0.41 0 .32 0 .04 0 .07 0 .13 0.02\nInfoTabs 0.23 0 .34 0 .05 0 .17 0 .15 0.06\nSciGen 0.11 0 .06 0 .03 0 .12 0 .41 0.27\nWikiBio 0.24 0 .38 0 .03 0 .10 0 .20 0.03\nTable 2: Distribution of arithmetic operations in the\npreprocessed datasets.\nTable 2 shows the ratio of samples per dataset\nthat we have identified as being inferable by\narithmetic operiations, i.e., occurence (OCC), or-\ndering (ORD), summation (SUM), subtraction\n(SUB), multiplication (MUL) or division (DIV).\nAppendix C provides a detailed analysis.\n5 Evaluation\nIn this section, we evaluate the impact of\nArithmetic-Based Pretraining on downstream appli-\ncations with BART (Lewis et al., 2020), T5 (Raffel\net al., 2019) and Flan-T5 (Chung et al., 2022) using\nin-domain data (Section 5.2), and out-of-domain\ndata (Section 5.3). For Arithmetic-Based Pretrain-\ning, we use the preprocessed subsets of the original\ndatasets as described in Section 4.2.\n5.1 Evaluation Metrics\nFor inference-on-tables, we evaluate the results\nusing Exact Match (EM score). For reading com-\nprehension, we additionally use F1 score. The\nEM score evaluates the prediction accuracy, i.e., if\nthe prediction exactly matches the target. It is the\npreferred metric for these tasks (Dua et al., 2019;\nGupta et al., 2020). The F1 score reports the over-\nlap between the prediction and the target. This re-\nsults in partial reward in cases where the prediction\nis partially correct. In case of table-to-text genera-\ntion, we conduct a human evaluation. This is due\nto the shortcomings of common automatic metrics\nfor this task, as they are hardly able to assess the\ncorrectness of information not directly contained\nin the source data, i.e., information obtained by rea-\nsoning (Moosavi et al., 2021; Chen et al., 2020b;\nSuadaa et al., 2021). We provide the results of the\nautomatic metrics in Appendix D.\nFor all experiments, Baseline represents the\nBART (Lewis et al., 2020), T5 (Raffel et al., 2019),\nand Flan-T5 (Chung et al., 2022) model directly\nfinetuned on the corresponding dataset without\nArithmetic-Based Pretraining. Ours represents\nthese models with Arithmetic-Based Pretraining.\nWe highlight statistically significant improvements\nof Ours over the respective baseline in the tables\n(independent two-sample t-test, p ≤0.05).\n5.2 In-Domain Pretraining\nThis section discusses the results on downstream\ntasks when using models that are pretrained using\nArithmetic-Based Pretraining with in-domain data.\nFor comparison, we will also report the results of\nthe specialised state-of-the-art model for each task.\nReading Comprehension. Table 3 shows the re-\nsults achieved on DROP (Dua et al., 2019).\nEM F1\nBART Baseline 36.00 39.26\nOurs 45.60 49.50\nT5 Baseline 10.40 14.60\nOurs 11.00 15.20\nFlan-T5Baseline 46.34 94.41\nOurs 72.18 97.65\nQDCAT 85.46 88.38\nTable 3: Evaluation on the DROP dataset. Our approach\noutperforms the baseline in all cases.\nIn all cases, Arithmetic-Based Pretraining im-\nproves the results over the baseline. Based on our\nanalysis of the test results, i.e., by comparing the\npredictions of Baseline with Ours, we find that our\napproach reduces the incorrectly predicted num-\nbers by 14.27% in case of BART (Lewis et al.,\n2020), 16.62% in case of T5 (Raffel et al., 2019),\nand 30.56% in case of Flan-T5 (Chung et al., 2022).\nThe results achieved with Flan-T5 even outperform\nthe results reported by Geva et al. (2020) for Gen-\nBERT (EM 68.6) 7. Regarding the performance\ndifferences between BART and T5, we attribute\nthis to the difference in model size. In this con-\ntext, the performance difference between BART\nand Flan-T5 is particularly interesting. We attribute\nthis to the fact that among other things, Flan-T5\nwas trained in arithmetic reasoning. QDCAT (Chen\n7We also did preliminary experiments with the math word\nproblems dataset provided by Geva et al. (Geva et al., 2020)\nas a first pretraining task but found that this does not improve\nthe results (see Appendix G).\n481\net al., 2020a) is the current state-of-the-art in the\nDROP task. It was built for reading comprehension\nand is based on RoBERTa (Liu et al., 2019), but\nadds an additional question-conditioned reasoning\nstep on top (using a graph-attention network).\nInference-on-Tables. Table 4 presents the pre-\ndiction accuracies (EM score) achieved on the In-\nfoTabs (Gupta et al., 2020) dataset.\nIn-Domain Cross-Domain Adversarial\nBART Baseline 33.30 23.67 27.68\nOurs 67.20 54.40 57.20\nT5 Baseline 32.00 11.76 13.00\nOurs 32.30 18.07 15.25\nFlan-T5Baseline 27.23 25.14 29.17\nOurs 34.04 26.14 29.04\nBPR 78.42 71.97 70.03\nTable 4: Evaluation on the InfoTabs dataset. Our ap-\nproach significantly improves the results on the in-\ndomain data.\nSimilarly to reading comprehension, Arithmetic-\nBased Pretraining significantly improves EM\nscores in all cases. This applies especially to the in-\ndomain test set. For the other two test sets, our ap-\nproach also shows improvements over the baselines\n(mostly for BART (Lewis et al., 2020)), indicating\nto improve the model’s robustness and capability\nto extrapolate to unseen data. We attribute perfor-\nmance differences to model sizes. Furthermore,\nanalysis of the in-domain test results shows that\nT5 and Flan-T5 are biased toward predicting en-\ntailment. Since we observe this in both Baseline\nand Ours, we do not attribute this to how the data\nwas preprocessed for the Inferable Number Predic-\ntion Task (Section 4.2). This is different for BART.\nAn analysis of the in-domain test results shows\nthat the model correctly predicts 60.30% of entail-\nments, 75.50% of contradictions, and 65.83% of\nneutrals. BPR (Neeraja et al., 2021) is the current\nstate-of-the-art in the InfoTabs task. It is based\non BERT (Devlin et al., 2019) but built for infer-\nence over tabular data. It provides an improved\nrepresentation of the input data, is pretrained on\nMultiNLI (Williams et al., 2018), and incorporates\nexternal knowledge.\nTable-to-Text Generation. For human evalua-\ntion8, we follow the approach used by Moosavi\net al. (2021) for evaluating the results on SciGen.\nAs this is very time-consuming, we only analyse\n8The human evaluation was conducted by one of the au-\nthors.\n100 random table-description pairs from each, the\nSciGen and WikiBio (Lebret et al., 2016) dataset,\nand also only from the BART (Lewis et al., 2020)\nexperiments. For SciGen, we use the results from\nthe large split experiment9.\nFor annotation, we break down each generated\noutput to its corresponding statements (facts). We\ncreate one CSV file for each dataset that contains\nthese statements in random order. This way, the\nannotator can not see whether a statement was gen-\nerated by Ours (BART with Arithmetic-Based Pre-\ntraining) or Baseline (BART without Arithmetic-\nBased Pretraining). Alongside with the generated\nstatements, this CSV file contains the original ta-\nbles and gold descriptions. The annotator then de-\ncides for each of the statements whether it belongs\nto one of the following labels:\n• Entailed: The statement is entailed in the gold\ndescription, e.g., a fact that is mentioned ei-\nther in a similar or different wording in the\ndescription.\n• Extra: The statement is not entailed in the\ngold description but is factually correct based\non the table’s content.\n• Incorrect: The statement is relevant to the\ntable, i.e., it contains relevant entities but is\nfactually incorrect. For instance, the state-\nment says system A outperforms system B by\n2 points while based on the table system A\nhas a lower performance than system B.\n• Hallucinated: The statement is not relevant to\nthe table.\nBased on these labels, we then\ncompute the recall ( #entailed/#gold),\nprecision (#entailed/#generated), correct-\nness ( (#entailed + #extra)/#generated), and\nhallucination (#hallucinated/#generated) scores\nfor the generated facts. #gold and #generated refers\nto the respective number of included statements,\nnot complete sequences. Table 5 shows the results.\nArithmetic-Based Pretraining improves the pre-\ncision, recall, and correctness for both SciGen and\nWikiBio. In case of WikiBio, it improves the pre-\ncision by 0.06 points, suggesting that generated\n9For SciGen, BART is the current state-of-the-art, and the\nbaseline results of our human evaluation are comparable with\nthose reported by Moosavi et al. (2021). We are not aware\nof a comparable human evaluation for WikiBio. Appendix D\nshows a comparison of automatic metrics for both datasets.\n482\nPrec. Rec. Cor. Hall.\nSciGen\nBaseline 0.08 0.02 0.31 0.29\nOurs 0.09 0.03 0.40 0.33\nWikiBio\nBaseline 0.22 0.07 0.33 0.03\nOurs 0.28 0.09 0.46 0.02\nTable 5: Results of the human evaluation. In both cases,\nour approach improves the correctness of the generated\nfacts.\nstatements are more concise and closer to the target\ndescription. It also improves the ratio of statements\nthat are factually correct by 0.13 points. In case\nof SciGen, the baseline results reflect the results\nreported by Moosavi et al. (2021), who also used\nthe large variant of BART for their experiments.\nOurs improves the results in almost every aspect\n(especially in case of factual correctness, where it\nimproves the results by 0.09 points). However, we\nobserve a slight increase in hallucinations, which\nis a minor deterioration. We found that while Base-\nline seems to generate descriptions close to the\ntarget, Ours is somewhat more oriented towards\nthe tabular values, whereby these values are used\nout-of-context in some cases which might be the\nreason for this deterioration. Nevertheless, all mod-\nels generate fluent and valid-looking descriptions\n(see Appendix H for examples). This suggests that\nArithmetic-Based Pretraining has no negative im-\npact on a model’s text generation capability. This\nis also supported by the results achieved using au-\ntomatic metrics (see Appendix D).\n5.3 Out-of-Domain Pretraining\nTo investigate whether the effectiveness of\nArithmetic-Based Pretraining is a result of using\nin-domain data for pretraining (domain-adaptive\npretraining) or improved numeracy, we evaluate\nour approach using out-of-domain data for pretrain-\ning. We focus on BART (Lewis et al., 2020) for\nthis experiment and perform Arithmetic-Based Pre-\ntraining on a different dataset before finetuning\non DROP (Dua et al., 2019) and InfoTabs (Gupta\net al., 2020). For instance, for the DROP exper-\niments, we pretrain models on WikiBio (Lebret\net al., 2016), SciGen (Moosavi et al., 2021), and\nInfoTabs, which all include data from a different\ndomain, before finetuning. For SciGen, we use the\nlarge split in this experiment.\nTable 6 shows the results. Overall, the models\npretrained using SciGen achieve the best out-of-\nEM F1\nDROP\nDROP (in-domain) 45.60 49.50\nWikibio→DROP 6.00 33.50\nInfoTabs→DROP 35.50 39.63\nSciGen→DROP 47.70 51.60\nInfoTabs\nInfoTabs (in-domain)67.20 -\nWikiBio→InfoTabs 33.15 -\nDROP→InfoTabs 32.80 -\nSciGen→InfoTabs 64.70 -\nTable 6: Results of the out-of-domain pretraining (see\nTables 3 and 4 for the in-domain experiments).\ndomain results in both cases. In case of DROP,\nthe results even exceed the ones achieved with\nin-domain pretraining. We find that the extent to\nwhich the pretraining dataset requires understand-\ning and working with numbers has a major im-\npact on the downstream performance (the more,\nthe greater the impact). Among the datasets used,\nSciGen is in particular designed for the task of text\ngeneration based on arithmetic reasoning. It has\na high number-to-word ratio and the subset used\nfor pretraining on the Inferable Number Predic-\ntion Task (see Section 3.2) predominantly depends\non arithmetic operations such as multiplications\nor divisions (see Table 2) instead of lookups or\norderings (like in the other datasets).\n6 Ablation Study\nIn this section, we investigate the impact of\nArithmetic-Based Pretraining on the numeracy of\na pretrained language model. Due to the short-\ncomings of automatic metrics in table-to-text gen-\neration (see Section 5.1) and because we want\nto be able to compare and discuss the impact of\neach component across datasets, we use the Infer-\nable Number Prediction task for this and evaluate\nthe number of correctly predicted context-related\nmasked numbers (please see Appendix E for abla-\ntion experiments in downstream tasks)10. We use\nthe preprocessed subsets of the original datasets for\nthe Inferable Number Prediction Task (see Sec-\ntion 4.2). For evaluation, we use Exact Match\n(EM score) and F1 score (see Section 5.1). Table 7\nshows the results.\nWe consider the large variant of BART (Lewis\net al., 2020) with its default tokenisation (DT)\nand masking procedure (DM) as baseline for this\n10In case of the contrastive loss, we also experiment with\nother number representations (see Appendix F).\n483\nEM F1\nWikiBio\nBART 29.69 48.12\nCLT + INP43.13 69.97\nOurs 77.38 74.69\nSciGen\nBART 7.04 32.21\nDT + INP 7.20 35.11\nCLT + INP12.26 36.78\nOurs 24.68 45.81\nOurs - INP21.49 40.51\nInfoTabs\nBART 12.43 22.17\nDT + INP 23.20 46.17\nCLT + INP59.09 73.88\nOurs 60.45 74.33\nOurs - INP59.66 72.71\nDROP\nBART 7.20 7.20\nDT + INP 6.33 55.51\nCLT + INP29.40 66.43\nOurs 30.58 67.07\nOurs - INP25.37 59.83\nTable 7: Ablation study on the Inferable Number Predic-\ntion Task. We conduct DT + INP and Ours - INP once\nfor each task and with SciGen (Moosavi et al., 2021) as\nrepresentative for table-to-text generation.\nexperiment. DT + INPuses the default tokenisa-\ntion but our masking procedure (INP). CLT + INP\nthen uses the character-level tokenisation for num-\nbers (CLT). Ours finally combines CLT and INP\nwith the contrastive loss (CL) as supporting signal\nto improve the representation of numbers. As last\nablation, Ours - INPcombines CLT with the con-\ntrastive loss but uses DM instead of INP and shows\nthe contribution of our masking procedure to the\neffectiveness of Arithmetic-Based Pretraining.\nIn comparison with BART, DT + INP shows\nthat our masking procedure improves the results\nacross all tasks. This is most significant in case\nof InfoTabs (up to 10.77 points in EM score). In\ncase of DROP, it raises the F1 score from 7.20 to\n55.51 points, meaning that there is a significantly\nlarger overlap between predicted numbers and tar-\nget numbers. Using character-level instead of de-\nfault tokenisation for numbers (CLT + INP) again\nimproves the results across all datasets, indicat-\ning improved capabilities for arithmetic operations.\nCompared to DT + INP, it improves the EM score\nby 35.89 points in case of InfoTabs, and by 23.07\npoints in case of DROP. Ours further improves the\nresults across all datasets. This is most significant\nin case of the table-to-text datasets, where it im-\nproves the EM score by 34.25 points in case of\nWikiBio (Lebret et al., 2016), and 12.42 points\nin case of SciGen (Moosavi et al., 2021). Since\nwe create the pairs for the contrastive loss batch-\nwise, i.e., we consider all numbers in a batch inde-\npendently from the samples (see Section 3.1), an\nadvantageous number-to-word ratio favors a good\npositive-negative pair ratio for the contrastive loss,\nas in the case of SciGen which has the highest\nnumber to word ratio in input tables (33.88%, see\nalso Section 4.1). This is counteracted by WikiBio\nwhich has a lower number-to-word ratio (16.32%).\nHowever, with 728, 321 samples, Wikibio is the\nlargest dataset. We therefore assume that more data\ncompensates for a poor number-to-word ratio. Ours\n- INP deteriorates the EM score by 5.21 points in\ncase of DROP,3.19 points in case of SciGen, and\n0.79 points in case of InfoTabs. This shows the\ncontribution of our masking procedure to the effec-\ntiveness of Arithmetic-Based Pretraining.\n7 Conclusions\nIn this paper, we propose Arithmetic-Based Pre-\ntraining, an approach for jointly addressing the\nshortcomings of pretrained language models in un-\nderstanding and working with numbers (usually\nreferred to as numeracy). In contrast to existing\napproaches, Arithmetic-Based Pretraining does not\nrequire architectural changes or pretraining from\nscratch. It uses contrastive learning to improve\nnumber representation and a novel extended pre-\ntraining objective, the Inferable Number Prediction\nTask, to improve numeracy in just one extended\npretraining step. Our experiments show perfor-\nmance improvements due to better numeracy in\nthree different state-of-the-art pretrained language\nmodels, BART, T5, and Flan-T5, across various\ntasks and domains, including reading comprehen-\nsion (DROP), inference-on-tables (InfoTabs), and\ntable-to-text generation (SciGen and WikiBio). We\nshow that the effectiveness of our approach is not\nlimited to in-domain pretraining, but rather depends\non the extent to which the dataset used in the In-\nferable Number Prediction Task requires under-\nstanding numbers. For example, pretraining on the\nSciGen dataset improves the results achieved on\nDROP. Our ablation studies show that contrastive\nlearning and the Inferable Number Prediction Task\nare key to improving the numeracy of the examined\nmodels.\n484\n8 Limitations\nOur work is subject to some limitations. First of\nall, due to hardware limitations, we could not use\nthe large variant of T5 (Raffel et al., 2019) and\nFlan-T5 (Chung et al., 2022) in a setting com-\nparable to our BART-large experiments. Further-\nmore, BART (Lewis et al., 2020) restricts the max-\nimum length of input sequences to 1024 char-\nacters11. For better comparability, we also use\nT5 and Flan-T5 accordingly. This limitation is\ndue to the increased computational complexity of\nlonger input sequences, but it is problematic with\ntable-to-text generation datasets. For example, Sci-\nGen (Moosavi et al., 2021) consists in large parts\nof tables that exceed this sequence length when rep-\nresented as a linearized sequence. While we have\ntried to take this into account by reducing the input\ndata to necessary information, it was not guaran-\nteed that the model always sees the complete infor-\nmation, which certainly has a negative impact on\nthe evaluation results achieved on the downstream\ntasks. We guess that the results would have been\nmore expressive if we would have used a different\nrepresentation for tables, or focused on models that\ndo not have this sequence length limitation.\nAnother limitation of our work concerns the im-\npact of contrastive learning. According to Hen-\nderson et al. (2017), the impact of contrastive loss\nis favored by large batch sizes. Due to hardware\nlimitations, we were only able to use small batch\nsizes (see Appendix A). The models might have\nadapted better if we would had the possibility to\ntrain with larger batch sizes. Regarding the weight-\ning of contrastive and masked loss in the joint loss\nfunction, we only use equal weighting for our ex-\nperiments, since we found that this already leads to\ngood results, and due to the already large number\nof experiments conducted in this paper, we did not\nexperiment with other weightings. However, opti-\nmizing this hyperparameter could further improve\nthe results.\nEvaluation is also a critical point. Although\nmetrics such as PARENT (Dhingra et al., 2019)\ntry to measure the factual correctness of generated\ndescriptions, it requires a more individual exam-\nination in many cases. Especially in such highly\nspecialized scenarios such as SciGen. Therefore,\nwe conduct a human evaluation in order to analyse\n11https://huggingface.co/docs/transformers/\nmodel_doc/bart#transformers.BartConfig, last ac-\ncessed on 10/02/23.\nthe impact of our Arithmetic-Based Pretraining on\nthe downstream tasks. However, due to limited re-\nsources, we were only able to conduct a small-scale\nhuman evaluation. At this point, we would also like\nto mention that our evaluation setup in general is\nsubject to limitations. As an extended pretrain-\ning approach, Arithmetic-Based Pretraining might\nhave a negative impact on a model’s general ap-\nplicability, i.e., downstream performance in tasks\nused for pretraining, e.g., translation in case of T5,\nor other non-number related tasks commonly used\nin model benchmarking, such as question answer-\ning, text classification, or sentiment analysis. We\nonly examined the impact on text generation as\npart of our human evaluation and with automatic\nmetrics (see Appendix D). However, since (1) the\nInferable Number Prediction Task (Section 3.2) is\na variation of the widely used masked language\nmodeling objective (Devlin et al., 2019), and (2)\ncharacter-level tokenisation does not introduce new\nembeddings into a pretrained language model, we\ndon’t expect a negative impact here.\nAnother limitation concerns the evaluation of\nthe Inferable Number Prediction Task on a model’s\nnumeracy. Since it is not reliably traceable whether\nand which arithmetic operation was used by a\nmodel to come to a specific result, we can only infer\nimproved capabilities for arithmetic operations by\nperformance improvements in the Inferable Num-\nber Prediction Task. We cannot clearly distinguish\nperformance improvements on specific arithmetic\noperations.\n9 Acknowledgements\nThis research work has been funded by the Ger-\nman Federal Ministry of Education and Research\nand the Hessian Ministry of Higher Education, Re-\nsearch, Science and the Arts within their joint sup-\nport of the National Research Center for Applied\nCybersecurity ATHENE. It also received funding\nfrom the German Research Foundation (DFG) un-\nder grant № EC 503/1-1 and GU 798/21-1.\nReferences\nDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler.\n2019. Giving BERT a calculator: Finding opera-\ntions and arguments with reading comprehension. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5947–\n485\n5952, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xi-\naochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan\nQi, and Wei Chu. 2020a. Question directed graph\nattention network for numerical reasoning over text.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6759–6768, Online. Association for Computa-\ntional Linguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020b. Logical natural lan-\nguage generation from open-domain tables. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7929–\n7942, Online. Association for Computational Lin-\nguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-\nWei Chang, Dipanjan Das, and William Cohen. 2019.\nHandling divergent reference texts when evaluating\ntable-to-text generation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4884–4895, Florence, Italy. Asso-\nciation for Computational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 946–958, Online. Association for Computa-\ntional Linguistics.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.\n2021. DeCLUTR: Deep contrastive learning for un-\nsupervised textual representations. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 879–895, Online.\nAssociation for Computational Linguistics.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek\nSrikumar. 2020. INFOTABS: Inference on tables\nas semi-structured data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2309–2324, Online. Association\nfor Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun-\nHsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Effi-\ncient natural language response suggestion for smart\nreply. arXiv preprint arXiv:1705.00652.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nMihir Kale and Abhinav Rastogi. 2020. Template\nguided text generation for task-oriented dialogue. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6505–6520, Online. Association for Computa-\ntional Linguistics.\nTaeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021.\nSelf-guided contrastive learning for BERT sentence\nrepresentations. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 2528–2540, Online. Association for\nComputational Linguistics.\nRémi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with ap-\nplication to the biography domain. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1203–1213, Austin,\nTexas. Association for Computational Linguistics.\n486\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nNafise Moosavi, Andreas Rücklé, Dan Roth, and Iryna\nGurevych. 2021. Scigen: a dataset for reasoning-\naware text generation from scientific tables. In Pro-\nceedings of the Neural Information Processing Sys-\ntems Track on Datasets and Benchmarks, volume 1.\nJ. Neeraja, Vivek Gupta, and Vivek Srikumar. 2021.\nIncorporating external knowledge to enhance tabular\nreasoning. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2799–2809, Online. Association\nfor Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. Advances in Neural Information Processing\nSystems 32, pages 8024–8035.\nShuai Peng, Ke Yuan, Liangcai Gao, and Zhi Tang.\n2021. Mathbert: A pre-trained model for math-\nematical formula understanding. arXiv preprint\narXiv:2105.00377.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nClément Rebuffel, Marco Roberti, Laure Soulier, Geof-\nfrey Scoutheeten, Rossella Cancelliere, and Patrick\nGallinari. 2021. Controlling hallucinations at word\nlevel in data-to-text generation.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning robust metrics for text genera-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881–7892, Online. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nLya Hulliyyatus Suadaa, Hidetaka Kamigaito, Kotaro\nFunakoshi, Manabu Okumura, and Hiroya Takamura.\n2021. Towards table-to-text generation with numer-\nical reasoning. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1451–1465, Online. Association for\nComputational Linguistics.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–656, Online. As-\nsociation for Computational Linguistics.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know num-\nbers? probing numeracy in embeddings. In Proceed-\nings of the 2019 Conference on Empirical Methods\n487\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5307–5315, Hong\nKong, China. Association for Computational Linguis-\ntics.\nDong Wang, Ning Ding, Piji Li, and Haitao Zheng.\n2021. CLINE: Contrastive learning with semantic\nnegative examples for natural language understand-\ning. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n2332–2342, Online. Association for Computational\nLinguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nXikun Zhang, Deepak Ramachandran, Ian Tenney,\nYanai Elazar, and Dan Roth. 2020. Do language\nembeddings capture scales? In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 4889–4896, Online. Association for Computa-\ntional Linguistics.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\nA Hyperparameters for Experiments\nTable 8 shows the hyperparameter configuration for\nour experiments. In order to not train longer than\nnecessary, we have determined the optimal number\nof epochs for each experiment by using early stop-\nping with a patience of 10. For the downstream\ntasks, we have used the MoverScore (Zhao et al.,\n2019) with the table-to-text generation datasets.\nFor DROP (Dua et al., 2019) and InfoTabs (Gupta\net al., 2020), we have used the EM score. All mod-\nels were trained for the same amount of epochs.\nBatch Size Epochs Learning Rate\nInferable Number Prediction Task\nSciGen 8 50 3e-5\nWikiBio 8 3 3e-5\nInfoTabs 8 21 3e-5\nDROP 8 48 3e-5\nDownstream Tasks\nSciGen 8 27 3e-5\nWikiBio 8 9 3e-5\nInfoTabs 8 14 3e-5\nDROP 8 10 3e-5\nTable 8: Hyperparameter Configuration.\nB Inferable Number Prediction Task –\nExample Input Data\nFor table-to-text generation, Figure 1 shows an ex-\nample of a (linearized) table from SciGen (Moosavi\net al., 2021) with its caption as C1, concatenated\nto its masked description C2 using </s>. <s> and\n</s> are special tokens used by BART (Lewis et al.,\n2020) to represent the beginning and ending of a\nsequence. In case of WikiBio (Lebret et al., 2016),\nthe input data is represented accordingly.\nFigure 1: Illustration of a linearized table that is used\nfor the Inferable Number Prediction Task. <R>, <C>\nand <CAP> symbolize the beginning of a new row, cell,\nand the table’s caption.\nFor DROP (Dua et al., 2019), Figure 2 shows\nan example. It consists of the paragraph C1, and a\n488\nquestion C2. The question contains a number ( 2)\nthat also occurs in the paragraph.\nFigure 2: Illustration of an input sample for the Inferable\nNumber Prediction Task using DROP.\nFigure 3 shows an example for the In-\nfoTabs (Gupta et al., 2020) datasets. It is basically\nthe same as for the table-to-text generation datasets,\nbut uses the hypothesis as C2.\nFigure 3: Illustration of an input sample for the Inferable\nNumber Prediction Task using InfoTabs.\nC Inferable Number Prediction Task –\nDataset Details\nIn this section, we want to provide more details\non the distribution of arithmetic operations across\ndatasets used for the Inferable Number Prediction\nTask. Table 9 shows the ratio of each arithmetic\noperation on the overall number of samples for each\nsplit for the InfoTabs (Gupta et al., 2020) dataset.\nOCC ORD SUM SUB MUL DIV\nTrain 0.24 0.35 0.05 0.16 0.15 0.05\nDev 0.15 0.34 0.07 0.18 0.20 0.06\nTest 0.22 0.16 0.09 0.23 0.23 0.07\nTable 9: Ratio of arithmetic operations for each split of\nthe InfoTabs dataset.\nTable 10 shows this ratio for the DROP (Dua\net al., 2019) dataset.\nOCC ORD SUM SUB MUL DIV\nTrain 0.41 0.32 0.4 0.07 0.13 0.03\nDev 0.42 0.31 0.05 0.05 0.14 0.03\nTest 0.43 0.30 0.04 0.05 0.15 0.03\nTable 10: Ratio of arithmetic operations for each split\nof the DROP dataset.\nTable 11 shows this ratio for the Sci-\nGen (Moosavi et al., 2021) dataset.\nOCC ORD SUM SUB MUL DIV\nTrain 0.11 0.06 0.04 0.12 0.40 0.27\nDev 0.11 0.05 0.04 0.12 0.43 0.25\nTest 0.15 0.09 0.02 0.19 0.43 0.13\nTable 11: Ratio of arithmetic operations for each split\nof the SciGen dataset.\nTable 12 shows this ratio for the WikiBio (Lebret\net al., 2016) dataset.\nOCC ORD SUM SUB MUL DIV\nTrain 0.25 0.38 0.03 0.10 0.20 0.03\nDev 0.25 0.38 0.03 0.10 0.19 0.04\nTest 0.25 0.38 0.03 0.11 0.20 0.03\nTable 12: Ratio of arithmetic operations for each split\nof the SciGen dataset.\nD Evaluation Using Automatic Metrics\nThis section presents the evaluation of our results\non table-to-text datasets using automatic metrics.\nFor this, we use a variety of metrics commonly\nused for this task, i.e.,BLEU (Papineni et al., 2002),\nMoverScore (Zhao et al., 2019), BLEURT (Sellam\net al., 2020), and PARENT (Dhingra et al., 2019).\nWhile BLEU calculates the concordance between\nthe predicted description and the actual target on\nword-level, MoverScore and BLEURT measure the\nsemantic concordance between the predicted de-\nscription and the target using BERT (Devlin et al.,\n2019). BLEURT also takes the fluency of the pre-\ndictions into account. PARENT estimates the fac-\ntual correctness by comparing the predicted de-\nscription to the original table and the target descrip-\ntion, and especially rewards correct information\nthat is contained in the table but not in the target. It\nhas a higher correlation with human judgment. Ta-\nble 13 reports the results. We highlight statistically\nsignificant improvements of our approach over the\nrespective baseline in the tables (independent two-\nsample t-test, p ≤0.05).\n489\nMoverS BLEU BLEURT PARENTSciGen\nBART\nBaseline Few 52.48 4.60 -0.63 3.38Medium 53.76 4.26 -0.69 3.72Large 53.43 4.87 -0.70 3.68\nOurs Few 53.301.73 -0.76 3.81Medium53.402.71 -0.78 3.45Large 55.00 9.30 -0.76 3.82BART (Moosavi et al.) Large 14.00 5.04 -0.71 -\nT5\nBaseline Few 52.30 2.96 -0.94 6.39Medium 51.79 2.67 -0.95 4.08Large 53.00 3.40 -0.70 5.18\nOurs Few 52.00 2.83 -0.98 4.32Medium 52.00 2.51 -0.86 4.70Large 53.40 2.96 -0.89 6.72BART (Moosavi et el.) Large 6.00 3.38 -0.79 -\nFlan-T5\nBaseline Few 53.03 2.76 -0.67 7.89Medium 53.56 3.03 -0.68 6.14Large 54.15 3.54 -0.65 7.94\nOurs Few 54.223.14 -0.65 8.54Medium54.763.25 -0.71 8.12Large 55.123.34 -0.61 9.32WikiBio\nBARTBaseline 61.50 17.98 -0.64 45.18Ours 62.78 18.54 -0.27 44.32\nT5 Baseline 60.30 17.94 -0.86 43.97Ours 60.10 20.00 -0.22 45.25\nFlan-T5Baseline 59.81 17.56 -0.78 44.67Ours 62.51 21.11 -0.18 46.10MBD - 41.56 - 56.16\nTable 13: Evaluation of our results on table-to-text\ndatasets using automatic metrics. Baseline presents\nthe results of the BART-large and Flan-T5-base models\nwithout Arithmetic-Based Pretraining. Ours shows the\nresults of these models with Arithmetic-Based Pretrain-\ning.\nThe results show that Arithmetic-Based Pretrain-\ning slightly improves the performance in most ex-\nperiments (based on PARENT and MoverScore),\nand has no negative impact text generation capa-\nbilities. However, as outlined in Section 5.1, none\nof these metrics can really assess the correctness\nof a fact that might be reasoned from the source\ndata (Moosavi et al., 2021; Chen et al., 2020b;\nSuadaa et al., 2021). PARENT tries to address\nthis, which is why this metric is the most appro-\npriate one. Like BLEURT, Moverscore measures\nthe semantic concordance between target and pre-\ndiction. The advantage of MoverScore is that it is\neasier to interpret.\nIn case of SciGen, even our baseline results for\nBART (Lewis et al., 2020) are better than reported\nby Moosavi et al. (2021). We attribute this to dif-\nferent training hyperparameters (they did not re-\nport hyperparameters). While BART (Lewis et al.,\n2020) and T5 (Raffel et al., 2019) are state-of-the-\nart in SciGen (Moosavi et al., 2021), MBD (Re-\nbuffel et al., 2021) is the state-of-the-art in Wik-\niBio (Lebret et al., 2016). It is a multi-branch de-\ncoder that was build to reduce the hallucination in\ndata-to-text tasks.\nE Ablation Study – Downstream Tasks\nThis section shows the results of our downstream\nablation experiments. For experiments, we use the\nsame setup as described in Section 6, i.e., we con-\nsider the large variant of BART (Lewis et al., 2020)\nwith its default tokenisation (DT) and masking pro-\ncedure (DM) as baseline for this experiment. Addi-\ntionally, we finetune the models in the downstream\ntask (using the hyperparameters described in Ap-\npendix A). For evaluation, we use the respective\ntest splits (in-domain in case of InfoTabs (Gupta\net al., 2020)). Table 14 and Table 15 show the re-\nsults of our ablation experiments in downstream\ntasks. We conduct the same experiments as for\nthe general ablation study (Section 6): DT + INP\nuses the default tokenisation but our masking pro-\ncedure (the Inferable Number Prediction Task, Sec-\ntion 3.2), CLT + INPuses the character-level to-\nkenisation for numbers (CLT),Ours combines CLT\nand INP with the contrastive loss (CL), and Ours\n- INPcombines CLT with the contrastive loss but\nuses DM instead of INP. Overall, the results reflect\nthe findings described in Section 6. We highlight\nstatistically significant improvements of our ap-\nproach over the respective baseline in the tables\n(independent two-sample t-test, p ≤0.05).\nMoverScore BLEU\nWikiBio\nBART 61.50 17.98\nDT + INP 61.74 17.31\nCLT + INP 62.01 18.42\nOurs 62.78 18.54\nOurs - INP 62.15 18.25\nSciGen\nBART 53.43 4.87\nDT + INP 53.76 4.65\nCLT + INP 54.12 6.45\nOurs 55.00 9.30\nOurs - INP 54.87 7.32\nTable 14: Downstream ablation study for SciGen and\nWikiBio.\nAccording to automatic metrics, the impact on\ntable-to-text generation is rather limited. We sus-\npect that this is partly due to their shortcomings\nin assessing the correctness of information not di-\nrectly included in the source data (see also Sec-\ntion 5.1). DT + INP shows that pretraining using\nour masking procedure slightly improves the re-\nsults in both cases. Using the character-level to-\nkenisation for numbers further improves the results\n(CLT + INP). In case of SciGen, the comparison be-\ntween Ours and Ours - INP suggests that using the\n490\ncharacter-level tokenisation and contrastive learn-\ning to improve the number representation has more\nimpact than pretraining using INP. In case of Wik-\niBio, the differences are rather negligible (although\nOurs outperforms the baseline). This might be due\nto the characteristics of the dataset. As described in\nSection 4.1, WikiBio rather requires copying num-\nbers from input tables to output text, than inferring\ncontext-related numbers (which is different in the\nother datasets).\nEM F1\nDROP\nBART 36.00 39.26\nDT + INP 39.87 43.77\nCLT + INP42.19 46.09\nOurs 45.60 49.50\nOurs - INP43.68 47.45\nInfoTabs\nBART 33.30 -\nDT + INP 48.21 -\nCLT + INP61.56 -\nOurs 67.20 -\nOurs - INP62.56 -\nTable 15: Downstream ablation study for DROP and\nInfoTabs\nIn case of DROP (Dua et al., 2019) and In-\nfoTabs (Gupta et al., 2020), the results are more\nexpressive. In both cases, we find that just us-\ning INP (DT + INP) as an extended pretraining\ntask already brings a significant improvement over\nthe baselines. This is further improved by using\ncharacter-level tokenisation for numbers (CLT +\nINP) and contrastive learning (Ours). Ours - INP\nshows that in both cases, INP has a significant im-\npact on performance improvements.\nF Experiments using other Contrastive\nRepresentations\nRegarding the contrastive representation, we also\nexperiment with number representations other than\nthe default subword-level one in order to improve\nthe representation of numbers using the character-\nlevel tokenisation, i.e., exponent-mantissa (Zhang\net al., 2020), a verbalized representation, and a com-\nbination of all of them using the Inferable Number\nPrediction Task. We focus on BART (Lewis et al.,\n2020) (the large variant) for this experiment. We\nconduct this experiment using the large split of the\nSciGen dataset (Moosavi et al., 2021). Table 16\nshows the results.\nNone of the other representations improves the\nExperiment EM F1\nBART (verb. repr.) 15.69 41.01\nBART (exp.-mant. repr) 18.13 36.78\nBART (subword-based tok.)24.68 45.81\nBART (combined) 17.92 38.43\nTable 16: Comparison of results when using different\nrepresentations for incorporating the character-level to-\nkenisation.\nresults over using the default subword-level tokeni-\nsation.\nG Preliminary Math Experiments\nWith GenBERT, Geva et al. (2020) propose to\nstart pretraining with math word problems in or-\nder to improve the model’s number understanding\nand capabilities for arithmetic operations. There-\nfore, following this idea would be an obvious\nstep in order to improve the numeracy of gen-\neral purpose pretrained language models. Table 17\nshows the results of a preliminary experiment using\nGenBERT’s math word problems dataset (MWP),\nBART (Lewis et al., 2020), and SciGen (Moosavi\net al., 2021) on the Inferable Number Prediction\nTask. We highlight statistically significant improve-\nments of our approach over the respective base-\nline in the tables (independent two-sample t-test,\np ≤0.05).\nExperiment EM F1\nBaseline 7.20 35.11\nMWP-pretrained Baseline 15.19 34.18\nMWP-pretrained Baseline + CLT 22.94 42.55\nMWP-pretrained Baseline + CLT + CL22.78 43.14\nOurs 24.68 45.81\nTable 17: Results achieved on the Inferable Number\nPrediction Task with and without pretraining using math\nword problems.\nBaseline refers to the BART-large model. MWP-\npretrained Baselineshows the results for Baseline,\nbut further pretrained on MWP. MWP-pretrained\nBaseline + CLT represents the results for the\nMWP-pretrained Baseline, but uses the character-\nlevel representation (CLT) for numbers instead of\nBART’s default tokenisation. Accordingly,MWP-\npretrained Baseline + CLT + CLincorporates the\ncontrastive loss (CL) as additional training signal.\nThe results show that pretraining using math word\nproblems as a first step, in general, improves the\nresults for the Inferable Number Prediction Task,\nbut not over using Arithmetic-Based Pretraining\n491\n(Ours).\nIn case of SciGen, the Inferable Number Predic-\ntion Task, only uses samples with target descrip-\ntions that contain numbers that are inferable from\nthe input table by lookup or arithmetic operations\n(see Section 4.2). Therefore, even though it is a\nsynthetic task, the results give insights on how ef-\nfective pretraining on math word problems is for\nimproving a model’s numeracy.\nH Examples from the Human Evaluation\nFigure 4 shows two sample generations from our\napproach and the BART (Lewis et al., 2020) base-\nline from the SciGen (Moosavi et al., 2021) experi-\nment using the medium split. Both read fluent and\nplausible.\n492\nFigure 4: Generation from our approach and the BART baseline from the SciGen experiment using the medium\nsplit.\n493",
  "topic": "Numeracy",
  "concepts": [
    {
      "name": "Numeracy",
      "score": 0.8054548501968384
    },
    {
      "name": "Computer science",
      "score": 0.6781544089317322
    },
    {
      "name": "Comprehension",
      "score": 0.5194101333618164
    },
    {
      "name": "Task (project management)",
      "score": 0.5045691728591919
    },
    {
      "name": "Arithmetic",
      "score": 0.4991879463195801
    },
    {
      "name": "Natural language processing",
      "score": 0.4661387801170349
    },
    {
      "name": "Inference",
      "score": 0.45065733790397644
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4426997900009155
    },
    {
      "name": "Literacy",
      "score": 0.2401096224784851
    },
    {
      "name": "Mathematics",
      "score": 0.14512813091278076
    },
    {
      "name": "Programming language",
      "score": 0.1336207389831543
    },
    {
      "name": "Psychology",
      "score": 0.1178569495677948
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    }
  ]
}